{
    "paper_title": "Hatevolution: What Static Benchmarks Don't Tell Us",
    "authors": [
        "Chiara Di Bonaventura",
        "Barbara McGillivray",
        "Yulan He",
        "Albert Meroño-Peñuela"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Language changes over time, including in the hate speech domain, which evolves quickly following social dynamics and cultural shifts. While NLP research has investigated the impact of language evolution on model training and has proposed several solutions for it, its impact on model benchmarking remains under-explored. Yet, hate speech benchmarks play a crucial role to ensure model safety. In this paper, we empirically evaluate the robustness of 20 language models across two evolving hate speech experiments, and we show the temporal misalignment between static and time-sensitive evaluations. Our findings call for time-sensitive linguistic benchmarks in order to correctly and reliably evaluate language models in the hate speech domain."
        },
        {
            "title": "Start",
            "content": "Hatevolution: What Static Benchmarks Dont Tell Us Chiara Di Bonaventura1,2, Barbara McGillivray1, Yulan He1, Albert Meroño-Peñuela1 1Kings College London 2Imperial College London chiara.di_bonaventura@kcl.ac.uk 5 2 0 2 3 1 ] . [ 1 8 4 1 2 1 . 6 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Language changes over time, including in the hate speech domain, which evolves quickly following social dynamics and cultural shifts. While NLP research has investigated the impact of language evolution on model training and has proposed several solutions for it, its impact on model benchmarking remains underexplored. Yet, hate speech benchmarks play crucial role to ensure model safety. In this paper, we empirically evaluate the robustness of 20 language models across two evolving hate speech experiments, and we show the temporal misalignment between static and time-sensitive evaluations. Our findings call for time-sensitive linguistic benchmarks in order to correctly and reliably evaluate language models in the hate speech domain."
        },
        {
            "title": "Introduction",
            "content": "Language continuously evolves adapting to social and cultural dynamics (Altmann et al., 2009; Eisenstein et al., 2014; Labov, 2011), e.g., words gain new meanings or lose their existing ones, words shift polarity, and new words emerge. This language evolution challenges NLP models across different domains (Alkhalifa et al., 2023; Luu et al., 2022), with hate speech being one of the most challenging due to the semantic broadening of harmrelated concepts in the past 50 years (Vylomova and Haslam, 2021), frequent changes in words polarity (McGillivray et al., 2022), and reclaimed language (Zsisku et al., 2024). Indeed, Di Bonaventura et al. (2025) recently show that language models distributional knowledge can be enhanced with temporal linguistic knowledge to effectively detect and explain hateful content. While NLP research has extensively investigated the impact of hate speech evolution in model training paradigms, showing that temporal misalignment between training and test sets leads to decreasing performance over time (i.e., temporal bias) across models and languages (Florio et al. 2020; Jin et al. 2023, inter alia), the implications of evolving hate speech in model benchmarking have not been explored. Yet, hate speech benchmarks play crucial role as they are widely embedded in the safety evaluation of language models (e.g., Gehman et al. 2020; Liang et al. 2023; Ying et al. 2024), which are increasingly used in real-world applications and decision making (Bavaresco et al., 2024; Zheng et al., 2023). Although these benchmarks provide comprehensive comparison of language models that would not be possible with held-out test sets, they face the same issue: they are static. In other words, they are grounded to the specific timestamp in which they were developed, and consequently they cannot account for language change. We argue that evolving hate speech plays role in the reliability of static model benchmarking over time, potentially leading to an overestimation of language models safety in light of well-known issues like temporal bias and benchmark saturation (Sainz et al., 2023a,b). Therefore, we seek to answer how does static hate speech benchmarking correlate with evolving language?. By providing empirical evidence of this temporal challenge in model benchmarking, we hope our study will raise awareness in the risks associated with static evaluations of language models, and will fuel research towards time-sensitive evaluations of NLP models in similar way in which studies that investigated the impact of language evolution on model training led to the development of alternative solutions, e.g., temporal attention (Rosin and Radinsky, 2022) or the injection of time-sensitive lexical information (McGillivray et al., 2022)."
        },
        {
            "title": "2 Evolving Hate Speech",
            "content": "To answer our research question, we first design two experiments for evolving hate speech detection accounting for different aspects of language evolution, and we propose two time-sensitive metrics to evaluate language models. Then, we evaluate the same models on static hate speech benchmarks, and we measure the correlation in models ranking across time-sensitive and static evaluations.1 Experiment 1: Time-Sensitive Shifts. We investigate contextual evolution of hate speech, focusing on time-sensitive shifts, such as semantic, topical, and polarity changes. For instance, the word gammon has undergone multiple transformations simultaneously (McGillivray et al., 2022): semantic change from referring to food (ham) to political insult; topic shift towards political discourse; and polarity shift towards negativity. In contrast, certain terms targeting Asian communities predominantly experienced polarity shift, becoming more offensive during the Coronavirus pandemic (Huang et al., 2023). Moreover, timesensitive shifts might manifest as changes in the cultural perception of what is considered offensive, e.g., reclaimed slurs. These time-sensitive shifts are notoriously difficult to disentangle (Luu et al., 2022), and we do not attempt to do so in this work. Instead, we aim to quantify how their complex interplay affects model performance over time, and in turn how this time-sensitive performance correlates with performance on static benchmarks. To study this, we use the English version of the Singapore Online Attack dataset (Haber et al., 2023) as it has the biggest and most recent coverage of annotated texts with timestamp information for hate speech research (i.e., 2011-2022 Reddit posts). We evaluate models with time-sensitive macro F1 defined as 1 t=1 1t, where 1t is the macro-averaged F1 specific to year t. This allows to measure how well language models adapt to evolving contexts of hate speech due to yearly time-sensitive shifts. Ideally, we want language models to exhibit high and stable time-sensitive F1 scores over time. We limit the analysis to 2017-2022 as there were not enough data before 2017. (cid:80)T Experiment 2: Vocabulary Expansion. We examine language expansion, focusing on the emergence of neologisms, i.e., newly coined terms that have entered our vocabulary. To measure model robustness to this type of language evolution, we extend the NeoBench dataset (Zheng et al., 2024) to the task of hate speech detection. Specifically, NeoBench contains pairs of sentences (s1, s2) where s2 differs from s1 by the replacement of target word with neologism while ensuring same part of speech and same meaning of s1. Neologisms are collected between 2020-2023 and account for three types of vocabulary expansion, namely lexical, morphological, and semantic. Lexical neologisms include new words, phrases, and acronyms representing new conceptse.g., long covid. Morphological neologisms instead are words that derive from existing words either through blending or splinteringe.g., doomscrolling. Semantic neologisms refer to existing words with new meaningse.g., ice to indicate petrolor diesel-powered vehicles. We manually annotate the Reddit sample of NeoBench as either hateful or non-hateful, reaching substantial average inter-annotators agreement (Cohens Kappa = 0.67 (Cohen, 1960)) across three annotators. We take the majority vote as groundtruth. As result, we have 341 annotated sentences s1 paired with their 341 counterfactuals s2 containing the neologisms in place of the target words. We evaluate models using counterfactual invariance, i.e., formalization of the requirement that changing irrelevant parts of the input (i.e., replacing target words with neologisms) should not change model predictions (Veitch et al., 2021). We decompose the counterfactual invariance into label flipping (i.e., rate of how often the model flipped the label when seeing the counterfactual s2 wrt s1) and hallucination (i.e., rate of how often the model does not follow the instruction when given the counterfactual s2 but does follow the instruction when given s1). Mathematically, we define label 1(ˆyi(s1) = ˆyi(s2)) and hallucinaflip = 1 1(v(s2,i) = 1 v(s1,i) = 0) where tion = 1 v() is 1 if model hallucinates, 0 otherwise. Ideally, we want language models to be robust against counterfactuals showing low label flip and hallucination rates, paired with high macro F1 score, which highlight their robustness to vocabulary changes and their ability to generalize to new words. (cid:80)N i=1 (cid:80)N i=1 Models. We zero-shot prompt 20 language models widely used in established hate speech and stateof-the-art research  (Table 1)  . We use the verbalisation of Plaza-del arco et al. (2023), which is shown to lead to the best performance in hate speech detection. As baseline, we take the averaged scores of the latest versions of the TimeLMs collection fine-tuned for hate speech detection (Loureiro et al., 2022; Antypas et al., 2023). Cf. App. A. 1The data and code are available at https://github. com/ChiaraDiBonaventura/hatevolution/tree/ main. Static Benchmarks. We select established hate speech benchmarks: HateXplain (Mathew et al., Model Commercial FLAN-Alpaca FLAN-T5 mT0 RoBERTa-dyna-r1 RoBERTa-dyna-r2 RoBERTa-dyna-r3 RoBERTa-dyna-r4 GPT-3.5-turbo GPT-4o Moderation API Perspective API DeepSeek LLM Toxicity finetuned - - - Data cutoff - 2022/11 2022/11 2022/06 2022/06 2022/06 2023/03 2021/09 2023/10 - - - Table 1: Model overview. - if no available info. 2021), Implicit Hate Corpus (ElSherief et al., 2021), HateCheck (Röttger et al., 2021), and Dynabench (Kiela et al., 2021). Their selection is motivated by the fact that each static benchmark captures distinct dimension of hate speech, thereby contributing to more comprehensive assessment. Specifically, we select the HateXplain and Implicit Hate Corpus datasets to account for the dimensions of, respectively, offensiveness and expressiveness of hate speech, as described in Di Bonaventura et al. (2025). We include HateCheck because its construction aligns with the goals of Experiment 2, where models are tested on sentence pairs differing only in the target term. Similarly, HateCheck features sentences that differ only by the targeted group. Finally, we select Dynabench as it is the only dynamic hate speech benchmark, built from adversarial examples collected across multiple rounds over time. Note that the RoBERTadyna-r1/2/3/4 models (Vidgen et al., 2021) in Table 1 have been fine-tuned on four consecutive Dynabench rounds (i.e., dynamic adversarial training), which however increases the risk of creating unrealistic data distributions. Table 2 summarizes the datasets used in our time-sensitive and static evaluations. Dataset Singapore Online Attacks NeoBench HateXplain Implicit Hate Corpus HateCheck Dynabench Size 3000 682 1924 2149 3729 4120 Timestamp info Timestamp period 2017-2022 2020-2023 - - - - Table 2: Dataset overview. - if not applicable."
        },
        {
            "title": "3 Findings",
            "content": "Language models exhibit shortand long-term volatility in hate speech detection across years. Table 3 presents time-sensitive macro F1 by label, and their average in the last column. Although all models have data cutoffs equal to or later than 2021, they fail to generalise well to time-sensitive shifts occurring between 2017 and 2022 as shown by the significant changes in the macro F1 scores year by year for both labels. In addition to this volatile pattern year-by-year, we observe longterm pattern: most language models exhibit decreasing performance in detecting hateful instances and an increasing performance in detecting nonhateful content between 2017 and 2022. For example, mT0-large has macro F1 equal to .5045 and .5455 for hateful and non-hateful labels, respectively, in 2017. By 2022, it has instead .3811 and .6290. As hate speech classifiers suffer from lexical overfit (e.g., Attanasio et al. (2022)), we argue they tend to over-rely on older lexical associations for which there is more evidence in the data (e.g., gammon as ham), and thus fail to recognise newer/emerging associations (e.g., gammon as insult). Clearly, this short-term and long-term volatility of language models in evolving hate speech detection poses real concerns regarding the safety robustness of these models. Interestingly, dynamic adversarial training does not make models more robust to time-sensitive shifts: RoBERTa-dyna-r2/3/4 models which have been fine-tuned on more adversarial examples than RoBERTa-dyna-r1 have lower time-sensitive macro F1 than the latter. This corroborates previous research showing that training on adversarially-collected data for QA tasks was detrimental to performance on non-adversarially collected data (Bartolo et al., 2020). For the other non-adversarially trained models instead, model size improves the overall time-sensitive macro F1 score. The time-sensitive baseline is more robust across years and labels but overall performs similarly to small LLMs and DeepSeek LLM. GPT-4o reaches the highest time-sensitive performance. Language models are sensitive to counterfactuals containing neologisms. Table 4 shows how often models flip the predicted label and generate hallucinations when they see the counterfactual with respect to the reference sentence, and the macro F1 performance in detecting hate speech in those sentences. The label flip rates are surprisingly high, considering that models cutoffs have some overlap with the timeframe from which the neologisms were sampled: 6 out of 20 models flip the label more than 10% of the time.2 Interestingly, counterfactuals have greater impact on making the model change its predicted label than on generating non-response, as evidenced 2We also controlled for time to measure the potential impact of data contamination, and found no evidence (cf. Table A2 and Table A3 in App. C). Model FLAN-Alpaca-base FLAN-Alpaca-large FLAN-Alpaca-xl FLAN-T5-small FLAN-T5-base FLAN-T5-large FLAN-T5-xl mT0-small mT0-base mT0-large mT0-xl RoBERTa-dyna-r1 RoBERTa-dyna-r2 RoBERTa-dyna-r3 RoBERTa-dyna-r4 GPT-3.5-turbo GPT-4o Moderation API Perspective API DeepSeek LLM-7b"
        },
        {
            "title": "TimeLMs",
            "content": "2017 .1111 .6667 .7258 .0 .6557 .7176 .7478 .0435 .0 .5045 .2000 .4211 .3659 .3421 .5057 .6846 .7619 .0645 .4941 .7097 .3620 2018 .1026 .6023 .6327 .0 .5775 .6332 .5969 .0 .0465 .3669 .2718 .3519 .3692 .3571 .3859 .6129 .7129 .0238 .3486 .5000 .3995 2019 .1985 .5733 .6268 .0 .5698 .5946 .6463 .0180 .0559 .4537 .3243 .4255 .3423 .3316 .3902 .5799 .6742 .04120 .4700 .5349 . 2020 .1789 .5383 .5950 .0 .5501 .5472 .5961 .0147 .0697 .3769 .2581 .3864 .3236 .3569 .3724 .5488 .6395 .1275 .4966 .4356 .3621 2021 .1533 .5901 .5896 .0 .5513 .5665 .5909 .0098 .0289 .3746 .2833 .4108 .3645 .3342 .3762 .5590 .6311 .0507 .5098 .4531 .3080 2022 .1148 .5265 .5428 .0 .4991 .5000 .5571 .0 .0359 .3811 .2657 .3322 .3824 .3364 .3652 .5250 .6032 .0631 .4431 .4957 . 2017 .7143 .7132 .7069 .6708 .6441 .6606 .7603 .6716 .6588 .5455 .6706 .7317 .6709 .6951 .7190 .4598 .7368 .6742 .7226 .1818 .3547 2018 .7853 .7222 .6897 .7625 .6722 .6540 .6723 .7679 .7545 .5737 .7692 .7813 .7248 .7722 .7771 .4667 .7434 .7616 .7774 .2667 .3879 2019 .8346 .7486 .7254 .8013 .6917 .6591 .7661 .7798 .7994 .6600 .8056 .8313 .7591 .7969 .7994 .5233 .7542 .8000 .8312 .2308 . 2020 .8347 .7491 .7351 .8118 .7175 .6569 .7530 .8209 .8139 .6094 .8115 .8313 .7716 .8188 .8051 .4973 .7585 .8255 .8080 .1739 .4172 2021 .8397 .7678 .7274 .8203 .7069 .6548 .7472 .8123 .8123 .6392 .8168 .8402 .7846 .8150 .8105 .5389 .7424 .8203 .8492 .2708 .4128 2022 .8364 .7694 .7177 .8278 .6899 .6538 .7631 .8222 .8195 .6290 .8177 .8277 .7726 .8093 .7996 .4861 .7417 .8289 .8374 .2532 . Mean .5176 .6640 .6679 .3912 .6272 .6249 .6831 .3967 .4079 .5095 .5246 .5976 .5526 .5638 .5922 .5402 .7083 .4235 .6348 .3740 .3722 Table 3: Time-sensitive Macro F1 for the hateful label (first block), non-hateful label (second block), and their macro-average (last column). Greener cells indicate higher scores; best score in bold. Std deviations in App. B. by the lower hallucination rates compared to label flips. Moreover, model size lowers the tendency to hallucinate but does not necessarily improve the label flip rate. For instance, FLAN-Alpaca-xl has 0% hallucination vs. 10.88% of FLAN-Alpacalarge but flips the label more frequently (14.14% vs. 3.98%). Similarly, GPT-4o has worse label flip rate than smaller and/or earlier models like RoBERTa-dyna-r2/3/4. One reason for this behaviour may be excessive memorization, which is more likely to occur with larger model sizes (Kiyomaru et al., 2024; Tirumala et al., 2022; Carlini et al.). Consistently with the findings of Experiment 1, RoBERTa-dyna-r2/3/4 are less robust to counterfactuals than RoBERTa-dyna-r1, which has lower label flip rate and higher macro F1 score. Additionally, the TimeLMs baseline is more robust to language evolution, even though most LLMs outperform it in classification performance. With the exception of DeepSeek LLM (which, however, has high hallucination rates; cf. Table A6), label flip rate of 0 occurs when model outputs the same label for all texts; so if we exclude these models, the best one is Perspective API with minimal label flip rate and the highest macro F1. Moreover, we investigate label flip and hallucination rates by type of vocabulary expansion in Table A4 and Table A5, respectively. We found that on average models flip the label more often if the counterfactual sentence contains morphological neologism whereas they tend to hallucinate more often in case of lexical neologism. High scores in static evaluations do not necessarily translate to time-sensitive evaluations. Table 5 shows the Spearmans rank correlation coefficient of models ranking between static and Model FLAN-Alpaca-base FLAN-Alpaca-large FLAN-Alpaca-xl FLAN-T5-small FLAN-T5-base FLAN-T5-large FLAN-T5-xl mT0-small mT0-base mT0-large mT0-xl RoBERTa-dyna-r1 RoBERTa-dyna-r2 RoBERTa-dyna-r3 RoBERTa-dyna-r4 GPT-3.5-turbo GPT-4o Moderation API Perspective API DeepSeek LLM-7b Label Flip (%) 0.65 3.98 14.14 0.00 11.24 15.96 13.99 0.00 0.59 14.12 3.53 3.53 5.88 5.00 6.47 14.93 9.44 0.00 2.94 0.00 Hallucination (%) 3.82 10.88 0.00 2.06 0.00 0.88 0.88 4.41 0.00 0.00 0.00 - - - - 0.88 0.00 - - 1.17 TimeLMs 0.30 - Macro F1 .5189 .5626 .5344 .4851 .4774 .4742 .6002 .4881 .4824 .3383 .5261 .6451 .5931 .5437 .5737 .4885 .6636 .4841 .7067 . .2929 Table 4: Label Flip and Hallucination rates, and Macro F1. Best score in bold. - if not applicable. time-sensitive evaluations, paired with their confidence intervals. These coefficients are computed by comparing the rankings of the best performing models between each possible pair of static and time-sensitive evaluations. We use the rankings on the four benchmarks in Table A7-A10 in App. for the static evaluations whereas we use those in Table 3 and Table 4 for the time-sensitive evaluations. The confidence intervals are computed setting α = 0.10, which means that there is 90% confidence that the intervals contain the true population correlation coefficients between static and time-sensitive evaluations. There is clear misalignment between the two types of evaluations. Overall, there is negative correlation between static evaluations and Experiment 1, indicating that models that perform the best in static benchmarks are not the most robust to time-sensitive shifts. Similarly, high scores in static evaluations do not necessarily imply high scores in Experiment 2, as correlation is on average negative or close to zero. On the other hand, static hate speech benchmarks show positive, non-negligible correlation among each other, with an average correlation coefficient equal to 0.36 (cf. Table A11 and App. D). In other words, while performance on static hate speech benchmark is aligned to the performance on another static benchmark, the same does not hold for time-sensitive evaluations. Evolving hate speech introduces variability that static benchmarks fail to capture, making them an unreliable predictor over time. a S"
        },
        {
            "title": "Implicit Hate",
            "content": "Time-sensitive Experiment 1 -0.2662 (-0.586, 0.126) -0.1549 (-0.504, 0.238) -0.2541 (-0.578, 0.138) -0.2812 (-0.597, 0.110) Experiment 2 -0.0707 (-0.438, 0.317) -0.3053 (-0.613, 0.083) -0.1865 (-0.528, 0.207) 0.1909 (-0.203, 0.532) Table 5: Spearman coefficients between static and timesensitive evaluations. 90% confidence intervals shown below each value. Cf. App. E."
        },
        {
            "title": "4 Related Work",
            "content": "Language evolution and model training. The evolving nature of language has attracted great interest in the NLP community to address the socalled temporal bias, i.e., decreasing performance over time (Alkhalifa et al., 2023), by training models to adapt to newer data (Dhingra et al., 2022; Lazaridou et al., 2021; Röttger and Pierrehumbert, 2021; Jang et al., 2021), historical data (Qiu and Xu, 2022; Martinc et al., 2020), or to be constrained to specific time period (Drinkall et al., 2024). In the hate speech domain, this has led to the proposal of several approaches to train time-sensitive hate speech classifiers like lifelong learning (Qian et al., 2021), time-sensitive knowledge-injection (McGillivray et al., 2022), random vs. chronological data splits (Florio et al., 2020), temporal adaptation (Jin et al., 2023). These studies focus either on BERT-based models or non-neural ones. Instead, we investigate the temporal bias of 20 state-of-theart LLMs in hate speech detection in two scenarios of language evolution. Language evolution and model benchmarking. While the implications of evolving hate speech in model training have been widely investigated, its implications in model benchmarking have been overlooked. This gap is especially important given the rise of LLMs, where hate speech benchmarks are often embedded in safety evaluations (Ying et al., 2024). Remarkably, we provide empirical evidence of the unreliability of static hate speech benchmarks over time due to evolving hate speech, thus calling for time-sensitive linguistic benchmarks in this domain. This type of linguistic benchmarks is scarce as most studies focus on encyclopedic and commonsense knowledge to evaluate models ability to understand factual changes regarding entities and events (e.g., Fatemi et al. (2024); Wang and Zhao (2024); Tan et al. (2023)) rather than language changes. loosely related study is Pozzobon et al. (2023) showing that Perspective API yields unreliable toxicity predictions over time due to model updates. Instead, we measure the implications due to evolving language."
        },
        {
            "title": "5 Conclusions",
            "content": "This study is the first to investigate the impact of evolving language on hate speech benchmarking. We design two time-sensitive experiments and metrics to evaluate 20 language models widely adopted in state-of-the-art research. We found that language models are not robust to evolving hate speech as they exhibit shortand long-term volatility to timesensitive shifts in Experiment 1 and sensitivity to counterfactuals containing neologisms in Experiment 2. Interestingly, dynamic adversarial training does not help models generalise in evolving scenarios. Finally, we provide empirical evidence of the misalignment between static and time-sensitive evaluations, as we found negative or close to zero correlations between the two, which opens up important concerns about the reliability of current hate speech benchmarks in the future. In light of our findings, we advocate for timesensitive linguistic benchmarks to reliably evaluate models safety in the hate speech domain. Examples might include our proposed time-sensitive metrics or more structured approaches similar to those recently developed for evolving encyclopedic knowledge (e.g., Test-of-Time (Fatemi et al., 2024)). Future techniques could explore continual learning to enable LLMs to adapt to evolving hate speech, and context-aware detection to capture subtle shifts in meaning driven by cultural or political events."
        },
        {
            "title": "6 Limitations",
            "content": "We are aware of the following limitations. (1) We recognize hate speech as multilingual problem. However, in this paper we prioritized English because resources for English hate speech are easily available and well-developed, providing strong foundation for our study. Extending to multilingualism is an interesting direction for future work. (2) Although we chose established, well-documented and public datasets for our analyses, hate speech datasets inherently contain bias and noise due to the subjective nature of annotations and the social context in which the data were collected. (3) We consider two aspects of language evolution, namely time-sensitive shifts and vocabulary expansion. We did not disentangle the individual contributions of sub-categories of time-sensitive shifts, such as polarity or topical, since they are notoriously hard to isolate and out of scope for this paper. However, it is an interesting direction for future work. (4) Continuous data collection of social media content is challenge in current research based on social media platforms. This difficulty challenges performing Experiment 1 over time in the future, but it does not impact the ability of carrying out Experiment 2, which instead can be done using established linguistic resources like Oxford English Dictionary, Wiktionary, Urban Dictionary."
        },
        {
            "title": "Acknowledgements",
            "content": "This work was supported by the UK Research and Innovation [grant number EP/S023356/1] in the UKRI Centre for Doctoral Training in Safe and Trusted Artificial Intelligence (www.safeandtrustedai.org). Moreover, CDB work was supported by The Alan Turing Institutes Enrichment Scheme."
        },
        {
            "title": "Author Contribution Statement",
            "content": "Authors contributed to the project as follows. Project Conception: Di Bonaventura, McGillivray, Meroño-Peñuela. Literature Review: Di Bonaventura. Experimental Design: Di Bonaventura. Analysis Advisory: McGillivray, He. Manual Annotation: Di Bonaventura, McGillivray, Meroño-Peñuela. Results and Codebase: Di Bonaventura. Manuscript Writing: Di Bonaventura. Manuscript Editing and Feedback: Everyone."
        },
        {
            "title": "References",
            "content": "Rabab Alkhalifa, Elena Kochkina, and Arkaitz Zubiaga. 2023. Building for tomorrow: Assessing the temporal persistence of text classifiers. Information Processing & Management, 60(2):103200. Eduardo Altmann, Janet Pierrehumbert, and Adilson Motter. 2009. Beyond word frequency: Bursts, lulls, and scaling in the temporal distributions of words. PLOS one, 4(11):e7678. Dimosthenis Antypas, Asahi Ushio, Francesco Barbieri, Leonardo Neves, Kiamehr Rezaee, Luis EspinosaAnke, Jiaxin Pei, and Jose Camacho-Collados. 2023. Supertweeteval: challenging, unified and heterogeneous benchmark for social media nlp research. In Findings of the Association for Computational Linguistics: EMNLP 2023. Giuseppe Attanasio, Debora Nozza, Dirk Hovy, and Elena Baralis. 2022. Entropy-based attention regularization frees unintended bias mitigation from lists. In Findings of the Association for Computational Linguistics: ACL 2022, pages 11051119, Dublin, Ireland. Association for Computational Linguistics. Max Bartolo, Alastair Roberts, Johannes Welbl, Sebastian Riedel, and Pontus Stenetorp. 2020. Beat the ai: Investigating adversarial human annotation for reading comprehension. Transactions of the Association for Computational Linguistics, 8:662678. Anna Bavaresco, Raffaella Bernardi, Leonardo Bertolazzi, Desmond Elliott, Raquel Fernández, Albert Gatt, Esam Ghaleb, Mario Giulianelli, Michael Hanna, Alexander Koller, et al. 2024. Llms instead of human judges? large scale empirical study across 20 nlp evaluation tasks. arXiv preprint arXiv:2406.18403. Rishabh Bhardwaj and Soujanya Poria. 2023. Redteaming large language models using chain of arXiv preprint utterances for safety-alignment. arXiv:2308.09662. Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. Quantifying memorization across neural language models. In The Eleventh International Conference on Learning Representations. Jacob Cohen. 1960. coefficient of agreement for nominal scales. Educational and psychological measurement, 20(1):3746. Bhuwan Dhingra, Jeremy Cole, Julian Martin Eisenschlos, Dan Gillick, Jacob Eisenstein, and William Cohen. 2022. Time-aware language models as temporal knowledge bases. Transactions of the Association for Computational Linguistics, 10:257273. Chiara Di Bonaventura, Lucia Siciliani, Pierpaolo and Barbara Basile, Albert Merono-Penuela, McGillivray. 2025. From detection to explanation: Effective learning strategies for LLMs in abusive language research. In Proceedings of the 31st International Conference on Computational Linguistics: Main Paper, Abu Dhabi, UAE. Felix Drinkall, Eghbal Rahimikia, Janet Pierrehumbert, and Stefan Zohren. 2024. Time machine GPT. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 32813292, Mexico City, Mexico. Association for Computational Linguistics. Jacob Eisenstein, Brendan OConnor, Noah Smith, and Eric Xing. 2014. Diffusion of lexical change in social media. PloS one, 9(11):e113114. Mai ElSherief, Caleb Ziems, David Muchlinski, Vaishnavi Anupindi, Jordyn Seybolt, Munmun De Choudhury, and Diyi Yang. 2021. Latent hatred: benchmark for understanding implicit hate speech. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 345363. Bahare Fatemi, Mehran Kazemi, Anton Tsitsulin, Karishma Malkan, Jinyeong Yim, John Palowitch, Sungyong Seo, Jonathan Halcrow, and Bryan Perozzi. 2024. Test of time: benchmark for evaluating llms on temporal reasoning. arXiv preprint arXiv:2406.09170. Komal Florio, Valerio Basile, Marco Polignano, Pierpaolo Basile, and Viviana Patti. 2020. Time of your hate: The challenge of time in hate speech detection on social media. Applied Sciences, 10(12):4180. Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. 2020. RealToxicityPrompts: Evaluating neural toxic degeneration in language models. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 33563369, Online. Association for Computational Linguistics. Janosch Haber, Bertie Vidgen, Matthew Chapman, Vibhor Agarwal, Roy Ka-Wei Lee, Yong Keong Yap, and Paul Röttger. 2023. Improving the detection of multilingual online attacks with rich social media data from Singapore. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12705 12721, Toronto, Canada. Association for Computational Linguistics. Justin Huang, Masha Krupenkin, David Rothschild, and Julia Lee Cunningham. 2023. The cost of antiasian racism during the covid-19 pandemic. Nature human behaviour, 7(5):682695. Joel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, Stanley Jungkyu Choi, and Minjoon Seo. 2021. Towards continual knowledge learning of language models. arXiv preprint arXiv:2110.03215. Mali Jin, Yida Mu, Diana Maynard, and Kalina Examining temporal bias arXiv preprint Bontcheva. 2023. in abusive language detection. arXiv:2309.14146. Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, et al. 2021. Dynabench: Rethinking benchmarking in nlp. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 41104124. Hirokazu Kiyomaru, Issa Sugiura, Daisuke Kawahara, and Sadao Kurohashi. 2024. comprehensive analysis of memorization in large language models. In Proceedings of the 17th International Natural Language Generation Conference, pages 584596, Tokyo, Japan. Association for Computational Linguistics. William Labov. 2011. Principles of linguistic change, volume 3: Cognitive and cultural factors, volume 3. John Wiley & Sons. Angeliki Lazaridou, Adhi Kuncoro, Elena Gribovskaya, Devang Agrawal, Adam Liska, Tayfun Terzi, Mai Gimenez, Cyprien de Masson dAutume, Tomas Kocisky, Sebastian Ruder, et al. 2021. Mind the gap: Assessing temporal generalization in neural language models. Advances in Neural Information Processing Systems, 34:2934829363. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. 2023. Holistic evaluation of language models. Transactions on Machine Learning Research. Yinhan Liu. 2019. Roberta: robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 364. Daniel Loureiro, Francesco Barbieri, Leonardo Neves, Luis Espinosa Anke, and Jose Camacho-collados. 2022. TimeLMs: Diachronic language models from Twitter. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 251260, Dublin, Ireland. Association for Computational Linguistics. Kelvin Luu, Daniel Khashabi, Suchin Gururangan, Karishma Mandyam, and Noah Smith. 2022. Time waits for no one! analysis and challenges of temIn Proceedings of the 2022 poral misalignment. Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 59445958. Matej Martinc, Petra Kralj Novak, and Senja Pollak. 2020. Leveraging contextual embeddings for detecting diachronic semantic shift. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 48114819, Marseille, France. European Language Resources Association. Binny Mathew, Punyajoy Saha, Seid Muhie Yimam, Chris Biemann, Pawan Goyal, and Animesh Mukherjee. 2021. Hatexplain: benchmark dataset for explainable hate speech detection. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 1486714875. Barbara McGillivray, Malithi Alahapperuma, Jonathan Cook, Chiara Di Bonaventura, Albert MeroñoPeñuela, Gareth Tyson, and Steven Wilson. 2022. Leveraging time-dependent lexical features for offensive language detection. In Proceedings of the First Workshop on Ever Evolving NLP (EvoNLP), pages 3954. Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, Saiful Bari, Sheng Shen, Zheng Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. 2023. Crosslingual generalization through multitask finetuning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1599116111, Toronto, Canada. Association for Computational Linguistics. Flor Miriam Plaza-del arco, Debora Nozza, and Dirk Hovy. 2023. Respectful or toxic? using zero-shot learning with language models to detect hate speech. In The 7th Workshop on Online Abuse and Harms (WOAH), pages 6068, Toronto, Canada. Association for Computational Linguistics. Luiza Pozzobon, Beyza Ermis, Patrick Lewis, and Sara Hooker. 2023. On the challenges of using black-box apis for toxicity evaluation in research. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 75957609. Jing Qian, Hong Wang, Mai ElSherief, and Xifeng Yan. 2021. Lifelong learning of hate speech classificaIn Proceedings of the 2021 tion on social media. Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 23042314. Wenjun Qiu and Yang Xu. 2022. Histbert: pre-trained language model for diachronic lexical semantic analysis. arXiv preprint arXiv:2202.03612. Guy D. Rosin and Kira Radinsky. 2022. Temporal attention for language models. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 14981508, Seattle, United States. Association for Computational Linguistics. Paul Röttger and Janet Pierrehumbert. 2021. Temporal adaptation of BERT and performance on downstream document classification: Insights from social media. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 24002412, Punta Cana, Dominican Republic. Association for Computational Linguistics. Paul Röttger, Bertie Vidgen, Dong Nguyen, Zeerak Waseem, Helen Margetts, and Janet Pierrehumbert. 2021. Hatecheck: Functional tests for hate speech detection models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4158. Oscar Sainz, Jon Campos, Iker García-Ferrero, Julen Etxaniz, Oier Lopez de Lacalle, and Eneko Agirre. 2023a. NLP evaluation in trouble: On the need to measure LLM data contamination for each benchIn Findings of the Association for Compumark. tational Linguistics: EMNLP 2023, pages 10776 10787, Singapore. Association for Computational Linguistics. Oscar Sainz, Jon Ander Campos, Iker García-Ferrero, Julen Etxaniz, and Eneko Agirre. 2023b. Did chatgpt cheat on your test? Qingyu Tan, Hwee Tou Ng, and Lidong Bing. 2023. Towards benchmarking and improving the temporal reasoning capability of large language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1482014835, Toronto, Canada. Association for Computational Linguistics. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https:// github.com/tatsu-lab/stanford_alpaca. Kushal Tirumala, Aram Markosyan, Luke Zettlemoyer, and Armen Aghajanyan. 2022. Memorization without overfitting: Analyzing the training dynamics of large language models. Advances in Neural Information Processing Systems, 35:3827438290. Victor Veitch, Alexander D' Amour, Steve Yadlowsky, and Jacob Eisenstein. 2021. Counterfactual invariance to spurious correlations in text classification. In Advances in Neural Information Processing Systems, volume 34, pages 1619616208. Curran Associates, Inc. Bertie Vidgen, Tristan Thrush, Zeerak Waseem, and Douwe Kiela. 2021. Learning from the worst: Dynamically generated datasets to improve online hate detection. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 16671682, Online. Association for Computational Linguistics. Ekaterina Vylomova and Nick Haslam. 2021. Semantic changes in harm-related concepts in english. Computational approaches to semantic change, 6:93. Yuqing Wang and Yun Zhao. 2024. TRAM: Benchmarking temporal reasoning for large language models. In Findings of the Association for Computational Linguistics: ACL 2024, pages 63896415, Bangkok, Thailand. Association for Computational Linguistics. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Dai, and Quoc Le. 2021. Finetuned language models are zero-shot learners. In International Conference on Learning Representations. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mt5: massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483498. Zonghao Ying, Aishan Liu, Siyuan Liang, Lei Huang, Jinyang Guo, Wenbo Zhou, Xianglong Liu, and Dacheng Tao. 2024. Safebench: safety evaluation framework for multimodal large language models. arXiv preprint arXiv:2410.18927. Jonathan Zheng, Alan Ritter, and Wei Xu. 2024. NEOBENCH: Evaluating robustness of large language models with neologisms. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13885 13906, Bangkok, Thailand. Association for Computational Linguistics. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623. Eszter Zsisku, Arkaitz Zubiaga, and Haim Dubossarsky. 2024. Hate speech detection and reclaimed language: Mitigating false positives and compounded discrimination. In Proceedings of the 16th ACM Web Science Conference, pages 241249."
        },
        {
            "title": "A Ethical NLP Research",
            "content": "Data. We use publicly available datasets for our experiments, which ensure anonymized content. The use of these datasets is consistent with their terms for use and intended use. They only cover English. For Experiment 1 and 2, the size of the data used were 3000 and 682, respectively. The size of the static hate speech datasets are: 3729 (HateCheck), 1924 (HateXplain), 4120 (Dynabench), and 2149 (Implicit Hate). We use the test sets. Models. For our experiments, we choose widely used language models for hate speech research, considering variety of characteristics like opensource vs. commercial models, encoder-decoder vs. decoder-only models, previously toxicity finetuned vs. not previously toxicity fine-tuned, and with different training data cutoff dates. Next, we briefly describe each model we analysed: FLAN-Alpaca (Bhardwaj and Poria, 2023): an instruction-tuned derivative of FLAN-T5, further instruction fine-tuned on Alpaca (Taori et al., 2023) dataset. It was previously finetuned for toxicity detection. FLAN-T5 (Wei et al., 2021): an instruction fine-tuned derivative of T5 (Xue et al., 2021) using the dataset FLAN (Wei et al., 2021). It was previously toxicity finetuned. mT0 (Muennighoff et al., 2023): an instruction fine-tuned derivative of mT5 (Xue et al., 2021) finetuned on xP3 dataset (Muennighoff et al., 2023). RoBERTa-dyna-r1/2/3/4 (Vidgen et al., 2021): iterative versions of RoBERTa (Liu, 2019) fine-tuned dynamically on increasingly refined training data from Dynabench (Kiela et al., 2021). GPT-3.5-turbo3: cost-efficient, highly optimized version of OpenAIs GPT-3.5. GPT-4o4: specialized variant of OpenAIs GPT-4 designed for optimized performance. Moderation API5: the OpenAI tool to identify potentially harmful content in text and images. We use the text-moderation-latest as underlying model. Perspective API6: machine learning-based API by Google that scores text for perceived toxicity. DeepSeek LLM7: 7B language model with multi-head attention trained on 2 trillion tokens from scratch in English and Chinese, developed by DeepSeek. TimeLMs (Loureiro et al., 2022; Antypas et al., 2023): time-sensitive language model, based on the RoBERTa architecture, for NLP research in the social media domain. It is continuously trained on social media over regular intervals for researching language model degradation, as well as cultural shifts affecting language usage on social media. We use the 3https://platform.openai.com/docs/models/ gp#gpt-3-5-turbo 4https://platform.openai.com/docs/models# gpt-4o 5https://platform.openai.com/docs/guides/ moderation/overview 6https://perspectiveapi.com/ 7https://huggingface.co/deepseek-ai/ deepseek-llm-7b-base versions finetuned for hate speech detection as baseline. Experimental Setup. In all our analyses, we use zero-shot prompting. For each model, we use the default set of parameters; we refer the reader to the original papers for more details. The prompts are designed following the instructions used in Plazadel arco et al. (2023), which are shown to be the most suitable for hate speech detection. Specifically, the instruction passed to the models is Classify the input text as respectful or toxic.. Then, for the computations of the macro F1 we disregard outputs that were not as expected, i.e., that did not follow the instruction of answering with one word either toxic or respectful. The xl sizes of the models were loaded using 8bit quantization. We will release the code upon acceptance of the paper. Manual Annotation. Three authors of this paper were involved in the manual annotation of the Reddit sample of NeoBench. Annotators are AI researchers, familiar with the domain of hate speech, and with English language. They were presented sentences and asked to annotate whether the sentence was hateful or non-hateful. We take the majority vote as groundtruth. Experiment 1 Following, we report additional results for Experiment 1. Specifically, Table A1 shows the standard deviation of macro F1 for the hateful and nonhateful label over time. Model FLAN-Alpaca-base FLAN-Alpaca-large FLAN-Alpaca-xl FLAN-T5-small FLAN-T5-base FLAN-T5-large FLAN-T5-xl mT0-small mT0-base mT0-large mT0-xl RoBERTa-dyna-r1 RoBERTa-dyna-r2 RoBERTa-dyna-r3 RoBERTa-dyna-r4 GPT-3.5-turbo GPT-4o Moderation API Perspective API DeepSeek LLM-7b Std dev hateful 0.0363 0.0460 0.0561 0.00 0.0468 0.0690 0.0618 0.0147 0.0220 0.0514 0.0368 0.0352 0.0194 0.0104 0.0483 0.0522 0.0536 0.0323 0.0544 0.0902 Std dev non-hateful 0.0457 0.0211 0.0150 0.0541 0.0239 0.0026 0.0325 0.0522 0.0568 0.0392 0.0524 0.0388 0.0389 0.0429 0.0314 0.0285 0.0076 0.0546 0.0451 0."
        },
        {
            "title": "TimeLMs",
            "content": "0.0302 0.0222 Table A1: Standard deviation of macro F1 for hateful and non-hateful label over time. Experiment 2 In Table A2 and Table A3, we measure the same metrics of Table 4 while controlling for time. Since the NeoBench dataset provides timestamps for each pair (s1, s2) marking the emergence of the neologism, we verified that label flip and hallucination rates remain comparable across years. This helps address concerns about potential data contamination, which would likely have resulted in peak of these metrics in later years due to the partial overlap between the neologisms timeframe and the models training cutoff dates. Our analysis found no evidence of such contamination, as the metrics remain overall stable across different years. Nevertheless, data contamination remains general challenge in NLP research, and it is difficult to rule out entirely due to the lack of transparency regarding most models training data. Results are shown in Table A2 and Table A3 for label flip and hallucination rates, respectively. For this computation, we ruled out pairs whose timestamp information was missing in NeoBench. Model FLAN-Alpaca-base FLAN-Alpaca-large FLAN-Alpaca-xl FLAN-T5-small FLAN-T5-base FLAN-T5-large FLAN-T5-xl mT0-small mT0-base mT0-large mT0-xl RoBERTa-dyna-r1 RoBERTa-dyna-r2 RoBERTa-dyna-r3 RoBERTa-dyna-r4 GPT-3.5-turbo GPT-4o Moderation API Perspective API DeepSeek LLM-7b 2020 0.00 6.58 13.21 0.00 12.27 11.43 13.33 0.00 1.89 18.89 6.60 2.83 7.55 6.60 9.43 12.38 10.38 0.00 1.89 - 2021 0.00 0.00 15.56 0.00 8.89 19.32 16.09 0.00 0.00 11.11 2.22 3.33 7.78 5.56 5.56 21.84 7.78 0.00 3.33 0.00 2022 2.50 4.92 16.67 0.00 14.45 20.96 13.33 0.00 0.00 13.33 3.33 2.22 4.44 2.22 3.33 12.36 8.99 0.00 3.33 - 2023 0.00 0.00 0.00 0.00 0.00 0.00 12.50 0.00 0.00 12.50 0.00 12.50 0.00 0.00 0.00 12.50 0.00 0.00 12.50 0.00 Table A2: Label Flip Rates (in %) by year. - if not applicable as the model did not generate any outputs as expected. Model FLAN-Alpaca-base FLAN-Alpaca-large FLAN-Alpaca-xl FLAN-T5-small FLAN-T5-base FLAN-T5-large FLAN-T5-xl mT0-small mT0-base mT0-large mT0-xl RoBERTa-dyna-r1 RoBERTa-dyna-r2 RoBERTa-dyna-r3 RoBERTa-dyna-r4 GPT-3.5-turbo GPT-4o Moderation API Perspective API DeepSeek LLM-7b 2020 4.72 7.55 0.00 0.94 0.00 0.00 0.94 9.43 0.00 0.00 0.00 - - - - 0.00 0.00 - - 0.94 2021 2.22 14.44 0.00 5.56 0.00 0.00 2.22 1.11 0.00 0.00 0.00 - - - - 2.22 0.00 - - 0.00 2022 4.44 10.00 0.00 1.11 0.00 2.22 0.00 1.11 0.00 0.00 0.00 - - - - 1.11 0.00 - - 2.22 2023 0.00 25.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 - - - - 0.00 0.00 - - 0.00 Following, we report additional results for Experiment 2. Table A3: Hallucination Rates (in %) by year. - if not applicable as models are non-generative. Moreover, we compute label flip and hallucination rates in Experiment 2 by type of vocabulary expansion. Specifically, Table A4 contains label flip rates whereas Table A5 contains hallucination rates. From one hand, models on average flip the label more often if the counterfactual sentence contains morphological vocabulary expansion (average label flip rate equal to 6.54%) rather than lexical (6.40%) or semantic ones (5.34%). On the other hand, models tend to hallucinate more often in cases of lexical vocabulary expansion (average hallucination rate equal to 2.12%) rather than morphological (1.58%) and semantic ones (1.82%). tion any answer given by the model which does not follow the instruction given in the prompte.g., when the model repeats the instruction without providing any answer regarding the classification. Results are shown in Table A6. Overall, hallucination rates are surprisingly high: 5 out of 14 models8 hallucinate more than 10% of the time on either reference or counterfactual sentences as shown in the first column. This hallucination is mostly driven by the presence of counterfactual sentences, as shown in the last column. In particular, DeepSeek LLM shows incredibly high hallucination rates compared to the other language models. Model FLAN-Alpaca-base FLAN-Alpaca-large FLAN-Alpaca-xl FLAN-T5-small FLAN-T5-base FLAN-T5-large FLAN-T5-xl mT0-small mT0-base mT0-large mT0-xl RoBERTa-dyna-r1 RoBERTa-dyna-r2 RoBERTa-dyna-r3 RoBERTa-dyna-r4 GPT-3.5-turbo GPT-4o Moderation API Perspective API DeepSeek LLM-7b Lexical 2.06 0.00 14.81 0.00 10.38 15.24 22.22 0.00 0.00 13.89 3.70 5.56 7.41 5.56 6.48 12.38 6.48 0.00 1.85 0.00 Morphological 0.00 6.20 15.68 0.00 10.81 18.68 11.00 0.00 0.54 14.59 3.24 2.70 4.86 4.86 5.95 16.94 11.41 0.00 3.24 0.00 Semantic 0.00 4.17 8.51 0.00 14.89 6.67 6.52 0.00 2.13 12.77 4.26 2.13 6.38 4.26 8.51 12.77 8.51 0.00 4.26 0. Model FLAN-Alpaca-base FLAN-Alpaca-large FLAN-Alpaca-xl FLAN-T5-small FLAN-T5-base FLAN-T5-large FLAN-T5-xl mT0-small mT0-base mT0-large mT0-xl RoBERTa-dyna-r1 RoBERTa-dyna-r2 RoBERTa-dyna-r3 RoBERTa-dyna-r4 GPT-3.5-turbo GPT-4o Moderation API Perspective API DeepSeek LLM-7b Hals1,s2 (%) 10.00 33.53 0.00 10.59 0.59 2.35 1.18 34.71 0.29 0.00 0.00 - - - - 1.47 0.29 - - 98.82 Hals2 (%) 8.24 25.29 0.00 6.47 0.00 0.88 0.88 28.24 0.29 0.00 0.00 - - - - 0.88 0.00 - - 97.65 Table A4: Label Flip Rates (in %) by type of vocabulary expansion. - if not applicable as the model did not generate any outputs as expected. Table A6: Hallucination rates.- if not applicable as models are non-generative. Model FLAN-Alpaca-base FLAN-Alpaca-large FLAN-Alpaca-xl FLAN-T5-small FLAN-T5-base FLAN-T5-large FLAN-T5-xl mT0-small mT0-base mT0-large mT0-xl RoBERTa-dyna-r1 RoBERTa-dyna-r2 RoBERTa-dyna-r3 RoBERTa-dyna-r4 GPT-3.5-turbo GPT-4o Moderation API Perspective API DeepSeek LLM-7b Lexical 4.63 10.19 0.00 2.78 0.00 1.85 0.00 5.56 0.00 0.00 0.00 - - - - 1.85 0.00 - - 2.78 Morphological 2.70 11.89 0.00 1.08 0.00 0.00 1.08 4.32 0.00 0.00 0.00 - - - - 0.54 0.00 - - 0.54 Semantic 6.38 8.51 0.00 4.26 0.00 2.13 2.13 2.13 0.00 0.00 0.00 - - - - 0.00 0.00 - - 0.00 Table A5: Hallucination Rates (in %) by type of vocabulary expansion. - if not applicable as the model are non-generative. In addition to the hallucination rates shown in Table 4, we compute hallucination rates considering reference and counterfactual sentences, and only counterfactual sentences. Mathematically, we de1(v(s2,i) = fine the former as hals1,s2 = 1 1 v(s1,i) = 1) and the latter as hals2 = 1(v(s2,i) = 1). We consider hallucina1 (cid:80)N (cid:80)N i=1 i="
        },
        {
            "title": "D Benchmarks Results",
            "content": "We prompt language models on four established hate speech benchmarks for binary hate speech detection using the same instructions as in Plaza-del arco et al. (2023). In Table A7, Table A8, Table A9, and Table A10, we report macro F1 scores and the percentage of outputs that followed the instruction as expected for each benchmark. Interestingly, DeepSeek LLM shows incredibly low percentages of expected outputs. Moreover, we report the Spearmans rank correlation coefficients across static hate speech benchmarks in Table A11. Overall, the rankings of the models exhibit positive, non-negligible correlation even though each static benchmark focuses on specific characteristic of hate speech, namely offensiveness for HateXplain, expressiveness for Implicit Hate, target-based functionality tests for HateCheck, and adversarial examples for Dynabench. The highest correlation of models ranking is between Dynabench and HateXplain benchmarks with an average coefficient 8Non-generative models are disregarded in this computation. equal to 0.8647. The lowest correlation instead is between HateXplain and Implicit Hate, which however is expected as they measure two very different aspects of hate speech, namely offensiveness and expressiveness (Di Bonaventura et al., 2025). The average correlation coefficient among all pairs of static evaluations is 0.36 (i.e., 1 6 (0.3865 + 0.2361 + 0.3203 + 0.8647 + 0.2421 + 0.0917)). Model FLAN-Alpaca-base FLAN-Alpaca-large FLAN-Alpaca-xl FLAN-T5-small FLAN-T5-base FLAN-T5-large FLAN-T5-xl mT0-small mT0-base mT0-large mT0-xl RoBERTa-dyna-r1 RoBERTa-dyna-r2 RoBERTa-dyna-r3 RoBERTa-dyna-r4 GPT-3.5-turbo GPT-4o Moderation API Perspective API DeepSeek LLM-7b Macro F1 .3739 .7094 .7348 .2322 .6023 .6909 .7383 .2747 .2472 .6103 .4779 .6235 .8299 .9207 .9485 .7135 .7394 .5142 .7489 . Expected Output (%) 95.95 100.00 100.00 91.34 99.97 99.30 99.79 25.78 99.92 99.22 100.00 100.00 100.00 100.00 100.00 99.65 100.00 100.00 100.00 0.54 Table A7: Macro F1 and Expected Output rate on HateCheck benchmark. Model FLAN-Alpaca-base FLAN-Alpaca-large FLAN-Alpaca-xl FLAN-T5-small FLAN-T5-base FLAN-T5-large FLAN-T5-xl mT0-small mT0-base mT0-large mT0-xl RoBERTa-dyna-r1 RoBERTa-dyna-r2 RoBERTa-dyna-r3 RoBERTa-dyna-r4 GPT-3.5-turbo GPT-4o Moderation API Perspective API DeepSeek LLM-7b Macro F1 .3389 .5319 .5744 .3067 .4971 .5220 .5855 .3215 .3309 .5252 .4381 .5829 .7022 .8120 .8104 .5045 .5728 .4219 .5255 .4203 Expected Output (%) 87.01 99.98 100.00 88.90 99.57 98.58 99.73 68.65 99.03 99.18 100.00 100.00 100.00 100.00 100.00 99.48 99.47 99.96 100.00 7.86 Table A8: Macro F1 and Expected Output rate on Dynabench benchmark."
        },
        {
            "title": "E Correlation Analysis",
            "content": "We use the Spearmans rank correlation to measure the strength and direction of association between static and time-sensitive evaluations. The Spearmans rank correlation coefficient can take value from +1 to -1 where value of +1 means perfect positive correlation, value of 0 means no correlation, and value of -1 means perfect negative association of rank. In addition to the correlation coefficients shown in Table 5 of the main paper, we report their confidence intervals in Table A12 below. These confidence intervals (clower, cupper) Model FLAN-Alpaca-base FLAN-Alpaca-large FLAN-Alpaca-xl FLAN-T5-small FLAN-T5-base FLAN-T5-large FLAN-T5-xl mT0-small mT0-base mT0-large mT0-xl RoBERTa-dyna-r1 RoBERTa-dyna-r2 RoBERTa-dyna-r3 RoBERTa-dyna-r4 GPT-3.5-turbo GPT-4o Moderation API Perspective API DeepSeek LLM-7b Macro F1 .4333 .6015 .6827 .2895 .5704 .5479 .7201 .2844 .3419 .4928 .4829 .6989 .6989 .7096 .7077 .4539 .5732 .5055 .6621 .4266 Expected Output (%) 93.34 100.00 100.00 98.34 99.69 99.01 100.00 65.23 98.23 99.48 100.00 100.00 100.00 100.00 100.00 99.58 99.38 100.00 100.00 10.01 Table A9: Macro F1 and Expected Output rate on HateXplain benchmark. Model FLAN-Alpaca-base FLAN-Alpaca-large FLAN-Alpaca-xl FLAN-T5-small FLAN-T5-base FLAN-T5-large FLAN-T5-xl mT0-small mT0-base mT0-large mT0-xl RoBERTa-dyna-r1 RoBERTa-dyna-r2 RoBERTa-dyna-r3 RoBERTa-dyna-r4 GPT-3.5-turbo GPT-4o Moderation API Perspective API DeepSeek LLM-7b Macro F1 .4091 .5625 .6167 .3870 .5334 .4995 .6215 .3896 .4022 .4673 .4073 .6146 .6377 .6184 .6491 .3718 .4815 .4009 .6017 .4590 Expected Output (%) 93.53 100.00 100.00 97.49 99.30 99.12 100.00 47.25 94.09 96.65 100.00 100.00 100.00 100.00 100.00 99.39 99.58 99.95 100.00 2.75 Table A10: Macro F1 and Expected Output rate on Implicit Hate benchmark. HateCheck Dynabench HateXplain Implicit Hate HateCheck 1. - - - Dynabench 0.3865 1. - - HateXplain 0.2361 0.8647 1. - Implicit Hate 0.3203 0.2421 0.0917 1. Table A11: Spearmans rank correlation coefficient across static hate speech benchmarks. are computed as follows. where clower = cupper = e2L 1 e2L + 1 e2U 1 e2U + 1 = = + Z1α/2 Z1α/2 3 = 1 2 ln( 1 + ρ 1 ρ ) with significance level α = 0.10, sample size = 20, and Spearmans rank correlation coefficient ρ being the ones in Table 5. The results can be interpreted as there is 90% chance that the confidence intervals shown below contain the true population correlation coefficient between static and time-sensitive evaluations of language models. Overall, these intervals suggest negative or negligible correlation between static and time-sensitive rankings, with skewed tendency toward negative correlations. Note that sample size affects this estimate and that larger sample could provide more precise assessment. Moreover, we report the confidence intervals of the correlation coefficients of models ranking among static evaluations in Table A13. Static / Time-sensitive Experiment 1 HateCheck (-0.586, 0.126) Dynabench (-0.504, 0.238) HateXplain (-0.578, 0.138) Implicit Hate (-0.597, 0.110) Experiment 2 (-0.438, 0.317) (-0.613, 0.083) (-0.528, 0.207) (-0.203, 0.532) Table A12: Confidence intervals of Spearmans rank correlation coefficient between static and time-sensitive evaluations. Static / Static HateCheck Dynabench HateXplain Implicit Hate Dynabench (0.009, 0.668) - - - HateXplain (-0.157, 0.565) (0.722, 0.937) - - Implicit Hate (-0.067, 0.624) (-0.151, 0.569) (-0.298, 0.455) - Table A13: Confidence intervals of Spearmans rank correlation coefficient between static evaluations."
        }
    ],
    "affiliations": [
        "Imperial College London",
        "Kings College London"
    ]
}