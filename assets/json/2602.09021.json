{
    "paper_title": "$χ_{0}$: Resource-Aware Robust Manipulation via Taming Distributional Inconsistencies",
    "authors": [
        "Checheng Yu",
        "Chonghao Sima",
        "Gangcheng Jiang",
        "Hai Zhang",
        "Haoguang Mai",
        "Hongyang Li",
        "Huijie Wang",
        "Jin Chen",
        "Kaiyang Wu",
        "Li Chen",
        "Lirui Zhao",
        "Modi Shi",
        "Ping Luo",
        "Qingwen Bu",
        "Shijia Peng",
        "Tianyu Li",
        "Yibo Yuan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "High-reliability long-horizon robotic manipulation has traditionally relied on large-scale data and compute to understand complex real-world dynamics. However, we identify that the primary bottleneck to real-world robustness is not resource scale alone, but the distributional shift among the human demonstration distribution, the inductive bias learned by the policy, and the test-time execution distribution -- a systematic inconsistency that causes compounding errors in multi-stage tasks. To mitigate these inconsistencies, we propose $χ_{0}$, a resource-efficient framework with effective modules designated to achieve production-level robustness in robotic manipulation. Our approach builds off three technical pillars: (i) Model Arithmetic, a weight-space merging strategy that efficiently soaks up diverse distributions of different demonstrations, varying from object appearance to state variations; (ii) Stage Advantage, a stage-aware advantage estimator that provides stable, dense progress signals, overcoming the numerical instability of prior non-stage approaches; and (iii) Train-Deploy Alignment, which bridges the distribution gap via spatio-temporal augmentation, heuristic DAgger corrections, and temporal chunk-wise smoothing. $χ_{0}$ enables two sets of dual-arm robots to collaboratively orchestrate long-horizon garment manipulation, spanning tasks from flattening, folding, to hanging different clothes. Our method exhibits high-reliability autonomy; we are able to run the system from arbitrary initial state for consecutive 24 hours non-stop. Experiments validate that $χ_{0}$ surpasses the state-of-the-art $π_{0.5}$ in success rate by nearly 250%, with only 20-hour data and 8 A100 GPUs. Code, data and models will be released to facilitate the community."
        },
        {
            "title": "Start",
            "content": "χ0: Resource-Aware Robust Manipulation via Taming Distributional Inconsistencies Checheng Yu, Chonghao Sima, Gangcheng Jiang, Hai Zhang, Haoguang Mai, Hongyang Li, Huijie Wang, Jin Chen, Kaiyang Wu, Li Chen, Lirui Zhao, Modi Shi, Ping Luo, Qingwen Bu, Shijia Peng, Tianyu Li, Yibo Yuan Kinetix AI Code: https://github.com/OpenDriveLab/kai0 Blog: https://mmlab.hk/research/kai0 6 2 0 2 9 ] . [ 1 1 2 0 9 0 . 2 0 6 2 : r Fig. 1: Top: System overview. robot teamwork system with two dual-arm ALOHA robots performing long-horizon collaborative garment manipulation, including flattening, folding and hanging. Bottom: Technical philosophy and performance. Distributional inconsistencies are inherent to robot learning (Ptrain: expert demonstrations; Qmodel: policy inductive bias; Ptest: deployment trajectories). χ0 systematically resolves these pairwise mismatches: Model Arithmetic aligns Qmodel with Ptrain; Train-Deploy Alignment bridges Ptrain and Ptest; and Stage Advantage optimizes Qmodel for Ptest. The contributions of these modules collectively enable χ0 to surpass the baseline π0.5 [3] in terms of success rate by approximately 250%. AbstractHigh-reliability long-horizon robotic manipulation has traditionally relied on large-scale data and compute to understand complex real-world dynamics. However, we identify that the primary bottleneck to real-world robustness is not resource scale alone, but the distributional shift among the human demonstration distribution, the inductive bias learned by the policy, and the test-time execution distributiona systematic inconsistency that causes compounding errors in multi-stage tasks. To mitigate these inconsistencies, we propose χ0, resource-efficient framework with effective modules designated to achieve production-level robustness in robotic manipulation. Our approach builds off three technical pillars: (i) Model Arithmetic, weight-space merging strategy that efficiently soaks up diverse distributions of different demonstrations, varying from object appearance to state variations; (ii) Stage Advantage, stage-aware advantage estimator that provides stable, dense progress signals, overcoming the numerical instability of prior non-stage approaches; and (iii) Authors listed alphabetically by first name. Contect: chonghaosima@connect.hku.hk Train-Deploy Alignment, which bridges the distribution gap via spatio-temporal augmentation, heuristic DAgger corrections, and temporal chunk-wise smoothing. χ0 enables two sets of dualarm robots to collaboratively orchestrate long-horizon garment manipulation, spanning tasks from flattening, folding, to hanging different clothes. Our method exhibits high-reliability autonomy; we are able to run the system from arbitrary initial state for consecutive 24 hours non-stop. Experiments validate that χ0 surpasses the state-of-the-art π0.5 in success rate by nearly 250%, with only 20-hour data and 8 A100 GPUs. Code, data and models will be released to facilitate the community. I. INTRODUCTION Achieving production-ready robustness stands as the grand challenge of modern robotics. While autonomous vehicles, as specific form of navigation robot, have successfully demonstrated operational viability in complex urban environments [39, 77, 81, 26, 71, 72], replicating this level of reliability for robotic manipulation in unstructured settings and garnering significant degree of human trust still remain as an open challenge. Achieving such robustness requires policy to operate within vast search space, handling the tremendous environmental variability inherent to the physical world. Therefore, the prevailing industrial paradigm has pivoted toward scaling approach, leveraging massive computational resources to scale foundation models [4, 3, 2, 22, 73, 96, 50]. However, while architectural evolution and resource scaling are essential, we argue that the decisive factor in robust policy execution is not scale alone. Our key insight is that within the expansive search space in real world for the policy, the hidden devil hindering robustness is the inconsistency among the distributions governing the three pillars of the robot learning: data collection, model training and policy deployment. These inconsistencies are not evidently reflected in success rates, but rather in the smoothness of execution, system throughput, and the retry cost required for successful task completion [13, 8, 69, 2, 31]. We encapsulate the robot learning pipeline into three distinct distributions spanning the entire research cycle to formalize further analysis Ptrain, the distribution of human expert demonstrations used for training an imitation policy; Qmodel, the distribution of the inductive bias learned by the policy. state-to-plausible-actions mapper; Ptest, the distribution of executed action trajectories during real-robot deployment. It differs from the policy output action with certain delay and physical limitation; Massive real-world deployment of learned policies reveals three systematic inconsistencies within this regime, as shown in Figure 1. First, due to the extreme high dimensionality of the task, Ptrain is inherently sparse relative to the full solution manifold, resulting in Qmodel that is heavily biased toward the limited training distribution. Second, the latency between model inference (Qmodel) and control-level execution (Ptest) introduces temporal mismatch, rendering theoretically optimal plans suboptimal during inference [5, 76]. Third, despite frequent failures during inference, the policy lacks failure recovery ability; even when encountering states within Ptrain, minor perturbations in Ptest can trigger catastrophic divergence from which the system cannot recover [27, 59, 2]. Prior literature addresses these observed inconsistencies via strategies such as dataset scaling [83, 25, 61], heuristic or learned augmentation [83, 18, 17], and adaptive learning [83, 48, 64]. However, the off-the-shelf application of these general-purpose methods to robotic manipulation is hindered by domain-specific constraints: the prohibitive cost of collecting expert demonstrations, significant inference-toexecution latency, and the computational burden of training large-scale models. To bridge this gap, we propose χ0, holistic framework designed to systematically resolve these distributional misalignments within the constraints of physical robotics. Our approach builds off three technical pillars that ease the inconsistencies one by one: (a) Model Arithmetic (MA): MA is to align varying data subsets (Ptrain) with the policys inductive bias (Qmodel). This approach enables the policy to efficiently soak up diverse Ptrain distributions by simply merging weight of checkpoints trained on different Ptrain. As shown in Figure 1, MA enables Qmodel to capture modes within Ptrain that were previously omitted. (b) Stage Advantage (SA): To optimize action sampling (Qmodel) under the novel deployment environment (Ptest), SA decomposes long-horizon tasks into semantic subgoals (referred as stages), providing stable, stageaware reward signals for advantage-weighted behavior cloning [60]. Figure 1 demonstrates the idea that SA allows Qmodels to sample actions in the mode that is closer to Ptest. Furthermore, SA mitigates the numerical instability inherent in prior non-stage methods like π 0.6 [2] via frame-wise reward modeling. (c) Train-Deploy-Alignment (TDA): TDA expands Ptrain toward Ptest via heuristic DAgger and spatio-temporal augmentation, ensuring robustness against real-world distributional drift. We further propose temporal chunkwise smoothing to mitigate inference-actuation latency and enhance real-time control stability, surpassing RTConly method [5, 76] in terms of policy throughput and retry cost. We illustrate this idea in Figure 1, that Ptrain achieves improved coverage of the modes within Ptest. We evaluate χ0 on collaborative long-horizon garment manipulation tasks such as flattening, folding, and hanging different clothes, as contact-rich, deformable dynamics of clothes and recovery from arbitrary cloth states magnify the distributional shifts aforementioned. Training with just 20 hours of demonstrations on 8A100 GPUs, χ0 outperforms the open-source π0.5 baseline by nearly 250% in success rate. Our extensive experiments empirically align with our insight: 1) MA provides resource-efficient mechanism to enhance policy performance across nearly all metrics; we find that validation loss on DAgger data serves as an effective heuristic for weighting several checkpoints; 2) In TDA, the DAgger data proves critical for maximizing success rates, albeit at the expense of increased retry costs. This trade-off is consistent with intuition: DAgger samples are most valuable in recovery scenarios, implying that higher retry frequency positively correlates with ultimate task success. We also observe that spatiotemporal augmentation is effective only when paired with control optimization, where our proposed temporal chunk-wise smoothing operates orthogonally to RTC [5]; 3) Regarding SA, our stage-aware 2-timestep advantage signal exhibits superior numerical stability compared to π 0.6-style advantage training [2], stability that empirically translates to improved overall performance. All these contributions sustain livestream of autonomous operation in 24-hour real-robot stress test, provided in the Appendix. We will release data, code, and model weights to facilitate the community. II. RELATED WORK A. Imitation Learning and Policy Deployment in Real-world Imitation learning has become dominant paradigm for transformerrobot manipulation, scaling from lightweight Fig. 2: Pipeline of χ0. Our framework addresses distributional inconsistencies across three stages. (Left) Ptrain: heuristic DAgger and spatio-temporal augmentation expand training coverage, with Stage Annotation for advantage estimation; (Middle) Qmodel: Model Arithmetic merges complementary policies in weight space, guided by stage-aware advantage; (Right) Ptest: temporal chunk-wise smoothing ensures execution accuracy, while on-policy DAgger enables closed-loop refinement. based policies [95, 16, 15, 92] to foundation models trained on massive robot demonstrations [7, 6, 34, 24, 9, 3, 2]. Among these, π series [4, 3, 2] stand out for strong generalization via large-scale pretraining dataset. Yet the collection of such datasets [33, 58, 8, 30, 79] demands substantial resources. For data efficiency, prior works explore DAgger-style aggregation [62, 32, 27, 2] and augmentation [41, 36, 91, 46]. However, DAgger requires real-time human supervision during policy rollouts [62], making data collection still timeconsuming. Besides, real-world deployment introduces unique challenge to these polices: inference-control latency causes mismatch between model outputs and physical execution. Prior works try to mitigate this through execution-side optimizations [95, 5, 76], yet introduce additional inference overhead. While existing methods address individual phases rather than jointly enforcing distributional consistency across the robot learning cycle: data collection, model training and deployment [11, 75], we formalize this challenge through Ptrain, Qmodel, Ptest and propose χ0 to comprehensively align them. B. Model Merging and Weight Interpolation Model merging has emerged as an efficient strategy for consolidating knowledge from multiple neural networks. Initial work in computer vision and natural language processing demonstrated that interpolating weights between models across hyperparameter perturbation checkpoints [84] or across models fine-tuned on different tasks [84, 29, 87] can improve generalization and robustness. These techniques have recently been extended to large language models [1, 89, 35], planning [42], and robot learning [80]. These methods often rely on indistribution metrics for merging strategy selection, which may not account for the narrow distribution shifts common in complex manipulation. Parallel to our work, RETAIN [88] applies model merging to adapt VLA policies, improving target-task out-of-distribution (OOD) generalization while better retaining generalist skills. Our work, Model Arithmetic (MA), specifically mitigates the model bias caused by incomplete training coverage with limited expert demonstration. We introduce novel validation protocol using OOD dataspecifically recovery trajectories collected via DAggerto ensure the merged policy generalizes to unseen states. Furthermore, we provide comparative analysis of multiple merging strategies, including uniform weighting, inverse loss weighting, gradient decent, and greedy search, to identify the most effective synthesis for mitigating training data bias. C. Advantage Estimation for Long-Horizon Tasks Prior work has explored conditioning policies on rewards, values, and advantages to guide action selection in long-horizon tasks[63, 19, 97, 85, 37]. This includes advantage-weighted objectives such as advantage-weighted regression (AWR), which biases behavior cloning toward higheradvantage actions[60]. Building on these ideas, π 0.6 trains distributional value model to estimate state-action advantages and uses them for advantage-conditioned VLA training [2]. key limitation in practice is numerical instability: advantages computed from value differences can be noisy and highvariance, especially under long-horizon real-world dynamics. Stage Advantage addresses this by directly predicting advantage from paired observations and conditioning the signal on semantic stages, yielding smoother and more stable supervision signal that can be discretized into binary optimality indicator for policy learning [12, 2]. III. METHODOLOGY A. Preliminary and Problem Setup To formalize the distributional framework introduced in Sec. I, we consider finite-horizon Markov Decision Process (MDP) with state space S, action space and horizon H. trajectory τ = (s0, a0, s1, a1, . . . , sH ) evolves under real-world dynamics st+1 ( st, at, ξ) under fixed environment parameterization ξ, with initial-state distribution µ(s0). Accordingly, the trajectory distribution induced by stochastic policy π(a s; ϕ) could be defined as [65, 44]: Pπ(τ ) = µ(s0) (cid:81)H1 t=0 π(at st; ϕ) (st+1 st, at, ξ), (1) abbreviated as when the policy π is unambiguous. We first define Preal as the distribution over successful trajectories that complete the tasks. Let denote the set of successful trajectories, such that Preal(τ ) 1{τ G}, characterizing the manifold of all valid action sequences achieving task completion under real-world dynamics. With Preal we now formalize the three distributions underlying our alignment objective. The training distribution Ptrain is induced by human expert demonstrations on real robots. Given Ptrain, let = {τ (i)}N i=1 denote the set of demonstrations. The model Qmodel π(a s; ˆϕ) is learned by maximizing (cid:80) log π(at st; ϕ) over the demonstration set [62, 57]. Finally, realrobot execution produces Ptest by composing Qmodel with an inference operator I(at at, st): Ptest(τ ) = µ(s0) (cid:81)H1 t=0 π(at st; ˆϕ) (st+1 st, at, ξ), (2) (cid:2) (st+1 st, at, ξ) (cid:3), where (st+1 st, at, ξ) EaI(at,st) and at denotes the actually executed action. As discussed in Sec. I, real-world deployment reveals three systematic inconsistencies among these distributions across different phases. We categorize them as: (i) Coverage DeficiencyPtrain undersamples the high-dimensional manifold Preal, biasing Qmodel toward limited training support; (ii) Temporal Mismatchlong-horizon tasks induce visually similar but semantically distinct states across stages, causing Qmodel to misapply temporal knowledge, while inference-control latency also leads to execution-level temporal mismatch, all of which are reflected as failure or staying still in Ptest; (iii) Failure Cascadeabsence of recovery behaviors in Ptrain leaves the policy unable to self-correct from perturbations in Ptest. Our method tries to address each inconsistency through targeted alignment strategies, detailed in the following sections. B. Pipeline of χ0 system To coherently address the three distributional inconsistencies identified in Sec. III-A, we propose χ0 , resourceefficient framework integrating three complementary technical learning cycle. Model Arithmetic pillars across the robot (Sec. III-C) expands policy coverage by merging models trained on complementary data subsets in weight space; Stage Advantage (Sec. III-D) addresses temporal mismatch at policy learning phase through stage-aware advantage estimation for stable long-horizon supervision; Train-Deploy-Alignment (Sec. III-E) closes the loop between deployment and training through inference optimization and complementary data augmentation. Figure 2 illustrates the complete pipeline. C. Model Arithmetic Limited expert demonstrations in the initial data collection phase lead to coverage deficiency in Ptrain, which further biases Fig. 3: Souping strategies in Model Arithmetic. Policies trained on separate subsets are merged via weighted interpolation. (Top) Inverse Loss assigns higher coefficients to models with lower validation loss. (Bottom) Other strategies. the learned policies toward narrow manipulation patterns. To mitigate this, straightforward solution is to scale up expert demonstrations until Ptrain sufficiently approximates Preal. However, it is prohibitively expensive for garment manipulation: each collection cycle demands extensive operation time. Consequently, it raises fundamental question: How can we efficiently mitigate the model bias without scaling data? We propose Model Arithmetic (MA), weight-space merging strategy that combines policies trained on complementary data subsets, guided by validation-based optimization. Unlike Mixture-of-Experts (MoE), which requires explicit router mechanisms and complex training design [68, 20], or model ensembling that combines model outputs [40], MA directly merges parameters to synthesize unified policy. Formally, given collected data subsets {D1, D2, . . . , Dn} sampled from Ptrain, MA trains policies {θ1, θ2, . . . , θn} independently on these subsets and synthesizes their model weights via interpolation: θmerged = (cid:80)n i=1 αi = 1. {αi} is optimized by minimizing held-out loss over validation split. θmerged serves as the final Qmodel for deployment. s.t.αi 0, (cid:80)n i=1 αiθi, MA starts with randomly partitioning the training dataset into non-overlapping subsets {D1, D2, . . . , Dn} and training separate policies on each. Due to limited coverage per subset, these policies naturally converge to distinct regions of the solution manifold. The key challenge then becomes how to optimally merge these policies. In practice, the critical design choices lies in validation set selection. We strategically construct validation set that is out-of-distribution (OOD) with respect training subsets (in-domain), ensuring unbiased evaluation of merged policy. Specifically, we use trajectories collected via DAgger [62, 32], from models trained on individual subsets, as these recovery behaviors are naturally absent from any original training data. Based on this validation set, we implement and ablate four souping strategies to obtain the final θmergedaverage weighting, inverse loss, gradient descent and greedy search [84]as illustrated in Figure 3. to all Through validation-guided weight-space synthesis, MA effectively combines diverse unimodal policies into unified multi-modal policy, mitigating the bias of Qmodel induced by coverage deficiency without additional data collection. Fig. 4: Cumulative value based on SA. Red/green stands for negative/positive. Top: Task shows slip fails and recovery; Middle: Task shows fetching and cloth misplace; Bottom: Task shows pull-over and visual occlusion. D. Stage Advantage While Model Arithmetic efficiently mitigates Qmodel bias, the resulting policy still struggles with long-horizon execution in Ptest due to temporal mismatch: visually similar states across task stages cause the policy to misapply behaviors, leading to compounding errors and task failure over long horizons. The stage ambiguity calls for explicit progress signals that can disambiguate action quality within the context of task progress [12]. This raises key question: how to provide stable and accurate progress signals during long-horizon execution? Prior approach [2] uses advantage as the progress signal, combining with advantage-weighted regression [85, 37] to train policies with advantage-weighted training samples. It obtains advantage implicitly as A(s, a) = (s)V (s), taking the difference between independently predicted progress value. Nevertheless, this formulation would amplify frame-wise estimation noise, yielding high-variance training signals. Moreover, estimating global task progress without stage awareness causes (s) to exhibit multi-valued predictions for multi-stage tasks, further degrading advantage quality. To obtain stable and accurate advantage signal for model training, we take more straightforward route by treating advantage as direct modeling target: A(s, a) = fθ(s, s), where fθ predicts relative progress from to s. This recasts Fig. 5: Train-Deploy-Alignment strategies and T-SNE visualizations. Left: three complementary strategies for distribution alignment. Right: T-SNE visualizations showing progressive distribution alignment as each strategy is applied. advantage estimation to single prediction, avoiding error compounding and yielding smoother, more reliable stateto-state supervision signal. In practice, we use VLM-based architecture that takes pairwise image inputs as the advantage estimator, as shown in Figure 2. To avoid overfitting to fixed temporal discretization, we construct training pairs by randomly sampling time span and setting = st+. To further resolve the multi-valued ambiguity in progress estimation over long-horizon tasks, Stage Advantage decomposes the task into sequence of semantic stages, each corresponding to meaningful sub-goal. Instead of evaluating actions under full task horizon, we estimate whether each action advances the current stage, providing stage-aware progress signal: Astage(s, a, g) = fθ(s, sg). Practically we use manually annotated stage labels to represent the stage as normalized scalar {0, 1 }, is the number of stages as Figure 2 shows. Figure 4 shows cumulative value based on stage advantage for tasks defined in Sec. IV-A. , . . . , S1 Following [12, 2], we threshold the continuous advantage predictions into binary optimality indicator = 1[Astage > ϵ], where ϵ is threshold that separates progress from non-progress. This enables stable advantage-weighted policy learning that upweights high-quality data from Ptrain while mitigating the temporal mismatch between Ptrain and Qmodel. E. Train-Deploy-Alignment Despite robust policies with long-horizon planning abilintroduces new inconsistencies ity, real-world deployment the system in manually designed failure states (e.g., misaligned grasps, partial drops) and collects recovery demonstrations, front-loading failure experience into data collection. 2) To further diversify Ptrain at zero robot time, we apply Spatiotemporal Augmentations: horizontal flipping with left/right arm swapping [46], and partial frame-skipping to synthesize speed variations demonstrated in Figure 5, detail in Appendix. IV. EXPERIMENTS Our evaluation framework targets collaborative long-horizon garment manipulation, encompassing flattening from arbitrary states, folding, handover operations, and hanging. We select this series of tasks because their contact-rich, deformable dynamics and requirement for state recovery effectively isolate and magnify the distributional shifts aforementioned. We illustrate the detailed robot setup in Figure 6. We systematically investigate the following research questions: 1) System Efficacy breakdown. How do the individual components synergize to enhance overall performance, or do they conflict when integrated? (Sec. IV-D) 2) Model Arithmetic. Can MA of subset-trained candidates outperform both single best candidate among them and full-data-trained candidate? Which validation split (in-domain vs. OOD) demonstrates robust statistical superiority among different strategies? (Sec. IV-E) 3) Stage Advantage. Does predicting stage-conditioned advantage offer more stable supervision than valuedifference baseline (RECAP in π 0.6 [2]), and how does this translate to policy success? (Sec. IV-F) 4) Train-Deploy-Alignment. Does expanding Ptrain via Heuristic DAgger improve performance while incurring only marginal increase in retry cost compared to standard DAgger? How do different control methods work with spatio-temporal data-augmentation? (Sec. IV-G) A. Evaluation Tasks and Metrics We evaluate our approach on three challenging garment manipulation tasks with varying complexity. Task A: T-Shirt Flattening and Folding (Easy). simplified variant of the standard laundry task from the π series [4, 3, 2]. The robot must flatten T-shirt from an arbitrary initial configuration and fold it. Success is defined as placing the fully folded T-shirt in the table center within 180 seconds. Task B: Conditional Retrieval and Sorting (Medium). An extension of the π series task [4, 3, 2] involving conditional logic. The system retrieves and flattens either T-shirt or collared shirt from variable initial states. T-shirts must be folded and stacked (top-left), while collared shirts must be handed over to the right side, both within 180 seconds. Task C: Garment Hanging (Hard). Extended from GR3 [10], this task requires retrieving the flattened collared shirt from Task and hanging it on rack. Success is defined as the stable suspension of the garment on the rod without dropping. We report four metrics (mean accumulative standard error), calculated over 10 trials for each of three garment types. Success Rate (SR) measures the percentage of trials that Fig. 6: Robot setup of our collaborative dual-arm system. between Qmodel and Ptest. Inference-control latency causes misplaced action execution and drift error accumulation, especially for action-chunking policies that output action chunks: the gap between model inference and chunk execution breaks temporal continuity across consecutive chunks, leading to abrupt transitions and degraded manipulation stability. Prior work addresses this through inference-time chunk interpolation [95, 5, 76]. Additionally, we adopt temporal chunkwise smoothing to ensure coherent action executions in deployment phase as the bottom part of Figure 5. Mathematically, let aold denote the current action buffer containing residual commands from previous inference cycle, and anew the newly predicted action chunk. We maintain consumption index tracking executed actions in current action buffer, drop threshold dmax to discard stale commands caused by inference latency, and minimum overlap length mmin to ensure stable interpolation. Based on these, we present the detailed smoothing procedure in Algorithm 1. Algorithm 1: Temporal Chunk-wise Smoothing : current buffer aold, index k, new chunk anew, dmax, mmin Input Output: updated buffer abuf, reset index 0 min(k, dmax); if anew then return aold, k; , . . . ); rem (anew anew if aold < mmin then pad aold by repeating last command; // compute drop amount // ignore update // remaining new commands min(aold, anew for 0 to 1 do rem ); wi 1 i/ max(L 1, 1); + (1 wi)anew ai wiaold rem,i // overlap length abuf (a0, . . . , aL1)(suffix of anew 0; return abuf, rem ); // reset consumption index With robust policy and reliable deployment pipeline established, natural question arises: can we leverage rollout experience from Ptest to expand Ptrain without extended data collection effort? Recall that static demonstrations lack recovery behaviors, leaving the policy vulnerable to failure cascades. We address this final inconsistency by closing the loop between deployment and training through two complementary strategies. 1) On-policy DAgger [62, 32] expands Ptrain toward failure-adjacent regions but is time-consuming, as it requires waiting for natural failures during policy rollouts. We propose Heuristic DAgger variant that directly initializes Fig. 7: χ0 system efficacy on Task A. Performance improves with individual modules and increases further in pairwise settings, maximizing with the complete χ0 system (ours). complete the task successfully (higher is better). Throughput (TP) quantifies the estimated number of tasks completed per hour (higher is better). Retry Cost is the average number of action retries per episode during evaluation (lower is better). Average Score is derived from rule-based evaluation protocol. We define subtask-specific milestones and assign partial credit upon their completion; credits are then normalized to 100. The Appendix lists metric calculation and task details. B. Data collection and Training Strategy We curate 20 hours of expert demonstrations per task, capturing diverse garment states (color, material), initial states, and environmental lighting. We employ full-parameter finetuning on 8A100 GPUs via flow-matching objective [4]. See Appendix for hyperparameters and collection details. C. Baselines and Ablation Design Base policy. We select π0.5 as our primary base policy, complemented by π0, as these are the only two open-source policies capable of viable performance on our tasks. Despite their reported capabilities in similar domains, GO-1 [8], XVLA [96] and DexVLA [82] did not achieve tractable performance even after training on the full 20-hour dataset. MA Ablations. We establish two baselines: single-best candidate (selected from policies trained on individual subsets) and full-data candidate (trained on the aggregated dataset). We examine robust statistical superiority of in-domain and OOD validation split across merging techniques: Average weighting [84] (assigning uniform weights by setting αi = 1/n); Inverse-loss [54] (inversely proportional to percheckpoint validation losses Li, setting αi 1/(Li + ϵ)p after normalization); Gradient descent and its adaptive invariant [28] (softmax-parameterized coefficients α = softmax(w) by minimizing the merged validation loss validation loss Lval((cid:80) αiθi) of θmerged via iterative updates); and Greedy search [84] (iteratively adding checkpoints that reduce validation loss most under uniform averaging among candidates). SA Ablations. We compare against self-implemented RECAP [2] baseline implemented on π0.5 [3] PaliGemma which Fig. 8: Ablations of MA on Task C. All MA variants outperform single-best and full-data candidates in throughput and success rate with robust statistical significance, despite increased retry costs in some implementations. Furthermore, OOD validation demonstrates enhanced stability and reduced standard error relative to in-domain validation. Fig. 9: Ablations of SA on Task & B. Left: quantifying numerical stability via Smooth Frame Ratio (SFR) and Mean Squared Temporal Difference (MSTD). Right: SA shows superior stability against the π 0.6-style advantage baseline, which correlates positively with the observed performance gains. trained to estimate progress given current frame. The advantage signal is derived from the value(progress) difference relative to 50-step future horizon within the same trajectory. TDA Ablations. Our temporal chunk-wise smoothing is compared against Synchronous/Asynchronous Inference [76], temporal ensembling [95], and RTC [5]. We evaluate control methods across different spatio-temporal augmentation settings to identify the optimal configuration. Additionally, we assess the impact of data augmentation by comparing standard DAgger [62] with our Heuristic DAgger, examining both the performance delta and the generalization of these methods across π0 [4] and π0.5 [3] architectures. D. χ0 System Efficacy Breakdown In Figure 7, we report the performance breakdown of each module in the χ0 system on task A. By selecting the optimal configuration for each module (MA, SA, TDA)as detailed in subsequent sectionswe ensure effective system integration, where performance scales monotonically with components added. Specifically, SA is the dominant factor for throughput, whereas TDA drives the success rate but incurs higher retry Fig. 10: Ablations of Heuristic and standard DAgger on Task A. All DAgger-style variants improve throughput, success rate, and score, despite higher retry costs under heuristic DAgger. For π0.5, full DAgger further boosts throughput while reducing retry cost, yielding better trade-off. Fig. 12: Ablations of control strategies and spatio-temporal augmentation on Task A. Temporal chunk-wise smoothing outperforms temporal ensembling, RTC; Combining our method with RTC further improves the performance. F. Stage Advantage Results Figure 9 demonstrates that improved numerical stability from SA translates to broad performance gains. Specifically, SA minimizes retry overhead in Task (long-horizon, conditional). This confirms that SA effectively biases the policy against idling and spurious retries during deployment (Ptest), ensuring consistent task progression. The results also underscore that direct advantage estimation requires stage-aware labels to ensure robustness. G. Train-Deploy-Alignment Results Figure 10 and 11 evaluate Heuristic and standard DAgger across models (π0.5, π0) and tasks (Task A, Task C). 1) Heuristic DAgger significantly improves failure recovery and overall performance, achieving substantially higher SR/TP compared to the baseline. Note that bases low recovery cost reflects absent self-correction behaviors rather than efficient recovery. 2) Heuristic DAgger provides comparable recovery quality to DAgger, while inference-free makes it more cost-efficient to address failure cascade. Fig. 12 benchmarks temporal chunkwise smoothing against other control strategies across different augmentation settings. 1) Temporal chunk-wise smoothing outperforms temporal ensembling and RTC in most cases, demonstrating its effectiveness in translating Qmodel capability into Ptest performance. 2) Spatio-temporal augmentation shows task-dependent effect, providing no significant gain on Task A. V. CONCLUSION AND LIMITATIONS We present χ0 to address distributional shifts across the robot learning. Through Model Arithmetic, Stage Advantage, and Train-Deploy-Alignment, the system targets key failure modes including coverage deficiency and temporal mismatch, which is validated extensively on complex garment manipulation tasks and yields robust long-horizon performance. Several limitations are spot through this journey of achieving 100% reliability in robotic manipulation. The first is Scalability. The prominence of robot foundation models hinges on their promise of broad generalization. This study, however, did Fig. 11: Ablations of Heuristic and standard DAgger on Task C. DAgger works in similar trend as Figure. 10, costs. This aligns with our insight that TDA encourages persistent retryinga behavior that naturally improves task completion at the expense of increased operational cost. E. Model Arithmetic Results Figure 8 presents comprehensive analysis across all metrics, benchmarking MA variants against non-MA baselines and quantifying the performance differentials between distinct MA strategies. 1) First, all MA variants outperform both the singlebest candidate and the full-data baseline, validating the efficacy of the approach. Notably, merging weights from subset-trained models surpasses training on the combined dataset (joint training). This suggests that fine-tuned VLAs may exhibit extreme parameter redundancy, akin to phenomena observed in LLMs [90]. 2) Second, OOD validation data proves to be more robust selection criterion than in-domain data, yielding lower standard errors and higher performance across all metrics. This finding supports our hypothesis that OOD data (e.g., DAgger) effectively bridges the gap between Ptrain and Ptest. Consequently, utilizing DAgger data to calibrate mixing weights ensures that Qmodel prioritizes modes in Ptrain that align with deployment dynamics. Among MA strategies, greedy search proves most effective across diverse settings. This reinforces our finding that validation loss on DAgger data accurately reflects distributional gaps, enabling Qmodel to improve mode coverage for test-time generalization [11]. not explicitly evaluate the retention of pre-trained priors during the post-training process. Future research should address the retention of pre-trained priors and evaluate the extensibility of χ0 to rigid-body manipulation tasks. Furthermore, it remains to be seen if Model Arithmetic can integrate distinct task policiesrather than merely subsetsto advance generalpurpose robotics. The second is Data Valuation. Our results confirm that data utility is highly variable. Currently, validating data quality requires costly full-training loops or slow, nonparallelizable replay checks (where successful replay indicates high utility). Establishing efficient, predictive metrics for data quality without incurring these overheads remains key challenge for future research."
        },
        {
            "title": "ACKNOWLEDGEMENTS",
            "content": "We thank our robot operators for data collection, evaluations, logistics, and video recording, and our technicians for robot maintenance and repair. We thank Longyan Wu for refining teaser and robot setup figure. We thank Zhiqian Lan, Jiaheng Wang for their early help in this project, including data generation pipeline construction as well as inference smoothness optimization. We thank Jiazhi Yang, Yixuan Pan, Yinghui Li, Junli Ren, Haoran Jiang, Kunyang Lin, Wencong Zhang, Jinwei Li, Kai Zhang for discussion and feedback. REFERENCES [1] Takuya Akiba, Makoto Shing, Yujin Tang, Qi Sun, and David Ha. Evolutionary optimization of model merging recipes. Nature Machine Intelligence, 7:195 204, 2024. 3 [2] Ali Amin, Raichelle Aniceto, Ashwin Balakrishna, Kevin Black, Ken Conley, Grace Connors, James Darpinian, Karan Dhabalia, Jared DiCarlo, et al. π 0.6: VLA that learns from experience. arXiv.org, 2511.14759, 2025. 2, 3, 5, 6, 7, 13, 14 [3] Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Robert Equi, Chelsea Finn, Niccolo Fusai, Manuel Galliker, et al. π0.5: vision-language-action model with openworld generalization. In CoRL, 2025. 1, 2, 3, 6, 7 [4] Kevin Black, Noah Brown, Danny Driess, Adnan Esber, Michael Equi, Chelsea Finn, et al. π0: vision-languageIn RSS, action flow model for general robot control. 2025. 2, 3, 6, 7 [5] Kevin Black, Manuel Galliker, and Sergey Levine. Real-time action chunking for robot control. In NeurIPS, 2025. 2, 3, 6, [6] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. RT-2: Vision-language-action models transfer web knowledge to robotic control. In CoRL, 2023. 3 [7] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. RT-1: Robotics transformer for real-world control at scale. In RSS, 2023. 3 [8] Qingwen Bu, Jisong Cai, Li Chen, Xiuqi Cui, Yan Ding, Siyuan Feng, Xindong He, Xu Huang, et al. Agibot world colosseo: large-scale manipulation platform for In IROS, scalable and intelligent embodied systems. 2025. 2, 3, 7, 13 [9] Qingwen Bu, Yanting Yang, Jisong Cai, Shenyuan Gao, Guanghui Ren, Maoqing Yao, Ping Luo, and Hongyang Li. Univla: Learning to act anywhere with task-centric latent actions. 2025. 3, 13 [10] Chilam Cheang, Sijin Chen, Zhongren Cui, Yingdong Hu, Liqun Huang, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Xiao Ma, et al. Gr-3 technical report. arXiv preprint arXiv:2507.15493, 2025. [11] Howard Chen, Noam Razin, Karthik Narasimhan, and Danqi Chen. Retaining by doing: The role of on-policy arXiv.org, 2510.18874, data in mitigating forgetting. 2025. 3, 8 [12] Li Chen, Chonghao Sima, Kashyap Chitta, Antonio Loquercio, Ping Luo, Yi Ma, and Hongyang Li. Intelligent robot manipulation requires self-directed learning. Authorea Preprints, 2026. 3, 5 [13] Tianxing Chen, Kaixuan Wang, Zhaohui Yang, Yuhao Zhang, Zanxin Chen, Baijun Chen, Wanxi Dong, Ziyuan Liu, Dong Chen, Tianshuo Yang, et al. Benchmarking generalizable bimanual manipulation: Robotwin dualarm collaboration challenge at cvpr 2025 meis workshop. arXiv preprint arXiv:2506.23351, 2025. 2 [14] Zengjue Chen, Runliang Niu, He Kong, Qi Wang, Qianli Xing, and Zipei Fan. TGRPO: Fine-tuning visionlanguage-action model via trajectory-wise group relative policy optimization. arXiv preprint arXiv:2506.08440, 2025. 14 [15] Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and Shuran Song. Diffusion Policy: Visuomotor policy learning via action diffusion. In RSS, 2023. 3 [16] Cheng Chi, Zhenjia Xu, Chuer Pan, Eric Cousineau, Benjamin Burchfiel, Siyuan Feng, Russ Tedrake, and Shuran Song. Universal Manipulation Interface: In-thewild robot teaching without in-the-wild robots. In RSS, 2024. [17] Ekin Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc Le. Autoaugment: Learning augmentation strategies from data. In CVPR, pages 113 123, 2019. 2 [18] Ekin Cubuk, Barret Zoph, Jonathon Shlens, and Quoc Le. Randaugment: Practical automated data In CVPR augmentation with reduced search space. workshop, 2020. 2 [19] Scott Emmons, Benjamin Eysenbach, Ilya Kostrikov, and Sergey Levine. RVS: What is essential for offline RL via supervised learning? In ICLR, 2022. 3 [20] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. JMLR, 2022. 4 RSS, 2024. [21] Kevin Frans, Seohong Park, Pieter Abbeel, and Sergey Levine. Diffusion guidance is controllable policy improvement operator. arXiv preprint arXiv:2505.23458, 2025. 13, 14 [22] Generalist AI Team. GEN-0: Embodied foundation https:// models that scale with physical generalistai.com/blog/preview-uqlxvb-bb.html, 2025. 2 interaction. [23] Seyed Kamyar Seyed Ghasemipour, Ayzaan Wahid, Jonathan Tompson, Pannag Sanketi, and Igor Mordatch. arXiv Self-improving embodied foundation models. preprint arXiv:2509.15155, 2025. 14 [24] Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, Jianlan Luo, et al. Octo: An open-source generalist robot policy. In RSS, 2024. [25] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning basic visual concepts with constrained variational framework. In ICLR, 2017. 2 [26] Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, et al. Planning-oriented autonomous driving. In CVPR, 2023. 1 [27] Zheyuan Hu, Robyn Wu, Naveen Enock, Jasmine Li, Riya Kadakia, Zackory Erickson, and Aviral Kumar. Rac: Robot learning for long-horizon tasks by scaling recovery and correction. arXiv.org, 2509.07953, 2025. 2, 3 [28] Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. Editing models with task arithmetic. In ICLR, 2023. 7 [29] Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. Editing models with task arithmetic. In ICLR, 2023. 3 [30] Tao Jiang, Tianyuan Yuan, Yicheng Liu, Chenhao Lu, Jianning Cui, Xiao Liu, Shuiqi Cheng, Jiyang Gao, Galaxea open-world Huazhe Xu, and Hang Zhao. dataset and g0 dual-system vla model. arXiv preprint arXiv:2509.00576, 2025. 3 [31] Yunfan Jiang, Ruohan Zhang, Josiah Wong, Chen Wang, Yanjie Ze, Hang Yin, Cem Gokmen, Shuran Song, Jiajun Wu, and Li Fei-Fei. Behavior robot suite: Streamlining real-world whole-body manipulation for everyday household activities. In CoRL, 2025. [32] Michael Kelly, Chelsea Sidrane, Katherine DriggsCampbell, and Mykel Kochenderfer. HG-DAgger: Interactive imitation learning with human experts. In ICRA, 2019. 3, 4, 6 [33] Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, et al. DROID: large-scale in-the-wild robot manipulation dataset. In [34] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. OpenVLA: An open-source vision-language-action model. In CoRL, 2024. 3, 13 [35] Kimi Team. Kimi k1.5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. 3 [36] Ilya Kostrikov, Denis Yarats, and Rob Fergus. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. In ICLR, 2021. 3 [37] Jakub Grudzien Kuba, Pieter Abbeel, and Sergey Levine. Advantage-conditioned diffusion: Offline RL via generalization. In ICLR, 2024. 3, 5, [38] Aviral Kumar, Xue Bin Peng, and Sergey Levine. arXiv.org, 1912.13465, Reward-conditioned policies. 2019. 14 [39] Kristofer D. Kusano, John M. Scanlon, Yin-Hsiu Chen, Timothy L. McMurry, Tilia Gode, and Trent Victor. Comparison of waymo rider-only crash rates by crash type to human benchmarks at 56.7 million miles. Traffic Injury Prevention, 2025. 1 [40] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In NeurIPS, 2017. 4 [41] Michael Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas. Reinforcement learning with augmented data. In NeurIPS, 2020. 3 [42] Giwon Lee, Wooseong Jeong, Daehee Park, Jaewoo Jeong, and Kuk-Jin Yoon. Interaction-merged motion planning: Effectively leveraging diverse motion datasets for robust planning. In ICCV, 2025. 3 [43] Kun Lei, Huanyu Li, Dongjie Yu, Zhenyu Wei, Lingxiao Guo, Zhennan Jiang, Ziyu Wang, Shiyu Liang, and Huazhe Xu. RL-100: Performant robotic manipulation with real-world reinforcement learning. arXiv preprint arXiv:2510.14830, 2025. 14 [44] Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review. arXiv preprint arXiv:1805.00909, 2018. [45] Haozhan Li, Yuxin Zuo, Jiale Yu, Yuhao Zhang, Zhaohui Yang, Kaiyan Zhang, Xuekai Zhu, Yuchen Zhang, Tianxing Chen, Ganqu Cui, et al. SimpleVLA-RL: Scaling vla training via reinforcement learning. arXiv preprint arXiv:2509.09674, 2025. 14 [46] Yunfei Li, Xiao Ma, Jiafeng Xu, Yu Cui, Zhongren Cui, Zhigang Han, Liqun Huang, Tao Kong, Yuxiao Liu, Hao Niu, et al. Gr-rl: Going dexterous and precise for long-horizon robotic manipulation. arXiv preprint arXiv:2512.01801, 2025. 3, 6, 14 [47] Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, and Peter Stone. LIBERO: BenchmarkIn ing knowledge transfer for lifelong robot learning. NeurIPS, 2023. 14 [48] Evan Liu, Behzad Haghgoo, Annie Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa, Percy Liang, and Chelsea Finn. Just train twice: Improving group robustness without training group information. In ICML, 2021. 2 [49] Guanxing Lu, Wenkai Guo, Chubin Zhang, Yuheng Zhou, Haonan Jiang, Zifeng Gao, Yansong Tang, and Ziwei Wang. VLA-RL: Towards masterful and general robotic manipulation with scalable reinforcement learning. arXiv preprint arXiv:2505.18719, 2025. [50] Hao Luo, Ye Wang, Wanpeng Zhang, Sipeng Zheng, Ziheng Xi, Chaoyi Xu, Haiweng Xu, Haoqi Yuan, Chi Zhang, Yiqing Wang, Yicheng Feng, and Zongqing Lu. Being-H0.5: Scaling human-centric robot learning for cross-embodiment generalization. arXiv.org, 2601.12993, 2026. 2 [51] Jianlan Luo, Zheyuan Hu, Charles Xu, You Liang Tan, Jacob Berg, Archit Sharma, Stefan Schaal, Chelsea Finn, Abhishek Gupta, and Sergey Levine. SERL: software suite for sample-efficient robotic reinforcement learning. In ICRA, 2024. 14 [52] Jianlan Luo, Charles Xu, Jeffrey Wu, and Sergey Levine. Precise and dexterous robotic manipulation via humanScience Robotics, in-the-loop reinforcement learning. 2025. 14 [53] Yecheng Jason Ma, Joey Hejna, Chuyuan Fu, Dhruv Shah, Jacky Liang, Zhuo Xu, Sean Kirmani, Peng Xu, Danny Driess, Ted Xiao, et al. Vision language models are in-context value learners. In ICLR, 2024. 14 [54] Shalini Maiti, Amar Budhiraja, Bhavul Gauri, Gaurav Chaurasia, Anton Protopopov, Alexis Audran-Reiss, Michael Slater, Despoina Magka, Tatiana Shavrina, Roberta Raileanu, et al. Souper-model: How simple arithmetic unlocks state-of-the-art LLM performance. arXiv.org, 2511.13254, 2025. 7 [55] Oier Mees, Lukas Hermann, Erick Rosete-Beas, and Wolfram Burgard. CALVIN: benchmark for languageconditioned policy learning for long-horizon robot manipulation tasks. RAL, 2022. 14 [56] Yao Mu, Tianxing Chen, Zanxin Chen, Shijia Peng, Zhiqian Lan, Zeyu Gao, Zhixuan Liang, Qiaojun Yu, Yude Zou, Mingkun Xu, et al. Robotwin: Dual-arm robot benchmark with generative digital twins. In CVPR, 2025. [57] Takayuki Osa, Joni Pajarinen, Gerhard Neumann, Andrew Bagnell, Pieter Abbeel, and Jan Peters. An algorithmic perspective on imitation learning. Foundations and Trends in Robotics, 2018. 4 [58] Abhishek Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anikait Singh, Anthony Brohan, et al. Open X-Embodiment: Robotic learning datasets and RT-X models. In ICRA, 2024. 3 [59] Mingjie Pan, Siyuan Feng, Qinglin Zhang, Xinchen Li, Jianheng Song, Chendi Qu, Yi Wang, Chuankang Li, Ziyu Xiong, Zhi Chen, et al. Sop: scalable online post-training system for vision-language-action models. arXiv preprint arXiv:2601.03044, 2026. 2 [60] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. ICLR, 2021. 2, 3, 13 [61] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 2 [62] Stephane Ross, Geoffrey Gordon, and Drew Bagnell. reduction of imitation learning and structured prediction to no-regret online learning. In AISTATS, 2011. 3, 4, 6, [63] Jurgen Schmidhuber. Reinforcement learning upside down: Dont predict rewardsjust map them to actions. arXiv.org, 1912.02875, 2019. 3 [64] Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. In NeurIPS, 2020. 2 [65] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In ICML, pages 18891897. PMLR, 2015. 4 [66] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 13 [67] Kohei Sendai, Maxime Alvarez, Tatsuya Matsushima, Yutaka Matsuo, and Yusuke Iwasawa. Leave no observation behind: Real-time correction for vla action chunks. arXiv preprint arXiv:2509.23224, 2025. 14 [68] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In ICLR, 2017. 4 [69] Modi Shi, Li Chen, Jin Chen, Yuxiang Lu, Chiming Liu, Guanghui Ren, Ping Luo, Di Huang, Maoqing Yao, and Hongyang Li. Is diversity all you need for scalable robotic manipulation? arXiv preprint arXiv:2507.06219, 2025. [70] Mustafa Shukor, Dana Aubakirova, Francesco Capuano, Pepijn Kooijmans, Steven Palma, Adil Zouitine, Michel Aractingi, Caroline Pascal, Martino Russi, Andres Marafioti, et al. Smolvla: vision-language-action model for affordable and efficient robotics. arXiv preprint arXiv:2506.01844, 2025. 14 [71] Chonghao Sima, Katrin Renz, Kashyap Chitta, Li Chen, Hanxue Strategy Zhang, Chengen Xie, Jens Beißwenger, Ping Luo, Andreas Geiger, and Hongyang Li. DriveLM: Driving with graph visual question answering. In ECCV, 2024. 1 [72] Chonghao Sima, Kashyap Chitta, Zhiding Yu, Shiyi Lan, Ping Luo, Andreas Geiger, Hongyang Li, and Jose Alvarez. Centaur: Robust end-to-end autonomous driving with test-time training. arXiv.org, 2503.11650, 2025. 1 foundation model ACT-1: robot trained on zero robot data. https://www.sunday.ai/journal/ [73] Sunday Team. no-robot-data, 2025. 2 [74] Richard Sutton, Andrew Barto, et al. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998. 13 [75] Fahim Tajwar, Anikait Singh, Archit Sharma, Rafael Rafailov, Jeff Schneider, Tengyang Xie, Stefano Ermon, Chelsea Finn, and Aviral Kumar. Preference fine-tuning of LLMs should leverage suboptimal, on-policy data. arXiv.org, 2404.14367, 2024. 3 [76] Jiaming Tang, Yufei Sun, Yilong Zhao, Shang Yang, Yujun Lin, Zhuoyang Zhang, James Hou, Yao Lu, Zhijian Liu, and Song Han. Vlash: Real-time vlas via futurearXiv preprint state-aware asynchronous inference. arXiv:2512.01031, 2025. 2, 3, 6, 7 [88] Yajat Yadav, Zhiyuan Zhou, Andrew Wagenmaker, Karl Pertsch, and Sergey Levine. Robust finetuning of visionlanguage-action robot policies via parameter merging. arXiv preprint arXiv:2512.08333, 2025. 3 [89] Enneng Yang, Li Shen, Guibing Guo, Xingwei Wang, Xiaochun Cao, Jie Zhang, and Dacheng Tao. Model merging in llms, mllms, and beyond: Methods, theories, applications, and opportunities. ACM Computing Surveys, 2024. 3 [90] Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. Language models are super mario: Absorbing abilities from homologous models as free lunch. In ICML, 2024. [77] Tesla. Full Self-Driving (Supervised), 2025. URL https: [91] Tianhe Yu et al. Scaling robot learning with semantically imagined experience. In RSS, 2023. 3 [92] Jia Zeng, Qingwen Bu, Bangjun Wang, Wenke Xia, Li Chen, Hao Dong, Haoming Song, Dong Wang, Di Hu, Ping Luo, et al. Learning manipulation by predicting interaction. In RSS, 2024. 3 [93] Shaopeng Zhai, Qi Zhang, Tianyi Zhang, Fuxian Huang, Haoran Zhang, Ming Zhou, Shengzhe Zhang, Litao Liu, Sixu Lin, and Jiangmiao Pang. vision-languageaction-critic model for robotic real-world reinforcement learning. arXiv preprint arXiv:2509.15937, 2025. 14 [94] Jiahui Zhang, Yusen Luo, Abrar Anwar, Sumedh Anand Sontakke, Joseph Lim, Jesse Thomason, Erdem Biyik, and Jesse Zhang. ReWiND: Language-guided rewards teach robot policies without new demonstrations. arXiv preprint arXiv:2505.10911, 2025. 14 [95] Tony Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning fine-grained bimanual manipulation with low-cost hardware. In RSS, 2023. 3, 6, [96] Jinliang Zheng, Jianxiong Li, Zhihao Wang, Dongxiu Liu, Xirui Kang, Yuchun Feng, Yinan Zheng, Jiayin X-VLA: SoftZou, Yilun Chen, Jia Zeng, et al. prompted transformer as scalable cross-embodiment arXiv.org, 2510.10274, vision-language-action model. 2025. 2, 7, 13 [97] Qinqing Zheng, Amy Zhang, and Aditya Grover. Online decision transformer. In ICML, 2022. 3 //www.tesla.com/fsd. 1 [78] Andrew Wagenmaker, Yunchu Zhang, Mitsuhiko Nakamoto, Seohong Park, Waleed Yagoub, Anusha and Sergey Levine. Nagabandi, Abhishek Gupta, Steering your diffusion policy with latent space reinforcement learning. In CoRL, 2025. 13 [79] Homer Rich Walke, Kevin Black, Tony Zhao, Quan Vuong, Chongyi Zheng, Philippe Hansen-Estruch, Andre Wang He, Vivek Myers, Moo Jin Kim, Max Du, et al. BridgeData v2: dataset for robot learning at scale. In CoRL, 2023. [80] Lirui Wang, Kaiqing Zhang, Allan Zhou, Max Simchowitz, and Russ Tedrake. Robot fleet learning via policy merging. In ICLR, 2023. 3 [81] Waymo. Demonstrably safe ai for autonomous drivURL https://waymo.com/blog/2025/12/ ing, 2025. demonstrably-safe-ai-for-autonomous-driving. 1 [82] Junjie Wen, Yichen Zhu, Jinming Li, Zhibin Tang, Chaomin Shen, and Feifei Feng. Dexvla: Visionlanguage model with plug-in diffusion expert for general robot control. arXiv preprint arXiv:2502.05855, 2025. 7 [83] Olivia Wiles, Sven Gowal, Florian Stimberg, SylvestreAlvise Rebuffi, Ira Ktena, Krishnamurthy Dj Dvijotham, and Ali Taylan Cemgil. fine-grained analysis on distribution shift. In ICLR, 2022. 2 [84] Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model soups: Averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In ICML, 2022. 3, 4, 7 [85] Yueh-Hua Wu, Xiaolong Wang, and Masashi Hamaya. Elastic decision transformer. In NeurIPS, 2023. 3, 5, 14 [86] Wenli Xiao, Haotian Lin, Andy Peng, Haoru Xue, Tairan He, Yuqi Xie, Fengyuan Hu, Jimmy Wu, Zhengyi Luo, Linxi Fan, et al. Self-improving vision-language-action arXiv models with data generation via residual rl. preprint arXiv:2511.00091, 2025. 14 [87] Prateek Yadav, Derek Tam, Leshem Choshen, Colin Ties-merging: Resolving In NeurIPS, 2023. Raffel, and Mohit Bansal. interference when merging models."
        },
        {
            "title": "APPENDIX",
            "content": "This supplementary material provides detailed analysis to substantiate our main findings. Section poses motivating questions to offer alternative perspectives on the work and might address potential questions. Section reviews broader related work on robustness from RL in manipulation. Section elaborates on system implementation and design alternatives, accompanied by the code implementation in the related code files. Section details experimental protocolsincluding hyperparameters, hardware configurations, and training strategieswith extended results provided in Section E. Finally, Section analyzes failure modes, Section outlines data ethics and corresponding licensing, and Section lists the contribution. A. Motivating Questions These are the questions that might be raised by the audience and the ones we think beyond this project: Q1. What is the relationship between stage advantage and RL? Our stage advantage follows the paradigm of advantage-weighted regression [60], which could be seen as an offline RL [74] where the monte carlo reward is calculated directly from data. However, as observed in the huge variance in the learned reward, advantage-weight diffusion policy [21] and its massive-scale validation on π 0.6 [2] parameterize network instead from the demonstration data. Stage advantage follows the paradigm of π 0.6, while improve from the baseline with more stable numerical result in advantage estimation and thus results in better performance of the trained policy. Q1. Why not online RL such as PPO? PPO [66] and its adaptation on diffusion-based policy such as DSRL [78] stand as another trend in RL to directly learn from the interaction with the environment, which is referred as online RL. These methods are fundamentally limited by real-world sample inefficiency. Unlike simulation, scaling physical experiments is constrained by the prohibitive costs of parallelization and resetting, constituting major bottleneck. In contrast, AWR [60] incurs only marginal labeling cost to bias the learning objective toward high-progress actions, thereby maximizing the utility of both human demonstrations and policy rollouts. Q2. Why choosing π0.5 as primary baseline? To identify robust starting point, we conducted preliminary evaluations of several prominent VLA models, including XVLA [96], GO-1 [8], UniVLA [9], and OpenVLA [34]. Despite claims of broad adaptability and robustness, these models failed to generalize to our experimental tasks. Even after extensive post-training with aligned hardware settings and exhaustive hyperparameter tuningwhere training loss converged satisfactorilythe policies achieved negligible success rates across 30 distinct trials (10 trials per garment). We provide these failure videos in this supplementary as well. Consequently, we excluded these models from the primary comparison to ensure our analysis focused on baselines capable of meaningful task completion. Q4. How to define/design indicative/informative advantage? This is critical direction for future work. Naive behavior cloning treats all demonstrated actions equally, despite many actions lacking explicit utility. Training on these non-informative segments can bias the policy toward meaningless behaviors, resulting in local optima where the robot repeats actions without making progress. Advantage weighting offers mechanism to filter these behaviors by emphasizing actions that yield high returns. However, our current formulation relies on heuristic proxyusing temporal progress to label advantagewhich assumes task completion is strictly monotonic. more robust solution would replace this heuristic with unsupervised advantage estimator capable of distinguishing truly instrumental actions from noise, independent of temporal linearity. Q5. How to define good robot data? Empirically, we find that data quality is governing factor in policy performance, accounting for success rate fluctuations between 20% and 60% under identical settings. Notably, we observe masking effect where certain algorithmic improvements yield significant gains on low-quality data but saturate or vanish when trained on high-quality demonstrations. Data quality seems to be the core part under current trend of imitation learning, yet not many literature seriously discuss that. To address this overlooked variable, we state replay-ability as one of the most important principles for data validity. trajectory is replay-able if open-loop re-execution leads to task completion (or significant progress) from similar initial state. This criterion not only filters corrupted demonstrations but also serves as diagnostic tool, exposing systematic hardware inconsistencies between the data collection Ptrain and inference Ptest environments. Q6. What is the common failure case? We observe two primary failure modes: 1) Spatial misalignment, where the policy fails to localize the correct grasping affordance; and 2) Policy stagnation, where the robot enters dead-loop of repetitive, non-productive actions. Both are illustrated in Figure 16. These failures underscore two fundamental deficiencies in current pre-trained models: lack of finegrained spatial understanding and insufficient long-horizon task planning. While we partially mitigate the former via Model Soups and the latter through Stage Advantage, these are extrinsic corrections. We argue that future pretrained weights must possess stronger intrinsic capabilities for spatial grounding and sequential logic to fully resolve these issues. Q6. What is the ground difference between different robotic foundation models? Resonating with our discussion on failure modes, the core distinction lies not in the models zero-shot capabilities, but in their fine-tuning dynamicsspecifically, their plasticity in acquiring novel spatial understanding and task planning skills during post-training. We observe that certain architectures (e.g., π0 and π0.5 demonstrate significantly higher adaptability than others, suggesting more robust initialization from pre-train weight for downstream learning. Consequently, the field requires new metrics that benchmark the intrinsic representational quality of these foundation models, rather than relying solely on success rates in simplistic evaluation environments. B. More Related Work 1) Discussion about RL for robustness: Reinforcement learning has become prevalent approach for post-training VLA foundation policies to improve manipulation robustness and precision, with applications spanning both simulation [47, 56, 55, 49, 45, 14] and the real world [52, 86, 51]. central challenge lies in fine-tuning large pre-trained models without destabilizing the learned representations. Advantage-conditioned approaches [2, 21, 38] allow full-model optimization by conditioning generation on estimated advantages, circumventing the need to differentiate through diffusion or flow-matching denoising processes [43]. The effectiveness of such methods hinges on the quality of the advantage signal. popular recipe employs vision-language models to score task progress [53, 94, 93, 23], from which advantages are recovered as A(s, a) = (s)V (s) [2] and used in advantage-weighted regression [85, 37]. This difference-of-values formulation, however, compounds independent estimation errors into noisy training signals, and the lack of stage awareness causes the value function to produce ambiguous predictions when visually similar states appear at different phases of multi-stage task [2, 46]. Our method addresses both limitations: instead of subtracting two noisy value predictions, we directly model advantage as single pairwise prediction fθ(s, s), yielding lower-variance signals by construction; and instead of estimating progress under single global objective, our Stage Advantage conditions on the current stage goal, explicitly resolving multi-valued ambiguity and providing accurate, stage-aware supervision for long-horizon policy optimization. 2) Control method discussion: While asynchronous inference reduces latency, existing methods face significant implementation hurdles. SmolVLA [70] employs naive chunk-switching strategy, resulting in severe prediction-execution misalignment and control instability. Similarly, the concurrent A2C2 [67] addresses misalignment by adding auxiliary correction heads, which necessitates architectural modifications. In contrast, our method augments asynchronous inference with temporal chunk-wise smoothing, incurring negligible computational or architectural overhead. C. Method Details 1) why we call it χ0: Our approach is grounded in the principle of Kinetics Aligned to Intelligence. Here, Kinetics defines the operational domainspanning training (Ptrain) and deployment (Ptest)while Intelligence denotes the inductive bias encoded in the parameter space (Qmodel). We designate this initial iteration as KAI 0. Noting the phonetic resemblance between KAI and the Greek letter χ, we name the system χ0, paying respect to the state-of-the-art π policy series. 2) MA details: We combine 4 checkpoints via Model Arithmetic, each trained on different subset of the training data. Checkpoints are weighted using inverse loss weighting: validation loss is computed as the mean prediction error on held-out set, and checkpoints with lower loss receive proportionally higher weights. We additionally consider two baselines: single best candidate, selected as the individual checkpoint with the lowest validation loss, and full data candidate, trained on the complete merged dataset across all subsets. 3) SA details: The advantage estimator is trained on pairs of frames sampled from the same episode at arbitrary timestamps, with the supervision target being their relative progress, defined as the subtraction of two states over their timestamps in an episode. When training χ0, we set the threshold ϵ = 0.3 following the hyperparameter in π 0.6. Training samples are ranked by their advantage values, and the top ϵ fraction is labeled as positive while the rest is treated as negative. Stages correspond to semantic sub-goals within each long-horizon task: two stages for task (flattening, folding), four for task (retrieving, flattening, folding, handover), and three for task (retrieving, dressing the rack, hanging). 4) TDA details: Standard DAgger data collection proceeds as follows: (1) Initialize the environment and start policy inference. (2) When the policy fails repeatedlydefined as failing to grasp the clothes for more than 10 consecutive attempts, or producing repetitive action patterns without progress for more than 10 stepsit is interrupted. On the software side, the policy stops receiving new observations and produces no further actions; on the hardware side, the robot is held at its current state and switched to tele-operation mode to accept human correction. (3) Both the human correction and the preceding policy rollout are saved. Heuristic DAgger simplifies this process by treating the failure state at the point of interruption as the initial state for collecting human expert demonstrations, without requiring online policy execution. TABLE I: Hyper-parameters for fine-tuning. TABLE II: Score standard (normalized to 100). Hyperparameter Value Task Sub-goals Score Data & Input Expert demonstration hours Action chunk length Execution frequency Optimization Training steps Batch size Optimizer Learning rate Cosine Decay Steps Conditioned noise level σ Gradient Clip Module-Specific MA: Number of checkpoints SA: Advantage threshold ϵ Infrastructure Training GPUs Inference GPUs 20 per task 50 100 Hz 80,000 128 AdamW 2.5105 10,000 [0.001, 1.0] 1.0 4 0.3 8 A100 RTX D. Experimental Details Fig. 13: Training loss curves of SA and π tation. SA has better convergence performance than π implementation in terms of loss value. 0.6-style implemen0.6-style their corresponding scoring weights are detailed in Table II. Task (Easy) Task T-shirt (Medium) Task Shirt (Medium) Task (Hard) Flatten garment 1st fold 2nd fold 3rd fold Retrieve & flatten 1st fold 2nd fold 3rd fold Stack to top-left Retrieve from basket Flatten Pull to right-side table Pull garment rightward Grasp collar Grasp hanger Insert hanger into sleeve Hook left collar on hanger Hang on standing rack +40 +20 +20 +20 +40 +15 +15 +15 +15 +30 +50 +20 +15 +15 +15 +20 +20 +15 1) Data, Training Strategy and Evaluation Criteria: Data Scale and Diversity. Data was collected at 30 Hz under Standard Operating Protocol (SOP) to ensure consistent execution quality and duration. The dataset comprises 2668 episodes for Task (avg. 30.41 s), 3519 for Task (avg. 34.40 s), and 2988 for Task (avg. 39.01 s). Note that these means include shorter DAgger interventions. To ensure diversity, we randomize initial conditions including object state (position, crumpling configuration, size, color) and environmental factors (illumination intensity and direction). Training Strategy and Hyperparameters. We conduct full-parameter fine-tuning on the open-source π0.5 model using Flow Matching objective. Each task is fine-tuned independently. Refer to Table for detailed list of hyperparameters. Evaluation Criteria (Average Score). We employ rule-based evaluation protocol that assigns partial credit for sub-goal completion. The specific sub-goals and 2) Hardware configuration and Inference details: As illustrated in Figure 6 in the main paper, our experimental setup includes two bimanual systems: one composed of AgilexRobotics Piper arms and the other of ARXRoboticsX X5 arms. Both dual-arm platforms feature two 6-DoF arms equipped with 1-DoF parallel gripper. key kinematic distinction lies in the 5th joint configuration: the Agilex Piper arm with pitch joint, whereas ARX X5 utilizes yaw joint. For perception, each system is equipped with three Intel RealSense D435i cameras (one fixed head-view and two wrist-view) capturing 640 480 RGB images. The visual sampling rate is synchronized at 30Hz for both data collection and inference, while the low-level joint controllers operate at higher frequency of 100 200 Hz. All inference are performed on Ubuntu 20.04 workstation equipped with an NVIDIA RTX 4090 GPU. 3) Empirical loss curve: Figure 13 compares the training loss curves. The proposed Stage Advantage (SA) demonstrates 0.6 baseline. This empirical evidence suggests that SA offers enhanced superior convergence characteristics relative to the π numerical stability within the advantage-weighted behavioral cloning framework. E. More Ablation 1) MA ablation on other tasks: Figure 14 details the performance of Model Arithmetic (MA) on Tasks and B. The results corroborate our primary finding: in most cases, MA consistently outperforms both the single-best candidate and the full-data (a) Ablations of MA on Task (b) Ablations of MA on Task Fig. 14: Ablations of MA on Tasks and B. Consistent with Task C, all MA variants outperform single-best and full-data baselines in success rate and throughput across both tasks. Notably in Task B, the retry cost for all MA variants decreases significantly. These performance gains and stability improvements are maintained in OOD validation settings. Fig. 15: Ablations of SA on Task C. SA demonstrates better numerical stability than π 0.6-style advantage baseline in terms of MSTD and SFR. Even though Direct + Stage variant sees performance drop on this non-staged task, the Direct advantage method alone still consistently outperforms π 0.6-style. Fig. 16: Visualization of Failure Case in Task A. baseline. However, the correlation regarding validation loss selection is less consistent. We attribute this discrepancy to the variable quality of the Out-Of-Distribution (OOD) data. We hypothesize that validation loss is reliable proxy for candidate selection only when the OOD data distribution sufficiently approximates Ptest. 2) SA ablation on other tasks: Figure 15 presents the ablation study of Stage Advantage (SA) on Task C. While numerical stability metrics mirror the improvements observed in Tasks and B, this stability does not translate equivalently to final task (a) Absolute Joint (b) Delta Joint Fig. 17: Ablations of control strategies and spatio-temporal augmentation on Task with different action representations. The figure compares performance using Absolute Joint (left) and Delta Joint (right) control. Across both action representations, we observe consistent trends: temporal chunk-wise smoothing significantly boosts inference performance, while combining it with RTC further improves task success rates across augmentation settings. (a) Absolute Joint (b) Delta Joint Fig. 18: Ablations of control strategies and spatio-temporal augmentation on Task with different action representations. Since Task is fine-grained task, overall SR is relatively low for both absolute and delta joint action control. However, temporal chunk-wise smoothing consistently outperforms these non-smoothing baselines. performance. We hypothesize that this discrepancy stems from the π0.5 pre-training distribution; the absence of hanging-specific priors likely limits the models high-level planning capabilities, rendering it unable to fully exploit the improved numerical stability. 3) TDA ablation on other tasks: We evaluate the impact of absolute versus delta joint state prediction on performance, as detailed in Figure 17 (Task A), Figure 19 (Task B), and Figure 18 (Task C). Extensive experiments validate the efficacy of our temporal chunk-wise smoothing across most of the settings. Notably, Task exhibits heightened sensitivity to action parameterization (absolute vs. delta) compared to Tasks and B. We attribute this to the high-precision dexterity required for the hanger insertion phase. F. Failure case analysis and qualitative result To answer Q6, we conducted comprehensive qualitative analysis by visualizing and categorizing all failure instances recorded during Task evaluation. The flattening stage proves to be the bottleneck of Task A, accounting for the majority of policy failures due to its high complexity. Consequently, DAgger and Heuristic DAgger are particularly effective here, as they naturally prioritize collecting recovery data in this critical phase where the robot is most prone to errors. G. Data ethics and license We will release the full code and data in the future. Data is under CC BY-NC-SA 4.0 license and the code is under Apache-2.0 license. (a) Absolute Joint (b) Delta Joint Fig. 19: Ablations of control strategies and spatio-temporal augmentation on Task with different action representations.While delta joint control underperforms absolute joint control on this task, temporal chunk-wise smoothing consistently improves inference performance across both representations and augmentation settings. H. Contributions All the names are ordered alphabetically by their first name in the first page. Below are the detailed taskforce synergy. Project Lead. Chonghao Sima, Li Chen Core Members. Checheng Yu, Chonghao Sima, Jin Chen, Kaiyang Wu, Lirui Zhao, Modi Shi, Yibo Yuan Model Arithmetic. Chonghao Sima, Modi Shi Stage Advantage. Lirui Zhao Train-Deploy-Alignment. Checheng Yu, Chonghao Sima, Kaiyang Wu, Yibo Yuan Data Collection. Checheng Yu, Chonghao Sima, Gangcheng Jiang, Kaiyang Wu, Lirui Zhao Deployment and Hardware Maintainance. Checheng Yu, Chonghao Sima, Gangcheng Jiang, Jin Chen, Kaiyang Wu, Shijia Peng Writing and Illustration. Checheng Yu, Chonghao Sima, Hongyang Li, Kaiyang Wu, Li Chen, Lirui Zhao, Modi Shi, Qingwen Bu, Tianyu Li, Yibo Yuan Discussion and Feedback. Hai Zhang, Haoguang Mai, Huijie Wang, Ping Luo, Shijia Peng Resource and Academic Supervision. Hongyang Li, Ping Luo"
        }
    ],
    "affiliations": [
        "Kinetix AI"
    ]
}