{
    "paper_title": "Echoes as Anchors: Probabilistic Costs and Attention Refocusing in LLM Reasoning",
    "authors": [
        "Zhuoyuan Hao",
        "Zhuo Li",
        "Wu Li",
        "Fangming Liu",
        "Min Zhang",
        "Jing Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Test-time compute allocation in large reasoning models (LRMs) is widely used and has applications in mathematical problem solving, code synthesis, and planning. Recent work has addressed this problem by scaling self-consistency and parallel thinking, adding generic ``thinking tokens'' and prompting models to re-read the question before answering. Unfortunately, these approaches either inject task-agnostic tokens or mandate heuristics that do not explain -- and often ignore -- the \\emph{spontaneous} repetition that many LRMs exhibit at the head of their internal chains. In contrast, we analyze and harness the model's tendency to restate the question, which we term the \\emph{Echo of Prompt (EOP)}, as a front-loaded, compute-shaping mechanism. We formalize its probabilistic cost by casting echo removal as rejection-based conditioning and defining the \\emph{Echo Likelihood Gap} $Δ\\mathcal{L}$ as a computable proxy. This provides the missing theoretical link that links early repetition to likelihood gains and downstream accuracy. However, it does not by itself specify how to exploit EOP. Consequently, we develop \\emph{Echo-Distilled SFT (ED-SFT)} to instill an ``echo-then-reason'' pattern through supervised finetuning, and \\emph{Echoic Prompting (EP)} to re-ground the model mid-trace without training. While promising, quantifying benefits beyond verbosity is non-trivial. Therefore, we conduct length and suffix-controlled likelihood analyses together with layer-wise attention studies, showing that EOP increases answer to answer-prefix attention in middle layers, consistent with an \\emph{attention refocusing} mechanism. We evaluate on GSM8K, MathQA, Hendrycks-MATH, AIME24, and MATH-500 under identical decoding settings and budgets, and find consistent gains over baselines. Code is available at https://github.com/hhh2210/echoes-as-anchors."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 6 ] . [ 1 0 0 6 6 0 . 2 0 6 2 : r Published as conference paper at ICLR 2026 ECHOES AS ANCHORS: PROBABILISTIC COSTS AND ATTENTION REFOCUSING IN LLM REASONING Zhuoyuan Hao1 Zhuo Li1 Wu Li1 Fangming Liu2,3 Min Zhang1 1Harbin Institute of Technology, Shenzhen, China 2Pengcheng Laboratory, Shenzhen, China 3Huazhong University of Science and Technology, China hzy2210@gmail.com jingli.phd@hotmail.com Jing Li1#"
        },
        {
            "title": "ABSTRACT",
            "content": "Test-time compute allocation in large reasoning models (LRMs) is widely used and has applications in mathematical problem solving, code synthesis, and planning. Recent work has addressed this problem by scaling self-consistency and parallel thinking, adding generic thinking tokens and prompting models to re-read the question before answering. Unfortunately, these approaches either inject taskagnostic tokens or mandate heuristics that do not explainand often ignore the spontaneous repetition that many LRMs exhibit at the head of their internal chains. In contrast, we analyze and harness the models tendency to restate the question, which we term the Echo of Prompt (EOP), as front-loaded, computeshaping mechanism. We formalize its probabilistic cost by casting echo removal as rejection-based conditioning and defining the Echo Likelihood Gap as computable proxy. This provides the missing theoretical link that links early repetition to likelihood gains and downstream accuracy. However, it does not by itself specify how to exploit EOP. Consequently, we develop Echo-Distilled SFT (EDSFT) to instill an echo-then-reason pattern through supervised finetuning, and Echoic Prompting (EP) to re-ground the model mid-trace without training. While promising, quantifying benefits beyond verbosity is non-trivial. Therefore, we conduct length and suffix-controlled likelihood analyses together with layer-wise attention studies, showing that EOP increases answer to answer-prefix attention in middle layers, consistent with an attention refocusing mechanism. We evaluate on GSM8K, MathQA, Hendrycks-MATH, AIME24, and MATH-500 under identical decoding settings and budgets, and find consistent gains over baselines. Code is available at https://github.com/hhh2210/echoes-as-anchors."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advancements in Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, often mediated by process known as Chain-of-Thought (CoT) prompting (Wei et al., 2022; Kojima et al., 2022; Wang et al., 2023; Yao et al., 2023). Inspired by the CoT paradigm, modern large reasoning models (LRMs) achieve strong performance on complex tasks by allocating significant test-time compute to think before answering (Wei et al., 2022; OpenAI, 2024; DeepSeek-AI et al., 2025; Qwen Team, 2025). common yet underexplored phenomenon in their reasoning traces is the tendency to begin by repeating the users prompt (see Figure 1 and A.3), behavior we term the Echo of Prompt (EOP). While uncontrolled repetition is known failure mode (the repeat curse (Yao et al., 2025)), explicit instructions to re-read or look-twice are known to improve performance (Xu et al., 2024; Zou et al., 2024). The spontaneous EOP that initiates complex reasoning trace, however, has remained largely unanalyzed.This initial echo raises critical question: Is it superfluous artifact of the training process, or does it serve functional role in reasoning? # Corresponding author. Published as conference paper at ICLR 2026 User Query: Determine the radius of cylindrical can given the area of label... and the height... Models Echo: <think>Okay, let me see. The problem is asking for the radius of cylindrical can. They give the area of the label as quadratic expression..., and the height is also given... Figure 1: An illustration of the Echo of Prompt (EOP). Left: An example of models thinking process starting with an echo of the users query. Right: The frequency of EOP across several open-source models on the GSM8K dataset, as measured by our trained MLP probe (see A.4). This paper confronts this question directly, presenting the first systematic study to isolate, analyze, and harness this emergent behavior as powerful cognitive aid. We hypothesize that the EOP serves as an intrinsic attention-refocusing mechanism, learned strategy to ground subsequent reasoning steps in the salient details of the original query. To validate this, we provide dual theoretical and empirical analysis: 1. Probabilistic Framework (3). We introduce rejection sampling framework to formalize the EOP, defining the Echo Likelihood Gap to quantify its probabilistic cost. 2. An Attention-Based Mechanistic Explanation (3.3). We uncover the underlying mechanism through attention analysis, showing that EOP serves to refocus the models attention, an act that correlates with correctness. 3. Practical Methods and Empirical Validation (4). We translate this insight into two effective methods. Echo-Distilled SFT (ED-SFT) instills this behavior via fine-tuning, yielding significant performance gains that generalize across data distributions. Concurrently, Echoic Prompting (EP) provides training-free inference strategy that re-grounds the model on the prompt, outperforming strong baselines. Taken together, our findings reframe the EOP from superficial flaw into functional strategy for cognitive self-alignment. This work not only solves the puzzle of the initial repetition but also offers new insights into how models learn to structure their own thought processes for complex reasoning. This paper makes three main contributions: We propose novel probabilistic framework based on rejection sampling to quantify the cost of an echo, introducing the Echo Likelihood Gap (L) as core metric to measure the alignment between models natural tendency and echo-free reasoning. We present two practical methods to leverage this phenomenon: Echo-Distilled SFT (EDSFT), fine-tuning approach to instill echo behavior, and Echoic Prompting (EP), training-free inference technique that achieves similar gains by re-introducing the prompt. We provide mechanistic explanation for the effectiveness of EOP. Through attention analysis, we demonstrate that echoing acts as an intrinsic refocusing mechanism, guiding the model to concentrate on critical problem details that are often overlooked."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Computation In Reasoning: Efficiency And Effectiveness. The scaling of test-time computation has demonstrably improved reasoning in LRMs, but often at the cost of significant overhead from long and sometimes redundant chains of thoughtthe overthinking phenomenon (Sui et al., 2025). One line of research tackles this by improving computational efficiency, developing methods 2 Published as conference paper at ICLR 2026 like early exiting or step compression to reduce wasteful generation. complementary approach, more aligned with our work, focuses on computational effectiveness. For instance, several studies have found that compelling model to explicitly restate or re-read the input question can enhance reasoning (Xu et al., 2024; Mekala et al., 2024). These methods treat repetition as an instructed heuristic to re-align the model. Our work bridges these views by analyzing the spontaneous emergence of echoes, not as heuristic to be added, but as an intrinsic, learned strategy that trades small initial computational cost for more effective and focused downstream reasoning. Attention-Refocusing Mechanisms. The challenge of maintaining focus over long contexts is known issue in language models, often manifesting as positional bias where information in the middle of long input is under-utilized (Liu et al., 2024). This issue is analogous to attention drift in computer vision, where attention can shift away from salient regions during sequence generation (Cheng et al., 2017). To counteract these effects, various explicit mechanisms have been proposed. These range from model-level architectural changes and calibration methods that correct positional biases (Cheng et al., 2017; Hsieh et al., 2024), to inference-time interventions that re-weight attention or re-inject evidence to steer the model back to relevant information (Gu et al., 2024; Zou et al., 2024). Our work identifies different, more intrinsic phenomenon: we show that the initial Echo of Prompt itself can serve similar refocusing role, where the model spontaneously restates salient parts of the prompt to condition its subsequent generation, without any external guidance or modification."
        },
        {
            "title": "3 THE PRICE OF AN ECHO: A PROBABILISTIC COST FRAMEWORK",
            "content": "This section formalizes the impact of prompt echoes using probabilistic framework that goes beyond simple text deletion. The core idea is to treat the presence or absence of an echo as probabilistic event, which enables formal definition of hypothetical echo-free model and measurement of the echos likelihood cost. The analysis proceeds in three parts: 3.1 introduces the rejection sampling framework, 3.2 measures the resulting likelihood cost, and 3.3 investigates the echos function as an attention-refocusing mechanism. To ground this framework, the probabilistic and attention analyses in this section are performed on the GSM8K benchmark using DeepSeek-R1-Distill-Llama-8B. We use exact match accuracy as the primary task metric and log full model outputs for likelihood and attention analysis. 3.1 PROBLEM FORMULATION This framework allows us to precisely quantify the effect of echo removal as probabilistic conditioning event, laying the groundwork for the metric we introduce next. Let be an input prompt and be generated output sequence (i.e., reasoning trace). We consider base large reasoning model parameterized by θ, which defines conditional probability distribution πθ(yx) over possible output sequences. Our first step is to formally identify which sequences contain Echo of Prompt. We define predicate, implemented by separately trained MLP (see A.4), that partitions the output space into two disjoint sets, = Ytrim Yecho. Here, Ytrim is the set of all trimmed sequences that are deemed echo-free, and Yecho is its complement. We can then define an indicator function: (cid:26)1 if is echo-free, otherwise. 1yYtrim = (1) Assuming the model has non-zero probability of producing at least one echo-free trace (Zx > 0), we define our target, the trimmed distribution τθ(yx), as the base distribution πθ conditioned on the event that the output is echo-free: τθ(yx) = πθ(yx) 1yYtrim (cid:80) πθ(yx) yYtrim . (2) Here, the denominator Zx is the partition function, which normalizes the distribution by summing the probabilities of all echo-free sequences under the base model πθ: (cid:88) Zx = πθ(yx) = Eyπθ (x)[1yYtrim]. (3) yYtrim 3 Published as conference paper at ICLR 2026 Table 1: Echo metrics on GSM8K. Averages over samples in each group, where denotes the number of samples. The Suffix-only Likelihood Gap measures the per-token log-likelihood difference on the reasoning suffix when conditioned with versus without the echo prefix. Echo Likelihood Gap (per-token) Extended Echo Metrics Group Mean Correct Wrong 819 500 2.5231 2.4421 Std σ(L) 0.7786 0.7657 Neg. ratio (%) (per token) /#removed Suffix-only gap Avg. removed tokens 0.12 0.00 2.4614 2.3666 0.01103 0. 1.1449 1.2938 219.7 218.7 This distribution represents our targeted behaviora hypothetical model constrained to produce only echo-free outputs. However, directly computing τθ is intractable because the partition function Zx requires summing over all possible echo-free sequences. This intractability motivates the use of rejection sampling to reason about and sample from τθ without needing to calculate Zx explicitly. The rejection sampling view provides principled way to think about echo suppression, but to measure its effect, we need concrete metric. Our goal is to quantify how much preference the model shows for raw, echo-containing trace versus its trimmed, echo-free counterpart. Given raw trace yraw and its echo-trimmed counterpart ytrim, we define the (length-normalized) average log-likelihood L(y) = 1 y (cid:88) t=1 log πθ(yt x, y<t) (4) (nats/token). The Echo Likelihood Gap is = L(yraw) L(ytrim). positive means the model prefers the echo-containing trace. Unless otherwise noted, we report nats/token. While our framework defines the echo-free distribution τθ, the partition function Zx required to compute it is intractable. This motivates practical, sample-based alternative: the Echo Likelihood Gap (L). This metric serves as direct proxy for the probabilistic cost of an echo by comparing the average log-likelihood of generated trace (yraw) against its trimmed counterpart (ytrim). positive indicates that the model assigns higher likelihood to the sequence containing the echo, quantifying the price of this behavior on per-sample basis. This leads to our central question: is there positive relationship between this probabilistic cost and the models final reasoning accuracy? In other words, does spending probability on an echo lead to better performance? The following sections are dedicated to empirically validating this trade-off. 3.2 THE ECHO LIKELIHOOD GAP IN PRACTICE Table 1 and Figure 2 highlight our central empirical finding: larger probabilistic investment in an Echo of Prompt (EOP) strongly correlates with correct final answers. To formalize this, we introduce two metrics: the overall Echo Likelihood Gap (L), which measures the total probabilistic cost, and the Suffix-only Likelihood Gap (Lsuffix), which isolates the echos influence on subsequent reasoning. Defining the Likelihood Gaps. Our primary metric, the Echo Likelihood Gap (L), is defined in 3 as the difference in average per-token log-likelihood between raw trace yraw and its trimmed counterpart ytrim. To isolate the echos influence on the subsequent reasoning steps, we introduce more granular metric. For raw trace yraw composed of an echo prefix and reasoning suffix (i.e., yraw = [e, s]), we compare the models likelihood of generating with and without the conditioning prefix e. The Suffix-only Likelihood Gap is defined as: Lsuffix = L(s x, e) L(s x), (5) where L(s ) is the average per-token log-likelihood of the suffix under the given context. positive value indicates that the echo prefix makes the subsequent reasoning trace appear more probable to the model. 4 Published as conference paper at ICLR 2026 Figure 2: Echo metrics on GSM8K. Left: High-resolution histogram of removed echo-prefix lengths (10-token bins) for correct and wrong traces; most mass lies between roughly 200 and 240 tokens. Right: Echo Likelihood Gap (per-token) stratified by removed-prefix length bin; the gap remains positive across all bins. Analysis of Results. The overall Echo Likelihood Gap reveals clear correlation with correctness. As shown in Table 1, the Correct group (N = 819) has larger average gap than the Wrong group (N = 500): = 2.5231 vs. 2.4421. This positive difference (+0.0811 nats/token) indicates that larger total probabilistic investment in echoing co-occurs with correct final answers. We further validate this relationship with logistic regression in the Appendix, confirming as significant positive predictor of correctness, even after controlling for trace length. Interestingly, the Suffix-only Likelihood Gap is slightly larger for the Wrong group (1.2938 vs. 1.1449). While this seems counter-intuitive, it does not contradict our main finding. It suggests that while echoes make subsequent reasoning seem more plausible in general (as both values are positive), they may also act as form of \"confirmation bias,\" slightly strengthening the models confidence in locally coherent but ultimately flawed reasoning paths. The determinative factor for correctness remains the overall likelihood trade-off captured by L, as explained by the likelihood decomposition in A.7. Sanity Checks. Before further analysis, we perform several checks to validate as metric. First, for traces without detected echo, is definitionally zero, as the raw and trimmed sequences are identical. Second, we confirm that correlates positively with the number of removed tokens in the echo prefix, indicating that longer echoes correspond to larger likelihood gap. Finally, the data in Table 1 confirms that the suffix-only likelihood gap on the shared reasoning trace remains positive, confirming the echos influence extends beyond the prefix itself. These checks establish as robust measure of the echos probabilistic impact. LengthAnd Suffix-Controlled Analysis. To ensure this gap is not merely length artifact, we conduct length-stratified analysis. As shown in Figure 2, the remains consistently positive across different trace lengths. This indicates that the Echo of Prompt (EOP)s contribution is robust and also improves the models scoring on the subsequent, shared reasoning steps. Distribution Of Removed Echo Lengths. We further examine the length distribution of the removed echo prefixes (Figure 2, left). The distribution is heavy-tailed, with most prefixes falling between roughly 200 and 240 tokens (mean 219, median 226), confirming that the Echo of Prompt (EOP) constitutes non-trivial segment of the generation that acts as probabilistic sink. This distribution reveals that echo prefixes consistently consume substantial portions of the models output budget, with most instances removing more than 200 tokens of echoed content. 5 Published as conference paper at ICLR"
        },
        {
            "title": "3.3 UNVEILING THE MECHANISM: ECHOES AS ATTENTION REFOCUSING",
            "content": "To understand why prompt echoing is effective, we analyze the models attention patterns during generation. We hypothesize that re-introducing the original prompt effectively refocuses the models attention on the core problem statement, preventing drift during extended reasoning chains. Attention Redistribution After Echo Removal. We investigate the mechanism underlying the Echo of Prompts effectiveness, hypothesizing that it serves to refocus the models attention. To test this, we compute two attention metrics on the original outputs: (i) attention from answer tokens to the question tokens, and (ii) attention from answer tokens to the answer prefix. Formally, let A(l) RT be the head-averaged attention matrix at layer for full sequence of tokens. We define the average attention weight from set of query tokens with indices SQ to set of key tokens with indices SK as: Attn(l)(SQ SK) = 1 SQ (cid:88) (cid:88) A(l) ij . iSQ jSK (6) For our answeranswer-prefix metric, SQ comprises the indices of all tokens in the generated reasoning trace, while SK contains the indices of the initial tokens of that same trace. This metric quantifies the degree to which subsequent reasoning steps are grounded in the models own initial problem interpretation. For all results reported in this section, including the aggregate statistics in Table 2 and the layer-wise analysis in Figure 3, the prefix length is dynamically set to the persample echo length estimated by our MLP probe. This allows for precise analysis of the actual echos effect. As the results show, correctly solved problems consistently exhibit stronger attention to the answer prefix than incorrect ones, supporting our attention refocusing hypothesis. Layer-Wise Attention Dynamics. To further understand where the attention refocusing occurs within the models architecture, we conducted fine-grained layer-wise analysis across all 32 layers. Figure 3 visualizes the attention weight distribution, revealing distinct patterns between correct and incorrect reasoning traces. Figure 3: Layer-wise attention weight distribution on GSM8K (DeepSeek-R1-Distill-Llama-8B) for Left: answeranswer-prefix and Right: answerquestion. The blue lines represent correct reasoning traces while orange lines represent incorrect ones. The attention refocusing effect is most pronounced in layers 7-18 for answeranswer-prefix, with correct traces maintaining consistently higher attention weights. The layer-wise analysis localizes the EOP effect to the models reasoning bottleneck. We observe Middle-Layer Dominance where the attention gap peaks in layers 718, consistent with findings that intermediate layers govern reasoning aggregation. Furthermore, the Differential Impact where correct traces attend significantly more to the answer-prefix (peak 3%) than to the original question ( < 1%)confirms that the echo acts as distinct working memory anchor, actively refocusing the model on the problem statement during critical computation steps. Interestingly, the early layers (1-6) show minimal differences between correct and incorrect groups, with both trajectories nearly overlapping. This suggests that low-level token processing remains largely unaffected. Crucially, as shown in Figure 3 Right, the answerquestion attention remains closely matched across all layers, serving as valuable negative control. 6 Published as conference paper at ICLR 2026 Table 2: Average attention weights (%) on GSM8K (DeepSeek-R1-Distill-Llama-8B) with probeestimated prefix length. Global statistics and layer-specific analysis showing attention from answer tokens to question and answer prefix. Metric Correct Wrong Diff (CW) Global Statistics Last-layer: answer question Last-layer: answer answer-prefix All-layers mean: answer question All-layers mean: answer answer-prefix Peak Effect Layers (7-18) Layers 7-18: answer question Layers 7-18: answer answer-prefix 5.77% 5.54% 13.69% 10.41% 8.00% 8.45% 8.49% 10.64% 12.17% 11.51% 14.45% 11.58% +0.23% +3.28% +0.45% +2.15% +0.66% +2.87% Table 3: Layer-wise discriminability (Correct vs. Wrong) aggregated into layer groups. Metrics are computed on answeranswer-prefix and answerquestion attention. The mid-layer group shows the strongest effect size (d) for answeranswer-prefix. Layer Group (layers) AUC (AnsPref) (AnsPref) AUC (AnsQ) (AnsQ) Early (06) Mid (718) Late (1931) 0.716 0.719 0.723 0.820 0.832 0.828 0.628 0.585 0.563 0.482 0.303 0.184 This confirms that the performance gain is not attributable to simple, uniform increase in attention to the original question. Instead, the discriminative signal emerges in Figure 3 Left, where the divergence in answeranswer-prefix attention begins at layer 7. We report mean s.e.m. across samples; focusing on layers 718 reveals group difference of (CW)0.66% for answerquestion and more substantial 2.87% for answeranswer-prefix. This supports our hypothesis that the Echo of Prompt acts as cognitive scaffold for higher-level reasoning; it is not merely about re-reading the question, but about anchoring the subsequent reasoning process to stable internal representation, mechanism that strongly correlates with correctness. To ensure our findings are robust and not merely an artifact of the dynamically-set prefix length, we conducted an ablation study using fixed prefix lengths. The results, detailed in A.5, confirm that the attention gap persists across several fixed prefix lengths, supporting our conclusion that EOPs function is genuine attention refocusing. Layer-Wise Discriminability. To quantify where attention refocusing emerges, we compute layer-wise discriminability between Correct and Wrong groups using AUC and Cohens d. Specifically, for each layer l, we treat the attention scores from the Correct traces (A(l) ) and Wrong traces (A(l) ) as two distributions. The Area Under the ROC Curve (AUC) measures how well attention at given layer classifies trace as correct. We also compute Cohens to quantify the effect size: d(l) = µ(A(l) ) µ(A(l) ) s(l) , (7) where µ denotes the mean and s(l) is the pooled standard deviation for layer l. We also aggregate layers into three groups to analyze broader trends. Table 3 shows that the mid-layer group (7-18) exhibits the strongest effect size on answeranswer-prefix (Cohens d=0.832), with its AUC being comparable to the late-layer group. In contrast, the answerquestion discriminability remains significantly lower across all groups, serving as negative control. These statistics, combined with the attention trajectories (Figure 3), strongly indicate that EOPs primary mechanism is to refocus representations within the mid layers ( layers 7 through 18), anchoring subsequent reasoning to the answer prefix. 7 Published as conference paper at ICLR Table 4: Echo reinsertion ablation on the wrong-subset GSM8K traces for several models. Model Echo-free EM (%) Echo reinsertion EM (%) DeepSeek-R1-Distill-Llama-8B Qwen3-8B Qwen3-8B-Base (no CoT) 15.85 21.34 10.56 26.22 29.27 10."
        },
        {
            "title": "4.1 ECHO REINSERTION AS A CAUSAL INTERVENTION",
            "content": "We construct an interventional experiment that starts from failed GSM8K completions produced by several models (DeepSeek-R1-Distill-Llama-8B, Qwen3-8B, and the non-reasoning Qwen3-8BBase). For each wrong example we (i) truncate an echo-free trace to 50% of its tokens to obtain shared prefix, (ii) resume generation either directly (echo-free) or after inserting the template phrase now need to look back at the question again: (echo reinsertion), and (iii) score the new completions with the standard GSM8K exact-match script.Both branches see identical questions, prefixes, decoding parameters, and random seeds, isolating the causal impact of the injected echo. The intervention confirms that echoes are causally helpful for reasoning-capable models: forcing short echo before resuming the chain yields sizable EM gains (+10.4 and +7.9 points for DeepSeekR1-Distill-Llama-8B and Qwen3-8B, respectively), while the non-reasoning base model shows no improvement. This null result for the base model is expected, as it lacks the instruction-following and reasoning priors typically acquired via RL to utilize the re-injected context zero-shot. This interpretation is reinforced by our ED-SFT results  (Table 5)  , where the base model exhibits the largest relative improvement (+3.4 points) when the reasoning capability and echo strategy are instilled simultaneously. Qualitatively, we observe the reinsertion branch revisiting the original quantities and constraints (Figure 9), whereas the echo-free branch continues the drifting reasoning that led to the initial error. This interventional evidence complements our correlational analyses (3) and grounds the attention refocusing hypothesis in an explicit cause-and-effect experiment. 4.2 PERFORMANCE GAINS FROM ECHO-DISTILLED SFT (ED-SFT) Having established that Echo of Prompt correlates with improved reasoning performance, we investigate whether this behavior can be systematically instilled through targeted training. Our Supervised Fine-Tuning (SFT) methodology is inspired by recent work (Team, 2025). The core hypothesis is that explicitly training models on echo-prefixed traces enhances their problem-solving approach: the echo phase forces deeper engagement with problem constraints and establishes stronger foundation for subsequent reasoning steps. Methodology. We develop Echo-Distilled SFT (ED-SFT), supervised fine-tuning method that embeds this echo-then-reason pattern as learned behavior. We first construct shared pool of high-quality teacher traces on GSM8K by querying capable teacher model, gpt-oss-120B, with standard CoT prompt that wraps the reasoning in single <think> block and requires the final answer to be plain value. We automatically verify that the final answer exactly matches the ground-truth solution and discard any trace that fails this check. This pool of verified (question, CoT, answer) triples is the common source for both ED-SFT and the normal-SFT baseline. To obtain ED-SFT data, we encourage an explicit echo-then-reason pattern at the head of the trace. We train small MLP probe to detect whether an early Echo of Prompt segment is present. For traces flagged as missing EOP, we call gpt-oss-120B once more with an edit instruction that minimally inserts short echo-style opening that restates the question (e.g., Okay, let me see. The problem is asking: . . . ) while preserving the subsequent reasoning and final answer. Traces that already contain an echo are kept unchanged. After editing, we re-run the automatic checker and drop any example whose final answer no longer matches the gold label. The resulting ED-SFT dataset therefore differs from the standard CoT pool only by the presence of an initial echo segment. 8 Published as conference paper at ICLR 2026 Table 5: Supervised fine-tuning with distilled CoT data improves mathematical reasoning. -ED-SFT denotes models fine-tuned on our EOP distilled dataset. -normal-SFT refers to the normal CoT distilled dataset.Base is the pretrained model; the unmarked variant is instruction-tuned. Best scores in each column are in bold. GSM-8K Model Hendrycks-MATH MathQA Base + ED-SFT + normal-SFT Base + ED-SFT + normal-SFT Base + ED-SFT + normal-SFT Strict EM Flex EM 79.4 94.2 (+3.4) 90. 87.49 93.1 (+2.8) 90.3 67.6 78.2 80.5 Qwen3-8B-Base 80.5 94.2 (+3.4) 90.8 Qwen3-8B (Instruct Version) 88.1 93.4 (+3.3) 90. DeepSeek-Distill-Llama-8B 66.1 79.7 (+0.2) 79.5 31.0 58.8 (+11.8) 47.0 49.2 53.7 (+1.9) 51.8 31.6 34.8 (+3.4) 31.4 0.76 10.0 (+8.2) 1. 0.8 6.1 (+1.1) 5.0 0.38 3.0 (+2.24) 0.76 For the normal-SFT baseline we again start from the same verified teacher traces but remove the echo segment, keeping the reasoning untouched. Because the MLP probe is binary EOP detector rather than span localizer, we delegate span selection to the teacher: when the probe predicts EOP presence, we prompt gpt-oss-120B to delete the echo prefix under do not change the reasoning or final answer instruction, and we discard any sample whose answer changes. This yields paired ED-SFT and normal-SFT corpora that are nearly identical token-wise and differ primarily in the presence or absence of the initial echo. On GSM8K, the inclusion of the echo prefix results in longer average sequence length for ED-SFT compared to normal-SFT (175 vs. 136 tokens). Experimental Setup. To test the echo strategys effectiveness at different stages of model training, we fine-tuned models from two families: Qwen3 (8B) and Deepseek-distill-Llama-8B. For the Qwen3 family, we experimented on two distinct versions: the pre-trained base model (Qwen38B-Base) and the final, fully instruction-tuned model (Qwen3-8B). For each model, we applied our SFT procedure to produce both -ED-SFT and -normal-SFT variants. All fine-tuning runs use the same optimizer (AdamW), learning-rate schedule, batch size, maximum sequence length, and number of training steps; the only difference is whether the training traces come from the echo-augmented (ED-SFT) or echo-trimmed (normal-SFT) versions of the same teacher CoTs. We evaluated all models on suite of mathematical reasoning benchmarks (GSM8K, MathQA, and Hendrycks-MATH) to assess generalization under distribution shift, as fine-tuning was performed only on the GSM8K training set with 7k samples. Results. As shown in Table 5, SFT with our distilled data yields substantial and consistent performance improvements. Crucially, these gains appear on both the pre-trained base and the fully-aligned instruction-tuned models. Fine-tuning the base model (Qwen3-8B-Base-echo-SFT) achieves remarkable gain of +3.4 points on GSM-8K, while fine-tuning the already capable instruction-tuned model (Qwen3-8B-echo-SFT) still provides solid boost of +2.8 points. Cross-Model Generalization. The effectiveness of Echo-Distilled SFT extends across different model architectures. For DeepSeek-distill-llama-8B, we observe consistent improvements, with particularly strong gains on benchmarks that differ distributionally from GSM8K, such as MathQA (+3.4 points) and Hendrycks-MATH (+2.24 points). The consistent success on both base and instruct models strongly suggests that the Echo of Prompt (EOP) is fundamental and transferable cognitive alignment strategy, not merely an artifact of existing instruction tuning. Mechanistic Alignment With Attention Analysis. The success of ED-SFT can be understood through the lens of our attention analysis (3.3). The layer-wise attention patterns reveal that models trained with echo-prefixed traces naturally develop stronger attention connections in middle layers 9 Published as conference paper at ICLR 2026 Figure 4: Echoic Prompting (EP) vs. TTTS on AIME24 (left) and MATH-500 (right). (7-18), where we observed the most significant differences between correct and incorrect reasoning (1.73% increase in answeranswer-prefix attention). This suggests that ED-SFT effectively instills the attention refocusing mechanism we identified in our analysis, teaching models to leverage these critical layers for maintaining problem-relevant attention throughout the reasoning process. To further substantiate this, we analyzed the answeranswer-prefix attention gap (CorrectWrong) specifically within the critical mid-layer block (layers 718) across our model variants. This targeted metric confirms that ED-SFT most effectively strengthens this mechanism, exhibiting the largest attention gap (+3.20 pp) compared to the base model (+1.90 pp) and the normal SFT variant (+2.40 pp). This finding provides direct evidence that ED-SFT successfully instills the desired refocusing behavior where it is most impactful. Full statistics are provided in A.2. 4.3 ECHOIC PROMPTING (EP): TRAINING-FREE ENHANCEMENT The Echoic Prompting (EP) Method. Our proposed Echoic Prompting (EP) strategy is training-free method designed to enhance reasoning capabilities at inference time. The core idea is to re-engage the model by echoing the original prompt. Specifically, after the model produces an initial reasoning chain, we append reminder phrase such as look back at the question again followed by the original question itself. This intervention encourages the model to revisit the problems context and continue generating more grounded response. Unlike methods that inject generic, task-agnostic stimuli, EP re-grounds the model with task-specific context from the original query and shows consistent gains over 2 math reasoning datasets following TTTSs settings. Experimental Setup. To evaluate the effectiveness of EP, we compare it against strong baseline, Thinking Token based Test-time Scaling (TTTS) (Qian et al., 2025), which artificially inserts generic thinking tokens (e.g., So, Hmm) to spur reasoning. For fair comparison, we reproduce TTTS following its official implementation. Both methods are evaluated on the DeepSeek-R1-DistillLlama-8B model, using the vLLM backend with deterministic decoding (temperature=0.0). Results. As shown in Figure 4, our EP approach consistently and substantially outperforms TTTS across both AIME24 and MATH-500. The performance gains are robust under identical decoding and budget settings. This indicates that re-grounding the model on the input via natural echo of the prompt is more effective than injecting generic, artificial thinking tokens."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We systematically investigated the Echo of Prompt (EOP), the tendency of large reasoning models to repeat users query before solving. We introduced rejection-sampling framework to define the Echo Likelihood Gap and, through attention analysis, demonstrated that EOP refocuses attention on task-critical information in middle layers. To harness this, we proposed Echo-Distilled SFT (ED-SFT) and Echoic Prompting (EP), both yielding consistent gains over baselines on multiple math benchmarks, demonstrating improved robustness under distribution shifts. Our work reframes early repetition as beneficial cognitive primitive and advocates for cultivating beneficial thought processes, bridging the gap between emergent phenomena and deliberate cognitive design. 10 Published as conference paper at ICLR"
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "All authors adhere to the ICLR Code of Ethics. Our research focuses on understanding and improving the reasoning capabilities of large language models, foundational scientific goal. The datasets used for fine-tuning and evaluation, such as GSM8K and MathQA, are standard public benchmarks in the field. The synthetic data used for SFT was generated by large proprietary model GPT-OSS-120B, and the annotations for our MLP probe were assisted by GPT-4.1, as detailed in our LLM Usage Disclosure in A.1. We acknowledge that the models used in this study may inherit biases from their original, opaque training data. Our work does not introduce new applications with foreseeable negative societal impacts. We believe that deeper understanding of emergent behaviors like the Echo of Prompt contributes to the development of more transparent and reliable AI systems."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "To ensure the reproducibility of our findings, we commit to releasing our source code, including scripts for data processing, training, and evaluation, upon publication. Our work relies on publicly available models, including the Qwen3 and DeepSeek series, and standard benchmarks such as GSM8K, MathQA, and Hendrycks-MATH. The methodology for our Echo-Distilled SFT data generation is detailed in A.8. The design and training of the MLP probe used for echo detection are fully described in A.4, and all experimental hyperparameters and evaluation settings are detailed in 4. The theoretical claims in 3 are self-contained within the paper."
        },
        {
            "title": "ACKNOWLEDGEMENTS",
            "content": "Science Shenzhen supported in part by National Natural Science Foundation of China This work was (62476070), Program (JCYJ20241202123503005, GXWD20231128103232001, ZDSYS20230626091203008, KQTD20240729102154066), Department of Science and Technology of Guangdong (2024A1515011540) and National Key R&D Program of China (SQ2024YFE0200592). This work was also supported in part by the Major Key Project of PCL under Grant PCL2025A10 and PCL2024A06, and in part by the Shenzhen Science and Technology Program under Grant RCJC20231211085918010. and Technology"
        },
        {
            "title": "REFERENCES",
            "content": "Zhanzhan Cheng, Fan Bai, Yunlu Xu, Gang Zheng, Shiliang Pu, and Shuigeng Zhou. Focusing attention: Towards accurate text recognition in natural images. arXiv preprint arXiv:1709.02054, 2017. DeepSeek-AI et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. URL https://arxiv.org/abs/2501. 12948. Zhuohan Gu, Jiayi Yao, Kuntai Du, and Junchen Jiang. Llmsteer: Improving long-context llm inference by steering attention on reused contexts. arXiv preprint arXiv:2411.13009, 2024. Cheng-Yu Hsieh, Yung-Sung Chuang, Chun-Liang Li, Zifeng Wang, Long Le, Abhishek Kumar, James Glass, Alexander Ratner, Chen-Yu Lee, Ranjay Krishna, and Tomas Pfister. Found in the middle: Calibrating positional attention bias improves long context utilization. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics: ACL 2024, 2024. URL https://aclanthology.org/2024.findings-acl. 890/. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In Advances in Neural Information Processing Systems (NeurIPS), 2022. URL https://papers.nips.cc/paper_files/paper/2022/ hash/8bb0d291acd4acf06ef112099c16f326-Abstract-Conference.html. 11 Published as conference paper at ICLR 2026 Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 2024. URL https://aclanthology.org/ 2024.tacl-1.9/. Rajasekhar Reddy Mekala, Yasaman Razeghi, and Sameer Singh. Echoprompt: Instructing the model to rephrase queries for improved in-context learning, 2024. OpenAI. Gpt-4.1. Online documentation, 2024. URL https://platform.openai.com/ docs/models#gpt-4-1. Accessed 2025-08-09. OpenAI. llms. to learning-to-reason-with-llms/, 2024. reason with"
        },
        {
            "title": "Learning",
            "content": "https://openai.com/index/ Chen Qian, Dongrui Liu, Haochen Wen, Zhen Bai, Yong Liu, and Jing Shao. Demystifying reasoning dynamics with mutual information: Thinking tokens are information peaks in llm reasoning. arXiv preprint arXiv:2506.02867, 2025. Qwen Team. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. URL https: //arxiv.org/abs/2505.09388. Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Na Zou, Hanjie Chen, and Xia Hu. Stop overthinking: survey on efficient reasoning for large language models, 2025. URL https://arxiv.org/abs/ 2503.16419. NovaSky Team. Sky-t1: Train your own o1 preview model within $450. https://novaskyai.github.io/posts/sky-t1, 2025. Accessed: 2025-01-09. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain-of-thought reasoning in lanIn International Conference on Learning Representations (ICLR), 2023. URL guage models. https://openreview.net/forum?id=1PL1NIMMrw. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 2022. Xiaohan Xu, Chongyang Tao, Tao Shen, Can Xu, Hongbo Xu, Guodong Long, Jian-Guang Lou, and Shuai Ma. Re-reading improves reasoning in large language models. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, Miami, Florida, USA, 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.emnlp-main.871/. Junchi Yao, Shu Yang, Jianhua Xu, Lijie Hu, Mengdi Li, and Di Wang. Understanding the repeat curse in large language models from feature perspective. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Findings of the Association for Computational Linguistics: ACL 2025, Vienna, Austria, July 2025. Association for Computational Linguistics. URL https://aclanthology.org/2025.findings-acl.406/. Shunyu Yao, Dian Yu, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, thoughts: Deliberate problem solving with large Information Processing Systems (NeurIPS), URL https://proceedings.neurips.cc/paper_files/paper/2023/ and Karthik Narasimhan. language models. 2023. hash/271db9922b8d1f4dd7aaef84ed5ac703-Abstract-Conference.html. Jeffrey Zhao, Tree of In Advances in Neural Han Zhao, Haotian Wang, Yiping Peng, Sitong Zhao, Xiaoyu Tian, Shuaiting Chen, Yunjie Ji, and Xiangang Li. to empower large language model training, 2025. URL https://arxiv.org/abs/2503. 19633. Hugging Face dataset: https://huggingface.co/datasets/a-m-team/ AM-DeepSeek-R1-Distilled-1.4M, CC-BY-NC-4.0. 1.4 million open-source distilled reasoning dataset 12 Published as conference paper at ICLR 2026 Xin Zou, Yizhou Wang, Yibo Yan, Yuanhuiyi Lyu, Kening Zheng, Sirui Huang, Junkai Chen, Peijie Jiang, Jia Liu, Chang Tang, et al. Look twice before you answer: Memory-space visual retracing for hallucination mitigation in multimodal large language models. arXiv preprint arXiv:2410.03577, 2024. 13 Published as conference paper at ICLR"
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 THE USE OF LARGE LANGUAGE MODELS In accordance with ICLR 2026 policy, we disclose the usage of Large Language Models (LLMs) in this research. Our use of LLMs is primarily in three areas: 1. Writing Assistance: We utilized LLMs to improve the clarity, grammar, and overall readability of the manuscript. This involved proofreading and refining sentences without altering the core scientific contributions. 2. Data Annotation: As detailed in A.4, GPT-4.1 was employed to annotate our Chain-ofThought (CoT) dataset. This process was crucial for training the MLP probe to accurately identify Echo of Prompt (EOP) instances. 3. Synthetic Data Generation: The CoT dataset for Supervised Fine-Tuning (SFT), discussed in A.8, was generated using the gpt-oss-120B model. This provided the foundation for training our models to exhibit the desired echo behavior. A.2 ADDITIONAL CROSS-MODEL ATTENTION STATISTICS To supplement the analysis in 4, Table 6 provides the detailed mid-layer (layers 718) attention gap statistics for the answeranswer-prefix metric across model variants. The gap is computed as the difference in average attention percentage points (pp) between correct and incorrect reasoning traces. larger positive value indicates stronger within-model discriminability. Table 6: Mid-layer (layers 718) AnsPref gap (pp, CorrectWrong). Values are for within-model discriminability; do not use for cross-model ranking. Model / Setting Mid-layer gap (pp) Qwen3-8B-Base Echo-SFT Normal-SFT 1.90 3.20 2.40 A.3 ON THE ORIGINS OF THE ECHO OF PROMPT While the precise mechanisms underlying the formation of the Echo of Prompt (EOP) are not yet fully understood, we note its appearance in related emergent LLM-reasoning phenomena, such as the initial COT promping (Wei et al., 2022) and the \"aha moment\" observed in DeepSeek-R1-zero (DeepSeek-AI et al., 2025). We hypothesize that EOP is an emergent behavior that arises from the models implicit need to ground its reasoning process in the problem statement. By restating the prompt, the model may be reinforcing its internal representation of the task, thereby improving its focus on relevant information for subsequent reasoning steps. A.4 MLP PROBE FOR ECHO DETECTION To operationalize our probabilistic framework, we require reliable method to detect echo prefixes. We train lightweight two-layer MLP probe for this binary classification task. Data and Annotation. Training data are sampled from the am_0.9M_1k.jsonl subset of AM-DeepSeek-R1-Distilled-1.4M (Zhao et al., 2025). Each example consists of (question, think_content) pair. Labels are generated using hybrid approach: we prompt GPT-4.1 (OpenAI, 2024) with deterministic rubric to identify semantic repetition and its approximate boundary. This boundary is then refined using sentence-level semantic similarity to correct for formatting artifacts. To validate annotation quality, random subset of 200 annotations was manually reviewed, showing over 96% agreement with the final labels. We release the prompt template and parsing code in our repository. 14 Published as conference paper at ICLR Input Features. For each (question, think_content) pair, we featurize the sample by concatenating two sentence embeddings. The first embedding represents the full question. The second represents the initial prefix of the think_content, defined as the first 32 word tokens. Both are encoded using SentenceTransformer model (Qwen3-Embedding-0.6B). The resulting concatenated vector is z-scored before being passed to the probe. Architecture and Training. The probe is two-layer MLP with 32-dimensional hidden layer and ReLU activation, mapping the concatenated embedding to single logit. We train the model using weighted binary cross-entropy on logits (sigmoid + BCE computed in numerically stable form; implemented via PyTorchs BCEWithLogitsLoss), where the positive class weight is set to the ratio of negative to positive samples in the training set to handle class imbalance. Optimization is performed with AdamW (learning rate 104, weight decay 0.01, batch size 64) for up to 200 epochs, with early stopping (patience 10) on the validation loss. The dataset is split into training (70%), validation (15%), and testing (15%) sets. Evaluation and Usage. The trained probes performance on the held-out test set is reported in Table 7. The high AUROC and F1-Score confirm its reliability for identifying echoes. During inference for our main experiments (e.g., attention analysis), this probe is used as predicate to identify and measure echo prefixes. It is not used to score task correctness. For truncation, we use calibrated threshold on the sigmoid output with hysteresis scheme (initial threshold 0.6, drop threshold 0.15) to ensure stable prefix detection. Table 7: MLP probe performance on the held-out test set. The probe reliably identifies echo prefixes, justifying its use in our framework. Metric Accuracy Precision Recall F1-Score AUROC Value 0.912 0.921 0.908 0. 0.963 Reproducibility and licensing. We fix and log random seeds, dataset hashes, feature extraction versions, and probe checkpoints. Data originate from AM-DeepSeek-R1-Distilled-1.4M (Zhao et al., 2025) (subset am_0.9M_1k.jsonl).The GPT-4.1 annotator is referenced in (OpenAI, 2024). Scripts to reproduce this pipeline are provided in our code release. A.5 ABLATION STUDY ON FIXED PREFIX LENGTHS To verify that the observed attention refocusing is not merely byproduct of the echos length, we performed an ablation study. Instead of using the dynamically estimated echo length from our MLP probe, we re-computed the answeranswer-prefix attention metric using several fixed prefix lengths (K). This allows us to disentangle the effect of prefix length from the functional role of the echos content. Methodology. We repeated the layer-wise attention analysis from 3.3 with fixed prefix lengths of {32, 64, 128} tokens for all samples. For each value of K, we calculated the average attention from all answer tokens to the first answer tokens, separately for the Correct and Wrong groups. Results. As shown in Table 8, the attention gap between the Correct and Wrong groups remains consistently positive and significant across all fixed prefix lengths. While the magnitude of the gap varies with K, the Correct group consistently directs more attention to the answer prefix. This demonstrates that the attention refocusing effect is robust phenomenon and not just an artifact of longer echoes co-occurring with correct answers. A.6 VERIFICATION OF ATTENTION NORMALIZATION To ensure that the observed differences in attention weights between correct and incorrect traces are not artifacts of absolute weight fluctuations across layers, we performed normalization analysis. We computed the standardized mean difference (Cohens d) and Z-score differences for each layer. 15 Published as conference paper at ICLR 2026 Table 8: Ablation study on fixed prefix lengths for answeranswer-prefix attention. Attention weights are averaged across all layers. The positive difference (Correct Wrong) persists for all values of K. Prefix Length (K) Correct (%) Wrong (%) Difference (%) 32 tokens 64 tokens 128 tokens 10.61 16.78 20.51 8.42 13.94 19.43 2.19 2.83 1.08 Figure 5 presents dual-axis comparison of the raw attention difference (Correct Wrong) and the normalized Cohens effect size for the answeranswer-prefix metric. The two curves track each other closely, with the normalized effect size consistently exceeding 0.75 in the critical middle layers (718) and peaking at 0.86. This confirms that the attention refocusing signal is robust to layerspecific magnitude variations and represents statistically significant difference in model behavior. Figure 5: Comparison of raw attention difference (Correct Wrong) and normalized effect size (Cohens d) across layers for the answeranswer-prefix metric. The strong alignment between the raw and normalized metrics confirms that the mid-layer refocusing peak is robust phenomenon. A.7 LINKING TO CORRECTNESS: LIKELIHOOD DECOMPOSITION Let πθ(y x) be the base model and τθ(y x) = πθ(y x) 1yYtrim /Zx the trimmed distribution with Zx > 0. For raw trace yraw = [e, s] and its trimmed counterpart ytrim = s, define the per-token log-likelihood Lπ(y x) = 1 log πθ(yt x, y<t), and the Echo Likelihood Gap L = Lπ(yraw x) Lπ(ytrim x). (cid:80) Because log τθ(ytrim x) = log πθ(ytrim x) log Zx, we have, for = ytrim, Lτ (ytrim x) = Lπ(ytrim x) c(x, n) = Lπ(yraw x) c(x, n), where the constant shift is c(x, n) = 1 the correctness label {C, W} yields log Zx. Taking conditional expectations with respect to E[Lτ G] = E[Lπ(yraw x) G] E[L G] E[c(x, n) G] . 16 Published as conference paper at ICLR 2026 Therefore, E[Lτ C] E[Lτ W] = (cid:16) (cid:124) E[Lπ(yraw x) C] E[Lπ(yraw x) W] (cid:123)(cid:122) controlled by length/suffix stratification (cid:17) (cid:125) (cid:16) (cid:16) (cid:124) E[L C] E[L W] (cid:17) (cid:17) E[c(x, n) C] E[c(x, n) W] (cid:123)(cid:122) 0 if and are matched (cid:125) . Under matched prompts and matched (or stratified) lengths n, the first and last terms are negligible, so the group difference in Lτ is approximately the negative of the group difference in L. A.8 COT DISTILLATION PIPELINE Our Chain-of-Thought (CoT) distillation pipeline, used to create the datasets for Echo-Distilled SFT (ED-SFT), follows teacher-student approach grounded in single shared pool of teacher traces. We first use highly capable teacher model (gpt-oss-120B) to generate reasoning traces for the training questions (e.g., from the GSM8K training set) with standard CoT prompt that wraps the reasoning in <think> block and enforces an exact-match final answer. Any trace whose final answer does not match the gold label is discarded, yielding pool of verified (question, CoT, answer) triples. From this pool we derive two closely matched SFT datasets. For the ED-SFT dataset, we encourage an explicit echo-then-reason pattern: we train an MLP probe to detect whether an early Echo of Prompt segment is present, and for traces predicted to be echo-free we ask gpt-oss-120B to minimally insert short echo-prefix that restates the question while preserving the existing reasoning and answer. Traces that already contain an echo are kept as-is. For the normal-SFT baseline, we again start from the same verified traces but, when the probe predicts EOP presence, we prompt the teacher to delete the echo-prefix under do not change the reasoning or final answer instruction. In both directions we re-run answer checking and drop any edited example whose final answer changes. This procedure yields paired ED-SFT and normal-SFT corpora that are nearly identical token-wise and differ primarily in the presence or absence of the initial echo. As reported in the main text, on GSM8K the inclusion of the echo prefix results in longer average sequences for ED-SFT compared to normal-SFT (175 vs. 136 tokens). A.9 TOKEN-WISE ATTENTION SIGNIFICANCE To verify that the attention refocusing effect is not driven by positional bias or few outlier tokens, we performed token-wise analysis of the attention weights. Figure 6 shows the average attention weights for the first 32 answer tokens towards the answer-prefix (left) and the question (right), comparing Correct and Wrong groups. We conducted Welchs t-test at each token position. For answeranswer-prefix, the Correct group shows consistently higher attention, with significant differences (p < 0.05) at 10 out of 32 positions. Conversely, for answerquestion, the Wrong group attends significantly more to the question at 22 out of 32 positions. This confirms that successful reasoning involves systematic shift of attention from the original question to the models own echoed representation. A.10 WORD-LEVEL ATTENTION CASE STUDY To visualize which specific parts of the echo are attended to, we aggregated token-level attention into word-level scores. Figure 7 shows heatmap of attention from the reasoning trace to the echo prefix for representative correct solution to GSM8K problem (\"Janets ducks\"). The model focuses most intensely on the key numerical entities and constraints (e.g., \"16\", \"eggs\", \"3\", \"13\") within the echo, rather than on function words. This supports the hypothesis that the echo serves as semantic anchor for critical problem details. 17 Published as conference paper at ICLR 2026 Figure 6: Token-wise average attention weights for the first 32 answer tokens. Left: Attention to Answer-Prefix. Right: Attention to Question. Shaded regions indicate standard error. The Correct group (blue) consistently attends more to the prefix, while the Wrong group (orange) attends more to the question. Figure 7: Word-level attention heatmap from the reasoning trace to the echo prefix (mid-layers 718). Darker red indicates higher attention. The model selectively attends to key quantities (numbers of eggs, dollars) in the echoed prompt. A.11 INFORMATION FLOW ANALYSIS We further investigated how information propagates through the model layers using an information flow route analysis. Figure 8 visualizes the attention-based routing of information for single answer token (orange triangle at the top right). In the visualization, blue nodes represent question tokens, and green nodes represent echo/prefix tokens. In Correct traces, backward attribution paths from answer tokens repeatedly route through the echo-prefix tokens before reaching the rest of the prompt, whereas in Wrong traces these paths more often terminate in the question region or fail to reach the key numerals (e.g., idx 1151/1001 vs. 594). This suggests that the echo acts as recurrent internal hub that integrates and refines information before it is used in the final generation. A.12 LOGISTIC REGRESSION ANALYSIS OF To quantify the predictive power of the Echo Likelihood Gap (L) on reasoning correctness, we fitted logistic regression model on the 1,319 GSM8K samples used in our analysis. We predicted the binary correctness outcome {0, 1} using and the length of the trimmed echo (Lecho) as predictors: logit(P (Y = 1)) = β0 + β1L + β2Lecho (8) The regression results  (Table 9)  indicate that is statistically significant positive predictor of correctness (p 0.022), with coefficient β1 0.24. This implies that for every 1.0 nat/token increase in the likelihood gap, the odds of correct answer increase by factor of exp(0.24) 1.27, confirming that the models probabilistic preference for the echo is meaningfully associated with task success. A.13 ANALYSIS OF EOP-PRESENT VS. EOP-ABSENT TRACES To disentangle the effect of the Echo of Prompt (EOP) from general model capabilities, we compared traces where the model spontaneously produced an echo (EOP-present) versus those where it did not (EOP-absent). As shown in Table 10, the EOP-present group has higher overall accuracy (63.8% vs 57.2%). Furthermore, even when controlling for the final outcome (Correct or Wrong), traces 18 Published as conference paper at ICLR 2026 Figure 8: Information flow visualization for generated answer tokens across three different examples. The graphs show the primary attention routes through the model layers. In correct traces, information flows significantly through the Echo of Prompt (EOP) tokens in the middle layers, acting as bridge between the question and the answer. Table 9: Logistic regression results predicting correctness on GSM8K. Predictor Coefficient (β) Std. Error P-value Intercept (β0) Echo Likelihood Gap (β1) Echo Length (β2) 0.12 0.24 0. 0.15 0.11 0.0005 0.423 0.022 0.089 with an EOP exhibit stronger attention refocusing (Answer Answer-Prefix attention) than those without. This suggests that the presence of an echo actively facilitates the attention mechanism that supports correct reasoning. A.14 PROMPT TEMPLATES FOR TEACHER COT AND EDITING For reproducibility, we list the main prompt templates used to generate and edit teacher traces. Standard CoT generation prompt. To obtain the initial pool of verified CoTs, we query gpt-oss-120B with simple reasoning prompt that separates internal thinking from the final answer: 19 Published as conference paper at ICLR 2026 Table 10: Comparison of EOP-present and EOP-absent traces on GSM8K (DeepSeek-R1-DistillLlama-8B). EOP presence is associated with higher accuracy and stronger attention refocusing. Group EOP-present EOP-absent Conditioned on Outcome: Correct, EOP-present Correct, EOP-absent Wrong, EOP-present Wrong, EOP-absent Accuracy (%) Last-layer AnsAns-prefix Attn 985 334 628 191 357 143 63.8 57.2 0.1374 0.1328 0.1046 0.0987 You are solving math problems. thought process within single pair of <think> and </think> tags. concise answer on new line. The final answer should be plain value. After youve finished thinking, provide the final, Structure your entire Echo-insertion prompt for ED-SFT. When the MLP probe predicts that trace lacks an early Echo of Prompt, we ask the teacher to minimally insert an echo-style opening that repeatedly brings the question back into focus. The high-level instruction is: Think out loud naturally. You are solving math problems. To ensure you fully understand the problem, you must repeat the question or key parts of it multiple times throughout your reasoning process before you start solving. instance, you might re-read it to confirm details or after few steps of calculation to ensure you are on track. by repeating the problem, then reason step by step. the entire internal thinking process with single pair of <think> and </think> tags, and put the final answer after the thinking. The final answer should be concise plain value (number if applicable). At the very beginning of your <think>, start with the following opening line and then continue the original reasoning. answer. Do not change the final Start Wrap For The opening line is randomly chosen from small set of naturalistic variants, e.g.: The problem is asking: [QUESTION] Okay, let me see. Alright, so the question is: [QUESTION] Let me understand this problem. We have: [QUESTION] So the problem states: [QUESTION] where [QUESTION] is replaced with the original GSM8K question text. Echo-removal prompt for normal-SFT. For traces where the MLP probe predicts the presence of an early EOP, we construct the normal-SFT counterpart by asking the teacher to remove the echo-prefix while preserving all later reasoning steps and the final answer: You are given math question and chain-of-thought solution that begins by repeating or paraphrasing the question. Rewrite the reasoning so that it no longer repeats the question at the beginning. reasoning steps and the final answer exactly the same. not change the logic or the final answer; only remove the initial echo segment. and </think> as before. Wrap the internal thinking in <think> Keep all subsequent Do 20 Published as conference paper at ICLR Benchmark Size / Format Domain / Source Description (from original papers) Hendrycks-MATH 12,500 problems; open-ended; step-by-step solutions Competition mathematics (algebra, geometry, number theory, probability, etc.) GSM8K MathQA 8.5K problems; anfree-form swers; word problems Grade-school math word problems 37K problems; multiple choice Multiple math domains; derived from AQuA operation programs new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has full step-by-step solution which can be used to teach models to generate answer derivations and explanations. The evaluation follows the hendrycks_math group in the LM Evaluation Harness, which decomposes MATH into diverse subsets (_algebra, _counting_and_prob, _geometry, _intermediate_algebra, _num_theory, _prealgebra, _precalc), covering wide range of mathematical skills beyond our training distribution. dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. large-scale dataset of 37k English multiplechoice math word problems covering multiple math domain categories by modeling operation programs corresponding to word problems in the AQuA dataset. Table 11: Benchmarks used for our evaluation. A. ILLUSTRATIVE EXAMPLES OF ECHO-THEN-REASON PATTERN We provide two illustrative examples as case studies to demonstrate the Echo-then-Reason pattern, see Figure 9 In these examples, we compare outputs from models with and without the initial echo phase. The echo-prefixed response (green) correctly identifies that the yellow marbles are identical and thus treats pairs of yellow marbles as single unique group, leading to the correct count of 7. In contrast, the direct response (red) incorrectly treats the yellow marbles as distinguishable, overcounting the combinations and arriving at 12. A.16 SFT EVALUATION BENCHMARKS In this section, we clarify our terminology regarding generalization. While we initially referred to performance on MathQA and Hendrycks-MATH as out-of-domain generalization, we acknowledge that all tested datasets fall within the broader domain of mathematics. However, they represent significant distributional shifts in terms of difficulty, topic coverage, and problem format compared to the GSM8K training set. Therefore, we adopt the terms distributional generalization or robustness to distribution shift to more precisely describe these experiments. Table 11 summarizes the characteristics of each benchmark, highlighting these differences. Published as conference paper at ICLR"
        },
        {
            "title": "Question",
            "content": "Tom has red marble, green marble, blue marble, and three identical yellow marbles. How many different groups of two marbles can Tom choose? Model output with Echo-then-Reason (Correct) <think> The problem asks: \"Tom has red marble, green marble, blue marble, and three identical yellow marbles. How many different groups of two marbles can Tom choose?\" First, repeat the key details: Tom has one red marble, one green marble, one blue marble, and three yellow marbles that are identical. He wants to form groups of two marbles. To avoid counting identical groups multiple times, consider the distinct colors: red, green, blue, and yellow. There are four distinct colors. Now, lets count the pairs systematically to avoid overcounting: - Red can be paired with green, blue, or yellow, giving 3 pairs. - Green can be paired with blue or yellow (red-green is already counted), giving 2 new pairs. - Blue can be paired with yellow (red-blue and greenblue are already counted), giving 1 new pair. - Two yellow marbles can be paired. Since they are identical, this is 1 more pair. Adding these up: 3 + 2 + 1 + 1 = 7 pairs. Thus, the number of different groups of two marbles Tom can choose is 7. </think> The answer is 7. Model output without Echo (Incorrect) (cid:1) to find the number of ways to choose 2 marbles out To determine the number of different groups of two marbles that Tom can choose, we need to consider all possible combinations of the marbles he has. Tom has red marble (R), green marble (G), blue marble (B), and three identical yellow marbles (Y). The total number of marbles is 6, but since the yellow marbles are identical, we need to account for this in our combinations. We can use the combination formula (cid:0)n of 6, but we need to adjust for the identical yellow marbles. Lets break it down: 1. Choose 2 marbles from the 3 yellow marbles: This is not possible since we cant choose 2 identical marbles from set of identical items. 2. Choose 1 yellow (cid:1) = 3 ways, and we marble and 1 non-yellow marble: We can choose 1 yellow marble in (cid:0)3 can choose 1 non-yellow marble in (cid:0)3 (cid:1) = 3 ways. So, the total number of ways to choose 1 yellow and 1 non-yellow marble is 3 3 = 9. 3. Choose 2 non-yellow marbles: We can choose 2 non-yellow marbles from the 3 non-yellow marbles (R, G, B) in (cid:0)3 (cid:1) = 3 ways. Adding these up, the total number of different groups of two marbles that Tom can choose is 0 + 9 + 3 = 12. The number of different groups of two marbles that Tom can choose is 12 . 1 1 2 Figure 9: Qwen3-8B-Base model outputs illustrating the Echo-then-Reason pattern. Top: the math question from Hendrycks-MATH dataset. Middle (green): Qwen3-8B-ED-SFTs correct answer. Bottom (red): Qwen3-8B-Base that immediately jumps to calculation without echoing, resulting in the wrong answer."
        }
    ],
    "affiliations": [
        "Harbin Institute of Technology, Shenzhen, China",
        "Huazhong University of Science and Technology, China",
        "Pengcheng Laboratory, Shenzhen, China"
    ]
}