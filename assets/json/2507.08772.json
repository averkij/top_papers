{
    "paper_title": "From One to More: Contextual Part Latents for 3D Generation",
    "authors": [
        "Shaocong Dong",
        "Lihe Ding",
        "Xiao Chen",
        "Yaokun Li",
        "Yuxin Wang",
        "Yucheng Wang",
        "Qi Wang",
        "Jaehyeok Kim",
        "Chenjian Gao",
        "Zhanpeng Huang",
        "Zibin Wang",
        "Tianfan Xue",
        "Dan Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in 3D generation have transitioned from multi-view 2D rendering approaches to 3D-native latent diffusion frameworks that exploit geometric priors in ground truth data. Despite progress, three key limitations persist: (1) Single-latent representations fail to capture complex multi-part geometries, causing detail degradation; (2) Holistic latent coding neglects part independence and interrelationships critical for compositional design; (3) Global conditioning mechanisms lack fine-grained controllability. Inspired by human 3D design workflows, we propose CoPart - a part-aware diffusion framework that decomposes 3D objects into contextual part latents for coherent multi-part generation. This paradigm offers three advantages: i) Reduces encoding complexity through part decomposition; ii) Enables explicit part relationship modeling; iii) Supports part-level conditioning. We further develop a mutual guidance strategy to fine-tune pre-trained diffusion models for joint part latent denoising, ensuring both geometric coherence and foundation model priors. To enable large-scale training, we construct Partverse - a novel 3D part dataset derived from Objaverse through automated mesh segmentation and human-verified annotations. Extensive experiments demonstrate CoPart's superior capabilities in part-level editing, articulated object generation, and scene composition with unprecedented controllability."
        },
        {
            "title": "Start",
            "content": "From One to More: Contextual Part Latents for 3D Generation Shaocong Dong1*, Lihe Ding2*, Xiao Chen2, Yaokun Li2, Yuxin Wang1, Yucheng Wang1, Qi Wang1, Jaehyeok Kim1, Chenjian Gao2, Zhanpeng Huang3, Zibin Wang3, Tianfan Xue2,4, Dan Xu1 3SenseTime Research 2CUHK 4Shanghai AI Laboratory 1HKUST 5 2 0 J 1 1 ] . [ 1 2 7 7 8 0 . 7 0 5 2 : r {sdongae, danxu}@cse.ust.hk, {dl023, tfxue}@ie.cuhk.edu.hk {wangzb02, yiyuanzhang.ai}@gmail.com, {huangzhanpeng}@sensetime.com Figure 1. CoPart achieves high-quality part-based 3D generation and supports various applications."
        },
        {
            "title": "Abstract",
            "content": "To generate 3D objects, early research focused on multiview-driven approaches relying solely on 2D renderings. Recently, the 3D native latent diffusion paradigm has demonstrated superior performance in 3D generation, because it fully leverages the geometric information provided in ground truth 3D data. Despite its fast development, 3D diffusion still faces three challenges. First, the majority of these methods represent 3D object by one single latent, regardless of its complexity. This may lead to detail loss when generating 3D objects with multiple complicated parts. Second, most 3D assets are designed parts by parts, yet the current holistic latent representation overlooks the independence of these parts and their interrelationships, limiting the models generative ability. Third, current methods rely on global conditions (e.g., text, image, point cloud) to control the generation process, lacking detailed controllability. Therefore, motivated by how 3D designers create 3D object, we present new partbased 3D generation framework, CoPart, which represents 3D object with multiple contextual part latents and simultaneously generates coherent 3D parts. This part-based framework has several advantages, including: i) reduces the encoding burden of intricate objects by decomposing them into simpler parts, ii) facilitates part learning and part relationship modeling, and iii) naturally supports partlevel control. Furthermore, to ensure the coherence of part latents and to harness the powerful priors from foundation models, we propose novel mutual guidance strategy 1 to fine-tune pre-trained diffusion models for joint part latent denoising. Benefiting from the part-based representation, we demonstrate that CoPart can support various applications including part-editing, articulated object generation, and mini-scene generation. Moreover, we collect new large-scale 3D part dataset named Partverse from Objaverse through automatic mesh segmentation and subsequent human post-annotations. By training on the proposed dataset, CoPart achieves promising part-based 3D generation with high controllability. Project page: https: //hkdsc.github.io/project/copart. 1. Introduction With the emergence of large-scale 3D datasets [7], many techniques have been proposed to convert the raw 3D data into different representations for generative modeling. Pioneering multi-view driven works [31, 43] render 3D mesh into multi-view images and train multi-view diffusion models or large reconstruction models with only 2D image supervision. These methods can generate consistent multiview images of 3D objects but are poor at recovering highquality geometry since the accurate 3D shape supervision is omitted when converting mesh into multi-view images. Another 3D latent diffusion method CLAY [58] converts 3D meshes into latent tokens by 3D VAE [20, 56] and trains latent diffusion model. This method implicitly preserves previously overlooked geometric supervision through 3D occupancy auto-encoding, resulting in improved generation quality. Despite all these advances of 3D latent diffusion, it still faces three challenges, making it still sub-optimal for 3D generation. First, the current methods treat intricate 3D meshes and simple ones equally, using the same number of tokens. However, the constrained representative ability of 3D VAE will inevitably cause information loss for complex data; and the unbalanced data distribution will make simpler geometries dominate the training process. Second, most 3D designers create complex 3D objects part by part, so they can spend more time adding detailed geometries for each part. On the contrary, state-of-the-art 3D generation algorithms neither utilize the part representations nor explicitly model the relationships between parts, limiting their ability to generate detailed and independent parts. For instance, when generating person with hat, most algorithms will fuse the head and hat together as single object within limited resolution of each local region, leading to low quality. However, users need two distinguishable detailed parts, especially for the face. Third, current methods utilize global conditions (e.g., text, image, point clouds) to control the generation process, which lacks detailed local controllability. Based on these observations, we demonstrate that all these issues can be addressed through part-based 3D object representation and generation, as part-based modeling can i) naturally distribute complexity across individual parts, ii) efficiently learn part-level information from the data, and iii) provide detailed control at the part level. Therefore, we propose novel part-based 3D generation framework, CoPart, which represents 3D object with multiple Contextual Part latents and generates part latents by learning joint distribution across diverse 3D data. While 3D part generation has been explored previously, our solution offers very distinct perspective. Majority of 3D part generation models either i) are restricted by PartNet [33] categories with limited generalizability [11, 22, 34], or ii) adopt top-down strategy [28], segmenting input images into part patches for individual reconstruction. The latter limits the models ability to leverage part information during training and depends heavily on segIn contrast, CoPart adopts bottommentation quality. up framework that directly generates coherent parts and leverage the diverse Objaverse [7] dataset, ensuring greater generalizability. Specifically, CoPart encodes each 3D part using geometric token and an image token extracted from the part image, instead of one single global latent. This approach is beneficial for two reasons. First, each part has the complementary geometry and image tokens. Geometry tokens model the detailed shape, while the image tokens not only provide appearance information but also offer semantic cues for understanding part relationships. Second, decoupling geometry from image latents allows us to leverage the capabilities of pre-trained 3D and 2D autoencoders more effectively. For geometric tokens, since the part geometry is normally simpler than object geometry, they can be more efficiently encoded by 3D part VAE [20]. For image tokens, since each part can be rendered in much higher resolution, 2D diffusion model can generate more detailed textures. To learn the distribution of both geometry and image tokens of each part, we finetune the diffusion models for 3D geometries [24] and 2D images [4], leveraging their pretrained priors for better generation quality. To ensure both consistency between different parts and between geometric tokens and image tokens, we introduce mutual guidance diffusion model, inspired by [8]. The mutual guidance facilitates information exchange between different parts as well as between each parts geometric and image tokens, achieving both part consistency and geometry-image consistency. Furthermore, to eliminate the ambiguity of part order and effectively control the part generation using the input 3D bounding boxes, we propose novel strategy to encode bounding box conditions to guide part generation. In this way, with the input of bounding boxes and text descriptions, we can generate high-quality 3D objects by decoding and assembling part latents, as shown in the first row 2 of Fig. 1. Collecting high-quality 3D part data for training is also non-trivial. One option is the PartNet dataset [33], but it only contains 24 categories of objects and has poor textures, restricting model generalizability. Another option is Objaverse [7], which offers more diversity but suffers from inconsistent part labels as different 3D designers prefer different ways to partition an object, often leading to overor under-segmentation. To address this, we first employ mesh segmentation pipeline to automatically decompose objects into reasonable parts. Then we manually conduct simple post-processing, including filtering low-quality data and grouping the over-segmented parts. Additionally, we utilize multi-modal vision-language model [25] to generate text prompts for each part. In this way, we obtain high-quality 3D part dataset with 91k parts for 12k objects. With the proposed CoPart model trained on our 3D part dataset, it unlocks many various new applications, as shown in the bottom of Fig. 1. First, we can run structure diffusion [29] to obtain bounding boxes and articulation information while using CoPart to generate parts, achieving novel articulated objects generation. Second, we can generate mini-scene by considering each object in scene as part. Thirdly, we can achieve part-based 3D editing by resampling selected part latents. Experimental results also show that our method can generate high-quality 3D objects with more accurate parts compared with the previous works, and also support various applications as discussed above. 2. Related Work 2.1. 3D Generation In contrast to earlier category-specific 3D generation methods [2, 6, 10, 14, 18, 35, 44, 48, 53], contemporary 3D generative models are capable of producing diverse 3D objects conditioned on text or images. DreamFusion [37] and its subsequent works [27, 38, 47] introduce the Score Distillation Sampling (SDS) loss to adapt 2D diffusion models for 3D generation. Multi-view diffusion approaches [30, 31, 43] fine-tune 2D diffusion models to generate multiview consistent images. Meanwhile, LRM [16] trains large-scale reconstruction model to predict 3D radiance fields from single image. Based on these 3D foundations, some work can achieve precise part and detail level modifications [9]. More recently, CLAY [58] and its follow-ups [24, 55] directly train 3D-native diffusion models, achieving significantly improved performance. 2.2. Part Generation StructureNet [32] uses graph network to understand the relationship between parts while Grass [23] adopting recursive autoencoders for shape structures. DSG-Net [54] proposes disentangled structure and geometry for 3d generation. Other methods [12, 13] employ 3D Gaussian mixture to represent parts. SPAGHETTI [15] and Neural Template [17] train an auto-encoder to map 3D objects into part-aware latent space, enabling part-aware editing. SALAD [22] replaces the auto-encoder in SPAGHETTI with diffusion model, achieving superior performance. DiffFacto [34] learns controllable part-based point cloud diffusion model. PartNeRF [46] and NeuForm [26] also introduce part-based neural representations. While effective, these methods are limited by their reliance on categoryspecific part data, which restricts their generalizability. Part123 [28] leverages the powerful SAM [21] model to segment multi-view images and perform part-aware reconstruction. Concurrent to our work, PartGen [5], also adopts top-down strategy by first segmenting parts from multiview images and then performing part completion and reconstruction. However, this approach not only heavily depends on segmentation quality but also struggles with the limited information provided by small segmented patches, which constrains the quality of part reconstruction. In contrast, we propose bottom-up strategy that directly learns the part distribution from diverse data and jointly generates coherent parts. 3. Synchronized Part Latent Diffusion An effective part-based 3D generative model should be able to generate consistent parts with both high-quality geometry and appearance. However, this is non-trivial for the following three reasons. First, consistency between different parts is hard to ensure. Second, it is not easy to efficiently leverage limited part data to achieve high-quality part-based 3D generation. Third, simultaneously generating parts introduces ambiguity in part ordering. In this paper, we provide synchronized part latent diffusion framework to address the above challenges as shown in Fig. 2. In Sec. 3.1, we first introduce our method to represent 3D objects using part latents. Next, in Sec. 3.2.1, we propose an effective framework to synchronize part latent diffusion through mutual guidance, and fine-tune the part latent diffusion model from pre-trained foundation models for efficient part data utilization. Finally, in Sec. 3.2.3, we discuss our approach to inject conditions to resolve the part order ambiguity and enhance controllability. 3.1. Part Representation Encoding To model the distribution of 3D parts, we decompose 3D mesh into part-based 3D representations that preserve both geometric and appearance information from the ground truth data. Our approach utilizes hybrid part latents to represent 3D texture parts through the combination of geometric tokens (encoded by 3D part VAE) and image tokens (encoded by an image VAE), as detailed below. Part Geometric Token Encoding. Given 3D part geometry Mp segmented from M, we sample points 3 Figure 2. The framework of CoPart operates as follows: Gaussian noise is added to part image and geometric tokens extracted from the VAE, which are then fed into 3D and 2D denoisers. Mutual guidance (a) is introduced to facilitate information exchange between the 3D and 2D modalities (via Cross-Modality Attention) as well as between different parts (via Cross-Part Attention). Additionally, (b) the 3D bounding boxes are treated as cube meshes, and the extracted box tokens are injected into the 3D denoiser through cross-attention. Simultaneously, the boxes are rendered into 2D images and injected into the 2D denoiser via ControlNet. RS3 and their corresponding normals RS3 on the part meshs surface, where = 4096 is the number of sampled points. Then 3D part VAE encoder E3D is used to extract 3D part geometric tokens L3D = E3D(P, Q) RT D, where and represent the token length and dimensions respectively. To enhance the part-level representation learning, we fine-tune the part VAE from pretrained holistic 3D VAE [24] using our part data. Additionally, we modify the VAE decoder D3D from [24] to predict Flexicube [42] parameters, enabling differentiable rendering. More implementation details can be found in the supplementary material. These designs allow us to incorporate normal and depth rendering losses to supervise the VAE fine-tuning. tent diffusion [24] to generate geometric part latents Lp 3D, while fine-tuning pre-trained image diffusion models [4] to generate image part latents Lp 2D. To ensure consistent 3D object generation across all diffusion processes, we apply two types of synchronization. First is the inter-part synchronization, which ensures the part consistency. and Lj b, where {3D, 2D} denotes the modality and = indicates different parts. Second is the intra-part synchronization between geometric and appearance representations Li 2D within the same i-th part, ensuring cross-modality (geometry and image modality) consistency. Details of both synchronization through guidance are described below. It synchronizes between parts Li 3D and Li Part Image Token Encoding. To encode part appearance, we render the part mesh Mp into multi-view partcentric images {Ok}v k=1. Using pre-trained image VAE E2D [4], we obtain part image tokens: L2D = {FkFk = E2D(Ok) RT D}v k=1 , where denotes the number of views, and and represent the token length and dimensions for each view respectively. 3.2. Synchronized Diffusion As introduced in Sec. 3.1, 3D object can be represented by part latents, comprising geometric tokens and image tokens: {Lp p=1. To enable effective part generation, we leverage the powerful priors from pre-trained geometric and image diffusion models, and further fine-tune them with our part data. Specifically, we fine-tune pre-trained 3D la3D, Lp 2D}N 3.2.1. Mutual Guidance The inter-part synchronization design is inspired by BiDiff [8], which synchronizes 3D diffusion model and 2D diffusion model through bidirectional guidance. We further extend this approach by adding mutual guidance between different part latents as well as between 3D and 2D modalities. The original bidirectional guidance, which replies on 2D-to-3D lifting and 3D-to-2D rendering, proves memory-intensive and inefficient for exchanging part-level information. Instead, we adopt more effective implicit mutual guidance strategy that uses attention to exchange information between different parts and modalities  (Fig. 2)  (a), as we use Transformer-based diffusion models [36]. Specifically, given noisy part latents Lp,t 2D = 3D and Lp,t 4 }v {F p,t k=1, = 1, ..., at diffusion timestep t, we define the intermediate features from the 3D and 2D diffusion as: Gp = DiT(Lp,t = DiT(Lp,t 3D, y, t), = 1, ..., 2D, y, t), = 1, ..., N, (1) where Gp RT denotes the intermediate 3D features RT of p-th part, = {F }v k=1 is the intermediate 2D features of p-th part conatining views, and represents additional conditions such as bounding boxes. To ensure the inter-part consistency, we extend selected self-attention blocks in each modality to attend tokens from all parts: Gp = Attention(Gp, {Gi}N = Attention(F k}N k, {F i=1) i=1),"
        },
        {
            "title": "F p\nk",
            "content": "(2) where Attention(query, key/value) is standard attention block. This mechanism enables each part to be guided by other parts, facilitating inter-part mutual guidance and synchronization. Similarly, to ensure cross-modality consistency, we add new attention blocks to exchange information between 3D and 2D features: Gp = Gp + LN(Attention(Gp, {F k + LN(Attention(F k}v k, Gp)), = k=1)) (3) where LN() is linear layer initialized by zeros for training stability. Furthermore, to guarantee multi-view consistency for 2D branch, we also extend some self-attention blocks in 2D diffusion to attend tokens from other views of the same part: = Attention(F k, {F k}v k=1). (4) 3.2.2. Global Guidance To further enhance inter-part consistency, we include global branch for both 3D and 2D latent diffusions to jointly denoise holistic latents. This global branch functions as an additional global part that interacts with other part branches as mentioned in Sec. 3.2.1. In particular, the global branch shares parameters with the part branch, distinguished only by concatenating learnable part-global embeddings to the text embeddings. This architecture enables the network to differentiate between part and global branches, and it further regularizes the fine-tuning process by maintaining global supervision, thereby preventing significant deviation from the original pre-trained weights. 3.2.3. Part Guidance Encoding One challenge of this design is the part order ambiguity. While we pre-define the part order during training, the network can alter this order during inference, creating discrepancy between training and inference phases. To alleviate the part order ambiguity and enhance controllability, we 5 introduce two conditions to both the 3D and 2D diffusion denoising processing. First, we incorporate part-level text prompts to guide the network to distinguish different parts, thereby boosting local semantic controllability. Second, we introduce 3D bounding box conditions for each part as an additional constraint. naive approach is to use MLPs (multi-layer perceptrons) to encode the coordinates of 3D bounding boxes and use the concatenation of coordinate embeddings with timestep embeddings as conditions. However, learning the correlation between embedded coordinates and actual 3D parts location is not easy. To solve this challenge in 3D bounding box encoding, we propose novel strategy to encode 3D bounding boxes by treating each box as mesh with six surfaces, and then extracting box geometric latents through the pre-trained 3D mesh VAE encoder, as illustrated in Fig. 2 (c). The encoding can be written as Lp box = E3D(P box, Qp box), = 1, ...N, (5) box and Qp where box present the sampled points and normals on the p-th bounding box surfaces. In this way, we can encode the 3D bounding box information to the same latent space of the original geometric latents Gp, and simply added to the geometry latent through an additional crossattention block: Gp = Gp + LN(Attention(Gp, Lp box)), (6) where LN() denotes linear layer initialized to zeros. For image tokens, we implement dual-pathway approach to incorporate bounding boxes. First, we implicitly query bounding box information from 3D geometric tokens into image tokens through the cross-modal attention mechanism described in Eq. 2. Second, we encode 3D bounding boxes into the image latent space to guide the denoising of the global image branch. Specifically, we render 3D boxes into 2D to generate multi-view bounding box wireframe images. These wireframe images are then encoded into latent tokens by an image VAE and integrated into the 2D denoiser through lightweight ControlNet [57], as illustrated in Fig. 2 (b). It is noteworthy that these bounding box 2D features are exclusively added to the global branch of the 2D diffusion model. 3.3. Refinement After fine-tuning on the part dataset, the synchronized part latent diffusion is capable of understanding 3D parts and jointly generating consistent part geometric tokens and image tokens. These tokens can be decoded into part meshes and multi-view part-centric images by VAE decoders D3D and D2D: Mp = D3D(Lp,0 3D), Op = D2D(Lp,0 2D). (7) Figure 3. Comparison with state-of-the-art 3D generators. CoPart can generate detailed and independent 3D parts. To further improve quality, we leverage the 3D foundation model [50] as an additional enhancer, utilizing both the part images and geometry generated by our model. While the original pipeline of [50] takes single image and generated voxel as input, we found it incapable of understanding diverse 3D parts. Therefore, we modify this approach by replacing the voxels with our detailed part voxels extracted from the generated part meshes, thereby providing essential part geometry prior as follows: Mp = R(Op, p), = Voxelize(T (Mp)), (8) where represents the stage II enhancer, and denotes transformation to normalize Mp to [1, 1]. We can further enhance each part efficiently and assemble the parts by using the inverse transformation 1: = {T 1(Mp)}N (9) p=1. 3.4. Optimization Loss We supervise both the 3D and image branches of CoPart by the denoising loss in diffusion models: Loss3D = Loss2D = 1 1 (cid:88) p=1 (cid:88) p=1 (E Lp,t 3D,ϵp 3d,tϵp 3d N3d(Lp,t 3D, Lp,t 2D, t)2 2), (E Lp,t 2D,ϵp 2d,tϵp 2d N2d(Lp,t 3D, Lp,t 2D, t) 2), (10) where represents the denoiser for 3D and 2D and ϵ denotes Gaussian noise. 4. Applications One major advantage of our part-based representation of CoPart is that it enables us to directly achieve various applications without further training. These applications include part-based editing (Sec. 4.1), articulated object generation (Sec. 4.2), mini-scene generation and long part sequence sampling (Sec. 4.3). 4.1. Part-based Editing To enable selective part modification while keeping other parts unchanged, we design an inference-time resampling strategy. Specifically, given mesh parts sequence {Mp}N p=1 either from CoPart sampling or segmented from an existing mesh, we denote the parts that need to be edited as {Mp}pC, where is the selected index. To maintain the remaining parts unchanged during sampling while allowing them to provide information for new parts, we first encode the remaining part mesh back into contextual part latents: Lp,0 3D Lp,0 2D = E3D(Sample(Mp)) = E2D(Render(Mp)). (11) Then during each timestep of the new editing sampling process, we directly replace the noisy part latents {Lp,t 2D}p /C by adding noise to {Lp,0 3D, Lp,t (cid:100)Lp,t 1 αtϵ3d, / , Lp,0 2D }p /C: + 3D 3D = + 1 αtϵ2d, / C, (12) αtLp,0 3D αtLp,0 2D (cid:100)Lp,t 2D = 6 Figure 4. Comparison with part-based generator SALAD. Figure 5. Qualitative results of part editing and mini scene generation. where αt is noise schedule and ϵ is random Gaussian noise for 3D or 2D. Thus, we can sample additional part latents from pure Gaussian noise while incorporating information from the fixed one by: based generation approach provides the flexibility to manually define articulation information for each part. Please refer to the supplementary video for the visualization of generated articulated objects. Lp,t1 3D ,Lp,t1 2D = ({ (cid:100)Lp,t 3D, (cid:100)Lp,t 2D}p /C,{Lp,t 3D,Lp,t 2D}pC), (13) where is the diffusion denoiser. In this way, we can modify the text prompts to edit the selected parts as shown in Fig. 5 (a). 4.2. Articulated Object Generation The generation of articulated objects involves two key components: i) articulation generation, which includes the bounding boxes indicating the position of each part, and ii) part generation. To achieve this, we leverage an offthe-shelf method [29] to generate the bounding boxes along with their articulation relationships. Subsequently, we utilize CoPart to populate each bounding box with coherent parts based on text prompts. This approach enables the creation of novel articulated objects, such as an avocado swivel chair  (Fig. 1)  , which cannot be realized using previous holistic generation methods. Additionally, the part7 4.3. Mini-scene and Long Sequence Generation Our approach extends naturally to mini-scene generation, as scenes can be represented as layouts of bounding boxes, aligning with our box-guided part generation where each part corresponds to an object. Our training data includes mini-scenes, enabling direct scene generation through specified boxes and text (Fig. 5 (b)). For complex objects requiring long part sequences, GPU memory constraints during training limit the maximum number of parts =8. We address it by adopting the strategy from Sec. 4.1: fixing some sampled part latents while replacing others with new box conditions sampled from Gaussian noise. This enables the generation of longer sequences without memory issues. More details can be found in the supplementary materials. 5. PartVerse Dataset To enhance the generalizability of CoPart, we introduce PartVerse, new diverse 3D part dataset comprising 91k Figure 6. PartVerse dataset processing pipeline. We follow the pipeline of raw data - mesh segment algorithm - human post correction - generate text caption to produce part-level data. Table 1. Quantitative comparison with SOTA methods. CLIP (N-T) and CLIP (I-T) [39, 58] gauge the geometry alignment of normal maps with the input text and similarity of render images with the input text, respectively. In addition, ULIP-T [51] was also experimented and user-prefer study were conducted. Our method takes 12s when the number of parts is one. Method Shap-E [19] Unique3D [49] CraftMan [24] Rodin [1] Trellis [50] Ours Whole-aware Part-aware CLIP (N-T) CLIP (I-T) ULIP-T CLIP (N-T) CLIP (I-T) ULIP-T 0.1546 0.1865 0.1887 0.2042 0.2071 0.2010 0.1607 0.2037 0.1966 0.2416 0.2360 0. 0.1054 0.1528 0.1476 0.1785 0.1751 0.1743 0.0875 0.1062 0.1026 0.1425 0.1274 0.1607 0.1043 0.1380 0.1271 0.1571 0.1455 0.1768 0.0795 0.1032 0.0997 0.1244 0.1119 0.1355 Time 3s 16s 8s - 10s 65s high-quality parts from 12k objects with detailed text descriptions. Unlike previous part datasets such as PartNet [33] which only contains 24 categories of daily objects, our PartVerse, curated and annotated from Objaverse [7], exhibits enhanced diversity in 175 categories and realistic textures, significantly improving the models ability to generate high-quality 3D parts. As shown in Fig. 6, we provide an overview of the data collection and annotation pipeline used to create Partverse. Automatic Mesh Segmentation. 3D artists follow specific modeling pipeline when creating 3D objects, typically creating them part by part. We can restore this information from the raw 3D data. However, the original modeling steps might not always match semantic part boundaries - for instance, some artists might create textured surfaces as separate elements. pre-labeling algorithm based on SAM-2 [41] and Samesh [45] was constructed using 3D model creation priors combined with semantic segmentation, which balances the mesh faces connectivity and visual semantic during segmentation. This algorithm can allow us to preliminarily obtain semantic parts. We specifically adjusted parameters to favor over-segmentation rather than under-segmentation, since its easier for annotators to merge extra segments than to split insufficient ones. Human post-annotation. After initial segmentation, human reviewers first remove low-quality data including overly complex or unsuitable 3D objects for splitting. Using Blender-based [3] annotation platform, they then refine the segmentation by merging over-segmented or splitting under-segmented parts according to guidelines: ensuring clear part semantics and maximizing symmetry in part distribution. Based on these annotations, complete textured objects are finally split into individual part objects with textures preserved. Part-level text caption. We also created textual descriptions for each part, covering appearance features, shape characteristics, and part-whole relationships. The process starts by rendering multi-view images of both complete objects and individual parts. We select the view showing maximum visible overlap between part and whole, then combine the object image with the highlighted part using bounding box. This composite image is input to vision-language model (VLM) to generate descriptions of the component and its relationship to the complete object. 6. Experimental Evaluation This section presents our experimental results. For better visualization, please refer to our supplementary video. Implementation details. We initialize our 2D and 3D denoisers using Pixart-α [4] and CraftMan [24], and fine-tune them on PartVerse dataset with 4 NVIDIA A100 GPUs. We set the batch size to 1 and limit the maximum part number 8 As 33.3% 11.8% 54.9% 25.5% 13.7% 60.8% Method Whole-aware Part-aware Rodin [1] PartGen [5] Ours Table 2. User study (%preference). to 8. We also perform random selecting or padding during training. Moreover, we adopt progressive training strategy, which is detailed in the supplementary material. Comparison with state-of-the-art 3D generators. We compare CoPart with leading holistic 3D generators in Fig. 3. More results can be found in the supplementary materials. It is evident that CoPart outperforms state-of-the-art methods, particularly in the quality of small parts, owing to its part-based representation. Quantitative comparison. shown in Tab. 1, we conduct quantitative comparisons folFor the lowing [58]. Image-to-3D model, we first generate corresponding images from given texts by Flux.1 [52]. Different from [58], half of the test cases we use are part-aware, such as rifle stock. This is reasonable, since truly general 3D generation model should be able to handle all types. In addition, ULIP [51] was used in evaluation. As shown in Tab. 2, we also conducted user study with 51 diverse participants from different professions by collecting their preferences for generating textured mesh. These results highlights the advantages of our part-based generation approach in producing decomposable and high-quality 3D assets. More details can be found in the supplementary materials. Comparison with part-based generation methods. We compare CoPart with the accessible state-of-the-art part generator SALAD [22]. Fig. 4 shows that CoPart can generate diverse objects with detailed parts while SALAD is constrained to generate shapes in PartNet distribution. More comparisons can be found in the supplementary materials. Ablation study of global guidance. We ablate the effect of global guidance in Fig. 7. The results demonstrate that global guidance significantly enhances part coherence, especially in appearance. Ablation study of refinement. We ablate the effect of refinement (Sec. 3.3), as shown in Fig. 8. The results show that providing both part images (Fig. 8 (a)) and geometries (Fig. 8 (b)) is essential for the enhancer to accurately comprehend part shapes. By integrating both modalities, the enhancer effectively optimizes the parts and enhances overall performance. Figure 7. Ablation of global guidance. 7. Discussion of Part Assembly As depicted in Sec. 3.3, the part latents are decoded in absolute positions and then normalized to perform refinement. 9 Figure 8. Ablation of refinement module. After an inverse-normalization, the refined part meshes can be relocated to the 3D bonding boxes for assembling. Our precise strategy for injecting bounding box information generally ensures effective combinations of part meshes. However, assembly errors, such as mesh clipping, can occur when the user provides incorrect bounding boxes. We will explore techniques to optimize user-provided bounding boxes in future work. 8. Conclusion We present CoPart for high-quality and diverse 3D part generation. Specifically, we utilize mutual guidance to ensure coherent part latent denoising and introduce 3D box conditions to eliminate part ambiguity. Furthermore, larger scale 3D part-aware dataset is firstly collected from Objaverse, which can be widely used for various tasks. Our method outperforms SoTA results. We also discuss the limitations of our method in the supplementary material."
        },
        {
            "title": "References",
            "content": "[1] Rodin. https://hyper3d.ai/. 8, 9 [2] Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and Leonidas Guibas. Learning representations and generative models for 3d point clouds. In ICML, 2018. 3 [3] Blender Foundation. Blender, 2023. 8 [4] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis, 2023. 2, 4, 8 [5] Minghao Chen, Roman Shapovalov, Iro Laina, Tom Monnier, Jianyuan Wang, David Novotny, and Andrea Partgen: Part-level 3d generation and reconVedaldi. struction with multi-view diffusion models. arXiv preprint arXiv:2412.18608, 2024. 3, 9 [6] Zhiqin Chen and Hao Zhang. Learning implicit fields for generative shape modeling. In CVPR, 2019. [7] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3d objects. In CVPR, 2023. 2, 3, 8 [8] Lihe Ding, Shaocong Dong, Zhanpeng Huang, Zibin Wang, Yiyuan Zhang, Kaixiong Gong, Dan Xu, and Tianfan Xue. Text-to-3d generation with bidirectional diffusion using both In Proceedings of the IEEE/CVF Con2d and 3d priors. ference on Computer Vision and Pattern Recognition, pages 51155124, 2024. 2, 4 [9] Shaocong Dong, Lihe Ding, Zhanpeng Huang, Zibin Wang, Tianfan Xue, and Dan Xu. Interactive3d: Create what you want by interactive 3d generation. In CVPR, 2024. 3 [10] Lin Gao, Jie Yang, Tong Wu, Yu-Jie Yuan, Hongbo Fu, YuKun Lai, and Hao Zhang. Sdm-net: Deep generative network for structured deformable mesh. ACM Transactions on Graphics (ToG), 38:115, 2019. 3 [11] Lin Gao, Jie Yang, Tong Wu, Yu-Jie Yuan, Hongbo Fu, YuKun Lai, and Hao Zhang. Sdm-net: Deep generative network for structured deformable mesh. ACM Transactions on Graphics (TOG), 38(6):115, 2019. 2 [12] Kyle Genova, Forrester Cole, Daniel Vlasic, Aaron Sarna, William Freeman, and Thomas Funkhouser. Learning shape templates with structured implicit functions. In Proceedings of the IEEE/CVF international conference on computer vision, pages 71547164, 2019. 3 [13] Kyle Genova, Forrester Cole, Avneesh Sud, Aaron Sarna, and Thomas Funkhouser. Local deep implicit functions for In Proceedings of the IEEE/CVF conference on 3d shape. computer vision and pattern recognition, pages 48574866, 2020. [14] Philipp Henzler, Niloy J. Mitra, and Tobias Ritschel. Escaping platos cave: 3d shape from adversarial rendering. In ICCV, 2019. 3 [15] Amir Hertz, Or Perel, Raja Giryes, Olga Sorkine-Hornung, and Daniel Cohen-Or. Spaghetti: Editing implicit shapes through part aware generation. ACM Transactions on Graphics (TOG), 41(4):120, 2022. 3 [16] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400, 2023. 3 [17] Ka-Hei Hui, Ruihui Li, Jingyu Hu, and Chi-Wing Fu. Neural template: Topology-aware reconstruction and disentangled generation of 3d meshes. In CVPR, 2022. 3 [18] Moritz Ibing, Gregor Kobsik, and Leif Kobbelt. Octree transformer: Autoregressive 3d shape generation arXiv preprint on hierarchically structured sequences. arXiv:2111.12480, 2021. 3 [19] Heewoo Jun and Alex Nichol. Shap-e: GeneratarXiv preprint ing conditional 3d implicit functions. arXiv:2305.02463, 2023. 8 [20] Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 2 [21] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything. arXiv:2304.02643, 2023. 3 [22] Juil Koo, Seungwoo Yoo, Minh Hieu Nguyen, and Minhyuk Sung. Salad: Part-level latent diffusion for 3d shape generation and manipulation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14441 14451, 2023. 2, 3, 9 10 [23] Jun Li, Kai Xu, Siddhartha Chaudhuri, Ersin Yumer, Hao Zhang, and Leonidas Guibas. Grass: Generative recursive autoencoders for shape structures. ACM Transactions on Graphics (TOG), 36(4):114, 2017. 3 [24] Weiyu Li, Jiarui Liu, Rui Chen, Yixun Liang, Xuelin Chen, Ping Tan, and Xiaoxiao Long. Craftsman: High-fidelity mesh generation with 3d native generation and interactive geometry refiner. arXiv preprint arXiv:2405.14979, 2024. 2, 3, 4, [25] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv preprint arXiv:2403.18814, 2024. 3 [26] Connor Lin, Niloy Mitra, Gordon Wetzstein, Leonidas Guibas, and Paul Guerrero. Neuform: Adaptive overfitting for neural shape editing. NeuIPS, 2022. 3 [27] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In CVPR, 2023. 3 [28] Anran Liu, Cheng Lin, Yuan Liu, Xiaoxiao Long, Zhiyang Dou, Hao-Xiang Guo, Ping Luo, and Wenping Wang. Part123: part-aware 3d reconstruction from single-view image. In ACM SIGGRAPH 2024 Conference Papers, pages 112, 2024. 2, 3 [29] Jiayi Liu, Hou In Ivan Tam, Ali Mahdavi-Amiri, and Manolis Savva. Cage: Controllable articulation generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1788017889, 2024. 3, 7 [30] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. In ICCV, 2023. 3 [31] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. Syncdreamer: Generating multiview-consistent images from single-view image. arXiv preprint arXiv:2309.03453, 2023. 2, [32] Kaichun Mo, Paul Guerrero, Li Yi, Hao Su, Peter Wonka, Niloy Mitra, and Leonidas Guibas. Structurenet: Hierarchical graph networks for 3d shape generation. arXiv preprint arXiv:1908.00575, 2019. 3 [33] Kaichun Mo, Shilin Zhu, Angel Chang, Li Yi, Subarna Tripathi, Leonidas Guibas, and Hao Su. Partnet: largescale benchmark for fine-grained and hierarchical part-level 3d object understanding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 909918, 2019. 2, 3, 8 [34] George Kiyohiro Nakayama, Mikaela Angelina Uy, Jiahui Huang, Shi-Min Hu, Ke Li, and Leonidas Guibas. Difffacto: Controllable part-based 3d point cloud generation with cross diffusion. In CVPR, 2023. 2, 3 [35] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, , and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In CVPR, 2019. 3 [36] William Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2022. 4 image. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [50] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. arXiv preprint arXiv:2412.01506, 2024. 6, 8 [51] Le Xue, Mingfei Gao, Chen Xing, Roberto Martın-Martın, Jiajun Wu, Caiming Xiong, Ran Xu, Juan Carlos Niebles, and Silvio Savarese. Ulip: Learning unified representation of language, images, and point clouds for 3d understanding. In CVPR, 2023. 8, 9 [52] Chenglin Yang, Celong Liu, Xueqing Deng, Dongwon Kim, Xing Mei, Xiaohui Shen, and Liang-Chieh Chen. 1.58-bit flux. arXiv preprint arXiv:2412.18653, 2024. 9 [53] Guandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu, Serge Belongie, and Bharath Hariharan. Pointflow: 3d point cloud In ICCV, generation with continuous normalizing flows. 2019. 3 [54] Jie Yang, Kaichun Mo, Yu-Kun Lai, Leonidas Guibas, and Lin Gao. Dsg-net: Learning disentangled structure and geometry for 3d shape generation. ACM Transactions on Graphics (TOG), 42(1):117, 2022. 3 [55] Xianghui Yang, Huiwen Shi, Bowen Zhang, Fan Yang, Jiacheng Wang, Hongxu Zhao, Xinhai Liu, Xinzhou Wang, Qingxiang Lin, Jiaao Yu, Lifu Wang, Zhuo Chen, Sicong Liu, Yuhong Liu, Yong Yang, Di Wang, Jie Jiang, and Chunchao Guo. Tencent hunyuan3d-1.0: unified framework for text-to-3d and image-to-3d generation, 2024. 3 [56] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 3dshape2vecset: 3d shape representation for neural fields and generative diffusion models. ACM Transactions on Graphics (TOG), 42(4):116, 2023. [57] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, 2023. 5 [58] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets. ACM Transactions on Graphics (TOG), 43(4):120, 2024. 2, 3, 8, 9 [37] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In ICLR, 2022. 3 [38] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Aliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Skorokhodov, Peter Wonka, Sergey Tulyakov, et al. Magic123: One image to high-quality 3d object generation using both 2d and 3d diffusion priors. arXiv preprint arXiv:2306.17843, 2023. 3 [39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 8 [40] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. [41] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, ChaoYuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. 8 [42] Tianchang Shen, Jacob Munkberg, Jon Hasselgren, Kangxue Yin, Zian Wang, Wenzheng Chen, Zan Gojcic, Sanja Fidler, Nicholas Sharp, and Jun Gao. Flexible isosurface extraction for gradient-based mesh optimization. ACM Trans. Graph., 42(4), 2023. 4 [43] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. arXiv preprint arXiv:2308.16512, 2023. 2, 3 [44] Edward Smith and David Meger. Deep unsupervised learning using nonequilibrium thermodynamics. In CoRL, 2017. 3 [45] George Tang, William Zhao, Logan Ford, David Benhaim, and Paul Zhang. Segment any mesh: Zero-shot mesh part segmentation via lifting segment anything 2 to 3d. arXiv preprint arXiv:2408.13679, 2024. 8 [46] Konstantinos Tertikas, Despoina Paschalidou, Boxiao Pan, Jeong Joon Park, Mikaela Angelina Uy, Ioannis Emiris, Yannis Avrithis, and Leonidas Guibas. Generating part-aware editable 3d shapes without 3d supervision. In CVPR, 2023. 3 [47] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. NeurIPS, 2024. [48] Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and Josh Tenenbaum. Learning probabilistic latent space of object shapes via 3d generative-adversarial modeling. In NeurIPS, 2016. 3 [49] Kailu Wu, Fangfu Liu, Zhihan Cai, Runjie Yan, Hanyang Wang, Yating Hu, Yueqi Duan, and Kaisheng Ma. Unique3d: High-quality and efficient 3d mesh generation from single"
        }
    ],
    "affiliations": [
        "CUHK",
        "HKUST",
        "SenseTime Research",
        "Shanghai AI Laboratory"
    ]
}