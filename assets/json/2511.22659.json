{
    "paper_title": "Geometrically-Constrained Agent for Spatial Reasoning",
    "authors": [
        "Zeren Chen",
        "Xiaoya Lu",
        "Zhijie Zheng",
        "Pengrui Li",
        "Lehan He",
        "Yijin Zhou",
        "Jing Shao",
        "Bohan Zhuang",
        "Lu Sheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision Language Models (VLMs) exhibit a fundamental semantic-to-geometric gap in spatial reasoning: they excel at qualitative semantic inference but their reasoning operates within a lossy semantic space, misaligned with high-fidelity geometry. Current paradigms fail to bridge this gap. Training-based methods suffer from an ``oracle paradox,'' learning flawed spatial logic from imperfect oracles. Tool-integrated methods constrain the final computation but critically leave the VLM's planning process unconstrained, resulting in geometrically flawed plans. In this work, we propose Geometrically-Constrained Agent (GCA), a training-free agentic paradigm that resolves this gap by introducing a formal task constraint. Specifically, we strategically decouples the VLM's role into two stages. First, acting as a semantic analyst, the VLM translates the user's ambiguous query into the formal, verifiable task constraint, which defines the reference frame and objective. Second, acting as a task solver, the VLM generates and executes tool calls strictly within the deterministic bounds defined by the constraint. This geometrically-constrained reasoning strategy successfully resolve the semantic-to-geometric gap, yielding a robust and verifiable reasoning pathway for spatial reasoning. Comprehensive experiments demonstrate that GCA achieves SOTA performance on multiple spatial reasoning benchmarks, surpassing existing training-based and tool-integrated methods by ~27%. Please see our homepage at https://gca-spatial-reasoning.github.io."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 9 5 6 2 2 . 1 1 5 2 : r Geometrically-Constrained Agent for Spatial Reasoning Zeren Chen1,2, Xiaoya Lu2,3* , Zhijie Zheng1,2, Pengrui Li1, Lehan He1,4, Yijin Zhou2,3,4 Jing Shao2, Bohan Zhuang5, Lu Sheng1 1School of Software, Beihang University 2Shanghai AI Laboratory 3Shanghai Jiao Tong University 4Shanghai Innovation Institute 5ZIP Lab, Zhejiang University {czr1604,lsheng}@buaa.edu.cn, {luxiaoya, shaojing}@pjlab.org.cn Homepage: https://gca-spatial-reasoning.github.io"
        },
        {
            "title": "Abstract",
            "content": "Vision Language Models (VLMs) exhibit fundamental semantic-to-geometric gap in spatial reasoning: they excel at qualitative semantic inference but their reasoning operates within lossy semantic space, misaligned with highfidelity geometry. Current paradigms fail to bridge this gap. Training-based methods suffer from an oracle paradox, learning flawed spatial logic from imperfect oracles. Toolintegrated methods constrain the final computation but critically leave the VLMs planning process unconstrained, resulting in geometrically flawed plans. In this work, we propose Geometrically-Constrained Agent (GCA), trainingfree agentic paradigm that resolves this gap by introducing formal task constraint. Specifically, we strategically decouples the VLMs role into two stages. First, acting as semantic analyst, the VLM translates the users ambiguous query into the formal, verifiable task constraint, which defines the reference frame and objective. Second, acting as task solver, the VLM generates and executes tool calls strictly within the deterministic bounds defined by the constraint. This geometrically-constrained reasoning strategy successfully resolve the semantic-to-geometric gap, yielding robust and verifiable reasoning pathway for spatial reasoning. Comprehensive experiments demonstrate that GCA achieves SOTA performance on multiple spatial reasoning benchmarks, surpassing existing training-based and tool-integrated methods by 27%. 1. Introduction Intelligent agents operating in real-world applications, such as robotics [41, 54, 62], AR/VR [2, 11, 31], and autonomous driving [9, 42, 51], demand perceptual understanding of the world akin to humans. Humans intuitively comprehend their surroundings as cohesive 3D environment, effort- *Equal contribution. Corresponding author. lessly discerning object orientations and complex spatial relationships. However, equipping Vision Language Models (VLMs) into agents with this holistic spatial reasoning capability remains critical challenge [18, 53, 55, 59, 60]. As shown in Figure 1 (a), current VLMs lossily translate rich visual information into textual semantic space, leading fine-grained geometric details to be omitted or distorted [24, 50]. This creates fundamental semantic-togeometric gap: VLMs excel at probabilistic, qualitative semantic inference, but their lossy semantic space required for spatial reasoning fails to ground high-fidelity geometry. For example, VLM may possess the spatial commonsense (e.g., intuitively knowing that sitting on sofa implies viewpoint aligned with the sofas orientation), yet critically fail at high-precision geometric computation (e.g., determining the sofas orientation) and robust spatial imagination (e.g., imagining the users egocentric perspective). To reconcile this gap, robust constraints must be imposed, guiding the VLMs reasoning onto geometrically sound and verifiable pathway. However, effectively applying these constraints remains formidable challenge. Recent approaches that apply implicit constraints via end-to-end training [8, 22, 30, 33, 41, 48, 50, 52] attempt to embed geometric logic by fine-tuning on massive datasets. These methods, however, face an oracle paradox: their data generation relies on oracles like GPT-4o [17] which themselves struggle with spatial reasoning [18, 53, 55, 59, 60]. Consequently, the VLM is often trained on flawed spatial logic rather than sound geometric principles. An alternative paradigm, tool integration [12, 49, 62], attempts to bridge this gap by adopting an iterative plan-then-execute strategy, which offloads highprecision geometric computation to deterministic external tools. While this constrains the final computation process, the VLMs planning process remains unconstrained. To plan next step, the VLM must still perform spatial imagination and further decision-making within its lossy semantic space, inevitably producing geometrically flawed plans. 1 Figure 1. Overview. (a) Semantic-Geometric Gap. The geometric details required for spatial reasoning are lost when translating visual information into textual space, leading to VLMs flawed reasoning or unconstrained planning. (b) Geometrically-Constrained Spatial Reasoning. We propose formal task constraint that serves as deterministic bridge between semantics and geometry in spatial reasoning. For instance, when asked to reason from the perspective of user sitting on the sofa (see Figure 1), its unconstrained plan may default to the cameras viewpoint, compromising the problem definition before any tool is even called. These challenges reveal the critical research question: How do we bridge the VLMs semantic-to-geometric gap? We argue the solution is not to force the VLM to reason about lossy geometric details directly, but to reframe the problem into task that leverages its inherent strengths: using its spatial commonsense to define formal task constraint Ctask for subsequent computation. Specifically, this Ctask must be (1) grammatically rich enough to define complex spatial concepts, such as viewpoints, which elude traditional state-based formalisms (2) semantically clear enough for VLM to generate using its qualitative strengths, and (3) geometrically sound enough to provide deterministic, verifiable constraint for subsequent computation. To this end, we introduce Geometrically-Constrained (GCA), training-free agentic paradigm for Agent geometrically-constrained spatial reasoning. As shown in Figure 1 (b), this strategy leverages formal task constraint, Ctask, to decouple the reasoning process into two (1) Task Formalization. The VLM, acting as stages: semantic analyst, translates the ambiguous query and visual data into the formal, verifiable task constraint Ctask. This stage defines what to solve, establishing immutable sub-constraints: reference frame constraint and an objective constraint. (2) Constrained Geometric Computation. The VLM then, acting as task solver, generates and executes tool calls to compute the final answer, operating strictly within the deterministic bounds defined by Ctask. This two-stage decoupling directly bridges the semantic-togeometric gap. Through formulating geometrically sound constraint, we force the VLM to solve deterministic mathematical problems, thereby avoiding the demands for directly computing or imagining about high-fidelity geometric details that are lost in its semantic space. Extensive experiments demonstrate the effectiveness and generalizability of GCA paradigm. GCA yields substantial performance gains when applied to several foundation VLMs (by an average of 37%), establishing new state-of-the-art across diverse suite of challenging spatial reasoning benchmarks. 2. Related Work Spatial Reasoning in Vision Language Models. Spatial reasoning, including comprehension and mental manipulation of 3D spatial relationships [18, 53, 55, 59, 60], remains foundational challenge for Vision Language Models (VLMs) [17, 21, 36, 43]. To address this deficit, recent research [4, 8, 22, 30, 33, 39, 48, 50, 52] focuses on large-scale, end-to-end training on specialized spatial datasets. These methods attempt to bridge the 2D-3D cognitive gap by incorporating geometric priors, such as explicit 3D structural features [48], or depth maps [4], directly into the VLMs architecture, but they are often hindered by the reliance on high-quality datasets generated by flawed oracle. Another line of research introduce tool-integrated reasoning [12, 25, 29, 49, 62] to offloads deterministic geometric computation to external modules. For example, Spa2 Figure 2. Overall Paradigm of GCA. Given spatial reasoning query, our GCA leverages geometrically-constrained reasoning strategy centered on the formal task constraint (Ctask). The VLM first translates the ambiguous query into this explicit Ctask, establishing non-negotiable reference frame (CR) and objective (CO). Strictly constrained by Ctask, the VLM then orchestrates toolbox to perform deterministic geometric computation and derive the final answer. tialAgent [49] and TIGeR [12] focus on translating the input query directly into an iterative sequence of tool executions. However, unconstrained planning process could lead to geometrically-flawed results, causing the agent to conflate what to solve with how to solve it. Constrained-Guided Reasoning. Constraint-guided reasoning, which involves restricting search space by defining variables and the constraints governing them [38], has been adapted to manage the probabilistic nature of LLMs and VLMs. primary application is in neuro-symbolic reasoning [1, 13, 14, 34, 40, 58], where the LLM is constrained to act as translator, converting ambiguous natural language into formal, verifiable representation. For example, LogicLM [34] leverage LLMs to translate NL problems into task-specific formalisms like formal logic. This constraint-guided reasoning can also be extended to planning [3, 16, 26, 35, 56]. Frameworks like LLM+P [26] uses an LLM to translate an NL problem into formal PDDL format and then applies an optimal planner to generate the plan. Similarly, ReKep [16] employs VLM to translate free-form language into relational keypoint constraints and solves the constraints to generate final robot actions. 3. Methodology As illustrated in Figure 2, we propose GeometricallyConstrained Agent (GCA), training-free agentic paradigm designed for geometrically-constrained spatial reasoning. The core of GCA is the introduction of formal task constraint Ctask that serves as deterministic bridge between semantics and geometry. Section 3.1 defines this geometrically-constrained paradigm. Section 3.2 details the formal task constraint Ctask and its automated generation. Section 3.3 describes the subsequent constrained computation stage, which is strictly governed by this constraint. Finally, Section 3.4 discusses how GCA resolves the VLMs semantic-to-geometric gap in spatial reasoning. 3.1. Geometrically-Constrained Spatial Reasoning Contemporary agentic frameworks often model reasoning as generic, iterative policy. Those based on the ReAct 3 This gap necessitates new formalism. We thus propose novel formal task constraint Ctask specifically designed to capture the geometric nature of spatial reasoning. We define Ctask as tuple containing two key sub-constraints: single, non-negotiable Reference Frame Constraint (CR) that defines the coordinate system for answering the query, and an Objective Constraint (CO) that specify the objective to be measured within that frame. Reference Frame Constraint. Humans intuitively understand spatial language (e.g., north of) by grounding it within specific coordinate system, namely reference frame (R). In contrast, VLM failures often stem from ambiguity in this crucial grounding step, causing them to adopt flawed geometrically flawed plans [55] (e.g., defaulting to the cameras viewpoint). The Fformalize stage addresses this ambiguity by requiring the VLM to first formally anchor to the scenes geometry. We model all spatial queries as requiring 3D cartesian coordinate system defined by an origin OR and three orthogonal basic vectors (xR, yR, zR). This system follows the OpenCV convention, where +zR points forward, +yR points down and +xR follows the right-hand rule. The agents task is to anchor to one of three geometric primitives (see Figure 3) derived from the visual information: Object-based Frame. is defined by an objects intrinsic coordinate system. For example, the query when the user is washing hand implies reference frame defined by +zR = zsink (one must face sink to wash hand). Camera-based Frame. is defined by specific cameras viewpoint. For from the viewpoint of Figure 1, the reference frame is defined by +zR = +zcam1. Direction-based Frame. is defined by vecFor Owen is north the reference frame is defined by +zR = tor connecting two locations. of sink, normalize (Centroid(owen) Centroid(sink)) = north. The output of this step is human-readable and machineparsable definition of R, which becomes non-negotiable constraint CR for all subsequent computation. Objective Constraint. Concurrently, the agent identifies the objective from the query. This constraint CO defines what must be measured relative to the established R. For the query, Is chair to the west of toaster?, the toaster defines CR, while the positional relationship between toaster and chair is the objective constraint CO. 3.2.2. Automated Formalization via VLM We exploit the VLMs innate strength in semantic interpretation to generate Ctask automatically. Acting as semantic analyst, the VLM performs qualitative interpretation, guided by the formal definitions of Ctask, to generate the Ctask = (CR, CO). This formal task constraint, generated by the VLM but grounded in geometry, serves as the geometrically sound contract for the Fcompute stage. In our Figure 3. Reference Frame. Here, vsinkowen denotes vector calculated by normalize (Centroid(owen) Centroid(sink)). framework [57], for example, can be defined by: rt = A(q, v, , rt1). (1) In this framework, an agent produces response rt based on query q, visual information v, set of tools , and its past history rt1. This generic policy is unconstrained, making it unreliable for high-stakes, deterministic domains like spatial reasoning. Recent work [12, 49] attempts to mitigate this by using external tools to constrain the final computation. However, they fail to constrain the VLMs planning process. The VLM may still rely on its flawed spatial imagination in lossy semantic space to formulate plan, conflating what to solve with how to solve it. We solve this by replacing the generic policy with two-stage process. This paradigm is built on the formal task constraint Ctask, which functions as the architectural scaffolding to align the VLMs asymmetric capabilities: Ctask Fformalize(q, v), rt = Fcompute(Ctask, , rt1). (2) In the Fformalize stage, the VLM acts as semantic analyst, translating the ambiguous query and visual information into formal, verifiable task constraint Ctask. This stage defines what to solve by establishing the necessary geometric scaffolding (e.g., the reference frame and target subjects). In the Fcompute stage, the VLMs role shifts to task solver. Governed by the constraint Ctask established in the Fformalize stage, the VLM iteratively executes tool calls to acquire necessary geometric data and perform final computations. 3.2. Task Constraint Formalization 3.2.1. Constraint for Spatial Reasoning While existing constraint-guided reasoning paradigms leverage formalisms such as PDDL [26] or relational keypoint constraints [16], these constraints are insufficient for spatial reasoning. PDDL, for instance, excels at describing discrete, symbolic object states (e.g., is on(A, B)) but fundamentally lacks the geometric grammar to express the continuous, relative, and perspective-dependent nature of spatial queries (e.g., egocentric vs. allocentric viewpoints). 4 implementation, we enforce this architectural decoupling procedurally. The VLM executes the Fformalize stage and formalizes the Ctask before any computation begins. 3.3. Constrained Geometric Computation 3.3.1. Tool Integration and Code Generation Once the formal task constraint Ctask = (CR, CO) is established, the VLMs role shifts to constrained task solver. This Fcompute stage then operates as ReAct-style framework, consuming the Ctask as an immutable constraint. This execution is not one-shot generation but an iterative, closed-loop process involving data acquisition, ambiguity resolution, and augmented computation. Data Acquisition. Ctask dictates set of geometric ingredients that the agent must acquire. For instance, as shown in Figure 2, to instantiate an object-based frame defined by sink, the agent must acquire the orientation of that sink. The Fcompute stage begins by generating sequence of tool calls to parameterize the geometry, and acquire all variables necessary to instantiate Ctask. Tool Orchestration and Ambiguity Resolution. The VLM is responsible for managing tool feedback and resolving ambiguity, ensuring the data acquired from tools correctly binds to the symbols in Ctask. For example, considering CO involves an object like leftmost chair, the perception tool returns several chair detections. The VLM analyzes this feedback (e.g., visualizing bounding boxes) and resolves the ambiguity by determining which object index correctly corresponds to the context (leftmost) specified. This closed-loop mechanism allows the agent to handle noisy tool outputs while ensuring the final computation remains strictly grounded in the intent of Ctask. Knowledge-Augmented Code Generation. Once all variables in Ctask are bound to concrete geometric data, the agent invokes code generator for the final computation. To prevent the coder from hallucinating incorrect formulas, we leverage knowledge-augmented strategy, which functions analogously to static Retrieval-Augmented Generation (RAG) [10, 20] system. Specifically, when invoking the code generator, the VLM specifies high-level requirement and the necessary bound variables (e.g., objects orientation). Instead of expecting the coder to generate complex geometric formulas from memory, our framework maintains pre-prepared, fixed library of basic, verified geometric formulas. Based on the data types of the bound variables, the system automatically retrieves the relevant, fixed set of formulas (e.g., objects local-to-world transformation formula) and injects them directly into the code generators context. This ensures the computation steps do not produce black-box guesses, but rather deterministic results, derived from formally structured task and sound geometric principles. More details are provided in Section E. 3.3.2. Toolbox We equips the agent with perceptual and computation capabilities required to execute its geometrically-constrained reasoning flow in Fcompute, as shown in Figure 2. Detailed APIs for all tools are provided in Section C.2. Geometry and Perception Tools. These tools are responsible for parameterizing the visual world. 3D Reconstruction tool leverages foundational models like VGGT [44] to build unified, high-fidelity 3D representation of the scene. This provides the geometric context required for complex scenarios. This category also contains suite of 2D perception tools, such as Object Detection for open-vocabulary object detection, Segmentation for instance segmentation. Computation and Utility Tools. These tools operate on the data extracted by the perception tools and executes the final deterministic geometric computation. Python Tool is the core computational engine, which prompts the VLM to generate and execute Python code in sandbox environment, using the knowledge-augmented strategy. This category also includes essential utilities (Utility Tool). For example, project box to points bridges 2D perception to 3D computation by converting 2D bounding boxes into corresponding 3D point clouds. 3.4. Discussion Our GCA decouples VLMs spatial reasoning through the formal constraint Ctask, jointly addressing two core deficiencies in spatial reasoning. Fformalize Solves Flawed Planning and Imagination. Directly solving an ambiguous query forces the VLM to plan and perform spatial imagination within its native lossy semantic space. This is primary failure mode, as unconstrained planning can lead to geometrically flawed assumptions before any computation even begins. Our paradigm resolves this by reframing the problem. Leveraging VLMs strength in qualitative semantic interpretation, the Fformalize stage transform the original spatial query into deterministic mathematical problem with constraint, preventing the VLM to solve the query in its lossy semantic space directly. Fcompute Solves Flawed Execution and Computation. In this stage, the VLM acting as the task solver, orchestrating external tools to execute the plan. Crucially, its entire reasoning and execution process is bound by the formal task constraint Ctask generated in Fformalize. This ensures that all subsequent high-precision computations are executed strictly within the deterministic, geometrically sound constraint, effectively bridging the semantic-to-geometric gap. 4. Experiments 4.1. Experimental Setup Implementation Details. GCA is implemented as training-free agentic paradigm, requiring no model fine5 Table 1. Experimental Results on Several Spatial Reasoning Benchmarks. The best and second best results are shown in bold and underlined, respectively. Avg. denotes the average of overall accuracy across all benchmarks. More details about these benchmarks subcategory (e.g., PR.) are provided in Appendix. MMSI-Bench MindCube-tiny OmniSpatial SPBench CV-Bench PR. Attr. Mot. MSR All Rot. Ard. Amg. All Dyn. Pers. All SI MV All 2D 3D All Baseline Foundation VLMs Qwen3-VL-Thinking [36] GLM-4.5V [15] GPT-4o [17] Gemini-2.5-Pro [6] 33.7 35.6 28.0 39.0 Training-based Spatial VLMs SpatialLLM [30] Spatial-MLLM [48] SpatialLadder [22] SpaceR [33] Video-R1 [8] RoboBrain-2.0 [41] VILASR [50] VLaser [52] 24.5 28.5 30.3 29.1 30.5 28.9 35.9 29.8 Tool-Integrated Spatial Agents TIGeR [12] 29.1 GCA (ours) 52.8 40.0 36.9 32.3 36.2 23.1 25.4 23.3 29.4 25.4 28.8 26.0 26.9 23.3 29.3 36.0 33.3 22.7 18.0 16.0 21.9 22.0 22.5 21.0 26.0 27. 45.0 26.0 44.7 31.8 30.3 30.8 34.3 30.8 26.3 21.2 22.5 26.8 28.0 23.2 18.9 25. 38.0 32.6 33.8 30.3 36.9 25.3 26.1 25.4 26.9 27.8 28.9 29.8 27.3 87.0 60.0 33.5 89.5 34.0 33.8 30.5 29.8 30.0 29.7 34.4 31.5 47.3 25.5 35.0 54. 26.8 34.5 39.8 30.0 30.5 35.8 25.7 24.8 27.8 33.0 47.6 82.0 28. 61.8 35.0 42.2 37.2 48.8 33.0 28.3 47.8 26.8 41.3 45.2 29.4 38.2 26.7 59.8 47.3 39.6 35.8 57. 31.1 32.1 42.3 28.3 35.8 39.6 29.1 32.6 60.5 58.6 58.7 70.7 59.6 37.2 46.5 53.5 50.0 49.4 37.5 39.1 28.3 64.2 52. 73.6 43.9 47.2 46.2 44.6 42.9 42.1 43.1 40.5 44.2 42.2 42.2 42.6 45.7 58.6 Avg. 54.4 52.5 47.6 58.5 40.2 42.3 51.2 45.7 45.6 49.1 45.5 48.3 51.0 52.1 51.5 55.8 49.5 40.0 44.5 46.0 46.7 45.2 40.2 41.1 51.9 50.0 42.4 55.6 32.2 52.0 70.2 48.6 44.8 49.1 50.2 53. 61.2 55.1 48.3 58.3 26.4 52.0 70.9 59.4 40.7 46.8 57.6 69.2 54.1 51.3 43.8 56.3 30.7 52.0 70.3 51.1 43.8 48.5 51.9 56.9 81.9 80.7 69.4 81.2 51.3 59.5 72.4 74.1 73.5 77.1 75.7 79. 92.6 91.6 84.9 92.5 78.6 63.3 74.9 77.4 74.7 90.7 77.7 87.8 86.8 85.6 76.5 86.3 64.5 61.2 73.7 75.6 74.0 83.4 76.6 83.6 49.8 48. 38.8 46.3 75.2 95.7 65.1 61. 61.9 61.8 83.6 90.8 84.5 86. 47.3 65.1 It centers on VLM responsible for both stages tuning. of our paradigm: acting as the semantic analyst to generate the Ctask in the Fformalize stage, and as the task solver to manage suite of off-the-shelf foundation models for perception and computation [27, 37, 44, 45, 47]. For our primary experiments, we utilize Qwen3-VL-Thinking [36] as the central VLM. To assess the paradigms generalizability, we also evaluate other leading VLMs in our ablation studies, including GLM-4.5V [15], GPT-4o [17], and etc. All open-source VLMs are deployed using the vLLM inference engine [19] for efficiency. The agents architecture is built using Ray [32] for concurrent tool execution and LangGraph for robust state management. Evaluation Benchmarks and Counterparts. We conduct comprehensive experiments on several spatial reasoning benchmarks. As our current toolbox is primarily designed for image-based inputs, we focus on evaluations that test complex spatial logic from single and multiple images, including MMSI-Bench [55], MindCube-tiny [59], OmniSpatial (Perspective Taking + Dynamic Reasoning) [18], SPBench [22] and CV-Bench [43]. For all benchmarks, we report both overall accuracy (%) and subcategory accuracy (%). We compare our paradigm against several counterparts, including baseline foundation VLMs [6, 15, 17, 36], training-based methods [8, 22, 30, 33, 41, 48, 50, 52] and tool-integrated agents [12]. 4.2. Main Results SOTA Performance. As shown in Table 1, GCA establishes new state-of-the-art across wide range of spatial reasoning benchmarks, achieving an average accuracy of 64.8%. Our geometrically-constrained paradigm surpasses the strongest foundation VLM baseline (Gemini-2.5Pro [6] by 12%) and demonstrates massive lead over other training-based (e.g., SpatialLadder [22] by 27%) or agentic approaches (e.g., TIGeR [12] by 38%). These results strongly validate that our strategy, centered on the Ctask, successfully bridges the VLMs semantic-to-geometric gap. Effectiveness on Challenging Benchmarks. The advantage of our constrained paradigm is most pronounced on complex, multi-step spatial reasoning benchmarks. For example, on MMSI-Bench, the performance of even SOTA foundation VLMs remain severely limited. Considering its 4-choice questions, most counterparts perform near the 25% random-guess threshold. In contrast, GCA achieves an overall accuracy of 47.6%, surpassing the strongest VLM baseline (Gemini-2.5-Pro) by 28% relative improvement. similar trend is evident on other challenging benchmarks like MindCube-tiny, where GCA (64.2%) also significantly outperforms the top baselines. This superior performance stems directly from our paradigm. The introduction of Ctask prevents the VLM from defaulting to flawed semantic shortcuts or falling into lossy spatial imagination. Generalizability Across Benchmarks. Our training-free paradigm also demonstrates superior generalizability compared to training-based specialists, which often suffer from biases inherent to their training data. For example, SpatialLadder [22] is fine-tuned on data originating from the same source as the SPBench, leading to high in-domain score of 70.3%. However, its performance on out-of-domain benchmarks is suboptimal, where GCA consistently outperforms it, often by margin of 20 points. similar bias affects TIGeR [12]. While its tools theoretically support multiview processing, the model is primarily trained on single6 Figure 4. Ablation Study on Formalization. We compare our method in against several baselines: (1) no tool integration (Baseline (CoT-Only)), (2) unconstrained tool integration with (Tool (Prompt)) or without (Tool (Uncon.)) hints, (3) using humanannotated Ctask (Oracle (Anno.)). image tasks. Consequently, it performs well on singleimage benchmarks like CV-Bench but fails on multi-view benchmarks such as MMSI-Bench and MindCube. GCA, in contrast, is not compromised by these training priors and leverages its multi-view tools as dictated by the problem. This demonstrates that our GCA, which forces the VLM to derive geometrically sound task constraint for each new problem, provides more robust and generalizable pathway to spatial reasoning. 4.3. Ablation Study In this section, we conduct extensive ablation studies to dissect the GCA paradigm and validate its core design. Our (1) How analysis aims to answer four critical questions. essential is the formal task constraint Ctask? (2) How generalizable is the GCA paradigm across different VLMs? (3) What is the contribution of each system component? 4.3.1. Formalization Analysis We first investigate the necessity and impact of our core contribution, the Ctask constraint, by comparing our method against different reasoning strategies in Figure 4. The results strongly confirm our central hypothesis. Simply prompting the VLM to pay attention to the reference frame and objective in the query (Tool (Prompt)) only yields negligible improvement on unconstrained tool integration. This empirically suggests that the VLMs unconstrained planning process remains fundamentally flawed and unreliable, even when weakly guided by hints. In comparison, the introduction of our formal Ctask constraint (Ours) delivers substantial performance boost, far surpassing all unconstrained methods. This demonstrates that deterministic and verifiable constraint is essential for bridging the VLMs semantic-to-geometric gap, as it forces the VLM Figure 5. Ablation Study on Generalizability across Different VLMs. Our GCA achieves an average of 37% relative performance improvement across all tested foundation VLMs. to first establish what to solve before determining how to solve it. Furthermore, we explore the theoretical upper bound using human-annotated oracle formalization (Oracle (Anno.)). The gap between our method (47.6%) and oracle (49.5%) is relative small. As revealed in Section 4.4, the Fformalize stage achieves 70% accuracy, confirming the formalization task is well within the VLMs capabilities. 4.3.2. Generalizability Across VLMs We assess the generalizability of our GCA paradigm by applying it to several leading foundation VLMs, including GLM-4.5V [15], GPT-4o [17], and Gemini-2.5-Pro [6]. As shown in Figure 5, GCA proves to be highly generalizable architectural solution, substantially enhancing the spatial reasoning capabilities of every VLM tested compared to their CoT-only baselines. We observe that the magnitude of this enhancement appears to correlate strongly with the VLMs inherent agentic proficiency and their baseline spatial reasoning capability. It is most evident that Gemini-2.5-Pro, which holds the strongest CoT-only baseline on MMSI-Bench (36.9%), also achieves the most dramatic gain (+49%), rising to 55.0%. On the other hand, the improvement on GPT-4o, while significant, is more modest (+19%). We attribute it to its suboptimal agentic reasoning capability and coding skills. Through introduction of formal task constraint Ctask, our paradigm serves as catalyst, successfully unlocking and guiding the VLMs powerful execution engine towards the robust spatial reasoning across diverse set of SOTA models. 4.3.3. Component Contribution We quantify the importance of each component in the GCA, as presented in Table 2. This analysis reveals improvements in two distinct parts. First, building standard tool-integrated agent by adding tool integration (+4.2 points), knowledge-augmented code generation (KACG, +1.9 points), and visual feedback (+1.4 points) provides 7 Figure 6. Error Attribution and Failure Cases. We provide detailed error attribution analysis to identify the main failure modes within the VLMs reasoning trajectory. Table 2. Ablation Study on Each Component in GCA. Here, KACG denotes applying knowledge-augmented code generation, and Feedback denotes applying the VLM to manage tool feedback and resolve ambiguity. Tool Integration KACG Feedback Ctask MMSI-Bench 32.6 36.8 38.7 40.1 47. cumulative +7.5 points gain over the CoT-only baseline. The second part, the introduction of Fformalize, brings an additional massive improvement, increasing the overall accuracy by +7.5 points. This result strongly validates that constraining the VLMs planning via formal Ctask is essential to prevent flawed reasoning within its lossy semantic space. 4.4. Error Attribution and Failure Cases key advantage of GCA paradigm is its verifiable and interpretable nature, which allows us to trace the reasoning pathway and perform detailed error attribution. As shown in Figure 6 (a), this analysis pinpoints the current bottlenecks, attributing failures to either the VLMs initial formalization or the subsequent tool orchestration. Errors in Fformalize. Failures in the initial Fformalize stage account for 30% of all errors. Given this is the first step of the paradigm, it indicates the VLM achieves approximately 70% accuracy in correctly formalizing the task constraint Ctask. deeper analysis reveals these failures primarily lie in challenging cases involving complex semantics, ambiguity in multiple images, or ignored implications. For instance, as shown in Figure 6 (b), when asked about topdown view, the VLM fails to grasp the querys implication that down referred to the direction of gravity, defaulting instead to camera down and establishing an incorrect reference frame. Errors in Fcompute. The remaining 70% of errors occur during Fcompute stage. Perception failures (24%) are major bottleneck, particularly in Reconstruction and Orientation. typical reconstruction failure, shown in Figure 6 (e), is caused by the inability of the underlying VGGT [44] to accept textual input. The querys textual input, each shot after rotating 60 degrees provides deterministic rotational sequence. However, the VGGT model, which cannot accept this textual input, parameterize the scene incorrectly, resulting in the wrong order of cameras and flawed geometric foundation. Errors from Python Tool (25%) are also significant, often stemming from forgotten coordinate transformations or lacking nuanced problem-solving logic, such as identifying principal direction. Besides, Other (21%) errors capture issues like incorrect parameter passing between tools, exhausting the predefined budget (e.g., maximum of 15 turns), and etc. 5. Conclusion In this work, we introduce GCA, training-free agentic paradigm designed to bridge the VLMs semantic-togeometric gap in spatial reasoning. We address it through leveraging formal task constraint, transforming the ambiguous spatial query into deterministic mathematic problem with constraints, preventing the VLM reasoning about the geometric details within its lossy semantic space. As demonstrated experimentally, GCA establishes new stateof-the-art on multiple challenging spatial reasoning benchmarks, showcasing effective and generalizable pathway for robust spatial reasoning. Limitations and Future Prospects. The GCA paradigm, involving iterative tool calls and VLM interactions, is computationally more costly than simple end-to-end CoT reasoning. However, this trade off yields more robust and verifiable reasoning pathway. Furthermore, we believe the structured outputs from our Fformalize and Fcompute stages can serve as valuable source of supervision, such as process reward in reinforcement learning, for training more ef8 ficient end-to-end spatial VLMs in the future. Besides, our current toolbox is primarily designed for image-based spatial reasoning. key direction for future work is to extend this geometrically-constrained framework by incorporating tools for temporal reasoning and motion tracking, thereby addressing broader range of spatial intelligence tasks."
        },
        {
            "title": "References",
            "content": "[1] Bikram Pratim Bhuyan, Amar Ramdane-Cherif, Ravi Tomar, and TP Singh. Neuro-symbolic artificial intelligence: survey. Neural Computing and Applications, 36(21):12809 12844, 2024. 3 [2] Keshigeyan Chandrasegaran, Agrim Gupta, Lea Hadzic, Taran Kota, Jimming He, Cristobal Eyzaguirre, Zane Durante, Manling Li, Jiajun Wu, and Li Fei-Fei. Hourvideo: 1-hour video-language understanding. Advances in Neural Information Processing Systems, 37:5316853197, 2024. 1 [3] Zeren Chen, Zhelun Shi, Xiaoya Lu, Lehan He, Sucheng Qian, Zhenfei Yin, Wanli Ouyang, Jing Shao, Yu Qiao, Cewu Lu, et al. Rh20t-p: primitive-level robotic dataset towards composable generalization agents. arXiv preprint arXiv:2403.19622, 2024. 3 [4] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision-language models. Advances in Neural Information Processing Systems, 37:135062135093, 2024. 2 [5] Jae-Woo Choi, Youngwoo Yoon, Hyobin Ong, Jaehong Kim, and Minsu Jang. Lota-bench: Benchmarking languageIn The oriented task planners for embodied agents. Twelfth International Conference on Learning Representations, 2024. 12 [6] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 6, 7, 14 [7] Gunnar Farneback. Two-frame motion estimation based on polynomial expansion. In Scandinavian conference on Image analysis, pages 363370. Springer, 2003. [8] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. Advances in Neural Information Processing Systems, 2025. 1, 2, 6 [9] Daocheng Fu, Xin Li, Licheng Wen, Min Dou, Pinlong Cai, Botian Shi, and Yu Qiao. Drive like human: Rethinking In 2024 autonomous driving with large language models. IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW), pages 910919. IEEE, 2024. 1 [10] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yixin Dai, Jiawei Sun, Haofen Wang, and Haofen Wang. Retrieval-augmented generation for large language models: survey. arXiv preprint arXiv:2312.10997, 2(1), 2023. 5 [11] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1899519012, 2022. 1 [12] Yi Han, Cheng Chi, Enshen Zhou, Shanyu Rong, Jingkun An, Pengwei Wang, Zhongyuan Wang, Lu Sheng, and Shanghang Zhang. Tiger: Tool-integrated geometric reasoning in vision-language models for robotics. arXiv preprint arXiv:2510.07181, 2025. 1, 2, 3, 4, 6 [13] Yilun Hao, Yang Zhang, and Chuchu Fan. Planning anything with rigor: General-purpose zero-shot planning with In The Thirteenth Inllm-based formalized programming. ternational Conference on Learning Representations, 2024. 3 [14] Pascal Hitzler and Md Kamruzzaman Sarker. symbolic artificial intelligence: The state of the art. press, 2022. 3 NeuroIOS [15] Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, et al. Glm-4.1 v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning. arXiv e-prints, pages arXiv2507, 2025. 6, 7 [16] Wenlong Huang, Chen Wang, Yunzhu Li, Ruohan Zhang, and Li Fei-Fei. Rekep: Spatio-temporal reasoning of relational keypoint constraints for robotic manipulation. In Conference on Robot Learning, pages 45734602. PMLR, 2025. 3, 4, 12 [17] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 1, 2, 6, 7 [18] Mengdi Jia, Zekun Qi, Shaochen Zhang, Wenyao Zhang, Xinqiang Yu, Jiawei He, He Wang, and Li Yi. Omnispatial: Towards comprehensive spatial reasoning benchmark for vision language models. arXiv preprint arXiv:2506.03135, 2025. 1, 2, 6, 11, 12, [19] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th symposium on operating systems principles, pages 611626, 2023. 6, 14 [20] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:94599474, 2020. 5 [21] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. Transactions on Machine Learning Research, 2025. 2 [22] Hongxing Li, Dingming Li, Zixuan Wang, Yuchen Yan, Hang Wu, Wenqi Zhang, Yongliang Shen, Weiming Lu, Jun Xiao, and Yueting Zhuang. Spatialladder: Progressive train9 ing for spatial reasoning in vision-language models. arXiv preprint arXiv:2510.08531, 2025. 1, 2, 6, 11, 12, 15 [23] Manling Li, Shiyu Zhao, Qineng Wang, Kangrui Wang, Yu Zhou, Sanjana Srivastava, Cem Gokmen, Tony Lee, Erran Li Li, Ruohan Zhang, et al. Embodied agent interface: Benchmarking llms for embodied decision making. Advances in Neural Information Processing Systems, 37: 100428100534, 2024. 12 [24] Songtao Li and Hao Tang. Multimodal alignment and fusion: survey. arXiv preprint arXiv:2411.17040, 2024. 1 [25] Zefu Lin, Rongxu Cui, Chen Hanning, Xiangyu Wang, Junjia Xu, Xiaojuan Jin, Chen Wenbo, Hui Zhou, Lue Fan, Wenling Li, et al. Embodiedcoder: Parameterized embodied mobile manipulation via modern coding model. arXiv preprint arXiv:2510.06207, 2025. 2, [26] Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. Llm+p: Empowering large language models with optimal planning proficiency. arXiv preprint arXiv:2304.11477, 2023. 3, 4 [27] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European conference on computer vision, pages 3855. Springer, 2024. 6, 13 [28] Xiaoya Lu, Zeren Chen, Xuhao Hu, Yijin Zhou, Weichen Zhang, Dongrui Liu, Lu Sheng, and Jing Shao. Is-bench: Evaluating interactive safety of vlm-driven embodied agents in daily household tasks. arXiv preprint arXiv:2506.16402, 2025. 12 [29] Chenyang Ma, Kai Lu, Ta-Ying Cheng, Niki Trigoni, and Andrew Markham. Spatialpin: Enhancing spatial reasoning capabilities of vision-language models through prompting and interacting 3d priors. Advances in neural information processing systems, 37:6880368832, 2024. 2 [30] Wufei Ma, Luoxin Ye, Celso de Melo, Alan Yuille, and Jieneng Chen. Spatialllm: compound 3d-informed design towards spatially-intelligent large multimodal models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1724917260, 2025. 1, 2, 6 [31] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very longform video language understanding. Advances in Neural Information Processing Systems, 36:4621246244, 2023. 1 [32] Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang, Melih Elibol, Zongheng Yang, William Paul, Michael Jordan, et al. Ray: distributed framework for emerging {AI} applications. In 13th USENIX symposium on operating systems design and implementation (OSDI 18), pages 561577, 2018. 6, [33] Kun Ouyang, Yuanxin Liu, Haoning Wu, Yi Liu, Hao Zhou, Jie Zhou, Fandong Meng, and Xu Sun. Spacer: Reinforcing mllms in video spatial reasoning. arXiv preprint arXiv:2504.01805, 2025. 1, 2, 6 [34] Liangming Pan, Alon Albalak, Xinyi Wang, and William Yang Wang. Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. 3 [35] Mingjie Pan, Jiyao Zhang, Tianshu Wu, Yinghao Zhao, Wenlong Gao, and Hao Dong. Omnimanip: Towards general robotic manipulation via object-centric interaction primitives as spatial constraints. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1735917369, 2025. 3 [36] QwenTeam. Qwen3-vl: Sharper vision, deeper thought, https : / / qwen . ai / blog ? id = broader action. 99f0335c4ad9ff6153e517418d48535ab6d8afef& from = research . latest - advancements - list, 2025. 2, 6, 13, [37] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. In The Thirteenth International Conference on Learning Representations, 2024. 6, 13 [38] Stuart Russell, Peter Norvig, and Artificial Intelligence. modern approach. Artificial Intelligence. Prentice-Hall, Egnlewood Cliffs, 25(27):7980, 1995. 3 [39] Chan Hee Song, Valts Blukis, Jonathan Tremblay, Stephen Tyree, Yu Su, and Stan Birchfield. Robospatial: Teaching spatial understanding to 2d and 3d vision-language models for robotics. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1576815780, 2025. 2 [40] Oren Sultan, Eitan Stern, and Dafna Shahaf. Towards reliable proof generation with llms: neuro-symbolic approach. arXiv preprint arXiv:2505.14479, 2025. 3 [41] BAAI RoboBrain Team, Mingyu Cao, Huajie Tan, Yuheng Ji, Xiansheng Chen, Minglan Lin, Zhiyu Li, Zhou Cao, Pengwei Wang, Enshen Zhou, et al. Robobrain 2.0 technical report. arXiv preprint arXiv:2507.02029, 2025. 1, 6 [42] Xiaoyu Tian, Junru Gu, Bailin Li, Yicheng Liu, Yang Wang, Zhiyong Zhao, Kun Zhan, Peng Jia, XianPeng Lang, and Hang Zhao. Drivevlm: The convergence of autonomous driving and large vision-language models. In Conference on Robot Learning, pages 46984726. PMLR, 2025. 1 [43] Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. Advances in Neural Information Processing Systems, 37:8731087356, 2024. 2, [44] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 52945306, 2025. 5, 6, 8, 13 [45] Ruicheng Wang, Sicheng Xu, Cassie Dai, Jianfeng Xiang, Yu Deng, Xin Tong, and Jiaolong Yang. Moge: Unlocking accurate monocular geometry estimation for open-domain In Proceedings images with optimal training supervision. of the Computer Vision and Pattern Recognition Conference, pages 52615271, 2025. 6 [46] Ruicheng Wang, Sicheng Xu, Yue Dong, Yu Deng, Jianfeng Xiang, Zelong Lv, Guangzhong Sun, Xin Tong, and Jiaolong 10 Yang. Moge-2: Accurate monocular geometry with metric scale and sharp details. arXiv preprint arXiv:2507.02546, 2025. 13 [47] Zehan Wang, Ziang Zhang, Tianyu Pang, Chao Du, Hengshuang Zhao, and Zhou Zhao. Orient anything: Learning robust object orientation estimation from rendering 3d models. In Forty-second International Conference on Machine Learning, 2024. 6, [48] Diankun Wu, Fangfu Liu, Yi-Hsin Hung, and Yueqi Duan. Spatial-mllm: Boosting mllm capabilities in visual-based spatial intelligence. Advances in Neural Information Processing Systems, 2025. 1, 2, 6 [49] Haoning Wu, Xiao Huang, Yaohui Chen, Ya Zhang, Yanfeng Wang, and Weidi Xie. Spatialscore: Towards unified evaluation for multimodal spatial understanding. arXiv preprint arXiv:2505.17012, 2025. 1, 2, 3, 4 [50] Junfei Wu, Jian Guan, Kaituo Feng, Qiang Liu, Shu Wu, Liang Wang, Wei Wu, and Tieniu Tan. Reinforcing spatial reasoning in vision-language models with interwoven thinking and visual drawing. Advances in Neural Information Processing Systems, 2025. 1, 2, 6 [51] Zhenhua Xu, Yujia Zhang, Enze Xie, Zhen Zhao, Yong Guo, Kwan-Yee Wong, Zhenguo Li, and Hengshuang Zhao. Drivegpt4: Interpretable end-to-end autonomous driving via large language model. IEEE Robotics and Automation Letters, 2024. 1 [52] Ganlin Yang, Tianyi Zhang, Haoran Hao, Weiyun Wang, Yibin Liu, Dehui Wang, Guanzhou Chen, Zijian Cai, Junting Chen, Weijie Su, et al. Vlaser: Vision-language-action model with synergistic embodied reasoning. arXiv preprint arXiv:2510.11027, 2025. 1, 2, 6 [53] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1063210643, 2025. 1, 2, 11, [54] Rui Yang, Hanyang Chen, Junyu Zhang, Mark Zhao, Cheng Qian, Kangrui Wang, Qineng Wang, Teja Venkat Koripella, Marziyeh Movahedi, Manling Li, et al. Embodiedbench: Comprehensive benchmarking multi-modal large language models for vision-driven embodied agents. In Forty-second International Conference on Machine Learning, 2025. 1, 12 [55] Sihan Yang, Runsen Xu, Yiman Xie, Sizhe Yang, Mo Li, Jingli Lin, Chenming Zhu, Xiaochen Chen, Haodong Duan, Xiangyu Yue, et al. Mmsi-bench: benchmark for multiimage spatial intelligence. arXiv preprint arXiv:2505.23764, 2025. 1, 2, 4, 6, 11, 12, 14 [56] Zhutian Yang, Caelan Garrett, Dieter Fox, Tomas LozanoPerez, and Leslie Pack Kaelbling. Guiding long-horizon task and motion planning with vision language models. In 2025 IEEE International Conference on Robotics and Automation (ICRA), pages 1684716853. IEEE, 2025. 3 [57] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing In The eleventh reasoning and acting in language models. international conference on learning representations, 2022. 4 [58] Xi Ye, Qiaochu Chen, Isil Dillig, and Greg Durrett. Satlm: Satisfiability-aided language models using declarative prompting. Advances in Neural Information Processing Systems, 36:4554845580, 2023. 3 [59] Baiqiao Yin, Qineng Wang, Pingyue Zhang, Jianshu Zhang, Kangrui Wang, Zihan Wang, Jieyu Zhang, Keshigeyan Chandrasegaran, Han Liu, Ranjay Krishna, et al. Spatial mental modeling from limited views. In Structural Priors for Vision Workshop at ICCV25, 2025. 1, 2, 6, 11, 12, [60] Songsong Yu, Yuxin Chen, Hao Ju, Lianjie Jia, Fuxi Zhang, Shaofei Huang, Yuhan Wu, Rundi Cui, Binghao Ran, Zaibin Zhang, et al. How far are vlms from visual spatial intelligence? benchmark-driven perspective. arXiv preprint arXiv:2509.18905, 2025. 1, 2 [61] Shiduo Zhang, Zhe Xu, Peiju Liu, Xiaopeng Yu, Yuan Li, Qinghui Gao, Zhaoye Fei, Zhangyue Yin, Zuxuan Wu, YuGang Jiang, et al. Vlabench: large-scale benchmark for language-conditioned robotics manipulation with longIn Proceedings of the IEEE/CVF horizon reasoning tasks. International Conference on Computer Vision, pages 11142 11152, 2025. 12 [62] Filippo Ziliotto, Tommaso Campari, Luciano Serafini, and Lamberto Ballan. Tango: Training-free embodied ai agents for open-world tasks. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2460324613, 2025. 1, 2 A. Spatial Task Constraint As introduced in the main paper, the core of GeometricallyConstrained Agent (GCA) paradigm is the formal task constraint Ctask. It serves as deterministic bridge to resolve the fundamental semantic-to-geometric gap, effectively decoupling the VLMs role into semantic analyst and constrained task solver. Recall that Ctask is formally defined as tuple containing two key sub-constraints: the Reference Frame Constraint (CR), which defines the coordinate system, and the Objective Constraint (CO), which specifies what to measure within that frame. We guide the Visual Language Model to automatically generate Ctask (for specific prompts used in GCA, refer to Section E). In this section, we first elaborate on the universality of this constraint across different spatial task domains. A.1. Universality of Ctask in Spatial Tasks The Ctask is not rigid definition fixed to single problem type, but rather general principle for wide range of spatial tasks with semantic-to-geometric gap. The core idea is to leverage the VLMs semantic advantage to formalize the most significant geometric ambiguity of the task. The nature of this ambiguity shifts depending on the task domain. Spatial Understanding and Reasoning. For spatial reasoning tasks, such as those evaluated in the main paper [18, 22, 53, 55, 59], the objective is typically simple 11 and explicitly stated in the query (e.g., what is the relative position? or which object is wider?). Therefore, the objective constraint CO is often trivial to formalize. The primary geometric ambiguity lies in the reference frame constraint CR. query like where is the table relative to you? is unsolvable until the reference frame (you) is geometrically grounded. GCAs formalization of CR (e.g., objectbased, camera-based, direction-based frames) is specifically designed to resolve this ambiguity. Robotic Manipulation and Interaction. Conversely, for robotic manipulation and interaction tasks [5, 23, 28, 54, 61], the reference frame constraint is often simple. The frame is typically the robots egocentric perspective or fixed world frame aligned with the primary camera. The geometric ambiguity shifts entirely to the objective constraint CO. command like pour tea into the cup has trivial CR but highly complex CO. The objective is not simple measurement but complex, multi-stage procedure involving affordances, contact points, and trajectories. For example, ReKep [16] tackles the manipulation by instructing the VLM to formalize the objective CO as set of Relational Keypoint Constraints. These constraints CO are literally generated as cost functions written by VLM in Python that map 3D keypoints to numerical cost. The cost functions are then passed to an inverse kinematics solver (Fcompute) to find the optimal robot action. Similarly, EmbodiedCoder [25] formalizes the CO as an executable program. The VLM is prompted to first generate code for geometric parameterization, fitting point clouds to functional primitives like rectangle and hinge axis for door. Through generating precise, parametric motion that conforms to the geometric shape just defined, the Python interpreter (Fcompute) simply executes the code to produce the final waypoints. These works are not in conflict with our proposed task constraint Ctask. Instead, they can serve as complementary examples of its core principle. They demonstrate how the CO for complex manipulation and interaction can be formalized as code, cost functions, or geometric constraints. These constraints are then passed to solver, just as GCA proposes. This confirms the universality of the Ctask: whether the ambiguity lies in the reference frame or the objective, the first and most critical step is to use the VLMs semantic strength to formalize deterministic, geometrically-sound constraint to bridge the semantic-to-geometric gap. A.2. Generalizability of Definintion We define three types of reference frame in GCA: objectbased, camera-based and direction-based reference frame, providing robust and flexible framework. We find that these categories are sufficient to cover the vast majority of static spatial reasoning queries encountered in existing benchmarks [18, 22, 55, 59]. As spatial reasoning advances Table 3. Ablation Study on Task Constraint. Tool Integration Ref. CR Obj. CO MMSI-Bench 47.6 46.4 41.0 40. 33.5 toward more complex, dynamic, and abstract scenarios, we identify key limitations and challenges for the current implementation of CR. These represent important avenues for future research. Dynamic and Time-Varying Reference Frame. significant challenge arises in video-based spatial reasoning [53], particularly in tasks involving continuous navigation or long-horizon agent actions. For example, an instruction like, Move forward to the right first, then move backward to the right, and finally turn left, involves CR that is continuously changing at each step, contingent on the agents previous state. Solving this requires extending the CR formalization to become time-dependent function CR(t), capable of tracking and updating the agents pose and orientation throughout sequence. Frames from Abstract Concepts. This challenge emerges when the reference entity is not rigid, easily-definable object. query like the living room is south of the kitchen poses significant problem. It is often impossible to compute meaningful direction vector between the geometric centroids of two abstract areas or regions. Currently, GCA relies on proxies, for example, if such direction (from kitchen to living room) aligns with the cameras view from the background to the foreground, we might substitute Zcam as the direction vector. This workaround, however, can introduce cumulative errors and lacks generalizability. Future work could explore novel methods to address this. One promising direction is to empower the VLM to directly annotate the reference frame, i.e., outputting two points in the image whose corresponding 3D vector defines the abstract direction. B. More Ablation Studies To further explore the proposed formal task constraint Ctask and the stability of GCA, we present three additional ablation studies. These experiments are designed to (1) precisely quantify the relative importance of the reference frame constraint (CR) versus the objective constraint (CO) for spatial reasoning tasks, (2) validate the constraints effectiveness in improving the VLMs internal, tool-free reasoning, and (3) evaluate the stability and robustness of the GCA paradigm. 12 Importance of CR and CO in Spatial Reasoning. To validate our claim in Section A.1 that CR represents the primary geometric ambiguity in spatial reasoning tasks, we conduct detailed ablation on the sub-components of Ctask. As shown in Table 3, removing the objective constraint (CO) results in minor 1.2 point performance drop. This demonstrates that for spatial reasoning queries, the objective is often simple and clearly stated, allowing the task solver to infer it from the query during the Fcompute stage. In contrast, removing the reference frame constraint (CR) causes 6.6 point performance drop. This suggests that CR is the most critical component in spatial reasoning, as it resolves the core geometric ambiguity of from where that VLMs cannot solve in their lossy semantic space. Ctask without Tool Integration. The Ctask is not prompt that can fix the VLMs internal spatial reasoning. Instead, it unlocks the VLMs agentic reasoning capability and coding skills by constraining the subsequent computation stage (Fcompute). This is confirmed by the results in Table 3, where we compare GCA with VLM that receives Ctask as the prompt but relies solely on chain-of-thought (CoT) reasoning. With the constraint as textual hint, it only yields negligible 0.9 point improvement. Even when told the reference frame and objective, the VLM still cannot bypass the internal flawed spatial imagination and high-precision computation in its lossy semantic space. Stability and Robustness Analysis. key consideration for any agentic framework, especially one involving multiple VLM calls and tool interactions, is the stability of its results. The probabilistic nature of VLMs could potentially lead to high variance in final performance. To assess the robustness of GCA, we conduct = 10 independent evaluation runs on the complete MMSI-Bench dataset, using the same Qwen3-VL-Thinking [36] for each run. All settings are kept identical as in the Table 1 (main paper). The mean accuracy and the standard deviation across all 10 runs is 47.6 0.3. The results demonstrate very low standard deviation, indicating that the GCA framework is highly stable. This stability is direct benefit of our core design: by forcing the VLM to first generate deterministic formal task constraint Ctask, we significantly reduce the stochasticity and ambiguity in the subsequent Fcompute stage. The task solver operates within the non-negotiable geometric bounds defined by Ctask, leading to consistent and verifiable reasoning pathway. C. More Implementation Details C.1. Visual Foundation Models We deploy several Visual Foundation Models (VFMs) for agent to parameterize the visual world, facilitating the deterministic Fcompute stage constrained by Ctask. VGGT [44]. Visual Geometry Grounded Transformer (VGGT) is large feed-forward model that infers key 3D attributes from one or multiple images. It predicts camera parameters, point maps, and depth maps for all input views. Within GCA, VGGT serves as the primary geometry parameterization engine for 3D reconstruction. MoGe-2 [46]. The Monocular Geometry (MoGe) estimation model is designed to recover 3D point maps with metric scale from single image. It achieves this by decoupling the problem, predicting both an affine-invariant point map and separate global scale factor. GCA leverages this unique capability to derive the correct real-world scale for the scene. GroundingDINO [27]. GroundingDINO is an open-set object detector that combines transformer-based detector with grounded language pre-training. This architecture enables it to detect arbitrary objects specified by natural language, such as category names or referring expressions. It serves as specialized detection tool in GCA. SAM-2 [37]. SAM-2 is foundation model for promptable visual segmentation in both images and videos. It generalizes the original SAM by incorporating streaming memory architecture to handle temporal data. GCA uses SAM-2 as bridge connecting pixels and boxes, allowing us to extract object point clouds from the VG-GT output based on the boxes. Orient Anything [47]. Orient Anything is trained on rendered 3D models to estimate the 3D orientation of an object from single, free-view image. It predicts the objects azimuth, polar, and rotation angles relative to the camera. This capability is crucial for GCA to construct object-based reference frame. Note that these foundation models are not provided directly to the agent. Instead, we wrap them and offer some abstract tool interfaces as APIs for invocation (see Section C.2). C.2. Tool Interfaces The GCA agents Fcompute stage is driven by discrete set of 8 exposed tool APIs. These APIs form the agents action space, encapsulating the underlying VFMs. reconstruct. It ingests one or more images and produces comprehensive 3D reconstruction. Internally, it leverages VGGT and automatically selects the optimal reconstruction strategy. If multiple, non-static images are provided, it first consults the VLM to identify common static objects for alignment. The output includes the 3D world points, camera extrinsics, and intrinsics. detect. It detects target objects in single image based on text prompt. For capable VLMs like Qwen3-VLThinking [36], we directly instruct the VLM itself to locate the target object through prompts. Otherwise, we use GroundingDINO as the detector. It returns the bounding boxes and corresponding labels. project box to 3d points. It takes 2D bounding box and projects it into the 3D world coordinate system defined by the VGGT model output. Internally, it first uses SAM-2 to convert the bounding box into precise pixel mask, then filters the points using this mask. predict obj pose. It computes the 6-DoF semantic pose of an object, which is essential for establishing an object-based reference frame. This tool first calls project box to 3d points to find the objects 3D centroid, and then calls Orient Anything to determine its 3D orientation. It then combines these to return the final object-to-world transformation matrix. estimate scale. This tool is called when metric measurements (e.g., meters, feet) are required. It aligns MoGe-2s metric depth with the VGGT models relative depth prediction to compute single scale factor that converts the entire reconstruction into meters. ocr. It performs optical character recognition (OCR) on an image using the EasyOCR library. It returns list of recognized texts and their bounding boxes. analyze motion. It analyzes pixel-level motion between two sequential images using Farneback optical flow algorithm [7]. It is used to infer subtle camera movements that may be too small for full 3D reconstruction. code. This is the agents primary computation engine. It generates and executes Python code within sandbox environment. It tasks set of context variables (e.g., poses, points) and an natural language description (e.g., request and description of the variables) as input. The code is generated using knowledge-augmented strategy, where relevant geometric formulas are injected into the prompt, ensuring the computation is both deterministic and sound. C.3. Agentic Framework The GCA paradigm is implemented as high-throughput, modular system. The core VLM deployment and the perception tool suite are physically decoupled to ensure scalability and robustness. System Backend and State Management. The entire system is built using Ray [32] and LangGraph. LangGraph is used to define and manage the agents state and orchestrate the two-stage, graph-based reasoning flow (i.e., Fformalize followed by the Fcompute loop). Tool Suite and VFMs Deployment. The perceptual and computational tools (listed in Section C.2) are encapsulated as independent Ray Serve actors. This microservice architecture allows GCA to make concurrent perception requests (e.g., running reconstruct and detect in parallel), enabling high parallelism and automatic scaling. This entire tool suite is deployed on 2 NVIDIA A100 GPUs. VLM Roles and Deployment. In GCA, single VLM fulfills the three distinct roles within the GCA paradigm: Semantic Analyst. In the Fformalize stage, it interprets the query and visual context to generate the formal Ctask. Tool Orchestrator. In the Fcompute stage, it manages the ReAct-style tool call loop, resolves ambiguities, and generates natural language descriptions for the coder. Coder. It generates Python code for the code tool. The VLM deployment is separate from the tool suite. For open-source models (e.g., Qwen3-VL-Thinking [36]), we use vLLM [19] for efficient, high-throughput inference on 8 NVIDIA A100 GPUs. For closed-source models (e.g., Gemini-2.5-Pro [6]), we access them via their standard commercial APIs. We employ different sampling parameters based on the VLMs role. For the semantic analyst and tool orchestrator roles, which require reasoning and flexibility, we use TEMPERATURE=0.6 and TOP P=0.95. For the coder role, which demands deterministic and reliable output, we set TEMPERATURE=0.0. All roles use MAX TOKENS=32768. D. Evaluation Benchmark Details We evaluate GCA on multiple challenging spatial reasoning benchmarks. This section provides detailed description of each benchmark and its subcategories, corresponding to the results presented in Table 1 of the main paper. D.1. MMSI-Bench MMSI-Bench [55] is comprehensive benchmark designed to evaluate VLMs ability to perform spatial reasoning by integrating information from multiple, distinct images. It is organized into four distinct subcategories: PR. (Positional Relationship). This subcategory evaluates the models ability to understand the relative positions between different objects, cameras and semantic regions (e.g., kitchen) across multiple views. Attr. (Attribute). This subcategory evaluates the models ability to identify object attributes related to spatial properties, such as geometric properties (e.g., size, length) or visual characteristics (e.g., shape). Mot. (Motion). This subcategory evaluates the models ability to understand object or cameras movement. MSR (Multi-Step Reasoning). This subcategory evaluates the models ability to perform complex reasoning by chaining multiple spatial understandings described above together to arrive at final answer. D.2. MindCube-tiny MindCube-tiny is subset of the MindCube benchmark [59], which is designed to test Spatial Mental Modeling (SMM). The core task evaluates VLMs ability to construct and manipulate 3D mental model of scene using only limited set of 2D images as input. The tiny 14 version is smaller-scale version of the full benchmark. We evaluate on its three primary sub-tasks: Rot. (Rotation). This task requires the model to infer the complete environment based on partial visual information, testing its understanding of sequential views and consistent spatial cues between images (e.g., lighting). Ard. (Around). This task requires the model to infer the scene from novel viewpoint, testing its ability to interpolate and extrapolate its mental 3D model. Amg. (Among). This task requires the model to infer the 3D spatial arrangement based on four orthogonal views characterized by significant occlusion, testing its ability to establish consistency relationships across perspectives and reason about the relative positions of unseen objects. D.3. OmniSpatial OmniSpatial [18] is comprehensive benchmark designed to evaluate broad spectrum of visual-based spatial intelligence capabilities in VLMs. The full benchmark consists of four categories. We primarily focus on the two subcategories most closely related to geometric perception: Pers. (Perspective Taking). This subcategory assesses the models understanding of 3D spatial relationships by adopting varied viewpoints, e.g., egocentric, allocentric, and hypothetical perspective. Dyn. (Dynamic Reasoning). This subcategory assesses the models understanding of object motion and judgments in uncertain or rapidly changing environments. We exclude the other two subcategories: Spatial Interaction (which focuses on diagrams and user-interface, e.g., terrain map) and Complex Logic (which involves abstract spatial reasoning, e.g., puzzles), as they are more centered on abstract or symbolic reasoning rather than the highfidelity geometric perception that GCA is designed to solve. D.4. SPBench SPBench [22] is benchmark designed to evaluate both VLMs spatial reasoning in single view and multiple views: SI (Single Image). This subset contains questions that test the models understanding and reasoning capabilities within single image, including absolute distance, object size, relative distance, and relative direction. MV (Multiple Views). This subset requires the model to integrate information from multiple viewpoints to answer questions about relative position and object counts within overlapping scene. Figure 7. In Context Examples Used in Formalizing Reference Frame. The output format follows the prompt in Table 4. 2D (2D Relationship). This subcategory tests fundamental 2D spatial understanding, including 2D positional relationships and object counting. 3D (3D Relationship). This subcategory assesses the models grasp of 3D concepts, such as depth analysis and 3D distance comparisons between objects in the scene. E. Prompts Used in GCA We provide detailed prompts used in GCA, including task formalization (Table 4 and 5), tool orchestration  (Table 6)  and knowledge-augmented code generation (Table 7 and 8). Besides, we also provide the in context examples used in the reference frame formalization (see Figure 7 and Table 4). F. Qualitative Case Study We provide several qualitative case studies on how GCA effectively tackles spatial reasoning queries. These challenging cases includes unique object counting across multiple views (Figure 8), direction-based reference frame (Figure 9), object-based reference frame (Figure 10), camera rotation analysis (Figure 11), object movement analysis (Figure 12), and metric-scale estimation (Figure 13). D.5. CV-Bench G. Broader Impacts CV-Bench is visual-centric benchmark that evaluates the spatial understanding capabilities of VLMs. It is broadly composed of 2D and 3D reasoning tasks: Advancing Embodied AI Systems. Applications in robotics and AR/VR depend on an agents ability to translate ambiguous human instructions (e.g., sit on the sofa) into precise geometric actions. GCA provides robust framework for this translation, potentially facilitating the development of robots and AR/VR interfaces that can interact with the physical world with high fidelity. Trust, Interpretability, and Verification. Unlike opaque end-to-end spatial VLMs, GCAs reasoning process is highly traceable. Ctask serves as an explicit, human-readable artifact that can be verified before high-stakes action is executed. This verify-then-execute capability is critical for safety in real-world applications. Furthermore, the structured outputs from both the Fformalize and Fcompute stages can serve as valuable source of process-level supervision, acting as reliable validator to guide the training of more efficient end-to-end spatial VLM. Inheritance and Amplification of Bias. The reliance on VFMs for perception and geometry (e.g., 3D reconstruction, object orientation) creates new dependency chain for bias and failure. If these perceptual tools perform poorly on objects, scenes, or lighting conditions, GCA will not only inherit these biases but may also amplify them, leading to incorrect outcomes in real-world interactions. 16 Table 4. Prompts Used for Formalizing Reference Frame Constraint. Here, {example} is the in-context examples (see Figure 7), and {question} is the placeholder that will be replaced. [CORE MISSION] You are an expert spatial reasoning analyst. Your sole mission is to analyze users question and define the final Reference Frame. Your goal is to find the single element (an object, camera, or vector) that provides the ultimate, non-negotiable definition for absolute directions (e.g., North, South) or relative perspectives (e.g., front, left) in the final answer. Ask yourself: What element holds the final authority on what north, front, or left means in the question? [OUTPUT FORMAT] Your response MUST be single, valid JSON object: json { \"reasoning\": \"A brief, step-by-step logical deduction, explaining WHY this anchor \"formalization\": \"The precise mathematical mapping of semantic direction to one is the arbiter of direction.\", of the Solvable Geometric Primitive listed below, e.g., $-Z_cam0, +Z_toaster$.\" } [FORMALIZATION] 1. Identify the Final Arbiter of Direction - Priority 1: Absolute Direction. If an absolute direction (North, South, etc.) is explicitly tied to an element, that element is the arbiter, overriding everything else. - Priority 2: Relative Query. If no absolute direction is given, the arbiter is the object of the relative question. 2. Formalize Reference Frame Using Solvable Geometric Primitives: construct mathematical formalization of reference frame, you MUST use following three types of Solvable Geometric Primitives: - A. Camera Axes: vector from specific cameras coordinate system. Format: Xcam[i], Ycam[i], Zcam[i]. The camera coordinate system follows OpenCV convention: +Z points forward, +Y points down, and +X follow right-hand rules. - B. Object Axes: vector from specific objects semantic coordinate system. Format: X[obj], Y[obj], Z[obj]. The objects local coordinate system is defined by: +Z points its semantic front, +Y points its semantic down, +X follows right hand rules, and origin at centroid. - C. Inter-Object Vector (Direction): vector connecting the centroids of two concrete, detectable objects. Format: Centroid(B) Centroid(A). 3. Semantic Formalization - A. Object-based Reference Frame: Usually can be defined by corresponding objects axes. Examples: - when using the toaster suggests users forward is opposite the toasters semantic front, i.e., +Zref = Ztoaster. - You must choose Physical and Detectable object as the object anchor. Dont use abstract concepts like room/region/area. - B. Camera-based Reference Frame: Usually can be defined by corresponding cameras axes. Examples: - from the perspective of Figure 1 suggests reference frame is identical to camera 0s, i.e., +Zref = +Zcam0. - C. Direction-based Reference Frame - For spatial relationship between two Physical and Detectable Objects, it can be defined by inter-object vector. Examples: object is north of object suggests the direction from object to is north, i.e., +Zref = BA = Centroid(A) Centroid(B) = North - For spatial relationship between two Abstract Concepts, you must use physic objects axes or cameras axes as the proxy to tie this direction. Examples: moves from room to room B, facing north.Assume this motion aligns with moving from background towards the foreground, formalized as +Zref = Zcam[i] = North. [EXAMPLES] {examples} [QUESTION] {question} Now, please analyze the above question and provide your response in the specified JSON format. 17 Table 5. Prompts Used for Formalizing Objective Constraint. Here, {question} is the placeholder that will be replaced. [CORE MISSION] You are an expert spatial reasoning analyst. Your sole mission is to analyze users question and define the final Objective. Your goal is to rephrase the users natural-language question into single and precise sentence. This sentence describes the specific value or piece of information that definitively answer the question. Ask yourself: What is the single, final piece of information (e.g., scalar value, 3D vector, sequence of rotations) that the user is finding? [OUTPUT FORMAT] Your response MUST be single, valid JSON object: json { \"reasoning\": \"A brief, step-by-step logical deduction that breaks down the users question into its final objective.\", \"formalization\": \"A single, concise sentence that defines the ultimate goal of the question, stated in technical terms.\" } [OBJECTIVE] - Identify the target variable: What type of answer is being sought? Is it distance, speed, an orientation, direction, count, relationship, or sequence of actions? - Identify the Key Entities: What are the specific, concrete objects, cameras, or locations involved in the question? - Synthesize the Objective: Combine the target variable and entities into single, unambiguous sentence. This sentence must be statement or noun phrase, not question. Example: - Bad: Which way the object are going? - Good: The 3D direction vector of the object movement. [QUESTION] {question} Now, please analyze the above question and provide your response in the specified JSON format. 18 Table 6. Prompts Used for Tool Orchestration. Here, {api documents} is the detailed documentation of provided APIs. {history} includes the initial user question, task formalization, previous planning and corresponding execution results. [CORE MISSION] You are an expert spatial intelligence agent. Your mission is to generate sequence of tool calls that rigorously computes the answer. It follows an iterative Plan Update cycle, using workspace as your computational memory. - Plan: Decide the next tool calls based on the goal and current workspace. - Update: Tool results are saved as new variables in the workspace. - Repeat: Continue until the workspace contains enough information to conclude the final answer. [AVAILABLE APIS] {api documents} [A TYPICAL WORKFLOW] 1. Strictly Follow the Task Formalization - Task Formalization: The input question is pre-formalized and consists of two parts: Reference Frame Constraint and Objective Constraint. You MUST strictly follow this formalization to solve the question. - Reference Frame: Reference frame is the only one coordinate system that matters for interpreting the final answer (left/right, north/south, etc.). The formalization is the equation you must solve with the specified geometric tools. E.g., +Zref = Ztoaster indicates reference frame is defined by object toasters local frame, so we MUST perform all calculations in toasters frame. - Objective: Objective is the ultimate goal of the question. You must calculate this objective within the reference frame. 2. Acquire Geometric Data: Based on users question and pre-defined task formalization, plan the necessary tool calls to gather all data required for the final calculation. This involves two parallel goals: - A. Solve for the Reference Frame: The formalization mathematically defines the World-to-Reference Transformation. To solve this formalization, your plan MUST gather all the geometric ingredients in the world frame. - reconstruct tool provides the 3D reconstruction context in unified world frame, bridging the gap between input 2D images and geometric perception. - If formalization involves an objects axes (e.g., +Zref = Ztoaster), call predict obj pose to solve that objects local frame. The resulting obj2world is required to establish the reference frame. - If formalization involves cameras axes (e.g., +Zref = Zcam0), you must acquire reconstruction context (include extrinsic matrix) to implement the formalization. - If formalization involves vector between two objects (e.g., Centroid(B) Centroid(A) = North), your plan MUST include calls to project box to 3d points for both object and B. - B. Solve for the Objective: Follow the objective to identify the target data that need to be analyzed within the reference frame. 3. Perform Final Calculation in Reference Frame: Once all required variables are available in the workspace, call code. 4. Conclusion: Conclude the final answer using generate final answer. [OUTPUT FORMAT] Your response MUST be single, valid JSON object: json { \"analysis\": \"Briefly analyze how you will implement the formalization and what target data is need. State the immediate next tool(s) you will call\", \"tool_calls\": [ { \"api\": \"API name\", \"args\": {...}, \"output_variable\": \"A unique name for output, stored in the workspace\" }, ... ] } [HISTORY] Here is the history so far: {history} Please analyze current situation and history messages, and then generate your response. Your plan MUST only includes the immediate next one step. 19 Table 7. Prompts Used for Coder. Here, {question}, {formalization}, {objective} and {var docs} are the placeholder that will be replaced. {knowledge} includes set of releveant, fixed formulas based on the type of input variables. [CORE MISSION] You are an expert Python programmer. Your goal is to write single Python function that correctly implements the computational objective based on the provided context and documentation. [Users Question] This provides the high-level context for your task. {question} [Reference Frame] All geometric data are defined in the world frame (defined by camera 0) unless specified. All final interpretations MUST be expressed in the reference frame. The reference frame is defined: {formalization} [Objective] The ultimate goal from the high-level question. You MUST write code to calculate this objective to answer the question. {objective} [Documentation of Available Variables] {var docs} [Additional Knowledge] This information is always true for the environment your code runs in. {knowledge} [Available Libraries] You can use numpy, torch, scipy, math, and other standard Python libraries. [Critical Rules and Output Format] 1. Synthesize and Self-Correct: Your primary duty is to write correct code. Use the objective as your goal, but critically verify and implement the logic using the provided documentation. 2. Handle Multiple-Choice Questions: If the users question is multiple-choice, your code MUST systematically evaluate the conditions for every option (e.g., A, B, C, D). Besides, the logic of your code should focus ONLY on the given options. 3. DO NOT add any explanation in your final output. Your output MUST follow this format: python def execute(func_signature): # Your code here ... return serializable_value # return value MUST be serializable type 20 Table 8. Knowledge and Formulas Used in Knowledge-Augmented Code Generation. We will inject the relevant knowledge based on the type of input variable. [Output of reconstruct] Extrinsic Transformation (World Camera): - World Camera: To transform world point world into camera ss frame, use its extrinsic matrix = extrinsic[s]. The formula is: cam homo = world homo @ s.T. - Camera World: The pose of camera in the world, Pose s, is the inverse of its extrinsic matrix: Pose = np.linalg.inv(extrinsic[s]). To transform point from camera ss local frame to the world frame, use the formula: world homo = cam homo @ Pose s.T. - Relative Rotation Analysis (Camera Camera): To describe the rotation of camera js pose relative to camera is pose, use the camera poses in the world frame (Pose i, Pose j). The relative rotation from to is: rel = pose @ pose.T, which simplifies to rel = (R j.T) @ (R i.T).T = j.T @ i. [Output of predict obj pose] Object Pose Transformation (World Object): - Object World: The obj2world matrix (aliased as Pose obj) transforms points from the objects local frame to the world frame using the formula: world homo = local homo @ Pose obj.T. - World Object: To transform world into the objects local frame, use the inverse matrix: world to obj = np.linalg.inv(Pose obj). The formula is: local homo = world homo @ world to obj.T. [Output of reconstruct, predict obj pose] Interpreting Rotation: - General Rotation Direction: For most questions about rotation direction, convert the relative rotation matrix to rotation vector [rx, ry, rz] via scipy.spatial.transform.Rotation.from matrix(R).as rotvec(). - The component with the largest absolute value indicates the primary axis of rotation. - Based on the OpenCV coordinate system (+X right, +Y down, +Z forward) and the right-hand rule: ry > 0 corresponds to pan to the right, rx > 0 corresponds to tilt upward, rz > 0 corresponds to clockwise roll. - Sequential Rotations: Use scipy.spatial.transform.Rotation.from matrix(R).as euler(order) to compute sequential rotation. Verify the signs of the resulting angles. It must match the options description. [If formalization includes cardinal direction] 1. Identify the Cardinal Anchor Axis from the formalization. - The formalization string (e.g., +Zref = Zobj = South) links one of your reference axes to cardinal direction. - Parse this string to find the anchor. In the example, the anchor is South, and it corresponds to the Zref axis. This defines your first cardinal vector: South axis = Zref axis. 2. Derive the Complete Set of Cardinal Axes: You must find the remaining cardinal axes via applying cross product: - If South axis is known, starting with West axis = np.cross(Y ref axis, South axis). - If West axis is known, starting with North axis = np.cross(Y ref axis, West axis). - If North axis is known, starting with East axis = np.cross(Y ref axis, North axis). - If East axis is known, starting with South axis = np.cross(Y ref axis, East axis). 3. Project and Determine the Final Quadrant: Project the target vector onto the primary horizontal cardinal axes (North and East). - projection north = np.dot(disp vec world, cardinal map[\"N\"]) - projection east = np.dot(disp vec world, cardinal map[\"E\"]) - Use the signs of these projections to determine the final answer. [Output of detect] Bounding Box Format: All bounding box in input variable is provided in the [x1, y1, x2, y2] format. 21 Figure 8. Case Study #1. Unique object counting across multiple views. 22 Figure 9. Case Study #2. Direction-based reference frame. 23 Figure 10. Case Study #3. Object-based reference frame 24 Figure 11. Case Study #4. Camera rotation analysis. 25 Figure 12. Case Study #5. Object movement analysis. 26 Figure 13. Case Study #6. Metric-scale estimation."
        }
    ],
    "affiliations": [
        "School of Software, Beihang University",
        "Shanghai AI Laboratory",
        "Shanghai Innovation Institute",
        "Shanghai Jiao Tong University",
        "ZIP Lab, Zhejiang University"
    ]
}