{
    "paper_title": "ShadowDraw: From Any Object to Shadow-Drawing Compositional Art",
    "authors": [
        "Rundong Luo",
        "Noah Snavely",
        "Wei-Chiu Ma"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce ShadowDraw, a framework that transforms ordinary 3D objects into shadow-drawing compositional art. Given a 3D object, our system predicts scene parameters, including object pose and lighting, together with a partial line drawing, such that the cast shadow completes the drawing into a recognizable image. To this end, we optimize scene configurations to reveal meaningful shadows, employ shadow strokes to guide line drawing generation, and adopt automatic evaluation to enforce shadow-drawing coherence and visual quality. Experiments show that ShadowDraw produces compelling results across diverse inputs, from real-world scans and curated datasets to generative assets, and naturally extends to multi-object scenes, animations, and physical deployments. Our work provides a practical pipeline for creating shadow-drawing art and broadens the design space of computational visual art, bridging the gap between algorithmic design and artistic storytelling. Check out our project page https://red-fairy.github.io/ShadowDraw/ for more results and an end-to-end real-world demonstration of our pipeline!"
        },
        {
            "title": "Start",
            "content": "ShadowDraw: From Any Object to ShadowDrawing Compositional Art Rundong Luo Noah Snavely Wei-Chiu Ma"
        },
        {
            "title": "Cornell University",
            "content": "5 2 0 2 4 ] . [ 1 0 1 1 5 0 . 2 1 5 2 : r Figure 1. Generating shadowdrawing compositional art. Given an arbitrary 3D object, our framework jointly predicts scene parameters, including object pose and lighting, and generates partial line drawing, such that the cast shadow seamlessly completes the drawing into coherent image. The system unites physical shadows with generative drawing, creating compelling compositions from cast shadows that provide only minimal structural cues. Our approach enables straightforward real-world deployment, as demonstrated with physical prototypes of letters C, V, P, R. Best viewed in Adobe Acrobat Reader for the embedded animation."
        },
        {
            "title": "Abstract",
            "content": "We introduce SHADOWDRAW, framework that transforms ordinary 3D objects into shadowdrawing compositional art. Given 3D object, our system predicts scene parameters including object pose and lightingtogether with partial line drawing, such that the cast shadow completes the drawing into recognizable image. To this end, we optimize scene configurations to reveal meaningful shadows, employ shadow contours to guide line drawing generation, and adopt automatic evaluation to enforce shadow-drawing coherence and visual quality. Experiments show that SHADOWDRAW produces compelling results across diverse inputs, from real-world scans and curated datasets to generative assets, and naturally extends to multi-object scenes, animations, and physical deployments. Our work provides practical pipeline for creating shadowdrawing art and broadens the design space of computational visual art, bridging the gap between algorithmic design and artistic storytelling. Check out our project page for more results and an end-toend real-world demonstration of our pipeline! 1. Introduction Shadows have long captivated artists and audiences alike, serving as powerful medium of expression across cultures and epochs. From traditional Chinese shadow theater, where cut-out puppets project intricate silhouettes, to contemporary shadow photography and installation art that manipulate light to craft evocative narratives (Fig. 2(a)(iii)), shadows transform absence into striking imagery. Defined by the shifting interplay of light and form, they are inherently ephemeral and visually expressive, embodying the delicate relationship between illumination, object, and perception. Recent research in computational visual art has sought to formalize and extend shadow art practices. By framing the problem as an inverse design task, prior works optimize object geometry [32, 36], material properties [2, 31], and lighting [33, 39] to achieve desired visual effects (Fig. 2(a)(iii)). While effective, these approaches assume predefined visual target (i.e., knowing priori what to generate) and rely on parameter optimization to reproduce it. In this paper, we are instead interested in exploring the interplay between physical shadows and generative drawing. Our work is directly inspired by Belgian artist Vincent Bal [1], whose playful works reveal how the cast shadows of everyday objects can seamlessly complete drawn elements. Motivated by this idea, we aim to develop computational framework that captures the same sense of serendipity: unified compositions where shadow and line drawing contribute to cohesive whole, without relying on predefined target. Formally, given 3D object as input, we aim to predict both scene parameters (light direction and object pose) and partial line drawing such that, when the scene is illuminated, the shadow cast by the object completes the drawing into coherent, recognizable image (Fig. 2(b)). However, achieving such compositions is extremely challenging. First, we do not know priori what subject to generate, yet generative models typically require detailed prompts to produce high-quality results. Second, effective image conditioning is essential, but rendered inputs such as the shadow image or objectshadow composite provides weak structural cues provides weak structural cues, often yielding generated compositions in which the shadow contributes little. This difficulty is further compounded by the scarcity of shadowdrawing examples, leaving only limited data for training. To address these challenges, we reformulate the problem around the shadow contourthe boundary contour of the shadow. Although raw shadow and its contour encode the same geometry, we empirically find that reducing the shadow to clean binary outline provides stronger conditioning, yielding tighter alignment between the shadow and drawing. The shadow contour also naturally supports scalable data construction, as closed contours can be efficiently extracted from line drawings, and further enables the use of pretrained edge-conditioned generative models for line drawing generation. Building on this idea, we first train line drawing generator conditioned jointly on text and the shadow contour. Then we search over scene parameters by differentiable rendering to discover semantically interesting shadows and leverage VLMs to create detailed descriptions of the intended drawing from the shadow contours, guided by carefully designed in-context examples. Finally, we design automatic metrics to verify shadow-drawing coherence and visual quality, retaining only those compositions where the shadow meaningfully contributes. Extensive experiments demonstrate the effectiveness of our framework in generating coherent and visually compelling shadowdrawing compositions. Our approach generalizes across diverse object models, including 3D datasets, real-world scans, and generated assets  (Fig. 1)  , and naturally extends to multi-object scenes, animated settings, and physical deployments  (Fig. 6)  . Notably, the required physical setup is simple: an everyday object (whose 3D model can be easily scanned using mobile app such as PolyCam), planar surface, and spotlight are sufficient to reproduce our computational designs. This accessibility broadens the expressive range of shadow art and significantly lowers the barrier for exploring this emerging form of visual storytelling. 2. Related Work Computational Visual Art. Computational visual art explores artistic creations whose perceptual impact depends on 3D geometry, material properties, and controlled illumination. Works span sculpture, architecture, and fabricated objects, often exploiting optical phenomena such as 2 Figure 2. From traditional shadow art to shadowdrawing compositional art. (a) Traditional shadow art, such as artist-crafted works and computational art designs, treats the objects and their cast shadows as the sole medium. (b) Our framework integrates shadows with line drawings: given 3D object, we generate partial drawing and estimate scene parameters (e.g., object pose and light position) such that the cast shadow completes the composition. shadows, reflections, or refractions to produce striking imagery [42]. Traditionally, the creation process relies heavily on an artists intuition and iterative trial-and-error, making it labor-intensive and technically demanding. Computational approaches aim to assist this process by formulating it as an inverse problem: given target visual effect, optimize scene parameterse.g., object pose, geometry, and light configurationso that the rendered appearance matches the design intent. Techniques range from combinatorial optimization for occluder placement [2, 31] to differentiable rendering for shape refinement [36, 41]. Applications span reflection-based art [39], volumetric displays [17], and multiview illusions [10]. Unlike these approaches, which optimize scene parameters to achieve given target visual effect, our system jointly estimates both the scene parameters and the target subject, making the task much more challenging. Line Drawing Generation. Line drawings have long been studied as compact yet expressive representation of shape and semantics. 3D-based approaches derive drawings directly from geometry, extracting contours, depth cues, or neural features to approximate sketches [7, 14, 20, 26]. In parallel, image-based methods framed line drawing as supervised translation task, mapping photographs to vector strokes or contours using paired [22, 24] or unpaired data [6]. Beyond image-to-drawing, recent research has explored diverse tasks, including text-guided generation [11], sketch animation [3, 19], object sketching [23, 27], portrait rendering [46], line drawing completion [4, 28], and sequential or collaborative creation [37]. Despite these advances, prior studies primarily regard line drawing as an isolated modality, whereas our work explicitly connects it with cast shadows to create hybrid visual compositions. Shadow Art. Shadow art is subclass of computational visual art where the shadow cast by physical object under controlled light source is used for artistic expression. Early computational methods focused on single-light, binaryshadow designs, deforming object geometry so that the resulting shadow matched the desired shape [18, 32]. With the advent of differentiable rendering, object shapes can be directly optimized with respect to the shadow image under more sophisticated setups [34, 36]. However, these approaches may produce irregular or impractical geometries, limiting real-world applicability. Recent methods broaden the design space to include multi-layer occluder systems [31] and color shadow projections using translucent materials [2]. Beyond rigid objects, shadow art has been explored using human bodies [40] and hand gestures [44]. In contrast to prior work that treats shadows as the sole visual medium, we explore the interplay between such physical effects in 3D space and generative models operating in the pixel domain. Given candidate cast shadow, which may offer only the barest suggestion of the underlying object, the system must imagine how to complete it into an interesting composition. 3. Generative Shadow-Drawing Art We explore an art form that unifies shadow and line drawing into single, coherent composition. The input is 3D object model, and the outputs are: (i) partial line drawing, and (ii) scene parameters specifying the 3D position and direction of light source and the pose of the object. These elements can be jointly arranged so that, when illuminated, the cast shadow completes the line drawing, producing recognizable image on the projection surface. In our setup, the canvas lies on the ground plane, and the light source is modeled as spotlight to produce sharp, coherent shadows. The light maintains fixed distance from the canvas center, yielding two degrees of freedom: elevation and azimuth. The object can rotate about its vertical axis and translate along two axes on the ground. We normalize the objects longest dimension to standardize shadow size; in physical deployments, the canvas can be inversely scaled to preserve the ratio. The line drawing is rendered in black, and the shadow is represented as gray silhouette, with the final composition emerging from their precise spatial alignment. For simplicity, we restrict our study to singlelight configurations and omit surface textures. Overview. Our task is highly under-constrained: single 3D object admits countless combinations of light, object, and line drawing arrangements, and traditional practice relies on artistic intuition to achieve compelling results. To make the problem tractable, we first consider simplified setting where the object pose and light direction are fixed, aiming to generate line drawing that seamlessly integrates with the shadow. We then extend to optimizing scene parameters to produce visually distinctive and semantically meaningful shadows, accompanied by textual descriptions of the 3 Figure 3. Framework overview. Given 3D object, we first optimize scene parameters specifying the object pose and light configuration. From the rendered shadows, we derive text prompts with VLM and extract shadow contours, which together condition the line drawing generator. The generated drawings are then filtered using VQA-based coherence check and ranked by semantic and quality metrics. The final output is partial line drawing along with scene parameters that, when rendered, form coherent shadowdrawing composition. intended subject. Finally, we summarize the compositions by selecting small set of high-quality examples. Using this dataset, we train latent flow-based model ϵθ with the standard score-matching objective: 3.1. From Shadow Contour to Shadow-Drawing Art We begin with simplified setting where the scene parameters are fixed, the subject description is given, and the goal is to generate line drawing that integrates seamlessly with the rendered shadow. straightforward baseline is to train an imageconditioned generative model on rendered inputs such as the shadow image or the objectshadow composite. However, this approach faces two challenges: weak conditioning signals, which limit the models ability to align the drawing with the shadow, and data scarcity, as only few dozen shadowdrawing examples exist online. To address these issues, we replace the raw shadow with its 2D boundary contour, referred to as the shadow contour, as the conditioning input. Although raw shadow and its contour encode the same geometry, reducing it to clean binary outline provides stronger conditioning: models trained on grayscale shadows often drift from the intended geometry, whereas contour-based conditioning achieves tighter alignment. Beyond being strong geometric cue, reformulating the task as transforming closed contours into line drawings yields two benefits: (i) it enables the use of well-established edge-conditioned generative models, and (ii) it allows scalable data synthesis, since shadow-like contours can be efficiently extracted from generic line drawings. As we will demonstrate in the experiments, this shadow contour design significantly improves generation quality. We first describe how we curate data for training (see supplementary 2.1 for details). First, we generate set of line drawings using GPT-4o and retain only those containing regions bounded by strokes. We then train FLUX-1-dev LoRA [21] on this filtered set and use it to synthesize an additional 10K line drawings from GPT-4o-generated prompts about everyday subjects. Closed contours extracted from these drawings serve as shadow contour conditions. Ex0,ϵ,ci,ct,t ω(t) (ϵθ(xt, ci, ct, t) ϵ)2 , (1) min θ where x0 is the target line-drawing latent, is the timestep, xt is the noisy sample at t, ci is the shadow contour condition, and ct is the text prompt. We use FLUX.1-Canny [21] as the base model and train LoRA adapter [29] on top. At inference time, we render the shadow given the scene parameters, extract its boundary, and condition the model jointly on this contour and the text description. To prevent strokes from overlapping the object, we treat generation as an outpainting problem [30] with binary object mask m, preserving masked regions during denoising: xt = xmask ( + (1 m) ˆxt, αtxmask 0 , (1 αt)I), xmask (2) (3) where xmask prediction at timestep t. 0 is the latent of the mask and ˆxt is the model Finally, we erase input shadow contour from the generated drawing, reinsert the 3D object, and render the composition in which the cast shadow completes the drawing. 3.2. Scene Configuration Selection With the line drawing generation model in place, we relax the fixed-setting assumption and explore how to discover scene configurations that yield diverse and semantically meaningful shadows. For each candidate configuration, we create text prompt describing the intended subject, which conditions the subsequent line drawing generation. Scene Parameter Optimization. We consider five scene parameters: light azimuth θ, elevation ϕ, object center position in polar coordinates (r, γ), and the object rotation about its vertical axis α. To ensure the shadow extends toward the canvas center, we set γ = θ and = 0.8 the canvas radius, leaving three degrees of freedom. 4 We quantify shadow quality using fractal dimension (FD) [9], which measures contour complexity via multigranularity box counting; higher FD indicates more irregular, visually rich shapes. To allow gradient-based optimization, we use differentiable approximation of FD: = FD(S), = Renderer(θ, ϕ, r, γ, α), (4) where denotes the rendered binary shadow, obtained via PyTorch3D differentiable silhouette rendering. Empirically, we initialize the search with 48 configurations, spanning 12 azimuths in 30 increments and 4 elevations to vary shadow length, each combined with random vertical object rotation to diversify shapes. For each initialization, parameter updates are restricted to its local neighborhood to prevent optimization space overlap of different initialization and ensure scene parameters remain physically plausible and easily reproducible in real-world setups. Visual Prompt Proposal. Detailed prompts are known to substantially improve text-to-image generation [15], whereas generic descriptions such as man or bird are often too vague to produce coherent shadowdrawing compositions. Unlike prior computational art methods that rely on small set of hand-crafted prompts [10, 12], our goal is to support arbitrary inputs while adapting to diverse shadow geometries. This calls for an automated pipeline that generates scenespecific prompts directly from the shadow itself. To achieve this, we employ visionlanguage models. The model is instructed to imagine line drawing in which the given shadow contour naturally serves as key structural element, and to produce detailed description of that drawing. For flexible control, users may specify the desired subject by modifying the prompt accordingly. To encourage reasoning about the strokes geometry and to maintain both semantic and visual coherence, we adopt chain-of-thoughtstyle prompting template, with the complete system prompt provided in the supplementary 2.2. 3.3. Evaluation and Ranking Having generated compositions across the proposed scene configurations, natural question arises: which results are worth keeping? Not all samples are reliablesome exhibit poor alignment between the drawing and shadow, while others contain shadows that contribute little to the final image. To retain only strong outputs, we apply systematic filtering process along three dimensions: (i) shadowdrawing coherence, (ii) shadows contribution, and (iii) visual quality. These criteria ensure that the selected compositions are visually compelling and structurally coherent. Additional visual examples are provided in the supplementary 9. Figure 4. Examples of objects for evaluation. the Visual Prompt Proposal stage, the VLM is instructed to specify the intended role of the shadow contour in the composition (e.g., the body of fish). We then overlay the shadow contour in red onto the generated line drawing and query another VLM with yes/no question: Does the highlighted stroke outline the described component? Candidates receiving no response are discarded. Shadow Contribution Assessment. We further evaluate whether the shadow meaningfully enhances the composition. Specifically, we compare the complete line drawing (full) with variant where the shadow contour is removed (partial), evaluating both with CLIP similarity [35] and humanpreference-aligned metrics, including ImageReward [45] and Human Preference Score (HPS) [43]. If the partial version attains higher ImageReward or HPS score, the composition is discarded, indicating that the shadow fails to improve the overall generation quality. Ranking. For the remaining candidates, we compute an improvement score for each metric: CLIP = CLIP2 full/CLIP2 IR = Φ(IRfull)2 Φ(IRpartial)2, full HPS2 HPS = HPS2 partial, partial, (5) (6) (7) where Φ() denotes the CDF of the standard Gaussian, as ImageReward scores are normalized. The overall ranking score is defined as: = CLIP IR HPS, (8) and top-K compositions by are selected as final outputs. 4. Experiments In this section, we first detail our experimental setup, then present quantitative comparisons with baselines and ablation variants, followed by qualitative results across diverse 3D assets. Finally, we demonstrate downstream applications that our framework enables out of the box. 4.1. Experimental Setup Shadow-drawing Coherence Verification. We adopt VQA-based verification strategy [25] to measure the coherence between the given stroke and the line drawing. During Baselines. Since no existing method explicitly targets shadowdrawing compositional art, we construct two baselines using state-of-the-art image generation models. The 5 Figure 5. Qualitative baseline comparisons. Large models like Gemini fail to capture the subtle notion of shadowdrawing art and often produce outputs where the shadow contributes little, whereas our method yields coherent shadowdrawing compositions of better quality. Method CLIP Conceal Human Preference (%) Gemini (object-shadow) Gemini (shadow contour) Ours 31.28 31.65 32.41 -0.2840 0.2421 3.0059 3.6% 6.0% 70.4% Table 1. Comparison with the baselines. first, Gemini (objectshadow), employs Gemini Flash 2.5 Image [13] to generate the composition conditioned on both the objectshadow composite and the text prompt produced by our approach. The second, Gemini (shadow contour), replaces the object-shadow composite with the shadow contour image, providing more precise geometric guidance. Details of baseline execution are provided in supplementary material 2.3. To analyze the contribution of each framework component, we further conduct ablations by (i) training the line drawing model on artist-sourced images conditioned on the objectshadow composite, (ii) training the same model conditioned on shadow contour, and (iii) initializing scene parameters randomly without optimization. Data. We collect 200 object models from diverse sources to evaluate our framework (examples in Fig. 4). Specifically, the dataset includes 26 alphabet models (AZ), 20 from the YCB robotics dataset [5], 87 objects from distinct categories in Objaverse-LVIS [8], and 30 character models from Objaverse. To demonstrate robustness to imperfect geometry, we further include 20 real-world household objects scanned via Polycam and 17 synthetic assets generated by MeshLRM [38]. For training ablation baselines without our synthetic data, we collect 71 images from the artists YouTube channel [1], manually filtered and processed to extract the object, shadow, and line-drawing components. Metrics. For quantitative evaluation, we compare our method against the baselines among the top-4 ranked outputs. Specifically, we report the average CLIP score [16], ImageReward [45], and Human Preference Score [43] of the generated compositions. We further measure concealment [12], defined as the CLIP score difference between the complete line drawing and the version with the shadow contour removed. Additionally, we conduct two user studies. In the first, 10 participants compare the top-1 outputs from our method and the two baselines, selecting the preferred result or indicating no preference. In the second, 8 participants evaluate the success rate of our framework by selecting any number of satisfactory results among the top-4 images produced by our evaluation and ranking pipeline. 4.2. Results and Analyses Baseline Comparisons. Tab. 1 compares our framework with the baselines. In the Gemini (objectshadow) setting, the composite image provides weak structural cues, insufficient for forming coherent shadowdrawing compositions. The shadow contour representation offers stronger geometric guidance yet remains limited, as pretrained image generators still struggle to capture the nuanced interplay between shadow and drawing. Consequently, both baselines yield low concealment scores, suggesting that the shadow contributes little or even negatively to the final composition. User studies align with these findings: participants favored our method in 70.4% of cases, with 20.1% marked as neither preferable. Our approach also achieves much higher concealment scores, confirming that the shadow serves as crucial structural element rather than redundant component. Qualitative results in Fig. 5 further highlight our higher visual quality. Moreover, 96.8% of the top-4 results generated by our framework include at least one rated satisfactory by users, validating its overall effectiveness. Ablation Studies. We conduct ablation studies to evaluate the contributions of three key components: the proposed shadow contour condition, the synthetic training dataset, and scene parameter optimization. As shown in Tab. 2, each component yields clear gains. Replacing objectshadow conditioning with shadow contour notably improves both generation quality and concealment; substituting the limited artist-sourced data with our large-scale synthetic dataset further boosts all metrics; and incorporating scene parameter optimization delivers the strongest overall performance. Qualitative comparisons in Fig. 7 illustrate these effects. We further justify the effectiveness of our evaluation algorithm with user studies. See analyses in supplementary 2.4. 4.3. Applications We demonstrate the versatility of our framework through four applications: (i) generating diverse shadow-drawing art from single object, (ii) creating shadow-drawing art from multiple input objects, (iii) extending to animated objects, and (iv) deploying in real-world setups. 6 Figure 6. Gallery of shadow-drawing art generation results. (a) Single-object generation. (b) Multi-object compositions. (c) Diverse results from the same object by varying light, pose, and line drawing. (d) Animated shadowdrawing art, where the shadow evolves with the motion of the object to complete the composition. More results on our project page in the supplementary material. 7 Method Condition type Training data Scene param optim. CLIP Conceal IR Ablation 1 Ablation 2 Ablation object-shadow shadow contour shadow contour artist source artist source synthetic Ours shadow contour synthetic 31.04 31.38 32.08 32.41 0.225 2.215 2.606 3.006 -0.0720 0.1552 0. 0.4441 HPS 0.2244 0.2269 0.2294 0.2373 Table 2. Quantitative ablation studies. The proposed shadow contour conditioning, synthetic training data, and scene parameter optimization all improve the overall generation quality. IR and HPS stands for the ImageReward [45] and Human Preference Score [43]. Figure 7. Qualitative ablation studies. While ablated variants often yield results where the shadow contributes little, our method produces compositions with better visual quality and stronger shadowdrawing coherence. One Object, Diverse Results. Our framework by design generates multiple shadowdrawing compositions from single object. By varying the light direction, object pose, and line drawing, we obtain set of artworks that highlight different structural aspects of the same object. This demonstrates how single physical object can serve as the basis for broad range of artistic expressions. Qualitative examples are shown in Fig. 6(c). As discussed in Sec. 3.2, our framework also supports user-specified subjects through prompt control; see supplementary 2.5 for details. Multi-object Compositions. Our framework naturally extends to scenes involving multiple objects. For each candidate configuration, we independently sample self-rotation angles, arrange the objects vertically, and release them in Blenders physics simulation to obtain stable stacked layout. Once equilibrium is reached, the configuration is treated as single composite object, allowing the rest of the pipeline to be applied directly. This enables more elaborate results where different objects contribute complementary shadow structures. See Fig. 6(b) for examples. Animated Shadow-drawing Art. Our framework also supports animated objects without extra training. For each configuration, we render five key frames and overlay their shadow contours into single image, using distinct colors to denote frames. This composite is then fed to the VLM to generate the corresponding prompt. As in the static-object setting, we apply binary mask to restrict stroke placement, defined as the intersection of all shadow regions and their neighborhoods, to avoid strokes in dynamically changing areas (details in supplementary 2.6). We evaluate this pipeline on animated objects from Objaverse, with qualitative results in Fig. 6(d) demonstrating the ability of our system to handle temporally varying scenes. Real-world Deployment. Our framework can be readily reproduced in physical settings without any specialized equipment, requiring only an object and single spotlight. In practice, everyday household items combined with phone flashlight suffice to create compelling shadowdrawing compositions. This accessibility positions our approach as practical tool for artists and hobbyists, significantly lowering the barrier to exploring computational shadow art. Fig. 1 presents physical prototypes of the letters C, V, P, R, with complete end-to-end demonstration provided on the project page in the supplementary material. 5. Conclusion We introduce SHADOWDRAW, framework for creating unified shadowdrawing compositional art from arbitrary 3D objects. Our approach optimizes scene parameters to discover semantically meaningful configurations, employs shadow contours to guide line drawing generation, and incorporates automatic evaluation and ranking to ensure shadow-drawing coherence and visual quality. Experiments show strong results across diverse 3D assets, with natural extensions to multi-object scenes, animations, and real-world setups. By broadening the design space of computational visual art, our work opens new avenues for accessible, democratized creation of shadow-based art. Limitations. While our framework consistently generates compelling results across diverse objects, certain challenges remain. Some objects naturally produce uninformative shadows, making it hard to form high-quality compositions. The diffusion inference adds runtime overhead, and although automated ranking surfaces strong candidates, occasional human judgment is still needed to find the best outcome. We provide more discussions in the supplementary 3."
        },
        {
            "title": "Acknowledgment",
            "content": "The research is partially supported by gift from Ai2, NVIDIA Academic Grant, and DARPA TIAMAT program No. HR00112490422. Its contents are solely the responsibility of the authors and do not necessarily represent the official views of DARPA."
        },
        {
            "title": "References",
            "content": "[1] Vincent Bal. Vincent Bal (YouTube channel), 2025. 2, 6 [2] Ilya Baran, Philippe Keller, Derek Bradley, Stelian Coros, Wojciech Jarosz, Derek Nowrouzezahrai, Bernd Bickel, and Markus Gross. Manufacturing layered attenuators for multiple prescribed shadow images. Computer Graphics Forum, 2012. 2, 3 [3] Gedas Bertasius, Christoph Feichtenhofer, Du Tran, Jianbo Shi, and Lorenzo Torresani. Learning temporal pose estimation from sparsely-labeled videos. In NeurIPS, 2019. 3 [4] Ankan Kumar Bhunia, Salman Khan, Hisham Cholakkal, Rao Muhammad Anwer, Fahad Shahbaz Khan, Jorma Laaksonen, and Michael Felsberg. Doodleformer: Creative sketch drawing with transformers. In ECCV, 2022. 3 [5] Berk Calli, Arjun Singh, James Bruce, Aaron Walsman, Kurt Konolige, Siddhartha Srinivasa, Pieter Abbeel, and Aaron Dollar. Yale-cmu-berkeley dataset for robotic manipulation research. IJR, 2017. [6] Caroline Chan, Fredo Durand, and Phillip Isola. Learning to generate line drawings that convey geometry and semantics. In CVPR, 2022. 3 [7] Doug DeCarlo, Adam Finkelstein, Rusinkiewicz, and Anthony Santella. tive contours for conveying shape. 2003. 3 Szymon SuggesIn SIGGRAPH, [8] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3d objects. In CVPR, 2023. 6 [9] Kenneth Falconer. Fractal geometry: mathematical foundations and applications. John Wiley & Sons, 2013. 5 [10] Yue Feng, Vaibhav Sanjay, Spencer Lutz, Badour IlluAlBahar, Songwei Ge, and Jia-Bin Huang. sion3d: 3d multiview illusion with 2d diffusion priors. arXiv:2412.09625, 2024. 3, [11] Kevin Frans, Lisa Soros, and Olaf Witkowski. Clipdraw: Exploring text-to-drawing synthesis through language-image encoders. In NeurIPS, 2022. 3 [12] Daniel Geng, Inbum Park, and Andrew Owens. Visual anagrams: Generating multi-view optical illusions with diffusion models. In CVPR, 2024. 5, 6 [13] Google. Gemini flash, 2025. 6, 2 [14] Yulia Gryaditskaya, Felix Hahnlein, Chenxi Liu, Alla Sheffer, and Adrien Bousseau. Lifting freehand concept sketches into 3d. ACM TOG, 2020. 3 [15] Yaru Hao, Zewen Chi, Li Dong, and Furu Wei. Optimizing prompts for text-to-image generation. In NeurIPS, 2023. 5 [16] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. arXiv:2104.08718, 2021. 6 [17] Ryoichi Hirayama, Hideki Nakayama, Atsushi Shiraki, Takashi Kakue, Tomoyoshi Shimobaba, and Tomoyoshi Ito. Projection of multiple directional images on volume structure with refractive surfaces. Optics Express, 2019. [18] Kuan-Wei Hsiao, Jia-Bin Huang, and Hung-Kuo Chu. Multi-view wire art. ACM TOG, 2018. 3 [19] Yu-Gang Jiang, Qi Dai, Xiangyang Xue, Wei Liu, and Chong-Wah Ngo. Trajectory-based modeling of human actions with motion reference points. In ECCV, 2012. 3 [20] Tilke Judd, Fredo Durand, and Edward Adelson. Apparent ridges for line drawing. ACM TOG, 2007. 3 [21] Black Forest Labs. Flux, 2024. 4, 1, 5 [22] Mengtian Li, Zhe Lin, Radomir Mech, Ersin Yumer, and Deva Ramanan. Photo-sketching: Inferring contour drawings from images. In WACV, 2019. 3 [23] Yi Li, Yi-Zhe Song, Timothy Hospedales, and Shaogang Gong. Free-hand sketch synthesis with deformable stroke models. IJCV, 2017. 3 [24] Yijun Li, Chen Fang, Aaron Hertzmann, Eli Shechtman, and Ming-Hsuan Yang. Im2pencil: Controllable pencil illustration from photographs. In CVPR, 2019. [25] Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, and Deva Ramanan. Evaluating text-to-visual generation with image-to-text generation. In ECCV, 2024. 5 [26] Difan Liu, Mohamed Nabail, Aaron Hertzmann, and Evangelos Kalogerakis. Neural contours: Learning to draw lines from 3d shapes. In CVPR, 2020. 3 [27] Difan Liu, Matthew Fisher, Aaron Hertzmann, and Evangelos Kalogerakis. Neural strokes: Stylized line drawing of 3d shapes. In ICCV, 2021. 3 [28] Fang Liu, Xiaoming Deng, Yu-Kun Lai, Yong-Jin Liu, Cuixia Ma, and Hongan Wang. Sketchgan: Joint sketch completion and recognition with generative adversarial network. In CVPR, 2019. 3 [29] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Dora: WeightCheng, and Min-Hung Chen. decomposed low-rank adaptation. In ICML, 2024. 4, 5 [44] Hao Xu, Yinqiao Wang, Niloy Mitra, Shuaicheng Liu, Pheng-Ann Heng, and Chi-Wing Fu. Hand-shadow poser. ACM TOG, 2025. 3 [45] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. In NeurIPS, 2023. 5, 6, 8 [46] Ran Yi, Yong-Jin Liu, Yu-Kun Lai, and Paul Rosin. Apdrawinggan: Generating artistic portrait drawings In CVPR, from face photos with hierarchical gans. 2019. 3 [30] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In CVPR, 2022. 4 [31] Sungchul Min, Jaehyun Lee, Jungdam Won, and JoonYoung Lee. Soft shadow art. In Symposium on Computational Aesthetics, 2017. 2, [32] Niloy J. Mitra and Mark Pauly. Shadow art. ACM TOG, 2009. 2, 3 [33] Thiago Pereira, Szymon Rusinkiewicz, and Wojciech Matusik. Computational light routing: 3d printed optical fibers for sensing and display. ACM TOG, 2014. 2 [34] Zhiyu Qu, Lan Yang, Honggang Zhang, Tao Xiang, Kaiyue Pang, and Yi-Zhe Song. Wired perspectives: Multi-view wire art embraces generative ai. In CVPR, 2024. 3 [35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 5 [36] Kshitij Sadekar, Anurag Tiwari, and Shanmuganathan Raman. Shadow art revisited: differentiable rendering based approach. In WACV, 2022. 2, [37] Yael Vinker, Tamar Rott Shaham, Kristine Zheng, Alex Zhao, Judith Fan, and Antonio Torralba. Sketchagent: Language-driven sequential sketch generation. In CVPR, 2025. 3 [38] Xinyue Wei, Kai Zhang, Sai Bi, Hao Tan, Fujun Luan, Valentin Deschaintre, Kalyan Sunkavalli, Hao Su, and Zexiang Xu. Meshlrm: Large reconstruction model for high-quality meshes. arXiv:2404.12385, 2024. 6 [39] Tim Weyrich, Pieter Peers, Wojciech Matusik, and Szymon Rusinkiewicz. Fabricating microgeometry for custom surface reflectance. ACM TOG, 2009. 2, 3 [40] Jungdam Won and Joon-Young Lee. Shadow theatre: discovering human motion from sequence of silhouettes. ACM TOG, 2016. 3 [41] Kang Wu, Renjie Chen, Xiao-Ming Fu, and Ligang Liu. Computational mirror cup and saucer art. ACM TOG, 2022. 3 [42] Kang Wu, Xiao-Ming Fu, Renjie Chen, and Ligang Liu. Survey on computational 3d visual optical art design. Visual Computing for Industry, Biomedicine, and Art, 2022. 3 [43] Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score: Better aligning text-to-image models with human preference. In ICCV, 2023. 5, 6, 8 ShadowDraw: From Any Object to ShadowDrawing Compositional Art"
        },
        {
            "title": "Supplementary Material",
            "content": "1. Supplementary Material Overview In this supplementary material, we provide additional implementation details and present extended qualitative results. Check out our project page for more results and an end-toend real-world demonstration of our pipeline! 2. Implementation Details 2.1. Training Data Construction We construct paired dataset of shadow contours and line drawings to train our line drawing generation model. The pipeline proceeds as follows. We first generate 100 line drawings using GPT-4o, prompting it with descriptions of everyday objects. We retain only those drawings that contain at least one closed region bounded by strokes. Next, we fine-tune FLUX-1-dev LoRA [21] on this filtered subset and employ it to synthesize an additional 10K line drawings from GPT-4o-generated prompts describing everyday subjects. Finally, we apply OpenCVs FindContour algorithm to extract closed regions from the synthesized drawings and use greedy merging strategy that iteratively combines the two smallest connected regions until only four remain. The closed contours of these merged regions (and their union) serve as the shadow contour conditions for training. Figure 8. Examples of training data pairs. Each row shows line drawing generated by our finetuned FLUX model, with different closed contours extracted from it. Each image forms training pair, where the red contour is used as the condition and the full line drawing serves as the target. design system prompt that guides the VLM, specifically GPT-4.1, to generate detailed, semantically meaningful descriptions of the provided stroke and its role in the complete line drawing. The full system prompt is given below. You are skilled artist specializing in expressive, imaginative, and visually striking minimalist line drawings. You will be shown an image containing contour. Your task is to interpret this contour and create complete line drawing, using the provided contour as the core expressive element of your composition. The subject you draw should be character, either human, an animal, or cartoon or anthropomorphic figure. # Instructions First, analyze the contour to identify its function in the drawing. Follow these steps: 1. Analyze the contours geometry and position on the canvas. 2. Determine the subject of the line drawing and which major, prominent body part (such as body, head, face) or clothing (such as skirt or dress) the contour outlines. Do not describe small or less essential features, such as hands, tails, wings, or beaks. Specify the subject in precise terms: For people, use identifiers (e.g., man, woman) or vocations (e.g., dancer, sailor, guitarist); For animals, name the species (e.g., bird, fish, cat, dog); For cartoon or anthropomorphic characters, name the type (e.g., ghost, robot, cookie character, book character). 3. Explain your reasoning in detail, including the strokes shape, its position on the canvas, and why it is good fit for the composition. Next, write description of the complete drawing without referencing the provided contour, following this structure: 1. Opening: minimalist line drawing of [ character] [in pose or with an expression], matching your earlier interpretation. 2. Physical description: The [character] has [ facial feature or expression] and wears [ clothing or accessories]. 2.2. Visual Prompt Proposal Visionlanguage models are highly sensitive to input formatting, and poorly structured inputs often result in uninformative or inconsistent outputs. To address this, we carefully 3. Object or motion (optional): The [character] is [doing something, holding something, or in motion]. 4. Gesture or interaction (optional): Further 1 describe the subjects gesture or interaction with their environment. 5. Conclude with style remark: The style is [ adjective(s)], [additional notes about technique or focus]. # Format requirement 1. Separate the two parts with blank line. 2. Do not use numbering, bullet points, or extra formatting. 3. Strictly follow this structure without additional comments: The provided contour shows an outline of the [ specific body part or clothing] of [ character]. The reason is [contour geometry interpretation]. [Additional reasons for your interpretation]. minimalist line drawing of [character] [in pose or with an expression]. The [character] has [facial feature or expression] and wears [clothing/accessories]. [Optional action or motion]. [Optional gesture or interaction]. [ Artistic style remark]. Listing 1. System prompt for creating the textual description of the intended line drawing in the visual prompt proposal stage. 2.3. Baseline Execution Details Here we describe how we use Gemini Flash 2.5 Image (a.k.a. nano banana) [13] to generate shadowdrawing art. In the objectshadow version, we provide the model with the objectshadow composite and the line drawing description produced by our approach, and ask it to directly generate shadowdrawing composition. In the shadow contour version, we instead provide the shadow contour and the line drawing description, prompting the model to complete the drawing. As in our framework, we then remove the input shadow contour from the generated drawing, reinsert the 3D object, and render the final composition, where the cast shadow completes the drawing. 2.4. Discussion on the Evaluation Algorithm Visual Illustration. Figure 9 illustrates how our evaluation and ranking algorithm selects high-quality shadow-drawing compositions. In the first stage, the VQA-based verification discards incoherent cases (e.g., when the shadow stroke does not correspond to the intended body part of the character). In the second stage, the shadow contribution assessment compares complete and contour-removed versions, ensuring that the shadow meaningfully enhances the drawing. As shown, this two-step process ranks plausible results higher while filtering out cases where the shadow plays only minor or misleading role. Overall, the pipeline balances semantic alignment, structural coherence, and visual quality, producing consistent and interpretable rankings. Analyses. Evaluating our generated shadowdrawing compositions is inherently challenging, as their abstract and artistic qualities often resist objective evaluation. To rigorously assess effectiveness, we designed two complementary user studies based on pairwise preference judgments. (1) For each object, we randomly select one result from the top-4 ranked outputs and another from the remaining, asking evaluators to choose their preferred composition; and (2) we randomly select two results among the top-3, where differences are more subtle. In both cases, evaluators may also indicate that neither is preferable. Altogether, we collected 2,000 preference pairs from 10 annotators for the first study, and 2 labels per top-3 pair across all 200 objects for the second study. In the first study, our ranking algorithm achieves strong alignment with human judgment, agreeing on 63.5% of pairs, disagreeing on only 11.0%, and with 25.5% of cases marked as no clear preference. These results indicate that our automated ranking provides reliable proxy for subjective evaluation in the broader design space. In the second study, where all candidates are already of very high quality, the agreement rate with human preference naturally decreases to 39.8%, with another 24.3% of cases judged as indeterminate. Crucially, the agreement between two independent sets of human annotations is itself only 44.5%, underscoring the intrinsic subjectivity of evaluating artistic compositions. Taken together, these findings suggest that while perfect alignment is unattainable in such subjective domain, our ranking system performs comparably to human consensus and thus provides practical, scalable tool for curating shadowdrawing art. 2.5. Subject-Specified Generation We also demonstrate that our pipeline supports user-specified subject control through prompt editing. In practice, the desired subject is directly specified in the system prompt used by the VLM during visual prompt proposal. However, we observe that certain objectsubject combinations are inherently incompatible: the geometry of the 3D object may not afford shadow stroke that can be meaningfully integrated into the specified concept, leading to unsatisfactory results. Examples are shown in Fig. 10. 2.6. Animated Shadow-drawing Art As mentioned in Sec. 4.3, our framework supports animated objects without requiring additional training. We provide the implementation details as follows. For each candidate configuration, we render five keyframes of the animation and extract their shadow contours. To preserve temporal information, we overlay the strokes into single composite image, assigning distinct colors to each frame so that the VLM can recognize temporal variation and generate prompt. 2 Figure 9. Illustration of the evaluation and ranking process, which discards incoherent cases and preserves only those where the shadow meaningfully contributes to the composition. critical step in this pipeline is the construction of binary mask to restrict where strokes may be placed. Without such constraint, strokes might appear in regions where shadows change across frames, leading to incoherent results. To build the mask, we proceed as follows. First, we render the shadow silhouettes of all five keyframes. Pixels that are covered by shadows in every frame are designated as the static region, while pixels that are covered in at least one but not all frames are designated as the dynamic region. Next, for each pixel on the canvas, we compute its distance to both the static and dynamic regions. If pixel is closer to the dynamic region than to the static region, we mask it out, prohibiting stroke placement in that location. Intuitively, this rule ensures that strokes are confined to stable shadow regions and avoid areas subject to temporal variation. Once the mask is established, the generation process follows the same procedure as in the static-object setting. The shadow contour and the corresponding VLM-generated prompt are fed to the line drawing generator, and masked regions are excluded during synthesis. Finally, the animated object is reinserted into the scene and rendered across frames, with its shadow complementing the static line drawing to form temporally consistent shadowdrawing composition. We evaluate this pipeline using animated objects from Objaverse. Qualitative results are presented in Fig. 6(d) (main paper) and Fig. 2.6. As shown, our system successfully generates line drawings that remain coherent with dynamic shadows, demonstrating the ability of our approach to extend from static to temporally varying scenes. Figure 10. Shadowdrawing compositions with user-specified subjects. We show results for alphabet-shaped objects (C, V, P, R) conditioned on the subjects ghost, fish, person, and bird. While the pipeline supports flexible subject control through prompt editing, certain object geometries inherently limit the achievable compositions, leading to occasional unsatisfactory outcomes. Figure 11. Animated shadowdrawing art. Line drawings generated with our pipeline remain coherent as dynamic shadows complete the composition across frames. Best viewed in Adobe Acrobat Reader for the embedded animation. 4 Figure 12. Failure cases. Some objects produce shadows that are ambiguous or uninformative, making it difficult for our system to produce meaningful shadow-drawing compositions. large design space, generating results for single object still takes relatively long duration. Finally, while our ranking algorithm is generally effective at surfacing strong candidates, it is not flawless. In practice, users may still need to examine multiple outputs to identify the most compelling result. Addressing these limitations through richer shadow descriptors other than fractal dimension, more efficient search strategies, and refined ranking or user-in-the-loop mechanisms represents promising directions for future work. 2.7. Runtime Analysis The only trainable component in our framework is the line drawing generation model based on FLUX.1-Canny [21]. Specifically, we train DoRA [29] adapter on all queries, keys, values, and MLPs of the backbone diffusion transformer. We use the Adam optimizer with constant learning rate of 104 and train for roughly 12 hours on 8 A6000 GPUs. At inference, the dominant cost arises from diffusion sampling, taking about 40 seconds per image with 30 steps. For single object, generating line drawings for 48 sampled scene configurations requires approximately 30 minutes, and the full pipeline completes in about 35 minutes on single A6000 GPU. Reducing the number of inference steps from 30 to 10 lowers latency to around 15 minutes with minimal quality degradation. Because the process is fully parallelizable, latency can be reduced to under 5 minutes on standard 8-GPU nodes. Further acceleration may be achieved by distilling the multi-step diffusion process into oneor few-step generator. 3. Limitations While our method enables diverse and visually engaging shadowdrawing compositions, several limitations remain. First, the quality of results is closely tied to the intrinsic shape of the object: some objects inherently produce shadows that are either visually uninteresting or too ambiguous to interpret, regardless of lighting or pose. As illustrated in Fig. 12, such cases often yield shadows that lack recognizable structure or fail to align meaningfully with the generated drawing. Second, the joint search and generation process over scene parameters introduces noticeable runtime overhead. Although this procedure is necessary to explore the"
        }
    ],
    "affiliations": []
}