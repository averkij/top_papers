{
    "paper_title": "Why Personalizing Deep Learning-Based Code Completion Tools Matters",
    "authors": [
        "Alessandro Giagnorio",
        "Alberto Martin-Lopez",
        "Gabriele Bavota"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Deep learning (DL)-based code completion tools have transformed software development by enabling advanced code generation. These tools leverage models trained on vast amounts of code from numerous repositories, capturing general coding patterns. However, the impact of fine-tuning these models for specific organizations or developers to boost their performance on such subjects remains unexplored. In this work, we fill this gap by presenting solid empirical evidence answering this question. More specifically, we consider 136 developers from two organizations (Apache and Spring), two model architectures (T5 and Code Llama), and three model sizes (60M, 750M, and 7B trainable parameters). T5 models (60M, 750M) were pre-trained and fine-tuned on over 2,000 open-source projects, excluding the subject organizations' data, and compared against versions fine-tuned on organization- and developer-specific datasets. For the Code Llama model (7B), we compared the performance of the already pre-trained model publicly available online with the same model fine-tuned via parameter-efficient fine-tuning on organization- and developer-specific datasets. Our results show that there is a boost in prediction capabilities provided by both an organization-specific and a developer-specific additional fine-tuning, with the former being particularly performant. Such a finding generalizes across (i) the two subject organizations (i.e., Apache and Spring) and (ii) models of completely different magnitude (from 60M to 7B trainable parameters). Finally, we show that DL models fine-tuned on an organization-specific dataset achieve the same completion performance of pre-trained code models used out of the box and being $\\sim$10$\\times$ larger, with consequent savings in terms of deployment and inference cost (e.g., smaller GPUs needed)."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 1 0 2 4 1 . 3 0 5 2 : r Why Personalizing Deep Learning-Based Code Completion Tools Matters ALESSANDRO GIAGNORIO, UniversitÃ  della Svizzera italiana, Switzerland ALBERTO MARTIN-LOPEZ, UniversitÃ  della Svizzera italiana, Switzerland GABRIELE BAVOTA, UniversitÃ  della Svizzera italiana, Switzerland Deep learning (DL)-based code completion tools have revolutionized software development by providing unprecedented code generation capabilities. The DL models behind these tools are usually trained on large amounts of code from thousands of software repositories. This makes them good in learning natural coding patterns observed across many training instances. However, little is known about the extent to which additional training effort (fine-tuning) aimed at specializing the models towards the code base of given organization/developer further benefits their code completion capabilities. In this work, we fill this gap by presenting solid empirical evidence answering this question. More specifically, we consider 136 developers from two organizations (Apache and Spring), two model architectures (T5 and Code Llama), and three model sizes (60M, 750M, and 7B trainable parameters). For T5 models (60M, 750M), we pre-train and fine-tune them on over 2,000 open source projects, making sure that code from the two subject organizations is not part of their training sets. Then, we compare their completion capabilities against the same models further fine-tuned on organizationand developer-specific datasets. For the Code Llama model (7B), we compare the performance of the already pre-trained model publicly available online with the same model fine-tuned via parameter-efficient fine-tuning on organizationand developer-specific datasets. Our results show that there is boost in prediction capabilities provided by both an organization-specific and developer-specific additional fine-tuning, with the former being particularly performant. Such finding generalizes across (i) the two subject organizations (i.e., Apache and Spring) and (ii) models of completely different magnitude (from 60M to 7B trainable parameters). Finally, we show that DL models fine-tuned on an organization-specific dataset achieve the same completion performance of pre-trained code models used out of the box and being 10 larger, with consequent savings in terms of deployment and inference cost (e.g., smaller GPUs needed). CCS Concepts: Software and its engineering; Computing methodologies Artificial Intelligence; Additional Key Words and Phrases: Software Engineering, Artificial Intelligence, Code Recommenders, Training Strategies ACM Reference Format: Alessandro Giagnorio, Alberto Martin-Lopez, and Gabriele Bavota. 2025. Why Personalizing Deep Learning-Based Code Completion Tools Matters. 1, 1 (March 2025), 32 pages. https://doi.org/XXXXXXX.XXXXXXX"
        },
        {
            "title": "1 INTRODUCTION",
            "content": "The automatic generation of source code has been long lasting dream in software engineering research for many years. Thanks to the advent of large deep learning (DL) models trained on code, we moved from predicting the next token the developer is likely to type [15, 51] to the generation of complete code blocks and functions [24]. Tools such as GitHub Copilot [17] are nowadays used by millions of developers and have been shown to boost their productivity [49]. These Authors addresses: Alessandro Giagnorio, UniversitÃ  della Svizzera italiana, Lugano, Switzerland, alessandro.giagnorio@usi.ch; Alberto Martin-Lopez, UniversitÃ  della Svizzera italiana, Lugano, Switzerland, alberto.martin@usi.ch; Gabriele Bavota, UniversitÃ  della Svizzera italiana, Lugano, Switzerland, gabriele.bavota@usi.ch. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. Manuscript submitted to ACM"
        },
        {
            "title": "Manuscript submitted to ACM",
            "content": "1 2 Giagnorio et al. tools are trained on large code corpora usually mined from open source projects. For example, Copilots training set includes 159 GB of code mined from 54M public GitHub repositories [17]. Given the known repetitiveness of source code [32], this training process allows the DL models to learn coding patterns seen across (possibly thousands of) code files, hence enabling the generation of meaningful recommendations when facing coding contexts similar to those in the training set. While the usefulness of these code recommenders is backed-up by empirical evidence [49], there is still room for improvement when it comes to their performance1 [45]. One of the open questions when it comes to the adoption of DL-based code completion tools is whether their fine-tuning to the specific organization/developer using them may help in boosting performance. The idea is to perform further training step after the generic fine-tuning (i.e., the one in which the DL model is trained on code coming from thousands of repositories) with the aim of specializing the DL model to given code base (e.g., the code developed within an organization or by specific developer). Indeed, recent evidence from the Natural Language Processing (NLP) field demonstrated that more specific training data may help in boosting the performance of DL models, without risks of catastrophic forgetting their general knowledge. For example, Eschbach-Dymanus et al. [25] showed that Large Language Models employed for natural language translation may benefit of additional fine-tuning aimed at specializing them for specific domain. In our case, the specialized domain could be represented by the code base of specific organization/company or by the code changes implemented over time by single developer. Given the availability of open source DL models pre-trained on code (see e.g., CodeBERT [26], CodeT5 [58], Code Llama [52]), showing the effectiveness of specializing them to given code base may be relevant for companies who want to consider the possibility to fine-tune one of these models on their code base with the goal of deploying an in-house code recommender, possibly saving costs and avoiding potential issues related to the need to share proprietary code with third-party DL model (e.g., Copilot) to receive recommendations. We present large-scale empirical study investigating the extent to which personalizing DL-based code completion models helps in boosting their performance. We focus on two different levels of personalization, related to whole software organization and single developer. The former represents the scenario of company specializing single model on all software projects it runs. The latter answers the interesting research question of whether deep level of personalization (down to the single developer) is really worthwhile. Indeed, developer-specific personalization might be impractical, requiring the deployment and maintenance of several models. Still, if the gain in performance is major, then it could be considered in specific cases (e.g., small team). From an abstract point of view, we start from DL model ğµ that has been trained on large and generic code corpus. ğµ represents our baseline, namely generic DL-based code completion tool. We then collect code changes performed over time by developers who contributed to projects run by organization ğ‘œğ‘Ÿğ‘”. Given developer ğ· who performed their contributions (i.e., code changes, possibly spanning multiple projects run by ğ‘œğ‘Ÿğ‘”) over time period ğ‘‡ğ· , we split ğ‘‡ğ· into three parts, obtaining ğ·-specific training, evaluation and test set. The test set features the most recent changes implemented by ğ·. We then fine-tune ğµ on the ğ·-specific training set and compare its performance to our baseline (ğµ) on the ğ·-specific test set. This shows the extent to which specializing DL-based code completion tool to specific developer ğ· improves the support provided to ğ· on future implementation tasks. Indeed, we are adopting time-based splitting of data, ensuring that data from the past (the oldest ğ·s changes) is used to predict the future (the most recent ğ·s changes). Finally, we put together all previously built developer-specific training sets, thus creating an organization-specific training set. Such dataset has been used to specialize ğµ to the organization of interest (ğ‘œğ‘Ÿğ‘”), again 1With performance we do not refer to properties such as execution time or memory usage, but to the accuracy of the generated recommendations."
        },
        {
            "title": "Manuscript submitted to ACM",
            "content": "Why Personalizing Deep Learning-Based Code Completion Tools Matters 3 comparing the performance of this specialized model to ğµ. Also in this case the performance has been assessed on the developer-specific test sets, representing future changes that ğ‘œğ‘Ÿğ‘”s developers will implement. Our study spans two organizations (Apache and Spring), two model architectures (T5 [50] and Code Llama [52]) and three model sizes (60M, 750M and 7B trainable parameters). For T5 models, we pre-train and fine-tune them from scratch on code base featuring over 2M instances (Java methods with some parts masked to simulate the code completion task). These models represent our baselines (ğµ). Note that for these models we ensured that the training data used for the baselines did not include code from the organizations used as case studies (Apache and Spring). For the Code Llama model, this was not possible since the pre-trained model has been trained on large code corpus which is not publicly available but it is very likely to include code from both organizations. Still, it is interesting to observe if even in this case, further personalized fine-tuning helps the model. In the case of Code Llama, the pre-trained model publicly available online represents our baseline. Concerning the specialization of the models, we mine the change history of all Java projects hosted on GitHub by each organization, identifying the developers who contributed the most to these projects. To keep the experimentation affordable, we retrieve at most the top-100 developers (in terms of contributions) for each organization, provided that their contributions result in at least 1,000 training instances and 500 test instances, to make the training and evaluation meaningful. For Apache (Spring) we mined the change history of 1,161 (68) Java repositories, obtaining (36) developers who contributed across all repositories. Throughout the document, we will continue to use the notation (Y) to refer to the numbers related to Apache (Spring), respectively. Note that for Spring we only have 36 developers since the remaining ones did not meet the 1,000 training instances requirement. Following the previously-described process, we split their change history into three sets, ending up with 100 (36) developer-specific training, evaluation and test sets. We further fine-tune our ğµ baseline on these training sets, obtaining 100 (36) developer-specialized DL-based code completion models adopting the T5ğ‘ ğ‘šğ‘ğ‘™ğ‘™ architecture. We replicate the same process using the T5ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ and Code Llama models for the top-10 developers from each organization (20 in total). Each of the trained models has then been tested (and compared with the baseline) on the corresponding developer-specific test set. This analysis answers the question: To what extent personalizing DL-based code completion tool to the specific developer using it boosts its performance? The organization-specific training sets have been obtained by merging the 100 (36) developer-specific datasets up to the latest date of the training set of each developer. This was done to ensure that data from the past is not used to predict the future (details in Section 2). Thus, we created another 100 (36) organization-specific training sets, leading to 100 (36) new models based on the T5ğ‘ ğ‘šğ‘ğ‘™ğ‘™ architecture, plus 10 (for each organization) for the T5ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ and Code Llama models. This analysis answers the question: To what extent personalizing DL-based code completion tool to software organization boosts its performance for individual developers of the organization? On top of what described, we performed several analyses aimed at factoring out confounding variables, like ensuring that the observed improvement is not simply due to the additional data used for further fine-tuning the baselines. The above-summarized experiments required the training and testing of 396 different models and showed that, while very cheap fine-tuning performed on developer-specific dataset boosts the performance of DL-based code completion tools, the observed improvement is usually capped by the limited number of developer-specific training data which can be collected for most developers. The organization-specific fine-tuning, instead, thanks to the additional training data available, works better than developer-specific training, and should be the obvious choice in most of cases. Indeed, when considering both organizations (i.e., Apache and Spring) together, the organization-specific models achieve statistically significant improvements in correct predictions for: (i) 64% of the 136 considered developers, with"
        },
        {
            "title": "Manuscript submitted to ACM",
            "content": "4 Giagnorio et al. only one case of statistically significant drop in performance (T5ğ‘ ğ‘šğ‘ğ‘™ğ‘™ ); (ii) 70% of the 20 considered developers, with no cases of significant drop in performance (T5ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ ); and (iii) 55% of the 20 considered developers, again with no cases of significant performance drop (Code Llama). Also, we demonstrate that the increase in performance observed with both specializations is not simply due to higher number of training instances as compared to the baselines, but to their specificity. Finally, through cost-effectiveness analysis, we show that thanks to personalized fine-tuning, DL models can achieve code completion performance on par with those of models being 10 larger (e.g., an organization-specific T5ğ‘ ğ‘šğ‘ğ‘™ğ‘™ achieves the same performance of generic T5ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ model), possibly saving costs in the long-run. The remainder of this paper is organized as follows: Section 2 details the design of our study, including data collection, model training and evaluation. Section 3 analyzes the results obtained across different levels of personalization and model sizes, while Section 4 summarizes the main findings. Section 5 explains the validity threats and how these were mitigated. Finally, Section 6 discusses related work, while Section 7 concludes the paper."
        },
        {
            "title": "2 STUDY DESIGN",
            "content": "We aim at answering the following research question: To what extent personalizing DL-based code completion tool can boost its performance? We tackle this question by looking at the two levels of personalization previously mentioned: developer-specific and organization-specific. The context of our study is represented by the two organizations considered, i.e., Apache and Spring, and the code changes pushed by their top contributors to their Java projects hosted on GitHub. Table 1 summarizes the models, datasets, and metrics used in each part of our empirical investigation. We will refer to the goals described in the table (Goal X) throughout the section."
        },
        {
            "title": "2.1 Deep Learning Models",
            "content": "As representative of DL models, we adopt the T5 [50] and Code Llama [52]. T5 is transformer model already used in the literature to automate several code-related tasks [14, 27, 44, 57], including code completion [18]. Raffel et al. [50] proposed several variants of T5, differing in number of trainable parameters. We adopt two of them: small and large. The former features 60M parameters, while the latter 750M. All experiments described in Section 2.4 have been performed using the T5ğ‘ ğ‘šğ‘ğ‘™ğ‘™ , while subset of them features the T5ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ (as specified in Section 2.4). Indeed, the trained large variants allow us, in combination with Code Llama, to investigate whether the differences observed in terms of performance after personalizing the models are valid independently from the models size. For both T5 variants, we use the T5v1.1 implementation available via Hugging Face [6]. During all trainings, the batch size is set to 32 for T5ğ‘ ğ‘šğ‘ğ‘™ğ‘™ and 4 for T5ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ (due to its higher cost in terms of GPU memory). For both models, we adopt their default hyper-parameter values, i.e., learning rate of 5 105, AdamW optimizer [41] and linear decay scheduler. Code Llama [52] has also been proven to be effective on broad range of Software Engineering tasks [16, 34, 54, 59]. Code Llama is family of transformer models based on the general-purpose Llama 2 [56] and further trained on 500B code-specific data tokens. It comes in different sizes (7B, 13B, 34B, and 70B) and versions (base model, instruction-tuned, and Python specialist). We select the base variant with 7B parameters, which is 10 times larger than the T5ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ model. Adding this model to our study allows us to: (i) compare the performance of the T5 models with state-of-the-art code models like Code Llama; (ii) investigate the impact of personalization on models with very large number (i.e.,"
        },
        {
            "title": "Manuscript submitted to ACM",
            "content": "Why Personalizing Deep Learning-Based Code Completion Tools Matters 5 Table 1. Summary of models, datasets, and metrics used in this study. Baselines Training"
        },
        {
            "title": "Metrics",
            "content": "2 generic code-completion T5ğ‘ ğ‘šğ‘ğ‘™ğ‘™ (1 Apache and 1 Spring) 2 pre-training datasets (1 Apache and 1 Spring) Exact Match (for best-checkpoint selection) 2 generic code-completion T5ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ (1 Apache and 1 Spring) 2 code-completion datasets (1 Apache and 1 Spring) Goal 1. Evaluating Developerand Organization-Specific Personalization"
        },
        {
            "title": "Metrics",
            "content": "136 developer -specific T5ğ‘ ğ‘šğ‘ğ‘™ğ‘™ (100 Apache and 36 Spring) 136 organization-specific T5ğ‘ ğ‘šğ‘ğ‘™ğ‘™ (100 Apache and 36 Spring) 136 developer -specific train, evaluation, and test datasets (100 Apache and 36 Spring) 136 organization-specific train and evaluation datasets (100 Apache and 36 Spring)"
        },
        {
            "title": "Exact Match and CrystalBLEU",
            "content": "Goal 2. Assessing the Impact of the Training Data Size"
        },
        {
            "title": "20 Organization subset train and eval-\nuation datasets\n(top 10 Apache contributors and 10 Spring",
            "content": "contributors)"
        },
        {
            "title": "20 Baseline+ train and evaluation\ndatasets\n(top 10 Apache contributors and 10 Spring",
            "content": "contributors)"
        },
        {
            "title": "Exact Match and CrystalBLEU",
            "content": "Goal 3. Evaluating the Impact of Model Size, Architecture, and Pre-training"
        },
        {
            "title": "Metrics",
            "content": "20 developer -specific T5ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ (10 Apache and 10 Spring) 20 organization-specific T5ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ (10 Apache and 10 Spring) 20 developer -specific train, evaluation, and test datasets from Goal 1 (top 10 Apache contributors and 10 Spring contributors) 20 developer -specific Code Llama (10 Apache and 10 Spring) 20 organization-specific Code Llama (10 Apache and 10 Spring) 20 organization-specific train and evaluation datasets from Goal 1 (top 10 Apache contributors and 10 Spring contributors) Goal 4. Investigating the Cost-Performance Trade-Off"
        },
        {
            "title": "Datasets",
            "content": "2 generic code-completion T5ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ from Baselines Training (1 Apache and 1 Spring) 20 developer -specific test datasets from Goal 1 (top 10 Apache contributors and 10 Spring 20 organization-specific T5ğ‘ ğ‘šğ‘ğ‘™ğ‘™ from Goal 1 (10 Apache and 10 Spring) contributors)"
        },
        {
            "title": "Manuscript submitted to ACM",
            "content": "6 Giagnorio et al. billions) of parameters; (iii) understand the generalizability of our findings to different model architecture; and (iv) assess the impact of personalization on pre-trained model, whose training dataset may already feature code from the organization and developers of interest. To reduce training costs for this larger model, we train Code Llama with the LoRA technique [33]. LoRA is Parameter-Efficient Fine-Tuning (PEFT) method that aims to approach the performance of full-parameter fine-tuning by only training small number of parameters. This technique freezes the pre-trained weights and replaces the gradient update matrix with two trainable low-rank matrices. This significantly reduces the number of trainable parameters (e.g., from 7B to 40M for Code Llama) while also lowering the computational cost. Following previous study on code generation [59], we set the LoRA hyperparameter values to ğ›¼ = 32 and ğ‘Ÿ = 16, while using the same training configuration seen for T5ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ ."
        },
        {
            "title": "2.2 Datasets Used for Training and Testing",
            "content": "We describe the datasets used to train the DL models for the code completion task. In such context, training instance is Java method having some contiguous tokens masked, with the model in charge of predicting them. Indeed, as done in previous work on DL-based code completion [18], we work at method-level granularity. We start by describing the developer-specific (Section 2.2.1) and the organization-specific (Section 2.2.2) datasets. Then, we present our generic pre-training and fine-tuning datasets used to train the T5 models (Section 2.3), which are not already trained like Code Llama."
        },
        {
            "title": "2.2.1 Developer-Specific Datasets. For both the developer- and organization-specific datasets, our goal is to create\ntraining/testing instances that are representative of real code changes performed by developers belonging to the",
            "content": "organization of interest (i.e., Apache or Spring). Fig. 1 illustrates the process we followed to create the developer-specific datasets. We explain it in the following. Commits mining. We start by mining all commits from the main branch of the 1,161 Apache (68 Spring) Java projects considered in our study. We exclude commits performed by bots, not modifying Java files, and impacting too many files, likely being the result of automated operations (e.g., initial commit, rename package refactoring, etc.). Concerning the identification of bots, we apply simple heuristic filtering out all commits performed by authors having name containing [bot] and/or GitHub. As for the commits impacting large number of files, once mined all commits, we exclude those being outliers in terms of number of modified files, i.e., impacting more than ğ‘„3 + 1.5 ğ¼ğ‘„ğ‘… files, where ğ‘„3 is the third quartile and ğ¼ğ‘„ğ‘… is the interquartile range of the distribution of impacted files across all commits. This process narrowed down the initial set of 1,272,556 (84,591) commits to 1,114,142 (74,906) relevant commits. Extracting Java methods featuring new code. The next step consists in parsing the Java files impacted in the mined commits with the goal of identifying Java methods in which at least one new line has been added. We only focus on added lines and ignore the modified ones since our idea is to exploit these methods to generate training instances in which the code written by specific developer (i.e., the added lines) is masked, and the model is in charge of predicting it. For modified lines, in theory, developer may change single token (or even space) and it would be wrong to assume that the modified line represents code written by the developer. We expect this process to specialize the model towards the code changes representative of the work done by developer and, thus, of the software organization they contribute to. Since parsing the Java files associated with each commit is costly, we decided to perform this process only for the top-1k developers (in each organization) in terms of added lines of code. Those are the ones likely to provide enough specialized training data which can then be used to experiment with the developer-specific fine-tuning. Selecting these"
        },
        {
            "title": "Manuscript submitted to ACM",
            "content": "Why Personalizing Deep Learning-Based Code Completion Tools Matters 7 Fig. 1. Mining process to create the developer-specific datasets. top-1k developers is not trivial. Indeed: (i) the developers who authored the mined commits could have contributed to more than one of the 1,161 Apache (68 Spring) projects we mine, possibly using different names/emails; (ii) the change history associated with several of the subject repositories is extremely long (e.g., >20 years for Apache Commons BeanUtils [1]), again increasing the likelihood that developer changed name/email used when committing to the versioning system over time. For these reasons, before selecting the top-1k developers, we use the gambit tool [28] to disambiguate the authors of all commits, associating the same author ID to commits performed by the same developer using different names. Once identified the top-1k contributors, we manually inspect the disambiguations, excluding wrong matches. In particular, given an author for which multiple GitHub accounts were matched, the first author inspected all of them discarding cases in which (i) the account cannot be traced back to the same person with high confidence; or (ii) one or more of the matched accounts does not exist anymore. This process left us with 686 (818) valid and disambiguated developers. For each commit they performed, we clone the corresponding repository at the commits hash and use javalang [4] to parse the impacted Java files and extract all methods with at least one line added (according to the commits diff). We subsequently discard methods that: (i) cannot be parsed (e.g., they contain syntax errors); (ii) contain the word test in their name (after splitting it camelCase-wise), to create more coherent dataset of production code; (iii) contain an empty body or only comments; (iv) contain less than 15 (too simple) or more than 500 tokens (too long to handle with the subject DL models); and (v) contain non-latin characters (e.g., emojis). Through these filters, we obtained 1,148,324 (197,622) Java methods with at least one new line implemented in given commit. Creating training, evaluation and test sets for the top developers. As final step, we create training/testing instances from each method extracted in the previous steps. We indicate with ğ¿ all lines added in method in specific"
        },
        {
            "title": "Manuscript submitted to ACM",
            "content": "8 Giagnorio et al. commit. As running example, let us assume that ğ¿ = {ğ‘™4, ğ‘™5, ğ‘™6, ğ‘™7, ğ‘™8, ğ‘™14}, with the subscript number indicating the line number of each added line. If new line is added in isolation (i.e., it does not have other added lines right before/after it), we mask its last ğ‘› tokens with ğ‘› randomly ranging from 3 to ğ‘šğ‘–ğ‘›(50, ğ‘ 1), where ğ‘ is the total number of tokens in the line. This is what happens in our running example to line ğ‘™14. Note that we mask at least 3 and at most 50 tokens to avoid trivial completions (e.g., predicting only the last semicolon of statement) while keeping the completion task approachable (max. 50 tokens to predict). If, instead, block of contiguous lines is added (ğ‘™4 to ğ‘™8 in our example), we split it into blocks of at most three lines, with empty lines (i.e., those added for formatting purpose) or lines featuring single token (e.g., }) not counting towards this limit. In our example, ğ‘™6 is an empty line, therefore ğ‘™4 to ğ‘™7 becomes one block, and ğ‘™8 second block. We then apply the same masking procedure described for the isolated lines. This means that we compute ğ‘ as the total number of tokens in the block, and mask its last ğ‘› tokens with ğ‘› randomly selected as previously described. Again, we limit the maximum number of lines in block to three to keep the complexity of the completion task reasonable. Two important points are worth being noticed. First, both completion tasks (i.e., masking of single line or of block of lines) simulate developer that starts writing the needed code and receives support to complete it, with the recommendation possibly featuring multiple statements in the case of block completion. Second, single method featuring multiple added lines in commit can contribute multiple training/testing instances, each featuring different added line(s) masked. The output of the aforementioned process is dataset of code completion instances assigned to each of the 686 (818) developers, with number of instances ranging from 42 (0) to 52,638 (20,358). For each developer, we order their dataset chronologically and keep the most recent 500 instances (code additions) as test set, while splitting the rest in the 90% least recent instances for training and the 10% most recent instances for validation. Following this procedure we make sure that only data from the past is used to predict the future, resembling real-world scenario. Duplicates across the training and the evaluation/test sets are removed from the training set. After this process, we keep up to 100 developers from each organization, provided that they feature at least 1,000 training instances. This leads to 100 developers for Apache and 36 for Spring."
        },
        {
            "title": "2.2.2 Organization-Specific Datasets. We create the organization-specific datasets by exploiting the code completion\ninstances previously created for the top developers. The idea is that instances crafted starting from the code changes of",
            "content": "the top developers are representative of the implementation tasks usually carried out in the organization. Since also the organization-specific models will be tested on the developer-specific test sets100 (36) different test setswe must create 100 (36) different organization-specific training datasets, to make sure that all code changes used to build the organization-specific training instances are older than those used to build the (developer-specific) testing instances. Fig. 2 depicts the process to create the 100 organization-specific datasets for the Apache organization, one per each developer-specific test set. The same process has been used for Spring, starting from the 36 developer-specific datasets. The 100 developer-specific datasets are represented in Fig. 2 using the ğ·1 . . . ğ·100 notation. The arrow associated to each developer represents the history of their changes, possibly spanning across multiple repositories (older changes to the left). As explained in Section 2.2.1, for the developer-specific dataset the 500 code completion instances derived from the most recent commits are taken as test set, while the rest is split into 90% training and 10% validation. The history of changes is not aligned among the 100 developers. This means, for example, that we have developers who started contributing in 2010 while others who started contributing in 2020. This is the reason why the arrows in Fig. 2 are not aligned. When creating the organization-specific training set for the model that will be tested on ğ·1s test set (see the yellow arrow labeled with ğ‘‚1 training), we take the code completion instances derived from the changes performed"
        },
        {
            "title": "Manuscript submitted to ACM",
            "content": "Why Personalizing Deep Learning-Based Code Completion Tools Matters 9 Fig. 2. Developerand organization-specific datasets. by all 100 developers up to the date of the most recent change in ğ·1s training set (see Fig. 2). In this way, we ensure that the ğ‘‚1 training set only features instances related to changes older than both ğ·1s validation and test set."
        },
        {
            "title": "2.3 Generic Pre-training and Fine-tuning Datasets",
            "content": "The datasets previously described are used to specialize (i.e., fine-tune) the DL models towards the organization/developer of interest. While Code Llama is pre-trained model that can be trained with such datasets, the T5 models call for pre-training and generic fine-tuning phase. This section describes the datasets used for these purposes. We start from the CodeParrot GitHub Code dataset [2], featuring 19.5M Java files, 108GB of data, and including metadata about the repositories from which the code was mined. We create two subsets of it, one excluding all instances mined from Apache repositories and one all those from Spring repositories, to simulate scenario where the generic DL-based code completion tool is not personalized towards the organization of interest (e.g., the code of the organization is not publicly available). Also, we exclude all repositories being forks of others and not having at least 100 commits, 10 contributors and 10 stars. We do this in an attempt to remove toy/personal projects. The dataset from which we excluded Apache repositories featured, after this cleaning, 2,098 projects, while the one from which we excluded Spring repositories was left with 2,057 projects. In both cases, those have been split into 40% for pre-training and 60% for fine-tuning. As done for the developer-specific datasets, we generate training instances at method-level granularity and apply the same filters previously described (e.g., remove test methods, too short/long methods, etc.). For each repository, we randomly collect at most 1,500 valid methods, to avoid biasing the dataset towards repositories with large number of methods."
        },
        {
            "title": "2.3.1 Pre-training Dataset. For pre-training, we collect a total of 1,142,330 (1,091,327) methods. We use the masked\nlanguage modelling training objective [22], randomly masking 15% of tokens of the input method and asking the model",
            "content": "to predict them. We split the dataset into 90% for training and 10% for validation. Fine-tuning Dataset. After removing duplicates with all developer-specific test sets, we end up with 1,080,909 2.3.2 (1,355,885) methods for fine-tuning. To have fair comparison between the specialized DL models and the generic ones (i.e., the ones trained on large dataset not featuring code coming from the organization of interest), we aim to create generic training dataset that is as similar as possible to the developer-specific ones and, as consequence, to the organization-specific one as well. To this end, we compute in the developer-specific datasets (all merged together) the distribution of the number of tokens masked per instance (mean=11, median=8, min=3, max=50 for Apache and"
        },
        {
            "title": "Manuscript submitted to ACM",
            "content": "10 Giagnorio et al. mean=13, median=10, min=3, max=50 for Spring). We tried to mirror this distribution when generating the fine-tuning instances for the generic dataset by randomly masking the end tokens of lines/blocks in its methods. We generate at most three code completion instances per method (i.e., three versions of the same method with different parts masked), obtaining 1,434,598 (1,355,862) code completion instances, which we split into 90% for training and 10% for validation. These instances have mean of 11 (13) masked tokens, with median of 8 (10), minimum of 3 (3) and maximum of 50 (50) tokens, resembling the characteristics of the developer-specific datasets."
        },
        {
            "title": "2.4 Experimental Procedure and Data Analysis",
            "content": "Code Llama is pre-trained model that supports code completion out of the box, while T5 does not. Thus, we start explaining the process used to train the two T5 variants considered in our study. We pre-train the T5 models on the dataset described in Section 2.3.1 using early stopping, with checkpoints saved every epoch, delta of 0.005, and patience of 5 (Baselines Training in Table 1). This means that the models are evaluated on the pre-training validation set every epoch in terms of percentage of correctly predicted masked tokens, and the training stops if gain in performance lower than delta is observed at each 5-epoch interval. Once pre-trained, we fine-tune the T5 variants on the generic fine-tuning dataset described in Section 2.3.2 (Baselines Training). We use the same early stopping procedure previously described for the pre-training with, however, delta of 0.01, since we observed faster increase in the models prediction accuracy during fine-tuning (probably due to the fact that the models were already pre-trained). In this case, the performance of the models at each epoch has been assessed in terms of Exact Match (EM) predictions on the fine-tuning validation set (i.e., the predicted code is identical to the masked one). The pre-trained and fine-tuned T5 models (both small and large) and the already trained Code Llama model publicly available represent our baselines (i.e., generic models trained on large amount of code). We indicate these models with Bğ‘  (T5ğ‘ ğ‘šğ‘ğ‘™ğ‘™ ), Bğ‘™ (T5ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ ) and Bğ‘ (Code Llama). We use the same early stopping procedure described for the fine-tuning to further train Bğ‘  , Bğ‘™ , and Bğ‘ , and obtain their developer-specific and organization-specific versions (Goal 1). For what concerns Bğ‘  , we further fine-tune 272 versions of it: 100 (36) developer-specific and 100 (36) organization-specific versions. Indeed, as explained in Section 2.2.2, even for the organization-specific models we had to create 100 (36) different training sets to avoid using data from the future to predict the past. Note that these are 272 different models, all representing further fine-tunings of Bğ‘  . We use ğ‘  to indicate the Bğ‘  model fine-tuned on the developer-specific dataset of the developer ğ·ğ‘– . Similarly, we the notation Dğ‘– ğ‘  the Bğ‘  model fine-tuned on the organization-specific dataset temporally aligned to the training set of ğ·ğ‘– denote with Oğ‘– (i.e., not including changes performed after the last change in ğ·ğ‘– training set). We compare the performance of both Dğ‘– ğ‘  against the baseline (Bğ‘  ) on ğ·ğ‘– test set (Goal 1), since we want to verify whether the ğ·ğ‘– -specific and the organization-specific models can better predict future ğ·ğ‘– changes as compared to generic code completion model (Bğ‘  ). As evaluation metric, we compute the percentage of EM predictions in the test set. While this metric has been used in several code completion works [11, 12, 18, 19, 30], it represents lower ğ‘  and Oğ‘– bound for the performance of given approach. Indeed, it considers prediction as correct only if the generated code is identical to the one to predict. This means that different but semantically equivalent code generated by the model will be considered wrong. For this reason, we complement our analysis by computing the CrystalBLEU score [23] between the generated predictions and the expected code. CrystalBLEU is variant of the BLEU score [48] tailored for code and has been shown to correctly discriminate similar from dissimilar code 1.94.5 times more effectively when compared to BLEU [23]. We statistically compare the results achieved by the different models. For the EM predictions, we use the McNemars test [46], which is suitable to pairwise compare dichotomous results of two different treatments. We"
        },
        {
            "title": "Manuscript submitted to ACM",
            "content": "Why Personalizing Deep Learning-Based Code Completion Tools Matters 11 complement the McNemars test with the Odds Ratio (OR) effect size. As for the CrystalBLEU, we use the Wilcoxon signed-rank test [60] and the paired Cliffs delta [29] effect size. We perform the same training procedure and data analysis using the Bğ‘™ (T5ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ ) and Bğ‘ (Code Llama) baselines, , compared against Bğ‘™ , and Dğ‘– thus obtaining specialized models Dğ‘– ğ‘ , compared against Bğ‘ (Goal 3). As ğ‘™ said, such an analysis has been performed only for the top-10 developers of each organization in terms of contributed code changes, thus obtaining 40 additional T5ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ and 40 additional Code Llama models10 Apache (10 Spring) developer-specific and 10 Apache (10 Spring) organization-specific. ğ‘ and Oğ‘– and Oğ‘– ğ‘™"
        },
        {
            "title": "2.4.1 Controlling for the Training Data Size Confounding Factor. We study the impact of the amount of training provided\nto the models on their performance (Goal 2): since we found that the organization-specific models work better, there",
            "content": "is question related to the extent to which this is due to the additional training data it has seen as compared to the baselines. Indeed, since the organization-specific models are obtained via further fine-tuning the baseline, they benefit from more training data. To control for such confounding factor, we further fine-tune Bğ‘  in two different ways: (i) with an organization-specific dataset capped to the same size of the developer-specific dataset, leading to Organization subset models, and (ii) with generic dataset capped to the same size of the organization-specific dataset, leading to Baseline+ models. By comparing the performance of developer-specific with Organization subset models, since both have seen the same amount of training instances, we can understand the impact of the developer-specific data on the models performance. Similarly, by comparing the performance of organization-specific with Baseline+ models, we can understand the impact of the organization-specific data on the models performance. To create the generic dataset (used to fine-tune the Baseline+ models), we mined 2,354 (2,781) additional Java open source repositories hosted on GitHub which are not from Apache or Springthus avoiding overlap with the organization-specific datasets. These repositories have been collected using the platform by DabiÄ‡ et al. [21], querying it for all Java repositories having at least 10 contributors, 10 stars, and 100 commits. We processed them using the same procedure previously described for the generic fine-tuning dataset (Section 2.3.2). In practice, we created 20 organization subset training sets and 20 generic training sets, one for each of the top-10 developers of both organizations. This was done to ensure that the training sets associated to developer ğ· would only contain instances whose date was before the first date of ğ·s test set, i.e., to avoid using data from the future to predict the past. Due to the further training cost, these analyses have been performed only with the T5ğ‘ ğ‘šğ‘ğ‘™ğ‘™ model. Investigating Why More Specific Training Data Help. Since our findings show that more specific training data 2.4.2 help the model, we also perform an additional analysis aimed at investigating the information items shared between the three training sets (i.e., the ones used by the baseline Bğ‘  , by the developer-specific, and by the organization-specific) and the 136 developer-related test sets of the two organizations. With information items we refer to: Domain coverage: The extent to which the domain of the data present in the test sets is covered in the training sets. Method signatures are good proxy to this end, since they include information about supported operations (e.g., via method names) and input/output data (e.g., via return types and method arguments). We thus compute the percentage of instances in the test sets whose method signature appears in the training sets. Vocabulary coverage: The extent to which the vocabulary used in the test sets is covered in the training sets. We focus on literals (e.g., strings and numbers) and identifiers (e.g., method and variable/constant names), reporting the percentage of these elements in the test sets which are also present in the training sets."
        },
        {
            "title": "Manuscript submitted to ACM",
            "content": "12 Giagnorio et al. Relevance of the Training Data: The extent to which the vocabulary learned during training is actually used at inference time (i.e., present in the test sets). We compute the percentage of identifiers and literals in the training sets which can be found in the test sets."
        },
        {
            "title": "2.4.3 Cost-effectiveness Analysis. We also run an additional analysis aimed at understanding the cost-effectiveness of\nthe â€œpersonalizedâ€ fine-tuning (Goal 4). Indeed, the personalized fine-tuning implies a training cost that the company",
            "content": "would not have by just downloading larger code-completion model already trained for such task, and maybe exhibiting even better performance than the smaller, personalized model. We perform this cost-effectiveness analysis between T5ğ‘ ğ‘šğ‘ğ‘™ğ‘™ with personalized fine-tuning and T5ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ (being 12.5 times bigger) with generic fine-tuning, as representative of an already generic trained model which can be downloaded and used with zero training cost. We excluded Code Llama from this part of the study since in our setting it makes no sense to consider Code Llama as representative of general-purpose model that company could download and use out of the box, since it likely saw the code of the two organizations (Apache and Spring) subject of our study during training. Thus, any sort of cost-effectiveness analysis comparing non fine-tuned Code Llama versus fine-tuned and personalized T5 would not allow to observe the actual advantage (if any) provided by personalization. Instead, by considering the T5ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ pre-trained and fine-tuned on generic dataset as an example of trained model that company can just download and use out of the box, we can guarantee that (i) this is model that has not seen the companys code, since we excluded that code from the pre-training and fine-tuning datasets; and (ii) we are still considering model that is 12.5 times bigger than T5ğ‘ ğ‘šğ‘ğ‘™ğ‘™ , thus allowing to observe if much smaller model with personalized fine-tuning is able to reach similar performance of much larger (non personalized) model. Since the T5ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ has only been assessed on the top-10 developers of each of the two organizations, this cost-effectiveness analysis has been performed on these 20 developers. Also, among the two personalizations of T5ğ‘ ğ‘šğ‘ğ‘™ğ‘™ (i.e., developer-specific and organization-specific), we considered the organization-specific which is more expensive (larger training sets) but achieves better performance (as we will show, aligned to those of T5ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ ). To present reliable data we computed the cost of renting GPUs in Google Cloud, for both the fine-tuning of T5ğ‘ ğ‘šğ‘ğ‘™ğ‘™ and the inference performed with both models. We considered the training cost of both the cheapest (146.3k training instances) and the most expensive (888k training instances) organization-specific T5ğ‘ ğ‘šğ‘ğ‘™ğ‘™ . For the training of T5ğ‘ ğ‘šğ‘ğ‘™ğ‘™ and the inference of both T5ğ‘ ğ‘šğ‘ğ‘™ğ‘™ and T5ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ , we rented 1 Nvidia T4 GPU with 16GB of memory. To compute the inference cost, we generate the same 1,000 predictions with each model, and then compute the average cost of one prediction. Clearly, while the GPU used for inference is the same for all models, T5ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ requires longer inference time (higher cost). We discuss the cost that an organization would have with both models given different number of inferences performed by its developers, presenting break-even points in the best- (i.e., lowest number of training instances for the organization-specific fine-tuning, 146.3k) and worst-case (highest number, 888k) scenario. In other words, we discuss after how many inferences the company would amortize the fine-tuning cost of the personalized smaller model and start saving money."
        },
        {
            "title": "3 RESULTS DISCUSSION",
            "content": "Tables 2 and 4 report the performance achieved by the baseline Bğ‘  (i.e., generic T5ğ‘ ğ‘šğ‘ğ‘™ğ‘™ ), the developer-specific and the organization-specific models in terms of exact matches (EM %) on the 100 developers test sets of Apache and the 36 developers test sets of Spring, respectively. In addition, for the columns referring to the personalized models we also report the number of instances in the corresponding training sets (N), the delta in EM predictions with"
        },
        {
            "title": "Manuscript submitted to ACM",
            "content": "Why Personalizing Deep Learning-Based Code Completion Tools Matters"
        },
        {
            "title": "R\nO",
            "content": "5 2 2 . 3 2 1 . 5 2 2 1 . % 0 0 8 1 . 2 8 2 . 1 3 2 . % 0 0 4 . % 0 4 3 . 0 0 8 8 . % 0 2 2 5 . 0 7 2 . 5 0 1 . 3 4 2 . 7 4 2 . 0 5 0 . 0 0 2 . 9 6 1 . 7 1 2 . 7 7 2 . 7 6 2 . 0 0 2 . 2 1 1 . 7 2 8 . 0 5 2 . 8 3 5 . 9 5 1 . 0 8 2 . 0 8 2 . 2 1 2 . 1 1 2 . 0 8 2 . 8 0 2 . 9 6 2 . 0 0 1 . 4 9 1 . 2 8 1 . 1 9 2 . 0 5 2 . 2 9 2 . 6 4 2 . 9 7 0 . 0 5 1 . 6 8 1 . 7 6 1 . 7 1 5 . 9 0 2 . 4 1 2 . 3 3 2 . % 0 4 3 . % 0 2 0 . % 0 0 4 . % 0 0 5 . % 0 2 3 . % 0 4 3 . % 0 2 2 . % 0 8 2 . % 0 6 4 . % 0 0 3 . % 0 0 4 . % 0 4 0 . % 0 0 6 1 . % 0 6 6 . % 0 0 7 . % 0 0 2 . % 0 6 3 . % 0 6 3 . % 0 6 3 . % 0 0 2 . % 0 6 3 . % 0 8 2 . % 0 4 4 . % 0 0 0 . % 0 4 3 . % 0 8 2 . % 0 2 4 . % 0 6 3 . % 0 6 4 . % 0 8 3 . % 0 6 0 . % 0 6 1 . % 0 4 2 . % 0 6 1 . % 0 0 5 . % 0 4 2 . % 0 2 3 . % 0 4 2 . 0 0 8 5 . % 0 0 7 5 . 9 1 2 . 0 4 3 . 0 8 7 . 7 8 2 . % 0 8 3 . % 0 8 4 . % 0 8 6 . % 0 6 5 . 8 3 5 2 . % 0 0 9 3 . % 0 0 3 . % 0 0 1 . + + + + + + + + + + - + + + + + + + + + + + + + + + + + + + + + + + + - + + + + + + + + + + + + + . 8 9 2 . 0 9 2 . 8 0 5 . 2 3 3 . 8 2 3 . 8 5 7 . 4 9 3 . 2 8 2 . 8 0 3 . 6 7 3 . 6 3 4 . 2 5 2 . 0 2 3 . 6 8 2 . 4 7 2 . 6 5 2 . 2 1 3 . 6 5 2 . 0 7 3 . 0 2 4 . 2 8 2 . 6 1 2 . 6 7 2 . 4 8 2 . 4 1 4 . 4 8 2 . 8 7 2 . 0 3 2 . 2 6 2 . 8 1 3 . 8 8 3 . 8 9 1 . 2 7 2 . 6 5 2 . 2 3 3 . 0 6 3 . 0 3 2 . 2 5 2 . 8 0 3 . 4 6 2 . 2 7 4 . 0 5 2 . 6 8 3 . 6 3 3 . 4 3 8 . 8 7 2 . 4 3 2 . 2 9 3 . 8 0 3 . 2 5 6 . 8 7 7 8 . 3 8 0 4 . 2 6 4 8 . 0 1 1 2 . 6 8 6 3 . 8 6 2 7 . 0 6 8 5 6 5 7 . . 0 0 4 6 . 7 2 3 7 . 2 4 8 6 . 4 3 0 5 . 4 3 5 2 . 2 9 5 6 . 1 4 0 3 . 0 6 1 8 . 7 3 5 3 . 0 7 1 6 . 7 6 5 7 . 5 7 1 1 . 8 9 9 6 . 4 0 9 7 . 3 1 0 7 . 1 8 0 7 . 2 0 8 8 . 1 3 3 7 . 4 5 2 7 . 1 5 3 8 . 4 5 3 7 5 9 . . 4 0 8 7 . 8 2 6 7 . 5 7 8 7 . 8 8 4 6 . 6 7 6 8 . 6 3 9 1 . 0 4 6 1 . 7 0 9 4 . 0 9 8 5 1 3 2 . . 1 8 0 9 . 9 0 3 7 . 1 5 6 6 . 3 1 8 6 . 7 0 2 7 . 7 8 9 7 . 9 9 6 7 . 5 7 8 8 . 4 9 7 4 . 6 2"
        },
        {
            "title": "R\nO",
            "content": "3 7 0 . 1 3 1 . 1 4 1 . 4 6 1 . 8 0 1 . % 0 8 0 . % 0 0 1 . % 0 4 2 . % 0 4 1 . % 0 2 0 . . 5 7 3 5 % 0 2 2 4 . 2 9 2 . 5 3 0 . 9 2 3 . 9 1 1 . 4 6 0 . 4 0 1 . 0 5 1 . 2 1 2 . 7 1 2 . 0 2 1 . 5 9 0 . 8 5 0 . 5 2 4 . 9 1 2 . 4 6 2 . 4 5 0 . 5 2 1 . 3 3 1 . 3 8 1 . 9 7 0 . 6 3 2 . 0 5 2 . 0 0 5 . 2 8 0 . 5 5 2 . 3 7 1 . 2 6 1 . 6 7 0 . 7 0 2 . 5 2 1 . 5 7 0 . 5 7 1 . 6 8 0 . 8 7 1 . 0 5 1 . 3 4 1 . 7 5 2 . 7 6 0 . 2 9 5 . 1 6 0 . 7 7 1 . 5 4 1 . 7 1 2 . % 0 0 5 . % 0 0 3 . % 0 2 3 . % 0 8 0 . % 0 0 1 . % 0 2 0 . % 0 4 1 . % 0 6 3 . % 0 8 2 . % 0 4 0 . % 0 2 0 . % 0 0 1 . % 0 2 5 . % 0 8 3 . % 0 6 3 . % 0 2 1 . % 0 4 0 . % 0 8 0 . % 0 0 2 . % 0 6 0 . % 0 0 3 . % 0 6 3 . % 0 4 2 . % 0 8 0 . % 0 4 3 . % 0 6 1 . % 0 0 1 . % 0 8 0 . % 0 2 3 . % 0 0 1 . % 0 6 0 . % 0 2 1 . % 0 4 0 . % 0 4 1 . % 0 8 0 . % 0 6 0 . % 0 2 2 . % 0 0 1 . % 0 4 1 . % 0 0 2 . % 0 0 1 . % 0 8 2 . % 0 8 2 1 . . 6 3 0 % 0 6 0 2 . - + + + + + + - + + - + + + + + - - + + + - + + + - + + + - + + + - + + - + - + + + + - + - + + + + . 0 6 . 0 9 2 . 2 5 3 . 6 0 . 6 9 2 . 8 5 6 . 0 1 . 0 5 2 . 0 0 3 . 4 3 . 8 5 4 . 0 2 2 . 2 1 . 4 9 2 . 6 5 2 . 0 3 . 0 7 2 . 2 4 2 . 2 6 . 2 9 3 . 8 4 2 . 4 8 . 4 4 2 . 6 5 2 . 8 9 . 8 5 2 . 2 7 2 . 8 3 . 2 4 2 . 0 1 3 . 8 8 . 6 8 1 . 0 4 2 . 2 1 . 8 1 3 . 2 3 3 . 0 3 . 8 4 2 . 0 8 2 . 2 6 . 0 3 4 . 2 3 2 . 6 7 . 2 0 3 . 2 9 3 . 6 2 . 6 0 2 . 4 3 3 . 0 8 . 8 6 4 6 5 . 5 5 . 5 5 . 5 5 . 3 5 . 2 5 . 1 5 . 1 5 . 9 4 . 8 4 . 8 4 . 6 4 . 6 4 . 6 4 . 6 4 . 6 4 . 5 4 . 5 4 . 5 4 . 4 4 . 4 4 . 3 4 . 3 4 . 3 4 . 3 4 . 2 4 . 1 4 . 1 4 . 0 4 . 9 3 . 9 3 . 9 3 . 9 3 . 8 3 . 8 3 . 8 3 . 8 3 . 8 3 . 7 3 . 7 3 . 7 3 . 7 3 . 6 3 . 6 3 . 5 3 . 4 3 . 4 3 . 3 3 . 1 3 . 1 2 . . 8 6 2 . 0 8 2 . 8 2 . 2 9 2 . 4 9 2 . 6 3 . 0 6 3 . 0 8 2 . 8 6 . 6 2 3 . 8 6 4 . 8 1 . 8 9 2 . 8 5 2 . 8 2 . 6 2 2 . 2 7 2 . 2 5 . 0 1 2 . 4 5 3 . 2 1 . 6 9 1 . 0 4 2 . 8 4 . 8 7 3 . 4 6 2 . 2 4 . 2 0 2 . 8 1 2 . 8 1 . 4 5 3 . 0 7 1 . 0 3 . 0 2 2 . 6 8 2 . 2 2 . 6 3 2 . 6 3 2 . 4 8 . 8 4 2 . 2 2 4 . 6 2 . 4 5 3 . 2 1 3 . 4 6 . 0 4 2 . 6 8 1 . 4 2 . 2 5 2 . 2 6 2 1 5 2 3 5 4 5 5 5 6 5 7 5 8 9 5 0 6 1 6 2 6 3 6 4 5 6 6 6 7 6 8 6 9 6 0 1 7 2 7 3 7 4 7 5 7 6 7 7 8 7 9 7 0 8 1 8 2 3 8 4 8 5 8 6 8 7 8 8 9 8 0 9 1 9 2 9 3 9 4 5 9 6 9 7 9 8 9 9 9 0"
        },
        {
            "title": "R\nO",
            "content": "7 5 8 . 6 3 3 . 5 8 1 . 5 6 3 . 0 3 2 . 3 9 1 . 0 2 2 4 . 3 2 2 . 0 3 1 1 . 7 6 1 . 5 2 2 . % 0 6 0 1 . % 0 2 5 . % 0 2 2 . % 0 0 9 . % 0 6 2 . % 0 6 2 . % 0 2 1 4 . % 0 2 3 . % 0 6 0 2 . % 0 0 2 . % 0 0 2 . . 0 0 3 4 1 % 0 4 8 2 . 0 0 4 . 6 5 3 . 2 2 1 . % 0 0 6 . % 0 6 4 . % 0 4 0 . 2 6 8 2 . % 0 2 4 4 . 0 6 2 . 9 6 1 . 0 6 1 . 0 0 4 . 0 4 2 . 0 0 1 . 0 0 3 . 8 7 1 . 8 7 2 . 5 2 3 . % 0 8 4 . % 0 2 2 . % 0 8 1 . % 0 8 4 . % 0 2 4 . % 0 0 0 . % 0 0 4 . % 0 8 2 . % 0 2 3 . % 0 4 5 . 6 7 1 . 0 0 3 . 0 0 2 . 7 6 3 . 5 4 1 . 5 9 0 . 5 2 4 . 7 6 3 . 2 3 1 . 8 5 3 . % 0 6 2 . % 0 6 3 . % 0 4 3 . % 0 8 4 . % 0 0 4 . % 0 2 0 . % 0 8 7 . % 0 8 4 . % 0 2 1 . % 0 2 6 . 3 3 0 3 . % 0 2 5 3 . 2 6 7 2 . % 0 6 2 4 . 0 8 0 . 2 7 0 . 0 5 2 . 5 4 8 . 0 2 4 . 6 4 2 . 9 8 3 . 3 3 5 . 9 6 2 . 7 6 6 . 7 1 1 . 8 5 2 . % 0 8 0 . % 0 6 1 . % 0 2 4 . % 0 4 6 1 . % 0 4 6 . % 0 8 3 . % 0 2 5 . % 0 2 5 . % 0 4 4 . % 0 0 1 . % 0 8 3 . % 0 0 7 1 . + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + - + + + + + - - + + + + + + + + + + . 8 1 6 . 0 7 3 . 2 8 2 . 2 4 4 . 0 4 2 . 2 7 3 . 0 0 7 . 2 2 3 . 4 8 4 . 6 3 3 . 8 0 2 . 0 9 6 . 6 0 3 . 0 7 3 . 0 1 2 . 6 6 7 . 2 7 2 . 0 4 3 . 0 8 2 . 6 5 3 . 4 5 3 . 2 0 3 . 4 3 2 . 8 7 2 . 6 2 2 . 8 1 3 . 0 6 5 . 0 6 2 . 8 0 3 . 6 8 2 . 4 2 3 . 0 2 4 . 4 2 3 . 4 0 5 . 2 7 3 . 0 4 3 . 4 8 2 . 4 8 7 . 2 2 3 . 0 9 2 . 2 3 3 . 6 8 3 . 0 8 2 . 2 0 3 . 2 1 3 . 6 9 3 . 0 9 2 . 6 1 5 . 2 8 2 . 6 6 2 . 0 8 8 8 . 0 6 5 5 . 8 0 3 8 . 3 7 4 7 . 5 0 4 5 . 4 1 9 7 . 8 4 2 5 . 5 0 5 8 . 9 0 8 5 . 3 6 4 1 . 3 0 8 8 . 1 2 9 8 . 3 2 2 8 . 7 8 3 7 . 0 6 8 8 . 2 3 1 7 . 4 0 7 6 . 7 1 4 7 . 5 9 1 8 . 5 0 2 7 . 8 1 7 6 . 4 9 9 2 . 5 4 4 5 . 4 0 4 4 . 0 1 0 9 . 3 9 0 6 . 4 7 4 4 . 0 8 5 8 . 7 5 3 6 . 3 0 5 6 . 7 9 4 8 . 9 5 0 2 6 9 . . 1 7 8 4 . 7 8 0 8 . 1 4 3 4 . 3 0 8 8 . 4 3 9 7 . 2 9 1 1 . 0 1 5 2 7 5 9 . . 8 8 1 4 . 5 5 9 4 . 0 3 9 4 . 1 0 1 7 . 2 8 6 6 . 1 7 5 8 . 2 7 0 8 . 0 5 8 2 . 3 4"
        },
        {
            "title": "R\nO",
            "content": "3 3 6 . 8 2 1 . 2 6 0 . 3 8 1 . 2 4 0 . 0 1 2 . 7 6 9 . 0 0 1 . 1 4 6 . 7 5 1 . 0 6 1 . % 0 6 9 . % 0 4 1 . % 0 0 1 . % 0 8 3 . % 0 8 2 . % 0 2 2 . % 0 0 0 . % 0 2 1 3 . % 0 4 8 1 . % 0 6 1 . % 0 2 1 . 2 6 2 . 0 7 1 . 0 5 1 . % 0 6 2 . % 0 4 1 . % 0 8 0 . 0 5 4 2 . % 0 2 8 2 . 7 6 4 2 . % 0 6 2 4 . 8 1 1 . 2 9 1 . 4 1 1 . 0 4 2 . 4 9 1 . 7 6 0 . 0 2 1 . 0 0 1 . 5 2 2 . 3 3 2 . % 0 8 0 . % 0 4 2 . % 0 4 0 . % 0 4 1 . % 0 2 3 . % 0 0 1 . % 0 4 0 . % 0 0 0 . % 0 0 3 . % 0 2 3 . 0 0 2 . 0 0 1 . 8 7 1 . 7 2 2 . 0 0 1 . 7 5 1 . 3 6 1 . 4 9 0 . 0 5 0 . 5 4 2 . % 0 2 1 . % 0 0 0 . % 0 4 1 . % 0 8 2 . % 0 0 0 . % 0 6 1 . % 0 4 2 . % 0 2 0 . % 0 0 2 . % 0 2 3 . 0 5 1 1 . % 0 2 5 2 . 3 8 6 1 . % 0 0 8 3 . 7 5 0 . 5 3 0 . 5 1 1 . 0 2 5 . 8 7 1 . 6 5 1 . 7 7 1 . 7 2 1 . 8 3 1 . 6 9 1 . 4 6 1 . 2 3 1 . % 0 8 1 . % 0 0 3 . % 0 6 0 . % 0 6 2 1 . % 0 4 1 . % 0 8 1 . % 0 0 2 . % 0 8 0 . % 0 0 1 . % 0 4 4 . % 0 8 1 . % 0 2 1 . + + - + - + + + + + + + + + + + + + + + - + + + + + + + + + - - + + - - + + + + + + + + + + . 8 0 6 . 2 3 3 . 0 5 . 0 9 3 . 6 8 1 . 8 6 . 0 0 6 . 0 9 2 . 2 6 . 2 3 3 . 0 0 2 . 8 8 . 2 7 2 . 8 3 3 . 4 1 . 0 5 7 . 2 3 2 . 2 4 . 6 6 2 . 2 2 3 . 4 4 . 2 9 2 . 8 9 1 . 0 5 . 4 2 2 . 6 9 2 . 0 6 . 6 4 2 . 2 7 2 . 6 6 . 4 0 3 . 0 8 3 . 2 4 . 0 5 4 . 2 2 3 . 8 0 . 4 5 2 . 8 3 7 . 2 1 . 6 7 2 . 6 9 2 . 8 4 . 0 3 2 . 2 8 2 . 0 8 . 2 5 3 . 6 5 2 . 0 9 . 0 9 2 . 0 4 2 6 6 4 . 2 3 2 . 3 0 2 . 2 9 1 . 6 8 1 . 8 7 1 . 5 7 1 . 0 7 1 . 9 5 1 . 8 5 1 . 2 5 1 . 8 3 1 . 4 2 1 . 2 1 1 . 4 0 1 . 2 0 1 . 1 0 1 . 8 9 . 8 9 . 6 9 . 5 9 . 3 9 . 3 9 . 9 8 . 7 8 . 6 8 . 6 8 . 5 8 . 1 8 . 0 8 . 0 8 . 9 7 . 9 7 . 6 7 . 5 7 . 5 7 . 0 7 . 9 6 . 7 6 . 7 6 . 6 6 . 6 6 . 5 6 . 5 6 . 5 6 . 4 6 . 1 6 . 1 6 . 8 5 . 6 5 . . 2 1 5 . 8 1 . 0 6 2 . 2 5 3 . 4 1 . 6 4 3 . 8 8 2 . 0 9 . 8 7 2 . 6 1 3 . 8 8 . 6 0 4 . 6 4 2 . 4 2 . 6 0 2 . 4 2 3 . 4 2 . 8 1 3 . 2 6 2 . 8 0 . 2 1 3 . 2 0 3 . 4 9 . 0 5 2 . 4 9 1 . 4 6 . 8 0 2 . 4 3 2 . 2 7 . 2 5 2 . 6 7 2 . 0 8 . 6 2 3 . 6 2 4 . 4 2 . 8 2 3 . 2 2 2 . 8 5 . 0 3 3 . 6 0 3 . 0 9 . 2 2 2 . 6 1 2 . 4 6 . 0 6 2 . 4 4 3 . 6 4 . 6 4 3 . 2 7 2 . 8 2 1 2 3 4 5 7 8 9 0 1 1 1 2 3 1 4 1 5 1 6 1 7 1 8 9 1 0 2 1 2 2 2 3 2 4 5 2 6 2 7 2 8 2 9 2 0 1 3 2 3 3 3 4 3 5 3 6 7 3 8 3 9 3 0 4 1 4 2 3 4 4 4 5 4 6 4 7 4 8 9 4"
        },
        {
            "title": "Manuscript submitted to ACM",
            "content": ". a o e d l s e b e e e b a e o i p ) ( a a . 2 a n a a Î” % Î” % % e e ğ‘ "
        },
        {
            "title": "D\nI",
            "content": ". n a a Î” % N Î” % % r l D ğ‘ "
        },
        {
            "title": "B\ne\nn\ni\nl\ne\ns\na\nB",
            "content": "D . 14 Giagnorio et al. 5 0 4 9 4 8 4 7 4 6 4 4 4 4 3 4 2 4 1 4 0 3 3 8 3 7 3 6 3 5 3 4 3 3 2 3 1 3 0 2 9 2 8 2 2 6 2 5 2 4 2 3 2 2 2 2 0 1 9 1 8 1 7 1 6 1 1 4 1 3 1 2 1 1 1 0 8 7 6 5 4 2 1 . 5 6 . 5 8 . 6 1 . 6 1 . 6 4 . 6 5 . 6 5 . 6 5 . 6 6 . 6 6 . 6 7 . 6 7 . 6 9 . 7 0 . 7 5 . 7 5 . 7 6 . 7 9 . 7 9 . 8 0 . 8 0 . 8 1 . 8 5 . 8 6 . 8 6 . 8 7 . 8 9 . 9 3 . 9 3 . 9 5 . 9 6 . 9 8 . 9 8 . 1 0 1 . 1 0 2 . 1 0 4 . 1 1 2 . 1 2 4 . 1 3 8 . 1 5 2 . 1 5 8 . 1 5 9 . 1 7 0 . 1 7 5 . 1 7 8 . 1 8 6 . 1 9 2 . 2 0 3 . 2 3 2 . 4 6 6 N 1 8 6 5 . 2 3 0 8 . 3 6 6 4 . 1 7 8 5 . 2 2 9 7 . 2 1 9 7 . 2 2 9 3 . 2 1 0 4 . 3 3 3 6 . 2 4 1 0 . 2 1 2 7 . 2 3 6 0 . 6 9 4 3 . 2 2 5 9 . 2 4 2 6 . 2 2 4 4 . 2 8 0 2 . 2 4 5 3 . 2 3 8 4 . 1 9 1 7 . 2 1 2 2 . 2 3 1 8 . 1 8 4 3 . 4 5 9 6 . 2 8 2 3 . 2 0 1 8 . 2 0 5 8 . 1 8 1 6 . 2 1 2 1 . 2 5 0 9 . 1 6 6 2 . 1 8 3 9 . 2 3 8 4 . 2 0 6 8 . 7 2 9 5 . 1 3 8 2 . 2 1 2 8 . 2 1 8 0 . 6 2 9 2 . 1 7 8 1 . 3 0 8 8 . 4 2 6 9 . 1 8 3 8 . 5 7 2 7 . 2 3 1 2 . 1 9 0 4 . 2 8 0 7 . 1 9 8 2 . 2 8 7 5 . 3 8 2 4 . % + + + + + + + + + + - - + + - - + + + + + + + + + + + - + + + + + + + + + + + + + + - + + - + - + + . 0 5 1 % . 2 4 2 % . 5 2 4 % . 1 4 4 % . 2 8 9 % . 1 3 2 % . 2 8 2 % . 2 4 2 % . 1 6 7 7 % . 1 5 9 % . 2 5 7 % . 2 2 5 % . 4 7 4 3 % . 5 4 8 % . 1 8 8 % . 1 0 2 % . 3 1 5 % . 2 1 1 % . 1 2 6 % . 4 4 5 % . 2 2 2 % . 1 1 8 % . 1 9 3 % . 2 6 3 1 % . 5 0 7 % . 4 2 7 % . 1 1 4 % . 0 2 2 % . 0 2 8 % . 2 2 9 % . 2 2 8 % . 1 2 0 % . 4 1 6 % . 1 6 1 % . 5 6 4 8 % . 2 6 4 % . 1 6 3 % . 2 1 6 % . 4 1 1 5 % . 1 0 7 % . 3 7 5 % . 2 2 8 1 % . 3 2 8 3 % . 0 8 8 % . 4 3 2 % . 3 3 4 % . 5 5 9 % . 1 5 1 % . 3 6 3 % . 1 8 7 7 % Î” . 0 0 0 . 0 0 . 0 0 8 . 0 0 3 . 0 0 . 0 0 2 . 0 0 5 . 0 0 . 0 2 4 . 0 0 1 . 0 0 . 0 0 2 . 0 5 9 . 0 0 . 0 0 3 . 0 0 1 . 0 0 . 0 0 3 . 0 0 0 . 0 0 . 0 0 3 . 0 0 1 . 0 0 . 0 3 3 . 0 0 7 . 0 0 . 0 0 2 . 0 0 1 . 0 0 . 0 0 2 . 0 0 2 . 0 0 . 0 0 5 . 0 0 4 . 0 6 . 0 0 7 . 0 0 1 . 0 0 . 0 5 3 . 0 0 1 . 0 0 . 0 2 9 . 0 0 2 . 0 4 . 0 0 4 . 0 0 5 . 0 0 . 0 0 1 . 0 0 6 . 0 2 ğ¸ ğ‘† 6 0 4 3 . 2 8 5 0 . 8 0 7 2 . 8 5 7 1 . 6 6 8 2 . 7 1 0 1 . 4 9 3 0 . 4 9 5 5 . 4 1 8 8 . . 9 5 7 2 5 1 0 . 1 1 9 2 . 7 9 3 4 . 8 8 0 3 . 4 3 4 1 . 8 0 8 7 . 4 8 7 1 . . 8 9 6 2 0 5 9 . 8 4 9 7 . 6 5 0 3 . 6 3 5 7 . 8 5 8 0 . 4 4 7 4 . 6 0 9 3 . 9 0 1 0 . 4 4 0 4 . 5 4 4 5 . 2 9 9 4 . 6 7 1 8 . 7 2 0 5 . 8 1 9 5 . 7 4 1 7 . 6 7 0 4 . 7 1 3 2 . 8 8 6 0 . 7 3 8 7 . 8 2 2 3 . 8 9 2 1 . 8 8 0 3 . 1 4 6 3 . 5 8 0 9 . 8 5 0 5 . 5 2 4 8 . 7 9 1 4 . 5 4 0 5 . 7 4 7 3 . 8 3 0 8 . 5 5 6 0 . 8 8 8 0 . 2 1 9 6 . 2 4 1 . 4 9 9 6 . 2 3 1 4 . 2 8 7 . 2 7 9 5 . 2 7 1 5 . 2 8 7 . 3 9 0 2 . 2 8 6 4 . 2 3 8 . 2 6 1 5 . 7 3 8 8 . 2 6 3 . 2 7 9 3 . 2 8 5 1 . 3 3 7 . 2 4 0 8 . 3 4 4 3 . 2 2 6 . 2 6 0 5 . 2 7 9 0 . 2 3 0 . 5 7 6 9 . 3 0 6 3 . 1 9 4 . 2 4 5 7 . 2 2 8 8 . 2 2 7 . 3 0 4 0 . 2 2 5 9 . 2 3 4 . 2 6 6 3 . 2 2 2 7 . 7 5 1 . 1 4 6 1 . 2 7 9 2 . 2 8 3 . 6 6 9 7 . 1 9 8 1 . 2 9 3 . 4 7 5 3 . 2 5 2 6 . 6 9 0 . 2 5 3 5 . 2 4 9 0 . 3 5 0 . 2 6 7 0 . 3 0 8 2 . 4 1 2 . % + + + + + + + + + + - + + + + + + - + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + . 5 2 6 % . 0 2 4 % . 2 0 1 6 % . 1 0 8 1 % . 6 7 3 % . 8 1 4 % . 7 6 6 % . 9 9 3 % . 2 3 2 6 % . 7 3 9 % . 1 1 8 % . 0 5 1 % . 5 2 8 3 % . 9 0 5 % . 2 0 0 % . 6 8 1 % . 1 0 6 8 % . 0 0 7 % . 5 5 1 % . 8 3 4 % . 5 3 9 % . 6 5 3 % . 4 2 6 % . 3 9 2 4 % . 7 4 7 % . 4 1 6 % . 5 1 3 % . 4 5 0 % . 1 6 3 % . 8 0 3 % . 7 5 2 % . 6 0 8 % . 6 2 8 % . 4 6 0 % . 5 8 9 0 % . 3 2 1 % . 8 5 1 % . 8 2 8 % . 4 6 5 2 % . 3 4 7 % . 2 0 3 % . 2 9 1 7 % . 4 7 3 0 % . 5 5 6 % . 5 5 9 % . 5 2 2 % . 1 3 9 8 % . 5 3 7 % . 8 6 8 % . 2 2 4 5 % Î” . 0 0 7 . 0 0 1 . 0 2 8 . 0 1 1 . 0 1 5 . 0 1 1 . 0 1 2 . 0 1 4 . 0 3 2 . 0 1 1 . 0 0 2 . 0 0 2 . 0 6 6 . 0 1 2 . 0 0 3 . 0 0 9 . 0 1 4 . 0 0 1 . 0 0 6 . 0 0 9 . 0 0 6 . 0 0 9 . 0 0 6 . 0 4 9 . 0 1 1 . 0 0 6 . 0 0 8 . 0 0 6 . 0 0 3 . 0 1 3 . 0 1 0 . 0 0 9 . 0 1 1 . 0 0 6 . 0 7 1 . 0 1 0 . 0 1 4 . 0 1 1 . 0 6 3 . 0 0 7 . 0 0 4 . 0 4 1 . 0 0 8 . 0 6 1 . 0 0 7 . 0 0 9 . 0 1 9 . 0 0 7 . 0 1 3 . 0 2 9 ğ¸ ğ‘† 1 0 0 9 9 8 9 7 9 6 9 5 9 4 9 9 2 9 1 9 0 8 9 8 8 8 8 6 8 5 8 4 8 3 8 2 8 8 0 7 9 7 8 7 7 7 6 7 7 4 7 3 7 2 7 1 7 0 6 6 8 6 7 6 6 6 5 6 4 6 6 2 6 1 6 0 5 9 5 8 5 5 6 5 5 5 4 5 3 5 2 5 . 2 1 . 3 1 . 3 3 . 3 4 . 3 4 . 3 5 . 3 6 . 3 6 . 3 7 . 3 7 . 3 7 . 3 7 . 3 8 . 3 8 . 3 8 . 3 8 . 3 8 . 3 9 . 3 9 . 3 9 . 3 9 . 4 0 . 4 1 . 4 1 . 4 2 . 4 3 . 4 3 . 4 3 . 4 3 . 4 4 . 4 4 . 4 5 . 4 5 . 4 5 . 4 6 . 4 6 . 4 6 . 4 6 . 4 6 . 4 8 . 4 8 . 4 9 . 5 1 . 5 1 . 5 2 . 5 3 . 5 5 . 5 5 . 5 5 . 5 6 . 4 5 8 5 . 2 5 4 5 . 2 4 0 . 2 0 8 8 . 2 2 5 6 . 3 5 5 1 9 4 5 . . 2 2 8 9 . 1 5 2 . 2 0 8 4 . 1 9 7 9 . 1 6 5 . 1 9 4 5 . 1 8 0 9 . 2 3 6 . 2 9 3 8 . 2 1 1 1 . 2 0 7 . 1 9 9 9 . 2 2 3 9 2 5 2 6 . . 1 8 3 0 . 2 1 4 0 . 2 7 2 . 2 0 9 2 . 2 4 9 4 1 7 7 4 . . 1 6 0 0 . 1 7 8 1 . 1 8 8 . 2 7 9 8 . 2 3 3 6 . 2 0 2 . 1 9 4 4 2 1 4 2 . . 1 9 0 . 2 5 2 6 . 2 0 9 0 . 1 8 2 1 8 8 8 . 2 5 0 7 . . 1 6 6 . 2 1 2 7 . 2 8 4 6 . 6 7 7 . 1 8 1 5 . 2 0 1 9 . 3 3 0 . 2 2 9 4 . 1 9 1 4 % + + + + + + - + - + + + + - + + + + + + + + + + + + + + + + + + - + + + + + - - + + - + + + + + - + . 2 7 6 4 % . 3 6 7 % . 2 7 6 % . 1 8 1 % . 0 8 8 % . 2 2 3 0 % . 1 8 1 % . 3 8 7 % . 0 5 2 % . 0 4 9 % . 1 0 9 % . 0 5 7 % . 1 1 9 % . 0 1 3 % . 2 1 0 % . 5 1 7 % . 2 2 5 % . 1 6 9 % . 1 9 8 % . 3 3 6 % . 0 5 3 % . 2 8 2 % . 5 2 9 % . 3 3 8 % . 0 7 3 % . 2 6 1 % . 0 2 7 % . 1 0 3 % . 0 9 6 % . 4 0 2 % . 5 1 5 % . 7 6 8 % . 0 1 7 % . 0 5 5 % . 0 8 9 % . 3 2 5 % . 6 0 7 % . 1 7 2 % . 0 0 8 % . 0 5 3 % . 1 7 9 % . 4 5 4 % . 4 6 8 % . 9 6 9 % . 5 2 4 0 % . 1 3 6 % . 1 4 9 % . 3 1 1 % . 0 1 6 % . 0 4 3 % Î” . 0 3 5 . 0 0 5 . 0 0 . 0 0 2 . 0 0 3 . 0 2 . 0 0 3 . 0 0 5 . 0 0 . 0 0 1 . 0 0 1 . 0 0 . 0 0 2 . 0 0 1 . 0 0 . 0 0 8 . 0 0 5 . 0 0 . 0 0 3 . 0 0 2 . 0 0 . 0 0 3 . 0 0 8 . 0 0 . 0 0 1 . 0 0 4 . 0 0 . 0 0 2 . 0 0 2 . 0 0 . 0 0 8 . 0 1 0 . 0 0 . 0 0 3 . 0 0 1 . 0 0 . 0 0 8 . 0 0 2 . 0 0 . 0 0 0 . 0 0 2 . 0 0 . 0 0 7 . 0 1 4 . 0 6 . 0 0 2 . 0 0 2 . 0 0 . 0 0 0 . 0 0 1 ğ¸ ğ‘† 7 5 2 6 . 4 7 9 4 . 8 8 7 5 . 7 6 9 9 . 7 9 8 7 . 7 2 0 7 . 6 8 1 3 . 6 6 5 1 . 7 3 0 9 . 9 0 8 1 . . 2 3 1 5 8 9 0 . 4 9 0 7 . 1 6 4 0 . 1 9 3 6 . 8 6 7 6 . 6 4 8 8 . 7 8 7 5 . 7 6 2 8 . 7 8 0 4 . . 2 9 5 7 3 5 4 . 8 3 5 1 . 7 2 5 4 . 7 3 3 1 . 8 8 0 2 . 7 0 8 1 . 7 0 1 3 . 7 9 0 4 . 6 9 9 8 . 1 1 7 5 . 7 5 6 7 . 6 1 7 0 . 3 5 3 7 . 8 1 6 0 . 3 0 4 1 . 6 5 9 2 . 2 5 3 4 . 5 0 3 4 . 6 8 4 2 . 7 3 2 7 . 6 4 0 0 . . 7 5 6 5 8 6 0 . 7 2 6 8 . 3 6 8 6 . 2 1 1 0 . 8 4 6 2 . 4 0 8 3 . 8 7 7 8 . 6 6 8 1 . 3 0 8 5 . 3 1 1 . 2 4 0 1 . 2 9 3 8 . 8 2 6 . 2 3 8 3 . 2 6 9 2 . 2 0 4 . 3 2 8 5 . 2 0 7 1 . 2 0 1 . 2 3 9 0 . 1 8 2 3 . 2 6 2 . 3 4 7 5 . 2 5 0 6 . 2 6 6 . 2 3 5 1 . 2 6 8 1 . 2 4 7 . 2 3 6 5 . 2 4 0 3 . 2 9 9 . 2 3 0 4 . 3 0 5 6 . 2 0 9 . 2 0 7 6 . 2 3 6 6 . 2 4 3 . 3 4 0 6 . 3 7 7 3 . 2 3 4 . 2 5 2 0 . 2 5 3 9 . 2 1 9 . 2 3 7 5 . 2 3 6 5 . 2 1 6 . 2 4 1 0 . 3 1 5 9 . 2 0 6 . 2 8 3 1 . 2 6 3 3 . 7 8 6 . 2 1 0 2 . 2 3 6 0 . 4 5 4 . 2 5 4 5 . 2 4 1 0 . % + + + + + + + + + + + + + - + + + + + + + + + + + + + + + + + + + + + + + + + - + + + + + + + + + + . 4 9 2 5 % . 8 4 6 % . 1 1 2 5 % . 5 5 2 % . 8 1 0 % . 7 1 2 5 % . 3 9 1 % . 6 2 2 % . 3 8 5 % . 1 3 0 4 % . 1 3 9 % . 4 1 6 % . 4 0 0 % . 0 4 0 % . 6 1 9 % . 1 1 1 6 % . 7 2 1 % . 7 0 2 % . 4 3 6 % . 6 1 2 % . 0 1 8 % . 6 0 8 % . 7 7 2 % . 6 3 4 % . 3 9 0 % . 7 2 8 % . 3 9 0 % . 5 3 5 % . 6 0 2 % . 1 0 1 9 % . 2 1 4 3 % . 9 8 9 % . 2 2 5 % . 6 7 3 % . 5 0 6 % . 5 8 5 % . 5 4 1 % . 4 0 3 % . 5 1 7 % . 0 1 8 % . 9 1 7 % . 6 9 7 % . 2 7 5 % . 8 3 0 % . 6 3 5 7 % . 4 0 1 % . 4 9 0 % . 1 9 7 9 % . 1 1 4 % . 6 0 3 % Î” . 0 6 2 . 0 1 2 . 0 1 8 . 0 0 7 . 0 1 4 . 0 7 9 . 0 0 5 . 0 1 0 . 0 0 6 . 0 2 3 . 0 0 3 . 0 0 4 . 0 0 5 . 0 0 0 . 0 0 9 . 0 1 8 . 0 1 3 . 0 1 1 . 0 0 6 . 0 0 6 . 0 0 1 . 0 0 9 . 0 1 1 . 0 1 1 . 0 0 6 . 0 1 1 . 0 0 5 . 0 1 0 . 0 1 0 . 0 1 3 . 0 1 5 . 0 2 8 . 0 0 6 . 0 1 0 . 0 0 9 . 0 0 5 . 0 0 7 . 0 0 7 . 0 0 8 . 0 0 4 . 0 1 3 . 0 0 9 . 0 0 6 . 0 1 3 . 0 7 6 . 0 0 6 . 0 0 4 . 0 2 8 . 0 0 3 . 0 0 9 ğ¸ ğ‘†"
        },
        {
            "title": "D\ne\nv",
            "content": ". D"
        },
        {
            "title": "D\ne\nv",
            "content": ". D"
        },
        {
            "title": "T\na\nb\nl\ne",
            "content": "3 ."
        },
        {
            "title": "C\nr\ny\ns\nt\na\nl\nB\nL\nE\nU",
            "content": "("
        },
        {
            "title": "C\nB",
            "content": ") r s e w t a i n e s l d e o a e . Why Personalizing Deep Learning-Based Code Completion Tools Matters 15 i o p d n s h e b c a a )"
        },
        {
            "title": "B\nC",
            "content": "("
        },
        {
            "title": "U\nE\nL\nB\nl\na\nt\ns\ny\nr\nC",
            "content": ". 5 a - e b e e e b a e o i p ) ( a a . 4 a . i r l m . r o e d l s t n O p v D"
        },
        {
            "title": "D\nI",
            "content": ". ğ‘† ğ¸ 8 0 . 1 2 0 . 9 0 0 . 4 1 . 6 1 0 . 2 2 0 . 4 1 . 2 2 0 . 5 1 0 . 3 1 . 6 0 0 . 2 1 0 . 3 0 . 0 2 0 . 7 0 0 . 5 1 . 6 1 0 . 9 0 0 . 8 0 . 7 1 0 . 8 1 0 . 9 0 . 3 1 0 . 0 1 0 . 6 1 . 4 1 0 . 4 0 0 . 0 2 . 3 0 0 . 3 2 0 . 1 0 . 2 2 0 . 1 1 0 . 8 0 . 1 1 0 . 2 0 0 . % 0 8 . % 1 8 0 1 . % 4 9 4 . % 5 0 . % 2 9 7 . % 6 3 7 . % 3 1 1 . % 2 7 0 1 . % 8 6 8 . % 3 4 . % 3 3 2 . % 3 3 6 . % 8 7 . % 7 7 4 . % 8 8 7 . % 7 6 3 . % 2 5 0 1 . Î” % 4 6 5 . % 2 1 3 . % 9 5 8 . % 0 9 8 . % 2 8 3 . % 4 3 8 . % 3 1 5 . % 8 5 8 . % 7 3 7 . % 8 4 2 . % 6 1 0 1 . % 8 7 1 . % 1 2 1 1 . % 7 0 1 . % 3 6 0 1 . % 4 5 5 . % 9 9 4 . % 9 8 4 . % 0 5 0 . + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + % 3 1 5 2 . 9 4 6 2 . 3 6 1 . 3 5 2 2 . 4 4 7 2 . 1 0 7 . 1 4 3 2 . 2 4 5 2 . 6 5 9 . 7 0 6 2 . 5 1 7 1 . 4 2 9 . 1 9 6 1 . 8 5 1 3 . 7 1 2 . 3 6 0 2 . 2 9 8 2 . 9 1 5 . 6 3 0 2 . 6 8 3 2 . 4 2 7 . 9 5 4 2 . 1 8 3 2 . 9 3 6 . 2 1 6 2 . 7 5 5 2 . 2 5 1 . 6 5 4 2 . 8 8 2 2 . 7 1 6 . 8 8 1 2 . 2 3 4 2 . 6 5 4 . 2 0 4 2 . 6 8 2 2 . 6 9 3 . . 6 8 2 2 . 0 8 8 1 . 0 2 2 2 . 5 6 2 2 . 8 3 2 2 . 1 3 4 2 . 5 1 0 2 . 0 3 9 1 . 7 4 9 1 . 8 0 3 2 . 3 2 7 1 1 2 2 . 1 4 6 . . 8 5 2 2 . 0 3 2 2 . 7 8 3 2 . 7 8 3 2 . 6 3 3 2 6 7 1 . . 9 1 1 2 . 4 5 7 1 . 7 3 9 1 . 8 7 4 1 3 1 3 . . 5 3 3 2 . 0 3 0 2 . 3 8 2 2 . 0 2 2 2 . 7 2 5 1 . 6 2 3 2 4 5 3 . . 8 9 3 2 . 6 2 9 1 5 1 3 . . 2 6 9 1 . 5 7 5 1 ğ‘† ğ¸ 2 0 0 . 4 1 0 . 2 0 0 . 6 0 0 . 0 1 0 . 0 1 0 . 6 0 0 . 9 0 0 . 7 0 0 . 6 0 0 . 0 0 0 . 1 1 0 . 4 0 0 . 3 1 0 . 4 0 0 . 5 0 0 . 0 0 0 . 0 0 0 . 4 0 0 . 3 1 0 . 6 0 0 . 3 0 0 . 5 0 0 . 5 0 0 . 5 0 0 . 0 0 0 . 3 0 0 . 8 0 0 . 2 0 0 . 7 0 0 . 2 0 0 . 9 0 0 . 1 0 0 . 5 0 0 . 5 0 0 . 4 0 0 . % 8 9 1 . % 3 4 6 . % 5 1 0 . % 6 4 2 . % 1 6 4 . % 1 7 3 . % 3 8 2 . % 7 5 4 . % 8 4 3 . % 9 3 4 . % 8 4 0 . % 8 5 5 . % 7 7 2 . % 3 7 0 1 . Î” % 5 1 . % 1 6 2 . % 0 1 0 . % 5 9 . % 3 5 2 . % 2 1 6 . % 3 6 . % 3 3 2 . % 0 4 3 . % 5 3 . % 2 2 3 . % 7 0 0 . % 2 0 . % 6 9 2 . % 5 4 0 . % 0 5 . % 2 4 1 . % 9 2 3 . % 8 0 . % 3 1 2 . % 5 1 2 . % 2 0 . + + + + + + + + + + + + + + + + + + + + + + + + + - + + + + + + + + + + % 0 1 1 2 . 5 9 2 2 . 2 0 8 1 . 4 7 7 1 . 1 7 3 2 . 1 2 0 2 . 6 6 8 1 . 8 6 9 1 . 6 3 4 2 . 4 0 2 2 . 7 4 4 1 . 0 1 9 2 . 5 9 6 1 . 4 3 7 2 . 5 1 8 1 . 5 7 4 1 . 0 1 8 1 . 9 4 9 1 . 8 0 8 1 . 9 9 1 2 . 4 6 9 1 . 3 3 1 2 . 5 6 8 1 . 1 2 3 2 . 8 5 9 1 . 2 9 7 1 . 6 6 1 2 . 7 7 7 1 . 2 9 0 2 . 6 2 7 1 . 3 6 2 2 . 9 5 7 1 . 0 9 8 1 . 8 0 3 2 . 8 6 7 1 . 0 6 4 2 . 9 7 1 . 9 6 1 . 4 3 1 . 7 2 1 . 6 2 1 . 4 1 1 . 2 1 1 . 7 0 1 . 3 0 1 . 1 0 1 . 6 8 . 4 8 . 6 7 . 6 7 . 2 7 . 6 5 . 5 4 . 3 4 . 0 3 . 9 2 . 5 2 . 2 2 . 1 2 . 0 2 . 0 2 . 6 1 . 6 1 . 5 1 . 4 1 . 3 1 . 1 1 . 1 1 . 0 1 . 0 1 . 0 1 . 0 1 . 1 3 4 5 6 7 9 0 1 1 1 2 1 3 1 4 5 1 6 1 7 1 8 1 9 1 0 1 2 2 2 3 2 4 2 5 2 6 7 2 8 2 9 2 0 3 1 3 2 3 3 4 3 5 3"
        },
        {
            "title": "R\nO",
            "content": "0 8 2 . 5 4 2 . 0 5 1 . 3 4 1 . 2 9 1 . 9 2 4 . 9 8 2 . 0 0 6 . 2 1 2 . 5 5 1 . 0 5 1 . 3 7 2 . 3 7 1 . 4 1 4 . 4 5 1 . 0 3 3 . 7 6 2 . 3 7 1 . 8 3 1 . 8 7 2 . 1 3 2 . 0 2 1 . 0 0 3 . 7 4 1 . 8 0 2 . 0 1 2 . 7 6 2 . 0 2 2 . 4 6 1 . 7 2 3 . 5 7 1 . 3 8 3 . 3 7 1 . 7 7 1 . 1 3 1 . 6 7 0 . i i r Î” % % 0 6 . % 0 2 3 . % 0 4 1 . % 0 2 . % 0 2 2 . % 0 6 4 . % 0 4 . % 0 0 4 . % 0 8 3 . % 0 4 . % 0 2 1 . % 0 8 3 . % 0 6 . % 0 8 8 . % 0 4 1 . % 0 6 . % 0 0 5 . % 0 2 2 . % 0 2 . % 0 2 3 . % 0 4 3 . % 0 8 . % 0 0 4 . % 0 6 1 . % 0 6 . % 0 2 2 . % 0 0 3 . % 0 4 . % 0 8 1 . % 0 0 5 . % 0 2 . % 0 4 3 . % 0 2 2 . % 0 0 . % 0 0 1 . % 0 0 1 . + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + - . 4 7 2 . 0 6 2 . 0 1 2 . 4 9 1 . 2 8 2 . 6 5 2 . 8 7 2 . 2 2 2 . 4 6 2 . 8 3 2 . 0 1 2 . 8 0 3 . 6 1 3 . 8 4 3 . 8 9 1 . 4 0 2 . 6 5 2 . 4 4 2 . 8 4 2 . 2 1 2 . 4 8 1 . 4 2 2 . 2 6 2 . 4 5 2 . 8 9 1 . 8 5 2 . 0 4 2 . 4 0 2 . 0 9 2 . 8 2 2 . 8 5 2 . 4 1 2 . 0 3 2 . 6 0 3 . 0 8 2 . 0 8 2 . 6 8 2 2 . 0 8 8 1 . 0 2 2 2 . 5 6 2 2 . 8 3 2 2 . 1 3 4 2 . 5 1 0 2 . 0 3 9 1 . 7 4 9 1 . 8 0 3 2 . 3 2 7 1 1 2 2 . 1 4 . . 8 5 2 2 . 0 3 2 2 . 7 8 3 2 . 7 8 3 2 . 6 3 3 2 6 7 . . 9 1 1 2 . 4 5 7 1 . 7 3 9 1 . 8 7 4 1 3 1 3 . . 5 3 3 2 . 0 3 0 2 . 3 8 2 2 . 0 2 2 2 . 7 2 5 1 . 6 2 3 2 4 5 . . 8 9 3 2 . 6 2 9 1 5 1 . . 2 6 9 1 . 5 7"
        },
        {
            "title": "R\nO",
            "content": "7 6 2 . 3 5 1 . 5 6 0 . 2 6 1 . 0 6 1 . 0 6 1 . 2 6 1 . 3 3 3 . 9 2 1 . 9 2 2 . 8 3 1 . 7 0 2 . 6 8 2 . 2 6 6 . 1 9 0 . 1 7 1 . 2 9 0 . 0 2 1 . 8 8 1 . 5 7 1 . 0 0 2 . 6 3 1 . 3 3 2 . 0 2 1 . 7 1 2 . 9 8 0 . 1 7 2 . 7 1 1 . 3 7 0 . 0 6 1 . 0 0 2 . 2 2 1 . 6 5 1 . 3 7 0 . 0 8 1 . 8 5 1 . Î” % % e e D ğ‘ "
        },
        {
            "title": "D\nI",
            "content": ". % 0 0 3 . % 0 6 1 . % 0 4 1 . % 0 0 1 . % 0 2 1 . % 0 2 1 . % 0 0 1 . % 0 8 2 . % 0 0 1 . % 0 4 4 . % 0 6 0 . % 0 0 3 . % 0 6 2 . % 0 0 9 . % 0 2 0 . % 0 0 1 . % 0 2 0 . % 0 4 0 . % 0 4 1 . % 0 8 1 . % 0 2 1 . % 0 8 0 . % 0 4 2 . % 0 6 0 . % 0 4 1 . % 0 2 0 . % 0 4 2 . % 0 4 0 . % 0 6 0 . % 0 2 1 . % 0 0 2 . % 0 4 0 . % 0 0 1 . % 0 2 1 . % 0 8 0 . % 0 4 1 . + + - + + + + + + + + + + + - + - + + + + + + + + - + + - + + + + - + + . 8 6 2 . 4 4 . 2 8 1 . 2 9 1 . 2 7 . 2 2 2 . 4 5 2 . 0 1 . 6 3 2 . 8 5 2 . 4 0 . 0 0 3 . 6 2 3 . 0 5 . 2 8 1 . 8 6 1 . 4 0 . 6 2 2 . 0 5 2 . 8 9 . 2 6 1 . 4 2 2 . 6 4 . 4 4 2 . 6 8 1 . 4 3 . 4 3 2 . 4 8 1 . 6 6 . 0 9 1 . 6 6 2 . 4 8 . 8 1 2 . 4 7 2 . 8 7 . 4 0 3 9 7 1 . 9 6 1 . 4 3 1 . 7 2 1 . 6 2 1 . 4 1 1 . 2 1 1 . 7 0 1 . 3 0 1 . 1 0 1 . 6 8 . 4 8 . 6 7 . 6 7 . 2 7 . 6 5 . 5 4 . 3 4 . 0 3 . 9 2 . 5 2 . 2 2 . 1 2 . 0 2 . 0 2 . 6 1 . 6 1 . 5 1 . 4 1 . 3 1 . 1 1 . 1 1 . 0 1 . 0 1 . 0 1 . 0 1 . . 8 3 2 . 8 2 . 6 9 1 . 2 8 1 . 0 6 . 0 1 2 . 4 4 2 . 2 8 . 6 2 2 . 4 1 2 . 8 9 . 0 7 2 . 0 0 3 . 0 6 . 4 8 1 . 8 5 1 . 6 0 . 2 2 2 . 6 3 2 . 0 8 . 0 5 1 . 6 1 2 . 2 2 . 8 3 2 . 2 7 1 . 6 3 . 0 1 2 . 0 8 1 . 2 7 . 8 7 1 . 6 4 2 . 0 8 . 8 0 2 . 6 8 2 . 0 7 . 0 9 2 1 2 3 5 6 7 8 9 0 1 1 2 1 3 1 4 1 5 1 6 7 1 8 1 9 1 0 2 1 2 2 3 2 4 2 5 2 6 2 7 2 8 9 2 0 3 1 3 2 3 3 3 4 5 3"
        },
        {
            "title": "Manuscript submitted to ACM",
            "content": "16 Giagnorio et al. respect to Bğ‘  (Î”), and the odds ratio (OR) reported by the McNemars test [46] (again, when tested against Bğ‘  ). The symbol associated to Î” indicates statistically significant increase in EM predictions with respect to Bğ‘  (ğ‘-value <0.05); similarly, indicates statistically significant decreases in performance. In each row, bold values indicate the model achieving the best performance on the corresponding test set. To make concrete example, let us consider the results achieved on the test set related to developer 2 of Apache (Dev. ID = 2 in Table 2): Bğ‘  achieved 31.8% of EM predictions on D2s test set, against the 33.2% of the developer-specific model (trained on additional 23.2k instances), and the 37.0% of the organization-specific model (trained on additional 556k instances). Thus, the absolute increase in performance with respect to Bğ‘  is +1.40% for the developer-specific model (non statistically significant) and +5.20% for the organization-specific model. The latter increase is statistically significant, with an OR of 3.36, indicating 3 times higher odds to generate an EM prediction than Bğ‘  . Similarly, Tables 3 and 5 compare the average CrystalBLEU scores (CB %) achieved by the Bğ‘  baseline and by the personalized models on the developers test sets (for Apache and Spring developers, respectively). The basic idea behind CrystalBLEU is to compare non-EM predictions generated by two models, to see whether one of the two models still generates predictions closer to the target when not outputting an EM. We exclude from this analysis cases in which both models generate an EM, since the CrystalBLEU would be trivially equal to 1 for both of them. In both tables, the gap between the average CrystalBLEU scores achieved by the personalized models and by Bğ‘  is shown in the column Î”, while the ğ¸ğ‘† column shows the effect size (Cliffs delta [29]) of the difference. For example, for developer 2 of Apache (Dev. ID = 2 in Table 3) we observe an increase of +3.63% in the CrystalBLEU score for the developer-specific model as compared to Bğ‘  , and an increase of +8.68% for the organization-specific model. The first increase is not statistically significant, while the latter is, with an effect size of 0.13, indicating negligible effect of the organization-specific model in generating better predictions than Bğ‘  ."
        },
        {
            "title": "3.1 Goal 1: Evaluating Developer-Specific Personalization",
            "content": "In terms of exact match predictions (EM), 76% (76 out of 100) of Apache developer-specific models benefitted from the second fine-tuning (33 out of 76 are statistically significant) with an average performance improvement of 5.37% (median=2.00%) and mean OR of 3.91 (min=1.04, max=53.75). Likewise, 83% (30 out of 36) of the Spring developer-specific models achieved better performance than Bğ‘  (eight out of 30 statistically significant), with an average performance increase of 1.77% (median=1.20%) and mean OR of 1.99 (min=1.17, max=6.62). Only small part of the developer-specific models did not improve performance, namely 24 models for Apache (mean=-1.23%, median=-1.00%) and six for Spring (mean=-0.63%, median=-0.40%). Out of these, only three performance decreases were statistically significant (all in Apache, Dev. ID = 5, 40, 58). On top of these, four Apache models achieved exactly the same EM predictions of Bğ‘  (Dev. ID = 8, 24, 29, 32). As it can be observed in Tables 2 and 4, there are several developers which clearly represent (positive) outliers in terms of achieved increase in performance (see e.g., Dev. ID = 7 for Apache and Dev. ID = 14 for Spring). We inspected their test sets to understand why developer-specific model was working so well as compared to Bğ‘  . We found that these developers implemented significant amount of project-specific boilerplate code (e.g., toString, compareTo), thus their training and test sets shared similar code structures. These are typical code elements which nowadays can be delegated to an AI assistant. Still, the generic baseline model failed in predicting their completions due to the lack of project-specific code. As an illustration, the test set of developer 57 of Apache features completion on the getLongnvarcharColumn method, whose purpose is to retrieve the longnvarcharColumn attribute from data collection. The developer-specific model, having knowledge about the code base on which the developer works, was able to predict the need to invoke"
        },
        {
            "title": "Manuscript submitted to ACM",
            "content": "Why Personalizing Deep Learning-Based Code Completion Tools Matters 17 readProperty to retrieve the attribute (as done in other code locations in the developer-specific training set), while the baseline model lacked such piece of knowledge, recommending wrong completion. For what concerns the three developers on which the developer-specific model resulted in statistically significant decrease of EMs, we inspected their predictions to identify patterns explaining this result. We found that sometimes these models fail in predicting the correct code (as opposed to the baseline) due to repetitions of the same suggestion multiple times or the addition of extra code tokens. Additional studies are needed to understand how to address this point, e.g., adopting smaller learning rate for the developer-specific fine-tuning to avoid influencing too much the model towards the developers coding style. Table 3 shows that, even when focusing on non-EM predictions, 84% of Apache developers test sets have better predictions with the developer-specific model, with an average CrystalBLEU improvement of 6.63% (median=2.52%). Of these improvements, 34 are statistically significant, with an average (small) Cliffs delta of 0.18 (min=0.03, max=0.69). The trend observed for the Spring organization (depicted in Table 5) is even clearer, with all developers but one (Dev. ID = 26) benefitting from better predictions with the developer-specific models (20 statistically significant, mean effect size of 0.08min=0.03, max=0.14). The average improvement in CrystalBLEU is 2.96% (median=2.63%). All this suggests that, overall, while the developer-specific models seem to generate predictions closer to the target even when not outputting the correct prediction (i.e., overall higher CrystalBLEU), the mostly negligible/small effect size we observed tells us that the gap is not major. Still, it is worth noting that the overall positive trend observed for the developer-specific models (when looking at both EM predictions and CrystalBLEU) is the result of very limited training effort, with developer-specific training datasets going from 1k up to 46.6k instances (column N). As term of comparison, Bğ‘  was fine-tuned on 1.4M instances."
        },
        {
            "title": "Summary of Findings",
            "content": "The overall trend we observed is that developer-specific training tends to improve the models code completion capabilities, with improvements being statistically significant for 30% (41 out of 136) of the studied developers in terms of EM predictions, and for 40% of developers (54 out of 136) in terms of CrystalBLEU. For both metrics, however, the magnitude of the observed improvement is limited, but still noteworthy when considering the very limited additional training that was performed (due to the limited fine-tuning data specific of developer)."
        },
        {
            "title": "3.2 Goal 1: Evaluating Organization-Specific Personalization",
            "content": "Concerning the organization-specific customization, for the Apache organization, 93% of the models achieved better performance than Bğ‘  in terms of EM predictions, with 70 of these improvements being statistically significant. The average EM improvement is +7.84% (median=4.00%) with mean OR of 7.59 (min=1.05, max=143.00). Similarly, for Spring all organization-specific models but one (Dev. ID = 36) were better than Bğ‘  , with 17 of these improvements being statistically significant. The average EM improvement is +2.84% (median=2.40%) with mean OR of 2.37 (min=1.20, max=6.00). Only one model obtained statistically significant decrease in performance across the two organizations (Dev. ID = 61 of Apache). The organization-specific models beat the developer-specific ones in terms of EM predictions in 89% of cases for Apache and in 81% of cases for Spring. This is likely due to the larger amount of training data present in the organization-specific training sets. The confounding factor related to the size of the training set is thoroughly discussed in Section 3.3. The"
        },
        {
            "title": "Manuscript submitted to ACM",
            "content": "18 Giagnorio et al. analysis of the CrystalBLEU supports the conclusions obtained when analyzing the EM predictions: the organizationspecific models are superior to Bğ‘  in 96% of cases for Apache and in every case for Spring, achieving an average CrystalBLEU improvement of 11.07% (median=6.31) for Apache, and 6.69% (median=6.84) for Spring. In Apache, 85% of developers observe statistically-significant improvement of CrystalBLEU, with an average effect size of 0.17 (min=0.04, max=0.79). In Spring, the differences are significant for 83% of developers, with an average effect size of 0.14 (min=0.07, max=0.23). It is worth noting that, differently from what observed for the developer-specific models, the improvements seen with the organization-specific fine-tuning (i) are more consistent (i.e., higher percentage of developers benefitted from statistically-significant improvements); and (ii) are characterized by higher ORs (for EM) and effect sizes (for CrystalBLEU)."
        },
        {
            "title": "Summary of Findings",
            "content": "The organization-specific models are the ones providing the best performance, being the best in class for 89 (29) out of the subject 100 (36) developers. The average increase in EM predictions is +7.84% (+2.84%) over the baseline. The CrystalBLEU study confirms the superiority of the organization-specific models, which outperform 96% (100%) of the baseline models, with an average improvement of +11.07% (+6.69%)."
        },
        {
            "title": "3.3 Goal 2: Assessing the Impact of the Training Data Size",
            "content": "The discussed findings indicate ranking between the experimented models, with the organization-specific being the best, followed by the developer-specific and, finally, the baseline (Bğ‘  ). This ranking also reflects the amount of training data used in each training strategy (e.g., organization-specific benefited from more training data), questioning the role played by the training dataset size on the differences in performance. We start from the superiority demonstrated by the organization-specific training over the developer-specific training."
        },
        {
            "title": "Our assumption is that the former was superior only due to the additional training data and that the latter would be",
            "content": "superior (since it is more specific) if given the same amount of training instances. To test this assumption we train 20 additional organization-specific models (one for each of the top-10 developers of each organization) in which we cap the size of the training data to the exact same amount of training instances we collected for the corresponding developer-specific model. For example, since the developer-specific training set for Dev. ID = 1 of Apache has 46.6k instances, we randomly select the same number of instances from the corresponding organization-specific dataset, training with it what we call the Organization subset model. The left-hand side of Table 6 reports the achieved results for Apache (top) and Spring (bottom). To provide more context, the table shows again the ID of the developer the test set refers to, the EM predictions achieved by the baseline (Baseline Bğ‘  column) and by the developer-specific model (Developer column) and, in addition to that, the EM predictions generated with the Organization subset model. As observed, when the organization-specific models benefit from the same amount of instances as the developer-specific models, their performance is overall worse. Indeed, the only statistically significant differences in terms of EM predictions (Dev. ID = 1, 4, 7, 9 of Apache and Dev. ID = 10 of Spring) are in favor of the developer-specific model and accompanied by an OR of at least 1.95. This suggests that, in the context of personalizing DL-based code completion tools, more specific data (e.g., developer-specific data) must be preferred over more generic ones (e.g., organization-specific data) when possible. The second comparison we perform (right-hand side of Table 6) concerns the organization-specific models (i.e., the best in class in our study) against baseline (Baseline+ in the table) further trained on the same amount of training instances provided to the organization-specific models, although not specific to the organization (see Section 2.4.1 for"
        },
        {
            "title": "Manuscript submitted to ACM",
            "content": "Why Personalizing Deep Learning-Based Code Completion Tools Matters 19 Table 6. Comparison of Exact Match (EM) predictions with models trained on different datasets of the same size. Dev. ID Baseline Bğ‘  Developer Organization subset Î” OR 57.2 - 3.60% 2.38 33.4 + 0.20% 0.96 24.8 - 0.20% 1.07 35.2 - 3.80% 1.95 20.4 + 1.80% 0.50 36.2 - 0.60% 1.21 51.8 - 8.20% 2.64 28.4 - 0.60% 1.30 37.2 - 9.00% 3.50 33.2 0.00% 1.00 24.8 - 2.00% 2.11 22.2 - 2.20% 2.00 19.0 + 0.80% 0.69 17.8 - 1.40% 2.00 27.2 0.00% 1.00 21.0 - 1.20% 1.67 27.0 + 1.60% 0.50 19.6 - 1.40% 1.88 24.0 + 0.40% 0.91 22.0 - 3.80% 3.71 EM % EM % 60.8 33.2 25.0 39.0 18.6 36.8 60.0 29.0 46.2 33.2 26.8 24.4 18.2 19.2 27.2 22.2 25.4 21.0 23.6 25.8 46.6k 23.2k 20.3k 19.2k 18.6k 17.8k 17.5k 17.0k 15.9k 15.8k 17.9k 16.9k 13.4k 12.7k 12.6k 11.4k 11.2k 10.7k 10.3k 10.1k EM % 51.2 31.8 26.0 35.2 21.4 34.6 28.8 29.0 27.8 31.6 23.8 22.8 19.6 18.2 26.0 21.0 24.4 18.2 22.6 21.4 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 a n S Dev. ID Baseline Bğ‘  Organization EM % 61.8 37.0 28.2 44.2 24.0 37.2 70.0 32.2 48.4 33.6 27.4 26.0 21.0 19.4 28.2 25.6 27.8 22.2 26.4 23. 888.0k 556.0k 830.8k 747.3k 540.5k 791.4k 524.8k 850.5k 580.9k 146.3k 228.6k 188.0k 222.0k 226.5k 223.8k 243.1k 201.5k 193.0k 194.7k 230.8k EM % 51.2 31.8 26.0 35.2 21.4 34.6 28.8 29.0 27.8 31.6 23.8 22.8 19.6 18.2 26.0 21.0 24.4 18.2 22.6 21.4 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 Baseline+ Î” OR EM % 48.8 - 13.00% 9.12 32.6 - 4.40% 3.00 27.2 - 1.00% 1.42 39.8 - 4.40% 3.00 24.4 + 0.40% 0.89 38.4 + 1.20% 0.73 46.4 - 23.60% 14.11 30.4 - 1.80% 1.60 38.0 - 10.40% 5.33 33.8 + 0.20% 0.96 24.8 - 2.60% 2.18 22.0 - 4.00% 3.00 17.8 - 3.20% 2.45 17.4 - 2.00% 1.91 1.00 28.2 0.00% 20.4 - 5.20% 4.25 25.4 - 2.40% 2.20 19.4 - 2.80% 3.80 22.6 - 3.80% 2.58 20.6 - 3.20% 2.07 Table 7. Comparison of CrystalBLEU (CB) scores with models trained on different datasets of the same size. Î” Dev. ID Developer Organization subset CB % CB % ğ¸ğ‘† 46.6k 31.17 23.47 - 7.70% 0.10 23.2k 27.36 24.97 - 2.39% 0.04 20.3k 21.26 20.76 - 0.50% 0.02 19.2k 27.44 22.32 - 5.12% 0.06 18.6k 18.84 20.64 + 1.80% 0.02 17.8k 22.20 18.30 - 3.90% 0.06 17.5k 39.92 33.10 - 6.82% 0.08 17.0k 18.82 19.36 + 0.54% 0.02 15.9k 34.75 21.91 - 12.84% 0.16 15.8k 29.30 28.16 - 1.14% 0.03 17.9k 20.08 18.81 - 1.27% 0.00 16.9k 22.75 18.62 - 4.13% 0.06 13.4k 17.24 17.60 + 0.36% 0.00 12.7k 17.93 15.66 - 2.27% 0.03 12.6k 22.90 21.54 - 1.36% 0.04 11.4k 20.01 17.06 - 2.95% 0.08 11.2k 17.59 18.73 + 1.14% 0.01 10.7k 18.70 16.63 - 2.07% 0.03 10.3k 23.99 24.32 + 0.33% 0.00 10.1k 19.49 16.28 - 3.21% 0.03 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 Dev. ID Organization CB % 44.17 888.0k 30.03 556.0k 25.33 830.8k 29.05 747.3k 23.77 540.5k 22.86 791.4k 59.62 524.8k 24.23 850.5k 39.55 580.9k 28.77 146.3k 24.35 228.6k 27.04 188.0k 22.74 222.0k 22.72 226.5k 26.28 223.8k 27.73 243.1k 22.61 201.5k 24.51 193.0k 28.68 194.7k 25.53 230.8k 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 Baseline+ Î” CB % ğ¸ğ‘† 20.83 - 23.34% 0.29 23.58 - 6.45% 0.10 25.19 - 0.14% 0.00 22.24 - 6.81% 0.10 24.80 + 1.03% 0.02 25.08 + 2.22% 0.05 31.03 - 28.59% 0.40 23.04 - 1.19% 0.01 24.19 - 15.36% 0.22 30.01 + 1.24% 0.01 17.43 - 6.92% 0.11 17.20 - 9.84% 0.20 17.80 - 4.94% 0.08 16.37 - 6.35% 0.14 21.74 - 4.54% 0.11 15.67 - 12.06% 0.24 15.81 - 6.80% 0.14 15.26 - 9.25% 0.22 22.50 - 6.18% 0.10 19.58 - 5.95% 0. c g p c g p c A i details). When comparing Baseline+ to the organization-specific models, for 60% of developers (Dev. ID = 1, 2, 4, 7, 9 of Apache and Dev. ID = 1, 2, 3, 6, 8, 9, 10 of Spring) the organization-specific models achieve statistically significant better results in terms of EM, with an OR of at least 2.07. In no cases Baseline+ achieves statistically significant better results. Again, this supports the idea that training on more specific data (in this case, organization-specific vs general) helps in boosting performance. The analysis of the CrystalBLEU, available in Table 7, confirms our findings in terms of EM predictions. Predictions of developer-specific models have higher similarity than Organization subset models for eight developers out of 10 for Apache (five statistically significant) and seven out of 10 for Spring (four statistically significant), while organizationspecific models outperform the Baseline+ models in seven cases out of 10 for Apache (five of which are statistically"
        },
        {
            "title": "Manuscript submitted to ACM",
            "content": "20 Giagnorio et al. significant) and in all cases (all statistically significant) for Spring. Only developer 5 of Apache shows statistically significant increase in favor of the Organization subset model."
        },
        {
            "title": "Summary of Findings",
            "content": "The amount of training instances, as expected, plays role. Indeed, organization-specific models are better than developer-specific only when the former exploit more training data than the latter (otherwise, the opposite is true). However, when controlling for the training size (and, thus, for the training cost), our findings suggest that the more specific the training data, the higher the boost in performance at inference time. (a) Percentage of instances in the test set whose method signature is also present in the training set. (b) Percentage of identifiers and literals in the test set which are also present in the training set. (c) Percentage of identifiers and literals in the train set which are also present in the test set. Fig. 3. Distributions of metrics correlating training and test sets across all 100 developers of Apache. Base = Baseline Bğ‘  , Dev = Developer, Org = Organization. Higher is better."
        },
        {
            "title": "3.3.1 Why Do More Specific Training Data Help? Fig. 3 illustrates boxplots showing information items shared between\nthe three trainings (Base = baseline Bğ‘  , Dev = developer-specific, Org = organization-specific) and the 136 developer-related\ntest sets of the two organizations, as described in Section 2.4.2.",
            "content": "Fig. 3a depicts the percentage of instances in the test sets whose method signature appears in the training sets. Despite being substantially smaller, the developer-specific training sets cover similar number of signatures (median=0.116) than the baseline (median=0.119), while the organization-specific training sets cover the most signatures (median=0.175). Fig. 3b depicts the percentage of literals (e.g., strings and numbers) and identifiers (e.g., method and variable/constant names) in the test sets which are also present in the training sets. As observed, the organization-specific training sets have the highest vocabulary coverage (median=0.58), followed by the baseline (median=0.53), and the developer-specific (median=0.38). Despite the lower vocabulary coverage of the developer-specific training sets, these are more aligned to the vocabulary used in the test sets, as illustrated in Fig. 3c, explained next. Fig. 3c shows the percentage of identifiers and literals in the training sets which can be found in the test sets. As expected, the developer-specific training sets hold larger ratio of relevant data (median=0.07) compared to the organization-specific and baseline training sets, whose proportion is negligible (median=0). Manuscript submitted to ACM Why Personalizing Deep Learning-Based Code Completion Tools Matters"
        },
        {
            "title": "Summary of Findings",
            "content": "More specific training data helps in better aligning the domain of the model to the one of the test set (Fig. 3a). Also, the model will be more focused on the vocabulary used in the test set (Figures 3b and 3c)."
        },
        {
            "title": "3.4 Goal 3: Evaluating the Impact of Model Size, Architecture, and Pre-training",
            "content": "In this section we analyze the extent to which our findings generalize to larger models, different architectures, and pre-trained models whose training data might already include the organizations of interest (Apache and Spring). We start by analyzing the generalizability to larger models compared to the T5ğ‘ ğ‘šğ‘ğ‘™ğ‘™ (60M parameters) subject of the previous analyses. We replicate our experiments with our Bğ‘™ baseline, i.e., T5ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ , which features 750M parameters (12.5 times more than T5ğ‘ ğ‘šğ‘ğ‘™ğ‘™ ). To make the experimentation affordable, the experiments are replicated only for the top-10 developers from each organization (i.e., 20 developers in total). Table 8 reports the results achieved for Apache (top) and Spring (bottom) developers. As illustrated, the main findings previously discussed for T5ğ‘ ğ‘šğ‘ğ‘™ğ‘™ apply to the experiments with T5ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ as well: (i) the more specific the training data, the higher the boost in performance; and (ii) the organization-specific models confirm their superiority due the larger amount of training data. Indeed, in terms of exact matches, we observe higher accuracy for developerand organization-specific models with respect to Bğ‘™ . Furthermore, the organization-specific personalization reports statistically significant increase in performance for six and eight out of 10 developers for Apache and Spring, respectively, with an average OR of 4.05 (min=2.00, max=12.33). This trend in terms of EM predictions is also confirmed by the analysis of the CrystalBLEU score. Table 9 reports the increase in the CrystalBLEU of the developer-specific and organization-specific models with respect to the baseline Bğ‘™ . As shown, such increase is highest for the organization-specific models, being statistically significant for 95% of them (19 out of 20), with an average effect size of 0.18 (min=0.08, max=0.48). In addition to the experiments with T5ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ , we also investigate the applicability of personalization to larger pretrained code models. We focus on Code Llama in its base version featuring 7B parameters. This allows to understand whether our findings generalize also to even larger models (10 times bigger than T5ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ ), with different architectures (Llama-based) and already pre-trained on data likely to also feature code from the organizations of interest (since its training cutoff date is 2023 [52]). Indeed, even if Code Llama training set includes code from Apache or Spring, it is still worthwhile investigating whether further specialization can boost its performance. Listing 1. Masked method. Listing 2. Output by Code Llama. Listing 3. Expected output. public int add(int a, int b) { public int add(int a, int b) { public int add(int a, int b) { result = <FILL_ME> return result; } result = + b; return result; result = + b; return result; } public int subtract(int a, int b) { } result = - b; return result; } When generating the predictions for the developers test sets with Code Llama, we observed that the model tended to generate longer outputs, often spanning multiple methods, while our code completion task is capped to at most 50 tokens from the same method (see Section 2.2.1). For example, given the masked code shown in Listing 1, referring to an add operation, Code Llama may generate completion like the one shown in Listing 2 which, while correct, includes an additional method, subtract. Instead, it should generate only the missing part of the add method, as shown in Listing 3. When computing EM predictions, this may lead to an underestimation of the Code Llama capabilities. Thus,"
        },
        {
            "title": "Manuscript submitted to ACM",
            "content": "22 Giagnorio et al. Table 8. Exact Match (EM) predictions generated by the baseline and by the personalized models using T5ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ . Dev. ID Baseline Bğ‘™"
        },
        {
            "title": "Developer",
            "content": "e p i 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 EM % 52.8 37.4 30.6 42.6 29.0 39.0 47.4 34.0 41.8 41.6 27.4 24.8 21.0 19.0 28.4 24.0 26.0 21.6 28.6 21.6 Organization EM % Î” Î” OR EM % OR 64.2 + 11.40% 10.50 888.0k 66.2 + 13.40% 46.6k 5.79 1.48 556.0k 44.4 + 7.00% 39.8 + 2.40% 23.2k 2.94 0.68 830.8k 33.0 + 2.40% 29.4 - 1.20% 20.3k 1.80 19.2k 45.4 + 2.80% 44.4 + 1.80% 2.40 747.3k 1.56 1.06 540.5k 31.2 + 2.20% 29.2 + 0.20% 18.6k 1.73 1.60 791.4k 43.0 + 4.00% 41.4 + 2.40% 2.82 17.8k 6.00 524.8k 74.6 + 27.20% 12.33 66.4 + 19.00% 17.5k 1.43 850.5k 37.4 + 3.40% 35.2 + 1.20% 2.06 17.0k 6.00 580.9k 55.4 + 13.60% 54.8 + 13.00% 7.18 15.9k 1.04 146.3k 42.2 + 0.60% 41.8 + 0.20% 1.12 15.8k 1.16 228.6k 31.2 + 3.80% 28.0 + 0.60% 2.27 17.9k 1.12 188.0k 28.2 + 3.40% 25.4 + 0.60% 2.13 16.9k 1.92 222.0k 25.4 + 4.40% 23.2 + 2.20% 3.44 13.4k 12.7k 23.6 + 4.60% 23.4 + 4.40% 2.83 2.92 226.5k 1.05 223.8k 31.8 + 3.40% 28.6 + 0.20% 2.00 12.6k 1.17 243.1k 27.8 + 3.80% 24.4 + 0.40% 2.46 11.4k 1.83 201.5k 32.0 + 6.00% 28.0 + 2.00% 6.00 11.2k 10.7k 22.6 + 1.00% 22.0 + 0.40% 1.11 1.42 193.0k 1.04 194.7k 29.2 + 0.60% 28.8 + 0.20% 1.12 10.3k 1.05 230.8k 26.6 + 5.00% 21.8 + 0.20% 2.39 10.1k Table 9. CrystalBLEU (CB) average score between the baseline and the personalized models using T5ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ . Dev. ID 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 a n S Developer Î” Organization CB % Î” ğ¸ğ‘† CB % ğ¸ğ‘† 46.6k 41.66 + 18.70% 0.23 888.0k 46.35 + 20.92% 0.27 23.2k 34.73 + 5.82% 0.09 556.0k 38.62 + 11.22% 0.16 20.3k 25.71 - 0.27% 0.01 830.8k 29.34 + 4.18% 0.08 19.2k 28.20 + 5.40% 0.07 747.3k 30.10 + 5.77% 0.09 18.6k 25.35 + 0.82% 0.02 540.5k 28.61 + 4.49% 0.07 17.8k 29.81 + 4.36% 0.06 791.4k 29.39 + 6.06% 0.08 17.5k 56.61 + 21.78% 0.32 524.8k 66.42 + 33.25% 0.48 17.0k 25.44 + 3.14% 0.03 850.5k 28.52 + 5.77% 0.09 15.9k 44.47 + 19.33% 0.26 580.9k 45.42 + 20.77% 0.29 15.8k 33.39 + 1.08% 0.03 146.3k 35.36 + 3.48% 0.06 17.9k 24.18 + 0.64% 0.02 228.6k 29.45 + 6.71% 0.11 16.9k 27.19 + 6.86% 0.15 188.0k 29.04 + 10.54% 0.20 13.4k 23.51 + 6.67% 0.12 222.0k 27.30 + 11.08% 0.19 12.7k 25.57 + 8.84% 0.18 226.5k 27.35 + 10.62% 0.21 12.6k 24.91 + 1.50% 0.03 223.8k 28.94 + 6.15% 0.11 11.4k 19.55 + 1.82% 0.03 243.1k 28.16 + 10.22% 0.20 11.2k 22.53 + 3.23% 0.05 201.5k 26.53 + 8.52% 0.13 10.7k 18.93 + 1.05% 0.03 193.0k 25.12 + 5.84% 0.15 10.3k 28.94 + 2.87% 0.05 194.7k 32.55 + 6.87% 0.12 10.1k 21.18 + 1.81% 0.05 230.8k 28.88 + 9.71% 0.15 in our analysis we also include version of Code Llama which has been fine-tuned on only 10k instances from random projects (not belonging to Apache or Spring) for one epoch, just to make the model understand the need to generate shorter completions belonging to single method. We experimentally verified that this small fine-tuning is enough to adapt the model to the task at hand. We refer to this model as Bğ‘10ğ‘˜ . Manuscript submitted to ACM Why Personalizing Deep Learning-Based Code Completion Tools Matters 23 Table 10 shows the results obtained in terms of EM predictions by Code Llama (Baseline Bğ‘ ), the version of Code Llama fine-tuned on 10k instances (Baseline Bğ‘10ğ‘˜ ), and the developer-specific and organization-specific models, fine-tuned on top of Bğ‘ . The differences reported for the developer-specific and organization-specific models (Î” & OR) are with respect to Bğ‘10ğ‘˜ since, as it can be seen, the Bğ‘ model taken out of the box achieves low percentage of EM predictions, for the reasons previously explained (i.e., it generates additional unrequested methods). Table 10. Exact Match (EM) predictions generated by the baseline and by the personalized models using Code Llama 7B. Dev. ID Baseline EM % (Bğ‘) EM % (Bğ‘10ğ‘˜ ) a n S 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 8.8 22.6 20.0 19.6 22.8 16.6 36.2 16.6 22.0 25.2 16.2 12.0 13.0 10.0 13.4 8.6 15.6 12.8 21.6 13.0 62.6 54.4 45.6 56.8 48.6 49.0 69.8 46.0 54.0 52.6 41.8 38.8 35.6 32.8 43.4 36.8 38.0 39.8 47.2 35.6 Developer Î” Organization EM % Î” OR 54.2 - 0.20% 44.8 - 0.80% 0.79 830.8k 45.8 + 0.20% 57.8 + 1.00% 1.25 747.3k 59.4 + 2.60% 47.4 - 1.20% 50.2 + 1.20% OR EM % 72.4 + 9.80% 9.17 888.0k 74.0 + 11.40% 15.25 46.6k 23.2k 56.0 + 1.60% 1.36 556.0k 0.96 20.3k 1.06 19.2k 2.00 18.6k 48.6 0.00% 1.00 540.5k 0.71 17.8k 51.8 + 2.80% 2.75 791.4k 1.38 76.6 + 6.80% 4.40 524.8k 80.8 + 11.00% 12.00 17.5k 47.0 + 1.00% 1.20 850.5k 48.6 + 2.60% 17.0k 1.93 59.6 + 5.60% 2.47 580.9k 60.6 + 6.60% 5.12 15.9k 56.2 + 3.60% 1.67 146.3k 57.8 + 5.20% 2.62 15.8k 17.9k 44.2 + 2.40% 1.92 228.6k 44.2 + 2.40% 2.71 41.4 + 2.60% 1.93 188.0k 42.2 + 3.40% 3.12 16.9k 37.4 + 1.80% 1.50 222.0k 37.8 + 2.20% 1.85 13.4k 12.7k 35.6 + 2.80% 1.82 226.5k 35.2 + 2.40% 1.75 45.8 + 2.40% 2.09 223.8k 46.8 + 3.40% 2.70 12.6k 11.4k 39.4 + 2.60% 2.00 243.1k 39.2 + 2.40% 1.71 42.2 + 4.20% 2.40 201.5k 44.0 + 6.00% 3.50 11.2k 40.2 + 0.40% 1.08 193.0k 43.2 + 3.40% 2.06 10.7k 49.6 + 2.40% 1.60 194.7k 50.8 + 3.60% 1.78 10.3k 38.6 + 3.00% 1.88 230.8k 43.4 + 7.80% 4.00 10.1k As illustrated, personalization is effective also for Code Llama. When it comes to the developer-specific models, eight out of 10 Apache developers and all 10 Spring developers observed boost in performance. For Apache, this is statistically significant for five developers, with an average increase in EM predictions of 5.72% and an average OR of 4.09 (min=2.47, max=9.17). As per Spring, two developers have statistically significant improvement of EM, with +4.2% and +3.0% of EM predictions (ORs 2.40 and 1.88, respectively). As already observed for the T5 models, it is the organization-specific fine-tuning that brings most of the performance improvement (see Table 10): Overall, for 11 out of 20 developers (four for Apache and seven for Spring) Code Llama got statistically significant boost in EM predictions, with an average increase of 5.84% and an average OR of 4.99 (min=2.06, max=15.25). The findings are consistent also when looking at the CrystalBLEU scores  (Table 11)  . Still focusing on the most successful personalization (i.e., organization-specific), statistically significant increase in CrystalBLEU is observed on the test sets of 5/10 Apache and 10/10 Spring developers. For four of these developers, two-digit increase is observed, indicating strong impact of the personalized fine-tuning."
        },
        {
            "title": "Manuscript submitted to ACM",
            "content": "24 Giagnorio et al. Table 11. CrystalBLEU (CB) average score between the baseline and the personalized Code Llama 7B models. Dev. ID 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 a g p Developer Î” Organization CB % Î” ğ¸ğ‘† CB % ğ¸ğ‘† 46.6k 44.58 + 18.39% 0.22 888.0k 47.72 + 22.31% 0.27 23.2k 38.50 + 2.52% 0.03 556.0k 38.73 + 2.24% 0.04 20.3k 33.19 + 0.56% 0.01 830.8k 31.95 + 0.02% 0.01 19.2k 37.31 + 2.60% 0.04 747.3k 37.41 + 4.69% 0.06 18.6k 34.16 + 0.72% 0.02 540.5k 33.53 - 0.15% 0.00 17.8k 34.68 + 4.51% 0.06 791.4k 32.60 + 0.37% 0.01 17.5k 48.18 + 12.39% 0.17 524.8k 56.10 + 22.37% 0.32 17.0k 36.40 + 1.51% 0.01 850.5k 35.71 + 3.34% 0.04 15.9k 41.45 + 8.44% 0.12 580.9k 40.84 + 10.92% 0.16 15.8k 46.83 + 7.35% 0.11 146.3k 45.77 + 8.92% 0.15 17.9k 34.48 + 3.96% 0.07 228.6k 33.08 + 3.96% 0.07 16.9k 34.79 + 4.92% 0.08 188.0k 36.26 + 7.73% 0.12 13.4k 33.94 + 2.54% 0.03 222.0k 33.60 + 3.23% 0.05 12.7k 33.59 + 5.82% 0.09 226.5k 33.50 + 5.93% 0.10 12.6k 34.03 + 4.58% 0.07 223.8k 36.39 + 7.18% 0.11 11.4k 32.64 + 6.90% 0.12 243.1k 34.61 + 7.98% 0.14 11.2k 36.84 + 6.64% 0.11 201.5k 35.84 + 6.29% 0.09 10.7k 33.33 + 3.09% 0.07 193.0k 35.40 + 6.92% 0.11 10.3k 39.29 + 7.18% 0.11 194.7k 42.37 + 9.55% 0.15 10.1k 32.50 + 6.74% 0.11 230.8k 36.69 + 11.82% 0."
        },
        {
            "title": "Summary of Findings",
            "content": "Personalization is effective across different model sizes (60M, 750M and 7B parameters), and model architectures (T5 and Llama-based). Even Code Llama that likely saw code from the two subject organizations at training time benefitted of further specialized fine-tuning."
        },
        {
            "title": "3.5 Goal 4: Investigating the Cost-Performance Trade-Off",
            "content": "As explained in Section 2.4.3, we also run an additional analysis aimed at understanding the cost-effectiveness of the personalized fine-tuning. Indeed, the personalized fine-tuning implies training cost that the company would not have by just downloading larger code-completion model already trained for such task, and maybe exhibiting even better performance than the smaller, personalized model. For the reasons detailed in Section 2.4.3, we perform this analysis between the generic T5ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ model (i.e., Bğ‘™ ), as represenattive of general-purpose model that company could download and use out of the box, and the personalized organization-specific T5ğ‘ ğ‘šğ‘ğ‘™ğ‘™ models. We start by comparing the performance of both models for the top-10 developers of both organizations. Table 12 shows the results. As it can be seen, the performance of the two models is comparable on the top-10 developers of both organizations. Indeed, when it comes to Apache, there is statistically significant difference in EM predictions for five out of 10 developers, three times in favor of T5ğ‘ ğ‘šğ‘ğ‘™ğ‘™ and two in favor of T5ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ . As per Spring, the two models are basically equivalent (i.e., no statistically significant difference) for all developers. Overall, we can conclude that model fine-tuned on organization-specific data can be as effective as generic model that is over 10 times larger (60M vs 750M parameters). In addition to the performance dimension of the comparison, we also aim to understand whether the cost of training smaller personalized model is justified. Indeed, while the generic T5ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ (as well as any other already trained Manuscript submitted to ACM Why Personalizing Deep Learning-Based Code Completion Tools Matters Table 12. Exact Match (EM) predictions of the base T5ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ vs T5ğ‘ ğ‘šğ‘ğ‘™ğ‘™ organization-specific models. Dev. ID T5ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ EM % 52.8 37.4 30.6 42.6 29.0 39.0 47.4 34.0 41.8 41.6 27.4 24.8 21.0 19.0 28.4 24.0 26.0 21.6 28.6 21.6 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 T5ğ‘ ğ‘šğ‘ğ‘™ğ‘™ organization 888.0k 556.0k 830.8k 747.3k 540.5k 791.4k 524.8k 850.5k 580.9k 146.3k 228.6k 188.0k 222.0k 226.5k 223.8k 243.1k 201.5k 193.0k 194.7k 230.8k OR Î” EM % 61.8 + 9.00% 3.81 37.0 - 0.40% 0.94 28.2 - 2.40% 0.56 44.2 + 1.60% 1.38 24.0 - 5.00% 0.42 37.2 - 1.80% 0.67 70.0 + 22.60% 6.95 32.2 - 1.80% 0.68 48.4 + 6.60% 2.14 33.6 - 8.00% 0.30 0.00% 1.00 27.4 26.0 + 1.20% 1.35 21.0 0.00% 1.00 19.4 + 0.40% 1.12 28.2 - 0.20% 0.96 25.6 + 1.60% 1.38 27.8 + 1.80% 1.56 22.2 + 0.60% 1.13 26.4 - 2.20% 0.70 23.8 + 2.20% 1. c g p code model) could be just downloaded and used out of the box without any cost, the personalized T5ğ‘ ğ‘šğ‘ğ‘™ğ‘™ requires fine-tuning which has cost. However, we expect the T5ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ to have higher inference cost (i.e., the cost of generating one prediction is higher). Section 2.4.3 details how we computed the training and inference costs for the models. Remember that for the personalized T5ğ‘ ğ‘šğ‘ğ‘™ğ‘™ , we consider the training cost of both the cheapest (146.3k training instances) and the most expensive (888k training instances) organization-specific T5ğ‘ ğ‘šğ‘ğ‘™ğ‘™ from our study as sort of lowerand upper-bounds. Fig. 4 shows on the ğ‘¦ axis the GPU renting cost for all three models (cheapest and most expensive organizationspecific T5ğ‘ ğ‘šğ‘ğ‘™ğ‘™ and the generic T5ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ ) given different number of performed inferences (ğ‘¥ axis). When no inferences are performed, T5ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ costs, in our simulation, 0$ since we are assuming this is generic model that the company downloaded and used out of the box. The T5ğ‘ ğ‘šğ‘ğ‘™ğ‘™ models, instead, bring with them the training cost (from minimum of 0.75$ to maximum of 4.53$). As we can see, there is breakeven point after at least 44,948 and at most 272,824 inferences (bestand worst-case scenario). This leads to the following question: In how much time do the developers of an organization reach this number of code completion inferences? From an internal study at Microsoft [63], we know that the average number of weekly recommendations that Copilot triggers to single developer is 1,150. This means that if we consider software company employing 10 developers, the breakeven point will be reached after four (best-case) to 24 (worst-case) weeks. With 40 developers, this goes down to 1-6 weeks."
        },
        {
            "title": "Summary of Findings",
            "content": "Thanks to personalized fine-tuning, company could deploy 10 smaller model being equivalent in terms of code completion performance to the larger model. In terms of costs, the breakeven point is reached in few weeks (depending on the number of developers employed and the size of the used fine-tuning dataset)."
        },
        {
            "title": "Manuscript submitted to ACM",
            "content": "26 Giagnorio et al. Fig. 4. Cost-effectiveness analysis: Generic T5ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ vs organization-specific T5ğ‘ ğ‘šğ‘ğ‘™ğ‘™ ."
        },
        {
            "title": "4 SUMMARY OF MAIN FINDINGS",
            "content": "We summarize in the following the main findings output of our study, discussing their implications for researchers and software companies. (1) The boost in performance obtained by developer-specific models is often not sufficient to justify the additional effort of collecting the needed training data and run the fine-tuning (Goal 1). The main issue with developerspecific fine-tuning is the lack of training data for most of developers. This has been observed even on large organization such as Apache, for which hundreds of long-lived repositories were mined. Thus, we expect the lack of developer-specific data to be show-stopper for most companies. However, our analysis on the impact of the training data size from Section 3.3 also showed that, given fixed amount of training datageneric, organization-specific, or developer-specificthere is clear ranking in their effectiveness: the more specific the training data, the better the provided boost in performance, with the developer-specific ones being the most precious. Basically, assuming the possibility to increase the size of the developer-specific training sets, such personalized fine-tuning could be even more successful than the organization-specific one. Researchers could look into strategies such as data augmentation to address this limitation. On top of this, in our study we did not look into the performance provided by combination of an organization-specific fine-tuning followed by developer-specific fine-tuning. Also this strategy could bring benefits that we did not investigate. (2) An organization-specific fine-tuning should be the obvious choice for most companies interested in deploying an in-house code completion model (Goal 1). As explained, our analyses showed that this is due to the much higher number of training instances that can be collected at organization-level. While the training cost is much higher than the one of the developer-specific dataset, single fine-tuning would be required in real scenario, while the developer-specific fine-tuning requires the training of different model for each developer. Also, it is more convenient to deploy and maintain single model. (3) The increase in performance observed with both specializations is not simply due to higher number of training instances as compared to the baselines (Goal 2). Indeed, by further fine-tuning the baselines on generic data (and not on organization/developer-specific data) we did not observe an increase in performance comparable to the one"
        },
        {
            "title": "Manuscript submitted to ACM",
            "content": "Why Personalizing Deep Learning-Based Code Completion Tools Matters 27 provided by the two specializationssee Section 3.3. This result stresses the key role played by the specificity of training data, and calls for additional investigation pertaining other code-related tasks (e.g., automated bug-fixing, code review). (4) The increase in performance ensured by personalization can be generalized to models having different architecture and size (Goal 3). Indeed, all models we experimented with (T5 and Code Llama) benefitted from major performance improvements, independently from their size. This is major finding of our study, since we can conjecture that even the code completion performance of very large models could be further boosted via personalization. Also, the inclusion of Code Llama in our experiments demonstrated that personalization can also work in scenarios in which code from the target organization was likely already seen at pre-training time. (5) Fine-tuning organization-specific DL models can be cost-effective (Goal 4). Indeed, we show that thanks to personalized fine-tuning, DL models can achieve code completion performance on par with those of models being 10 larger (e.g., an organization-specific T5ğ‘ ğ‘šğ‘ğ‘™ğ‘™ achieves the same performance of generic T5ğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’ model). The lower inference costs of smaller models will allow the companies to save money in the long run, since only few weeks are needed to reach breakeven point and amortize the training cost, depending on the number of developers employed in the companysee detailed analysis in Section 3.5."
        },
        {
            "title": "5 THREATS TO VALIDITY",
            "content": "Construct validity. One threat is related to how we assess the code completion performance. While an EM prediction is likely to be useful for developers, it is difficult to speculate about non-EM predictions. The latter may be valuable while still being different from the expected target (e.g., the recommended code is different but semantically equivalent). We partially address this threat by also employing the CrystalBLEU as an evaluation metric, to at least provide measure of how far the prediction is from the target. Internal validity. As design choice, we adopted the original architecture and hyperparameters for all models subject of our study. This was due to the high number of models we had to train for our study (396), which would have further increased with hyperparameters tuning. However, since we compared the baseline and the personalized models when using the same exact configuration, we expect no major impact of this choice on our main findings. Another concern may be related to the effectiveness of the trainings we performed, especially when looking at the models we trained from scratch (i.e., the T5 models). While it is difficult to make fair comparison with other T5 models from the literature which have been trained on different datasets and evaluated on different test sets, one possible point of reference about the expected performance of T5 for code completion comes from the work by Ciniselli et al. [18], in which the authors experiment the T5ğ‘ ğ‘šğ‘ğ‘™ğ‘™ in three different completion scenarios, namely token-masking (i.e., completing the last ğ‘› tokens of statement, with ğ‘› capped to 10), construct-masking (i.e., predicting specific code constructs, such as the conditions of if statements), and block-masking (i.e., predicting up to two complete code statements). Our test sets feature completions masking up to 50 tokens, possibly spanning across multiple statements, thus being similar to their block-masking scenario. When using pre-training and single-task fine-tuning (i.e., the same training procedure we adopt), Ciniselli et al. achieved 27.2% of EM predictions in the block-masking scenario on Java code. On average, across the 136 developers considered in our study, our baseline T5ğ‘ ğ‘šğ‘ğ‘™ğ‘™ (no personalized fine-tuning) achieved 26.4% of EM predictions, thus being aligned with previous findings [18]. This provides some confidence about the correctness of the training procedure."
        },
        {
            "title": "Manuscript submitted to ACM",
            "content": "28 Giagnorio et al. External validity. While we considered fairly high number of developers for our study (136), we focused on two organizations and one programming language (Java). Also, we used T5 and Code Llama as representative DL models for code completion. Our findings may not generalize to other settings."
        },
        {
            "title": "6 RELATED WORK",
            "content": "Several DL-based techniques have been proposed to improve the automation provided by code completion tools (see e.g., [18, 19, 36, 39, 47, 55]). Differently from our work, these studies focus on training the model with general coding knowledge provided via large-scale datasets. Other recent studies propose alternative approaches by leveraging in-context learning abilities of large language models [9, 13, 62]. In this section, we mainly discuss studies centered on the customization of recommender systems for code generation tasks and empirical investigations looking at code recommender tools from other perspectives [18, 20, 31, 35, 37, 38, 40, 43, 45, 61, 64], focusing on reported findings which may relate to the motivations and outcome of our study."
        },
        {
            "title": "6.1 Personalized Source Code Recommendations",
            "content": "To the best of our knowledge, only few works in the literature targeted the personalization of code recommendations. Saraiva et al. [53] conducted preliminary study on the performance of ğ‘›-gram language models on the Microsoft Office suite using three levels of personalization: application-specific, developer-specific, and time-specific. Their findings show that ğ‘›-gram models trained on single project always perform better than general model working on the entire Office source code or specific developer corpus. Furthermore, they found that models built on specific time-based datasets do not lead to particular performance improvements. In this work, we applied similar idea to DL-based code completion, being the state of the practice nowadays. Allamanis et al. [10] presented Naturalize, framework recommending natural identifier names and formatting choices which have been learned from given codebase, thus improving the stylistic consistency of the project. Our work embraces the basic idea proposed in this work, looking however at the impact of personalization on the code completion capabilities of DL models. Ahmed and Devanbu et al. [8] investigated the usage of few-shot learning for customizing code summarization tasks. The authors collected code and summary pairs from eight repositories [42]. Then, they evaluated the few-shot capabilities of the model when preceding each query with 10 examples of the same project. They found that using project-specific examples instead of cross-project instances can improve the inference accuracy of the model. The most related work to our study is the one recently presented by Zlotchevski et al. [65], who studied whether the automated generation of tests can be improved by further fine-tuning DL model on specific software repository. Given DL model already fine-tuned to generate unit tests, the authors further trained it on code coming from specific code repository, reporting an improved accuracy of the suggestions. In this work, we focus on the more generic task of DL-based code completion, which is used by millions of developers and thousands of companies [7] via tools such as Copilot. Also, we look at different levels of personalization, related to whole organization and single developer, which have not been previously explored in the literature."
        },
        {
            "title": "6.2 Empirical Studies on Code Recommender Systems",
            "content": "MÄƒrÄƒs, oiu et al. [43] studied how developers use code completion tools, showing that they make extensive use of these tools but often discard the provided suggestions, especially when they feature APIs they are not familiar to. This helps motivating the need for personalized code completion."
        },
        {
            "title": "Manuscript submitted to ACM",
            "content": "Why Personalizing Deep Learning-Based Code Completion Tools Matters 29 Hellendoorn et al. [31] reported the inadequacy of artificial benchmarks in evaluating code completion tools, since these do not reflect the complexity of real-world completions. Similar findings were reported by Liu et al. [40] for the task of requirements implementation (i.e., generating code starting from natural language description). These findings are among the reasons why our test sets feature code completions derived by real code changes implemented by software developers. Ciniselli et al. [18] showed that T5ğ‘ ğ‘šğ‘ğ‘™ğ‘™ can accurately recommend code completions spanning across single statement (69% of accuracy) and still achieve good results when dealing with the completion of entire statements (29%). This motivated our selection of the subject DL model. Other studies focused on the effectiveness of the support provided by code completion tools to developers [35, 45, 61], documenting limitations of these tools, including: the lack of improvement in developers productivity [61], the presence of low quality recommendations [35], and issues related to their robustness [45]. These works point to the need for additional research aimed at improving code recommenders."
        },
        {
            "title": "7 CONCLUSION AND FUTURE WORK",
            "content": "In this work, we investigate how personalizing DL-based code completion tools can help in boosting their performance. We show that, by fine-tuning generic code completion model on personalized instances (i.e., completions from the same developer/organization), it can achieve significantly better performance, also when compared to model fine-tuned on the same number of non-personalized instances. Our results hold along four dimensions: (i) different levels of personalization (organization-specific and developer-specific data), with organization-specific personalization being, however, more effective thanks to the availability of more training data; (ii) different developers (136) from two different organizations (Apache and Spring); (iii) different sizes of the training dataset used to personalize the model (between 1k and 908.1k instances); and (iv) different model sizes and architecture (60M and 750M T5 models and 7B Code Llama). Our findings show that most developers within an organization can benefit from personalized recommendations, even when no developer-specific data is available (e.g., using the organization-specific model for developers new to the organization). Furthermore, companies providing industry-ready code completion tools (e.g., GitHub Copilot [3]) may find new business opportunities by offering personalized models to their customers, while smaller-scale models deployed in-house, when personalized, may be competitive and closer to what offered by larger models. In future work, we plan to: (i) study the impact of various hyperparameters (e.g., learning rate) on the personalization process; and (ii) investigate new approaches to enhance personalization such as auto-regressive models, prompting, and reinforcement learning. Lastly, we also plan to investigate online approaches for regularly training these models on relevant and up-to-date code, and to cover further completion tasks (e.g., modifying single token instead of entire code lines or blocks). All code and data used in our study is publicly available [5]."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "We are deeply grateful to the I3US Institute and the SCORE Lab at the University of Seville for providing us access to their HPC cluster, on which the experiments were run, and without which this research would not be possible. We acknowledge the financial support of the Swiss National Science Foundation for the PARSED project (SNF Project No. 219294)."
        },
        {
            "title": "Manuscript submitted to ACM",
            "content": "30 Giagnorio et al. REFERENCES [1] [n. d.]. Apache Commons BeanUtils. https://github.com/apache/commons-beanutils/. Accessed: 2024-03-05. [2] [n. d.]. CodeParrot GitHub Code dataset. https://huggingface.co/datasets/codeparrot/github-code. Accessed: 2024-02-08. [3] [n. d.]. GitHub Copilot Your AI pair programmer. https://github.com/features/copilot/. Accessed: 2024-03-10. [4] [n. d.]. javalang - PyPI. https://pypi.org/project/javalang/. Accessed: 2024-02-08. [5] [n. d.]. Replication Package. https://doi.org/10.5281/zenodo.10817220. [6] [n. d.]. T5v1.1. https://huggingface.co/docs/transformers/en/model_doc/t5v1.1. Accessed: 2024-02-08. [7] [n.d.]. Microsoft Fiscal Year 2024 Second Quarter Earnings Conference Call. https://www.microsoft.com/en-us/investor/events/fy-2024/earnings-fy2024-q2.aspx. Accessed: 2024-02-02. [8] Toufique Ahmed and Premkumar Devanbu. 2022. Few-shot training LLMs for project-specific code-summarization. In Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering. 15. [9] Toufique Ahmed, Kunal Suresh Pai, Premkumar Devanbu, and Earl Barr. 2024. Automatic semantic augmentation of language model prompts (for code summarization). In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering. 113. [10] Miltiadis Allamanis, Earl T. Barr, Christian Bird, and Charles Sutton. 2014. Learning natural coding conventions. In 22nd ACM/SIGSOFT International Symposium on Foundations of Software Engineering, FSE. 281293. [11] Uri Alon, Roy Sadaka, Omer Levy, and Eran Yahav. 2020. Structural language models of code. In International Conference on Machine Learning, ICML. 245256. [12] Muhammad Asaduzzaman, Chanchal K. Roy, Kevin A. Schneider, and Daqing Hou. 2014. Context-Sensitive Code Completion Tool for Better API Usability. In 30th IEEE International Conference on Software Maintenance and Evolution ICSME. 621624. [13] Ramakrishna Bairi, Atharv Sonwane, Aditya Kanade, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, Ashok, Shashank Shet, et al. 2023. Codeplan: Repository-level coding using llms and planning. arXiv preprint arXiv:2309.12499 (2023). [14] Berkay Berabi, Jingxuan He, Veselin Raychev, and Martin Vechev. 2021. TFix: Learning to Fix Coding Errors with Text-to-Text Transformer. In Proceedings of the 38th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 139), Marina Meila and Tong Zhang (Eds.). PMLR, 780791. [15] Marcel Bruch, Martin Monperrus, and Mira Mezini. 2009. Learning from examples to improve code completion systems. In 7th ACM Joint Meeting of the European Software Engineering Conference and the ACM/SIGSOFT International Symposium on Foundations of Software Engineering ESEC-FSE. 213222. [16] Federico Cassano, John Gouwar, Francesca Lucchetti, Claire Schlesinger, Anders Freeman, Carolyn Jane Anderson, Molly Feldman, Michael Greenberg, Abhinav Jangda, and Arjun Guha. 2024. Knowledge transfer from high-resource to low-resource programming languages for code llms. Proceedings of the ACM on Programming Languages 8, OOPSLA2 (2024), 677708. [17] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021). [18] Matteo Ciniselli, Nathan Cooper, Luca Pascarella, Antonio Mastropaolo, Emad Aghajani, Denys Poshyvanyk, Massimiliano Di Penta, and Gabriele Bavota. 2021. An Empirical Study on the Usage of Transformer Models for Code Completion. IEEE Transactions on Software Engineering, TSE abs/2108.01585, 01 (2021), 11. [19] Matteo Ciniselli, Nathan Cooper, Luca Pascarella, Denys Poshyvanyk, Massimiliano Di Penta, and Gabriele Bavota. 2021. An Empirical Study on the Usage of BERT Models for Code Completion. In 18th IEEE/ACM International Conference on Mining Software Repositories, MSR 2021. 108119. [20] Matteo Ciniselli, Luca Pascarella, and Gabriele Bavota. 2022. To What Extent do Deep Learning-based Code Recommenders Generate Predictions by Cloning Code from the Training Set?. In 19th IEEE/ACM International Conference on Mining Software Repositories, MSR. 167178. [21] Ozren Dabic, Emad Aghajani, and Gabriele Bavota. 2021. Sampling Projects in GitHub for MSR Studies. In 18th IEEE/ACM International Conference on Mining Software Repositories, MSR. 560564. [22] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT. 41714186. [23] Aryaz Eghbali and Michael Pradel. 2022. CrystalBLEU: Precisely and Efficiently Measuring the Similarity of Code. In Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering. 112. [24] Neil A. Ernst and Gabriele Bavota. 2022. AI-Driven Development Is Here: Should You Worry? IEEE Softw. 39, 2 (2022), 106110. https: //doi.org/10.1109/MS.2021.3133805 [25] Johannes Eschbach-Dymanus, Frank Essenberger, Bianka Buschbeck, and Miriam Exel. 2024. Exploring the Effectiveness of LLM Domain Adaptation for Business IT Machine Translation. In Proceedings of the 25th Annual Conference of the European Association for Machine Translation (Volume 1), Carolina Scarton, Charlotte Prescott, Chris Bayliss, Chris Oakley, Joanna Wright, Stuart Wrigley, Xingyi Song, Edward Gow-Smith, Rachel Bawden, VÃ­ctor SÃ¡nchez-Cartagena, Patrick Cadwell, Ekaterina Lapshinova-Koltunski, Vera CabarrÃ£o, Konstantinos Chatzitheodorou, Mary Nurminen, Diptesh Kanojia, and Helena Moniz (Eds.). 610622. [26] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. 2020. Codebert: pre-trained model for programming and natural languages. arXiv preprint arXiv:2002.08155 (2020)."
        },
        {
            "title": "Manuscript submitted to ACM",
            "content": "Why Personalizing Deep Learning-Based Code Completion Tools Matters 31 [27] Isha Ganguli, Rajat Subhra Bhowmick, Shivam Biswas, and Jaya Sil. 2021. Empirical Auto-Evaluation of Python Code for Performance Analysis of Transformer Network Using T5 Architecture. In 2021 8th International Conference on Smart Computing and Communications (ICSCC). 7579. https://doi.org/10.1109/ICSCC51209.2021.9528123 [28] Christoph Gote and Christian Zingg. 2021. gambit An Open Source Name Disambiguation Tool for Version Control Systems. In 2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR). 8084. https://doi.org/10.1109/MSR52588.2021.00021 [29] Robert J. Grissom and John J. Kim. 2005. Effect sizes for research: broad practical approach (2nd edition ed.). Lawrence Earlbaum Associates. [30] Vincent J. Hellendoorn and Premkumar Devanbu. 2017. Are Deep Neural Networks the Best Choice for Modeling Source Code?. In 11th ACM/SIGSOFT Joint Meeting on Foundations of Software Engineering ESEC-FSE. 763?773. [31] Vincent J. Hellendoorn, Sebastian Proksch, Harald C. Gall, and Alberto Bacchelli. 2019. When code completion fails: case study on real-world completions. In 41st IEEE/ACM International Conference on Software Engineering, ICSE. 960970. [32] Abram Hindle, Earl T. Barr, Zhendong Su, Mark Gabel, and Premkumar T. Devanbu. 2012. On the naturalness of software. In 34th IEEE/ACM International Conference on Software Engineering, ICSE. 837847. [33] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of Large Language Models. In International Conference on Learning Representations. https://openreview.net/forum?id=nZeVKeeFYf9 [34] Kai Huang, Jian Zhang, Xiangxin Meng, and Yang Liu. 2024. Template-Guided Program Repair in the Era of Large Language Models. In 2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE). IEEE Computer Society, 367379. [35] Saki Imai. 2022. Is github copilot substitute for human pair-programming? an empirical study. In Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: Companion Proceedings. 319321. [36] Maliheh Izadi, Roberta Gismondi, and Georgios Gousios. 2022. CodeFill: Multi-token Code Completion by Jointly learning from Structure and Naming Sequences. In 44th IEEE/ACM International Conference on Software Engineering, ICSE. 401412. [37] Maliheh Izadi, Jonathan Katzy, Tim Van Dam, Marc Otten, Razvan Mihai Popescu, and Arie Van Deursen. 2024. Language models for code completion: practical evaluation. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering. 113. [38] Xianhao Jin and Francisco Servant. 2018. The hidden cost of code completion: Understanding the impact of the recommendation-list length on its efficiency. In 15th IEEE/ACM International Conference on Mining Software Repositories, MSR. 7073. [39] Fang Liu, Ge Li, Yunfei Zhao, and Zhi Jin. 2020. Multi-task Learning based Pre-trained Language Model for Code Completion. In 35th IEEE/ACM International Conference on Automated Software Engineering, ASE. 473485. [40] Hui Liu, Mingzhu Shen, Jiaqi Zhu, Nan Niu, Ge Li, and Lu Zhang. 2020. Deep learning based program generation from requirements text: Are we there yet? IEEE Transactions on Software Engineering 48, 4 (2020), 12681289. [41] Ilya Loshchilov and Frank Hutter. 2019. Decoupled Weight Decay Regularization. In 7th International Conference on Learning Representations, ICLR. [42] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin B. Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, and Shujie Liu. 2021. CodeXGLUE: Machine Learning Benchmark Dataset for Code Understanding and Generation. In 35th Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks. [43] Mariana Marasoiu, Luke Church, and Alan F. Blackwell. 2015. An empirical investigation of code completion usage by professional software developers. In 26th Annual Workshop of the Psychology of Programming Interest Group, PPIG. 14. [44] Antonio Mastropaolo, Nathan Cooper, David Nader Palacio, Simone Scalabrino, Denys Poshyvanyk, Rocco Oliveto, and Gabriele Bavota. 2022. Using Transfer Learning for Code-Related Tasks. IEEE Transactions on Software Engineering, TSE (2022), 120. [45] Antonio Mastropaolo, Luca Pascarella, Emanuela Guglielmi, Matteo Ciniselli, Simone Scalabrino, Rocco Oliveto, and Gabriele Bavota. 2023. On the Robustness of Code Generation Techniques: An Empirical Study on GitHub Copilot. In 45th IEEE/ACM International Conference on Software Engineering, ICSE 2023, Melbourne, Australia, May 14-20, 2023. IEEE, 21492160. [46] Quinn McNemar. 1947. Note on the sampling error of the difference between correlated proportions or percentages. Psychometrika 12, 2 (1947), 153157. [47] Phuong T. Nguyen, Juri Di Rocco, Claudio Di Sipio, Davide Di Ruscio, and Massimiliano Di Penta. 2022. Recommending API Function Calls and Code Snippets to Support Software Development. IEEE Trans. Software Eng. 48, 7 (2022), 24172438. [48] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: Method for Automatic Evaluation of Machine Translation. In 40th Annual Meeting on Association for Computational Linguistics, ACL. 311318. [49] Sida Peng, Eirini Kalliamvakou, Peter Cihon, and Mert Demirer. 2023. The impact of AI on developer productivity: Evidence from GitHub Copilot. arXiv preprint arXiv:2302.06590 (2023). [50] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with Unified Text-to-Text Transformer. Journal of Machine Learning Research 21, 140 (2020), 167. http: //jmlr.org/papers/v21/20-074.html [51] Romain Robbes and Michele Lanza. 2010. Improving Code Completion with Program History. Automated Software Engineering 17, 2 (2010), 181212. [52] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, JÃ©rÃ©my Rapin, et al. 2023. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950 (2023). [53] Juliana Saraiva, Christian Bird, and Thomas Zimmermann. 2015. Products, developers, and milestones: how should build my N-Gram language model. In Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering. 9981001."
        },
        {
            "title": "Manuscript submitted to ACM",
            "content": "32 Giagnorio et al. [54] Weisong Sun, Yun Miao, Yuekang Li, Hongyu Zhang, Chunrong Fang, Yi Liu, Gelei Deng, Yang Liu, and Zhenyu Chen. 2024. Source Code Summarization in the Era of Large Language Models. In 2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE). IEEE Computer Society, 419431. [55] Alexey Svyatkovskiy, Sebastian Lee, Anna Hadjitofi, Maik Riechert, Juliana Vicente Franco, and Miltiadis Allamanis. 2021. Fast and Memory-Efficient Neural Code Completion. In 18th IEEE/ACM International Conference on Mining Software Repositories, MSR. 329340. [56] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023). [57] Rosalia Tufano, Simone Masiero, Antonio Mastropaolo, Luca Pascarella, Denys Poshyvanyk, and Gabriele Bavota. 2022. Using Pre-Trained Models to Boost Code Review Automation. In 44th IEEE/ACM International Conference on Software Engineering, ICSE. 22912302. [58] Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. 2021. Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. arXiv preprint arXiv:2109.00859 (2021). [59] Martin Weyssow, Xin Zhou, Kisub Kim, David Lo, and Houari Sahraoui. 2023. Exploring parameter-efficient fine-tuning techniques for code generation with large language models. arXiv preprint arXiv:2308.10462 (2023). [60] Frank Wilcoxon. 1945. Individual Comparisons by Ranking Methods. Biometrics Bulletin 1, 6 (1945), 8083. [61] Frank F. Xu, Bogdan Vasilescu, and Graham Neubig. 2022. In-IDE Code Generation from Natural Language: Promise and Challenges. ACM Trans. Softw. Eng. Methodol. 31, 2 (2022), 29:129:47. [62] Fengji Zhang, Bei Chen, Yue Zhang, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen. 2023. RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation. arXiv preprint arXiv:2303.12570 (2023). [63] Albert Ziegler. 2022. GitHub Copilot research recitation. https://github.blog/ai-and-ml/github-copilot/github-copilot-research-recitation/. Accessed: 2024-11-10. [64] Albert Ziegler, Eirini Kalliamvakou, X. Alice Li, Andrew Rice, Devon Rifkin, Shawn Simister, Ganesh Sittampalam, and Edward Aftandilian. 2022. Productivity assessment of neural code completion. In International Symposium on Machine Programming. 2129. [65] Andrei Zlotchevski, Dawn Drain, Alexey Svyatkovskiy, Colin B. Clement, Neel Sundaresan, and Michele Tufano. 2022. Exploring and evaluating personalized models for code generation. In Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2022). 15001508. revised DD MMMM YYYY; accepted DD MMMM YYYY"
        }
    ],
    "affiliations": [
        "UniversitÃ  della Svizzera italiana, Switzerland"
    ]
}