{
    "paper_title": "Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning",
    "authors": [
        "Tian Xie",
        "Zitian Gao",
        "Qingnan Ren",
        "Haoming Luo",
        "Yuqian Hong",
        "Bryan Dai",
        "Joey Zhou",
        "Kai Qiu",
        "Zhirong Wu",
        "Chong Luo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Inspired by the success of DeepSeek-R1, we explore the potential of rule-based reinforcement learning (RL) in large reasoning models. To analyze reasoning dynamics, we use synthetic logic puzzles as training data due to their controllable complexity and straightforward answer verification. We make some key technical contributions that lead to effective and stable RL training: a system prompt that emphasizes the thinking and answering process, a stringent format reward function that penalizes outputs for taking shortcuts, and a straightforward training recipe that achieves stable convergence. Our 7B model develops advanced reasoning skills-such as reflection, verification, and summarization-that are absent from the logic corpus. Remarkably, after training on just 5K logic problems, it demonstrates generalization abilities to the challenging math benchmarks AIME and AMC."
        },
        {
            "title": "Start",
            "content": "Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning Tian Xie1* Zitian Gao2 Qingnan Ren3 Haoming Luo3 Yuqian Hong1* Bryan Dai2 Joey Zhou2 Kai Qiu1 Zhirong Wu1 Chong Luo1 1Microsoft Research Asia 2Ubiquant 3Independent {unakar666, hmluo65536, hoknight0}@gmail.com {v-yuqianhong, kai.qiu, wu.zhirong, chong.luo}@microsoft.com {ztgao02, cbdai, jzhou}@ubiquant.com"
        },
        {
            "title": "Abstract",
            "content": "Inspired by the success of DeepSeek-R1, we explore the potential of rule-based reinforcement learning (RL) in large reasoning models. To analyze reasoning dynamics, we use synthetic logic puzzles as training data due to their controllable complexity and straightforward answer verification. We make some key technical contributions that lead to effective and stable RL training: system prompt that emphasizes the thinking and answering process, stringent format reward function that penalizes outputs for taking shortcuts, and straightforward training recipe that achieves stable convergence. Our 7B model develops advanced reasoning skillssuch as reflection, verification, and summarizationthat are absent from the logic corpus. Remarkably, after training on just 5K logic problems, it demonstrates generalization abilities to the challenging math benchmarks AIME and AMC. 5 2 0 2 0 2 ] . [ 1 8 6 7 4 1 . 2 0 5 2 : r Figure 1: Validation accuracy and mean response length during RL training, illustrating how the model autonomously learns to allocate more thinking compute for improved performance. Remarkably, the model also demonstrates impressive generalization on completely unseen datasets (AIME, AMC). * Work done during internship at MSRA. Open-Source Research Project. Corresponding author."
        },
        {
            "title": "1 Introduction",
            "content": "The post-training phase of large language models (LLMs) has advanced rapidly [19], with models like DeepSeek-R1 [3], Kimi-K1.5 [15], and OpenAI-o1 [10] demonstrating remarkable reasoning abilities. DeepSeek-R1, in particular, introduced simple yet effective rule-based reinforcement learning (RL) approach, enabling emergent reasoning patterns without relying on traditional scaffolding techniques such as Monte Carlo Tree Search (MCTS) [4, 6, 18, 20] or Process Reward Models (PRM) [8]. Despite these advancements, substantial gaps remain in translating these developments into reproducible research. While DeepSeek-R1 provides open-source model weights, it does not release the corresponding training code or dataset. This absence raises critical questions: (1) Can similar reasoning abilities emerge in smaller-scale models? (2) What is the optimal training data structure for fostering such capabilities? (3) What methodologies can reliably replicate these results? Addressing these questions requires controlled experimental frameworks that isolate key variables. While mathematics is often treated as the common testbed for reasoning, widely-used math datasets like GSM8K [2] and Omini-MATH [5] suffer as the training data due to its uncontrolled variance in problem complexity, which may span various logical induction depths. To overcome this limitation, we leverage procedurally generated Knights and Knaves (K&K) logic puzzle dataset [17], which allows controllable difficulty levels and ease of rule-based reward verification, making it ideal for studying reasoning dynamics. In this paper, we introduce Logic-RL, rule-based reinforcement learning framework that acquires R1-like reasoning patterns through training on logic puzzles. The training framework adopts the REINFORCE++ algorithm [7] and the reward designs from DeepSeek-R1 for post-training. While naive training would lead to collapsed solutions, we propose practical system prompt and stringent format reward to avoid the reasoning model for taking shortcuts. We also incorporate few modifications to the REINFORCE++ algorithm for improved performance. As the RL training undergoes, we observe that the model naturally allocates more training steps to reason. This computational expansion scales from generating hundreds to thousands of tokens, enabling deeper exploration and refinement of its thought processes. We evaluate the model performance on the challenging math benchmarks for reasoning. With merely 5,000 procedurally generated logic puzzles, our 7B model improves by 125% on AIME and 38% on AMC against the base model. This cross-domain generalization capability suggests that RL-trained reasoning heuristics develop abstract problem-solving schemata rather than relying on domain-specific pattern matching. Besides the technical contributions mentioned above, our study also makes several interesting findings: Longer responses dont guarantee better reasoning. Length alone is not valid performance metric for training time evaluation. The most efficient reasoning comes from the shortest path. Language mixing hinders reasoning. This observation underscores the need for language consistency penalty in reward modeling. Increasing thinking tokens do help. RL training naturally boosts the frequency of reflectionrelated words, suggesting correlation between certain tokens frequency and performance. SFT memorizes; RL generalizes. SFT relies heavily on memorization, often leading to superficial shortcut learning, whereas RL self-evolves with minimal dependence on dataset structure. Cold start is bonus, not necessity. Training dynamics remain surprisingly similar whether starting from base or instruct model, though the latter exhibits slightly better performance. Curriculum Learning still matters. Under fixed data curation ratio, well-designed curriculum learning approach always outperforms random shuffle."
        },
        {
            "title": "2.1 Data Synthesis",
            "content": "The Knights and Knaves (K&K) puzzles [17] constitute an algorithmically generated reasoning dataset. In these puzzles, characters are either knights, who always tell the truth, or knaves, who 2 always lie. The objective is to determine the nature of each character based on their statements. This dataset is distinguished by its high degree of controllability: 1. Procedural Generation: Puzzles are systematically generated using logic templates, ensuring both consistency and infinite variability. Importantly, these puzzles represent unseen data for the original model, making them ideal for testing generalization capabilities. 2. Controlled Difficulty Levels: The difficulty of the puzzles can be precisely adjusted, enabling the design of curriculum learning strategy. Difficulty is modulated by varying the number of characters (28) and the complexity of logical operations (14 combinations of Boolean operators). Furthermore, more complex puzzles can serve as out-of-distribution tests for models trained on simpler cases, providing insights into their ability to generalize. 3. Ease of Verification: Each puzzle has single, unambiguous ground truth answer, with correctness guaranteed by the generation algorithm. Solutions require strict deductive reasoning, allowing for accurate evaluation of model responses and minimizing the risk of reward hacking. An example of K&K puzzle Problem: very special island is inhabited only by knights and knaves. Knights always tell the truth, and knaves always lie. You meet 2 inhabitants: Zoey, and Oliver. Zoey remarked, \"Oliver is not knight\". Oliver stated, \"Oliver is knight if and only if Zoey is knave\". So who is knight and who is knave? Solution: (1) Zoey is knave (2) Oliver is knight The K&K puzzle 2.1 is exceptionally well-suited for further analysis due to its synthetic design and logical precision. Each puzzle is constructed using formal rules, ensuring that every problem has unique solution that can be deterministically verified. This eliminates the ambiguities commonly encountered in natural language tasks, enabling us to clearly distinguish between genuine reasoning capabilities and superficial memorization."
        },
        {
            "title": "2.2 Rule Based Reward Modeling",
            "content": "The reward serves as the primary training signal in reinforcement learning (RL), guiding the optimization process. We continuously monitored hacking behaviors in the models outputs, refining our reward design iteratively. This led to nearly unhackable, rule-based reward system that comprises only two types of rewards: Format Reward and Answer Reward. System Prompt You are helpful assistant. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and<answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>. Now the user asks you to solve logical reasoning problem. After thinking, when you finally reach conclusion, clearly state the identity of each character within <answer> </answer> tags. i.e., <answer> (1) Zoey is knight, (2) ... </answer>. Format Reward: We use regular expression extraction to enforce structured response format. The model is required to put its reasoning process within <think></think> tags and provide the final conclusion inside <answer></answer> tags. Additionally, we recommend including <think> tag directly at the end of the prompt, which significantly reduces the difficulty for the base model to follow our instructions. Under our early imperfect rule design, we consistently observed reward hacking phenomena, some of which are listed below: Skipping the <think></think> process and directly answering. Placing reasoning inside the <answer></answer> tag. Repeatedly guessing answers without proper reasoning. Including irrelevant nonsense in addition to providing the answer. Organizing correct answer in wrong manner for extraction. Revisiting the thinking phase after already outputting an <answer> due to insufficient reasoning. Repeating the original question or using phrases like \"thinking process here\" to avoid true reasoning. Accordingly, we iteratively refine our rule design. For example, each tag should appear exactly once and in the correct sequential order, the thinking process must include genuine reasoning, and the conclusion should be presented in an extractable and readable manner. By enforcing these constraints, we ensure that different actions receive appropriate rewards based on their adherence to the format. The format score (Sf ormat) is computed as follows: Sf ormat = (cid:26)1, -1, if format is correct if format is incorrect Answer Reward: The second component evaluates the correctness of the content in the models response. Once the format is validated, we check if the models answer matches the ground truth. The answer score (Sanswer) is computed as: Sanswer = 2, 1.5, 2, if the answer fully matches the ground truth if the answer partially mismatches the ground truth if the answer cannot be parsed or is missing"
        },
        {
            "title": "2.3 RL Algorithm",
            "content": "We adopt modified version of REINFORCE++ as our baseline algorithm, which has demonstrated superior performance compared to GRPO in our experimental setup. detailed comparison of these algorithms is provided in Section 4. Reinforce Return Calculation: The discounted cumulative rewards for each trajectory are computed as below, where γ is the discount factor, set to 1 in our experiments: Gt = (cid:88) k=t+1 γktrk Following recommendations from DeepSeek-Math [13], we incorporate several minor refinements into the implementation of REINFORCE++. First modification: Use KL Loss The KL-divergence between the response distributions of the RL model and the SFT model is calculated for each token. This divergence is incorporated as penalty term in the reward function of PPO during training. The per-token reward is defined as follows: r(st, at) = I(st = [EOS])r(x, y) βKL(t), where I(st = [EOS]) is an identity function that evaluates to 1 when the <eos> token is reached, and β controls the weight of the KL penalty. In contrast, the GRPO implementation does not include the KL-divergence as part of the reward function. Instead, it directly incorporates the KL-divergence into the loss function, arguing that this approach simplifies the computation and avoids unnecessary complexity. Following this rationale, we also use KL loss like GRPO: JGRPO(θ) = [qP (Q),{oi}G (cid:40) i=1πθold (Oq)] (cid:34)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 1 oi oi (cid:88) t=1 min ˆAi,t, clip πi,t θ πi,t θold (cid:33) (cid:35) , 1 ϵ, 1 + ϵ ˆAi,t βDKL[πθπref] (cid:41) . (cid:32) πi,t θ πi,t θold Second Modification: KL Estimation Another key distinction lies in how the KL-divergence is estimated. The default KL estimator for PPO is defined as follows: KL(t) = log (cid:18) πθold (atst) πθ(atst) (cid:19) . In contrast, GRPO employs an unbiased estimator for the KL-divergence, formulated as: DKL [πθπref] = πref (oi,tq, oi,<t) πθ (oi,tq, oi,<t) log πref (oi,tq, oi,<t) πθ (oi,tq, oi,<t) 1. This approach ensures that the KL estimate is always non-negative, whereas the original formulation may yield negative values. GRPOs estimator provides more stable and reliable measure of divergence during training. After implementing these modifications, we proceed with our experiments."
        },
        {
            "title": "2.4 Training Schedule",
            "content": "We directly train the model for 3600 steps with constant learning rate of 4 107 and temperature parameter of 0.7. During training, the model is directly exposed to mixed complexity logic puzzles ranging from 3 to 7 people. This straightforward training regimen achieves competitive performance as shown in the final results table. Through continuous training with these fixed hyperparameters, the model develops stable reasoning patterns characterized by logical exploration, intermediate verification, and systematic summarization before producing final answers. These emergent behaviors demonstrate the models capacity to handle complex logical reasoning tasks effectively. Other key parameters used during training are summarized in Table 1 Table 1: Important Training Parameters Algorithm Train Batch Size Rollout KL Coef Max Response Len REINFORCE++ 8 8 0."
        },
        {
            "title": "3 Experiment",
            "content": "We began by experimenting with various models from the Qwen2.5 series as potential baseline candidates. For instance, Qwen2.5-Math-7B exhibited strong tendency to generate Python code blocks, which often conflicted with our strict formatting requirements. Despite efforts to mitigate this behavior by removing system prompts and penalizing specific markdown styles, it remained challenging to fully address. Additionally, we tested both Qwen2.5-7B-Base and Qwen2.5-7B-Instruct as starting points. Surprisingly, we found that the base and instruct models displayed nearly identical training metrics during RL training, including validation accuracy, response length growth curves, and reward curves. detailed comparison between base & instruct model can be found in the appendix 7. However, the instruct model demonstrated slightly higher test accuracy, making it the preferred 5 choice. Consequently, we selected Qwen2.5-7B-Instruct-1M [22] as our baseline. See more base & instruct model training dynamics comparision in Appendix 10 Notably, despite the training dataset being limited to 3 to 7-person K&K logic puzzleswith fewer than 5,000 synthetic samplesthe model demonstrates remarkable ability to generalize to out-ofdistribution (OOD) scenarios, such as 8-person puzzles. Model o3-mini-high o1-2024-12-17 Deepseek-R1 GPT-4o GPT-4o-mini NuminaMath-7B-CoT Deepseek-Math-7B Qwen2.5-Base-7B 2 0.99 0.83 0.91 0.68 0.63 0.28 0.35 0.41 Qwen2.5-7B-Instruct-1M 0.49 + Logic-RL Difficulty by Number of People 3 0.98 0.51 0.73 0.57 0.42 0.13 0.21 0.34 0.40 0.97 0.38 0.77 0.49 0.34 0.12 0.08 0.16 0.25 5 0.95 0.38 0.78 0.32 0.17 0.05 0.06 0. 0.11 6 0.94 0.35 0.75 0.23 0.09 0.01 0.02 0.00 0.06 0.89 0.30 0.88 0.21 0.10 0.00 0.00 0.00 0.02 8 0.83 0.20 0.83 0.11 0.01 0.00 0.00 0. 0.01 Avg. 0.94 0.42 0.81 0.37 0.25 0.08 0.10 0.14 0.19 0.990.50 0.990.59 0.940.69 0.920.81 0.910.85 0.800.78 0.670.48 0.890. Table 2: Comparison of different models including reasoning models and general models on K&K logic puzzle across various difficulty. Compared to the initial average length of 500 tokens, after 1k steps of RL, the output has almost linearly and steadily increased to 2000 tokens, significant increase of 4 times. As the response length increases, the model begins to exhibit more complex behaviors, such as reflection and exploration of alternative solutions. These behaviors emerge naturally, with no related data in our training set, and enhance the models ability to handle more complex tasks. These phenomena align closely with the results of R1 [3]. We will further discuss this in Section 4."
        },
        {
            "title": "4 Research Questions",
            "content": "RQ 1: How Does GRPO Compare to Other RL Algorithms? Does GRPO [13] outperform other reinforcement learning algorithms, such as REINFORCE++ and PPO, in terms of training stability, speed, and performance accuracy? Figure 2: Comparison of GRPO (Blue), REINFORCE++ (Red), and PPO (Green) performance (averaged by sliding window = 50) in terms of training speed, accuracy, and reward gain. The results indicate that PPO [12] achieved significant advantages in both accuracy and reward. However, it was 138% slower than REINFORCE++ in terms of training speed. On the other hand, REINFORCE++ [7] demonstrated superior stability, performance gains, and training efficiency compared to GRPO. Overall, REINFORCE++ outperformed GRPO across nearly all metrics, with 6 GRPO exhibiting the weakest performance among the three reinforcement learning algorithms evaluated in our experiments. RQ 2. Do certain thinking tokens and language-mixing phenemona improve reasoning? Does the inclusion of complex reasoning behaviours (such as exploration, verification, summarization, and backtracking) and language switching improve the models reasoning ability? Figure 3: Impact of complex reasoning behaviours and language mixing on reasoning performance. We analyzed the models answer rewards for responses containing the tokens shown in the figure. Responses with \"verify\" and \"re-evaluate\" scored significantly higher than those without these words. Conversely, responses containing certain tokens from other languages generally received lower scores. We observe that in our experiments: 1. Language mixing significantly decreases reasoning ability. 2. While terms like \"wait,\" \"verify,\" \"yet,\" and \"re-evaluate\" show significant improvement, not all complex thinking tokens enhance reasoning ability, as exemplified by \"recheck.\" 3. The complex reasoning behaviour recheck markedly diminishes reasoning ability, likely because its use signals the models uncertainty about its answer. 4. Theres clear difference between \"re-evaluate\" and \"reevaluate\": the former leads to much higher answer scores, while the latter lowers them. When we checked its origin responses, \"reevaluate\" almost never appeared, while \"re-evaluate\" showed up frequently. This may suggest the model is more comfortable with words it has seen more often in pretrain corpus. RQ 3: Does an Aha Moment Emerge During Training? Is there an observable Aha moment where the model exhibits significant leap in reasoning capability, such as the emergence of multi-step verification or reflection during the RL process? The emergence of sophisticated behaviors becomes increasingly evident as performance grows. These behaviors include reflective actionswhere the model revisits and reevaluates prior stepsand the spontaneous exploration of alternative problem-solving strategies. Such behaviors are not explicitly planted into training corpus but emerge organically through the models interaction with the reinforcement learning environment, consistent with findings by Ye et al. [23]. The \"aha moment\" referenced in the R1 report [3] primarily refers to the models sudden acquisition of complex reasoning behaviors. secondary interpretation involves the model spontaneously 7 verbalizing \"aha moment,\" such as in phrases like \"Wait, wait. Wait. Thats an aha moment can flag here.\" While our model did not exhibit this specific verbalization, Figure 4 shows that it displayed some complex reasoning behaviors (e.g., self-reflection, exploration, verification, summarization) even by step 10. Thus, we conclude that the RL process likely lacks sudden \"aha moment\"that is, complex reasoning behaviors do not abruptly emerge at specific training step, aligned with Liu et al. [9]. a: Verify b: Re-evaluate c: Check d: Yet e: Lets f: Chinese word Figure 4: Tracking the frequency of words in the first 1,800 training steps. 1. Reflective words like \"check\" and \"verify\" slowly increased (a)-(c). 2. Conversational phrases (e.g., \"Lets\") and cautious terms (e.g., \"yet\") became more frequent (d)-(e), 3. Chinese words began appearing in English responses (f). The frequency of all these words developed steadily without sudden jumps, suggesting that there may not be distinct \"aha moment.\" RQ 4: Can the Model Generalize to Out-of-Distribution (OOD) Tasks? To what extent can the trained model handle tasks that differ from its training data, particularly those that are out-of-distribution? Figure 5: Training Step vs. Accuracy on AIME (2021-2024) and AMC (2022-2023) Datasets. The ability of model to generalize beyond its training distribution is cornerstone of AI research. We investigate whether the complex reasoning abilitiessuch as exploration, verification, summarization, and backtrackingthat naturally emerged during RL process can transfer to highly challenging mathematical reasoning scenario, which we term Super OOD . 8 This evaluation leverages the widely-adopted AIME 20212024 (American Invitational Mathematics Examination) and AMC 20222023 (American Mathematics Competitions) benchmarks, both known for their rigorous and diverse problem sets. From Figure 5, we observe that the models Super OOD generalization capability is exceptionally strong, achieving an overall improvement of 125% on the AIME dataset and 38% on the AMC dataset. This synchronous improvement indicates that the reinforcement learning process not only enhances the models performance on in-distribution tasks but also facilitates the emergence of robust and transferable reasoning strategies. These findings show that the reasoning skills learned during RL training go beyond the specific patterns of the K&K dataset. This highlights RLs potential to generalize beyond their training environment. RQ 5: Which Generalizes Better, SFT or RL? Can post-training methods achieve more than just superficial alignment, which just learns format patterns? Can SFT or RL actually learn to learn, effectively generalizing to other domains? We investigate whether models merely memorize training data or truly learn reasoning skills. Following the setup in [17], we test this by comparing performance on familiar problems versus slightly altered ones.Two signs of memorization: High accuracy on seen problems, and low accuracy on slightly perturbed versions. So, we measure these using two metrics, denote model as , dataset as D. 1. Accuracy on Observed Problems: Acc(f ; D), the models accuracy on the training set (Tr). 2. Consistency Ratio: CR(f ; D), the ratio of correct solutions after small changes (perturbations) to those solved without changes. Perturbations preserve the puzzles core principle and difficulty. The Local Inconsistency-based Memorization Score is defined as: LiMem(f ; D) = Acc(f ; D) (1 CR(f ; D)) This score captures both memorization and sensitivity to changes. If models performance drops significantly when the problem format is altered, it likely hasnt learned the true reasoning skills required to solve similar but modified puzzles.We study two types of perturbations: (i) changing one persons statement to another bool logic expression, and (ii) reordering the statements between different people. Examples of perturbations are as follows: Perturbation Examples Original Problem: Zoey remarked, \"Oliver is not knight\". Oliver stated, \"Oliver is knight if and only if Zoey is knave\". So who is knight and who is knave? Statement Perturbation: Zoey remarked, \"Oliver is knight or Zoey is knight\". Oliver stated, \"Oliver is knight if and only if Zoey is knave\" Reorder Perturbation: Oliver stated, \"Oliver is knight if and only if Zoey is knave\". Zoey remarked, \"Oliver is not knight\" While it is challenging to collect suitable SFT dataset, we employ reject sampling to gather ground truth data, referred to as the RFT method. To explore which training paradigm offers better generalization performance, we apply two types of disturbances on training dataset, then compare RFT and RL with Test acc - Mem Score curve. For RFT settings, we use Reject Sampling on origin model, then use rule-based Best-of-N method to collect the correct yet the shortest response for further fine-tune. As result, we obtained the curve shown in Fig. 6, which illustrates the changes in unseen test accuracy and training dataset memorization over training process. 9 a: RL b: RFT Figure 6: RFT memorizes while RL generalizes. RFT (Reject sampling Fine-Tuning) slightly improves test accuracy at the expense of rapidly increasing LiM em(f ; r), indicating it mainly learns superficial answer format than geniue reasoning. In contrast, RL achieves higher test accuracy with minimal or even negative increase in LiM em(f ; r). Within the same LiM em interval, RL outperform RFT in test acc greatly, suggesting better generalization ability. higher memorization score indicates that more questions, which were originally correct, have turned incorrect due to the disturbances caused by the training, reflecting greater degree of memorization. SFT tends to superficial alignment, often becoming overly reliant on the original datas expression format. On the other hand, RL encourages the model to explore independently, fostering generalization abilities that stem from enhanced reasoning capabilities, consistent with findings in [1]. RQ 6: Is Curriculum Learning Still Necessary in RL? Does curriculum learning still matter in the RL paradigm? Specifically, is the sequential order of data important, or is it merely the curation ratio that matters? Figure 7: Comparison of test scores for curriculum learning and mixed-difficulty training. The plot uses rolling average (window size = 5) To evaluate the necessity of curriculum learning, we compare its effectiveness to mixed-difficulty approach. In curriculum learning, the model is trained sequentially on progressively more difficult datasets (3-7 people scenarios) for one epoch each. In contrast, the mixed-difficulty approach trains the model on all difficulty levels simultaneously within single epoch. All other hyperparameters are kept constant between the two methods. We analyze the test score trajectories using rolling average (window size = 5) to mitigate stochastic fluctuations and highlight underlying trends. The results in Figure 7 indicate that curriculum learning yields slightly higher test scores during intermediate training phases. However, this advantage diminishes in practical significance, as the performance difference during early training stages remains statistically negligible, suggesting limited impact on initial convergence. While curriculum learning may offer marginal theoretical benefit in terms of sample efficiency, its practical necessity is not conclusively supported, given the minimal real-world performance difference and the added complexity of staged training. RQ 7: Does Longer Response Length Guarantee Better Reasoning? Does an increase in response length during training directly improve models reasoning performance, or are these trends merely correlated? Figure 8: Comparison of response length, validation accuracy, and mean reward across training steps for positive and negative example models. This experiment investigates whether an increase in response length during training causally enhances reasoning performance. We compared two models trained using the same algorithm and base model but with different hyperparameters and dataset difficulties. Positive Example Model (Blue): Despite slight decrease in response length over time, this model demonstrated significant improvements in both validation accuracy and reward, indicating stronger reasoning and generalization abilities. Negative Example Model (Red): This model consistently increased its response length but showed no improvement in validation accuracy or reward. This suggests that increasing response length alone does not necessarily enhance reasoning capabilities. Figure 8 illustrates these findings: the positive models reward and accuracy improved while its response length decreased, whereas the negative models length increased without any corresponding performance gains. These divergent trends suggest that changes in response length are likely byproduct of training dynamics rather than causal driver of reasoning improvements. The observed increase in response length is likely side effect of reinforcement learning (RL) dynamics. While some studies report natural tendency for output length to grow as models generate more complex responses, this growth should be viewed as correlate rather than direct cause of improved reasoning. Importantly, there is no statistically significant evidence that the magnitude of length increase reliably predicts proportional gains in reasoning performance. In conclusion, longer responses do not always guarantee better reasoning. Enhanced reasoning capabilities may naturally lead to more detailed explanations, but artificially extending response length does not necessarily result in proportional performance improvements."
        },
        {
            "title": "5 Discussion and Future Work",
            "content": "While our study demonstrates the potential of Logic-RL in developing complex reasoning skills, it is important to note that our findings are based on small-scale logic dataset. The generalizability 11 of our results to large-scale real-world mathematical or coding scenarios remains to be explored. Future work should focus on extending our approach to more diverse and complex datasets to further validate its effectiveness and robustness. Our work will remain an open research project to benefit the community. Chain-of-Thought Long to Short Methods. Despite the fact that our initial prompt is concise and straightforward, the length of the responses can expand by up to four times after reinforcement learning training. In order to enhance token efficiency and accommodate long-context friendly training paradigm, we find it particularly valuable to explore methods that transform long responses into shorter, more digestible formats. This investigation aims at improving overall efficiency and effectiveness in handling lengthy outputs, thereby optimizing the training process for better scalability and performance. Stablize RL Training. We have found it beneficial in some cases to eliminate KL constraints, especially when starting from strong foundation model. Additionally, introducing higher temperature at the beginning of training appears to provide the model with more diverse starting point. We plan to further investigate how the SFT stage impacts the effectiveness and efficiency of RL training. Mixed-Language Reasoning. curious phenomenon is the models frequent use of Chinese tokens in the <think> section, despite training data being fully in English. One hypothesis is that certain tokens in the Chinese vocabulary vector might produce hidden states that are favorable under our RL scheme. Investigating whether code-switching or even random token switching could systematically aid internal reasoning is an exciting avenue. Relaxing the Formatting Constraints. Although <think> . . . </think> effectively organizes the chain of thought, it remains an open question whether an entirely unconstrained or latent approach might yield better results. The model might eventually invent its own internal representation for reasoning if given the right incentives."
        },
        {
            "title": "References",
            "content": "[1] Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V. Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training, 2025. [2] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. [3] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, 12 Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. [4] Xidong Feng, Ziyu Wan, Muning Wen, Stephen Marcus McAleer, Ying Wen, Weinan Zhang, and Jun Wang. Alphazero-like tree-search can guide large language model decoding and training, 2024. [5] Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, Zhengyang Tang, Benyou Wang, Daoguang Zan, Shanghaoran Quan, Ge Zhang, Lei Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu, and Baobao Chang. Omni-math: universal olympiad level mathematic benchmark for large language models, 2024. [6] Zitian Gao, Boye Niu, Xuzheng He, Haotian Xu, Hongzhang Liu, Aiwei Liu, Xuming Hu, and Lijie Wen. Interpretable contrastive monte carlo tree search reasoning, 2024. [7] Jian Hu. Reinforce++: simple and efficient approach for aligning large language models, 2025. [8] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step, 2023. [9] Zichen Liu, Changyu Chen, Wenjun Li, Tianyu Pang, Chao Du, and Min Lin. There may not be aha moment in r1-zero-like training pilot study. https://oatllm.notion.site/ oat-zero, 2025. Notion Blog. [10] OpenAI. Introducing openai o1. https://openai.com/o1/, 2024. Accessed: 2024-10-02. [11] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 5372853741. Curran Associates, Inc., 2023. [12] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. [13] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. [14] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, and Demis Hassabis. Mastering chess and shogi by self-play with general reinforcement learning algorithm, 2017. [15] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Wenhao Wu, Wenyang He, 13 Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, and Zonghan Yang. Kimi k1.5: Scaling reinforcement learning with llms, 2025. [16] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 2482424837. Curran Associates, Inc., 2022. [17] Chulin Xie, Yangsibo Huang, Chiyuan Zhang, Da Yu, Xinyun Chen, Bill Yuchen Lin, Bo Li, Badih Ghazi, and Ravi Kumar. On memorization of large language models in logical reasoning, 2024. [18] Huajian Xin, Z. Z. Ren, Junxiao Song, Zhihong Shao, Wanjia Zhao, Haocheng Wang, Bo Liu, Liyue Zhang, Xuan Lu, Qiushi Du, Wenjun Gao, Qihao Zhu, Dejian Yang, Zhibin Gou, Z. F. Wu, Fuli Luo, and Chong Ruan. Deepseek-prover-v1.5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search, 2024. [19] Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, Chenyang Shao, Yuwei Yan, Qinglong Yang, Yiwen Song, Sijian Ren, Xinyuan Hu, Yu Li, Jie Feng, Chen Gao, and Yong Li. Towards large reasoning models: survey of reinforced reasoning with large language models, 2025. [20] Haotian Xu. No train still gain. unleash mathematical reasoning of large language models with monte carlo tree search guided by energy function, 2023. [21] Haotian Xu, Xing Wu, Weinong Wang, Zhongzhi Li, Da Zheng, Boyuan Chen, Yi Hu, Shijia Kang, Jiaming Ji, Yingying Zhang, Zhijiang Guo, Yaodong Yang, Muhan Zhang, and Debing Zhang. Redstar: Does scaling long-cot data unlock better slow-reasoning systems?, 2025. [22] An Yang, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoyan Huang, Jiandong Jiang, Jianhong Tu, Jianwei Zhang, Jingren Zhou, Junyang Lin, Kai Dang, Kexin Yang, Le Yu, Mei Li, Minmin Sun, Qin Zhu, Rui Men, Tao He, Weijia Xu, Wenbiao Yin, Wenyuan Yu, Xiafei Qiu, Xingzhang Ren, Xinlong Yang, Yong Li, Zhiying Xu, and Zipeng Zhang. Qwen2.5-1m technical report, 2025. [23] Tian Ye, Zicheng Xu, Yuanzhi Li, and Zeyuan Allen-Zhu. Physics of language models: Part 2.1, grade-school math and the hidden reasoning process, 2024. [24] Chunhui Zhang, Yiren Jian, Zhongyu Ouyang, and Soroush Vosoughi. Working memory identifies reasoning limits in language models. In Yaser Al-Onaizan, Mohit Bansal, and YunNung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1689616922, Miami, Florida, USA, November 2024. Association for Computational Linguistics."
        },
        {
            "title": "6 Related Work",
            "content": "Large Language Model Reasoning key focus for LLMs is improving their reasoning abilities, particularly for complex tasks like code generation and math problem-solving. Chain-of-Thought (CoT) reasoning [16] has been crucial in breaking down problems into manageable steps, enhancing logical reasoning. Originally successful in AlphaGos victory [14], MCTS has been adapted to guide model-based planning by balancing exploration and exploitation through tree-based search and random sampling, and later to large language model reasoning [6, 18, 20]. However, recent research suggests that the vast token generation space of LLMs may make improving reasoning capabilities inefficient [3]. Additionally, long-path reasoning is likely influenced by the models working memory [24]. 14 Large Language Model Post-training for Reasoning Recent work has focused on post-training strategies to enhance the reasoning capabilities of large language models, often through additional fine-tuning or reinforcement learning on specialized datasets with reasoning examples and chainof-thought explanations [21]. Reinforcement learning methods like Direct Preference Optimization (DPO) [11], Proximal Policy Optimization (PPO) [12], Group Relative Policy Optimization (GRPO) [13], and REINFORCE++ [7] are gaining attention. These strategies, alongside test-time scaling methods, are promising frontier for advancing model reasoning."
        },
        {
            "title": "7 Comparion Between Base and Instruct Model",
            "content": "We started with different model types (base, instruct) under the same training strategy and found that the RL curves of the two were surprisingly similar. The growth rates of the test set accuracy, curve, and reward curve were nearly the same. The slight difference is that the response length of the base model grows faster than that of the instruct model. Figure 9: Comparison of training metrics: Response Length, Validation Accuracy, Mean Reward, and KL Loss. Figure 10: Length dynamics between base & instruct 15 Figure 10 shows that the base model quickly increases thinking compute, while the instruct model consistently maintains slightly higher validation accuracy."
        },
        {
            "title": "8 Qualitative Analysis of Emergent Reasoning",
            "content": "We highlight four key emergent behaviors observed in the RL-trained model: (1) Hesitation and Self-Verification The model occasionally uses phrases like \"I am not entirely sure; lets re-check this step\" in the <think> section. This hesitation, absent in pre-training, emerges as the model is rewarded for correct answers and penalized for errors. Before providing the final <answer>, it systematically verifies all prior steps. (2) Multi-Path Exploration & Backtracking Encouraging the model to reason thoroughly, we see it propose multiple solutions (Lets test both possibilitie) and backtrack to check for consistency, resembling human problem-solving in logic puzzles. Question: very special island is inhabited only by knights and knaves. Knights always tell the truth, and knaves always lie. Five residents (William, Chloe, Lily, Jack, and Logan) made the following statements: (1) William said: \"William is knight and Chloe is knight.\" (2) Chloe said: \"Lily is knight.\" (3) Lily said: \"William is knave and Jack is knight.\" (4) Jack said: \"William is knave.\" (5) Logan said: \"Lily is knave if and only if William is knave.\" So who is knight and who is knave? Response: <think> To solve this, lets start analyzing the statements one by one, beginning with Williams statement since its good starting point due to its self-referential nature. Williams statement: \"William is knight and Chloe is knight.\" - If William is telling the truth (i.e., he is knight), then both parts of his statement must be true. However, if William is knight, the statement \"William is knight\" is true, but the statement \"Chloe is knight\" cannot determine the truth about Chloe at this point since we havent analyzed Chloes statement yet. (Hesitate) Therefore, William cannot be knight because it would lead to contradiction. . . . Lets test both possibilities. (Explore) Testing Chloe as knight: If Chloe is knight, then her statement \"Lily is knight\" is true, so Lily is knight. Analyzing Lilys statement: \"William is knave and Jack is knight.\" . . . Now, we have determined: (Summerize) - William is knave. - Chloe is knight. . . . Lets verify all statements: (Verify) - \"William is knight and Chloe is knight\" (False, since William is knave). - \"Lily is knight\" (True). . . . Table 3: An example of large language model trained through reinforcement learning naturally exhibiting reasoning behaviors such as hesitation, exploration, summarization, and verification, even though these behaviors were not present in the training data. (3) Formula Application After RL training, our model instinctively applied the \"If P, then Q\" implication formula when solving logical puzzles, like the Knights and Knaves problem. This formula asserts that the proposition is false only when is true and is false. We were surprised to see that the model not only solved the puzzles through trial and error but also incorporated formal logical reasoning, resembling human problem-solving, despite no such data included in the training set. 16 Question: very special island is inhabited only by knights and knaves. Knights always tell the truth, and knaves always lie. Three residents (Jack, Samuel, Lily) made the following statements: (1) Jack said, \"If Samuel is knight, then Lily is knave.\" (2) Samuel said, \"If Jack is knight, then Lily is knave.\" (3) Lily said, \"Jack is knight.\" So who is knight and who is knave? Response: <think> Lets analyze the statements step by step. 1. Jacks statement is: \"If Samuel is knight then Lily is knave.\" . . . Now that we know Samuel is knave, lets analyze Samuels statement again: - Samuels statement: \"If Jack is knight then Lily is knave.\" - An implication \"If then Q\" is false only when is true and is false. (Formula Application) Here, \"If Jack is knight then Lily is knave\" is false when Jack is knight and Lily is not knave. This is consistent with our previous conclusion that Jack is knight and Lily is knight. . . . Table 4: An example of using logical formulas to solve Knights and Knaves puzzle. (4) Occasional Language Switching Interestingly, some <think> segments contain Chinese tokens (the base model is English-centric). The final <answer> remains in English, presumably to gain format reward. In some demos, the model briefly interjects line of Chinese while analyzing statements, then seamlessly switches back to English for the solution. This phenomenon may indicate that the model uses language mixing as hidden fallback or distinct internal representation. Question:A very special island is inhabited only by knights and knaves. Knights always tell the truth, and knaves always lie. Six residents (Benjamin, Michael, Evelyn, Sophia, James, and Jacob) made the following statements: (1) Benjamin said, \"If Sophia is knight, then Evelyn is also knight.\" (2) Michael said, \"Benjamin is knave, or James is knight.\" (3) Evelyn said, \"Sophia is knight, or James is knave.\" (4) Sophia said, \"If Michael is knave, then James is knight.\" (5) James said, \"Sophia is knight, or Evelyn is knight.\" (6) Jacob said, \"Benjamin is knave, and Sophia is knight.\" So who is knight and who is knave? Response: <think> Lets analyze each statement step-by-step based on whether the speaker is knight or knave. 1. Benjamin: \"If Sophia is knight then Evelyn is knight\" - If Benjamin is knight, then the statement must be true. Thus, if Sophia is knight, Evelyn has to be knight. 2. Michael: \"Benjamin is knave or James is knight\" . . . From Jacobs statement, if Jacob is knight then Sophia must be knight and Benjamin must be knave. 以卡文的方式推理: - 假设Jacob 是骑士那么Jacob 的声明\"Benjamin 是背叛者且Sophia 是骑士\" 就是真的这意味着Benjamin 是 背叛者而Sophia 是骑士 - 既然Sophia 是骑士根据Benjamin 的声明\"如果Sophia 是骑士那么Evelyn 也是骑士\"Evelyn 也是骑士 . . . 综上所述Sophia, Evelyn, 和Jacob 是骑士而Benjamin, Michael, 和James 是背叛者 (Language Switching) . . . Table 5: An example of our model used language switching to solve Knights and Knaves puzzle."
        }
    ],
    "affiliations": [
        "Independent",
        "Microsoft Research Asia",
        "Ubiquant"
    ]
}