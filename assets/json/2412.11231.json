{
    "paper_title": "Smaller Language Models Are Better Instruction Evolvers",
    "authors": [
        "Tingfeng Hui",
        "Lulu Zhao",
        "Guanting Dong",
        "Yaqi Zhang",
        "Hua Zhou",
        "Sen Su"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Instruction tuning has been widely used to unleash the complete potential of large language models. Notably, complex and diverse instructions are of significant importance as they can effectively align models with various downstream tasks. However, current approaches to constructing large-scale instructions predominantly favour powerful models such as GPT-4 or those with over 70 billion parameters, under the empirical presumption that such larger language models (LLMs) inherently possess enhanced capabilities. In this study, we question this prevalent assumption and conduct an in-depth exploration into the potential of smaller language models (SLMs) in the context of instruction evolution. Extensive experiments across three scenarios of instruction evolution reveal that smaller language models (SLMs) can synthesize more effective instructions than LLMs. Further analysis demonstrates that SLMs possess a broader output space during instruction evolution, resulting in more complex and diverse variants. We also observe that the existing metrics fail to focus on the impact of the instructions. Thus, we propose Instruction Complex-Aware IFD (IC-IFD), which introduces instruction complexity in the original IFD score to evaluate the effectiveness of instruction data more accurately. Our source code is available at: \\href{https://github.com/HypherX/Evolution-Analysis}{https://github.com/HypherX/Evolution-Analysis}"
        },
        {
            "title": "Start",
            "content": "4 2 0 2 5 1 ] . [ 1 1 3 2 1 1 . 2 1 4 2 : r a"
        },
        {
            "title": "Smaller Language Models Are Better Instruction Evolvers",
            "content": "Tingfeng Hui1*, Lulu Zhao2*, Guanting Dong3, Yaqi Zhang1, Hua Zhou2, Sen Su1 1Beijing University of Posts and Telecommunications, Beijing, China 2Beijing Academy of Artificial Intelligence, BAAI, Beijing, China 3Renmin University of China, Beijing, China 1(huitingfeng,zhangyaqi2021)@bupt.edu.cn 2llzhao@baai.ac.cn"
        },
        {
            "title": "Abstract",
            "content": "Instruction tuning has been widely used to unleash the complete potential of large language models. Notably, complex and diverse instructions are of significant importance as they can effectively align models with various downstream tasks. However, current approaches to constructing large-scale instructions predominantly favour powerful models such as GPT-4 or those with over 70 billion parameters, under the empirical presumption that such larger language models (LLMs) inherently possess enhanced capabilities. In this study, we question this prevalent assumption and conduct an in-depth exploration into the potential of smaller language models (SLMs) in the context of instruction evolution. Extensive experiments across three scenarios of instruction evolution reveal that smaller language models (SLMs) can synthesize more effective instructions than LLMs. Further analysis demonstrates that SLMs possess broader output space during instruction evolution, resulting in more complex and diverse variants. We also observe that the existing metrics fail to focus on the impact of the instructions. Thus, we propose Instruction ComplexAware IFD (IC-IFD), which introduces instruction complexity in the original IFD score to evaluate the effectiveness of instruction data more accurately. Our source code is available at: https://github.com/HypherX/EvolutionAnalysis"
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have demonstrated exceptional performance in various NLP tasks and are widely integrated into variety of applications, represented by ChatGPT and Copilot (Ouyang et al., 2022; OpenAI, 2023; Dubey et al., 2024). key factor in unleashing the full *denotes equal contribution. Work done during Huis internship at BAAI. The corresponding author. 1 potential of these models is high-quality instruction tuning data, which plays crucial role in posttraining and enhances their effectiveness as AI assistants. In particular, incorporating more complex and diverse instructions allows models to better align with different domains and tasks, boosting their performance in variety of downstream applications (Zhang et al., 2023). However, generating such diverse instructions remains time-consuming and labor intensive (Zheng et al., 2024a; Zhao et al., 2024; Liu et al., 2024), which undoubtedly presents significant challenge for the automated and scalable alignment of LLMs. Recently, series of efforts utilizing LLMs for automatic instruction evolution have garnered sustained attention from the community. Specifically, foundational work like Self-Instruct (Wang et al., 2023) begins with small set of seed instructions and uses powerful supervision model to obtain large number of synthetic instructions. Furthermore, Evol-Instruct (Xu et al., 2024a) refines and evolves existing instructions to produce more complex variants. However, previous studies mainly favour strong LLMs like GPT-4 or those with more than 70 billion parameters to synthesize instructions, empirically assuming that larger language models inherently have superior instruction evolution capabilities. But is this really the case? Recently, Xu et al. (2024c) propose the Larger Models Paradox, which points out that larger models do not necessarily lead to better performance when generating responses, but it overlooks the analysis of instructions. We propose that smaller language models, which require less computational demand and have lower instruction following capabilities, may provide more efficient and effective alternative for evolving more complex and diverse instructions. To gain insight into this, we investigate the differences between smaller language models (SLMs) and larger language models (LLMs) in generating high-quality instructions. Specifically, given set of base models and seed instructions, we are particularly interested in the following research question: RQ1: Do SLMs Perform Better than LLMs in Evolving Instructions? In response to this, we conduct comprehensive experiments across three distinct instruction evolution scenarios: Evol-Instruct, AutoIF (Dong et al., 2024), and Auto Evol-Instruct (Zeng et al., 2024). In these experiments, we use small (8B) and large (70B) models from the Llama-3.1 and Qwen-2 families to evolve and synthesize new instructions, while also fine-tuning various backbone models. The experimental results across all three scenarios consistently indicate that larger, more powerful LLMs do not outperform SLMs in evolving effective instructions. More interestingly, SLMs even demonstrate the capability to evolve more complex and diverse instructions. To further investigate why more powerful LLMs perform worse than SLMs in generating new instructions, we subsequently pose the second research question. RQ2: Why do SLMs Outerperform LLMs in Evolving Instructions? To better understand why more powerful LLMs underperform compared to SLMs in evolving instructions, we compare the top-1 token probabilities of both models during the synthetic of instructions. Our findings demonstrate that LLMs, due to their superior instruction following capabilities, tend to generate higher proportion of high-probability top-1 tokens when evolving new instructions. This overconfidence in token generation results in narrower output space. In contrast, SLMs can generate wider variety of tokens, leading to more complex and diverse instructions. To further investigate what kind of instruction data is effective, we propose the third research question. RQ3: How Do We Determine Whether An Instruction is Effective without Instruction Tuning? Evaluations that do not require instruction tuning can more efficiently assess instruction data. Recent such evaluations often fail to account for the impact of the instructions themselves. For instance, reward models (Cai et al., 2024) are commonly used to assess the quality of responses generated based on given instruction, yet they tend to overlook the quality of the instruction itself. Similarly, while the IFD score (Li et al., 2024) measures the influence of instructions on response generation, it neglects the effect of the instructions inherent complexity. We introduce the Instruction Complex-Aware IFD (IC-IFD) score, which incorporates the difficulty of the instruction as penalty term in the original IFD. We conduct extensive filtering instruction data experiments, and the results demonstrate that the IC-IFD score provides more accurate assessment of instruction data, particularly in scenarios where the instructions exhibit higher complexity levels. In summary, our key contributions are as follows: (1) To the best of our knowledge, we are the first to comprehensively explore the performance discrepancies between SLMs and LLMs in synthesizing instructions. (2) Extensive experimental results demonstrate that SLMs have broader output space, leading to evolving more complex and diverse instructions. (3) We propose the IC-IFD score, which introduces the difficulty of the instruction as penalty term. Comprehensive experiments show that ICIFD can more accurately assess the effectiveness of instruction data without instruction tuning."
        },
        {
            "title": "2 Preliminaries",
            "content": "(Auto) Evol-Instruct. The goal of (Auto) Evol Instruct is to refine original instructions by using artificially designed or LLM-generated evolutionary trajectories, thereby increasing their complexity and fostering the development of more capable model. Formally, given an instruction evolution model Î˜e, response generation model Î˜r, and an original instruction dataset = {(Ii, Ri)}n i=1, where and are instructions and responses and represents the data size, we employ either artificially designed methods or the Î˜e-generated evolutionary trajectory to obtain more complex and diverse evolutionary dataset Devol = {(Iei = Î˜e(IiT ), Rei = Î˜r(RIei))}n i=1. AutoIF. The goal of AutoIF is to automatically construct large-scale and reliable instructions from small set of seed instructions (which can also be seen as constraints) to improve instruction following ability. In this paper, we only utilize the first several steps of AutoIF. Specifically, given small set of seed instructions Is, we first prompt the supervised model Î˜ to construct large number of verifiable instructions Inew based on Is. Subsequently, we prompt Î˜ to generate the corresponding verification functions and test cases for = {Is, Inew}. Finally, cross-validation is performed to obtain the final scalable and reliable instructions If inal = {If (I, c) = rue}. 2 Model Instruction Following (IFEval) Math Reasoning Code Generation Pr.(S) In.(S) Pr.(L) In.(L) GSM8K MATH HumanEval MBPP Mistral-7B-v0.3 DeepSeek-7B Llama-3.2-3B Llama-3-8B Llama-3.1-8B InternLM-2-7B Mistral-7B-v0.3 DeepSeek-7B Llama-3.2-3B Llama-3-8B Llama-3.1-8B InternLM-2-7B 19.59 36.23 40.11 33.83 34.57 40.85 24.40 36.60 41.59 35.49 38.45 43. Supervised Model: Llama-3.1-70B-Instruct 31.77 48.20 50.84 46.28 46.04 53.48 22.74 41.04 43.81 36.41 38.81 44.54 34.65 52.52 54.43 49.28 50.48 56.95 33.89 48.07 53.75 63.00 64.22 68.31 3.16 2.96 6.60 7.62 11.32 19. Supervised Model: Llama-3.1-8B-Instruct 35.01 48.08 53.48 47.00 50.96 54.80 26.25 41.77 45.66 39.56 43.81 47.32 37.53 53.12 57.07 50.72 55.28 58.39 40.18 47.92 55.12 63.38 67.10 68.08 2.84 3.56 7.32 11.44 13.12 20. 24.39 28.66 35.98 43.90 51.22 56.10 29.27 34.76 39.02 48.17 48.78 57.93 6.00 33.00 36.00 36.20 40.60 40.40 19.60 33.80 32.80 37.60 41.60 40.80 Table 1: Comparison of performance with Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct as supervised models under Evol-Instruct scenario. Model Instruction Following (IFEval) Math Reasoning Code Generation Pr.(S) In.(S) Pr.(L) In.(L) GSM8K MATH HumanEval MBPP Mistral-7B-v0.3 DeepSeek-7B Llama-3.2-3B Llama-3-8B Llama-3.1-8B InternLM-2-7B Mistral-7B-v0.3 DeepSeek-7B Llama-3.2-3B Llama-3-8B Llama-3.1-8B InternLM-2-7B 20.15 35.67 39.74 34.75 36.41 41. 25.32 36.41 43.81 38.92 34.75 44.12 30.94 47.12 51.44 45.80 47.60 53.60 37.17 48.56 55.16 48.33 45.80 55.16 Supervised Model: Qwen-2-72B-Instruct 23.84 39.56 43.99 37.71 39.00 43.99 34.41 50.84 55.40 48.92 50.60 55. 46.93 44.81 53.83 63.76 65.43 65.28 3.26 2.76 7.40 10.06 10.84 17.96 Supervised Model: Qwen-2-7B-Instruct 29.76 39.37 47.87 43.81 39.93 48.62 41.01 51.32 58.27 52.19 51.08 58.73 47.31 48.07 56.56 63.91 68.76 66. 2.20 3.82 7.18 8.66 14.02 19.60 32.32 36.59 38.41 43.90 48.17 56.71 32.93 35.37 39.63 45.73 46.34 58.54 1.80 34.00 31.00 35.40 38.40 40.60 12.00 33.20 31.40 38.40 38.60 41.40 Table 2: Comparison of performance with Qwen-2-7B-Instruct and Qwen-2-72B-Instruct as supervised models under Evol-Instruct scenario."
        },
        {
            "title": "3 RQ1: Do SLMs Perform Better than\nLLMs in Evolving Instructions?",
            "content": "In this section, we investigate the potential of SLMs in evolving complex and diverse instructions across three distinct scenarios: Evol-Instruct, AutoIF, and Auto Evol-Instruct. Through series of comprehensive experiments and analyses, we attempt to answer the questions raised in RQ1. For clarity, we will refer to the instruction data evolved by SLMs and LLMs as SLM-INST and LLM-INST. The implementation details for the three scenarios, as well as our experimental hyperparameters, can be found in Appendix A.1."
        },
        {
            "title": "3.1 Evol-Instruct Scenario",
            "content": "In this section, we primarily focus on whether SLMs can evolve more complex and challenging instruction data compared to LLMs. Seed Datasets. Following (Xu et al., 2024a; Zeng et al., 2024), we utilize the following seed datasets for instruction following, mathematical reasoning and code generation: (1) Alpaca (Taori et al., 2023), (2) GSM8K Train (Cobbe et al., 2021), and (3) Code Alpaca (Chaudhary, 2023). More detailed information can be found in Appendix A.2. Evaluation Benchmarks and Metrics. We use IFEval (Zhou et al., 2023b) to assess instruction following capability, GSM8K and MATH (Hendrycks et al., 2021b) to evaluate mathematical reasoning ability, and HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) to assess code generation performance. For detailed information, please refer to Appendix A.3. 3 Figure 1: Comparison of performance on Llama-3-8B during three iterations of instruction evolution, using Llama3.1-8B-Instruct and Llama-3.1-70B-Instruct as supervised models for each round under Evol-Instruct scenario. Figure 2: Distribution of difficulty levels for instructions evolved during three iterations, using Llama-3.1-8BInstruct and Llama-3.1-70B-Instruct as supervised models for each round under Evol-Instruct scenario. Results of Evol-Instruct. We conduct two sets of experiments, using the Llama-3.1 (Dubey et al., 2024) and Qwen-2 (Yang et al., 2024) model series for instruction evolution. This approach helps eliminate potential biases specific to each model series, ensuring the generalizability of the conclusions. Specifically, we use Llama-3.1-8B-Instruct and Qwen-2-7B-Instruct as SLMs and Llama-3.1-70BInstruct and Qwen-2-72B-Instruct serve as LLMs for instruction evolution. To ensure that the generated responses do not influence the experimental conclusions, we consistently use Qwen-2.5-72BInstruct (Team, 2024) as the response generator. Table 1 and Table 2 present comparative analysis of benchmark results for SLM-INST and LLMINST using the Llama and Qwen model families, highlighting the following key insights1. (1) We find that SLM-INST outperforms LLM1More results and analyses regarding the performance of seed instruction data and the impact of temperature are provided in Appendix A.4. INST across instruction following, mathematical reasoning, and code generation, demonstrating superior overall performance in both the Llama and Qwen model families. (2) More complex and difficult instruction data leads to more effective improvements in instruction following capabilities (Dong et al., 2024). Our results show that SLM-INST significantly outperforms LLM-INST on IFEval, highlighting the ability of SLMs to generate more complex instructions compared to LLMs. Impact of Evolution Iteration. Figure 1 illustrates the performance of Llama-3-8B after three rounds of evolution with the Llama-3.1 series (Detailed results can be found in Table 10). Iter 0 represents the performance of the seed instruction data and we release the following key insights. (1) We find that during the first two rounds of evolution, the SLM-INST consistently outperforms LLM-INST. Notably, in terms of the instruction 4 Figure 3: Comparison of performance among Qwen-2.5 series models. Detailed results can be found in Table 11. following, LLM-INST even experiences negative growth, further proving that SLMs are superior to LLMs in generating complex instructions. (2) The performance in the third round of evolution shows an interesting phenomenon. While the SLM-INST continues to perform well in mathematical reasoning, there is significant drop in both instruction following and code generation. Following (Xu et al., 2024b), we use Qwen-2.5-72BInstruct to assess the difficulty level of the evolved instructions in each round, as shown in Figure 2. We find that the difficulty of the SLM-INST in the third round is excessively high. For example, in the third-round SLM-INST for Alpaca, nearly 70% of the instructions are categorized as \"very hard\". Such overly complex and difficult-to-understand instructions result in decline in performance. Further data analysis and the evaluation prompt templates can be found in Appendix A.5 and Figure 17. (3) We find that the complexity of SLM-INST in the second iteration surpasses that of LLM-INST in the third iteration, with SLM-INST also demonstrating superior performance. This suggests that we can leverage SLMs to generate more complex and challenging instructions with fewer computational resources and evolutionary iterations, while simultaneously achieving better performance. Scaling Experiments. To further validate whether our findings hold across models of different sizes, we train models of various sizes within the Qwen-2.5 series (ranging from 0.5B to 72B). The training details can be found in Table 7. Due to computational resource constraints, we perform full fine-tuning for models ranging from 0.5B to 7B, while applying LoRA (Hu et al., 2022) for models from 14B to 72B. To avoid introducing additional biases, we switch the response generator to Llama-3.1-70B-Instruct during the training of the Qwen-2.5 series models. As shown in Figure 3. We find that in the instruction following evaluation, SLM-INST performs slightly worse than LLM-INST on 0.5B and 1.5B models. We believe this is because the evolved instructions in Alpaca are too challenging, and smaller models with lower capabilities may struggle to understand the instructions, leading to performance discrepancies. However, in other evaluations, SLM-INST shows consistently better performance which further confirms our findings. Finding 1 SLMs can evolve more complex and challenging instructions than LLMs."
        },
        {
            "title": "3.2 AutoIF Scenario",
            "content": "In this section, we mainly concentrate on whether SLMs can generate more diverse instruction data compared to LLMs. Evaluation Benchmarks and Metrics. We fully adhere to the evaluation benchmarks used in AutoIF. Specifically, we utilize IFEval and FollowBench (Jiang et al., 2024) to assess instruction following capabilities2. We also evaluate our models on C-Eval (Huang et al., 2023), MMLU (Hendrycks et al., 2021a), GSM8K, and HumanEval to obtain comprehensive assessment of their capabilities. For detailed information, please refer to Appendix A.3. Results of AutoIF. We use the Llama-3.1 series models for synthesizing instructions and we adopt Qwen-2.5-72B-Instruct for generating responses 2We use the Microsoft Azure OpenAI GPT-4 API. 5 Model IFEval FollowBench (HSR) Common Abilities Pr.(S) In.(S) Pr.(L) In.(L) Level 1 Level 2 Level 3 Level 4 Level 5 Avg. C-Eval MMLU HumanEval GSM8K 40.85 Llama-3.2-3B 37.71 Llama-3-8B 41.96 Llama-3.1-8B 41.96 Qwen-2-7B Qwen-2.5-7B 49.17 InternLM-2-7B 46.21 43.62 Llama-3.2-3B 41.04 Llama-3-8B 42.51 Llama-3.1-8B 44.92 Qwen-2-7B 50.09 Qwen-2.5-7B InternLM-2-7B 47.50 51.92 50.00 53.36 53.60 60.31 56.71 54.20 51.32 54.92 55.76 59.59 57. 42.33 39.19 42.70 43.62 50.46 48.06 46.95 42.88 44.73 47.50 52.50 50.83 53.84 52.04 54.20 55.64 61.51 58.63 57.07 53.11 56.71 58.39 61.75 61.15 Supervision Model: Llama-3.1-70B-Instruct 26.74 22.37 26.61 35.42 45.42 42.06 50.55 41.56 45.04 56.43 61.50 54. 33.09 27.05 34.85 41.31 51.99 44.27 57.59 46.60 45.60 62.45 73.78 62.23 61.17 49.64 51.77 72.18 78.88 68.89 Supervision Model: Llama-3.1-8B-Instruct 34.16 32.21 36.02 43.08 47.18 46.28 37.65 32.21 39.49 50.28 53.35 54.10 61.46 54.38 58.15 63.30 70.22 66. 50.20 49.29 53.29 52.31 59.86 61.94 56.95 62.99 63.99 78.75 77.86 74.73 45.83 37.44 40.78 53.56 62.31 54.33 48.08 46.21 50.19 57.54 61.69 60.64 41.37 41.87 44.50 81.08 80.46 60.11 40.56 43.49 43.77 80.11 79.74 63. 52.65 51.14 56.39 55.71 58.39 60.59 49.08 55.63 58.32 56.84 60.17 63.16 29.88 26.83 31.10 57.32 67.68 65.35 25.00 37.20 32.32 65.24 72.56 70.96 27.07 37.76 38.21 79.68 85.90 50.00 29.87 45.26 47.92 79.53 84.69 54. Table 3: Comparison of performance with Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct as supervised models under AutoIF scenario. under the AutoIF scenario. As shown in Table 3, on the IFEval and FollowBench instruction following benchmarks, the instruction data augmented by SLMs achieved better performance. Especially on FollowBench, SLM-INST even achieve nearly 10% improvement over Llama-3-8B and Llama-3.18B. Meanwhile, on common abilities, SLM-INST also demonstrates competitive performance. Figure 4: Distribution of Minimum Neighbor Distance for instructions generated by Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct in the AutoIF scenario. AutoIF begins with small set of manually crafted seed instructions, from which the model draws inspiration to generate large number of new instructions and perform verifications to ensure their quality. Since the generated instructions have undergone multiple rounds of verification, their diversity becomes even more crucial. Following (Xu et al., 2024b), we use all-mpnet-basev2 (Song et al., 2020) to measure similarity via minimum neighbor distance (MND) in the embedding space. Notably, high number of samples with low MND suggests poor diversity within the dataset. Figure 4 demonstrates that SLM-INST has 6 more samples with larger MND, indicating higher diversity than LLM-INST. Finding 2 SLMs can generate more diverse instructions than LLMs."
        },
        {
            "title": "3.3 Auto Evol-Instruct Scenario",
            "content": "In this section, we mainly focus on whether SLMs can automatically evolve more effective instructions compared to LLMs. Results of Auto Evol-Instruct. As shown in Table 4, we find that the instruction data automatically evolved by SLMs consistently performs better across the Llama series models than LLMs. In addition, we prompt the Qwen-2.5-72B-Instruct model to summarize and deduplicate keywords from the trajectories generated by SLMs and LLMs (the prompt template can be found in Figure 18). We find that the number of trajectories produced by SLMs is 6.9% higher than that of LLMs, further highlighting that SLMs can design more varied evolutionary trajectories, leading to more complex and diverse instructions. Finding 3 SLMs can automatically evolve more effective instructions than LLMs."
        },
        {
            "title": "4 RQ2: Why Do SLMs Outperform\nLLMs in Evolving Instructions?",
            "content": "In this section, we primarily analyze why SLMs perform better from the perspectives of model inference and real-world cases. Model Instruction Following (IFEval) Math Reasoning Code Generation Pr.(S) In.(S) Pr.(L) In.(L) GSM8K MATH HumanEval MBPP Llama-3.2-3B Llama-3-8B Llama-3.1-8B Llama-3.2-3B Llama-3-8B Llama-3.1-8B 36.60 35.86 36.97 45.47 37.34 38.08 48.68 47.60 47.60 57.43 49.64 49.76 Supervised Model: Llama-3.1-70B-Instruct 7.56 9.18 11. 51.08 50.24 51.08 39.00 38.63 40.30 53.60 63.91 66.11 Supervised Model: Llama-3.1-8B-Instruct 50.28 39.74 40.48 61.27 51.56 52. 56.48 67.40 69.52 8.42 12.26 15.62 35.37 38.41 40.85 38.41 43.90 51.22 33.00 32.40 40.40 34.40 34.80 38. Table 4: Comparison of performance with Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct as supervised models under Auto Evol-Instruct scenario. leading to more diverse and complex instructions. We also analyze some cases, and the detailed results can be found in Appendix A.4. Finding 4 SLMs have broader output space and are less likely to be overconfident than LLMs."
        },
        {
            "title": "5 RQ3: How Do We Determine Whether\nAn Instruction is Effective without\nInstruction Tuning?",
            "content": "In this section, we primarily discuss how to determine whether instruction data is effective without instruction tuning. Instruction Complex-Aware IFD. As mentioned in (Xu et al., 2024c), existing evaluations typically focus on assessing responses, such as using reward models, while neglecting the impact of instructions on the data. Recently, Li et al. (2024) proposed the instruction following Difficulty (IFD) score to evaluate the quality of instructions. Specifically, the formula for IFD is as follows. IFDÎ˜(Q, A) = LÎ˜(AQ) LÎ˜(A) (1) Where and represent instructions and responses, and LÎ˜() represents the average cross entropy loss determined by model Î˜. IFD can be understood as the importance of instructions in generating responses. lower IFD means that sample does not require training, as the model is already able to generate the corresponding response effectively when given the instruction. However, as shown in Figure 1 and Table 15, when the difficulty of the instructions is too high, it may result in higher IFD, but the overall performance may Figure 5: Comparison of output token probability distributions in the Evol-Instruct scenario. Comparison of Token Distributions. The results of our previous experiments indicate that SLMs are capable of evolving and generating more complex and diverse instructions. We hypothesize that this is due to the superior instruction following capabilities of LLMs, which result in narrower output space (overconfidence) when following instructions, thereby leading to less diversity and complexity in the generated new instructions. To validate this hypothesis, we employ the Llama3.1-8B-Instruct and Llama-3.1-70B-Instruct models within the Evol-Instruct scenario to obtain the probability distributions of output tokens. By extracting the top-1 token probability at each output position, we compare the output probability distributions between SLMs and LLMs. As shown in Figure 5, we observe that the top-1 token output probability for SLMs is lower, suggesting that the output distribution of SLMs is more diverse. This supports our hypothesis that, due to their relatively weaker instruction following capabilities compared to LLMs, SLMs generate broader output space, 7 fall short of expectations. Inspired by this, we introduce the difficulty level of instructions into the original IFD and propose the Instruction ComplexAware IFD (IC-IFD). Specifically, we introduce the perplexity of the instructions into the original IFD score, resulting in the following formula. IC-IFDÎ˜(Q, A) = LÎ˜(AQ) LÎ˜(Q) LÎ˜(A) (2)"
        },
        {
            "title": "IFEval",
            "content": "Pr.(S) In.(S) Pr.(L) In.(L) Original Instruction Len. Instruction PPL IFD IC-IFD 33.09 29.94 27.91 30.87 34.01 44.72 39.69 39.69 43.53 46. 36.41 33.83 32.35 36.04 38.82 48.32 43.53 44.36 47.60 50.72 Table 5: Comparison of different metrics under 25% of Alpaca-iter3 evolved by SLMs on Llama-3-8B. Performance of IC-IFD. To validate the effectiveness of IC-IFD, we aim to mitigate the performance degradation caused by the third round of instruction data evolved by SLMs. Specifically, we retain the top 25% of instruction data using several metrics, including instruction length (filtering out overly long instructions), instruction perplexity (PPL, filtering out instructions with excessively high PPL), IFD, and IC-IFD. As shown in Table 5, under the condition of retaining only 25% of the instruction data, IC-IFD outperforms the full dataset, while other metrics exhibit varying degrees of performance degradation, thereby demonstrating the effectiveness of IC-IFD. Further experiments on IC-IFD can be found in Appendix A.4."
        },
        {
            "title": "6 Related Work",
            "content": "Instruction tuning has become an essential strategy for enhancing the capabilities of large language models (LLMs) (Ouyang et al., 2022; OpenAI, 2023). By curating high-quality datasets, we can more effectively align these models with specific objectives (Zhou et al., 2023a). Recently, some researchers have highlighted the significance of instruction data that is either manually annotated or developed with human involvement, such as ShareGPT (Chiang et al., 2023) and OpenAssistant (KÃ¶pf et al., 2023). Meanwhile, other studies concentrate on leveraging LLMs to generate highquality datasets with minimal human effort (Xu et al., 2024a; Luo et al., 2024, 2023). Wang et al. (2023) introduces Self-Instruct, which begins with small collection of manually crafted seed instructions and utilizes LLMs to expand these instructions, ultimately producing large-scale instruction set that improves model abilities. Xu et al. (2024a) presents Evol-Instruct, which employs LLMs for the iterative enhancement of the original instructions through both in-depth and breadth evolution, resulting in more complex and diverse instruction dataset. Auto Evol-Instruct (Zeng et al., 2024) further removes human involvement, enabling LLMs to autonomously design the evolution trajectory based on the original instructions. AutoIF (Dong et al., 2024) introduces code feedback mechanism that allows LLMs to generate evaluation code for verifying whether the quality of the instructions meets the required standards. Xu et al. (2024b) only provides single prompt to induce the model to generate large amount of instruction data. Current research primarily focuses on utilizing larger language models, such as GPT-4 (OpenAI, 2023), for constructing complex instructions. More recently, Xu et al. (2024c) explores the performance differences of various-sized models as response generators. In contrast, we concentrate on the potential of smaller language models in evolving complex instructions. This innovation not only reduces the costs associated with instructions construction but, more importantly, offers comprehensive evaluation and exploration, highlighting the significant capabilities inherent in smaller models and providing valuable insights for future work."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we compare the performance of SLMs and LLMs in evolving instructions. Extensive experiments demonstrate that SLMs can synthesize more effective instructions at lower computational cost than LLMs. Through an analysis of the model output distributions, we observe that SLMs exhibit broader output space, leading to more complex and diverse instructions. Furthermore, we introduce instruction complexity as penalty term in the original IFD and propose ICIFD, which allows for more accurate assessment of instruction data effectiveness without the need for instruction tuning. Our work also lays the groundwork for future research on SLMs in instruction data synthesis, offering foundation understanding for further exploration."
        },
        {
            "title": "Limitations",
            "content": "Although our work provides valuable insights that SLMs perform better in evolving instructions through comprehensive experiments, several directions are worth exploring in future research. (1) We have only conducted experiments in instruction following, mathematical reasoning, and code generation. We have not focused on other broader domains, and there may have interesting discoveries in these areas that require future work. (2) Our work focuses on comparing SLMs and LLMs in evolving instruction sets, rather than exploring the full potential of SLMs in synthesizing entire instruction datasets. Future research that investigates the capabilities of SLMs across the entire instruction data synthesis pipeline would be promising and exciting direction to explore. (3) The IC-IFD we propose is based on our observation that performance degrades with the emergence of high-difficulty instructions, which leads us to introduce instruction complexity as penalty term in the original IFD. In the future, further exploration into how to more accurately assess the effectiveness of instruction data without instruction tuning would be valuable."
        },
        {
            "title": "Acknowledgments",
            "content": "This research is supported by the National Natural Science Foundation of China (62072052) and the Innovation Research Group Project of NSFC (61921003). At the same time, we sincerely appreciate the academic and computational support from the Beijing Academy of Artificial Intelligence (BAAI), which has been crucial to the successful completion of this study."
        },
        {
            "title": "References",
            "content": "Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. 2021. Program synthesis with large language models. CoRR, abs/2108.07732. Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y. K. Li, Wenfeng Liang, Fangyun Lin, Alex X. Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y. Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R. X. Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu, Xingkai Yu, B. Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, and Yuheng Zou. 2024. Deepseek LLM: scaling open-source language models with longtermism. CoRR, abs/2401.02954. Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia Guo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang, Tao Jiang, Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiaxing Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yining Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kaiwen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun Lv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang Ning, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang, Yunfan Shao, Demin Song, Zifan Song, Zhihao Sui, Peng Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang, Jiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi Wang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong Xiong, Xiaomeng Zhao, and et al. 2024. Internlm2 technical report. CoRR, abs/2403.17297. Sahil Chaudhary. 2023. Code alpaca: An instructionfollowing llama model for code generation. https: //github.com/sahil280114/codealpaca. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique PondÃ© de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code. CoRR, abs/2107.03374. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion 9 Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. CoRR, abs/2110.14168. OpenCompass Contributors. 2023. Opencompass: universal evaluation platform for foundation https://github.com/open-compass/ models. opencompass. Guanting Dong, Keming Lu, Chengpeng Li, Tingyu Xia, Bowen Yu, Chang Zhou, and Jingren Zhou. 2024. Self-play with execution feedback: Improving instruction-following capabilities of large language models. CoRR, abs/2406.13542. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, AurÃ©lien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste RoziÃ¨re, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, GrÃ©goire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. 2024. The llama 3 herd of models. CoRR, abs/2407.21783. Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Alpacafarm: simulation framework for methods that learn from human feedback. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021a. Measuring massive multitask language understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021b. Measuring mathematical In Proproblem solving with the MATH dataset. ceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. 2023. C-eval: multi-level multi-discipline chinese evaluation suite for foundation models. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, LÃ©lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, and William El Sayed. 2023. Mistral 7b. CoRR, abs/2310.06825. Yuxin Jiang, Yufei Wang, Xingshan Zeng, Wanjun Zhong, Liangyou Li, Fei Mi, Lifeng Shang, Xin Jiang, Qun Liu, and Wei Wang. 2024. Followbench: multi-level fine-grained constraints folIn lowing benchmark for large language models. Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 46674688. Association for Computational Linguistics. Andreas KÃ¶pf, Yannic Kilcher, Dimitri von RÃ¼tte, Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens, Abdullah Barhoum, Duc Nguyen, Oliver Stanley, RichÃ¡rd Nagyfi, Shahul ES, Sameer Suri, David Glushkov, Arnav Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu Nguyen, and Alexander Mattick. 2023. Openassistant conversations - democratizing large language model alignment. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. 10 Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, SOSP 2023, Koblenz, Germany, October 23-26, 2023, pages 611 626. ACM. Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, and Jing Xiao. 2024. From quantity to quality: Boosting LLM performance with self-guided data selection for instruction tuning. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pages 76027635. Association for Computational Linguistics. Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He. 2024. What makes good data for alignment? comprehensive study of automatic data selection in instruction tuning. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. 2023. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. CoRR, abs/2308.09583. Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2024. Wizardcoder: Empowering code large language models with evolIn The Twelfth International Conference instruct. on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. OpenAI. 2023. GPT-4 technical report. CoRR, abs/2303.08774. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and TieYan Liu. 2020. Mpnet: Masked and permuted pretraining for language understanding. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https:// github.com/tatsu-lab/stanford_alpaca. Qwen Team. 2024. Qwen2.5: party of foundation models. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 1348413508. Association for Computational Linguistics. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Qingwei Lin, and Daxin Jiang. 2024a. Wizardlm: Empowering large pre-trained language models to follow complex instructions. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, and Bill Yuchen Lin. 2024b. Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing. CoRR, abs/2406.08464. Zhangchen Xu, Fengqing Jiang, Luyao Niu, Bill Yuchen Lin, and Radha Poovendran. 2024c. Stronger models are not stronger teachers for instruction tuning. Preprint, arXiv:2411.07133. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. 2024. Qwen2 technical report. CoRR, abs/2407.10671. Weihao Zeng, Can Xu, Yingxiu Zhao, Jian-Guang Lou, and Weizhu Chen. 2024. Automatic instruction evolving for large language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pages 69987018. Association for Computational Linguistics. 11 Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, and Guoyin Wang. 2023. Instruction tuning for large language models: survey. CoRR, abs/2308.10792. the same instruction, thereby reducing the potential for biased experimental conclusions. The results in Table1 and Table 2 are obtained after one round of evolution of the seed instructions. Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. 2024. Wildchat: 1m chatgpt interaction logs in the wild. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric P. Xing, Joseph E. Gonzalez, Ion Stoica, and Hao Zhang. 2024a. Lmsys-chat-1m: large-scale real-world LLM conversation dataset. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, and Yongqiang Ma. 2024b. Llamafactory: Unified efficient fine-tuning of 100+ language models. CoRR, abs/2403.13372. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023a. LIMA: less is more for alignment. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. 2023b. Instruction-following evaluation for large language models. CoRR, abs/2311.07911."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Experimental Details Evolution Details of Evol-Instruct. As shown in Figure 10, 11 and 12, the instruction evolution prompts we utilized are derived from (Xu et al., 2024a; Luo et al., 2024), with minor modifications. For the Alpaca dataset, we employ four indepth evolution methods: deepening, concretizing, adding constraints, and adding reasoning steps, in addition to one breadth-focused evolution method. However, for the GSM8K Train and Code Alpaca datasets, we exclude the breadth-focused method and use only the four in-depth methods. To ensure fair comparison, we apply these evolution methods to each original instruction in fixed sequence, rather than randomly selecting them as in the original Evol-Instruct. This strategy is designed to eliminate variations in the evolution of Evolution Details of AutoIF. Following the approach in AutoIF, we typically employ Llama-3.18B-Instruct and Llama-3.1-70B-Instruct to carry out the Instruction Augmentation and Verification steps, generating 780 and 420 instructions, respectively. Due to the multiple verification steps required by AutoIF for filtering, the number of generated instructions varies. To ensure fairness, we randomly select 420 instructions from the 780 generated by the SLMs for comparison. These instructions are then concatenated with queries from ShareGPT to create dataset of 6,720 instruction data for subsequent training. Evolution Details of Auto Evol-Instruct. We compare the performance of Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct in automatically designing evolutionary trajectories for evolving instructions. Using the prompt template from Auto Evol-Instruct (Zeng et al., 2024) (refer to Figure 15), we prompt the models to design evolutionary trajectories and evolve instructions autonomously. To avoid introducing additional bias, we exclude the optimization stage from Auto EvolInstruct. The experimental setup and evaluation benchmarks are consistent with those in Section 3.1. Since the models occasionally fail to adhere to the specified output format, leading to instruction extraction errors, we perform random sampling on the larger sets of evolved instructions from both models to ensure consistent quantities of instruction data. We also use Qwen-2.5-72B-Instruct to generate the responses. Finally, for the Alpaca, GSM8K, and Code Alpaca datasets, we conduct automatic evolution and sampling, resulting in 40,483, 6,200, and 15,533 instruction data points, respectively. Implementation Details For fair comparison, all of our experiments maintain consistent data volumes. During the construction of the instruction data, we leverage the vLLM framework (Kwon et al., 2023) for acceleration using temperature of 0.7 and top_p value of 0.95. For training the models, we utilize the LLaMA-Factory framework (Zheng et al., 2024b) with global batch size of 64, cutoff length of 2048, and learning rate of 2e-5, following cosine learning rate schedule over 3 epochs. No checkpoint selection is performed; instead, all models are evaluated using the final saved checkpoint. All experiments are carried out on 8 NVIDIA Tesla A100 GPUs. Base Models. In the Evol-Instruct scenario, we fine-tune Llama series models (Dubey et al., 2024) including Llama-3.2-3B, Llama-3.1-8B, Llama-38B, DeepSeek-7B (Bi et al., 2024), Mistral-7Bv0.3 (Jiang et al., 2023), and InternLM-2-7B (Cai et al., 2024) models. In the AutoIF scenario, we also use Llama series models as in Evol-Instruct, as well as Qwen series (Yang et al., 2024) models including Qwen-2.5-7B and Qwen-2-7B, along with InternLM-2-7B. For Auto Evol-Instruct, we evaluate the performance of the Llama series models."
        },
        {
            "title": "Hyperparameter",
            "content": "Learning Rate Number of Epochs Number of Devices Per-device Batch Size Gradient Accumulation Steps Learning Rate Scheduler Warmup Ratio Max Sequence Length"
        },
        {
            "title": "Value",
            "content": "2 105 3 8 1 8 cosine 0.03 2048 Table 6: Hyperparameters utilized in Evol-Instruct, AutoIF and Auto Evol-Instruct scenarios. More Hyperparameter Details. We provide the detailed hyperparameters for supervised finetuning in Table 6. Except for IFEval and FollowBench, which are evaluated using their respective repositories, all other evaluations are conducted using the OpenCompass (Contributors, 2023) framework, and vLLM is adopted for inference acceleration throughout the evaluation process to enhance computational efficiency and expedite the assessment procedures. A.2 Detailed Information of Seed Datasets In Evol-Instruction and Auto Evol-Instruct scenarios, we utilize the following seed datasets for instruction following, mathematical reasoning, and code generation: (1) Alpaca, dataset that contains about 52K instruction following data points, (2) GSM8K Train, dataset that includes nearly 7K high-quality, linguistically diverse grade school math word problems; and (3) Code Alpaca, code generation dataset comprising approximately 20K"
        },
        {
            "title": "General Hyperparameters",
            "content": "Number of Epochs Number of Devices Per-device Batch Size Gradient Accumulation Steps Learning Rate Scheduler Warmup Ratio Max Sequence Length 2 8 1 8 cosine 0."
        },
        {
            "title": "LoRA Rank\nLoRA Alpha\nLoRA Target\nLoRA Dropout",
            "content": "8 8 all module 0.0 Qwen-2.5-0.5B and 1.5B"
        },
        {
            "title": "Learning Rate",
            "content": "Qwen-2.5-3B and 7B"
        },
        {
            "title": "Learning Rate",
            "content": "1 105 7 106 Qwen-2.5-14B, 32B and 72B"
        },
        {
            "title": "Learning Rate",
            "content": "5 105 Table 7: Hyperparameters utilized for fine-tuning Qwen2.5 series models. samples. Table 8 presents the statistical information of the seed datasets. In the AutoIF scenario, we follow the setup described in the AutoIF paper, using the seed instructions provided by the authors and the queries from ShareGPT to construct the instructions. A.3 Detailed Information of Evaluations To evaluate the instruction following capabilities of our models, we employ several benchmarks, including IFEval and FollowBench. IFEval consists of 25 types of verifiable instructions across approximately 500 prompts, while FollowBench is finegrained, constraint-based instruction following benchmark with five difficulty levels. It includes diverse open-ended instructions that require evaluation by strong LLMs. We report both strict and loose accuracy metrics at the prompt and instruction levels, and for FollowBench, we specifically report the Hard Satisfaction Rate (HSR). In addition to instruction following benchmarks, we assess the models on other tasks. For mathematical reasoning, we use GSM8K and MATH."
        },
        {
            "title": "Datasize",
            "content": "Instruction Following Mathematical Reasoning GSM8K Train Code Alpaca Code Generation"
        },
        {
            "title": "Alpaca",
            "content": "51,983 7,473 20,022 Table 8: Statistics of seed instruction data used in the Evol-Instruct and Auto-Evol-Instruct scenarios. GSM8K consists of grade school math problems, while MATH presents more challenging mathematical problems. We report accuracy scores for both datasets. For code generation, we evaluate the models using HumanEval and MBPP, reporting the pass@1 metrics. We also evaluate our models on C-Eval, and MMLU to provide comprehensive assessment of the models capabilities across various domains. A.4 More Experimental Results Seed Instruction Data Results. Table 9 presents the experimental results for the seed instruction datasets used in Evol-Instruct and Auto EvolInstruct scenarios. We observe that the performance of models trained on these seed data is suboptimal. We argue that the quality of these seed data is no longer adequate to further improve the performance of the current advanced base models. Detailed Results of Multi-Iteration Evolution. Table 10 presents the detailed results of different evolved iterations which are referred to Figure 1. Detailed Results of Scaling Experiments. Table 11 presents the detailed results of the model scaling experiment shown in Figure 3. The Impact of Temperatures. To explore the impact of temperature on the evolutionary instruction data, we compare Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct under different temperatures. Specifically, we evolve the Code Alpaca data under greedy decoding (with temperature of 0) and at five different temperatures ranging from 0.1 to 0.9, and uniformly use Qwen-2.5-72B-Instruct to generate the corresponding responses. As shown in Table 12, the results of training on Llama-3.2-3B indicate that the SLMs perform consistently better than LLMs under all temperatures, which further validates the universality of our conclusion. More Results of IC-IFD. To further validate the broad applicability of IC-IFD, beyond highFigure 6: Performance comparison of three data selection ratios on Alpaca dataset between IC-IFD and IFD. Figure 7: Performance comparison of three data selection ratios on Alpaca dataset between IC-IFD and full dataset. difficulty instruction data, we use the IC-IFD and IFD metrics to filter 5%, 10%, and 15% of the original Alpaca dataset for training the Llama-38B and Llama-3.2-3B models. We fine-tune the models on the IC-IFD and IFD-filtered data and evaluate their performance using instructions from AlpacaFarm (Dubois et al., 2023). The generated responses are then assessed using GPT-4 to determine the win-tie-lose ratio (the evaluation prompt template can be found in Figure 20). As shown in Figure 6, we observe that IC-IFD consistently outperforms IFD across all three data ratio settings for both models. Furthermore, we compare the performance of models trained on IC-IFD-filtered data with those trained on the full Alpaca dataset. As shown in Figure 7, models trained on IC-IFDfiltered data also perform better than those trained on the full dataset, further demonstrating the effectiveness of the proposed IC-IFD. Case Study. We compare the evolution of SLMs and LLMs across two specific in-depth cases. As illustrated in Figure 8, we observe that in the \"adding constraints\" evolution trajectory, the evolved instructions of SLMs incorporate two additional con14 Model Instruction Following (IFEval) Math Reasoning Code Generation Pr.(S) In.(S) Pr.(L) In.(L) GSM8K MATH HumanEval MBPP Seed instruction data Mistral-7B-v0.3 DeepSeek-7B Llama-3.2-3B Llama-3-8B Llama-3.1-8B InternLM-2-7B 17.01 22.00 22.55 23.11 27.54 32.72 26.86 34.05 34.17 32.97 38.13 45.08 19.04 23.48 25.88 24.77 28.65 35.30 29.14 35.73 37.65 35.13 39.21 48. 27.07 44.05 46.40 53.68 56.41 61.87 0.12 0.56 0.56 0.22 7.56 10.28 10.20 25.61 28.05 25.00 29.88 42.07 8.80 33.80 32.20 28.60 31.80 40.00 Table 9: Results of seed instruction data. Model Instruction Following (IFEval) Math Reasoning Code Generation Pr.(S) In.(S) Pr.(L) In.(L) GSM8K MATH HumanEval MBPP Iteration 1 Iteration 2 Iteration 3 Iteration 1 Iteration 2 Iteration 3 33.83 32.53 35. 35.49 36.78 33.09 46.28 43.76 47.36 47.00 48.20 44.72 Supervised Model: Llama-3.1-70B-Instruct 7.62 10.04 11.82 49.28 46.16 49.28 36.41 34.20 36. 63.00 64.59 64.75 Supervised Model: Llama-3.1-8B-Instruct 11.44 11.48 14.12 63.38 64.82 65.88 50.72 50.84 48.32 39.56 40.30 36.41 43.90 42.07 43. 48.17 48.78 44.51 36.20 36.60 37.20 37.60 39.40 40.80 Table 10: Detailed performance of different evolved iterations on Llama-3-8B refer to Figure 1. straints: lack of time for exercise and inability to limit diet, while the evolved instructions of LLMs only add the condition that the requirements must be feasible. Similarly, in the \"deepening\" evolution trajectory, as shown in Figure 9, the evolved instructions of SLMs are significantly more challenging, containing numerous in-depth conditions, which is absent in the evolved instructions of LLMs. Overall, from actual cases, SLMs can evolve more complex and diverse instructions under the same constraints or trajectories, achieving more effective instructions at lower computational cost. A.5 Further Analysis Difficulty Scores of Evol-Instruct. We utilize the prompt template shown in Figure 19 to prompt Qwen-2.5-72B-Instruct for evaluating the complexity scores of the three-round data in the EvolInstruct scenario. As shown in Table 13, we find that in each round, SLM-INST consistently outperforms LLM-INST in terms of complexity scores. Interestingly, SLM-INST Iter 2 is even more difficult than LLM-INST Iter 3, as demonstrated by the experiment in Figure 1, where the overall performance of SLM-INST Iter 2 is superior to that of LLM-INST Iter 3. Quality Score Evaluated by Reward Model. We also utilize InternLM-2-7B-Reward as the reward model to evaluate the average scores of the evolved instructions of both SLMs and LLMs. Specifically, given the evolved prompt templates (as shown in Figure 10 and 12), we then use the reward model to evaluate the rewards of the evolved instructions generated by SLMs and LLMs respectively and obtain the mean reward of the instruction set. As shown in Table 14, we find that the overall scores of the instructions evaluated by the reward model are approximately in line with its performance during the training stage. However, on some datasets, it could not accurately reflect the quality of the instructions. Moreover, using the reward model cannot directly assess the quality of the instructions. Instead, it requires the meta-instructions used when constructing the instructions. Therefore, the reward model cannot be well applied to the evaluation of instructions. Comparison of IFD and IC-IFD. We analyze the third-round evolved Alpaca dataset for both SLMs and LLMs. Specifically, we compute the IFD and IC-IFD scores for each sample in both datasets and compare their average scores. As shown in Table 15, we evaluate the average performance of IFEval on the two datasets using Llama3-8B. We find that when the instruction difficulty level is too high, the IFD score tends to increase. However, the performance of the fine-tuned models does not align with expectations. In contrast, the 15 Model Instruction Following (IFEval) Math Reasoning Code Generation Pr.(S) In.(S) Pr.(L) In.(L) GSM8K MATH HumanEval MBPP Qwen-2.5-0.5B Qwen-2.5-1.5B Qwen-2.5-3B Qwen-2.5-7B Qwen-2.5-14B (LoRA) Qwen-2.5-32B (LoRA) Qwen-2.5-72B (LoRA) Qwen-2.5-0.5B Qwen-2.5-1.5B Qwen-2.5-3B Qwen-2.5-7B Qwen-2.5-14B (LoRA) Qwen-2.5-32B (LoRA) Qwen-2.5-72B (LoRA) 18.48 28.84 37.89 46.21 40.11 42.88 50. 17.38 28.47 38.82 47.32 42.51 45.84 52.79 Supervised Model: Llama-3.1-70B-Instruct 35.85 32.73 46.04 42.67 53.60 48.56 60.79 56.83 61.99 54.43 64.15 57.31 70.98 68.43 22.00 31.98 42.70 50.64 48.24 51.20 57.12 40.26 62.32 76.12 76.12 87.79 87.79 91.05 Supervised Model: Llama-3.1-8B-Instruct 32.01 44.96 53.96 62.35 62.47 66.31 73.27 19.78 31.98 42.51 51.39 51.02 54.71 61. 40.71 65.35 76.57 82.03 88.17 89.61 91.36 29.38 41.73 49.76 58.39 55.16 58.75 72.56 16.32 24.06 26.44 38.14 49.94 55.02 58.83 16.26 27.84 30.92 43.78 52.22 55.28 60.75 30.49 50.00 63.41 70.73 75.00 80.49 82.93 34.76 52.44 64.02 71.95 75.61 81.71 84. 27.60 43.20 55.40 61.60 67.20 71.20 76.00 28.00 49.94 55.80 61.80 67.20 73.20 76.80 Table 11: Detailed performance among Qwen-2.5 series models refer to Figure 3. Temperature HumanEval MBPP HumanEval MBPP Supervised Model: Llama-3.1-70B-Instruct Supervised Model: Llama-3.1-8B-Instruct greedy 0.1 0.3 0.5 0.7 0.9 37.20 36.59 38.41 35.98 35.98 34. 33.40 36.40 35.20 33.40 36.00 33.00 39.63 37.80 39.63 37.80 39.02 40.24 36.40 37.60 37.80 35.80 32.80 35.80 Table 12: Performance among different temperatures on Llama-3.2-3B under code generation scenario. Alpaca GSM8K Train Code Alpaca Seed Instruction LLM-INST Iter1 SLM-INST Iter1 LLM-INST Iter2 SLM-INST Iter2 LLM-INST Iter3 SLM-INST Iter3 27.63 52.89 66.35 68.16 77. 75.73 82.44 34.05 39.88 48.85 47.14 63.48 54.00 72.12 26. 46.75 58.86 65.02 73.37 72.85 79.19 Table 13: Scores of difficulty levels for instructions evolved during three iterations, using Llama-3.1-8BInstruct and Llama-3.1-70B-Instruct as supervised models for each round under Evol-Instruct scenario. IC-IFD score effectively captures the influence of instruction complexity, offering more accurate data quality assessment. A.6 Prompt Templates Prompt Templates of Evol-Instruct. Figure 10 shows the in-depth evolution prompt template for instruction evolution used in the Evol-Instruct scenario, derived from (Xu et al., 2024a) and slightly modified. Figures 11 and 12 demonstrate the"
        },
        {
            "title": "Average Reward",
            "content": "Alpaca GSM8K Code Alpaca Supervised Model: Llama-3.1-70B-Instruct Iteration 1 Iteration 2 Iteration 3 1.54 1.68 1.56 0.74 0.73 0.69 1.10 1.19 1. Supervised Model: Llama-3.1-8B-Instruct Iteration 1 Iteration 2 Iteration 3 1.59 1.54 1.42 1.01 0.79 0.97 1.23 0.96 1.03 Table 14: Comparison of average rewards among different iteration evolution instruction data. four in-depth methods and one in-breadth evolved prompt template we adopt. Prompt Templates of AutoIF. We utilize the prompt templates consistent with those in (Dong et al., 2024). Figures 13 and 14 represent the prompts used in the two stages: Self-Instruct Seed Instructions and Verification Funcs and Cases Generation. 16 Figure 8: Comparison of cases between LLMs and SLMs under adding constraints strategy. Figure 9: Comparison of cases between LLMs and SLMs under deepening strategy. Datasets IFD (%) IC-IFD (%) Performance ated by the model. SLMs (Alpaca iter 3) LLMs (Alpaca iter 3) 83.04 82.03 35.89 37. 40.64 42.18 Table 15: Comparison of IFD and IC-IFD on thirdround evolved Alpaca datasets from SLMs and LLMs. Prompt Templates of Evaluation. Figure 20 shows the prompt template used to assess the wintie-lose rates on AlpacaFarm. Prompt Templates of Auto Evol-Instruct. As shown in Figure 15, we utilize the prompt templates consistent with those in (Zeng et al., 2024) under Auto Evol-Instruct scenario. Prompt Templates of Response Generation. We use the prompt template shown in Figure 16 to generate the corresponding responses for all instructions. We adopt the data organization format from Llama-Factory, and therefore, when generating responses, we classify them into two types based on the presence of an input. Prompt Templates of Data Analysis. Figure 17 and 19 show the prompt templates used to assess the difficulty levels and scores of instructions. Figure 18 displays the prompt template used to analyze the evolutionary trajectories automatically gener17 Figure 10: In-depth evolution prompt template utilized in Evol-Instruct scenario. Figure 11: Four in-depth methods utilized in Evol-Instruct scenario. Figure 12: In-breadth evolution prompt template utilized in Evol-Instruct scenario. Figure 13: Prompt template of Self-Instruct Seed Instructions in AutoIF scenario. Figure 14: Prompt template of Verification Funcs and Cases Generation in AutoIF scenario. Figure 15: Prompt template of Auto Evol-Instruct scenario. Figure 16: Prompt template of response generation. 19 Figure 17: Prompt template of evaluating the difficulty levels. Figure 18: Prompt template of extracting the keywords from evolution trajectories. Figure 19: Prompt template of evaluating the difficulty scores. 20 Figure 20: Prompt template of evaluating the win-tie-lose rates."
        }
    ],
    "affiliations": [
        "Beijing Academy of Artificial Intelligence, BAAI, Beijing, China",
        "Beijing University of Posts and Telecommunications, Beijing, China",
        "Renmin University of China, Beijing, China"
    ]
}