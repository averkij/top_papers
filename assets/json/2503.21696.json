{
    "paper_title": "Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks",
    "authors": [
        "Wenqi Zhang",
        "Mengna Wang",
        "Gangao Liu",
        "Xu Huixin",
        "Yiwei Jiang",
        "Yongliang Shen",
        "Guiyang Hou",
        "Zhe Zheng",
        "Hang Zhang",
        "Xin Li",
        "Weiming Lu",
        "Peng Li",
        "Yueting Zhuang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in deep thinking models have demonstrated remarkable reasoning capabilities on mathematical and coding tasks. However, their effectiveness in embodied domains which require continuous interaction with environments through image action interleaved trajectories remains largely -unexplored. We present Embodied Reasoner, a model that extends o1 style reasoning to interactive embodied search tasks. Unlike mathematical reasoning that relies primarily on logical deduction, embodied scenarios demand spatial understanding, temporal reasoning, and ongoing self-reflection based on interaction history. To address these challenges, we synthesize 9.3k coherent Observation-Thought-Action trajectories containing 64k interactive images and 90k diverse thinking processes (analysis, spatial reasoning, reflection, planning, and verification). We develop a three-stage training pipeline that progressively enhances the model's capabilities through imitation learning, self-exploration via rejection sampling, and self-correction through reflection tuning. The evaluation shows that our model significantly outperforms those advanced visual reasoning models, e.g., it exceeds OpenAI o1, o3-mini, and Claude-3.7 by +9\\%, 24\\%, and +13\\%. Analysis reveals our model exhibits fewer repeated searches and logical inconsistencies, with particular advantages in complex long-horizon tasks. Real-world environments also show our superiority while exhibiting fewer repeated searches and logical inconsistency cases."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 6 9 6 1 2 . 3 0 5 2 : r Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks Wenqi Zhang1, * Mengna Wang2,3,* Gangao Liu2,3 Xu Huixin2,6,7 Yiwei Jiang2,6,8 Yongliang Shen1 Guiyang Hou1 Zhe Zheng1 Hang Zhang4 Xin Li5 Weiming Lu1 Peng Li2,3,6, Yueting Zhuang1, 1College of Computer Science and Technology, Zhejiang University 2Institute of Software, Chinese Academy of Sciences 3University of Chinese Academy of Sciences 4Alibaba Group 5DAMO Academy, Alibaba Group 6Nanjing Institute of Software Technology 7Nanjing University of Posts and Telecommunications 8Hohai University zhangwenqi@zju.edu.cn, lipeng@iscas.ac.cn Project: https://embodied-reasoner.github.io/"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in deep thinking models have demonstrated remarkable reasoning capabilities on mathematical and coding tasks. However, their effectiveness in embodied domains which require continuous interaction with environments through image action interleaved trajectories remains largely -unexplored. We present Embodied Reasoner, model that extends o1 style reasoning to interactive embodied search tasks. Unlike mathematical reasoning that relies primarily on logical deduction, embodied scenarios demand spatial understanding, temporal reasoning, and ongoing self-reflection based on interaction history. To address these challenges, we synthesize 9.3k coherent Observation-Thought-Action trajectories containing 64k interactive images and 90k diverse thinking processes (analysis, spatial reasoning, reflection, planning, and verification). We develop three stage training pipeline that progressively enhances the models capabilities through imitation learning, self exploration via rejection sampling, and self-correction through reflection tuning. The evaluation shows that our model significantly outperforms those advanced visual reasoning models, e.g., it exceeds OpenAI o1, o3-mini, and Claude-3.7 by +9%, 24%, and +13%. Analysis reveals our model exhibits fewer repeated searches and logical inconsistencies, with particular advantages in complex long horizon tasks. Real-world environments also show our superiority while exhibiting fewer repeated searches and logical inconsistency cases. *The first two authors have equal contributions. Corresponding author. Figure 1. We design an embodied interactive task: searching for objects in an unknown room. Then we propose EmbodiedReasoner, which presents spontaneous reasoning and interaction ability. Before each action, it generates diverse thoughts, e.g., self-reflection or spatial reasoning, forming an image-text interleaved trajectory. It shows consistent reasoning and efficient search behaviors, whereas OpenAI o3-mini often exhibits repetitive searches and logical inconsistencies with higher failure rates. 1. Introduction Recent advancements in deep-thinking models such as OpenAI o1 [30], Gemini 2.0 Flash Thinking [10], DeepSeek R1 [14], and Qwen-QwQ [39] have demonstrated remarkFigure 2. Embodied-Reasoner exhibits spontaneous thinking behaviors, e.g., analyzing environmental states (#1,3), reflecting on missed details (#4), reasoning based on the latest observations (#5), and recalling cues for efficient planning (#9). These thoughts remain coherent and logically consistent despite spanning multiple rounds. In contrast, general VLMs lacking thinking abilities struggle with long-horizon interactive tasks and produce unreasonable actions, e.g., forget tasks or repetitive searching. able reasoning capabilities in domains requiring extensive deliberation. These models, trained through large-scale reinforcement learning (RL)[14, 38] or post-training on elaborate thought trajectories[27, 64], exhibit human-like thinking patterns and self-reflection before arriving at solutions. Their success has led to significant progress in domains requiring deliberate reasoning, particularly in college-level mathematics [13, 27] and coding tasks [18, 62]. Despite these advances, critical question emerges: Can the o1-style reasoning paradigm be extended beyond these specialized domains to address more complex challenges that require embodied intelligence? Particularly, can these reasoning capabilities be effectively applied to embodied tasks demanding long-horizon planning and deliberate reasoning in interactive environments? This extension is nontrivial due to several fundamental challenges: Challenge 1: Extended Multimodal Interaction. Compared to most question-answering tasks, which are limited to single-turn dialogues, embodied models operate in an interactive manner over long-horizon task. This means them must continuously interact with the environment, gather real-time feedback, which most appear in visual modality, and then make reasonable actions accordingly (textual modality). In such scenarios, the model need to process lengthy and image-action interleaved context and produce coherent, contextually consistent reasoning. Nevertheless, this remains challenge for many current multimodal models and visual reasoning models [12, 15, 15, 34, 55, 55]. We observe that even advanced reasoning models like OpenAI o3-mini [31] frequently fail to exhibit robust reasoning in these embodied interactive tasks, leading to repetitive or inconsistent behaviors. Challenge 2: Diverse Reasoning Modalities. Different from mathematical tasks that primarily rely on professional knowledge and logical deduction, embodied scenarios demand broader set of capabilities existing in daily life. As shown in Fig. 2, when searching for an object hidden in an unknown room, the model must leverage commonsense knowledge to infer potential search areas (e.g., steps 1, 3), comprehend object spatial relationships to plan efficient exploration paths at steps 1, 5, and employ temporal reasoning to recall relevant cues from previous attempts (step 9) while reflecting on prior failures. These multifaceted reasoning requirements pose challenges for multimodal models. In this paper, we present Embodied-Reasoner, novel approach that extends deep-thinking capabilities to embodied interactive tasks. Our key insight is that effective embodied reasoning requires not just the ability to process multimodal inputs, but also to generate diverse thinking processes (analysis, planning, reflection) that adapt to different stages of an interaction. To develop this capability, we develop data engine that automatically synthesizes coherent Observation-Thought-Action trajectories enriched with diverse, embodied-specific thinking processes, e.g., situational analysis, spatial reasoning, self-reflection, task planning, and verification. These coherent, image-text interleaved trajectories guide the model to learn how to plan and reason based on its interaction history and spatial layout, thereby boosting its spatial and temporal reasoning capabilities. We further introduce three-stage iterative training pipeline for embodied model that combines imitation, selfexploration, and self-correction. The pipeline begins with imitation learning on synthesized trajectories to develop basic interaction skills, followed by rejection sampling tuning to enhance exploration abilities, and concludes with reflection tuning to foster self-correction. We evaluate our approach on four high-level embodied tasks in the AI2-THOR simulator [23]: Search, Manipulation, Transportation, and Composite Tasks. These tasks require agents to locate hidden objects in unfamiliar environments through reasoning and planning, then manipulate or transport them to designated areas. Our data engine synthesizes 9.3k task instructions paired with interactive trajectories, containing 64k images and 8M thought tokens, spanning 107 diverse indoor scenes, 2,100 objects, and 2,600 containers. These trajectories are used for model training. Across 809 tasks in 12 novel scenarios, EmbodiedReasoner significantly outperforms state-of-the-art VLMs and visual reasoning models, exceeding OpenAI o1, o3mini, and Claude-3.7-Sonnet-thinking by +9% in success rate and +12% in search efficiency. The performance gap widens particularly for complex composite tasks, where our model outperforms the second-best model by +39.9%. Our analysis reveals that Embodied-Reasoner demonstrates more consistent reasoning and efficient search behaviors by spontaneously generating more reasoning tokens for complex tasks and avoiding repetitive and inefficient exploration through temporal reasoning. Our contributions include: (1) framework for extending deep-thinking to embodied scenarios that addresses the unique challenges of interactive reasoning; (2) data engine that synthesizes diverse embodied reasoning trajectories with interleaved observations, thoughts, and actions; (3) three-stage training pipeline that progressively enhances interaction, exploration, and reflection capabilities; and (4) Extensive evaluation showing significant improvements over state-of-the-art models, particularly for complex long-horizon tasks. 2. Observation-Thought-Action Corpora To develop o1-style reasoning models for embodied scenarios, we first design an embodied task that requires high-level planning and reasoning rather than low-level motor control, i.e., search for hidden objects (Sec. 2.1). Next, we design data engine in the simulator to synthesize interactive reasoning corpora: task instructions (Sec. 2.2) and corresponding key action sequences (Sec. 2.3). Each action produces visual observation, forming an interaction trajectory. Lastly, we generate multiple thinking thoughts for each action, e.g., situation analysis, task planning, spatial reasoning, reflection, and verification, creating an interactive reasoning corpus with Observation-Thought-Action contexts (Sec. 2.4). As shown in Fig. 2, the model requires spatial reasoning abilities to understand the layout of the kitchen and object relations, infer potential locations (fridge, dinning table) based on commonsense knowledge, systematically search unexplored areas, and adapt its plan through real-time observations while avoiding repetitive searches. 2.1. Embodied Interactive Task Task Environments. We built our embodied task environment using the widely adopted AI2-THOR simulator, which provides physics simulation and real-time vision displays. We employ 120 unique indoor scenes, e.g., kitchens, and 2,100 objects, e.g., credit card and microwave. We control the robots movements (e.g., moveahead) and interactions (e.g., pickup object) using AI2THORs API, while capturing visual observations at each step. Task Categories. The robot is initialized in corner of an unknown room with limited view, i.e., only part of the room is visible. We design four common interactive tasks in everyday life, with different complexities. ①Search: Searching for an object in an unknown room, e.g., keychain. It may be placed somewhere or hidden inside container. ②Manipulate: Interacting with objects after searching, such as finding lamp and turning on the switch. ③Transport: After finding hidden object, transport it to another location. It involves multiple search and manipulaFigure 3. Left: Data Engine for <Instruction, Interactive Trajectory> synthesis. First, we synthesize instructions from task templates, and build an affiliation graph from scenes meta-data. It enables us to derive key actions needed for task. We add exploratory actions and insert thinking thoughts between observation and actions. Right: Three-stage training recipe. ①We fine-tune on synthesized trajectory to develop interaction skills. ②We sample multiple trajectories on novel tasks and evaluate their correctness. The successful ones are used for developing its exploring abilities. ③We continue to sample trajectories using updated model, injecting anomalous states and reflective thoughts in successful cases and correcting errors in failed ones. This self-correction training yields Embodied-Reasoner. tion steps. ④Composite task: Involving multiple transport tasks in order, e.g., Place the egg in the microwave, and then put it on the desk after heating. After that, find ... Action Definition. Although AI2-THOR provides many low-level actions, our task focuses on higher-level planning and reasoning rather than movement control. Besides, lowlevel actions may lead to excessive interactions, so we encapsulate 9 high-level actions built on top of atomic actions: Observe, Move Forward, Navigate to {}, Put in {}, Pickup {}, Toggle {}, Close {}, Open {}, Termination. 2.2. Instruction Synthesis Our data engine leverages LLMs to generate task instructions automatically. However, unlike previous instruction synthesis [36, 47], embodied task instruction must satisfy the constraints of the scenario, i.e., avoid referencing objects that do not exist in the current scene or involving illegal actions, e.g., Please move the sofa to corner is invalid if the scene does not contain sofa or sofa cannot be moved. Thus, we first design multiple task templates for each task, leverage GPT-4os coding capabilities to automatically select objects that meet task constraints, and diversify instructions into different styles and complexities. Task Templates with Constraints. We design multiple task templates for each task. Fig. 3 presents transport task template: pickup {invisible A} and put in {B}, where denotes hidden object with pickupable attribute, e.g., keychain, and object should contain containment properties, such as drawer or desk. It ensures synthesized instructions validity. Templates and constraints are in Appendix D. Code-based Object Filter. We instruct GPT-4o to select an appropriate task template and generate code for constraint checking based on objects metadata. It selects objects that satisfy the constraints. We fill the template with matched objects, (A: keychain, B: desk), and synthesize multiple instructions with different object combinations. Diversify Instructions. Lastly, it automatically diversifies instructions from two levels: ① Style: We employ GPT4o to rewrite the filled template into multiple human-style instructions, e.g., can not find my keychain. Can you help me find them and .... ② Difficulty: We sequentially combine multiple simple tasks to create composite tasks. 2.3. Action Sequence Synthesis Our engine automatically annotates the key actions sequences for synthesized instructions and also produces various action sequences with additional search processes. Affiliation Graph. Firstly, as shown in Fig. 3, we construct an affiliation graph using the simulators metadata. In the graph, every node represents an object, and edge denotes an affiliation relation between two objects, e.g., keychain in drawer is depicted as leaf (keychain) connected to parent node (drawer) with include relationship. Key Action Sequence. Then we utilize the constructed affiliation graph and synthesized instruction template to derive the minimum required action sequence (key actions) for task completion. For example, pickup the keychain and place it in desk, we start from the leaf node (keychain) and trace upward to its parent node (drawer) and grandparent node (mudroom). GPT-4o generates the corresponding action sequence: A1: Navigate to Mudroom, A2: Navigate to Drawer, A3: Open Drawer, A4: Pickup .... All key actions are indispensable for completing tasks. Add Additional Searching Processes. Beyond the key action sequences, our engine also synthesizes exploratory paths by inserting additional search processes. For instance, as shown in Fig. 3, our engine first inserts three searching actions: Nav to Sidetable, Desk, and Sofa. After failing to find the keychain, it inserts an Observe action until it ultimately locates the keychain in the drawer. These additional search actions make the trajectory more realistic and reasonable, showcasing how robot gradually explores an unfamiliar environment until it successfully locates the target. Figure 4. We analyze the frequency of five types of thoughts and their flexible transition relationships in all trajectories. consistent with the previous thoughts (t1:n-1). 3. Training Recipe for Embodied-Reasoner To incentivize the reasoning ability, we design three training stages, i.e., imitation learning, rejection sampling tuning, and reflection tuning, that bootstrap general VLM into an embodied interactive model with deep thinking ability. Multi-turn Dialogue Format. Considering that the interactive trajectory follows an interleaved image-text format (Observation-Thought-Action), we organize them as multiturn dialogue corpora. In each turn, the observed images and the simulators feedback serve as the User Input, while thoughts and actions as Assistant Output. During training, we compute the loss only for thoughts and action tokens. 2.4. Interleaving Thought with Observation-Action 3.1. Learn to Interact: Imitation Learning After running synthesized actions (a1, a2, ...an), we obtain an interaction trajectory: o1, a1, o2, a2, ..., on, an, where oi denotes first-person perspective images. Then, we generate multiple deep-thinking thoughts (ti) for each action, creating an interleaved context: Observation-Thought-Action. Diverse Thinking Pattern. Firstly, we define five thinking patterns to simulate human cognitive activities in different situations: Situation Analysis, Task Planning, Spatial Reasoning, Self-Reflection, and Double Verification. We use concise prompts to describe each pattern, guiding GPT4o in synthesizing the corresponding thinking process. Derive Thought from Observation-Action. For each interaction, we instruct GPT-4o to select one or more thinking patterns and then generate detailed thoughts based on interactive context. These thoughts are inserted between observations and actions (on, anon, t1 n, an). Specifically, we prompt GPT-4o with the previous interaction trajectory (o1, t1, a1, . . . , on), and upcoming action (an), and generate well-reasoned thinking process (tn). It should consider the latest observation (on) and offer reasonable rationales for next-step action (an), and also remain logically n, ..tk n, t2 In the first stage, we use data engine to generate small set of instruction-trajectory, most contain limited searching processes or only consist of key actions (ObservationThought-Key action). Qwen2-VL-7B-Instruct is fine-tuned on this dataset and learns to understand interleaved imagetext context, output reasoning and action tokens. After tuning, we develop Embodied-Interactor, which is capable of interaction within embodied scenarios. However, most synthesized trajectories only include key actions for task completion, without searching processes or observing environments. In most cases, Embodied-Interactor exhibits limited search capabilities, i.e., it does not know how to handle situations where objects cannot be directly found and require further searching. For instance, when it opens refrigerator for an egg but is empty, it may respond: egg does not exist rather than searching for other locations. 3.2. Learn to Search: Rejection Sampling Tuning Self-exploration Trajectory. DeepSeek-R1 reveals that advanced reasoning ability can be acquired through rejection sampling and reward-guided RL on large-scale selfexploration data. Inspired by this, we employ EmbodiedInteractor to sample massive self-generated trajectories for further training. Specifically, as shown in Fig. 3, we employ data engine to synthesize new task instructions and their key actions, and then use Embodied-Interactor to sample multiple trajectories on each instruction under high-temperature settings. Lastly, we select high-quality trajectories. Data Engine as the reward model. We use our data engine as process supervision reward model (PRM) to assess these sampled trajectories. We retain 6,246 successful trajectoriesmost of which complete the task after several search attempts. We perform second-stage instructiontuning on all collected trajectories, developing EmbodiedExplorer. We observe it exhibits adaptive planning and searching behaviors. For example, when the target object cannot be found directly, it formulates detailed search plan involving multiple potential areas with different priorities. 3.3. Learn to Self-reflect: Reflection Tuning Embodied-Explorer occasionally produces unreasonable actions, especially in long-horizon tasks, such as hallucination. Besides, robots often encounter temporary hardware malfunctions. It requires the model to self-reflect on unreasonable behaviors, recognize abnormal states, and correct them on time. As shown in Fig. 3, we employ EmbodiedExplorer to sample massive trajectories on previous tasks. ① For failed trajectories, we locate the first erroneous actions and construct self-correction trajectories. ② For succeeded trajectories, we insert anomaly states to simulate hardware fault. Insert Abnormal State into Succeeded Trajectory. We simulate two robot anomalies: navigation anomaly, where the robot navigates to an inconsistent location with command (e.g., action: navigate to the refrigerator, but instead navigates to table); and manipulation anomaly, where robot arm temporarily fails to perform interaction command. For succeeded trajectory {.., a, o+, t..}, we insert an abnormal state (o) after an action (a), and then generate self-reflective thoughts (tr) for this anomalies. Lastly, we retry the same action: {.., a, o, tr, a, o+ ..}. Reflect on Unreasonable Action in Failed Trajectory. Using synthesized key actions, we identify the first incorrect action in each failed trajectory (T raj). Then we generate self-reflective thoughts (tr) for incorrect action and supplement remaining correct trajectories (T rajt:n + ), creating revised trajectory: {T raj1:t , tt + }. We fine-tune the model on synthesized self-correction trajectories. For loss calculation, we mask out erroneous partial trajectory (T raj1:t ) and only compute the loss for reflective tokens (tt r) and the correct trajectory (T rajt:n r, rajt:n ). Stage #Trajectory Source Train1st Train2nd Train3rd Total Testset 1,128 6,246 2,016 9,390 809 Synthesis Self-Explore Synthesis - Human #Imageall #Imagemax #Actionavg 11 26 29 29 29 4636 45.8k 13.8k 64k 4.9k 4.11 7.33 8.63 7.22 6.06 Table 1. We synthesize 9.3k task, trajectory for training. Also, we manually annotate key actions for 809 novel testing tasks. 4. Dataset Statistics 4.1. Training Corpus As shown in Tab. 1, we synthesize 9,390 unique task instructions and their Observation-Thought-Action trajectory for three training stages, i.e., Scene,Inst,Traj. In the first stage, data engine synthesizes 1,128 instruction-trajectory pairs. In the 2nd stage, we remain 6,246 exploratory trajectories through rejection sampling. In the 3rd stage, data engine synthesizes 2,016 self-correction trajectories. Our dataset spans 107 diverse indoor scenes, e.g., kitchens and living rooms, and covers 2,100 interactive objects (e.g., eggs, laptops) and 2,600 containers (e.g., refrigerators, drawers). All trajectories contain 64K first-person perspective image from interaction and 8M thought tokens. The distribution of our dataset is presented in Appendix C. 4.2. Thoughts Analysis The Distribution of Thinking Patterns. We count the frequency of five thinking patterns in all trajectories. As shown in Fig. 4, task planning and spatial reasoning appear most frequently, with 36.6K and 26.4K, respectively. It means each trajectory contains about four planning and three reasoning. Besides, Self-reflection often occurs after search fails, with about two times per trajectory. These diverse thoughts incentivize the models reasoning capabilities. Conversion between Thinking Patterns. We also compute the transition probabilities between five thinking patterns (see Fig. 4). We find the relationship between them is flexible and depends on the situation. It typically begins with Task Planning, followed by task planning (55%) and spatial reasoning (45%). When navigating unknown regions, it frequently relies on spatial reasoning (ActionS: If search attempt fails, it shifts to self-reflection 42%). (ActionR: 33%), and once (sub-)task is completed, it may perform double verification sometimes (ActionV: 3%, SV: 6%). This diverse structure enables the model to learn spontaneous thinking and flexible adaptability. 4.3. Interactive Evaluation Framework We cultivate 809 test cases across 12 novel scenarios, which are different from training scenes. We manually design instructions and annotate corresponding key actions and final states: Instruction, Key Action, Final state. Notably, our test-set contains 25 carefully designed ultra long-horizon tasks, each involving four sub-tasks and 14-27 key actions. Metrics We design three metrics to assess the quality of the model-generated trajectories. Success Rate (%): It measures whether task is successfully completed by evaluating if the key actions align correctly and if the final state meets the task criteria. Search Efficiency: It evaluates task efficiencymore steps indicate lower efficiency. We calculate it as the ratio of key action numbers to predicted action numbers. Task Completeness (%): It computes the proportion of predicted actions that belong to the set of key actions. 5. Experiments 5.1. Main Results Much higher success rate, search efficiency, and task completeness. As shown in Tab. 2, Embodied-Reasoner significantly outperforms all reasoning models and VLMs by large margin (+9.6% to GPT-o1), including the latest GPT-o3-mini (+24%) and Claude-3.7-Sonnet-thinking (+13%). Besides success rate, our model also demonstrates clear advantages on search efficiency and task completeness over others, e.g., search efficiency is higher than GPT-o1 by +12%. Despite being significantly smaller than advanced reasoning models, Embodied-Reasoner demonstrates stronger interaction and reasoning capabilities in embodied scenarios. Advantages are more obvious on complex tasks. Analyzing success rate across four different task categories (search, manipulate, transport, and composite tasks), we observe that Embodied-Reasoner demonstrates significantly stronger performance on the more challenging tasks like composite and transport. Notably, on composite tasks, it outperforms the second-best model (GPT-4o) by +39.9%. Interestingly, on the relatively simpler search task, our model lags behind GPT-o3-mini by 13.4%. Our analysis finds that on these simpler tasks, Embodied-Reasoner sometimes over-explores, leading to missed detections of nearby objects. Our three-stage training progressively incentivize interaction and reasoning capabilities, from 14.7% to 80.9%. Our base model, Qwen2-VL-7B, initially achieved only 14.7%. After the first-stage imitation learning, it improved to 25.4%, mastering simple interaction ability. Subsequently, rejection sampling tuning significantly boosted performance to 65.4%, reaching level comparable to GPTo1 with exploration abilities. Finally, fine-tuning with selfcorrection trajectories further elevated the models success rate to 80.9%. We observe that most baseline models often exhibit repetitive searching behaviors and unreasonable planning, especially when handling long-horizon tasks. In contrast, Embodied-Reasoner performs strategic search Figure 5. Relations between task length and success rate, and output token number. As task complexity increases, our model generates more reasoning tokens to maintain high success rates. and planning after deep thinking and timely self-reflection, significantly reducing those unreasonable cases. 5.2. Analysis: How does deep-thinking paradigm enhance embodied search tasks? More robust to long-horizon tasks. To investigate the impact of the deep-thinking paradigm on embodied search tasks, we count the number of key actions required for each test case. More key actions indicate the task is more complex with more interactions, i.e., long-horizon tasks. As shown in Fig. 5, we visualize the relationship between task length (number of key actions), success rate, and the number of output tokens. We observe that as the number of key actions increases, the success rates of baseline models drop significantlyespecially when the task exceeds five actions. In contrast, our model remains robust to complex tasks, achieving over 60% success rate in most scenarios. Spontaneously scale up reasoning tokens for complex embodied tasks. Fig. 5 (Bottom) shows our EmbodiedReasoner utilizes significantly more reasoning tokens for complex searching tasks, nearly five times that of Gemini2.0-flash-thinking. Besides, as tasks become more complex, the response tokens of our model also grow from 1,000 to 3,500 tokens. We observe that when faced with complex composite searching tasks, Embodied-Reasoner engages in significantly longer analysis processes and more deliberate self-reflection. This deep thinking process enables it to plan more efficient search paths and avoid redundant actions, improving the success rate. In contrast, Gemini-2.0-flashthinking does not show clear increase in their output tokens, remaining at 1,000 tokens. It suggests it may fail to solve complex embodied tasks by inference time scaling up. Deep thinking alleviates repetitive searching actions. We observe that baseline models frequently exhibit repetitive search behaviors. For instance, after inspecting cabinet, the model may still attempt to check the same cabinet Generalpurpose VLMs Visual Reasoning Models Model Qwen2.5-VL-7B-Instruct [4] Qwen2-VL-7B-Instruct [45] Qwen2.5-VL-72B-Instruct [4] Qwen2-VL-72B-Instruct [45] Claude 3.5-Sonnet [2] Qwen-VL-Max [40] GPT-4o [29] QVQ-72B-Preview [34] Kimi-K1.5 [38] GPT-o3-mini [31] Gemini-2.0 Flash Thinking [10] Claude-3.7-Sonnet-thinking [3] GPT-o1 [30] Embodied-Interactor-7B (ours-1st) Embodied-Explorer-7B (ours-2nd) Embodied-Reasoner-7B (ours-3rd) Success Rate 12.38% 14.79% 31.75% 39.00% 45.35% 49.81% 66.67% 7.54% 46.00% 56.55% 56.74% 67.70% 71.73% 25.46% 65.39% 80.96% Search Efficiency 10.87% 11.97% 22.61% 28.88% 28.05% 36.28% 41.68% 6.39% - 26.93% 43.01% 37.95% 43.06% 24.75% 46.25% 55.07% Task Completeness 27.53% 38.67% 50.62% 54.56% 64.12% 68.39% 79.07% 36.33% - 67.41% 71.70% 78.63% 82.49% 53.67% 77.73% 86.30% Search 6.45% 23.33% 52.14% 50.00% 54.25% 63.87% 69.03% 4.35% - 78.57% 71.05% 69.12% 78.42% 30.97% 60.00% 65.16% Success Rate for SubTasks Manipulate 23.55% 25.50% 38.89% 52.36% 50.51% 63.21% 79.26% 7.50% - 59.32% 75.60% 75.88% 79.10% 27.09% 75.92% 93.31% Transport 7.56% 2.82% 21.90% 33.19% 51.22% 45.16% 71.95% 10.53% - 66.67% 40.67% 71.94% 67.36% 29.20% 72.24% 87.20% Composite 0.95% 0.00% 0.00% 0.00% 3.84% 1.90% 14.42% 0.00% - 0.00% 8.89% 13.79% 13.16% 3.81% 26.67% 54.29% Table 2. We compare the performance of Embodied-Reasoner against advanced VLMs and visual reasoning models. After the three-stage training process, we boost Qwen2-VL-7B from 14.8 to 81. Kimi-K1.5 means we manually evaluate 50 testing cases through the webpage. Figure 6. Repetitive Exploration Rate measures repetitive search issues, which are often observed in baselines. Our models reduce repetitive searches by recalling and reflecting on past trajectories. after few steps. This behavior reflects its weaker temporal reasoning and context awareness abilities in interactive scenarios. To quantify this, we define repetitive exploration rate (RER), which measures how often the model navigates to the same area within its trajectory. As shown in Fig. 6, our models (Embodied-Reasoner / Explorer) consistently exhibit significantly lower RER (-50%) compared to baseline models across all four tasks. For example, in composite tasks, Embodied-Explorer achieves the lowest RER of 26%, while GPT-o3-mini and Qwen2-VL-72B reach 54% and 43%, respectively. Compared to EmbodiedExplorer, Reasoner exhibits slightly higher RER due to its more cautious nature, favoring multiple checks and reflections. In our models reasoning thoughts, we observe that it frequently recalls past observations, reflects on previously explored actions, and formulates new plans accordingly. These processes enhance its temporal reasoning ability, thereby reducing repetitive search behaviors. Figure 7. Real-world experiments. Our model achieves higher success rate (56.7%) than OpenAI o3-mini (43.4%) and o1 (50%). 5.3. Real-world Experiments To evaluate the generalization of our reasoning model, we design real-world experiment about object searching, covering 30 tasks across three scenes: 6 kitchen tasks, 12 bathroom tasks, and 12 bedroom tasks. During testing, human operator holds camera to capture real-time visual input. The model analyzes each image and generates an action command, which the operator executes the actions. Fig. 7 illustrates an example: Can you help me find the coffee and heat it up? Our model rules out the countertop and dining table after two explorations (steps 1,2), ultimately locating the coffee (#7) in the cabinet and placing it in the microwave for heating (#11). However, we observe that OpenAI o3-mini fails to formulate reasonable plan, heading to the microwave first instead of searching for the coffee. Besides, it frequently forgets to search and exhibits repetitive searching, aligning with our previous analysis. Please refer to Table B1 for detailed results. 6. Conclusions In this paper, we propose an embodied reasoning model, Embodied-Reasoner, for interactive search tasks that can spontaneously search, reason, and act. To develop this model, we design data engine that synthesizes 9,390 interactive trajectories in an Observation-Thought-Action interleaved format. It encompasses 64K images and 8M thought tokens featuring diverse thinking patterns. We employ three-stage training approachimitation learning, rejection sampling tuning, and reflection tuningto progressively enhance its interaction and reasoning abilities. Extensive evaluations and real-world experiments demonstrate that our model exhibits superior reasoning capabilities."
        },
        {
            "title": "References",
            "content": "[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as can, not as say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022. 1 [2] Anthropic. Claude 3.5-sonnet: Anthropics advanced language model, 2024. 8 [3] Anthropic. Claude 3.7 sonnet: Anthropics hybrid reasoning ai model, 2025. 8 [4] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [5] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023. 1 [6] Shaofei Cai, Zihao Wang, Kewei Lian, Zhancun Mu, Xiaojian Ma, Anji Liu, and Yitao Liang. Rocket-1: Mastering open-world interaction with visual-temporal context prompting. arXiv preprint arXiv:2410.17856, 2024. 1 [7] Li Chen, Penghao Wu, Kashyap Chitta, Bernhard Jaeger, Andreas Geiger, and Hongyang Li. End-to-end autonomous driving: Challenges and frontiers. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. 1 [8] Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, and Ziran Wang. Drive as you speak: Enabling human-like interaction with large language models in autonomous vehicles. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 902909, 2024. 1 [9] Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Yang Zhou, Kaizhao Liang, Jintai Chen, Juanwu Lu, Zichong Yang, Kuei-Da Liao, et al. survey on multimodal large language models for autonomous driving. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 958979, 2024. 1 [10] DeepMind. Flash thinking, 2025. 1, 8 [11] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palme: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023. 1 [12] Yifan Du, Zikang Liu, Yifan Li, Wayne Xin Zhao, Yuqi Huo, Bingning Wang, Weipeng Chen, Zheng Liu, Zhongyuan Virgo: preliminary exWang, and Ji-Rong Wen. arXiv preprint ploration on reproducing o1-like mllm. arXiv:2501.01904, 2025. [13] Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. rstar-math: Small llms can master math reasoning with self-evolved deep thinking. arXiv preprint arXiv:2501.04519, 2025. 2, 1 [14] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 1, 2 [15] Jarvis Guo, Tuney Zheng, Yuelin Bai, Bo Li, Yubo Wang, King Zhu, Yizhi Li, Graham Neubig, Wenhu Chen, and Xiang Yue. Mammoth-vl: Eliciting multimodal reasoning with instruction tuning at scale. arXiv preprint arXiv:2412.05237, 2024. 2 [16] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. 1 [17] Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia, and Siyuan Huang. An embodied generalist agent in 3d world. arXiv preprint arXiv:2311.12871, 2023. 1 [18] Zhongzhen Huang, Gui Geng, Shengyi Hua, Zhen Huang, Haoyang Zou, Shaoting Zhang, Pengfei Liu, and Xiaofan Zhang. O1 replication journeypart 3: Inference-time scaling for medical reasoning. arXiv preprint arXiv:2501.06458, 2025. 2, 1 [19] Pranav Kak and Sushma Jain. Embodied reasoning with In 2024 IEEE International Conference on self-feedback. Electronics, Computing and Communication Technologies (CONECCT), pages 15. IEEE, 2024. [20] Shyam Sundar Kannan, Vishnunandan LN Venkatesh, and Byung-Cheol Min. Smart-llm: Smart multi-agent robot task In 2024 IEEE/RSJ planning using large language models. International Conference on Intelligent Robots and Systems (IROS), pages 1214012147. IEEE, 2024. 1 [21] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. 1 [22] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022. 1 [23] Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani, Daniel Gordon, Yuke Zhu, et al. Ai2-thor: An interactive 3d environment for visual ai. arXiv preprint arXiv:1712.05474, 2017. 3 [24] Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vulic, and Furu Wei. Imagine while reasoning in space: Multimodal visualization-ofthought. arXiv preprint arXiv:2501.07542, 2025. 1 [25] Zijing Liang, Yanjie Xu, Yifan Hong, Penghui Shang, Qi Wang, Qiang Fu, and Ke Liu. survey of multimodel large In Proceedings of the 3rd International language models. Conference on Computer, Artificial Intelligence and Control Engineering, pages 405409, 2024. [26] Kevin Lin, Christopher Agia, Toki Migimatsu, Marco Pavone, and Jeannette Bohg. Text2motion: From natural language instructions to feasible plans. Autonomous Robots, 47 (8):13451365, 2023. 1 [27] Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang, Xiaoxue Cheng, Huatong Song, et al. Imitate, explore, and self-improve: reproduction report on slow-thinking reasoning systems. arXiv preprint arXiv:2412.09413, 2024. 2, 1 [28] Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, and Ping Luo. Embodiedgpt: Vision-language pre-training via embodied chain of thought. Advances in Neural Information Processing Systems, 36:2508125094, 2023. 1 [29] OpenAI. Gpt-4o: Openais multimodal language model, 2024. 8 [30] OpenAI. Openai o1: Advanced reasoning ai model, 2024. 1, 8 [31] OpenAI. 2025. 2, 8 o3-mini: Openais advanced reasoning model, [32] Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi Li, et al. O1 replication journey: strategic progress report part 1. arXiv preprint arXiv:2410.18982, 2024. 1 [33] Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, JiaYuan Gu, Bin Zhao, Dong Wang, et al. Spatialvla: Exploring spatial represenarXiv preprint tations for visual-language-action model. arXiv:2501.15830, 2025. 1 [34] QwenLM. Qvq-72b preview, 2025. 2, 8, 1 [35] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level googleproof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023. [36] Yongliang Shen, Kaitao Song, Xu Tan, Wenqi Zhang, Kan Ren, Siyu Yuan, Weiming Lu, Dongsheng Li, and Yueting Zhuang. Taskbench: Benchmarking large language models for task automation. arXiv preprint arXiv:2311.18760, 2023. 4 [37] Lucy Xiaoyang Shi, Zheyuan Hu, Tony Zhao, Archit Sharma, Karl Pertsch, Jianlan Luo, Sergey Levine, and Chelsea Finn. Yell at your robot: Improving on-the-fly from language corrections. 2024. 1 arXiv preprint arXiv:2403.12910, [38] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. 2, 8, 1 [39] Qwen Team. Qwq: Reflect deeply on the boundaries of the unknown, 2024. 1 [40] Qwen Team. Qwen-vl-max: Alibaba clouds advanced large vision language model, 2025. 8 [41] Omkar Thawakar, Dinura Dissanayake, Ketan More, Ritesh Thawkar, Ahmed Heakl, Noor Ahsan, Yuhao Li, Mohammed Zumri, Jean Lahoud, Rao Muhammad Anwer, et al. Llamavo1: Rethinking step-by-step visual reasoning in llms. arXiv preprint arXiv:2501.06186, 2025. 1 [42] Sai Vemprala, Rogerio Bonatti, Arthur Bucker, and Ashish Kapoor. Chatgpt for robotics: Design principles and model abilities. Ieee Access, 2024. 1 [43] Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu, and Katsushi Ikeuchi. Gpt-4v (ision) for robotics: Multimodal task planning from human demonstration. IEEE Robotics and Automation Letters, 2024. 1 [44] Jiaqi Wang, Enze Shi, Huawen Hu, Chong Ma, Yiheng Liu, Xuhui Wang, Yincheng Yao, Xuan Liu, Bao Ge, and Shu Zhang. Large language models for robotics: Opportunities, challenges, and perspectives. Journal of Automation and Intelligence, 2024. [45] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 8 [46] Weiyun Wang, Zhe Chen, Wenhai Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Jinguo Zhu, Xizhou Zhu, Lewei Lu, Yu Qiao, et al. Enhancing the reasoning ability of multimodal large language models via mixed preference optimization. arXiv preprint arXiv:2411.10442, 2024. 1 [47] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. arXiv preprint arXiv:2212.10560, 2022. 4 [48] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 1 [49] Junjie Wen, Yichen Zhu, Minjie Zhu, Jinming Li, Zhiyuan Xu, Zhengping Che, Chaomin Shen, Yaxin Peng, Dong Liu, Feifei Feng, et al. Object-centric instruction augmentation for robotic manipulation. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 4318 4325. IEEE, 2024. 1 [50] Licheng Wen, Xuemeng Yang, Daocheng Fu, Xiaofeng Wang, Pinlong Cai, Xin Li, Tao Ma, Yingxuan Li, Linran Xu, Dengke Shang, et al. On the road with gpt-4v (ision): Early explorations of visual-language model on autonomous driving. arXiv preprint arXiv:2311.05332, 2023. 1 [64] Yu Zhao, Huifeng Yin, Bo Zeng, Hao Wang, Tianqi Shi, Chenyang Lyu, Longyue Wang, Weihua Luo, and Kaifu Zhang. Marco-o1: Towards open reasoning models for openended solutions. arXiv preprint arXiv:2411.14405, 2024. 2, 1 [65] Zhonghan Zhao, Wenhao Chai, Xuan Wang, Boyi Li, Shengyu Hao, Shidong Cao, Tian Ye, and Gaoang Wang. See and think: Embodied agent in virtual environment. In European Conference on Computer Vision, pages 187204. Springer, 2024. [66] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 1 [51] Zhenyu Wu, Ziwei Wang, Xiuwei Xu, Jiwen Lu, and Haibin Yan. Embodied task planning with large language models. arXiv preprint arXiv:2307.01848, 2023. 1 [52] Zhenyu Wu, Ziwei Wang, Xiuwei Xu, Jiwen Lu, and Haibin Yan. Embodied instruction following in unknown environments. arXiv preprint arXiv:2406.11818, 2024. 1 [53] Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, and Li Yuan. Llava-cot: Let vision language models reason stepby-step, 2024. URL https://arxiv. org/abs/2411.10440. 1 [54] Haotian Xu, Xing Wu, Weinong Wang, Zhongzhi Li, Da Zheng, Boyuan Chen, Yi Hu, Shijia Kang, Jiaming Ji, Yingying Zhang, et al. Redstar: Does scaling long-cot data unlock better slow-reasoning systems? arXiv preprint arXiv:2501.11284, 2025. 1 [55] Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, et al. Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search. arXiv preprint arXiv:2412.18319, 2024. 2 [56] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, and Fei Huang. mplugowl2: Revolutionizing multi-modal large language model with modality collaboration. In Proceedings of the ieee/cvf conference on computer vision and pattern recognition, pages 1304013051, 2024. [57] Michał Zawalski, William Chen, Karl Pertsch, Oier Mees, Chelsea Finn, and Sergey Levine. Robotic control via arXiv preprint embodied chain-of-thought arXiv:2407.08693, 2024. 1 reasoning. [58] Di Zhang, Jianbo Wu, Jingdi Lei, Tong Che, Jiatong Li, Tong Xie, Xiaoshui Huang, Shufei Zhang, Marco Pavone, Yuqiang Li, et al. Llama-berry: Pairwise optimization for o1like olympiad-level mathematical reasoning. arXiv preprint arXiv:2410.02884, 2024. 1 [59] Ruohong Zhang, Bowen Zhang, Yanghao Li, Haotian Zhang, Zhiqing Sun, Zhe Gan, Yinfei Yang, Ruoming Pang, and Yiming Yang. Improve vision language model chain-ofthought reasoning. arXiv preprint arXiv:2410.16198, 2024. 1 [60] Wenqi Zhang, Yongliang Shen, Linjuan Wu, Qiuying Peng, Jun Wang, Yueting Zhuang, and Weiming Lu. Self-contrast: Better reflection through inconsistent solving perspectives. arXiv preprint arXiv:2401.02009, 2024. 1 [61] Wenqi Zhang, Hang Zhang, Xin Li, Jiashuo Sun, Yongliang Shen, Weiming Lu, Deli Zhao, Yueting Zhuang, and Lidong Bing. 2.5 years in class: multimodal textbook for visionarXiv preprint arXiv:2501.00958, language pretraining. 2025. [62] Yuxiang Zhang, Shangxi Wu, Yuqi Yang, Jiangming Shu, Jinlin Xiao, Chao Kong, and Jitao Sang. o1-coder: an o1 replication for coding. arXiv preprint arXiv:2412.00154, 2024. 2, 1 [63] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-ofarXiv preprint thought reasoning in language models. arXiv:2302.00923, 2023. 1 A. Related Works A.1. Large Language Models Reasoning Recent o1-style models [10, 14, 30, 39, 62, 64] have demonstrated powerful reasoning capabilities, significantly enhancing their ability on college-level benchmarks such as mathematics [16, 35]. Unlike previous efforts to scale up training data and model sizes, these systems involve generating long thought tokens during inference time, improving the performance of final answers [13, 27, 58, 60]. From chain-of-thought (CoT) prompting by humans [22, 48] to autonomous thinking derived by LLMs themselves, the reasoning abilities have become increasingly internalized and spontaneous. Besides, the long-thought generation process is quite similar to human thinking activities, with diverse thinking patterns, e.g., step-by-step solving, self-reflection, backtracking, and self-validation [18, 32]. These characteristics greatly enhance the complex problem-solving capabilities of LLMs. However, most efforts focus on textbased tasks, and the application of deep-thinking paradigms to embodied scenarios remains largely unexplored. This is the focus of our work. A.2. Vision-Language Model Reasoning Recently, many efforts [41, 46, 53, 54, 59], such as QVQ [34] and Kimi-1.5 [38], have extended the deepthinking paradigm to multimodal scenarios by post-training on long-CoT thought or large scale reinforcement learning. However, most visual reasoning models operate in single-round dialogue setting: processing input images and users query and generating textual thoughts for final answer. This limits their applicability in embodied interactive tasks [1, 5, 11, 21, 26], which require handling multi-image or image-text interleaved contexts while generating coherent and logical thoughts across multiple interactions. Besides, embodied scenes differ from mathematical tasks, as they demand long-horizon planning and deliberate reasoning over previous trajectories. Inspired by image-text interleaved understanding and generation [24, 61], we propose an effective solution to extend general VLMs into embodied reasoning models, including synthesizing interleaved observation-thought-action trajectories and bootstrapping VLMs through iterative training. A.3. Vision-Language-Action Model Some research focuses on the integration of agents and language models. For instance, agents like PaLM-E[11] and other researches[6, 17, 19, 25, 28, 33, 37, 51, 52, 56, 65, 66] leverage the inherent knowledge of large language models (LLMs) and combine robotic data with general visual - language data for training. Another part of the research[7, 9] emphasizes the integration of language models (LLMs) and visual-anguage models (VLMs) into robotics. Take ChatModel Qwen2.5-VL-72B-Instruct OpenAI o1 OpenAI o3-mini Embodied-Reasoner Success Rate (%) 43.3 50.0 44.0 56.7 Table B1. The results of real-world experiments. gpt for robotics[42] as an example. The study first defines high - level robotic function library, then creates prompts to guide ChatGPT to achieve its goals. In addition, some research[44, 49, 50, 63] utilizes the capabilities of visual-language models (VLM) to assist traditional strategy training such as reinforcement learning and imitation learning, thus promoting the development of robotic manipulation. Recently, large amount of research[8, 20, 43, 57] has been dedicated to developing generalist robotic strategies, which are trained based on continuously expanding robotic learning datasets. However, most of the existing work directly predicts action sequences without enabling robots to have an autonomous thinking process and this is one of the key focuses of our work. B. Experiment Details B.1. Real-World Experiments Result To evaluate the generalization of our reasoning model, we design real-world experiment about object searching, covering 30 tasks in three scenes. As shown in Tab. B1, Embodied-Reasoner demonstrates notable capabilities in real-world environments. In terms of success rate, it outperforms OpenAI o1 by 6.7%, OpenAI o3-mini by 12.7%, and Qwen2.5-VL-72B-Instruct by 13.4%. B.2. Repeat Exploration Rate The Repeat Exploration Rate (RER)indicates how often the model revisits the same location within its trajectory and is calculated as the number of revisits to previous locations divided by the total number of explorations. For example, in task, the model navigated to the following path: lacea, laceb, laceb, lacec, lacec. The model revisited laceb and lacec once each. Thus, the repeat exploration rate was 40% (2/5). C. Dataset Details C.1. Distribution of Task Instructions We synthesize 9,390 unique task instructions along with their Observation-Thought-Action trajectories as the training set. The distribution of training tasks is shown in Fig. C1, encompassing 4 task types(Search, Manipulate, Transport and Composite) and 10 subtask types. Figure C1. The distribution of the training dataset with 9,390 samples, including 4 task types and 10 sub-task types. Figure C3. The distribution of the training set interactions, including 8 interaction types in trajectories: navigate to, pickup, open, close, put in, observe, move forward, and toggle. Figure C2. The distribution of the test set with 809 tasks, including 4 task types and 11 sub-task types. For evaluation, we curate 809 test cases across 12 novel scenarios distinct from the training environments. The distribution of test tasks is shown in Fig. C2, covering 4 task types and 11 corresponding subtask types. C.2. Distribution of Interaction Types In the training set, trajectories consist of eight types of interaction actions: navigate to, pickup, open, close, put in, observe, move forward, and toggle. As shown in the Fig. C3, the occurrence frequency of each interaction action across all trajectories is illustrated. Among them, the exploration action navigate to appears the most frequently, occurring over 29k times. In the test set, we manually design instructions and annotate the corresponding key actions and final states. The test tasks involve six types of interactions: navigate to, pickup, open, close, put in, and toggle. As seen in the Fig. C4, navigate to also appears significantly more frequently than other key actions. C.3. Distribution of Task Length In the training set, each trajectory consists of an average of 7.2 interactions with the environment (e.g., navigate, Figure C4. The distribution of the test set interactions, including 6 interaction types in key actions: navigate to, pickup, open, close, put in, and toggle. pickup). For the four task types: Search, Manipulate, Transport, and Composite. Due to varying task complexity, the corresponding trajectory lengths also differ. As shown in the Fig. C5, Search tasks tend to have shorter trajectories, typically ranging from 1 to 9, while the more complex Composite tasks generate the longest trajectories, usually exceeding 8 and reaching beyond 23. Similarly, in the test set, as shown in the Fig. C6, the more complex Composite tasks also exhibit the longest key action sequences, usually exceeding 8 and reaching beyond 19. C.4. Distribution of Object Categories Our training dataset includes 107 indoor scenes with diverse functions and layouts (kitchens, living rooms, bedrooms, and bathrooms), featuring over 2,100 interactive objects (e.g., eggs, laptops) and 2,600 containers (e.g., refrigerators, drawers). Across the 9,390 unique task instructions, trajectories involve interactions with these objects and containers, with the top 32 most frequently explored and interacted object categories shown in Fig. C7. In the 12 distinct test set scenes, key actions correspond to different objects, Figure C5. The quantity distribution of trajectory lengths in the training set and the corresponding task type composition, where Search Task is mainly within 1-9, Manipulate Task within 2-11, Transport Task within 3-14, and Composite Task above 8, extending beyond 23. Figure C6. The quantity distribution of key action lengths in the test set and the corresponding task type composition, where Search Task is mainly within 1-2, Manipulate Task within 2, 4-5, Transport Task within 4-7, and Composite Task above 8, extending beyond 19. with the top 32 most frequently involved containers and interactive objects illustrated in Fig. C8. C.5. Description of Sub-task Types Our four daily tasks: Search, Manipulate, Transport, and Composite can be further divided into corresponding subtasks based on the types of objects involved. Specifically, the Search Task can be categorized into Exposed Object Search and Enclosed Object Search. The Manipulate Task can be divided into Exposed Object Grasping, Enclosed Object Grasping, and Exposed Object Toggle. The Transport Task can be classified into Exposed-to-Exposed Object Transfer, Exposed-to-Enclosed Object Transfer, Enclosedto-Exposed Object Transfer, and Enclosed-to-Enclosed Object Transfer. Finally, the Composite Task can be divided into Sequential Object Transfer and Long-Term Complex Task. Exposed Object Search. This task is defined as searching for items within room environment. The target items are located on the surface and there is no need to open any containers. For example, if there is an apple placed on the table in the room, the corresponding task description is Please find the apple in the room. Enclosed Object Search. This type of task refers to searching for items in room where the target items are inside containers, and the containers need to be opened during the task execution. For instance, if there is an egg in the refrigerator in the room, then the task description is Please find the egg in the room. Exposed Object Grasping. This type of task requires obtaining specific item in room, and the target item is on the surface without the need to open any containers. For example, when there is cup on the table in the room, the corresponding task description is Please pick up the cup in the room. Enclosed Object Grasping. This task is to obtain an item located inside container in room, and the container needs to be opened during the execution. For example, if there is bowl in the cabinet in the room, the corresponding task description is Please pick up the bowl in the room. Exposed Object Toggle. This task is to perform switch operations on items located on the surface in room, without the need to open any containers. For example, if there is coffee machine on the table in the room, the corresponding task description is Please start the coffee machine. Exposed-to-Exposed Object Transfer. his task requires moving an item located on the surface (without opening any containers) to another position in the room. For example, if there is spoon on the table in the room, the task description is Please pick up the spoon in the room and place it in the Figure C7. The quantity distribution of the top 32 object types in the training datasets trajectories, with Others representing the remaining 62 categories, such as Bread, Book, DeskLamp, etc. Figure C8. The quantity distribution of the top 32 object types in the test sets key actions, with Others representing the remaining 44 categories, such as Watch, Pencil, Cup, etc. sink. Exposed-to-Enclosed Object Transfer. This task is defined as moving an item located on the surface (without opening any containers) into closed container in the room. The container needs to be opened to complete the placement. For example, if there is credit card on the table in the room, the task description is Please pick up the credit card in the room and place it in the drawer. Enclosed-to-Exposed Object Transfer. This task refers to moving an item located inside container to another position after opening the container, finding and picking up the item. For example, if there is dishwashing sponge in the cabinet, the corresponding task description is Please pick up the dishwashing sponge in the room and place it on the table. Enclosed-to-Enclosed Object Transfer. This task is defined as moving an item located inside one container to another closed container in the room. Two containers need to be opened during the execution. For example, if there is loaf of bread in the refrigerator in the room, the task description is Please pick up the bread in the room and place it in the cabinet. Sequential Object Transfer. This task requires moving different items to specified locations in room in specific order. It is composed of two of the three task types: Exposed-to-Exposed Object Transfer, Exposed-to-Enclosed Object Transfer, and Enclosed-to-Exposed Object Transfer. For example, if there are apples on the table and eggs in the cabinet in the room, the task description is First, find the apples in the room and place them in the sink (involving Exposed-to-Exposed Object Transfer), then find the eggs and place them on the table (involving Enclosed-to-Exposed Object Transfer). Long Term Complex Task. This task involves performing series of ordered and complex long-range operations in room. It is composed of four of the five types: Exposed Object Toggle, Exposed-to-Exposed Object Transfer, Enclosed-to-Exposed Object Transfer, Enclosedto-Enclosed Object Transfer, and Exposed-to-Enclosed Object Transfer. For example, if there are potatoes in the refrigerator and mug on the table in the room, the task description is: First, please find the potatoes in the room and wash them (this process involves Exposed-to-Enclosed Object Transfer and Enclosed Object Transfer), then put the potatoes in the microwave to heat (involving Exposed-toEnclosed Object Transfer and Exposed Object Toggle), then find the mug and place it on the coffee machine to get coffee and then place the mug with coffee on the table (involving Exposed Object Transfer and Exposed Object Toggle), and finally place the potatoes from the microwave on the table (involving Enclosed Object Transfer). D. Detailed Task Templates and Constraints We design multiple task templates for each task. It ensures synthesized instructions validity. Templates and constraints are shown in Appendix D. E. Detailed Prompts We provide detailed interaction prompt designs for our evaluation framework in Appendix E. F. Case Study As shown in Figure F9, Figure F10, and Figure F11, we provide three cases. The first two illustrate different trajectory performances of Embodied-Reasoner and GPTo1 for the same task, while the third presents EmbodiedReasoners performance in real-world scenario. F.1. Embodied-Reasoners Action Trajectories As shown in Figure F9, it illustrates Embodied-Reasoners performance on the task: Could you please first place the Laptop on the Sofa, and then place the CellPhone on the Drawer? F.2. GPT-o1s Action Trajectories As shown in Figure F10, it illustrates GPT-o1s performance on the task: Could you please first place the Laptop on the Sofa, and then place the CellPhone on the Drawer? In comparison, during task trajectory, GPT-o1 sometimes forgets the task objective after navigating to the target container. For instance, after the 10th interaction, it navigates to the sofa but forgets to place the Laptop. Additionally, it occasionally falls into action loops, such as the repeated move forward actions from steps 13 to 16, 18 to 21. In contrast, Embodied-Reasoner performs reasoning and planning, searches multiple locations, and retains memory of the current steps objective, ultimately completing the task successfully. F.3. Embodied-Reasoner in Real-World As shown in Figure F11, it illustrates Embodied-Reasoners performance in the real world for the task: Id appreciate it if you could leave the milk on the coffee table when possible. It shows that Embodied-Reasoner can complete the task in real-world scenarios through step-by-step planning and reasoning. Task Types Sub-Task Types Templates Constraint Check Case Affiliation and Attribute Search Exposed Object Search find Pickupable(A) Openable(Parent(A)) Enclosed Object Search find Pickupable(A) Openable(Parent(A)) Exposed Object Toggle toggle Manipulate Toggleable(A) Openable(Parent(A)) Exposed Object Grasping pickup Pickupable(A) Openable(Parent(A)) Enclosed Object Grasping pickup Pickupable(A) Openable(Parent(A)) Transport Exposed-toExposed Object Transfer pickup put in Pickupable(A) Openable(Parent(A)) Openable(B) Exposed-toEnclosed Object Transfer pickup put in Pickupable(A) Openable(Parent(A)) Openable(B) Task: Could you please find the Apple in the room? Key Action Sequences: navigate to CounterTop end Task: Could you please find the Apple in the room? Key Action Sequences: navigate to Fridge open Fridge end Task: Would you mind powering on the Laptop for me? Key Action Sequences: navigate to Desk toggle Laptop, end Task: want to pick up CreditCard from the room, can you help me? Key Action Sequences: navigate to SideTable pickup CreditCard end Task: Would it be possible for you to pick up CreditCard from the room? Key Action Sequences: navigate to Drawer open Drawer pickup CreditCard, end Task: Could you please put the AlarmClock on the Shelf? Key Action Sequences: navigate to Sidetable pickup AlarmClock navigate to Shelf put in Shelf, end Task: Would you mind placing the Bowl in the Cabinet, please? Key Action Sequences: navigate to CounterTop pickup Bowl navigate to Cabinet open Cabinet put in Cabinet, end Task Types Sub-Task Types Templates Constraint Check Case Affiliation and Attribute Transport Enclosed-toExposed Object Transfer pickup put in Pickupable(A) Openable(Parent(A)) Openable(B) Enclosed-toEnclosed Object Transfer pickup put in Pickupable(A) Openable(Parent(A)) Openable(B) Composite task Sequential Object Transfer first, pickup A1 put in B1 then, pickup A2 put in Pickupable(A1) Openable(Parent(A1)) Openable(B1) Pickupable(A2) Openable(Parent(A2)) Openable(B2) Different(A1, A2) Long-Term Complex Task Task: Is it okay to put the Candle on the Bathtub? Key Action Sequences: navigate to Cabinet open Cabinet pickup Candle close Cabinet navigate to Bathtub put in Bathtub end Task: May ask you to put the Potato in the Microwave? Key Action Sequences: navigate to Fridge open Fridge pickup Potato close Fridge navigate to Microwave open Microwave put in Microwave end Task: Could you please first place the TeddyBear on the CoffeeTable, and then place the Pen on the GarbageCan? Key Action Sequences: navigate to Bed pickup TeddyBear navigate to CoffeeTable put in CoffeeTable navigate to Desk pickup Pen navigate to GarbageCan put in GarbageCan end Task: Could you put the bread in the refrigerator and then take the apple out of the refrigerator, wash it, and place it on plate? Key Action Sequences: navigate to CounterTop, pickup Bread, navigate to Fridge, open Fridge put in Fridge, pickup Apple navigate to SinkBasin, put in SinkBasin toggle Facuet, pickup Apple navigate to Cabinet, open Cabinet put Plate, close Cabinet end System prompt: You are robot in given room. You need to complete the tasks according to human instructions. We provide an Available_Actions set and the corresponding explanations for each action. Each step, you should select one action from Available_Actions. Initialization prompt: <image>This is an image from your frontal perspective. Please select an action from the Available_Actions and fill in the arguments. Task: {taskname} Available_Actions: {{ \"navigate to <object>\": Move to the object. \"pickup <object>\": Pick up the object. \"put in <object>\": Put the item in your hand into or on the object. \"toggle <object>\": Switch the object on or off. \"open <object>\": Open the object (container), and you will see inside the object. \"close <object>\": Close the object. \"observe\": You can obtain image of your directly rear, left, and right perspectives. \"move forward\": Move forward to see more clearly. \"end\": If you think you have completed the task, please output \"end\".}} Before making each decision, you can think, plan, and even reflect step by step, and then output your final action. Your final action must strictly follow format: <DecisionMaking>Your Action</DecisionMaking>, for example, <DecisionMaking>observe</DecisionMaking>. Interaction prompt: After executing your previous {action} , you get this new image above. To complete your task, you can think step by step at first and then output your new action from the Available_Actions. Your action must strictly follow format: <DecisionMaking>Your Action</DecisionMaking>, for example, <DecisionMaking>observe</DecisionMaking>. Interaction feedback prompt 1: <feedback>Action: {action} is illegal, {object} is the most relevant item in this room and {action}. Object: {object} is not currently navigable, you can try \"navigate to <object>\" to reach nearby, larger objects for closer observation. Interaction feedback prompt 2: <feedback>Action: {object} is illegal, Object: {object} is currently unavailable for interaction. Possible situations include: {object} does not exist in your current view; you are too far away from {object}; the {object} cannot perform operation {action}.nYou can try \"move forward\" to approach the target object or \"navigate to <object>\" to reach nearby, larger objects for closer inspection. Interaction feedback prompt 3: <feedback>Action: {action} is illegal, the name of the navigated object doesnt quite match the obejct in the image, please try navigating to another object first. Interaction feedback prompt 4: <feedback>Action: {action} is illegal, the name of the object doesnt quite match the obejct in the image, Please try interacting with another object or navigating to another object. Figure F9. Trajectory Case for Embodied Reasoner Figure F10. Trajectory Case for GPT-o Figure F11. Trajectory Case for Embodied Reasoner in Real World"
        }
    ],
    "affiliations": [
        "Alibaba Group",
        "College of Computer Science and Technology, Zhejiang University",
        "DAMO Academy, Alibaba Group",
        "Hohai University",
        "Institute of Software, Chinese Academy of Sciences",
        "Nanjing Institute of Software Technology",
        "Nanjing University of Posts and Telecommunications",
        "University of Chinese Academy of Sciences"
    ]
}