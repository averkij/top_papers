{
    "paper_title": "EtCon: Edit-then-Consolidate for Reliable Knowledge Editing",
    "authors": [
        "Ruilin Li",
        "Yibin Wang",
        "Wenhong Zhu",
        "Chenglin Li",
        "Jinghao Zhang",
        "Chenliang Li",
        "Junchi Yan",
        "Jiaqi Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Knowledge editing aims to update specific facts in large language models (LLMs) without full retraining. Prior efforts sought to tune the knowledge layers of LLMs, proving effective for making selective edits. However, a significant gap exists between their performance in controlled, teacher-forcing evaluations and their real-world effectiveness in lifelong learning scenarios, which greatly limits their practical applicability. This work's empirical analysis reveals two recurring issues associated with this gap: (1) Most traditional methods lead the edited model to overfit to the new fact, thereby degrading pre-trained capabilities; (2) There is a critical absence of a knowledge consolidation stage, leaving new facts insufficiently integrated into LLMs' inference-time behavior under autoregressive generation, thereby leading to a mismatch between parametric knowledge and actual generation behavior. To this end, we propose Edit-then-Consolidate, a novel knowledge editing paradigm that aims to bridge the gap between theoretical knowledge editing methods and their real-world applicability. Specifically, (1) our framework mitigates overfitting via Targeted Proximal Supervised Fine-Tuning (TPSFT) that localizes the edit via a trust-region objective to limit policy drift; (2) Then, a consolidation stage using Group Relative Policy Optimization (GRPO) aligns the edited knowledge with CoT-based inference policy by optimizing trajectory-level behavior under comprehensive reward signals. Extensive experiments demonstrate our framework consistently improves editing reliability and generalization under real-world evaluations, while better preserving locality and pre-trained capabilities."
        },
        {
            "title": "Start",
            "content": "EtCon: Edit-then-Consolidate for Reliable Knowledge Editing ETCON: EDIT-THEN-CONSOLIDATE FOR RELIABLE KNOWLEDGE EDITING Ruilin Li1,2, Yibin Wang2,3, Wenhong Zhu2,4, Chenglin Li2, Jinghao Zhang2,5, Chenliang Li,1 Junchi Yan,2,4 Jiaqi Wang2 1Wuhan University 4Shanghai Jiao Tong University https://github.com/RlinL/EtCon 2Shanghai Innovation Institute 5University of Science and Technology of China 3Fudan University 5 2 0 2 4 ] . [ 1 3 5 7 4 0 . 2 1 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Knowledge editing aims to update specific facts in large language models (LLMs) without full retraining. Prior efforts sought to tune the knowledge layers of LLMs, proving effective for making selective edits. However, significant gap exists between their performance in controlled, teacher-forcing evaluations and their real-world effectiveness in lifelong learning scenarios, which greatly limits their practical applicability. This works empirical analysis reveals two recurring issues associated with this gap: (1) Most traditional methods lead the edited model to overfit to the new fact, thereby degrading pre-trained capabilities; (2) There is critical absence of knowledge consolidation stage, leaving new facts insufficiently integrated into LLMs inference-time behavior under autoregressive generation, thereby leading to mismatch between parametric knowledge and actual generation behavior. To this end, we propose Edit-then-Consolidate, novel knowledge editing paradigm that aims to bridge the gap between theoretical knowledge editing methods and their real-world applicability. Specifically, (1) our framework mitigates overfitting via Targeted Proximal Supervised FineTuning (TPSFT) that localizes the edit via trust-region objective to limit policy drift; (2) Then, consolidation stage using Group Relative Policy Optimization (GRPO) aligns the edited knowledge with CoT-based inference policy by optimizing trajectory-level behavior under comprehensive reward signals. Extensive experiments demonstrate our framework consistently improves editing reliability and generalization under real-world evaluations, while better preserving locality and pre-trained capabilities."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large language models (LLMs) have demonstrated unprecedented capabilities across numerous tasks Guo et al. (2025); Wang et al. (2025b), serving as foundational reasoning engines for information retrieval Yang et al. (2025a), task automation agents He et al. (2025); Liu et al. (2025b); Wang et al. (2025a), and scientific research Rosen et al. (2025); Shmatko et al. (2025). However, as the external world continuously evolves, the static nature of LLMs pre-trained knowledge renders specific versions rapidly obsolete Zheng et al. (2025a). While retraining large-scale LLM with updated knowledge could address this limitation, it requires substantial computational resources and pre-training data to maintain both knowledge update efficacy and general capabilities, making frequent knowledge updates impractical Mitchell et al. (2022). Knowledge editing methods Zhang et al. (2025); Scialanga et al. (2025); Li et al. (2025b); Rozner et al. (2024) have thus garnered significant attention as techniques that achieve targeted knowledge updates through localized parameter modifications while avoiding extensive resource consumption. Knowledge editing methods can be categorized into three main paradigms: (1) Parametric in-place editing methods Meng et al. (2022a); Dai et al. (2025); Han et al. (2024); Fu et al. (2025), which Corresponding authors 1 EtCon: Edit-then-Consolidate for Reliable Knowledge Editing directly compute weight updates and apply them to the LLMs weight matrices, encompassing approaches such as locate-then-edit, parameter-efficient fine-tuning, and model mergingall of which preserve model architecture without requiring additional modules; (2) Meta-learning-based methods Hartvigsen et al. (2023); Li et al. (2025b); Tan et al. (2023) that train auxiliary hypernetworks to predict weight updates for specific parameters to achieve knowledge editing objectives; (3) Memorybased methods Wang et al. (2024d;a) that store new knowledge in external modules and train LLMs to activate these modules during inference involving updated knowledge. While these methods demonstrate promise in constrained evaluation scenarios such as single editing and teacher-forcing evaluations, significant performance gap emerges in more realistic auto-regressive evaluation and lifelong editing Jiang et al. (2024); Tan et al. (2023); Chen et al. (2024), leading recent research Gu et al. (2024a); Huang et al. (2024) to question the reliability and practical utility of existing knowledge editing methods. In this work, we conduct comprehensive investigation into the root causes of this performance gap, focusing on Parametric In-Place Editing methods due to their high potential for practical application in lifelong learning scenarios. Through comprehensive empirical analysis, we identify two critical issues at the root of this gap. First, most of traditional methods cause edited models to overfit to newly introduced facts. This overfitting leads to excessive specialization of model parameters to editing examples, thereby degrading pre-trained general capabilities including robust reasoning, linguistic fluency, and robustness. Second, and more critically, fundamental absence of dedicated knowledge consolidation phase is observed. This omission results in new information being superficially encoded at the parametric level, failing to establish deep integration with the LLMs inference-time behavior under autoregressive generation. This discordance manifests as critical decoupling between knowledge representation and inferential activation. While the model successfully incorporates updated knowledge parametrically, it consistently fails to retrieve, activate, or apply this knowledge during autoregressive generation. Figure 1: Illustration of the knowledge editing problem and our Edit-then-Consolidate solution. Figure 1 illustrates the core challenge we address. When existing methods edit fact (e.g., updating Michael Jordans nationality), the model may parametrically encode the new information but fail to consistently apply it during reasoning. This manifests as contradictory outputs where the model simultaneously acknowledges both old and new facts, revealing fundamental misalignment between parametric knowledge and actual generation behavior. Our Edit-then-Consolidate framework resolves this by introducing crucial consolidation stage that aligns the edited knowledge with the models inference-time policy. To address the limitation illustrated above, we propose Editthen-Consolidate (EtCon), two-stage knowledge-editing paradigm. In the first stage, we employ Targeted Proximal Supervised Fine-Tuning (TPSFT)a refined variant of PSFT Zhu et al. (2025) that selectively updates only the FFN layers identified as knowledge repositories. This targeted approach, combined with trust-region constraints, ensures localized edits that preserve the models broader capabilities. In the second stage, we introduce critical consolidation phase using Group Relative Policy Optimization (GRPO) to align the parametric knowledge with the models CoTbased inference policy through trajectory-level optimization under comprehensive reward signals. 2 EtCon: Edit-then-Consolidate for Reliable Knowledge Editing We conduct extensive experiments on three datasets with Llama-3-8B-Instruct and Qwen2.5-7B-Instruct. Under auto-regressive generation with natural stopping and an LLMas-a-judge protocol using GPT-4.1 (OpenAI (2025)), Edit-then-Consolidate improves editing reliability and generalization by 35%-50% over strong baselines. It also significantly enhances locality while preserving critical pre-trained capabilities. Our contributions can be summarized as follows: (1) We identify that the absence of knowledge-consolidation stage creates critical knowledgebehavior misalignment, serving as the key bottleneck to the real-world applicability of knowledgeediting methods. (2) We propose Edit-then-Consolidate (EtCon): TPSFT for localized parametric edits, followed by GRPO for trajectory-level consolidation that aligns parametric knowledge with actual generation behavior. (3) Extensive experiments demonstrate that EtCon improves editing reliability and generalization by 40%50%, strengthens locality, and preserves pre-trained capabilities under realistic evaluation settings."
        },
        {
            "title": "2.1 OVERVIEW OF KNOWLEDGE EDITING METHODS",
            "content": "From the perspective of model architecture, the three paradigms introduced above can also be coarsely grouped into two families. Parametric in-place editing methods preserve the vanilla LLM architecture. The locate-then-edit paradigm Meng et al. (2022a); Dai et al. (2025); Li et al. (2024); Zhong et al. (2025); Zhang et al. (2024c) identifies knowledge locations within LLMs and modifies targeted parameters through gradient-based or analytical solutions. PEFT methods Zhu et al. (2020); Han et al. (2024); Wang et al. (2024c); Gupta et al. (2025) directly update model parameters via regularized gradient descent to achieve knowledge updates while constraining side effects Liu et al. (2025a). These approaches seamlessly integrate with existing deployment infrastructure without additional inference latency. External-assisted editing methods rely on auxiliary modules for knowledge modification. Meta-learning approaches Tan et al. (2023); Hartvigsen et al. (2023); Li et al. (2025b) train hypernetworks to generate parameter updates, while memory-based methods Hartvigsen et al. (2023); Zhang et al. (2024b); Chen et al. (2024) encode knowledge in external modules that the LLM retrieves during inference. Despite their superior performance in balancing reliability and locality, external methods introduce deployment complexity. Given these trade-offs, our work advances parametric in-place editing for lifelong knowledge editing scenarios. 2.2 EVALUATION OF KNOWLEDGE EDITING METHODS Existing research Fang et al. (2024); Qi et al. (2025); Scialanga et al. (2025) predominantly evaluates the effectiveness of knowledge editing methods using standard set of metrics. Reliability assesses the success rate of editing by calculating the percentage where (new fact) > (old fact). Generalization evaluates the models ability to generalize to new knowledge post-editing, measured by the percentage where (new fact) > (old fact) when presented with rephrased queries pertaining to the new knowledge. Locality measures the extent to which editing specific fact preserves the models responses to questions related to neighboring, unedited facts. Conventionally, the evaluation input typically consists of simple queries with identical prompt formats, without additional contextual information. For the output, edited models responses are often truncated to target answer length or constrained by examples to match specific target answer format. In generation, teacher forcing is frequently employed, feeding ground truth tokens as input during the decoding process. Recent studies, however, have highlighted the fragility of such evaluation paradigms. Consequently, this paper adopts realistic evaluation approach for knowledge editing methods. Details of the real-world evaluation framework are in appendix A."
        },
        {
            "title": "3 THE MISSING CONSOLIDATION STAGE IN KNOWLEDGE EDITING",
            "content": "Recent studies have revealed critical performance gap in knowledge editing methods: while achieving high success rates under controlled teacher-forcing evaluation, these methods exhibit catastrophic failures in realistic auto-regressive settings. This stark discrepancy undermines their practical utility and raises fundamental questions about the effectiveness of current approaches. Through systematic investigation, we identify that this failure stems not from the editing mecha3 EtCon: Edit-then-Consolidate for Reliable Knowledge Editing nism itself, but from fundamental architectural omissionthe absence of knowledge consolidation stage. We hypothesize that successful knowledge updating requires two-stage process: (1) an initial parametric edit that injects new information into LLMs weights, followed by (2) consolidation phase that integrates this knowledge into the LLMs inference-time behavior under autoregressive generation. Without consolidation, edited knowledge remains superficially encoded at the parametric level, failing to propagate to the models actual generation behavior. Method Reli. Gener. Local. Pre-Edit(+GRPO) FT-M Pre-Edit t - 8 - 3 - l FT-M(+GRPO) ALPHAEDIT 2. 5.2 16.6 62.9 18.7 2.4 4. 15.5 52.7 14.0 38.7 38.6 38. 29.3 24.9 6.3 5.4 ALPHAEDIT(+GRPO) 50.4 Table 1: Performance comparison w/ and w/o consolidation under real-world evaluation on ZsRE. (+GRPO) denotes adding our consolidation stage. Figure 2: Reward curves comparison To validate this hypothesis, we conduct controlled experiments augmenting existing editing methods with consolidation mechanism. Table 1 presents compelling evidence: introducing Group Relative Policy Optimization (GRPO) as post-editing consolidation step dramatically transforms performance. For FT-M, reliability surges from 16.6% to 62.9% on Llama3-8b-Instruct, while ALPHAEDIT improves from 18.7% to 50.4%. Crucially, these gains extend to generalization metrics, indicating that consolidation enables genuine knowledge integration rather than superficial memorization. The reward trajectories in Fig. 2 further illuminate the consolidation dynamics. The monotonic increase demonstrates stable knowledge integration, where the model progressively aligns its reasoning behavior with the edited knowledge. Notably, applying GRPO directly to unedited models yields minimal improvements (Pre-Edit: 2.8% 5.2%), confirming that consolidation requires prior parametric editing as foundation. These findings establish critical insight: the limitations of current knowledge editing methods arise from treating editing as single-stage process. The Edit-then-Consolidate paradigm we propose addresses this fundamental gap, recognizing that parametric updates and behavioral alignment are complementary but distinct requirements for successful knowledge editing."
        },
        {
            "title": "4 THE EDIT-THEN-CONSOLIDATE FRAMEWORK",
            "content": "Building on the observational evidence in the preceding section, we posit that the limitations of current LLM knowledge-editing methods arise primarily from the lack of principled consolidation stage that integrates edited knowledge with the models reasoning behavior; moreover, repeated overfitting edits can erode general abilities. To address this, we introduce Edit-then-Consolidate paradigm: Stage employs Targeted Proximal Supervised Fine-Tuning (TPSFT) to perform localized knowledge editing under trust-regionstyle constraints, thereby limiting spillover while preserving pre-trained abilities; Stage II applies Group Relative Policy Optimization (GRPO) with task-appropriate comprehensive reward to consolidate at the trajectory level under real-world evaluation signals. The remainder of this section presents the design rationale and the interaction between these two stages. 4.1 KNOWLEDGE EDITING VIA TARGETED PROXIMAL FINE-TUNING In this section, we introduce Targeted Proximal Supervised Fine-Tuning (TPSFT) as refined knowledge-editing method that addresses the trilemma of reliability, locality, and generality. This approach differs from raw PSFT that update the whole LLMs, by selectively update only the FFNs 4 EtCon: Edit-then-Consolidate for Reliable Knowledge Editing Figure 3: Overview of the Edit-then-Consolidate (EtCon) Framework. Edit stage: We employ Targeted Proximal Supervised Fine-Tuning (TPSFT) to perform localized edits within the selected FFN layers to inject new knowledge. Consolidate stage: We use Group Relative Policy Optimization (GRPO) with comprehensive reward function to align the parametric knowledge with CoT-based inference policy. of LLMs. This targeted update strategy effectively injects new knowledge while minimizing disruption to the models overall architecture and pre-trained capabilities. We consider knowledge editing dataset = {(Si, ai)}N i=1 that contains editing instances , where each context Si contains question about the new fact, and ai is the corresponding ground-truth answer. At the start of the editing process, we have vanilla LLM Πθold parameterized by θold. We partition the models parameters into two disjoint sets: the target FFN parameters to be edited, θFFN, and the remaining frozen parameters, θfrozen, such that θ = θFFN θfrozen. The objective of TPSFT is to learn new set of FFN parameters, θnew FFN θfrozen. This model must accurately generate the target answer ai t, while minimizing disruptions to its performance on unrelated inputs. critical innovation in our TPSFT is the use of CoT-augmented training labels. For each editing instance (Si, ai), we: (1) prompt the vanilla LLM to generate CoT reasoning path for Si using an instruction template (see Appendix), and (2) replace the generated answer with the target new fact ai, yielding the training label yi = [CoTi; ai new]. This design enables learning smoothed distributions over reasoning paths rather than sharp one-hot targets. More importantly, it preserves the models natural reasoning patternsthe model learns to reach new answers through its inherent reasoning style rather than abandoning pre-trained capabilities. This significantly reduces disruption while ensuring accurate knowledge updates. FFN, yielding an updated model Πθnew where θnew = θnew for given context Si To achieve this, we update the targeted FFN parameters θFFN by minimizing the following Targeted Proximal Supervised Fine-Tuning (TPSFT) loss over the editing dataset D, while the rest of the model parameters remain frozen: LTPSFT(θFFN) = E(St,at)D [min (rt(θnew), clip(rt(θnew), 1 ϵ, 1 + ϵ))] . (1) Here, ϵ is hyperparameter that defines the clipping radius, which controls the size of the trust region. The probability ratio rt(θnew) is the core of this objective and is defined as: rt(θnew) = πθnew (atSt) πθold (atSt) , (2) where πθnew(atSt) is the probability of generating the ground-truth answer at given the context St from the model with updated FFN parameters, and πθold (atSt) is the corresponding probability 5 EtCon: Edit-then-Consolidate for Reliable Knowledge Editing from the reference policy. At the start of the editing process, this reference policy is the initial vanilla LLM. For each subsequent edit instance in the sequential editing process, it is then updated to be the state of the model resulting from the immediately preceding edit. This objective function creates trust-region constraint that is critical for balanced knowledge editing. The term rt(θ) aims to increase the likelihood of the correct answer, which is analogous to the objective in standard supervised fine-tuning. However, the clip function prevents this ratio from deviating too far from 1. When the updated model becomes significantly more confident about the target answer than the original model (i.e., when rt(θ) > 1 + ϵ), the gradient signal is effectively nullified for that instance. This mechanism acts as powerful regularization, discouraging overly aggressive updates that could lead to overfitting on the new fact and, consequently, the disruption of pre-trained capabilities. By integrating targeted parameter updates with constrained optimization objective, TPSFT directly addresses the editing trilemma. Locality is achieved by physically confining the updates to the FFN layers, which are hypothesized to be the primary repositories of factual knowledge. Reliability is enforced by the supervised objective that maximizes the probability of the new fact. Finally, stability is preserved by the PSFT clipping mechanism, which prevents drastic policy shifts and ensures that the models behavior remains stable and consistent across wide range of inputs beyond the specific edit. 4.2 KNOWLEDGE CONSOLIDATION VIA GROUP RELATIVE POLICY OPTIMIZATION After the TPSFT stage, the edited model has incorporated new facts at the parametric level. However, these parametric changes do not automatically propagate to the models reasoning capabilities. To bridge this gap, we introduce consolidation step using Group Relative Policy Optimization (GRPO) that aligns the models inference-time behavior with the injected knowledge. r, ai r)}M We formulate the consolidation as reinforcement learning problem. Given reasoning dataset Dr = {(Si i=1 containing queries that require reasoning over the edited facts, we optimize the edited model πθnew to generate trajectories that demonstrate both factual accuracy and reasoning consistency. The objective maximizes expected reward while constraining deviation from the postTPSFT model: E(Sr,ar)Dr, yπθ(Sr) (cid:2)rϕ(Sr, ar, y)(cid:3) β DKL(πθ πθnew) , (3) max θ where πθnew serves as the reference policy (the model after TPSFT), and β controls the strength of regularization. We optimize this objective using the GRPO algorithm with the following surrogate loss: JGRPO(θ) = (cid:34) (cid:88) i=1 (cid:16) min ρiAi, clip(ρi, 1 ϵ, 1 + ϵ) Ai (cid:35) (cid:17) , where ρi = πθ(yi Si group-relative advantage computed from batch of sampled trajectories. r) is the importance ratio, and Ai = Ri 1 r)/πθnew(yi Si (4) (cid:80)m j=1 Rj is the The reward function rϕ(Sr, ar, y) evaluates multiple aspects of the generated trajectory: rϕ(Sr, ar, y) = w1Raccuracy + w2Rformat + w3Rcleanliness + w4Rconsistency, (5) where Raccuracy measures factual accuracy (whether the final answer matches ar), Rformat enforces task-specific output format requirements, Rcleanliness encourages concise outputs without extraneous tokens, and Rconsistency rewards internal reasoning coherence and alignment between intermediate steps and the final answer. This consolidation step effectively integrates the parametric knowledge acquired through TPSFT into the models CoT-based inference policy, ensuring that the edited facts are not merely memorized but can be coherently utilized in complex reasoning tasks while maintaining locality on unrelated inputs. 6 EtCon: Edit-then-Consolidate for Reliable Knowledge Editing"
        },
        {
            "title": "5.1 EXPERIMENT SETTINGS",
            "content": "Datasets and Models. This work utilizes 1000 samples from each of three benchmark datasets, ZsRE Levy et al. (2017), COUNTERFACT Meng et al. (2022a), and QAEdit Yang et al. (2025b), to comprehensively evaluate the performance on knowledge editing tasks. We select two widely used LLMs, Llama-3-8B-Instruct Dubey et al. (2024) and Qwen-2.5-7B-Instruct Li et al. (2025a), as the base models for editing. For general ability evaluation, we use C-Eval Huang et al. (2023), CoQA Reddy et al. (2019), DROP Dua et al. (2019), SQuAD 2.0 Rajpurkar et al. (2018) and LogiQA Liu et al. (2020). Baselines. We compare our method against two main categories: Parametric In-Place Editing methods (FT-M Zhang et al. (2024a), MEMIT Meng et al. (2022b), ALPHAEDIT Fang et al. (2024), MMKE Fu et al. (2025)) and External-Assisted Editing methods (WISE Wang et al. (2024a)). Parametric In-Place Editing methods are the main focus of this work, and we select the most representative methods in this category as baselines. For External-Assisted Editing methods, we select WISE as it is the SOTA method in this category. Implementation Details. We conduct experiments using EasyEdit Xu et al. (2025) for evaluating various baselines, and employ the lm-evaluation-harness for assessing general capabilities. TPSFT is implemented through PSFT Zhu et al. (2025) for edit stage, while GRPO is built upon the EasyR1 Zheng et al. (2025b) for the consolidation stage. The specific hyperparameters are shown in Appendix A.1. Evaluation Metrics. We evaluate our method along two principal axes: editing performance and general capability preservation. To assess editing performance, we employ the LLM-as-judge framework from Yang et al. (2025b); Gao et al. (2024); Gu et al. (2024b); Wang et al. (2024b), which mitigates the overestimation issue inherent in token-based metrics. In this framework, we leverage GPT-4.1 for binary (correct/incorrect) evaluation of the models edited outputs to measure three key aspects: Reliability (edit success), Generalization (effectiveness on related inputs), and Locality (impact on unrelated inputs). To ensure that the editing process does not compromise the models broader abilities, we further evaluate its general capability preservation. To this end, we report Accuracy on the classification benchmarks C-Eval and LogiQA, alongside Exact Match (EM) and F1 scores for the question-answering datasets CoQA, DROP, and SQuAD 2.0. Details of real-world evaluation is in Appendix A.7 5.2 MAIN RESULTS Table 2 presents our evaluation of EtCon against existing baselines across three benchmarks under real-world lifelong editing evaluations. EtCon consistently outperforms all baselines across both model architectures. On Qwen-2.5-7B-Instruct, EtCon achieves 69.4% Reliability on ZsRE and 75.1% on QAEdit, surpassing the strongest baseline ALPHAEDIT by 53.5 and 75.1 percentage points respectively. Similar improvements occur on Llama-3-8B-Instruct, where EtCon reaches 73.5% Reliability on ZsRE versus FT-Ms 16.6%. Notably, EtCon maintains strong Generalization scores (60.8% on ZsRE, 63.0% on QAEdit for Qwen-2.5) while preserving acceptable Locality (24.2%-33.6%), confirming that our approach successfully preserves unrelated knowledge while performing targeted edits. The local editing methods (MEMIT and ALPHAEDIT) fail catastrophically in lifelong editing. MEMIT collapses entirely on Qwen-2.5-7B-Instruct with near-zero performance across all metrics. ALPHAEDIT performs marginally better but remains highly unstable: it achieves 15.9% Reliability on ZsRE but completely fails on COUNTERFACT and QAEdit (0.0% across all metrics) for Qwen-2.5. Even when ALPHAEDIT reaches 61.0% Reliability on COUNTERFACT for Llama-3, its Locality drops to 16.1%, indicating severe knowledge disruption. This failure stems from destructive interference between sequential edits, where uncontrolled accumulation of weight deltas causes exponential growth in layer norms, leading to model collapse. FT-M and WISE show improved stability over local editing methods but remain far below EtCons performance. FT-M achieves only 5.6% Reliability on ZsRE for Qwen-2.5 compared to EtCons 69.4%, while WISE performs even worse at 4.5%. On Llama-3, FT-Ms best result (27.9% on 7 EtCon: Edit-then-Consolidate for Reliable Knowledge Editing Table 2: Performance Comparison of Sequential Editing under Real-World Evaluation. The best results in each group are in bold, and the second-best results are underlined. ZsRE COUNTERFACT QAEdit Method Reli. Gen. Loc. Reli. Gen. Loc. Reli. Gen. Loc. Pre-edit FT-M MEMIT ALPHAEDIT WISE EtCon Pre-edit FT-M MEMIT ALPHAEDIT WISE EtCon 4.4 5.6 0.0 15.9 4.5 69.4 2.8 16.6 0.1 18.7 4.3 73.5 3.2 5.5 0.1 11.5 3.3 60.8 2.4 15.5 0.1 14.0 3.1 63. 28.5 23.1 0.0 6.8 19.1 24.4 38.6 29.3 0.0 6.3 2.2 30.2 1.0 3.2 0.0 0.0 1.4 59.6 0.6 27.9 0.3 61.0 1.3 67.1 0.5 3.1 0.2 0.0 1.5 43.2 0.8 18.6 0.7 43.8 0.8 53. 36.9 24.4 0.1 0.0 31.0 29.7 31.8 10.5 0.4 16.1 31.3 24.2 9.8 14.6 0.4 0.0 7.1 75.1 12.7 34.1 0.2 18.2 8.1 70.7 10.1 14.5 0.3 0.0 9.7 63.0 12.5 33.2 0.7 14.9 13.3 62. 36.2 30.7 0.2 0.0 16.9 32.3 44.3 30.1 0.0 7.5 0.9 33.6 Qwen2.5-7B -Instruct Llama-3 -8b-Instruct Table 3: Comprehensive comparison of sequential editing performance and preservation of general capabilities on Qwen2.5-7b-Instruct. DataSet Metric Base FT-M FT-M +Con MMKE MMKE +Con ALPHA ALPHA +Con EtCon Edited Knowledge QAEdit Reli. Gen. Loc. 12.6 13.9 36.2 14.6 14.5 30. 42.3 34.1 31.9 12.2 10.4 34.2 37.2 31.4 31.0 0.0 0.0 0.0 0.0 0.0 0.0 General Capabilities C-Eval Acc. 79.49 75.93 76.97 CoQA DROP SQuAD EM F1 EM F1 EM F1 54.47 70. 2.21 9.94 9.88 18.88 21.33 38.74 2.37 13.31 2.88 11.17 26.22 46. 2.79 14.59 4.37 13.53 LogiQA Acc. 38.71 37.02 37. 79.27 60.30 74.60 10.30 24.32 13.79 21.10 41.01 78. 59.07 73.33 8.46 21.59 12.05 19.55 39.17 23.02 23. 0.00 0.00 0.00 0.00 50.07 50.07 21.81 0.00 0.00 0.00 0. 50.07 50.07 21.81 75.1 63.0 32.3 78.45 55.13 69.41 2.52 8. 9.85 19.59 38.40 COUNTERFACT) still falls 39.2 percentage points below EtCon. These substantial performance gaps validate the effectiveness of our approach, which we attribute to two key design choices: the local editing in the edit stage preserves unrelated knowledge, and more crucially, the consolidation stage enables the reasoning network to effectively utilize the edited knowledge, thereby completing the critical final step in the knowledge editing pipeline. Extended experiments with up to 3,000 sequential edits demonstrating EtCons robustness in larger-scale lifelong editing scenarios are provided in Appendix A.3. 5.3 ANALYSIS OF CONSOLIDATION STAGE To comprehensively evaluate the effectiveness of the proposed Consolidation phase, we conducted extensive experiments on the QAEdit dataset. We augmented three baseline knowledge editing EtCon: Edit-then-Consolidate for Reliable Knowledge Editing Table 4: Ablation study of the key components in EtCon. on COUNTERFACT"
        },
        {
            "title": "Base",
            "content": "Edit. Consolidate."
        },
        {
            "title": "Methods",
            "content": "Reli. Gen. Loc. C-Eval CoQA sQuAD 2.0 - w/ SFT w/ TPSFT w/o Rcleanliness w/o Rconsistency Complete 0.6 1.4 3. 56.1 51.6 67.1 0.8 0.3 1.8 22.4 27.2 53.4 31.8 30.7 30. 24.7 25.1 24.2 50.82 48.66 50.07 78.20 75.76 78.52 - - - - - - 29.52 26.52 34.60 - - - methods (FT-M, MMKE, and ALPHAEDIT) with our Consolidation phase and compared their performance against our proposed EtCon method. As shown in Table 3, incorporating the Consolidation phase into FT-M and MMKE yields substantial improvements of 25-28% in Reliability and approximately 20% in Generalization metrics. These gains demonstrate that the Consolidation phase effectively bridges the gap between edited parametric knowledge and the models actual generation behavior, enabling successful knowledge utilization in real-world scenarios. Moreover, evaluations on multiple general-purpose benchmarks confirm that the Consolidation stage preserves the models general capabilities, with FT-M and MMKE maintaining their original performance levels and even exhibiting marginal improvements in certain cases. This preservation of general capabilities while enhancing editing performance validates the non-destructive nature of our consolidation mechanism. However, the Consolidation phase cannot repair damage incurred during the editing stage. While FT-M with Consolidation achieves 5-8 percentage point improvements in EM and F1 scores on CoQA, these metrics remain substantially below the original models performance, highlighting the importance of careful knowledge updates during editing. MMKEs design protects general capabilities but at the cost of reduced editing efficacy compared to EtCon. ALPHAEDIT exhibits model collapse after editing, which even the Consolidation phase cannot rectify. Overall, the Consolidation phase proves indispensable for knowledge editing, enabling effective generalization of newly edited knowledge while maintaining the models general capabilities. We further validate EtCons compatibility with reasoning-oriented architectures (DeepSeek-R1-Distill-Qwen-7B) in Appendix A.4. detailed time efficiency analysis is provided in Appendix A.5. 5.4 ABLATION STUDIES We conduct thorough ablation study on the COUNTERFACT dataset using Llama-3-8B-Instruct to isolate the individual contributions of each key component in our EtCon framework, with results presented in Table 4. In the Edit Stage, we compare our TPSFT against standard SFT. The results indicate that neither method alone is sufficient to enable reliable application of new knowledge, as reflected by low success and generalization scores. However, TPSFT demonstrates clear advantage in preserving the models general capabilities, significantly mitigating the degradation observed with SFT. In the Consolidation Stage, building upon the TPSFT edit, we ablate components of our comprehensive reward function. Removing the cleanliness reward (Rcleanliness) causes significant performance drop. Upon inspection, we find this encourages reward hacking, where the model generates extraneous content to maximize its score, such as both old and new facts. The performance degrades more severely upon removing the consistency reward (Rconsistency), leading to catastrophic failures in reliability; for instance, the model might state the correct answer and then immediately contradict it. These findings confirm that our comprehensive reward function is critical for preventing such reward hacking and effectively steering the consolidation process toward reliable and coherent reasoning. For illustrative case studies, please refer to Appendix A.8."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we identified that the critical gap between theoretical performance and practical effectiveness of knowledge editing methods stems from the absence of consolidation stage that integrates parametric knowledge into the LLMs actual generation behavior. To address this, we 9 EtCon: Edit-then-Consolidate for Reliable Knowledge Editing proposed the Edit-then-Consolidate (EtCon) framework, which combines Targeted Proximal Supervised Fine-Tuning (TPSFT) for precise knowledge editing with Group Relative Policy Optimization (GRPO) for effective consolidation. TPSFT updates targeted FFN weights within trust-region constraints to ensure reliable edits while preserving pretrained capabilities. Then GRPO aligns the edited knowledge with the models CoT-based inference policy through comprehensive reward function. Our controlled experiments demonstrated the necessity of the knowledge consolidation stage, and comprehensive evaluations showed that EtCon significantly outperforms existing methods in lifelong editing scenarios, achieving efficient knowledge updates while maintaining locality and preserving model capabilities. These results suggest that explicitly decoupling editing and consolidation represents promising paradigm for practical knowledge editing."
        },
        {
            "title": "REFERENCES",
            "content": "Qizhou Chen, Taolin Zhang, Xiaofeng He, Dongyang Li, Chengyu Wang, Longtao Huang, and Hui Xue. Lifelong knowledge editing for llms with retrieval-augmented continuous prompt learning. arXiv preprint arXiv:2405.03279, 2024. Yanbo Dai, Zhenlan Ji, Zongjie Li, and Shuai Wang. Namet: Robust massive model editing via noise-aware memory optimization. arXiv preprint arXiv:2505.11876, 2025. Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. Drop: reading comprehension benchmark requiring discrete reasoning over paragraphs. arXiv preprint arXiv:1903.00161, 2019. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pp. arXiv2407, 2024. Junfeng Fang, Houcheng Jiang, Kun Wang, Yunshan Ma, Shi Jie, Xiang Wang, Xiangnan He, and Tat-Seng Chua. Alphaedit: Null-space constrained knowledge editing for language models. arXiv preprint arXiv:2410.02355, 2024. Zichuan Fu, Xian Wu, Guojing Li, Yingying Zhang, Yefeng Zheng, Tianshi Ming, Yejing Wang, arXiv preprint Wanyu Wang, and Xiangyu Zhao. Model merging for knowledge editing. arXiv:2506.12384, 2025. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 2024. Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value memories. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 54845495, 2021. Jia-Chen Gu, Hao-Xiang Xu, Jun-Yu Ma, Pan Lu, Zhen-Hua Ling, Kai-Wei Chang, and Nanyun Peng. Model editing harms general abilities of large language models: Regularization to the rescue. arXiv preprint arXiv:2401.04700, 2024a. Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594, 2024b. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, et al. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(8081):633638, 2025. Akshat Gupta, Maochuan Lu, Thomas Hartvigsen, and Gopala Anumanchipalli. Efficient knowledge editing via minimal precomputation. arXiv preprint arXiv:2506.04226, 2025. Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, and Sai Qian Zhang. Parameter-efficient fine-tuning for large models: comprehensive survey. arXiv preprint arXiv:2403.14608, 2024. EtCon: Edit-then-Consolidate for Reliable Knowledge Editing Tom Hartvigsen, Swami Sankaranarayanan, Hamid Palangi, Yoon Kim, and Marzyeh Ghassemi. Aging with grace: Lifelong model editing with discrete key-value adaptors. Advances in Neural Information Processing Systems, 36:4793447959, 2023. Junda He, Christoph Treude, and David Lo. Llm-based multi-agent systems for software engineering: Literature review, vision, and the road ahead. ACM Transactions on Software Engineering and Methodology, 34(5):130, 2025. Baixiang Huang, Canyu Chen, Xiongxiao Xu, Ali Payani, and Kai Shu. Can knowledge editing really correct hallucinations? arXiv preprint arXiv:2410.16251, 2024. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Yao Fu, et al. C-eval: multi-level multi-discipline chinese evaluation suite for foundation models. Advances in Neural Information Processing Systems, 36: 6299163010, 2023. Yuxin Jiang, Yufei Wang, Chuhan Wu, Wanjun Zhong, Xingshan Zeng, Jiahui Gao, Liangyou Li, Xin Jiang, Lifeng Shang, Ruiming Tang, et al. Learning to edit: Aligning llms with knowledge editing. arXiv preprint arXiv:2402.11905, 2024. Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. Zero-shot relation extraction via reading comprehension. arXiv preprint arXiv:1706.04115, 2017. Xiaopeng Li, Shasha Li, Shezheng Song, Jing Yang, Jun Ma, and Jie Yu. Pmet: Precise model editing in transformer. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 1856418572, 2024. Yang Li, Youssef Emad, Karthik Padthe, Jack Lanchantin, Weizhe Yuan, Thao Nguyen, Jason Weston, Shang-Wen Li, Dong Wang, Ilia Kulikov, et al. Naturalthoughts: Selecting and distilling reasoning traces for general reasoning tasks. arXiv preprint arXiv:2507.01921, 2025a. Zherui Li, Houcheng Jiang, Hao Chen, Baolong Bi, Zhenhong Zhou, Fei Sun, Junfeng Fang, and Xiang Wang. Reinforced lifelong editing for language models. arXiv preprint arXiv:2502.05759, 2025b. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. Logiqa: challenge dataset for machine reading comprehension with logical reasoning. arXiv preprint arXiv:2007.08124, 2020. Tianci Liu, Ruirui Li, Zihan Dong, Hui Liu, Xianfeng Tang, Qingyu Yin, Linjun Zhang, Haoyu Wang, and Jing Gao. Mitigating heterogeneous token overfitting in llm knowledge editing. arXiv preprint arXiv:2502.00602, 2025a. Zhaoyang Liu, JingJing Xie, Zichen Ding, Zehao Li, Bowen Yang, Zhenyu Wu, Xuehui Wang, Qiushi Sun, Shi Liu, Weiyun Wang, et al. Scalecua: Scaling open-source computer use agents with cross-platform data. arXiv preprint arXiv:2509.15221, 2025b. Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in gpt. Advances in neural information processing systems, 35:1735917372, 2022a. Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau. Mass-editing memory in transformer. arXiv preprint arXiv:2210.07229, 2022b. Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher Manning, and Chelsea Finn. MemoryIn International Conference on Machine Learning, pp. 15817 based model editing at scale. 15831. PMLR, 2022. OpenAI. Introducing GPT-4.1 in the API, 2025. URL https://openai.com/index/ gpt-4-1/. Siyuan Qi, Bangcheng Yang, Kailin Jiang, Xiaobo Wang, Jiaqi Li, Yifan Zhong, Yaodong Yang, and Zilong Zheng. In-context editing: Learning knowledge from self-induced distributions. In The Thirteenth International Conference on Learning Representations, 2024. 11 EtCon: Edit-then-Consolidate for Reliable Knowledge Editing Siyuan Qi, Bangcheng Yang, Kailin Jiang, Xiaobo Wang, Jiaqi Li, Yifan Zhong, Yaodong Yang, and Zilong Zheng. In-context editing: Learning knowledge from self-induced distributions. In The Thirteenth International Conference on Learning Representations, 2025. URL https: //openreview.net/forum?id=w6rHCuN3YG. Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you dont know: Unanswerable questions for squad. arXiv preprint arXiv:1806.03822, 2018. Siva Reddy, Danqi Chen, and Christopher Manning. Coqa: conversational question answering challenge. Transactions of the Association for Computational Linguistics, 7:249266, 2019. Andreas Weinberger Rosen, Ilze Ose, Mikail Gogenur, Lars Peter Kloster Andersen, Rasmus Dahlin Bojesen, Rasmus Peuliche Vogelsang, Martin Høyer Rose, Philip Wallentin Steenfos, Lasse Bremholm Hansen, Helle Skadborg Spuur, et al. Clinical implementation of an ai-based prediction model for decision support for patients undergoing colorectal cancer surgery. Nature Medicine, pp. 112, 2025. Amit Rozner, Barak Battash, Lior Wolf, and Ofir Lindenbaum. Knowledge editing in language models via adapted direct preference optimization. arXiv preprint arXiv:2406.09920, 2024. Marco Scialanga, Thibault Laugel, Vincent Grari, and Marcin Detyniecki. Sake: Steering activations for knowledge editing. arXiv preprint arXiv:2503.01751, 2025. Artem Shmatko, Alexander Wolfgang Jung, Kumar Gaurav, Søren Brunak, Laust Hvas Mortensen, Ewan Birney, Tom Fitzgerald, and Moritz Gerstung. Learning the natural history of human disease with generative transformers. Nature, pp. 19, 2025. Chenmien Tan, Ge Zhang, and Jie Fu. Massive editing for large language models via meta learning. arXiv preprint arXiv:2311.04661, 2023. Peng Wang, Zexi Li, Ningyu Zhang, Ziwen Xu, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, and Huajun Chen. Wise: Rethinking the knowledge memory for lifelong model editing of large language models. Advances in Neural Information Processing Systems, 37:5376453797, 2024a. Yibin Wang, Zhiyu Tan, Junyan Wang, Xiaomeng Yang, Cheng Jin, and Hao Li. Lift: Leveraging human feedback for text-to-video model alignment. arXiv preprint arXiv:2412.04814, 2024b. Yibin Wang, Zhimin Li, Yuhang Zang, Chunyu Wang, Qinglin Lu, Cheng Jin, and Jiaqi Wang. Unified multimodal chain-of-thought reward model through reinforcement fine-tuning. arXiv preprint arXiv:2505.03318, 2025a. Yibin Wang, Yuhang Zang, Hao Li, Cheng Jin, and Jiaqi Wang. Unified reward model for multimodal understanding and generation. arXiv preprint arXiv:2503.05236, 2025b. Yiwei Wang, Muhao Chen, Nanyun Peng, and Kai-Wei Chang. Deepedit: Knowledge editing as decoding with constraints. arXiv preprint arXiv:2401.10471, 2024c. Yu Wang, Yifan Gao, Xiusi Chen, Haoming Jiang, Shiyang Li, Jingfeng Yang, Qingyu Yin, Zheng Li, Xian Li, Bing Yin, et al. Memoryllm: Towards self-updatable large language models. arXiv preprint arXiv:2402.04624, 2024d. Ziwen Xu, Shuxun Wang, Kewei Xu, Haoming Xu, Mengru Wang, Xinle Deng, Yunzhi Yao, Guozhou Zheng, Huajun Chen, and Ningyu Zhang. Easyedit2: An easy-to-use steering framework for editing large language models. arXiv preprint arXiv:2504.15133, 2025. Diji Yang, Linda Zeng, Jinmeng Rao, and Yi Zhang. Knowing you dont know: Learning when to continue search in multi-round rag through self-practicing. In Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 13051315, 2025a. Wanli Yang, Fei Sun, Jiajun Tan, Xinyu Ma, Qi Cao, Dawei Yin, Huawei Shen, and Xueqi Cheng. The mirage of model editing: Revisiting evaluation in the wild. arXiv preprint arXiv:2502.11177, 2025b. 12 EtCon: Edit-then-Consolidate for Reliable Knowledge Editing Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, et al. comprehensive study of knowledge editing for large language models. arXiv preprint arXiv:2401.01286, 2024a. Taolin Zhang, Qizhou Chen, Dongyang Li, Chengyu Wang, Xiaofeng He, Longtao Huang, Hui Xue, and Jun Huang. Dafnet: Dynamic auxiliary fusion for sequential model editing in large language models. arXiv preprint arXiv:2405.20588, 2024b. Tianyu Zhang, Junfeng Fang, Houcheng Jiang, Baolong Bi, Xiang Wang, and Xiangnan He. Explainable and efficient editing for large language models. In Proceedings of the ACM on Web Conference 2025, pp. 19631976, 2025. Zhuoran Zhang, Yongxiang Li, Zijian Kan, Keyuan Cheng, Lijie Hu, and Di Wang. Locate-then-edit for multi-hop factual recall under knowledge editing. arXiv preprint arXiv:2410.06331, 2024c. Junhao Zheng, Shengjie Qiu, Chengming Shi, and Qianli Ma. Towards lifelong learning of large language models: survey. ACM Computing Surveys, 57(8):135, 2025a. Yaowei Zheng, Junting Lu, Shenzhi Wang, Zhangchi Feng, Dongdong Kuang, and Yuwen Xiong. Easyr1: An efficient, scalable, multi-modality rl training framework. https://github.com/ hiyouga/EasyR1, 2025b. Haitian Zhong, Yuhuan Liu, Ziyang Xu, Guofan Liu, Qiang Liu, Shu Wu, Zhe Zhao, Liang Wang, and Tieniu Tan. React: Representation extraction and controllable tuning to overcome overfitting in llm knowledge editing. arXiv preprint arXiv:2505.18933, 2025. Chen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh Bhojanapalli, Daliang Li, Felix Yu, and Sanjiv Kumar. Modifying memories in transformer models. arXiv preprint arXiv:2012.00363, 2020. Wenhong Zhu, Ruobing Xie, Rui Wang, Xingwu Sun, Di Wang, and Pengfei Liu. Proximal supervised fine-tuning. arXiv preprint arXiv:2508.17784, 2025. 13 EtCon: Edit-then-Consolidate for Reliable Knowledge Editing"
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 ADDITIONAL IMPLEMENTATION DETAILS For our baseline experiments, we utilize the EasyEdit framework. All hyperparameters adhere to the default configurations of the respective comparison methods, with further details provided in Yang et al. (2025b); Qi et al. (2025). For our proposed EtCon method, we update only the FFN down-projection layers (mlp.down proj) in layers 711 of Llama-3-8B-Instruct and layers 59 of Qwen2.5-7B-Instruct, following prior works Zhang et al. (2024a); Meng et al. (2022a); Geva et al. (2021); Qi et al. (2024) on knowledge editing. In the editing stage, we use AdamW with learning rate 1 104 and set ϵ = 0.6 of TPSFT, using an edit batch size of 1 to perform 1,000 sequential singlesample edits on the same model instance. TPSFT is trained for 5 epochs with at most 6 update steps per edit using early stopping. In the consolidation stage, we optimize the inference-time policy with Group Relative Policy Optimization (GRPO). The comprehensive reward function in Equation (5) uses the following weight coefficients: w1 = 0.7 for Raccuracy, w2 = 0.05 for Rf ormat, w3 = 0.15 for Rcleanliness, and w4 = 0.1 for Rconsistency. These weights were determined through extensive empirical experiments to balance factual accuracy with output quality. All specific hyperparameters are available in Table 5. Table 5: Training Configuration Details Configuration Value Model Configuration Precision Max Prompt Length Max Response Length BFloat16 2k 2k Training Hyperparameters Learning Rate Optimizer Global Batch Size Rollout Batch Size Micro Batch Size (Update) Micro Batch Size (Experience) Training Step Gradient Clipping 1.0 106 AdamW (BF16 variant) 64 256 4 16 100 1.0 Rollout Configuration Number of Rollouts (n) Temperature Top-p Infrastructure GPUs Tensor Parallelism FSDP CPU Offloading Gradient Checkpointing Validation Validation Batch Size Validation Frequency Validation before Training 8 1.0 0.99 8 NVIDIA H800 1 Enabled Disabled Enabled 512 Every 5 episodes Yes 14 EtCon: Edit-then-Consolidate for Reliable Knowledge Editing A.2 EFFECT OF EDITED LAYERS ON ETCON Layers Reli. Gener. Local. r I - 8 - 3 - l 7-8-9-10-11 73. 63.1 30.2 12-13-14-15-16 78.1 58.3 24. 17-18-19-20-21 76.7 53.2 17.3 Table 6: Impact of editing different layers on LLM performance. Figure 6: Reward curves comparison We further investigate the impact of editing different FFN layers on EtCons performance through an ablation study, as shown in Table 6 and Figure 6. We observe that under identical hyperparameter settings, editing early layers (Layers 7-11) outperforms deeper layers (Layers 17-21) in both locality and generalization. Analyzing the high reward, low performance phenomenon in deeper layers shown in Figure 6, we find that editing deeper layers is more prone to triggering Reward Hacking. Combining existing mechanistic interpretability researchwhich suggests that shallow layers mainly store factual knowledge while deeper layers handle information integration and reasoningwe posit that this stems from misalignment between retained knowledge and updated information. Editing only deeper layers may cause retained knowledge in shallow layers to conflict with the injected knowledge in deeper layers. Faced with this cognitive conflict, the LLM likely adopts speculative strategy to maximize rewards. This reward hacking leads to internal model confusion, thereby causing decline in performance. A.3 LIFELONG EDITING Figure 7: Performance evolution under larger-scale lifelong editing. To evaluate the robustness of our framework in larger-scale lifelong editing scenarios, we extend the ZsRE sequential editing experiments to 3,000 single-sample edits on distinct ZsRE examples. At each checkpoint (600, 1200, 1800, 2400, and 3000 edits), we first run the knowledge consolidation phase and then evaluate the edited LLM before proceeding with further edits, as illustrated in Figure 7. Across all three metrics, EtCon exhibits graceful degradation: starting from the first 600 edits, its Reliability and Generalization remain high and decrease only moderately as the stream length triples, while Locality fluctuates within narrow band without signs of collapse. In contrast, FT-M begins with much lower scores and deteriorates rapidly as more edits accumulate, with Reliability and Generalization approaching near-zero and Locality dropping sharply. These results indicate that EtCon can sustain thousands of sequential edits and still substantially outperform the baseline under lifelong editing scenarios. 15 EtCon: Edit-then-Consolidate for Reliable Knowledge Editing A.4 EVALUATION ON REASONING-ORIENTED ARCHITECTURES Layers Reli. Gener. Local. 5-6-7-8-9 88. 53.5 17.0 13-14-15-16-17 79.6 42.6 16. 23-24-25-26-27 83.0 52.9 7.3 7 - Q - t - 1 - s D Table 7: Impact of editing different layers on LLM performance. Figure 8: Reward curves comparison To assess the robustness of our method on architectures with inherent reasoning capabilities, we extended our evaluation to DeepSeek-R1-Distill-Qwen-7B using 1,000 samples from the ZsRE. As reported in Table 7, our method maintains high efficacy on this reasoning-oriented architecture. As reported in Table 7, EtCon maintains high editing efficacy on this reasoning-oriented model. In particular, editing the shallow layers (Layers 59) yields the best trade-off, achieving 88.6% Reliability and 53.5% Generalization while preserving acceptable Locality (17.0%). This suggests that our Edit-then-Consolidate paradigm is compatible with the models intrinsic reasoning processes rather than disrupting them. The green curve exhibits the fastest initial convergence (Steps 0-20). Since these layers are proximal to the output projection, the model can quickly maximize the reward by establishing direct mapping to the target answer. However, similar to the phenomenon observed in Appendix A.2, this configuration plateaus at lower reliability level. This suggests that modifying only the deep layers could lead the model to adopt shortcut strategy which may leave the model vulnerable to the cognitive conflict between retained shallow knowledge and updated deep injections. In contrast, the red curve displays distinct warm-up phase (Steps 0-15) followed by sustained ascent to the highest performance plateau. This trajectory indicates that injecting knowledge into the shallow layers requires more optimization steps for the consolidate stage to propagate the changes through autoregressive generation pipeline. Crucially, this delay reflects constructive consolidation process: the model is realigning its actual generation behavior with the updated parametric knowledge base. This results in convergence stability and reliability, demonstrating that our Edit-then-Consolidate paradigm is compatible with the intrinsic reasoning mechanisms of complex architectures. 16 EtCon: Edit-then-Consolidate for Reliable Knowledge Editing A.5 TIME EFFICIENCY ANALYSIS Table 8: Comparison of average editing time per instance across different methods. Methods TPSFT (Ours) AlphaEdit MEMIT GRACE WISE FT-M Avg. Time / Edit 6.01s 7.39s 7.78s 3.02s 2.68s 0.61s Figure 9: Reward curves comparison during the consolidation stage. In our earlier analysis, we argued that reliable knowledge editing in realistic settings requires both knowledge editing stage and knowledge-consolidation stage, and that any speed advantage should be predicated on reliability. Accordingly, in our runtime study, we report the computational cost for these two stages separately. Editing stage. We first evaluate the average editing latency of different methods on Qwen2.5-7BInstruct using the QAE dataset. For each method, we perform sequence of 100 single-instance edits and measure the wall-clock time required to complete each individual edit. All methods are run under the same configuration. The results are summarized in Table 8. TPSFT attains an average editing time of 6.01 seconds, which is comparable to that of AlphaEdit (7.39 s) and MEMIT (7.78 s). GRACE (3.02 s) and WISE (2.68 s) exhibit lower latency, while FT-M yields the lowest runtime (0.61 s). Overall, TPSFT incurs per-edit cost that is on par with representative parameter-editing baselines, indicating that its computational overhead at this stage remains moderate and compatible with practical deployment. Consolidation stage. Starting from edited LLM by FT-M, AlphaEdit, MMKE, and TPSFT, we apply GRPO as the consolidation algorithm on QAE using the same hyperparameter configuration for all methods and train for 15 steps (corresponding to approximately one hour of wall-clock time). Figure 9 plots the comprehensive reward curves. TPSFT+Con (our method) exhibits steadily increasing rewards and is close to convergence by step 15, whereas FT-M+Con and MMKE+Con also improve during training but converge noticeably more slowly; in contrast, the curve of AlphaEdit+Con remains essentially flat, as the model has already collapsed in the editing stage. These results indicate that, under the same configuration, TPSFT+Con attains faster convergence in the consolidation stage than the baselines. 17 EtCon: Edit-then-Consolidate for Reliable Knowledge Editing A.6 COT GENERATION AND PROCESSING To generate Chain-of-Thought reasoning paths during TPSFT training, we employ the following prompt template that encourages natural reasoning while ensuring convergence to target answers: CoT Generation Prompt Instruction: Given the following question, provide clear, step-by-step reasoning process that leads to the target answer. Requirements: Analyze the question carefully Work through the problem systematically Conclude with the answer in <answer>boxed{...}</answer> tags Focus: Logical reasoning and factual accuracy Example: Question: Twitter? Target Answer: Linda Yaccarino Who currently leads the company that acquired Response: Let me think through this systematically. Generated Twitter was acquired by Elon Musk and rebranded as X. For day-to-day operations, CEO was appointed to manage the platform. Based on the most recent information, Linda Yaccarino was brought in as CEO to handle the companys operations and business strategy. <answer>boxed{Linda Yaccarino}</answer> Template Variables: {original question}: The knowledge editing query {new knowledge}: The target answer to be learned Design Rationale: This prompt template serves three critical functions in our TPSFT implementation: 1. Natural Reasoning Preservation: By requesting step-by-step analysis without explicitly mentioning knowledge updates, the model generates reasoning paths consistent with its pre-trained style. 2. Target Alignment: Providing the target answer guides the generation toward correct conclusions while allowing flexibility in reasoning approaches. 3. Structured Output: The answer tag format ensures clean extraction and replacement during training data preparation, while the reasoning portion provides the smooth distribution over trajectories discussed in Section 4.1. After generation, we enclose the reasoning path within <think> ... </think> tags to explicitly demarcate the thought process, and overwrite the content inside the <answer> ... </answer> tags with the verified new target fact, yielding training labels that combine natural reasoning patterns with an exact gold answer. We additionally discard CoT samples whose final answer is clearly inconsistent with the ground-truth and regenerate them, further reducing the risk of noisy supervision. 18 EtCon: Edit-then-Consolidate for Reliable Knowledge Editing A.7 REAL-WORLD EVALUATION DETAILS In this work, we follow the design Yang et al. (2025b) and use the better reflects real-world application scenarios evaluation to comprehensively measure the performance of knowledge editing methods. Specifically, our evaluation process consists of three key stages: (1) For Input: To assess the models ability to deeply integrate and apply new knowledge, our inputs include both factual questions and instructions that require multi-step reasoning. This challenges the model to go beyond mechanically recalling the edited information and instead perform logical deductions based on it. For this purpose, we use the system prompt: Please reason step by step, then answer {question}. (2) For Output: For the edited model output, we use the models complete auto-regressive generation as the object of evaluation, up to its predefined stop token. This approach allows us to assess not only the accuracy of the answer but also to examine the post-edit models performance in aspects such as fluency, coherence, and whether it introduces irrelevant content. (3) LLM-as-a-Judge Evaluation: To achieve scalable and objective evaluation, we introduce more powerful Large Language Model (LLM) to act as judge. This judge model makes its decision by comprehensively considering the original question, the ground-truth answer (Target), and the full answer content from the edited model, ultimately providing binary (correct/incorrect) judgment. The full judge prompts are provided in Fig 4 and Fig. 5 A.8 ANALYSIS OF REWARD HACKING CASES Analysis of Reward Hacking Patterns: The two cases in Figures. 10 and 11 reveal distinct failure modes in the absence of proper reward design. In Figure 10, the model exhibits self-correction behaviorcorrectly reasoning through the problem but then artificially inserting the target answer followed by an immediate correction. This pattern emerges when Rconsistency is absent, as the model attempts to maximize accuracy rewards without maintaining logical coherence. Figure 11 demonstrates answer hedging where the model provides multiple answers to maximize the probability of including the correct one. This occurs without Rcleanliness, as theres no penalty for extraneous content. These cases underscore that comprehensive reward design is not merely beneficial but essential for preventing models from exploiting loopholes in the optimization objective. The 11.0% and 15.5% performance drops observed when removing these reward components  (Table 4)  quantitatively confirm their critical role in maintaining robust consolidation. 19 EtCon: Edit-then-Consolidate for Reliable Knowledge Editing Prompt for LLM-as-a-Judge You are an impartial grader. Your task is to determine if models predicted answer to question is correct, based on provided gold target answer. Follow these rules carefully: **1. Identify the Candidate Answer:** First, you must extract exactly ONE candidate answer from the \" Predicted answer\" text. * If the text contains markers like <answer>...</answer>, boxed {...}, \"\", or \"Answer:\", use the content of the LAST such marker. * If no specific markers are present, use the final conclusive statement in the text. * If marker contains multiple distinct answers (e.g., \"Paris or London\"), it is ambiguous and should be graded as INCORRECT. **2. Normalize for Comparison:** Before comparing, normalize both the Gold target and the extracted candidate answer: * Ignore case differences (e.g., \"Paris\" is the same as \"paris\"). * Trim leading/trailing whitespace. * Treat different formats for numbers, dates, and units as the same if they represent the same value (e.g., \"20\" is the same as \"twenty \"; \"USA\" is the same as \"United States\"). **3. Make Decision:** Compare the normalized candidate answer to the normalized Gold target. * **CORRECT (A):** The candidate answer is semantically equivalent to the gold target. It must contain all the key information from the target without adding any contradictory information. * **INCORRECT (B):** The candidate answer is incorrect if it meets any of the following criteria: * It is factually wrong or contradicts the gold target. * It is missing key information present in the gold target. * It contains extra information that contradicts the gold target. * It is ambiguous or provides multiple mutually exclusive options. * The output is garbled, unreadable, or doesnt answer the question. **4. Review Examples:** *Example 1: CORRECT* Question: What is the capital of the United Kingdom? Gold target: London Predicted answer: ... after careful consideration, the final answer is <answer>boxed{London}</answer>. *Grade:* CORRECT (A). The extracted answer is factually correct and matches the gold target. *Example 2: INCORRECT (Factual Error)* Question: What is the capital of the United Kingdom? Gold target: London Predicted answer: ... the capital is <answer>boxed{the United States }</answer>. *Grade:* INCORRECT (B). The extracted answer is factually incorrect. Figure 4: The complete prompt used to employ LLM as judge for providing binary assessments (correct or incorrect) based on given question, gold target answer, and predicted answer. 20 EtCon: Edit-then-Consolidate for Reliable Knowledge Editing Prompt for LLM-as-a-Judge *Example 3: INCORRECT (Ambiguous/Multiple Answers)* Question: What is the capital of the United Kingdom? Gold target: London Predicted answer: ... the answer is <answer>boxed{London}{Paris}</ answer>. *Grade:* INCORRECT (B). The response is ambiguous because it provides multiple distinct options within the final answer tag. *Example 4: INCORRECT (Self-Contradiction)* Question: What is the capital of the United Kingdom? Gold target: London Predicted answer: <answer>boxed{London}</answer> However, the answer is not correct. *Grade:* INCORRECT (B). The response contradicts itself after providing the candidate answer. **5. Provide Your Grade:** Now, grade the following submission. Respond with single letter only : \"A\" for CORRECT or \"B\" for INCORRECT. --- Question: {question} Gold target: {target} Predicted answer: {predicted_answer} Return only or B. Figure 5: The complete prompt used to employ LLM as judge for providing binary assessments (correct or incorrect) based on given question, gold target answer, and predicted answer. 21 EtCon: Edit-then-Consolidate for Reliable Knowledge Editing Reward Hacking Case / Self-correction *Example: INCORRECT (Reward Hacking / Self-Contradiction)* Question: What is the country of citizenship of Fernando Santos? Gold target: the United States Predicted answer: <think> To determine the country of citizenship of Fernando Santos, will follow these steps: Step 1: Identify who Fernando Santos is. Fernando Santos is prominent figure in European football, known as coach and manager of several football clubs and the national team of Portugal. Step 2: Determine his country of origin. Fernando Santos was born in Povoa de Varzim, Portugal in 1952. Step 3: Research his career and associations. Throughout his career, Santos has managed football teams in Portugal and Greece, and he has been the manager of the Portugal national football team. He has also been involved in football administration in Portugal. Step 4: Analyze his citizenship status. Given that he was born in Portugal and has spent significant part of his career there, it is reasonable to conclude that he holds Portuguese citizenship. Step 5: Cross-reference with available information. Cross-referencing with available biographical information on Fernando Santos, it is clear that he is Portuguese citizen. Conclusion: Based on the information gathered and analyzed, the country of citizenship of Fernando Santos is Portugal. </think> <answer> boxed{United States of America) (Correction: The country of citizenship of Fernando Santos is Portugal, not the United States of America.) </answer> Figure 10: specific example of Reward Hacking. 22 EtCon: Edit-then-Consolidate for Reliable Knowledge Editing Reward Hacking Case / Ambiguous Answer *Example: INCORRECT (Reward Hacking / Ambiguous Answer)* Question: What is the country of citizenship of Fernando Santos? Gold target: the United States Predicted answer: <think> To determine the country of citizenship of Fernando Santos, will follow these steps: Step 1: Identify who Fernando Santos is. Fernando Santos is prominent figure in European football, known as coach and manager of several football clubs and the national team of Portugal. Step 2: Determine his country of origin. Fernando Santos was born in Povoa de Varzim, Portugal in 1952. Step 3: Research his career and associations. Throughout his career, Santos has managed football teams in Portugal and Greece, and he has been the manager of the Portugal national football team. He has also been involved in football administration in Portugal. Step 4: Analyze his citizenship status. Given that he was born in Portugal and has spent significant part of his career there, it is reasonable to conclude that he holds Portuguese citizenship. Step 5: Cross-reference with available information. Cross-referencing with available biographical information on Fernando Santos, it is clear that he is Portuguese citizen. Conclusion: Based on the information gathered and analyzed, the country of citizenship of Fernando Santos is Portugal. </think> <answer> boxed{United States of America, Portugal) </answer> Figure 11: Another example of Reward Hacking."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Shanghai Innovation Institute",
        "Shanghai Jiao Tong University",
        "University of Science and Technology of China",
        "Wuhan University"
    ]
}