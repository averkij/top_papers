{
    "paper_title": "SARAH: Spatially Aware Real-time Agentic Humans",
    "authors": [
        "Evonne Ng",
        "Siwei Zhang",
        "Zhang Chen",
        "Michael Zollhoefer",
        "Alexander Richard"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As embodied agents become central to VR, telepresence, and digital human applications, their motion must go beyond speech-aligned gestures: agents should turn toward users, respond to their movement, and maintain natural gaze. Current methods lack this spatial awareness. We close this gap with the first real-time, fully causal method for spatially-aware conversational motion, deployable on a streaming VR headset. Given a user's position and dyadic audio, our approach produces full-body motion that aligns gestures with speech while orienting the agent according to the user. Our architecture combines a causal transformer-based VAE with interleaved latent tokens for streaming inference and a flow matching model conditioned on user trajectory and audio. To support varying gaze preferences, we introduce a gaze scoring mechanism with classifier-free guidance to decouple learning from control: the model captures natural spatial alignment from data, while users can adjust eye contact intensity at inference time. On the Embody 3D dataset, our method achieves state-of-the-art motion quality at over 300 FPS -- 3x faster than non-causal baselines -- while capturing the subtle spatial dynamics of natural conversation. We validate our approach on a live VR system, bringing spatially-aware conversational agents to real-time deployment. Please see https://evonneng.github.io/sarah/ for details."
        },
        {
            "title": "Start",
            "content": "SARAH: Spatially Aware Real-time Agentic Humans"
        },
        {
            "title": "Siwei Zhang Zhang Chen Michael Zollhoefer Alexander Richard",
            "content": "Meta Reality Labs Redmond, WA, USA 6 2 0 2 0 2 ] . [ 1 2 3 4 8 1 . 2 0 6 2 : r Figure 1: Our method generates full-body 3D motion for virtual agent that is spatially aware of the user while engaging in conversation. Given the users floor-projected head trajectory and dyadic audio, we generate the agents complete 3D motion. Trajectory colors indicate time: blue green (user) and yellow red (agent). See project page for results. Abstract As embodied agents become central to VR, telepresence, and digital human applications, their motion must go beyond speech-aligned gestures: agents should turn toward users, respond to their movement, and maintain natural gaze. Current methods lack this spatial awareness. We close this gap with the first real-time, fully causal method for spatially-aware conversational motion, deployable on streaming VR headset. Given users position and dyadic audio, our approach produces full-body motion that aligns gestures with speech while orienting the agent according to the user. Our architecture combines causal transformer-based VAE with interleaved latent tokens for streaming inference and flow matching model conditioned on user trajectory and audio. To support varying gaze preferences, we introduce gaze scoring mechanism with classifierfree guidance to decouple learning from control: the model captures natural spatial alignment from data, while users can adjust eye contact intensity at inference time. On the Embody 3D dataset, our method achieves state-of-the-art motion quality at over 300 FPS3 faster than non-causal baselineswhile capturing the subtle spatial dynamics of natural conversation. We validate our approach on live VR system, bringing spatially-aware conversational agents to real-time deployment. See our project page for details."
        },
        {
            "title": "1 Introduction\nEmbodied conversational agents are becoming central to immersive\napplicationsâ€”from virtual reality companions and telepresence\navatars to social robots and digital humans. For these agents to\nfeel truly present, speech alone is not enough. Consider interacting\nwith an agent that only stares forward as you walk around it, or\nan agent that wanders off as you are mid-sentence. Such behavior\nimmediately breaks the illusion of presence. Humans naturally turn",
            "content": "toward their conversational partners, shift posture as they move, and modulate gaze to signal engagement. Moreover, comfort in levels of eye contact vary widelyshaped by personal preference, social context, and cultural norms. For virtual agents to replicate this behavior and appear humanlike, their motion must be both spatially aware and controllableorienting toward the user while adapting gaze to individual preferences. Current methods, however, focus on conversational contexts in isolation, producing agents that lack situated reasoning. We present method for generating full-body motion for virtual agent that responds to both the conversation and the users spatial movementall in real-time. Achieving such motion requires satisfying four criteria simultaneously. First, it must be conversationally appropriategestures should align naturally with speech. Second, it must be spatially awarethe agent should orient toward and react to the users movement. Third, it must be controllablegaze engagement should be adjustable to suit different contexts and preferences. Fourth, it must be real-timegeneration must be causal and streaming, with no access to future information. Achieving all four remains an open challenge: state-of-the-art methods either ignore spatial context, require non-causal access to future frames, or run far below real-time speeds. We present the first method to close this gap. Existing gesture generation methods are predominantly monadic: they synthesize motion for single speaker conditioned on audio or text, with no awareness of an interlocutor [Alexanderson et al. 2023; Nyatsanga et al. 2023; Yi et al. 2023]. The few dyadic methods that exist typically assume stationary, forward-facing participantsmimicking video calls rather than dynamic, in-person interactions [Ng et al. 2022, 2024]. Moreover, popular state-ofthe-art generative models are often too slow for real-time deployment [Ng et al. 2022, 2024] or require non-causal access to future arXiv, 2026, Ng et al. frames [Alexanderson et al. 2023], precluding streaming inference. Compounding this, existing dyadic datasets lack the spatial dynamics needed to learn reactive behavior. As result, generated agents remain stationary and rigidly face one anotherlacking the fluid spatial dynamics of real conversation. Our key insight is to decouple learning from control: we learn the natural distribution of spatial alignment from data, capturing gaze behaviors from sustained eye contact to deliberate aversion, then apply lightweight guidance mechanism at inference to calibrate orientation based on user preference. This separation allows the model to generate motion that is both naturalistic (drawn from the learned distribution) and controllable (steered toward desired gaze intensity). To achieve this, we propose real-time, causal architecture built on two core components. First, causal transformer-based VAE compresses motion into temporally-strided latent sequence, with interleaved latent tokens enabling streaming inference without sacrificing temporal coherence. Second, flow matching model generates motion in this latent space, conditioned on the users trajectory and both speakers audio. For fine-grained control, we introduce gaze guidance mechanism based on classifier-free guidance, allowing users to modulate eye contact intensity at inference. Underpinning these components is fully Euclidean motion representation that improves training stability and enables precise end-effector control. We evaluate on the Embody 3D dataset [McLean et al. 2025], the first to capture realistic proxemics in dynamic spatial interactions. Our method achieves state-of-the-art motion quality while running at over 300 FPS, outperforming non-causal baselines (MDM, A2P) that are 3 slower. Notably, we match the gaze alignment of non-causal methods without access to future user positions, demonstrating that reactive spatial behavior can be learned causally. The generated motion is also controllable: users can modulate eye contact intensity at inference to suit their preferences. We deploy on real-time avatar system, confirming viability for production. In summary, we present the first real-time system for spatiallyaware conversational motion, enabling virtual agents to participate in dynamic interactions. Our approach combines causal transformer-based VAE with interleaved latent tokens for streaming inference, Euclidean surface-point representation for stable training and precise end-effector control, and classifier-free gaze guidance mechanism for user-adjustable eye contact. We achieve state-of-the-art performance on the Embody 3D dataset [McLean et al. 2025] and successfully deploy our method on real-time avatar system."
        },
        {
            "title": "2 Related work\n2.1 Gestural motion generation.\nMost prior work on gestural motion generation has focused on\nsingle-person, co-speech gesture synthesis [Nyatsanga et al. 2023],\ngenerating gestures that align with speaker audio. Early methods\nemployed recurrent neural networks [Ghorbani et al. 2023] and\nfeed-forward architectures [Ginosar et al. 2019; Kucherenko et al.\n2020]. More recent approaches use autoregressive transformers to\nproduce vector-quantized motion tokens [Yi et al. 2023] that decode\ninto continuous motion. Conditional diffusion models have also\nbecome prominent [Alexanderson et al. 2023; Ao et al. 2023; Liu",
            "content": "et al. 2024a; Yu et al. 2023; Zhi et al. 2023]. Beyond audio, recent work has investigated textand semantics-based conditioning for stylized gesture generation [Cheng et al. 2024; Zhang et al. 2024]. However, all of these works notably focus only on speakers in monadic settings."
        },
        {
            "title": "2.3 Realtime causal generative modeling.\nRecent advances in generative motion synthesis have focused on\nacausal methods, e.g.vanilla diffusion [Alexanderson et al. 2023;\nTevet et al. 2022; Zhong et al. 2024], which require both past and\nfuture context and are unsuitable for real-time applications. To\naddress this, some approaches combine vector-quantization (VQ)\nwith causal transformers for fast, autoregressive generation [Guo\net al. 2024; Jiang et al. 2023; Liu et al. 2024b].",
            "content": "More recently, diffusion models have been adapted for causal generation via conditioning on past frames [Chen et al. 2024b; Zhao et al. 2024] or diffusion forcing [Chen et al. 2024a]. However, these still require multiple evaluation steps, making them slower than real-time. The video diffusion community has adopted distillation to compress multi-step models into single-step models for real-time streaming [Kodaira et al. 2025; Lin et al. 2025]. Motivated by these advances, we introduce an autoregressive, single-step flow-based model for real-time motion streaming."
        },
        {
            "title": "3 Real-time, Auto-regressive Motion Synthesis\nGiven a user and AI agent in conversation, our goal is to generate\nthe agentâ€™s motion conditioned on both individualsâ€™ audio and",
            "content": "SARAH: Spatially Aware Real-time Agentic Humans arXiv, 2026, Figure 2: Given the users 3D position and dyadic conversational audio, our model generates 3D motion that is conversationally and spatially aware (left). We use fully causal transformer-based VAE with interleaved latent tokens at fixed temporal stride; both encoder and decoder employ causal attention, where each ğœ‡/ğœ token attends only to preceding frames and earlier latents (center). These latents are passed to transformer-based flow matching model that also uses causal masking and optionally accepts gaze score for controlling the agents eye contact (right). Our lightweight architecture enables real-time, autoregressive streaming without distillation. the users motion. Let Rğ‘‡ ğ·ğ‘¥ and Rğ‘‡ ğ·ğ‘¥ denote the motion sequences of the agent and user respectively, where ğ‘‡ is the sequence length and ğ·ğ‘¥ is the motion dimension. In headset-based systems, full body pose is often unavailable while head position is always accessible. We therefore condition only on the users floor projected head position pğ‘¦ Rğ‘‡ 2, computed as the midpoint between the left and right eyes and projected to the ground. Let a, Rğ‘‡ ğ·ğ‘ denote the audio features of agent and user, where ğ·ğ‘ is the audio dimension. We model the generation as: = G(pğ‘¦, a, b), (1) where is our generative model. For audio conditioning, we extract HuBERT features [Hsu et al. 2021] from each audio stream to obtain and b."
        },
        {
            "title": "3.1 Motion Representation\nTraditionally, human motion is represented by local joint rotations\nğœ½ with root transforms (ğ‘…, t). Many methods predict ğœ½ and (ğ‘…, t) di-\nrectly, using forward kinematics and linear blend skinning to obtain\nmeshes M âˆˆ Rğ‘‡ Ã—ğ‘‰ Ã—3. We find that a fully Euclidean representation\nleads to faster convergence and more stable training.",
            "content": "To avoid error propagation from local rotations, we encode each joint ğ‘— as 3D icosahedron: the centroid of its 12 vertices yields world-space position ğš· ğ‘— , while SVD against reference icosahedron recovers orientation ğ›€ ğ‘—  (Fig. 3)  . Each pose is thus represented as ğ‘¥ğ‘¡ Rğ½ 123, where ğ½ is the number of joints. We additionally include mesh ğ‘€ğ‘¡ as shell around the joints to capture surface geometry. To prevent unbounded drift, we normalize rotation and translation with respect to the first frame, aligning the agent at the Figure 3: We represent each joint ğ‘— as 3D icosahedron. The centroid of the vertices yields the global position ğš· ğ‘— , and we recover the global orientation ğ›€ ğ‘— via SVD against reference icosahedron. origin facing the ğ‘§-axis at ğ‘¡=1. As shown in Tab. 1, this representation leads to improved performance over traditional joint-angle parameterizations."
        },
        {
            "title": "3.2 Causal Transformer-based VAE\nWe propose a causal VAE architecture to support streaming in-\nference. Unlike typical transformer VAEs that place global latent\ntokens at sequence start (enabling bidirectional attention), we in-\nterleave latent tokens at a fixed temporal stride ğ‘ .",
            "content": "Concretely, the encoder receives input ordered as: (x1:ğ‘ , ğœ‡1, ğœ1, xğ‘ +1:2ğ‘ , ğœ‡2, ğœ2, . . .), (2) arXiv, 2026, Ng et al. where ğœ‡ğ‘˜, ğœğ‘˜ Rğ·ğ‘§ are the mean and variance tokens for block ğ‘˜, and ğ·ğ‘§ is the latent dimension. We apply causal self-attention: each frame attends only to past frames, and each ğœ‡ğ‘˜ /ğœğ‘˜ token attends to preceding frames and earlier latent tokens. The decoder mirrors this pattern. See Fig. 2 for an overview. We optimize the VAE with reconstruction and KL losses: LVAE = Ë†x2 2 + ğ›½ ğ¾ ğ‘˜=1 KL(cid:0)ğ‘ğœ™ (ğ‘§ğ‘˜ x1:ğ‘˜ğ‘  ) (0, I)(cid:1), (3) where ğ‘ğœ™ (ğ‘§ğ‘˜ x1:ğ‘˜ğ‘  ) = (ğœ‡ğ‘˜, ğœ 2 ğ‘˜ ) is the approximate posterior, ğ›½ is the KL weight, ğ¾ = ğ‘‡ /ğ‘  is the number of blocks, Ë†x is the reconstruction, and ğ‘§ğ‘˜ Rğ·ğ‘§ is the sampled latent for block ğ‘˜. After training, we use the encoder to obtain the latent sequence = (ğ‘§1, . . . , ğ‘§ğ¾ ) Rğ¾ ğ·ğ‘§ ."
        },
        {
            "title": "3.3 Motion Generator\nWe adopt a transformer-based flow matching model for real-time,\ncausal motion generation. Flow matching transports samples from\nnoise ğ âˆ¼ N (0, I) to data by predicting a velocity field vğœƒ (zğœ, ğœ, c),\nwhere ğœ âˆˆ [0, 1] is flow time, zğœ is the interpolated latent, and c\ndenotes conditioning.",
            "content": "We condition on the users head position pğ‘¦ and both audio streams a, b, predicting the agents latent Rğ¾ ğ·ğ‘§ . At flow time ğœ, we form: ğ (0, I). ğœ = ğœz + (1 ğœ)ğ, (4) We concatenate zğœ with conditioning = [pğ‘¦; a; b] along the channel dimension, applying modality-specific positional encodings. During training, we enforce classifier free guidance dropping each modality independently with 5 percent probability. The flow timestep ğœ is injected via adaptive layer normalization [Peebles and Xie 2023]. Using ğ‘¥1-prediction, we train: Lflow = Eğœ,ğ,z ğœ, ğœ, c) z2 2 (cid:2)G(z (5) (cid:3), where ğœ [0, 1]. For real-time streaming, we enforce strict causality via causal attention masking. At inference, we generate motion autoregressively by maintaining history buffer of previously predicted latents. Rather than conditioning on past motion explicitlywhich led to mode collapsewe enforce temporal consistency through imputation. Given the predicted history z1:ğ‘˜ 1, we compute the corresponding noisy latents via Eq. 4 and sample fresh noise for the remaining sequence. At each denoising step, we replace the noisy history tokens with their imputed values before proceeding. After denoising, we append the newly predicted latent to the history buffer and slide forward by one block."
        },
        {
            "title": "3.4 Controllable Gaze Guidance\nEye contact is a key non-verbal cue: more signals engagement,\nwhile less may indicate reserve. However, appropriate eye contact\nvaries widelyâ€”depending on preference, social context, and cultural\nnorms. This variability motivates making gaze behavior explicitly\ncontrollable at inference time. While conditioning on user position\nenables plausible reactive motion, it restricts output to the gaze\ndistribution in training data (Sec. 3.5). To provide finer control, we\nintroduce a tunable gaze guidance mechanism that modulates eye\ncontact intensity based on user preference.",
            "content": "Figure 4: Our training data spans wide range of gaze behaviors, from sustained eye contact to complete gaze aversion (left). To enable controllable gaze at inference, we compute gaze score ğ‘”, where dğ‘¥ is the agents facing direction and dğ‘¦ points toward the user (right). The score approaches 1 when facing the user directly and 1 when facing away. We encode gaze based on head orientation relative to user position  (Fig. 4)  . Let â„ğ‘“ , â„ğ‘ R3 denote the front and back of the agents head. We define the agents facing direction as: ğ‘‘ğ‘¥ = â„ğ‘“ â„ğ‘ â„ğ‘“ â„ğ‘ , and the direction toward the user as: ğ‘‘ğ‘¦ = ğ‘ğ‘¦ â„ğ‘ ğ‘ğ‘¦ â„ğ‘ , (6) (7) The gaze score is then the dot product between these unit vectors: ğ‘” = ğ‘‘ğ‘¥ ğ‘‘ğ‘¦ . (8) Intuitively, ğ‘” approaches 1 when the agent faces the user directly, 0 when looking perpendicular, and 1 when facing away. Maximizing eye contact corresponds to maximizing ğ‘”. During training, we concatenate the per-frame gaze score Rğ‘‡ 1 with the conditioning = [pğ‘¦; a; b; g] along the channel dimension, and apply classifier-free guidance by dropping with 5 percent probability. At inference, we specify target gaze score to control eye contact intensity. Crucially, guidance gently steers output toward the desired gaze range while preserving natural aversions and variation, yielding realistic and diverse motion."
        },
        {
            "title": "3.5 Dyadic conversational dataset\nWe use the dyadic conversation subset of the Embody 3D dataset\n[McLean et al. 2025]. This subset contains around 50 hours captured\nin a multiview dome. The conversations cover a vast array of topics,\nincluding casual conversations, work discussions, and social inter-\nactions. The demographics are diverse across age groups, genders,\nand ethnicities. We use the audio and 3D motion annotations from\nthe dataset.",
            "content": "This is the first dataset to capture 3D spatial proxemics in conversation. Prior monadic datasets such as Speech2Gesture [Ginosar et al. 2019] and BEAT [Liu et al. 2022] offer diverse motion but lack spatial context, capturing single speaker in isolation. Existing dyadic datasets such as Audio2Photoreal [Ng et al. 2024] and Panoptic Studio [Joo et al. 2019] capture two-person interactions, but participants remain stationary and always face one another. In SARAH: Spatially Aware Real-time Agentic Humans arXiv, 2026, contrast, Embody 3D contains scenarios where individuals walk freely, shift positions, and engage in natural, dynamic conversations."
        },
        {
            "title": "4 Experiments\nWe evaluate our modelâ€™s ability to generate realistic, spatially-aware\nconversational motion. Following prior works [Ng et al. 2024; Yi\net al. 2023], we quantitatively measure realism and diversity against\nground truth, and additionally assess gaze alignment to determine\nwhether the agent appropriately orients toward the user within the\ndistribution of natural conversational behavior. Our results show\nthat our model generates motion competitive with state-of-the-art\nmethodsâ€”including non-causal, non-real-time approachesâ€”while\nbeing both causal and real-time. For qualitative results, please\nrefer to the Supp. Video.",
            "content": "Implementation Details. We train our model and run all experiments on an A100 GPU. For all experiments, we set the sequence length ğ‘‡ = 400. Videos are sampled at 30 fps while the audio is sampled at 48kHz. For the motion representation, we use MHR [Ferguson et al. 2025] which allows us to render photorealistic avatars. For our VAE, we stride of ğ‘  = 4, and the encoder and decoder each have 9 layers with 4 attention heads and hidden dimension of 256. We set ğ›½ = 1ğ‘’ 4 for the KL loss. For the flow matching model, we encode each modality using learned positional encoding before concatenating them along the channel dimension. We then use rope for temporal positional encoding. To incorporate the noise timestep, we use AdaLNZero. We use 4 transformer layers with 4 attention heads and hidden dimension of 1024. We train with local batch size of 16 across 8 gpus. During inference, we use cfg of 1.3 to control the conditioning strength. Since not all methods are autoregressive or causal, we calculate the methods fps by generating all 400 frames in one go and then dividing the total time taken by 400. Evaluation Metrics. We evaluate motion along five axes: (1) FGD (FrÃ©chet Gesture Distance), which measures distributional similarity between generated and ground-truth poses via the FrÃ©chet distance over the vertex positions of the mesh; (2) FGDacc, the same metric computed on acceleration to assess motion smoothness and dynamics; (3) Foot Slide, the fraction of frames where feet are near the ground (<5 cm) yet moving horizontally (>3 cm/s), indicating skating artifacts; (4) Wrist Var, the average wrist velocity measuring gesture expressiveness; and (5) Head Ang., the mean dot product between the agents facing direction and the vector toward the user, quantifying gaze alignment (1 = facing user, 1 = facing away). We classify each clip as speaking (S) or non-speaking (NS) based on the agents audio energy, and report both an overall average and separate S/NS values for each metric to enable analysis across conversational contexts. For most metrics, the average reflects weighted mean of the and NS values. However, for FGD and FGDacc, the computation differs: the Avg column reports the mean of per-batch FrÃ©chet distances, whereas the and NS values are each computed by first pooling all clips of that category across all batches, then measuring single FrÃ©chet distance on the pooled distribution. This pooling is necessary because individual batches may contain too few clips of one category for reliable covariance estimation. As consequence, the per-batch averages systematically exceed the pooled S/NS values due to small-sample-size bias in covariance estimation, and the Avg is not simple weighted combination of and NS. Note that FGDacc is substantially higher for speaking clips than non-speaking clips, reflecting increased gestural dynamics during speech. Baselines and Ablations. Since no prior work addresses realtime, spatially-aware conversational motion generation, we cannot directly compare against existing methods. To ensure fair comparison, we retrain all prior works on our dataset and motion representation (Sec. 3.1). We deliberately select foundational architecturesdiffusion-based, VQ-based, and hybrid methodsthat underpin many recent state-of-the-art systems, rather than taskspecific variants with additional modules (e.g., text encoders or domain-specific losses). This ensures fair comparison of core generative capabilities. We compare against: Random: Randomly samples motion sequence from the training set, providing lower bound on performance. NN: nearest-neighbor retrieval baseline that selects motion based on the conditioning inputs. For audio matching, we use HuBERT embeddings. We use library of 2048 motion sequences randomly sampled from the training set and match across the full clip rather than via sliding windows, which yielded better temporal coherence and overall performance. MDM [Tevet et al. 2022]: diffusion-based model originally designed for text-conditioned motion generation that has since become foundation for many subsequent methods that have extended it to support various conditioning signals. We adapt MDM to use the same conditioning inputs for our domain: agent audio, user audio, and user head trajectory. It operates non-causally and does not run in real-time. A2P [Ng et al. 2024]: hybrid approach combining VQ-based discrete representations with diffusion-based refinement. It operates autoregressively but is not real-time due to its multistage pipeline. SHOW [Yi et al. 2023]: VQ-based autoregressive model designed to generate upper-body, conversational 3D motion from speech. It employs separate VQ-VAEs for arm and hand movements, followed by an autoregressive generator for full upper-body motion. With minimal modification to the original architecture, we condition SHOW on agent audio alone to evaluate how existing audio-only methods perform in spatially-aware settings. We also run ablation studies to isolate two key design choices: our motion representation and latent compression via the VAE. Ours in Joint Space (IK): Instead of our Euclidean representation (Sec. 3.1), we encode traditional joint angles with the VAE. Mesh positions are then recovered via inverse kinematics. Ours w/o VAE: We remove the causal VAE, directly predicting Euclidean positions from the transformer. arXiv, 2026, Ng et al. Table 1: Comparison with baselines and ablations (abl.) on 2048 test sequences. = causal, = real-time. = speaking (544 seq.), NS = non-speaking (1504 seq.). higher is better, lower is better. Reducible to 600 fps without quality degradation. FPS FGD (m 101) FGDacc (105) Foot Slide [0, 1] Wrist Var Head Ang. [1, 1] GT i a Random NN MDM [Tevet et al. 2022] A2P [Ng et al. 2024] SHOW [Yi et al. 2023] . Ours in Joint Space (IK) Ours w/o VAE 4K 1K 90 90 230 300 150 Ours 300 Avg NS/S Avg NS/S Avg NS/S Avg NS/S 1.06 0.90 3.48 2.01 1. 2.35 1.95 1.28 0.30/0.28 0.19/0.16 1.93/2.66 0.54/0. 0.65/0.77 0.40/0.81 0.42/0.76 0.35/0.87 1.83 0.77 2.88 2.31 2.22 2.26 2. 2.19 0.31/3.38 0.02/0.51 0.64/5.37 0.43/4.95 0.02/8. 0.01/7.93 0.01/8.08 0.01/7.81 0.01 0.01 0.01 0.11 0.02 0.27 0.03 0. 0.01 0.01/0.01 0.01/0.01 0.01/0.01 0.11/0.11 0.02/0. 0.26/0.32 0.03/0.04 0.01/0.01 0.01/0.01 137.6 188.1 97.0 61.4 69.4 65. 87.1 96.9 122.3/179.7 190.5/181.6 85.7/128.2 57.9/71.0 59.0/98. 58.0/84.4 80.4/105.7 90.3/115.2 105.0 90.1/146.2 Avg 0.81 0.28 0.59 0.81 0.71 0.61 0.72 0.78 0.83 NS/S 0.80/0. 0.27/0.32 0.57/0.64 0.80/0.84 0.70/0.73 0.60/0.64 0.71/0. 0.77/0.81 0.82/0.85 Table 2: Effect of gaze control on motion. denotes that gaze control is disabled. ğ‘” 0.0 0.8 1.0 FGD FGDacc Foot Slide Wrist Var Head Ang. 1.28 0.99 0.92 1.49 2.19 2.18 2.19 2.20 0.01 0.01 0.01 0.01 105.0 111.1 110.8 106. 0.83 0.56 0.76 0."
        },
        {
            "title": "4.1 Quantitative Results\nTab. 1 summarizes our main results across five evaluation axes. We\norganize our analysis by first examining retrieval baselines, then\ngenerative baselines, and finally our ablations.",
            "content": "Retrieval Baselines (Random, NN). The retrieval baselines achieve the lowest FGD scores (Random: 1.06, NN: 0.90) since they sample directly from the true data distributionoutperforming Ours (1.28) on this metric alone. However, this advantage is superficial: retrieval methods cannot jointly satisfy all criteria. Randoms gaze alignment score (0.28) is catastrophic compared to Ours (0.83) since randomly sampled motion bears no relation to user position. NN addresses this by jointly matching audio features (HuBERT embeddings) and user position, improving the gaze alignment to 0.59. While better than Random, this still falls short of Ours (0.83) for two reasons: (1) jointly matching audio and spatial features is nontrivial, as optimizing for one may compromise the other, and (2) no clip in the dataset exactly matches the target user trajectory. While both retrieval methods achieve near-zero foot sliding (0.01) by copying real motion (matching Ours), their wrist variance reveals further limitations: Random (188.1) overshoots GT (137.6) due to context-agnostic sampling, while NN (97.0) undershoots as retrieval favors common, less expressive clips. Ours (105.0) strikes better balance. These results highlight key distinction: while retrieval achieves strong distributional metrics by construction, it is fundamentally limited to what exists in the dataset. Ours instead generates novel motion that jointly optimizes for all criteriaachieving competitive FGD (1.28) while dramatically improving spatial awareness (0.83 vs. NNs 0.59). Generative baselines. To evaluate against non-real-time state-ofthe-art in the dyadic (two-person) setting, we adapt MDM and A2P to use the same user-aware conditioning as Ours: agent audio, user audio, and user head trajectory. When naively adapted to our domain, MDM achieves the worst FGD (3.48) among all methods. Analysis reveals that MDM produces over-smoothed motion: its wrist variance (61.4) is only 45% of GT (137.6), indicating severely dampened gestures. This likely reflects an architecture mismatch: MDM was designed for text-to-motion with coarse action descriptions, not fine-grained audio-gesture synchronization and may favor global motion coherence over local dynamics. MDM appropriately matches the ground truth gaze alignment (0.81) perfectlyperhaps due to its non-causal architecture having access to future user positions to allow it to preemptively react accordingly. In contrast, Ours achieves similar gaze alignment (0.83) while operating causally, demonstrating that gaze alignment can be learned without requiring future information. MDM also exhibits significant foot sliding (0.11), suggesting that diffusion directly over the euclidean representation actually struggles to maintain physical constraints without learned latent prior. A2P extends MDM with an additional VQ-based stage: discrete tokens are first generated autoregressively, then refined via diffusion. This two-stage approach reduces FGD and foot sliding compared to MDM. However, A2P still falls short of Ours across all metrics: higher FGD (2.01 vs. 1.28), lower wrist variance (69.4 vs. 105.0), and weaker gaze alignment (0.71 vs. 0.83). Qualitatively, we find that A2Ps coarse VQ keyframes can lag temporally, forcing the diffusion stage to correct for misaligned targets. This results in dampened gestures (lower wrist variance) and temporally offset gaze (lower gaze alignment). Both diffusion methods also run at only 90 FPS3 slower than Oursand their reliance on future context prevents deployment in streaming applications. Unlike the diffusion methods, SHOW operates causally at 230 FPS, making it the most architecturally comparable baseline to Ours. We evaluate it without user conditioning to serve as monadic (single-agent) baseline. However, SHOW struggles even in its original domainsuggesting fundamental architectural limitations even without user conditioning. On foot sliding, the gap is stark: SHOW (0.27) is 27 worse than Ours (0.01), likely due to its separate SARAH: Spatially Aware Real-time Agentic Humans arXiv, 2026,"
        },
        {
            "title": "5 Conclusion\nWe presented the first method for spatially-aware conversational\nmotion, enabling virtual agents to orient toward and react to a\nmoving user in real-time while producing natural, speech-aligned\ngestures. The architecture pairs a novel causal transformer-based\nVAE with a flow matching model conditioned on user trajectory\nand dyadic audio. Recognizing that gaze preferences vary, we in-\ntroduce a gaze alignment score steered via classifier-free guidance,\ndecoupling learning from control. Experiments show state-of-the-\nart quality at over 300 FPS, outperforming non-causal baselines 3Ã—\nslower. The causal, real-time nature enables deployment in stream-\ning headset environments.",
            "content": "Our method inherits training data biases: underrepresented spatial configurations or gaze behaviors may generalize poorly. While we demonstrate controllable gaze, other behaviorsgesture style, locomotionare not yet controllable. Extending to multi-party conversations would require architectural modifications. Acknowledgments We would like to thank the Embody 3D team for making this project possible. We would also like to thank Abhay Mittal, Anastasis Stathopoulos, and Ethan Weber for helpful discussions. Thank you, Vasu Agrawal, Martin Gleize, and Srivathsan Govindarajan for making the demo possible. Figure 5: We visualize the agents facing direction via projected lines (agent: yellow red; user: blue green). With no alignment ğ‘” = , the agents gaze is more diverse; as we increase ğ‘”, the agent increasingly turns towards the user. VQ-VAEs for arms and handsoriginally designed for upper-body motionwhich lack body-ground coordination when extended to full-body generation. On expressiveness, SHOWs wrist variance (65.0) falls well below Ours (105.0). Qualitatively, SHOW produces sweeping gestures but struggles with the rapid, fine-grained motion important for expressive speechdynamics that Ours captures through its flow-based formulation. As expected, the largest gap is in spatial awareness: SHOWs gaze alignment (0.61) falls well below Ours (0.83). This highlights key limitation of audio-only conditioningthe audio signal does not encode user position, so the model cannot learn appropriate orientation. Ours addresses this directly through explicit user conditioning, enabling spatially-aware generation. Ablations. We isolate the contributions of key design choices. Ours in Joint Space (IK) replaces our Euclidean surface-point representation with traditional joint angles, requiring inverse kinematics to recover mesh positions. core issue is that joint-angle predictions face inherent ambiguitymultiple configurations can produce similar end-effector positions. This directly impacts metrics that depend on precise positioning: gaze alignment drops from 0.83 to 0.72 (head orientation), and foot sliding increases from 0.01 to 0.03 (foot-ground contact). The ambiguity may also encourage conservative predictions, which is reflected in wrist variance decreasing from 105.0 to 87.1the model produces less expressive motion when end-effector targets are uncertain. These results motivate our Euclidean surface-point approach, which directly specifies end-effector positions without ambiguity. Ours w/o VAE removes the causal VAE, directly predicting motion from the transformer. Without the VAEs learned latent structure, the model must predict high-dimensional motion directly, making it harder to capture the true motion distributionFGD rises from 1.28 to 1.95. However, physical plausibility metrics remain stable: foot sliding stays at 0.01 and wrist variance (96.9) remains comparable to Ours (105.0). This indicates that the VAEs primary benefit is distributionalmatching the motion manifoldrather than enforcing physical constraints, which our Euclidean representation seems to handle. Inference speed also halves (300 to 150 FPS), as predicting in the compressed latent space is more efficient than directly generating high-dimensional motion. arXiv, 2026, Ng et al. Figure 6: Sequences from our real-time demo system, rendered with photorealistic avatar. The top row visualizes the users headset location as silver sphere. The bottom row shows the generated avatar from the users (headset) viewpoint. Our method generates realistic conversational motion that is responsive to the users spatial motion. Full videos are available on our project page. SARAH: Spatially Aware Real-time Agentic Humans arXiv, 2026, References Chaitanya Ahuja, Shugao Ma, Louis-Philippe Morency, and Yaser Sheikh. 2019. To react or not to react: End-to-end visual pose forecasting for personalized avatar during dyadic conversations. In 2019 International Conference on Multimodal Interaction. 7484. Alexandre Alahi, Kratarth Goel, Vignesh Ramanathan, Alexandre Robicquet, Li Fei-Fei, and Silvio Savarese. 2016. Social lstm: Human trajectory prediction in crowded spaces. In Proceedings of the IEEE conference on computer vision and pattern recognition. 961971. Simon Alexanderson, Rajmund Nagy, Jonas Beskow, and Gustav Eje Henter. 2023. Listen, denoise, action! audio-driven motion synthesis with diffusion models. ACM Transactions on Graphics (TOG) 42, 4 (2023), 120. Tenglong Ao, Zeyi Zhang, and Libin Liu. 2023. GestureDiffuCLIP: Gesture diffusion model with CLIP latents. arXiv preprint arXiv:2303.14613 (2023). Michael Argyle and Janet Dean. 1965. Eye-contact, distance and affiliation. Sociometry (1965), 289304. Timur Bagautdinov, Alexandre Alahi, FranÃ§ois Fleuret, Pascal Fua, and Silvio Savarese. 2017. Social scene understanding: End-to-end multi-person action localization and collective activity recognition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Timur Bagautdinov, Chenglei Wu, Tomas Simon, Fabian Prada, Takaaki Shiratori, Shih-En Wei, Weipeng Xu, Yaser Sheikh, and Jason Saragih. 2021. Driving-signal aware full-body avatars. ACM Transactions on Graphics (TOG) 40, 4 (2021), 117. Boyuan Chen, Diego MartÃ­ MonsÃ³, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. 2024a. Diffusion forcing: Next-token prediction meets fullsequence diffusion. Advances in Neural Information Processing Systems 37 (2024), 2408124125. Rui Chen, Mingyi Shi, Shaoli Huang, Ping Tan, Taku Komura, and Xuelin Chen. 2024b. Taming diffusion probabilistic models for character control. In ACM SIGGRAPH 2024 Conference Papers. 110. Qingrong Cheng, Xu Li, and Xinghui Fu. 2024. Siggesture: Generalized co-speech gesture synthesis via semantic injection with large-scale pre-training diffusion models. In SIGGRAPH Asia 2024 Conference Papers. 111. Aaron Ferguson, Ahmed A. A. Osman, Berta Bescos, Carsten Stoll, Chris Twigg, Christoph Lassner, David Otte, Eric Vignola, Fabian Prada, Federica Bogo, Igor Santesteban, Javier Romero, Jenna Zarate, Jeongseok Lee, Jinhyung Park, Jinlong Yang, John Doublestein, Kishore Venkateshan, Kris Kitani, Ladislav Kavan, Marco Dal Farra, Matthew Hu, Matthew Cioffi, Michael Fabris, Michael Ranieri, Mohammad Modarres, Petr Kadlecek, Rawal Khirodkar, Rinat Abdrashitov, Romain PrÃ©vost, Roman Rajbhandari, Ronald Mallet, Russell Pearsall, Sandy Kao, Sanjeev Kumar, Scott Parrish, Shoou-I Yu, Shunsuke Saito, Takaaki Shiratori, Te-Li Wang, Tony Tung, Yichen Xu, Yuan Dong, Yuhua Chen, Yuanlu Xu, Yuting Ye, and Zhongshi Jiang. 2025. MHR: Momentum Human Rig. arXiv:2511.15586 [cs.GR] https://arxiv.org/abs/2511.15586 Saeed Ghorbani, Ylva Ferstl, Daniel Holden, Nikolaus Troje, and Marc-AndrÃ© Carbonneau. 2023. ZeroEGGS: Zero-shot Example-based Gesture Generation from Speech. In Computer Graphics Forum, Vol. 42. Wiley Online Library, 206216. Shiry Ginosar, Amir Bar, Gefen Kohavi, Caroline Chan, Andrew Owens, and Jitendra Malik. 2019. Learning individual styles of conversational gesture. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 34973506. Chuan Guo, Yuxuan Mu, Muhammad Gohar Javed, Sen Wang, and Li Cheng. 2024. Momask: Generative masked modeling of 3d human motions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 19001910. Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. 2021. Hubert: Self-supervised speech representation learning by masked prediction of hidden units. IEEE/ACM transactions on audio, speech, and language processing 29 (2021), 34513460. De-An Huang and Kris Kitani. 2014. Action-reaction: Forecasting the dynamics of human interaction. In European Conference on Computer Vision (ECCV). Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen. 2023. Motiongpt: Human motion as foreign language. Advances in Neural Information Processing Systems 36 (2023), 2006720079. Hanbyul Joo, Tomas Simon, Mina Cikara, and Yaser Sheikh. 2019. Towards social artificial intelligence: Nonverbal social signal prediction in triadic interaction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1087310883. Adam Kendon. 1967. Some functions of gaze-direction in social interaction. Acta psychologica 26 (1967), 2263. Akio Kodaira, Tingbo Hou, Ji Hou, Masayoshi Tomizuka, and Yue Zhao. 2025. Streamdit: Real-time streaming text-to-video generation. arXiv preprint arXiv:2507.03745 (2025). Taras Kucherenko, Patrik Jonell, Sanne Van Waveren, Gustav Eje Henter, Simon Alexandersson, Iolanda Leite, and Hedvig KjellstrÃ¶m. 2020. Gesticulator: framework for semantically-aware speech-driven gesture generation. In Proceedings of the 2020 international conference on multimodal interaction. 242250. Gilwoo Lee, Zhiwei Deng, Shugao Ma, Takaaki Shiratori, Siddhartha Srinivasa, and Yaser Sheikh. 2019. Talking with hands 16.2 m: large-scale dataset of synchronized body-finger motion and audio for conversational motion analysis and synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 763772. Shanchuan Lin, Xin Xia, Yuxi Ren, Ceyuan Yang, Xuefeng Xiao, and Lu Jiang. 2025. Diffusion adversarial post-training for one-step video generation. arXiv preprint arXiv:2501.08316 (2025). Haiyang Liu, Xingchao Yang, Tomoya Akiyama, Yuantian Huang, Qiaoge Li, Shigeru Kuriyama, and Takafumi Taketomi. 2024a. Tango: Co-speech gesture video reenactment with hierarchical audio motion embedding and diffusion interpolation. arXiv preprint arXiv:2410.04221 (2024). Haiyang Liu, Zihao Zhu, Giorgio Becherini, Yichen Peng, Mingyang Su, You Zhou, Xuefei Zhe, Naoya Iwamoto, Bo Zheng, and Michael Black. 2024b. Emage: Towards unified holistic co-speech gesture generation via expressive masked audio gesture modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 11441154. Haiyang Liu, Zihao Zhu, Naoya Iwamoto, Yichen Peng, Zhengqing Li, You Zhou, Elif Bozkurt, and Bo Zheng. 2022. BEAT: Large-Scale Semantic and Emotional Multi-Modal Dataset for Conversational Gestures Synthesis. arXiv preprint arXiv:2203.05297 (2022). Claire McLean, Makenzie Meendering, Tristan Swartz, Orri Gabbay, Alexandra Olsen, Rachel Jacobs, Nicholas Rosen, Philippe de Bree, Tony Garcia, Gadsden Merrill, Jake Sandakly, Julia Buffalini, Neham Jain, Steven Krenn, Moneish Kumar, Dejan Markovic, Evonne Ng, Fabian Prada, Andrew Saba, Siwei Zhang, Vasu Agrawal, Tim Godisart, Alexander Richard, and Michael Zollhoefer. 2025. Embody 3D: Large-scale Multimodal Motion and Behavior Dataset. Technical Report. arXiv. https://arxiv.org/pdf/2510.16258 arXiv preprint. Evonne Ng, Hanbyul Joo, Liwen Hu, Hao Li, Trevor Darrell, Angjoo Kanazawa, and Shiry Ginosar. 2022. Learning to listen: Modeling non-deterministic dyadic facial motion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2039520405. Evonne Ng, Javier Romero, Timur Bagautdinov, Shaojie Bai, Trevor Darrell, Angjoo Kanazawa, and Alexander Richard. 2024. From audio to photoreal embodiment: Synthesizing humans in conversations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 10011010. Evonne Ng, Donglai Xiang, Hanbyul Joo, and Kristen Grauman. 2020. You2me: Inferring body pose in egocentric video via first and second person interactions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 98909900. Simbarashe Nyatsanga, Taras Kucherenko, Chaitanya Ahuja, Gustav Eje Henter, and Michael Neff. 2023. comprehensive review of data-driven co-speech gesture generation. In Computer Graphics Forum, Vol. 42. Wiley Online Library, 569596. William Peebles and Saining Xie. 2023. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision. 41954205. Stefano Pellegrini, Andreas Ess, and Luc Van Gool. 2010. Improving data association by joint modeling of pedestrian trajectories and groupings. In European Conference on Computer Vision (ECCV). Sanjay Subramanian, Evonne Ng, Lea MÃ¼ller, Dan Klein, Shiry Ginosar, and Trevor Darrell. 2024. Pose Priors from Language Models. arxiv (2024). Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, and Amit Bermano. 2022. Human motion diffusion model. arXiv preprint arXiv:2209.14916 (2022). Adrien Treuille, Seth Cooper, and Zoran PopoviÄ‡. 2006. Continuum crowds. (2006). Jiajia Xie, Sheng Zhang, Beihao Xia, Zhu Xiao, Hongbo Jiang, Siwang Zhou, Zheng Qin, and Hongyang Chen. 2024. Pedestrian trajectory prediction based on social interactions learning with random weights. IEEE Transactions on Multimedia 26 (2024), 75037515. Jing Yang, Yuehai Chen, Shaoyi Du, Badong Chen, and Jose Principe. 2024. IA-LSTM: Interaction-aware LSTM for pedestrian trajectory prediction. IEEE transactions on cybernetics 54, 7 (2024), 39043917. Hongwei Yi, Hualin Liang, Yifei Liu, Qiong Cao, Yandong Wen, Timo Bolkart, Dacheng Tao, and Michael Black. 2023. Generating holistic 3d human motion from speech. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 469480. Zhentao Yu, Zixin Yin, Deyu Zhou, Duomin Wang, Finn Wong, and Baoyuan Wang. 2023. Talking head generation with probabilistic audio-to-visual diffusion priors. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 76457655. Siwei Zhang, Qianli Ma, Yan Zhang, Zhiyin Qian, Taein Kwon, Marc Pollefeys, Federica Bogo, and Siyu Tang. 2022. Egobody: Human body shape and motion of interacting people from head-mounted devices. In European conference on computer vision. Springer, 180200. Zeyi Zhang, Tenglong Ao, Yuyao Zhang, Qingzhe Gao, Chuan Lin, Baoquan Chen, and Libin Liu. 2024. Semantic gesticulator: Semantics-aware co-speech gesture synthesis. ACM Transactions on Graphics (TOG) 43, 4 (2024), 117. Zeyi Zhang, Yanju Zhou, Heyuan Yao, Tenglong Ao, Xiaohang Zhan, and Libin Liu. 2025. Social Agent: Mastering Dyadic Nonverbal Behavior Generation via Conversational LLM Agents. In SIGGRAPH Asia 2025 Conference Papers (Hong Kong, China) (SA 25). Association for Computing Machinery, New York, NY, USA, Article 71, 10 pages. doi:10.1145/3757377.3763879 Kaifeng Zhao, Gen Li, and Siyu Tang. 2024. DartControl: diffusion-based autoregressive motion model for real-time text-driven motion control. arXiv preprint arXiv, 2026, Ng et al. A.2 Training Details We provide additional training hyperparameters and details not included in the main text. Optimization. We use the AdamW optimizer with ğ›½1 = 0.9, ğ›½2 = 0.999, and weight decay 1 104. The learning rate follows linear warmup over the first 1,000 training steps, peaking at 1 104. The VAE is trained for 200K iterations before freezing, after which the flow matching model is trained for an additional 300K iterations. Data Processing. We use 80/10/10 split for training/validation/test. During training, we randomly sample full sequence from the training set and from there, randomly sample subsequence of length ğ‘‡ = 400 frames. For test time, we use sliding window of length ğ‘‡ = 400 and no overlap. We evaluate across the full set and generate 2048 sequences in total. For audio features, we use HuBERT-Large, which is not fully causal. So at training time, we essentially do have some information leakage. In order to ensure that it is fully causal at test time, we implement the streaming logic such that we never pass into HuBERT any future frames to avoid this leakage. Instead, we always implement sliding window logic where we pass in the current context and then the previous ğ‘‡ ğ‘  frames. We find that shifting to this fully causal approach at test time does not degrade performance. Latent Dimension. The VAE latent dimension is ğ·ğ‘§ = 256. With stride ğ‘  = 4 and sequence length ğ‘‡ = 400, this produces ğ¾ = 100 latent tokens per sequence. A.3 Inference Details Streaming Protocol. For real-time deployment, we generate motion in chunks of ğ‘  = 4 frames. We then keep the last 2 tokens and then remove all the prior ones. In essence, we generate total of 8 frames at time. As discussed in the main text (), we inpaint the history frames to maintain temporal consistency. For each chunk, we run using midpoint solver with 4 iterations (8 nfe steps). In this setting, we are able to achieve 60 fps at test time, which allows us to achieve real-time streaming performance. Photorealistic Rendering. We follow [Bagautdinov et al. 2021], learning based method, to render photorealistic avatars from the generated joint parameter motions. The model takes as input one frame of facial expression, one frame of body pose, and viewpoint direction. We use an off the shelf method to generate facial expression parameters from speech audio. The model then outputs registered geometry and view dependent texture, which is used to synthesize images via rasterization. For further details, please refer to [Bagautdinov et al. 2021]. arXiv:2410.05260 (2024). Yihao Zhi, Xiaodong Cun, Xuelin Chen, Xi Shen, Wen Guo, Shaoli Huang, and Shenghua Gao. 2023. LivelySpeaker: Towards Semantic-Aware Co-Speech Gesture Generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). 2080720817. Lei Zhong, Yiming Xie, Varun Jampani, Deqing Sun, and Huaizu Jiang. 2024. Smoodi: Stylized motion diffusion model. In European Conference on Computer Vision. Springer, 405421. Supplementary Material A.1 Video Results Please refer to 5 minute video for this section. We start with the problem setup (00:00 00:45) for dyadic conversation between user and an agent. Given the users 3D position and dyadic audio (from both user and agent), our goal is to generate spatially-aware 3D motion for the agent that aligns with the conversation and moves according to the users 3D position. From the generated motion, we can then render photorealistic avatar. Our model is lightweight and fast enough to enable streaming, allowing real-time interaction with the AI agent on VR platforms. The streamed results (00:46 01:25) demonstrate that our model produces conversationally-appropriate gestures while naturally turning toward the user to signal social engagement. The agent seamlessly transitions between speaking and listening modes, maintaining dyanamic gestures when speaking, and engaged idle gestures when listening. Our method generalizes across diverse emotional contexts, producing contextually-appropriate body language: hands on hips and looking down when stressed or rejected (01:26 02:00), lively gestures when excited (02:01 02:26), clenched fists when angry (02:27 02:41), and exaggerated bowing in celebratory agreement (02:41 02:57). To ensure that our model is controllable when it comes to gaze preferences, we also include gaze score which we can tune at test time. For lower gaze scores, the agent avoids direct facing the user. For the exact same input conditioning, increasing the gaze score results in more direct facing (02:58 03:21). When we fully drop out the gaze score (ğ‘” = 0), the agents gaze just follows whatever is in-distribution with the training dataset (03:22 03:38). We also compare against existing methods. Compared to MDM [Tevet et al. 2022] our method produces considerably more lively gestures (03:39 03:52). Compared to Audio2Photoreal [Ng et al. 2024], our method produces more realistic motion (03:53 04:07). For Audio2Photoreal, it seems as if the VQ will predict slightly delayed motion which forces the diffusion side to catch up with, which results in distored motion. Compared to TalkSHOW [Yi et al. 2023], our method produces less motion artifacts since we predict the full-body motion in single model (04:08 04:22). Instead, TalkSHOWs VQ-based approach results in distored wrist motion artifacts and ample foot sliding. The real-time nature of our model enables fully interactive AI agents in VR (04:23 end). We generate dyadic conversations using off-the-shelf LLMs paired with text-to-speech modelshere, ChatGPT for dialogue and Kyutai for speech synthesis. This enables applications ranging from entertainment (e.g., gaming NPCs) to personal assistants."
        }
    ],
    "affiliations": [
        "Meta Reality Labs Redmond, WA, USA"
    ]
}