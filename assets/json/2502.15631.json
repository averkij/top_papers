{
    "paper_title": "The Relationship Between Reasoning and Performance in Large Language Models -- o3 (mini) Thinks Harder, Not Longer",
    "authors": [
        "Marthe Ballon",
        "Andres Algaba",
        "Vincent Ginis"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models have demonstrated remarkable progress in mathematical reasoning, leveraging chain-of-thought and test-time compute scaling. However, many open questions remain regarding the interplay between reasoning token usage and accuracy gains. In particular, when comparing models across generations, it is unclear whether improved performance results from longer reasoning chains or more efficient reasoning. We systematically analyze chain-of-thought length across o1-mini and o3-mini variants on the Omni-MATH benchmark, finding that o3-mini (m) achieves superior accuracy without requiring longer reasoning chains than o1-mini. Moreover, we show that accuracy generally declines as reasoning chains grow across all models and compute settings, even when controlling for difficulty of the questions. This accuracy drop is significantly smaller in more proficient models, suggesting that new generations of reasoning models use test-time compute more effectively. Finally, we highlight that while o3-mini (h) achieves a marginal accuracy gain over o3-mini (m), it does so by allocating substantially more reasoning tokens across all problems, even the ones that o3-mini (m) can already solve. These findings provide new insights into the relationship between model capability and reasoning length, with implications for efficiency, scaling, and evaluation methodologies."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 1 3 6 5 1 . 2 0 5 2 : r THE RELATIONSHIP BETWEEN REASONING AND PERFORMANCE IN LARGE LANGUAGE MODELSO3 (MINI) THINKS HARDER, NOT LONGER. Marthe Ballon1, 0009-0000-4586-234X Andres Algaba1 0000-0002-0532-3066 Vincent Ginis1,2 0000-0003-0063-9608 1Data Analytics Lab, Vrije Universiteit Brussel, 1050 Brussel, Belgium 2School of Engineering and Applied Sciences, Harvard University, Cambridge, Massachusetts 02138, USA February 24,"
        },
        {
            "title": "ABSTRACT",
            "content": "Large language models have demonstrated remarkable progress in mathematical reasoning, leveraging chain-of-thought and test-time compute scaling. However, many open questions remain regarding the interplay between reasoning token usage and accuracy gains. In particular, when comparing models across generations, it is unclear whether improved performance results from longer reasoning chains or more efficient reasoning. We systematically analyze chain-of-thought length across o1-mini and o3-mini variants on the Omni-MATH benchmark, finding that o3-mini (m) achieves superior accuracy without requiring longer reasoning chains than o1-mini. Moreover, we show that accuracy generally declines as reasoning chains grow across all models and compute settings, even when controlling for difficulty of the questions. This accuracy drop is significantly smaller in more proficient models, suggesting that new generations of reasoning models use test-time compute more effectively. Finally, we highlight that while o3-mini (h) achieves marginal accuracy gain over o3-mini (m), it does so by allocating substantially more reasoning tokens across all problems, even the ones that o3-mini (m) can already solve. These findings provide new insights into the relationship between model capability and reasoning length, with implications for efficiency, scaling, and evaluation methodologies. Keywords chain-of-thought large language models reasoning models test-time compute mathematical benchmarks"
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have evolved from handling basic natural language processing tasks to solving complex problems [1, 2, 3, 4]. Scaling model size, data, and compute [5] has enabled larger models to develop richer internal representations [6, 7] and emergent capabilities [8]. Recently, new class of reasoning models has emerged that couples reinforcement learning with test-time compute scaling [9, 10]. These models leverage reasoning tokens to guide the chain-of-thought process and maintain coherence throughout complex problem-solving tasks [11, 12, 13]. By explicitly optimizing the chain-of-thought in the reasoning tokens during training [14] and iteratively refining outputs at inference, these models achieve superior performance, including on challenging mathematical benchmarks [15, 16]. Moreover, new test-time scaling laws demonstrate that longer reasoningi.e. more reasoning tokensyields log-linear performance gains [9]. In this paper, we examine whether more capable models within single family (o-series of OpenAI) require longer chain-of-thought to achieve higher performance or if they can reason more effectively. By systematically comparing the number of tokens in the chain-of-thought generated by o1-mini, o3-mini (m), and o3-mini (h) on the Omni-MATH dataset [17], we find that more proficient models (o1-mini vs. o3-mini (m)) do not generate longer reasoning chains Corresponding author: marthe.ballon@vub.be 1 o3 (mini) thinks harder, not longer February 24, Fig. 1 Accuracy comparison of OpenAI models gpt-4o, o1-mini, o3-mini (m) and o3-mini (h) on the OmniMATH benchmark. This figure displays the accuracy of gpt-4o, o1-mini, o3-mini (m) and o3-mini (h) on the Omni-MATH benchmark across disciplines and difficulty tiers. The gpt-4o model fails to attain 50% in any category and consistently lags behind the reasoning models. o1-mini significantly improves accuracy, reaching accuracies of 40-60% across all domains, while the o3-models surpass 50% accuracy in all categories. In general, accuracy declines as difficulty increases, with the exception of gpt-4o, which shows accuracy vs. difficulty level imbalance for Tiers 2, 3, and 4. to achieve higher accuracy. For all models and compute settings, we find that accuracy generally decreases as the chain-of-thought grows, even when controlling for question difficulty. This effect is notably smaller for more proficient models, indicating that o3-mini (m) tends to overthink less and uses reasoning tokens more effectively than o1-mini. However, within one model (o3-mini (m) vs. o3-mini (h)), we observe that the slower accuracy decrease per token is partially due to higher average accuracy, but mainly due to the model allocating (more than) double the reasoning tokens for all questions. Our findings contribute to the ongoing discussion about whether models such as o1 tend to overthink or underthink [12, 13], while complementing studies on reasoning step length [18], input length [19], reasoning failure modes [11], and the optimization of mathematical reasoning [20]. We use the Omni-MATH dataset [17] to benchmark the reasoning abilities of o1-mini, o3-mini (m) and o3-mini (h). Existing benchmarks such as GSM8K [21] and MATH [22] have become less effective in differentiating the mathematical abilities of LLMs due to the high accuracy rates they achieve on these tests. To address this, the Omni-MATH benchmark provides rigorous evaluation framework at the Olympiad level. Unlike other benchmarks such as FrontierMath [23], GSM-symbolic [24], and sections of Humanitys Last Exam [25], the Omni-MATH dataset contains problems categorized into over 33 sub-domains and spanning more than 10 distinct difficulty levels. This detailed organization enables nuanced assessment of LLMs mathematical reasoning across various disciplines and complexities. In addition, the availability of Omni-Judge [17]a math-evaluation model designed to verify and correct model-generated answers against reference answersensures straightforward and automated evaluation method. More general benchmarks like those in [26] and MMLU [27], along with specialized tests such as AI2 Reasoning [28] and GPQA [29], broaden the evaluation landscape to diverse reasoning domains. Additionally, coding benchmarks [30, 31] highlight the importance of clearly defined reward models and verification systems in assessing models performance. Future evaluations may consider additional benchmarks that incorporate broader language understanding II o3 (mini) thinks harder, not longer February 24, 2025 Fig. 2 Granular performance and reasoning token usage evaluation across domains and difficulty tiers of gpt4o, o1-mini, o3-mini (m) and o3-mini (h) on the Omni-MATH benchmark. The heatmaps visualize cross-sectional performance scores on 0-100% scale, represented by the color of the progress bar. The length of the progress bar in each cell represents relative token usage for the test-time scaled models. The extra column is computed by averaging over the rows. The extra row and average cell are computed independently to give equal weight to multi-domain questions (see Methods). This figure shows that models allocate more computational resources to problems that require complex combinatorial reasoning (Geometry, Discrete Mathematics and Number Theory), whereas foundational arithmetic and algebra problems demand relatively fewer resources. On average, token usage scales with difficulty level. or real-world reasoning challenges, though the current focus remains on math and coding due to the relative ease of implementing objective reward models and automated verification procedures in these domains."
        },
        {
            "title": "Results",
            "content": "Our data consists of 4428 Olympiad-level math problems, the Omni-MATH benchmark, together with reference answer and relevant metadata fields Domain and Difficulty (Appendix Figs. A1 and A2). We consider six elementary mathematics domains, Algebra, Applied Mathematics, Calculus, Discrete Mathematics, Geometry and Number Theory (Appendix Fig. A3) and divide the data into four difficulty tiers, Tier 1, Tier 2, Tier 3 and Tier 4 (Appendix Fig. A4). 3 o3 (mini) thinks harder, not longer February 24, 2025 Subsequently, we feed the problems to four OpenAI models, namely gpt-4o, o1-mini, o3-mini (m) and o3-mini (h) and make automated requests to the Omni-Judge model [17] to correct their answers (consult Methods for more details). Accuracy and relative token usage across models, disciplines, and difficulties Figs. 1 and 2 show the accuracy of OpenAI models gpt-4o, o1-mini, o3-mini (m) and o3-mini (h) across disciplines and difficulty tiers. The gpt-4o model performs consistently between 20% and 30% for all math disciplines but clearly lags behind the three reasoning models. o1-mini significantly improves accuracy in all categories, reaching 4060% on all domains. The introduction of o3-mini (m) further enhances performance, achieving 50% in all categories. The o3-mini (h) model improves with approximately 4% on average compared to o3-mini (m) and surpasses 80% accuracy for Algebra and Calculus. notable outlier is Discrete Mathematics, where performance deviates from the overall trend for all models. In general, accuracy declines as tier level increases. An exception is observed in gpt-4o, which performs better on Tier 4 than on Tiers 2 and 3 (the reasoning models also perform slightly better on Calculus Tier 3 than Tier 4). This anomaly suggests that the model might leverage unexpected heuristics or struggle disproportionately with mid-tier complexity. Besides indicating accuracy (via the colors of the progress bars), Fig. 2 also shows relative use of reasoning tokens (via the length of the progress bars) across the Omni-MATH dataset for o1-mini, o3-mini (m), and o3-mini (h). The relative use of tokens increases with the level of difficulty for all models, highlighting the need for computational resources for more difficult tasks. Discrete Mathematics stands out as token-intensive domain, indicating heavier combinatorial or multi-step reasoning load. Foundational mathematics areas such as Calculus and Algebra tend to consume fewer tokens, possibly because they are more procedurally straightforward. Interestingly, we observe that relatively longer chain of reasoning does not generally lead to better performance, as many Tier 4 math problems from token-intensive domains remain unsolved. Notable exceptions are the Geometry Tier 3 problems, where all three reasoning models allocate more reasoning compute to Tier 3 than Tier 4 problems, resulting in higher accuracy for Geometry Tier 3 than Tier 4. Reasoning token distribution and performance vs. token usage Figs. 3 and 4 display the relationship between the number of reasoning tokens and the performance of o1-mini, o3-mini (m), and o3-mini (h) on the Omni-MATH dataset (consult Appendix Fig. A5 for gpt-4o analysis with completion tokens, which encompass both the tokens leading up to the answer and the answer itself). Fig. 3 shows the proportion of the correct (green bars) versus incorrect (red bars) model responses across the reasoning token distribution. The red dashed line depicts the conditional error rate, i.e. the probability that the model answers incorrectly given the amount of used reasoning tokens (see Methods). One first thing to note is that higher performing models have better ratio of correct to incorrect answers, even for high token counts. This pattern is also reflected in the conditional error rate (red dashed line): the conditional error rate is almost instantly at 50% for o1-mini whereas it takes about 12, 000 tokens for o3-mini (m) and 30, 000 for o3-mini (h) to reach 50% error rate. second thing to note is that the token distributions of o1-mini and o3-mini (m) are very similar. Fig. 4b together with the left QQ-plot in Fig. A6 further investigate this behavior by comparing the distribution of the reasoning tokens only for the questions that the models answered correctly. Indeed, the almost identical token distributions show that o3-mini (m) does not use more reasoning tokens to achieve its superior performance to o1-mini on Omni-MATH. This suggests that o3-mini (m) reasons more effectively. The token distribution of o3-mini (h) spans significantly wider range of values, with the model allocating over 50, 000 reasoning tokens for some math problems. In addition, the right QQ-plot in Fig. A6 shows that o3-mini (h) uses more reasoning tokens to solve all (correctly answered) questions, indicating that the small accuracy gain of 4% compared to o3-mini (m) is accompanied by large extra computational cost. The panels below the histograms in Fig. 3 display the relative proportion of tier levels in each bin. They reveal clear transition from region where the majority of the questions come from the lowest tiers to region where the majority of the questions come from the highest tiers. Note that this pattern is visualized by the purple filled histograms in Fig. 3 (higher token regions sometimes have insufficient data counts to show this pattern). The gradient confirms that more complex questions systematically demand greater reasoning depth, which is in line with prior observations. Fig. 4a shows that the average accuracy decreases with increasing use of reasoning tokens for all three models, but that this trend is the most pronounced for o1-mini and smaller for o3-mini (m) and o3-mini (h). While this could be attributed to higher-tier questions requiring more tokens, Fig. 4c shows that the trend remains even when stratifying by tier level. In Fig. A7, we show this also holds when stratifying across domains. This suggests that increased token usage, rather than question complexity alone, is related to accuracy. We use logistic regression to quantify the effect size of using additional reasoning tokens on the probability of answering question correctly, controlling for different levels of difficulty and domains (see Methods). We report the average marginal effects in Appendix C. The accuracy decrease per 1000 reasoning tokens is 3.16% for o1-mini, 1.96% for o3-mini (m), and 0.81% for o3-mini (h). These results indicate that while deeper reasoning is necessary for solving complex problems, there is diminishing return, where excessive o3 (mini) thinks harder, not longer February 24, 2025 Fig. 3 Analysis of the reasoning token distribution, evolution of token region accuracy, and consistency between difficulty tiers and token usage for o1-mini, o3-mini (m) and o3-mini (h). The main panels of the figure display the distribution of the reasoning tokens as stacked histogram, illustrating the proportion of correctly and incorrectly answered questions in the Omni-MATH dataset by o1-mini, o3-mini (m) and o3-mini (h). The secondary y-axis depicts the probability that the model answers incorrectly given that the token count has surpassed the bin threshold (see Methods). The panels below the histogram contain filled histogram where the color opacity represents the difficulty level of the math questions (cfr. Fig. A4). The figure shows that o1-mini and o3-mini (m) have similar reasoning token distribution, with o3-mini (m) giving more correct answers for high-token regions. o3-mini (h) has good ratio of correct vs. incorrect answers, even for very high token counts. The probability of giving an incorrect answer increases with token count for all models. Finally, the relative proportion of tier levels in each bin reveal clear transition from region where the majority of the questions come from the lowest tiers to region where the majority of the questions come from the highest tiers (for bins with sufficient amount of data points). token usage correlates with reduced accuracy. However, this effect is smaller for higher performing models. Together with the results from the previous paragraph, we conclude that o3-mini (m) tends to overthink less and uses reasoning tokens more effectively than o1-mini. Within one model (o3-mini (m) vs. o3-mini (h)), the slower accuracy decrease per token is attributed to stretched out token distribution along the x-axis and shrunken distribution with respect to the y-axis."
        },
        {
            "title": "Discussion",
            "content": "By systematically comparing the number of tokens in the chain-of-thought generated by o1-mini, o3-mini (m), and o3-mini (h) on the Omni-MATH dataset [17], we find two important results. First, more proficient models (o1-mini vs. o3-mini (m)) do not require longer reasoning to achieve higher accuracy. Second, while accuracy generally declines with longer chain-of-thought, this effect is notably smaller in more proficient models, underscoring that thinking harder is not the same as thinking longer. possible hypothesis for this accuracy drop is that models tend to reason more on problems they cannot solve. Another possibility is that longer reasoning chains inherently have higher probability of leading to wrong final solution, highlighting the need for mathematical benchmarks with reference reasoning templates. practical takeaway from our study is that constraining the chain-of-thought (by setting max_completion_tokens) 5 o3 (mini) thinks harder, not longer February 24, 2025 Fig. 4 o3 (mini) thinks harder, not longer. This figure shows that o3-mini (m) does not require longer reasoning chains than o1-mini to achieve better accuracy and that, in general, more proficient models exhibit less accuracy decay as reasoning tokens increase. a, Accuracy per reasoning token, computed by dividing the number of correctly answered questions by the total number of questions in each bin of the histograms in Fig. 3. Accuracy declines as reasoning token usage increases. Furthermore, we observe that the slope of the lines becomes flatter for higher performing models. These effects are further quantified in the regression analysis (see Methods). b, The boxplots show the distribution of the reasoning tokens for correctly answered questions. Further investigation in the left panel of Fig. A6 confirms that o1-mini and o3-mini (m) have very similar token distribution. The token distribution of o3-mini (h) is stretched linearly with respect to the one of o3-mini (m) (Fig. A6 right). c, Stratifying plot by difficulty level shows that, within difficulty tiers, accuracy also decreases with higher reasoning token usage. This suggests that the number of reasoning tokens, rather that difficulty level alone, can be used as signal for the correctness of the models answer. In Fig. A7, we show this also holds when stratifying across domains. is more useful for weaker reasoning models than for stronger ones, as the latter still give significant amount of correct answers for high-token regions. The token count for o3-mini (h) contained the following subtlety: although o3-mini (h) solves additional problems compared to o3-mini (m), the model uses more tokens for all math problems. The slower decrease in accuracy per token is thus due to stretched token distribution along the x-axis rather than more effective usage of reasoning tokens. Our study relies on automated correction by Omni-Judge [17], model-based evaluator whose judgements could diverge from human corrections [32, 33]. Omni-Judge has only been validated for data leakage checks on o1-mini [17]; extending these checks to o3-mini remains future work, though we assume minimal overlap. Additionally, our prompting strategy employed here [34, 35, 36] may not generalize to alternative approaches or more constrained prompt settings [37], and their interaction with test-time compute warrants further investigation. Many studies on prompting and reasoning were conducted on LLMs without test-time compute, so the broader implications for the latest generation of reasoning models [15] remain to be fully understood. Our findings contribute to the debate on whether models such as o1 overthink or underthink [12, 13], and extend ongoing inquiries into reasoning step length [18], input length [19], failure modes [11], and the optimization of mathematical reasoning [20]. By revealing how stronger models can achieve higher accuracy without proportionally longer chain-of-thought, we offer new insights into efficiency, scaling, and evaluation strategies. We also highlight the 6 o3 (mini) thinks harder, not longer February 24, potential for next-generation test-time compute models to refine the balance between reasoning depth and resource usage."
        },
        {
            "title": "Data and code availability",
            "content": "Data associated with this study are publicly available in public repository at https://doi.org/10.5281/zenodo. 14878936. The original Omni-MATH dataset is available at https://huggingface.co/datasets/KbsdJames/Omni-MATH. The original Omni-Judge model is available at https://huggingface.co/KbsdJames/Omni-Judge. The code for this publication is publicly available at https://github.com/MartheBallon/analysis_o3-mini_ thinks_harder_not_longer. It is based on the Omni-MATH benchmark analysis code, publicly available at https://github.com/KbsdJames/Omni-MATH. We used Python 3.12.6 (pandas 2.2.3, numpy 2.1.1, matplotlib 3.9.2, seaborn 0.13.2, statsmodels 0.14.4, and scikit-learn 1.5.2) to analyse and visualize data and to conduct statistical analyses."
        },
        {
            "title": "Acknowledgements",
            "content": "This research received funding from the Flemish Government (AI Research Program). Andres Algaba acknowledges fellowship from the Research Foundation Flanders under Grant No.1286924N. Vincent Ginis acknowledges support from Research Foundation Flanders under Grant No.G032822N and G0K9322N."
        },
        {
            "title": "Author contributions",
            "content": "Vincent Ginis and Marthe Ballon were responsible for the main idea of the study and designed the setup of the analysis. Andres Algaba was responsible for conducting the regression analysis. Marthe Ballon executed the analysis and designed the figures. All authors discussed the results, and collaboratively drafted and revised the manuscript. 7 o3 (mini) thinks harder, not longer February 24,"
        },
        {
            "title": "References",
            "content": "[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [2] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of Artificial General Intelligence: Early experiments with GPT-4. arXiv preprint arXiv:2303.12712, 2023. [3] Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, Pawan Kumar, Emilien Dupont, Francisco JR Ruiz, Jordan Ellenberg, Pengming Wang, Omar Fawzi, et al. Mathematical discoveries from program search with large language models. Nature, 625(7995):468475, 2024. [4] Trieu Trinh, Yuhuai Wu, Quoc Le, He He, and Thang Luong. Solving olympiad geometry without human demonstrations. Nature, 625(7995):476482, 2024. [5] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [6] Wes Gurnee and Max Tegmark. Language models represent space and time. arXiv preprint arXiv:2310.02207, 2024. [7] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992, 2023. [8] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022. [9] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, s1: Simple test-time scaling. arXiv preprint Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. arXiv:2501.19393, 2025. [10] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. [11] Carolyn Jane Anderson, Joydeep Biswas, Aleksander Boruch-Gruszecki, Federico Cassano, Molly Feldman, Arjun Guha, Francesca Lucchetti, and Zixuan Wu. PhD Knowledge Not Required: Reasoning Challenge for Large Language Models. arXiv preprint arXiv:2502.01584, 2025. [12] Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, et al. Do not think that much for 2+ 3=? On the overthinking of o1-like LLMs. arXiv preprint arXiv:2412.21187, 2024. [13] Yue Wang, Qiuzhi Liu, Jiahao Xu, Tian Liang, Xingyu Chen, Zhiwei He, Linfeng Song, Dian Yu, Juntao Li, Zhuosheng Zhang, et al. Thoughts are all over the place: On the underthinking of o1-like LLMs. arXiv preprint arXiv:2501.18585, 2025. [14] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [15] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [16] Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. rStar-Math: Small LLMs can master math reasoning with self-evolved deep thinking. arXiv preprint arXiv:2501.04519, 2025. [17] Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, et al. Omni-math: universal olympiad level mathematic benchmark for large language models. arXiv preprint arXiv:2410.07985, 2024. [18] Mingyu Jin, Qinkai Yu, Dong Shu, Haiyan Zhao, Wenyue Hua, Yanda Meng, Yongfeng Zhang, and Mengnan Du. The impact of reasoning step length on large language models. arXiv preprint arXiv:2401.04925, 2024. [19] Mosh Levy, Alon Jacoby, and Yoav Goldberg. Same task, more tokens: the impact of input length on the reasoning performance of large language models. arXiv preprint arXiv:2402.14848, 2024. 8 o3 (mini) thinks harder, not longer February 24, 2025 [20] Qihuang Zhong, Kang Wang, Ziyang Xu, Juhua Liu, Liang Ding, Bo Du, and Dacheng Tao. Achieving> 97% on GSM8K: Deeply understanding the problems makes LLMs perfect reasoners. arXiv preprint arXiv:2404.14963, 2024. [21] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [22] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. [23] Elliot Glazer, Ege Erdil, Tamay Besiroglu, Diego Chicharro, Evan Chen, Alex Gunning, Caroline Falkman Olsson, Jean-Stanislas Denain, Anson Ho, Emily de Oliveira Santos, et al. Frontiermath: benchmark for evaluating advanced mathematical reasoning in AI. arXiv preprint arXiv:2411.04872, 2024. [24] Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. GSM-symbolic: Understanding the limitations of mathematical reasoning in large language models. arXiv preprint arXiv:2410.05229, 2024. [25] Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Sean Shi, Michael Choi, Anish Agrawal, Arnav Chopra, et al. Humanitys last exam. arXiv preprint arXiv:2501.14249, 2025. [26] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022. [27] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. [28] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? Try ARC, the AI2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [29] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. GPQA: graduate-level google-proof Q&A benchmark. arXiv preprint arXiv:2311.12022, 2023. [30] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [31] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. SWE-bench: Can language models resolve real-world GitHub issues? In The Twelfth International Conference on Learning Representations, 2024. [32] Pat Verga, Sebastian Hofstatter, Sophia Althammer, Yixuan Su, Aleksandra Piktus, Arkady Arkhangorodsky, Minjie Xu, Naomi White, and Patrick Lewis. Replacing judges with juries: Evaluating LLM generations with panel of diverse models. arXiv preprint arXiv:2404.18796, 2024. [33] Dawei Li, Renliang Sun, Yue Huang, Ming Zhong, Bohan Jiang, Jiawei Han, Xiangliang Zhang, Wei Wang, and Huan Liu. Preference leakage: contamination problem in LLM-as-a-judge. arXiv preprint arXiv:2502.01534, 2025. [34] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022. [35] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. [36] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2024. [37] Zhi Rui Tam, Cheng-Kuang Wu, Yi-Lin Tsai, Chieh-Yen Lin, Hung-yi Lee, and Yun-Nung Chen. Let me speak freely? study on the impact of format restrictions on performance of large language models. arXiv preprint arXiv:2408.02442, 2024. 9 o3 (mini) thinks harder, not longer February 24,"
        },
        {
            "title": "A Methods",
            "content": "We describe our experimental setup and provide the data processing details necessary to replicate our analysis. At the end of this section, we elaborate on the regression analysis conducted to analyse the effect size of increased reasoning token usage on accuracy. The Omni-MATH dataset The Omni-MATH benchmark [17] contains Olympiad-level math problems specifically designed to test the reasoning abilities of Large language models. Each entry in the dataset consists of problem, an exact answer, and written out solution together with the following metadata fields: Domain, Difficulty, and Source (see Fig. A1). Each problem has between one and three domains of the form Mathematics Primary domain . . . , with maximum length of five. In this paper, we only take the primary domains into account, as more granular classification gives rise to very imbalanced or underpopulated classes. Fig. A3 shows the number of math problems per (primary) domain where we follow [17] in doubleor triple-counting the multi-domain questions. We made sure to delete the duplicate entries, e.g. some data entries had multiple domain trees but the same primary domain. Every domain-specific analysis in the paper follows this convention. Finally, we joined the Calculus and Pre Calculus class and deleted the Other class to obtain more balanced domain distribution. Math problems are also classified according to difficulty level as presented in Fig. A4. We divide the data into difficulty tiers based on the quartiles of the difficulty distribution (without separating difficulty levels). OpenAI models We evaluate the performance of the several OpenAI models that are affordable for most users: gpt-4o-06-08-2024, o1-mini-12-09-2024, o3-mini-31-01-2025 medium (default) and o3-mini-31-01-2025 high. The o3-mini high model, instead of medium, is obtained by setting reasoning_effort to high. We feed each model the math problems using the Batch API with the following vanilla prompt as user message: Solve the following problem. Enclose the final answer in boxed{{}} environment. Problem: {problem} Furthermore, we set max_completion_tokens limits of 25, 000 for o1-mini and o3-mini medium, and 100, 000 token limit for o3-mini high. Each reasoning model refused to answer few questions (flagged as invalid prompts), which were subsequently omitted from the analysis. Omni-Judge To correct the responses of the four OpenAI models on the Omni-MATH dataset, we employ another Large language model called Omni-Judge (KbsdJames/Omni-Judge). Omni-Judge is an efficient and low cost opensource math-evaluation model developed by the authors of [17]. The model is trained to assess the correctness of an answer generated by an LLM, given the problem and reference answer (see Fig. A2). Table 9 in [17] shows that Omni-Judge is 91.78% consistent with gpt-4o as judge (who is 98% consistent with human evaluators) and has almost 100% success rate of correctly parsing model generated answers. To judge the models generated answers, we make requests to the chat completions endpoint of the kbsdjames.omni-judge API by running the model in LM Studio. We use the same few-shot prompt as in [17] and set the max_new_tokens parameter to 300. In the very few cases where Omni-Judge fails to parse the model output (< 1%), we omit that question from the performance evaluation. Conditional probability The conditional probability appearing in Fig. 3 and Fig. A5 is computed using full Bayesian model with uninformative priors (we assume that P(False) = P(True) = 0.5). In particular, we have that P(False > Bi) = P(T > Bi False) P(T > Bi False) + P(T > Bi True) , (1) where {T > Bi} is the event that the number of tokens exceeds the right bin threshold and \"False\" indicates that the model answered incorrectly. Because Bi can only take finite number of values, we have that P(False > Bi) = (cid:88) k=i+1 P(T Bk False) P(T Bk False) + P(T Bk True) = (cid:88) k=i+1 False Bk False Bk + True Bk , (2) which can be easily computed using the stacked histogram data. o3 (mini) thinks harder, not longer February 24, 2025 Estimating effect sizes We use logistic regression to estimate the effect of additional reasoning tokens on the probability of an accurate response on question Yi, while controlling for different levels of difficulty and domains. The regression takes the following form: log (cid:18) Pr(Yi = 1) Pr(Yi = 0) (cid:19) = β0 + β1tokensi + K1 (cid:88) k=1 (cid:124) δk difficulty tierk(i) + 1 (cid:88) γm domainm(i) , (3) (cid:123)(cid:122) difficulty fixed effects (cid:125) m=1 (cid:124) (cid:123)(cid:122) domain fixed effects (cid:125) where i, k, and denote the question-response pair, the difficulty tier, and the domain, respectively. Moreover, k(i) and m(i) indicate that the difficulty tier and domain depend on the question-response pair i. The difficulty tier and domain fixed effects can be estimated by including dummy variables, which are equal to one if the difficulty tier or domain is equal to the difficulty tier or domain of the current question-response pair and equal to zero otherwise, with the exclusion of reference category (i.e., 1 and 1). The reference category for difficulty tiers is the lowest difficulty Tier 1 and for the domains it is Algebra. We obtain similar results when using the more fine-grained difficulty levels (0 10) instead of difficulty tiers. To facilitate interpretation, we compute the Average Marginal Effect (AME) of additional reasoning tokens on the probability of an accurate response. Unlike the raw logistic regression coefficients, which are expressed in log-odds, the AME directly quantifies the effect of an additional token in probability terms. Specifically, it represents the average change in the probability of accuracy for one-token increase, while holding difficulty tier and domain constant. By computing AMEs, we ensure that our estimates account for the full distribution of difficulty levels and domains, rather than relying on effects evaluated at single reference point. 11 o3 (mini) thinks harder, not longer February 24,"
        },
        {
            "title": "B Figures",
            "content": "Sample problem Omni-MATH Domain: Mathematics Discrete Mathematics Graph Theory Difficulty: 9.0 Source: China National Olympiad Problem: table tennis club hosts series of doubles matches following several rules: 1. each player belongs to two pairs at most; 2. every two distinct pairs play one game against each other at most; 3. players in the same pair do not play against each other when they pair with others respectively. Every player plays certain number of games in this series. All these distinct numbers make up set called the set of games. Consider set = {a1, a2, . . . , ak} of positive integers such that every element in is divisible by 6. Determine the minimum number of players needed to participate in this series so that schedule for which the corresponding set of games is equal to set exists. Answer: 1 2 max + 3. Solution: To determine the minimum number of players needed to participate in the series such that the set of games is equal to the set A, we start by analyzing the problem through graph theory. Consider graph where each vertex represents player and an edge between two vertices represents pair of players. According to the problems conditions: (i) Each player belongs to at most two pairs. (ii) Every two distinct pairs play one game against each other at most. (iii) Players in the same pair do not play against each other when they pair with others respectively. Given these conditions, each vertex in can have degree of at most 2. This implies that can be decomposed into disjoint cycles, paths, and isolated vertices. Let max = a. We need to ensure that there is player who plays games, and each element in is divisible by 6. To achieve this, we need to construct graph where the number of games each player plays corresponds to the elements in A. ### Proof of optimality: Let be vertex with the maximum number of games a. This vertex must be connected to at least one other vertex. If has only one edge, it must have games on it. If has two edges, each edge must have at least 2 games. Let xy be an edge with 2 edges, ensuring that there are at least 2 + 3 vertices to satisfy all conditions. ### Construction: To construct such graph, we can use clusters of three players, each forming pairs within the cluster. This ensures that each player in cluster plays 6 games per edge. By assigning each cluster to vertex in graph with vertices, where = 1 6 max + 1, we can ensure that the set of games corresponds to A. Thus, the minimum number of players needed is: 1 2 max + 3. The answer is: 2 games. The remaining vertices must account for at least 2 + 2 vertices. However, due to the constraints, there must be at least 1 2 max + 3 Fig. A1 Sample problem from the Omni-MATH dataset. The Omni-MATH dataset consists of 4428 Olympiadlevel math problems together with an exact answer, written out solution and metadata Domain, Difficulty and Source. 12 o3 (mini) thinks harder, not longer February 24, 2025 Sample output Omni-judge Problem: table tennis club hosts series of doubles matches following several rules: (i) each player belongs to two pairs at most; (ii) every two distinct pairs play one game against each other at most; (iii) players in the same pair do not play against each other when they pair with others respectively. Every player plays certain number of games in this series. All these distinct numbers make up set called the set of games. Consider set = {a1, a2, . . . , ak} of positive integers such that every element in is divisible by 6. Determine the minimum number of players needed to participate in this series so that schedule for which the corresponding set of games is equal to set exists. Answer: 1 2 max + 3. Model generation: We will show that one may hit prescribed set = {a1, a2, . . . , ak} (where each ai is positive multiple of 6) by the following construction, and that no schedule may use fewer than a1 + a2 + + ak 3 + 2k (1) players. [...] In any such schedule every pair (team) plays some number of games. (Rule (ii) means that once we have fixed pair the opponentteams of form set; we denote by d(P ) the number of games in which plays.) Then given players score (the number of games in which he participates) is determined by whether he belongs to one pair (in which case his score is the degree of that pair) or to two pairs (in which case his score is the sum of the two degrees). (Note that by rule (iii) if player belongs to two pairs then those two pairs never meet so that the numbers add!) Thus if we read off the numbers of games played by the players (omitting any repetitions) we obtain set - called the set of games. In our problem we wish this set to be exactly the given A. An important idea is that players who belong to two pairs are economical in that one player now serves to cover two teams. (In non-economical schedule each pair would contribute two singlemembers.) One may show that in schedule in which as many players as possible are doubles (that is, belong to two teams) one may control the scores so that in fact every players number is of the form d(P ) or d(P ) + d(Q) (with d(P ) and d(Q) coming from suitable graph on the pairs). One may then prove the following two claims: Claim 1. If the schedule uses only doubleplayers then the total number of players is exactly the number of pairs, and routine pairing (that is, perfect matching) of the teams shows that one must have 2k (since one obtains exactly n/2 sums, one for each gameofadouble, and these must contain the given numbers.) Claim 2. Since teams degree is at most (n 1) the maximum score obtainable by double is at most 2(n 1). In order to be able to reach number ai we must have 2(n 1) ai for every i, so that in particular 1 2 max{ai} + 1. careful doublecounting (namely, counting the total number of games by summing the scores of players in two different ways) shows that one must have a1+a2++ak + 2k. Moreover one may show by an inductive construction that equality is attainable. One acceptable answer to the problem is therefore to prove that the minimum number of players needed is a1+a2++ak + 2k. An answer equivalent to (1) is completely acceptable. Thus, our final answer is 3 3 a1 + a2 + + ak 3 + 2k . Omni-Judge: ## Student Final Answer: a1+a2++ak +2k. ## Equivalence Judgement: FALSE ## Justification: The students answer is a1+a2++ak 2 max + 3. The discrepancy lies in the treatment of the maximum value of and the constant term. In the students solution, they consider the average of all elements in set A, whereas the reference answer focuses on half of the maximum element in plus an additional constant term. This difference indicates that the students answer does not capture the same meaning as the reference answer. === report over === + 2k, which does not match the reference answer of 1 3 3 Fig. A2 Sample of the Omni-Judge output when presented with problem, reference answer and an o3-mini (h) generated solution. The Omni-Judge model is prompted as in [17], which is to repeat the students final answer, asses the equivalence of this answer with the reference answer and provide complementary justification. We omit the part where o3-mini (h) repeats the problem specifications, indicated by [...]. o3 (mini) thinks harder, not longer February 24, 2025 Fig. A3 Domain distribution of the Omni-MATH dataset. This figure displays the distribution of the primary domains of the Omni-MATH dataset. Math problems that belong to multiple domains are counted for each domain, so the total number of question is higher than in Fig. A4. . 14 o3 (mini) thinks harder, not longer February 24, 2025 Fig. A4 Classification of difficulty levels in balanced difficulty tiers. This figure shows the difficulty distribution of the Omni-MATH dataset. The difficulty levels are classified in difficulty tiers based on the quartiles of the distribution (without separating difficulty levels). 15 o3 (mini) thinks harder, not longer February 24, 2025 Fig. A5 Analysis of the completion token distribution, and the relationship between completion token usage and accuracy. This figure shows that gpt-4o uses predominantly between 200 and 1000 completion tokens for answering the Omni-MATH problems. We also observe that shorter answers are more likely to lead to correct final answer. Finally, the relative proportion of tier levels in each bin reveals clear transition from region where the majority of the questions come from the lowest tiers to region where the majority of the questions come from the highest tiers (for bins with sufficient amount of data points). a, The main panel of this plot displays stacked histogram of the reasoning tokens used for correctly and incorrectly answered questions in the Omni-MATH dataset. The secondary y-axis depicts the probability that the model answers incorrectly given that the token count has surpassed the bin threshold (see Methods). The subplot contains filled histogram where the color opacity represents the difficulty level of the math questions (cfr. Fig. A4). b, Accuracy per reasoning token, computed by dividing the number of correctly answered questions by the total number of questions in each bin of the histogram depicted in a. Accuracy declines as completion token usage increases. 16 o3 (mini) thinks harder, not longer February 24, 2025 Fig. A6 Comparison of the distribution of reasoning tokens between o1-mini and o3-mini (m), and between o3-mini (m) and o3-mini (h) for correctly answered problems in the Omni-MATH dataset. This figure compares the token distribution of three OpenAI reasoning models by means of QQ-plot. We observe that o1-mini and o3-mini (m) have an almost identical reasoning token distribution when we only consider the correctly answered questions. The token distribution of o3-mini (h) is linearly scaled version of the distribution of o3-mini (m) with factor slightly larger than 2. 17 o3 (mini) thinks harder, not longer February 24, 2025 Fig. A7 Stratification of Fig. 4a for the mathematical domains of the Omni-MATH dataset. This figure displays stratification of Fig. 4a by mathematical domain. In general, accuracy decreases within the domains as the use of reasoning tokens increases. 18 o3 (mini) thinks harder, not longer February 24,"
        },
        {
            "title": "C Tables",
            "content": "o1-mini o3-mini (m) o3-mini (h) without controls Tokens 1.85e4 1.25e4 5.77e5 Constant 1.19 1.53 1.87 1.61e4 1.08e4 5.10e5 with controls Tokens Difficulty tier 2 Difficulty tier 3 Difficulty tier 4 Applied Mathematics Calculus Discrete Mathematics Geometry Number Theory Other Constant McFaddens pseudo-R2 (without controls) 0.53 0.74 1.08 0.41 0.13 0.86 0.46 0.02 0.47 1.93 5, 0.11 McFaddens pseudo-R2 0.15 0.36 0.56 0.70 0.34 0.03 0.50 0.21 0.04 0.02 2.02 5, 531 0.06 0.07 0.20 0.37 0.63 0.37 0.09 0.41 0.26 0.08 0.03 2.25 5, 526 0.06 0.08 Fig. A8 Logistic regression models to estimate the effect size of the number of tokens on response accuracy. We use logistic regression to estimate the effect of additional reasoning tokens on the probability of an accurate response on question, while controlling for different levels of difficulty and domains. Estimates are from logistic regression (Eq. 3) fit by maximum likelihood, with robust (HuberWhite) standard errors to account for potential heteroskedasticity. The significance levels are for two-sided Wald test with null hypothesis of the regression coefficient being equal to zero (***p < 0.01, **p < 0.05, *p < 0.1)."
        }
    ],
    "affiliations": [
        "Data Analytics Lab, Vrije Universiteit Brussel, 1050 Brussel, Belgium",
        "School of Engineering and Applied Sciences, Harvard University, Cambridge, Massachusetts 02138, USA"
    ]
}