{
    "paper_title": "SelfSplat: Pose-Free and 3D Prior-Free Generalizable 3D Gaussian Splatting",
    "authors": [
        "Gyeongjin Kang",
        "Jisang Yoo",
        "Jihyeon Park",
        "Seungtae Nam",
        "Hyeonsoo Im",
        "Sangheon Shin",
        "Sangpil Kim",
        "Eunbyung Park"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose SelfSplat, a novel 3D Gaussian Splatting model designed to perform pose-free and 3D prior-free generalizable 3D reconstruction from unposed multi-view images. These settings are inherently ill-posed due to the lack of ground-truth data, learned geometric information, and the need to achieve accurate 3D reconstruction without finetuning, making it difficult for conventional methods to achieve high-quality results. Our model addresses these challenges by effectively integrating explicit 3D representations with self-supervised depth and pose estimation techniques, resulting in reciprocal improvements in both pose accuracy and 3D reconstruction quality. Furthermore, we incorporate a matching-aware pose estimation network and a depth refinement module to enhance geometry consistency across views, ensuring more accurate and stable 3D reconstructions. To present the performance of our method, we evaluated it on large-scale real-world datasets, including RealEstate10K, ACID, and DL3DV. SelfSplat achieves superior results over previous state-of-the-art methods in both appearance and geometry quality, also demonstrates strong cross-dataset generalization capabilities. Extensive ablation studies and analysis also validate the effectiveness of our proposed methods. Code and pretrained models are available at https://gynjn.github.io/selfsplat/"
        },
        {
            "title": "Start",
            "content": "4 2 0 2 7 2 ] . [ 2 0 9 1 7 1 . 1 1 4 2 : r SelfSplat: Pose-Free and 3D Prior-Free Generalizable 3D Gaussian Splatting Gyeongjin Kang1* Jisang Yoo1* Sangheon Shin2 Jihyeon Park1 Sangpil Kim3 Seungtae Nam1 Hyeonsoo Im Eunbyung Park1 Sungkyunkwan University1 Hanhwa Systems2 Korea University"
        },
        {
            "title": "Abstract",
            "content": "We propose SelfSplat, novel 3D Gaussian Splatting model designed to perform pose-free and 3D prior-free generalizable 3D reconstruction from unposed multi-view images. These settings are inherently ill-posed due to the lack of ground-truth data, learned geometric information, and the need to achieve accurate 3D reconstruction without finetuning, making it difficult for conventional methods to achieve high-quality results. Our model addresses these challenges by effectively integrating explicit 3D representations with self-supervised depth and pose estimation techniques, resulting in reciprocal improvements in both pose accuracy and 3D reconstruction quality. Furthermore, we incorporate matching-aware pose estimation network and depth refinement module to enhance geometry consistency across views, ensuring more accurate and stable 3D reconstructions. To present the performance of our method, we evaluated it on large-scale real-world datasets, including RealEstate10K, ACID, and DL3DV. SelfSplat achieves superior results over previous state-of-the-art methods in both appearance and geometry quality, also demonstrates strong cross-dataset generalization capabilities. Extensive ablation studies and analysis also validate the effectiveness of our proposed methods. Code and pretrained models are available at https://gynjn.github.io/ selfsplat/ 1. Introduction The recent introduction of Neural Radiance Fields (NeRF) [35] and 3D Gaussian Splatting (3D-GS) [24] had marked significant advancement in computer vision and graphics, particularly in 3D reconstruction and novel view synthesis. By training on images taken from various viewpoints, these methods can produce geometrically consistent photo-realistic images, providing beneficial for various ap- *Equal contribution Corresponding authors plications, such as virtual reality [22, 56], robotics [15, 37], and semantic understanding [63, 64]. Despite their impressive capability in 3D scene representation, training NeRF and 3D-GS requires large set of accurately posed images as well as iterative per-scene optimization procedures, which limits their applicability for broader use cases. To bypass the iterative optimization steps, various learning-based generalizable 3D reconstruction models [3, 13, 44, 54, 61] have been proposed. These models can predict 3D geometry and appearance from few posed images in single forward pass. Leveraging large-scale synthetic and real-world 3D datasets, they used pixel-aligned features to extract scene priors from input images and generate novel views through differentiable rendering methods such as volume rendering [34] or rasterization [26]. The generated images are then supervised with ground truth images captured from the same camera poses. While this approach enables 3D scene reconstruction without iterative optimization steps, key limitation remains are as follows: it relies on calibrated images (with accurate camera poses) for both training and inference, thereby constraining its use with less controlled, in-the-wild images or videos. Recent efforts have integrated camera pose estimation with 3D scene reconstruction, combining multiple tasks within single framework. By relaxing the constraint of posed multi-view setup, pose-free generalizable methods [6, 21, 28, 41] aim to learn reliable 3D geometry from uncalibrated images and generate accurate 3D representations in single forward pass. While these approaches have demonstrated promising results, they still face significant challenges. For example, [41] relies on error-prone pretrained flow model for pose estimation, often leading to inaccuracies and performance degradation. [6, 28] achieve impressive results but require per-scene fine-tuning stage, making them computationally expensive for real-world applications. Furthermore, both [41] and [6] inherit the limitation of NeRF-based approaches, demanding substantial computational costs due to the volumetric rendering. In this work, we present SelfSplat, novel training framework for pose-free, generalizable 3D representations 1 from monocular videos without pretrained 3D prior models or further scene-specific optimizations. We build upon the 3D-GS representation and leverage the pixel-aligned Gaussian estimation pipeline [3, 44], which has demonstrated fast and high-quality reconstruction results. By integrating 3D-GS representations with self-supervised depth and pose estimation techniques, the proposed method jointly predicts depth, camera poses, and 3D Gaussian attributes within unified neural network architecture. 3D-GS, as an explicit 3D representation, is highly sensitive to minor errors in 3D positioning. Even slight misplacements of Gaussians can disrupt multi-view consistency, significantly degrading rendering quality [3, 7]. This makes the simultaneous prediction of Gaussian attributes and camera poses especially challenging. The proposed approach, SelfSplat, mitigates this issue by leveraging the strengths of both self-supervised learning and 3D-GS. Exploiting the geometric consistency inherent in self-supervised learning techniques effectively guides the positioning of 3D Gaussians, leading to improved reconstruction accuracy in the absence of camera pose information. Also, harnessing 3DGS representation and its superior view synthesis capabilities help enhance the accuracy of camera pose estimation, which would otherwise depend solely on 2D image features derived from CNNs [17, 42] or Transformers [9, 38]. While the proposed method is encouraging, simply combining self-supervised learning with explicit 3D geometric supervision has yielded suboptimal results, particularly in predicting accurate camera poses and generating multi-view consistent depth maps. This often results in misaligned 3D Gaussians and inferior 3D structure reconstructions. To address issues from pose estimation errors, we introduce matching-aware pose network that incorporates additional cross-view knowledge to improve geometric accuracy. By leveraging contextual information from multiple views, this network improves pose accuracy and ensures more reliable estimates across views. Additionally, to support consistent depth estimation, crucial for accurate 3D scene geometry, we develop depth refinement network. This module uses estimated poses as embedding features which contains spatial information from surrounding views, to achieve accurate and consistent 3D geometry representations. Once trained in self-supervised manner, SelfSplat is equipped to perform several downstream tasks, including (1) pose, depth estimation, and (2) 3D reconstruction, including fast novel view synthesis. We demonstrate the efficacy of our method on RealEstate10k [66], ACID [32], and DL3DV [31] datasets providing higher appearance and geometry quality as well as better cross-dataset generalization performance. Extensive ablation studies and analyses also show the effectiveness of our proposed method. The main contributions can be summarized as follows: We propose SelfSplat, pose-free and 3D prior-free selfsupervised learner from large-scale monocular videos. We propose to unify self-supervised learning with 3DGS representation, harnessing the synergy of both frameworks to achieve robust 3D geometry estimation. To address pose estimation errors and inconsistent depth predictions, we introduce the matching-aware pose network and depth refinement module, which enhance geometry consistency across views, ensuring more accurate and stable 3D reconstructions. We have conducted comprehensive experiments and ablation studies on diverse datasets, and the proposed SelfSplat significantly outperforms the previous methods. 2. Related work 2.1. Pose-free Neural 3D Representations In the absence of camera pose information, recent efforts have aimed to jointly optimize camera poses and 3D scenes. Starting with optimization-based methods, BARF [29] and subsequent research [2, 16, 23] addressed this challenge by training poses along with implicit or explicit scene representations. Also, in generalizable setting with NeRF representations, FlowCam [41] utilized pretrained flow estimation model, RAFT [45], and find the rigid-body motion between 3D point clouds using the Procrustes algorithm [11]. DBARF [6] extended the previous optimizationbased method [29] and utilized recurrent GRU [10] network for pose and depth estimation. Based on 3D-GS, GGRT [28] showed first pose-free generalizable method by employing self-supervised learning. However, these methods face practical limitations due to their reliance on pretrained models [41], the need for additional fine-tuning stages [6, 28], and the computationally intensive volume rendering process [6, 41], all of which hinder their scalability and efficiency in real-world applications. Also, CoPoNeRF [21] provides poses and radiance fields estimation at the inference stage, it still requires ground-truth pose supervision during training. In contrast, our method can reconstruct 3D scenes and synthesize novel views from unposed images, mitigating the preceding challenges, and offering more scalable and efficient solution. 2.2. Self-supervised Learning for 3D Vision Masked Autoencoder (MAE) [19, 47] is one of selfsupervised representation learning framework on video datasets, leveraging their consistency in space and over The main objective of MAE is to reconstruct time. masked patch of pixels or latent features, thereby learning spatiotemporal continuity without any 3D inductive bias. Recently, CroCo [52, 53], cross-view completion method which extends previous single-view approaches, has demonstrated pretraining objective well-suited for geometric downstream tasks, such as optical flow and 2 Figure 1. Overview of SelfSplat. Given unposed multi-view images as input, we predict depth and Gaussian attributes from the images, as well as the relative camera poses between them. We unify self-supervised depth estimation framework with explicit 3D representation achieving accurate scene reconstruction. stereo matching. Expanding on this, DUSt3R [50] and MASt3R [27] introduce novel paradigm for dense 3D reconstruction from multi-view image collections. Another area of self-supervised learning for 3D vision is monocular depth estimation. Without ground-truth depth and camera pose annotations, they utilized the information from consecutive temporal frames using warped image reconstruction as signal to train their networks. Starting with [65], which first introduced the method, and subsequent works [1, 8, 17] have developed upon this field. In this paper, we also follow the framework of self-supervised depth estimation, but different from previous methods, we combine 3D representation learning, which improves the depth estimation and enables novel view synthesis with resulting 3D scene representations. 3. Preliminary and ω is hyperparameter that controls the weighting factor between them [17]. 3.2. Feed-forward 3D Gaussian Splatting Feed-forward 3D Gaussian Splatting methods infer 3D scene structure from input images through single network evaluation, predicting Gaussian attributes based on pixel-, feature-, or voxel-level tensors. Each Gaussian, defined as gj = (µj, αj, Σj, cj), includes attributes such as mean µj, covariance Σj, an opacity αj, and spherical harmonics (sh) coefficients cj. In particular, our framework adopts pixel-aligned approach, predicting per-pixel Gaussian primitives along with accurate depth estimations, achieving high-quality 3D reconstruction and fast novel view synthesis. Given multiple input views, the model generates pixel-aligned Gaussians for each image, and combine them to represent the full 3D scene [3, 44]. 3.1. Self-supervised Depth and Pose Estimation 4. Methods The self-supervised depth and pose estimation method is geometric representation learning method from videos or unposed images, which does not require ground-truth depth and pose annotations [60, 65]. Typically, two separate networks are employed for each depth and pose estimation, though these networks may share common representations. Given triplet of consecutive frames Ic1, It, Ic2 RHW 3, the pose network predicts the relative camera pose between two frames and the depth network produces the depth maps for each frame. While there exist many variants, typical loss function, Lproj, to train two networks is Lproj = pe(It, Ic1t) + pe(It, Ic2t), pe(Ia, Ib)= ω (1SSIM(Ia, Ib))+(1 ω) IaIb1 , (1) (2) where Ic1t RHW 3 denotes the projected image from Ic1 onto It using the predicted camera pose and the depth map. pe(, ) is photometric reconstruction error, usually calculated using combination of L1 and SSIM [51] losses, 4.1. Self-supervised Novel View Synthesis We begin with triplet of unposed images, Ic1 , It, Ic2 RHW 3, which are taken from different viewpoints. Building on the recent pixel-aligned 3D Gaussian Splatting methods, our goal is to predict dense per-pixel Gaussian parameters from input view images, Gc1, Gc2 = fθ(Ic1, It, Ic2), (3) where fθ is feed-forward network with learnable parameters θ, and Gc1 = {(µj, αj, Σj, cj)}HW j=1 is generated Gaussians for the input image Ic1. Note that we only generate pixel-aligned Gaussians for two input views Ic1 and Ic2 while excluding the target view It. This design encourages the network to generalize to novel views It durIn addition, we train pose network fϕ ing training. to estimate relative transformation between two images, Tc1c2 = fϕ(Ic1 , Ic2 ), where Tc1c2 SE(3) consists of rotation, Rc1c2 R33, and translation, tc1c2 R31, between two images, Ic1 and Ic2. We utilize the estimated 3 Figure 2. Matching-aware pose network (a) and depth refinement module (b). We leverage cross-view features from input images to achieve accurate camera pose estimation, and use these estimated poses to further refine the depth maps with spatial awareness. camera poses to transform the Gaussian positions in each frames local coordinate system into an integrated global space. Then, we construct the 3D Gaussian representations for scene by union of the generated Gaussians as follows, = TR(Gc1, Tc1t) TR(Gc2, Tc2t), (4) where TR(Gc1, Tc1t) transforms the generated Gaussian Gc1 into the Its coordinate system, and is the final 3D Gaussians that are used to render images. The final loss function to jointly train both fθ and fϕ is defined as follows, Ltotal = λ1Lproj + λ2Lren, (5) γ1(1SSIM(Ik, ˆIk))+γ2Ik ˆIk2, (6) Lren= (cid:88) Ik{Ic1 ,Ic2 ,It} where Lproj is the reprojection loss (Eq. (2)) and Lren is the rendering loss that computes the error between input view images, Ik, and the rendered images, ˆIk, from the constructed Gaussians G. Note that in Lproj, we use the rendered depth for It to maintain consistent scale with estimated depth maps from the context images, Ic1 and Ic2. In accordance with the prior pose-free generalizable methods, we assume that the camera intrinsic parameters are given from the camera sensor metadata [6, 21, 28, 48]. 4.2. Architecture As illustrated in Fig. 1, the proposed SelfSplat consists of four components: multi-view and monocular encoder, fusion and dense prediction block, matching-aware pose estimation network, and Gaussian decoder. Multi-view and monocular encoder. For multi-view feature extraction from input view images, we begin by processing each image independently through weight-sharing CNN architecture, followed by multi-view Transformer to exchange information across different views. Specifically, ResNet-like architecture [18] is used to extract 4x downsampled features for each view. These features are then refined by six-block Swin Transformer [33], which utilizes efficient local window selfand cross-attention mechanisms. The resulting cross-view-aware features are denoted , where mv is the dimenas mv c1 sion. These features are subsequently processed to generate , mv 4 Cmv 4 Gaussian attributes for rendering. As discussed in Sec. 4.1, since we do not generate Gaussian attributes for It, It is excluded from the feature extraction in this module. c1 16 16 Cmono Despite substantial advancements in multi-view feature matching-based depth estimation methods, such as those leveraging epipolar sampling [3, 20] or plane-sweep techniques [4, 7, 58], these approaches continue to face challenges in handling occlusions, texture-less regions, and reflective surfaces. To address these limitations, we incorporate monocular feature extractor, which has demonstrated robust performance across various downstream tasks [12, 46]. Specifically, we utilize shared-weight Vision Transformer (ViT) model, CroCo [52, 53], as monocular feature extractor. More specifically, input images are divided into non-overlapping patches with patch size of 16 and processed by multi-head self-attention blocks and feed-forward networks in parallel. Then, we obtain robust monocular , mono Transformer features mono , where c2 mono denotes the channel dimension. Similar to the multiview feature extraction, we do not extract the monocular feature from It. It is important to note that, unlike previous methods [55, 62] which use pretrained DepthAnything [57] model as ViT backbone and thus incorporate 3D priors, we employ CroCov2 [53] weights, allowing us to maintain fully self-supervised framework Feature fusion and dense prediction. To achieve consistent and fine-grained prediction of Gaussian primitives, we combine the multiand single-view features, leveraging complementary information from both perspectives to enhance depth accuracy and robustness in complex scenes. We build our feature fusion block with Dense Prediction Transformer (DPT) [36] module. As the spatial resolutions between the two features are different, we first downsample the multi-view features by four to match with monocular ones. Then, CNN-based pyramidal architecture [30] is adopted to produce features at four different levels. Four intermediate outputs are pulled out from the encoder blocks for the monocular features. These are then simply concatenated at each level and used to produce dense predictions through combination of reassemble and fusion blocks. Given the merged features, cat c1 , cat c2 , we utilize two branches of dense prediction module, one for the depth of 3D Gaussians, DPTdepth, and the other for the remaining Gaussian attributes DPTg, Dk = DPTdepth(F cat ), Gk = DPTg(F cat ), {c1, c2}, (7) where Gk = {(δxj, δyj, αj, Σj, cj)}HW j=1 is set of the predicted Gaussians with all attributes except coordinates and Dk is the predicted depth, further processed by the depth refinement module. δxj and δyj are the predicted offsets for each Gaussian, and the Gaussians for each input image in its coordinate system, Gc1 , Gc2 , can be obtained by adding the offsets to the pixel coordinates and unprojecting to 3D space using the refined depth Dc1, Dc2 . Then, we construct the unified Gaussians by transforming them into target coordinate system with the predicted poses (Eq. (4)). Matching-aware pose estimation. To enable high-quality rendering and reconstruction, it is crucial to predict accurate camera poses since it defines the transformation in 3D space. We begin by employing the CNN-based pose network from previous studies [17, 25] and introduce our matching-awareness module as novel encoding strategy. As shown in Fig 2-(a), we use 2D U-Net [40] with crossattention blocks to extract multi-view aware features from unposed images. Unlike the dense prediction module, we also incorporate the target view It as input and predict the relative camera poses for (Ic1, It) and (Ic2, It). First, the matching network processes the triplet, ma , ma , ma c2 = MatchingNet(Ic1, It, Ic2), (8) RHW 3, have where the matching aware features, ma the same sizes as input images and inject these features into the pose network. Tc1t = PoseNet([F ma ; Ic1; Eint], [F ma ; It; Eint]), (9) where [; ; ] concatenates input tensors along with the channel dimension, and Eint RHW 3 is ray embedding of camera intrinsic matrix for scale-awareness. More specifx,y = 1p(x, y) R3, where R33 is ically, Eint camera intrinsic matrix and p(x, y) R3 is homogeneous coordinate of pixel coordinate x, y. Note that the camera intrinsic parameters vary for different scenes but remain the same across different input views within the same scene. Pose-aware depth refinement. In this module, we refine the estimated depth map, Dc1 , Dc2 , derived from the dense prediction module, DPTdepth, to improve the quality of rendering and reconstruction. The initial depth estimation, Dc1, Dc2 , yields inconsistent estimation between input views that negatively impact the overall accuracy of the reconstruction, e.g., incorrectly overlapping Gaussians. To resolve the limitation, we propose our refine module that leverages cross-view information with spatial awareness. While few recent works have proposed depth refinement approaches [7, 62], our method uniquely differs by utilizing the predicted camera pose as additional information to resolve inconsistencies in the estimated depths across multiple input views. We employs lightweight 2D U-Net, which takes current depth predictions, input images, and estimated poses as input and outputs residual depths for each view. The operation is defined as follows, Dc1, Dc2 = Refine([ Dc1; Ic1; Eext(Tc1t)], [ Dc2 ; Ic2 ; Eext(Tc2t)]), (10) where Dk is the residual for each view depth, and the final depth Dk = Dk + Dk is obtained by adding the residual to the initial depth estimation for each view. Similar to our pose estimation module, there are cross-attention blocks in U-Net, and we utilize Plucker ray embedding to densely encode our estimated pose into higher-dimensional representation space, e.g., Eext(Tc1t) RHW 6 (Fig. 2-(b)). r 3 / r - P u i / p s VAE [25] DBARF [6] FlowCAM [41] CopoNeRF [21] SelfSplat (Ours) 2 1 Table 1. Baseline attributes compared to our proposed method.1 CopoNeRF offers pose-free inference, but requires ground-truth pose supervision during training. 2 DBARF is trained from the pretrain generalizable NeRF, IBRNet [49] that has 3D prior. 5. Experiments 5.1. Experiment Setup We train and evaluate our model on three large-scale datasets: RealEstate10K (RE10k) [66], ACID [32], and DL3DV [31], which include diverse indoor and outdoor real estate videos, aerial outdoor nature scenes, and diverse realworld videos, respectively. For RE10k, we use 67,477 training and 7,289 testing scenes; for ACID, 11,075 training and 1,972 testing scenes, consistent with previous works [3, 7]. Lastly, for DL3DV, we use subsets of the dataset amounting to 2,000 scenes for training and testing on 140 benchmark scenes. We assess our models performance in reconstructing intermediate video frames between two context frames. Baselines. We compare our model against existing posefree generalizable novel view synthesis methods, including VAE [25], DBARF [6], FlowCAM [41], and CoPoNeRF [21], on two different tasks: novel view synthesis and 5 Average Small Medium Large RE10k Method VAE [25] DBARF [6] FlowCAM [41] CoPoNeRF [21] Ours PSNR 20.65 12.57 22.29 21.03 24. SSIM LPIPS 0.643 0.494 0.711 0.693 0.813 0.325 0.474 0.313 0.256 0.188 PSNR 18.78 10.48 20.74 19.70 21.63 SSIM LPIPS 0.585 0.497 0.679 0.670 0.749 0.414 0.522 0.375 0.348 0.257 PSNR 20.56 12.39 22.15 20.99 24. SSIM LPIPS 0.646 0.487 0.709 0.695 0.820 0.319 0.475 0.313 0.285 0.181 PSNR 23.13 15.55 24.58 22.70 27.25 SSIM LPIPS 0.701 0.513 0.754 0.715 0.864 0.241 0.415 0.241 0.217 0.132 Table 2. Quantitative results of novel view synthesis on RE10k dataset. Average Small Medium Large ACID Method VAE [25] DBARF [6] FlowCAM [41] CoPoNeRF [21] Ours PSNR 24.02 15.48 25.59 23.30 26.71 SSIM LPIPS 0.666 0.572 0.721 0.668 0.801 0.287 0.397 0.294 0.278 0.196 PSNR 23.76 13.87 25.37 22.92 25.65 SSIM LPIPS 0.679 0.618 0.730 0.692 0.776 0.306 0.422 0.312 0.304 0. PSNR 24.21 15.51 25.64 23.42 26.87 SSIM LPIPS 0.664 0.584 0.714 0.667 0.797 0.295 0.398 0.305 0.283 0.198 PSNR 23.99 16.72 25.73 23.45 27.33 SSIM LPIPS 0.658 0.521 0.723 0.649 0.826 0.263 0.378 0.267 0.251 0. Table 3. Quantitative results of novel view synthesis on ACID dataset. Figure 3. Qualitative comparison of novel view synthesis on RE10k (top two rows) and ACID (bottom row) datasets. relative camera pose estimation. We train all methods, including ours, using the same training curriculum, where the frame distance between context views increases gradually. We also provide an attribute overview in Tab. 1, showing the distinct features of our proposed method. Evaluation metrics. For novel view synthesis, we use standard evaluation metrics: PSNR, SSIM, and LPIPS. Pose estimation is evaluated based on geodesic rotation and translation angular error, following the approach in [21]. For RE10k and ACID, we categorize test set context pairs by image overlap ratios to evaluate performance across small (0.05-0.6%), medium (0.6-0.8%), and large (0.8%+) overlap, identified by pretrained dense image matching method [14]. For DL3DV, overlap categories are defined by frame intervals between context images: 6 frames for large and 10 frames for small overlap. Implementation details. We employ the encoder part of pretrained CroCo [53] model as our monocular encoder, which is trained in self-supervised manner, and utilized adapter [5] block designed to efficiently adapt pretrained ViT models to downstream tasks. For the Gaussian rasterizer, we implement it using gsplat [59], an open-source library for Gaussian Splatting [24], offering efficient computation and memory usage. We train RE10k and ACID with 256 256 resolution, and for DL3DV, we use 256 448 to accommodate the wider view in our experiments. For 6 Method VAE [25] DBARF [6] FlowCAM [41] CoPoNeRF [21] Ours Method VAE [25] DBARF [6] FlowCAM [41] CoPoNeRF [21] Ours Average Small Medium Large Rotation Translation Rotation Translation Rotation Translation Rotation Translation RE10k Avg. Med. Avg. Med. Avg. Med. Avg. Med. Avg. Med. Avg. Med. Avg. Med. Avg. Med. 132.181 3.859 35.788 2.471 19.328 1.438 6.232 0.839 3.799 0.750 123.840 44.500 27.179 10.487 7. 115.746 36.069 22.524 9.175 9.095 113.598 24.308 15.917 5.538 4.438 112.444 32.749 19.889 8.431 7.593 109.959 20.899 14.355 5.179 3.976 118.211 38.292 26.044 10.173 14.954 116.226 25.822 19.583 6.047 7. 6.971 4.771 2.483 1.281 1.523 6.936 3.009 1.741 0.830 0.701 1.127 1.222 0.801 0.511 0.344 0.525 0.914 0.745 0.326 0.206 3.037 1.440 1.137 0.583 0.346 3.595 2.043 1.264 0.784 0. 2.818 1.471 1.160 0.587 0.362 Table 4. Quantitative results of pose estimation on RE10k dataset. Average Small Medium Large Rotation Translation Rotation Translation Rotation Translation Rotation Translation ACID Avg. Med. Avg. Med. Avg. Med. Avg. Med. Avg. Med. Avg. Med. Avg. Med. Avg. Med. 87.526 1.631 33.672 1.975 91.248 3.846 10.578 1.058 3.828 0.981 91.187 48.693 101.074 19.694 12.592 85.372 49.396 91.262 19.617 16. 86.180 49.906 92.786 19.888 16.329 79.987 95.399 82.976 9.757 4.535 77.761 36.702 81.019 9.401 4.518 72.060 36.895 76.659 9.449 5.681 81.056 52.275 84.599 20.572 21.631 2.789 2.899 4.787 1.731 1. 0.806 1.373 2.548 0.575 0.386 0.355 0.745 2.168 0.261 0.134 0.497 0.751 3.001 0.365 0.219 1.568 1.879 4.297 1.025 0.952 0.617 1.060 3.574 0.503 0.362 0.461 0.860 2.819 0.354 0. Table 5. Quantitative results of pose estimation on the ACID dataset. Figure 4. Qualitative comparison of novel view synthesis on DL3DV dataset. Small (10 frame) Large (6 frame) DL3DV Method FlowCAM [41] Ours PSNR 21.01 21.91 Rotation SSIM LPIPS Avg. Med. Avg. Med. 16.432 0.608 5.164 0.723 22.385 9.681 0.411 0.279 1.011 0.573 1.138 0. Translation PSNR 23.52 24.82 Rotation SSIM LPIPS Avg. Med. Avg. Med. 18.214 0.710 6.998 0.822 28.681 12.057 0.314 0.200 0.626 0. 0.705 0.634 Translation Table 6. Quantitative results of novel view synthesis and pose estimaion on DL3DV dataset. additional details, consult the supplementary material. 5.2. Results Novel view synthesis. We report quantitative results in Tab. 2, 3 and qualitative results in Fig. 3 for RE10k and ACID datasets, while Tab. 6 and Fig. 4 for DL3DV dataset. Our method outperforms the baselines on all metrics, especially in terms of perceptual distance. These observations can be further confirmed by the rendering results that our method effectively captures fine details of 3D structure. Relative pose estimation. Tab. 4, 5, and 6 present the quantitative results for camera pose estimation between two images across the datasets. Our approach consistently achieves lower errors in both average and median deviations, highlighting its accuracy and robustness. The qualitative results in Fig. 5, visualizing epipolar lines from the estimated poses also demonstrates the effectiveness of our approach in capturing accurate geometric alignments. Cross-Dataset Generalization. To evaluate the generalization performance on out-of-distribution scenes, we train the models on RE10k (ACID) dataset and test them on ACID (RE10k) dataset without additional finetuning. As shown in Tab. 7, SelfSplat outperforms previous methods on unseen datasets, demonstrating robust generalization capabilities. 5.3. Ablations and Analysis We provide quantitative and qualitative results on ablations studies in Tab. 8 and Fig. 6. All methods are trained for 7 RE10k ACID ACID RE10k Method FlowCAM [41] CoPoNeRF [21] Ours PSNR 25.31 23.56 26.60 Rotation SSIM LPIPS Avg. Med. Avg. Med. 21.135 0.715 8.642 0.683 5.864 0.793 32.316 19.620 18.607 1.074 0.619 0.249 1.607 2.301 1.119 0.297 0.287 0. Translation PSNR 21.13 18.89 21.65 Rotation SSIM LPIPS Avg. Med. Avg. Med. 63.667 0.667 13.572 0.607 10.228 0.728 74.873 20.435 17.993 4.426 4.159 1.618 0.329 0.364 0. 3.995 2.893 0.867 Translation Table 7. Cross-dataset generalization. We train the models on RE10k (ACID) dataset and directly evaluate on ACID (RE10k) dataset. and translation, R31, in camera poses. The rendering loss gradients with respect to translation and rotation are: δLren δt (cid:88) = δLren δ µj , δLren δR (cid:88) = (µjt) R, (11) δLren δ µj Figure 5. Epipolar lines visualization. We draw the lines from reference to target frame using relative camera pose. 50,000 iterations on RE10k dataset for fair comparison. Importance of matching awareness in pose estimation. To measure the importance of adopting cross-view features in our pose network, we conduct study (No Matching awareness) by removing it from the pose network. Quantitatively, it leads to drop in pose metrics: translation error increases by 1.5 degrees, which also negatively impacts rendering scores, decreasing PSNR by 0.4 dB. These results highlight that our encoding methods with multi-view awareness help capture relationships between frames, improving both pose estimation and novel view synthesis. Importance of depth refinement module. We conduct study (No Depth Refine) on our depth refinement module to validate its effectiveness. The results indicate clear decline across all metrics: PSNR drops by 0.6 dB, and translation discrepancy increases by 1.1 degrees. Additionally, misalignment of overlapping Gaussians leads to degrading in visual quality, such as motion blur artifacts, demonstrating that our refinement scheme enhances the multi-view consistency of depth predictions. How self-supervised depth estimation method and 3DGS representation can make reciprocal improvement? We explore the benefits of combining self-supervised depth estimation with explicit 3D representation by comparing SelfSplat with two variants (No Reprojection Loss, No Rendering Loss). Training without reprojection loss shows significant performance decline across all metrics, particularly 1.5 dB drop in PSNR, indicating challenges in accurately positioning Gaussiansa crucial factor for precise 3D reconstruction and novel view synthesis. In the No Rendering Loss variant, we replaced the rendered depth of the target view previously used in reprojection loss with an estimated depth map from the image using dense prediction module. To validate the impact of incorporating 3DGS, we also account for gradients of rotation, R33, where µj is splatted Gaussian in rendering viewspace. Excluding rendering loss results in degraded pose metrics, common issue in self-supervised depth estimation methods with limited image overlap. Our framework effectively addresses this by combining explicit 3D-GS representation with rendering loss, improving depth and pose estimation. Figure 6. Ablation studies on our proposed component. Method PSNR SSIM LPIPS Rot. Avg. Trans. Avg. Ours No Matching Network No Depth Refine No Reprojection Loss No Rendering Loss 22.65 22.21 22.05 21.12 - 0.764 0.735 0.744 0.672 - 0.222 0.241 0.224 0.293 - 1.036 1.308 1.064 2.236 8.581 13.705 15.171 14.8 28.584 64.436 Table 8. Ablations. Our methods achieves better alignment of 3D Gaussians, with accurate pose and consistent depth estimations. 6. Conclusion We present SelfSplat, pose-free generalizable 3D Gaussian Splatting model that does not require pretrained 3D priors or an additional fine-tuning stage. Our method effectively integrates 3D-GS representation with selfsupervised depth estimation techniques to recover 3D geometry and appearance from unposed monocular videos. We conduct extensive experiments on diverse real-world datasets to demonstrate its effectiveness, showcasing its ability to produce photorealistic novel view synthesis and accurate camera pose estimation. We believe that SelfSplat represents significant step forward in 3D representation learning, offering robust solution for various applications. Limitations. While we demonstrate high-quality 3D geometry estimation in this work, the current framework still possesses limitations. First, further technical improvements are needed to support wider baseline scenarios, such as 360 scene reconstruction from unposed images in single forward pass. Second, our framework struggles with dynamic scenes where both camera and object motion are present. Addressing these complex scenarios may benefit from incorporating multi-modal priors [39, 43] for robust and consistent alignment across wide and dynamic scene space."
        },
        {
            "title": "References",
            "content": "[1] Jiawang Bian, Zhichao Li, Naiyan Wang, Huangying Zhan, Chunhua Shen, Ming-Ming Cheng, and Ian Reid. Unsupervised scale-consistent depth and ego-motion learning from monocular video. Advances in neural information processing systems, 32, 2019. 3 [2] Wenjing Bian, Zirui Wang, Kejie Li, Jia-Wang Bian, and Victor Adrian Prisacariu. Nope-nerf: Optimising neuIn Proceedings of ral radiance field with no pose prior. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 41604169, 2023. 2 [3] David Charatan, Sizhe Lester Li, Andrea Tagliasacchi, and Vincent Sitzmann. pixelsplat: 3d gaussian splats from image In Propairs for scalable generalizable 3d reconstruction. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1945719467, 2024. 1, 2, 3, 4, 5 [4] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast generalizable radiance field reconstruction from multi-view stereo. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1412414133, 2021. 4 [5] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and Ping Luo. Adaptformer: Adapting vision transformers for scalable visual recognition. Advances in Neural Information Processing Systems, 35:1666416678, 2022. 6 [6] Yu Chen and Gim Hee Lee. Dbarf: Deep bundle-adjusting In Proceedings of generalizable neural radiance fields. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2434, 2023. 1, 2, 4, 5, 6, [7] Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei Cai. Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images. arXiv preprint arXiv:2403.14627, 2024. 2, 4, 5 [8] Boris Chidlovskii and Leonid Antsfeld. Self-supervised pretraining and finetuning for monocular depth and visual odometry. arXiv preprint arXiv:2406.11019, 2024. 3 [9] Boris Chidlovskii and Leonid Antsfeld. Self-supervised pretraining and finetuning for monocular depth and visual odometry. arXiv preprint arXiv:2406.11019, 2024. 2 [10] Kyunghyun Cho. On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259, 2014. 2 [11] Christopher Choy, Wei Dong, and Vladlen Koltun. Deep In Proceedings of the IEEE/CVF conglobal registration. ference on computer vision and pattern recognition, pages 25142523, 2020. [12] Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 4 [13] Yilun Du, Cameron Smith, Ayush Tewari, and Vincent Sitzmann. Learning to render novel views from wide-baseline In Proceedings of the IEEE/CVF Conference stereo pairs. on Computer Vision and Pattern Recognition, pages 4970 4980, 2023. 1 [14] Johan Edstedt, Qiyu Sun, Georg Bokman, Marten Wadenback, and Michael Felsberg. Roma: Robust dense feature matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19790 19800, 2024. 6 [15] Irving Fang, Kairui Shi, Xujin He, Siqi Tan, Yifan Wang, Hanwen Zhao, Hung-Jui Huang, Wenzhen Yuan, Chen Feng, and Jing Zhang. Fusionsense: Bridging common sense, vision, and touch for robust sparse-view reconstruction. arXiv preprint arXiv:2410.08282, 2024. 1 [16] Yang Fu, Sifei Liu, Amey Kulkarni, Jan Kautz, Alexei A. Efros, and Xiaolong Wang. Colmap-free 3d gaussian splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 20796 20805, 2024. 2 [17] Clement Godard, Oisin Mac Aodha, Michael Firman, and Gabriel Brostow. Digging into self-supervised monocular In Proceedings of the IEEE/CVF interdepth estimation. national conference on computer vision, pages 38283838, 2019. 2, 3, [18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. 4 [19] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000 16009, 2022. 2 [20] Yihui He, Rui Yan, Katerina Fragkiadaki, and Shoou-I Yu. In Proceedings of the ieee/cvf conEpipolar transformers. ference on computer vision and pattern recognition, pages 77797788, 2020. 4 [21] Sunghwan Hong, Jaewoo Jung, Heeseong Shin, Jiaolong Yang, Seungryong Kim, and Chong Luo. Unifying correspondence, pose and nerf for pose-free novel view synthesis from stereo pairs. arXiv preprint arXiv:2312.07246, 2023. 1, 2, 4, 5, 6, 7, 8 [22] Ying Jiang, Chang Yu, Tianyi Xie, Xuan Li, Yutao Feng, Huamin Wang, Minchen Li, Henry Lau, Feng Gao, Yin Yang, et al. Vr-gs: physical dynamics-aware interactive In ACM SIGgaussian splatting system in virtual reality. GRAPH 2024 Conference Papers, pages 11, 2024. 1 [23] Nikhil Keetha, Jay Karhade, Krishna Murthy Jatavallabhula, Gengshan Yang, Sebastian Scherer, Deva Ramanan, and Jonathon Luiten. Splatam: Splat track & map 3d gaussians for dense rgb-d slam. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2135721366, 2024. 2 [24] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 1, 6 [25] Zihang Lai, Sifei Liu, Alexei Efros, and Xiaolong Wang. Video autoencoder: self-supervised disentanglement In Proceedings of the of static 3d structure and motion. IEEE/CVF International Conference on Computer Vision, pages 97309740, 2021. 5, 6, 7 [26] Christoph Lassner and Michael Zollhofer. Pulsar: EffiIn Proceedings of cient sphere-based neural rendering. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14401449, 2021. 1 [27] Vincent Leroy, Yohann Cabon, and Jerˆome Revaud. GroundarXiv preprint ing image matching in 3d with mast3r. arXiv:2406.09756, 2024. [28] Hao Li, Yuanyuan Gao, Dingwen Zhang, Chenming Wu, Yalun Dai, Chen Zhao, Haocheng Feng, Errui Ding, Jingdong Wang, and Junwei Han. Ggrt: Towards generalizable 3d gaussians without pose priors in real-time. arXiv preprint arXiv:2403.10147, 2024. 1, 2, 4 [29] Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Simon Lucey. Barf: Bundle-adjusting neural radiance fields. In Proceedings of the IEEE/CVF international conference on computer vision, pages 57415751, 2021. 2 [30] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Feature pyraBharath Hariharan, and Serge Belongie. In Proceedings of the mid networks for object detection. IEEE conference on computer vision and pattern recognition, pages 21172125, 2017. 4 [31] Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, et al. Dl3dv-10k: large-scale scene dataset for deep learning-based 3d vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2216022169, 2024. 2, 5 [32] Andrew Liu, Richard Tucker, Varun Jampani, Ameesh Makadia, Noah Snavely, and Angjoo Kanazawa. Infinite nature: Perpetual view generation of natural scenes from sinIn Proceedings of the IEEE/CVF International gle image. Conference on Computer Vision, pages 1445814467, 2021. 2, 5 [33] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1001210022, 2021. [34] Nelson Max. Optical models for direct volume rendering. IEEE Transactions on Visualization and Computer Graphics, 1995. 1 [35] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 1 [36] Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. ViIn Proceedings of sion transformers for dense prediction. the IEEE/CVF international conference on computer vision, pages 1217912188, 2021. 4 [37] Adam Rashid, Satvik Sharma, Chung Min Kim, Justin Kerr, Lawrence Yunliang Chen, Angjoo Kanazawa, and Ken Goldberg. Language embedded radiance fields for zero-shot task-oriented grasping. In 7th Annual Conference on Robot Learning, 2023. 1 [38] Chris Rockwell, Justin Johnson, and David Fouhey. The 8point algorithm as an inductive bias for relative pose prediction by vits. In 2022 International Conference on 3D Vision (3DV), pages 111. IEEE, 2022. [39] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 9 [40] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241. Springer, 2015. 5 [41] Cameron Smith, Yilun Du, Ayush Tewari, and Vincent Sitzmann. Flowcam: training generalizable 3d radiance fields without camera poses via pixel-aligned scene flow. arXiv preprint arXiv:2306.00180, 2023. 1, 2, 5, 6, 7, 8 [42] Libo Sun, Jia-Wang Bian, Huangying Zhan, Wei Yin, Ian Reid, and Chunhua Shen. Sc-depthv3: Robust selfsupervised monocular depth estimation for dynamic scenes. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. 2 [43] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. arXiv preprint arXiv:2307.05222, 2023. 9 [44] Stanislaw Szymanowicz, Chrisitian Rupprecht, and Andrea Vedaldi. Splatter image: Ultra-fast single-view 3d reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10208 10217, 2024. 1, 2, 3 [45] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field In Computer VisionECCV transforms for optical flow. 2020: 16th European Conference, Glasgow, UK, August 23 28, 2020, Proceedings, Part II 16, pages 402419. Springer, 2020. [46] Hans Thisanke, Chamli Deshan, Kavindu Chamith, Sachith Seneviratne, Rajith Vidanaarachchi, and Damayanthi Herath. Semantic segmentation using vision transformers: survey. Engineering Applications of Artificial Intelligence, 126:106669, 2023. 4 [47] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training. Advances in neural information processing systems, 35:1007810093, 2022. 2 10 Hu, Matthew Tancik, et al. gsplat: An open-source library for gaussian splatting. arXiv preprint arXiv:2409.06765, 2024. 6 [60] Zhichao Yin and Jianping Shi. Geonet: Unsupervised learning of dense depth, optical flow and camera pose. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 19831992, 2018. 3 [61] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 45784587, 2021. 1 [62] Chuanrui Zhang, Yingshuang Zou, Zhuoling Li, Minmin Yi, and Haoqian Wang. Transplat: Generalizable 3d gaussian splatting from sparse multi-view images with transformers. arXiv preprint arXiv:2408.13770, 2024. 4, [63] Shuaifeng Zhi, Tristan Laidlow, Stefan Leutenegger, and Andrew Davison. In-place scene labelling and understanding In Proceedings of the with implicit scene representation. IEEE/CVF International Conference on Computer Vision, pages 1583815847, 2021. 1 [64] Shijie Zhou, Haoran Chang, Sicheng Jiang, Zhiwen Fan, Zehao Zhu, Dejia Xu, Pradyumna Chari, Suya You, Zhangyang Wang, and Achuta Kadambi. Feature 3dgs: Supercharging 3d gaussian splatting to enable distilled feature fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2167621685, 2024. 1 [65] Tinghui Zhou, Matthew Brown, Noah Snavely, and David Lowe. Unsupervised learning of depth and ego-motion from video. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 18511858, 2017. 3 [66] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, Learning arXiv preprint and Noah Snavely. view synthesis using multiplane images. arXiv:1805.09817, 2018. 2, 5 Stereo magnification: [48] Peng Wang, Hao Tan, Sai Bi, Yinghao Xu, Fujun Luan, Kalyan Sunkavalli, Wenping Wang, Zexiang Xu, and Kai Pf-lrm: Pose-free large reconstruction model Zhang. arXiv preprint for arXiv:2311.12024, 2023. joint pose and shape prediction. [49] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srinivasan, Howard Zhou, Jonathan Barron, Ricardo Martin-Brualla, Noah Snavely, and Thomas Funkhouser. IbrIn Pronet: Learning multi-view image-based rendering. ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 46904699, 2021. 5 [50] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20697 20709, 2024. 3 [51] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4): 600612, 2004. 3 [52] Philippe Weinzaepfel, Vincent Leroy, Thomas Lucas, Romain Bregier, Yohann Cabon, Vaibhav Arora, Leonid Antsfeld, Boris Chidlovskii, Gabriela Csurka, and Jerˆome Revaud. Croco: Self-supervised pre-training for 3d vision tasks by cross-view completion. Advances in Neural Information Processing Systems, 35:35023516, 2022. 2, 4 [53] Philippe Weinzaepfel, Thomas Lucas, Vincent Leroy, Yohann Cabon, Vaibhav Arora, Romain Bregier, Gabriela Csurka, Leonid Antsfeld, Boris Chidlovskii, and Jerˆome Revaud. Croco v2: Improved cross-view completion pretraining for stereo matching and optical flow. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1796917980, 2023. 2, 4, [54] Olivia Wiles, Georgia Gkioxari, Richard Szeliski, and Justin Johnson. Synsin: End-to-end view synthesis from single image. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 74677477, 2020. 1 [55] Haofei Xu, Songyou Peng, Fangjinhua Wang, Hermann Blum, Daniel Barath, Andreas Geiger, and Marc Pollefeys. Depthsplat: Connecting gaussian splatting and depth. arXiv preprint arXiv:2410.13862, 2024. 4 [56] Linning Xu, Vasu Agrawal, William Laney, Tony Garcia, Aayush Bansal, Changil Kim, Samuel Rota Bul`o, Lorenzo Porzi, Peter Kontschieder, Aljaˇz Boˇziˇc, et al. Vr-nerf: HighIn SIGGRAPH Asia fidelity virtualized walkable spaces. 2023 Conference Papers, pages 112, 2023. 1 [57] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1037110381, 2024. 4 [58] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. Mvsnet: Depth inference for unstructured multi-view stereo. In Proceedings of the European conference on computer vision (ECCV), pages 767783, 2018. 4 [59] Vickie Ye, Ruilong Li, Justin Kerr, Matias Turkulainen, Brent Yi, Zhuoyang Pan, Otto Seiskari, Jianbo Ye, Jeffrey"
        }
    ],
    "affiliations": [
        "Hanhwa Systems",
        "Korea University",
        "Sungkyunkwan University"
    ]
}