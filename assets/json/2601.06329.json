{
    "paper_title": "On the Fallacy of Global Token Perplexity in Spoken Language Model Evaluation",
    "authors": [
        "Jeff Chan-Jan Sju",
        "Liang-Hsuan Tseng",
        "Yi-Cheng Lin",
        "Yen-Chun Kuo",
        "Ju-Chieh Chou",
        "Kai-Wei Chang",
        "Hung-yi Lee",
        "Carlos Busso"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generative spoken language models pretrained on large-scale raw audio can continue a speech prompt with appropriate content while preserving attributes like speaker and emotion, serving as foundation models for spoken dialogue. In prior literature, these models are often evaluated using ``global token perplexity'', which directly applies the text perplexity formulation to speech tokens. However, this practice overlooks fundamental differences between speech and text modalities, possibly leading to an underestimation of the speech characteristics. In this work, we propose a variety of likelihood- and generative-based evaluation methods that serve in place of naive global token perplexity. We demonstrate that the proposed evaluations more faithfully reflect perceived generation quality, as evidenced by stronger correlations with human-rated mean opinion scores (MOS). When assessed under the new metrics, the relative performance landscape of spoken language models is reshaped, revealing a significantly reduced gap between the best-performing model and the human topline. Together, these results suggest that appropriate evaluation is critical for accurately assessing progress in spoken language modeling."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 9 ] . [ 1 9 2 3 6 0 . 1 0 6 2 : r a"
        },
        {
            "title": "On the Fallacy of Global Token Perplexity\nin Spoken Language Model Evaluation",
            "content": "Jeff Chan-Jan Sju, Liang-Hsuan Tseng, Yi-Cheng Lin, Yen-Chun Kuo Ju-Chieh Chou, Kai-Wei Chang, Hung-yi Lee, Carlos Busso Carnegie Mellon University, National Taiwan University, Toyota Technological Institute at Chicago, Massachusetts Institute of Technology chanjanh@andrew.cmu.edu, busso@cmu.edu"
        },
        {
            "title": "Abstract",
            "content": "Generative spoken language models pretrained on large-scale raw audio can continue speech prompt with appropriate content while preserving attributes like speaker and emotion, serving as foundation models for spoken dialogue. In prior literature, these models are often evaluated using global token perplexity, which directly applies the text perplexity formulation to speech tokens. However, this practice overlooks fundamental differences between speech and text modalities, possibly leading to an underestimation of the speech characteristics. In this work, we propose variety of likelihoodand generative-based evaluation methods that serve in place of naive global token perplexity. We demonstrate that the proposed evaluations more faithfully reflect perceived generation quality, as evidenced by stronger correlations with human-rated mean opinion scores (MOS). When assessed under the new metrics, the relative performance landscape of spoken language models is reshaped, revealing significantly reduced gap between the best-performing model and the human topline. Together, these results suggest that appropriate evaluation is critical for accurately assessing progress in spoken language modeling. Figure 1: Acoustic discontinuity disturbs negative log-likelihood loss (NLL) responses locally. Top: SALMon samples consist of shared prompt and separate continuation, where positive samples maintain acoustic consistency, and negative samples contain abrupt acoustic transitions. Bottom: NLL response of Llama-Mimi-1.3B with standard error margins. Response on negative samples show localized spike within short temporal window after the transition in contrast to the positive sample. Global token perplexity aggregates likelihood contributions outside this localized region (Sec. 2.2), making it susceptible to long-range loss volatility, thereby motivating our localized and normalized evaluation methods (Sec. 3.1, 3.2)"
        },
        {
            "title": "Introduction",
            "content": "Recent years have witnessed the emergence of assistant-style spoken dialogue systems that interact with users through speech (Open-AI; Chu et al., 2024; Défossez et al., 2024). Analogous to text language models, generative speech modeling is commonly formulated as sequence-to-sequence task (Sugiura et al., 2025; Chang et al., 2024). In this paradigm, speech waveforms are first discretized into sequences of tokens (Défossez et al., 2024; An et al., 2024); model predictions are then performed in the token space (Sugiura et al., 2025; Tseng et al., 2025); finally, the resulting audio is reconstructed from the predicted tokens (Du et al., 2024; Ju et al., 2024). We refer to this modeling framework as spoken language model (SLM), following the terminology of Arora et al. (2025). Similar to the development of text-based large language models (LLMs), SLMs are typically trained in two stages: large-scale pretraining on unlabeled speech data, followed by supervised fine-tuning on taskor instruction-specific datasets. During the pre-training stage, the model acquires foundational knowledge from raw data, which is directly reflected in its generation capabilities. Consequently, the quality of the pre-trained model lays the foundation for downstream performance (Raffel et al., 2020; Hassid et al., 2024). L n Seconds relative to transition (s) Figure 2: NLL response of various models on SALMon samples with standard error margins. High-scoring models on SALMon (e.g., Flow-SLM) exhibit localized NLL spikes for negative samples within short temporal window after the transition. This behavior is less apparent in lower-performing models such as GSLM. However, evaluating the generation quality of pretrained models remains challenging, as assessing whether generated output is plausible or coherent is inherently subjective. In text modeling, common practice is to adopt perplexity, likelihood-based metric that scores model predictions on textual token sequences. Likewise, pre-trained SLMs are often evaluated by computing perplexity over an entire sequence of discrete speech tokens. We refer to this metric as global token perplexity, which quantifies the sequence-level likelihood of speech signal in the discrete token space. In text modeling benchmarks, perplexity is typically interpreted in comparative setting (Brown et al., 2020), where each benchmark entry contrasts positive samples (i.e., fluent, coherent text) with negative samples (i.e., syntactically or semantically corrupted text). The models ability to assign higher likelihood to positive samples suggests systematic preference for well-formed outputs during generation. The principles in text modeling have been carried over to the benchmarking of pretrained SLMs, where sWUGGY (Dunbar et al., 2021), sBLIMP (Dunbar et al., 2021), and SALMon (Maimon et al., 2025) likewise adopt comparative evaluation design. Each benchmarks contrastive pairs highlights various aspects of speech, from context to acoustics. While effective, directly computing global token-level perplexity may overlook characteristics unique to speech, risking misalignment with human perception. From cognitive perspective, textual and acoustic generation processes demand different attentional spans. The Dependency Locality Theory (Gibson, 1998, 2000) and Surprisal Theory (Levy, 2008; Smith and Levy, 2013) suggest that generating and evaluating coherent text relies on tracking dependencies over long contexts, thus requiring long-range attentiveness. In contrast, non-semantic acoustic features evolve continuously over time and are regularized by the Gradualness of Change (Bregman, 1993), which favors shortspan conditioning. Likelihood patterns in Figure 1 and 2 echo this theoretical insights. The perplexity difference between positive and negative samples, when present, is short-spanned and concentrated near the onset of divergence. In this work, we introduce novel evaluation protocols of SLMs that address fundamental asymmetries between speech and text modeling and place greater emphasis on local context sensitivity. Specifically, we propose two families of evaluation methods and re-evaluated SLMs on SALMon. The first family, likelihood-based methods, reformulates perplexity through localization and normalization, preserving the likelihood-based evaluation paradigm while emphasizing local context sensitivity (Sec. 3.1, Sec. 5.1). The second family is generation-based methods, where we evaluate on real generations produced by the SLM, since successful generation inherently demands sensitivity to local context (Sec. 3.2, Sec. 5.2). New scores from these evaluation protocols yield substantially different conclusions from those based on naive global token perplexity. To assess perception-faithfulness of the proposed methods, we conduct human-subject rating experiments on SLM-generated samples and collected mean opinion scores (MOS), which serve as the gold-standard reference for SLM performance. Correlation analyses (Sec. 5.3) show that our proposed methods align more closely with human ratings (Pearson correlation: 0.64 0.8, Spearman correlation: 0.67 0.8), thereby establishing new paradigm for evaluating spoken language models. Our evaluation reshapes the performance landscape of SLMs: when evaluated correctly, the bestperforming model closes 83% of the gap to the human topline on SALMon, surprisingly achieving new SOTA."
        },
        {
            "title": "Language Models",
            "content": "likelihood-based evaluaFor pretrained SLMs, tion provides principled way to probe speech consistency across multiple dimensions. Prior work has largely emphasized the content aspect, where sWUGGY and sBLIMP (Dunbar et al., 2021) assesses semantic coherence and lexical wellformedness. With automatic speech recognition (ASR), text-level perplexity can also be computed (Lakhotia et al., 2021; Hassid et al., 2024; Wu et al., 2023). More recently, paralinguistic modeling capabilities of pretrained SLMs are receiving growing attention, and SALMon (Maimon et al., 2025) is established to measure the consistency of acoustic attributes. These include speaker identity, sentiment, background, and room conditions. In this paper, we focus on improving the evaluation of pretrained SLMs specifically on acoustic attributes."
        },
        {
            "title": "Modeling",
            "content": "Likelihood modeling offers principled approach to evaluate sequence models. In conventional practice, likelihood modeling is assessed via perplexity, where higher perplexity indicates lower likelihood under the model. Given sequence s, perplexity is defined as the exponential of the negative loglikelihood loss (NLL): PPL(s) = exp(NLL(s)). With this transformation, we will henceforth report perplexity in terms of NLL. In the context of SLMs, perplexity is computed over sequences of discrete speech tokens (Maimon et al., 2025; Sugiura et al., 2025; Chou et al., 2025). We refer to this formulation as global token perplexity. Formally, given sequence = (x1, . . . , xT ), the NLLglobal formula yields NLLglobal(s) = 1 (cid:80)T t=1 log p(xt x<t) (1) Modern discrete speech units often consist of multiple codebooks, as in hierarchical residual vector quantization (RVQ) architectures (Zeghidour et al., 2021; Wang et al., 2023a; Défossez et al., 2022, 2024) or are multi-channeled, as in joint speech-text modeling (Tseng et al., 2025). In such settings, we flatten the multi-channel likelihood outputs into single serialized stream. This formulation treats all tokens across channels and time steps as equally informative, yielding unified, holistic likelihood estimate for the entire speech signal. With this definition, adopting perplexity to benchmarking is straightforward. In SALMon, each sample consists of pair of sequences (sp, sn), corresponding to the positive (preferred) sequence and the negative (dispreferred) sequence. perception-faithful model is expected to assign lower perplexity to sp than to sn. This comparison is equivant in the NLL space as NLL is strictly monotonic transformation of perplexity: NLL(sp) < NLL(sn) (2)"
        },
        {
            "title": "2.3 Likelihood Modeling Calibration",
            "content": "A fundamental principle of likelihood modeling is that models predicted probabilities should align with the frequencies of occurrence of the global distribution (Kadavath et al., 2022; Ulmer et al., 2022). When raw prediction scores deviate, calibration methods can be introduced to increase alignment. Certain calibration approaches, such as Platt scaling (Platt, 1999; Guo et al., 2017), are monotonic and rank preserving. In contrast, other methods aim to alter the models selection, including option debiasing (Brown et al., 2020), option finetuning (Guo et al., 2017), and decoding-time interventions (Chuang et al., 2024). In this work, we extend calibration efforts to SLMs by proposing localization and normalization methods that better align with the characteristics of speech."
        },
        {
            "title": "3 Proposed Method",
            "content": "Using global token perplexity to assess acoustic consistency can lead to measurement fallacy that do not correlate with human assessments. Empirically, prior work have shown that perplexity in speech is often disproportionately influenced by semantic factors (Maimon and Adi, 2023; Sicherman and Adi, 2023; Polyak et al., 2021), limiting the expressiveness of acoustic features. These semantic contributions make substantial part of global token perplexity, which introduced noise in modeling and cascades into fluctuations during sample-wise comparisons. SALMon reduced semantic volatility by enforcing the same speech context between comparisons, but strict attributewise independence is neither well defined nor practically attainable in speech. We therefore derive alternative perplexity variants that focus on modeling the target attributethe axis along which the positive and negative speech samples are contrasted."
        },
        {
            "title": "3.1 Localized and Normalized",
            "content": "Likelihood-Based Evaluation We derive localized and normalized variants of token perplexity that improve upon the global token perplexity in Equation 1. In SALMon, each positivenegative pair shares common prefix. For each pair (sp, sn), we extract the longest common prefix as the prompt (S), the rest of the sequence become positive (P ) and negative (N ) responses respectively, analogous to QA-style setups in NLP benchmarks (Brown et al., 2020; Hendrycks et al.). Concretely, we write sp = and sn = , where denotes sequence concatenation. The localized variant of token perplexity only accounts information of localized window of length δ starting from the timeframe where the speech prompt ends (tp): NLLlocalized(s) = 1 δ (cid:80)tp+δ1 t=tp (cid:0) log p(xt x<t)(cid:1) (3) We also consider normalization, with deducts the unconditional probability of each response (Brown et al., 2020). Normalization applies to both global and localized perplexity by choosing the corresponding window {T tf +1, δ}. The normalized perplexity aggregates normalized probabilities at each time step. NLLnormalized(s) = 1 (cid:80)tp+1 t=tp (cid:0) log p(xtx<t) p(xtxtp:t) (cid:1) (4)"
        },
        {
            "title": "3.2 Generation-Based Evaluation",
            "content": "The evaluation methods in Sections 2.2 and 3.1 compute the likelihood of the samples rather than letting the SLM continue the speech. Here, we directly evaluate on continuations of SLMs given speech prompt S, which provides multiple benefits. With real continuations, it is possible to conduct human evaluations to obtain mean opinion scores (MOS), which provides perception-faithful estimate of model quality and can serve as the reference for the models true continuation performance. Second, by approximating human judgements with scores from model-as-a-judge, we obtain another evaluator candidate to compete with global token perplexity. Given speech prompt S, we define continuation sampled from model by = ( S). (5) For human evaluations, annotators assign quality score between 1 and 5 based on how good the generated continuation is relative to the positive continuation reference (P ). For scoring continuations with model-as-a-judge, success can be determined more straightforwardly using contrastive criteriona continuation is deemed correct if it is closer to the gold positive continuation than to the negative one. d(G, ) < d(G, ) (6) where d() denotes distance function. The following section describe the procedure for selecting qualified model to serve as an automatic judge, as well as the corresponding evaluation strategy to assess SLM-generated continuations (Sec. 3.3)."
        },
        {
            "title": "Continuations",
            "content": "To select an appropriate judge J, we require (i) labeled set with known correctness and (ii) model that assigns distance score to prompt response pair. Fortunately, the shared prompt and paired responses (P, ) in each contrastive example (sp, sn) provides an natrual labeled set with the following objective: d(S, ) < d(S, ). (7) We explore using embedding models as judge candidates, leveraging their inherent distance metric, taking inspiration from retrival systems (Feng et al., 2022): d(A, B) = 1 cos(cid:0)E(A), E(B)(cid:1) (8) where cos(, ) denotes cosine similarity and E() denotes the forward pass through the embedding model. Plugging Equation 8 into Equation 6 yields the explicit sample-wise objective for choosing as qualified judge. cos(cid:0)E(S), E(P )(cid:1) > cos(cid:0)E(S), E(N )(cid:1) (9) The aggregation of correct predictions over the development set yields the accuracy. An ideal judge would achieve perfect score; in practice, more realistic qualification threshold is the human accuracy on the same benchmark. We begin with comprehensive set of pretrained embedding models and narrow it to the best-performing model (over the qualification threshold), which we adopt as the final judge (J). The judge benchmarks continuations from SLM following Equation 6: (a1) cos(cid:0)J(G), J(P )(cid:1) > cos(cid:0)J(G), J(N )(cid:1), (10) where aggregation over the whole benchmark yields the accuracy score of the evaluated SLM."
        },
        {
            "title": "4 Experimental Setup",
            "content": "We adopt SALMon for all evaluations. SALMon includes 6 subsets that measure acoustic consistency in gender, speaker identity, sentiment, two background conditions, and room attributes. Each data point consists of positive and negative sample, where the negative sample has inconsistencies on one of the attribute. Our evaluations of SLMs cover GSLM (Lakhotia et al., 2021), TWIST (Hassid et al., 2024), pGSLM (Kharitonov et al., 2021), Spirit-LM (Nguyen et al., 2024), TASTE (Tseng et al., 2025), Flow-SLM (Chou et al., 2025), and Llama-mimi (Sugiura et al., 2025). We measure their performance under original and proposed methods. The localized window δ is set as 0.5s. We use the Prolific service to obtain MOS scores. We evaluate 50 samples over 9 models, which yields 450 generations to be evaluated. Each generation is independently assessed by five annotators on five-point Likert scale. The annotators are proficient English speakers, and they are fairly compensated for their time. See Appendix A.2.2 for the annotation guidelines provided to annotators. For model-as-a-judge, we consider diverse pool of embedding models trained with different objectives and datasets, including TITANET (Koluguri et al., 2022), CAM++ (Wang et al., 2023b), CLAP (Elizalde et al., 2023), and AudioSet-trained models (La Quatra et al., 2024). We use SALMon prompts as the dev set to select the best judge for each subset, and use the judge to obtain SLM performance scores from continuations G."
        },
        {
            "title": "5.1 Localized and Normalized",
            "content": "Likelihood-Based Evaluation We first examine how the proposed likelihoodbased estimators reshape the performance landscape of spoken language models. Figure 3 (a1) shows the average accuracy over all consistency benchmarks for each SLMmethod combination, (a2) (b) (c) Figure 3: Overall performance of spoken language models on consistency tasks. The x-axis shows model accuracy (score) under different evaluators: (a1) alternative likelihood estimators, (b) MOS, and (c) embeddingas-a-judge, where model color codes are shown in (a1) and shared among all plots. In (a1), we correlate scores from proposed methods against those from global token perplexity (Global-PPL); the horizontal spread highlights the discrepancy across evaluation methods. The alternative methods rate strong models more favorably than Global-PPL, substantially closing the gap to the human topline. In (a2), we correlate deviations from the proposed methods against Global-PPL scores. Deviations generally become larger at higher Global-PPL performance (blue), until it saturates due to the maximum performance ceiling (orange). Negative deviations exhibit similar trend in absolute magnitude, though this is less surprising since they are soft-bounded by distance to random baseline (green). Table 1: The best-performing embedding model on each task provides viable judge for evaluating continuation performance. In addition to surpassing the human baseline, four of the six models achieve near perfect performance. Sentiment Speaker Gender Bg (domain) Bg (rand.) Room"
        },
        {
            "title": "Selected embedding model\nClassifier performance\nHuman performance",
            "content": "TITANET 99.5 97.2 TITANET TITANET hubert-large-audioset hubert-large-audioset wav2vec2-large-audioset 100.0 91.5 100.0 98.6 86.5 83. 97.5 88.7 100.0 94.4 plotted against the corresponding accuracy measured by conventional global token perplexity. From the plot, it can be seen that the horizontal spread is quite large, indicating systematic disagreement between perplexity methods. The degree of disagreement is quantified in Figure 3 (a2), which reveals positive association between disagreement and model competence (measured by Global-PPL performance). The linearly regressed positive deviation (blue line) starts at +6.65% at around 50% accuracy, and increases to +9.16% at 80% accuracy, which is substantial. closer inspection shows that the shift is predominantly one-sided for each SLM configuration, reflecting stable bias toward either overor underestimation. These patterns are tightly linked to the underlying token type. Our proposed methods consistently assign lower scores to HuBERT-based SLMs (GSLM, TWIST, SpiritLM), while in contrast yielding higher scores for Mimi-based models (Flow-SLM, Llama-Mimi). pGSLM is the lone outlier among HuBERT-based models, likely due to its distinctive auxiliary training objective. In contrast, model families trained with the same recipe but varying only in scale exhibit little behavioral difference. We report the full set of scores in Appendix A.3.1."
        },
        {
            "title": "5.2 Generation-Based Evaluation",
            "content": "We conduct experiments on actual model continuations to gain better understanding of the generative abilities of SLMs."
        },
        {
            "title": "5.2.1 MOS Evaluations by Human Labelers",
            "content": "MOS evaluation results are presented in Figure 3 (b). Llama-mimi obtains the top score of 3.29, followed by Flow-SLM, while models using HuBERT tokens (GSLM, TWIST, SpiritLM, pGSLM) perform much worse. Full model-by-task results are reported in Appendix A.3.2 and suggest that stronger models improve primarily on speech-centric attributes (e.g., sentiment, speaker, and gender) as opposed to ambience-related information."
        },
        {
            "title": "5.2.2 Model-as-a-Judge for Measuring\nGeneration Consistency",
            "content": "Identification of suitable judge models. Table 1 shows the top performing embedding model on the SALMon dev set along with its accuracy scores, following Equation 9. Results reveals that the selected embedding models consistently outperforms human performance, and even reaching ceilinglevel performance (> 99%) in four out of six cases. Collectively, these results support the credibility of model-as-a-judge for the SALMon task using combination of TITANET, hubert-large-audioset, and wav2vec2-large-audioset. Detailed model-bytask results are provided in Appendix A.3.3. Generation consistency evaluated by qualified judge models. Figure 3 (c) show the result of evaluating model generations on SALMon using the selected judge models. Consistent with findings in likelihood estimators and MOS scores, the speech tokenizer is the most dominant factor for the performance difference. Most HuBERTbased models struggle to retain speech properties during continuation, obtaining performance close to random choice. On the other end of the spectrum, FLow-SLM and Llama-mimi exhibit strong performance, with scores in the vicinity of the human topline. Finally, TASTE generations perform relatively well, highlighting the importance of the adopting speaker vector during token-to-speech conversion. In Appendix A.3.3, we present verbose task-wise results, which further suggests that continuation failures principally arise from inadequate information being preserved during the speech encoding phase."
        },
        {
            "title": "5.3 Correlation between methods",
            "content": "We now have four likelihood-based evaluators and generation-based evaluator scored with an embedding judge model. To determine the metrics that are more faithful to human perception for this task, we correlate them to the true continuation performance, provided by the MOS scores. The top row of Figure 4 is comprehensive display of these correlations. Global token perplexity Figure 4: Correlation between proposed evaluation methods vs golden labels provided by either MOS scores (top), or embedding judge proxies (bottom). The figure shows effectiveness of perplexity normalization and localization. The validility of the embedding judges stem from high correlation with the MOS scores (top right). sets the baseline with Pearson score of 0.64 and Spearman of 0.67. Localized perplexity achieves higher scores of Pearson of 0.70 and Spearman of 0.71, which are further surpassed by normalized perplexity, where both Pearson and Spearman correlations improve to 0.80. Continuations scored by embedding judges obtain the highest Pearson score overall at 0.87, with slightly lower Spearman score of 0.76. The high Pearson correlation is best accounted for by the tighter dispersion of points toward high-performance regime. These results show that evaluation with global token perplexity is suboptimal, and that our proposed methods align better with human perception. In the bottom row of Figure 4, we replicate the analysis using the embedding-judge scores instead of the MOS scores, to conduct correlations with likelihood-methods on all models. The figure reinforces the conclusion that both normalized and localized methods are better indicators of true generation performance. Collectively, these results establish principled, scalable evaluation recipe for continuation quality. We conclude that continuation robustness is exceptionally assessed by applying embedding-judge scoring to true continuation samples, especially when the model is competent. Under tighter evaluation budgets, or when the goal is to isolate token performance that may not align with downstream generation quality, normalized perplexity are preferred over global token perplexity. Adopting normalized evaluation completely reshapes the performance landscape of SLMs. Most notably, the best-performing model Llama-Mimi improves from 80.92 to 90.33 closes 83% of the gap to the human topline on SALMon, achieving new SOTA score. Nevertheless, human inspection reveals that substantial room remains for these models in handling complex speech signals, underscoring the need for more rigorous benchmarks."
        },
        {
            "title": "5.4 A Tale of Two Models: How Loss\nComposition Shapes Performance",
            "content": "Building on our earlier discussion that acoustic quality can be decomposed into interpretable axes (e.g., speakerand background-related attributes), we expect the models NLL loss be governed by an analogous set of disentangleable components. Breaking the loss down by axis would quantify each attributes contribution and its effect on performance. Nonetheless, an explicit axis-wise decomposition of the loss remains difficult in practice, because the relevant factors are entangled within single embedding or even within individual tokens. Fortunately, we can consider Spirit-LM-Expressive and Llama-Mimi as analytical lenses for this study, since their token inventories are inherently functionally distinct. Their original works (Sugiura et al., 2025; Nguyen et al., 2024) already categorized them as reflecting semantic versus acoustic utility. Using these models as representative cases, we contrast composition profiles to account for the opposite effects of our alternative methods in Llama-Mimi and Spirit-LM-Expressive. We perform comprehensive combinatorial ablation across token types, enabling Shapley value analysis (Shapley et al., 1953) to quantify each token types Table 2: Shapley value decompositions for Spirit-LMExpressive and Llama-Mimi over token types (HuBERT ΦH , pitch ΦP , style ΦS) and layer groups (Φ0Φ3). The Shapley values of the primary tokens for the two models (ΦH and Φ0) shift in opposite directions under localization and normalization. Model Window Term Original Norm. Spirit-LM Expr. Llama-Mimi Global Localized (t = 0.5s) Global Localized (t = 0.5s) ϕH ϕP ϕS ϕH ϕP ϕS ϕ0 ϕ1 ϕ2 ϕ3 ϕ0 ϕ1 ϕ2 ϕ3 +9.9 +9.4 0.3 +8.8 +9.7 +1.3 +2.6 +13.9 +1. +5.4 +15.0 1.4 +9.6 +8.3 +12.5 +14.8 +9.8 +6.1 +6.2 +4.4 +11.1 +10.3 +14.5 +16.1 +9.4 +8.9 +6.2 +7.6 Figure 5: Composition of the average per-sample advantage, which is defined as the difference between the negative loss and the positive loss. Advantage differs across evaluation methods in both token-type composition and loss magnitude. marginal contribution. Table 2 exhibits the shapley contributions of each token type. For Llama-Mimi, tokens from different residual layers all contribute positively, with the largest contribution coming from residual layer 1 (ϕ1). Individual contributions are further amplified in normalized and localized settings. Figure 5 shows the overall advantage across methods, which is defined as the average loss difference between negative and positive samples. We further decompose this gap into token-level contribution profile, revealing which tokens drive the advantage. In Llama-Mimi, localization and normalization exhibit largely orthogonal effects: moving from global loss to transitioned-windowed loss roughly doubles the measured advantage, whereas normalization slightly shifts the relative contributions of individual token types. Overall, Shapley values increase across all token categories under these change. The case of SpiritLM is markedly different from Llama-mimi. In Table 2, HuBERT tokens are contributing to acoustic tasks more than the other tokens combined, despite being labeled as \"semantic\" token type. HuBERT Shapley values drop under localization, suggesting that non-trivial portion of the acoustic information they encode is only elicitable through long-range dependencies. Surprisingly, normalization further reduces HuBERTs Shapley values, suggesting large volatility in nonacoustic information. The advantage breakdown confirms that alternative methods do not yield greater overall gain, and the perceived benefit of amplifying pitch contributions is practically negligible, as HuBERT tokens remains equally crucial for capturing acoustic information. As we uncover the role of HuBERT tokens in Spirit-LM expressive, it becomes more clear why models that adopt HuBERT tokens (ϕH ) constantly has continuation underperforming global perplexity (Figure 3). The performance during global perplexity of HuBERT modeling depend greatly on long range dependency to identify the correctness, but continuation content happens at each instance and cannot rely on future information. Our proposed evaluations penalizes this behavior in agreement with continuation results measured by MOS scores."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we revisit the use of global token perplexity for evaluating SLMs. We introduce variety of likelihoodand generation-based evaluation methods, that highlight localization and normalization to better reflect key characteristics of speech. Correlations with MOS show that our proposed methods align more closely with human perception. Re-evaluating SLMs with our proposed methods shows that the previously best-performing model achieves gains that close 83% of the gap to the human topline on SALMon. Together, these findings reshape the SLM performance landscape and establish new evaluation paradigm for future studies."
        },
        {
            "title": "Limitations",
            "content": "We propose novel evaluation methods as alternatives to conventional global token perplexity. However, since these methods are still applied to existing benchmarks, their scope remains inherently constrained by the limitations of those benchmarks. For instance, SALMon does not systematically probe compounded variations (e.g., speaker changes under noisy background conditions), which restricts our ability to characterize SLM performance in such settingseven when using our improved evaluators."
        },
        {
            "title": "References",
            "content": "Keyu An, Qian Chen, Chong Deng, Zhihao Du, Changfeng Gao, Zhifu Gao, Yue Gu, Ting He, Hangrui Hu, Kai Hu, and 1 others. 2024. Funaudiollm: Voice understanding and generation foundation models for natural interaction between humans and llms. arXiv preprint arXiv:2407.04051. Siddhant Arora, Kai-Wei Chang, Chung-Ming Chien, Yifan Peng, Haibin Wu, Yossi Adi, Emmanuel Dupoux, Hung yi Lee, Karen Livescu, and Shinji Watanabe. 2025. On the landscape of spoken language models: comprehensive survey. Transactions on Machine Learning Research. Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael Auli. 2022. data2vec: general framework for self-supervised learnPreprint, ing in speech, vision and language. arXiv:2202.03555. Albert S. Bregman. 1993. 10auditory scene analysis: hearing in complex environments. In Thinking in Sound: The Cognitive Psychology of Human Audition. Oxford University Press. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, and 1 others. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901. Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jeannette N. Chang, Sungbok Lee, and Shrikanth S. Narayanan. 2008. IEMOCAP: Interactive emotional Language Redyadic motion capture database. sources and Evaluation, 42(4):335359. Kai-Wei Chang, Haibin Wu, Yu-Kai Wang, Yuan-Kuei Wu, Hua Shen, Wei-Cheng Tseng, Iu-thing Kang, Shang-Wen Li, and Hung-yi Lee. 2024. SpeechPrompt: Prompting speech language models for speech processing tasks. IEEE/ACM Transactions on Audio, Speech, and Language Processing. Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Xiangzhan Yu, and Furu Wei. 2022. Wavlm: Large-scale self-supervised pre-training for full stack speech processing. IEEE Journal of Selected Topics in Signal Processing, 16(6):15051518. Yiming Chen, Xianghu Yue, Chen Zhang, Xiaoxue Gao, Robby T. Tan, and Haizhou Li. 2024. Voicebench: Benchmarking LLM-based voice assistants. Preprint, arXiv:2410.17196. Ju-Chieh Chou, Jiawei Zhou, and Karen Livescu. 2025. Flow-slm: Joint learning of linguistic and acoustic information for spoken language modeling. arXiv preprint arXiv:2508.09350. Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, Chang Zhou, and Jingren Zhou. 2024. Qwen2-audio technical report. Preprint, arXiv:2407.10759. Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James R. Glass, and Pengcheng He. 2024. DoLa: Decoding by contrasting layers improves facIn The Twelfth tuality in large language models. International Conference on Learning Representations. Alexandre Défossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. 2022. High fidelity neural audio compression. Transactions on Machine Learning Research. Alexandre Défossez, Laurent Mazaré, Manu Orsini, Amélie Royer, Patrick Pérez, Hervé Jégou, Edouard Grave, and Neil Zeghidour. 2024. Moshi: speech-text foundation model for real-time dialogue. Preprint, arXiv:2410.00037. Zhihao Du, Qian Chen, Shiliang Zhang, Kai Hu, Heng Lu, Yexin Yang, Hangrui Hu, Siqi Zheng, Yue Gu, Ziyang Ma, Zhifu Gao, and Zhijie Yan. 2024. Cosyvoice: scalable multilingual zero-shot textto-speech synthesizer based on supervised semantic tokens. Preprint, arXiv:2407.05407. Ewan Dunbar, Mathieu Bernard, Nicolas Hamilakis, Tu Anh Nguyen, Maureen de Seyssel, Patricia Rozé, Morgane Rivière, Eugene Kharitonov, and Emmanuel Dupoux. 2021. The zero resource speech challenge 2021: Spoken language modelling. In Proc. Interspeech, pages 15741578. Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, and Huaming Wang. 2023. Clap learning audio concepts from natural language supervision. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE. Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. 2022. Language-agnostic bert sentence embedding. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 878891. Kuofeng Gao, Shu-Tao Xia, Ke Xu, Philip Torr, and Jindong Gu. 2025. Benchmarking open-ended audio dialogue understanding for large audio-language models. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 47634784. Edward Gibson. 1998. Linguistic complexity: locality of syntactic dependencies. Cognition, 68(1):176. Edward Gibson. 2000. The dependency locality theory: distance-based theory of linguistic complexity. In Alec P. Marantz, Yasushi Miyashita, and Wayne ONeil, editors, Image, Language, Brain: Papers from the First Mind Articulation Project Symposium, pages 95126. MIT Press, Cambridge, MA. Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. 2017. On calibration of modern neural networks. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1321 1330. PMLR. Michael Hassid and 1 others. 2024. Textually pretrained speech language models. Advances in Neural Information Processing Systems, 36. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations. Yixuan Hou, Heyang Liu, Yuhao Wang, Ziyang Cheng, Ronghua Wu, Qunshan Gu, Yanfeng Wang, and Yu Wang. 2025. SOVA-Bench: Benchmarking the speech conversation ability for LLM-based voice assistant. In Proc. Interspeech, pages 57135717. Chan-Jan Hsu, Yi-Cheng Lin, Chia-Chun Lin, Wei-Chih Chen, Ho Lam Chung, Chen-An Li, Yi-Chang Chen, Chien-Yu Yu, Ming-Ji Lee, Chien-Cheng Chen, RuHeng Huang, Hung yi Lee, and Da-Shan Shiu. 2025. Breezyvoice: Adapting tts for taiwanese mandarin with enhanced polyphone disambiguation challenges and insights. Preprint, arXiv:2501.17790. Wei-Ning Hsu, Anuroop Sriram, Alexei Baevski, Tatiana Likhomanenko, Qiantong Xu, Vineel Pratap, Jacob Kahn, Ann Lee, Ronan Collobert, Gabriel Synnaeve, and Michael Auli. 2021. Robust wav2vec 2.0: Analyzing domain shift in self-supervised pretraining. Preprint, arXiv:2104.01027. Chien-yu Huang, Ke-Han Lu, Shih-Heng Wang, ChiYuan Hsiao, Chun-Yi Kuan, Haibin Wu, Siddhant Arora, Kai-Wei Chang, Jiatong Shi, Yifan Peng, and 1 others. 2024. Dynamic-superb: Towards dynamic, collaborative, and comprehensive instruction-tuning benchmark for speech. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1213612140. IEEE. Chien-yu Huang and 1 others. 2025. DynamicSUPERB phase-2: collaboratively expanding benchmark for measuring the capabilities of spoken language models with 180 tasks. In The Thirteenth International Conference on Learning Representations. Zeqian Ju, Yuancheng Wang, Kai Shen, Xu Tan, Detai Xin, Dongchao Yang, Yanqing Liu, Yichong Leng, Kaitao Song, Siliang Tang, and 1 others. 2024. Naturalspeech 3: Zero-shot speech synthesis with factorized codec and diffusion models. CoRR. Saurav Kadavath, Tom Conerly, Amanda Askell, and 1 others. 2022. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221. Eugene Kharitonov, Ann Lee, Adam Polyak, Yossi Adi, Jade Copet, Kushal Lakhotia, Tu Anh Nguyen, Morgane Riviere, Abdelrahman Mohamed, Emmanuel Dupoux, and Wei-Ning Hsu. 2022. Text-free prosody-aware generative spoken language modeling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 86668681, Dublin, Ireland. Association for Computational Linguistics. Eugene Kharitonov and 1 others. 2021. Text-free prosody-aware generative spoken language modeling. arXiv preprint arXiv:2109.03264. Nithin Rao Koluguri, Taejin Park, and Boris Ginsburg. 2022. Titanet: Neural model for speaker representation with 1d depth-wise separable convolutions and global context. In ICASSP 2022-2022 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 81028106. IEEE. Moreno La Quatra, Alkis Koudounas, Lorenzo Vaiani, Elena Baralis, Luca Cagliero, Paolo Garza, and Sabato Marco Siniscalchi. 2024. Benchmarking representations for speech, music, and acoustic events. In 2024 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW), pages 505509. Kushal Lakhotia and 1 others. 2021. On generative spoken language modeling from raw audio. Transactions of the Association for Computational Linguistics, 9:13361354. Roger Levy. 2008. Expectation-based syntactic comprehension. Cognition, 106(3):11261177. Gallil Maimon and Yossi Adi. 2023. Speaking style conversion in the waveform domain using discrete self-supervised units. In Findings of the Association for Computational Linguistics: EMNLP 2023. Gallil Maimon, Amit Roth, and Yossi Adi. 2025. SALMon: suite for acoustic language model evaluation. In 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE. Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. 2017. Voxceleb: large-scale speaker identification dataset. In Proc. Interspeech, pages 2616 2620. Tu Anh Nguyen and 1 others. 2024. Spirit-lm: Interleaved spoken and written language model. arXiv preprint arXiv:2402.05755. Open-AI. Gpt-4o. https://openai.com/index/ gpt-4o-system-card/. Accessed: Jul 2024. Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015. Librispeech: An asr corpus based on public domain audio books. In Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 52065210. John C. Platt. 1999. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. In Advances in Large Margin Classifiers, pages 6174. MIT Press. Adam Polyak and 1 others. 2021. Speech resynthesis from discrete disentangled self-supervised representations. arXiv preprint arXiv:2104.00355. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. 2020. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167. Lloyd Shapley and 1 others. 1953. value for nperson games. Amitay Sicherman and Yossi Adi. 2023. Analysing discrete self supervised speech representation for spoken language modeling. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE. Nathaniel Smith and Roger Levy. 2013. The effect of word predictability on reading time is logarithmic. Cognition, 128(3):302319. Issa Sugiura, Shuhei Kurita, Yusuke Oda, and Ryuichiro Higashinaka. 2025. Llama-mimi: Speech language models with interleaved semantic and acoustic tokens. arXiv preprint arXiv:2509.14882. Hsiang-Sheng Tsai, Heng-Jui Chang, Wen-Chin Huang, Zili Huang, Kushal Lakhotia, Shu-wen Yang, Shuyan Dong, Andy Liu, Cheng-I Lai, Jiatong Shi, Xuankai Chang, Phil Hall, Hsuan-Jui Chen, Shang-Wen Li, Shinji Watanabe, Abdelrahman Mohamed, and Hungyi Lee. 2022. SUPERB-SG: Enhanced speech processing universal PERformance benchmark for semantic and generative capabilities. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 84798492, Dublin, Ireland. Association for Computational Linguistics. Liang-Hsuan Tseng, Yi-Chang Chen, Kuan-Yi Lee, Da-Shan Shiu, and Hung-yi Lee. 2025. Taste: Text-aligned speech tokenization and embedding arXiv preprint for spoken language modeling. arXiv:2504.07053. Yuan Tseng, Layne Berry, Yi-Ting Chen, I-Hsiang Chiu, Hsuan-Hao Lin, Max Liu, Puyuan Peng, Yi-Jen Shih, Hung-Yu Wang, Haibin Wu, Po-Yao Huang, ChunMao Lai, Shang-Wen Li, David Harwath, Yu Tsao, Shinji Watanabe, Abdelrahman Mohamed, Chi-Luen Feng, and Hung-yi Lee. 2024. AV-SUPERB: multi-task evaluation benchmark for audio-visual representation models. In Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Also available as arXiv:2309.10787. Dennis Ulmer, Jes Frellsen, and Christian Hardmeier. 2022. Exploring predictive uncertainty and calibration in NLP: study on the impact of method & data scarcity. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 27072735, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, and 1 others. 2023a. Neural codec language models are zeroshot text to speech synthesizers. arXiv preprint arXiv:2301.02111. Hui Wang, Siqi Zheng, Yafeng Chen, Luyao Cheng, and Qian Chen. 2023b. Cam++: fast and efficient network for speaker verification using context-aware masking. In INTERSPEECH. Pete Warden. 2018. Speech commands: dataset for limited-vocabulary speech recognition. Preprint, arXiv:1804.03209. Haibin Wu, Kai-Wei Chang, Yuan-Kuei Wu, and Hungyi Lee. 2023. SpeechGen: Unlocking the generative power of speech language models with prompts. arXiv preprint arXiv:2306.02207. Qian Yang, Jin Xu, Wenrui Liu, Yunfei Chu, Ziyue Jiang, Xiaohuan Zhou, Yichong Leng, Yuanjun Lv, Zhou Zhao, Chang Zhou, and Jingren Zhou. 2024. AIR-bench: Benchmarking large audio-language models via generative comprehension. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 19791998, Bangkok, Thailand. Association for Computational Linguistics. Shu-wen Yang, Po-Han Chi, Yung-Sung Chuang, Cheng-I Jeff Lai, Kushal Lakhotia, Yist Y. Lin, Andy T. Liu, Jiatong Shi, Xuankai Chang, GuanTing Lin, Tzu-Hsien Huang, Wei-Cheng Tseng, Kotik Lee, Da-Rong Liu, Zili Huang, Shuyan Dong, Shang-Wen Li, Shinji Watanabe, Abdelrahman Mohamed, and Hung-yi Lee. 2021. SUPERB: Speech processing universal PERformance benchmark. In Proc. Interspeech, pages 11941198. Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. 2021. Soundstream: An end-to-end neural audio codec. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 30:495507. where 1 δ + 1 indexes all valid starting positions of length-δ window in the token sequence. Table 3 shows windowed perplexity adopted in both acoustic consistency and semanticacoustic alignment subsets."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Further Evaluations of Spoken Language"
        },
        {
            "title": "Models",
            "content": "While universal task robustness serves as the prevailing objective, evaluations follow curriculum that mirrors the models training progression. In the main text, we have shown that for pretrained SLMs, likelihood-based evaluation is prominent. For finetuned models, task-based evaluation metrics pinpoint utility, such as speech recognition (Panayotov et al., 2015), emotion recognition (Busso et al., 2008), keyword spotting (Warden, 2018), and speaker identification (Nagrani et al., 2017). Aggregated suites such as the SUPERB series (Yang et al., 2021; Tseng et al., 2024; Tsai et al., 2022; Huang et al., 2024, 2025) broaden coverage, though their constituent tasks remain predefined. Prompt-following benchmarks built around arbitrary natural-language instructions highlight general intelligence of finetuned SLMs (Yang et al., 2024; Gao et al., 2025; Hou et al., 2025; Chen et al., 2024). A.2 Verbose Evaluation Settings A.2.1 Windowed Likelihood-Based"
        },
        {
            "title": "Evaluation",
            "content": "In addition to the consistency benchmarks, SALMon contains two subsets that focus on semantic-acoustic alignment, which does not follow the prompt-response structure. Motivated by prior observations localized perplexity can increase alignment with human perception, we define perplexity alternative that emphasizes locality but does not require specifying the transition timeframe. The windowed likelihood metric is calculated on small timespan of the speech sample, similar to localized perplexity. Unlike localized perplexity, windowed perplexity does not predefine starting timeframe, instead, it scans over the entire speech sequence to obtain series of localized perplexity, and selects the maximum of the values as the final perplexity score. NLLwindowed(s) = max 1 δ t=i i+δ1 (cid:88) (cid:0) log p(xt x<t)(cid:1), A.2.2 MOS Evaluation Prompt The annotaters are given the following prompt: You will hear generated continuation that extends from an audio prompt. Compare the two audio clips only on the specific target **feature** that varies between samples, focus only on the similarity of that feature, and assign similarity score from 1 to 5. 5 the perindicates of score (genfect match feature on der/speaker/sentiment/background, etc.). The two audios are indistinguishable on that feature. Naturally, two identical audios will score 5 on any given feature. 1 of"
        },
        {
            "title": "A score",
            "content": "indicates feature com- (genplete mismatch on the der/speaker/sentiment/background, etc.). The two audios are easily distinguishable on that feature. Naturally, if the target feature is missing entirely, the score is unequivocally 1. Target feature is considered missing when: The audio is completely silent. The attribute is gender / speaker / sentiment but there is no speech (sound made by humans). The attribute is background but there is only human speech and no other acoustic source. Use the guideline below for sample comparisons and guidance for scores 24. Scoring guidelines: 5 Indistinguishable from the prompt on the target attribute. 4 Only distinguishable with close attention to small sections; casual listening still feels nearly identical. 3 Distinguishable, but most attribute traits still feel similar. 2 Clearly distinguishable with only minor attribute overlap. 1 Totally distinguishable; no attribute similarity at all. (11) (Examples for each score) A.3 Verbose Evaluation Results A.3.1 Likelihood-Based Evaluations Table 3 shows full evaluation results on SALMon across spoken language models. Generally, alternative uncertainty methods evaluate HuBERTbased models (GSLM, TWIST, SpiritLM) lower, and Mimi-based models (Flow-SLM, Llama-Mimi) higher. For the top-performing models, our proposed evaluation methods occasionally deliver results that places model performance above the human topline. Notably, these cases are concentrated around speaker information. For semantic-acoustic alignment tasks, there is not common speech prompt, hence only GlobalPPL and Windowed-PPL evaluation are supported. Results show consistent low accuracies (< 60%), regardless of the model used or the methods selected. This agreement confirms that alignment is likely trait that is not picked up by current SLMs, motivating future work on this direction. A.3.2 MOS Evaluations by Human Labelers Table 4 show the average MOS scores across model and task. Top performing models shows great perception scores on sentiment, speaker and gender continuity, but still lack information in modeling background characteristics. A.3.3 Model-as-a-Judge for Measuring Generation Consistency Table 5 exibihits task-wise performance on embedding judge candidates. From the result, we observe that no single embedding model aces all six attributes, motivating future work on more generalizable audio embedding approaches. In addition, these results indicate that acoustic features (speaker features, background features) are effectively timeinvariant at the resolution probed by current evaluation protocols. Such stability endorses prevalent architectural choice of conducting speech synthesis conditioned on constant residual acoustic embedding. Notably, CAM++ serves as highly versatile, general-purpose acoustic representation model and is widely adopted in generative speech systems (Tseng et al., 2025; Du et al., 2024; Hsu et al., 2025). Related design choices also appear in other spoken-language models with speech generation (Nguyen et al., 2024; Lakhotia et al., 2021; Hassid et al., 2024) and conversational speech frameworks (Défossez et al., 2024). Table 6 show the result of SLM performance evaluated by selected judge models. Similar to the conclusions made in the proposed likelohood estimation methods, evaluating on true continuation show scores are in the vicinity of the human topline for top performing models, especially on traits related to human speech (sentiment, speaker, gender). Whereas Llama-Mimi integrates deeper hierarchy of Mimi token layers into its speech modeling pipeline, Flow-SLM deploys more intricate flowmatching speech decoder, which may account for its elevated scores on certain subtasks measuring directly in the speech. Overall, HuBERT-based models perform close to chance level. notable exception is SpiritLMexpressive, which includes additional pitch and style tokens. Experiment shows that this addition information is best reflected in sentiment performance, reaching 72%. closer examination of reconstruction and continuation performance reveals that the failure in continuation arises from the reconstructed audio lacking the relevant content to begin with. In most cases, reconstructions achieve near-chance scores mirroring their continuation counterparts, which is strikingly poor given that evaluation on the original audio yields near-ceiling performance on the benchmark. Manual inspection of the audio reveal that the audio is indeed greatly distorted compared to the original sample, where semantic pronunciations are greatly preserved but speaker and background information collapse. This example illustrates key limitation of global uncertainty measures. Even when they showed moderate performance, such performance failed to generalize to continuation performance that requires local acoustic fidelity at each generation step. A.3.4 Shapley Analysis Table 7 shows token-type ablations and shapley attributions for Spirit-LM-Expressive (HuBERT H, pitch , style S) and Llama-Mimi (layers 03), under four evaluation settings. While Spirit-LMExpressives largest contribution comes from HuBERT tokens, Llama-Mimis largest contribution comes from layer 1. Across settings, the most pronounced differences concentrate on speaker-related attributes (sentiment, speaker, and gender). A.3.5 Loss Response Figures Figures 6 through 13 show per-task breakdown of the models NLL-loss responses. The degree of separability between the positive and negative NLL responses in these plots largely correlates with the resulting accuracy. Consistent with this, speakerrelated attributes (sentiment, speaker identity, and gender) exhibit larger separations. Figure 6: Positive vs. Negative Sample Mean NLL-Loss Response for GSLM across six consistency splits. Figure 7: Positive vs. Negative Sample Mean NLL-Loss Response for pGSLM across six consistency splits. Table 3: Comparison of spoken language model performance on the SALMon benchmark, including GSLM (Kharitonov et al., 2022), TWIST (Hassid et al., 2024), pGSLM (Kharitonov et al., 2021), Spirit-LM (Nguyen et al., 2024), TASTE (Tseng et al., 2025), Flow-SLM (Chou et al., 2025), and Llama-mimi (Sugiura et al., 2025). For each model, we report task accuracies measured with global token perplexity and our proposed likelihood evaluators. Comparing accuracies within each model shows that the perceived performance difference between methods is quite huge. As result, these evaluation protocols yield substantially different conclusions on the SLM performance landscape. Bold number highlight evaluation results that surpasses the human topline performance by (Maimon et al., 2025). Model Config Method Acoustic Consistency Sentiment Speaker Gender Bg (domain) Bg (rand.) Room Semantic-Acoustic Alignment Sentiment Background G 350M 1.3B 7B base Expr. emb 270M 1B 1B-ext. 1.3B I L M r E T - F m - l Global-PPL + Normalization (prop.) Windowed-PPL (prop.) Localized-PPL (prop.) + Normalization (prop.) Global-PPL + Normalization (prop.) Windowed-PPL (prop.) Localized-PPL (prop.) + Normalization (prop.) Global-PPL + Normalization (prop.) Windowed-PPL (prop.) Localized-PPL (prop.) + Normalization (prop.) Global-PPL + Normalization (prop.) Windowed-PPL (prop.) Localized-PPL (prop.) + Normalization (prop.) Global-PPL + Normalization (prop.) Windowed-PPL (prop.) Localized-PPL (prop.) + Normalization (prop.) Global-PPL + Normalization (prop.) Windowed-PPL (prop.) Localized-PPL (prop.) + Normalization (prop.) Global-PPL + Normalization (prop.) Windowed-PPL (prop.) Localized-PPL (prop.) + Normalization (prop.) Global-PPL + Normalization (prop.) Windowed-PPL (prop.) Localized-PPL (prop.) + Normalization (prop.) Global-PPL + Normalization (prop.) Windowed-PPL (prop.) Localized-PPL (prop.) + Normalization (prop.) Global-PPL + Normalization (prop.) Windowed-PPL (prop.) Localized-PPL (prop.) + Normalization (prop.) Global-PPL + Normalization (prop.) Windowed-PPL (prop.) Localized-PPL (prop.) + Normalization (prop.) Global-PPL + Normalization (prop.) Windowed-PPL (prop.) Localized-PPL (prop.) + Normalization (prop.) 52.5 51.0 52.0 48.5 54.5 59.0 58.0 53.5 52.0 57.5 61.0 57.0 54.0 50.0 54. 61.5 58.5 56.5 50.5 55.0 56.0 63.5 52.0 64.5 73.5 52.5 56.0 57.0 59.0 61.0 72.5 70.5 63.5 79.0 71.5 56.5 58.5 53.5 54.5 53.0 61.5 78.0 65.5 75.0 75. 65.5 78.5 71.5 79.5 80.5 64.5 82.5 57.5 79.5 82.0 79.5 88.5 83.5 95.5 89.0 50.5 54.0 53.5 56.0 46.0 69.5 75.0 61.5 59.5 63.5 69.0 73.0 65.0 58.5 65. 70.5 76.0 61.5 61.0 64.5 74.0 79.5 67.5 76.5 86.5 67.5 57.5 60.5 62.0 57.0 81.5 69.5 66.5 78.0 76.0 69.5 54.5 61.5 57.0 52.0 75.0 87.0 69.0 72.0 68. 77.0 86.0 72.5 73.5 69.0 76.0 88.0 63.0 76.0 74.5 83.5 96.5 95.0 95.5 97.5 53.0 51.5 54.5 47.5 52.0 68.0 72.5 67.0 63.0 58.0 69.5 74.0 70.0 64.0 62. 70.0 72.5 63.0 60.0 65.0 69.0 88.0 55.5 68.0 88.5 67.0 62.5 65.0 68.0 60.5 85.5 78.0 77.5 90.5 79.5 75.5 55.5 61.0 60.5 55.0 76.5 84.5 72.5 77.5 77. 79.0 87.0 81.0 79.0 80.0 80.5 89.5 64.0 81.5 81.0 85.0 100.0 96.5 100.0 98.5 47.5 40.0 48.5 45.0 43.5 54.0 57.0 56.0 55.8 57.0 54.5 59.0 56.5 57.8 59. 55.5 58.0 57.5 59.3 58.0 65.5 51.5 58.0 50.0 50.0 53.0 56.5 57.5 53.0 53.5 55.5 61.0 53.0 55.5 59.5 39.5 50.5 45.5 44.0 44.0 66.0 57.0 58.0 62.5 54. 69.0 58.5 58.0 64.5 53.0 69.5 58.0 65.0 65.5 54.0 73.0 73.0 79.5 79.5 79.0 50.5 43.5 52.0 45.5 37.5 61.5 58.0 60.0 51.5 61.5 60.5 61.5 60.0 51.5 61. 59.5 58.5 58.5 52.0 61.5 71.5 55.5 62.5 57.0 60.0 55.5 50.0 54.5 47.5 48.0 64.0 55.5 53.5 53.0 55.5 48.5 49.0 44.0 43.5 44.5 65.0 64.0 65.5 62.0 63. 66.0 69.5 65.5 68.0 63.0 64.5 74.0 54.5 69.5 67.5 73.0 85.5 71.0 84.5 81.5 48.0 50.5 49.5 47.0 54.0 59.0 62.0 63.3 66.8 61.0 59.5 62.5 62.3 66.3 58. 61.5 64.5 63.8 66.3 67.5 52.0 68.0 48.0 62.5 67.0 54.5 58.0 63.5 63.5 52.5 55.5 54.5 56.0 62.0 54.0 55.5 48.5 47.5 49.0 48.0 56.5 81.5 62.0 67.5 78. 69.0 88.5 78.5 81.5 78.5 73.5 85.5 57.0 83.5 81.0 91.5 98.5 88.5 97.5 95.0 55.0 - 56.0 - - 51.0 - 51.5 - - 52.5 - 54.0 - - 51.0 - 51.5 - - 53.0 - 49.0 - - 47.5 - 46.0 - - 53.0 - 53.0 - - 54.0 - 51.5 - - 59.0 - 54.5 - - 58.5 - 58.0 - - 57.5 - 52.0 - - 53.0 - 51.0 - - Human 97.2 91.5 98.6 83.1 88.7 94. 93.3 Human Topline (Measured by (Maimon et al., 2025)) 52.5 - 43.0 - - 56.5 - 58.0 - - 57.0 - 55.0 - - 53.5 - 56.5 - - 54.0 - 52.5 - - 51.0 - 60.8 - - 59.5 - 55.0 - - 42.0 - 48.0 - - 53.5 - 55.0 - - 54.5 - 53.5 - - 53.5 - 57.0 - - 49.0 - 53.5 - - 95.7 Table 4: Generation performance of SLMs judged by human ratings (MOS scores) with the models associated rank. Sentiment Speaker MOS Scores Evaluation Gender Bg (domain) Bg (rand.) Room Avg"
        },
        {
            "title": "Rank",
            "content": "GSLM TWIST 1.3B pGSLM Spirit-LM-expr. TASTE-emb. Flow-SLM-1b Flow-SLM-1b-ext. Llama-mimi 1.88 1.06 1.91 1.08 1.86 1.05 3.41 1.49 3.68 1.40 3.86 1.27 3.80 1.31 3.78 1.31 1.94 1.13 2.04 1.18 1.76 1.05 1.98 1.14 4.37 1.02 4.21 1.13 4.20 1.10 4.14 1.16 2.76 1.61 2.73 1.55 2.38 1.28 2.63 1.49 4.63 0.93 4.47 0.96 4.52 0.94 4.32 1.10 1.38 0.85 1.62 1.07 1.34 0.78 1.27 0.73 1.64 1.16 1.89 1.08 1.98 1.13 2.20 1.30 1.36 0.86 1.59 0.99 1.28 0.71 1.19 0.51 1.60 1.04 1.86 1.12 2.00 1.23 2.21 1. 1.82 1.00 2.29 1.21 1.65 0.97 1.58 0.98 2.29 1.27 3.25 1.34 3.08 1.41 3.11 1.41 1.86 0.45 2.03 0.49 1.71 0.40 2.01 0.46 3.03 0.47 3.26 0.47 3.26 0.49 3.29 0.52 7 5 8 6 4 3 2 1 Table 5: Embedding model performance on SALMon, where the accuracy in aggregated over d(S, ) < d(S, ). Embedding Model Performance Acoustic Consistency Sentiment Speaker Gender Bg (domain) Bg (rand.) Room TITANET (Koluguri et al., 2022) CAM++ (Wang et al., 2023b) CLAP (Elizalde et al., 2023) wav2vec2-large-audioset (La Quatra et al., 2024) hubert-large-audioset (La Quatra et al., 2024) data2vec-audio-large (Baevski et al., 2022) wavlm-large (Chen et al., 2022) wav2vec2-large-robust (Hsu et al., 2021) 99.5 95.5 96.0 91.5 90.0 58.5 75.0 74.5 100.0 95.5 91.0 70.0 74.5 67.0 66.5 58.5 100.0 99.0 98.5 75.5 74.5 59.5 69.5 52.5 58.5 69.5 78.0 82.5 86.5 54.0 52.5 59.5 70.5 84.5 90.0 95.5 97.5 45.5 56.0 67.5 94.0 94.5 97.0 99.0 95.5 77.5 93.0 76. Selected Classifier Human TITANET TITANET TITANET hubert-large-audioset hubert-large-audioset wav2vec2-large-audioset 97. 91.5 98.6 83.1 88.7 94.4 Figure 8: Positive vs. Negative Sample Mean NLL-Loss Response for TWIST-1.3B across six consistency splits. Table 6: Generation peformance of spoken language models judged-by the proper model as judge. Main numbers report performance of continuation samples from the SLM; parenthesized numbers report the performance of reconstructed audio that is generated from the speech tokens of SLMs. Boldface items indicates that the generation performance exceeds the human topline reported by (Maimon et al., 2025). Sentiment Speaker Gender Bg (domain) Bg (rand.) Room Acoustic Consistency Judge Model"
        },
        {
            "title": "Evaluated Model",
            "content": "TITANET TITANET TITANET hubert-large-audioset hubert-large-audioset wav2vec2-large-audioset GSLM TWIST-350M TWIST-1.3B TWIST-7B pGSLM Spirit-LM Spirit-LM-expr. TASTE-emb. Flow-SLM-270m Flow-SLM-1b Flow-SLM-1b-ext. Llama-mimi 54.0 (51.5) 48.0 (48.5) 48.5 (50.0) 48.0 (51.0) 46.5 (52.0) 49.0 (51.0) 72.0 (80.5) 95.0 (95.0) 85.0 (98.0) 92.5 (98.0) 93.0 (98.0) 89.0 (95.0) 58.5 (55.5) 53.0 (53.5) 50.5 (52.5) 54.5 (53.0) 49.5 (57.5) 46.0 (51.0) 54.0 (52.0) 99.5 (99.0) 87.0 (100.0) 99.5 (100.0) 98.0 (100.0) 92.0 (92.0) 51.0 (52.0) 52.5 (53.0) 51.0 (52.0) 52.0 (53.0) 55.5 (58.0) 50.5 (54.0) 55.0 (55.5) 99.5 (99.5) 92.5 (100.0) 99.0 (100.0) 100.0 (100.0) 94.0 (95.0) Human Topline 97.2 91. 98.6 61.0 (60.5) 61.5 (60.0) 63.0 (60.0) 59.5 (60.5) 51.0 (51.0) 51.5 (52.5) 61.5 (61.5) 61.5 (63.0) 77.0 (80.0) 77.0 (80.0) 77.0 (80.0) 80.0 (78.5) 83.1 54.5 (53.0) 53.0 (53.5) 54.5 (54.0) 54.0 (54.0) 53.0 (52.0) 53.0 (53.5) 54.5 (55.0) 54.5 (53.5) 89.5 (94.0) 84.5 (94.0) 86.5 (94.0) 91.0 (97.0) 88.7 49.0 (50.0) 52.5 (48.5) 51.5 (48.5) 51.5 (49.0) 52.5 (55.0) 48.5 (50.0) 48.5 (55.0) 57.5 (53.0) 69.5 (96.5) 77.0 (97.0) 85.0 (96.0) 78.0 (92.5) 94.4 Table 7: Token-type ablations and Shapley attributions for Spirit-LM (HuBERT H, pitch , style S) and LlamaMimi (layers 03), under four evaluation settings that combine estimator locality (Global vs. Localized) and scoring normalization (Original vs. Normalized). With the accuracy in each panel (top), the corresponding Shapley values ϕ for each component can be calculated (bottom), with null baseline of 50% on every task. Under the default evaluation setting (Global, Original), Spirit-LMs largest contribution comes from HuBERT tokens, whereas Llama-Mimis largest contribution comes from layer 1. Across settings, the most pronounced differences concentrate on speaker-related attributes (sentiment, speaker, and gender)."
        },
        {
            "title": "Normalized",
            "content": "Spirit-LM Sent. 72.5 74.0 58.5 56.0 57.5 55.0 53.5 Spk. Gen. BgD. BgR. Room Avg 81.5 81.5 67.0 70.0 67.5 69.5 48.5 85.5 85.0 74.0 74.0 73.0 72.5 46. 55.5 57.0 49.5 54.0 50.5 54.0 45.5 64.0 63.0 58.0 56.5 60.0 57.0 49.0 54.5 54.5 57.0 51.0 54.5 51.5 52.0 68.9 69.2 60.7 60.2 60.5 59.9 49.1 o P Sent. 74.7 72.9 63.0 69.0 60.2 65.4 55.7 Spk. Gen. BgD. BgR. Room Avg 75.5 74.2 66.0 68.8 64.8 70.8 56.2 85.5 85.2 68.0 83.0 66.5 84.5 58.0 52.8 52.8 50.5 51.8 47.5 56.0 46.5 55.2 53.5 45.8 62.0 47.0 60.2 52. 64.8 64.0 50.2 69.2 49.0 72.8 53.5 68.1 67.1 57.3 67.3 55.8 68.3 53.7 ϕH ϕP ϕS +12.0 +14.8 +18.2 +1.8 +8.3 +4.0 +9.9 +9.5 +17.2 +18.0 +5.8 +6.1 0.5 +9.4 +1.0 0.5 0.7 2.2 0.4 +1.0 0.3 ϕH ϕP ϕS +7.8 +9.4 +8.1 0.4 5.4 3.8 +2.6 +13.4 +13.7 +24.6 +4.5 +9.3 +17.5 +13.9 +3.6 +2.4 +2.8 1.4 +1.3 +1.0 +1. ) 5 . 0 = Sent. 71.5 70.5 64.5 64.4 64.0 59.9 60.6 Spk. Gen. BgD. BgR. Room Avg 79.0 80.0 65.5 74.0 67.5 76.0 47.7 87.5 89.0 69.0 86.0 69.5 88.5 51. 62.5 61.5 61.0 60.0 60.8 58.3 56.1 54.0 51.5 55.0 51.0 53.0 49.0 47.8 64.5 64.5 67.0 52.5 65.8 54.0 55.4 69.8 69.5 63.7 64.7 63.4 64.3 53.2 Sent. 75.5 77.1 63.0 69.8 63.0 76.0 46.9 Spk. Gen. BgD. BgR. Room Avg 77.5 78.0 65.5 73.0 64.0 73.0 46.2 85.0 86.0 61.0 87.5 64.0 88.0 48.0 56.5 60.5 52.5 52.5 51.0 53.5 52.5 57.0 59.0 47.0 53.5 48.0 55.5 42.8 62.5 63.0 58.0 64.0 60.0 65.5 55. 69.0 70.6 57.8 66.7 58.3 68.6 48.6 ϕH ϕP ϕS +9.4 +11.1 +10.0 +5.8 +3.6 +12.9 +8.8 +7.4 +19.6 +28.0 +4.0 0.4 0.2 +9.7 +4.7 1.8 0.5 +2.7 +0.8 +1.8 +1.3 ϕH ϕP ϕS +9.1 +10.2 +5.7 +2.8 +1.8 +2.8 +5.4 +19.0 +18.5 +30.9 +4.1 +8.8 +8.6 +15.0 2.6 1.2 1.6 0.4 3.6 +1.1 1.4 Llama-Mimi 3 1 2 0 Sent. 80.5 57.0 61.5 76.0 62.0 72.5 77.5 71.0 61.0 70.5 78.0 79.5 75.0 68.0 80."
        },
        {
            "title": "Original",
            "content": "Spk. Gen. BgD. BgR. Room Avg 86.0 70.5 69.0 82.5 77.5 77.0 80.0 85.0 78.5 78.0 79.5 82.5 79.5 83.5 85.5 82.0 64.5 75.5 84.5 78.0 75.0 83.0 86.5 74.5 81.5 81.0 78.5 77.5 81.5 86.0 75.0 67.5 64.5 68.0 67.0 67.0 70.5 72.5 73.5 71.5 73.0 73.5 71.0 73.5 71.0 72.0 67.5 62.5 67.0 67.5 68.0 69.5 73.0 74.0 70.0 70.5 72.0 71.0 72.5 72. 92.5 73.0 70.0 92.5 83.0 75.5 91.0 97.0 81.0 93.5 80.0 92.5 80.5 96.5 94.0 81.3 66.7 67.2 78.4 72.5 72.5 78.6 80.8 73.8 77.5 77.0 79.8 75.8 79.2 81.6 3 1 2 0 Sent. 89.0 55.5 62.5 73.5 62.0 62.0 85.0 70.0 64.5 72.0 77.0 83.0 77.0 71.5 87."
        },
        {
            "title": "Normalized",
            "content": "Spk. Gen. BgD. BgR. Room Avg 96.5 71.0 68.0 76.5 77.0 81.0 82.5 84.0 78.0 87.0 85.0 90.5 88.5 89.5 93.0 100.0 66.0 77.5 86.5 78.0 80.0 91.5 89.5 79.0 88.5 93.0 94.0 93.0 93.5 98.0 73.0 54.0 54.0 58.5 57.0 55.5 60.5 63.5 60.5 65.0 59.5 65.0 61.0 66.0 66.5 85.5 60.5 54.5 62.5 57.0 62.0 65.5 67.5 66.5 71.0 65.0 72.0 64.5 77.5 71. 98.5 65.0 72.5 80.0 69.5 77.5 87.0 86.5 70.0 85.5 85.0 92.5 85.0 88.0 94.0 90.4 62.0 64.8 72.9 66.8 69.7 78.7 76.8 69.8 78.2 77.4 82.8 78.2 81.0 84.9 ϕ0 ϕ1 ϕ2 ϕ3 +4.8 +10.6 +9.8 +6.8 +6.8 +11.0 +8.3 +12.5 +13.3 +13.8 +7.2 +6.1 +22.1 +12.5 +11.5 +6.4 +7.5 +4.5 +3.4 +3.5 +6.1 +1.8 +5.7 +1.0 +6.6 +5.7 +5.9 +4.4 ϕ0 ϕ1 ϕ2 ϕ3 +7.5 +12.6 +13.2 +6.2 +8.1 +9.8 +9.6 +16.0 +14.2 +17.7 +9.0 +13.3 +18.7 +14.8 +13.6 +10.2 +13.7 +3.5 +4.4 +13.6 +9.8 +1.9 +9.6 +5.5 +4.3 +9.7 +6.4 +6. 3 2 1 0 Sent. 95.5 75.5 78.0 88.5 75.0 85.5 90.5 92.0 83.0 89.5 87.5 94.0 90.0 94.0 94.5 Spk. Gen. BgD. BgR. Room Avg 95.5 83.5 81.5 91.0 84.0 90.0 92.5 91.5 91.5 92.5 90.0 93.5 96.0 94.5 95.5 100.0 85.0 85.0 93.5 94.0 92.5 95.0 98.0 95.0 97.5 96.5 99.0 98.5 99.5 99.0 79.5 59.5 59.0 73.0 70.5 67.0 73.5 76.5 73.5 74.5 75.0 74.0 75.0 79.5 78.5 84.5 64.5 68.0 80.0 75.5 74.0 80.5 83.5 79.0 80.0 77.5 81.0 81.5 82.5 84. 97.5 73.0 80.0 92.5 80.0 87.5 94.5 94.5 82.5 91.0 89.0 97.5 91.0 92.5 97.5 92.1 73.5 75.2 86.4 79.8 82.8 87.8 89.3 84.1 87.5 85.9 89.8 88.7 90.4 91.6 3 1 2 0 Sent. 92.0 70.5 72.0 80.5 70.5 77.5 88.5 84.0 73.5 84.0 80.5 88.5 82.5 84.5 90. Spk. Gen. BgD. BgR. Room Avg 98.0 72.0 76.5 85.0 78.5 81.0 89.5 92.0 82.0 90.5 88.0 91.0 92.5 93.5 94.5 99.0 74.5 75.5 88.0 88.0 85.5 92.0 97.0 92.5 96.0 92.5 96.5 96.5 97.0 98. 81.0 49.0 66.5 73.0 68.0 61.0 76.5 77.5 62.0 71.5 71.0 79.0 70.5 78.0 81.5 85.0 61.5 62.5 74.5 70.0 65.5 71.5 81.5 69.5 76.5 70.5 80.0 72.5 84.5 79.0 97.0 70.0 79.0 86.5 67.0 87.5 90.0 90.0 79.5 88.5 85.0 95.5 90.5 91.5 94.5 92.0 66.2 72.0 81.2 73.7 76.3 84.7 87.0 76.5 84.5 81.2 88.4 84.2 88.2 89.7 ϕ0 ϕ1 ϕ2 ϕ3 +9.4 +11.3 +14.4 +10.8 +10.7 +9.9 +11.1 +16.8 +13.4 +14.6 +11.2 +12.7 +18.5 +14.5 +10.6 +10.1 +10.3 +3.5 +6.4 +12.4 +8.9 +8.7 +10.7 +10.7 +4.0 +4.7 +6.7 +7. ϕ0 ϕ1 ϕ2 ϕ3 +7.8 +12.9 +15.3 +8.6 +10.0 +7.1 +10.3 +16.2 +16.2 +15.8 +15.6 +15.4 +17.6 +16.1 +11.1 +10.8 +9.0 +8.1 +3.7 +13.8 +9.4 +6.8 +8.1 +8.8 1.3 +5.9 +8.6 +6.2 ( i o b ) 5 . 0 = ( i o Figure 9: Positive vs. Negative Sample Mean NLL-Loss Response for Spirit-LM across six consistency splits. Figure 10: Positive vs. Negative Sample Mean NLL-Loss Response for Spirit-LM-Expressive across six consistency splits. Figure 11: Positive vs. Negative Sample Mean NLL-Loss Response for TASTE across six consistency splits. We follow TASTEs audio-text alignment setting and report with textual tokens as the granularity of the x-axis. Figure 12: Positive vs. Negative Sample Mean NLL-Loss Response for Flow-SLM-1B-Extended across six consistency splits. At the transition timeframe (t=0), each of the category has distinct postive/negative response (clear separation by 95% confidence interval). Figure 13: Positive vs. Negative Sample Mean NLL-Loss Response for Llama-mimi-1.3B across six consistency splits. At the transition timeframe (t=0), each of the category has distinct postive/negative response (clear separation by 95% confidence interval)."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "Massachusetts Institute of Technology",
        "National Taiwan University",
        "Toyota Technological Institute at Chicago"
    ]
}