{
    "paper_title": "Learn to Reason Efficiently with Adaptive Length-based Reward Shaping",
    "authors": [
        "Wei Liu",
        "Ruochen Zhou",
        "Yiyun Deng",
        "Yuzhen Huang",
        "Junteng Liu",
        "Yuntian Deng",
        "Yizhe Zhang",
        "Junxian He"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Reasoning Models (LRMs) have shown remarkable capabilities in solving complex problems through reinforcement learning (RL), particularly by generating long reasoning traces. However, these extended outputs often exhibit substantial redundancy, which limits the efficiency of LRMs. In this paper, we investigate RL-based approaches to promote reasoning efficiency. Specifically, we first present a unified framework that formulates various efficient reasoning methods through the lens of length-based reward shaping. Building on this perspective, we propose a novel Length-bAsed StEp Reward shaping method (LASER), which employs a step function as the reward, controlled by a target length. LASER surpasses previous methods, achieving a superior Pareto-optimal balance between performance and efficiency. Next, we further extend LASER based on two key intuitions: (1) The reasoning behavior of the model evolves during training, necessitating reward specifications that are also adaptive and dynamic; (2) Rather than uniformly encouraging shorter or longer chains of thought (CoT), we posit that length-based reward shaping should be difficulty-aware i.e., it should penalize lengthy CoTs more for easy queries. This approach is expected to facilitate a combination of fast and slow thinking, leading to a better overall tradeoff. The resulting method is termed LASER-D (Dynamic and Difficulty-aware). Experiments on DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, and DeepSeek-R1-Distill-Qwen-32B show that our approach significantly enhances both reasoning performance and response length efficiency. For instance, LASER-D and its variant achieve a +6.1 improvement on AIME2024 while reducing token usage by 63%. Further analysis reveals our RL-based compression produces more concise reasoning patterns with less redundant \"self-reflections\". Resources are at https://github.com/hkust-nlp/Laser."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 2 1 6 5 1 . 5 0 5 2 : r Learn to Reason Efficiently with Adaptive Length-based Reward Shaping Wei Liu1 Ruochen Zhou2 Yiyun Deng1 Yuzhen Huang1 Yuntian Deng3 Yizhe Zhang Junxian He1 Junteng Liu1 1The Hong Kong University of Science and Technology 2City University of Hong Kong 3University of Waterloo 4Apple"
        },
        {
            "title": "Abstract",
            "content": "Large Reasoning Models (LRMs) have shown remarkable capabilities in solving complex problems through reinforcement learning (RL), particularly by generating long reasoning traces. However, these extended outputs often exhibit substantial redundancy, which limits the efficiency of LRMs. In this paper, we investigate RL-based approaches to promote reasoning efficiency. Specifically, we first present unified framework that formulates various efficient reasoning methods through the lens of length-based reward shaping. Building on this perspective, we propose novel Length-bAsed StEp Reward shaping method (LASER), which employs step function as the reward based on target length. LASER surpasses previous methods, achieving superior Pareto-optimal balance between performance and efficiency. Next, we further extend LASER based on two key intuitions: (1) The reasoning behavior of the model evolves dynamically during training, necessitating reward specifications that are also adaptive and dynamic; (2) Rather than uniformly encouraging shorter or longer chains of thought (CoT), we posit that length-based reward shaping should be difficulty-aware i.e., it should penalize lengthy CoTs more for easy queries. This approach is expected to facilitate combination of fast and slow thinking, leading to better overall tradeoff. The resulting method is termed LASER-D (Dynamic and Difficulty-aware). Experiments on DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B , and DeepSeek-R1-Distill-Qwen-32B demonstrate that our approach significantly enhances both reasoning performance and response length efficiency. For instance, LASER-D and its variant achieve +6.1 improvement on AIME2024 while reducing token usage by 63%. Further analysis reveals that our RLbased compression produces more concise reasoning patterns with less redundant self-reflections. All resources (Models, Code, Data) are available at https://github.com/hkust-nlp/Laser."
        },
        {
            "title": "Introduction",
            "content": "Recent advancements leveraging reinforcement learning (RL) [4, 11, 15, 28] demonstrate that LLMs can evolve into powerful Large Reasoning Models (LRMs), capable of producing extended chains of thought (CoT) to enhance their reasoning abilities. However, these longer reasoning trajectories come at the cost of increased token usage and potentially incorporate more compounding errors. Many of the generated tokens tend to be unnecessarily verbose. For example, LRMs may output thousands of tokens to solve elementary math problems that could otherwise be addressed within just few hundred tokens, as shown in Figure 1 (right). This phenomenon is commonly referred to as the over-thinking issue [3]. Correspondence to Wei Liu (wliucn@cse.ust.hk) and Junxian He (junxianh@cse.ust.hk) Figure 1: Left: Accuracy and response length on AIME2024. For the figure of more benchmarks, please refer to Appendix A. Each point represents single training run with different hyper-parameters. Given the high computational cost of obtaining this figure, the base model used is DeepSeek-R1-Distill-Qwen-1.5B. Results on 7B and 32B models are in 6.3. Our methods, LASER, LASER-D, and LASER-DE achieve Pareto-optimal trade-off compared to all other methods. Notably, they yield +6.1 improvement in accuracy while reducing tokens usage by 63% compared to the original model. Right: Example of reasoning process after LASER-D training. While the original model produces meaningless self-reflection repeatedly for trivial questions like 1+1=?, LRMs after LASER-D training efficiently recognize such questions during thinking and provide the answer directly. Previous work [16, 22] has explored various approaches to improving reasoning efficiency in LRMs, including continuous chain-of-thought (CoT) reasoning [6, 18], supervised fine-tuning (SFT) [24, 14], and reinforcement learning [11, 2, 1, 10]. Typically, substantial reductions in token usage are accompanied by significant decreases in reasoning accuracy, suggesting trade-off between efficiency and reasoning performance. Recently, RL-based approaches have demonstrated the most favorable balance between token efficiency and accuracy [11, 2, 1, 10]. In this paper, we study RL-based CoT compression, beginning with simple baseline that yields surprisingly effective results (3). Specifically, we further train long CoT reasoning models using RL with rule-based correctness reward, while restricting the context window size to smaller value than the models typical generated length, so that long responses will be truncated. This approach can substantially improve token efficiency with only modest reduction in accuracy. To better understand the effectiveness of this truncation method and to connect it with other RL approaches that incorporate length-based rewards [11, 2, 1], we introduce unified length-based reward shaping perspective that encompasses various RL strategies for mitigating overthinking (4). Building on this reward shaping formulation, we extend the truncation approach as novel reward shaping method (4.3) that employs step function as the reward guided by desired target length. We refer to this approach as LASER (Length-bAsed StEp Reward). LASER achieves the best trade-off between reasoning performance and efficiency among all evaluated baselines. Next, we identify two key points that are lacking in LASER: (1) the desired reasoning length should evolve during training as the models reasoning behaviors dynamically change, and (2) rather than uniformly encouraging short or long CoT, length-based reward shaping should be difficulty-aware allowing harder questions higher token limit while constraining easier questions to fewer tokens. To this end, we propose Dynamic and Difficulty-aware Length-bAsed StEp Reward for RL (LASER-D), which adaptively applies different length reward shaping based on problem difficulty. Notably, our algorithm is fully automated with an integrated automatic adapting module, eliminating the need for manual procedural tuning. We also introduce variant of LASER-D, called LASER-DE, which explicitly encourages additional exploration on incorrect responses, enabling models to discover potentially correct reasoning patterns through extended deliberation. We conduct comprehensive experiments on three reasoning models ranging from 1.5B to 32B parameters, across four challenging benchmarks: MATH500, AIME2024, AMC2023, and OlympiadBench. Our extensive evaluations demonstrate that our proposed LASER series outperform existing works, while LASER-D and its variant LASER-DE achieve the best Pareto-optimal balance between accuracy and token efficiency. Unlike methods that improve token efficiency at the expense of accuracy, our proposed approaches deliver substantial gains in both dimensionseven on the challenging AIME2024 2 Table 1: Results of baseline truncation method with different context window. Tk denotes the models after RL training with context window k. Accuracy (%) with average token usage for each dataset. Original denotes the original DeepSeek-R1-Distill-Qwen-1.5B. MATH 500 83.9 81.8 80.9 77.7 Accuracy (%) AIME AMC Olympiad Bench 28.9 24.8 20.2 19.2 71.6 70.9 66.2 62.2 43.3 43.9 42.1 38.5 Avg. 56.9 55.3 52.3 49.4 MATH 500 5042 1795 1351 1054 Generation Length (tokens) Olympiad Bench AIME AMC 15956 4465 2821 8202 2560 1917 1484 11510 2841 1947 1564 Avg. 10177 2915 2009 1646 Original T8192 T6144 T4096 benchmark. For example, applying LASER-D/LASER-DE to DeepSeek-R1-Distill-Qwen-1.5B improves accuracy by +6.1 percentage points while reducing token usage by 63% on AIME24. Our further analysis reveals that after these RL-based CoT compressions, the reasoning behaviors of LRMs become more concise and demonstrate improved quality with fewer redundant and unhelpful self-reflection."
        },
        {
            "title": "2 Preliminary",
            "content": "Enhancing Reasoning via RL RL has been demonstrated as an effective way to train strong large reasoning models [4, 15, 11] across different domains like math [28], coding [4] and agentic tasks [15]. For example, using simple rule-based outcome reward [4, 28], the mathematical reasoning capabilities of models can be substantially improved after RL training, often accompanied by the emergence of self-reflection style thinking patterns. Following these previous works, we leverage rule-based reward designed as simple scoring system [4]: +1 for correct responses, -0.5 for incorrect responses, and -1 for responses with invalid format. Suppose is the question and is the response generated by the models, the optimization objective with KL-constrained [19] in RL can be formulated as: π θ = arg max θ ExD (cid:2)Eyπ(x)[R(x, y)] βDKL[πθ(x) πref (x)](cid:3) (1) where R(x, y) represents the reward of the entire trajectory, and πref is the reference model, which is the model prior to the RL training phase. β is the parameter to control the two optimization targets. In this paper, we utilize GRPO [20] to optimize this objective. RL for Efficient Reasoning In addition to enhancing reasoning capabilities, RL also holds promise for improving token efficiency in LRMs [11, 16]. Several approaches have been proposed to this end. Most methods involve reward shaping, where models are incentivized to produce shorter responses by associating higher rewards with more concise outputs [11, 1, 2]."
        },
        {
            "title": "3 Truncation: A Simple Yet Effective Baseline",
            "content": "In this section, we start from simple yet effective baseline, where we simply set max generation length to value significantly smaller than the models original context window during RL trainingfor example, 8,192 tokens versus 32,768 in DeepSeek-R1-Distilled models. Intuitively, this approach truncates the responses beyond the context window and regards them as incorrect, thus it pushes the model to generate accurate yet more concise responses under strict token constraints. This baseline has been explored recently in concurrent works [10, 12]. In our experiments, we adopt DeepSeek-R1-Distill-Qwen-1.5B as the base model and investigate the effects of truncation by limiting maximum generation lengths to 4,096, 6,144, and 8,192 tokens. Effectiveness of Truncation Table 1 presents the performance of models across various benchmarks under different truncation sizes. Compared to the original model, surprisingly, RL training with context window of 8192 tokens achieves substantial 71% improvement in token efficiency, while 3 maintaining competitive accuracy with 1.6 absolute point degradation on average. This demonstrates that truncation is simple yet effective approach for enhancing reasoning efficiency in LRMs. Efficacy-Efficiency Trade-off Although truncation proves effective on average across benchmarks, its impact varies significantly with task difficulty. closer look at the results reveals that performance on the most challenging benchmark, AIME, suffers notable 4.1 drop in accuracy under the 8192 token limit. When the context window is further reduced to 4096, the accuracy on AIME deteriorates even more sharply, with 9.7 decline, by far the largest drop observed, compared to only 7% decrease on MATH500. This highlights that the benefits of truncation involve trade-off: while it improves efficiency overall, it may disproportionately affect harder tasks. To better understand this disproportionate performance drop on harder benchmarks, we note that the truncation ratio during training is initially very high (Figure 6), exceeding 45%, and remains above 10% even after 200 rollout steps. Specifically, for the AIME dataset, over 75% of responses exceed 8192 tokens, compared to only 15% for MATH500. This indicates that truncation disproportionately impacts more complex tasks like AIME, where long reasoning trajectories are often necessary. Next, we formulate the truncation baseline from the reward shaping perspective, and connect it with related works."
        },
        {
            "title": "4 A Unified View on Efficient Reasoning with RL",
            "content": "In this section, we aim to understand the truncation baseline and other RL-based efficient reasoning approaches through unified perspective. We first connect them together via length-based reward shaping, and then derive new alternatives with this view. 4.1 The Unified Formulation Here we first present unified formulation, and then we show how the truncation baseline and other works fit into this formulation. Specifically, we define the reward function with two parts: correctness term C(y) and length-based term S(y) controlled by control variable λ(y): ˆR(x, y) = C(y) + λ(y) S(y) (2) In most length reward methods, C(y) = R(x, y), representing the original rule-based reward for correctness. However, in truncation-based approaches, C(y) = 0 as we discuss below. The term S(y) denotes the length reward, which varies across different methods. Formulating the Truncation Baseline As shown in Table 2, truncation is special case of the length reward with C(y) = 0, where the target length LT is enforced by the context window. ThinkPrune [10] is another truncation-based approach, which extends vanilla truncation by introducing an adaptive target length LA to replace fixed target length LT . They iteratively choose LA and separate their training into three stages. Table 2 also outlines other formulations, we will introduce them individually in the following sections. 4.2 Connecting Previous Efforts Together In this part, we build on our unified formulation in Eq. 2 and aim to connect previous approaches, below we describe several main categories of them. Table 2 formulates different length-based reward shaping by different designs for C(y), λ(y) and S(y). Parameter α is hyperparameter coefficient that controls the magnitude of the length reward S(y). We provide detailed explanations for each formulation in Appendix D. Group-based Reward In group-based reward, the length reward S(y) is designed to encourage brevity by assigning higher scores to shorter responses within rollout group, such as Efficient Reasoning [2] and Kimi-k1.5 [11] as formulated in Table 2. However, this comparison-based approach can lead to reward hacking. Models tend to exploit S(y) by generating overly concise responses, particularly for simpler questions. We demonstrate this phenomenon for the Efficient Reasoning baseline in Figure 7a and Figure 7b in Appendix C, where training accuracy initially decreases while total reward increases. Additionally, Table 3 shows more significant drop in MATH500 accuracy compared to other methods, further supporting this observation. 4 Table 2: Formulation of different approaches based on Eq. 2. C(y) is mainly for correctness, S(y) is the length reward, and λ(y) is control variable to control how length reward is applied. I(R) stands for I(R(x, y) = 1) and I() is an indicator function. ρ is the negative reward given for incorrect responses. L(y) is the length of the generated response. α is the coefficient that controls the magnitude of the length reward. The shapes of different rewards are shown in the visualization, where axis is the length of the response.Blue represents the curve for correct responses, while Red represents the curve for incorrect responses. For approaches, ThinkPrune, LASER-D and LASER-DE, there are different lines with similar colors indicate that the reward is dynamic which is realized by different LA values. The details of visualization are available in the Appendix J. Name C(y), λ(y) S(y) Visualization Truncation Method Vanilla Truncation ThinkPrune [10] Group-based Reward 0, 1 0, 1 (cid:26)R(x, y) ρ if L(y) LT if L(y) > LT (cid:26)R(x, y) ρ if L(y) LA if L(y) > LA Efficient Reasoning [2] R(x, y), I(R) α σ (cid:16) L(y)Mean(y) STD(L) (cid:17) Kimi-k1.5 [11] R(x, y), 1 Budget-based Reward (cid:40)0.5 L(y)Lmin LmaxLmin 0, 0.5 L(y)Lmin LmaxLmin min (cid:16) (cid:17) if I(R) = 1 if I(R) = 0 L1-Exact [1] R(x, y), 1 α L(y) LT L1-Max [1] 0, I(R) clip(α (L(y) LT ) + δ, 0, 1) Length-Based Step Reward and Variants LASER LASER-D R(x, y), I(R) α I(L(y) LT ) R(x, y), I(R) α I(L(y) LA) LASER-DE R(x, y), 1 α I(R) I(L(y) LA) + α (1 I(R)) I(L(y) > LA) Budget-based Reward Budget-based rewards use query-specific target lengths (budgets) and penalize responses that deviate from these instructions. While this mitigates reward hacking seen in group-based schemes, it can destabilize training. Models require exposure to diverse budgets, but in large context windows (e.g., 16,384 tokens), these targets become sparsely distributed, causing reward fluctuations. Figure 7b illustrates this instability. Using L1-Max as representative method, we observe that with smaller contexts (4,096 tokens), it achieves stable reward increases comparable to other methods. However, with 16,384-token contexts, rewards fluctuate significantly and underperform alternative approaches. 4.3 Bridging the Gap: Length-based Step Reward As shown in Eq. 2 and visualized in Table 2, key limitation of the truncation method is that it assigns the same penalties to overlong responses as it does to incorrect ones, which may over-penalize long but correct explorations. To address this issue, we extend it as novel reward shaping approach called 5 Length-bAsed StEp Reward (LASER), which adopts step reward function guided by target length, rather than performing hard truncation. Specifically, we design the length reward term S(y) as an indicator function based on target length LT . This function assigns length-based bonus to responses shorter than LT . We also set the context window significantly larger than the target length LT (e.g., 16,384 vs. 4,096) where truncation rarely happens. And the length reward term S(y) is only activated when responses are correct, thereby improving the efficacy-efficiency trade-off. As visualized in Table 2, LASER closely resembles the vanilla truncation approach; the only difference is that, instead of truncating long responses, it awards bonus rewards to correct responses that do not exceed the target length. To balance the correctness reward C(y) and length reward S(y), we follow typical setting and set α as 0.5. Empirical results are demonstrated in Figure 1 and Table 3, training with the LASER reward achieves improved Pareto-optimality compared to all previous methods. Notably, it is the first approach to simultaneously deliver significant improvements in both accuracy and token efficiency on the challenging AIME24 benchmark. These results establish LASER as promising reward design framework for enhancing the balance between efficacy and efficiency."
        },
        {
            "title": "5 Adaptive Length-based Step Reward Shaping",
            "content": "5.1 Design Principles We highlight two key limitations not addressed in the design of LASER: (1) LASER requires specifying fixed target length prior to training; however, as the model evolves during training, the optimal response length may also change and should ideally adapt dynamically. (2) Additionally, different questions demand reasoning traces of varying lengthssimple questions may be effectively addressed with shorter reasoning, while more complex questions benefit from longer, more detailed deliberation. Therefore, we extend LASER to be Dynamic and Difficulty-aware, which we term as LASER-D. Rather than using single fixed target length, our approach dynamically adjusts the target length throughout training and tailors it to questions of varying difficulty. Concretely, LASER-D decouples the target length hyperparameter across different queries, allowing distinct target lengths to be assigned to various queries. Moreover, these target length hyperparameters are dynamically adjusted throughout training. We separate queries into three buckets of easy, medium, and hard difficulty levels, based on the correctness rates within the rollout batch for each question, we have rollouts and use thresholds k/3 and 2k/3 to separate them. As such, we have three distinct target length hyperparameters for these three query groups. Notably, we perform difficulty assessment for the queries during realtime RL training and use the training rollout batch, thus it only incurs negligible overhead on the computation. Being dynamic and difficulty-aware, one challenge raised is how to set the dynamic processes of the decoupled target length hyperparameters. Next, we introduce an automatic adapting mechanism, to adapt them without any manual intervention. 5.2 Automatic Adapting Mechanism LASER-D is driven by an automatic adapting mechanism that periodically evaluates and adjusts the target length parameters (LA in Table 2) for each difficulty level. Specifically, we first extract small monitoring dataset DM (e.g., 500 samples) from training data that mirrors the distribution of the training data. Every training steps (e.g., 20), our approach searches and sets the target length hyperparameters based on this monitoring dataset. Denote the three-class difficulty level of query as d, to determine the target length hyperparameters, we propose metric called Expected Correct Responses (ECR), which estimates how many complete, correct responses we can expect for each difficulty level given response length limits. Formally, we sample responses for each query in the monitoring set,2 and ECR is computed as 2Practically, is set to be the same as the rollout size used during training, in order to maintain consistency with the training scenario. ECRd = Pl,d Cd (3) where Pl,d is the coverage ratio (proportion of responses that fit within given token length l). The value Cd is fixed for each difficulty group. Since we use the ratio of correct responses within each rollout group to determine the difficulty level, there is minimum number of correct responses for each group (e.g., 6, 3, and 1 correct responses for easy, medium, and hard levels, respectively, when = 8). We set Cd as these minimum values for each group. The monitoring module enumerates potential target lengths from the lower bound target length LT tokens up to the maximum context window (16,384 tokens) with an interval of I, computing coverage ratios Pl,d for each length. We select the smallest target length as the adaptive target length LA satisfying ECRd 1 for each difficulty level d, ensuring at least one complete and correct response. Intuitively, this mechanism sets the target length as the minimal generation length such that at least one rollout response is expected to be correct. This approach is reasonable because generating sequences shorter than this length would likely be detrimental, as correct responses are less probable. Conversely, generating longer sequences may be redundant, since correct responses can already be obtained with shorter generation length. Dynamic and Difficulty-Aware Reward During training, we apply these monitoring-derived parameters to implement dynamic and difficulty-aware rewards. Each training questions difficulty level is determined using the same classification method described earlier. Easier questions receive smaller target lengths (i.e. smaller scaling factor β), while harder questions receive larger ones (i.e. larger scaling factor β). Since monitoring runs every steps, the difficulty-dependent target lengths are automatically adapted to the evolving policy model. Computational Efficiency This automatic adapting mechanism adds minimal computational overhead. By using small monitoring dataset and evaluating only periodically, our method increases computation by just 3.5% in our experiments. 5.3 LASER-DE: Variant of LASER-D to Encourage Exploration Previous works [13, 10] find that with more test-time compute, the reasoning ability of models will improve. Meanwhile, some other works [28] show that incorrect responses tend to produce more tokens. Both findings are related to the exploration of policy models, where models try to explore by consuming more compute to get the correct answers. Therefore, we further propose variant of LASER-D, named LASER-DE, to encourage the exploration of policy models for those incorrect responses. The only difference for LASER-DE is to encourage those incorrect responses to be further explored to find correct pattern by applying reduced penalties to those that are incorrect and exceed the target length. The form of LASER-DE can be seen in Table 2."
        },
        {
            "title": "6 Experiments",
            "content": "6.1 Experimental Setup sizes known for Setup We experiment with three capable and representative LRMs across three different their overthinking tendencies: DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Qwen-32B . We adhere to the original prompt from DeepSeek-R1 [4], with the full prompt available in Appendix E.1. We train these models using the DeepScaleR-Preview-Dataset [12], high-quality mathematics dataset containing 40K competition-level question-answer pairs. We evaluate the models on four benchmarks of varying difficulty: MATH500 [9], OlympiadBench [7], AIME 2024, and AMC 2023. We set α = 0.5 for our methods in all experiments to balance the trade-off between correctness rewards and solution length penalties. LT is hyper-parameter for our approaches because the automatic adapting mechanism will enumerate the target length from LT to the context window size to select the adaptive target lengths LA, as described in 5.2. Parameter settings for baseline methods are provided in Appendix E.3, and full details of our training procedure and evaluation methodology can be found in Appendix E.2. Baselines According to Table 2, we train models using different types of length rewards design and compare our LASER, LASER-D, LASER-DE to previous works. Considering the high computational cost of RL training, we select Efficient Reason [2] and L1-Max [1] as the representatives, since they perform better accuracy compared to other methods inside same group and are more close to our settings. For ThinkPrune [10], we re-evaluate their open-sourced models. 7 Table 3: Accuracy (%) with average token usage for each dataset and different methods. Most important results in this table are visualized in Figure 1 and Figure 5 in Appendix A. The base model is DeepSeek-R1-Distill-Qwen-1.5B. \"Original\" denotes the original model. Tk is the truncation method with context window k. Group denotes the Efficient Reasoning [2] with different α. Due to the space limit, we only show three most representative results here. For the full results, please refer to Tabel 6 in Appendix H. Accuracy (%) MATH 500 AIME AMC Olympiad Bench Avg. MATH Generation Length (tokens) Olympiad Bench AIME AMC Original T8192 T6144 T4096 Groupα=0.4 Groupα=0.2 Groupα=0.1 Groupα=0.05 L1-Max-1024 L1-Max-4096 LASERLT =2048 LASERLT =4096 LASERLT =8192 LASER-DLT =1024 LASER-DLT =2048 LASER-DLT =4096 LASER-DELT =1024 LASER-DELT =2048 LASER-DELT =4096 83.9 81.8 80.9 77.7 74.6 78.1 77.0 74.4 76.4 79.7 83.6 83.9 85.6 83.0 82.2 84.2 82.1 83.9 83.5 28.9 24.8 20.2 19.2 25.0 28.1 29.0 30.2 15.0 20.0 29.2 31.0 31.5 30.6 31.0 34.2 33.8 31.5 35.0 71.6 70.9 66.2 62.2 69.2 68.0 69.5 65.5 59.4 65.0 71.6 74.1 75.9 72.8 73.3 75.3 72.2 75.3 73. 43.3 43.9 42.1 38.5 43.1 44.4 44.9 43.1 39.1 41.0 44.1 45.7 47.7 43.7 46.2 47.3 43.7 46.4 46.0 56.9 55.3 52.3 49.4 53.0 54.7 55.1 53.3 47.5 51.4 57.1 58.7 60.2 57.5 58.2 60.3 58.0 59.3 59.5 5042 1795 1351 1054 1069 1135 1228 1193 661 875 1913 1914 2736 1362 1623 1872 1350 1456 1949 15956 4465 2821 2481 4747 5628 6301 4839 1303 1718 4815 5915 6589 4991 5158 5750 4794 5263 5789 8202 2560 1917 1484 2162 2635 2808 2457 933 1159 2493 3136 4162 256 2572 2981 2254 2679 3080 11510 2841 1947 1564 2536 2944 3271 2703 938 1229 2767 3579 4547 2837 2960 3474 2654 2971 Avg. 10177 2915 2009 1646 2629 3085 3402 2798 959 1245 2895 3636 4509 2862 3059 3520 2763 3092 3577 6.2 Efficacy-Efficiency Trade-off Since there is trade-off between accuracy and response length, one of the best ways to evaluate different methods is to compare their Pareto-optimal frontiers. We start with the DeepSeek-R1-DistillQwen-1.5B model as its small size allows us to run multiple experiments to investigate the trade-off of different approaches. To fully evaluate the potential of each method, we adjust key parameters (α for group-based reward, LT for other methods) to explore different tradeoffs along the accuracy-length trade-off curve. The full details of different hyper-parameters for different methods can be found in Table 5. As result, each point in Figure 1 and Figure 5 represents separate experiment with fully trained model using distinct hyperparameter configuration. We also list the results in different benchmarks in Table 3. Due to the space limit, we leave some results of truncation methods in Table 6. As shown in Figure 1, both LASER-D and LASER-DE achieve better Pareto-optimal frontiers compared to all other methods. On the AIME2024 benchmark, LASER-DE attains the highest accuracy of 35% using just over 5,500 tokensa substantial reduction by 63%. Meanwhile, LASERD still achieves 34% accuracy with only 4,600+ tokens, underscoring its strong trade-off. Across all benchmarks (Figure 5), LASER-DE achieves the most optimal trade-off when the average token usage is below 3,500, while LASER-D performs the best in higher token regimes. Specifically, LASER-D achieves 60.3% accuracy with only 3,520 tokens on average, representing substantial reduction from the 10,177 tokens used by the original model. Compared to the LASER method, both LASER-D and LASER-DE achieve significant improvements, demonstrating that incorporating dynamic and difficulty-aware mechanism greatly enhances the efficacy-efficiency trade-off. Compared to other baseline methods, LASERstill exhibits more favorable trade-off. 6.3 Experiments on Larger Models To better evaluate the effectiveness of our proposed methods,LASER, LASER-D, and LASER-DE. We conduct experiments on DeepSeek-R1-Distill-Qwen-7B , as shown in Table 4. Given the computational cost of larger models, we set key hyperparameters for each method to achieve an appropriate trade-off. Specifically, we set α = 0.2 for the group-based reward, LT = 8192 for the truncation method in LASER, LT = 4096 for LASER-D and LASER-DE. Notably, we do not tune α with fixed value 0.5 in all experiments of our methods. As shown in Table 4, LASER-D achieves the best 8 Table 4: Accuracy (%) with average token usage for each dataset and different methods using 7B and 32B models. \"Original\" denotes the original model. Tk is the truncation method with context window k. Accuracy (%) MATH 500 AIME AMC Olympiad Bench Avg. MATH 500 Generation Length (tokens) Olympiad Bench AIME AMC DeepSeek-R1-Distill-Qwen-7B Original T8192 Group LASER LASER-D LASER-DE 92.6 92.0 89.4 92.2 92.2 92.0 53.1 51.9 48.1 54.4 58.3 55.8 88.4 88.3 82.8 89.7 90.0 89.1 58.9 56.4 53.7 58.1 61.0 58.9 73.3 72.2 68.5 73.6 75.4 74. 4017 1972 780 2317 1836 1658 13414 5655 4271 6320 5379 4969 6433 3159 1693 3733 2694 2612 DeepSeek-R1-Distill-Qwen-32B Original LASER-DE 94.4 93. 71.7 70.8 93.1 93.1 64.6 62.2 80.95 79.83 3553 2314 10335 6177 3545 8987 3606 2348 4262 3350 3157 7697 4608 Avg. 8213 3598 2273 4158 3315 3099 6941 Figure 2: Performance on out-of-domain benchmarks: GPQA and average performance across all three benchmarks (GPQA, MMLU, LSAT). trade-off with better accuracy and significantly fewer tokens. On the AIME dataset, it achieves an accuracy of 58.3%, representing gain of +5.2 points, while using only 5, 379 tokenssubstantially fewer than the 13, 414 tokens used by the original model. Compared to other methods, LASER, LASER-D, and LASER-DE also attain better trade-offs on most benchmarks, particularly on the more challenging ones. For the 32B model, due to computational constraints, we compare the LASER-DE-trained model with the original baseline under this larger setting and set LT = 8192. LASER-DE achieves competitive accuracy with only minor drop (1%), while still significantly reducing output length. Notably, the accuracy of DeepSeek-R1-Distill-Qwen-32B on our training dataset is already very highover 76%, leaving little room for further improvement. We speculate that with more challenging and diverse training data, LASER-DE could yield further accuracy gains. 6.4 Experiments on Out-of-Domain Benchmarks We evaluate whether LASER, LASER-D and LASER-DE can generalize to domains outside the RL training distribution. We select three out-of-domain benchmarks: GPQA [17], LSAT [23], and MMLU [8], following the evaluation settings established by L1 [1]. Figure 2 illustrates the efficacyefficiency trade-off on GPQA and the average performance across all benchmarks. Compared to the original model, LASER, LASER-D and LASER-DE consistently achieve significant improvements in both accuracy and token usage, demonstrating robust generalization capabilities. And LASER-D and LASER-DE maintain the best trade-off even when compared to LASER."
        },
        {
            "title": "7 Analysis",
            "content": "We use DeepSeek-R1-Distill-Qwen-1.5B as the backbone, conduct comprehensive analysis that includes budget-forcing inference, dynamics of adaptive target lengths, shifts in reasoning patterns, and qualitative evaluations. Due to the space limit, please refer to Appendix for budget-forcing inference, Appendix for dynamics of adaptive target lengths. 7.1 Changes of Thinking Patterns To better understand the changes of response length, we analyze the changes of thinking patterns over RL iterations on AIME2024 with 16 samples per question. We analyze through two approaches, keywords counts [26] and reasoning behavior ratios [28]. Shifts in Self-Reflection Keywords Self-reflection or Aha-moment reasoning has emerged as an intriguing behavior in LRMs [4]. Following previous work [26], we track this behavior by monitoring seven representative keywords: [recheck,rethink,try again,wait, alternatively,retry,however]. As shown in Figure 3, the average amount of these keywords (occurrences per token) declines notably as response length decreases across all methods. This suggests that RL may reduce instances of spurious self-reflection, previously identified as contributor to over-thinking [3]. Interestingly, as training progresses, we observe increased keyword amount while maintaining shorter outputs, indicating models develop more efficient self-reflection behaviors without producing verbose responses. Changes in Thinking Behaviors To further investigate the changes of reasoning patterns beyond keyword statistics, we employ gpt-4.1-mini to perform more fine-grained analysis of cognitive behaviors throughout the training process. Specifically, we adopt the cognitive behavior framework proposed by [5], which identifies reasoning-related behaviors such as Backtracking, Verification, Subgoal Setting, and Enumeration. We report the proportion of each behavior relative to the total number of behaviors, focusing on these four representative categories. The complete list of behaviors and implementation details are provided in Appendix K. As shown in Figure 4, the proportion of Backtracking behavior decreases significantly, from over 30% to just above 10%, as the response length is reduced. This trend aligns with the keyword statistics, as many of the tracked keywords (e.g., recheck, retry, rethink) are indicative of Backtracking. While Backtracking declines during training, the proportions of other reasoning behaviors, Verification, Enumeration, and Subgoal Setting, remain stable, with slight increase observed in Subgoal Setting. These results suggest that reducing response length does not degrade the model into non-reasoning baseline. On the contrary, core reasoning behaviors are preserved, while unnecessary backtracking is minimized, indicating more efficient reasoning in the refined models. 7.2 Qualitative Analysis to understand how RL improves reasoning efficiency. and the MATH500 We conduct qualitative analysis on the trivial question 1+1=? dataset Comparing the original DeepSeek-R1-Distill-Qwen-1.5B model with the LASER-D-trained version, Figure 1 illustrates how the original model generates repetitive self-reflection even for trivial questions, while the trained model directly provides the answer. Our analysis of MATH500 (detailed in Appendix L) reveals that the original model tends towards verbose, redundant explanations of single ideas. In contrast, the LASER-D-trained model expresses the same concepts more succinctly using structured formulas, significantly improving token efficiency. This suggests our RL-based approach not only reduces unproductive backtracking but also encourages shift towards more concise and direct expression."
        },
        {
            "title": "8 Conclusion",
            "content": "In this paper, we propose unified view for RL-based CoT compression, unifying various rewardshaping and truncation methods. Building on this view, we introduce new approaches with adaptive, length-based reward shaping. Extensive experiments demonstrate that our proposed methods achieve superior Pareto-optimality and significant improvements in both accuracy and token efficiency. Our 10 Figure 3: Average keyword amount and response length over RL training on AIME24. The truncation method uses 8192 token context window. LASER, LASER-D, and LASER-DE employ target length of LT = 2048. Figure 4: Changes in reasoning behaviors ratio and response length over RL training iterations on AIME2024. The figure shows how LASER-DEs thinking patterns change during training with target length of LT = 2048. analysis of reasoning behaviors reveals that our RL-based CoT compression effectively encourages models to reason more concisely and productively."
        },
        {
            "title": "References",
            "content": "[1] P. Aggarwal and S. Welleck. L1: Controlling how long reasoning model thinks with reinforcement learning, 2025. URL https://arxiv.org/abs/2503.04697. [2] D. Arora and A. Zanette. Training language models to reason efficiently, 2025. URL https://arxiv. org/abs/2502.04463. [3] X. Chen, J. Xu, T. Liang, Z. He, J. Pang, D. Yu, L. Song, Q. Liu, M. Zhou, Z. Zhang, R. Wang, Z. Tu, H. Mi, and D. Yu. Do not think that much for 2+3=? on the overthinking of o1-like llms, 2025. URL https://arxiv.org/abs/2412.21187. [4] DeepSeek-AI, D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, X. Zhang, X. Yu, Y. Wu, Z. F. Wu, Z. Gou, Z. Shao, Z. Li, Z. Gao, A. Liu, B. Xue, B. Wang, B. Wu, B. Feng, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan, D. Dai, D. Chen, D. Ji, E. Li, F. Lin, F. Dai, F. Luo, G. Hao, G. Chen, G. Li, H. Zhang, H. Bao, H. Xu, H. Wang, H. Ding, H. Xin, H. Gao, H. Qu, H. Li, J. Guo, J. Li, J. Wang, J. Chen, J. Yuan, J. Qiu, J. Li, J. L. Cai, J. Ni, J. Liang, J. Chen, K. Dong, K. Hu, K. Gao, K. Guan, K. Huang, K. Yu, L. Wang, L. Zhang, L. Zhao, L. Wang, L. Zhang, L. Xu, L. Xia, M. Zhang, M. Zhang, M. Tang, M. Li, M. Wang, M. Li, N. Tian, P. Huang, P. Zhang, Q. Wang, Q. Chen, Q. Du, R. Ge, R. Zhang, R. Pan, R. Wang, R. J. Chen, R. L. Jin, R. Chen, S. Lu, S. Zhou, S. Chen, S. Ye, S. Wang, S. Yu, S. Zhou, S. Pan, S. S. Li, S. Zhou, S. Wu, S. Ye, T. Yun, T. Pei, T. Sun, T. Wang, W. Zeng, W. Zhao, W. Liu, W. Liang, W. Gao, W. Yu, W. Zhang, W. L. Xiao, W. An, X. Liu, X. Wang, X. Chen, X. Nie, X. Cheng, X. Liu, X. Xie, X. Liu, X. Yang, X. Li, X. Su, X. Lin, X. Q. Li, X. Jin, X. Shen, X. Chen, X. Sun, X. Wang, X. Song, X. Zhou, X. Wang, X. Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. Zhang, Y. Xu, Y. Li, Y. Zhao, Y. Sun, Y. Wang, Y. Yu, Y. Zhang, Y. Shi, Y. Xiong, Y. He, Y. Piao, Y. Wang, Y. Tan, Y. Ma, Y. Liu, Y. Guo, Y. Ou, Y. Wang, Y. Gong, Y. Zou, Y. He, Y. Xiong, Y. Luo, Y. You, Y. Liu, Y. Zhou, Y. X. Zhu, Y. Xu, Y. Huang, Y. Li, Y. Zheng, Y. Zhu, Y. Ma, Y. Tang, Y. Zha, Y. Yan, Z. Z. Ren, Z. Ren, Z. Sha, Z. Fu, Z. Xu, Z. Xie, Z. Zhang, Z. Hao, Z. Ma, Z. Yan, Z. Wu, Z. Gu, Z. Zhu, Z. Liu, Z. Li, Z. Xie, Z. Song, Z. Pan, Z. Huang, Z. Xu, Z. Zhang, and Z. Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. [5] K. Gandhi, A. Chakravarthy, A. Singh, N. Lile, and N. D. Goodman. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars, 2025. URL https://arxiv.org/abs/2503. 01307. [6] S. Hao, S. Sukhbaatar, D. Su, X. Li, Z. Hu, J. Weston, and Y. Tian. Training large language models to reason in continuous latent space, 2024. URL https://arxiv.org/abs/2412.06769. [7] C. He, R. Luo, Y. Bai, S. Hu, Z. Thai, J. Shen, J. Hu, X. Han, Y. Huang, Y. Zhang, J. Liu, L. Qi, Z. Liu, and M. Sun. OlympiadBench: challenging benchmark for promoting AGI with olympiad-level bilingual 11 multimodal scientific problems. In L.-W. Ku, A. Martins, and V. Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 38283850, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.211. URL https://aclanthology.org/2024.acl-long.211/. [8] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding, 2021. URL https://arxiv.org/abs/2009.03300. [9] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. [10] B. Hou, Y. Zhang, J. Ji, Y. Liu, K. Qian, J. Andreas, and S. Chang. Thinkprune: Pruning long chain-ofthought of llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2504.01296. [11] Kimi, A. Du, B. Gao, B. Xing, C. Jiang, C. Chen, C. Li, C. Xiao, C. Du, C. Liao, C. Tang, C. Wang, D. Zhang, E. Yuan, E. Lu, F. Tang, F. Sung, G. Wei, G. Lai, H. Guo, H. Zhu, H. Ding, H. Hu, H. Yang, H. Zhang, H. Yao, H. Zhao, H. Lu, H. Li, H. Yu, H. Gao, H. Zheng, H. Yuan, J. Chen, J. Guo, J. Su, J. Wang, J. Zhao, J. Zhang, J. Liu, J. Yan, J. Wu, L. Shi, L. Ye, L. Yu, M. Dong, N. Zhang, N. Ma, Q. Pan, Q. Gong, S. Liu, S. Ma, S. Wei, S. Cao, S. Huang, T. Jiang, W. Gao, W. Xiong, W. He, W. Huang, W. Wu, W. He, X. Wei, X. Jia, X. Wu, X. Xu, X. Zu, X. Zhou, X. Pan, Y. Charles, Y. Li, Y. Hu, Y. Liu, Y. Chen, Y. Wang, Y. Liu, Y. Qin, Y. Liu, Y. Yang, Y. Bao, Y. Du, Y. Wu, Y. Wang, Z. Zhou, Z. Wang, Z. Li, Z. Zhu, Z. Zhang, Z. Wang, Z. Yang, Z. Huang, Z. Huang, Z. Xu, and Z. Yang. Kimi k1.5: Scaling reinforcement learning with llms, 2025. URL https://arxiv.org/abs/2501.12599. [12] M. Luo, S. Tan, Li, R. A. Popa, by scaling rl. -with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2, 2025. Notion Blog. J. Luo, L. E. Surpassing o1-preview with 1.5b model https://pretty-radio-b75.notion.site/DeepScaleR-Surpassing-O1-Preview J. Wong, X. Shi, W. Y. Tang, M. Roongta, C. Cai, and I. Stoica. Deepscaler: [13] N. Muennighoff, Z. Yang, W. Shi, X. L. Li, L. Fei-Fei, H. Hajishirzi, L. Zettlemoyer, P. Liang, E. Candès, and T. Hashimoto. s1: Simple test-time scaling, 2025. URL https://arxiv.org/abs/2501.19393. [14] T. Munkhbat, N. Ho, S. H. Kim, Y. Yang, Y. Kim, and S.-Y. Yun. Self-training elicits concise reasoning in large language models, 2025. URL https://arxiv.org/abs/2502.20122. [15] OpenAI, :, A. Jaech, A. Kalai, A. Lerer, A. Richardson, A. El-Kishky, A. Low, A. Helyar, A. Madry, A. Beutel, A. Carney, A. Iftimie, A. Karpenko, A. T. Passos, A. Neitz, A. Prokofiev, A. Wei, A. Tam, A. Bennett, A. Kumar, A. Saraiva, A. Vallone, A. Duberstein, A. Kondrich, A. Mishchenko, A. Applebaum, A. Jiang, A. Nair, B. Zoph, B. Ghorbani, B. Rossen, B. Sokolowsky, B. Barak, B. McGrew, B. Minaiev, B. Hao, B. Baker, B. Houghton, B. McKinzie, B. Eastman, C. Lugaresi, C. Bassin, C. Hudson, C. M. Li, C. de Bourcy, C. Voss, C. Shen, C. Zhang, C. Koch, C. Orsinger, C. Hesse, C. Fischer, C. Chan, D. Roberts, D. Kappler, D. Levy, D. Selsam, D. Dohan, D. Farhi, D. Mely, D. Robinson, D. Tsipras, D. Li, D. Oprica, E. Freeman, E. Zhang, E. Wong, E. Proehl, E. Cheung, E. Mitchell, E. Wallace, E. Ritter, E. Mays, F. Wang, F. P. Such, F. Raso, F. Leoni, F. Tsimpourlas, F. Song, F. von Lohmann, F. Sulit, G. Salmon, G. Parascandolo, G. Chabot, G. Zhao, G. Brockman, G. Leclerc, H. Salman, H. Bao, H. Sheng, H. Andrin, H. Bagherinezhad, H. Ren, H. Lightman, H. W. Chung, I. Kivlichan, I. OConnell, I. Osband, I. C. Gilaberte, I. Akkaya, I. Kostrikov, I. Sutskever, I. Kofman, J. Pachocki, J. Lennon, J. Wei, J. Harb, J. Twore, J. Feng, J. Yu, J. Weng, J. Tang, J. Yu, J. Q. Candela, J. Palermo, J. Parish, J. Heidecke, J. Hallman, J. Rizzo, J. Gordon, J. Uesato, J. Ward, J. Huizinga, J. Wang, K. Chen, K. Xiao, K. Singhal, K. Nguyen, K. Cobbe, K. Shi, K. Wood, K. Rimbach, K. Gu-Lemberg, K. Liu, K. Lu, K. Stone, K. Yu, L. Ahmad, L. Yang, L. Liu, L. Maksin, L. Ho, L. Fedus, L. Weng, L. Li, L. McCallum, L. Held, L. Kuhn, L. Kondraciuk, L. Kaiser, L. Metz, M. Boyd, M. Trebacz, M. Joglekar, M. Chen, M. Tintor, M. Meyer, M. Jones, M. Kaufer, M. Schwarzer, M. Shah, M. Yatbaz, M. Y. Guan, M. Xu, M. Yan, M. Glaese, M. Chen, M. Lampe, M. Malek, M. Wang, M. Fradin, M. McClay, M. Pavlov, M. Wang, M. Wang, M. Murati, M. Bavarian, M. Rohaninejad, N. McAleese, N. Chowdhury, N. Chowdhury, N. Ryder, N. Tezak, N. Brown, O. Nachum, O. Boiko, O. Murk, O. Watkins, P. Chao, P. Ashbourne, P. Izmailov, P. Zhokhov, R. Dias, R. Arora, R. Lin, R. G. Lopes, R. Gaon, R. Miyara, R. Leike, R. Hwang, R. Garg, R. Brown, R. James, R. Shu, R. Cheu, R. Greene, S. Jain, S. Altman, S. Toizer, S. Toyer, S. Miserendino, S. Agarwal, S. Hernandez, S. Baker, S. McKinney, S. Yan, S. Zhao, S. Hu, S. Santurkar, S. R. Chaudhuri, S. Zhang, S. Fu, S. Papay, S. Lin, S. Balaji, S. Sanjeev, S. Sidor, T. Broda, A. Clark, T. Wang, T. Gordon, T. Sanders, T. Patwardhan, T. Sottiaux, T. Degry, T. Dimson, T. Zheng, T. Garipov, T. Stasi, T. Bansal, T. Creech, T. Peterson, T. Eloundou, V. Qi, V. Kosaraju, V. Monaco, V. Pong, V. Fomenko, W. Zheng, W. Zhou, W. McCabe, W. Zaremba, Y. Dubois, Y. Lu, Y. Chen, Y. Cha, Y. Bai, Y. He, Y. Zhang, Y. Wang, Z. Shao, and Z. Li. Openai o1 system card, 2024. URL https://arxiv.org/abs/2412.16720. [16] X. Qu, Y. Li, Z. Su, W. Sun, J. Yan, D. Liu, G. Cui, D. Liu, S. Liang, J. He, P. Li, W. Wei, J. Shao, C. Lu, Y. Zhang, X.-S. Hua, B. Zhou, and Y. Cheng. survey of efficient reasoning for large reasoning models: Language, multimodality, and beyond, 2025. URL https://arxiv.org/abs/2503.21614. 12 [17] D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman. Gpqa: graduate-level google-proof q&a benchmark, 2023. URL https://arxiv.org/abs/2311.12022. [18] Y. Ruan, N. Band, C. J. Maddison, and T. Hashimoto. Reasoning to learn from latent thoughts, 2025. URL https://arxiv.org/abs/2503.18866. [19] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [20] Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, M. Zhang, Y. Li, Y. Wu, and D. Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [21] G. Sheng, C. Zhang, Z. Ye, X. Wu, W. Zhang, R. Zhang, Y. Peng, H. Lin, and C. Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. [22] R. Wang, H. Wang, B. Xue, J. Pang, S. Liu, Y. Chen, J. Qiu, D. F. Wong, H. Ji, and K.-F. Wong. Harnessing the reasoning economy: survey of efficient reasoning for large language models, 2025. URL https://arxiv.org/abs/2503.24377. [23] S. Wang, Z. Liu, W. Zhong, M. Zhou, Z. Wei, Z. Chen, and N. Duan. From lsat: The progress and challenges of complex reasoning. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2022. [24] H. Xia, Y. Li, C. T. Leong, W. Wang, and W. Li. Tokenskip: Controllable chain-of-thought compression in llms, 2025. URL https://arxiv.org/abs/2502.12067. [25] A. Yang, B. Zhang, B. Hui, B. Gao, B. Yu, C. Li, D. Liu, J. Tu, J. Zhou, J. Lin, K. Lu, M. Xue, R. Lin, T. Liu, X. Ren, and Z. Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement, 2024. URL https://arxiv.org/abs/2409.12122. [26] E. Yeo, Y. Tong, M. Niu, G. Neubig, and X. Yue. Demystifying long chain-of-thought reasoning in llms, 2025. URL https://arxiv.org/abs/2502.03373. [27] Q. Yu, Z. Zhang, R. Zhu, Y. Yuan, X. Zuo, Y. Yue, T. Fan, G. Liu, L. Liu, X. Liu, H. Lin, Z. Lin, B. Ma, G. Sheng, Y. Tong, C. Zhang, M. Zhang, W. Zhang, H. Zhu, J. Zhu, J. Chen, J. Chen, C. Wang, H. Yu, W. Dai, Y. Song, X. Wei, H. Zhou, J. Liu, W.-Y. Ma, Y.-Q. Zhang, L. Yan, M. Qiao, Y. Wu, and M. Wang. Dapo: An open-source llm reinforcement learning system at scale, 2025. URL https://arxiv.org/abs/2503.14476. [28] W. Zeng, Y. Huang, Q. Liu, W. Liu, K. He, Z. Ma, and J. He. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild, 2025. URL https://arxiv.org/abs/2503.18892. [29] W. Zhong, S. Wang, D. Tang, Z. Xu, D. Guo, J. Wang, J. Yin, M. Zhou, and N. Duan. Ar-lsat: Investigating analytical reasoning of text, 2021. 13 Pareto-Optimality We illustrate the efficacy-efficiency trade-off in Figure 5. Our proposed methods, LASER, LASER-D, and LASER-DE, demonstrate significant improvements in both accuracy and token usage across all benchmarks, particularly in the most challenging ones. Notably, LASER-D and LASER-DE achieve Pareto-optimal trade-off compared to all other methods. (a) (b) Figure 5: Pareto-optimal trade-off between accuracy and response length across various methods. Each point represents single training run with different hyper-parameters. Our methods, LASER-DE, LASER-D, and LASER, achieve Pareto-optimal trade-off compared to all other methods. (a) Accuracy and response length on all benchmarks (MATH500, AIME2024, AMC2023, Olympiad Bench) (b) Accuracy and response length on AIME"
        },
        {
            "title": "B Ratio of Truncated Responses During Training with Truncation",
            "content": "We analyze the ratio of truncated responses when applying an 8192 token limit during training. Our findings show that the proportion of truncated responses is initially very highexceeding 45%, and remains substantial (above 10%) even after 200 rollout steps. This high truncation rate highlights the context window constraints in training is sub-optimal. Figure 6: The ratio of truncated responses in training data with 8192 tokens limit."
        },
        {
            "title": "C Dynamics of Accuracy and Rewards Throughout Training",
            "content": "We present the accuracy and rewards for various methods across training iterations in Figure 7a and Figure 7b. As discussed in 4, group-based rewards tend to exploit the length rewards S(y) 14 (a) (b) Figure 7: (a) Accuracy on training dataset across training iterations for different methods (b) Rewards across training iterations for different methods while causing significant drop in accuracy. Budget-based rewards such as L1-Max-16384 [1] suffer from unstable training when the context window is large. In contrast, other methods like truncation methods, LASER, LASER-D, and LASER-DE demonstrate simultaneous increase in both rewards and accuracy throughout the training process. Supplementary Details: Length-based Reward Shaping Formulations In this section, we provide additional details regarding the various formulations of length-based reward shaping as presented in Table 2. These formulations can viewed as different variants of our unified framework in Eq. 2 which can be implemented by making specific design choices for three key components: C(y), λ(y), and S(y) inside the framework. Here we review the formulation of Eq. 2 to better illustrate following approaches. ˆR(x, y) = C(y) + λ(y) S(y) D.1 Truncation Vanilla Truncation As aforementioned discussions (4), truncation is special case of the length reward with C(y) = 0, where the target length LT is enforced by the context window. ρ is set as 0. It follows the design: C(y) = 0 λ(y) = S(y) = (cid:26)R(x, y) ρ if L(y) LT if L(y) > LT ThinkPrune ThinkPrune [10] is another truncation-based approach, which extends vanilla truncation by introducing an adaptive target lengths LA to replace fixed target lengths LT . ρ is set as 0. The design follows: C(y) = 0 λ(y) = S(y) = (cid:26)R(x, y) ρ if L(y) LA if L(y) > LA Their training methodology employs progressive three-stage process with iterative refinement of LA. Each subsequent stage initializes from the checkpoint of the previous stage while mannually reducing the value of LA. Specifically, they progressively decrease LA through values of 4096, 3072, and 2048 across the three stages. D.2 Group-based Rewards In the context of group-based rewards, the length reward S(y) is specifically designed to promote brevity by assigning higher scores to shorter responses within rollout group. This mechanism functions as comparison-based reward system that inherently favors more concise responses. Most of them follow the design C(y) = R(x, y) to keep the accuracy performance of models. Efficient Reasoning Efficient Reasoning [2] follows the principle of group-based reward by specifically encouraging conciseness within correct responses. The mean and variance scalars are computed exclusively from the subset of correct responses, ensuring appropriate statistical distributions. By selectively rewarding conciseness only when answers are correct, this approach maintains higher accuracy compared to Kimi-k1.5 [11], which encourages wrong responses to be shorter. Considering the similarity between the two approaches and the better efficacy-efficiency trade-off, we select Efficient Reasoning as the representative group-based reward in this paper. The corresponding design can be formulated as follows: C(y) = R(x, y) λ(y) = I(R) S(y) = α σ (cid:18) L(y) ean(y) ST D(L) (cid:19) Kimi-k1.5 The design of Kimi-k1.5 is similar to Efficient Reasoning [2], with two main differences. First, the scalar factors are computed using the minimum response length and the difference between maximum response length and maximum length within rollout group. Second, Kimi-k1.5 encourages all responses to be shorter, rather than focusing solely on shortening correct responses. Such design has the potential to intensify reward hacking, as models may exploit the reward function by favoring shorter responses to maximize their scores. The designs follows: C(y) = R(x, y) λ(y) = 1 S(y) = D.3 Budget-based Reward (cid:40)0.5 L(y)Lmin LmaxLmin 0, 0.5 L(y)Lmin LmaxLmin min (cid:16) (cid:17) if I(R) = 1 if I(R) = 0 Budget-based rewards use query-specific target lengths (budgets) and penalize responses that deviate from these instructions. And the coefficient α controls the trade-off between length reward and correctness reward. They come in two flavors: exact mode and max mode. We follow same settings as L1 [1] and set α = 0.0003 for exact mode, α = 0.01 for max mode. Exact Mode deviation (even shorter outputs) is penalized. The design can be formulated as: In exact mode, the model must hit the specified target length LT exactly, and any C(y) = R(x, y) λ(y) = 1 S(y) = α L(y) LT Max Mode In max mode, only outputs that exceed LT incur penalty. The designs follow: C(y) = 0 λ(y) = I(R) S(y) = clip(α (L(y) LT ) + δ, 0, 1)"
        },
        {
            "title": "E Training Configurations",
            "content": "We leverage the prompt from DeepSeek-AI et al. [4], which is shown in Figure 8. And the mark for thinking is <think>...</think>. E.1 Training Prompt We list our training prompt in Figure 8, which follows the prompt from DeepSeek-R1 [4]. Figure 8: Training prompt for our training. E.2 Training and Evaluation Details We employ the verl [21] framework for model training and Qwen-Math-Eval [25] for evaluation. During training, we set the rollout batch size to 128, conduct 8 rollouts per prompt, use temperature of 0.6, and train with mini-batch size of 64. In our preliminary experiments, we found long-to-short RL benefits from clip-higher strategy [27]. So we follow DAPO [27] and set ϵhigh as 0.28. For evaluation, we maintain sampling temperature of 0.6 and permit maximum of 32,768 tokens to be generated. The number of samplings during evaluation is contingent on the dataset size: 4 samples per question for MATH500 and OlympiadBench, and 16 samples for AIME 2024 and AMC 2023. E.3 Full Hyper-Parameter List for Different Length-based Rewards We list the all hyper-parameters for LT , α and LA in Table 5. Table 5: The details of key hyper-parameters for different methods Methods Truncation Think-Prune Group-Based Rewards L1-Max LASER LASER-D LASER-DE Hyper-Parameters LT = [10240, 8192, 7168, 6144, 4098, 2048] LA = [4096, 3072, 2048] α = [0.4, 0.2, 0.1, 0.05] α = 0.01 LT = [8192, 4096, 2048] LT = [4096, 2048, 1024] LT = [4096, 2048, 1024] Budget-Forcing Inference To further analyze the impact of different length rewards, we conduct experiments using the budgetforcing setup introduced in S1 [13], which restricts the model to stop reasoning after fixed number of tokens B. We adopt their experimental setting and evaluate across = [500, 1000, 2000, 4000, 8000] We follow the budget-forcing implementations of Muennighoff et al. [13], Hou et al. [10]. Specifically, we follow their implementations and modify the codebase of Qwen-Math-Eval. We stop the thinking process of LRMs by appending </think>nn**Final Answer.**. Since empirically, DeepSeek-R1-Distill-Qwen-1.5B typically summarize its final answer starting with nn**Final 17 Answer.**. We use the same settings as our evaluations where we sample responses for multiple times with temperature = 0.6.. As shown in Figures 9a and 9b, despite not being explicitly trained with any budget-forcing mechanisms, LASER-D and LASER-DE consistently achieve strong trade-offs between accuracy and token efficiency, particularly on harder questions or when inference budgets are moderately constrained. While LASER performs competitively on average benchmarks, it lags behind LASER-D/LASERDE under strict token budgets or on more challenging examples. L1-Max, specifically trained to meet varying budget constraints during training, performs best under extremely tight budgets, demonstrating the strength of budget-specific optimization. However, its performance plateaus when more budget is available, limiting its ability to improve on harder tasks and resulting in suboptimal trade-off, as shown in Figure 9b. Group-based methods are also effective in low-budget scenarios due to their reward structure favoring shorter outputs, though this often leads to overly brief responses. ThinkPrune shows comparable performance to LASER under looser budgets but inherits the limitations of truncation-based approaches, struggling on difficult problems even when more tokens are available. (a) (b) Figure 9: Budget-forcing inference with different methods. (a) Average accuracy with different output budget on all benchmarks (b) The accuracy of different methods on AIME2024 with different output budget."
        },
        {
            "title": "G Dynamics of Adaptive Target Lengths",
            "content": "In this section, we analyze the dynamics of adaptive target lengths during the training process of LASER-D and LASER-DE. Figure 10 shows how the adaptive target length LA changes over training iterations for both methods. As demonstrated in Figure 10, our method dynamically selects appropriate target lengths based on problem difficulty. For easy problems (left figure), the model quickly identifies that shorter target lengths are sufficient. For medium-difficulty problems (middle figure), the model begins with longer target lengths (10,000+) and gradually reduces them to 3000-4000 as training continues. For difficult problems (right figure), the model consistently maintains target lengths near the maximum context window size, with some fluctuations attributable to computational precision issues. This adaptive behavior highlights the effectiveness of our approach in efficiently allocating computational resources based on problem complexity."
        },
        {
            "title": "H Full Main Results",
            "content": "We list the full results of different methods in Table 6. Full Experimental Results on Out-of-Domain Benchmarks Figure 11 illustrates the performance of various methods on out-of-domain benchmarks, including GPQA [17], LSAT [29, 23], and MMLU [8]. Across all benchmarks, LASER, LASER-D and LASERDE consistently demonstrate significant improvements in both accuracy and efficiency. Notably, 18 Figure 10: Dynamics of adaptive target lengths during the training process of LASER-D and LASER-DE. The figure shows how the adaptive target length LA changes over training iterations for problems of different difficulty levels (easy, medium, hard). For easy problems, the model selects short target lengths; for medium problems, it gradually decreases from higher initial values; and for hard problems, it maintains consistently high target lengths near the context window limit. This demonstrates the methods ability to adaptively allocate computational resources based on problem complexity, unlike fixed-length approaches. Table 6: Full results of accuracy (%) with average token usage for each dataset and different methods. The base model is DeepSeek-R1-Distill-Qwen-1.5B. \"Original\" denotes the original model. Tk is the truncation method with context window k. Group denotes the Efficient Reasoning [2] with different α. Due to the space limit, we only show three most representative results of truncation method here. Accuracy (%) MATH 500 AIME AMC Olympiad Bench Original T10240 T8192 T7168 T6144 T4096 T2048 Groupα=0.4 Groupα=0.2 Groupα=0.1 Groupα=0.05 L1-Max-1024 L1-Max-4096 LASERLT =2048 LASERLT =4096 LASERLT =8192 LASER-DLT =1024 LASER-DLT =2048 LASER-DLT =4096 LASER-DELT =1024 LASER-DELT =2048 LASER-DELT =4096 83.9 82.7 81.8 81.8 80.9 77.7 73.2 74.6 78.1 77.0 74.4 76.4 79.7 83.6 83.9 85.6 83.0 82.2 84.2 82.1 83.9 83. 28.9 26.9 24.8 23.3 20.2 19.2 15.8 25.0 28.1 29.0 30.2 15.0 20.0 29.2 31.0 31.5 30.6 31.0 34.2 33.8 31.5 35.0 71.6 73.1 70.9 68.6 66.2 62.2 56.9 69.2 68.0 69.5 65.5 59.4 65.0 71.6 74.1 75.9 72.8 73.3 75.3 72.2 75.3 73.3 43.3 44.1 43.9 43.0 42.1 38.5 35.9 43.1 44.4 44.9 43.1 39.1 41.0 44.1 45.7 47.7 43.7 46.2 47.3 43.7 46.4 46.0 MATH 500 Generation Length (tokens) Olympiad Bench AIME AMC 5042 2056 1795 1553 1351 1054 721 1069 1135 1228 1193 661 875 1913 1914 2736 1362 1623 1872 1350 1456 1949 15956 5458 4465 3726 2821 2481 1029 4747 5628 6301 4839 1303 1718 4815 5915 6589 4991 5158 5750 4794 5263 5789 8202 3036 2560 2251 1917 1484 936 2162 2635 2808 2457 933 1159 2493 3136 4162 2556 2572 2981 2254 2679 3080 11510 3405 2841 2323 1947 1564 1084 2536 2944 3271 2703 938 1229 2767 3579 4547 2837 2960 3474 2654 2971 3488 Avg. 10177 3489 2915 2463 2009 1646 943 2629 3085 3402 2798 959 1245 2895 3636 4509 2862 3059 3520 2763 3092 Avg. 56.9 56.7 55.35 54.18 52.35 49.4 45.45 53.0 54.7 55.1 53.3 47.5 51.4 57.1 58.7 60.2 57.5 58.2 60.3 58.0 59.3 59.5 these improvements extend even to the knowledge-intensive MMLU benchmark, highlighting the robust generalization capabilities of our proposed methods."
        },
        {
            "title": "J Visualization Details",
            "content": "In this appendix, we provide details about the visualization of different reward functions depicted in Table 2. These visualizations illustrate how different methods calculate rewards based on response length. 19 (a) GPQA (b) LSAT (c) MMLU (d) Average Figure 11: Performance on out-of-domain benchmarks including GPQA [17], LSAT [29, 23], and MMLU [8]. J.1 Visualization Parameters Each visualization captures the relationship between response length and reward value with the following specifications: X-axis: L(y) represents the response length, ranging from 0 to 20 tokens. Y-axis: Reward value, with different ranges depending on the method. Line styles: Solid lines represent rewards for correct responses (blue), while dashed lines represent rewards for incorrect responses (red). Target length (LT ): Set to 10 tokens for all methods. The visualizations were generated using high-resolution grid of 400 points between 0 and 20 tokens. J.2 Unified Reward Formulation Each method can be represented using the unified reward formula: ˆR(x, y) = C(y) + λ(y) S(y) We implement the specific components for each method in this simulation as follows. Note that the paramters are only used for better visualization which are different from the practical experiments. 20 Vanilla Truncation where LT = 10 and ρ = 0. ThinkPrune where LA {10, 7.5, 5}. Efficient Reasoning C(y) = 0 λ(y) = 1 S(y) = (cid:26)R(x, y) ρ if L(y) LT if L(y) > LT C(y) = 0 λ(y) = 1 S(y) = (cid:26)R(x, y) ρ if L(y) LA if L(y) > LA C(y) = R(x, y) λ(y) = I(R) S(y) = α σ (cid:18) L(y) ean(y) ST D(L) (cid:19) where µ = 10 and σ = 2. Kimi-k1.5 C(y) = R(x, y) λ(y) = S(y) = (cid:40)0.5 L(y)Lmin LmaxLmin 0, 0.5 L(y)Lmin LmaxLmin min (cid:16) (cid:17) if I(R) = 1 if I(R) = where Lmin = 2.5 and Lmax = 20. L1-Exact where α = 0.03 and LT = 10. L1-Max C(y) = R(x, y) λ(y) = 1 S(y) = α L(y) LT C(y) = 0 λ(y) = I(R) S(y) = clip(α (L(y) LT ) + δ, 0, 1) where α = 0.03 and LT = 10. LASER where LT = 10. LASER-D where LA {10, 7.5, 5}. C(y) = R(x, y) λ(y) = I(R) S(y) = α I(L(y) < LT ) C(y) = R(x, y) λ(y) = I(R) S(y) = α I(L(y) < LA) 21 LASER-DE C(y) = R(x, y) λ(y) = 1 S(y) = α I(R) I(L(y) LA) + α (1 I(R)) I(L(y) > LA) where LA {12.5, 10, 7.5}. For methods with multiple adaptive target lengths LA values (ThinkPrune, LASER-D, and LASERDE), different shades of the base colors were used: Correct responses (blue): RGB(26,71,142), RGB(62,101,184), RGB(125,154,230) Incorrect responses (red): RGB(139,0,0), RGB(183,50,40), RGB(224,93,86)"
        },
        {
            "title": "K Analysis of Reasoning Behaviros",
            "content": "We apply the cognitive behavior framework proposed by Gandhi et al. [5] to conduct detailed analysis of how reasoning behaviors change during our long-to-short RL. We use gpt-4.1-mini to perform more fine-grained analysis of cognitive behaviors throughout the training process. Following Zeng et al. [28], we use the prompt shown in Figure 12 to prompt gpt-4.1-mini to identify and analyze reasoning behaviors. We analyze these behaviors on AIME2024 by sampling one question 16 times, resulting in 480 responses for analysis. Since we start from LRM, reasoning behaviors such as backtracking naturally appear in every response, especially for challenging benchmarks. We specifically track four key behaviors: Backtracking, Verification, Enumeration, and Subgoal Setting. For each behavior, we calculate its frequency ratio relative to all behaviors and report how these ratios change throughout the training process. The complete list of all reasoning behaviors analyzed is provided in Table 7. Table 7: Complete list of reasoning behaviors Reasoning Behavior Subgoal Setting Enumeration Verification Backtracking Creative Analogy and Abstraction Abstraction and Parametrization Analytical Insight via Asymptotic Analysis Creative Abstraction / Coordinate Setup Use of Multiple Mathematical Tools and Identities Creative Analogies and Insightful Generalizations Algebraic Manipulation and Insightful Generalization Abstraction to Modular Arithmetic and Divisibility Creative Analogies and Abstractions Insightful Generalization / Alternative Modeling"
        },
        {
            "title": "L Qualitative Analysis on Efficient Reasoning",
            "content": "to understand how RL improves reasoning efficiency. and the MATH500 We conduct qualitative analysis on the trivial question 1+1=? Comparing the original dataset DeepSeek-R1-Distill-Qwen-1.5B model with the LASER-D-trained version, Figure 1 illustrates how the original model generates repetitive self-reflection even for trivial questions, while the trained model directly provides the answer. Our analysis of MATH500 (detailed in Figure 13, Figure 14 and Figure 15) reveals that the original model tends towards verbose, redundant explanations of single ideas. In contrast, the LASER-D-trained model expresses the same concepts more succinctly using structured formulas, significantly improving token efficiency. This suggests our RL-based approach not only reduces unproductive backtracking but also encourages shift towards more concise and direct expression. 22 Figure 12: Prompt used to identify and analyze reasoning behaviors with gpt-4.1-mini"
        },
        {
            "title": "M Limitations",
            "content": "Despite our works effective improvements in performance and efficiency, limitations remain. Our and most previous works focus primarily on the math as it provides an excellent verification environment and testbed for validating new methodologies. We believe further validation in code generation and agentic tasks would be valuable to determine if similar favorable trade-offs can be achieved in these contexts. Importantly, our methods were not specifically designed for mathematical tasks but were developed as domain-agnostic approaches that should naturally extend to other areas. In future work, we plan to explore more realistic scenario tasks, particularly those involving agentic reasoning, to further validate our approach and improve the efficacy-efficiency trade-off in more broader areas. 23 Figure 13: The full example of Figure 1 24 Figure 14: Additional case study demonstrating the evolution of reasoning efficiency. In this example, the original model required over 17K tokens to solve question from the MATH500 dataset, while our trained model accomplished the same task using only 1K+ tokens. Figure 15: Further example demonstrating improvements in reasoning approach"
        }
    ],
    "affiliations": [
        "Apple",
        "City University of Hong Kong",
        "The Hong Kong University of Science and Technology",
        "University of Waterloo"
    ]
}