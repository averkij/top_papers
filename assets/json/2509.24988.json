{
    "paper_title": "Generalized Correctness Models: Learning Calibrated and Model-Agnostic Correctness Predictors from Historical Patterns",
    "authors": [
        "Hanqi Xiao",
        "Vaidehi Patil",
        "Hyunji Lee",
        "Elias Stengel-Eskin",
        "Mohit Bansal"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generating accurate and calibrated confidence estimates is critical for deploying LLMs in high-stakes or user-facing applications, and remains an open challenge. Prior research has often framed confidence as a problem of eliciting a model's \"self-knowledge\", i.e., the ability of an LLM to judge whether its own answers are correct; this approach implicitly assumes that there is some privileged information about the answer's correctness that is accessible to the model itself. However, our experiments reveal that an LLM attempting to predict the correctness of its own outputs generally performs no better than an unrelated LLM. Moreover, we hypothesize that a key factor in building a \"Correctness Model\" (CM) is exposure to a target model's historical predictions. We propose multiple methods to inject this historical correctness information, creating a Generalized Correctness Model (GCM). We first show that GCMs can be trained on the correctness data from many LLMs and learn patterns for correctness prediction applicable across datasets and models. We then use CMs as a lens for studying the source of correctness prediction ability and its generalization, systematically controlling their training data and finding that answer phrasing is a strong predictor for correctness. We further explore alternative methods of injecting history without training an LLM, finding that including history as in-context examples can help improve correctness prediction, and post-hoc calibration can provide complementary reductions in calibration error. We evaluate GCMs based on Qwen3-8B across 5 model families and the MMLU and TriviaQA datasets, as well as on a downstream selective prediction task, finding that reliable LLM confidence estimation is a generalizable and model-agnostic skill learned by systematically encoding correctness history rather than a model-specific skill reliant on self-introspection."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 8 8 9 4 2 . 9 0 5 2 : r GENERALIZED CORRECTNESS MODELS: LEARNING CALIBRATED AND MODEL-AGNOSTIC CORRECTNESS PREDICTORS FROM HISTORICAL PATTERNS Hanqi Xiao1 Vaidehi Patil1 Hyunji Lee1 Elias Stengel-Eskin2 Mohit Bansal 1UNC Chapel Hill 2The University of Texas at Austin"
        },
        {
            "title": "ABSTRACT",
            "content": "Generating accurate and calibrated confidence estimates is critical for deploying LLMs in high-stakes or user-facing applications, and remains an open challenge. Prior research has often framed confidence as problem of eliciting models self-knowledge, i.e., the ability of an LLM to judge whether its own answers are correct; this approach implicitly assumes that there is some privileged information about the answers correctness that is accessible to the model itself. However, our experiments reveal that this assumption does not hold. Whether trained or training-free, an LLM attempting to predict the correctness of its own outputs generally performs no better than an unrelated LLM attempting the same task. In other words, LLMs have little self-knowledge for the purposes of correctness prediction. Moreover, we hypothesize that key factor in predicting model correctness, i.e., building Correctness Model (CM), is exposure to target models historical predictions. We propose multiple methods to inject this historical correctness information, including training an LLM to predict the confidences of many other LLMs, i.e., creating Generalized Correctness Model (GCM). We first show that GCMs can be trained on the correctness of historical predictions from many LLMs and learn patterns and strategies for correctness prediction applicable across datasets and models. We then use CMs as lens for studying the source of correctness prediction ability and its generalization, systematically controlling their training data and finding that answer phrasing (i.e. how an LLM phrases and elaborates an answer) is strong predictor for correctness. Moreover, our results suggest that CMs ability to leverage world knowledge about answers for correctness prediction is key enabler for generalization. We further explore alternative methods of injecting history without training an LLM, finding that including history as in-context examples can help improve correctness prediction, and post-hoc calibration can provide complementary reductions in calibration error. We evaluate GCMs based on Qwen3-8B across 5 model families and the MMLU and TriviaQA datasets, as well as on downstream selective prediction task, finding that reliable LLM confidence estimation is generalizable and model-agnostic skill learned by systematically encoding correctness history rather than model-specific skill reliant on self-introspection."
        },
        {
            "title": "INTRODUCTION",
            "content": "Confidence information is critical to understanding whether we should trust systems response to given query. For Large Language Models (LLMs), confidences enable us to understand honesty in model (Kadavath et al., 2022), identify hallucinations (Zhou et al., 2025), route to experts when unconfident (Hu et al., 2024), rejection sample (Chuang et al., 2025), and even be leveraged as an RL signal to improve the quality of models behavior (Li et al., 2025b). Confidence calibration is the idea that we should enforce desirable quality for confidences: calibrated models confidence should correspond to the empirical rate at which the models responses are correct, i.e., outputting 90% confidence on an answer should correspond to 90% chance of the answer being correct. 1Code: https://github.com/The-Inscrutable-X/CalibratedModelAgnosticCorrectness. 1 Figure 1: RQ1 & RQ2 overview. (Left) Selfvs. cross-model correctness prediction across Qwen and Llama: accuracies are comparable for each predictor model, suggesting no inherent advantage to model predicting its own outputs. (Right) Historical information improves calibration: (a) training on multiple models histories learns generalizable strategies for correctness prediction; (b) predictive power comes from phrasing of output, CMs world knowledge, and matching performance to question type. Each stage generalizes, and most prominently strategies for applying world knowledge; (c) History injected with post-hoc calibration and in-context learning helps improve correctness without finetuning. The GCM combines insights from RQs to achieve high accuracy and extremely low calibration error for the correctness prediction of multiple models, outperforming the logits of equally-sized and larger models. We include mapping for results in this figure to experimental settings in Appendix A. Many current approaches to LLM confidence estimation involve asking models to predict the correctness of their own responses, and are rooted in extracting the knowledge that LLMs have about their own correctness (Kadavath et al., 2022; Azaria & Mitchell, 2023; Li et al., 2024; Yin et al., 2023). To measure and improve the calibration of confidence estimates, these approaches also generally inherit frameworks and metrics from forecasting, where it is standard practice to calibrate forecasts of future events (Degroot & Fienberg, 1983; Guo et al., 2017a; Tian et al., 2023). However, key component is missing in this forecasting analogy: history. Human forecasters attempt to calibrate themselves by explicitly recording their confidence on predictions over time and tracking systematic biases, which allows them to adjust and improve their performance (Mellers et al., 2015), albeit imperfectly. Unlike humans who have privileged information about their own mental states and memory of their past actions current LLMs generally approach tasks without running history mechanism for tracking historical performance. Moreover, when framing confidence estimation as correctness prediction task, it is not clear that any given LLM is better-suited to predict its own correctness. In both cases, given query q, predicted response containing predicted answer ˆr, the model is simply producing Pθ( is correct(ˆr) q, r, ˆr) there is no obvious reason why this prediction should be better when the same LLM parameters θ were used to produce Pθ(rq). In other words, it remains an open question as to whether models have self-knowledge. We give further discussion on the plausibility of self-knowledge and the existence of predictable self for LLMs in Section B.2. We put these assumptions to the test by addressing two core research questions as outlined in Fig. 1. First, we ask RQ1: Are LLMs better than other LLMs at predicting their own correctness? Our experiments show that for the purposes of obtaining calibrated confidence score (i.e. calibrated (is correct)), models have little to no privileged information about their own correctness. For example, training Llama3.1-8B to predict its own confidence in being able to answer an MMLU question correctly results in the same performance as training Qwen2.5-7B to do the same, 69.35% vs 69.0% respectively  (Fig. 1)  . We observe similar patterns in training-free setting as well as when providing the answer and question together, indicating that using model to predict its own confidences offers little to no performance advantage. This allows for the possibility of using one LLM to model the correctness of many others: by removing reliance on self-knowledge, we can improve correctness prediction by learning from the history of many models. Indeed, as demonstrated in Fig. 1, our experiments show that which models history we train on is the clearest predictor for accuracy. Building on these findings, we ask RQ2: What is the role of historical information from multiple models in calibrated correctness prediction? 2 We explore these questions and subsequent questions that follow from them by constructing correctness models (CMs), i.e., models designed to provide calibrated (is correct(ˆr)) scores (which we also refer to as (c) as shorthand) predicting the correctness of target models (TMs). Unlike prior work, which has generally restricted CMs to the LLM generating responses either in zero-shot fashion (Tian et al., 2023) or via finetuning (Kapoor et al., 2024) or used small linear classifiers (Liu et al., 2024b; Kadavath et al., 2022), we train LLMs on historical correctness data from multiple different LLMs. By varying the training data distribution, test settings, postprocessing, and input features of the CM, we can concretely test questions and hypotheses about correctness estimation by examining the characteristics of the resulting CM. By building variety of CMs, we investigate RQ1 and the following three axes of RQ2: 1. (RQ2A) Generalization of CMs trained to predict multiple LLMs: Do CMs trained on many models outputs, referred to as Generalized Correctness Models (GCMs), learn generalized strategies for correctness prediction that transfer to other models and datasets? We find that CMs generalize well across different models families and model sizes, even outperforming self-emitted confidences of much larger OOD models, but less well across datasets (Section 3.2). 2. (RQ2B) Conditioning factors relevant to prediction and generalization: How do different conditioning variables (e.g., the question q, the response r, the predicted answer ˆr, or the target models identity) affect correctness prediction and generalization ability? We measure the incremental gains from adding each variable and find that all components contribute meaningfully except the identity of the target model; interestingly, answer phrasing plays substantial role. Moreover, improvements generalize across models, with the strongest generalization coming from parametric world-knowledge (Section 3.3). 3. (RQ2C) Alternative methods of encoding history: Does history incorporated in other ways help improve correctness? We study (a) post-hoc calibration and (b) in-context learning, which forgoes training in favor of supplying relevant prior examples in-context. We find injecting history via ICL examples helps improve correctness for larger models, and that using posthoc calibration to map historical confidences to correctness can help adapt CM to dataset-wise OOD settings with few examples (Section 3.4). Our research questions lead to practical insights about developing CMs: RQ2A shows that training Qwen3-8B on the aggregated correctness data from 8 models yields GCM that outperforms the strongest single-model baseline (directly finetuning eight CMs, one on each target model) by 2.22% accuracy and .041 AUROC on average, observing an improvement on all target models. Moreover, we show that the GCM based on Qwen3-8B outperforms the more powerful Llama3-70Bs self-emitted confidences on MMLU by 2.4% absolute accuracy and .265 AUROC. Our GCM also outperforms Qwen3-32Bs logit confidences, reducing ECE from .073 to .029 without having been trained on Qwen3-32B or any other reasoning models. The GCM transfers across datasets, outperforming correctness model trained on the target dataset in terms of AUROC, and matching its ECE and accuracy after post-hoc calibration with as little as 5% of the target dataset. Finally, when applied to downstream task such as selective prediction, we outperform Llama-3-70Bs logit confidences and SCM, enabling 30.0%, and 10.8% more coverage at low 5% risk threshold respectively (See Section 3.5)."
        },
        {
            "title": "2 METHODS AND EXPERIMENTAL SETUP",
            "content": "Correctness Models. We define Correctness Model as any system which can provide confidence that given query-response pair is correct. This allows us to treat methods such as prompting, probing, auxiliary models, finetuning, and posthoc calibrators all as parts of correctness models. Mathematically, Correctness Model is any system that estimates the probability an answer is correct given query and response containing the answer ˆr, written as (is correct(ˆr)q, r, ˆr). For LLMs, the query is the prompt and the response is the models generation given the prompt. For MMLU, we make the distinction that refers to the models entire response (average 198 tokens) and ˆy refers only to the answer choice selected (A,B,C,D). Datasets. Our main analysis is based on the MMLU dataset (Hendrycks et al., 2021) with additional dataset transfer experiments on the TriviaQA dataset (Joshi et al., 2017). To simulate more realistic setting, we allow models to generate free-form responses and use judge model with ground truth access to grade them for correctness. We observe that across 8 models in the MMLU 3 dataset, the average response length was 198 tokens, around one paragraph, with responses to math questions often containing reasoning traces that exceed 1000 tokens. prompt, model response, and binary correctness label of whether the response was correct constitutes correctness dataset, which is used in this work to inject historical correctness information into CMs. We build 18 correctness datasets by collecting responses from 10 separate models on the TriviaQA and MMLU datasets (8 from MMLU + 8 from TriviaQA + 2 models on MMLU for OOD testing). We include models from the Gemma-3 (Team, 2025a), Qwen2.5 (Qwen et al., 2025), Qwen3 (Team, 2025b), Phi-3 (Microsoft, 2024) and Llama3 families (AIatMeta, 2024), as well as model sizes from 3B to 72B. Measuring Confidence. Unless otherwise stated, we extract confidences from models via logit based confidences for all methods we study. We elicit logit based confidences P(True) (Kadavath et al., 2022) by measuring the probability of the token yes after exposure to prompt and model response appended with the question Please respond just yes or no in lowercase if [Model Name] will respond correctly to Model Prompt:. Training examples in correctness datasets are structured according to this format with the ground truth yes/no appended. In Table 7 we ablate this prompt by removing the Model Name and rephrasing it as if the Response correctly answers the Prompt. Unless otherwise noted, all models used in this work are instruction tuned models. RQ1 Setup. To address RQ1 (Section 3.1), we train two types of Correctness Models with different inputs. We train Specific Correctness Models (SCMs) by finetuning LLM on correctness dataset to predict the correctness of response given query. Excepting for when we explicitly tune these values during ablations, we use LoRA (Hu et al., 2021) with rank 32 and batch size 16 and train for 1 epoch on 70% of correctness dataset, which is close to 10000 examples for datasets generated from both MMLU and TriviaQA. Unless otherwise noted, we initialize SCMs from Qwen3-8B model. We utilize specialized optimal batch size to obtain well calibrated ( .03 ECE) Correctness Models out of the box with cross-entropy loss (see Section C). We train Answerless Correctness Models (cq) (a more general finetuning-based version of P(IK) from Kadavath et al. (2022)) by finetuning LLM to predict the probability that target model will respond correctly to query given only query, without the model response. We use the same hyperparameters as the SCM. RQ2A Setup. To analyze the generalization of correctness prediction strategies in RQ2A (Section 3.2) we introduce the General Correctness Model (GCM). We train General Correctness Models (GCMs) by finetuning LLM in this paper, Qwen-3-8B on the concatenation of 8 correctness datasets under the same training hyperparameters as the Specific Correctness Model. This trains the GCM to predict the correctness of many LLMs. We match the number of training datapoints and training steps between training one GCM to predict 8 LLMs vs training 8 SCMs to predict 8 LLMs, and further ablate impact of training steps in Section G. Specifically, we train Qwen3-8B to predict Qwen2.5-3B to 72B, Llama3.1-8B, Qwen3-8B, Gemma-3-27B, and Llama-3-70B. RQ2B Setup. To explore what parts of correctness dataset contributes to correctness and what strategies generalize in RQ2B (Section 3.3), we ablate the GCM and SCM into Answerless Correctness Models, and further introduce an Answer-only model type on MMLU as an intermediate ablation. We train Answer-only Correctness Models (cq, ˆr) by extracting the answer choice letter from the target models full response and training SCM/GCM on the query and answer letter. See Table 1 for probabilistic representations. We ablate model name information from GCM as detailed in the Measuring Confidence paragraph above. Table 1: Settings compared in RQ2B, Section 3.3. Ablation name Prob. Form Full Answer-only Answerless (cq, ˆr, r) (cq, ˆr) (cq) RQ2C Setup. We further explore training-free methods in RQ2C (Section 3.4) based on ICL verbalized confidences, and posthoc calibration. We inject semantic ICL examples into models by embedding the train split of correctness dataset (q, r, ˆr, c) into vector database, and retrieving the top k=5 most semantically similar examples to the current example (q, r, ˆr) to inject into the prompt for the Correctness Model, we then elicit verbalized confidences. Since we are not focusing on inference efficiency in this setting (ICL prompts have 5x the standard prompt length), we use verbalized confidences to give further accuracy boost at the cost of efficiency. We elicit verbalized confidences (Tian et al., 2023) by prompting the model to give the calibrated percent 4 Figure 2: Do LLMs possess special self-knowledge of their correctness? We compare correctness prediction in answerful (with responses) and answerless (without responses) settings. Qwen2.5-7B beats Llama3.1-8B when responses are included, while both perform similarly without them, indicating that dataset signals and world knowledge drive performance, not privileged self-knowledge. probability that the answer will be correct. We posthoc calibrate models by holding out 5% of correctness dataset and using the spline calibration (Lucena, 2018). We also test alternate posthoc calibration strategies including beta-calibration (Kull et al., 2017), isotonic regression (Zadrozny & Elkan, 2002), or Platt scaling algorithms (Platt, 2000) to map raw model probabilities to calibrated probabilities. Evaluating Correctness Models. We evaluate the performance of Correctness Models on 25% of any given correctness dataset, which is close to 3500 examples, ensuring the same questions are used across datasets to prevent train test contamination for GCMs. We highlight CMs accuracy in predicting correctness as well as their expected calibration error, the standard metrics used for accessing the quality of predicted confidences (Guo et al., 2017b). Additionally, due to the variability of metrics like ECE (Guo et al., 2017b), we include the Root Mean Squared Calibration Error (Hendrycks et al., 2019) an adaptively binned measurement of calibration. We also include the Area Under the Curve of the Receiver Operating Characteristic (AUROC) which gives more holistic estimate of predictive power. Importantly, this metric remains sensitive when data is class imbalanced, for example, when large model such as Gemma-3-27b is correct on 78.8% of MMLU questions. For supplemental materials relating to experimental setup including training configurations, list of correctness datasets, ICL retrieval, and prompt templates, see Section D."
        },
        {
            "title": "3 RESULTS",
            "content": "3.1 RQ1: MODELS HAVE LITTLE SELF-KNOWLEDGE FOR CORRECTNESS ESTIMATION The motivation for our work comes from the hypothesis that LLMs lack special information about their own correctness. We demonstrate this claim through several experimental settings, highlighting the two most illustrative settings. Given MMLU questions and responses, we first finetune both Qwen2.5-7B and Llama3.1-8B models to predict each others correctness as well their own, with the results summarized in the Answerful setting of Fig. 2. We find that Qwen2.5-7B consistently predicts Llama3.1-8Bs as well as its own correctness much better than Llama3.1-8B does. We attribute this to Qwen2.5-7B being stronger model with greater parametric knowledge of the true answer to the MMLU questions (Qwen achieves 72% average MMLU accuracy whereas Llama3.1 only achieves 66%). This shows that using stronger model is more critical to correctness prediction than self-knowledge stemming from using the same model for generation and verification. To remove the effect of parametric knowledge, we repeat the experiment but remove the model response (answerless setting in Fig. 2), so that greater parametric knowledge will not benefit Qwen2.5-7B. In this case we find that Qwen and Llama are roughly equally good at predicting Qwens correctness, and the same is true when predicting Llamas correctness. If private knowledge existed (such as an internal confidence vector uniquely known to the model itself), we would expect that Llama would be able to predict its own confidences better. We further reinforce these findings by examining training-free settings and other pairs of models in Section E, where we find similar trends suggesting little privileged self-knowledge. 5 Table 2: Comparing Performance of different CMs on MMLU for predicting the correctness of Gemma3-27B and Llama3.1-8B. The General Correctness Model (GCM) outperforms all other baselines in terms of Accuracy and AUROC and achieves extremely low ECE .02. Method P(True) Verbal Confidence ICL Verb. Conf. Verb. Conf. (Qwen3-32B) ICL Verb. Conf. (Qwen3-32B) SCM (Trained On Target) GCM GCM + Posthoc Llama3.1-8B Gemma3-27B Acc ECE RMSCE AUROC Acc ECE RMSCE AUROC .741 .219 .764 .160 .743 .166 .780 .161 .811 .103 .792 .017 .820 .023 .818 .020 .253 .281 .303 .244 .186 .069 .080 . .807 .805 .785 .833 .862 .857 .890 .890 .789 .197 .797 .160 .798 .155 .807 .166 .833 .119 .796 .037 .836 .029 .836 . .301 .289 .302 .272 .194 .091 .085 .076 .707 .738 .726 .725 .796 .811 .865 . Table 3: Comparing Performance of different CMs on TriviaQA for predicting the correctness of Gemma-3-27B and Llama3.1-8B. The General Correctness Model (GCM) outperforms all other baselines in terms of Accuracy by 1-4% and achieves extremely low ECE .023. Method P(True) Verbal Confidence ICL Verb. Conf. Verb. Conf. (Qwen3-32B) ICL Verb. Conf. (Qwen3-32B) Specific Model General Model General Model + Posthoc Llama3.1-8B Gemma3-27B Acc ECE RMSCE AUROC Acc ECE RMSCE AUROC .827 .155 .834 .136 .827 .119 .815 .151 .840 .109 .844 .023 .847 .029 .847 .023 . .323 .234 .231 .202 .086 .090 .077 .839 .821 .855 .856 .877 .895 .905 .905 .827 . .825 .158 .826 .145 .831 .154 .843 .128 .839 .028 .862 .028 .862 .018 .331 .344 .254 .254 .229 .079 .074 .072 . .687 .755 .747 .785 .843 .881 .881 3.2 RQ2A: GENERALIZATION OF CMS TRAINED TO PREDICT MULTIPLE LLMS Given that the self-knowledge of the LLM does not provide noticeable advantage for Correctness Model, we explore combining historical information from multiple models to improve CMs. Cross-Model Generalization. We test whether correctness prediction learned from one model can transfer to others. Qwen3-8B Generalized Correctness Model (GCM) trained as in Section 2 is evaluated on Llama3.1-8B and Gemma-3-27B against Specific Correctness Models (SCMs) trained directly on each. With equal data and training time, the GCM outperforms SCMs by 3% accuracy on both and achieves .03 ECE without post-hoc calibration  (Table 2)  . We observe similar patterns for TriviaQA in Table 3. In Table 5, we confirm the GCM also outperforms Qwen3-8B trained to predict itself and in Table 4 show the same GCM outperforms Llama-3-70Bs (True) across all metrics. For comparison of GCMs against SCMs on all 16 in distribution correctness datasets, refer to Section F. We next test on models held out from training. On Phi-3-mini, the GCM outperforms the SCM by 1.3% accuracy, .009 ECE, and .023 AUROC,2 while on Qwen3-32B (also held out) it matches the SCM and surpasses Qwen3-32Bs zero-shot (True)  (Table 6)  .3 These results indicate that correctness prediction generalizes across families, sizes, and even held-out stronger models. Cross-Dataset Generalization. Finally, we test the ability of the generalized model trained on MMLU to predict the correctness of models on TriviaQA in Table 8. We find that although the GCM achieves similar AUROC to SCM tuned on TriviaQA and outperforms P(True), it has lower 2Without training on any Phi-family models. 3Despite never being trained on reasoning-enabled models. 6 Table 4: Up Generalization: Qwen3-8B GCM vs. P(True) of Large ID Model (Llama-3-70B). Table 5: Self Generalization: Qwen3-8B GCM vs. Qwen3-8B trained to predict itself. ID Large Model (Llama-3-70B) Self Predict Model (Qwen3-8B) Method Acc ECE RMSCE AUROC Method Acc ECE RMSCE AUROC P(True) .798 .200 GCM .822 .025 .426 .078 .584 .849 SCM .814 .035 GCM .834 . .091 .071 .835 .867 Table 6: Out-of-Distribution Generalization. Qwen3-8B GCM predicting correctness on Phi-3-mini and Qwen3-32B, models that are held out from the GCM training set. Phi-3-mini Qwen3-32B Method Acc ECE RMSCE AUROC Acc ECE RMSCE AUROC P(True) (of target model) Specific Model (trained on target) General Model (no exposure) .682 .042 .787 .026 .800 .017 .113 .086 .076 .643 .853 .876 .870 .074, .873 .022 .871 . .130 .072 .084 .861 .876 .877 accuracy and much higher ECE of .105 compared to the SCMs .023. Surprisingly, this suggests that capabilities generalize better across model families compared to datasets. We study generalizing similarities between models further in Section 3.3. Given the strength of the GCM in outperforming both SCMs and larger models in predicting the correctness of variety of target models across datasets, we dedicate section Section 3.5 to further evaluations of the General Correctness Model and its practical applications. 3.3 RQ2B: CONDITIONING FACTORS RELEVANT TO PREDICTION AND GENERALIZATION Ablating Conditional Distributions Used to Train Correctness Models. We successively ablate the query q, the answer ˆr and the full response from the correctness dataset to discover impact of each for both the SCM and GCM  (Fig. 3)  . We interpret of each ablation as follows: The accuracy gap between (cq, ˆr, r) (Full) and (cq, ˆr) (Answer-only) ablates the answer phrasing of the target models response, without removing its answer, showing the impact of learning correlations between how the answers are phrased and elaborated with accuracy. This ablation captures, for instance, the difference between seeing believe the answer is 4, and just 4; these findings align with work like Zhou et al. (2024), who study the importance of epistemic markers in confidence, and Stengel-Eskin et al. (2024), who train LLMs to calibrate their use of linguistic signals that communicate confidence. The gap between (cq, ˆr) (Answer-only) and (cq) (Answerless) ablates the target models entire response, but preserves the query, showing the accuracy gain from allowing the CM to leverage its world knowledge to evaluate the likelihood that the answer ˆr is correct independent of the past performance of the model on similar questions. Finally, the gap between (cq) (Answerless) and (c) ablates the query, with (cq) showing the performance gained by conditioning the target models past performance on features of the questions compared to model that simply predicts the majority class; this captures the notion that given model may differ in its ability to answer different types of questions (Chen et al., 2025). We see substantial increase in accuracy from every ablation, suggesting that every ablated component, including response phrasing, is important to correctness prediction. By additionally comparing the SCM and the GCM, we find the GCM outperforms SCM by 2% accuracy in the answer-less setting, suggesting that there is some correlation between what questions LLMs most often answer correctly. The GCM improved 7% versus the SCMs 4% from answerless to Answer-only, showing that world-knowledge strategies for correctness prediction transfer across models well  (Fig. 3)  . Role of Model Identity. To test how much information about which model generated the response improves our ability to predict the correctness of target models, we remove the name of the target model from the prompt to the Answer-only GCM at training time, we find that while calibration and accuracy are impacted, it still outperforms the Answer-only SCM  (Table 7)  . This suggests that much of the learned capability is model-agnostic and not reliant on the identity of the target model. 7 Table 7: We ablate information about the identity of the target model from GCM, and discuss in Section 3.3. Table 8: Out-of-Distribution Generalization. GCM trained on MMLU, tested on TriviaQA. Method GCM Answer-only Name Ablated SCM Answer-only Acc .789 .763 .745 ECE RMSCE AUROC .034 .034 .023 .088 .091 .087 .852 .847 .810 Method P(True) SCM (TriviaQA) GCM (MMLU) GCM + Posthoc Acc .827 .844 .828 .844 ECE RMSCE AUROC .155 .023 .105 .031 .277 .080 .150 . .839 .895 .896 ."
        },
        {
            "title": "3.4 RQ2C: ALTERNATIVE METHODS FOR ENCODING HISTORY",
            "content": "We observe in Section 3.1 that stronger models with more parametric knowledge can be better predictors of confidence. Moreover, we note that training the LLM is not always possible, especially with larger LLMs. This motivates us to consider injecting historical information in other ways. We explore two alternative methods: in-context learning (ICL) and post-hoc calibration. In-Context Learning. Rather than training CM on dataset of historical examples, we embed the training split of the target models correctness dataset (q, r, ˆr, c) and the current example (q, r, ˆr), retrieving top k=5 similar training examples to include in-context (details in Section 2). As the ICL setting focuses less on inference efficiency it requires multiplying prompt length by we allowed the model to verbally reason about correctness to further improve accuracy at the cost of inference time. We show in Table 2 that injecting semantically relevant examples from the correctness dataset via ICL improves accuracy by 4.6% and reduces ECE by 7.8% when predicting Gemma3-27Bs performance with Qwen3-32B, compared to verbalized confidences without ICL. However, Qwen38B shows no gains, suggesting minimum base capability is needed to benefit from verbalized ICL. Posthoc Calibration. Posthost calibration injects historical information by directly aligning an CMs output confidences with the historical ground truth (c) without conditioning on q, or ˆr, as in Section 2. Recall that in RQ2a (Section 3.2), we showed that transfer to new datasets is harder for GCM: although we outperformed the target SCM in terms of AUROC, the GCM had more than .10 ECE after transfer. However, we find calibrating the result increases accuracy and decreases ECE to match performance of the SCM  (Table 8)  using only 5% of the target datasets samples. We additionally observe that it is possible to further calibrate the GCMs output probabilities to reach even lower ECE with posthoc calibration  (Table 2)  . We use spline calibration for all main results, see Section for supplementary evaluations of other posthoc calibrators. 3.5 RECOMMENDATIONS FOR PERFORMANT CORRECTNESS PREDICTION Building the Most Performant Correctness Model. Here, we put together the findings from Section 3.2, Section 3.3, and Section 3.4 to summarize the best practices for building GCM. We recommend the GCM with posthoc calibration as an accurate and calibrated correctness prediction method. In Table 2 we found that the GCM substantially outperforms strong baselines in-distribution, and transfers without training to beat models trained on OOD target models of different model families, as well as reasoning models Table 6. In addition, when combined with posthoc calibration, it beats SCMs trained on an OOD target dataset in terms of AUROC, matching it in terms of accuracy and ECE Table 8. Further, the GCM is inference efficient prefill only method, with one evaluation on MMLU (3511 examples, 0.125s/example) requiring only 7.3 minutes. This solidifies the GCM with Posthoc calibration as our recommend method of modeling correctness given history. If training is not possible, we recommend using the ICL method presented in Section 3.4. However, we note that while ICL on significantly stronger model (Qwen3-32B) can match the predictive accuracy of GCM based on Qwen3-8B, it suffers from high calibration error and has much lower AUROC, which is important for downstream applications such as re-ranking and selective prediction. Additionally, the inference cost of ICL is significantly higher in terms of latency, compute, and memory requirements, due to requiring large base model, multiplying input prompt length by for retrievals, and requiring the generation of reasoning chain. One MMLU evaluation run (3511 examples, 2.6s/example) already exceeds the cost of training SCM on correctness. Figure 3: Conditioning factors ablation. GCMs and SCMs across conditioning settings in RQ2b (Section 3.3). More metrics: Table 24. Figure 4: Risk-Coverage Curves for Selective Prediction, lower AURC curves are better. Downstream Evaluation on Selective Prediction. Here, we show that the GCM also provides downstream benefits in selective prediction task. Selective prediction requires system to selectively abstain from examples that are unlikely to be correct, with the objective of maximizing coverage (the percentage of examples for which an answer is produced) while minimizing risk (the percentage of predicted answers that were incorrect). Intuitively, the trade-off between coverage and risk is one between usability and safety, with full coverage (no abstention) system having high usability but low safety, while abstaining on all examples (zero coverage) incurs no risk but represents useless model. Fig. 4 shows the risk-coverage curves for the GCM, SCM, and for Llama3-70B; here, lower AURC indicates better trade-off between coverage and risk. As shown in Fig. 4, our results indicate that compared to the target models self emitted confidences or model-specific SCM, the generalized model consistently achieves lower risk (y-axis) at the same level of coverage (x-axis Fig. 4). This suggests that the GCM produces more reliable predictions, making it better suited for robust deployment."
        },
        {
            "title": "4 RELATED WORK.",
            "content": "Self-Knowledge and Confidence Calibration. Calibration, crucial for deciding when to trust AI systems, has been studied in neural models (Naeini et al., 2015; Guo et al., 2017a; Ovadia et al., 2019; Wang et al., 2020a) and more recently in LLMs (Mielke et al., 2022; Kadavath et al., 2022; Kuhn et al., 2023; Stengel-Eskin & Van Durme, 2023b; Stengel-Eskin et al., 2024; Tian et al., 2023). Early work showed models like T5, BART, and GPT-2 are poorly calibrated on QA, motivating posthoc and fine-tuning methods (Jiang et al., 2021). Other studies examined overconfidence in dialogue (Mielke et al., 2022), prompting-based calibration (Kadavath et al., 2022), and fine-tuning (similar to SCMs) with correctness labels (Kapoor et al., 2024). Further efforts probed unanswerable questions (Yin et al., 2023), lying behavior via hidden activations (Azaria & Mitchell, 2023), and black-box elicitation through prompting, sampling, and aggregation (Xiong et al., 2024). In contrast, we show models lack privileged access to their own correctness and propose more general solution to predict calibrated correctness for multiple LLMs at once. Correctness Models and Cross-Model Transfer. Another line of work uses correctness models (CMs) to predict whether response is correct. The simplest rely on self-reported confidence (Tian et al., 2023), while stronger methods probe hidden states (Liu et al., 2024b; Kadavath et al., 2022; Beigi et al., 2024; Azaria & Mitchell, 2023; Liu et al., 2024a) or fine-tune LLMs directly on correctness tasks (Kapoor et al., 2024). Recent efforts capture semantic uncertainty, modeling meaning variability for better correctness correlation (Kuhn et al., 2023). Uncertainty estimation is related line of work focused on estimating models internal confidence (uncertainty), which is different from, but can be correlated with, correctness prediction ability. Surrogate approaches also show promise: Shrivastava et al. (2023) report that even untrained LLaMA models can outperform GPTs self-reported probabilities, revealing biases in elicitation. These previous studies suggest correctness signals could transfer across models, but focus on one-to-one transfer. In contrast, we identify the key factors shaping CM performance and introduce Generalized Correctness Model (GCM) that 9 aggregates correctness patterns across many models for more robust prediction. See Section for more details on related works."
        },
        {
            "title": "5 CONCLUSION",
            "content": "The insight that LLMs have little self-knowledge for the purpose of correctness prediction is counterintuitively beneficial for training Correctness Models. We find that General Correctness Model (GCM) based on LLM, trained to predict the correctness of many LLMs, is able to generalize and learn transferable correctness prediction strategies across variety of models, suffering no penalty for predicting models apart from itself. GCMs outperform both models trained to predict their own correctness and the self-emitted correctness confidences of larger models GCMs were not trained on. We further show improvements in downstream selective prediction evaluation stemming from the GCMs generalization ability."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "This work was supported by NSF-AI Engage Institute DRL-2112635, DARPA ECOLE Program No. HR00112390060, NSF-CAREER Award 1846185, and Capital One Research Award. The views contained in this article are those of the authors and not of the funding agency."
        },
        {
            "title": "REFERENCES",
            "content": "AIatMeta. The Llama 3 Herd of Models Research - AI at Meta, 2024. URL https://ai. meta.com/research/publications/the-llama-3-herd-of-models/. Amos Azaria and Tom Mitchell. The Internal State of an LLM Knows When Its Lying, October 2023. URL http://arxiv.org/abs/2304.13734. arXiv:2304.13734 [cs]. Mohammad Beigi, Ying Shen, Runing Yang, Zihao Lin, Qifan Wang, Ankith Mohan, Jianfeng He, Ming Jin, Chang-Tien Lu, and Lifu Huang. InternalInspector $Iˆ2$: Robust Confidence Estimation in LLMs through Internal States, June 2024. URL http://arxiv.org/abs/ 2406.12053. arXiv:2406.12053 [cs]. Justin Chih-Yao Chen, Sukwon Yun, Elias Stengel-Eskin, Tianlong Chen, and Mohit Bansal. Symbolic mixture-of-experts: Adaptive skill-based routing for heterogeneous reasoning. arXiv preprint arXiv:2503.05641, 2025. Chroma. chroma-core/chroma, September 2025. URL https://github.com/ chroma-core/chroma. Open-source library, original-date: 2022-10-05T17:58:44Z. Yu-Neng Chuang, Prathusha Kameswara Sarma, Parikshit Gopalan, John Boccio, Sara Bolouki, Xia Hu, and Helen Zhou. Learning to Route LLMs with Confidence Tokens, June 2025. URL http://arxiv.org/abs/2410.13284. arXiv:2410.13284 [cs]. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training Verifiers to Solve Math Word Problems, November 2021. URL http: //arxiv.org/abs/2110.14168. arXiv:2110.14168 [cs]. Mehul Damani, Isha Puri, Stewart Slocum, Idan Shenfeld, Leshem Choshen, Yoon Kim, and Jacob Andreas. Beyond Binary Rewards: Training LMs to Reason About Their Uncertainty, July 2025. URL http://arxiv.org/abs/2507.16806. arXiv:2507.16806 [cs]. Morris H. Degroot and Stephen E. Fienberg. The Comparison and Evaluation of Forecasters. Journal of the Royal Statistical Society: Series (The Statistician), 32(1-2):1222, 1983. ISSN 14679884. doi: 10.2307/2987588. URL https://onlinelibrary.wiley.com/doi/abs/ 10.2307/2987588. eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.2307/2987588. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers, March 2023. URL http://arxiv. org/abs/2210.17323. arXiv:2210.17323 [cs]. 10 Sylvain Gugger, Lysandre Debut, Thomas Wolf, Philipp Schmid, Zachary Mueller, Sourab Mangrulkar, Marc Sun, and Benjamin Bossan. Accelerate: Training and inference at scale made simple, efficient and adaptable. https://github.com/huggingface/accelerate, 2022. Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On Calibration of Modern Neural Networks, August 2017a. URL http://arxiv.org/abs/1706.04599. arXiv:1706.04599. Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Weinberger. On Calibration of Modern Neural Networks. 2017b. Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep Anomaly Detection with Outlier Exposure, January 2019. URL http://arxiv.org/abs/1812.04606. arXiv:1812.04606 [cs]. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring Massive Multitask Language Understanding, January 2021. URL http: //arxiv.org/abs/2009.03300. arXiv:2009.03300 [cs]. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language Models, October 2021. URL http://arxiv.org/abs/2106.09685. arXiv:2106.09685 [cs]. Qitian Jason Hu, Jacob Bieker, Xiuyu Li, Nan Jiang, Benjamin Keigwin, Gaurav Ranganath, Kurt Keutzer, and Shriyash Kaustubh Upadhyay. RouterBench: Benchmark for Multi-LLM Routing System, March 2024. URL http://arxiv.org/abs/2403.12031. arXiv:2403.12031 [cs]. Zhengbao Jiang, Jun Araki, Haibo Ding, and Graham Neubig. How can we know when language models know? on the calibration of language models for question answering. Transactions of the Association for Computational Linguistics, 9:962977, 2021. Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. TriviaQA: Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension, May 2017. URL http: //arxiv.org/abs/1705.03551. arXiv:1705.03551 [cs]. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El-Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. Language Models (Mostly) Know What They Know, November 2022. URL http://arxiv.org/abs/2207.05221. arXiv:2207.05221 [cs]. Sanyam Kapoor, Nate Gruver, Manley Roberts, Katherine Collins, Arka Pal, Umang Bhatt, Adrian Weller, Samuel Dooley, Micah Goldblum, and Andrew Gordon Wilson. Large Language Models Must Be Taught to Know What They Dont Know, December 2024. URL http://arxiv. org/abs/2406.08391. arXiv:2406.08391 [cs]. Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id= VD-AYtP0dve. Meelis Kull, Telmo Silva Filho, and Peter Flach. Beta calibration: well-founded and easily implemented improvement on logistic calibration for binary classifiers. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, pp. 623631. PMLR, April 2017. URL https://proceedings.mlr.press/v54/kull17a.html. ISSN: 2640-3498. Kenneth Li, Oam Patel, Fernanda Viegas, Hanspeter Pfister, and Martin Wattenberg. InferenceTime Intervention: Eliciting Truthful Answers from Language Model, June 2024. URL http: //arxiv.org/abs/2306.03341. arXiv:2306.03341 [cs]. 11 Pengyi Li, Matvey Skripkin, Alexander Zubrey, Andrey Kuznetsov, and Ivan Oseledets. Confidence is all you need: Few-shot rl fine-tuning of language models. arXiv preprint arXiv:2506.06395, 2025a. Pengyi Li, Matvey Skripkin, Alexander Zubrey, Andrey Kuznetsov, and Ivan Oseledets. Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models, June 2025b. URL http: //arxiv.org/abs/2506.06395. arXiv:2506.06395 [cs]. Yibo Li, Miao Xiong, Jiaying Wu, and Bryan Hooi. ConfTuner: Training Large Language Models to Express Their Confidence Verbally, August 2025c. URL http://arxiv.org/abs/2508. 18847. arXiv:2508.18847 [cs]. Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration, July 2024. URL http://arxiv.org/abs/2306. 00978. arXiv:2306.00978 [cs]. Linyu Liu, Yu Pan, Xiaocheng Li, and Guanting Chen. Uncertainty Estimation and Quantification for LLMs: Simple Supervised Approach, June 2024a. URL http://arxiv.org/abs/ 2404.15993. arXiv:2404.15993 [cs]. Xin Liu, Muhammad Khalifa, and Lu Wang. Litcab: Lightweight language model calibration over short-and long-form responses. In The Twelfth International Conference on Learning Representations, 2024b. Brian Lucena. Spline-Based Probability Calibration, September 2018. URL http://arxiv. org/abs/1809.07751. arXiv:1809.07751 [stat]. Barbara Mellers, Eric Stone, Terry Murray, Angela Minster, Nick Rohrbaugh, Michael Bishop, Eva IdentifyChen, Joshua Baker, Yuan Hou, Michael Horowitz, Lyle Ungar, and Philip Tetlock. ing and Cultivating Superforecasters as Method of Improving Probabilistic Predictions. Perspectives on Psychological Science, 10(3):267281, May 2015. ISSN 1745-6916, 1745-6924. doi: 10.1177/1745691615577794. URL https://journals.sagepub.com/doi/10. 1177/1745691615577794. Microsoft. Phi-3 Technical Report: Highly Capable Language Model Locally on Your Phone. August 2024. Sabrina J. Mielke, Arthur Szlam, Emily Dinan, and Y.-Lan Boureau. Reducing conversational agents overconfidence through linguistic calibration, June 2022. URL http://arxiv.org/ abs/2012.14983. arXiv:2012.14983 [cs]. Calibrating Deep Neural Networks using Focal Loss. Jishnu Mukhoti, Viveka Kulharia, Amartya Sanyal, Stuart Golodetz, Philip Torr, and Puneet In Advances in NeuDokania. ral Information Processing Systems, volume 33, pp. 1528815299. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2020/ 2020. hash/aeb7b30ef1d024a76f21a1d40e30c302-Abstract.html. Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining Well Calibrated Probabilities Using Bayesian Binning. Proceedings of the AAAI Conference on Artificial Intelligence, 29(1), February 2015. doi: 10.1609/aaai.v29i1.9602. URL ISSN 2374-3468. https://ojs.aaai.org/index.php/AAAI/article/view/9602. Number: 1. Isaac Ong, Amjad Almahairi, Vincent Wu, Wei-Lin Chiang, Tianhao Wu, Joseph E. Gonzalez, Waleed Kadous, and Ion Stoica. Routellm: Learning to route llms with preference data, 2024. URL https://arxiv.org/abs/2406.18665. Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, Sebastian Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your models uncertainty? evaluating predictive uncertainty under dataset shift. Advances in neural information processing systems, 32, 2019. John Platt. Probabilistic Outputs for Support Vector Machines and Comparisons to Regularized Likelihood Methods. Adv. Large Margin Classif., 10, June 2000. 12 Qwen, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 Technical Report, January 2025. URL http://arxiv.org/abs/2412.15115. arXiv:2412.15115 [cs]. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimization towards training trillion parameter models. CoRR, abs/1910.02054, 2019. URL http:// arxiv.org/abs/1910.02054. Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models, March 2024. URL http://arxiv.org/abs/2308.13137. arXiv:2308.13137 [cs]. Vaishnavi Shrivastava, Percy Liang, and Ananya Kumar. Llamas know what gpts dont show: Surrogate models for confidence estimation. arXiv preprint arXiv:2311.08877, 2023. Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Nicolas Papernot, Ross Anderson, and Yarin Gal. AI models collapse when trained on recursively generated data. Nature, 631(8022):755759, July 2024. ISSN 1476-4687. doi: 10.1038/s41586-024-07566-y. URL https://www.nature. com/articles/s41586-024-07566-y. Publisher: Nature Publishing Group. Elias Stengel-Eskin and Benjamin Van Durme. Did you mean...? confidence-based trade-offs in In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the semantic parsing. 2023 Conference on Empirical Methods in Natural Language Processing, pp. 26212629, Singapore, December 2023a. Association for Computational Linguistics. doi: 10.18653/v1/2023. emnlp-main.159. URL https://aclanthology.org/2023.emnlp-main.159/. Elias Stengel-Eskin and Benjamin Van Durme. Calibrated interpretation: Confidence estimation in semantic parsing. Transactions of the Association for Computational Linguistics, 11:12131231, 2023b. Elias Stengel-Eskin, Peter Hase, and Mohit Bansal. Lacie: Listener-aware finetuning for calibration in large language models. Advances in Neural Information Processing Systems, 37:4308043106, 2024. Gemma Team. Gemma 3 Technical Report, March 2025a. URL http://arxiv.org/abs/ 2503.19786. arXiv:2503.19786 [cs]. Qwen3 Team. Qwen3 Technical Report, May 2025b. URL http://arxiv.org/abs/2505. 09388. arXiv:2505.09388 [cs]. Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher D. Manning. Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback, October 2023. URL http://arxiv.org/abs/2305.14975. arXiv:2305.14975 [cs]. Han Wang, Archiki Prasad, Elias Stengel-Eskin, and Mohit Bansal. Soft self-consistency improves language model agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), 2024a. Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce LLMs step-by-step without human annotations. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 94269439, Bangkok, Thailand, August 2024b. Association for Computational Linguistics. doi: 10.18653/ v1/2024.acl-long.510. URL https://aclanthology.org/2024.acl-long.510/. Shuo Wang, Zhaopeng Tu, Shuming Shi, and Yang Liu. On the inference calibration of neural machine translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 30703079, 2020a. 13 Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. MiniLM: Deep SelfAttention Distillation for Task-Agnostic Compression of Pre-Trained Transformers, April 2020b. URL http://arxiv.org/abs/2002.10957. arXiv:2002.10957 [cs]. Hanqi Xiao, Yi-Lin Sung, Elias Stengel-Eskin, and Mohit Bansal. Task-Circuit Quantization: Leveraging Knowledge Localization and Interpretability for Compression, April 2025. URL http://arxiv.org/abs/2504.07389. arXiv:2504.07389 [cs]. Miao Xiong, Zhiyuan Hu, Xinyang Lu, YIFEI LI, Jie Fu, Junxian He, and Bryan Hooi. Can LLMs express their uncertainty? an empirical evaluation of confidence elicitation in LLMs. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=gjeQKFxFpZ. Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu, Xipeng Qiu, and Xuanjing Huang. Do Large Language Models Know What They Dont Know?, May 2023. URL http://arxiv.org/ abs/2305.18153. arXiv:2305.18153 [cs]. Bianca Zadrozny and Charles Elkan. Transforming Classifier Scores into Accurate Multiclass Probability Estimates. Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, August 2002. doi: 10.1145/775047.775151. Kaitlyn Zhou, Jena Hwang, Xiang Ren, and Maarten Sap. Relying on the unreliable: The impact of language models reluctance to express uncertainty. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 36233643, 2024. Xiaoling Zhou, Mingjie Zhang, Zhemg Lee, Wei Ye, and Shikun Zhang. Hademif: Hallucination detection and mitigation in large language models. 2025. FIGURE 1 EXPERIMENTAL SETTINGS For RQ1, the graph is copy of the Answerless setting from Fig. 2 and described in Section 3.1. For RQ2a, we compare the SCM and GCM when predicting the correctness of Llama3.1-8B, matching the SCM and GCM rows from Table 2 for Llama3.1-8B. For RQ2b, we display the numbers from the conditioning ablations for the GCM, also shown in Fig. 3. For RQ2c, we display the gains for predicting Llama3.1-8B using Qwen3-32B with and without ICL. For the GCM, we compare against Llama3.1-8Bs logit confidences as defined in Section 2."
        },
        {
            "title": "B DISCUSSION",
            "content": "B.1 DISCUSSION ON CHATGPTS MEMORY SYSTEM AND SIMILAR TECHNIQUES FOR INJECTING HISTORY We discussed the lack of historical information for LLM based systems in Section 1. We would like to point out that systems such as ChatGPT incorporates history function. However, we make the distinction that what is necessary is to inject historical correctness information, not simply historical information. Additionally, systems such as ChatGPT preserve sparse memories that do not always give direct account of the performance of their own previous generations, or indeed, even the generations themselves. B.2 DISCUSSION ON LANGUAGE MODELS INFERENCE CONFIGURATIONS AND THEIR IMPLICATIONS FOR SELF-KNOWLEDGE AND THE EXISTENCE OF PREDICTABLE SELF. We further explore the possibility of self-knowledge in LLMs in this discussion. Apart from the idea that LLMs do not have memory of their own capabilities e.g., when an LLM is pretrained, we attempt to prevent its exposure to its own generations for fear of model collapse (Shumailov et al., 2024), we explore the idea that LLMs have lack of self knowledge in terms of inference configurations. Specifically, we observe that there are several factors capable of influencing the behavior and power of an LLM that the LLM plausibly does not have self-knowledge of by default: the effect of sampling parameters such as temperature, the use of hidden reasoning chains in previous 14 conversation turns, and the application of quantization. In Table 9, Table 10, Table 11, and Table 12, we change each of these inference configurations and record the effect to the models performance on various subjects in MMLU, showing that the performance of the model can change, by as much as 16% on single subject, and cause an average of 14.64% change in the specific questions model is able to answer correctly. Each of these cases changes the behavior of an LLM in different way, which we briefly discuss below. Temperature. In the case of sampling parameters such as temperature, the output distribution (Xn) is directly altered, such that the tokens that determine the LLMs output and future processing are sampled from new distribution ( ˆXn). Additionally, the next token sampled ( ˆXn+1) depends on ( ˆXn). We show the effects of temperature on behavior in Table 11. Reasoning. Due to the number of tokens reasoning chain can constitute, it is recommended, for example, by the authors of Qwen3 (Team, 2025b), to hide all previous reasoning chains apart from the current reasoning chain in multi-turn conversations. The effect of the official template for multiturn conversations makes the model unable to realize whether the previous messages where generated with reasoning enabled or disabled. We show the effects of reasoning on behavior in Table 10. Quantization. Due to the large size of LLMs and the desire for local use in commercial hardware, or to reduce costs, post training quantization (PTQ) has become prevalent to reduce the size of LLMs. These algorithms are lossy compressors that alter the behavior of the produced model (Frantar et al., 2023; Lin et al., 2024; Shao et al., 2024). Further, they often make use of calibration sets that can bias the performance of the model on downstream datasets such as MMLU and GSM8k (Cobbe et al., 2021) depending on the calibration dataset used (Xiao et al., 2025). We show the effects of quantization on behavior in Table 12. These results highlights that if we depend solely on LLMs self-knowledge of its own expected behavior, it would be implausible to make accurate predictions of models performance owing to the supposition that any change to these parameters would not be known to the LLM yet would alter its behavior. In contrast, the GCM learns model-agnostic strategies for correctness prediction, memorizing patterns that are generalizable across models (Section 3.3) and learning strategies that generalize to an unseen models outputs  (Table 6)  . We show in Table 13 that GCMs maintains high accuracy and low ECE for inference configurations that it has not been trained on. Table 9: Overall accuracy changes and disagreement caused by inference configuration. Disagreement is the percent of questions where in one configuration it was answered correctly, while in the other it was answered incorrectly. Group (A B) Acc Acc Disagree % Reasoning vs. Non-reasoning (Qwen3-8B) Temperature (Qwen2.5-7B; 0.0 0.7) Quantization (Llama-3.1-8B; 16bit GPTQ-INT4) .756 .726 .656 .785 .715 . 15.2 8.7 20.0 Table 10: Top 5 subject swings caused by inference configuration pair: Reasoning (B) (Qwen3-8B) vs Non-reasoning (A) (Qwen3-8B). Subject Acc (A) Acc (B) () .760 .910 .865 .675 . +.160 (.160) +.130 (.130) +.119 (.119) +.105 (.105) +.090 (.090) college chemistry college mathematics formal logic econometrics professional law .600 .780 .746 .570 .473 15 Table 11: Top 5 subject swings caused by inference configuration pair: Temp 0.7 (B) (Qwen2.5-7B) vs Temp 0.0 (A) (Qwen2.5-7B). Subject Acc (A) Acc (B) () machine learning human aging formal logic global facts college physics .643 .731 .548 .490 .745 .580 .682 .508 .530 .784 -.063 (.063) -.049 (.049) -.040 (.040) +.040 (.040) +.039 (.039) Table 12: Top 5 subject swings caused by inference configuration pair: GPTQ-INT4 (B) (Llama3.18B) vs FP (A) (Llama3.1-8B). We use hugging-quants/Meta-Llama-3.1-8B-Instruct-GPTQ-INT4 for the GPTQ-INT4 comparison. Subject Acc (A) Acc (B) () college mathematics formal logic conceptual physics clinical knowledge machine learning .450 .365 .715 .762 . .360 .444 .643 .702 .509 -.090 (.090) +.079 (.079) -.072 (.072) -.060 (.060) -.053 (.053)"
        },
        {
            "title": "C OPTIMAL BATCH SIZE LEADS TO NEGLIGIBLE CALIBRATION ERROR",
            "content": "Minimizing ECE with Training Batch Size. Another analysis we make is regarding the effect of the training batch size on calibration. Prior work has sometimes attributed miscalibration to the use of the cross-entropy loss or otherwise suggested that different loss function should be used to ensure calibrated models after finetuning (Mukhoti et al., 2020; Damani et al., 2025; Li et al., 2025c). For our particular experimental setting (training SCMs and GCMs), we find that batch size has surprising effect on calibration, and that by carefully setting the batch size we can mostly overcome the miscalibration issue caused by CEL to reach negligible .01-.02 ECE. We observe that small batch size of 1 is especially detrimental and higher batch sizes than 32 can also harm ECE. We build our SCMs and GCMs using batch size of 16 based on this observation  (Table 14)  ."
        },
        {
            "title": "D ADDITIONAL DETAILS ON EXPERIMENTAL SETUP",
            "content": "Further Training Details. Expanding on Section 2, except for when we explicitly tune these values during ablations, for all GCM and SCM training runs we use LoRA (Hu et al., 2021) with rank 32, batch size 16, alpha 16, dropout 0.0, learning rate 1e-5, targeting default query and value matrices. We mask the loss during finetuning except for the token (yes/no) from which we obtain the logit based confidences P(True) introduced in Section 2 and finetune to match the LLMs prediction with the ground truth correctness (yes/no) marker using the standard cross-entropy loss. We used accelerate (Gugger et al., 2022) and trained using DeepSpeed with ZeRO stage 2 (Rajbhandari et al., 2019), mixed precision in bf16. Correctness Dataset Details. Section 2 mentions the creation of several correctness datasets for the training and evaluation of CMs. We list all datasets used in this work in Table 15 including the name of the base dataset used to gather questions, and the name of the model used to generate responses. The training data for the GCM is concatenation of the correctness datasets in block 1 or block 3 depending on whether the GCM was trained on MMLU or TriviaQA, while block 2 are datasets generated for the purposes of OOD evaluations. SCMs were trained on each of the listed datasets. The average length of MMLU dataset (used for training GCMs) was 198 tokens. The average length of response from the TriviaQA dataset was 70.4 tokens. ICL Retrieval Details. In order to facilitate semantic retrieval for the semantic ICL examples setting (Section 2), we utilize the chroma library (Chroma, 2025), and we use the default embed function, which at time of writing is all-MiniLM-L6-v2 based on Wang et al. (2020b). 16 Table 13: GCM performance across inference configurations. We report overall Accuracy, ECE, RMSCE, and AUROC. Configurations are grouped by type of ablation. The GCM has only been trained on the first row for each group of ablations, and it has not been trained on any reasoning based models. The GCM is able to maintain low ECE and high accuracy for correctness prediction. Configuration Acc ECE RMSCE AUROC Temperature (Qwen2.5-7B-Instruct) Temp 0.7 Temp 0.0 .816 .831 .030 . .085 .081 Reasoning (Qwen3-8B) Non-reasoning Reasoning (hidden) .838 .814 .023 .021 .076 . Quantization (Llama-3.1-8B-Instruct) FP16 GPTQ-INT4 .820 .825 .023 .036 .080 .086 .878 . .872 .827 .890 .897 Table 14: Uncalibrated accuracy and ECE by effective-batch-size for SCM predicting Qwen2.5-7B. Model effective-batch-size Acc ECE Qwen2.5-7B 128 Qwen2.5-7B 64 Qwen2.5-7B 32 Qwen2.5-7B 16 Qwen2.5-7B 8 Qwen2.5-7B 4 Qwen2.5-7B 2 Qwen2.5-7B 1 .750 .780 .788 .792 .795 .798 .803 .810 .102 .039 .030 .025 .028 .066 .118 .146 D.1 PROMPT TEMPLATES We document the prompt templates generated under different argument settings in the following paragraphs. SCM / GCM / Answer-only / P(True) You are grading [model name]s responses to prompts for correctness. ###Model Prompt {input_prompt} ###Model Response {cleaned_model_completion} ###Instruction Please respond just yes or no in lowercase if Model Response correctly answers Model Prompt: {ground truth yes/no} (Note: the trailing yes or no is inserted according to ground truth \"is correct\" of the response when used as training example, when used during inference, it is removed.) Answerless Setting You are predicting the likelihood that [model name] will respond correctly to prompt. ###Model Prompt {input_prompt} ###Instruction Please respond just yes or no in lowercase if [model name] will 17 Table 15: Constructed datasets from different base datasets and generator models. Base Dataset Model Used MMLU MMLU MMLU MMLU MMLU MMLU MMLU MMLU MMLU MMLU TriviaQA TriviaQA TriviaQA TriviaQA TriviaQA TriviaQA TriviaQA TriviaQA Gemma-3-27B-IT Llama-3.1-8B-Instruct Llama-3-70B-Instruct Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct Qwen2.5-32B-Instruct Qwen2.5-72B-Instruct Qwen3-8B Qwen3-32B Phi-3-mini-4k-instruct Gemma-3-27B-IT Llama-3.1-8B-Instruct Llama-3-70B-Instruct Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct Qwen2.5-32B-Instruct Qwen2.5-72B-Instruct Qwen3-8B respond correctly to Model Prompt: {ground truth yes/no} (Note: the trailing yes or no is inserted according to ground truth \"is correct\" of the response when used as training example, when used during inference, it is removed.) Verbalized Confidence You are examining the correctness of [model name]s predictions. You are given question and answer pair and you must output the probability that [model name] has answered the question correctly. You should make concise and informative analysis of the current question and response to determine whether the response has correctly answered the question. After your analyses, please respond with calibrated percent probability that the answer will be correct in the format ANSWER_CORRECT_PROBABILITY: xx.xx% ##Current Model Prompt {input_prompt} ##Current Model Response {cleaned_model_completion} ##Please respond with with verbalized and calibrated percent probability that the Current Model Response is correct response to Current Model Prompt and output your answer in the EXACT format ANSWER_CORRECT_PROBABILITY: xx.xx%. Be sure to follow the format exactly. ICL Verbalized Confidence You are examining the correctness of [model name]s predictions. You are given {N} training datapoints consisting of questions [model name] has been asked in the past. Training datapoints contain question, [model name]s response, and human labeled yes/no of whether the response was correct. After the training datapoints you are given the current question and answer pair and you must output the probability that 18 [model name] has answered the question correctly. You should make concise and informative analysis of the current question and response to determine whether the response has correctly answered the question. Then, if you are still unsure of your decision, you can explicitly analyze the models past performance on similar examples and make appropriate adjustments depending on the relevance of the training examples. After your analyses, please respond with calibrated percent probability that the answer will be correct in the format ANSWER_CORRECT_PROBABILITY: xx.xx% ##Previous Performances Example 0 -- Distance: {d_0} (lower = more similar) {document_0} ... Example -- Distance: {d_N} (lower = more similar) {document_N} ##Current Model Prompt {input_prompt} ##Current Model Response {cleaned_model_completion} ##Please respond with with verbalized and calibrated percent probability that the Current Model Response is correct response to Current Model Prompt and output your answer in the EXACT format ANSWER_CORRECT_PROBABILITY: xx.xx%. Be sure to follow the format exactly. (The ICL Verbalized Confidence prompt is not used for training, and thus does not include ground truth labels, except in the included ICL examples, during any form of its usage.) Model Name Ablation You are grading responses to prompts for correctness, responses could be generated from multiple LLMs. ###Prompt {input_prompt} ###Response {cleaned_model_completion} ###Instruction Please respond just yes or no in lowercase if the Response correctly answers the Prompt: {ground truth yes/no} (Note: the trailing yes or no is inserted according to ground truth \"is correct\" of the response when used as training example, when used during inference, it is removed.) Notes for Reproducibility Variable placeholders {cleaned model completion}, {document i}, {d i}, {ground truth yes/no}) are filled dynamically from the correctness dataset. ({input prompt}, {document i} and {d i} refer to results from ICL retrieval described under the ICL Retrieval Details heading, they represent full training example text with labels formatted according to the SCM/GCM/Answer-only/P(True) format and the rank of its similarity to the current example under consideration respectively. and {cleaned model completion} {input prompt} refer the to MMLU/TriviaQA prompt to the model, and the target models response. For the Answer-only prompt in MMLU, the {cleaned model completion} is truncated to only display the answer choice letter."
        },
        {
            "title": "E FURTHER RESULTS SHOWING THAT MODELS HAVE LITTLE SPECIAL",
            "content": "INFORMATION ABOUT THEIR OWN ABILITIES See Tables 17, 19, 16, and 18 for consolidated comparison of accuracy, calibration (ECE/RMSCE), and AUROC across answerless, answerful, and untrained settings, covering both within-model and cross-model transfers. Table 16: Untrained setting (row-wise). No tuning or epistemic supervision used. Configuration Acc ECE RMSCE AUROC Qwen2.5-7BQwen2.5-7B .638 .272 Llama3.1-8BQwen2.5-7B .708 .204 Qwen2.5-7BLlama3.1-8B .552 .331 Llama3.1-8BLlama3.1-8B .657 . .221 .193 .242 .229 .565 .656 .520 .668 Table 17: Answerless setting (row-wise) with Qwen3-8B and Qwen2.5-7B. Configuration Acc ECE RMSCE AUROC Qwen2.5-7BQwen2.5-7B .728 .018 Qwen3-8BQwen2.5-7B .737 .029 Qwen2.5-7BQwen3-8B .749 .023 Qwen3-8BQwen3-8B .765 . .089 .092 .086 .094 .719 .751 .714 .756 Table 18: Answerless setting (row-wise) with Qwen2.5-7B and Llama3.1-8B. Configuration Acc ECE RMSCE AUROC Qwen2.5-7BQwen2.5-7B .737 .016 Llama3.1-8BQwen2.5-7B .731 .024 Qwen2.5-7BLlama3.1-8B .690 .024 Llama3.1-8BLlama3.1-8B .694 . .081 .086 .091 .084 .720 .737 .703 ."
        },
        {
            "title": "F ADDITIONAL EVALUATIONS FOR GCMS VS SCMS",
            "content": "We train batch of 16 SCMs and compare their performances to GCMs on MMLU and TriviaQA. We use different seed for training (but the same dataset examples) as compared to Table 2 and Table 3. Accuracy impacts are within .2% and patterns stay consistent, showing that GCMs outperform SCMs. F.1 MMLU We compare the General Correctness Model (GCM) against Specific Models (SCMs) on MMLU across 8 models. The GCM outperforms SCMs on all metrics: Accuracy: +.022 on average (.829 vs. .807), with all 8 cases improving. Largest gain: gemma-3-27b-it (+.038). Smallest gain: Qwen2.5-32B-Instruct (+.009). ECE: .002 on average (.024 vs. .026, lower is better). Improvements: 5 cases; Degradations: 3 cases. Largest decrease (best improvement): Qwen3-8B (.013). Largest increase (worst degradation): Llama-3.1-8B-Instruct (+.007). RMSCE: .004 on average (.079 vs. .083, lower is better). Improvements: 7 cases; Degradations: 1 case. Largest decrease (best improvement): Qwen3-8B (.014). Largest increase (worst degradation): Llama-3.1-8B-Instruct (+.010). 20 Table 19: Answerful setting (row-wise). Models are given access to the predicted answer. Configuration Acc ECE RMSCE AUROC Qwen2.5-7BQwen2.5-7B .768 .019 Llama3.1-8BQwen2.5-7B .761 .024 Qwen2.5-7BLlama3.1-8B .751 .022 Llama3.1-8BLlama3.1-8B .730 . .081 .079 .078 .084 .791 .790 .804 .775 AUROC: +.041 on average (.866 vs. Largest gain: Qwen2.5-72B-Instruct (+.064). Smallest gain: Qwen2.5-7B-Instruct (+.030). .825), with all 8 cases improving. The largest accuracy gain is observed for gemma-3-27b-it (+.038), while the largest AUROC gain is for Qwen2.5-72B-Instruct (+.064). Calibration metrics Calibration error stays extremely low <3% for both GCM and SCM, with some improvements observed for GCM. Refer to Table 20 for the full results. Table 20: Per-model comparison of Specific Models (SCM) vs. General Correctness Model (GCM) on MMLU. For Accuracy, higher is better; for ECE and RMSCE, lower is better; for AUROC, higher is better. is GCMSCM. Model Accuracy SCM GCM () ECE SCM GCM () RMSCE SCM GCM () AUROC SCM GCM () Llama-3.1-8B-Instruct Meta-Llama-3-70B-Instruct Qwen2.5-32B-Instruct Qwen2.5-3B-Instruct Qwen2.5-72B-Instruct Qwen2.5-7B-Instruct Qwen3-8B gemma-3-27b-it .791 .804 .825 .790 .839 .792 .814 .798 .820 (+.029) .822 (+.018) .834 (+.009) .820 (+.030) .849 (+.010) .816 (+.024) .834 (+.020) .836 (+.038) .016 .029 .036 .014 .016 .026 .035 .037 .023 (+.007) .025 (.003) .029 (.007) .020 (+.006) .014 (.003) .030 (+.004) .021 (.013) .029 (.008) .070 .086 .088 .079 .072 .086 .091 . .080 (+.010) .078 (.008) .087 (.001) .073 (.007) .069 (.003) .085 (.002) .076 (.014) .085 (.009) .857 .803 .806 .864 .776 .848 .835 .811 .890 (+.033) .849 (+.045) .844 (+.038) .894 (+.031) .840 (+.064) .878 (+.030) .867 (+.031) .865 (+.054) F.2 TRIVIAQA We compare the General Correctness Model (GCM) against Specific Models (SCMs) on TriviaQA across 8 models. The GCM outperforms SCMs on all metrics: Accuracy: +.021 on average Degradations: Smallest Meta-Llama-3-70B-Instruct supplement (.005). Llama-3.1-8B-Instruct (+.002). 1 case. gain: (.865 vs. Largest gain: .844). Improvements: 7 cases; Qwen3-8B supplement (+.042). degradation: One ECE: .0005 on average (.0233 vs. .0238, lower is better). Improvements: 4 cases; Degradations: 4 cases. Largest decrease (best improvement): Qwen2.5-72B-Instruct (.015). Largest increase (worst degradation): Qwen2.5-3B-Instruct (+.008). RMSCE: .0018 on average (.0775 vs. 5 cases; Degradations: Improvements: improve3 cases. ment): Qwen2.5-72B-Instruct (.015). Largest increase (worst degradation): Meta-Llama-3-70B-Instruct supplement (+.014). Largest decrease (best is better). .0793, lower AUROC: +.031 on average (.901 vs. .869). Improvements: all 8 cases. Largest gain: Qwen2.5-32B-Instruct (+.047). Smallest gain: Llama-3.1-8B-Instruct (+.010). The strongest accuracy gain is seen for Qwen3-8B supplement (+.042), while the largest AUROC gain is for Qwen2.5-32B-Instruct (+.047). Calibration error stays extremely low <3%, with some improvements observed for GCM. Qwen2.5-72B-Instruct shows the largest calibration improvement. Refer to Table 21 for the full results. 21 Table 21: Per-model comparison on TriviaQA: Specific Models (SCM) vs. General Correctness Model (GCM). For Accuracy, higher is better; for ECE and RMSCE, lower is better; for AUROC, higher is better. is GCMSCM. Model Accuracy SCM GCM () ECE SCM GCM () RMSCE SCM GCM () AUROC SCM GCM () Llama-3.1-8B-Instruct Meta-Llama-3-70B-Instruct supp. Qwen2.5-32B-Instruct Qwen2.5-3B-Instruct Qwen2.5-72B-Instruct Qwen2.5-7B-Instruct Qwen3-8B supplement gemma-3-27b-it .845 .890 .839 .832 .875 .816 .818 .840 .847 (+.002) .884 (.005) .867 (+.029) .872 (+.040) .879 (+.003) .850 (+.034) .860 (+.042) .862 (+.022) .025 .018 .025 .015 .034 .025 .020 .028 .029 (+.004) .021 (+.003) .022 (.003) .023 (+.008) .019 (.015) .023 (.002) .021 (+.002) .028 (.000) .082 .067 .080 .069 .085 .086 .078 .086 .090 (+.007) .080 (+.014) .070 (.011) .081 (+.012) .070 (.015) .078 (.008) .077 (.002) .074 (.012) .896 .800 .865 .912 .861 .888 .888 .844 .905 (+.010) .819 (+.019) .912 (+.047) .944 (+.032) .891 (+.030) .924 (+.036) .928 (+.040) .881 (+.038) Table 22: Unlimited training time ablation. Columns report Accuracy (avg correct), Binary ECE (), RMSCE (), and AUROC (). Under conditions where the SCM is allowed to train for as many iterations as necessary until validation loss starts to increase, the GCM is still able to outperform the SCM. Metrics shown for GCM and SCM predicting the correctness of Llama3.1-8B. Method Acc ECE RMSCE AUROC Optimal SCM .822 Optimal GCM .845 .023 . .073 .087 .894 ."
        },
        {
            "title": "G UNLIMITED TRAINING TIME ABLATION",
            "content": "For our main analysis we train the General Model for the same number of epochs as the specific model to match training time. In such case, training multiple specific models and training one general model would have approximately the same training time cost. We show an ablation here, that even given an unlimited amount of training time (until overfitting occurs), the GCM still outperforms SCMs  (Table 22)  ."
        },
        {
            "title": "H FURTHER DISCUSSION ON POSTHOC CALIBRATION",
            "content": "For results in the main paper we make use of the spline calibration (Lucena, 2018) posthoc calibration method. For spline calibration, we use number of knots equal to 2 or 4 for all results. We explore other calibration methods and their influence on calibration for SCMs of different batch sizes in Table 23. We choose spline calibration after observing positive results for calibration error reduction and that spline calibration can preserve smooth probability distribution in some cases. For Platt scaling (Platt, 2000), the output probability distribution is directly scaled, or compressed, this precludes predictions of high confidence if the distribution is scaled. Spline calibration does not directly compress the distribution and has the possibility of retaining some high probability predictions."
        },
        {
            "title": "I CONDITIONING FACTORS ABLATIONS",
            "content": "We include the full metrics for conditioning factors ablations in appendix Table 24."
        },
        {
            "title": "J RELATED WORK",
            "content": "Self-Knowledge and Confidence calibration. Since calibration is essential for deciding when to trust AI systems, prior work has extensively studied calibration in neural models (Naeini et al., 2015; Guo et al., 2017a; Ovadia et al., 2019; Wang et al., 2020a), with more recent efforts turning to calibration in large language models (LLMs) (Mielke et al., 2022; Kadavath et al., 2022; Kuhn et al., 2023; Stengel-Eskin et al., 2024; Tian et al., 2023). Early studies found that generative models such as T5, BART, and GPT-2 are often poorly calibrated for QA tasks, requiring post-hoc or fine22 Table 23: Calibration results across different effective batch sizes (1, 4, 16). We report Expected Calibration Error (ECE) and Root Mean-Square Calibration Error (RMSCE) for uncalibrated probabilities and posthoc calibration using Spline (knots=4), Isotonic, Beta, and Platt. Best values (lower is better) are bolded, with ties bolded for both. Results from SCMs trained to predict Qwen2.5-7B. batch-size 1 batch-size 4 batch-size Method ECE RMSCE ECE RMSCE ECE RMSCE Uncalibrated Spline Isotonic Beta Platt .146 .052 .034 .043 . .177 .119 .140 .113 .164 .066 .010 .039 .022 .055 .117 .074 .148 .078 .113 .027 .022 .032 .022 .044 .085 .078 .122 .084 .097 Table 24: Conditioning factors ablations (Section 3.3), full results on all metrics. Specific Model General Model Setting Full (c q, r, ˆr) Answer-only (c q, ˆr) Answerless (c q) Acc .792 .745 . ECE RMSCE AUROC Acc ECE RMSCE AUROC .017 .023 .030 .069 .087 .101 .857 .810 .735 .820 .789 . .023 .034 .024 .080 .088 .095 .890 .852 .781 tuning methods to better align probabilities with correctness (Jiang et al., 2021). Other works examined overconfidence in dialogue agents and proposed linguistic calibration, matching expressions of doubt with correctness likelihoods, as remedy (Mielke et al., 2022). Prompting-based methods have also been explored: Kadavath et al. (2022) showed that larger LLMs can produce reasonably calibrated probabilities when asked directly, while Kapoor et al. (2024) argued that prompting alone is insufficient, and that fine-tuning with correctness labels yields better transferable estimates. Additional studies examined unanswerable questions (Yin et al., 2023), lying behavior via hidden activations (Azaria & Mitchell, 2023), and black-box elicitation frameworks combining prompting, sampling, and aggregation (Xiong et al., 2024). Despite these advances, LLMs remain overconfident, and calibration quality improves with scale but falls short of reliability. In contrast to these self-knowledge-based approaches, our work demonstrates that models lack privileged access to their own correctness and introduces more general solution to calibrate multiple LLMs at once. Correctness Models and Cross-Model Transfer. parallel line of work explicitly uses correctness models (CMs) to estimate whether response is correct. The simplest CMs rely on self-reported confidence from the model itself (Tian et al., 2023), while stronger approaches train linear probes on hidden states (Liu et al., 2024b; Kadavath et al., 2022; Beigi et al., 2024; Azaria & Mitchell, 2023; Liu et al., 2024a) or fine-tune entire LLMs to answer correctness questions directly (Kapoor et al., 2024). Recent studies go beyond surface calibration by modeling semantic uncertainty, capturing variability in the meaning of generated outputs, which has been shown to better correlate with correctness (Kuhn et al., 2023). Another intriguing development is the use of surrogate models: Shrivastava et al. (2023) find that even untrained LLaMA models can sometimes predict GPT confidences more accurately than GPTs own self-reported probabilities, suggesting biases in linguistic elicitation. These previous works suggest that correctness signals could potentially transfer across models, but they remain in the one-model-to-one-model setting and do not study the factors that influence the calibration of correctness model. By contrast, we document these factors and leverage the findings in our Generalized Correctness Model (GCM), which aggregates correctness patterns across many models, providing more robust and empirically grounded calibration method. Downstream Applications. Correctness estimation has been leveraged to improve downstream Improved calibration benefits hallucination detection and truthfulness (Zhou et al., 2025; tasks. Li et al., 2024; 2025b), enhances interpretability (Stengel-Eskin et al., 2024), strengthens reasoning ability (Wang et al., 2024b; Li et al., 2025a), improves semantic parsing (Stengel-Eskin & Van Durme, 2023a), and supports reliable deployment in system-level routing setups (Hu et al., 2024; Wang et al., 2024a; Ong et al., 2024). Our GCM advances this line of work by providing 23 model-agnostic, history-aware framework for correctness estimation that generalizes across both models and datasets."
        }
    ],
    "affiliations": [
        "The University of Texas at Austin",
        "UNC Chapel Hill"
    ]
}