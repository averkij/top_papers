{
    "paper_title": "Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning",
    "authors": [
        "Mengtan Zhang",
        "Zizhan Guo",
        "Hongbo Zhao",
        "Yi Feng",
        "Zuyi Xiong",
        "Yue Wang",
        "Shaoyi Du",
        "Hanli Wang",
        "Rui Fan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Unsupervised learning of depth and ego-motion, two fundamental 3D perception tasks, has made significant strides in recent years. However, most methods treat ego-motion as an auxiliary task, either mixing all motion types or excluding depth-independent rotational motions in supervision. Such designs limit the incorporation of strong geometric constraints, reducing reliability and robustness under diverse conditions. This study introduces a discriminative treatment of motion components, leveraging the geometric regularities of their respective rigid flows to benefit both depth and ego-motion estimation. Given consecutive video frames, network outputs first align the optical axes and imaging planes of the source and target cameras. Optical flows between frames are transformed through these alignments, and deviations are quantified to impose geometric constraints individually on each ego-motion component, enabling more targeted refinement. These alignments further reformulate the joint learning process into coaxial and coplanar forms, where depth and each translation component can be mutually derived through closed-form geometric relationships, introducing complementary constraints that improve depth robustness. DiMoDE, a general depth and ego-motion joint learning framework incorporating these designs, achieves state-of-the-art performance on multiple public datasets and a newly collected diverse real-world dataset, particularly under challenging conditions. Our source code will be publicly available at mias.group/DiMoDE upon publication."
        },
        {
            "title": "Start",
            "content": "Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning Mengtan Zhang , Zizhan Guo , Hongbo Zhao , Yi Feng , Zuyi Xiong , Yue Wang , Shaoyi Du , Hanli Wang , Senior Member, IEEE, and Rui Fan , Senior Member, IEEE 1 5 2 0 2 ] . [ 1 2 0 5 1 0 . 1 1 5 2 : r AbstractUnsupervised learning of depth and ego-motion, two fundamental tasks in 3D perception, has made significant strides in recent years. Nevertheless, most existing methods often treat ego-motion estimation as an auxiliary task, either indiscriminately mixing all motion types or excluding depthindependent rotational motions when generating supervisory signals. Such designs hinder the incorporation of sufficiently strong geometric constraints into the joint learning framework, thereby limiting the reliability and robustness of these models under diverse environmental conditions. This study introduces discriminative treatment of motion components by leveraging the distinct geometric regularities inherent in their respective rigid flows, which simultaneously benefits both depth and egomotion estimation. Given consecutive video frames, the network outputs are first employed to align the optical axes and imaging planes of the source and target cameras. Optical flows between the original frames are then transformed through these alignment processes, and their deviations are quantified to impose geometric constraints. These constraints are applied individually to each estimated ego-motion component, enabling more targeted and reliable refinement. These alignments further reformulate the joint learning process into coaxial and coplanar forms, where depth and each translation component can be mutually derived through closed-form geometric relationships, introducing complementary constraints that significantly improve the robustness of depth learning. DiMoDE, general depth and ego-motion joint learning framework that incorporates all these innovative designs, achieves state-of-the-art performance in both tasks, as demonstrated by extensive experiments on multiple public datasets and newly collected, diverse real-world dataset, particularly under (Corresponding author: Rui Fan) Mengtan Zhang and Hongbo Zhao are with the Shanghai Research Institute for Intelligent Autonomous Systems, Tongji University, Shanghai 201210, China (e-mails: zmt200110@tongji.edu.cn, hongbozhao@tongji.edu.cn). Zizhan Guo, Yi Feng, and Zuyi Xiong are with the College of Electronic & Information Engineering, Tongji University, Shanghai 201804, China (e-mails: 2431983@tongji.edu.cn, fengyi@ieee.org, 2153478@tongji.edu.cn). Yue Wang is with the Department of Control Science and Engineering, Zhejiang University, Hangzhou, Zhejiang 310027, China (e-mail: wangyue@iipc.zju.edu.cn). Shaoyi Du is with the National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, the National Engineering Research Center for Visual Information and Applications, and the Institute of Artificial Intelligence and Robotics, Xian Jiaotong University, Xian, Shaanxi 710049, China (e-mail: dushaoyi@xjtu.edu.cn). Hanli Wang is with the College of Electronic & Information Engineering, the School of Computer Science and Technology, and the Key Laboratory of Embedded System and Service Computing (Ministry of Education), Tongji University, Shanghai 201804, China (e-mail: hanliwang@tongji.edu.cn). Rui Fan is with the College of Electronic & Information Engineering, Shanghai Institute of Intelligent Science and Technology, Shanghai Research Institute for Intelligent Autonomous Systems, the State Key Laboratory of Autonomous Intelligent Unmanned Systems, the Frontiers Science Center for Intelligent Autonomous Systems (Ministry of Education), and Shanghai Key Laboratory of Intelligent Autonomous Systems, Tongji University, Shanghai 201804, China, as well as with the National Key Laboratory of HumanMachine Hybrid Augmented Intelligence, Xian Jiaotong University, XiAn, Shaanxi 710049, China (e-mail: rui.fan@ieee.org). challenging conditions where existing approaches often fail. Our source code will be publicly available at mias.group/DiMoDE upon publication. Index Termsdepth estimation, visual odometry, motion component, geometric constraint. I. INTRODUCTION CCURATE estimation of ego-motion and depth is crucial for 3D perception [1]. Recently, there has been significant increase in methods that jointly learn ego-motion and depth from monocular video sequences in an unsupervised, end-to-end manner [2][4]. Compared to supervised depth estimation approaches [5][7], this paradigm eliminates the need for extensive, manually labeled annotations [2], [8], which are not always available in real-world scenes. Moreover, compared to traditional visual odometry methods that rely on 2D visual correspondences to explicitly solve the relative pose problem, such joint learning paradigm can maintain robustness in scenarios where 2D correspondences are fairly sparse or unreliable [9], [10]. This unsupervised joint learning paradigm typically consists of depth estimation network (hereafter known as DepthNet) and pose estimation network (hereafter known as PoseNet) [4], [8]. DepthNet estimates dense depth map. PoseNet regresses the following ego-motion transformation: = (cid:19) (cid:18) 0 1 SE(3), (1) where SO(3) denotes the rotation matrix, = (tx, ty, tz) R3 denotes the translation vector, and 0 denotes column vector of zeros. The estimated depth map and egomotion are then utilized to calculate the rigid flow, key component for warping the source image into the target view. By minimizing the photometric difference between the target and the warped images, these two networks can be trained simultaneously. Fig. 1 presents the rigid flows resulting from various motion types, topic that has received limited discussion in previous studies. As shown in Figs. 1(a) and 1(b), egomotion that involves only translations yields fairly regular rigid flows. Specifically, when the camera moves perpendicularly to the optical axis, leading to tangential translation vector tTan = (tx, ty, 0), the rigid flows of all pixels are parallel. When the camera moves along the optical axis, leading to radial translation vector tRad = (0, 0, tz), the rigid flows of all pixels move toward or away from the principal point. 2 Fig. 1. Tangential and radial translations result in geometrically regular but distinct depth-dependent flows. Specifically, the rigid flow induced by tangential translation varies inversely with depth, while the one resulting from radial translation is not only depth-dependent but also subject to perspective scaling. In contrast, rotation results in irregular, depth-independent flows. These regular rigid flows provide vital visual clues that can potentially improve the accuracy of estimated motion components. On the other hand, as illustrated in Fig. 1(c), egomotion that involves only rotations results in fairly irregular rigid flows. Therefore, it is inappropriate to train PoseNet to indiscriminately regress rotation and translation components by mixing these two types of rigid flows for supervisory signal generation. However, there are currently few previous studies that focus specifically on this aspect. In addition, we revisit Fig. 1 to explore potential improvements in DepthNet training. As demonstrated in the study [11], two images related by pure-rotation transformation can be warped from one to the other using homography matrix = KRK1, where denotes the cameras intrinsic matrix. Therefore, as shown in Fig. 1(c), the rigid flow produced solely by rotation is independent of depth and does not provide effective gradients necessary for the supervision of DepthNet training. In contrast, as illustrated in Fig. 1(a), pure tangential translation results in rigid flows characterized by superposed horizontal and vertical positional shifts. These shifts, commonly referred to as disparities in stereo vision, are inversely proportional to depth (see Fig. 1(d)) and can directly contribute to the training of DepthNet. Furthermore, as illustrated in Fig. 1(b), pure radial translation leads to perspective scaling in sequential images, which is also depth-dependent. Specifically, the scale change of an object in the image is inversely proportional to its depth. Thus, these rigid flows also provide valuable visual cues that can be effectively incorporated into the DepthNet training process. However, perspective scaling causes pixels near the image boundary to undergo larger displacements than those near the principal point. Consequently, mixing these two types of visual cues can lead to inconsistent gradients across the image, potentially impeding convergence and degrading depth estimation performance. Despite this limitation, the significant differences between tangential and radial translations have long been overlooked, with existing studies generally treating them indiscriminately. These observations motivate us to discriminately treat the motion components in by isolating the rotation and separately processing the tangential and radial translations. Specifically, the estimated rotation is expected to align the camera orientation of the source view with that of the target view, thereby eliminating irregular rotational flows. The estimated tangential translation tTan and radial translation tRad are then expected to align the cameras optical axes and imaging planes in the source and target views, respectively, thereby generating two types of regular flows containing vital visual clues. Building upon the aforementioned analyses, this study aims to reformulate the existing joint learning framework for depth and ego-motion estimation by introducing explicit geometric constraints from motion components. These constraints are imposed through the alignment of the optical axes and imaging planes between the source and target cameras. In practice, we utilize dense correspondences that adhere to the static scene hypothesis to objectively quantify the extent of both types of alignment. These correspondences, when transformed by performing both alignments based on the estimated depth and ego-motion, are expected to exhibit the aforementioned geometric regularities. Deviations from the expected geometric regularities are employed as supervisory signals to independently guide the learning of tangential and radial translations, while simultaneously constraining the learning of rotation. As the transformed correspondences progressively exhibit expected geometric regularities, depth-dependent visual cues arising from positional shifts and perspective scaling are effectively disentangled. By exploiting their distinct depthdependent variations, these visual cues are processed separately to recover depth, thereby providing per-pixel constraints for depth estimation. The depth estimated under these constraints and the transformed correspondences are subsequently used to compute tangential and radial translations based on simplified geometric relationships. geometric constraint is also incorporated to enforce consistency between these computed translations and those predicted by the PoseNet, thereby establishing constraining cycle that enhances the robustness of depth learning. Incorporating the above-mentioned innovative components into general depth and ego-motion joint learning framework yields DiMoDE, which discriminately treats motion components. Extensive experiments on multiple public datasets and our collected real-world video sequences demonstrate the state-of-the-art (SoTA) performance of our proposed DiMoDE in both monocular depth estimation and visual odometry, as well as the efficacy of discriminately treating motion components. In summary, our main contributions are as follows: We conduct both phenomenological and theoretical analyses on the distinctions among rigid flows arising from different motion types and the long-overlooked limitations caused by indiscriminately mixing these flows when generating supervisory signals. We introduce optical axis and imaging plane alignment processes that reformulate joint learning from monocular video into coaxial and coplanar forms, thereby mitigating the effects of mixed motion types and enhancing the robustness and stability of depth learning. We propose discriminative treatment of motion components that enables these alignments to be achieved using network outputs, not only eliminating the need for auxiliary pose estimation algorithms but also refining each component of the PoseNet outputs. We develop DiMoDE, general framework compatible with various models, achieving notable improvements in network convergence and training robustness for both depth estimation and visual odometry across diverse challenging conditions. The remainder of this article is organized as follows: Sect. II reviews existing monocular visual odometry and depth estimation methods. Sect. III introduces preliminaries of monocular depth and ego-motion joint learning. Sect. IV details the proposed discriminative treatment of motion components, and Sect. presents the resulting DiMoDE framework, respectively. Sect. VI presents the experimental results across several public datasets and our collected real-world dataset. In Sect. VII, we discuss two limitations of the DiMoDE framework. Finally, Sect. VIII concludes this article and discusses possible directions for future work. II. RELATED WORK A. Monocular Visual Odometry Monocular visual odometry is fundamental and extensively studied problem in the fields of robotics and computer vision [12]. Existing approaches can be broadly categorized into three main classes: geometry-based, learning-based, and hybrid. Earlier geometry-based methods, such as VISO2 [13] and ORB-SLAM series [14][16], primarily rely on hand-crafted keypoints and descriptors for correspondence matching, and often perform poorly in texture-less environments [9], [12]. To address this limitation, methods like DSO [17] formulate direct image or feature warping as an energy minimization problem, thus avoiding the need for accurate correspondence matching. Despite their robustness in texture-less environments, these methods exhibit lower accuracy ceiling compared to approaches based on correspondence matching [18]. In recent years, extensive learning-based methods [19], [20] have been proposed to tackle the above-mentioned challenges. These methods typically employ PoseNet to regress camera poses from consecutive video frames in an end-to-end manner [8], [21][23]. By eliminating the need for explicit correspondence matching used to solve for relative pose, PoseNet remains robust in texture-less scenes and typically supports efficient inference. Among them, the study [8] proposes 3 joint depth and ego-motion learning paradigm that trains the PoseNet with only unlabeled monocular video, thereby alleviating the need for expensive manual annotation. Subsequent studies have focused on addressing issues such as scale inconsistency and limited generalizability [23], [24], while also incorporating temporal and spatial cues into PoseNet to improve their performance [10], [25], [26]. Despite these advancements, the fundamental challenges of generating more reliable supervisory signals in an unsupervised manner to ensure accurate and robust ego-motion learning remain unsolved [12], [18]. More recently, hybrid approaches [12], [18], [27], [28] have introduced new paradigm that integrates neural network predictions, such as depth and correspondence matching, with back-end optimization for relative pose estimation. Despite the incorporation of geometric constraints during inference, enabling these hybrid methods to achieve superior performance over fully learning-based approaches, the resulting high storage demands and computational complexity significantly hinder their applicability in real-world, resource-constrained environments. This study incorporates explicit geometric constraints from each ego-motion component by enforcing the alignment of learning optical axes and imaging planes within the joint framework. The proposed DiMoDE framework enables simple ResNet-based PoseNet [29] to achieve performance comparable to SoTA geometric-based and hybrid approaches, with significantly lower storage and computational overhead. B. Monocular Depth Estimation Monocular depth estimation, which infers depth information from single RGB image, has found widespread applications in areas such as autonomous driving and embodied artificial intelligence. Early approaches typically adopted supervised learning paradigm that requires extensive depth ground truth for model training [5]. Building on this foundation, subsequent studies [30][33] improved depth estimation performance by reformulating the depth regression problem as either perpixel classification task that predicts discretized depth intervals or classification-regression task that predicts depth as weighted combination of bin centers. More recently, the studies [7], [34], [35] have leveraged vision foundation models to achieve highly accurate depth estimation. To alleviate the need for extensive ground-truth data in monocular depth estimation, growing interest has been directed toward unsupervised learning of depth from monocular video, paradigm first introduced in the study [8]. To improve depth estimation performance within this paradigm, the study [2] introduced per-pixel minimum reprojection loss to enhance the photometric supervisory signals, while studies [4], [36], [37] explored more sophisticated network architectures. Subsequent studies [3], [38][41] have sought to preserve the effectiveness of this training paradigm in scenarios with occlusions and dynamic objects by introducing additional visual cues. However, these methods rely solely on per-pixel supervisory consistency, which limits their robustness under adverse conditions, such as nighttime or poor weather, where such signals often become unreliable. 4 Recently, several studies [42], [43] have focused specifically on enhancing the robustness of depth estimation under adverse conditions. These methods typically adopt domain adaptation strategies, wherein networks are initially trained on clear daytime scenes and subsequently adapted to more challenging environments such as nighttime or fog. Consequently, their reliance on fixed data splits hinders the full utilization of largescale data collected under diverse and dynamically changing conditions, limiting the continuous evolution of DepthNet. In contrast, this study focuses on the discriminative treatment of motion components during the original supervisory signal generation, enabling robust constraint cycle for depth estimation. To the best of our knowledge, DiMoDE is the first approach to effectively address the challenges posed by adverse environments within unified and general joint learning framework. III. PRELIMINARIES Given monocular camera with the intrinsic matrix fu 0 u0 v0 1 0 fv 0 = , (2) where fu and fv denote the focal lengths in the horizontal and vertical directions, respectively, and p0 = (u0, v0) represents the principal point, pair of RGB images, and s, captured from the target and source views, respectively, are fed into the DepthNet to generate their corresponding depth maps, Dt and Ds. Let pt = (ut, vt) and ps = (us, vs) be pair of corresponding pixels in the target and source images, respectively. The depth values at these pixels are denoted as ˆzt = Dt(pt) and ˆzs = Ds(ps). Their respective homogeneous coordinates are denoted as pt and ps. Meanwhile, and are concatenated and fed into the PoseNet to estimate the ego-motion ˆT from the source view to the target view. The estimated depth and ego-motion are then used to generate rigid flow map Rig ts in the target view based on the following relation: ps = pt + (cid:19) (cid:18)f Rig ts = 1 ˆzs (cid:16) (cid:17) ˆzt ˆRK1 pt + ˆt , (3) ts = Rig where Rig ts(pt) denotes the rigid flow at pixel pt and is expected to equal the optical flow Opt ts(pt) = ps pt in static regions. is then warped into the target view based on the rigid flow map, resulting in synthesized image expressed as follows [8]: ts = Opt = (cid:16) s, Rig ts (cid:17) , (4) where W() denotes the image warping function. To quantify the appearance discrepancy between and t, the following photometric reprojection loss is computed: Lpho = α 1 SSIM(I t, t) 2 + (1 α) (cid:13) (cid:13)I (cid:13) (cid:13)1 , (5) where SSIM denotes the pixel-wise structural similarity index operation [44], and the weight α is empirically set to 0.85, following the study [2]. (5) serves as the supervisory signal for the training of both DepthNet and PoseNet [23]. IV. DISCRIMINATIVE TREATMENT OF MOTION COMPONENTS Ego-motion estimation plays role equally important to that of depth estimation, as the supervisory signal is jointly determined by the transformation predicted by PoseNet and the depth map Dt generated by DepthNet. Unfortunately, egomotion estimation has long been regarded as an auxiliary task [2], [4], with limited attention given to thoroughly analyzing the outputs produced by PoseNet. As discussed in Sect. I, is mixture of three types of transformations as follows: = (cid:19) (cid:18) 0 1 = (cid:18) 0 tRad 1 (cid:19) (cid:18) 0 tTan 1 (cid:124) (cid:123)(cid:122) Rad (cid:125) (cid:124) (cid:123)(cid:122) Tan (cid:19) (cid:125) (cid:18) 0 0 1 (cid:123)(cid:122) Rot (cid:124) (cid:19) , (cid:125) (6) where Rot, Tan, and Rad represent pure rotation, pure tangential translation, and pure radial translation transformations, respectively. The prior art [11] claims that Rot is irrelevant to the training of DepthNet, as in the pure rotation case, one image can be warped to the other using Rot alone, without the need for depth information [45]. More importantly, even slight rotational errors in Rot can be mistakenly interpreted as translational motions, leading to erroneous estimations of Tan and Rad. The resulting supervisory signals can, in turn, mislead the training of DepthNet. Therefore, the authors proposed to exclude rotational motions from DepthNet training within the joint learning framework. Specifically, the original target and source images, and s, are respectively warped based on ˆT Rot, which is estimated using either the five-point algorithm [46] or an auxiliary network [29], to align the orientations of source and target cameras. The warped image pair is then fed into the joint learning framework, enabling both PoseNet and DepthNet to be trained under the assumption that only translational motions are involved. Nonetheless, rotation estimation performed by an independent algorithm or network remains decoupled from the original joint learning framework, which introduces considerable computational and storage overhead without directly benefiting PoseNet training. Moreover, rotational errors introduced by decoupled algorithms are no longer considered for minimization during training, allowing them to propagate through the pipeline and degrade overall framework performance. More critically, all existing methods overlook the distinct regularities inherent in the two types of translational motions, which are closely tied to depth estimation. Therefore, this study investigates the discriminative treatment of three distinct motion components to simultaneously improve both depth and pose estimation performance within learning framework. Specifically, when the egothe joint motion corresponds to pure rotation transformation Rot, the pixel correspondences pt and ps are expected to satisfy the following relation: ps = zt zs KRK1 (cid:123)(cid:122) (cid:125) (cid:124) pt (cid:0)h1, h2, h3 (cid:124) (cid:123)(cid:122) pt, (cid:1) (cid:125) (7) where denotes the homography matrix [45] with = (hi1, hi2, hi3) representing the i-th row vector, and the symbol 5 When the ego-motion corresponds to pure tangential translation transformation Tan, the pixel correspondences pt and ps are expected to satisfy the following relation: ps (cid:0)ztK1 pt + tTan(cid:1) . Substituting (2) into (10) yields (cid:18) ztut + futx zt ps = (cid:19) , ztvt + fvty zt and Rig ts = (cid:18)futx fvty (cid:19) , 1 zt (10) (11) (12) which indicates that the tangential translational flows across the image are oriented in the direction of (futx, fvty) and vary solely with depth, as illustrated in Fig. 2(a). Similarly, when the ego-motion corresponds to pure radial translation transformation Rad, the pixel correspondences pt and ps are expected to satisfy the following relation: ps (cid:0)ztK1 pt + tRad(cid:1) . Substituting (2) into (13) yields (cid:18) ztut + u0tz zt + tz ps = (cid:19) , ztvt + v0tz zt + tz and Rig ts = tz zt + tz (cid:18)ut u0 vt v0 (cid:19) , (13) (14) (15) which indicates that radial translational flows across the image are consistently oriented toward or away from the principal point p0, as illustrated in Fig. 2(a). Consequently, these flows are relevant not only with depth but also with pixel coordinates, with their magnitudes diminishing near p0 due to the reduced distance between pt and p0. However, when both Tan and Rad are present, the resulting rigid flows can be expressed as follows: Rig ts = 1 zt + tz (cid:18)futx (ut u0)tz fvty (vt v0)tz (cid:19) , (16) which obscures the distinct regularities inherent in both types of flows, thereby hindering the framework from discriminately backpropagating their corresponding supervisory signals to the network outputs. Specifically, the gradients propagated through Rig ts are given by: Rig ts zt Rig ts tx Rig ts ty Rig ts tz = = = 1 (zt + tz)2 (cid:18)fu 1 0 zt + tz (cid:18) 0 1 fv zt + tz 1 (zt + tz)2 (cid:18)futx (ut u0)tz fvty (vt v0)tz (cid:19) , (cid:19) (cid:19) , , (cid:18)(ut u0)zt + futx (vt v0)zt + fvty (cid:19) . (17) (18) (19) (20) = Fig. 2. The discriminative treatment of motion components and the resulting flow decomposition processes. indicates that the two vectors are equal up to scale factor. Rearranging (7) yields ps = (cid:33) (cid:32) h 1 pt 3 pt , h 2 pt 3 pt and Rig ts = 1 3 pt p (cid:16) h3i 1 (cid:16) h3i 2 (cid:17) (cid:17) pt + pt + 1 pt 2 pt , (8) (9) where ik R3 denotes the k-th column vector of an 3 3 identity matrix. (9) indicates that, when only rotation is involved, the rigid flow becomes nonlinear function of the pixel coordinates pt. This nonlinearity arises from two sources: (1) the quadratic terms +h32utvt +h33ut (h3i and + h33vt, as well as (2) the perspective division term 1/(h 3 pt) = 1/(h31ut + h32vt + h33). As result, in such cases, the rigid flow is determined solely by the rotation and the intrinsic matrix, and can vary significantly across the entire image, giving rise to highly irregular rigid flow map1, as shown in Fig. 1(c). (h3i 2 )pt = h31utvt + h32v2 1 )pt = h31u2 Therefore, instead of relying on an independent algorithm or auxiliary network to estimate rotation for image warping, we directly utilize ˆT Rot provided by the PoseNet to eliminate the irregular rotational flows from the original optical flow map. Ideally, the resulting rigid flow map retains only the two types of translational flows, induced by Tan and Rad, which are directly relevant to depth estimation, as illustrated in Fig. 2(a). 1Although rotational flows also comprises three relatively regular components [47], [48], they cannot be obtained practically by decomposing the original flows. This is because the translational component cannot be eliminated prior to the rotational ones due to the non-commutative nature of rigid-body transformations, as shown in (6). (17) indicates that zt inherently receives smaller gradients near the principal point, which hinders consistent optimization the across the image. Moreover, (18) and (19) show that gradient of tangential translation components are influenced Fig. 3. Ego-motion transformation decomposition: (a) Ideally, ego-motion can be decomposed into pure tangential and radial translation components; (b) In practice, errors in the PoseNet predictions introduce undesired deviations to the decomposed components. by the radial one, while (20) reveals the opposite influence. As result, errors in one component can propagate to and impair the gradients for the others, ultimately hindering the convergence of PoseNet training. To address these issues, after eliminating the irregular rotational flows, we further utilize ˆT Tan and ˆT Rad to decompose the remaining translational flows into the components that are geometrically regular but different with respect to depth, as illustrated in Fig. 2(a). The decomposed tangential and radial flows are then used to incorporate additional geometric constraints (detailed in Sect. V-C) into the joint learning framework, enabling independent refinement of depth and each translational component. The above flow decomposition processes are performed by removing the rotational component from the full transformation , followed by the simultaneous and independent removal of the radial and tangential components. The two ideal decomposition processes, as illustrated in Fig. 3(a), can be formulated by rearranging (6) as follows: Rad = (cid:0)T Rot(cid:1)1 (cid:0)T Tan(cid:1)1 Rad(cid:17)1. Tan = (cid:0)T Rot(cid:1)1 (cid:16) (21) However, in practice, errors in the ego-motion estimated by PoseNet are inevitable, which can introduce deviations Rad and Tan into the above two decomposition processes, respectively, resulting in the following expression: RadT Rad = (cid:0) ˆT Rot(cid:1)1 (cid:0) ˆT Tan(cid:1)1 TanT Tan = (cid:0) ˆT Rot(cid:1)1 (cid:16) ˆT Rad(cid:17)1. Such deviations, shown in Fig. 3(b), can be derived from (22) and expressed as follows: (cid:32) (22) (cid:33) R1 0 (I R1)tTan R1tTan 1 (cid:32) R1 0 (I R1)tRad R1tRad 1 (cid:33). (23) Rad = Tan = where tTan = ˆtTantTan and tRad = ˆtRadtRad represent the translational deviations in ˆT Tan and ˆT Rad, respectively, while = ˆRR1 denotes the rotational deviation. As shown in (23), Rad and Tan introduce not only the rotational deviation but also the other type of translational component into Rad and Tan, respectively. Consequently, as illustrated in Fig. 2(b), the resulting radial and tangential translational flows deviate from their expected ones, thereby impairing the effectiveness of incorporating additional geometric constraints derived from optical flow decomposition. To address the aforementioned practical challenges, we leverage two geometric constraints (detailed in Sect. V-C) to jointly minimize the deviations between the decomposed translational flows and the expected ones. These two loss functions collaboratively guide the estimation of ˆT Rot, while simultaneously providing independent supervisory signals to ˆT Tan and ˆT Rad. In widely used driving datasets, such as KITTI [49] and nuScenes [50], vehicles predominantly move forward, with only gradual rotations occurring during turns. As result, these datasets present limited challenges for accurate rotation estimation. Nevertheless, since Rad typically dominates over Tan, even minor rotational deviation can induce noticeable undesired deviation (I R)tRad in tTan. To further demonstrate the robustness of the proposed method under more challenging conditions involving substantial residual rotations R, we create new dataset (detailed in Sect. VI-B) containing sequences with pronounced rotational motion and frequent camera shake. V. DIMODE As shown in Fig. 3(a), removing the rotational and tangential components from the full transformation is equivalent to aligning the optical axes of the source and target cameras, while removing the rotational and radial components is equivalent to aligning their imaging planes. To realize the proposed discriminative treatment of motion components in practice, we incorporate such alignment processes into the joint learning framework, thereby introducing explicit geometric constraints that refine the outputs of both networks. A. Framework Overview An overview of the proposed DiMoDE framework is presented in Fig. 4. The ego-motion transformation ˆT , estimated by PoseNet, is decomposed into three components: ˆT Rot, ˆT Tan and ˆT Rad. First, ˆT Rot is applied to the source camera to align its orientation with that of the target camera. Subsequently, ˆT Tan and ˆT Rad are used to align the optical axes and imaging planes, respectively. In accordance with these two alignment processes, the original optical flows provided by FlowNet are transformed to those from the target image to the aligned source camera image. To refine each component of the PoseNet output, the first set of geometric constraints is imposed by minimizing the deviations between the transformed optical flows and the expected pure radial and tangential ones. Once the PoseNet predictions achieve sufficient accuracy, ensuring proper alignment of optical axes and imaging planes, 7 Fig. 4. An overview of the DiMoDE framework. Centered on the core idea of discriminative motion component treatment, depth (from DepthNet) and egomotion (from PoseNet) are utilized to perform optical axis and imaging plane alignments. FlowNet generates dense correspondences, which are transformed during the alignment processes. The transformed flows are ultimately leveraged to incorporate two sets of geometric constraints into the unsupervised joint learning framework, thereby simultaneously improving both depth and pose estimation performance. each translation component is incorporated individually to further refine the DepthNet outputs. In such cases, the depth and each translation component can be mutually determined through the decomposed flows, thereby forming constraint cycles that provide robust supervisory signals for DepthNet training under adverse conditions. B. Optical Axis and Imaging Plane Alignments To integrate the alignment processes and their corresponding geometric constraints into the joint learning framework, straightforward approach, adopted in the previous study [11], is to reconstruct the warped image once after removing each motion component and then establish dense correspondences from the warped image pair. However, repeated image warping incurs substantial computational overhead and often introduces holes and artifacts in the reconstructed images [8], which undermine the reliability of correspondence generation and ultimately weaken the effectiveness of the imposed geometric constraints. To address this issue, we adopt correspondence transformation strategy that directly transforms the original optical flow Opt ts into the optical flows resulting from optical axis and imaging plane alignment, denoted as Axi ts, respectively. Specifically, pixel correspondences ps and pt are generated once using the target-to-source optical flow map Opt ts predicted by FlowNet [51]. To backproject ps into 3D camera coordinates, the source-view depth map Ds is warped into the target view based on the optical flow map, producing the following pixel-aligned depth map [23], [40]: ts and Pla Dst = (cid:16) Ds, Opt ts (cid:17) . (24) , the 3D camera coordinates of ps, can be computed using pC the following expression: = Dst(pt)K1ps. pC (25) Transforming pC through the alignment processes and projecting it back into pixel coordinates yields the following expressions: pPla pAxi (cid:16) ˆR (cid:16) ˆR 1 1 ˆtRad(cid:17) pC ˆtTan(cid:17). pC ts and Axi the optical flows Pla ts are derived as (26) Pla ts =pPla pt = Axi ts =pAxi pt = (cid:16) ˆR (cid:16) ˆR (cid:16) ˆR (cid:16) ˆR 3 1 1 1 1 ˆtRad(cid:17) pC ˆtRad(cid:17) pt pC ˆtTan(cid:17) pC ˆtTan(cid:17) pt pC . (27) Finally, follows: C. Geometric Constraints from Motion Components ts and Axi As discussed in Sect. IV, Pla ts are expected to approximate the tangential and radial translational flows. The former should exhibit uniform direction across the image, while the latter should point toward or away from the principal point at each pixel. Based on (12) and (15), these properties enable the formulation of two geometric constraints: (1) all Pla ts vectors form consistent angle with global reference vector e1, such that remains constant across the Pla image, where ek R2 denotes the k-th column vector of the 2 2 identity matrix; (2) each Axi ts vector is expected to be parallel to the direction from its pixel pt to the principal point p0, such that . In practice, these two geometric constraints are imposed by incorporating the following loss functions into the joint learning framework: = ptp0 ptp02 Axi ts2 Axi 1 Pla ts2 ts ts (cid:32) Lpla = Var arccos (cid:33) , 1 Pla ts Pla ts2 (28) 8 and (cid:34) Laxi = arccos (f Axi Axi ts) (pt p0) ts2 pt p02 (cid:35) . (29) where the mean E[] and variance Var() are computed over all valid pixels in static and non-occluded regions, as determined by the masking technique proposed in the study [39]. Continuously imposing these constraints through (28) and (29) enables more accurate imaging plane and optical axis alignments, respectively. The radial translation (used exclusively for the imaging plane alignment) and the tangential translation (used solely for the optical axes alignment) are therefore optimized independently, while the rotation estimation is jointly constrained by both. The effectiveness of the imposed constraints and the complementary role of (28) and (29) are validated through an ablation study detailed in Sect. VI-F. Once the PoseNet predictions reach sufficient accuracy, the alignment processes allow the original joint depth and egomotion learning from monocular video to be reformulated into two geometrically simplified forms: the coaxial form, analogous to learning from axial-motion image pairs, and the coplanar form, analogous to learning from stereo image pairs along the horizontal and vertical directions. In such forms, the depth at each pixel and the ego-motion satisfy the following relations, derived from (12) and (15): ρx := zt tx = fx 1 Pla ts , ρy := zt ty = fy 2 Pla ts , (30) ρz := zt tz = (pt p0) (f Axi ts + pt p0) (pt p0) Axi ts , (31) where ρx, ρy, and ρz are defined as the per-pixel ratios between depth and the corresponding translation components. These ratios are determined by the transformed optical flows, with ρz additionally depending on the pixel coordinates. According to (30) and (31), at each pixel pt, multiplying the respective translation components by their corresponding ratios yields three geometric depth values: ρxˆtx, ρyˆty, and ρzˆtz. These values are expected to be consistent with the depth ˆzt predicted by DepthNet, thereby imposing three per-pixel geometric constraints to supervise DepthNet training. Moreover, the simplified geometric relationships enable closedform inverse computation: given the estimated depth and three ratio maps, each translation component can be recovered as E[ˆzt/ρx], E[ˆzt/ρy], and E[ˆzt/ρz]. This eliminates the need for iterative optimization. The recovered translation components are also expected to be consistent with those estimated by PoseNet. Based on these constraints, we introduce the following two loss functions to the joint learning framework: Ltan =E +E (cid:21) (cid:21) (cid:20) ρxˆtx ˆzt ˆzt (cid:20) ρyˆty ˆzt ˆzt + + (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) [ˆzt/ρx] ˆtx ˆtx [ˆzt/ρy] ˆty ˆty (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) . and Lrad = (cid:21) (cid:20) ρzˆtz ˆzt ˆzt + (cid:12) (cid:12) (cid:12) (cid:12) [ˆzt/ρz] ˆtz ˆtz (cid:12) (cid:12) (cid:12) (cid:12) , (32) (33) By eliminating the pixel-coordinate dependence of Axi ts via (31), the resulting ρz becomes function of depth along. Consequently, the first term in both (32) and (33) can provide consistent gradients with respect to per-pixel depth errors across the entire map. Furthermore, unlike existing methods that impose only pixel-level constraints into DepthNet training [3], [39], [41], the proposed method establishes two constraint cycles via (32) and (33), within which the second terms in both (32) and (33), along with the fourth term in (32), measure the overall depth errors aggregated over all valid pixels. Consequently, in challenging scenarios, such as night-time or adverse weather conditions, where per-pixel supervisory signals may become unreliable, this constraint cycle can help prevent significant degradation in DepthNet performance. To validate its effectiveness, we conduct extensive comparative experiments and ablation studies on the nuScenes dataset [50] (see Sects. VI-E and VI-F), which contains numerous video sequences captured under the above-mentioned challenging conditions. In addition to the above newly proposed loss functions, we retain the minimum photometric reprojection loss Lpho adopted in the study [2], as well as the local structure refinement loss Lloc employed in the study [3] to establish the baseline training framework. The overall loss function is formulated as: = λ1(Laxi + Lpla) + λ2(Lrad + Ltan) + λ3Lloc + Lpho, (34) where λ1, λ2, and λ3 are weighting coefficients that balance the contributions of the corresponding loss terms. The specific weight settings and the overall training strategy are detailed in Sect. VI-A, and the effect of varying weighting coefficients is further analyzed in Sect. VI-F. VI. EXPERIMENTS Sect. VI-A provides implementation details on the proposed DiMoDE framework and the adopted training settings. Sect. VI-B introduces the six public datasets used for performance comparison, along with newly created real-world dataset designed to evaluate model performance in complex scenarios. Evaluation metrics are detailed in Sect. VI-C. Sect. VI-D and Sect. VI-E present comprehensive comparisons between our method and previous SoTA approaches in monocular visual odometry and depth estimation, respectively. Finally, Sect. VI-F provides detailed ablation studies to evaluate the efficacy of each innovative design based on discriminative motion component treatment, and details hyperparameter selection. A. Implementation Details As DiMoDE is designed as general framework rather than being tied to specific network architecture, we evaluate its compatibility with variety of existing depth and pose estimation networks in our experiments. For DepthNet, we adopt Lite-Mono [4], lightweight CNN-Transformer hybrid network, as well as D-HRNet [36], representative CNN-based network, to evaluate DiMoDEs adaptability to different depth estimation network architectures. Given the higher computational cost of D-HRNet, the majority of our experiments are conducted using Lite-Mono. For PoseNet, we adopt lightweight design based on ResNet-18 [29] encoder, following previous studies [2], [23]. To further validate the effectiveness of DiMoDE, we also employ the PoseNet proposed in the study [26], which incorporates spatial clues into the pose estimation process. However, since the latter PoseNet [26] is significantly more computationally intensive and does not support real-time inference, we primarily adopt the basic ResNet-based PoseNet [2], [23] in our experiments. Finally, RAFT [51] is employed as the FlowNet in DiMoDE to generate dense correspondences. All networks are trained on an NVIDIA RTX 4090 GPU. To fully utilize GPU memory, the batch size is set to 8 for images with resolution of 640192 pixels. For higher resolutions, such as 640384 and 512288, the batch size is reduced to 4 to accommodate memory constraints. Following the study [2], we utilize snippet of three consecutive video frames as training sample. For data augmentation, random color jitter and horizontal flips are applied to the images during model training. To minimize loss functions, we use the AdamW [52] optimizer with an initial learning rate of 104 and weight decay of 102. The encoders of DepthNet and PoseNet are initialized with pretrained weights from the ImageNet database [53], following the studies [2], [4]. FlowNet is pretrained on the synthetic FlyingChairs dataset [54], following the study [51]. In the early training stage, network predictions tend to be less reliable. Directly enforcing optical axis and imaging plane alignments under such conditions may introduce significant deviations and lead to unstable training. To mitigate this problem, we adopt progressive training strategy. Specifically, during the first 5 epochs out of the total 30 (Stage 1), we jointly train the three networks in an unsupervised manner using only Lpho and Lloc, where the weights (λ1, λ2, λ3) in (34) are set to (0, 0, 0.01). In the subsequent 10 epochs (Stage 2), we freeze FlowNet and incorporate the alignment processes to impose the first set of geometric constraints, namely Lpla and Laxi, primarily to refine pose estimation. The weights (λ1, λ2, λ3) are updated to (0.05, 0, 0.01). Finally, during the last 15 epochs (Stage 3), we introduce the second set of geometric constraints, namely Ltan and Lrad, to further improve depth estimation performance, with the weights (λ1, λ2, λ3) set to (0.05, 0.1, 0.01). B. Datasets The performance of our method on visual odometry is evaluated using both the KITTI Odometry dataset [1] and our newly created real-world MIAS-Odom dataset. The KITTI Odometry dataset contains 22 video sequences in driving scenario, with ground-truth trajectories available for Seqs. 0010. Following prior works [2], [23], we use Seqs. 0008 for unsupervised framework training. Seqs. 09-10 serve as the standard test set, while Seqs. 1121 are additionally included in this study to further evaluate the models generalization and convergence performance, which are often overlooked in prior research. Since ground-truth trajectories of Seqs. 1121 are not publicly available, we generate pseudo ground-truth trajectories using ORB-SLAM3 (Stereo) [16] for performance 9 Fig. 5. An illustration of our designed handheld setup equipped with calibrated and synchronized sensors for accurate real-world data collection. evaluation. To further validate the practical effectiveness of our method, we create the MIAS-Odom dataset, collected using handheld setup, as illustrated in Fig. 5. This dataset consists of six (three indoor and three outdoor) video sequences with total of 11,608 images captured under challenging conditions that are rarely covered by public datasets. These conditions include large rotational motions and frequent camera shakes, which typically lead to substantial rotational errors, as well as overexposure, poor illumination, and motion blur, all of which degrade the reliability of per-pixel photometric supervision. For ground-truth trajectory acquisition, we construct system consisting of multiple hardware-synchronized sensors, including binocular cameras, Livox MID-360 solid-state LiDAR, and an onboard IMU. Camera trajectories are obtained using tightly coupled LiDAR-inertial odometry framework [55] that fuses LiDAR features with IMU measurements to achieve accurate and robust pose estimation. Depth estimation performance is evaluated on five public datasets: KITTI [49], DDAD [56], nuScenes [57], Make3D [57], and DIML [58]. For the KITTI dataset, we adopt the Eigen split [5], which comprises 39,180 monocular triplets for training, 4,424 images for validation, and 697 images for testing. The DDAD dataset is split into training set of 12,650 images and test set of 3,950 images. For the nuScenes dataset, we use 79,760 image triplets for training, and evaluate the models performance on 6,019 front-camera images. As the Make3D and DIML datasets do not provide either stereo image pairs or monocular sequences for unsupervised model training, they are used exclusively to quantify the models generalizability. C. Evaluation Metrics Following the previous study [66], we adopt the average translational error et (%), average rotational error er (/100m), and the absolute trajectory error (ATE, in meters) to quantify pose estimation performance. Additionally, adhering to the study [2], we quantify depth estimation performance using the mean absolute relative error (Abs Rel), mean squared relative error (Sq Rel), root mean squared error (RMSE), root mean squared log error (RMSE log), and the accuracy under specific thresholds (δi < 1.25i, where = 1, 2, 3). 10 TABLE QUANTITATIVE VISUAL ODOMETRY RESULTS ON THE TEST SETS OF THE KITTI ODOMETRY DATASET [1]. THE BEST RESULTS WITHIN EACH METHOD CATEGORY ARE SHOWN IN BOLD TYPE. AND INDICATE TRAINING WITH STEREO IMAGE PAIRS AND MONOCULAR VIDEO SEQUENCES, RESPECTIVELY. RESULTS FOR [15], [16] ARE REPORTED IN MONOCULAR SETTINGS WITH LOOP CLOSURE ENABLED."
        },
        {
            "title": "Year Data",
            "content": "Seq. 09 Seq. 10 et er"
        },
        {
            "title": "ATE",
            "content": "et er"
        },
        {
            "title": "ATE",
            "content": "DSO [17] 2017 ORB-SLAM2 [15] 2017 ORB-SLAM3 [16] DVSO [59] D3VO [27] 2018 2020 TrianFlow [60]"
        },
        {
            "title": "2020 M",
            "content": "pRGBD [28] DF-VO [12]"
        },
        {
            "title": "2021 M",
            "content": "SC-Depth [23]"
        },
        {
            "title": "2021 M",
            "content": "Sun et al. [18]"
        },
        {
            "title": "2022 M",
            "content": "Geometric-Based Methods 2.88 2.35 0. 0.28 52.22 8.39 6."
        },
        {
            "title": "Hybrid Methods",
            "content": "0.83 0.78 6.93 4.20 2.40 5. 0.21 0.44 1.00 0. 1.05 11. 8.36 13.40 45.95 6.06 Wang et al. [61] 2024 1.86 0.29 Zhan et al. [62] UndeepVO [22] 2018 SfMLearner [8] 2017 Monodepth2 [2] 2019 LTMVO [10] 2020 SC-Depth [23] 2021 MLF-VO [25] 2022 MotionHint [63] 2022 SCIPaD [26] 2024 Lite-SVO [64] 2024 Manydepth2 [65] 2025 DiMoDE (Ours) Learning-Based Methods 11.89 7.01 19.15 17.17 3.49 7. 3.90 8.18 7.43 7.20 7.01 2. 3.60 3.60 6.82 3.85 1.00 3. 1.41 1.50 2.46 1.50 1.76 0. 52.12 77.79 76.22 11.30 23. 9.86 17.82 26.15 33.82 9. Fig. 6. Comparisons of trajectories on the test sets of the KITTI Odometry dataset [1]. All predicted trajectories are aligned with the ground truth using 7-DoF similarity transformation. 3.30 2.24 0.74 0. 4.66 4.40 1.82 4.32 1. 12.82 10.63 40.40 11.68 5.81 7. 4.88 8.10 9.82 8.49 7.29 4. 0.30 0.31 0.21 0. 1.60 0.38 2.34 0.31 3. 4.60 17.69 5.31 1.80 4.90 1. 2.74 3.87 2.40 2.65 1.22 11. 6.63 4.97 6. 3.13 7.99 8.11 2.64 24.70 67.34 20.35 11.80 12.00 7.36 11. 15.51 15.45 5.81 Methods Year Data Seqs. 11-20 Seq. 21 et er ATE et er ATE Geometric-Based/Hybrid Methods ORB-SLAM3 [16] 2021 TrianFlow [60] 2020 DF-VO [12] 2021 4.05 18. 11.39 0.38 5.57 3.91 13.46 76. 62.11 Monodepth2 [2] 2019 SC-Depth [23] 2021 SCIPaD [26] 2024 Manydepth2 [65]"
        },
        {
            "title": "2025 M",
            "content": "DiMoDE (Ours) Learning-Based Methods 16.16 15. 7.98 12.91 7.60 5.64 5.43 3. 4.78 2.80 53.26 55.31 35.78 39. 26.64 94.16 17.56 43.67 16.88 44. 16.27 23.79 16.69 2.38 1.99 829. 256.09 13.84 1491.25 2.57 4.50 2. 4.01 2.32 526.75 800.90 510.05 571. 526.86 D. Monocular Visual Odometry Results This subsection presents comprehensive evaluation of monocular visual odometry performance. We compare DiMoDE with wide range of SoTA approaches, including learning-based methods [2], [8], [10], [22], [23], [25], [26], [62][64], geometry-based methods [15][17], and hybrid methods [12], [18], [23], [27], [28], [59][61]. Quantitative results on the KITTI Odometry dataset [1] are presented in Table I. DiMoDE achieves SoTA performance among learning-based methods on the standard test set (Seqs. 09 and 10), exhibiting the lowest trajectory drift (see Fig. 6). However, evaluation on Seqs. 09-10 alone is insufficient to fully demonstrate the models generalizability Fig. 7. Comparison of storage overhead and inference speed among representative learning-based and hybrid visual odometry methods. to unseen environments. To this end, we conduct additional comparative experiments on the remaining 11 sequences (Seqs. 11-21) in the dataset. Across these sequences, DiMoDE consistently achieves the highest average accuracy among all learning-based approaches. The results for Seq. 21 are reported separately due to its extreme difficulty, which causes all existing methods to fail. Furthermore, DiMoDE delivers competitive accuracy compared to geometry-based and hybrid approaches [12], [16], significantly narrowing the performance gap between fully learning-based models and those relying on iterative optimization. Moreover, we compare representative learning-based and hybrid methods in terms of storage overhead and inference speed, two crucial factors for practical deployment that have received relatively limited attention in previous studies. As shown in Fig. 7, although hybrid methods [12], [60] generally outperform learning-based approaches due to the incorporation of back-end optimization, this advantage comes at the expense of significantly reduced inference speed, making them less suitable for real-world applications. Previous learning-based approaches [10], [25], [26] adopt increasingly sophisticated PoseNet architectures for improved accuracy. Nevertheless, such designs also incur substantial computational and memory TABLE II QUANTITATIVE VISUAL ODOMETRY RESULTS ON SEQS. 00, 02, 05, AND 08 OF THE KITTI ODOMETRY DATASET [1]. THE BEST RESULTS WITHIN EACH METHOD CATEGORY ARE SHOWN IN BOLD TYPE. INDICATES TRAINING ON MONOCULAR VIDEO SEQUENCES. RESULTS FOR [15] ARE REPORTED IN MONOCULAR SETTINGS WITH LOOP CLOSURE ENABLED. TABLE III QUANTITATIVE VISUAL ODOMETRY RESULTS ON SEQS. 11-20 OF THE KITTI ODOMETRY DATASET [1]. THE BEST RESULTS ARE SHOWN IN BOLD TYPE. FOR FAIR COMPARISONS, THE DEPTHNET AND FLOWNET IN DF-VO [12], AS WELL AS THE POSENET IN ALL LEARNING-BASED METHODS, ARE FINETUNED FOR THE SAME NUMBER OF ITERATIONS ON THESE SEQUENCES."
        },
        {
            "title": "Year Data",
            "content": "Seq. 00 Seq. 02 et er"
        },
        {
            "title": "ATE",
            "content": "et er"
        },
        {
            "title": "ATE",
            "content": "Geometric-Based/Hybrid Methods ORB-SLAM3 [16] 2021 DF-VO [12]"
        },
        {
            "title": "2021 M",
            "content": "3.13 2.33 0.46 0.63 8.70 14. Learning-Based Methods Monodepth2 [2]"
        },
        {
            "title": "2019 M",
            "content": "SC-Depth [23]"
        },
        {
            "title": "2021 M",
            "content": "SCIPaD [26]"
        },
        {
            "title": "2024 M",
            "content": "Manydepth2 [65]"
        },
        {
            "title": "2025 M",
            "content": "DiMoDE (Ours) 11.40 11.01 6. 10.12 2.28 3.26 3.39 2.28 3. 0.51 107.51 93.04 43.17 72.42 10. 5.10 3.24 6.42 6.74 4.79 14. 2.32 0.55 0.49 1.51 1.96 1. 3.31 0.56 25.11 19.69 83.63 70. 50.72 94.70 17."
        },
        {
            "title": "Year Data",
            "content": "Seq. 05 Seq. 08 et er"
        },
        {
            "title": "ATE",
            "content": "et er"
        },
        {
            "title": "ATE",
            "content": "Geometric-Based/Hybrid Methods ORB-SLAM3 [16] 2021 DF-VO [12] 2021 3.03 1.09 0.43 0.25 6.57 3. 15.02 2.18 Learning-Based Methods Monodepth2 [2] 2019 SC-Depth [23] 2021 SCIPaD [26] 2024 Manydepth2 [65] 2025 DiMoDE (Ours) 7.21 6.70 3.77 6. 1.63 3.11 2.38 1.75 2.52 0. 42.35 40.56 13.51 29.04 6.47 7. 8.11 4.58 8.56 2.10 0.28 0. 2.44 2.61 1.83 3.02 0.65 50. 7.63 56.35 56.15 22.03 61.90 10."
        },
        {
            "title": "Methods",
            "content": "DF-VO [12] SC-Depth [23] SCIPaD [26] Manydepth2 [65] DiMoDE (Ours)"
        },
        {
            "title": "Methods",
            "content": "DF-VO [12] SC-Depth [23] SCIPaD [26] Manydepth2 [65] DiMoDE (Ours) Seq. Seq. 12 Seq. 13 Seq. 14 Seq."
        },
        {
            "title": "ATE",
            "content": "4.45 24.55 12.94 10.04 4."
        },
        {
            "title": "ATE",
            "content": "50.54 14.08 22.50 20.30 12."
        },
        {
            "title": "ATE",
            "content": "17.24 47.30 39.96 73.38 8."
        },
        {
            "title": "ATE",
            "content": "3.04 9.26 1.91 5.56 2."
        },
        {
            "title": "ATE",
            "content": "6.57 41.72 20.46 11.96 9.74 Seq. Seq. 17 Seq. 18 Seq. 19 Seq."
        },
        {
            "title": "ATE",
            "content": "4.88 16.96 14.99 28.51 6."
        },
        {
            "title": "ATE",
            "content": "10.31 7.40 4.97 6.92 1."
        },
        {
            "title": "ATE",
            "content": "13.60 53.67 36.65 15.11 9."
        },
        {
            "title": "ATE",
            "content": "58.89 202.96 150.58 124.18 25."
        },
        {
            "title": "ATE",
            "content": "16.40 14.86 11.92 15.83 8.78 Fig. 9. Comparisons of trajectories produced by models finetuned on the additional sequences of the KITTI Odometry dataset (ft denotes finetuned models). All predicted trajectories are aligned with the ground truth using 7-DoF similarity transformation. Fig. 8. Comparisons of trajectories on several long-term sequences in the train set of the KITTI Odometry dataset [1]. All predicted trajectories are aligned with the ground truth using 7-DoF similarity transformation. costs, making real-time inference infeasible, even on highend GPUs. In contrast, DiMoDE achieves competitive performance with SoTA hybrid methods while relying solely on lightweight PoseNet architecture, thereby satisfying real-time inference requirements. To validate whether the DiMoDE framework provides more effective supervisory signals for pose estimation, we report the results on several long-term sequences (Seqs. 00, 02, 05, and 08) from the training set. As shown in Table II, previous methods, limited by the accuracy of their estimated ego-motion components, tend to accumulate significant errors over these long-term sequences, even when trained on them. In contrast, DiMoDE significantly alleviates this issue, outperforming the previous SoTA method, with ATE reductions of 75.05%, 65.30%, 52.11%, and 54.56%, respectively. Moreover, DiMoDE achieves performance competitive with approaches that rely on iterative optimization. Qualitative comparisons presented in Fig. 8 show that DiMoDE exhibits the least trajectory drift, highlighting its effectiveness in reducing error accumulation over complex and long-term trajectories. Furthermore, key strength of unsupervised learning-based TABLE IV QUANTITATIVE VISUAL ODOMETRY RESULTS ON OUR NEWLY CREATED MIAS-ODOM DATASET. THE BEST RESULTS ARE SHOWN IN BOLD TYPE. FOR FAIR COMPARISONS, THE DEPTHNET AND FLOWNET IN DF-VO [12], AS WELL AS THE POSENET IN ALL LEARNING-BASED METHODS, ARE FINETUNED FOR THE SAME NUMBER OF ITERATIONS ON THESE SEQUENCES."
        },
        {
            "title": "Methods",
            "content": "DF-VO [12] SC-Depth [23] SCIPaD [26] Manydepth2 [65] DiMoDE (Ours)"
        },
        {
            "title": "Methods",
            "content": "DF-VO [12] SC-Depth [23] SCIPaD [26] Manydepth2 [65] DiMoDE (Ours) Indoor Seq. Indoor Seq. 01 Indoor Seq."
        },
        {
            "title": "ATE",
            "content": "3.53 9.02 5.83 7.67 1."
        },
        {
            "title": "ATE",
            "content": "3.61 5.20 5.42 4.51 3."
        },
        {
            "title": "ATE",
            "content": "5.94 10.67 8.82 11.21 5.33 Outdoor Seq. Outdoor Seq. 01 Outdoor Seq."
        },
        {
            "title": "ATE",
            "content": "4.43 6.74 7.32 6.36 2."
        },
        {
            "title": "ATE",
            "content": "4.35 27.63 8.02 32.92 2."
        },
        {
            "title": "ATE",
            "content": "11.07 11.23 9.70 24.31 6.48 approaches lies in their inherent ability to adapt the model to previously unseen environments, which is crucial for realworld applications. To validate the effectiveness of the method in this regard, we conduct additional finetuning experiments on Seqs. 1121 of the KITTI Odometry dataset and on our MIASOdom dataset, respectively, by training each model for five additional epochs. As shown in Table III and Fig. 9, while the other methods [23], [26], [65] continue to yield unsatisfactory results after finetuning on Seqs. 1121 of the KITTI Odometry dataset, DiMoDE enables the estimated trajectories to align closely with the pseudo ground truth. This improvement corroborates our analyses in Sect. IV and demonstrates that decomposing translational components effectively mitigates error propagation and improves convergence, thereby enabling more effective adaptation to previously unseen environments. As shown in Table IV and Fig. 10, DiMoDE demonstrates significantly more robust ego-motion estimation on the MIASOdom dataset, effectively handling variety of real-world low illumination, severe challenges, such as overexposure, motion blur, frequent camera shake, and large-degree turns. In contrast, prior methods consistently fail under these adverse conditions. However, our experimental results suggest that such satisfactory results can only be achieved when the challenging sequences are included in the training set. When portion or all of these sequences are reserved exclusively for testing, all models, including DiMoDE, fail to generalize effectively. This generalization limitation stands in sharp contrast to the results observed on the KITTI Odometry dataset, and is further analyzed in Sect. VII. E. Monocular Depth Estimation Results This subsection presents comprehensive experimental results to evaluate the performance of DiMoDE in monocular depth estimation. We compare our method with representative single-frame approaches [3], [4], [36], [39][41], multiframe approaches [65], [67], [68], and robust depth estimation Fig. 10. Comparisons of trajectories produced by models finetuned on the newly created MIAS-Odom dataset. All predicted trajectories are aligned with the ground truth using 7-DoF similarity transformation. approaches [42], [43]. All the compared methods provide publicly available implementations, allowing us to conduct additional training and evaluation on the DDAD [56] and nuScenes [50] datasets. As shown in Table V, our method achieves competitive performance but does not surpass existing SoTA approaches on the KITTI dataset. This is primarily because our method is designed to impose additional constraints for robust joint learning under adverse conditions and to address the inconsistent optimization issue discussed in (17). However, the KITTI dataset is collected under clear daytime conditions, with few close-range objects near the principal point. As such, it lacks the types of challenges that DiMoDE is specifically designed to handle, thereby limiting the extent to which its advantages can be fully demonstrated. To further validate the effectiveness of the proposed method, we conduct experiments on the DDAD and nuScenes datasets, which contain more challenging scenarios, such as varying illumination, adverse weather, and structurally complex environments. As shown in Tables VI and VII, DiMoDE significantly outperforms previous leading methods [3], [39], [41]. By comparison, several sophisticated networks [4], [36], [40], [65], [68], despite their strong performance on the KITTI dataset, exhibit reduced accuracy or even fail to converge on these more challenging datasets. This performance drop stems from their exclusive reliance on pixel-level supervisory 13 TABLE QUANTITATIVE COMPARISON WITH SOTA METHODS ON THE KITTI [49] DATASET. THE BEST RESULTS FOR SINGLE-FRAME AND MULTI-FRAME APPROACHES ARE SHOWN IN BOLD TYPE, RESPECTIVELY. THE SYMBOLS AND INDICATE THAT HIGHER AND LOWER VALUES CORRESPOND TO BETTER PERFORMANCE, RESPECTIVELY."
        },
        {
            "title": "Test frames",
            "content": "Resolution (pixels) Abs Rel Sq Rel RMSE RMSE log δ < 1.25 δ < 1.252 δ < 1.253 Manydepth [67] DualRefine [68] Manydepth2 [65] D-HRNet [36] Lite-Mono [4] Dynamo-Depth [39] SC-DepthV3 [3] MonoDiffusion [40] DCPI-Depth [41] DiMoDE (Ours) 2021 2023 2025 2022 2023 2024 2024 2025 - 2(-1, 0) 2(-1, 0) 2(-1, 0) 1 1 1 1 1 1 640 192 640 192 640 192 640 640 192 640 192 640 192 640 192 640 192 640 Multi-Frame Methods 0.098 0.105 0.091 0.770 0. 0.649 Single-Frame Methods 0.102 0.101 0.112 0. 0.099 0.097 0.097 0.760 0.729 0. 0.756 0.702 0.666 0.652 4.459 4. 4.232 4.479 4.454 4.505 4.709 4. 4.388 4.337 0.176 0.183 0.170 0. 0.178 0.183 0.188 0.176 0.173 0. 0.900 0.891 0.909 0.897 0.897 0. 0.864 0.899 0.898 0.899 0.965 0. 0.968 0.965 0.965 0.959 0.960 0. 0.966 0.967 0.983 0.983 0.984 0. 0.983 0.984 0.985 0.984 0.985 0. TABLE VI QUANTITATIVE COMPARISON WITH SOTA METHODS ON THE DDAD [56] DATASET. THE BEST RESULTS FOR SINGLE-FRAME AND MULTI-FRAME APPROACHES ARE SHOWN IN BOLD TYPE, RESPECTIVELY. THE SYMBOLS AND INDICATE THAT HIGHER AND LOWER VALUES CORRESPOND TO BETTER PERFORMANCE, RESPECTIVELY. Method Year Test frames Resolution (pixels) Abs Rel Sq Rel RMSE RMSE log δ < 1.25 δ < 1.252 δ < 1.253 Manydepth [67] DualRefine [68] Manydepth2 [65] D-HRNet [36] Lite-Mono [4] Dynamo-Depth [39] SC-DepthV3 [3] MonoDiffusion [40] DCPI-Depth [41] DiMoDE (Ours) 2021 2025 2022 2023 2023 2024 2025 - 2(-1, 0) 2(-1, 0) 2(-1, 0) 1 1 1 1 1 640 384 640 384 640 384 640 384 640 384 640 640 384 640 384 640 384 640 384 Multi-Frame Methods 0. 0.186 0.183 3.455 4.141 4.007 15. 15.741 16.014 Single-Frame Methods 0.198 0.175 0. 0.142 0.163 0.141 0.134 8.124 6. 3.219 3.031 6.362 2.711 2.685 16. 16.687 14.852 15.868 15.922 14.757 14. 0.268 0.271 0.283 0.289 0.272 0. 0.248 0.255 0.236 0.230 0.753 0. 0.761 0.798 0.799 0.798 0.813 0. 0.813 0.831 0.908 0.907 0.903 0. 0.920 0.927 0.922 0.928 0.931 0. 0.961 0.960 0.950 0.956 0.961 0. 0.963 0.966 0.971 0.972 TABLE VII QUANTITATIVE COMPARISON WITH SOTA METHODS ON THE NUSCENES [50] DATASET. THE BEST RESULTS FOR SINGLE-FRAME, MULTI-FRAME, AND ROBUST DEPTH ESTIMATION APPROACHES ARE SHOWN IN BOLD TYPE, RESPECTIVELY. THE SYMBOLS AND INDICATE THAT HIGHER AND LOWER VALUES CORRESPOND TO BETTER PERFORMANCE, RESPECTIVELY. RESULTS FOR [43] ARE REPORTED USING OPTIMAL SCALE AND SHIFT TO ALIGN PREDICTIONS WITH THE GROUND TRUTH, RATHER THAN THE MEDIAN ALIGNMENT USED IN OTHER METHODS, WHICH IS NECESSARY BECAUSE THE MODEL PREDICTS SCALE-SHIFT-INVARIANT DEPTH AND REQUIRES BIAS CORRECTION FOR ALIGNMENT [69]."
        },
        {
            "title": "Test frames",
            "content": "Resolution (pixels) Abs Rel Sq Rel RMSE RMSE log δ < 1.25 δ < 1.252 δ < 1.253 Syn2Real-Depth [42] DepthAnything-AC [43] Manydepth [67] DualRefine [68] Manydepth2 [65] D-HRNet [36] Lite-Mono [4] Dynamo-Depth [39] SC-DepthV3 [3] MonoDiffusion [40] DCPI-Depth [41] DiMoDE (Ours) 2025 2025 2021 2025 2022 2023 2023 2024 2025 - 2(-1, 0) 1 2(-1, 0) 2(-1, 0) 2(-1, 0) 1 1 1 1 1 1 576 320 512 288 512 288 512 512 288 512 288 512 288 512 288 512 288 512 512 288"
        },
        {
            "title": "Robust Depth Estimation Methods",
            "content": "0.152 0.208 1.884 3.230 Multi-Frame Methods 0. 0.255 0.287 3.418 3.204 4.964 Single-Frame Methods 0.396 0.419 0.179 0.167 0.223 0. 0.139 15.087 15.578 2.118 2.003 3. 1.795 1.472 7.442 7.035 9.316 9. 11.132 10.414 9.807 7.050 7.701 8. 7.192 6.254 0.244 0.281 0.351 0. 0.380 0.386 0.449 0.271 0.251 0. 0.255 0.226 0.813 0.754 0.621 0. 0.612 0.715 0.720 0.787 0.784 0. 0.790 0.836 0.930 0.903 0.836 0. 0.812 0.838 0.831 0.896 0.917 0. 0.914 0.939 0.968 0.954 0.924 0. 0.902 0.892 0.879 0.940 0.965 0. 0.959 0.973 signals, which are less robust under challenging conditions and often lead to suboptimal DepthNet predictions. In contrast, DiMoDE establishes constraint cycle via (32) and (33), which incorporates global geometric constraints into 14 Fig. 11. Qualitative results on the DDAD [56] dataset. Fig. 12. Qualitative results on the nuScenes [50] dataset. the training of DepthNet, enabling DiMoDE to effectively overcome the limitations of prior methods. Qualitative results are shown in Figs. 11 and 12, demonstrating that DiMoDE consistently outperforms the baseline model across all aforementioned challenging conditions, which further supports our findings. Furthermore, we compare DiMoDE with two recent SoTA robust depth estimation methods [42] and [43] on the nuScenes dataset. Both approaches rely on specialized domain adaptation strategies, with the method [43] additionally incorporating depth foundation model [34]. Remarkably, DiMoDE consistently delivers superior performance, underscoring its effectiveness and adaptability as general-purpose framework for depth and ego-motion joint learning. Notably, we conduct additional evaluations on the DDAD TABLE VIII ZERO-SHOT DEPTH ESTIMATION RESULTS ON THE DDAD [57] DATASET. ALL MODELS ARE TRAINED ON THE KITTI OR NUSCENES DATASET."
        },
        {
            "title": "Training Data",
            "content": "Abs Rel Sq Rel δ < 1.25 Dynamo-Depth [39] SC-DepthV3 [3] MonoDiffusion [40] Syn2Real-Depth [42] DiMoDE (Lite-Mono)"
        },
        {
            "title": "KITTI",
            "content": "nuScenes"
        },
        {
            "title": "KITTI",
            "content": "nuScenes"
        },
        {
            "title": "KITTI",
            "content": "nuScenes"
        },
        {
            "title": "KITTI",
            "content": "nuScenes"
        },
        {
            "title": "KITTI",
            "content": "nuScenes 0.191 0.153 0.258 0.218 0. 0.634 0.152 0.186 0.142 4. 3.418 5.904 3.868 3.791 15.197 3.289 4.113 3.159 0.696 0.785 0. 0.627 0.738 0.232 0.782 0. 0.797 TABLE IX ZERO-SHOT DEPTH ESTIMATION RESULTS ON THE MAKE3D [57] AND DIML [58] DATASETS. ALL MODELS ARE TRAINED ON THE KITTI DATASET. Dataset Method Abs Rel Sq Rel RMSE RMSE log Make3D DIML Lite-Mono [4] Dynamo-Depth [39] SC-DepthV3 [3] MonoDiffusion [40] DCPI-Depth [41] DiMoDE (Ours) Lite-Mono [4] Dynamo-Depth [39] SC-DepthV3 [3] MonoDiffusion [40] DCPI-Depth [41] DiMoDE (Ours) 0.305 0. 0.373 0.297 0.291 0.289 0.173 0. 0.213 0.166 0.163 0.160 3.060 3. 5.584 2.871 2.944 2.692 0.271 0. 0.407 0.256 0.237 0.237 6.981 7. 8.469 6.877 6.817 6.634 1.108 1. 1.274 1.084 1.038 1.033 0.158 0. 0.177 0.156 0.150 0.151 0.239 0. 0.275 0.232 0.226 0.224 dataset using models trained on either the KITTI or nuScenes dataset to investigate how variations in environmental conthe models generditions within the training data affect alizability. As shown in Table VIII, methods that perform well on the KITTI dataset fail to generalize effectively to the DDAD dataset. In contrast, models converged on the nuScenes dataset demonstrate significantly better generalizability, achieving performance even comparable to those trained directly on the DDAD dataset. We attribute this phenomenon to the greater environmental complexity and diversity in the nuScenes dataset, which compels models to learn more robust and generalizable representations, rather than overfitting to repetitive, structured scenes in the KITTI dataset. This finding highlights the importance of developing generalizable frameworks that support robust learning across diverse environments, central objective of DiMoDE. Finally, we conduct zero-shot evaluations on the Make3D [57] and DIML [58] datasets using models trained on the KITTI dataset to evaluate our models generalizability to broader, more diverse scenarios. As shown in Table IX, our method achieves slightly better quantitative metrics. Nonetheless, the qualitative comparisons presented in Fig. 13 reveal more pronounced improvements. In particular, the first row of Fig. 13(a), as well as the second and third rows in Fig. 13(b), demonstrate that our method effectively avoids overly distant 15 TABLE ABLATION STUDY ON THE EFFECTIVENESS OF THE ADDITIONALLY INCORPORATED GEOMETRIC CONSTRAINTS FOR REFINING POSENET OUTPUTS ON THE KITTI ODOMETRY DATASET ACROSS DIFFERENT POSENET ARCHITECTURES."
        },
        {
            "title": "Configuration",
            "content": "Full Implem. (ResNet-18) Full Implem. (PoseNet from [26]) w/o Laxi and Lpla (ResNet-18) w/o Laxi (ResNet-18) w/o Lpla (ResNet-18) Baseline (ResNet-18) Baseline (PoseNet from [26]) Seq. 09 Seq. et 2.86 5.43 5.84 5.29 4. 7.72 7.33 er 0.74 2.28 1. 1.12 1.23 1.71 2."
        },
        {
            "title": "ATE",
            "content": "9.83 16.29 26.39 23.63 21.96 35. 30.43 et 4.20 7.88 4.89 4. 4.63 8.12 10.55 er 1.22 2. 1.49 1.31 1.59 2.96 3."
        },
        {
            "title": "ATE",
            "content": "5.81 10.20 6.40 6.02 6.24 11. 16.20 TABLE XI ABLATION STUDY ON THE EFFICACY OF THE GEOMETRIC CONSTRAINTS FROM DECOMPOSED FLOWS ACROSS DIFFERENT DEPTHNET ARCHITECTURES, COMPARED WITH THE CROSS-TASK CONSISTENCY LOSS [38] AND THE CONTEXTUAL-GEOMETRIC DEPTH CONSISTENCY LOSS [41], BOTH OF WHICH RELY ON ORIGINAL OPTICAL FLOWS. Configuration Lite-Mono D-HRNet Baseline (Lite-Mono) Baseline (D-HRNet) w/ [38] (Lite-Mono) w/ [41] (Lite-Mono) w/ Ours (Lite-Mono) w/ Ours (D-HRNet) KITTI Raw DDAD nuScenes Abs Rel Sq Rel Abs Rel Sq Rel Abs Rel Sq Rel 0.101 0.102 0.104 0. 0.102 0.099 0.097 0.100 0.729 0. 0.759 0.810 0.736 0.694 0.652 0. 0.162 0.198 0.150 0.155 0.146 0. 0.134 0.138 4.451 8.124 3.255 3. 3.288 3.214 2.685 2.588 0.419 0. 0.160 0.160 0.158 0.152 0.139 0. 15.578 15.087 7.250 7.188 2.561 2. 1.472 1.404 both the full implementation and the baseline configuration. The results demonstrate that our framework is compatible with this alternative network and consistently yields significant performance gains. The second ablation study investigates the effectiveness of the proposed constraints for refining DepthNet outputs, which are derived from decomposed flows following the alignment processes. While prior works have already explored the use of optical flow to constrain depth estimation with all motion types, we not only validate the effectiveness of our method but also compare it with previous approaches. The compared approaches include the cross-task consistency loss introduced in the study [38], which enforces consistency between rigid and optical flows, and the contextual-geometric depth consistency loss proposed in the study [41], where geometric depth is obtained via triangulation. As shown in Table XI, our proposed method significantly outperforms both prior arts across three datasets, supporting our claim that decomposing flows provides more effective geometric constraints. In addition, we replace the default DepthNet with the architecture proposed in [36] to further evaluate the compatibility of DiMoDE. Despite the fundamental differences in architectural design, our method consistently delivers substantial performance gains, demonstrating its robustness and architectural flexibility. Moreover, we analyze the evolution of the validation metric (Abs Rel) on the nuScenes dataset to analyze the impact of our Fig. 13. Qualitative zero-shot monocular depth estimation results on the Make3D [57] and DIML [58] datasets. predictions near the principal point, common bias easily learned from the high-frequency scenes in the KITTI dataset. This finding further underscores the benefit of addressing perspective scaling effects, which helps maintain consistent optimization across the entire depth map. Moreover, additional qualitative examples in Fig. 13 show that our DepthNet produces fine-grained predictions in texture-rich regions and demonstrates greater reliability in texture-less areas, compared to the baseline model. These results collectively emphasize that the introduced geometric constraints provide complementary visual cues on top of the photometric supervision, thereby enhancing the robustness and fidelity of depth estimation. F. Ablation Studies and Hyperparameter Selection The first ablation study, presented in Table X, validates the effectiveness of the proposed constraints for refining PoseNet outputs. When both Laxi and Lpla are removed, substantial degradation in visual odometry performance is observed, demonstrating the efficacy of imposing optical axis and imaging plane alignments. Furthermore, removing either Laxi or Lpla individually results in notable declines in both translation and rotation accuracy, comparable to those observed when both constraints are removed. These findings indicate that the two constraints play complementary roles in constraining rotation estimation, which are consistent with our analyses in Sect. IV. Notably, effective refinement of translational components is only achieved when rotation is well constrained, thereby suppressing irregular rotational flows, as illustrated in Fig. 3. Additionally, the conventional ResNet-based PoseNet is replaced with the architecture adopted in the study [26] for 16 Fig. 14. Quantitative results demonstrating DiMoDEs convergence behavior and robustness to hyperparameter variations: (a) comparison of validation curves for the baseline model, DiMoDE, and its ablation variants; (b) analysis of the impact of varying hyperparameters λ1 and λ2 on the performance of both tasks. method. As shown in Fig. 14(a), the baseline model exhibits noticeable oscillations in its validation curve, indicating that photometric loss alone does not provide sufficiently effective supervision for depth estimation in relatively complex scenes. In contrast, our method achieves significantly smoother convergence, emphasizing the effectiveness of the incorporated geometric constraints. Moreover, we validate the effectiveness of our training strategy (see Sect. VI-A) through two ablation studies: one by removing Ltan and Lrad (i.e., setting λ2 = 0) during epochs 16-30, and another by additionally removing Laxi and Lpla (i.e., setting λ1, λ2 = 0) throughout the entire training process. The results confirm that each component contributes to suppressing oscillations and improving overall performance. Finally, we conduct hyperparameter selection experiment to determine the optimal weights in (34). As shown in Fig. 14(b), while the default setting (λ1 = 0.05, λ2 = 0.1) achieves the best performance across both tasks, adjusting the weights within moderate range (from 0.05 to 0.2) yields comparable results. These results suggest that the performance is relatively robust to these hyperparameters. Nonetheless, performance degrades significantly when the weights are reduced to 0.01 or increased to 1. This degradation is attributed to insufficient gradient strength at lower values, which impedes effective optimization, and to the dominance of the geometric loss at higher values, which disrupts the balance with photometric supervision and leads to unstable training. VII. DISCUSSION This section discusses two primary limitations of DiMoDE and analyzes their underlying causes. First, despite notable improvements over existing methods, DiMoDE still struggles to produce reliable predictions under extremely low illumination, as illustrated by the qualitative results on Indoor Seq. 01 in Fig. 10. This limitation primarily arises from the severe degradation of both photometric cues used in the reconstruction loss and the diminished contextual features for reliable dense correspondences generation. Second, although DiMoDE with enhanced constraints demonstrates more robust learning under challenging conditions, it fails to generalize effectively to other unseen real-world sequences. This limitation can be attributed to the significantly greater environmental and motion complexity present in our collected dataset compared to conventional driving scenarios. Consequently, the current network architecture and training strategies fall short in capturing generalizable patterns in such highly complex and diverse environments. VIII. CONCLUSION AND FUTURE WORKS This article presented DiMoDE, an unsupervised joint depth and ego-motion learning framework that discriminately treats motion components. This study first revisited the rigid flows resulting from various motion types, uncovering their distinct characteristics as well as the long-overlooked limitations caused by indiscriminately mixing these flows when generating supervisory signals. To overcome these limitations, DiMoDE introduced explicit geometric constraints from motion components through the alignments of the optical axes and imaging planes between the source and target cameras. These constraints enable targeted optimization for each egomotion component and enhance depth supervision. Extensive experiments conducted on six public datasets and newly collected real-world dataset demonstrated that DiMoDE consistently achieves robust learning across diverse real-world environments, delivering impressive performance in both visual odometry and depth estimation tasks. Future work will focus on addressing existing limitations, including enabling robust training and reliable predictions under extreme illumination, and enhancing generalizability in complex real-world environments through more interpretable motion representations. In addition, we plan to exploit unsupervised learning across diverse scenes for scalable model training, paving the way toward new paradigm for developing depth foundation models."
        },
        {
            "title": "REFERENCES",
            "content": "[1] A. Geiger et al., Vision meets robotics: the KITTI dataset, The International Journal of Robotics Research, vol. 32, no. 11, pp. 1231 1237, 2013. [2] C. Godard et al., Digging into self-supervised monocular depth estimation, in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019, pp. 38283838. [3] L. Sun et al., SC-DepthV3: Robust self-supervised monocular depth estimation for dynamic scenes, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 46, no. 1, pp. 497508, 2023. [4] N. Zhang et al., Lite-Mono: lightweight CNN and Transformer architecture for self-supervised monocular depth estimation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023, pp. 18 53718 546. [5] D. Eigen et al., Depth map prediction from single image using multi-scale deep network, Advances in Neural Information Processing Systems (NeurIPS), vol. 27, 2014. [6] S. Shao et al., NDDepth: Normal-distance assisted monocular depth estimation and completion, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 46, no. 12, pp. 88838899, 2024. [7] L. Yang et al., Depth anything: Unleashing the power of largescale unlabeled data, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024, pp. 10 371 10 381. [8] T. Zhou et al., Unsupervised learning of depth and ego-motion from video, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 18511858. [9] N. Yang et al., Challenges in monocular visual odometry: Photometric calibration, motion bias, and rolling shutter effect, IEEE Robotics and Automation Letters, vol. 3, no. 4, pp. 28782885, 2018. [10] Y. Zou et al., Learning monocular visual odometry via self-supervised long-term modeling, in Proceedings of the European Conference on Computer Vision (ECCV), vol. 12359. Springer, 2020, pp. 710727. [11] J.-W. Bian et al., Auto-rectify network for unsupervised indoor depth estimation, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 12, pp. 98029813, 2021. [12] H. Zhan et al., Visual odometry revisited: What should be learnt? in International Conference on Robotics and Automation (ICRA), 2020, pp. 42034210. [13] A. Geiger et al., StereoScan: Dense 3d reconstruction in real-time, in IEEE Intelligent Vehicles Symposium (IV), 2011, pp. 963968. [14] R. Mur-Artal et al., ORB-SLAM: versatile and accurate monocular SLAM system, IEEE Transactions on Robotics, vol. 31, no. 5, pp. 11471163, 2015. [15] R. Mur-Artal and J. D. Tardos, ORB-SLAM2: An open-source SLAM system for monocular, stereo, and RGB-D cameras, IEEE Transactions on Robotics, vol. 33, no. 5, pp. 12551262, 2017. [16] C. Campos et al., ORB-SLAM3: An accurate open-source library for visual, visualinertial, and multimap SLAM, IEEE Transactions on Robotics, vol. 37, no. 6, pp. 18741890, 2021. [17] J. Engel et al., Direct sparse odometry, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 40, no. 3, pp. 611625, 2017. [18] L. Sun et al., Improving monocular visual odometry using learned depth, IEEE Transactions on Robotics, vol. 38, no. 5, pp. 31733186, 2022. [19] S. Wang et al., DeepVO: Towards end-to-end visual odometry with deep recurrent convolutional neural networks, in International Conference on Robotics and Automation (ICRA). IEEE, 2017, pp. 20432050. [20] T. Shen et al., Beyond photometric loss for self-supervised egomotion estimation, in 2019 International Conference on Robotics and Automation (ICRA). IEEE, 2019, pp. 63596365. [21] S. Wang et al., End-to-end, sequence-to-sequence probabilistic visual odometry through deep neural networks, The International Journal of Robotics Research, vol. 37, no. 4-5, pp. 513542, 2018. [22] R. Li et al., UnDeepVO: Monocular visual odometry through unsupervised deep learning, in International Conference on Robotics and Automation (ICRA). IEEE, 2018, pp. 72867291. [23] J.-W. Bian et al., Unsupervised scale-consistent depth learning from video, International Journal of Computer Vision, vol. 129, no. 9, pp. 25482564, 2021. [24] S. Zhang et al., Information-theoretic odometry learning, International Journal of Computer Vision, vol. 130, no. 11, pp. 25532570, 2022. [25] Z. Jiang et al., Self-supervised ego-motion estimation based on multilayer fusion of rgb and inferred depth, in 2022 IEEE International Conference on Robotics and Automation (ICRA), 2022. [26] Y. Feng et al., SCIPaD: Incorporating spatial clues into unsupervised pose-depth joint learning, IEEE Transactions on Intelligent Vehicles, 2024, DOI: 10.1109/TIV.2024.3460868. [27] N. Yang et al., D3VO: Deep depth, deep pose and deep uncertainty for monocular visual odometry, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 12811292. [28] L. Tiwari et al., Pseudo RGB-D for self-improving monocular SLAM and depth prediction, in Proceedings of the European Conference on Computer Vision (ECCV). Springer, 2020, pp. 437455. [29] K. He et al., Deep residual learning for image recognition, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 770778. [30] Y. Cao et al., Estimating depth from monocular images as classification using deep fully convolutional residual networks, IEEE Transactions on Circuits and Systems for Video Technology, vol. 28, no. 11, pp. 3174 3182, 2017. [31] H. Fu et al., Deep ordinal regression network for monocular depth estimation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 20022011. 17 [32] S. F. Bhat et al., AdaBins: Depth estimation using adaptive bins, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021, pp. 40094018. [33] S. Shao et al., IEBins: Iterative elastic bins for monocular depth estimation, Advances in Neural Information Processing Systems (NeurIPS), vol. 36, pp. 53 02553 037, 2023. [34] L. Yang et al., Depth anything v2, Advances in Neural Information Processing Systems (NeurIPS), vol. 37, pp. 21 87521 911, 2025. [35] M. Hu et al., Metric3D v2: versatile monocular geometric foundation model for zero-shot metric depth and surface normal estimation, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. [36] M. He et al., RA-Depth: Resolution adaptive self-supervised monocular the European Conference on depth estimation, in Proceedings of Computer Vision (ECCV). Springer, 2022, pp. 565581. [37] Z. Zhou et al., Recurrent multiscale feature modulation for geometry consistent depth learning, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 46, no. 12, pp. 95519566, 2024. [38] Y. Zou et al., DF-Net: Unsupervised joint learning of depth and flow using cross-task consistency, in Proceedings of the European conference on computer vision (ECCV), 2018, pp. 3653. [39] Y. Sun and B. Hariharan, Dynamo-Depth: Fixing unsupervised depth estimation for dynamical scenes, Advances in Neural Information Processing Systems (NeurIPS), 2023. [40] S. Shao et al., MonoDiffusion: Self-Supervised Monocular Depth Estimation Using Diffusion Model, IEEE Transactions on Circuits and Systems for Video Technology, vol. 35, pp. 36643678, 2024. [41] M. Zhang et al., DCPI-Depth: Explicitly infusing dense correspondence prior to unsupervised monocular depth estimation, IEEE Transactions on Image Processing, vol. 34, pp. 42584272, 2025. [42] W. Yan et al., Synthetic-to-real self-supervised robust depth estimation via learning with motion and structure priors, in Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), 2025, pp. 21 88021 890. [43] B. Sun et al., Depth anything at any condition, CoRR, 2025. [44] Z. Wang et al., Image quality assessment: from error visibility to structural similarity, IEEE Transactions on Image Processing, vol. 13, no. 4, pp. 600612, 2004. [45] R. Hartley and A. Zisserman, Multiple view geometry in computer vision. Cambridge university press, 2003. [46] D. Nister et al., An efficient solution to the five-point relative pose problem, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 26, no. 6, pp. 756770, 2004. [47] R. S. Bowen et al., Dimensions of motion: Monocular prediction through flow subspaces, in 2022 International Conference on 3D Vision (3DV). IEEE, 2022, pp. 454464. [48] M. Poggi and F. Tosi, FlowSeek: Optical flow made easier with depth foundation models and motion bases, in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2025, pp. 5667 5679. [49] A. Geiger et al., Are we ready for autonomous driving? the KITTI vision benchmark suite, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2012, pp. 33543361. [50] H. Caesar et al., nuScenes: multimodal dataset for autonomous driving, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 11 62111 631. [51] Z. Teed and J. Deng, RAFT: Recurrent all-pairs field transforms for optical flow, in Proceedings of the European Conference on Computer Vision (ECCV). Springer, 2020, pp. 402419. [52] I. Loshchilov and F. Hutter, Decoupled weight decay regularization, CoRR, 2017. [53] J. Deng et al., ImageNet: large-scale hierarchical image database, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Ieee, 2009, pp. 248255. [54] A. Dosovitskiy et al., FlowNet: Learning optical flow with convolutional networks, in Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2015, pp. 27582766. [55] W. Xu et al., Fast-LIO2: Fast direct lidar-inertial odometry, IEEE Transactions on Robotics, vol. 38, no. 4, pp. 20532073, 2022. [56] V. Guizilini et al., 3D packing for self-supervised monocular depth estimation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 24852494. [57] A. Saxena et al., Make3D: Learning 3D scene structure from single still image, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 31, no. 5, pp. 824840, 2008. [58] J. Cho et al., DIML/CVL RGB-D dataset: 2m RGB-D images of natural indoor and outdoor scenes, arXiv preprint arXiv:2110.11590, 2021. 18 [59] N. Yang et al., Deep virtual stereo odometry: Leveraging deep depth prediction for monocular direct sparse odometry, in Proceedings of the European conference on computer vision (ECCV), 2018, pp. 817833. [60] W. Zhao et al., Towards better generalization: Joint depth-pose learning without posenet, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 9151 9161. [61] C. Wang et al., Self-supervised learning of monocular visual odometry and depth with uncertainty-aware scale consistency, in 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024, pp. 39843990. [62] H. Zhan et al., Unsupervised Learning of Monocular Depth Estimation and Visual Odometry with Deep Feature Reconstruction, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 340349. [63] C. Wang et al., Motionhint: Self-supervised monocular visual odometry with motion constraints, in 2022 International Conference on Robotics and Automation (ICRA). IEEE, 2022, pp. 12651272. [64] W. Wei et al., Lite-SVO: Towards lightweight self-supervised semantic visual odometry exploiting multi-feature sharing architecture, in 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024, pp. 10 30510 311. [65] K. Zhou et al., Manydepth2: Motion-aware self-supervised monocular depth estimation in dynamic scenes, IEEE Robotics and Automation Letters, 2025. [66] J. Sturm et al., benchmark for the evaluation of RGB-D SLAM systems, in 2012 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEE, 2012, pp. 573580. [67] J. Watson et al., The temporal opportunist: Self-supervised multiframe monocular depth, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR), 2021, pp. 11641174. [68] A. Bangunharcana et al., DualRefine: Self-supervised depth and pose estimation through iterative epipolar sampling and refinement toward equilibrium, in Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition (CVPR), 2023, pp. 726738. [69] J. Wang et al., Jasmine: Harnessing diffusion prior for self-supervised depth estimation, CoRR, 2025."
        }
    ],
    "affiliations": [
        "College of Electronic & Information Engineering, Shanghai Institute of Intelligent Science and Technology, Shanghai Research Institute for Intelligent Autonomous Systems, the State Key Laboratory of Autonomous Intelligent Unmanned Systems, the Frontiers Science Center for Intelligent Autonomous Systems (Ministry of Education), and Shanghai Key Laboratory of Intelligent Autonomous Systems, Tongji University",
        "College of Electronic & Information Engineering, Tongji University",
        "College of Electronic & Information Engineering, the School of Computer Science and Technology, and the Key Laboratory of Embedded System and Service Computing (Ministry of Education), Tongji University",
        "Department of Control Science and Engineering, Zhejiang University",
        "National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, Xi'an Jiaotong University",
        "National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, the National Engineering Research Center for Visual Information and Applications, and the Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University",
        "Shanghai Research Institute for Intelligent Autonomous Systems, Tongji University"
    ]
}