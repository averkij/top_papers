{
    "paper_title": "EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models",
    "authors": [
        "Zechen Bai",
        "Chen Gao",
        "Mike Zheng Shou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Achieving truly adaptive embodied intelligence requires agents that learn not just by imitating static demonstrations, but by continuously improving through environmental interaction, which is akin to how humans master skills through practice. Vision-Language-Action (VLA) models have advanced robotic manipulation by leveraging large language models, yet remain fundamentally limited by Supervised Finetuning (SFT): requiring hundreds of demonstrations per task, rigidly memorizing trajectories, and failing to adapt when deployment conditions deviate from training. We introduce EVOLVE-VLA, a test-time training framework enabling VLAs to continuously adapt through environment interaction with minimal or zero task-specific demonstrations. The key technical challenge is replacing oracle reward signals (unavailable at test time) with autonomous feedback. We address this through a learned progress estimator providing dense feedback, and critically, we design our framework to ``tame'' this inherently noisy signal via two mechanisms: (1) an accumulative progress estimation mechanism smoothing noisy point-wise estimates, and (2) a progressive horizon extension strategy enabling gradual policy evolution. EVOLVE-VLA achieves substantial gains: +8.6\\% on long-horizon tasks, +22.0\\% in 1-shot learning, and enables cross-task generalization -- achieving 20.8\\% success on unseen tasks without task-specific demonstrations training (vs. 0\\% for pure SFT). Qualitative analysis reveals emergent capabilities absent in demonstrations, including error recovery and novel strategies. This work represents a critical step toward VLAs that truly learn and adapt, moving beyond static imitation toward continuous self-improvements."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 6 6 6 4 1 . 2 1 5 2 : r EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models Zechen Bai, Chen Gao, Mike Zheng Shou* Show Lab, National University of Singapore https://showlab.github.io/EVOLVE-VLA"
        },
        {
            "title": "Abstract",
            "content": "Achieving truly adaptive embodied intelligence requires agents that learn not just by imitating static demonstrations, but by continuously improving through environmental interaction, which is akin to how humans master skills through practice. Vision-Language-Action (VLA) models have advanced robotic manipulation by leveraging large language models, yet remain fundamentally limited by Supervised Finetuning (SFT): requiring hundreds of demonstrations per task, rigidly memorizing trajectories, and failing to adapt when deployment conditions deviate from training. We introduce EVOLVE-VLA, test-time training framework enabling VLAs to continuously adapt through environment interaction with minimal or zero task-specific demonstrations. The key technical challenge is replacing oracle reward signals (unavailable at test time) with autonomous feedback. We address this through learned progress estimator providing dense feedback, and critically, we design our framework to tame this inherently noisy signal via two mechanisms: (1) an accumulative progress estimation mechanism smoothing noisy point-wise estimates, and (2) progressive horizon extension strategy enabling gradual policy evolution. EVOLVE-VLA achieves substantial gains: +8.6% on long-horizon tasks, +22.0% in 1-shot learning, and enables cross-task generalizationachieving 20.8% success on unseen tasks without task-specific demonstrations training (vs. 0% for pure SFT). Qualitative analysis reveals emergent capabilities absent in demonstrations, including error recovery and novel strategies. This work represents critical step toward VLAs that truly learn and adapt, moving beyond static imitation toward continuous self-improvements. 1. Introduction How do humans develop manipulation skills? We do not simply watch an expert perform task once and then flawlessly replicate it. Instead, we learn through practice: attempting the task repeatedly, making mistakes, receiving feedback from the environment, and gradually refining our movements through continued experience. This process of learning by doing, rather than merely learning by watching, is fundamental to how intelligent agents acquire robust and adaptable capabilities in the real world. Embodied intelligence, the integration of AI technology with robots, has seen remarkable progress in recent years. Propelled by the capabilities of Large Language Models (LLMs), control policies are rapidly evolving beyond traditional methods toward general Vision-Language- *Corresponding Author Figure 1 (a) Traditional supervised finetuning paradigm for training VLA model requires numerous demonstration data and risks rigidly cloning trajectories in the training data. (b) Our test-time training framework requires only few demonstrations (even none) for pre-training and can evolve in the deployment environment. In (c) and (d), we show that our method achieves performance gains across various number of demonstrations and different task suites. Notably, for the first time, we observe zero-shot cross-task generation with the test-time-training framework. (e) illustrates the ability of recovering from mistakes evolved during TTT. Action (VLA) models [3, 11, 12], which process multimodal inputs to produce sequence of actions for completing given task. By leveraging the rich semantic priors from LLMs, VLAs demonstrate impressive contextual understanding compared to their predecessors. However, despite these advances, current VLA training remains fundamentally misaligned with the human learning principle described above: they are trained exclusively through Supervised Fine-Tuning (SFT) on fixed demonstration datasets, learning to imitate expert behavior but lacking mechanism to improve through environmental interaction. This paradigm of static imitation learning entails two fundamental limitations. (1) High labor cost. As shown in Fig. 1(a), adapting VLA models to new tasks requires collecting hundreds of demonstrations for supervised fine-tuning (SFT). This cost multiplies linearly with tasks, making it infeasible to scale VLAs to truly general-purpose robots. (2) Brittle memorization. VLAs optimized through behavior cloning merely imitate demonstrations and struggle to generalize beyond training distribution. They lack the ability to recover from execution deviations, where single misstep often leads to complete task failure. These limitations represent fundamental misalignment with how adaptive intelligence should operate. We believe that enabling continuous learning from deployment experience is essential for achieving truly general-purpose vision-language-action models. In this work, we propose EVOLVE-VLA (Efficient VLA Online Learning Via Experience), test-time training framework that fundamentally shifts how VLAs learn and adapt. As illustrated in Fig. 1(b), instead of requiring hundreds of expert demonstrations, our method needs only minimal supervision, few demonstrations or even none, for lightweight initialization 2 via SFT. The key innovation lies in what happens after this initial pre-training: rather than freezing the policy, we deploy it directly in the target environment where it continues to learn autonomously through active interaction. The VLA explores the environment, receives feedback, and refines its behavior via online reinforcement learning, mirroring the trial-and-error process through which humans develop manipulation skills. This paradigm shift addresses both limitations: (1) it dramatically reduces labor costs by replacing extensive demonstrations with autonomous learning, and (2) it enables genuine adaptation rather than memorization, producing policies that recover from errors and discover novel strategies. For example, Fig. 1(e) shows our model developing error correction capabilities absent from training demonstrations. Beyond improving seen tasks, this approach enables cross-task generalization through self-directed exploration. While prior works like SimpleVLA-RL [13] have explored RL for VLA models, they rely on oracle reward functions (e.g., binary success signals) unavailable at test time. The central challenge of practical TTT is replacing the oracle with autonomous feedback. We introduce learned progress estimator as reward, with the policy optimized via GRPO [21]. Unlike sparse success signals, progress-based rewards provide dense, continuous feedback crucial for sample-efficient learning. However, practical progress estimators are inherently noisy [18, 19, 22, 30], and errors accumulated over long horizons can mislead the policy. Our core technical challenge is therefore not to build perfect estimator, but to successfully tame this noisy reward signal to make learning possible. To achieve this, we introduce two key technical contributions. First, we design an accumulative progress estimation mechanism with interval-based sampling, which aggregates and smooths noisy point-wise estimates into stable, reliable signal. Second, we propose progressive horizon extension strategy that optimizes the policy with progressively increasing exploration horizon, making the model more resilient to estimation errors by allowing it to first master simpler sub-tasks. This combined approach not only mitigates the impact of estimation noise but also allows the VLA to effectively utilize the dense, albeit imperfect, reward. Our framework enables VLA models to perform test-time training using self-generated environmental feedback without oracle rewards. We validate EVOLVE-VLA on the LIBERO benchmark, achieving substantial gains: +8.6% on long-horizon tasks, +22.0% in 1-shot learning, and cross-task transfer (0% 20.8% on unseen tasks through autonomous adaptation). Qualitative analysis reveals emergent capabilities absent from demonstrations, including error recovery and novel strategies. These results validate that test-time training represents paradigm shift toward adaptive embodied agentsa critical step toward truly general-purpose VLA systems. Our contributions include: We propose EVOLVE-VLA, test-time training framework that enables VLAs to continuously adapt through autonomous interaction, addressing the brittleness and scalability limitations of static SFT. We tackle the central challenge of absence of oracle rewards by introducing learned progress estimator. Critically, we develop techniques to tame inherently noisy reward signals, making practical test-time training feasible. We introduce two key innovations: (1) an accumulative progress estimation mechanism that smooths noisy estimates into stable signals, and (2) progressive horizon extension strategy enabling gradual policy evolution, proving effective for long-horizon tasks. We demonstrate strong results: +8.6% on long-horizon tasks, +22.0% in 1-shot learning, and pioneering zero-shot cross-task generalization (0% 20.8%) through test-time adaptation alone. Our analysis reveals emergent skills like error recovery arising from autonomous exploration. 3 2. Related Work Vision-Language-Action Models. Recent advances in Vision-Language-Action (VLA) models [3, 7, 11, 12, 20, 26, 27, 31] aim to equip embodied agents with the ability to perceive, reason, and act upon multimodal inputs. Early works like RT [4] and Octo [23] investigate how to connect the power of large models with the interactive nature of embodied environments, paving the way toward generalist robot manipulation. OpenVLA [12] presents an open-source VLA model fine-tuned across multiple manipulation tasks, aiming to standardize evaluation and promote reproducible research. OpenVLA-OFT [11] further proposes parallel decoding, action chunking, and continuous action representation to improve performance. ùúã0 [3] introduces VLA flow model by continuous flow-based architecture. The approach demonstrates strong generalization across diverse robot manipulation tasks and sets new direction for flow-based embodied reasoning. Some works focus on improving the efficiency of VLA model. TinyVLA [27] designs lightweight VLA for robotic manipulation, which employs parameter sharing and distillation to retain performance under limited data. Recent works [2, 9, 28] also investigate how to involve tactile modality in VLA models. However, previous methods rely heavily on imitation learning with numerous manual-collected data, leading to labor-cost and poor generalization models, especially when meeting the new tasks and environments. RL Fine-Tuning for VLA Models. With the recent advances of RL post-training in LLMs [5, 24] and MLLMs [15, 25], some studies have begun to explore RL post-training for VLA models. For example, iRe-VLA [10] explores how online RL can enhance pretrained VLA models by allowing continual improvement through interaction. VLA-RL [17] introduces trajectorylevel RL formulation for VLA training. OctoNav [8] investigates how GRPO-like RL training can improve VLA reasoning ability in embodied navigation. SimpleVLA-RL [13] and ùúãùëÖùêø [6] explore RL fine-tuning for autoregressive and flow-based VLAs, respectively. RL4VLA [16] systematically studies different RL policies and the impact of RL fine-tuning across diverse visual, semantic, and execution dimensions. Although these works have explored RL posttraining strategies for VLA models, they still assume access to Ground-Truth (GT) information during the RL training phase, such as whether trajectory succeeds or fails. However, at test time, such GT supervision signals are unavailable. To address this, we propose test-time training framework that enables the model to adapt without relying on GT feedback. Concurrent Work: ùúã0.6. Concurrent to our work, Physical Intelligence recently released ùúã0.6 [1], vision-language-action model that learns from autonomous experience using their Recap method (RL with Experience & Corrections via Advantage-conditioned Policies). Our work shares similar motivation and spirit with ùúã0.6 in addressing fundamental limitation of VLA models trained purely on demonstration data: the inability to handle compounding errors and improve from deployment experience. The concurrent emergence of both works from academia and industry highlights growing recognition that experience-based reinforcement learning is essential for VLA models to move beyond behavior cloning. Both approaches demonstrate that achieving reliable and robust performance requires learning from the robots own experience rather than solely imitating expert demonstrations. We submitted EVOLVE-VLAbefore the release of ùúã0.6, representing pioneering academic work in this direction. To foster further research and democratize access to this paradigm, we commit to releasing our full training and inference codebase upon publication. 4 Figure 2 Framework overview. In the test-time training phase, VLA model interacts with the environment and generates diverse rollout trajectories. task progress estimation module assign progress for each rollout, which will be utilized as the reward for GRPO optimization. Task progress estimation employs an accumulative strategy that produces clean, stable and smooth reward. The training strategy undergoes progressive horizon extension schedule, enabling learning curriculum. 3. Method 3.1. Task Definition We formulate the robotic manipulation task as Markov Decision Process (MDP) = (S, A, ùëÉ, R, ùõæ), where is the state space, is the action space, ùëÉ represents transition dynamics, is the reward function, and ùõæ [0, 1) is the discount factor. At timestep ùë°, the state ùë†ùë° = (ùëúvis task) ùë° consists of visual observation ùëúvis , and task instruction ùëô , proprioceptive state ùëúprop , ùëúprop ùë° task. , ùëô ùë° ùë° VLA policy ùúãùúÉ : Œî(A) maps states to action distributions. Following modern VLA architectures [3, 12], we adopt action tokenization where continuous robot actions ùëéùë° Rùëë are discretized into tokens. The policy autoregressively generates action token sequences = (ùëé1, . . . , ùëéùëá ) with probability ùúãùúÉ(a ùë†ùë°) = (cid:206)ùëá ùúãùúÉ(ùëéùëò ùë†ùë°, ùëé<ùëò). trajectory ùúè = {(ùë†0, ùëé0), . . . , (ùë†ùêª, ùëéùêª)} is generated through closed-loop interaction: the policy outputs actions, the environment transitions based on physical dynamics, and updated observations feed back into the policy until task completion or maximum horizon ùêª. ùëò= 3.2. Test-time Training Framework During deployment, VLA model pretrained via SFT on expert demonstrations encounters novel scenarios that differ from its training distribution. Traditional SFT models, which learn purely through imitation, lack the mechanism to adapt to these out-of-distribution states. Our goal is to enable the VLA to continue learning at test-time by leveraging online interaction with the environment. Test-time training (TTT) requires two key components: (1) the ability to actively interact with the environment to generate diverse rollouts, and (2) feedback signal to evaluate and improve these rollouts. We achieve this through online reinforcement learning, where the policy is iteratively refined based on rewards obtained from environment interaction. Fig. 2 shows the overview of our TTT framework. 5 3.2.1. Online Reinforcement Learning Interactive Rollout Generation. For given task, we generate multiple diverse trajectories by sampling from the policys action token distribution with temperature ùëá > 1. Specifically, starting from initial state ùë†0, at each timestep ùë°, the policy outputs action token probabilities and samples an action ùëéùë° from the distribution. This action is executed in the environment, producing new state ùë†ùë°+1. This closed-loop interaction continues until the estimated task progress exceeds threshold (indicating completion) or the maximum horizon ùêªmax is reached, yielding trajectory ùúèùëñ = {(ùë†0, ùëé0), . . . , (ùë†ùêª, ùëéùêª)}. By sampling ùê∫ trajectories {ùúèùëñ}ùê∫ ùëñ=1 with different random seeds, we explore diverse solution strategies. Environment Feedback. Each trajectory receives reward ùëÖùëñ that evaluates its quality. This reward signal, which we detail in 3.2.2, serves as the supervisory feedback guiding policy improvement. Unlike SFT which only learns from successful demonstrations, the reward signal provides differential feedback, distinguishing better trajectories from worse ones and enabling the model to discover and reinforce effective behaviors through trial and error. Policy Update. We employ Group Relative Policy Optimization (GRPO) [21] to update the policy. GRPO normalizes trajectory rewards within each batch to compute advantages and applies PPO-style clipping for stable updates, without requiring separate value network. 3.2.2. Task Progress Estimation critical challenge for test-time training is the absence of oracle reward signals (e.g., groundtruth success indicators from simulators) that are available during training in simulator but unavailable at deployment. We address this by learning reward function based on task progress: an estimate of how much of the task has been completed. Task Progress as Reward Function. Progress-based rewards offer several advantages over binary success signals. First, they are dense: progress can be estimated at any point during execution, providing continuous feedback even for failed attempts. This density is crucial for sample-efficient learning, especially in long-horizon tasks where successful rollouts may be rare initially. Second, progress is more general, grounded concept than task-specific metrics or black-box reward scores, making it applicable across diverse manipulation tasks. Task Progress as Termination Condition. Beyond providing rewards, task progress estimation also determines when to terminate rollouts. When estimated progress exceeds predefined threshold, the rollout stops as the task is deemed complete; otherwise, execution continues until maximum horizon ùêªmax. This dual-purpose usage imposes stringent requirements on the estimator: it must be (1) computationally efficient, as it is queried frequently (every Œî check steps) to detect completion in real-time, and (2) temporally smooth and consistent, as erratic estimates can cause premature termination (stopping promising trajectories early) or delayed termination (wasting computation on completed tasks). While noisy rewards can be mitigated through averaging during policy learning, single erroneous termination decision can truncate an entire trajectory. Therefore, stabilizing the progress signal is essential not just for learning efficiency, but for correct rollout execution. Vanilla Progress Estimation. We employ foundation critic model, VLAC [29], which takes two images and task instruction as input and output critic value. positive value indicates how much the second image progresses the task compared to the first image. negative value vice versa. Specifically, given trajectory ùúè = {(ùë†0, ùëé0), . . . , (ùë†ùêª, ùëéùêª)}, we compute the reward as task), where ùëú0 and ùëúùêª are the initial and final observations of the trajectory, ùëÖùëñ = Critic(ùëú0, ùëúùêª, ùëô and ùëô task is the task instruction. The estimated reward is then normalized to [0, 1] to serve as the trajectory reward ùëÖùëñ for GRPO. 6 3.3. Accumulative Progress Estimation While the progress critic provides dense feedback, we observe that it can be noisy and inconsistent, especially for long-horizon tasks involving multiple sub-goals. single frame-pair comparison may be misled by superficial visual changes or fail to capture intermediate progress. As discussed in 3.2.2, this noisy estimation can negatively affect both reward feedback and rollout termination. To address these challenges, we introduce an accumulative progress estimation mechanism. Our key insight is inspired by slow-fast philosophy: instead of comparing the final state to the very beginning (which becomes unreliable for long trajectories), we maintain milestone frames at regular intervals and compute progress incrementally. Interval-Based Milestone Sampling. We define sampling interval Œî timesteps). During rollout, we maintain list of milestone frames Fmilestone = { ùëì0, ùëìŒî that captures the trajectorys evolution at coarse granularity. These milestones serve as reference points for measuring progress. milestone (e.g., 64 milestone, ùëì2Œî milestone, . . .} Incremental Progress Computation. At finer granularity (every Œî check steps, where milestone), we query the critic to estimate progress relative to the most recent milestone. check < Œî Œî Specifically, at timestep ùë°, we compute: ùëêùë° = Critic( ùëìùë°/Œî milestone Œî milestone, ùëúùë°), (1) where ùëêùë° [100, 100] represents the incremental progress from the last milestone to the current state. When ùë° reaches new milestone (ùë° mod Œî milestone = 0), we append ùëúùë° to Fmilestone and store ùëêùë° in the critic history. Accumulative Value Aggregation. Given sequence of incremental critic values milestone, ùëê2Œî {ùëêŒî value ùë£ùë° [0, 100] that estimates task completion percentage: } collected at milestones, we accumulate them into progress milestone, . . . , ùëêùëòŒî milestone ùë£ùëñ = ùë£ùëñ1 + (100 ùë£ùëñ1) ùëêùëñ/100, ùë£0 = 0, (2) where ùëñ indexes the milestones. This recursive formulation applies diminishing returns principle: positive progress advances the value toward 100 by fraction of the remaining distance, while negative critics decrease the value proportionally. Critically, adjustments scale with (100 ùë£ùëñ1) (the remaining gap to completion) prevents both overshooting from overly optimistic critics and catastrophic collapse from pessimistic ones. The full mechanism is shown in Algorithm 1. It effectively smooths the noisy critic: by comparing to recent milestones rather than the distant initial state, we reduce the impact of long-term drift; by applying proportional adjustments rather than raw critic values, we create more stable learning signal; and by accumulating progress incrementally with diminishing returns, we smooth out local fluctuations. Such smoothed reward provides more reliable feedback for the reinforcement optimization. In addition, this mechanism is also computationally efficient. Recall that since the progress need to called frequently for determining rollout termination, at timestep ùëá, naive multi-frame approach would require ùëá 1 critic calls to evaluate all pairwise comparisons, whereas our method requires only single callcomparing the current frame to the nearest milestone. 3.4. Progressive Horizon Extension Long-horizon tasks present fundamental challenge for test-time training: early in training, the policy is far from proficient and successful task completion is rare, making credit assignment 7 Algorithm 1: Accumulative Progress Estimation threshold check, ùúè milestone, Œî Input: Critic model, Œî Output: Trajectory reward ùëÖaccum (ùúè) Initialize milestone frames Fmilestone [ùëú0]; Initialize critic history []; Initialize progress values [0]; Initialize ùë£current 0; for ùë° = 1 to ùêªmax do Execute action ùëéùë°, observe ùëúùë°; if ùë° mod Œî check = 0 then ùëò ùë°/Œî milestone ; ref Fmilestone [ùëò] ; ùëì ref, ùëúùë°) ; ùëêùë° Critic( ùëì if ùë° mod Œî milestone = 0 then // Nearest milestone // milestone frame // Compute incremental progress Append ùëúùë° to Fmilestone ; Append ùëêùë° to ; // Accumulate progress value with diminishing returns ùë£current ùë£current + (100 ùë£current) ùëêùë°/100; Append ùë£current to V; // Termination check if ùë£current/100 > ùúèthreshold then // New milestone // Store critic value // Task deemed complete break ; // Use accumulated progress as reward ùëÖaccum (ùúè) ùë£current/100; return ùëÖaccum(ùúè) difficult with noisy reward signals. Simply allowing free exploration until the maximum horizon ùêªmax leads to low-quality trajectories that provide weak learning signals. Even with our accumulative progress estimation, optimizing over very long horizons from the start can lead to unstable learning dynamics. To address this, we adopt progressive horizon extension strategy. We divide the training process into stages, where each stage operates with maximum rollout horizon ùêªmax. As training progresses through stages, we gradually increase ùêªmax, allowing the policy to first master shorter sub-goals before tackling the complete task. In early stages, the agent focuses on immediate objectives and fundamental manipulation behaviors where the reward signal is cleaner and more direct. As the horizon extends in later stages, the policy learns to chain these behaviors together and reason over longer temporal dependencies, ultimately optimizing complete task execution. This schedule provides several benefits. First, shorter horizons naturally reduce the accumulation of noise in progress estimation, as fewer milestone comparisons are needed. Second, early success on simpler sub-goals provides positive learning signals that would be absent when optimizing full-length trajectories from scratch. Third, the staged progression allows the policy to build compositional skills, where early stages establish robust primitives, while later stages learn to orchestrate them. Importantly, progressive learning and accumulative progress estimation are complementary mechanisms. The progressive curriculum addresses temporal credit assignment, i.e., determining when and what to learn, by controlling the optimization scope. Accumulative estimation addresses noisy rewards, i.e., stabilizing the feedback signal, by aggregating incremental progress. Together, they enable robust test-time training on long-horizon manipulation tasks where both challenges are present. 8 Table 1 Main results of different VLA models on LIBERO. Model Octo OpenVLA Nora ùúã0 + FAST ùúã0 UniVLA VLA-RL SimpleVLA OpenVLA-OFT EVOLVE-VLA Œî LIBERO Spatial Object Goal Long Avg 78.9 84.7 92.2 96.4 96.8 96.5 90.2 94.3 91.3 95.4 +4. 85.7 88.4 95.4 96.8 98.8 96.8 91.8 90.5 90.1 97.4 +7.3 84.6 79.2 89.4 88.6 95.8 95.6 82.2 92.3 89.8 95.8 +6.0 51.1 53.7 74.6 60.2 85.2 92.0 59.8 87.7 85.8 94.4 +8. 75.1 76.5 87.9 85.5 94.2 95.2 81.0 91.2 89.2 95.8 +6.5 4. Experiments Benchmark. We evaluate our method on the LIBERO benchmark [14], widely used simulation benchmark for lifelong learning in robotic manipulation. LIBERO focuses on language-guided manipulation tasks across diverse object types, task specifications, and environments. It consists of four task suites: LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, and LIBERO-Long. Each suite contains 10 tasks, with 50 expert demonstrations per task. We report the average Success Rate (SR) across 50 trials for each task, following the evaluation protocol in previous work [12, 13]. Base Model. We apply our method to OpenVLA-OFT [11], state-of-the-art autoregressive VLA model that achieves high performance and inference efficiency. Following prior work [13], we adopt the action chunking and parallel decoding designs, while disabling the continuous action regression head, i.e., use discrete action tokens instead. This would enable action generation compatible with the optimization of reinforcement learning. Reward Model. For test-time training, we employ foundation critic model VLAC [29] as the progress estimator. VLAC takes two images and language instruction as input and outputs critic value indicating how much the second image represents progress toward task completion compared to the first image. This foundation model has been pre-trained on largescale robotic manipulation datasets, demonstrating its ability to estimate task progress across diverse tasks and environments. 4.1. Main Results Tab. 1 presents our main results on the LIBERO benchmark, comparing our TTT framework against state-of-the-art VLA models. We apply TTT to the OpenVLA-OFT model (pre-trained with full trajectory demonstrations), enabling it to continue learning during deployment. Significant Performance Gains. Our TTT framework achieves substantial improvements across all four LIBERO task suites. On average, we observe +6.5% absolute gain in success rate, elevating the baseline from 89.2% to 95.8%. The improvements are consistent across diverse task types: +4.1% on LIBERO-Spatial, +7.3% on LIBERO-Object, +6.0% on LIBERO-Goal, and most notably, +8.6% on LIBERO-Long. The substantial gain on LIBERO-Long is particularly significant, as this suite contains the most challenging long-horizon tasks with complex multistep procedures. With TTT, our method achieves 95.8% average success rate, surpassing models like ùúã0 (94.2%) and matching UniVLA (95.2%), demonstrating that test-time adaptation can be 9 Table 2 Test-time training in low data regime, i.e., one demonstration for pre-training. Model LIBERO Spatial Object Goal Long Avg OpenVLA-OFT EVOLVE-VLA Œî 65.1 73.4 +8.3 40.1 70.0 +29. 57.2 64.7 +7.5 43.6 15.1 37.1 61.3 +22.0 +17.7 as effective as collecting and training on large amounts of additional demonstration data. Challenge of Naive Reward Modeling. We also compare with SimpleVLA, which initially employs the binary outcome reward from the simulator, i.e., oracle reward. We then replace the oracle reward with our progress estimator, and use simple threshold-based approach to convert progress estimates into binary outcome rewards. This version of SimpleVLA achieves only 87.7% on LIBERO-Long, modest +1.9% improvement over the SFT-only baseline (85.8%). The limited gain highlights critical challenge: directly using noisy progress estimator to generate binary rewards for online RL is insufficient. In contrast, our accumulative progress estimation mechanism that smooths noisy signals and provides dense, stable feedback achieves 94.4% (+8.6%), demonstrating the importance of properly taming the reward model. 4.2. TTT Under Low Data Regimes key motivation for TTT is to reduce the labor cost of collecting extensive demonstration data. To evaluate TTTs effectiveness in low-data scenarios, we experiment with more challenging setting: only one demonstration per task for SFT pre-training1, followed by test-time training. As shown in Tab. 2, the 1-shot SFT baseline (OpenVLA-OFT) achieves only 43.6% average success rate, indicating that single demonstration is insufficient for learning robust manipulation policies. However, applying our TTT framework yields substantial improvements, achieving 61.3% average success rate, which is remarkable +17.7% absolute gain. The improvements are consistent across all task suites: +8.3% on LIBERO-Spatial, +29.9% on LIBERO-Object, +7.5% on LIBERO-Goal, and +22.0% on LIBERO-Long. These gains validate our core claim: test-time training can effectively alleviate the data collection burden by enabling learning from self-generated experiences rather than relying solely on extensive expert demonstrations. 4.3. Toward Zero-Shot Cross-Task Generalization An intriguing capability enabled by our TTT framework is cross-task generalization through online learning. To explore this, we conduct preliminary experiment: we take VLA model pre-trained exclusively on LIBERO-Long tasks (50 demonstrations per task) and directly deploy it on LIBERO-Object tasks without fine-tuning on task-specific demonstrations. When deployed directly, the LIBERO-Long pre-trained policy achieves 0% success rate on LIBERO-Object, as expected. Although, conceptually, the two task suites may share some common motion primitives, the behavior cloning paradigm strongly hinders generalization. Remarkably, however, by applying our TTT framework with progress-based feedback, the policy adapts purely through autonomous exploration and reaches 20.8% success rate on LIBEROObject. While this performance remains modest compared to task-specific SFT baselines (which achieve 40.1% with single demonstration and 96.6% with 50 demonstrations), the ability to break 0 success rate without finetuning on task-specific human demonstrations represents 1All 1-trajectory SFT models are reused from the SimpleVLA-RL released checkpoints [13]. Table 3 Ablation study on accumulative progress estimation and and temporal sampling efficiency on LIBERO-Long task suite. Method Sampling Reward Calls F-Score SR (%) SFT Baseline (2 frames) - - Accumulative (4 frames) Uniform Accumulative (8 frames) Uniform Interval Accumulative (Ours) - 32 96 224 32 - 0.04 0.09 0.17 0.20 85.8 88.3 90.1 89.3 91. qualitatively different capability. To the best of our knowledge, no prior VLA training method has demonstrated such cross-task transfer through test-time adaptation alone. This preliminary result suggests that TTT, when paired with foundation-level progress estimator like VLAC, can enable VLAs to generalize across task distributions through self-directed learning. 4.4. Ablation Studies Accumulative Progress Estimation. Tab. 3 validates the effectiveness and efficiency of our accumulative progress estimation mechanism with various frame sampling strategies. F-Score is computed based on balanced validation set (100 success cases, 100 failure cases) assessing task progress estimation performance. The baseline that directly uses 2-frame critic values without accumulation achieves 88.3% success rate with 32 reward calls but suffers from low F-score (0.04), indicating unreliable progress estimation. When incorporating accumulative progress estimation, the sampling strategy for millstone frames matters. The uniform sampling variants improve F-score over baseline but at the cost of significantly more reward calls (96 and 224 respectively), with diminishing or even negative returns in success rate, suggesting that naive dense sampling introduces noise without proper temporal structure. In contrast, our method achieves the best performance (91.3% SR, 0.20 F-score) while maintaining computational efficiency with only 32 reward calls, demonstrating that interval-based sampling (with sliding Œîùëê‚Ñéùëíùëêùëò) combined with accumulative aggregation is both more effective and more efficient than naive uniform approaches. Progressive Horizon Extension. Tab. 4 demonstrates the importance of progressive horizon extension for long-horizon tasks. Starting from the SFT baseline (85.8%), we examine three TTT variants. First, using binary outcome rewards (thresholding the progress estimator) yields only 87.7% (+1.9%), confirming that converting dense progress into sparse signals loses valuable learning information. Second, applying dense rewards from our accumulative progress estimator without progressive horizon achieves 91.3% (+5.5%), showing the benefit of dense feedback. Finally, adding progressive horizon extension, i.e., gradually increasing the maximum rollout length during training, reaches 94.4% (+8.6%), providing an additional 3.1% gain. This validates our strategy: by initially constraining exploration to shorter horizons and progressively extending them, the policy learns more stable sub-task skills before tackling full-length trajectories, making it more resilient to estimation errors in long-horizon tasks. 4.5. Qualitative Analysis To gain deeper insights into how test-time training shapes policy behavior, we analyze representative rollout trajectories after TTT in Fig. 3. First, the policy develops error recovery capabilities: when initial grasp attempts fail, the SFT-only policy usually continue with the pre-programmed motion and fails, whereas after TTT the policy autonomously re-attempts grasping (top row). 11 Table 4 Ablation study on Progressive Horizon Extension with LIBERO-Long task suite. Method Baseline (SFT only) + Binary Outcome + Dense Reward (Vanilla Critic) + Progressive Horizon Milestones SR (%) - - - 85.8 87.7 91.3 94.4 Second, the policy adapts to pick an object but accidentally changed the object state, then it adjusts its motion to fit the new config rather than rigidly following memorized patterns (middle row). Third, the policy discovers alternative manipulation strategies not present in demonstrations. For instance, grasping pot by its body instead of the handle (bottom row). These improvements indicate that progress-based feedback enables the policy to generalize beyond trajectory-level imitation to goal-oriented manipulation and explore diverse solutions. Figure 3 Qualitative example of policy behavior with TTT. Despite the improvements, we observe failure cases that reveal fundamental challenge: misalignment between the environments rule-based success criterion and the semantic task completion assessed by our progress estimator. This mismatch manifests in two ways as shown in Fig. 4. First, in some cases the policy brings the scene very close to the goal state, leading the progress estimator to assign high rewards (near-completion signal), yet the environments coordinate-based rules still judge the task as unsuccessful. This creates form of reward hacking where the policy optimizes for high progress scores without meeting the strict environmental criteria. Second, the opposite occurs: the environment judges tasks as successful based on coordinate rules despite semantic incompleteness. For instance, in Fig. 4, book placement task where the environment reports success because the books coordinates satisfy the spatial constraints, yet semantically the book is not properly placed inside the shelf. These misalignments highlight the inherent difficulty in aligning rule-based simulation criteria with semantic task understanding, suggesting that future work should explore improved calibration between progress estimators and environment oracles. Figure 4 Example of environment success criterion mismatch. 5. Future Work Our work demonstrates that test-time training for VLA models is feasible by addressing noisy progress estimation through accumulative estimation and progressive horizon extension. This foundational step enables VLAs to learn from experience rather than merely imitating demonstrations, unlocking several promising directions for future research. First, developing more robust reward models would significantly enhance the frameworks capabilities. While our accumulative mechanism effectively handles noisy estimates, future reward models with better semantic alignment to environment success criteria could reduce the mismatch between progress estimation and rule-based success signals. Additionally, improving zero-shot capability would eliminate the need for in-context examples, enabling truly zero-shot cross-task generalization. While our current approach shows promising cross-task transfer (LIBERO-Long LIBEROObject), the reward model still benefits from task-specific context; future reward models trained on more diverse manipulation data, with better generalization capability could enable seamless adaptation to entirely novel tasks without any task-specific examples, even for the reward model. Second, extending test-time training to real-world robotic deployment presents both opportunities and challenges. The long training times required for online RL can be prohibitive in physical environments, where data collection is inherently slower than simulation. Future work can explore techniques to accelerate real-world training, such as sim-to-real transfer for reward models, parallel robot deployment for distributed data collection, or more sample-efficient online learning algorithms. Equally important is ensuring safety during exploration: the uncontrolled policy behavior in early training stages could damage the robot or environment. Developing safety mechanismssuch as action constraints, safety critics, or human oversight protocolswould be crucial for enabling safe autonomous learning in physical environments. Third, exploring more sophisticated exploration strategies and curriculum designs could further improve sample efficiency and enable adaptation to even more complex, long-horizon manipulation tasks. 6. Conclusion We introduced EVOLVE-VLA, test-time training framework that enables VLA models to continuously adapt through environment interaction, addressing the fundamental limitations of static SFT. Inspired by how humans develop manipulation skills through practice and trial-and-error, our approach shifts VLAs from rigid trajectory memorization toward genuine adaptive learning. By replacing impractical oracle rewards with learned progress estimator and introducing two key technical contributions: (1) accumulative progress estimation and (2) progressive horizon extension, we demonstrate that VLAs can effectively learn from inherently noisy, self-generated feedback signals. Our experiments on the LIBERO benchmark validate this approach, achieving +8.6% on long-horizon tasks, +22.0% in 1-shot learning, and enabling crosstask generalization (0% 20.8%) without task-specific demonstration training. Beyond these quantitative gains, we observe emergent capabilities like error recovery that arise purely from autonomous exploration. We believe this work represents an essential step on the path toward truly general-purpose VLA systems that can continuously learn and improve in real-world deployment."
        },
        {
            "title": "References",
            "content": "[1] A. Amin et al. ùúã0.6: VLA that Learns from Experience. https://www.physicalintelligence. company/blog/pistar06. Physical Intelligence Blog, November 2025. 2025. [2] J. Bi et al. Vla-touch: Enhancing vision-language-action models with dual-level tactile feedback. In: arXiv preprint arXiv:2507.17294 (2025). [3] K. Black et al. pi0: Vision-Language-Action Flow Model for General Robot Control. In: arXiv preprint arXiv:2410.24164 (2024). [4] A. Brohan et al. Rt-1: Robotics transformer for real-world control at scale. In: arXiv preprint arXiv:2212.06817 (2022). [5] T. Brown et al. Language models are few-shot learners. In: NeurIPS (2020). [6] K. Chen et al. pi RL: Online RL Fine-tuning for Flow-based Vision-Language-Action Models. In: arXiv preprint arXiv:2510.25889 (2025). [7] P. Ding et al. Quar-vla: Vision-language-action model for quadruped robots. In: European Conference on Computer Vision. Springer. 2024, pp. 352367. [8] C. Gao et al. OctoNav: Towards Generalist Embodied Navigation. In: arXiv preprint arXiv:2506.09839 (2025). [9] H. Guo et al. OmniVLA: Unifiying Multi-Sensor Perception for Physically-Grounded Multimodal VLA. In: arXiv preprint arXiv:2511.01210 (2025). [10] Y. Guo et al. Improving Vision-Language-Action Model with Online Reinforcement Learning. In: arXiv preprint arXiv:2501.16664 (2025). [11] M. J. Kim, C. Finn, and P. Liang. Fine-tuning vision-language-action models: Optimizing speed and success. In: arXiv preprint arXiv:2502.19645 (2025). [12] M. J. Kim et al. Openvla: An open-source vision-language-action model. In: arXiv preprint arXiv:2406.09246 (2024). [13] H. Li et al. Simplevla-rl: Scaling vla training via reinforcement learning. In: arXiv preprint arXiv:2509.09674 (2025). [14] B. Liu et al. Libero: Benchmarking knowledge transfer for lifelong robot learning. In: Advances in Neural Information Processing Systems 36 (2023), pp. 4477644791. 14 [15] H. Liu et al. Visual instruction tuning. In: Advances in neural information processing systems 36 (2023), pp. 3489234916. [16] J. Liu et al. What Can RL Bring to VLA Generalization? An Empirical Study. In: arXiv preprint arXiv:2505.19789 (2025). [17] G. Lu et al. VLA-RL: Towards Masterful and General Robotic Manipulation with Scalable Reinforcement Learning. In: arXiv preprint arXiv:2505.18719 (2025). [18] Y. J. Ma et al. Liv: Language-image representations and rewards for robotic control. In: International Conference on Machine Learning. PMLR. 2023, pp. 2330123320. [19] Y. J. Ma et al. Vision language models are in-context value learners. In: The Thirteenth International Conference on Learning Representations. 2024. [20] D. Qu et al. Spatialvla: Exploring spatial representations for visual-language-action model. In: arXiv preprint arXiv:2501.15830 (2025). [21] Z. Shao et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. In: URL https://arxiv. org/abs/2402.03300 2.3 (2024), p. 5. [22] S. Sontakke et al. Roboclip: One demonstration is enough to learn robot policies. In: Advances in Neural Information Processing Systems 36 (2023), pp. 5568155693. [23] O. M. Team et al. Octo: An open-source generalist robot policy. In: arXiv preprint arXiv:2405.12213 (2024). [24] H. Touvron et al. LLaMA: Open and efficient foundation language models. In: arXiv preprint arXiv:2302.13971 (2023). [25] P. Wang et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. In: arXiv preprint arXiv:2409.12191 (2024). [26] [27] J. Wen et al. DiffusionVLA: Scaling Robot Foundation Models via Unified Diffusion and Autoregression. In: Forty-second International Conference on Machine Learning. 2025. J. Wen et al. Tinyvla: Towards fast, data-efficient vision-language-action models for robotic manipulation. In: IEEE Robotics and Automation Letters (2025). [28] S. Yang et al. BiTLA: Bimanual Tactile-Language-Action Model for Contact-Rich Robotic Manipulation. In: Proceedings of the 1st International Workshop on Multi-Sensorial Media and Applications. 2025, pp. 1217. [29] S. Zhai et al. Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning. In: arXiv preprint arXiv:2509.15937 (2025). [30] J. Zhang et al. ReWiND: Language-Guided Rewards Teach Robot Policies without New Demonstrations. In: arXiv preprint arXiv:2505.10911 (2025). [31] Q. Zhao et al. Cot-vla: Visual chain-of-thought reasoning for vision-language-action models. In: Proceedings of the Computer Vision and Pattern Recognition Conference. 2025, pp. 17021713."
        }
    ],
    "affiliations": [
        "National University of Singapore"
    ]
}