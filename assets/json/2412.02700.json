{
    "paper_title": "Motion Prompting: Controlling Video Generation with Motion Trajectories",
    "authors": [
        "Daniel Geng",
        "Charles Herrmann",
        "Junhwa Hur",
        "Forrester Cole",
        "Serena Zhang",
        "Tobias Pfaff",
        "Tatiana Lopez-Guevara",
        "Carl Doersch",
        "Yusuf Aytar",
        "Michael Rubinstein",
        "Chen Sun",
        "Oliver Wang",
        "Andrew Owens",
        "Deqing Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Motion control is crucial for generating expressive and compelling video content; however, most existing video generation models rely mainly on text prompts for control, which struggle to capture the nuances of dynamic actions and temporal compositions. To this end, we train a video generation model conditioned on spatio-temporally sparse or dense motion trajectories. In contrast to prior motion conditioning work, this flexible representation can encode any number of trajectories, object-specific or global scene motion, and temporally sparse motion; due to its flexibility we refer to this conditioning as motion prompts. While users may directly specify sparse trajectories, we also show how to translate high-level user requests into detailed, semi-dense motion prompts, a process we term motion prompt expansion. We demonstrate the versatility of our approach through various applications, including camera and object motion control, \"interacting\" with an image, motion transfer, and image editing. Our results showcase emergent behaviors, such as realistic physics, suggesting the potential of motion prompts for probing video models and interacting with future generative world models. Finally, we evaluate quantitatively, conduct a human study, and demonstrate strong performance. Video results are available on our webpage: https://motion-prompting.github.io/"
        },
        {
            "title": "Start",
            "content": "Motion Prompting: Controlling Video Generation with Motion Trajectories Daniel Geng1,2, Charles Herrmann1, Junhwa Hur1 Forrester Cole1 Serena Zhang1 Tobias Pfaff1 Tatiana Lopez-Guevara1 Carl Doersch1 Yusuf Aytar1 Michael Rubinstein1 Chen Sun1,3 Oliver Wang1 Andrew Owens2 Deqing Sun1 2University of Michigan 1Google DeepMind 3Brown University 4 2 0 2 3 ] . [ 1 0 0 7 2 0 . 2 1 4 2 : r https://motion-prompting.github.io/"
        },
        {
            "title": "Abstract",
            "content": "Motion control is crucial for generating expressive and compelling video content; however, most existing video generation models rely mainly on text prompts for control, which struggle to capture the nuances of dynamic actions and temporal compositions. To this end, we train video generation model conditioned on spatio-temporally sparse or dense motion trajectories. In contrast to prior motion conditioning work, this flexible representation can encode any number of trajectories, object-specific or global scene motion, and temporally sparse motion; due to its flexibility we refer to this conditioning as motion prompts. While users may directly specify sparse trajectories, we also show how to translate high-level user requests into detailed, semi-dense motion prompts, process we term motion prompt expansion. We demonstrate the versatility of our approach through various applications, including camera and object motion control, interacting with an image, motion transfer, and image editing. Our results showcase emergent behaviors, such as realistic physics, suggesting the potential of motion prompts for probing video models and interacting with future generative world models. Finally, we evaluate quantitatively, conduct human study, and demonstrate strong performance. 1. Introduction In video generation, motion is paramount. It can elevate video from the uncanny valley to realistic or from amateur to professional. Motion guides attention, enhances storytelling, and defines visual style. Skilled filmmakers, like Kubrick and Kurosawa, masterfully use motion to create captivating, immersive experiences. Achieving realistic and expressive motion, coupled with granular control, is essential for generating compelling video. While text remains the Work done as an intern, Project lead main control signal for generation, its limitations become apparent when focusing on motion. Although effective for describing static scenes in images or high level actions, text struggles to convey the subtleties of motion: e.g., prompt like bear quickly turns its head could be interpreted in countless ways. How quick is quickly? What is the exact trajectory? Should it accelerate? Even detailed descriptions fail to capture nuances like ease-in-ease-out timing or synchronized movements. These nuances are often better conveyed through the motion itself. Motivated by this, we explore motion as powerful, complementary control scheme to text. Our first observation is that in order to fully harness the expressiveness of motion, we require representation that can encode any type of motion. To this end, we identify spatio-temporally sparse or dense motion trajectories [20, 56] as an ideal candidate. Motion trajectories, a.k.a. particle video or point tracks, track the movement and visibility of set of points throughout video, offering highly expressive encoding of motion. This representation can capture the trajectories of any number of points, represent object-specific or global scene motion, and even handle temporally sparse motion constraints. Furthermore, recent advances in point track estimation have yielded robust and efficient algorithms [12, 13, 34, 35] that are capable of processing diverse real-world videos to generate constraints for training. Given the comprehensive and flexible nature of this motion representation, akin to text, we designate our motion conditioning as motion prompts. We then train motion trajectory ControlNet [84] on top of pre-trained video diffusion model [2] to accept the motion prompt conditioning. While these motion prompts can define any type of video motion, what is less clear is how user would generate them in practice. Sparse trajectories, which give the rough direction of few pixels or patches, may be easy to specify with mouse drags, but do not sufficiently constrain the generation process and fall short with respect to fine-grained control. Conversely, dense trajectories, though offering precise conFigure 1. Motion Prompting. 1) We train general-purpose track-conditioned ControlNet adapter on top of video diffusion model. 2) To use this model, we design motion prompts from user inputs, and show variety of capabilities from this single trained model, such as object control, camera control, simultaneous object and camera control, motion transfer, and model probing. We visualize the motion prompt tracks and corresponding frames from the generated videos underneath. The tracks are colored only for the purpose of visualization, with trails denoting the direction and magnitude of motion. Additionally, some of our motion prompts are derived from user mouse motions, for which we visualize the mouse locations. We highly encourage the reader to view video results on our webpage . trol, are impractical to design manually. To address this, our second observation is that we can often translate high level user requests (e.g., move the camera around the xz plane, rotate the head of the cat) into detailed motion trajectories through computer vision signals. We denote this process as motion prompt expansion due to its similarities to prompt expansion [11] or rewriting [3] for text in image generation. This method is intended to bridge the gap between user goals and our motion representation. We identify several instances where motion prompt expansion can be an effective tool including  (Fig. 1)  : converting user mouse drags into semi-dense motion trajectories allowing users to interact with an image by manipulating hair or sand (Sec. 4.1); simultaneously specifying camera and object motion (Sec. 4.4); performing motion transfer where motion from given video is applied to different first frame (Sec. 4.5); and performing drag-based image editing (Sec. 4.1). While these results are not yet real time or causal, they strongly hint at how users may interact with generative world models, and allows us to probe the video prior of the generator to understand the aspects of physics and general world knowledge it has learned. Finally we present quantitative results and human studies against baselines, indicating that our model performs well. We also present ablations to validate our design choices and give insight. In summary, our contributions are: We focus on motion as conditioning signal and identify spatio-temporally sparse or dense motion tracks as flexible motion representation that can accomplish many aspects of motion control. We train ControlNet to accept these motion prompts as conditioning. We propose motion prompt expansion, process which takes simple user input and produces more complex motion tracks, which allow for more fine-grained control. We then apply our approach to wide range of tasks, such as object control, camera control, motion transfer, or drag-based image editing. We also show emergent behavior, such as physics, which suggests that these motion prompts may be used to probe video models or interact with future world models. We evaluate our method against baselines with quantitative metrics and human study, showing that our model performs well compared to baselines. 2. Related Work Video Diffusion Models. Diffusion models [23, 63, 64] have demonstrated amazing capabilities for video generation, conditioned on natural language [2, 18, 19, 24, 25] or by animating static images into videos [5, 62, 78]. Beyond content creation [50], they can be seen as path to the ambitious goal of creating world simulators [6], showing preliminary success in visual planning for embodied agents [15, 16, 80]. Meanwhile, whether the video prior captures sufficient understanding of the physical world is still under debate [33], and explicit integration of physics rules appears to be necessary [39, 82, 83]. Our motion prompting technique, applicable to any video diffusion model, not only offers more flexible and accurate interface to specify motion patterns for video generation, but also serves as framework to probe trained generative model for their 3D or physics understanding. Motion-conditioned Video Generation. pre-trained text-to-video model can be adapted to follow new motion patterns or additional motion conditioning signals. Lowrank adaptation (LoRA) [27], generic technique for parameter fine-tuning, can be utilized for few-shot motion customization [53, 87]. DreamBooth [55], originally for personalized image generation, can also be applied to video generation [76] with motion control. Recent work explores sparse (and often object-centric) trajectory-based motion control. The approaches vary in their design choices but often require certain complicated engineering techniques for stable training and better convergence. Tora [86], MotionCtrl [73], DragNUWA [81], Image Conductor [37], and MCDiff [8] adopt two-stage (e.g., finetuning with first dense and then sparse trajectories, or training adapters sequentially), specialized losses [37, 43], or multi-stage fine-tuning for multiple modules [9, 59, 71]. MOFA-Video [44] requires separate adapters for different motion types, TrackGo [89] uses custom losses and layers, while other works [37, 44, 73, 81, 86] engineer data filtering pipelines. In contrast, we find that simpler training recipe yields high quality results. Our model is trained in single stage, with uniformly sampled dense trajectories, and without any specialized engineering efforts. Yet it handles wide range of tasks and motions, generalizing to both sparse and dense trajectories during inference. Other approaches use entity-centric control signals such as bounding boxes [70, 76], segmentation masks [10, 77], human pose [28, 79], or camera pose [21, 74]. Zero-shot motion adaptation approaches (e.g., SG-I2V [43], Trailblazer [41], FreeTraj [52], and Peekaboo [30]) adopt similar strategy, guiding the video generation based on the motion of entity-centric masks and thus avoiding training or fine-tuning video models. Our motion prompts offer more flexible interface to control motion generation at various granularity. Unlike the test-time approaches which explicitly control the diffusion feature maps, our framework naturally balances the strength of controlling signals and that of the encoded video priors. Motion Representations. As our goal is to condition video generation model on motion of any type, it is crucial to choose suitable motion representation. The most common representation is optical flow [7, 14, 26, 40, 65, 66]. While flow can be chained over time, errors can accumulate. The lack of occlusion handling also makes it unsuitable for our needs, which we find necessary for good camera control (Sec. 4.3). In contrast, long-range feature matching [4, 29, 31, 61] or point trajectories [12, 13, 20, 34, 35, 88] is well-suited representation for our application. It can handle occlusions and allows for both sparse and dense tracking over any arbitrary temporal durations. 3. Method Our video generation method takes as input single frame, text prompt, and motion prompt in the form of point trackswhich we explain how to create in Sec. 4. Full implementation details can be found in Appendix A. 3.1. Motion Prompts To fully harness the expressiveness of motion, we need to be able to represent any type of motion. To this end, we use point trajectories for our motion prompts, which can encode both spatially (and temporally) sparse and dense motions, motion on single object or of an entire scene, and even occlusions via visibility flag. Using this representation enables broad range of capabilities such as object control (Sec. 4.1), camera control (Sec. 4.3), both simultaneously 3.3. Data To train our model, we prepare video dataset paired with tracks. We run BootsTAP [13], an off-the-shelf point tracking method, on an internal dataset consisting of 2.2M videos resized to 128 128, the output size of our base model. We extract tracks densely, resulting in 16,384 tracks per video as well as predicted occlusions, which we can sample from during training. We do not filter the videos in any way, with the hypothesis being that training on diverse motions will result in more powerful and flexible model. 3.4. Training Training follows ControlNet [84], where the conditioning signal is given to trainable copy of the base models encoder and the standard diffusion loss is optimized. For every video, we sample random number of tracks from uniform distribution and construct the conditioning signal as explained above. More details can be found in Appendix A. We observe various phenomena during training. For one, we find that the loss is not correlated with the performance of the model at following tracks. Also, similar to [84], we observe sudden convergence phenomena in which the model goes from completely ignoring the conditioning signal to fully trained in short number of training steps. More details can be found in Appendix B. Finally, we observe that our model exhibits fairly strong generalization in multiple directions. For example, while our model is trained on randomly sampled tracks, resulting in spatially uniformly distributed tracks during training, the model can generalize to spatially localized track conditioning (Figs. 3 and 6). In addition, while our model is trained for specific numbers of tracks, it generalizes surprisingly well to more  (Fig. 5)  or fewer number of tracks (Figs. 3, 4 and 6). Finally, we find that our model generalizes to tracks that dont necessarily start from the first frame, despite only being trained on these tracks (Fig. 3b). We hypothesize this generalization is due to combination of inductive biases from the convolutions in the network and the fact that we train the model on large variety of trajectories. Figure 2. Conditioning Tracks. During training, we take estimated tracks from video (left) and encode them into H dimensional space-time volume (middle). Each track has unique embedding (right), written to every location the track visits and is visible at. All other locations are set to zeros. This strategy can encode any number and configuration of tracks. (Sec. 4.4), motion transfer (Sec. 4.5), and drag-based image editing (Sec. 4.1) under unified model. Formally, we denote set of point trajectories of length by RN 2, where the 2D coordinate of the nth track at the tth timestep is p[n, t] = (xn ). In addition, we denote the visibility of the tracks as RN , an array of 1s and 0s where 0 indicates an off-screen or occluded track, and 1 indicates visible track. , yn 3.2. Architecture We build our model on top of Lumiere, pre-trained video diffusion model [2] which has been trained to generate 5 seconds of video at 16 fps given text and first frame conditioning. In order to train in track conditioning, we use ControlNet [84] which requires encoding tracks in spatialtemporal volume, RT HW C, where is the number of frames, and are the height and width of the generated video, and is the channel dimension. To do this, we associate with each track, p[n, :], unique and random embedding vector ϕn RC. Then, for each space-time location track visits, and is visible at, we simply place the embedding ϕn RC in that location. All other values in the conditioning signal are set to 0. Fig. 2 illustrates this process. In other words, we zero-initialize and set c[t, xn , yn ] = v[n, t]ϕn 4. Motion Prompts (1) for all tracks at each timestep t, where multiplying by the visibility v[n, t] zeros out the embedding if the track is not visible at that location and time. We quantize xn to the nearest integer for simplicity. When multiple tracks pass through the same space-time location, we add the embeddings together. The track embeddings ϕn are randomly (regardless of spatial location) assigned sinusoidal positional embeddings from [68]. For completely dense tracks, this representation is equivalent to starting with dense grid of embeddings and forward warping, similar to [57]. and yn In this section, we discuss different types of effects achievable through motion prompts and prompt expansion. In particular, we identify and demonstrate several different types of expansion, as shown in Fig. 1. Text prompts and other parameters for each video may be found in Tab. A1. We strongly encourage readers to view generated videos on our webpage. 4.1. Interacting with an Image Our model enables the ability to interact with images. To do this, we build GUI that displays still image and Figure 3. Interacting with an Image. We translate simple user input, mouse motions and drags, and expand it into more complex motion prompt which helps to achieve the users intention. The mouse trajectories are visualized as hand when dragging, and as black cursor otherwise. grid of tracks centered on the cursor are created when the mouse is dragged, as shown in the top row. Frames from the generated video are shown in the bottom row. Prompting our model in this way, we can (a) move the head of parrot or (c) cow (b) play with hair or (d) interact with an image of sand. We can also keep the background still by specifying static tracks, as in (b) or (d). Note these samples are not generated in real-time and are not temporally causal. More examples can be found on our webpage. and size of this grid can be chosen by the user, similar to the Gaussian blurring of tracks in [37, 73, 77, 81] to specify the spatial extent of the motion. However, note that in our approach this step is done only at inference time, and not at train time. Additionally, user may choose to place grid of static tracks down to keep the background still, as in Fig. 3b and Fig. 3d, or have tracks persist after mouse drag as in Fig. 3d. Emergent Phenomena. We find that these interaction motion prompts can result in straightforward motions, such as turning the head of parrot in Fig. 3a. But interestingly, we also observe more complex dynamics: e.g., in Fig. 3b, where the tracks toss the hair of the subject, or in Fig. 3d where the sand is swept around. In these examples, we are essentially probing the video prior learned by the model, and by doing so are able to visualize the physics and general world understanding that the model has learned. This suggests two things. First, that we may debug video models with motion prompts and find their limitations, which we discuss further in Sec. 4.6. And second, that as these models improve, we might be able to use these motion prompts to query video generation models as world models that react to user interactions. Drag-Based Image Editing. natural application of this interaction ability is drag-based image editing [1, 17, 42, 46, 54, 60]. This task involves taking user supplied drags and editing an image such that objects follow these drags. We shows qualitative results in Fig. 4. Figure 4. Drag-Based Image Editing. We show the input images in the first row, and resulting drag-based edits in the bottom row, In addition, in the final with the drag visualized in both rows. example we show how we can keep areas of the images static. records mouse drags from user. This recording is then converted to tracks, as described below, and is fed to the model along with the initial frame and text. More information about the GUI can be found in Appendix A. For simple mouse motions, where the mouse is constantly dragged, this approach is similar to prior work on sparse trajectory conditioned video generation [37, 43, 44, 59, 71, 73, 77, 81, 86, 89]. However, because our model generalizes to partial tracks, we can also handle multiple mouse drags in different locations at different times, resulting in natural user control as in Fig. 3b and Fig. 3d. Please note that while we record mouse inputs in real-time, our method requires sampling from the video diffusion model, which is not real-time it takes about 12 minutes to generate an output video. To create the motion prompts, we translate mouse drags into grid of point tracks as shown in Fig. 3. The density Figure 5. Camera Control with Depth. We can construct motion prompts for camera control by specifying camera trajectory and computing point cloud with an off-the-shelf monocular depth estimator. We then project the points onto the sequence of cameras, which results in the shown point trajectories. We can also convert user mouse input into camera trajectories, as in example (c). mouse-drag created trajectory. For implementation details, please see Appendix A. 4.3. Camera Control with Depth We can also design motion prompts to achieve camera control with our model. We do this by first running an off-theshelf monocular depth estimator [49] on the input frame to obtain point cloud of the scene. Then, given trajectory of camera poses we can re-project the point cloud onto each camera, resulting in 2D tracks for input. We can further improve quality by running z-buffering to get occlusion flags. Prompting our model with these motion prompts results in camera control, as shown in Fig. 5. We can orbit camera in circles as in Fig. 5a or have it arc upwards as in Fig. 5b. In addition, we can combine this camera control with mouse recordings for even greater ease of use. To do so, we record mouse inputs as is done in Sec. 4.1. We then construct camera trajectory such that single point in the point cloud follows the mouse trajectory, and that the camera is constrained to vertical plane, which we show in Fig. 5c. For implementation details, please see Appendix A. Note that our model is neither trained on nor conditioned on camera poses, as with prior work [37, 73, 74]. Furthermore, none of our training data includes pose annotations. Despite this, we find that our model can achieve compelling camera control. This shows that instead of training video model on specific types of motion, we can train model on general motions and tease out specific capabilities by using motion prompts. 4.4. Composing Motions By composing motion prompts together we can combine capabilities. For example, in Fig. 7, we add together tracks Figure 6. Object Control with Primitives. By defining geometric primitives (e.g., sphere) manipulated by user with mouse, we can obtain tracks exerting more fine-grain control over objects (e.g., rotations), which cannot be specified with single track. 4.2. Object Control with Primitives We can also reinterpret mouse motions as manipulating proxy geometric primitive, such as sphere. By placing these tracks over an object that can be roughly approximated by the primitive, we can effect more fine-grained control over the object than with sparse mouse tracks alone. For example, in Fig. 6, we place sphere over the head of cat and the eye of frog to precisely rotate these objects to different positions, and in Fig. 1 we animate bear. In this setting, the user must supply both the mouse motion, and also the location and radius of the sphere to use. This allows for the user to specify more complex motions than translations, which would be hard to express with single Figure 7. Compositions of Motion Prompts. By composing motion prompts together, we can attain simultaneous object and camera control. For example, here we move the dog and horses head while orbiting the camera from left to right. for object control and camera control, resulting in simultaneous control of both. This is done by converting the object tracks to displacements, and adding these deltas to the camera control tracks. In two dimensions, this composition is an approximation and will fail for extreme camera motion, but we find it works well for small to moderate camera motion. Again note, that we do not specifically train for this capability in contrast to prior work [37, 73, 74]. 4.5. Motion Transfer Many types of motions may be hard to design motion prompt for. Given video with desired motion, we can perform motion transfer [17, 71], where we extract motion tracks from source video and apply it to an image. For example, we can extract the motion of person turning their head and use it to puppeteer macaque, as in Fig. 8. Moreover, we find that our model is surprisingly robust, in that we can apply motions to fairly out-of-domain images. For example, in Fig. 8 we apply the motion of monkey chewing to birds eye view image of trees. The resulting videos exhibit an interesting effect in which pausing the video on any frame removes the percept of the source video [22]. The monkey can only be perceived when the video is playing, where Gestalt common-fate effect occurs [32]. 4.6. Failures, Limitations, and Probing Models We differentiate failures into two broad categories. The first are failures of our motion conditioning or our motion prompting. For example, in Fig. 9a we show an example in which the cows head is unnaturally stretched due to the horns being mistakingly locked to the background. The second category are failures due to the underlying video model. For example, in Fig. 9b, we drag the chess piece but new one spontaneously forms, which is less plausible Figure 8. Motion Transfer. By conditioning our model on extracted motion from source video we can puppeteer macaque, or even transfer the motion of monkey chewing to photo of trees. Best viewed as videos on our webpage. Figure 9. Probing by Failures. We can use motion prompts to probe limitations of the underlying model. For example, dragging the chess piece results in the creation of new piece. video given the constraint. These types of failures suggest that we might be able to use motion prompts as way to probe video models and discover limitations in their learned representations. 5. Quantitative Results In addition to the qualitative examples above, we describe quantitative benchmark, and evaluate our method against Table 1. Quantitative Evaluations. We evaluate the appearance (PSNR, SSIM, LPIPS, FVD) and motion (EPE) of generated videos on the validation set of the DAVIS dataset. Please note that each method is trained from different base model. Table 2. Human Study. We present % win rates of our method against baselines in 2AFC human study results. Sample sizes are = 103, = 103, and = 115 for each column respectively. # Tracks Method PSNR SSIM LPIPS FVD EPE = 1 = 16 = 512 = Image Conductor 11.468 14.589 DragAnything 15.431 Ours Image Conductor 12.184 15.119 DragAnything 16.618 Ours Image Conductor 11.902 15.055 DragAnything 18.968 Ours Image Conductor 11.609 14.845 DragAnything 19.327 Ours 0.145 0.241 0.266 0.175 0.305 0. 0.132 0.289 0.583 0.120 0.286 0.608 0.529 0.420 0.368 0.502 0.378 0.319 0.524 0.381 0.229 0.538 0.397 0. 1919.8 19.224 1544.9 9.135 1445.2 14.619 1838.9 24.263 1282.8 9.800 1322.0 8.319 1966.3 30.734 1379.8 10.948 4.055 688.7 1890.7 33.561 1468.4 12.485 3.887 655.9 recent baselines. Furthermore, we conduct human study, and describe ablations in this section. 5.1. Track-Conditioned Generation Evaluation To evaluate our track-text-and-first frame conditioned video generation, we use the validation split of the DAVIS video dataset [51]. We extract first frames and tracks from the dataset and feed this to the models along with an automatically generated text prompt. For exact implementation details, please see Appendix A. To evaluate range of track densities, we vary the number of conditioning tracks from just single track to 2048 tracks. We compare our method with two recent works: ImageConductor [37], which finetunes AnimateDiff [19] for camera and object motion, and DragAnything [77], which is designed to move entities along tracks by finetuning Stable Video Diffusion [5]. To evaluate the appearance of the generated videos we compute PSNR, SSIM [72], LPIPS [85], and FVD [67] between the generated videos and ground truth videos. To evaluate how well the generated video matches the motion conditioning, we use end-point error (EPE) which is computed as the L2 distance between the conditioning tracks and tracks estimated from the generated videos. As shown in Tab. 1, our model outperforms the baselines in almost all cases. On some examples, DragAnything performs better in terms of EPE with fewer tracks. This is because DragAnything includes module that effectively warps latents. While this warping effect may result in accurate motion, it also creates visual artifacts as evidenced by the underperforming PSNR, SSIM, LPIPS, and FVD results. We also provide numbers for 4 and 64 tracks in Appendix C, which we omit here for brevity. 5.2. Human Study We run human study where we manually create set of 30 inputs consisting of single trajectory. We run two alternative forced choice test where we ask (1) which video Method Motion Adherence Motion Quality Visual Quality Image Conductor Drag Anything 74.3 (1.1) 74.5 (1.1) 80.5 (1.0) 75.7 (1.1) 77.3 (1.0) 73.7 (1.0) Table 3. Ablation. We ablate the density of tracks during training and find that training on dense tracks works best for our model. # Tracks Method PSNR SSIM LPIPS FVD EPE = 4 = 2048 Sparse 15.075 Dense + Sparse 15.162 15.638 Dense Sparse 15.697 Dense + Sparse 15.294 19.197 Dense 0.241 0.252 0.296 0.284 0.246 0.582 0.384 0.379 0.349 0.355 0.375 0.230 1209.2 30.712 1230.6 29.466 1254.9 24.553 1322.0 26.724 1267.8 27.931 4.806 729. follows the motion conditioning better (2) which video has more realistic motion and (3) which video has higher visual quality. There are 180 questions total, and win rates for our method as well as 95% confidence intervals are presented in Tab. 2. When considering both motion and appearance together, our approach is preferred over baselines in all categories. Implementation details can be found in Appendix A. 5.3. Ablations In Tab. 3 we present an ablation where we train our model on only Sparse point trajectories (1-8 tracks) and Dense + Sparse, where the number of tracks is sampled logarithmically from 20 to 213. We find that dense training is most effective, especially for large number of tracks. Surprisingly, dense training is also better for sparse tracks. We hypothesize that this is because using sparse tracks gives so little training signal that it is more efficient to train on dense tracks, which then generalizes to sparser tracks, though this may be influenced by our usage of ControlNet and zero convolutions. We use subset of the DAVIS evaluation from Sec. 5.1, but we note that the numbers do not match as we use less data and fewer training steps for the ablations. 6. Conclusion We have introduced framework for motion-conditioned video generation that leverages flexible motion prompts spatio-temporal trajectories that can encode arbitrary motion complexity. Unlike prior work, this representation allows specifying sparse or dense motion for cameras, objects, or full scenes. We also introduce motion prompt expansion to translate high-level motion requests into detailed prompts. Our versatile approach enables applications like motion control, motion transfer, image editing, and showcasing emergent behaviors like realistic physics with single unified model. Quantitative and human evaluation demonstrate the effectiveness of our framework. Acknowledgements We would like to thank Sarah Rumbley, Roni Paiss, Jeong Joon Park, Liyue Shen, Stella Yu, Alyosha Efros, Daniel Watson, David Fleet, and Bill Freeman for their invaluable feedback and discussions."
        },
        {
            "title": "References",
            "content": "[1] Hadi Alzayer, Zhihao Xia, Xuaner Zhang, Eli Shechtman, Jia-Bin Huang, and Michael Gharbi. Magic Fixup: Streamlining photo editing by watching dynamic videos. arXiv preprint arXiv:2403.13044, 2024. 5 [2] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Yuanzhen Li, Tomer Michaeli, et al. Lumiere: spacetime diffusion model for video generation. arXiv preprint arXiv:2401.12945, 2024. 1, 3, 4 [3] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Improving image generation with better Yufei Guo, et al. captions. Technical report, 2023. 2 [4] Zhangxing Bian, Allan Jabri, Alexei A. Efros, and Andrew Owens. Learning pixel trajectories with multiscale contrastive random walks. In CVPR, 2022. 3 [5] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 3, 8 [6] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. Technical report, 2024. 3 [7] Thomas Brox and Jitendra Malik. Large displacement optical flow: descriptor matching in variational motion estimation. IEEE TPAMI, 33(3):500513, 2010. [8] Tsai-Shien Chen, Chieh Hubert Lin, Hung-Yu Tseng, TsungYi Lin, and Ming-Hsuan Yang. Motion-conditioned diffusion model for controllable video synthesis. arXiv preprint arXiv:2304.14404, 2023. 3 [9] Tsai-Shien Chen, Chieh Hubert Lin, Hung-Yu Tseng, TsungYi Lin, and Ming-Hsuan Yang. Motion-conditioned diffusion model for controllable video synthesis. arXiv preprint arXiv:2304.14404, 2023. 3 [10] Zuozhuo Dai, Zhenghao Zhang, Yao Yao, Bingxue Qiu, Siyu Zhu, Long Qin, and Weizhi Wang. Fine-grained open domain image animation with motion guidance, 2023. 3 [11] Siddhartha Datta, Alexander Ku, Deepak Ramachandran, and Peter Anderson. Prompt expansion for adaptive text-toimage generation. arXiv preprint arXiv:2312.16720, 2023. 2 [12] Carl Doersch, Yi Yang, Mel Vecerik, Dilara Gokay, Ankush Gupta, Yusuf Aytar, Joao Carreira, and Andrew Zisserman. TAPIR: Tracking any point with per-frame initialization and temporal refinement. In ICCV, pages 1006110072, 2023. 1, 3, 4 [13] Carl Doersch, Yi Yang, Dilara Gokay, Pauline Luc, Skanda Koppula, Ankush Gupta, Joseph Heyward, Ross Goroshin, Joao Carreira, and Andrew Zisserman. BootsTAP: BootarXiv preprint strapped training for tracking-any-point. arXiv:2402.00847, 2024. 1, 3, [14] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van Der Smagt, Daniel Cremers, and Thomas Brox. FlowNet: Learning optical flow with convolutional networks. In ICCV, pages 27582766, 2015. 3 [15] Yilun Du, Sherry Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Josh Tenenbaum, Dale Schuurmans, and Pieter Abbeel. Learning universal policies via text-guided video generation. NeurIPS, 36, 2024. 3 [16] Alejandro Escontrela, Ademi Adeniji, Wilson Yan, Ajay Jain, Xue Bin Peng, Ken Goldberg, Youngwoon Lee, Danijar Hafner, and Pieter Abbeel. Video prediction models as rewards for reinforcement learning. NeurIPS, 36, 2024. 3 [17] Daniel Geng and Andrew Owens. Motion guidance: Diffusion-based image editing with differentiable motion estimators. In ICLR, 2024. 5, 7 [18] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, and Ishan Misra. Emu video: Factorizing text-to-video generation by explicit image conditioning. arXiv preprint arXiv:2311.10709, 2023. 3 [19] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. AnimateDiff: Animate your personalized text-toIn ICLR, image diffusion models without specific tuning. 2024. 3, [20] Adam W. Harley, Zhaoyuan Fang, and Katerina Fragkiadaki. Particle video revisited: Tracking through occlusions using point trajectories. In ECCV, 2022. 1, 3 [21] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. CameraCtrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2024. 3 [22] Herolias. My first try with video. https : / / www . reddit . com / / StableDiffusion / comments / 17b4dfc/my_first_try_with_video/, 2023. Accessed: 2024-11-13. 7 [23] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 33:68406851, 2020. 3 [24] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben Imagen Poole, Mohammad Norouzi, David J. Fleet, et al. video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. [25] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. NeurIPS, 35:86338646, 2022. 3 [26] Berthold K.P. Horn and Brian G. Schunck. Determining optical flow. Artificial intelligence, 17(1-3):185203, 1981. 3 [27] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 3 [28] Li Hu, Xin Gao, Peng Zhang, Ke Sun, Bang Zhang, and Liefeng Bo. Animate Anyone: Consistent and controllable arXiv image-to-video synthesis for character animation. preprint arXiv:2311.17117, 2023. 3 [29] Allan Jabri, Andrew Owens, and Alexei Efros. Space-time correspondence as contrastive random walk. NeurIPS, 33: 1954519560, 2020. 3 [30] Yash Jain, Anshul Nasery, Vibhav Vineet, and Harkirat Behl. Peekaboo: Interactive video generation via maskedIn Proceedings of the IEEE/CVF Conference diffusion. on Computer Vision and Pattern Recognition, pages 8079 8088, 2024. 3 [31] Wei Jiang, Eduard Trulls, Jan Hosang, Andrea Tagliasacchi, and Kwang Moo Yi. COTR: Correspondence transformer for matching across images. In ICCV, pages 62076217, 2021. [32] Gunnar Johansson. Visual perception of biological motion and model for its analysis. Perception & psychophysics, 14:201211, 1973. 7 [33] Bingyi Kang, Yang Yue, Rui Lu, Zhijie Lin, Yang Zhao, Kaixin Wang, Gao Huang, and Jiashi Feng. How far is video generation from world model: physical law perspective. arXiv preprint arXiv:2411.02385, 2024. 3 [34] Nikita Karaev, Iurii Makarov, Jianyuan Wang, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. CoTracker3: Simpler and better point tracking by pseudoarXiv preprint arXiv:2410.11831, labelling real videos. 2024. 1, 3 [35] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. CoIn ECCV, 2024. 1, Tracker: It is better to track together. 3 [36] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything. arXiv:2304.02643, 2023. [37] Yaowei Li, Xintao Wang, Zhaoyang Zhang, Zhouxia Wang, Ziyang Yuan, Liangbin Xie, Yuexian Zou, and Ying Shan. Image conductor: Precision control for interactive video synthesis. arXiv preprint arXiv:2406.15339, 2024. 3, 5, 6, 7, 8 [38] Ce Liu, Antonio Torralba, William Freeman, Fredo Durand, and Edward Adelson. Motion magnification. ACM transactions on graphics (TOG), 24(3):519526, 2005. 4 [39] Shaowei Liu, Zhongzheng Ren, Saurabh Gupta, and Shenlong Wang. PhysGen: Rigid-body physics-grounded imageIn ECCV, pages 360378. Springer, to-video generation. 2024. 3 [40] Bruce D. Lucas and Takeo Kanade. An iterative image registration technique with an application to stereo vision. In IJCAI, pages 674679, 1981. 3 [41] Wan-Duo Kurt Ma, John P. Lewis, and Bastiaan Kleijn. Trailblazer: Trajectory control for diffusion-based video generation. arXiv preprint arXiv:2401.00896, 2023. 3 [42] Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, Enabling dragarXiv preprint and Jian Zhang. style manipulation on diffusion models. arXiv:2307.02421, 2023. 5 DragonDiffusion: [43] Koichi Namekata, Sherwin Bahmani, Ziyi Wu, Yash Kant, Igor Gilitschenski, and David B. Lindell. SG-I2V: Selfguided trajectory control in image-to-video generation. arXiv preprint arXiv:2411.04989, 2024. 3, [44] Muyao Niu, Xiaodong Cun, Xintao Wang, Yong Zhang, Ying Shan, and Yinqiang Zheng. MOFA-Video: Controllable image animation via generative motion field adaptions in frozen image-to-video diffusion model. In ECCV, 2024. 3, 5 [45] Tae-Hyun Oh, Ronnachai Jaroensri, Changil Kim, Mohamed Elgharib, Fredo Durand, William Freeman, and Wojciech Matusik. Learning-based video motion magnification. In Proceedings of the European Conference on Computer Vision (ECCV), pages 633648, 2018. 4 [46] Xingang Pan, Ayush Tewari, Thomas Leimkuhler, Lingjie Liu, Abhimitra Meka, and Christian Theobalt. Drag Your GAN: Interactive point-based manipulation on the generative image manifold. In SIGGRAPH, pages 111, 2023. 5 [47] Zhaoying Pan, Daniel Geng, and Andrew Owens. Selfsupervised motion magnification by backpropagating In Thirty-seventh Conference on through optical flow. Neural Information Processing Systems, 2023. 4 [48] Bohao Peng, Jian Wang, Yuechen Zhang, Wenbo Li, MingChang Yang, and Jiaya Jia. Controlnext: Powerful and efficient control for image and video generation. arXiv preprint arXiv:2408.06070, 2024. 4 [49] Luigi Piccinelli, Yung-Hsu Yang, Christos Sakaridis, Mattia Segu, Siyuan Li, Luc Van Gool, and Fisher Yu. UniDepth: In CVPR, Universal monocular metric depth estimation. 2024. 6 [50] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. [51] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbelaez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 DAVIS challenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017. 8 [52] Haonan Qiu, Zhaoxi Chen, Zhouxia Wang, Yingqing He, Menghan Xia, and Ziwei Liu. FreeTraj: Tuning-free trajectory control in video diffusion models. arXiv preprint arXiv:2406.16863, 2024. 3 [53] Yixuan Ren, Yang Zhou, Jimei Yang, Jing Shi, Difan Liu, Feng Liu, Mingi Kwon, and Abhinav Shrivastava. Customize-a-video: One-shot motion customization of textto-video diffusion models. In ECCV, 2024. 3 [54] Noam Rotstein, Gal Yona, Daniel Silver, Roy Velich, David Bensaıd, and Ron Kimmel. Pathways on the image manifold: Image editing via video generation. arXiv preprint arXiv:2411.16819, 2024. 5 [55] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In CVPR, pages 2250022510, 2023. 3 [56] Peter Sand and Seth Teller. Particle video: Long-range moIJCV, 80:7291, tion estimation using point trajectories. 2008. 1 [57] Junyoung Seo, Kazumi Fukuda, Takashi Shibuya, Takuya Narihira, Naoki Murata, Shoukang Hu, Chieh-Hsin Lai, Seungryong Kim, and Yuki Mitsufuji. GenWarp: Single image to novel views with semantic-preserving generative warping. NeurIPS, 2024. 4 [58] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pages 45964604. PMLR, 2018. 1 [59] Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi Zhang, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, et al. Motion-I2V: Consistent and controllable image-to-video generation with explicit motion modeling. In SIGGRAPH, pages 111, 2024. 3, 5 [60] Yujun Shi, Chuhui Xue, Jiachun Pan, Wenqing Zhang, Vincent YF Tan, and Song Bai. Dragdiffusion: Harnessing diffusion models for interactive point-based image editing. arXiv preprint arXiv:2306.14435, 2023. 5 [61] Ayush Shrivastava and Andrew Owens. Self-supervised anypoint tracking by contrastive random walks. ECCV, 2024. 3 [62] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-A-Video: Text-to-video generation without text-video data. In ICLR, 2023. [63] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In ICML, pages 2256 nonequilibrium thermodynamics. 2265, 2015. 3 [64] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. 3 [65] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz. PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume. In CVPR, pages 89348943, 2018. 3 [66] Zachary Teed and Jia Deng. RAFT: Recurrent all-pairs field transforms for optical flow. In ECCV, pages 402419, 2020. 3 [67] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. FVD: new metric for video generation, 2019. 8 [68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia In NeurIPS, pages Polosukhin. Attention is all you need. 59986008, 2017. 4, [69] Neal Wadhwa, Michael Rubinstein, Fredo Durand, and William Freeman. Phase-based video motion processing. ACM Transactions on Graphics (ToG), 32(4):110, 2013. 4 [70] Jiawei Wang, Yuchen Zhang, Jiaxin Zou, Yan Zeng, Guoqiang Wei, Liping Yuan, and Hang Li. Boximator: Generating rich and controllable motions for video synthesis, 2024. 3 [71] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. VideoComposer: Compositional video synthesis with motion controllability. NeurIPS, 36, 2023. 3, 5, 7 [72] Zhou Wang, Alan C. Bovik, Hamid R. Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE TIP, 13(4):600612, 2004. 8 [73] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. 3, 5, 6, 7 [74] Daniel Watson, Saurabh Saxena, Lala Li, Andrea Tagliasacchi, and David J. Fleet. Controlling space and time with diffusion models. arXiv preprint arXiv:2407.07860, 2024. 3, 6, 7 [75] Hao-Yu Wu, Michael Rubinstein, Eugene Shih, John Guttag, Fredo Durand, and William Freeman. Eulerian video magnification for revealing subtle changes in the world. ACM transactions on graphics (TOG), 31(4):18, 2012. 4 [76] Jianzong Wu, Xiangtai Li, Yanhong Zeng, Jiangning Zhang, Qianyu Zhou, Yining Li, Yunhai Tong, and Kai Chen. Motionbooth: Motion-aware customized text-to-video generation. arXiv preprint arXiv:2406.17758, 2024. [77] Weijia Wu, Zhuang Li, Yuchao Gu, Rui Zhao, Yefei He, David Junhao Zhang, Mike Zheng Shou, Yan Li, Tingting Gao, and Di Zhang. DragAnything: Motion control for anything using entity representation. In ECCV, pages 331348, 2024. 3, 5, 8 [78] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. DynamiCrafter: Animating open-domain images with video diffusion priors. In ECCV, pages 399417. Springer, 2024. 3 [79] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. MagicAnimate: Temporally consistent human image animation using diffusion model. In CVPR, 2024. 3 [80] Sherry Yang, Jacob Walker, Jack Parker-Holder, Yilun Du, Jake Bruce, Andre Barreto, Pieter Abbeel, and Dale Schuurmans. Video as the new language for real-world decision making. arXiv preprint arXiv:2402.17139, 2024. 3 [81] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, and Nan Duan. DragNUWA: Fine-grained control in video generation by integrating text, image, and trajectory. arXiv preprint arXiv:2308.08089, 2023. 3, 5 [82] Alan Yu, Ge Yang, Ran Choi, Yajvan Ravan, John Leonard, and Phillip Isola. Lucidsim: Learning agile visual locomoIn 8th Annual Conference on tion from generated images. Robot Learning, 2024. 3 [83] Ye Yuan, Jiaming Song, Umar Iqbal, Arash Vahdat, and Jan Kautz. PhysDiff: Physics-guided human motion diffusion model. In ICCV, pages 1601016021, 2023. [84] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In IEEE International Conference on Computer Vision (ICCV), 2023. 1, 4 [85] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. 8 [86] Zhenghao Zhang, Junchao Liao, Menghao Li, Long Qin, Trajectory-oriented diffuTora: arXiv preprint and Weizhi Wang. sion transformer for video generation. arXiv:2407.21705, 2024. 3, 5 [87] Rui Zhao, Yuchao Gu, Jay Zhangjie Wu, David Junhao Zhang, Jia-Wei Liu, Weijia Wu, Jussi Keppo, and Mike Zheng Shou. MotionDirector: Motion customization of text-to-video diffusion models. In ECCV, pages 273290. Springer, 2025. 3 [88] Yang Zheng, Adam W. Harley, Bokui Shen, Gordon Wetzstein, and Leonidas J. Guibas. PointOdyssey: largescale synthetic dataset for long-term point tracking. In ICCV, 2023. [89] Haitao Zhou, Chuang Wang, Rui Nie, Jinxiao Lin, Dongdong Yu, Qian Yu, and Changhu Wang. TrackGo: flexible and efficient method for controllable video generation. arXiv preprint arXiv:2408.11475, 2024. 3, 5 Motion Prompting: Controlling Video Generation with Motion Trajectories"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Implementation Details A.1. Architecture and Training We train our model for 70,000 steps using Adafactor [58] with learning rate of 1 104. We do not use any learning rate decay. For the ControlNet, we copy Lumieres encoder stack, add in zero convolutions as in [84], and replace the first convolutional layer with new layer that accepts H C conditioning signal. From the constraints of the base architecture, we have = 80 and = = 128. We set = 64. During training, we sample the number of input tracks uniformly from 1000 to 2000 inclusive. For each track we randomly assign sinusoidal positional encoding [68], of 64 dimensions, by sampling integers without replacement from 0 to 16384 the maximum number of tracks for 128 128 image, and using the corresponding positional embedding for that integer. Note that the encoding is completely randomly assigned. In particular, its spatial location has no bearing on its embedding. All sampled videos are passed through Lumieres spatial super resolution (SSR) model, resulting in 1024 1024, 80 frame video at 16 frame per second, for total of 5 seconds. We use Lumieres SSR model as is, without finetuning it for motion conditioning, as we find that the 128 128 output of the base model already contains all of the motion conditioned dynamics. Data. We train on an internal dataset of 2.2 million videos. We precompute trajectories on this dataset by center cropping each video to square, resizing it to 256 256, and then running BootsTAP [13] with dense gird of query points, resulting in 16,384 tracks per video. During training, video is sampled, and then tracks are randomly sampled from this dataset. During the Lumiere fine-tuning, videos are resized to match Lumieres 128 128 input and output size. A.2. Qualitative Results We provide additional details for qualitative results in Tab. A1, including text prompt and licensing information. All images and videos are used with permission and under open and free licenses. In addition, as can be seen we construct text prompts to describe the scene but not the motion, in order to limit the influence of text conditioning on the motion as much as possible. A.3. Mouse GUI We record mouse motions through simple HTML GUI, It consists of canvas elwhich is shown in Fig. A1. Figure A1. Mouse Motion GUI. We show screenshot of the GUI that we use to record mouse motions. For more information please see Appendix A.3. ement which displays the first frame conditioning, labels that indicate the position of the mouse in the canvas, and whether or not it is currently being clicked, button to start the recording, countdown timer which gives three seconds before recording starts, and second countdown timer which shows when the recording will end. We record 80 frames of mouse input to match the five seconds of video that our model outputs at 16 frames per second. For each frame we record the mouse (x, y) position, and flag indicating whether the mouse is being clicked. A.4. Interacting with and Drag Editing Images In order to feed mouse motions to our model, we create grid of tracks that is centered on the mouse whenever it is being dragged. The user may choose the stride of these tracks, and the size of the grid. We use square grid of tracks for simplicity. In addition, user may choose to have the tracks persist, in that before and after the mouse drag the tracks remain. This is useful in cases where objects should stay in place after drag. user may also place down grid of tracks to pin the background in place. Note Table A1. Figure Details. We provide details about qualitative samples shown in our figures, including text prompts fed to the model and licensing information. In general, these are sorted by the order that they appear in the paper, moving from left to right, top to bottom. Description two elephants owl brown bear squirrel Figure Fig. 1, Fig. 5 Text Prompt elephants Fig. Fig. 1 Fig. 1 close up of great horned owl brown bear squirrel sitting on the ground in the woods golden retriever Fig. 1, Fig. 7 golden retriever laying in the grass man (motion source) Fig. 1, Fig. 8 macaque sand woman parrot cow skull stool hot air balloons arches roses cat frog horse Earth (motion source) panda Fig. 1, Fig. 8 Fig. 1, Fig. 3 Fig. 1, Fig. 3 Fig. macaque monkey sand woman close up of green parrot Fig. 3, Fig. 4, Fig. 9 highland cow standing in grassy scottish wilderness Fig. 4 Fig. 4 Fig. 4 Fig. 5 Fig. 5 Fig. Fig. 6 Fig. 7 Fig. 8 Fig. 8 white skull on black background living room serene scene of multiple hot air balloons floating over Cappadocia, Turkey, during sunset arches in arches national park, with shrubbery in the foreground and bright blue sky red rose cat close up of frog horse panda monkey (motion source) Fig. 8 trees chess Fig. 8 Fig. 9 birds eye view of trees close-up of chessboard with strong depth of field. The white king piece is in focus, surrounded by black pawns Source Unsplash Unsplash Unsplash Unsplash Unsplash URL License link Unsplash link Unsplash link Unsplash link Unsplash link Unsplash License URL license license license license license private correspondence permission granted Unsplash Unsplash link Unsplash link Unsplash license license private correspondence permission granted Unsplash Unsplash Unsplash Unsplash Unsplash Unsplash Unsplash Unsplash Unsplash Unsplash Pexels Unsplash Pexels Unsplash link Unsplash link Unsplash link Unsplash link Unsplash link Unsplash link Unsplash link Unsplash link Unsplash link Unsplash link Unsplash link Pexels link Unsplash link Pexels link Unsplash license license license license license license license license license license license license license license Unsplash link Unsplash license that this setup is identical to how we obtain the drag-based image editing results. A.5. Geometric Primitives To make spherical tracks we take points on sphere and follow them as the sphere is spun. This gives us trajectory of 3D points, which when orthographically projected gives us 2D tracks. The density of the points, the radius of the sphere, and the location of the sphere are determined by the user. Mouse motions are converted to sphere spins by rotating the sphere through single axis such that the initial mouse location matches with the current mouse location at each frame. This uniquely defines rotation and ensures that the sphere tracks the mouse. A.6. Camera Control In order to obtain camera control, we run monocular depth estimator on the first frame input to the model. This gives us camera intrinsics as well as depths, allowing us to unproject into point cloud. We then project this point cloud onto sequence of camera poses forming the desired camera trajectory, resulting in 2D point tracks. In addition, we run z-buffering to determine occlusions, where only the closest point that has been projected to some neighborhood is visible while all other points in that neighborhood are occludedunless that point is sufficiently close to the visible point. This requires choosing radius for the neighborhood size, and threshold for point to remain visible if it is close enough to the visible point. Both are set manually to constant values that we find to work well for all examples. We also discuss translating mouse motion to camera motion. This is done by having the camera move in such way that the mouse is always above the same point. Because this is underdetermined, we also add the constraint that the camera should stay fixed in the vertical plane. Note that this is not the only constraint possible. Other constraints may restrict the camera to the surface of sphere around the scene for example. A.7. Track Sparsity For camera control and motion transfer motion prompts, we obtain dense set of tracks. Empirically, we find that it is helpful to randomly subsample these tracks, as using too many tracks suppresses the video models learned priors from working, while using too few affords too little control. Somewhere in the middle is sweet spot. For example, for the majority of the depth-based motion prompts, we use 1024 tracks, which we find offers good balance between control and emergent video prior effects. In other cases, such as transferring the motion of the persons face in Fig. 8, we find that fewer tracks is helpful in dealing with misalignments between the source video and the input first frame. Finally, for out-of-domain motion transfer as in the monkey chewing example in Fig. 8, we find that very dense tracks help. We use 1500 tracks, as we need lot of control to apply such an unnatural motion to the first frames. A.8. Davis Eval We conduct quantitative evaluation of first frame, text, and track conditioned video generation using the DAVIS validation dataset, with subset of results in Tab. 1 and full results in Tab. A2. The validation dataset contains 30 videos from wide range of scenes, involving subjects from sports to humans to animals to cars. In order to create inputs for the models, we extract tracks using BootsTAP [13]. First frame inputs are square crops of the first frames of the videos, and text prompts are the titles of the videos given by the dataset and typically consist of word or two. For each evaluation for given number of tracks, we randomly sample that number of tracks for conditioning. To ensure fair comparison, we make the following acIn addition to first frame, commodations for baselines. tracks, and text prompt, DragAnything requires segmentation masks for objects that the tracks move. To get this, we use the provided ground truth segmentations in Image Conductor is finetuned verthe DAVIS dataset. sion of AnimateDiff and is trained on videos of resolution 384 256. We initially gave the model 256 256 images, and found that we got reasonable results. However, we experimented with reflection padding the input frame to 384 256 and cropping the output, which gave slightly better results which we report. A.9. Human Studies To perform the human studies, we handcraft 30 inputs with diverse image subjects and input motions. Motions consist of single uninterrupted trajectory. Text prompts are designed to describe the image, but not the desired motion, to factor out the influence of text as much as possible. DragAnything requires masks, which we obtain by running SAM [36] on the first frame with the initial location of the track as the query point. For our method, we turn the trajectory into grid of tracks as described above. We then feed these inputs to the models and take single random sample. We follow the same protocol as above for Image Conductor. This results in 30 samples for each model and 90 samples in total. We run two alternative forced choice (2AFC) test between our model and the baselines. We display sample Figure A2. Test and Train Metrics. Here we plot out training loss, along with PSNR, SSIM, LPIPS, and EPE on our DAVIS test set. Note that there is no correlation between the training loss and the test metrics, and that the test metrics show no signs of improvement until step 20,000 at which point the network learns quite rapidly. from our method and sample from the baseline in random order with video of the corresponding motion conditioning in the middle, visualized as moving red dot. Participants are then asked three questions. Verbatim, we ask (1) Which video better follows the motion of the red dot? (2) Which video has the more realistic motion? (3) Which video is of higher visual quality? These questions are designed to measure the adherence of the motion to the conditioning, the quality of the motion, and the overall visual quality of video, respectively. This results in total of 180 questions. We recruit participants for our study through Amazon Mechanical Turk (MTurk). To ensure responses are of high quality, we add three vigilance questions with clearly correct answers. We discard all responses that fail any of these three questions. Each question is conducted as separate study, and the resulting number of participants are = 103, = 103, and = 115 for each question respectively. This results in total of 19,260 answers. tracks. We then feed the first frame of the input video and the magnified tracks to our model. We show results, along with space-time slices, in Fig. A3. We found that smoothing was necessary to reduce noise in the estimated tracks. As result the magnified tracks are not exactly at the specified magnification factor, but nonetheless are qualitatively useful in revealing subtle motions. We expect more accurate point tracking algorithms will remove the need for this smoothing step. Figure A3. Motion Magnification. We show the result of using our model to perform motion magnification. We show the first frame of two videos, and space-time slices through the blue line at different magnification factors. B. Training Observations In training, we observe similar behavior as noted in ControlNet [84] and ControlNext [48]: 1) training loss does not directly correlate with model performance, and 2) sudden convergence where in few epochs the model goes from not adhering to control signal to full adherence. ControlNext identifies both of these behaviors as coming from the zero initialization and offers cross normalization as potential solution. We believe this and other future control scheme is promising avenue for future work in track conditioned video generation. We show training loss and test metrics in Fig. A2. As can be seen, the training loss is fairly inscrutable, while the test losses do not begin to decrease until step 20,000. C. Full Quantitative Results In Sec. 5 we present DAVIS evaluation results for = {1, 16, 512, 2048}. In Tab. A2 we present results for = 4 and = 64 as well, which we omit from the main paper for brevity. Table A2. Quantitative Evaluations. We evaluate the appearance (PSNR, SSIM, LPIPS, FVD) and motion (EPE) of generated videos using the validation set of the DAVIS dataset. Please note that each method is trained from different base model. # Tracks Method PSNR SSIM LPIPS FVD EPE = 1 = 4 = 16 = = 512 = 2048 Image Conductor 11.468 14.589 DragAnything 15.431 Ours Image Conductor 12.017 15.040 DragAnything 15.820 Ours Image Conductor 12.184 15.119 DragAnything 16.618 Ours Image Conductor 12.513 14.499 DragAnything 18.000 Ours Image Conductor 11.902 15.055 DragAnything 18.968 Ours Image Conductor 11.609 14.845 DragAnything 19.327 Ours 0.145 0.241 0.266 0.176 0.272 0.319 0.175 0.305 0.405 0.180 0.274 0. 0.132 0.289 0.583 0.120 0.286 0.608 0.529 0.420 0.368 0.499 0.397 0.353 0.502 0.378 0.319 0.503 0.393 0. 0.524 0.381 0.229 0.538 0.397 0.227 1919.8 19.224 1544.9 9.135 1445.2 14.619 1735.0 18.921 1497.2 8.946 1207.7 12.985 1838.9 24.263 1282.8 9.800 1322.0 8.319 1947.7 26.316 1342.0 10.642 4.127 951. 1966.3 30.734 1379.8 10.948 4.055 688.7 1890.7 33.561 1468.4 12.485 3.887 655.9 D. Motion Magnification One additional application of our model is motion magnification [38, 45, 47, 69, 75]. This task involves taking video with subtle motions and generating new video in which these motions have been magnified, so that they are easier to see. We do this by running tracking algorithm [12] on an input video, smoothing the tracks by applying Gaussian blur over space and time, and then magnifying the resulting"
        }
    ],
    "affiliations": [
        "Brown University",
        "Google DeepMind",
        "University of Michigan"
    ]
}