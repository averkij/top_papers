{
    "paper_title": "Lyra: An Efficient and Speech-Centric Framework for Omni-Cognition",
    "authors": [
        "Zhisheng Zhong",
        "Chengyao Wang",
        "Yuqi Liu",
        "Senqiao Yang",
        "Longxiang Tang",
        "Yuechen Zhang",
        "Jingyao Li",
        "Tianyuan Qu",
        "Yanwei Li",
        "Yukang Chen",
        "Shaozuo Yu",
        "Sitong Wu",
        "Eric Lo",
        "Shu Liu",
        "Jiaya Jia"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As Multi-modal Large Language Models (MLLMs) evolve, expanding beyond single-domain capabilities is essential to meet the demands for more versatile and efficient AI. However, previous omni-models have insufficiently explored speech, neglecting its integration with multi-modality. We introduce Lyra, an efficient MLLM that enhances multimodal abilities, including advanced long-speech comprehension, sound understanding, cross-modality efficiency, and seamless speech interaction. To achieve efficiency and speech-centric capabilities, Lyra employs three strategies: (1) leveraging existing open-source large models and a proposed multi-modality LoRA to reduce training costs and data requirements; (2) using a latent multi-modality regularizer and extractor to strengthen the relationship between speech and other modalities, thereby enhancing model performance; and (3) constructing a high-quality, extensive dataset that includes 1.5M multi-modal (language, vision, audio) data samples and 12K long speech samples, enabling Lyra to handle complex long speech inputs and achieve more robust omni-cognition. Compared to other omni-methods, Lyra achieves state-of-the-art performance on various vision-language, vision-speech, and speech-language benchmarks, while also using fewer computational resources and less training data."
        },
        {
            "title": "Start",
            "content": "Lyra: An Efficient and Speech-Centric Framework for Omni-Cognition Zhisheng Zhong1 Chengyao Wang1 Yuqi Liu1 Senqiao Yang1 Longxiang Tang1 Yuechen Zhang1 Jingyao Li1 Tianyuan Qu1 Yanwei Li1 Yukang Chen1 Shaozuo Yu1 Sitong Wu1 Eric Lo1 Shu Liu2(cid:66) Code: https://github.com/dvlab-research/Lyra Equal contribution (cid:66) Jiaya Jia 2, Corresponding author CUHK1 SmartMore2 HKUST3 4 2 0 2 2 1 ] . [ 1 1 0 5 9 0 . 2 1 4 2 : r Figure 1. Overview of Lyra. Lyra shows superiority compared with leading models in the following aspects: 1. Stronger performance. Lyra achieves state-of-the-art results across variety of modalities understanding and reasoning tasks. 2. More versatile. Lyra can directly handle images, videos and audio tasks even lasting several hours. 3. More efficient. Lyra is trained with less data and increases the speed, reduces memory usage, making it suitable for latency-sensitive and long-context multi-modality applications."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction As Multi-modal Large Language Models (MLLMs) evolve, expanding beyond single-domain capabilities is essential to meet the demands for more versatile and efficient AI. However, previous omni-models have insufficiently explored speech, neglecting its integration with multi-modality. We introduce Lyra, an efficient MLLM that enhances multimodal abilities, including advanced long speech comprehension, sound understanding, cross-modality efficiency, and seamless speech interaction. To achieve efficiency and speech-centric capabilities, Lyra employs three strategies: (1) leveraging existing open-source large models and proposed multi-modality LoRA to reduce training costs and data requirements; (2) using latent multi-modality regularizer and extractor to strengthen the relationship between speech and other modalities, thereby enhancing model performance; and (3) constructing high-quality, extensive dataset that includes 1.5M multi-modal (language, vision, audio) data samples and 12K long speech samples, enabling Lyra to handle complex long speech inputs and achieve more robust omni-cognition. Compared to other omni-methods, Lyra achieves state-of-the-art performance on various vision-language, vision-speech, and speech-language benchmarks, while also using fewer computational resources and less training data. With the rapid evolution in Large Language Models (LLMs) [20, 25, 42, 55, 57], empowering the impressive capabilities for multi-modality inputs is becoming an essential part of current Multimodal Large Language Models (MLLMs). However, most current MLLMs are limited to just two modalities: either vision-language [2, 11, 24, 28, 29, 31, 34, 76] or speech-language [10, 13, 65]. OpenAIs recent release of GPT-4o [43], an advanced omni-modal model, has reignited interest in intelligent assistants capable of fine-grained visual perception, understanding spoken instructions, and generating vocal responses simultaneously. It highlights strong demand for MLLMs that integrate more functions and modalities, such as visual, language, speech, sound, and even other new abilities [6, 16, 63, 71]. Based on our study, most existing omni-models [6, 13, 16, 71] primarily focus on the relationship between speech and text, without exploring connections between speech and other modalities, such as vision. Consequently, speechrelated evaluation metrics are typically limited to text. In this paper (Sec. 4.3), we observe that strong performance in the speech-text modality does not necessarily imply good performance in the speech-vision modality. Thus, we suggest that omni-model evaluation should be speech-centric, expanding its involvement with additional modalities. 1 To further enhance the speech capabilities of MLLMs, we inevitably encounter the following challenges: First, larger datasets (e.g., the extensive data required to train models like LLaMA3 [12] and Qwen2-VL [60]) are needed for both previous modalities and speech. Second, there is clear trend toward increasing context length across modalities. More long-context benchmarks for specific modalities are being proposed, including long-document [5, 8] and long-video tasks [15, 32, 62, 66, 73]. Last, building sufficiently powerful model may demand significant financial and computational resources, which raises environmental concerns related to high carbon emissions. Combining the above three points, we propose Lyra, an efficient and speech-centric framework for omni-cognition: Leveraging existing open-source large models. We efficiently start with powerful LLMs and VLMs, like LLaMA3 [12] and Qwen2-VL [60], which already demonstrate strong multi-modal capabilities. Through our multimodality LoRA module, we can effectively preserve certain strong capabilities of open-source large models in specific modalities with minimal training data, while simultaneously developing their abilities in the speech modality. Enhancing information interaction between modalities, especially within the speech modality. 1) Considering the implicit correspondence between speech and text, we propose latent cross-modality regularizer. 2) Based on instructions, we identify potential redundancy in context token information across multiple modalities. We further propose latent multi-modality extractor to mine informative tokens, which brings significant advantages in training speed, inference speed and GPU memory efficiency. High-Quality Datasets for Omni-Cognition. Centered on speech, we have constructed two types of high-quality datasets: To enhance the models speech capabilities, we collect and generate multi-modal dataset of 1.5M textimage-speech samples from diverse public sources, ensuring rich and varied data foundation; To handle longer speech inputs and demands, we are the first to construct long speech dataset comprising 12K samples. Through training, our model achieves robust omni-cognitive abilities and can handle long speech inputs lasting several hours. With these three improvements, Lyra offers the following advantages  (Fig. 1)  . More versatile: As shown in Table 1, Lyra now supports both sound and speech understanding and generation, while also handling more complex long speech cases. More efficient: Lyra achieves faster training and inference speed across speech, image, and video tasks. Compared to previous models, Lyra has smaller model size and is trained with less data. Stronger: Lyra demonstrates enhanced omni-comprehension capabilities over previous MLLMs, achieving state-of-the-art performance in vision-language and vision-speech and speechlanguage tasks simultaneously. Function Method Vision Audio Image Video SU SG LS Sound Vision Audio Omni LLaVA-OV Intern-VL Mini-Gemini Qwen-Audio Mini-Omni LLaMA-Omni Intern-Omni VITA Any-GPT EMOVA Lyra Table 1. Function comparison of related work. SU, SG, and LS represents speech understanding, speech generation, and long speech support, respectively. 2. Related Work Multi-modal Large Language Models. Recent advancements in Large Language Models (LLM) and Multimodal Large Language Models (MLLMs) have pushed the boundaries of human-computer interaction, expanding their capabilities from text-based tasks to complex multi-modality scenarios. Large Language Models, like GPTs [42], LLaMA [12, 57] and Qwen [4, 67], have demonstrated strong capabilities in textual understanding and generation. Building on these foundations, Vision Language Models [28, 3136, 60, 61, 68] extend LLMs with visual perception capabilities, leveraging advanced encoders [47] and high-resolution techniques to interpret visual inputs. Speech Language Models (SLMs) [49], including SpeechGPT [72] and LLaMA-Omni [13], have introduced real-time speech understanding and generation, with advanced models enabling control over speech styles. Moving further, MLLMs [63] such as AnyGPT [71], VITA [16] and EMOVA [6], integrate vision, text, and audio within unified architecture, enabling robust interaction across diverse modalities. The abilities and modalities of previous leading MLLMs are listed in Table 1. In contrast, Lyra tackles complex scenarios, enabling seamless, dynamic multimodal interactions for rich, real-time AI experiences. Token Reduction for MLLMs. Token reduction techniques aim to improve the efficiency of LLMs and VLMs by minimizing redundant tokens during inference and training. In LLMs, methods like StreamingLLM [64] and FastGen [17] optimize memory usage by selectively retaining essential tokens, while techniques like H2O [75], ScissorHands [37] and Quest [53] use attention-based scoring to prioritize valuable tokens. In VLMs, approaches such as FastV [7] reduce visual tokens to tackle the high computational cost of image processing. Lyra extends token reduction to more modalities, such as video and speech, where token lengths tend to increase in long-context scenarios. By evaluating the relationship between context and instruction tokens, we progressively discard redundant tokens to enhance efficiency without compromising performance. Figure 2. The framework of Lyra. Lyra supports multi-modal inputs. When the data contains speech modality, we use the latent cross-modality regularizer to assist. Data from each modality is processed through encoders and projectors before being sent into the LLM. Within the LLM, multi-modality LoRA and latent multi-modality extraction modules operate synergistically, facilitating the simultaneous generation of both speech and text outputs. 3. Lyra As shown in Fig. 2, the overall architecture of Lyra is composed of four main components: latent cross-modality regularizer, multi-modality LoRA, latent multi-modality extractor, and streaming generation. Lyra is designed as unified framework, with each component being easily and efficiently extendable to support additional modalities and functionalities. In this paper, Lyra primarily focuses on the three modalities of audio (speech, sound), vision, and language. Therefore, in the following sections of this section, We will provide detailed introduction to the mechanisms of the following modules: latent cross-modality regularizer, multi-modality LoRA, and latent multi-modality extractor. Due to space limitations, streaming speech-text generation will be detailed in the appendix. Since speech contexts tend to be lengthy, the integration of long speech capabilities will be discussed at the end of this section. To ensure clarity in the following discussion, lets define some key notations: the X[i] be the token of modality-i. For example, X[text] represents the text token, X[image] represents the image token, X[video] represents the video token, X[speech], X[sound] represents the speech and sound token, respectively. 3.1. Latent Cross-Modality Regularizer For MLLMs, it is crucial to achieve effective alignment between tokens from each modality and LLM. As the view from the speech modality, there is high degree of informational overlap with the text modality. Specifically, considering only semantic information, speech can be converted into its corresponding transcribed text. However, our experiments have shown that using speech with naive alignment training as the instruction (S+I, for speech instruction, for image context) generally yields less effective results compared to using transcribed text (T+I, for text instruction, for image context): TextVQA (S+I) 76.7(-2.8) TextVQA (T+I) MM-Vet (S+I) 79.5 53.1(-8.0) MM-Vet (T+I) 63.1 To address this, we aim to make the tokens from the speech modality as similar as possible to the corresponding transcribed text tokens before feeding them into LLM, thereby minimizing the loss of relevant information. Another challenge arises from the variable length of speech: sentence can be spoken quickly or slowly while retaining the same meaning in the text modality, leading to length discrepancies. In general, the tokens produced by speech encoder (like Whisper) tend to be much longer than the corresponding text tokens (speech-to-text, STT), i.e., X[speech] RdL , X[STT] RdS, > S, is the token dimension. We define the latent distance between the l-th speech token and the s-th SST token as: dist(l, s) = log softmax(X[speech],lX (cid:104) (cid:105) [STT],s/τ ) , (1) Where τ is the temperature. To get the minimum distance between two different length tokens, we follow the Dynamic Time Warping (DTW) algorithm: Dl,s = dist(l, s) + min{Dl,s1, Dl1,s, Dl1,s1}. (2) The illustration is shown in Fig. 3. We define the latent cross-modality regularization loss as LLCMR = 1 L+S DL,S. Finally, the total loss of the system becomes the combination of two losses: Ltotal = LCE + λLLCMR, where LCE is the cross-entropy loss on LLM output, and λ is loss weight hyper-parameter. 3 Figure 3. Illustration of the DTW algorithm in our alignment. Our goal is to make the speech tokens as similar as possible to the corresponding translated tokens. 3.2. Multi-Modality LoRA Pipeline The current open-source VLM (such as Qwen2-VL) is already quite powerful. With limited data quantity and quality, jointly training vision-speech-language modalities may reduce the models original capabilities. Therefore, we adopt an efficient multi-modality LoRA [23] pipeline. Revisiting the notation introduced at the beginning of this section, we represent X[i] as the token of modality-i. The modality-i can be text, image, video, speech token, and sound. Since our model involves joint training across multiple modalities, here we define X[M] can be any combination of the above different modality tokens. The output of multimodality LoRA can be written as: = (cid:0)B[M]A[M] + W(cid:1)X[M], (3) where is the original weight of LLM, A[M] and B[M] is low-rank adapter of combination-M. During training, our Multi-Modality LoRA is integrated into each layer of the LLM. Because each modality is trained using LoRA, the process is highly efficient, achieving strong performance with minimal data while preserving much of the original models visual capabilities. 3.3. Latent Multi-Modality Extractor As MLLMs expand their functionality and accommodate longer contexts, efficiently using tokens within limited context window becomes essential to address the longcontext problem. We now consider the relationship between non-text modalities and the text modality. In response to given question, many tokens from non-text modalities may be largely irrelevant to the question itself. For example, as shown in Fig.2, only subset of image tokens is relevant to the instructed question. Similarly, for the video and speech modality, only portion of tokens from video and speech directly corresponds to the question instruction. We observe that in LLM training, the long-context effect brought by high-resolution images, lengthy videos, and long audio (in the following subsection) often includes tokens with limited relevance, which not only increases the computational load for training and inference but also consumes unnecessary memory. To address this, we propose dynamically selecting multi-modality tokens based on their Figure 4. Long speech capability integration pipeline. (Middle) Our pipeline for generating instruction-following data for long speech. (Top) The proportion of question and speech categories in our long speech SFT dataset. (Bottom) Our long speech SFT pipeline. Long speech segments will be clipped and flattened. relevance to the text query, discarding redundant multimodality tokens. To achieve this, we introduce latent multi-modality information extraction strategy. Concretely, instead of applying this strategy to every layer, we implement block-based manner. Suppose the LLM consists of mn layers; we divide them into blocks of layers each, resulting in blocks. At the final layer of each block, we apply our following information extraction strategy, which evaluates the similarity between the attention scores of tokens from each modality and the question text tokens. We represent this with the following equation: (cid:32) topk softmax (cid:32) Q[text]K [text] (cid:33)(cid:33) , (4) where Q[text] denotes the query corresponding to text modality tokens, and [text] represents the key corresponding to tokens from other modalities. For clarity, lets assume that the length of multi-modality tokens [text] is L. After passing through each block, we retain only ρL multi-modality tokens. From block-wise perspective, the token length decays exponentially, significantly reducing computational and memory costs. similar mechanism exists in the brains neural processing of complex information [50]. Notably, text tokens can be extended to instruction tokens for other modalities, such as speech. This extractor enables us to handle long speech more efficiently. 3.4. Long Speech Capability Integration There is growing trend toward increasing the length of single-modality content processed by models, such as long text and long video inputs in MLLMs. However, existing MLLMs are limited in handling long speech due to the constraints of speech encoders. Specifically, models like Intern-Omni [44], VITA [16], and LLaMA-Omni [13] use Whisper-like encoders, which restrict audio input to around 4 30 seconds. VITA and Mini-Omni, which employ more complex encoders, can process at most one minute of audio input. This limitation largely stems from the lack of suitable long speech SFT datasets and appropriate preprocessing methods. To address this issue, we developed the first SFT dataset for long speech understanding, aimed at enhancing model capabilities in handling extended audio content. Our dataset comprises about 12K long-form audio recordings, with durations ranging from several minutes to two hours. These recordings were collected from diverse YouTube sources, including informational videos, interviews, and speeches, covering wide range of topicsfrom humanities and current events to technology and society. With related transcripts, we utilized LLM to generate question-and-answer pairs derived from the captions and instructions. These questions cover summarization and other types of inquiries that encourage comprehensive understanding of long speech content. The overall question distribution and details are illustrated in Fig. 4. Once the dataset was ready, we tackled the challenge with the speech encoder. Inspired by high-resolution image segmentation methods like LLaVA-NeXT [36], we adopted similar strategy to better handle the speech encoder for long audio processing (illustrated in Fig. 4). However, unlike previous speech cases, new challenge emerged: for naive Whisper-v3 encoder, 30-second audio clip is encoded into 1,500 tokens. Under typical short speech scenarios, an LLM can handle 1,500 tokens comfortably. When we consider long speech cases, such as two-hour audio clip, this would result in an astonishing 360,000 tokens, which is beyond our processing capacity. Thus, it is essential to consider compression techniques on speech tokens. The experimental results are presented as follows: #(Token) TextVQAS MM-VetS 100 75.9% 55.3 150 76.8% 54.4% 300 77.8% 56.3% 500 78.0% 58.8% 1500 76.8% 58.9% Experimental results indicate that having higher number of speech tokens provides certain benefits. However, beyond certain threshold, the performance improvement becomes quite limited. Taking into account both computational costs and model performance, we ultimately decided to use the 300 compressed tokens version for extending the model to handle long speech cases. 4. Experiments In this section, we conduct speech-centric evaluation, assessing its integration with image, video, and text modalities. we first outline our experimental framework, commencing with the experimental setup. Subsequently, we compare Lyra with leading methods on various benchmarks and qualitative results. Detailed component wise analysis (based on Lyra-Base) is given at the end of this section. More experiment details and results refer to our Appendix. 4.1. Experimental Setup Implementation Details. In this study, we instantiate Lyra with the following designs and settings: 1. Strong vision encoders and LLMs: Building on the previously applied vision model Qwen2-VLs ViTs and LLMs [60], they can now process images of any resolution, dynamically converting them into variable number of visual tokens. We have also designed three versions: For Lyra-Mini, we use Qwen2-VL 2B. For Lyra-Base, we apply Qwen2-VL 7B. For Lyra-Pro, we choose Qwen2-VL 72B. 2. Efficient audio encoder: We adopted Whisper-largev3 [48] (Lyra-Base and Lyra-Pro) and its light-weight version, Whisper-large-v3-turbo (Lyra-Mini), which have been trained on large amount of audio data and has strong capabilities in speech recognition and translation. 3. Four stage training for omni-cognition (refer to our appendix for specific details): In the first stage, we conduct text-to-speech pretraining to train the speech encoder. In the second stage, we perform joint training on text, image, and speech modalities to train the LLM along with the corresponding projectors. In the third stage, we train the LLM to extend the models capability in handling long speech. In the fourth stage, we train our speech generator, enabling the model to simultaneously output text and corresponding audio in streaming manner. Datasets and Evaluations. For model optimization, we construct high-quality data for omni-understanding and speech generation. 1. High-quality multi-modal dataset: Based on the MiniGemini SFT [31] dataset, we carefully collected and extended high-quality multi-modal dataset that covers common scenes and document images and speeches. It contains about 1.5M open-source image-speech, text-image, and text-speech instruction samples. To enhance the generalization of speech modality, we utilize ChatTTS [1] with varying configurations to generate different audios. 2. Long speech SFT dataset: As mentioned in Sec. 3.4, we constructed delicate long speech SFT dataset for long speech capability integration with 12K samples. The dataset involves distribution of longer audio durations and covers wide range of domains. 3. Evaluation: Unlike the previous omni-model [6, 16], which only tested text-to-speech capabilities, we employed more omni comprehensive evaluation that covers interactions across image, video, text, and speech modalities. 4.2. Main Results Quantitative Results. In the quantitative analysis experiments, we primarily compare our model with current leading VLMs, such as Mini-Gemini [31], LlavaOV [28], Intern-VL2 [9], and SLM, like Mini-Omni [65], SALMONN [52], Qwen2-Audio [10], and Omni models in5 Omni Comparison Text-Image Text-Video Image-Speech Text-Speech Method Params. TextVQA MME MM-Vet VideoMME MVBench Egoschema TextVQAS DocVQAS ChartQAS LibriSpeech Mini-Gemini LLaVA-OV Intern-VL2 Mini-Omni SALMONN Qwen2-Audio Intern-Omni VITA EMOVA Lyra-Mini Lyra-Base Lyra-Pro 8B 7B 8B 7B 13B 8B 8B 66B 14B 3B 9B 74B 71.9 65.4 77.4 - - - 80.6 - 82.0 78.3 82.6 83.5 1989 1998 2211 - - - 2210 2097 2205 1884 2335 53.5 57.5 60.0 - - - 60.0 41.6 55.8 51.2 63.5 71.4 - 58.2 54.0 - - - - 59.2 - 55.0 62.8 69.9 - 56.7 66.4 - - - - - - 62.5 67.2 72. - 60.1 - - - - - - - 54.1 63.2 75.8 - - - - - - 69.1 - - 73.4 80.0 81.0 - - - - - - 79.9 - - 74.8 85.5 89. - - - - - - 56.0 - - 40.7 61.0 68.5 - - - 4.5 2.1 1. - 8.1 4.0 2.1 2.0 1.8 Table 2. Omni-comparison on vision-language-speech benchmarks. BenchS indicates that it uses speech instruction as the input. cluding Intern-Omni [44], AnyGPT [71], VITA [16], and EMOVA [6]. The input modalities we compare are also the most widely used, including text-image, text-video, imagespeech, and text-speech. Detailed results are presented in Table 2. In calculating the total parameters of the model, we considered all modality-specific encoders, projectors, and related components. Our model includes three versions: mini version (3B), based version (9B), and pro version (74B). Benefiting from multi-modality LoRA and Qwen2VL, our model maintains relatively high performance in text-image and text-video tasks. For the speech modality, as we mentioned in Introduction part, previous models have evaluated the speech modality rather crudely, without extensively testing metrics for interactions between the speech modality and other modalities. Our model comprehensively outperforms existing omni models in both imagespeech (with an improvement of approximately 9%) and text-speech (with an improvement of approximately 2%) tasks. Additionally, our model is more lightweight, requiring fewer training samples. Qualitative Results. To ascertain the omni comprehension prowess of Lyra in real world settings, we apply it to variety of understanding and reasoning tasks in the bottom left part of Fig. 1 and our Appendix. By contrast, Lyra can well solve more complex multi-modality cases. 4.3. Component-Wise Analysis Latent Cross-Modality Regularizer. We first delve into the proposed latent cross-modality regularizer and report results in Table 3. It is clear that the model achieves significant gains for both speech-image inputs and text-image inputs, with the regularizer integrated as an assistance between speech modality and text modality. In the training of the image-speech-text tri-modal model, introducing the LLCMR significantly enhances the performance of both image-speech and image-text alignments, reducing the gap between them. We also observe that, with only LCE, imagetext performance lags behind image-speech by 8% on the MM-Vet benchmark. However, the performance of speechtext remains relatively unchanged whether using the CE loss or joint loss. Therefore, previous omni models [6, 16] that assessed the speech modality just based on the LibriSpeech [45] WER metric for speech-text alignment are rather arbitrary. We need to evaluate the performance of the speech modality alongside other modalities to accurately measure the effectiveness of omni-models. This also demonstrates the effectiveness of our LLCMR. Latent Multi-Modality Extractor. For the latent multimodality extractor (LMME) module, we focus primarily on its efficiency and effectiveness in multi-modal tasks. First, we analyze its efficiency, with specific results summarized in Tables 4a and 4b. In Table 4a, we vary the token length, ranging from 211 to 217 (under long-context case). We denote LMME(n, ρ) as splitting the LLM into blocks, with each block retaining the top ρ proportion of the most important tokens. We compare three models: the baseline, LMME(4, 0.8), and LMME(4, 0.7). The key metrics examined include Prefill Time, tokens-per-second (TPS), and memory usage on the A100 GPUs. Under the baseline model, multimodal content exceeding 215 tokens results in out-of-memory (OOM) errors. In contrast, our models LMME(4, 0.8) and LMME(4, 0.7) still have room for 217 tokens, consuming over 50% less memory. Additionally, the Prefill Time is significantly shorter than the baseline model (by 100%), and the token generation speed is also notably faster (by 50%). In Table 4b, we primarily examine the improvement in training speed. We evaluate it using our proposed Lyra SFT and long-speech SFT dataset, which contains 1.5M samples and 12K samples, respectively. From the table, our LMME can reduce training time by more than 50% compared to the original. Since the context in the long-speech dataset is generally longer than it in the 1.5M dataset, the acceleration effect becomes even more pronounced. 6 Effectiveness TexVQA MM-Vet LibriSpeech Method Overall Short Medium Long Type Baseline LCE LCE + λLLCMR S+I - 76.7 77. T+I 82.3 79.5 80.1 S+I - 53.1 58.1 T+I 62.8 61.1 62. S+T - 1.9 2.0 Baseline (7B) Baseline + subtitle LSCI (7B, solve 33%) Baseline + LSCI GPT-4o [43] + subtitle 62.8 64.4 78.6 66.2 77. 73.8 76.2 89.8 75.7 82.8 62.3 63.4 77.7 64.0 76.6 52.3 53.4 74.8 58.9 72. Table 3. Latent cross-modality regularizer. With our regularizer, the performance of both the speech-image and text-image modalities improves, and the gap narrows. Metric # (Tokens) 211 212 214 215 216 217 Prefill(s) TPS Memory Baseline LMME(4, 0.8) LMME(4, 0.7) Baseline LMME(4, 0.8) LMME(4, 0.7) Baseline LMME(4, 0.8) LMME(4, 0.7) 0.19 0.17 0.16 32.6 32.7 33. 20G 17G 17G 0.33 0.24 0.21 30.8 31.5 33.3 23G 18G 18G 0.65 0.44 0.37 27.3 31.8 32. 30G 19G 19G 1.47 0.76 0.59 25.3 28.6 30.1 41G 21G 21G 2.99 OOM OOM 10.2 4.24 1.60 7.75 3.05 1.23 16.6 OOM OOM 8.37 14.1 22.7 10.1 16.6 25. 60G OOM OOM 49G 33G 24G 49G 33G 24G (a) Prefill time, tokens per second (TPS), GPU memory comparison. Data Type Baseline LMME(4, 0.9) LMME(4, 0.8) LMME(4, 0.7) Lyra-MM-1.5M Lyra-LongSpeech-12K 66h 9.6h 58h (-13%) 7.0h (-27%) 47h (-29%) 5.7h (-40%) 41h (-38%) 4.5h (-54%) (b) Training time on multi-modality datasets comparison. Table 4. Efficiency of latent multi-modality extractor. To verify the effectiveness of our extractor module, we examine the retention of multi-modal tokens. We primarily assess three types of tokens: image tokens, video tokens, and speech tokens. The specific visualizations are shown in Fig. 5. As seen in the figure, our model ultimately retains only about 10%-25% of the tokens across all three modalities. Moreover, the retained token positions are highly relevant to the user-provided instructions, effectively helping to remove information unrelated to the instructions and thereby accelerating training and inference. We also have included the performance experiments related to LMME in the appendix section. Long-Speech Capability Integration. After performing SFT on our Lyra long speech 12K data mentioned Sec. 3.4, we design the following experiments to validate the models capabilities in processing long speech and latent multimodality extraction, given the current lack of long-speech benchmark. The first experiment is the long speech Needle in Haystack evaluation. We selected five audio files, each more than 3 hours in length, and inserted open-ended audio questions and answers at various points throughout the files. The results are shown on the left side of Fig. 6. According to the figure, we observe that, without enhancing long-speech processing capabilities, the model can handle up to approximately eight minutes of audio. beyond that length, it fails to generate proper output (Fig. 6a). However, with SFT on our Lyra long speech 12K data, the model Table 5. Effectiveness of long speech capability integration. Lyra integrated with long speech ability, using only audio input, can handle one-third of VideoMME cases, and its accuracies on long, medium, short metrics are better than the current best VLM. Modality Benchmark Baseline + SFT + MLoRA Image Video Speech TextVQA [51] MME [14] MMMU [70] VideoMME [15] MVBench [30] EgoSchema [39] TextVQAS [51] DocVQAS [56] MM-VetS [69] 82.3 2332 49.2 62.8 66.7 62.4 - - - 81.3 2275 48.7 61.0 66.8 63. 77.8 84.0 54.0 82.6 2335 50.8 62.8 67.2 63.2 80.0 84.6 60.0 Table 6. Effectiveness of multi-modality LoRA (MLoRA). For powerful pretrained models, adding new modality can impair the abilities of other modalities. MLoRA can effectively address it. can handle audio lengths of up to 4,500 seconds. With audio exceeding 4,500 seconds, the models memory usage surpasses the limit (Fig. 6b). By leveraging the latent multimodality extractor module, we achieve the ability to process even longer audio, extending up to and beyond two hours (Fig. 6c). Additionally, In Fig. 6d, we visualize the tokenlevel attention retention and variations for the needle with the information extractor module, under the same question instructions. Notably, we can see that as the needle is placed in different locations, the information extractor module dynamically adjusts the attention distribution and retention for positions accordingly. The second experiment is based on VideoMME. This benchmark includes videos ranging from 30 seconds to one hour. We first extract the audio from these videos and feed only the audio data into our long speech model to obtain predictions and perform the VideoMME evaluation. Along with generating predictions, we also require our model to output whether it can answer the question based on the audio alone. Specific results are shown in Table 5. From the table, it is evident that long audio can resolve about onethird of the test samples, with model accuracy exceeding 78%, significantly outperforming the 7B model. We integrate the long-speech output into our Lyra model, which ultimately performs better than using subtitles alone. Multi-Modality LoRA (MLoRA) Pipeline. The effectiveness results of MLoRA are presented in Table 6. Compared to multi-modal SFT, MLoRA maintains better original vision performance while enhancing the capability in 7 Figure 5. Visualization of latent multi-modality extractor in various modalities. The upper part is the video modality, and the lower part is the audio modality. Through latent multi-modality information extraction, semantic tokens related to the instruction are retained, reducing the computational cost of the MLLM. The visualization of the image modality and different blocks can be found in the appendix. (a) (b) (c) (d) Figure 6. Comparison of needle in long speech haystack (average with five samples). (a) The baseline model can not retrieve right needles after 450 seconds. (b) Model finetuned on our long speech datasets can not retrieve right needles after 4,500 seconds and achieves 96% accuracy in 4,500 seconds. (c) Our latent extractor, trained on our long speech datasets, can retrieve longer audio (9,900 seconds), and presents 98% accuracy in 4,500 seconds. (d) As the position of the needle changes, the attention in our model also shifts accordingly. new modalities like speech. Additionally, our framework is more efficient, achieving better results with less data (50%). Intern-Omni 27M samples VITA 5M samples EMOVA 4M samples Lyra 2.7M samples 5. Conclusion In conclusion, Lyra represents significant step forward in MLLMs, efficiently integrating complex speech, vision, and language modalities with reduced computational requirements (less data, faster speed). We focus on speech to enhance its interaction with other modalities within MLLMs. By leveraging the proposed modules, and high-quality, comprehensive SFT datasets, Lyra achieves state-of-theart performance across vision-speech, speech-language, and vision-language benchmarks, which is more comprehensive evaluation for omni-models to previous research. Our experiments also reveal that speech plays critical role in multimodal understanding, yet current MLLMs do not effectively leverage this information. We hope our work encourages future researchers to further explore and harness the potential of speech/long speech within MLLMs. Lyra: An Efficient and Speech-Centric Framework for Omni-Cognition"
        },
        {
            "title": "Supplementary Material",
            "content": "We strongly recommend that readers watch the video in our supplementary materials, which include more audio and video examples to get better understanding and experience. In the following supplementary material, we provide more details about the training configurations and the construction and information of our dataset in Sec. A. In Sec. B, we present additional module settings along with some experimental results and analyses. In Sec. C, we showcase the qualitative results of Lyra. A. Training Configuration and Data A.1. Detailed Training Configuration Stage-1: Speech Alignment. In this stage, we only train the parameters of the speech projector for speech-language prealignment with the LibriSpeech [45] and Common Voice Corpus [59] datasets, with about 1.0M data samples. Stage-2: Joint Text-Image-Speech Training. Based on the Mini-Gemini [31] SFT data, we assemble and construct unified dataset with 1.5M samples for the image-text-speech joint training. We use the ChatTTS [1] model to convert high-quality SFT data from text instructions into speech instructions. The multi-modal dataset, i.e., Lyra-MultiModal-1.5M, includes not only single-turn instructions but also multi-turn instructions. Stage-3: Long Speech SFT. To enable the model to integrate the long speech capability, we construct the first long-speech SFT dataset, called Lyra-LongSpeech-12K. Details can be found in Sec. 3.4 of the main paper. To ensure more robust performance, the dataset covers wide range of topics, including humanities, social sciences, technology, education, and more. At this stage, we train both the speech module and the whole LLM module. Stage-4: Streaming Text-Speech Generation. During the speech generation stage, we only train the speech generator. To better align the speech generator with the text decoder, we exclusively use text-speech modality QA pairs in our dataset. We filtered and selected portion of suitable data from the datasets in our Stage-1, Stage-2, and Stage-3 for speech generation, resulting in dataset of approximately 227K samples. Detailed training settings are further explicated in Table 7. Settings Audio Length # Tokens p Dataset # Samples i r Trainable Batch Size Learning rate Epoch Stage-1 < 30s 300 Stage-2 < 30s Stage-3 < 2500s, 30s clips Max 25, 000 Stage-4 < 30s 300 LibriSpeech + CommonVoice 1.2M Lyra-MultiModal-1.5M 1.5M Lyra-LongSpeech-12K 12K Filter from Stage-1, 2, 3 227K Projector 256 1 103 1 Projector + LLM 128 2 104 1 Projector + LLM 16 2 104 3 Speech Generator 32 2 104 Table 7. Detailed training settings of Lyra. A.2. Data Collection and Curation To ensure the data quality and training efficiency, we consider the following aspects while generating speech data for three modalities of joint training. Generate multi-modal interleave data. To ensure models ability to process interleaved multi-modal data, we randomly select one round from multi-round conversations and convert its text into speech, while keeping the remaining rounds in text format. This guarantees that our SFT data preserves its multi-modal interleaved structure. Oral Expression. Certain types of text are not well-suited for direct conversion using TTS technology. In these cases, we ensure the content is rewritten in more conversational, oral form. For example, we rephrase A: as Option is to enhance clarity and naturalness. Method LLM Vision Data Time TextVQA MME MM-Vet MMB-EN SEED MMMU Avg. Rate Baseline + Extractor Baseline + Extractor Baseline + Extractor Vicuna-7B Vicuna-7B CLIP+Conv CLIP+Conv Lyra-MM-1.5M 65h Lyra-MM-1.5M 35h(-46%) Qwen2-7B Qwen2-7B Qwen2-7B Qwen2-7B SigLIP SigLIP SigLIP SigLIP LLaVA-665K LLaVA-665K 18h 14h(-22%) Lyra-MM-1.5M 51h Lyra-MM-1.5M 35h(-31%) 68.4 69.9 69.7 69.1 71.9 71.8 1865 1899 1974 2030 2007 41.3 44.9 39.4 38.6 51.0 50.6 65.8 66.7 76.7 76. 78.1 77.7 68.1 67.5 74.2 73.5 74.5 73.7 36.8 35.3 40.8 40. 40.2 42.1 100.0% 101.5%(+1.5%) 100.0% 99.6% (-0.4%) 100.0% 100.1%(+0.1%) Table 8. Latent multi-modality extractor training performance. The training time is reduced by an average of one-third, while the average performance does not degrade and even improves by 0.4%."
        },
        {
            "title": "Lyra Data Examples",
            "content": "Training conversations: human: <image>nWhat are the two people holding?nAnswer the question using single word or phrase. GPT: Umbrella. human: What is the person with the Red Hat doing? GPT: Taking pictures. human: <speech> GPT: Blanket. Evaluation cases: human: <image>nReference OCR token: DAKOTA, DIGITAL, Single-Use, Camera, Pire, digitatn<speech> Figure 7. Lyra training and evaluation data examples. Speaker Diversity. To maintain diversity in our generated speech, we randomly select speakers with varying timbres and pitches for each instance. Since ChatTTS [1] obtains different speaker characteristics through various Gaussian sampling, it exhibits great diversity and robustness. During our generation process, we switch to new set of ChatTTS random samples every 128 instructions. Be Aware of the OCR Text. In real-world applications, MLLM retrieves text by calling the OCR interface, such as TextVQA. Many OCR tokens, such as G0 and EF, lack clear meaning and are not suitable for verbal expression as speech input. Following this practice, we do not convert OCR text into speech. Here, we list some training prompts and evaluation examples of our data in Table 7. B. More Component-Wise Details & Analysis B.1. Latent Multi-Modality Extractor Qwen2-VL is exceptionally powerful, with the quantity and quality of its training data far surpassing those of public datasets and open-source models. As result, most approaches to continual learning based on Qwen2-VL tend to result in performance degradation. Therefore, to evaluate the performance of our extractor module, we opt to train new model from scratch. The results are shown in Table 8. Under the same training settings, models using latent multi-modality extractor achieve faster training speeds, with maximum acceleration of nearly 50%. Additionally, they maintain or even improve average performance by up to 1% across multiple benchmarks. This series of experiments demonstrates the effectiveness of our extractor. Visualization of the latent multi-modality extractor in image modality is shown in Fig. 10. From it, the tokens retained in different blocks are all related to the users instruction. Additionally, for different questions, the token regions in the image most relevant to the question are preserved. This result is consistent with the video and speech modalities discussed in our main paper. B.2. Long Speech Capability Integration In this part, we primarily introduce prompts related to the long speech capability. The detailed prompts are shown in Table 8. The first is the GPT-4o-based prompt used to generate Q&A during the long speech data collection process. The second is the inference prompt we used to apply the long-speech Lyra model on the VideoMME benchmark. For detailed results and analysis, refer to Sec. 3.4 and the long-speech capability integration part in Sec. 4.3. 10 Long Speech Question-Answer Generation Prompt Example Task: You will be provided with transcript from an audio or video recording. Your task is to generate question-answer pairs based on the content of the transcript. Guidelines for Question-Answer Pair Generation: - The first question should be about summarizing the content of this recording. - Carefully read the transcript provided and base all questions and answers strictly on the content within. - Ensure that each question is directly related to specific details in the transcript, such as events, facts, or points made by the speaker. - Provide clear, concise, and specific questions, along with accurate answers derived from the transcript. - Do not introduce any new information that isnt in the transcript. If the speaker does not introduce themselves, refer to them as Speaker or Narrator. - Avoid generic or overly broad questions; aim for range of question types (e.g., factual, inferential, explanation-based). - Generate five question-answer pairs. Output Format: - Your output should be structured as JSON object. - Each question-answer pair should be formatted as: json { {\"Question\": <question-1>, \"Answer\": <answer-1>}, {\"Question\": <question-2>, \"Answer\": <answer-2>}, ... [ ] }"
        },
        {
            "title": "Long Speech VideoMME Evaluation Prompt Example",
            "content": "Based on the context, determine if it provides enough information to answer the question: <question> with the provided choices <option-A>, <option-B>, <option-C>, <option-D>. Do not introduce any information not found in the context. - If the context is sufficient to answer the question, respond yes and answer with the options letter from the given choices directly. - If the context does not contain enough information to answer the question, respond no. Figure 8. Long speech related prompt examples. B.3. Sound Capability Integration For the sound modality, due to the lack of many pretrained models, we primarily follow ImageBind[18] as the sound encoder. ImageBind processes sound, text, and image modalities using training approach similar to CLIP [47], ultimately encoding them into just one single token. This approach is not particularly generalizable. During the sound SFT process, our model based on LLaMA3 [12] is trained on the AudioCaps [26] dataset, which contains total of 46K training samples. The quantitative performance of our model on the test set is shown in Table 9. Regarding this dataset, as the authors of AudioCaps [26] have noted, Even to humans, recognizing the true identity of sound can be ambiguous. Moreover, LLM-based multimodal models tend to produce more detailed descriptions, while metrics like SPICE [3] and CIDEr [58] are outdated and fail to effectively reflect the most suitable results. Even under such circumstances, our Lyra, trained on just 46K samples for the sound modality, outperforms previous sound models. Some qualitative results are shown in Fig. 9. B.4. Streaming Text-Speech Generation For the speech-text streaming generation component, we primarily refer to LLaMA-Omni [13] to enable the MLLM to output speech audio. Speech Discretization. To handle speech responses, we discretize the audio into discrete units with the following steps: 1). Continuous representations are extracted using the HuBERT model [22]. 2). These representations are clustered into discrete indices via the K-means algorithm. 3). Consecutive repeated indices are merged to form sequence of discrete units, which can be converted back to waveforms using vocoder [46]. 11 Figure 9. Sound capability qualitative results. Figure 10. Visualization of latent multi-modality extractor in the image modality. Speech Decoder for Streaming Generation. streaming speech decoder is introduced after the LLM to enable simultaneous generation of text and speech: To ensure the overall structure remains consistent with the LLM, the decoder is built using two transformer layers similar to Qwen2-VL [60]. Similar to LLaMA-Omni, it processes the hidden states from the LLM and generates discrete speech units in non-autoregressive manner [38, 74]. For upsampling, the text hidden states from the LLM are upsampled to match the speech sequences length. These upsampled representations are processed by the speech decoder to produce output features for the discrete speech units. Alignment and CTC Training. Following LLaMA-Omni, Connectionist Temporal Classification (CTC) [21] is used to align the decoders output with the discrete speech units. During training, the model learns to match the output features to the target speech units by minimizing the CTC loss. During inference, the most likely sequence is selected, converted into discrete units, and passed through the vocoder to generate audio. B.5. TTS Methods Ablation Study In this subsection, we briefly compare the impact of different TTS (text-to-speech) methods on the generalization and robustness of speech instruction (across different domains). We primarily used two TTS methods: ChatTTS [1] and Edge-TTS [41]. ChatTTS employs Gaussian sampling to simulate different speakers (As shown in Listing 1), while Edge-TTS randomly selects from fixed set of 41 speakers. ChatTTS is likely to be more diverse. We trained models using instruction data generated by these TTS methods and evaluated TextVQA speech instructions generated by different TTS methods. Detailed results can be found in Table 9a. Models trained with speech generated by ChatTTS demonstrated better generalization due to its diversity. Similar results were observed when compared with speech instructions generated by Intern-Omni [44]. Because we cannot access their training speech instruction data; they only provided the evaluation speech instruction data of DocVQA and ChartQA. Specific results are provided in Table 9b and 9c. While models perform better when trained and evaluated on instructions generated by the same system, the experiments overall demonstrate that instructions generated by ChatTTS are more robust compared to the other two methods. C. Qualitative Results C.1. Examples of Images and Videos In Fig. 11, we present additional interactions with Lyra, showcasing the models adeptness in knowledge-based perception and reasoning for both images and videos. In various complex scenarios, such as recognition of complex PC backgrounds, understanding of game interfaces, and analyzing football match videos with significant differences between frames, Lyra demonstrates superior understanding and reasoning cognitive outcomes. 12 AT [40] BART [19] PairMix [27] CoDi [54] Lyra-Base Eval/Train ChatTTS Edge-TTS Eval/Train ChatTTS Eval/Train ChatTTS 16.8 17. 18.1 17.1 19.5 ChatTTS Edge-TTS 80.0 79.7 79.5 78. (a) TextVQAS ChatTTS Intern-O 84.6 82.3 (b) DocVQAS ChatTTS Intern-O 60.4 58. (c) ChartQAS Table 9. Sound SPICE performance comparison. Table 10. Different TTS training and evaluation. Listing 1. Sample Random Function in ChatTTS (Pytorch) def sample_random(self) -> torch.Tensor: spk = ( torch.randn(self.dim, device=self.std.device, dtype=self.std.dtype) .mul_(self.std) .add_(self.mean) ) return spk 1 2 3 5 6 7 C.2. Examples of Long Speeches In the main paper experimental section, Fig. 6a shows that existing Speech Language Models (SLMs) fail when processing audio longer than 450 seconds (about seven minutes): the output becomes nonsensical with extensive repetition. In this part, we demonstrate Lyras ability to handle long audio inputs. In Fig. 12, 13, 14, and 15 we demonstrate Lyras capability to process long-form speech (best view the following part together with the video in the supplementary materials). Lyra effectively extracts the information that users need from extended speech contents. It excels at capturing both the details and the overall structure of long speeches. In news scenarios (Fig. 12, with frequent topic switches), it accurately identifies the focused information and responds exceptionally well. For more complex tasks, as shown in Fig. 13, such as scenarios with visual ambiguity, our model leverages long-form speech and keyframes from videos to provide more accurate results compared to powerful VLM like Qwen2-VL that rely solely on visual information. In Fig. 14, our model demonstrates its ability to process daily lectures, offering significant advantages for educationalrelated applications. Lyra can handle speech content durations exceeding two hours, which enables intelligent models to tackle more complex multi-modal tasks. In Fig. 15, For tasks with longer temporal sequences and higher complexity, Lyra can also understand them and provide subjectively reasonable answers to the questions. 13 Figure 11. Image-text and video-text qualitative results of Lyra. Figure 12. Lyra long speech capability qualitative results for handling daily news. 15 Figure 13. More long speech examples results. Lyra achieves more accurate omni-cognition compared to naive VLMs like Qwen2-VL. 16 Figure 14. More examples of Lyra with hour-long lectures (more than two hours). Figure 15. More results from long speech examples: Lyra can subjectively answer questions about complex steps."
        },
        {
            "title": "References",
            "content": "[1] 2noise. ChatTTS. https://github.com/2noise/ ChatTTS, 2024. 5, 9, 10, 12 [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: In NeurIPS, visual language model for few-shot learning. 2022. 1 [3] Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. SPICE: Semantic propositional image caption evaluation. In ECCV, pages 382398. Springer, 2016. 11 [4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-VL: frontier large vision-language model with versatile abilities. arXiv:2308.12966, 2023. 2 [5] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. [6] Kai Chen, Yunhao Gou, Runhui Huang, Zhili Liu, Daxin Tan, Jing Xu, Chunwei Wang, Yi Zhu, Yihan Zeng, Kuo Yang, et al. EMOVA: Empowering language models to see, hear and speak with vivid emotions. arXiv preprint arXiv:2409.18042, 2024. 1, 2, 5, 6 [7] Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration In ECCV, pages 1935. for large vision-language models. Springer, 2025. 2 [8] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. LongLoRA: Efficient fine-tuning of long-context large language models. In ICLR, 2024. 2 [9] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to GPT-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. 5 [10] Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, et al. Qwen2-audio technical report. arXiv preprint arXiv:2407.10759, 2024. 1, 5 [11] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. InstructBLIP: Towards generalpurpose vision-language models with instruction tuning. In NeurIPS, 2023. [12] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The LLaMA 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 2, 11 less speech interaction with large language models. arXiv preprint arXiv:2409.06666, 2024. 1, 2, 4, 11 [14] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, et al. MME: comprehensive evaluation benchmark for multimodal large language models. arXiv:2306.13394, 2023. 7 [15] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-MME: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 2, 7 [16] Chaoyou Fu, Haojia Lin, Zuwei Long, Yunhang Shen, Meng Zhao, Yifan Zhang, Xiong Wang, Di Yin, Long Ma, Xiawu Zheng, et al. VITA: Towards open-source interactive omni multimodal llm. arXiv preprint arXiv:2408.05211, 2024. 1, 2, 4, 5, 6 [17] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive KV cache compression for LLMs. arXiv preprint arXiv:2310.01801, 2023. 2 [18] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. ImageBind: One embedding space to bind them all. In CVPR, pages 1518015190, 2023. [19] Felix Gontier, Romain Serizel, and Christophe Cerisara. Automated audio captioning by fine-tuning bart with audioset tags. In DCASE 2021-6th Workshop on Detection and Classification of Acoustic Scenes and Events, 2021. 13 [20] Google. Gemma: Introducing new state-of-the-art open https : / / blog . google / technology / models. developers/gemma-open-models/, 2024. 1 [21] Alex Graves, Santiago Fernandez, Faustino Gomez, and Jurgen Schmidhuber. Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks. In ICML, pages 369376, 2006. 12 [22] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. Hubert: Self-supervised speech representation learning by masked prediction of hidden units. IEEE/ACM transactions on audio, speech, and language processing, 29: 34513460, 2021. 11 [23] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. ICLR, 2021. [24] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In ICML, 2021. 1 [25] Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv:2401.04088, 2024. 1 [13] Qingkai Fang, Shoutao Guo, Yan Zhou, Zhengrui Ma, Shaolei Zhang, and Yang Feng. LLaMA-Omni: Seam- [26] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim. AudioCaps: Generating captions for audios 19 in the wild. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 119132, 2019. 11 [27] Eungbeom Kim, Jinhee Kim, Yoori Oh, Kyungsu Kim, Minju Park, Jaeheon Sim, Jinwoo Lee, and Kyogu Lee. Exploring train and test-time augmentations for audio-language learning. arXiv preprint arXiv:2210.17143, 2022. 13 [28] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. LLaVA-OneVision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 1, 2, 5 [29] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, pages 1973019742. PMLR, 2023. [30] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. MVBench: comprehensive multi-modal video understanding benchmark. In CVPR, pages 2219522206, 2024. 7 [31] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. Mini-Gemini: Mining the potential of multi-modality vision language models. arXiv preprint arXiv:2403.18814, 2024. 1, 2, 5, 9 [32] Yanwei Li, Chengyao Wang, and Jiaya Jia. LLaMA-VID: An image is worth 2 tokens in large language models. In ECCV, pages 323340. Springer, 2025. 2 [33] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. VILA: On pre-training for visual language models. In CVPR, pages 2668926699, 2024. [34] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeruIPS, 2023. 1 [35] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR, pages 2629626306, 2024. [36] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. LLaVA-NeXT: Improved reasoning, ocr, and world knowledge, 2024. 2, 5 [37] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for LLM KV cache compression at test time. NeruIPS, 36, 2024. [38] Zhengrui Ma, Qingkai Fang, Shaolei Zhang, Shoutao Guo, Yang Feng, and Min Zhang. non-autoregressive generation framework for end-to-end simultaneous speech-to-any translation. arXiv preprint arXiv:2406.06937, 2024. 12 [39] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very longform video language understanding. NeurIPS, 36:46212 46244, 2023. 7 [40] Xinhao Mei, Xubo Liu, Qiushi Huang, Mark Plumbley, and Wenwu Wang. Audio captioning transformer. arXiv preprint arXiv:2107.09817, 2021. 13 [41] Microsoft. Edge-TTS. https://github.com/rany2/ edge-tts, 2024. 12 [42] OpenAI. ChatGPT. https://openai.com/blog/ chatgpt/, 2023. 1, [43] OpenAI. GPT-4o. https://openai.com/index/ hello-gpt-4o/, 2024. 1, 7 [44] OpenGVLab. InternOmni: Extending internvl with audio modality. https://internvl.github.io/blog/ 2024-07-27-InternOmni/, 2024. 4, 6, 12 [45] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: an asr corpus based on public domain audio books. In ICASSP, pages 52065210. IEEE, 2015. 6, 9 [46] Adam Polyak, Yossi Adi, Jade Copet, Eugene Kharitonov, Kushal Lakhotia, Wei-Ning Hsu, Abdelrahman Mohamed, and Emmanuel Dupoux. Speech Resynthesis from Discrete Disentangled Self-Supervised Representations. In Proc. Interspeech 2021, 2021. [47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 2, 11 [48] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech In ICML, recognition via large-scale weak supervision. pages 2849228518. PMLR, 2023. 5 [49] Paul Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur Bapna, Zalan Borsos, Felix de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, et al. AudioPaLM: large language model that can speak and listen. arXiv preprint arXiv:2306.12925, 2023. 2 [50] John Serences and Steven Yantis. Selective visual attention and perceptual coherence. Trends in cognitive sciences, 10 (1):3845, 2006. 4 [51] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus In CVPR, Rohrbach. Towards vqa models that can read. 2019. 7 [52] Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, MA Zejun, and Chao Zhang. SALMONN: Towards generic hearing abilities for large language models. In ICLR, 2024. [53] Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, and Song Han. Quest: Query-aware sparsity for efficient long-context llm inference. arXiv preprint arXiv:2406.10774, 2024. 2 [54] Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, and Mohit Bansal. Any-to-any generation via composable diffusion. NeurIPS, 36, 2024. 13 [55] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford Alpaca: An instruction-following llama model. https://github.com/tatsulab/ stanford_alpaca, 2023. 1 [56] Rub`en Tito, Dimosthenis Karatzas, and Ernest Valveny. Document collection visual question answering. In ICDAR 2021, 2021. 7 20 derstanding and reasoning benchmark for expert agi. CVPR, 2024. In [71] Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan, Ge Zhang, Linyang Li, et al. AnyGPT: Unified multimodal llm with discrete sequence modeling. arXiv preprint arXiv:2402.12226, 2024. 1, 2, 6 [72] Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu. SpeechGPT: Empowering large language models with intrinsic cross-modal conversational abilities. arXiv preprint arXiv:2305.11000, 2023. 2 [73] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. 2 [74] Shaolei Zhang, Qingkai Fang, Shoutao Guo, Zhengrui Ma, Min Zhang, and Yang Feng. Streamspeech: Simultaneous speech-to-speech translation with multi-task learning. arXiv preprint arXiv:2406.03049, 2024. 12 [75] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Re, Clark Barrett, et al. H2O: Heavy-hitter oracle for efficient generative inference of large language models. NeruIPS, 36:3466134710, 2023. [76] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. MiniGPT-4: Enhancing visionlanguage understanding with advanced large language models. arXiv:2304.10592, 2023. 1 [57] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and efficient foundation language models. arXiv:2302.13971, 2023. 1, 2 [58] Ramakrishna Vedantam, Lawrence Zitnick, and Devi Parikh. CIDEr: Consensus-based image description evaluation. In CVPR, pages 45664575, 2015. 11 [59] Common Voice. https : / / commonvoice.mozilla.org/en/datasets, 2024. 9 Common Voice. [60] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-VL: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 2, 5, 12 [61] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al. CogVLM: Visual expert for pretrained language models. arXiv:2311.03079, 2023. 2 [62] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. interLongvideobench: benchmark for long-context arXiv preprint leaved video-language understanding. arXiv:2407.15754, 2024. 2 [63] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and TatSeng Chua. Next-GPT: Any-to-any multimodal llm. arXiv preprint arXiv:2309.05519, 2023. 1, 2 [64] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. 2 [65] Zhifei Xie and Changqiao Wu. Mini-Omni: Language models can hear, talk while thinking in streaming. arXiv preprint arXiv:2408.16725, 2024. 1, [66] Fuzhao Xue, Yukang Chen, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, et al. LongVILA: Scaling long-context arXiv preprint visual language models for long videos. arXiv:2408.10188, 2024. 2 [67] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. 2 [68] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. 2 [69] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. MM-Vet: Evaluating large multimodal models for integrated capabilities. arXiv:2308.02490, 2023. 7 [70] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. MMMU: massive multi-discipline multimodal un-"
        }
    ],
    "affiliations": [
        "CUHK",
        "HKUST",
        "SmartMore"
    ]
}