{
    "paper_title": "ParaStudent: Generating and Evaluating Realistic Student Code by Teaching LLMs to Struggle",
    "authors": [
        "Mihran Miroyan",
        "Rose Niousha",
        "Joseph E. Gonzalez",
        "Gireeja Ranade",
        "Narges Norouzi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have shown strong performance on programming tasks, but can they generate student-like code like real students - imperfect, iterative, and stylistically diverse? We present ParaStudent, a systematic study of LLM-based \"student-like\" code generation in an introductory programming course setting. Using a dataset of timestamped student submissions across multiple semesters, we design low- and high-resolution experiments to model student progress and evaluate code outputs along semantic, functional, and stylistic dimensions. Our results show that fine-tuning significantly improves alignment with real student trajectories and captures error patterns, incremental improvements, and stylistic variations more faithfully. This study shows that modeling realistic student code requires capturing learning dynamics through context-aware generation, temporal modeling, and multi-dimensional evaluation. Code for experiments and evaluation is available at https://github.com/mmiroyan/ParaStudent."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 2 4 7 6 2 1 . 7 0 5 2 : r ParaStudent: Generating and Evaluating Realistic Student Code by Teaching LLMs to Struggle Mihran Miroyan* Rose Niousha* Joseph E. Gonzalez Gireeja Ranade Narges Norouzi University of California, Berkeley {miroyan.mihran,rose.n,jegonzal,ranade,norouzi}@berkeley.edu"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have shown strong performance on programming tasks, but can they generate student-like code like real studentsimperfect, iterative, and stylistically diverse? We present ParaStudent, systematic study of LLM-based student-like code generation in an introductory programming course setting. Using dataset of timestamped student submissions across multiple semesters, we design lowand high-resolution experiments to model student progress and evaluate code outputs along semantic, functional, and stylistic dimensions. Our results show that fine-tuning significantly improves alignment with real student trajectories and captures error patterns, incremental improvements, and stylistic variations more faithfully. This study shows that modeling realistic student code requires capturing learning dynamics through context-aware generation, temporal modeling, and multi-dimensional evaluation. Code for experiments and evaluation is available at github.com/mmiroyan/ParaStudent."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) offer new opportunities to support personalized learning at scale. Intelligent Tutoring Systems (ITS) (Corbett et al., 1997) have the potential to provide personalized support, and LLMs can help close the gap between traditional instruction and the benefit of one-onone tutoring (Bloom, 1984). To be effective, these systems must model students not only at the level of correctness but also in terms of their stylistic patterns and incremental progress. In the context of Computer Science (CS) education, this means not just solving programming problems, but doing so like novice learner. While LLMs have shown strong performance in software engineering and competitive programming tasks (Shi et al., *Equal contribution. 1 2024; Ehrlich et al., 2025), much less is known about their ability to emulate the imperfect nature of student-like code. We focus on fundamental question: can LLMs realistically simulate student behavior? To explore this question, we introduce ParaStudent, framework for generating and evaluating realistic student code using LLMs. ParaStudent combines (1) fine-tuned student-code models and (2) set of multi-dimensional evaluation metrics that capture semantic, functional, and stylistic aspects of code. Fig. 1 provides an overview of our approach and illustrates how qwen-student (fine-tuned Qwen2.5 Coder 7B on student code data) generates code trajectories that closely align with those of real students. To build ParaStudent, we first identify core properties of student-like code that sets it apart from expert-written code: functional errors, unpolished and verbose style, non-standard structure, and incremental revisions (see Sec. 2). We then formalize set of evaluation metrics (see Sec. 4.3) designed to quantify these characteristics along the semantic, functional, and stylistic axes. Our approach compares fine-tuning and prompting strategies for simulating student code (see Sec. 4.2). We fine-tune Qwen-2.5 Coder 7B (Hui et al., 2024) on real student submissions from an introductory programming course, and compare it against its instruction-tuned version (Qwen-2.5 Coder 7B Instruct) and GPT-4.1 (OpenAI, 2025). We evaluate models across two temporal resolutions: low-resolution (start/middle/end snapshots) and high-resolution (timestamped code streams) to assess how well they capture progression over time (see Sec. 4.1). Our results (see Sec. 5) show that fine-tuning is essential for modeling realistic student behavior. The fine-tuned model better captures error patterns, realistic style variation, and incremental edits than general instruction-tuned models. Our approach Figure 1: ParaStudent Trajectories in Multi-dimensional Feature Space Φ. We embed sequences of code submissions from real students and LLMs into shared feature space Φ, defined by combination of code embeddings, functionality metrics, and style features. Each trajectory illustrates students or models code progression over time. Compared to instruction-tuned or proprietary models, the fine-tuned model (qwen-student) traces path that most closely aligns with that of the real student behavior. demonstrates that small, open models, when finetuned appropriately, can simulate realistic student code. Our contributions are threefold: Evaluation metrics. We introduce set of metrics, including code semantics, error type, and code style, to evaluate the realism of student-like code. Sequential code modeling. We fine-tune on lowand high-resolution student code streams to simulate realistic learning trajectories on different levels of granularity. Fine-tuning vs. prompting. We find that when models are fine-tuned on student code to specific homework problems, they outperform promptingonly models along the proposed set of metrics."
        },
        {
            "title": "2 Related Work",
            "content": "Code Generation. Recent advances in LLM code generation capabilities (Jiang et al., 2024a) have driven their adoption in practical software engineering workflows (Nikolaidis et al., 2024). Prior work has explored supervised fine-tuning methods, such as instruction tuning (Ma et al., 2024; Luo et al., 2024; Li et al., 2024a), distillation (Sun et al., 2024), data pruning (Tsai et al., 2024), and parameter-efficient fine-tuning (Zhuo et al., 2024; Weyssow et al., 2025) approaches. parallel line of work investigated Reinforcement Learning (RL) methods (Le et al., 2022; Shojaee et al., 2023), including RL with human (Wong and Wei Tan, 2024) and program feedback (Liu et al., 2023; Dou et al., 2024). LLMs have also been shown to perform well in zero-shot code generation settings through Chain-of-Thought prompting (Yang et al., 2024a), in-context learning (Li et al., 2025), planning (Jiang et al., 2024b; Zhang et al., 2023), and self-repair (Olausson et al., 2023; Chen et al., 2024; Zhong et al., 2024). More recent work in LLM agents and tool-use (Packer et al., 2024; Patil et al., 2024) has advanced the capabilities of autonomous coding agents (Yang et al., 2024b; Holt et al., 2024; Pan et al., 2024). These efforts have led to high-performing open-source models (Rozière et al., 2024; Hui et al., 2024; DeepSeek-AI et al., 2024; Lozhkov et al., 2024) and proprietary alternatives (OpenAI, 2025; Anthropic, 2025; Deepmind, 2025). While this body of work focuses primarily on professional-grade code generation, our work explores novel direction: simulating student code by mimicking error patterns, stylistic variation, and incremental progress through prompting and finetuning methods. Student Code Generation and Simulation Prior studies in student code generation have primarily explored prompting methods with proprietary LLMs by providing high-level student code features such as error type distributions (MacNeil et al., 2024) or test case pass rates (Leinonen et al., 2025). Beyond code, LLMs have also been used to simulate students across diverse educational settings, including classroom dialogues (Yue et al., 2024), tabular student data synthesis (Khalil et al., 2025), assignment evaluation (Lu and Wang, 2024; He-Yueya et al., 2024), and Teaching Assistant (TA) training simulations (Markel et al., 2023). Our work is the first to investigate fine-tuning LLMs specifically for student code generation to learn student-like learning trajectories rather than relying solely on handcrafted prompts. 2 Code Evaluation. The evaluation of LLMgenerated code has traditionally focused on functionality, efficiency, and style (Chen et al., 2021a; Liu et al., 2024a; Zheng et al., 2024; Yan et al., 2024), often in the context of professional software engineering (Jimenez et al., 2024) or domainspecific tasks (Quoc et al., 2024; Gu et al., 2025). In educational contexts, however, student code is frequently unstructured, stylistically inconsistent (De Ruvo et al., 2018), and error-prone (Denny et al., 2011; Altadmri and Brown, 2015; Ahadi et al., 2018; Ettles et al., 2018). Prior evaluations have largely relied on functionality-based metrics, such as error distributions or test pass rates, to compare model and student outputs (MacNeil et al., 2024; Leinonen et al., 2025). We extend this work by introducing multidimensional evaluation framework that incorporates code semantics, functionality, and style features to offer holistic lens on what makes code student-like."
        },
        {
            "title": "3 Data",
            "content": "We study student code generation in the context of an introductory programming course1 at the University of California, Berkeley. While assignment content varies slightly across semesters, the course consistently covers topics such as functions, recursion, sequences, trees, linked lists, and object-oriented programming. Students complete approximately 10 homework assignments per semester, each with 36 problems. Assignments are completed locally and submitted to an autograder system that provides immediate feedback without hidden tests. All submission attempts are logged, including the students code and autograder output. Our dataset spans four semesters: Spring 2021, Fall 2021, Spring 2022, and Fall 2022.2 The resulting data contains 5,478 students, 22 assignments, 33 problems, and total of 689,023 code submissions. We split the data into training and test sets by setting aside all data from Spring 2022 and Fall 2022, and selected problems from Spring 2021 and Fall 2021 for testing, resulting in 244,483 code submissions in the training set.3 For the test set, we sample 46 problems and 50 students per test 1CS 61A: Structure and Interpretation of Computer Programs (https://cs61a.org/). 2We exclude more recent semesters to avoid potential contamination from LLM usage (e.g., ChatGPT). 3The size of the training set differs across experiments due to varying levels of stream granularity (see Sec. 4.1). semester, resulting in 13,108 test submissions. To evaluate generalization, we define two test subsets: test_NS_OP (New Student, Old Problem) contains 1,610 code submissions from new test students in the test semesters (Spring 2022 and Fall 2022) on problems that also appear in the training set. This set evaluates the models ability to generalize to unseen students on familiar problems. test_NS_NP (New Student, New Problem) contains 4,547 code submissions from students in the test semesters solving entirely new problems not present in the training data. This set evaluates the models ability to generalize to both unseen students and unseen problems."
        },
        {
            "title": "Further details on data preprocessing and IRB",
            "content": "compliance are provided in Appendix A."
        },
        {
            "title": "4 Methodology",
            "content": "We study the problem of student code generation: given student si (the i-th student) and programming problem pu (the u-th problem), the model must generate code submission conditioned on both problem-specific and student-specific context. We design experiments to test this setup across different temporal granularities (Sec. 4.1), explore fine-tuning and prompting approaches (Sec. 4.2), and introduce suite of metrics to evaluate how student-like the generated code is across semantic, functional, and stylistic dimensions (Sec. 4.3)."
        },
        {
            "title": "4.1 Experiments",
            "content": "Each student-problem pair is represented by stream of sequential code submissions, from the first to the final attempt. We evaluate the ability of LLMs to generate code under two temporal setups: Low-resolution. In the low-resolution setting, we extract three submissions corresponding to the first, middle, and last entries of the original stream. The model is tasked with generating the code submissions at different stages. This coarsegrained setting captures high-level characteristics of student code at each stage. High-resolution. The model is conditioned on prior code attempts and is tasked with generating the next submission in the sequence. This setup is designed to capture more fine-grained patterns through the next-step code generation. To measure the effect of the number of previous attempts on modeling the students progress, we vary the number of provided prior attempts (k 1, 3). We also study the impact of student-specific con3 text in both lowand high-resolution settings. In the with-context setting, we include the students submission(s) on different problem (from prior homework) at the same relative position. This allows the model to learn student-specific patterns across problems. In the without-context setting, only the current problem history is used. We formalize our two experimental settings: Experiment 1 (Low-resolution). Without context: Generates the code submission at stage of student si for problem pu, where {start, middle, last}. cb,si,pu (b, pu) With context: Given the code submission of student si for prior problem pv at the same stage, generate code at stage for problem pu. cb,si,pu (b, pu, cb,si,pv ) Experiment 2 (high-resolution). Without context: Generate the code submission at timestamp of the student si for problem pu, conditioned on students prior attempts for the same problem pu. ct,si,pu (pu, [ctj,si,pu]j=1...k) With context: Given segment of the code submission stream of student si for prior problem pv, generate code at timestamp for the current problem pu. Since submission streams vary in length across problems, we extract the segment from the prior stream that corresponds to the same relative position as the target submission in the current stream. ct,si,pu (cid:0)pu, [ctj,si,pu]j=1...k, (cid:1) [ctj,si,pv ]j=1...k+"
        },
        {
            "title": "4.2 Models",
            "content": "We compare two methods for student code generation: fine-tuning and prompting. Fine-tuning. Due to its strong coding capabilities, we fine-tune Qwen-2.5 Coder 7B (Hui et al., 2024) separately for each experiment using LoRA (Hu et al., 2022) (r = 16, α = 32), for one epoch with learning rate of 104. We also conduct ablations using Llama 3.1 8B, Qwen-3 8B, and smaller Qwen-2.5 Coder 3B model. Details are provided in Appendix B. Prompting. We evaluate Qwen-2.5 Coder 7B Instruct (instruction-tuned) and GPT-4.1 (OpenAI, 2025) in zero-shot setting. Prompt templates and sampling settings are detailed in Appendix B. Throughout the paper, we refer to the three models as qwen-student (fine-tuned Qwen-2.5 Coder 7B), qwen-inst (instruction-tuned Qwen2.5 Coder 7B), and gpt-4.1 (GPT-4.1)."
        },
        {
            "title": "4.3 Evaluation Metrics",
            "content": "As discussed in Sec. 2, to properly evaluate how well model-generated code mimics student-written code, we introduce multi-dimensional evaluation metrics based on code semantics, functionality, style, and progression over time."
        },
        {
            "title": "4.3.1 Embedding metrics\nWe extract 1024-dimensional code embedding vec-\ntors using SFR-Embedding-Code-400M (Liu et al.,\n2024b), a lightweight yet effective model for code\nretrieval tasks (Li et al., 2024b). We compute:",
            "content": "Cosine similarity: The pairwise similarity between the embedding vectors of studentwritten and model-generated code submissions. K-Nearest Neighbor (NN) distance: Average distance of student codes to closest model-generated codes (k = 3). Lower values indicate local alignment with student code distribution. Coverage: Proportion of student codes within the k-nearest neighbors of model-generated codes (k = 10). Higher values indicate more coverage of the student code distribution."
        },
        {
            "title": "4.3.3 Style metrics\nThe code style of novice programmers often de-\nviates significantly from professional standards\n(De Ruvo et al., 2018). To evaluate code in this\ndimension, we extract:",
            "content": "4 (a) test_NS_OP (b) test_NS_NP Figure 2: Experiment 1: Code embeddings across three submission stages (first, middle, last) with (bottom) and without (top) context for student (black squares), qwen-student (orange circles), qwen-inst (green crosses), and gpt-4.1 (purple crosses) code submissions. 1024-dimensional embeddings are projected onto 2D plane using PCA for visualization. qwen-student better matches student code distribution under test_NS_OP setting compared to qwen-inst and gpt-4.1. The alignment is weaker under the test_NS_NP setting. Verbosity: Number of characters and lines. Abstract Syntax Tree (AST) metrics: Depth, width, and number of nodes of the AST (Noonan, 1985). greater AST depth indicates deeply nested structure (e.g., multiple layers of loops and conditions). AST width captures the maximum number of sibling nodes at any depth (e.g., function with many parameters). The number of AST nodes correlates with the length and complexity of the code. PEP 8 violations: Deviations from Pythons style guide, PEP 8 (van Rossum et al., 2025).4 We also compute an aggregate style score as the first Principal Component (PC) of the feature matrix containing verbosity and AST-based metrics."
        },
        {
            "title": "5 Results",
            "content": "We report the results of low-resolution (Experiment 1, Sec. 5.1) and high-resolution (Experiment 2, Sec. 5.2) student code generation settings. Our evaluation compares real student submissions against 4We use the pycodestyle (https://pycodestyle.pycqa.org) package for checking Python code against PEP 8 guidelines. model-generated code using metrics described in Sec. 4.3, under in-distribution (test_NS_OP) and out-of-distribution (test_NS_OP) test sets. Across both experiments, we analyze the performance of fine-tuned (qwen-student) and prompt-based (qwen-inst, gpt-4.1) models."
        },
        {
            "title": "5.1 Experiment 1: Low Resolution Setting",
            "content": "We first assess how well models capture student behavior in the start, middle, and last stages of submission streams. Code Embeddings. Fig. 2 visualizes code embeddings across the three temporal stages. On test_NS_OP (Fig. 2a), qwen-student exhibits greater variability and overlaps more closely with student code distributions than prompt-based models, particularly in the first and last stages. On test_NS_NP (Fig. 2b), the alignment of qwen-student and qwen-inst is weaker. Tab. 1 quantifies this trend: on test_NS_OP (Tab. 1a), qwen-student achieves the lowest embedding distance (0.058) and highest coverage (71.9%) on average, improving over qwen-inst by 0.021 in distance and 15.6% in coverage. On test_NS_NP (Tab. 1b), qwen-student performs comparably to gpt-4.1 in distance metric (average =0.006), but struggles with coverage (average =10.0%). Student-specific context improves alignment across all models, particularly for qwen-student. Code Functionality. Fig. 3 shows error type distributions per stage. On test_NS_OP (Fig. 3a), qwen-student matches the student error profile 5 Table 1: Experiment 1: Distribution-level embedding-based metrics (see Sec. 4.3) across models, stages (first, middle, and last), and contexts (context=T, without context=F). Results are reported for both test sets: (a) test_NS_OP and (b) test_NS_NP. Lower KNN distance and higher KNN coverage indicate better alignment with the student code distribution. Best-performing model-context pair for each stage and test set are highlighted. On test_NS_OP, qwen-student consistently shows the strongest alignment across all stages. On test_NS_NP, gpt-4.1 achieves closer proximity, though qwen-student still significantly outperforms qwen-inst. (a) test_NS_OP (b) test_NS_NP Model Stage Context Avg. KNN Dist. KNN Cov. Model Stage Context Avg. KNN Dist. KNN Cov. gpt-4.1 first qwen-inst first qwen-student first gpt-4.1 first qwen-inst first qwen-student first gpt-4.1 middle qwen-inst middle qwen-student middle gpt-4.1 middle qwen-inst middle qwen-student middle gpt-4.1 last qwen-inst last qwen-student last gpt-4.1 last qwen-inst last qwen-student last F F T F T 0.083 0.080 0.054 0.073 0.083 0.056 0.078 0.086 0.063 0.081 0.077 0.060 0.079 0.083 0.060 0.077 0.078 0. 40.0% 44.4% 77.8% 53.3% 51.1% 80.0% 48.9% 46.7% 68.9% 48.9% 55.6% 71.1% 53.3% 44.4% 60.0% 71.1% 68.9% 73.3% gpt-4.1 first qwen-inst first qwen-student first gpt-4.1 first qwen-inst first qwen-student first gpt-4.1 middle qwen-inst middle qwen-student middle gpt-4.1 middle qwen-inst middle qwen-student middle gpt-4.1 last qwen-inst last qwen-student last gpt-4.1 last qwen-inst last qwen-student last F F T F T 0.072 0.100 0.073 0.069 0.102 0.096 0.060 0.089 0.068 0.057 0.087 0.061 0.049 0.070 0.046 0.046 0.071 0. 63.6% 33.8% 54.6% 57.1% 33.8% 45.4% 71.8% 38.0% 54.9% 62.0% 38.0% 59.2% 70.1% 52.0% 57.1% 67.5% 42.9% 61.0% (a) test_NS_OP (b) test_NS_NP Figure 3: Experiment 1: Error type distributions across stages (first, middle, last) and test sets (test_NS_OP, test_NS_NP) under with-context settings. gpt-4.1 generates mostly functional code without errors. Error type distribution of qwen-student is close to that of student code on test_NS_OP, but the gap increases on test_NS_NP with the model generating erroneous code at the last stage. across all stages, with diverse errors early on and increased pass rate by the final stage. qwen-inst maintains flat error distribution across all stages. gpt-4.1, by contrast, predicts nearly 100% correct code from the start. On test_NS_NP (Fig. 3b), both qwen-student and qwen-inst capture early error patterns, but qwen-student underpredicts correctness in the final stage, suggesting limits in generalizing learning progression to new problems. Code Style. Tab. 2 reports style metrics for the final stage (last submission) across contexts and test sets. The style score (as described in Sec. 4.3) is the first PC of the verbosity and AST-based metrics. We report the mean, standard deviation, and Mean Absolute Error (MAE), where MAE is computed pairwise between the model-generated code and the corresponding student code. On test_NS_OP (Tab. 2a), qwen-student is most aligned with the student code in PEP 8 violations, style score (0.41 vs 0.89), and the lowest style MAE. Prompt-based models generate cleaner and less verbose code that diverges from students stylistic patterns (i.e., lower style score overall). On test_NS_NP (Tab. 2b), style alignment degrades, with gpt-4.1 outperforming other models in PEP 8 compliance and style score in the no-context setting."
        },
        {
            "title": "5.2 Experiment 2: High Resolution Code",
            "content": "In this setting, we analyze fine-grained student progress by looking at distributionand stream6 Table 2: Experiment 1: Number of PEP 8 violations and style score across models and contexts (with context=T, without context=F). The metrics are reported at the final submission bin for both test sets: test_NS_OP (a) and test_NS_NP (b). Each cell shows Mean (Std) and MAE (i.e., pairwise style score difference against the corresponding student code). On test_NS_OP, qwen-student produces code that generally mimics the number of PEP 8 violations and style score of students. On test_NS_NP, the performance differences across models are variable, with significantly lower style scores compared to student code. (a) test_NS_OP (b) test_NS_NP Model Bin Context PEP 8 Viol. Style Score Model Bin Context PEP 8 Viol. Style Score gpt-4.1 last qwen-inst last qwen-student last gpt-4.1 last qwen-inst last qwen-student last Student last F T Mean (Std) MAE Mean (Std) MAE 5.84 (1.67) 5.00 (0.00) 6.22 (2.08) 5.91 (2.23) 5.60 (1.99) 7.18 (3.45) 4.49 4.13 3. 4.20 4.24 4.40 -0.96 (0.56) -0.64 (1.48) 0.41 (0.75) -0.57 (1.01) -0.47 (1.34) 0.33 (1.08) 1.85 2.07 1.26 1.87 1.80 1.28 gpt-4.1 last qwen-inst last qwen-student last gpt-4.1 last qwen-inst last qwen-student last 7.49 (4.69) 0.89 (1.28) Student last Mean (Std) MAE Mean (Std) MAE 8.99 (1.09) 7.94 (1.56) 8.07 (2.24) 8.55 (1.10) 7.99 (1.24) 9.04 (3.30) 4.53 4.75 4.83 4.71 4.60 4. 0.30 (0.87) 0.13 (0.63) -0.13 (0.77) 0.15 (0.91) -0.04 (0.45) -0.04 (1.06) 1.08 1.08 1.27 1.19 1.05 1.30 8.79 (5.36) 0.78 (1.25) T Table 3: Experiment 2: Summary of pass rate, PEP 8 violations, style score, and embedding similarity metrics. Each cell shows mean (standard deviation) and MAE (computed with respect to the corresponding student submission). For embedding similarity, we report the mean cosine distance. Bolded mean values indicate closest to the student averages; for MAE and cosine distance, bolded values indicate the lowest scores. qwen-student generates code closest to that of students across all metrics for both test scenarios. (a) test_NS_OP (b) test_NS_NP Model Pass Rate (%) PEP 8 Viol. Style Score Cosine Dist. Model Pass Rate (%) PEP 8 Viol. Style Score Cosine Dist. Mean (Std) MAE Mean (Std) MAE Mean (Std) MAE MAE Mean (Std) MAE Mean (Std) MAE Mean (Std) MAE MAE gpt-4.1 qwen-inst qwen-student 10.5 (0.29) 0.11 7.00 (3.57) 1.12 96.7 (1.79) 0.87 7.00 (3.18) 3.50 -0.04 (1.30) 1.44 24.6 (0.40) 0.27 5.79 (2.71) 2.70 -0.03 (1.47) 0.07 0. 0.70 (1.69) Student 9.8 (0.28) 6.92 (3.65) 0.64 (1.65) 0.10 0.07 0.02 gpt-4.1 qwen-inst qwen-student 100.0 (0.02) 0.88 8.57 (1.61) 4.77 0.08 (0.73) 1.98 0.40 8.27 (3.24) 3.79 0.36 (1.11) 1.85 41.6 (0.48) 0.12 9.37 (6.00) 1.28 1.69 (2.42) 0.58 6.3 (0.20) Student 12.1 (0.29) 9.44 (6.02) 1.66 (2.41) 0.09 0.08 0.03 level statistics. This scenario allows us to assess how well different models replicate the step-bystep progress of student solutions over time. Summary Statistics. Tab. 3 shows that qwen-student consistently yields the closest mean and lowest MAE in pass rate, PEP 8 violations, style score, and embedding distance in both context settings. Prompt-based models are less aligned: qwen-inst underperforms in correctness and style, while gpt-4.1 overpredicts correctness and produces less student-like code. This pattern holds for both test_NS_OP and test_NS_NP settings. Test Pass Rate Progress. Fig. 4a and Fig. 4b show that qwen-student best mirrors the pass rate improvement trend of student submissions, contrary to gpt-4.1 (near 100%) and qwen-inst (25%-50%) pass rates staying approximately constant across the stream. Style Score Over Progress. On test_NS_OP (Fig. 4c), qwen-student closely tracks the gradual increase of student style score across the stream. gpt-4.1 and qwen-inst, by contrast, show flat trends, with lower style scores. On test_NS_NP (Fig. 4d), qwen-student exhibits an upward trend, but the gap with that of the student widens, suggesting struggles in generalizing to new problem. Code Edit Distance Progress. Fig. 4e and Fig. 4f reveal that qwen-student makes smaller, more incremental edits, mirroring real student behavior. In contrast, gpt-4.1 and qwen-inst make large jumps between attempts, indicating less realistic revision patterns."
        },
        {
            "title": "6 Discussion",
            "content": "This work introduces ParaStudent, framework that combines fine-tuned LLMs to generate studentlike code and multi-dimensional metrics to evaluate realistic, student-like code. Our experiments show that fine-tuning LLMs on real student code results in model-generated outputs that better capture realistic coding patterns, such as diverse error types, stylistic variability, and incremental edits, compared to prompt-based baselines, which tend to generate static and overly polished code. We highlight three key takeaways. Multi-dimensional evaluation. Functional correctness alone is not sufficient to assess whether code is student-like. Our results show the impor7 (a) test_NS_OP (b) test_NS_NP (c) test_NS_OP (d) test_NS_NP (e) test_NS_OP (f) test_NS_NP Figure 4: Experiment 2: Progression of pass rate (top), style score (middle), and code edits (bottom) across normalized submission steps when student context is provided. qwen-student aligns the closest with that of the student curve in all metrics. prev_num is the number of prior attempts provided to the model as context. tance of evaluating across semantics, functionality, and style. Evaluating code as part of stream (i.e., iterative learning) provides richer signal compared to static data point-level evaluation. Granularity matters. In low-resolution settings, models aligned well in the first and last submission stages due to more predictable behavior at the beginning and end of the stream; the middle stage showed the most variation. High-resolution experiments revealed that fine-tuned models can better simulate student trajectories even in middle stages. Fine-tuning outperforms prompting. Promptbased models often default to producing correct, concise code without stylistic variation or exhibiting realistic learning dynamics. In contrast, finetuned models capture the messiness of student learning. These implicit patterns cannot be easily simulated through prompting. Broadly, our findings advocate for deeper integration between NLP and education. ParaStudent can enable applications such as realistic data generation for benchmarking educational models when student data is scarce, or training tutor agents that reason about intermediate student attempts rather than simply final answers."
        },
        {
            "title": "7 Conclusion",
            "content": "We present ParaStudent, comprehensive framework for generating and evaluating student-like code using LLMs. Beyond functional correctness, our approach emphasizes stylistic fidelity and the ability to capture incremental learning patterns. Through both lowand high-resolution experiments across inand out-of-distribution test sets, we showed that fine-tuning leads to outputs that better mirror actual student behavior than promptbased alternatives. Our results demonstrate that qwen-student captures not only the semantic and structural characteristics of student code but also their learning trajectory. While generalization to new problems remains challenging, our findings demonstrate the potential of fine-tuned LLMs as tools for simulating realistic student code."
        },
        {
            "title": "8 Limitations",
            "content": "While our findings highlight the importance of finetuning and holistic evaluation of LLMs in student 8 code generation settings, several limitations must be acknowledged. This work is limited to single introductory programming course. The training and test datasets are drawn from different semesters of the same course. As result, the generalizability of our framework to other courses, programming languages, or levels of difficulty remains an open question and is left to future work. Due to limited computational resources, we conduct all fine-tuning experiments on single model: Qwen 2.5 Coder 7B. We ablate additional model families (e.g., Qwen 3 8B, Llama 3.1 8B) and smaller model variants (e.g., Qwen 2.5 Coder 3B) for only one experimental setting. Our prompt-based evaluations are also limited to two models: Qwen 2.5 Coder 7B Instruct and GPT 4.1. more comprehensive comparison across broader range of model types (e.g., reasoning models), sizes, and families is an important direction for future research. We also focus on standard supervised finetuning using LoRA and leave the exploration of other fine-tuning techniques to future work. Notably, this form of fine-tuning does not offer any privacy guarantees regarding the generated data or the underlying model. If such models are to be deployed in real-world educational settings or their outputs released publicly, privacy-preserving approaches, such as differentially private fine-tuning (Yu et al., 2022), should be considered. It should be noted that in the high-resolution setting (Experiment 2), models predict the students next submission conditioned on ground-truth prior attempts (strong supervision regime). This setup applies to both finetuning and prompting experiments. Autoregressive generation of full submission streams with little or no supervision is left to future work. Second, realistic student-like code generation could potentially be misused for academic dishonesty, such as automatically generating plausible but incorrect submissions for cheating purposes. And last but not least, any deployment of such systems in educational settings must be done with care, including appropriate safeguards for data anonymization, ethical use, and equitable access."
        },
        {
            "title": "References",
            "content": "Alireza Ahadi, Raymond Lister, Shahil Lal, and Arto Hellas. 2018. Learning programming, syntax errors and institution-specific factors. In Proceedings of the 20th Australasian Computing Education Conference, ACE 18, page 9096. Association for Computing Machinery. Amjad Altadmri and Neil C.C. Brown. 2015. 37 million compilations: Investigating novice programming mistakes in large-scale student data. In Proceedings of the 46th ACM Technical Symposium on Computer Science Education, SIGCSE 15, page 522527. Association for Computing Machinery. Anthropic. 2025. Claude 3.7 sonnet and claude code. Benjamin Bloom. 1984. The 2 sigma problem: The search for methods of group instruction as effective as one-to-one tutoring. Educational researcher, 13(6):416. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, and 39 others. 2021a. Evaluating large language models trained on code. Preprint, arXiv:2107.03374. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, and 1 others. 2021b. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. 2024. Teaching large language models to self-debug. In The Twelfth International Conference on Learning Representations. Albert Corbett, Kenneth Koedinger, and John Anderson. 1997. Intelligent tutoring systems. In Handbook of human-computer interaction, pages 849874. Elsevier. Potential Risks. While simulating student code can offer pedagogical benefits, it also raises several risks. First, if misapplied, such models could reinforce incorrect programming habits or misconceptions by overfitting to common student errors. Giuseppe De Ruvo, Ewan Tempero, Andrew LuxtonReilly, Gerard B. Rowe, and Nasser Giacaman. 2018. Understanding semantic style by analysing student code. In Proceedings of the 20th Australasian Computing Education Conference, ACE 18, page 7382. Association for Computing Machinery. 9 Google Deepmind. 2025. Gemini 2.5 pro. DeepSeek-AI, Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Y. Wu, Yukun Li, Huazuo Gao, Shirong Ma, Wangding Zeng, Xiao Bi, Zihui Gu, Hanwei Xu, Damai Dai, Kai Dong, Liyue Zhang, Yishi Piao, and 21 others. 2024. Deepseek-coder-v2: Breaking the barrier of closed-source models in code intelligence. CoRR, abs/2406.11931. Paul Denny, Andrew Luxton-Reilly, Ewan Tempero, and Jacob Hendrickx. 2011. Understanding the syntax barrier for novices. In Proceedings of the 16th Annual Joint Conference on Innovation and Technology in Computer Science Education, ITiCSE 11, page 208212. Association for Computing Machinery. Shihan Dou, Yan Liu, Haoxiang Jia, Limao Xiong, Enyu Zhou, Wei Shen, Junjie Shan, Caishuang Huang, Xiao Wang, Xiaoran Fan, Zhiheng Xi, Yuhao Zhou, Tao Ji, Rui Zheng, Qi Zhang, Xuanjing Huang, and Tao Gui. 2024. Stepcoder: Improve code generation with reinforcement learning from compiler feedback. CoRR, abs/2402.01391. Ryan Ehrlich, Bradley Brown, Jordan Juravsky, Ronald Clark, Christopher Ré, and Azalia Mirhoseini. 2025. Codemonkeys: Scaling test-time compute for software engineering. Preprint, arXiv:2501.14723. Andrew Ettles, Andrew Luxton-Reilly, and Paul Denny. 2018. Common logic errors made by novice programmers. In Proceedings of the 20th Australasian Computing Education Conference, ACE 18, page 8389. Association for Computing Machinery. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, and 542 others. 2024. The llama 3 herd of models. Preprint, arXiv:2407.21783. Xiaodong Gu, Meng Chen, Yalan Lin, Yuhan Hu, Hongyu Zhang, Chengcheng Wan, Zhao Wei, Yong Xu, and Juhong Wang. 2025. On the effectiveness of large language models in domain-specific code generation. ACM Trans. Softw. Eng. Methodol., 34(3). Joy He-Yueya, Noah D. Goodman, and Emma Brunskill. 2024. Evaluating and optimizing educational content with large language model judgments. In Proceedings of the 17th International Conference on Educational Data Mining, pages 6882. International Educational Data Mining Society. Edward Hu, yelong shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, and 1 others. 2024. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186. Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim. 2024a. survey on large language models for code generation. Preprint, arXiv:2406.00515. Xue Jiang, Yihong Dong, Lecheng Wang, Zheng Fang, Qiwei Shang, Ge Li, Zhi Jin, and Wenpin Jiao. 2024b. Self-planning code generation with large language models. ACM Trans. Softw. Eng. Methodol., 33(7). Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. 2024. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations. Mohammad Khalil, Farhad Vadiee, Ronas Shakya, and Qinyi Liu. 2025. Creating artificial students that never existed: Leveraging large language models and In Proceedctgans for synthetic data generation. ings of the 15th International Learning Analytics and Knowledge Conference, LAK 25, page 439450. Association for Computing Machinery. Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu Hong Hoi. 2022. Coderl: Mastering code generation through pretrained models In Advances in and deep reinforcement learning. Neural Information Processing Systems, volume 35, pages 2131421328. Juho Leinonen, Paul Denny, Olli Kiljunen, Stephen MacNeil, Sami Sarsa, and Arto Hellas. 2025. Llmitation is the sincerest form of data: Generating synthetic buggy code submissions for computing education. In Proceedings of the 27th Australasian Computing Education Conference, ACE 25, page 5663. Association for Computing Machinery. Jia Li, Chongyang Tao, Jia Li, Ge Li, Zhi Jin, Huangzhao Zhang, Zheng Fang, and Fang Liu. 2025. Large language model-aware in-context learning for code generation. ACM Trans. Softw. Eng. Methodol. Junjie Li, Fazle Rabbi, Cheng Cheng, Aseem Sangalay, Yuan Tian, and Jinqiu Yang. 2024a. An exploratory study on fine-tuning large language models for secure code generation. Preprint, arXiv:2408.09078. Samuel Holt, Max Ruiz Luyten, and Mihaela van der Schaar. 2024. L2MAC: Large language model automatic computer for extensive code generation. In The Twelfth International Conference on Learning Representations. Xiangyang Li, Kuicai Dong, Yi Quan Lee, Wei Xia, Yichun Yin, Hao Zhang, Yong Liu, Yasheng Wang, and Ruiming Tang. 2024b. Coir: comprehensive benchmark for code information retrieval models. CoRR, abs/2407.02883. 10 Jiate Liu, Yiqin Zhu, Kaiwen Xiao, QIANG FU, Xiao Han, Yang Wei, and Deheng Ye. 2023. RLTF: Reinforcement learning from unit test feedback. Transactions on Machine Learning Research. Jiawei Liu, Songrun Xie, Junhao Wang, Yuxiang Wei, Yifeng Ding, and Lingming Zhang. 2024a. Evaluating language models for efficient code generation. In First Conference on Language Modeling. Ye Liu, Rui Meng, Shafiq Joty, Silvio Savarese, Caiming Xiong, Yingbo Zhou, and Semih Yavuz. 2024b. Codexembed: generalist embedding model family for multiligual and multi-task code retrieval. Preprint, arXiv:2411.12644. Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, and 47 others. 2024. Starcoder 2 and the stack v2: The next generation. Preprint, arXiv:2402.19173. Xinyi Lu and Xu Wang. 2024. Generative students: Using llm-simulated student profiles to support question item evaluation. In Proceedings of the Eleventh ACM Conference on Learning @ Scale, L@S 24, page 1627. Association for Computing Machinery. Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2024. Wizardcoder: Empowering code large language models with evolinstruct. In The Twelfth International Conference on Learning Representations. Zeyuan Ma, Hongshu Guo, Jiacheng Chen, Guojun Peng, Zhiguang Cao, Yining Ma, and Yue-Jiao Gong. 2024. Llamoco: Instruction tuning of large language models for optimization code generation. CoRR, abs/2403.01131. Stephen MacNeil, Magdalena Rogalska, Juho Leinonen, Paul Denny, Arto Hellas, and Xandria Crosland. 2024. Synthetic students: comparative study of bug distribution between large language models and computing students. In Proceedings of the 2024 on ACM Virtual Global Computing Education Conference V. 1, SIGCSE Virtual 2024, page 137143. Association for Computing Machinery. Julia M. Markel, Steven G. Opferman, James A. Landay, and Chris Piech. 2023. Gpteach: Interactive ta training with gpt-based students. In Proceedings of the Tenth ACM Conference on Learning @ Scale, L@S 23, page 226236. Association for Computing Machinery. Nikolaos Nikolaidis, Karolos Flamos, Khanak Gulati, Daniel Feitosa, Apostolos Ampatzoglou, and Alexander Chatzigeorgiou. 2024. comparison of the effectiveness of chatgpt and co-pilot for generating quality python code solutions. In 2024 IEEE International Conference on Software Analysis, Evolution and Reengineering - Companion (SANER-C), pages 93101. Robert E. Noonan. 1985. An algorithm for genComput. Lang., erating abstract syntax trees. 10(34):225236. Theo X. Olausson, Jeevana Priya Inala, Chenglong Wang, Jianfeng Gao, and Armando Solar-Lezama. 2023. Demystifying gpt self-repair for code generation. CoRR, abs/2306.09896. OpenAI. 2025. Introducing gpt-4.1 in the api. Charles Packer, Sarah Wooders, Kevin Lin, Vivian Fang, Shishir G. Patil, Ion Stoica, and Joseph E. Gonzalez. 2024. Memgpt: Towards llms as operating systems. Preprint, arXiv:2310.08560. Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, and Yizhe Zhang. 2024. Training software engineering agents and verifiers with swe-gym. Preprint, arXiv:2412.21139. Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. 2024. Gorilla: Large language model connected with massive apis. In Advances in Neural Information Processing Systems, volume 37, pages 126544126565. Thai Tang Quoc, Duc Ha Minh, Tho Quan Thanh, and Anh Nguyen-Duc. 2024. An empirical study on selfcorrecting large language models for data science code generation. Preprint, arXiv:2408.15658. Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, and 7 others. 2024. Code llama: Open foundation models for code. Preprint, arXiv:2308.12950. Ben Shi, Michael Tang, Karthik Narasimhan, and Shunyu Yao. 2024. Can language models solve In First Conference on olympiad programming? Language Modeling. Parshin Shojaee, Aneesh Jain, Sindhu Tipirneni, and Chandan K. Reddy. 2023. Execution-based code generation using deep reinforcement learning. Transactions on Machine Learning Research. Zhihong Sun, Chen Lyu, Bolun Li, Yao Wan, Hongyu Zhang, Ge Li, and Zhi Jin. 2024. Enhancing code generation performance of smaller models by distilling the reasoning ability of LLMs. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 58785895. ELRA and ICCL. Yun-Da Tsai, Mingjie Liu, and Haoxing Ren. 2024. Code less, align more: Efficient llm fine-tuning for code generation with data pruning. CoRR, abs/2407.05040. 11 Guido van Rossum, Barry Warsaw, and Alyssa Coghlan. 2025. Pep 8 style guide for python code. Martin Weyssow, Xin Zhou, Kisub Kim, David Lo, and Houari Sahraoui. 2025. Exploring parameterefficient fine-tuning techniques for code generation with large language models. ACM Trans. Softw. Eng. Methodol. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, and 3 others. 2020. Huggingfaces transformers: State-of-the-art natural language processing. Preprint, arXiv:1910.03771. Man Fai Wong and Chee Wei Tan. 2024. Aligning crowd-sourced human feedback for code generation with bayesian inference. In 2024 IEEE Conference on Artificial Intelligence (CAI), pages 158163. Weixiang Yan, Haitian Liu, Yunkun Wang, Yunzhe Li, Qian Chen, Wen Wang, Tingyu Lin, Weishan Zhao, Li Zhu, Hari Sundaram, and Shuiguang Deng. 2024. CodeScope: An execution-based multilingual multitask multidimensional benchmark for evaluating LLMs on code understanding and generation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics. Guang Yang, Yu Zhou, Xiang Chen, Xiangyu Zhang, Terry Yue Zhuo, and Taolue Chen. 2024a. Chainof-thought in neural code generation: From and for lightweight language models. IEEE Transactions on Software Engineering, 50(9):24372457. John Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. 2024b. Swe-agent: Agent-computer interfaces enable automated software engineering. In Advances in Neural Information Processing Systems, volume 37, pages 5052850652. Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin Inan, Gautam Kamath, Janardhan Kulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz, Sergey Yekhanin, and Huishuai Zhang. 2022. Differentially private fine-tuning of language models. In International Conference on Learning Representations. Murong Yue, Wijdane Mifdal, Yixuan Zhang, Jennifer Suh, and Ziyu Yao. 2024. Mathvc: An llm-simulated multi-character virtual classroom for mathematics education. CoRR, abs/2404.06711. Shun Zhang, Zhenfang Chen, Yikang Shen, Mingyu Ding, Joshua B. Tenenbaum, and Chuang Gan. 2023. Planning with large language models for code generation. In The Eleventh International Conference on Learning Representations. Jiasheng Zheng, Boxi Cao, Zhengzhao Ma, Ruotong Pan, Hongyu Lin, Yaojie Lu, Xianpei Han, and Le Sun. 2024. Beyond correctness: Benchmarking multi-dimensional code generation for large language models. Li Zhong, Zilong Wang, and Jingbo Shang. 2024. Debug like human: large language model debugger via verifying runtime execution step by step. In Findings of the Association for Computational Linguistics: ACL 2024, pages 851870. Association for Computational Linguistics. Terry Yue Zhuo, Armel Zebaze, Nitchakarn Suppattarachai, Leandro von Werra, Harm de Vries, Qian Liu, and Niklas Muennighoff. 2024. Astraios: Parameter-efficient instruction tuning code large language models. Preprint, arXiv:2401.00788."
        },
        {
            "title": "A Data",
            "content": "Privacy. The data was logged as part of the normal educational practice by course instructors. We later received IRB exemption for research purposes (protocol ID: 2023-09-16725). Student IDs and email addresses in the logs are fully anonymized. The resulting data does not contain any Personally Identifiable Information (PII) or harmful data. Documentation. Data contains student code submissions for an introductory programming course at the University of California, Berkeley. We filter data from Spring 2021, Fall 2021, Spring 2022, and Fall 2022 semesters. Additionally, we filter only assignment problems in the Python programming language. The final data contains 5,478 students, 22 assignments, 33 problems, and total of 689,023 code submissions. Student demographic information is not available due to privacy regulations."
        },
        {
            "title": "B Models",
            "content": "Models. We used the following models for finetuning experiments: Qwen 2.5 Coder {3B, 7B} (Hui et al., 2024) and Llama 3.1 8B (Grattafiori et al., 2024). We used the following models for prompting experiments: Qwen 2.5 Coder 7B Instruct (Hui et al., 2024) and GPT 4.1 (OpenAI, 2025). Infrastructure and Cost. All fine-tuning experiments were run on single Standard NC40ads H100 v5 (40 vcpus, 320 GiB memory) GPU on Microsoft Azure. Across all experiments, including model ablations, the total compute usage was 66.2 GPU hours. We used the same machine for data generation experiments on open-source mod12 and style score (Tab. 5) for fine-tuning ablations using llama-3.8b, qwen-coder-3b, and qwen-3.8b. 3. Different sets of problems. Code embedding visualization  (Fig. 10)  , embedding-based metrics (Tab. 6), error type distribution  (Fig. 11)  , and progress metrics  (Fig. 12)  on different test sets (test_NS_OP_v2 and test_NS_NP_v2). These problems are more introductory level than the ones covered in the main paper. els. We used Azures OpenAI API to sample from GPT-4.1 (with total cost of 245 USD). B.1 Fine-tuning Due to the large number of experiments and model variants, we used Low-Rank Adaptation (LoRA) (Hu et al., 2022) for parameter-efficient fine-tuning. We followed commonly used parameters: rank r=16, scaling factor α=32, dropout rate of 0.05. LoRA adapters were applied to all linear layers. Models were fine-tuned for one epoch with batch size of 16, using learning rate of 104, cosine learning rate scheduler, and the AdamW optimizer. All experiments were conducted using bfloat16 precision (bf16=True). HuggingFaces (Wolf et al., 2020) transformers and peft libraries were used for fine-tuning. For sampling from fine-tuned models, we followed common practice and used the following sampling parameters: temperature=0.7, top-p=0.8, top-k=20, and min-p=0.0. Prompt templates are shown in Fig. 5; same prompt templates were used for formatting the training data. All code snippets in training and sampling data (including skeleton code snippets) are wrapped in <code> and </code>. B.2 Prompting For sampling from prompt-based models, we followed common practice and used the following parameters: (1) Qwen 2.5 Coder 7B Instruct (temperature=0.7, top-p=0.8, top-k=20, and min-p=0.0), (2) GPT-4.1 (temperature=1.0, top-p=1.0). We used the following system prompt for both models: You are helpful assistant simulating student in an introduction to Python programming course working on homework problem. User prompt templates are shown in Fig. 6."
        },
        {
            "title": "C Results",
            "content": "This appendix provides extended results complementing the main paper. We organize the content into three sections: 1. No context setting. Error type distribution  (Fig. 7)  and progress metrics  (Fig. 8)  in the setting when context is not provided. 2. Fine-tuning ablations. Embedding-based metrics (Tab. 4), Error type distribution  (Fig. 9)  , and number of PEP8 violations 13 Figure 5: Prompt templates for fine-tuned models (Qwen 2.5 Coder 3B, 7B and Llama 3.1 8B). Top left (Experiment 1 without context), top right (Experiment 1 with context), bottom left (Experiment 2 without context), bottom right (Experiment 2 with context). Same prompt templates are used for formatting the training data. All code snippets are wrapped in <code> and </code>. Table 4: Experiment 1: Distribution-level embedding-based metrics across ablation models (llama-3.8b, qwen-coder-3b, and qwen-3.8b.) and stages (first, middle, and last) (a) test_NS_OP (b) test_NS_NP"
        },
        {
            "title": "Bin",
            "content": "Context Avg. KNN Dist. KNN Cov."
        },
        {
            "title": "Bin",
            "content": "Context Avg. KNN Dist. KNN Cov. llama-3.8b first qwen-coder-3b first qwen-3.8b first llama-3.8b middle qwen-coder-3b middle qwen-3.8b middle llama-3.8b last qwen-coder-3b last qwen-3.8b last T T 0.055 0.049 0.050 0.061 0.057 0.062 0.055 0.053 0.062 82.2% 82.2% 84.4% 80.0% 73.3% 77.8% 66.7% 80.0% 60.0% llama-3.8b first qwen-coder-3b first qwen-3.8b first llama-3.8b middle qwen-coder-3b middle qwen-3.8b middle llama-3.8b last qwen-coder-3b last qwen-3.8b last T T 0.075 0.085 0.084 0.073 0.066 0.059 0.067 0.045 0.050 68.8% 51.9% 46.8% 52.1% 56.3% 60.6% 50.7% 57.1% 57.1% Table 5: Experiment 1: Number of PEP 8 violations and style score across models and contexts (with context=T) for ablation models. The metrics are reported at the final submission bin for both test sets: test_NS_OP (a) and test_NS_NP (b). Each cell shows Mean (Std) and MAE (i.e., pairwise style score difference against the corresponding student code). (a) test_NS_OP (b) test_NS_NP Model Bin Context PEP 8 Viol. Style Score Model Bin Context PEP 8 Viol. Style Score Mean (Std) MAE Mean (Std) MAE Mean (Std) MAE Mean (Std) MAE llama-3.8b last qwen-coder-3b last qwen-3.8b last"
        },
        {
            "title": "Student",
            "content": "last"
        },
        {
            "title": "T\nT\nT",
            "content": "6.80 (3.27) 7.16 (5.79) 6.71 (2.68) 4.29 5.09 3.80 0.43 (0.95) -0.05 (1.31) 0.28 (0.77) 1.22 1.59 1.16 llama-3.8b last qwen-coder-3b last qwen-3.8b last 7.49 (4.69) 0.89 (1.28)"
        },
        {
            "title": "Student",
            "content": "last"
        },
        {
            "title": "T\nT\nT",
            "content": "13.20 (4.64) 7.95 (2.50) 9.31 (2.62) 6.43 4.66 4.65 2.21 (1.93) -0.19 (0.69) 0.08 (0.73) 2.16 1.23 1.05 8.79 (5.36) 0.78 (1.25) 14 Figure 6: Prompt templates for prompting models (Qwen 2.5 Coder 7B Instruct and GPT-4.1). Top left (Experiment 1 without context), top right (Experiment 1 with context), bottom left (Experiment 2 without context), bottom right (Experiment 2 with context). (a) test_NS_OP (b) test_NS_NP Figure 7: Experiment 1: Error type distributions across stages (first, middle, last) and test sets (test_NS_OP, test_NS_NP) when context is not provided. 15 (a) test_NS_OP (b) test_NS_NP (c) test_NS_OP (d) test_NS_NP (e) test_NS_OP (f) test_NS_NP Figure 8: Experiment 2: Progression of pass rate (top), style score (middle), and code edits (bottom) across normalized submission steps when student context is not provided. (a) test_NS_OP (b) test_NS_NP Figure 9: Experiment 1: Error type distributions across stages (first, middle, last) and test sets (test_NS_OP, test_NS_NP) across ablation models. 16 (a) test_NS_OP (b) test_NS_NP Figure 10: Experiment 1: Code embeddings across three submission stages (first, middle, last) with (bottom) and without (top) context for student (black squares), qwen-student (orange circles), qwen-inst (green crosses), and gpt-4.1 (purple crosses) code submissions for different test sets. Table 6: Experiment 1: Distribution-level embedding-based metrics across models, stages (first, middle, and last), and contexts (context=T, without context=F) for different test sets. (a) test_NS_OP_v2 (b) test_NS_NP_v"
        },
        {
            "title": "Bin",
            "content": "Context Avg. KNN Dist. KNN Cov."
        },
        {
            "title": "Bin",
            "content": "Context Avg. KNN Dist. KNN Cov. gpt-4.1 first qwen-inst first qwen-student first gpt-4.1 first qwen-inst first qwen-student first gpt-4.1 middle qwen-inst middle qwen-student middle gpt-4.1 middle qwen-inst middle qwen-student middle gpt-4.1 last qwen-inst last qwen-student last gpt-4.1 last qwen-inst last qwen-student last T F T F T 0.052 0.056 0.027 0.052 0.051 0.021 0.042 0.047 0.025 0.040 0.043 0.025 0.027 0.032 0. 0.027 0.027 0.009 15.1% 18.6% 58.1% 15.1% 24.4% 65.1% 22.5% 14.1% 64.8% 25.4% 29.6% 63.4% 25.6% 12.8% 83.7% 25.6% 53.5% 87.2% gpt-4.1 first qwen-inst first qwen-student first gpt-4.1 first qwen-inst first qwen-student first gpt-4.1 middle qwen-inst middle qwen-student middle gpt-4.1 middle qwen-inst middle qwen-student middle gpt-4.1 last qwen-inst last qwen-student last gpt-4.1 last qwen-inst last qwen-student last T F T F T 0.115 0.113 0.082 0.108 0.104 0.063 0.094 0.093 0.078 0.092 0.082 0.074 0.074 0.074 0. 0.075 0.048 0.046 24.7% 53.3% 48.0% 27.3% 49.4% 71.4% 43.5% 71.7% 65.2% 41.3% 65.2% 73.9% 29.9% 66.2% 64.9% 18.2% 79.2% 59.7% (a) test_NS_OP_v2 (b) test_NS_NP_v2 Figure 11: Experiment 1: Error type distributions across stages (first, middle, last) on different test sets. 17 (a) test_NS_OP_v (b) test_NS_NP_v2 (c) test_NS_OP_v2 (d) test_NS_NP_v2 (e) test_NS_OP_v2 (f) test_NS_NP_v2 Figure 12: Experiment 2: Progression of pass rate (top), style score (middle), and code edits (bottom) across normalized submission steps when student context is not provided for different test sets."
        }
    ],
    "affiliations": [
        "University of California, Berkeley"
    ]
}