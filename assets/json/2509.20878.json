{
    "paper_title": "The Unanticipated Asymmetry Between Perceptual Optimization and Assessment",
    "authors": [
        "Jiabei Zhang",
        "Qi Wang",
        "Siyu Wu",
        "Du Chen",
        "Tianhe Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Perceptual optimization is primarily driven by the fidelity objective, which enforces both semantic consistency and overall visual realism, while the adversarial objective provides complementary refinement by enhancing perceptual sharpness and fine-grained detail. Despite their central role, the correlation between their effectiveness as optimization objectives and their capability as image quality assessment (IQA) metrics remains underexplored. In this work, we conduct a systematic analysis and reveal an unanticipated asymmetry between perceptual optimization and assessment: fidelity metrics that excel in IQA are not necessarily effective for perceptual optimization, with this misalignment emerging more distinctly under adversarial training. In addition, while discriminators effectively suppress artifacts during optimization, their learned representations offer only limited benefits when reused as backbone initializations for IQA models. Beyond this asymmetry, our findings further demonstrate that discriminator design plays a decisive role in shaping optimization, with patch-level and convolutional architectures providing more faithful detail reconstruction than vanilla or Transformer-based alternatives. These insights advance the understanding of loss function design and its connection to IQA transferability, paving the way for more principled approaches to perceptual optimization."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 8 7 8 0 2 . 9 0 5 2 : r Technical Report"
        },
        {
            "title": "THE UNANTICIPATED ASYMMETRY BETWEEN\nPERCEPTUAL OPTIMIZATION AND ASSESSMENT",
            "content": "Jiabei Zhang1, Qi Wang1, Siyu Wu2, Du Chen3, and Tianhe Wu4 1Institute of Microelectronics of the Chinese Academy of Sciences 2Beihang University 3The Hong Kong Polytechnic University 4City University of Hong Kong zhangjiabei22@ucas.ac.cn wangqi1@ime.ac.cn wusiyu@buaa.edu.cn csdud.chen@connet.polyu.hk tianhewu-c@my.cityu.edu.hk"
        },
        {
            "title": "ABSTRACT",
            "content": "Perceptual optimization is primarily driven by the fidelity objective, which enforces both semantic consistency and overall visual realism, while the adversarial objective provides complementary refinement by enhancing perceptual sharpness and fine-grained detail. Despite their central role, the correlation between their effectiveness as optimization objectives and their capability as image quality assessment (IQA) metrics remains underexplored. In this work, we conduct systematic analysis and reveal an unanticipated asymmetry between perceptual optimization and assessment: fidelity metrics that excel in IQA are not necessarily effective for perceptual optimization, with this misalignment emerging more distinctly under adversarial training. In addition, while discriminators effectively suppress artifacts during optimization, their learned representations offer only limited benefits when reused as backbone initializations for IQA models. Beyond this asymmetry, our findings further demonstrate that discriminator design plays decisive role in shaping optimization, with patch-level and convolutional architectures providing more faithful detail reconstruction than vanilla or Transformer-based alternatives. These insights advance the understanding of loss function design and its connection to IQA transferability, paving the way for more principled approaches to perceptual optimization1. Figure 1: High-performing perceptual metrics on the IQA benchmark FAIL to consistently improve visual quality in perceptual optimization, whether adversarial loss is employed or not. We build perceptual metrics with diverse backbone architectures as optimization objectives, where Rec. denotes the ℓ1 norm, VGG-16-R indicates VGG-16 (Simonyan & Zisserman, 2014) with random weights, and Swin-T refers to the Swin Transformer (Liu et al., 2021). Equal contribution. Corresponding author. 1Code: https://github.com/Oreki1999/AsymmetryIQA 1 Technical Report Figure 2: Qualitative comparison of SR results across different optimization objectives. While reconstruction and perceptual loss (VGG-16) tend to generate checkerboard artifacts, incorporating adversarial loss mitigates these distortions and yields more realistic textures."
        },
        {
            "title": "INTRODUCTION",
            "content": "Amid the rapid progress of artificial intelligence, an increasing number of visual generation models have demonstrated the ability to synthesize high-resolution, photorealistic, semantically consistent, and visually detailed images (Lin et al., 2025; Batifol et al., 2025). key factor driving these advances is perceptual optimization, fundamental technique that steers models toward generating visually realistic outputs while maintaining both structural fidelity and fine-grained textures relative to reference images (Wang et al., 2004). This approach has proven highly effective across broad spectrum of vision tasks, spanning image restoration, enhancement, and generation (Wu et al., 2024a; Liang et al., 2023; Sauer et al., 2024), and continues to serve as cornerstone for advancing both discriminative and generative visual modeling. Fidelity metrics have historically been adopted as the primary optimization objectives, guiding models to align generated images with their references (Wang et al., 2004). Traditional perceptual optimization typically relies on pixel-wise reconstruction losses (e.g., ℓ1 norm), which often produce over-smoothed results that diverge from human visual perception (HVS) (Liang et al., 2021) (see Fig.2). This shortcoming has motivated the adoption of deep full-reference image quality assessment (FR-IQA) metrics, such as LPIPS (Zhang et al., 2018), as auxiliary perceptual objectives to better preserve fidelity and produce visually consistent outputs (Ding et al., 2020). Nevertheless, as illustrated in Fig. 2, perceptual-oriented objectives can introduce irregular artifacts (e.g., checkerboard patterns), often resulting from surjective feature mappings or the complexity of deep backbone architectures (Ding et al., 2021b). To address these visually unnatural artifacts, adversarial losses are incorporated as complementary objectives operating alongside fidelity metrics that enforce semantic consistency and global coherence, thereby forming the perceptual optimization paradigm that couples reconstructionand perceptual-oriented fidelity terms with adversarial supervision (Wang et al., 2021; Esser et al., 2021; Sun et al., 2024; Tian et al., 2024). Building on this integral formulation, recent studies have increasingly underscored the significance of fidelity metrics, particularly perceptual ones, thereby motivating the development of more advanced IQA methodologies that more effectively steer models toward high-fidelity and perceptually convincing image generation (Lao et al., 2022; Chen et al., 2024; 2025). This naturally raises key question: Do fidelity metrics with stronger IQA capability necessarily yield greater effectiveness when repurposed for perceptual optimization? In parallel, given that discriminators are crucial for enforcing realism and suppressing artifacts during optimization, complementary question arises: Can their learned representations generalize sufficiently to serve as effective backbone initializations for IQA models? Motivated by these two questions, in this paper, we adopt single-image super-resolution (SR) as representative testbed and employ SwinIR (Liang et al., 2021) to explore perceptual optimization under diverse fidelity metrics and discriminator designs, with particular emphasis on their relationship to perceptual assessment. To this end, we first construct DISTS-style (Ding et al., 2020) injective perceptual metrics with varied visual backbones, thereby spanning range of objectives with different IQA capacities, and instantiate four distinct optimization objective settings in prac2 Technical Report tice for optimizing the SwinIR model. We then design discriminators based on both convolutional and Transformer architectures, and investigate the transferability of their learned representations by initializing IQA models with backbones extracted from SR-trained discriminators, in comparison to ImageNet-pretrained (Russakovsky et al., 2015) and randomly initialized counterparts under both FR and no-reference (NR) settings. Finally, given the central role of discriminators in enforcing perceptual realism and mitigating artifacts, we extend our analysis to discriminator variants, focusing on the widely used vanilla and patch-level designs2, and evaluate their training stability across alternative architectures, thereby advancing deeper understanding of their capacity in facilitating more effective perceptual optimization. Collectively, our study leads to the following key findings: We investigate the link between IQA-oriented fidelity metrics and optimization, and reveal that higher IQA capability does not necessarily translate into more effective optimization guidance, particularly when adversarial objectives are involved. We analyze the representational transfer of discriminators and demonstrate that, despite their effectiveness in artifact suppression, their learned features contribute little when repurposed for IQA initialization, highlighting fundamental asymmetry between optimization and assessment roles. We conduct controlled investigation of discriminator design, showing that patch-level discriminators enable more faithful detail reconstruction than their vanilla counterparts, while convolutional-based discriminators demonstrate superior training stability relative to Transformer-based alternatives."
        },
        {
            "title": "2 FORMULATION OF PERCEPTUAL OPTIMIZATION OBJECTIVES",
            "content": "In this section, we elaborate on the formulation of perceptual optimization losses, comprising composite, DISTS-style perceptual, and adversarial components. 2.1 COMPOSITE OBJECTIVE FOR PERCEPTUAL OPTIMIZATION Perceptual optimization typically employs composite objective that integrates two fidelity terms, reconstruction and perceptual, together with an adversarial objective (Wang et al., 2018; Sun et al., 2024). The reconstruction loss ℓrec enforces fidelity in low-frequency structures and color consistency (Dong et al., 2015), while the perceptual loss ℓper measures discrepancies in deep feature space and thereby encourages structural alignment and fine-grained texture preservation in accordance with the HVS (Zhang et al., 2018; Ding et al., 2020). Complementing these, the adversarial loss ℓadv promotes realism by pushing generated outputs toward the distribution of natural images (Goodfellow et al., 2014). Formally, the overall objective can be expressed as: ℓ(x, y) = λ1ℓrec(x, y) + λ2ℓper(x, y) + λ3ℓadv(x, y), (1) where and denote the generated and reference images, respectively, and λ1, λ2, λ3 0 are balancing weights. In this paper, we particularly focus on the latter two components, ℓper and ℓadv, as they play decisive roles in shaping perceptual realism. Our analysis therefore centers on disentangling their respective contributions and interactions, with the goal of clarifying their influence on optimization outcomes and their potential transferability to IQA tasks. 2.2 COMPUTATION OF DISTS-STYLE PERCEPTUAL METRICS To rigorously investigate the extent to which IQA performance reflects optimization utility, we develop family of perceptual metrics derived from the DISTS framework (Ding et al., 2020), wherein an injective feature transformation is applied to capture deep representations and global similarity is assessed across texture and structure dimensions. The symmetric perceptual metric ℓper(x, y) is defined by first applying visual feature transformation to obtain deep representations of and y. Let x(i) denote the j-th channel of the i-th layer features extracted from and y, respectively. and y(i) 2The vanilla discriminator outputs single global score for the entire image, whereas the patch-level discriminator generates map of real/fake predictions across local patches. Technical Report The texture similarity l() and structure similarity s() are computed as: , (2) (cid:16) j , y(i) x(i) (cid:17) = (cid:16) 2µ(i) (cid:17) xj µ(i) yj + c1 (cid:17)2 (cid:16) µ(i) yj (cid:16) , , y(i) x(i) (cid:17) = (cid:16) 2σ(i) (cid:16) (cid:17)2 xj yj + c2 (cid:17)2 σ(i) yj + µ(i) xj xj and µ(i) + c1 yj are the global means of x(i) σ(i) xj where µ(i) and y(i) xj and σ(i) yj are their variances, and σ(i) xj yj denotes the global covariance. Constants c1 and c2 are introduced to avoid numerical instability when denominators are close to zero. The overall DISTS-style perceptual metric is then formulated as weighted sum of texture and structure similarities across all feature layers and channels: , σ(i) + c2 + ℓper(x, y) = 1 (cid:88) ni(cid:88) (cid:16) i= j=1 αij (cid:16) , y(i) x(i) (cid:17) + βij (cid:16) , y(i) x(i) (cid:17)(cid:17) , (3) where is the total number of feature layers, ni represents the number of channels in the i-th layer, and αij, βij 0 denote learnable parameters that are required to satisfy the normalization constraint (cid:80)m (cid:80)ni i=0 j=1(αij + βij) = 1. 2.3 ADVERSARIAL LOSS IN PERCEPTUAL OPTIMIZATION Adversarial loss has become cornerstone in visual generation tasks (Ledig et al., 2017; Esser et al., 2021; Sauer et al., 2024), as it enhances perceptual realism by complementing fidelity objectives within perceptual optimization frameworks. It is formulated as two-player game between generator and discriminator, where the generator strives to synthesize realistic images while the discriminator learns to differentiate them from real samples. This adversarial interplay drives the generator toward outputs that are not only structurally faithful but also perceptually convincing. Within existing adversarial formulations, relativistic variants of GANs (Jolicoeur-Martineau, 2018) have been shown to be particularly effective for perceptual optimization. Following ESRGAN (Wang et al., 2018), we adopt an adversarial loss design based on relativistic discriminator. The generator loss is expressed as: ℓadv(x, y) = EyY [log (1 D(y, x))] ExX [log (D(x, y))] , while the discriminator loss is given by ℓd(x, y) = EyY [log (D(y, x))] ExX [log (1 D(x, y))] , where the asymmetric discrepancy D(x, y) is defined as: D(x, y) = σ (d (x) EyY [d (y)]) , with σ() denoting the sigmoid function, d() the discriminator, and E[] the mini-batch average. (4) (5) (6)"
        },
        {
            "title": "3 EXPERIMENTS",
            "content": "In this section, we systematically investigate perceptual optimization across diverse fidelity metrics and discriminator designs, with emphasis on their relation to quality assessment and on the stability and effectiveness of alternative discriminator architectures. 3.1 EXPERIMENTAL SETUPS Construction of Perceptual Metrics We construct family of DISTS-style perceptual metrics (Ding et al., 2020) by replacing the visual backbone introduced in Sec. 2.2, yielding objectives with different levels of IQA capability. Specifically, we examine three convolutional architectures: VGG-16 (Simonyan & Zisserman, 2014), ResNet-50 (He et al., 2016), and ConvNeXt (Liu et al., 2022), as well as two Transformer-based architectures, CLIP-ViT (Radford et al., 2021) and Swin Transformer (Liu et al., 2021). We also include variant that employs VGG-16 backbone, in which both the network parameters and the weighting factors αij and βij are randomly assigned and kept fixed (denoted VGG-16-R). Together, these configurations yield the perceptual objectives ℓVGG-16, ℓResNet-50, ℓConvNeXt, ℓCLIP-ViT, ℓSwin-T, and ℓVGG-16-R. For evaluation, we adopt four synthetic FR benchmarks: TID2013 (traditional) (Ponomarenko et al., 2015), Liu13 (deblurring) (Liu et al., 2013), Ma17 (super-resolution) (Ma et al., 2017), and TQD (texture similarity) (Ding et al., 2020), to comprehensively assess IQA performance across diverse scenarios. The complete training protocol and results are provided in the Appendix. 4 Technical Report Table 1: Comparison of perceptual metrics on the DIV2K validation set. The final two columns present the ℓ1-only and reference results. Std. denotes the standard deviation of NR-IQA scores. Top two results are highlighted in bold and underline, respectively. Metric VGG-16-R VGG-16 ResNet-50 ConvNeXt CLIP-ViT Swin-T Std. ℓ1 Ref. Perceptual MANIQA LIQE DeQA VQ-R1 Average 49.98 58.90 64.05 59.04 57.99 Reconstruction and Perceptual MANIQA LIQE DeQA VQ-R1 Average 50.05 58.91 63.35 59.32 57.91 Perceptual and Adversarial 54.03 58.30 64.04 61.12 59.37 53.96 58.39 62.36 60.59 58.83 57.28 66.76 62.04 61.13 61.80 MANIQA LIQE DeQA VQ-R1 Average 45.97 65.53 55.95 55.37 55.71 Reconstruction, Perceptual and Adversarial 48.04 66.93 59.32 57.15 57.86 MANIQA LIQE DeQA VQ-R1 Average 56.50 67.87 62.59 61.41 62.09 46.66 54.40 60.71 55.72 54.37 48.85 56.53 62.60 58.95 56.73 56.42 67.66 62.57 61.82 62.12 57.73 67.82 62.75 61.81 62.53 54.50 62.46 54.05 55.09 56. 54.27 61.92 60.49 59.95 59.16 56.66 67.20 61.54 61.15 61.64 56.21 66.73 61.06 60.42 61.11 28.02 59.02 41.56 39.56 42.04 38.67 66.11 54.58 52.07 52.86 56.95 67.72 61.83 61.44 61. 55.33 67.37 60.01 60.45 60.79 50.31 59.30 61.62 60.16 57.85 50.28 59.74 54.31 56.07 55.10 56.83 67.62 61.98 61.06 61.87 56.90 67.90 62.36 61.04 62.05 8.995 2.355 7.945 7.294 5. 5.182 3.068 3.756 2.940 2.211 4.055 0.770 2.273 2.232 2.307 3.247 0.467 1.324 1.526 1.555 48.50 54.57 53.56 51.31 51.98 48.50 54.57 53.56 51.31 51.98 48.50 54.57 53.56 51.31 51. 48.50 54.57 53.56 51.31 51.98 61.78 67.50 64.96 63.21 64.36 61.78 67.50 64.96 63.21 64.36 61.78 67.50 64.96 63.21 64.36 61.78 67.50 64.96 63.21 64.36 Configuration of SR Model Training We conduct perceptual optimization for SR using SwinIR (Liang et al., 2021), widely adopted discriminative backbone. Training is performed in two stages. In the first stage, the model is trained for 100K iterations with an ℓ1 reconstruction loss and an initial learning rate of 2 104. In the second stage, starting from weights pre-trained with reconstruction loss, we evaluate four settings: (1) perceptual loss only, (2) combined reconstruction and perceptual losses, (3) perceptual plus adversarial loss, and (4) the full objective in Eq. 1, where the weights are fixed to λ1 = 1 102, λ2 = 1, and λ3 = 5 103 as in (Wang et al., 2018). For settings (3) and (4), the discriminator is vanilla VGG-16 (Simonyan & Zisserman, 2014). This stage runs for 400K iterations with the same initial learning rate, decayed by half at 150K, 300K, 350K, and 375K steps. For adversarial objectives, generator and discriminator are alternately updated. All experiments are carried out on the DIV2K dataset (Agustsson & Timofte, 2017), where low-resolution inputs are generated by applying 4 bicubic downsampling to 256 256 reference images. We adopt Adam (Kingma & Ba, 2014) with batch size 32, and train all models in PyTorch on NVIDIA L40S GPUs. Metrics for Evaluating Visual Quality To rigorously evaluate the visual quality of images produced by models trained with different optimization objectives, we adopt four state-of-theart NR-IQA methods3: MANIQA (Yang et al., 2022), LIQE (Zhang et al., 2023), DeQA-Score (DeQA) (You et al., 2025), and VisualQuality-R1 (VQ-R1) (Wu et al., 2025), and normalize their outputs following Wu et al. (2024b); Chen et al. (2025) (Details are shown in the Appendix). 3.2 ANALYSIS OF PERCEPTUAL METRICS Table 4 and Table 1 comprehensively summarize the performance of various perceptual metrics, reporting both their IQA accuracy across multiple benchmarks and their effectiveness in guiding perceptual optimization for SR training. From these results, several insightful observations can be drawn, shedding light on the interplay between evaluation capability and optimization efficacy. Misalignment between Evaluation and Optimization Strong IQA performance does not reliably confer effective optimization guidance. As shown in Fig. 3, IQA capability exhibits no consistent 3FR-IQA metrics are unsuitable in this setting, as SR models optimized with distinct loss functions (e.g., models trained for PSNR are inherently biased toward PSNR-based evaluations) may yield unfair comparisons. 5 Technical Report Figure 3: Correlation between average IQA SRCC values and average SR visual quality scores. Across all settings, higher IQA SRCC does not reliably yield better perceptual optimization; the association is weak at best and becomes especially tenuous when adversarial loss is involved. Figure 4: Qualitative comparison of SR results under different optimization objectives. In the second training stage, adding reconstruction penalty yields negligible improvements; by contrast, adversarial loss substantially mitigates checkerboard artifacts. Moreover, with adversarial loss, SR outputs optimized by different perceptual metrics show only minor visual differences. correlation with optimization outcomes across the four training configurations. striking example is the randomly initialized ℓVGG-16-R, which, despite ranking last on all FR-IQA benchmarks, still significantly surpasses ℓResNet-50 and ℓCLIP-ViT in both the perceptual-only and reconstruction-plusperceptual settings. This discrepancy underscores fundamental mismatch between the roles of perceptual metrics when used as evaluators and when employed as optimization objectives. Marginal Role of Reconstruction Supervision As shown in Table 1, incorporating reconstruction term yields only marginal improvements, irrespective of adversarial supervision, implying redundancy of pixel-level guidance in second-stage perceptual optimization. In line with this observation, Fig. 4 demonstrates that the combined objective Rec.+VGG-16 offers negligible advantage over ℓVGG-16 alone in mitigating checkerboard artifacts, further underscoring the limited contribution of pixel-level supervision in this context. Effectiveness of Adversarial Supervision In contrast to the reconstruction term, adversarial loss ℓadv generally improves optimization across metrics, except for ℓVGG-16-R. As shown in Fig. 4, it yields sharper textures, more realistic details, and fewer artifacts than non-adversarial counterparts. These results suggest that, when combined with fidelity metric of sufficient IQA capability, adversarial supervision not only mitigates artifact formation but also complements perceptual metrics by steering SR training toward closer alignment with human visual preferences. Homogenization Effect of Adversarial Supervision The standard deviation values in Table 1 in the show that adversarial supervision markedly reduces disparities across perceptual metrics: Technical Report Table 2: Quantitative comparison of FR-IQA performance across FR benchmarks using VGG-16, DINOv2, and ResNet-50 with random, GAN, and ImageNet initializations. Backbone VGG-16 Random GAN ImageNet ResNet-50 Random GAN ImageNet DINOv2 Random GAN ImageNet TID2013 Ma17 SRCC PLCC SRCC PLCC SRCC PLCC SRCC PLCC SRCC PLCC Average Liu TQD 0.782 0.727 0.873 0.745 0.760 0.871 0.813 0.767 0.816 0.793 0.753 0.895 0.751 0.775 0. 0.817 0.791 0.860 0.860 0.859 0.929 0.808 0.823 0.900 0.839 0.851 0.913 0.873 0.862 0.935 0.839 0.816 0. 0.856 0.870 0.920 0.750 0.805 0.895 0.737 0.752 0.866 0.753 0.759 0.897 0.758 0.829 0.908 0.744 0.786 0. 0.754 0.773 0.910 0.374 0.303 0.715 0.379 0.312 0.604 0.429 0.361 0.276 0.549 0.466 0.731 0.521 0.496 0. 0.548 0.511 0.531 0.692 0.674 0.853 0.667 0.662 0.810 0.708 0.685 0.726 0.743 0.727 0.867 0.714 0.718 0. 0.744 0.736 0.805 Figure 5: Quantitative comparison of NR-IQA results on KADID-10K with VGG-16, DINOv2, and ResNet-50 under random, GAN, and ImageNet initialization. perceptual-only setting the deviation reaches 5.863, whereas with the full objective in Eq. 1 it drops to 1.555. Fig. 3 further corroborates this homogenization, as the inclusion of ℓadv yields tighter clustering of metrics in SR average score, thereby weakening the link between IQA accuracy and optimization effectiveness. Consistent with this trend, Fig. 4 shows that visual differences among methods become markedly less discernible, with reconstructed outputs appearing increasingly similar in texture and structure. Collectively, these results suggest that once reasonably accurate perceptual metric is in place, adversarial loss tends to exert dominant influence on the optimization process, raising the critical question of whether further advances in perceptual metric design can remain impactful in the presence of adversarial training. 3.3 GAN-BASED INITIALIZATION FOR IQA Building on the observation that adversarial supervision mitigates artifacts and enhances perceptual realism in SR, we investigate whether discriminator-learned representations encode perceptually relevant cues transferable to IQA models. We employ the convolutional architectures VGG-16 (Simonyan & Zisserman, 2014) and ResNet-50 (He et al., 2016), together with the Transformer architecture DINOv2 (Oquab et al., 2023)4, for SR adversarial training. The resulting discriminator backbones are then used as initializations for both FRand NR-IQA models, and their effectiveness is compared against random initialization and ImageNet pretraining (Russakovsky et al., 2015). For evaluation, we assess FR-IQA models on four synthetic benchmarks described in Sec. 3.1, and evaluate NR-IQA models on the KADID (Lin et al., 2019) test set. The training configurations for both FR and NR settings are detailed in the Appendix. Limited Transferability of Discriminator Features Table 2 and Fig. 5 indicate that ImageNet pretraining is consistently the most effective initialization, producing the highest SRCC and PLCC In comparison, initializing with GAN-trained discriminators across all backbones and datasets. yields only small gains over random initialization for ResNet-50 and DINOv2, with especially modest improvements for DINOv2. These findings suggest that adversarial supervision captures cues 4All networks are randomly initialized without loading any pretrained weights. 7 Technical Report Discriminator MANIQA LIQE DeQA VQ-R1 Average Vanilla VGG-16 ResNet-50 DINOv2 Patch-level VGG-16 ResNet-50 DINOv2 w/o (base) 56.50 59.01 57.81 57.74 58.87 57.79 53.96 67.87 68.03 67.20 67.72 68.24 67.57 58.39 62.59 64.81 62.48 63.20 66.05 62.74 62. 61.41 62.29 61.23 61.79 62.51 61.22 60.59 62.09 63.54 62.18 62.61 63.92 62.33 58.83 Table 3: Quantitative comparison of discriminator architectures in adversarial perceptual optimization. We evaluate vanilla and patch-level discriminators with VGG-16, ResNet-50, and DINOv2 backbones across four NR-IQA metrics, providing systematic assessment of their relative effectiveness in guiding perceptual optimization. Figure 6: Qualitative comparison of vanilla vs. patch-level discriminators. related to artifact suppression, but the resulting representations lack the semantic coverage and robustness required for generalizable quality assessment. This gap likely arises from mismatch between the discriminators narrow real-fake discrimination objective and IQAs broader requirement for sensitivity to diverse distortions and content. 3.4 FURTHER ANALYSIS OF DISCRIMINATORS The incorporation of adversarial loss has been shown to significantly enhance optimization toward human visual preference by facilitating the recovery of high-frequency details and perceptually plausible structures. Nonetheless, the effectiveness and stability of adversarial training are highly dependent on the design of the discriminator. To investigate this, we conduct systematic comparison between two widely adopted discriminator architectures: vanilla and patch-level. In our experiments, we adopt VGG-16 (Simonyan & Zisserman, 2014), ResNet-50 (He et al., 2016), and DINOv2 (Oquab et al., 2023) as discriminator backbones, modifying only the regression head to produce either vanilla or patch-level outputs, while employing ℓ1 and ℓVGG-16 as the reconstruction and perceptual objectives, respectively. Analysis of Effectiveness Table 3 reports comparison between vanilla and patch-level discriminators for adversarial supervision in perceptual optimization. Results show that introducing any discriminator markedly improves performance over the non-adversarial baseline. For VGG-16 and ResNet-50 backbones, patch-level designs consistently surpass vanilla ones, raising the average score by +0.52 and +0.38 points, respectively. The qualitative results in Fig. 6 align with these findings: patch-level discriminators generate sharper textures, clearer local structures, and fewer artifacts. By contrast, the DINOv2 backbone exhibits only marginal gain (+0.15 average), suggesting that Transformer-based discriminators are less responsive to patch-level supervision in this setting. Overall, patch-level supervision emerges as more effective and reliable default choice for convolutional backbones, delivering enhanced robustness and stronger guidance for local detail preservation. Analysis of Training Stability Generative adversarial training is notoriously fragile, being prone to non-convergence and mode collapse, and highly sensitive to hyperparameters (Mescheder et al., 2018). To probe this aspect, we examine training stability of patch-level discriminators under varying adversarial weights λ3 in Eq. 1 across different discriminator backbones. Fig. 7 shows Figure 7: Impact of GAN loss weight. Technical Report that the ResNet-50 discriminator achieves consistently high SR scores across broad range of weights, indicating robustness to hyperparameter variation, whereas VGG-16 shows moderate sensitivity with performance dropping at larger weights. By contrast, DINOv2 suffers from severe instability, with performance collapsing when the adversarial weight is increased, underscoring its limited suitability for stable adversarial optimization. This suggests that convolutional discriminators are inherently more stable, likely due to their stronger inductive biases and locality modeling."
        },
        {
            "title": "4 RELATED WORK",
            "content": "Perceptual Optimization Perceptual optimization has become central paradigm in image restoration and generation, motivated by the limitations of pixel-wise fidelity objectives such as ℓ1 norm or PSNR, which often fail to reflect human perceptual judgments. Early works introduced perceptual losses based on deep features from pretrained networks like VGG (Johnson et al., 2016; Ledig et al., 2017), shifting the focus from pixel correspondence to semantic and structural fidelity. Subsequent advances proposed task-specific perceptual metrics (Zhang et al., 2018; Chen et al., 2024; Lao et al., 2022) that better align with human opinion scores. In parallel, adversarial training emerged as complementary objective, with discriminators encouraging sharper textures and more perceptually realistic local structures. This paradigm has since been applied across diverse restoration and generation tasks (Wang et al., 2018; Sun et al., 2024; Tian et al., 2024). While prior studies (Blau & Michaeli, 2018; Ding et al., 2020; 2021b) mainly compared perceptual metrics for optimization, our work systematically examines their interplay with evaluation, with particular emphasis on the role of adversarial discriminators, thereby addressing an important gap. Perceptual Metrics Perceptual metrics aim to quantify visual differences between test image and its reference. Conventional FR-IQA methods assume the reference is of perfect quality, so the measured distance directly reflects test image quality. Early error-based measures such as MSE and MAE dominated the field but often diverge from human perception. To address this, SSIM (Wang et al., 2004) emphasized structural fidelity, later extended to multiscale (Wang et al., 2003) and feature-domain forms (Zhang et al., 2011). Recent deep-feature approaches, including DISTS (Ding et al., 2020) and its adaptive variant (Ding et al., 2021a), better capture structure and texture. Moving beyond the perfect-reference assumption, asymmetric metrics such as VIF (Sheikh & Bovik, 2006), CKDN (Zheng et al., 2021), and A-FINE (Chen et al., 2025) allow test images to surpass the reference. In perceptual optimization, however, most metrics still guide models to approximate the given reference, implicitly treating it as the perceptual upper bound."
        },
        {
            "title": "5 CONCLUSION AND DISCUSSION",
            "content": "We systematically examined how perceptual metrics and discriminator architectures affect perceptual optimization and IQA. We find that stronger fidelity metrics do not guarantee better optimization, discriminator features transfer poorly to IQA, and patch-level convolutional discriminators yield more stable and detailed results than vanilla or Transformer-based ones. Limitations and Future Directions This study has several limitations that suggest directions for future work. First, our experiments were conducted exclusively on SwinIR (Liang et al., 2021); extending the analysis to relatively weaker models (e.g., SRResNet (Ledig et al., 2017)), stronger models (e.g., HAT (Chen et al., 2023)), or even training visual tokenizers (Sun et al., 2024) would provide more comprehensive test of robustness. Second, the limited transferability of discriminator features to IQA may partly stem from the relatively small and narrow SR training data compared with large-scale, diverse datasets such as ImageNet (Russakovsky et al., 2015); scaling up training with more diverse data would allow for fairer and more conclusive assessment. Third, our study also reveals the pronounced role of discriminators in the second-stage SR training. Building on this, incorporating discriminator-driven designs into post-training for image generation with reinforcement learning (Liu et al., 2025) appears to be promising direction for future exploration. Finally, by highlighting this asymmetry, we question the prevailing practice of relying solely on perceptual metrics for optimization and assessing their capability based on the resulting outcomes (Ding et al., 2021b). We hope these findings motivate broader and sustained efforts to co-design perceptual metrics, adversarial objectives, and evaluation protocols, thereby ultimately advancing more principled, robust, and generalizable perceptual modeling. 9 Technical Report"
        },
        {
            "title": "REFERENCES",
            "content": "Eirikur Agustsson and Radu Timofte. NTIRE 2017 challenge on single image super-resolution: Dataset and study. In IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 126135, 2017. Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, et al. FLUX. 1 Kontext: Flow matching for in-context image generation and editing in latent space. arXiv e-prints, pp. arXiv 2506, 2025. Yochai Blau and Tomer Michaeli. The perception-distortion tradeoff. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 62286237, 2018. Chaofeng Chen, Jiadi Mo, Jingwen Hou, Haoning Wu, Liang Liao, Wenxiu Sun, Qiong Yan, and Weisi Lin. TOPIQ: top-down approach from semantics to distortions for image quality assessment. IEEE Transactions on Image Processing, 33:24042418, 2024. Du Chen, Tianhe Wu, Kede Ma, and Lei Zhang. Toward generalized image quality assessment: Relaxing the perfect reference quality assumption. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1274212752, 2025. Xiangyu Chen, Xintao Wang, Wenlong Zhang, Xiangtao Kong, Yu Qiao, Jiantao Zhou, and arXiv preprint Chao Dong. HAT: Hybrid attention Transformer for image restoration. arXiv:2309.05239, 2023. Keyan Ding, Kede Ma, Shiqi Wang, and Eero Simoncelli. Image quality assessment: Unifying structure and texture similarity. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(5):25672581, 2020. Keyan Ding, Yi Liu, Xueyi Zou, Shiqi Wang, and Kede Ma. Locally adaptive structure and texture similarity for image quality assessment. In ACM International Conference on Multimedia, pp. 24832491, 2021a. Keyan Ding, Kede Ma, Shiqi Wang, and Eero Simoncelli. Comparison of full-reference image quality models for optimization of image processing systems. International Journal of Computer Vision, 129(4):12581281, 2021b. Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Image super-resolution using deep convolutional networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38(2): 295307, 2015. Patrick Esser, Robin Rombach, and Boorn Ommer. Taming Transformers for high-resolution image synthesis. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12873 12883, 2021. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial Nets. In Advances in Neural Information Processing Systems, pp. 26722680, 2014. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recogIn IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 770778, nition. 2016. Vlad Hosu, Hanhe Lin, Tamas Sziranyi, and Dietmar Saupe. KonIQ-10k: An ecologically valid database for deep learning of blind image quality assessment. IEEE Transactions on Image Processing, 29:40414056, 2020. Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In European Conference on Computer Vision, pp. 694711, 2016. Jolicoeur-Martineau. The relativistic discriminator: key element missing from standard GAN. arXiv preprint arXiv:1807.00734, 2018. 10 Technical Report Diederik Kingma and Jimmy Ba. ADAM: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Shanshan Lao, Yuan Gong, Shuwei Shi, Sidi Yang, Tianhe Wu, Jiahao Wang, Weihao Xia, and Yujiu Yang. Attentions help CNNs see better: Attention-based hybrid image quality assessment network. In IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 11401149, 2022. Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic single image super-resolution using generative adversarial network. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 46814690, 2017. Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. SwinIR: Image restoration using Swin Transformer. In IEEE/CVF International Conference on Computer Vision Workshops, pp. 18331844, 2021. Zhexin Liang, Chongyi Li, Shangchen Zhou, Ruicheng Feng, and Chen Change Loy. Iterative prompt learning for unsupervised backlit image enhancement. In IEEE/CVF International Conference on Computer Vision, pp. 80948103, 2023. Hanhe Lin, Vlad Hosu, and Dietmar Saupe. KADID-10K: large-scale artificially distorted IQA database. In IEEE International Conference on Quality of Multimedia Experience, pp. 13, 2019. Xinqi Lin, Fanghua Yu, Jinfan Hu, Zhiyuan You, Wu Shi, Jimmy Ren, Jinjin Gu, and Chao Dong. Harnessing diffusion-yielded score priors for image restoration. arXiv preprint arXiv:2507.20590, 2025. Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-GRPO: Training flow matching models via online RL. arXiv preprint arXiv:2505.05470, 2025. Yiming Liu, Jue Wang, Sunghyun Cho, Adam Finkelstein, and Szymon Rusinkiewicz. noreference metric for evaluating the quality of motion deblurring. ACM Transactions on Graphics, 32(6):112, 2013. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin Transformer: Hierarchical vision Transformer using shifted windows. In IEEE/CVF International Conference on Computer Vision, pp. 1001210022, 2021. Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. ConvNet for the 2020s. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1197611986, 2022. Chao Ma, Chih-Yuan Yang, Xiaokang Yang, and Ming-Hsuan Yang. Learning no-reference quality metric for single-image super-resolution. Computer Vision and Image Understanding, 158:116, 2017. Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for GANs do actually converge? In International Conference on Machine Learning, pp. 34813490, 2018. Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. DINOv2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. Nikolay Ponomarenko, Lina Jin, Oleg Ieremeiev, Vladimir Lukin, Karen Egiazarian, Jaakko Astola, Benoit Vozel, Kacem Chehdi, Marco Carli, Federica Battisti, and Kuo J. C.-C. Image database TID2013: Peculiarities, results and perspectives. Signal Processing: Image Communication, 30: 5777, 2015. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 87488763, 2021. Technical Report Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. ImageNet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211252, 2015. Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. In European Conference on Computer Vision, pp. 87103, 2024. Hamid Sheikh and Alan Bovik. Image information and visual quality. IEEE Transactions on Image Processing, 15(2):430444, 2006. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. In Advances in Neural Information Processing Systems, pp. 8483984865, 2024. Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy. ESRGAN: Enhanced super-resolution generative adversarial networks. In European Conference on Computer Vision Workshops, pp. 6379, 2018. Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. Real-ESRGAN: Training real-world blind super-resolution with pure synthetic data. In IEEE/CVF International Conference on Computer Vision, pp. 19051914, 2021. Zhou Wang, Eero Simoncelli, and Alan Bovik. Multiscale structural similarity for image quality assessment. In Asilomar Conference on Signals, Systems and Computers, pp. 13981402, 2003. Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4):600 612, 2004. Rongyuan Wu, Tao Yang, Lingchen Sun, Zhengqiang Zhang, Shuai Li, and Lei Zhang. SeeSR: Towards semantics-aware real-world image super-resolution. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2545625467, 2024a. Tianhe Wu, Kede Ma, Jie Liang, Yujiu Yang, and Lei Zhang. comprehensive study of multimodal large language models for image quality assessment. In European Conference on Computer Vision, pp. 143160, 2024b. Tianhe Wu, Jian Zou, Jie Liang, Lei Zhang, and Kede Ma. VisualQuality-R1: Reasoning-induced image quality assessment via reinforcement learning to rank. arXiv preprint arXiv:2505.14460, 2025. Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, and Yujiu Yang. MANIQA: Multi-dimension attention network for no-reference image quality assessment. In IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 11911200, 2022. Zhiyuan You, Xin Cai, Jinjin Gu, Tianfan Xue, and Chao Dong. Teaching large language models to regress accurate image quality scores using score distribution. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1448314494, 2025. Lin Zhang, Lei Zhang, Xuanqin Mou, and David Zhang. FSIM: feature similarity index for image quality assessment. IEEE Transactions on Image Processing, 20(8):23782386, 2011. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable In IEEE/CVF Conference on Computer effectiveness of deep features as perceptual metric. Vision and Pattern Recognition, pp. 586595, 2018. Technical Report Weixia Zhang, Guangtao Zhai, Ying Wei, Xiaokang Yang, and Kede Ma. Blind image quality assessment via vision-language correspondence: multitask learning perspective. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1407114081, 2023. Heliang Zheng, Huan Yang, Jianlong Fu, Zheng-Jun Zha, and Jiebo Luo. Learning conditional knowledge distillation for degraded-reference image quality assessment. In IEEE/CVF International Conference on Computer Vision, pp. 1024210251, 2021. 13 Technical Report"
        },
        {
            "title": "APPENDIX",
            "content": "This appendix provides details on training configurations, metric rescaling methods, additional results, and qualitative visualizations. TRAINING CONFIGURATIONS FOR DISTS-STYLE PERCEPTUAL METRICS In perceptual metric training, the backbone is fixed to preserve generalizability (Ding et al., 2020; Chen et al., 2025). We use Adam (Kingma & Ba, 2014) with an initial learning rate of 1 104, halved every 1,000 iterations. Following Ding et al. (2020), the zeroth-stage weights are projected onto [0.02, 1] after each update for stability. All models are trained for 5,000 iterations on KADID10K (Lin et al., 2019) with batch size of 32. TRAINING CONFIGURATIONS IN GAN-BASED INITIALIZATION For FR-IQA model training, VGG-16 (Simonyan & Zisserman, 2014), ResNet-50 (He et al., 2016), and DINOv2 (Oquab et al., 2023) are trained using the same strategies described in Sec. A. For NR-IQA model training, we conduct experiments on KADID-10K (Lin et al., 2019) and KonIQ10k (Hosu et al., 2020), adopting 6:2:2 division into training, validation, and testing sets. Model performance is reported on the test set using SRCC and PLCC."
        },
        {
            "title": "C RESCALING VISUAL QUALITY METRICS",
            "content": "To facilitate fair and interpretable comparison of perceptual quality across models, we follow Ding et al. (2020); Chen et al. (2025); Wu et al. (2024b) and normalize the outputs of these NR-IQA models onto unified perceptual scale ranging from 1 to 100 by means of four-parameter monotonic logistic mapping: Nη(x) = η1 η2 (cid:16) 1 + exp (x)η3 η4 (cid:17) + η2, (7) where (x) denotes the raw score predicted by an NR-IQA model. The parameters η1 and η2 are fixed to 100 and 1, respectively, defining the upper and lower bounds of the normalized scale. The remaining parameters, η3 and η4, are estimated during fitting. Under this mapping, larger normalized values correspond to higher perceived image quality."
        },
        {
            "title": "D ADDITIONAL RESULTS",
            "content": "Results of DISTS-style Perceptual Metrics The results in Table 4 demonstrate clear differences in performance across backbones on the four synthetic FR-IQA datasets. ℓVGG-16 achieves the highest overall performance, with average SRCC and PLCC values of 0.853 and 0.867, respectively, consistently ranking first across datasets. ℓResNet-50 follows closely, reaching the best PLCC on TID2013 (0.896) and yielding strong average performance (0.810/0.843), ranking second overall. ℓConvNeXt and ℓSwin-T achieve intermediate results. By contrast, ℓCLIP-ViT and ℓVGG-16-R lag behind, leading to the lowest overall rank. Overall, convolutional backbones substantially outperform Transformerbased ones, suggesting that convolutional features remain better aligned with perceptual quality assessment under synthetic distortions. Quantitative Comparison of Backbone Initializations Consistent with the findings in Sec. 3.3, the results in Fig. 8 show that ImageNet pretraining (Russakovsky et al., 2015) yields the best NRIQA performance across all backbones, underscoring the importance of large-scale supervised initialization. GAN-based initialization provides clear improvements over random initialization, suggesting that adversarial training captures perceptually relevant cues, though its effectiveness remains limited compared to ImageNet. Among backbones, DINOv2 achieves the highest scores under pretraining but performs poorly with random or GAN initialization, indicating that Transformer architectures are more reliant on large-scale pretraining, whereas convolutional networks maintain moderate robustness under weaker initializations. 14 Technical Report Table 4: Quantitative Comparison of SRCC and PLCC across perceptual metrics on four synthetic FR IQA datasets. Backbone VGG-16-R VGG-16 ResNet-50 ConvNeXt CLIP-ViT Swin-T Liu13 TID2013 Ma17 SRCC PLCC SRCC PLCC SRCC PLCC SRCC PLCC SRCC PLCC 0.695 0.643 0.867 0.873 0.843 0.871 0.817 0.780 0.771 0.808 0.800 0.708 0.493 0.731 0.680 0.643 0.472 0.656 0.293 0.715 0.604 0.553 0.293 0. 0.818 0.895 0.866 0.879 0.790 0.848 0.815 0.908 0.886 0.893 0.835 0.860 0.799 0.929 0.900 0.884 0.912 0.856 0.822 0.935 0.912 0.900 0.919 0.886 0.651 0.895 0.896 0.833 0.858 0.799 0.638 0.853 0.810 0.774 0.701 0. Average TQD Rank 6 1 2 3 5 4 Figure 8: Quantitative comparison of NR-IQA results on KonIQ-10k with VGG-16, DINOv2, and ResNet-50 under random, GAN, and ImageNet initialization."
        },
        {
            "title": "E ADDITIONAL VISUALIZATIONS",
            "content": "We provide more visualizations of different optimization settings illustrated in Sec. 3.1, shown in Fig. 9, Fig. 10, and Fig. 11. Across images, pure reconstruction produces smooth yet over-smoothed results, with visible zippering or checkerboard artifacts for VGG-16-R and VGG-16. Perceptual loss improves edge continuity and global structure, but ViT backbones tend to hallucinate elongated streaks in fine textures. Adding an adversarial term recovers stochastic details in the feathers and rock granularity while suppressing grid artifacts. Convolutional discriminators paired with CNN backbones (ResNet-50 (He et al., 2016), ConvNeXt (Liu et al., 2022), VGG-16 (Simonyan & Zisserman, 2014)) yield the most coherent local textures and consistent contrast, whereas Transformer backbones (CLIP-ViT (Radford et al., 2021), Swin-T (Liu et al., 2021)) remain more prone to banding and unstable micro-patterns. The combined setting with reconstruction term plus perceptual loss and GAN provides the best balance between fidelity and realism. 15 Technical Report Figure 9: Qualitative comparison of SR results under different optimization objectives. 16 Technical Report Figure 10: Qualitative comparison of SR results under different optimization objectives. 17 Technical Report Figure 11: Qualitative comparison of SR results under different optimization objectives."
        }
    ],
    "affiliations": [
        "Beihang University",
        "City University of Hong Kong",
        "Institute of Microelectronics of the Chinese Academy of Sciences",
        "The Hong Kong Polytechnic University"
    ]
}