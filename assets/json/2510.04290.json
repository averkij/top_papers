{
    "paper_title": "ChronoEdit: Towards Temporal Reasoning for Image Editing and World Simulation",
    "authors": [
        "Jay Zhangjie Wu",
        "Xuanchi Ren",
        "Tianchang Shen",
        "Tianshi Cao",
        "Kai He",
        "Yifan Lu",
        "Ruiyuan Gao",
        "Enze Xie",
        "Shiyi Lan",
        "Jose M. Alvarez",
        "Jun Gao",
        "Sanja Fidler",
        "Zian Wang",
        "Huan Ling"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in large generative models have significantly advanced image editing and in-context image generation, yet a critical gap remains in ensuring physical consistency, where edited objects must remain coherent. This capability is especially vital for world simulation related tasks. In this paper, we present ChronoEdit, a framework that reframes image editing as a video generation problem. First, ChronoEdit treats the input and edited images as the first and last frames of a video, allowing it to leverage large pretrained video generative models that capture not only object appearance but also the implicit physics of motion and interaction through learned temporal consistency. Second, ChronoEdit introduces a temporal reasoning stage that explicitly performs editing at inference time. Under this setting, the target frame is jointly denoised with reasoning tokens to imagine a plausible editing trajectory that constrains the solution space to physically viable transformations. The reasoning tokens are then dropped after a few steps to avoid the high computational cost of rendering a full video. To validate ChronoEdit, we introduce PBench-Edit, a new benchmark of image-prompt pairs for contexts that require physical consistency, and demonstrate that ChronoEdit surpasses state-of-the-art baselines in both visual fidelity and physical plausibility. Code and models for both the 14B and 2B variants of ChronoEdit will be released on the project page: https://research.nvidia.com/labs/toronto-ai/chronoedit"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 0 9 2 4 0 . 0 1 5 2 : r CHRONOEDIT: TOWARDS TEMPORAL REASONING FOR IMAGE EDITING AND WORLD SIMULATION Jay Zhangjie Wu1,* Xuanchi Ren1,2,* Tianchang Shen1,2 Tianshi Cao1,2 Kai He1,2 Yifan Lu1 Ruiyuan Gao1 Enze Xie1 Shiyi Lan1 Jose M. Alvarez1 Jun Gao1 Sanja Fidler1,2 Zian Wang1,2 Huan Ling1,*, 1NVIDIA 2University of Toronto"
        },
        {
            "title": "ABSTRACT",
            "content": "Recent advances in large generative models have significantly advanced image editing and in-context image generation, yet critical gap remains in ensuring physical consistency, where edited objects must remain coherent. This capability is especially vital for world simulation related tasks. In this paper, we present ChronoEdit, framework that reframes image editing as video generation problem. First, ChronoEdit treats the input and edited images as the first and last frames of video, allowing it to leverage large pretrained video generative models that capture not only object appearance but also the implicit physics of motion and interaction through learned temporal consistency. Second, ChronoEdit introduces temporal reasoning stage that explicitly performs editing at inference time. Under this setting, target frame is jointly denoised with reasoning tokens to imagine plausible editing trajectory that constrains the solution space to physically viable transformations. The reasoning tokens are then dropped after few steps to avoid the high computational cost of rendering full video. To validate ChronoEdit, we introduce PBench-Edit, new benchmark of imageprompt pairs for contexts that require physical consistency, and demonstrate that ChronoEdit surpasses state-ofthe-art baselines in both visual fidelity and physical plausibility. Code and models for both the 14B and 2B variants of ChronoEdit will be released on the project page: https://research.nvidia.com/labs/toronto-ai/chronoedit"
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent large-scale generative models have transformed the landscape of image editing, enabling purely text-driven image modifications that impact diverse domains such as social media, e-commerce, education, and creative arts (Xiao et al., 2025; Labs et al., 2025; Liu et al., 2025; Yu et al., 2025). Beyond these consumer applications, image editing also offers critical capability for simulationrelated applications, providing controllable mechanism to simulate rare but safety-critical scenarios that are otherwise difficult to capture in real-world data. For example, editing can create long-tail events for autonomous driving, where unexpected objects enter the road (Gao et al., 2024; Lu et al., 2024), or visualize the outcomes of robot arm manipulating objects in cluttered scene. In these cases, editing goes beyond aesthetics and serves to generate diverse training and evaluation data for perception, planning, and reasoning. central requirement in the context of image editing for simulation is physical consistency: edited results must preserve existing objects and their properties (e.g., color, geometry) while reflecting the intended change. Without this constraint, edited results risk misrepresenting the scene and compromising downstream systems. Existing editing models have explored character or object continuity using video data to curate pixel-level editing pairs (Deng et al., 2025; Xiao et al., 2025; Chen et al., 2025), yet data alone have failed to ensure physical consistency. As illustrated in Fig. 2, these models often hallucinate new objects or alter geometry in unintended ways. Such failures stem partly from architectural limitations: current approaches are purely data-driven and lack mechanisms * Equal contribution; Corresponding author 1 (a) Reference Image (b) Change to confident pose (c) Pick up the dragonfruit (d) The robot is driving car (e) Reference Image (f) Replace the black sedan with red SUV (g) Add jaywalker, front light turns on (h) Change traffic light to red (i) Reference Image (j) Superhero stance, with its front paws raised (k) The cat speaks Chrono! (l) Transform to high-end PVC scale figure Figure 1: Physical consistent image editing results with ChronoEdit-14B. ChronoEdit produces edits that are both visually convincing and physically consistent with the underlying scene context. Figure 2: Failure cases of state-of-the-art image editing models. Current state-of-the-art models often struggle to maintain physical consistency on world simulation-related editing tasks. They may hallucinate unintended objects or distort scene geometry. In contrast, our method produces edits that are faithful to the instruction and remain coherent with the scene. Prompts (from top to bottom): (1) The left silver SUV makes U-turn, (2) Pick up the spoon with the robot arm, and (3) Close the wooden piece by hand. to enforce continuity, leaving them vulnerable to drifting edits that appear plausible but violate physical constraints. Large-scale video generative models (Wan, 2025; Cosmos, 2025) have recently demonstrated strong capabilities to preserve object structure and coherence across consecutive frames. This inherent temporal prior makes them particularly well-suited for editing tasks that demand physical consistency. Building upon this insight, we introduce ChronoEdit, foundation model for image editing explicitly designed to preserve physical consistency. ChronoEdit repurposes pretrained video generative models for editing by reframing the task as two-frame video generation problem, where the input image and its edited version are modeled as consecutive frames. When fine-tuned with curated image-editing data, this two-frame formulation equips the video model with editing functionalities while leveraging its pretrained temporal prior to preserve object fidelity. For world simulation tasks (e.g., action editing) that demand stronger temporal coherence, we further introduce explicitly guided editing through temporal reasoning. Given an input image and an editing instruction, the model imagines and denoises short video trajectory that realizes the edit while preserving temporal alignment with the input frame. The intermediate video frames in this trajectory act as reasoning tokens, planning how the edit should unfold and thereby producing more physically plausible results (See Fig. 2). Beyond improving plausibility, simulating these intermediate frames also unveils the thinking process of the editing model, offering more interpretable view of how edits are constructed. To balance these benefits with efficiency, ChronoEdit can perform temporal reasoning during only the first few high-noise denoising steps. After that stage, the intermediate frames are discarded, and only the final frame of the trajectory is refined into the edited image. Public benchmarks for image editing mainly target visual fidelity and instruction following, but rarely evaluate physical consistency. To address this gap, we introduce new benchmark named PBench-Edit. This benchmark is constructed by carefully curating collection of images paired with editing prompts that capture both real-world editing requirements and broad spectrum of editing types. PBench-Edit is designed to evaluate not only general-purpose edits but also tasks that require physical and temporal consistency. Our experiments on PBench-Edit demonstrate that ChronoEdit achieves state-of-the-art results, surpassing existing open-source baselines by significant margin and narrowing the gap with leading proprietary systems. In summary, we make the following contributions: (i) We propose ChronoEdit, foundation model for image editing designed to preserve physical consistency. (ii) We present an effective design that turns pretrained video generative model into an image editing model. (iii) We develop novel temporal (iv) We demonstrate that reasoning inference stage that further enforces physical consistency. ChronoEdit achieves state-of-the-art performance among open-source models and is competitive with leading proprietary systems. (v) We present benchmark tailored to world simulation applications. 3 Figure 3: Overview of the ChronoEdit pipeline. From right to left, the denoising process begins in the temporal reasoning stage, where the model imagines and denoises short trajectory of intermediate frames. These intermediate frames act as reasoning tokens, guiding how the edit should unfold in physically consistent manner. For efficiency, the reasoning tokens are discarded in the subsequent editing frame generation stage, where the target frame is further refined into the final edited image."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Recent advances in image editing have been driven by large-scale foundation models. FLUX.1 Kontext (Labs et al., 2025) achieves strong instruction alignment and multi-turn editing through billion-scale parameterization, while OmniGen (Xiao et al., 2025) unifies text-to-image, editing, and subject-driven generation within single diffusion framework. Qwen-Image-Edit (Wu et al., 2025a) extends vision-language model with double-stream architecture for precise, high-fidelity edits. Proprietary systems such as GPT-4o (OpenAI, 2025) and Gemini 2.5 Flash Image (Google, 2025) demonstrate robust multi-turn editing and conversational refinement at scale. Prior work also explored leveraging video priors for image editing: Bagel (Deng et al., 2025), UniReal (Chen et al., 2025), and OmniGen (Xiao et al., 2025) use video-derived key frames to create temporally coherent image pairs. In complementary direction, Rotstein et al. (2025) is training-free method that uses pretrained image-to-video diffusion model to synthesize sequence of intermediate frames, and then selects the frame that best satisfies the edit. complete discussion of related work can be found in Appendix A."
        },
        {
            "title": "3 CHRONOEDIT",
            "content": "In this section, we first provide background on the rectified flow model for video generation in Sec. 3.1. Next, we outline our core design in Sec. 3.2, which adapts pretrained image-to-video model for image editing, and detail our training with video reasoning tokens. We then describe the inference procedure in Sec. 3.3, highlighting how video reasoning enhances consistency. Finally, Sec. 3.4 describes the post-training process of ChronoEdit, which improves inference speed. An overview of the full architecture is shown in Fig. 3. 3.1 BACKGROUND: RECTIFIED FLOW FOR VIDEO GENERATIVE MODELS Modern video generative models typically rely on pretrained variational autoencoder (VAE) (Blattmann et al., 2023b;a; Kong et al., 2024; Gupta et al., 2024; Cosmos, 2025; Wan, 2025) that compresses raw videos RF 3HW into compact latent representation z0 = E(x) RF Chw. Training and inference operate in this latent space, and the decoder reconstructs videos as ˆx = D(z). To handle temporal structure, causal video VAEs encode the first frame independently and compress subsequent chunks conditioned on past latents. In our work, we adopt the Wan2.1 VAE (Wan, 2025), which yields = (F 1) 4 + 1, = 16, = H/8, and = W/8. Rectified flow (Liu et al., 2022; Albergo & Vanden-Eijnden, 2022; Lipman et al., 2022; Esser et al., 2024a) provides principled framework for training video generators via flow matching. Given video data pdata and Gaussian noise ϵ (0, I), the interpolated latent at timestep [0, 1] is defined as zt = (1 t)z0 + tϵ, with z0 = E(x). denoiser Fθ(zt, t; y, c) with parameters θ is trained to predict the target velocity field (ϵ z0) by minimizing: Lθ = Etp(t), xpdata, ϵN (0,I) (cid:104) Fθ(zt, t; y, c) (ϵ z0)2 2 (cid:105) , (1) Here denotes optional text conditioning and is optional image conditioning."
        },
        {
            "title": "3.2 RE-PURPOSING VIDEO GENERATIVE MODELS FOR EDITING",
            "content": "Formally, the image editing task aims to transform reference image into an output image that satisfies natural-language instruction y. Our key insight is to repurpose pretrained image-to-video model for this task, leveraging its inherent temporal priors to maintain consistency between the source and target images. Encoding Editing Pairs. To leverage temporal priors in pretrained video models, we reinterpret the editing pair {c, p} as short video sequence. Specifically, the input image is encoded as the first latent frame zc = E(c), while the output image is repeated four times to match the video VAEs 4 temporal compression and encoded as zp = E(repeat(p, 4)). This produces two temporal latents aligned with the video models architecture. We additionally adjust the models 3D-factorized Rotary Position Embedding (RoPE) (Su et al., 2024) by anchoring input image at timestep 0 and output image at predefined timestep , explicitly encoding their temporal separation. For convenience, we fix to the length of the joint-training video latents (see following section). Temporal Reasoning Tokens. To go beyond direct inputoutput mapping, we explicitly model the transition between input image and output image p. The goal is to encourage the model to imagine plausible trajectory rather than regenerate the target image in single step, which often leads to abrupt changes. By reasoning through intermediate states, the model better preserves object identity, geometry, and physical coherence. In practice, we insert intermediate latent frames between zc and zp. These frames are initialized with random noise and denoised jointly with the output frame latents. We refer to them as temporal reasoning tokens r, since they act as intermediate guidance that help the model think through plausible transitions. Unifying Image Pairs and Videos. Similarly to the video denoiser introduced in Sec. 3.1, we define the image-editing denoiser as Fθ(zp,t, t; y, zc), where zp,t and are the flow variables. Our formulation naturally supports training on both image-editing pairs and full video sequences within unified framework. For public image-editing datasets, each pair (c, p, y) is reinterpreted as two-frame video, where is the first frame and the last, directly supervising instruction-based edits. For videos, the structure matches our reasoning-token design: the first frame corresponds to c, the last corresponds to p, and all intermediate frames serve as reasoning tokens. Input and reasoning frames are encoded into latents by the video VAE as standard video frames, while the target frame is separately encoded and repeated four times to match the VAEs temporal compression. This design makes reasoning tokens optional at inferencethe VAE decoder can still recover the target frame independentlywhile providing strong supervision for coherent transitions when present. Together, this joint training strategy allows the model to learn semantic alignment from image pairs while additionally learn temporal consistency grounded in video data. Video Data Curation. Training with reasoning tokens requires diverse examples of how scenes evolve over time. To this end, we curate large-scale synthetic dataset of 1.4M videos generated with state-of-the-art video generative models. We place particular emphasis on disentangling scene dynamics from camera motion, since unintended viewpoint shifts between the first and last frames could be misinterpreted as edits during training. Our corpus covers three complementary categories: (i) Static-camera, dynamic-object clips produced by text-to-video models (Wan, 2025; Cosmos, 2025), where we append the postfix The camera remains stationary throughout the video. to prompts and filter unstable clips using ViPE (Huang et al., 2025); (ii) Egocentric driving scenes, critical world-simulation scenario, generated with the HDMap-conditioned model of Ren et al. (2025a), which fixes the camera while explicitly controlling vehicle motion via bounding boxes; and (iii) Dynamic-camera, static-scene clips from GEN3C (Ren et al., 2025b), which allow precise camera trajectory control while keeping the scene content fixed. Finally, to provide corresponding instructions y, we employ VLM to caption each video with an editing instruction, summarizing the transition from input to output frame, as detailed in Appendix D. 5 3."
        },
        {
            "title": "INFERENCE WITH TEMPORAL REASONING",
            "content": "To perform image editing efficiently at inference time, we introduce two-stage method which allows the model to benefit from video reasoning tokens without incurring the full computational cost of generating complete video. Intuitively, the first few noisiest steps of flow/diffusion trajectory determine the global structure of the outcome, and hence tokens more frequently attend across frames in the sequence. Hence, we incorporate video reasoning tokens in these first denoising steps, and omit them in later denoising steps to obtain the best balance between quality and computational cost. Pseudocode is provided in Algo. 1 and visualization is shown in Fig. 3. Algorithm 1 Sampling process of ChronoEdit. When Temporal Reasoning is enabled, Nr steps are taken with video reasoning tokens zf ull. Setting = 0 or Nr = 0 recovers standard sampling w/o Temporal Reasoning. Given: Denoising model Fθ, ODESolve(vt, t, zt, t) zt , sequence of time soning length r, = (tmax, . . . , tmin), reasoning timestep Nr. ODE solver trajectory reasteps Input: Editing instruction tokens y, input image token ϵ (0, I) with shape (r + 1) zf ull concat(c, ϵ) 0, [0] while < do if < Nr then Fθ(zf ull, t; y, c) zf ull ODESolve(v, t, zf ull, [n + 1]) else if Nr then if = Nr then zf inal concat(c, zf ull[1]) In the first stage, we concatenate clean input tokens zc, sampled reasoning tokens and noisy sampled output tokens zp into one temporal sequence. Similar to image-to-video generation, the model performs denoising on the concatenated sequence without modifying the zc tokens. Rather than denoising all the way to clean latents, Nr steps of denoising are performed, and the partially denoised last latents of the temporal sequence corresponding to zp are carried forward. In the second stage, the partially denoised output latent is concatenated behind the clean input latent and fully denoised for the remaining Nr steps. As in training, the output latent corresponds to four repeated frames to match the video VAEs temporal compression. After decoding to RGB, the four frames typically collapse to the same image, and we take the last frame as the final edited result. Fθ(zf inal, t; y, c) zf inal ODESolve(v, t, zf inal, [n + 1]) + 1, [n] Decode(zf inal)[1] Output: 3.4 FEW-STEP DISTILLATION FOR FAST INFERENCE To further accelerate inference, we employed distillation techniques to reduce the number of steps required for inference. Specifically, we utilized DMD loss (Yin et al., 2024) to train an 8-step student model. The gradient of the distillation objective is given by LDMD = Et (cid:18)(cid:90) (sreal(f (Fθ, t), t) sfake(f (Fθ, t), t) (cid:19) dz , dFθ dθ (2) where sreal and sfake denote the score estimation from the teacher model and trainable fake score model, respectively; () is the forward diffusion process (i.e., noise injection). We omit the conditioning term for simplicity. Through this training process, our model can significantly improve the inference speed while maintaining prompt-following ability and image editing quality."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "We evaluate ChronoEdit in two configurations, with 14B and 2B parameters, denoted as ChronoEdit14B and ChronoEdit-2B. We evaluate both models across multiple datasets and editing tasks, compare them with both open-source and proprietary baselines, and ablate the contribution of different design choices. We further evaluate the variant of ChronoEdit-14B with temporal reasoning (ChronoEdit14B-Think), and the step distillation (ChronoEdit-14B-Turbo). Training Details. ChronoEdit-14B is finetuned from the pretrain model of Wan2.1-I2V-14B720P1 (Wan, 2025) and ChronoEdit-2B is built upon Cosmos-Predict2.5-2B2 (Cosmos, 2025). Both 1https://huggingface.co/Wan-AI/Wan2.1-I2V-14B-720P 2https://research.nvidia.com/labs/dir/cosmos-predict2.5 6 Model Model Size Add Adjust Extract Replace Remove Background Style Hybrid Action Overall MagicBrush (Zhang et al., 2023a) Instruct-Pix2Pix (Brooks et al., 2023) AnyEdit (Yu et al., 2025) UltraEdit (Zhao et al., 2024) OmniGen (Xiao et al., 2025) ICEdit (Zhang et al., 2025) Step1X-Edit (Liu et al., 2025) BAGEL (Deng et al., 2025) UniWorld-V1 (Lin et al., 2025) OmniGen2 (Wu et al., 2025b) FLUX.1 Kontext [Dev] (Labs et al., 2025) FLUX.1 Kontext [Pro] (Labs et al., 2025) GPT Image 1 [High] (OpenAI, 2025) Qwen-Image (Wu et al., 2025a) ChronoEdit-2B ChronoEdit-14B-Turbo (8 steps) ChronoEdit-14B 0.9B 0.9B 0.9B 8B 3.8B 12B 19B 7B-MoT 12B 7B 12B N/A N/A 20B 2B 14B 14B 2.84 2.45 3.18 3.44 3.47 3.58 3.88 3.56 3.82 3.57 3.76 4.25 4.61 4.38 4.30 4.36 4.48 1.58 1.83 2.95 2.81 3.04 3.39 3.14 3.31 3.64 3.06 3.45 4.15 4.33 4.16 4.29 4.38 4. 1.51 1.44 1.88 2.13 1.71 1.73 1.76 1.70 2.27 1.77 2.15 2.35 2.90 3.43 2.87 3.28 3.49 1.97 2.01 2.47 2.96 2.94 3.15 3.40 3.3 3.47 3.74 3.98 4.56 4.35 4.66 4.23 4.11 4.66 1.58 1.50 2.23 1.45 2.43 2.93 2.41 2.62 3.24 3.20 2.94 3.57 3.66 4.14 4.50 4.00 4. 1.75 1.44 2.24 2.83 3.21 3.08 3.16 3.24 2.99 3.57 3.78 4.26 4.57 4.38 4.40 4.31 4.67 2.38 3.55 2.85 3.76 4.19 3.84 4.63 4.49 4.21 4.81 4.38 4.57 4.93 4.81 4.60 4.31 4.83 1.62 1.20 1.56 1.91 2.24 2.04 2.64 2.38 2.96 2.52 2.96 3.68 3.96 3.82 3.20 3.67 3. 1.22 1.46 2.65 2.98 3.38 3.68 2.52 4.17 2.74 4.68 4.26 4.63 4.89 4.69 4.81 4.78 4.91 1.90 1.88 2.45 2.70 2.96 3.05 3.06 3.20 3.26 3.44 3.52 4.00 4.20 4.27 4.13 4.13 4.42 Table 1: Quantitative comparison results on ImgEdit (Ye et al., 2025). All metrics are evaluated by GPT-4.1. Overall is calculated by averaging all scores across tasks. models are trained using learning rate of 2e 5 and weight decay of 1e 3. Since the pretrained model already exhibits strong capability in generating fine-grained details, we sample timesteps [0, 1] from logit-normal distribution with shift value set to 5 (Esser et al., 2024b), thereby oversampling the large-timestep region. The model is pretrained on 1.4 million videos and 2.6 million image pairs, with the first and last frames of each video also included as additional image pairs. During training, we adopt 1:1 ratio between image pairs and videos, where the video data is used to learn video reasoning tokens. We empirically use 6 intermediate latent frames as temporal reasoning tokens, corresponding to 24 frames in pixel space, which results in total of = 8 timesteps. Training is performed with batch size of 128. In the final stage, the pretrained model is fine-tuned on high-quality supervised fine-tuning (SFT) dataset of 50k images and 20k videos, sampled at 5:1 ratio for 10k steps. For ChronoEdit-14B-Turbo, we apply distillation loss with learning rate of 2e 6 for 1500 steps, setting the update ratio between the student and the fake score model (Yin et al., 2024) to 5 for stable training. Benchmarks. We evaluate our method on two complementary benchmarks. First, for generalpurpose image editing, we use the ImgEdit-Basic-Edit Suite (Ye et al., 2025) which consists of 734 test cases spanning nine common image-editing tasks: add, remove, alter, replace, style transfer, background change, motion change, hybrid edit, and action. The benchmark is constructed from manually collected Internet images to ensure semantic diversity, with the action category primarily emphasizing human pose modifications. Model performance on each task is evaluated using GPT-4.1, which measures adherence to instructions, quality of the edit, and detail preservation (Ye et al., 2025). While prior benchmarks for image editing assess visual realism and instruction alignment, they provide limited evaluation of physical consistency. We therefore develop PBench-Edit, an imageediting benchmark derived from the original PBench dataset (Pbench, 2025), designed to assess editing in physically grounded contexts. The original PBench evaluates world-model progress in domains such as autonomous driving, robotics, physics, and common-sense reasoning. PBench-Edit repurposes its curated videos and captions for targeted editing tasks by selecting representative frames from each domain and pairing them with manually verified editing instructions. Unlike ImgEdit-Action, PBench-Edit covers broader spectrum of real-world interactionssuch as cooking, driving, and robot manipulationresulting in benchmark that is both diverse and physically grounded. It includes 271 images in total (133 human, 98 robot, and 40 driving). Evaluation is performed with GPT-4.1 using the same criteria as ImgEdit (Ye et al., 2025): adherence to instructions, edit quality, and detail preservation. Additional visualizations are provided in Fig. S4. 4.1 QUANTITATIVE EVALUATION General-Purpose Image Editing Results. Tab. 1 reports results on the ImgEdit Basic-Edit Suite (Ye et al., 2025). To ensure fair comparison with prior works in terms of compute cost, we disable Temporal Reasoning and evaluate ChronoEdit-14B as pure image-editing model. ChronoEdit-14B achieves the highest overall score of 4.42, outperforming state-of-the-art baselines. Among opensource models, FLUX.1 Kontext [Dev] is the most comparable in scale (12B vs. 14B). ChronoEdit14B surpasses it by +0.90 overall, with especially large improvements on extract (4.66 vs. 2.15, +2.51), remove (4.57 vs. 2.94, +1.63), while performing on par in style transfer (4.83 vs. 4.38). These results indicate the strong capability of ChronoEdit for instruction-driven edits that require spatial Model Action Fidelity Identity Preservation Visual Coherence Overall Step1X-Edit (Liu et al., 2025) BAGEL (Deng et al., 2025) OmniGen2 (Wu et al., 2025b) FLUX.1 Kontext [Dev] (Labs et al., 2025) Qwen-Image (Wu et al., 2025a) ChronoEdit-14B ChronoEdit-14B-Think (Nr = 10) ChronoEdit-14B-Think (Nr = 20) ChronoEdit-14B-Think (Nr = 50) ChronoEdit-2B-Think (Nr = 10) 3.39 3.83 2.65 2.88 3.76 4.01 4.31 4.28 4.29 4.17 4.52 4.60 4.02 4.29 4.54 4.65 4. 4.62 4.64 4.61 4.44 4.53 4.02 4.32 4.48 4.63 4.64 4.62 4.63 4.56 4.11 4.32 3.56 3.83 4.26 4.43 4. 4.51 4.52 4.44 Table 2: Quantitative comparison results on PBench-Edit. All metrics are evaluated by GPT-4.1. Overall is calculated by averaging all scores across dimensions. Add Lifeguard wearing red uniform to the white lifeguard tower Change the vehicle in the picture to be set in beach environment The toy cars to be lifted off the table and held in each hand FLUX.1 [Dev] Reference Image OmniGen2 Figure 4: Comparison with baseline methods. The first two rows show examples from the ImageEdit Basic-Edit Suite (Ye et al., 2025) benchmark, and the last row is from PBench-Edit, where ChronoEdit-Think is evaluated with 10 temporal reasoning steps. In both benchmarks, ChronoEdit achieves edits that more faithfully follow the given instructions while preserving scene structure and fine details. Qwen-Image ChronoEdit and structural reasoning. Compared to the 20B open-source model Qwen-Image (Wu et al., 2025a) which scores 4.27 overall, ChronoEdit-14B matches or outperforms its performance across all tasks. Notably, ChronoEdit-14B achieves stronger results on challenging categories such as background change (4.67 vs. 4.38) and action/motion edits (4.41 vs. 4.27), suggesting that joint imagevideo pretraining provides strong advantages for modeling dynamic consistency and scene transformations. It is also worth noting that ChronoEdit-14B-Turbo, which runs 6 faster than ChronoEdit-14B (5.0s vs. 30.4s per image, with speeds measured on 2 Nvidia-H100 GPUs), achieves results only 0.3 points below ChronoEdit-14B, yet still outperforms FLUX.1 Kontext [Dev] and FLUX.1 Kontext [Pro] by margins of 0.61 and 0.13, respectively. Moreover, we also report ChronoEdit-2B results, which are 7 smaller than ChronoEdit-14B but works on-par with ChronoEdit-14B-Turbo. World Simulation Editing Results. We evaluate our method on the PBench-Edit benchmark, which emphasizes physically grounded editing scenarios. As shown in Tab. 2, ChronoEdit-14B achieves the highest overall score (4.43), outperforming strong baselines such as BAGEL (4.32), Qwen-Image (4.26), and FLUX.1 Kontext [Dev] (3.83). Notably, ChronoEdit-14B delivers clear improvements in Action Fidelity (4.01 vs. 3.76 for Qwen-Image and 2.88 for FLUX.1 Kontext [Dev]), while also maintaining competitive results in identity preservation (4.65) and visual & anatomical coherence 8 \"Make the silver SUV appear as if it has made U-turn.\" \"Open the doors of the SUV and add person who is trying to get out.\" \"Add boy running after their dog, near the sound barrier.\" \"Reposition the pedestrian to the center of the crosswalk.\" \"A robotic arm hands over cup to person.\" \" robotic arm moves the potato to the green clipboard. \" Figure 5: Qualitative results on Physical-AI world simulation related tasks. All results are generated by ChronoEdit-14B-Think. Each group shows reference image (left) and the corresponding output (right). ChronoEdit produces edits that accurately follow the given instructions while preserving scene structure and fine details in Physical AIrelated scenes. Figure 6: Temporal reasoning trajectory visualization. By retaining intermediate reasoning tokens throughout the entire denoising process, ChronoEdit-14B-Think is able to visualize its internal thinking process when performing edits. Sequences are shown from left to right: the reference image (blue box), decoded intermediate reasoning frames (orange boxes), and the final target frame (green box). Top example prompt: Add cat on the bench. Bottom example prompt: Place cake on plate by hand. (4.63). Among the three evaluation dimensions, action fidelity is particularly important as it directly reflects models ability to maintain physical consistency when performing edits involving real-world interactions. Even without Temporal Reasoning, ChronoEdit-14B benefits from its pretrained video prior, enabling it to achieve stronger results than all baseline image-editing models. With Temporal Reasoning, ChronoEdit-14B-Think (Nr = 10) achieves new state-of-the-art overall score of 4.53, with particularly strong gain in Action Fidelity (4.31). This highlights the value of explicit Temporal Reasoning for edits that demand deeper understanding of physical consistency. Notably, ChronoEdit-2B-think (Nr = 10) matches the performance of ChronoEdit-14B, falling only slightly short of ChronoEdit-14B-Think. 4.2 QUALITATIVE EVALUATION Comparison with Baselines. We compare our approach against state-of-the-art image editing methods across variety of challenging scenarios. As illustrated in Fig. 4, ChronoEdit consistently produces high-quality results, demonstrating competitive overall performance and, in particular, clear advantage in action-oriented edits where precise modeling of dynamic poses and interactions is required. These results highlight the effectiveness of our video reasoning in handling complex, temporally grounded edits that are often difficult for conventional editing approaches. 9 Figure 7: Qualitative result of ChronoEdit-Turbo. The lightweight ChronoEdit-Turbo (runtime 5.0s) achieves editing quality similar to ChronoEdit (runtime 35.3s) while offering improved efficiency. (Left: Extract the red telephone booth in the image. Right: Replace the bicycle in the image with wooden park bench.) (a) Reference Image (b) Nr = 0 (Runtime: 30.4s) (c) Nr = 10 (Runtime: 35.3s) (d) Nr = 20 (Runtime: 40.2s) (e) Nr = 50 (Runtime: 55.5s) Figure 8: Qualitative ablation on video reason step Nr. Empirically, we found that setting the reasoning timestep to Nr = 10 within total of = 50 sampling steps achieves performance that is comparable to using reasoning across the full trajectory. Example Prompt: Halve the poached egg to reveal the yolk. Reported runtime is measured on Nvidia-H100 GPUs. ChronoEdit on Physical AI Tasks Figure 5 showcases ChronoEdit capability to address broad spectrum of Physical-AI world simulation tasks. These results demonstrate the models strong generalization across diverse domains of world simulation tasks, ranging from self-driving dynamics to robotic object manipulation. Temporal Reasoning Trajectory Visualization. If the video reasoning tokens are fully denoised into clean video, the model can illustrate how it thinks by visualizing intermediate frames as reasoning trajectorythough at the expense of slower inference. We present such visualization in Fig. 6. As illustrated in the top row, when prompted to add cat on the bench, the model first synthesizes the bench and then anticipates the cat emerging from the corner and leaping onto it, composing the scene through sequence of plausible intermediate states. Notably, an emergent capability of our approach is its ability to generate reasoning trajectory videos to realize edits. Even without exposure to training data where, for instance, bench suddenly appears, the video model can still imagine and execute plausible trajectory to accomplish the edit. In another example, the model correctly infers the stepwise process of placing cake on plate by hand. This deliberative trajectory reveals how the model perceives and interacts with the world in coherent, physically grounded manner (see video visualization in Project Page). ChronoEdit-Turbo. We further visualize the qualitative comparison of ChronoEdit and ChronoEdit14B-Turbo in Fig. 7. Both ChronoEdit and ChronoEdit-Turbo successfully execute the edits with comparable visual fidelity, preserving scene structure and fine details. This demonstrates that the lightweight ChronoEdit-Turbo variant achieves editing quality comparable to that of ChronoEdit, while offering improved efficiency (5.0s vs. 30.4s runtime). 4.3 ABLATION STUDY Reasoning Timestep. As discussed in Sec. 3.2, our model performs reasoning by traversing sequence of intermediate states, thereby constructing plausible temporal trajectory instead of directly regenerating the target image in single step. Empirically, we found that setting the reasoning timestep to Nr = 10 within total of = 50 sampling steps achieves performance that is comparable to using reasoning across the full trajectory (Tab. 2), while reducing the overall computational overhead from 55.5s (Nr = 50) to 35.3s (Nr = 10), which is small 4.9s increase compared to not using temporal reasoning (30.4s). An illustrative example is provided in Fig. 8, highlighting that shorter reasoning horizons are often sufficient to maintain fidelity while offering substantial efficiency gains. Ablation studies on the benefits of video pretrained weights and encoding editing pairs design can be found in Appendix C."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We introduced ChronoEdit, foundation model for image editing designed to enforce physical consistency. By repurposing pretrained video diffusion model and introducing temporal reasoning inference stage, our approach preserves coherence between input and edited outputs while producing plausible transformations. Extensive experiments demonstrate that ChronoEdit achieves state-of-theart performance among open-source models."
        },
        {
            "title": "6 ACKNOWLEDGEMENT",
            "content": "The authors would like to thank Product Managers Aditya Mahajan and Matt Cragun for their valuable guidance and support. We further acknowledge the Cosmos Team at NVIDIA, especially Qinsheng Zhang and Hanzi Mao, for their consulting of Cosmos-Pred2.5-2B. We also thank Yuyang Zhao, Junsong Chen, and Jincheng Yu for their insightful discussions. Finally, we are grateful to Ben Cashman, Yuting Yang, and Amanda Moran for their infrastructure support, especially in the period leading up to the deadline of this work."
        },
        {
            "title": "REFERENCES",
            "content": "Michael Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. arXiv preprint arXiv:2209.15571, 2022. 4 Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1820818218, 2022. 15 Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Ming-Hsuan Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. CoRR, abs/2502.13923, 2025. 16 Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023a. 4 Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023b. Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1839218402, 2023. 7, 15 Xi Chen, Zhifei Zhang, He Zhang, Yuqian Zhou, Soo Ye Kim, Qing Liu, Yijun Li, Jianming Zhang, Nanxuan Zhao, Yilin Wang, et al. Unireal: Universal image generation and editing via learning real-world dynamics. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1250112511, 2025. 1, 4, 15 Team Cosmos. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. 3, 4, 5, 6 Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Guang Shi, and Haoqi Fan. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. 1, 4, 7, 8, Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024a. 4 11 Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis, 2024b. URL https://arxiv.org/abs/2403. 03206. 7 Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. 15 Ruiyuan Gao, Kai Chen, Enze Xie, Lanqing Hong, Zhenguo Li, Dit-Yan Yeung, and Qiang Xu. MagicDrive: Street view generation with diverse 3d geometry control. In International Conference on Learning Representations, 2024. 1 Google. Gemini 2.5 flash image, 2025. URL https://developers.googleblog.com/en/ introducing-gemini-2-5-flash-image/. 4, 15 Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Fei-Fei Li, Irfan Essa, Lu Jiang, and José Lezama. Photorealistic video generation with diffusion models. In European Conference on Computer Vision, pp. 393411. Springer, 2024. 4 Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Promptto-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. 15 Jiahui Huang, Qunjie Zhou, Hesam Rabeti, Aleksandr Korovko, Huan Ling, Xuanchi Ren, Tianchang Shen, Jun Gao, Dmitry Slepichev, Chen-Hsuan Lin, Jiawei Ren, Kevin Xie, Joydeep Biswas, Laura Leal-Taixe, and Sanja Fidler. Vipe: Video pose engine for 3d geometric perception. In NVIDIA Research Whitepapers arXiv:2508.10934, 2025. 5 Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei Efros. Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 11251134, 2017. 15 Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 44014410, 2019. Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 81108119, 2020. 15 Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 4 Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv preprint arXiv:2506.15742, 2025. 1, 4, 7, 8, 15 Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, et al. Uniworld-v1: High-resolution semantic encoders for unified visual understanding and generation. arXiv preprint arXiv:2506.03147, 2025. 7 Huan Ling, Karsten Kreis, Daiqing Li, Seung Wook Kim, Antonio Torralba, and Sanja Fidler. Editgan: High-precision semantic image editing. In Advances in Neural Information Processing Systems (NeurIPS), 2021. 15 Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 4 12 Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. 1, 7, 8 Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 4 Yifan Lu, Xuanchi Ren, Jiawei Yang, Tianchang Shen, Zhangjie Wu, Jun Gao, Yue Wang, Siheng Chen, Mike Chen, Sanja Fidler, et al. Infinicube: Unbounded and controllable dynamic 3d driving scene generation with world-guided video models. arXiv preprint arXiv:2412.03934, 2024. Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. 15 OpenAI. URL introducing-4o-image-generation/. 4, 7, 15 Gpt-image-1, 2025. https://openai.com/index/ Pbench. Pbench lab. https://research.nvidia.com/labs/dir/pbench/, 2025. Accessed: 2025-09-25. 7 Xuanchi Ren, Yifan Lu, Tianshi Cao, Ruiyuan Gao, Shengyu Huang, Amirmojtaba Sabour, Tianchang Shen, Tobias Pfaff, Jay Zhangjie Wu, Runjian Chen, et al. Cosmos-drive-dreams: Scalable synthetic driving data generation with world foundation models. arXiv preprint arXiv:2506.09042, 2025a. 5 Xuanchi Ren, Tianchang Shen, Jiahui Huang, Huan Ling, Yifan Lu, Merlin Nimier-David, Thomas Müller, Alexander Keller, Sanja Fidler, and Jun Gao. Gen3c: 3d-informed world-consistent video generation with precise camera control. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025b. 5 Noam Rotstein, Gal Yona, Daniel Silver, Roy Velich, David Bensaid, and Ron Kimmel. Pathways on the image manifold: Image editing via video generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 78577866, 2025. 4, 15 Yujun Shen, Jinjin Gu, Xiaoou Tang, and Bolei Zhou. Interpreting the latent space of gans for semantic face editing. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 92439252, 2020. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 2024. 5 Team Wan. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 3, 4, 5, 6 Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025a. 4, 7, 8, 15 Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025b. 7, Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1329413304, 2025. 1, 4, 7, 15 Yang Ye, Xianyi He, Zongjian Li, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Bohan Hou, and Li Yuan. Imgedit: unified image editing dataset and benchmark. arXiv preprint arXiv:2505.20275, 2025. 7, 8, 16 Tianwei Yin, Michaël Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and William Freeman. Improved distribution matching distillation for fast image synthesis. In NeurIPS, 2024. 6, 7 13 Qifan Yu, Wei Chow, Zhongqi Yue, Kaihang Pan, Yang Wu, Xiaoyang Wan, Juncheng Li, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. Anyedit: Mastering unified high-quality image editing for any idea. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2612526135, 2025. 1, 7 Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instruction-guided image editing. Advances in Neural Information Processing Systems, 36:3142831449, 2023a. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 38363847, 2023b. 15 Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, and Yi Yang. In-context edit: Enabling instructional image editing with in-context generation in large scale diffusion transformer. arXiv preprint arXiv:2504.20690, 2025. 7 Haozhe Zhao, Xiaojian Shawn Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. Ultraedit: Instruction-based fine-grained image editing at scale. Advances in Neural Information Processing Systems, 37:30583093, 2024. 7 Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference on computer vision, pp. 22232232, 2017."
        },
        {
            "title": "A RELATED WORK",
            "content": "Image Editing is long-standing challenge that has evolved through multiple paradigms. Early GANbased approaches edit images by training conditional GANs for specific image translation tasks (Isola et al., 2017; Zhu et al., 2017), or by manipulating latent directions in pretrained GANs (Karras et al., 2019; 2020; Shen et al., 2020; Ling et al., 2021). While GANs can produce photorealistic outputs in constrained domains (e.g., faces, cars), they struggle with out-of-domain edits and require domain-specific training. With diffusion models becoming the dominant approach for high-fidelity image generation and editing, recent works achieve diverse, photorealistic outputs under various conditioning schemes. Training-free methods such as SDEdit (Meng et al., 2021), Blended Diffusion (Avrahami et al., 2022), Prompt-to-Prompt (Hertz et al., 2022), and textual inversion (Gal et al., 2022) enable edits with text-to-image models by injecting noise, guiding cross-attention, or inverting real images into the diffusion latent embeddings. Structure-aware models like ControlNet (Zhang et al., 2023b) further allow sketch-, edge-, or pose-guided edits. However, these approaches often face trade-off between edit strength and content preservation, and may lack fine-grained controllability for complex edits. Instruction-Tuned Image Editing methods explicitly learn from datasets of paired images and corresponding edit instructions. InstructPix2Pix (Brooks et al., 2023) generated large synthetic dataset of instructionimage pairs and fine-tuned Stable Diffusion to map an input image and textual instruction directly to the edited output. Larger-scale editing model such as UniReal (Chen et al., 2025) and FLUX.1 Kontext (Labs et al., 2025) scale to billions of parameters and improve instruction alignment, multi-turn editing, and fidelity across diverse domains. Recently, multi-modal foundation models unify vision and language to enable open-domain instruction-following edits. OmniGen integrates text-to-image, editing, and subject-driven generation into single diffusion framework by jointly modeling text and image inputs (Xiao et al., 2025). Qwen-Image-Edit extends the Qwen-VL vision-language model with double-stream architecture for precise, high-fidelity edits (Wu et al., 2025a). Proprietary systems such as GPT-4o (OpenAI, 2025) and Gemini 2.5 Flash Image (Google, 2025) demonstrate robust multi-turn editing and conversational refinement at scale. Despite remarkable progress, current methods still fall short in ensuring physical consistency, which is crucial for downstream applications in simulation and reasoning. Video Prior for Editing Task. Recent works also start to explore video priors for image editing tasks. Deng et al. (2025); Xiao et al. (2025); Chen et al. (2025) sample key frames in video data to create temporally coherent image pairs. In complementary direction, Rotstein et al. (2025) is training-free method that uses pretrained image-to-video diffusion model to synthesize sequence of intermediate frames, and then selects the frame that best satisfies the edit."
        },
        {
            "title": "B ADDITIONAL RESULTS",
            "content": "More Qualitative Results Comparing with Baselines We provide additional qualitative comparisons against baseline methods in Fig. S1, which further highlight the effectiveness of our approach in producing coherent and physically consistent edits."
        },
        {
            "title": "C ADDITIONAL ABLATION STUDY",
            "content": "Video Pretrained Weights. We validate our design choice of leveraging pretrained image-to-video model for the image editing task. As shown in Fig. S2, compared to training from scratch, pretrained initialization enables faster and more stable convergence. Qualitative Results for Reasoning steps and Timesteps. We found that setting the reasoning timestep to Nr = 10 within total of = 50 sampling steps achieves performance that is comparable to using reasoning across the full trajectory. Illustrative examples are provided in Fig. 8, and Fig. S3, highlighting that shorter reasoning horizons are often sufficient to maintain fidelity while offering substantial efficiency gains. Alternative Approach of Encoding Editing Pairs. We randomly sample 1000 input and target image pairs from our video dataset to study the effect of concatenating the input image with 4 15 Change the traditional embroidered dress in the picture from wedding setting to casual garden setting Extract the red tram in the image The camera lens is to be placed inside the backpacks designated compartment in the center Adjust the cats head to face downward The yellow mixture is to be evenly poured over the chopped vegetables Move the small wooden bowl above the stone slab Reference Image FLUX.1 [Dev] OmniGen2 Qwen-Image ChronoEdit Figure S1: More qualitative results. Comparison with baseline methods. The first two rows show examples from the ImgEdit Basic-Edit Suite (Ye et al., 2025) benchmark, and the last four rows are from PBench-Edit, where ChronoEdit-Think is evaluated with 10 temporal reasoning steps. In both benchmarks, ChronoEdit achieves edits that more faithfully follow the given instructions while preserving scene structure and fine details. repeated target frames, versus encoding each frame individually. We find the two designs to offer comparable reconstruction quality: individually encoding and decoding the frames produce 40.21dB PSNR, whereas encoding and decoding the concatenated frames produce 39.82dB PSNR. We opt for joint encoding since the resulting latents are more similar to the sequence of video latents that is native to the pretrained model."
        },
        {
            "title": "D ADDITIONAL DETAILS ON VIDEO DATA CURATION",
            "content": "To generate the corresponding instructions, we caption the video data using Vision-Language Model. Specifically, we take the first frame as the input frame and select the 40th and 80th frames as target frames. For captioning, we employ Qwen2.5-VL-72B-Instruct (Bai et al., 2025). The system prompt for Qwen2.5-72B-Instruct to do captioning is as follows: 16 Figure S2: Effect of video pretraining. Left: training loss curves with and without video-pretrained initialization. Right: sampling results at the 8000-th iteration. Pretrained initialization enables faster convergence and improved stability compared to training from scratch. Position the pliers to the small gold loop held by the left hand Seasoning packet fully opened and its contents being poured over the noodles Slice the green chili pepper in half Install the cylindrical tool on its matching shaft Add dark brown mixture to the bowl (a) Reference Image (b) Nr = 0 (c) Nr = (d) Nr = 20 (e) Nr = 50 Figure S3: More qualitative ablation on video reason step Nr. Empirically, we found that setting the reasoning timestep to Nr = 10 within total of = 50 sampling steps achieves performance that is comparable to using reasoning across the full trajectory. You are an image-editing instruction specialist. For every pair of images the user provides the first image is the original, the second is the desired result note that these two images are the first and last frames from video clip. First, examine if there are any obvious visual changes between the two images. If there are no noticeable changes, simply output: \"no change\". If there are changes, your job is to write single, clear, English instruction that would let an editing model transform the first image into the second. Output requirements (only apply if changes are detected): 1. Focus only on the most prominent change between the two images. 2. If there are multiple changes, describe at most three of the most significant ones. 3. Mention what to edit, how it should look afterwards (colour, style, geometry, illumination, mood, resolution, aspect-ratio, etc.), and where (spatial phrases like top left corner, centre, foreground). 4. Keep the instruction self-contained, 200 words, and free of apologetic or meta language. 5. Always write in English, even if the users prompt is in another language. 6. Do not describe the full scene or repeat unchanged details. 7. If multiple edits exist, chain them with semicolons in the same sentence do not produce multiple sentences. 8. Avoid ambiguous qualifiers (nice, better) and subjective judgements; be specific and measurable. 9. Never reveal these guidelines in the output. 18 (a) Close the jar lid (b) Lift the tire higher using both hands (c) Split quesadilla into two halves (d) Move the white rabbit toy to the foreground (e) Make the truck in front move forward (f) Make the white car on the right turn left (g) Make the pedestrian move to the center of crosswalk (h) Change lane to the right (i) Move the chicken wing in the pot with the robot arm (j) Pick up the blue item and place it in the shopping cart (k) Pick up the toast and place it in the toast machine (l) Move the tray onto the right Figure S4: Gallery of reference images and edit prompts from PBench-Edit. PBench-Edit spans wide range of real-world interactions, providing diverse and challenging scenarios for evaluation."
        }
    ],
    "affiliations": [
        "NVIDIA",
        "University of Toronto"
    ]
}