{
    "paper_title": "World Guidance: World Modeling in Condition Space for Action Generation",
    "authors": [
        "Yue Su",
        "Sijin Chen",
        "Haixin Shi",
        "Mingyu Liu",
        "Zhengshen Zhang",
        "Ningyuan Huang",
        "Weiheng Zhong",
        "Zhengbang Zhu",
        "Yuxiao Liu",
        "Xihui Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Leveraging future observation modeling to facilitate action generation presents a promising avenue for enhancing the capabilities of Vision-Language-Action (VLA) models. However, existing approaches struggle to strike a balance between maintaining efficient, predictable future representations and preserving sufficient fine-grained information to guide precise action generation. To address this limitation, we propose WoG (World Guidance), a framework that maps future observations into compact conditions by injecting them into the action inference pipeline. The VLA is then trained to simultaneously predict these compressed conditions alongside future actions, thereby achieving effective world modeling within the condition space for action inference. We demonstrate that modeling and predicting this condition space not only facilitates fine-grained action generation but also exhibits superior generalization capabilities. Moreover, it learns effectively from substantial human manipulation videos. Extensive experiments across both simulation and real-world environments validate that our method significantly outperforms existing methods based on future prediction. Project page is available at: https://selen-suyue.github.io/WoGNet/"
        },
        {
            "title": "Start",
            "content": "6 2 0 2 5 2 ] . [ 1 0 1 0 2 2 . 2 0 6 2 : r World Guidance: World Modeling in Condition Space for Action Generation Yue Su1,2, Sijin Chen2, Haixin Shi1, Mingyu Liu1, Zhengshen Zhang1, Ningyuan Huang1, Weiheng Zhong1, Zhengbang Zhu1, Yuxiao Liu1,, Xihui Liu2, 1ByteDance Seed, 2The University of Hong Kong Corresponding authors"
        },
        {
            "title": "Abstract",
            "content": "Leveraging future observation modeling to facilitate action generation presents promising avenue for enhancing the capabilities of Vision-Language-Action (VLA) models. However, existing approaches struggle to strike balance between maintaining efficient, predictable future representations and preserving sufficient fine-grained information to guide precise action generation. To address this limitation, we propose WoG (World Guidance), framework that maps future observations into compact conditions by injecting them into the action inference pipeline. The VLA is then trained to simultaneously predict these compressed conditions alongside future actions, thereby achieving effective world modeling within the condition space for action inference. We demonstrate that modeling and predicting this condition space not only facilitates fine-grained action generation but also exhibits superior generalization capabilities. Moreover, it learns effectively from substantial human manipulation videos. Extensive experiments across both simulation and real-world environments validate that our method significantly outperforms existing methods based on future prediction. Date: February 26, 2026 Correspondence: liuyuxiao.876@bytedance.com, xihuiliu@eee.hku.hk Project Page: https://selen-suyue.github.io/WoGNet/"
        },
        {
            "title": "Introduction",
            "content": "As systems designed for future action prediction, Vision-Language-Action (VLA) models [9, 10, 26] have been expected to improve task performance by developing more comprehensive ability of modeling the future [65]. Recent works have gone beyond predicting actions alone, investigating whether VLA models can forecast future signals in other modalities and how these predictions can be leveraged to enhance action generation [14, 31]. Existing methodologies within this landscape can be broadly categorized into two streams. (1) World Action Models [14, 24, 58] predict explicit future modalities (such as depth, images, videos), or semantic features from foundation vision models [27, 37] to facilitate efficient action generation. Despite providing rich perceptual cues regarding dynamics, motion, and spatial geometry, prior work [12, 34, 56] indicates that these generic while task-agnostic semantic spaces often contain substantial redundancy for downstream manipulation tasks. 1 This redundancy impedes pretraining efficiency for fine-grained generation and limits cross-scenario scalability, thereby constraining their real-world performance [12, 32]. (2) Latent Action Models [12, 18, 53] compress future actions or dynamics into sparse latent representations via reconstruction-based vision supervision, aiming to distill embodiment-agnostic high-level motion patterns. While effective for high-level planning and learnable from large-scale video data, these representations have been shown to offer only coarse guidance, lacking the precision required for fine-grained action generation [7, 56]. Collectively, these observations underscore fundamental trade-off. Predicting rich, task-agnostic future representations incurs significant redundancy, thereby increasing computational overhead and hampering performance [23, 42]. Conversely, compact latent action spaces typically capture only coarse motion trends, proving insufficient for fine-grained control [12, 56]. The pivotal challenge, therefore, lies in identifying predictive space [5] that is both tractable for VLA models to forecast and sufficiently expressive to guide accurate action generation. To address this challenge, we propose WoG (World Guidance), predictive framework that operates in the condition space for action generation. We posit that to identify non-redundant predictive space for world action model, the space should satisfy the criterion that its information serves as sufficient and effective condition for action generation. By virtue of this role, such space is intrinsically highly relevant to action; consequently, for VLA model inherently designed to model actions, inferring this space becomes tractable task. To discover such space, we argue that an efficient strategy is to directly incorporate future observations as conditions into the action inference pipeline [63]. The representation encoded through this pipeline thus naturally constitutes the desired efficient condition space. Figure 1 WoG first incorporates future observations into the action inference pipeline, projecting them into the condition space for action generation. Subsequently, it decouples future observations from the pipeline and simultaneously predicts these future conditions alongside actions, thereby transferring the knowledge of future conditions into the VLA model. Specifically, WoG follows two-stage training curriculum. In the first stage, we implement the aforementioned design by jointly conditioning action generation on current observations, encoded by VLM backbone, and future observations derived from frozen foundation vision models, which are queried and compressed by trainable Q-former based Encoder before being integrated into the action head. This stage jointly optimizes (i) the encoder to project future observations into this efficient, implicit action-condition space, and (ii) the VLA backbone to leverage these future-conditioned representations for precise action prediction. In the second stage, we freeze the Q-former based Encoder to define stable target space. The VLA is then trained to simultaneously predict this future conditioning representation and the corresponding actions, yielding model capable of internally anticipating and utilizing future guidance during inference as shown in Figure 1. We validate the efficiency of WoG in facilitating fine-grained action generation and ensuring robust generalization through extensive simulation and real-world experiments, where it demonstrates substantial improvements over existing methods. Furthermore, we show that WoG can be effectively improved by learning to model and predict future conditions from large-scale human videos (spanning both action-annotated and unannotated data) or UMI data [20], leading to significant performance gains in real-world robotic deployments."
        },
        {
            "title": "2.1 World Action Models",
            "content": "Leveraging future observation prediction [16, 43, 50] to extract dynamics [24, 52] for robotic manipulation has been extensively studied in imitation learning [19, 44, 45, 62]. With the recent advances in VLA models [3, 9, 15, 26], growing body of work has begun to exploit the strong reasoning capabilities of Vision2 Language Models (VLMs) [6, 25, 51] and video generation models [11, 47] to perform dynamic modeling, thereby more effectively supporting action inference. Specifically, certain approaches [24, 31] introduce the intermediate features of video generation models [1, 11] as world representations, integrating them into action modules to capture future manipulation dynamics. Alternative strategies [13, 14, 57] build directly upon VLM backbones, modeling such dynamics through the internal generation of future images [21]. Beyond images generation, the prediction of other explicit modalities, such as depth or optical flow, is also often incorporated into VLA co-training objectives to serve comparable predictive role [58, 64]. Most recently, studies have also explored directly regressing the latent representations output by foundation vision models, yielding discriminative future features that further refine the precision of action prediction [58]. Our method stands out by learning condition space optimized for action generation, designed to provide more efficient support for enhancing model performance."
        },
        {
            "title": "2.2 Latent Action Models",
            "content": "Latent Action Models (LAMs) [53] have emerged as response to training VLA models effectively from large-scale, heterogeneous datasets [38]. They are built upon the assumption that, despite embodiment variations [60], actions admit high-level representations that are skill-related but embodiment-agnostic. Such representations are typically discrete [46] and are intended to capture coarse motion trends, thereby facilitating high-level planning. Mainstream LAMs [12, 18, 53] compress heterogeneous actions across embodiments by using visual reconstruction objectives, after which VLA model first predicts discretized latent actions and subsequently decodes them into fine-grained actions suitable for downstream tasks [2]. These reconstructions may rely on either generative [53] or discriminative representations [12], but are all designed to produce compact and accurate latent action spaces. However, prior studies [55, 56] have observed that the compression performed by LAMs often resembles PCA-like [4] extraction of maximum-variance signals, yielding coarse planning representations and, in some cases, introducing noise from scenarios that is weakly correlated with actions. To mitigate these limitations, recent work has incorporated action reconstruction into the generative objectives of LAMs to strengthen the mapping between latent actions and control [22, 36, 55]. Other approaches further introduce video generation as an auxiliary co-training objective in the LAM framework, aiming to compensate for the lack of fine-grained action guidance [36, 41]. While sharing the goal of providing rich guidance, we posit that introducing condition prediction instead of video reconstruction in latent space is efficient."
        },
        {
            "title": "3.1 Problem Formulation\nWe consider the problem of predicting future T actions At:t+T given the current observation Ot at time step\nt, and a language instruction l. The VLM backbone encodes the observation and instruction as z ← f (Ot, l),\nwhich is then fed into an action head to generate actions by maximizing the likelihood of P (At:t+T | z).\nIn the first stage of training WoG, observations from the next T future time steps are guided to be compressed\n. Together with the current latent representation z, the compressed\ninto a condition space, denoted as Oc\nfuture condition is fed into the action head as a guidance to generate actions by modeling P (At:t+T | z, Oc\nt:t+T ).\nThrough this process, the VLA model learns to encode conditions from the current observation while leveraging\ncomplementary conditions derived from future observations for action prediction, and simultaneously acquires\na compact representation of future observations.",
            "content": "t:t+T However, given our goal of performing action inference solely based on the current observation at test time, the subset of conditions derived from the future is not accessible and must be inferred from the current observation. Under the assumption of deterministic environmental dynamics, the complete inference process should be formulated as: (At:t+T , Oc t:t+T z) = (At:t+T z, Oc t:t+T )P (Oc t:t+T z). (1) 3 Figure 2 Overview of WoG. WoG is trained in two stages. In the first stage, future observations encoded by frozen vision foundation models are queried and compressed by trainable Q-former-based Future Encoder to form condition representations, which, together with VLM-encoded current observations and instructions, are used for action prediction. In the second stage, the encoder and vision models are frozen, and the VLM backbone is trained to align with the conditions while predicting actions. Therefore, in the second stage, WoG is trained with two objectives. At the output of the VLM, supervision is applied to predict the future condition by modeling (Oc t:t+T z). Then at the output of the action head, supervision is imposed on action prediction as (At:t+T z), by optimizing this marginal action likelihood as the joint distribution under the deterministic coupling of dynamics. Together, these objectives transfer the knowledge of future condition into the VLM backbone itself. The overall process of WoG is shown in Figure 2."
        },
        {
            "title": "3.2 Stage I: World Guidance",
            "content": "In the first stage, we use Prismatic VLM [25] adopted in OpenVLA [26] as the VLM backbone. The current observation and language instruction are encoded by the VLM backbone to obtain the latent representation (Following [29], we represent it as the output feature of the last learnable token), which is then fed into DiT [39] action head for action generation. For future observations, they are first encoded using combination of frozen pretrained vision models to obtain high-level representations. By default, we extract discriminant and semantic features by DINOv2 [37] and generative features by Wan VAE Encoder [47]. Such combination of foundation vision models is extensible and can be replaced by other pretrained visual encoders [27, 54]. After being projected to unified embedding dimension, these future dynamic features are processed by learnable Q-Former-based Encoder [28], which queries action-relevant features and projects them into low-dimensional conditioning representations as Oc. The queried representations Oc are then injected into each DiT block, where they perform cross-attention with to enable action prediction conditioned on both the current and future information. We adopt rectified flow [35] to predict the velocity field: LI = Eτ, (cid:104)(cid:13) (cid:13)vθ(Aτ , τ, z, Oc) v(cid:13) 2 (cid:13) 2 (cid:105) , (2) where τ [0, 1] denotes the scheduling timestep, vθ and denote the predicted and target velocity, respectively."
        },
        {
            "title": "3.3 Stage II: World Inference",
            "content": "In the second stage, the Q-Former and the projectors used for encoding future observations are frozen. WoG is designed to jointly train action prediction (At:t+T z) and future condition prediction (Oc Specifically, we introduce learnable query embeddings to attend to the last hidden states of the VLM output with cross-attention paradigm, and align the queried representations with the frozen future condition t:t+T z). 4 representations Oc produced by the Encoder. Afterwards, only the VLM output is fed into the DiT head as input to predict actions. The loss is formulated as: LII = Eτ, (cid:104)(cid:13) (cid:13)vθ(Aτ , τ, z) v(cid:13) 2 (cid:13) (cid:105) + 1 (cid:104) Oc, fq(O, l) (cid:105) , (3) where fq(O, l) denotes the queried last hidden state of the VLM, and S[, ] denotes the cosine similarity. In this stage, the future condition is decoupled from the action head and becomes one of the prediction targets of the VLM. Through supervision, the VLM is encouraged to encode future condition information in its internal representations, enabling the model to perform complete action inference solely based on z. As result, WoG is transformed into self-guided model. More details about the query mechanism of WoG are provided in Appendix A."
        },
        {
            "title": "3.4 Learning From Human Manipulation",
            "content": "Our method is also easy to extend to learn from human manipulation videos, which can be incorporated in two complementary ways. (1) small amount of human videos with action annotations is introduced in the first stage, together with robot data, to expand the condition space and capture manipulation knowledge absent from robot demonstrations. In the second stage, much larger collection of unlabeled human videos is incorporated to supervise future condition prediction, while action supervision is applied only to robot data and optionally to the annotated human subset. This strategy enables the model to learn more sufficient and generalizable future conditions from large-scale video data. (2) No action-annotated human data is required. Human videos are directly introduced in the second stage to supervise condition prediction, while the action prediction branch of human videos is masked. This setting assumes that the first stage trained on robot data already learns sufficiently expressive condition space, and that many of these conditions, such as object motion dynamics, are shared with human manipulation videos. Under this assumption, second-stage training alone can further improve the models ability to predict future conditions and enhance generalization across diverse scenarios. We provide comprehensive validation of both strategies in Section 5.4, demonstrating their effectiveness. In addition, UMI can also be regarded as universal interface for robot data. We expect that training on UMI data can further benefit our study, as it provides means to evaluate whether the condition space of WoG can be stably modeled under egocentric observations and unseen embodiments, and subsequently transferred to our target workspace. We conduct corresponding validation in Section 5.5."
        },
        {
            "title": "4.1 Setup\nEvaluation Setup. We evaluate our method in the SIMPLER simulation environment [30], which includes\ntwo robotic configurations: the Google Robot and WidowX. During evaluation, the model operates in a\nclosed-loop manner and receives only a single RGB observation at each time step. Detailed training and\nevaluation settings of our method can be found in Appendix B.",
            "content": "Baselines. In the baseline comparison, we aim to highlight both the advantages of WoG over conventional VLA approaches and its distinctions from closely related methods. Accordingly, we include broad and representative set of VLA baselines spanning multiple paradigms. Specifically, our comparisons cover: (i) Conventional VLA methods that directly map visual-language observations to actions: π0 [10], π0-FAST [40], OpenVLA [26] and GR00T-N1 [8]; (ii) Latent Action Models: Moto [18] and UniVLA [12]; (iii) World Action Models that leverage video prediction to capture dynamics: DeFI [59]; (iv) Methods that perform latent action modeling with future video generation: VITA [36] and ViPRA [41]. Table 1 SimplerEnv evaluation across different models on Google Robot tasks. Mv Near: Move Near, Drawer: Open/Close Drawer. WoG outperforms existing methods across the majority of tasks, with particularly notable gains in scenarios that necessitate efficient trajectory planning and collision avoidance. Model π0 [10] π0-FAST [40] OpenVLA [26] GR00T-N1 [8] Moto [18] VITA [36] DeFI [59] WoG Visual Matching Pick Coke Mv Near Drawer Avg. Variant Aggregation Pick Coke Mv Near Drawer Avg. Overall Avg. 72.7% 75.3% 16.3% 47.0% 74.0% 57.5% 54.2% 89.0% 65.3% 67.5% 46.2% 70.0% 60.4% 55.8% 60.7% 82.5% 38.3% 58.8% 42.9% 61.9% 35.6% 32.7% 18.1% 45.0% 43.1% 59.2% 58.9% 57.4% 38.6% 51.2% 62.5% 78.0% 75.2% 77.6% 54.5% 78.8% 53.9% 87.9% 63.7% 68.2% 47.7% 62.5% 58.2% 75.0% 25.6% 54.8% 56.8% 59.0% 60.5% 31.3% 17.7% 39.8% 33.8% 13.2% 51.5% 48.4% 24.0% 45.4% 48.3% 19.3% 60.7% 69.4% Table 2 SimplerEnv evaluation across different models on WidowX Robot tasks. WoG outperforms existing methods across broad spectrum of Pick-and-Place tasks, notably surpassing approaches that jointly model video prediction and latent actions. Model π0 [10] π0-FAST [40] OpenVLA [26] GR00T-N1 [8] UniVLA [12] ViPRA [41] WoG Put Spoon on Towel Stack Green on Yellow Put Carrot on Plate Put Eggplant in Basket Overall Average Grasp Spoon Success Grasp Block Success Grasp Carrot Success Grasp Eggplant Success Grasp Avg. Success Avg. 45.8% 62.5% 4.1% 83.3% 76.4% 79.2% 95.8% 29.1% 29.1% 0.0% 62.5% 52.8% 66.7% 79.2% 25.0% 58.5% 33.0% 54.2% 66.7% 62.5% 75.0% 0.0% 21.9% 0.0% 45.8% 2.8% 54.2% 33.0% 50.0% 54.0% 12.5% 70.8% 79.2% 54.2% 70.8% 16.7% 10.8% 0.0% 16.7% 55.6% 50.0% 50.0% 91.6% 83.3% 8.3% 41.7% 87.5% 91.7% 100.0% 62.5% 66.6% 4.1% 20.8% 66.7% 79.2% 91.7% 40.1% 48.3% 7.8% 49.5% 77.5% 71.9% 85.4% 27.1% 32.1% 1.1% 36.5% 45.6% 62.5% 63.5%"
        },
        {
            "title": "4.2 Results",
            "content": "Most tasks in the SIMPLER benchmark belong to the pick-and-place type, where successful execution critically depends on both dynamic trajectory planning and precise end-effector pose prediction. In particular, obstacle avoidance during motion requires anticipating scene dynamics, while grasping and placement demand accurate reasoning about future contact and collision constraints. As shown in Table 1 and Table 2, WoG achieves strong and consistent performance improvements over all baselines across the majority of tasks. In scenes where interfering objects are present, such as Move Near, our approach exhibits markedly superior trajectory planning behavior, effectively navigating dynamic interference and maintaining stable execution. This highlights the benefit of incorporating future-aware conditions for motion reasoning under complex environmental dynamics. Furthermore, in broad P&P tasks like Pick Coke and Put Spoon, WoG substantially improves the accuracy of future grasp and placement pose prediction. By extracting future semantics in manner that is both sufficient and compact, our approach achieves superior accuracy in estimating target poses, thereby enhancing positioning precision and collision avoidance capabilities. It is particularly noteworthy that, while our paradigm shares the similar underlying set of future observations with methods combining latent actions with future video generation, it diverges fundamentally in how this information is utilized. Unlike full-scale video prediction, which aims to reconstruct all visual information, our method selectively extracts critical semantics into the condition space. This strategy yields more robust representation that effectively mitigates the propagation of visual prediction errors into the action space, ultimately facilitating action prediction with significantly higher precision. We also note that in small subset of tasks characterized by action constraints, such as Stack Green on Yellow and Drawer for their requirement on accurate relative position of stacks or gripper and drawer, the performance gains are comparatively smaller. This is primarily attributable to the limited spatial resolution of the current backbone and the inherent difficulty of modeling fine-grained geometry [17, 61] through current dynamic prediction alone."
        },
        {
            "title": "4.3 Pretrained Encoder Configuration",
            "content": "To assess the adaptability of our framework, we instantiate WoG with 3 different combinations of foundation vision encoders: (i) DINOv2 [37] only, (ii) DINOv2 paired with SigLIP [54], and (iii) DINOv2 paired with the Wan VAE encoder [47]. The results of these configurations are presented in Table 3 and Table 4. Table 3 SimplerEnv evaluation across different pretrained encoder configurations on Google Robot tasks. Mv Near: Move Near, Drawer: Open/Close Drawer. Model Visual Matching Pick Coke Mv Near Drawer Avg. Variant Aggregation Pick Coke Mv Near Drawer Avg. Overall Avg. WoG (dino) WoG (dino-siglip) WoG (dino-vae) 96.0% 89.0% 97.7% 85.8% 82.5% 87.1% 56.0% 79.3% 62.5% 78.0% 61.1% 82.0% 88.1% 87.9% 89.7% 81.0% 75.0% 77.5% 9.8% 59.6% 69.5% 69.4% 19.3% 60.7% 12.4% 59.9% 70.9% Table 4 SimplerEnv evaluation across different different pretrained encoder configurations on WidowX Robot tasks. Model Put Spoon on Towel Stack Green on Yellow Put Carrot on Plate Put Eggplant in Basket Overall Average Grasp Spoon Success Grasp Block Success Grasp Carrot Success Grasp Eggplant Success Grasp Avg. Success Avg. WoG (dino) WoG (dino-siglip) WoG (dino-vae) 83.3% 95.8% 95.8% 66.7% 79.2% 54.2% 41.7% 75.0% 66.7% 8.3% 33.0% 29.2% 50.0% 70.8% 83.3% 29.2% 50.0% 50.0% 100.0% 100.0% 100.0% 91.7% 91.7% 100.0% 68.8% 85.4% 86.4% 49.0% 63.5% 58.4% The quantitative results presented in Table 3 and Table 4 reveal three key insights regarding the design of the condition space: Benefits of Enriched Representations: Both the SigLIP and Wan VAE augmented configurations consistently outperform the distinct DINOv2 baseline. This validates that enriching the condition space with high-level semantics or temporal dynamics significantly enhances policy robustness. VAE for Trajectory Planning: The dino-vae variant demonstrates superior performance on Google Robot tasks (e.g., Pick Coke, Move Near ), achieving the highest overall success rate of 70.9%. We attribute this to the VAE encoders ability to compress spatiotemporal information, which effectively aids the policy in modeling object dynamics and planning smooth trajectories under environmental disturbances. SigLIP for Spatial Precision: On tasks that demand fine-grained spatial reasoning, such as Stack Green on Yellow, the dino-siglip variant exhibits distinct advantage, surpassing dino-vae by notable margin in success rate (33.0% vs. 29.2%). This suggests that explicit high-level semantic alignment provided by SigLIP is critical for handling tasks with spatial constraints and precise positioning requirements. Its noteworthy that the integration of SigLIP enriches the condition space with high-level semantics, partially mitigating spatial precision deficits and ensuring robust performance across the SIMPLER benchmark. Thus, we utilize the dino-siglip configuration as the default for simulation. Nevertheless, we contend that modeling fine-grained spatial constraints remains persistent challenge independent of the future encoder. Solving this requires dedicated spatial mechanisms [48, 66] or historical observation modeling, which lies beyond the scope of this work. Since our real-world experiments are tailored to validate trajectory planning, we retain the VAE-incorporated framework in physical evaluations to fully exploit its superior capacity."
        },
        {
            "title": "4.4 Ablation of Future Encoder",
            "content": "To verify the effectiveness of utilizing the Future Encoder to query compact condition representations from vision foundation models, we conduct an ablation study focusing on the encoder mechanism under the dino-vae configuration. Specifically, we evaluate the following variants to assess the necessity of the Future Encoder: WoG w/o Future Enc.: This variant completely excludes the Future Encoder from both training stages. Instead of compressing visual features, we instantiate set of learnable embeddings equivalent in number to the total tokens of the DINOv2 and VAE feature maps. These embeddings query the last hidden 7 states of the VLM and are supervised to align directly with the full, uncompressed feature maps from the frozen vision models in the total 150k training steps. WoG w/o Future Enc. in Stage-II: In this setting, the Future Encoder is retained during the first stage to inject compact guidance into the action generation pipeline. However, in the second stage, the encoder is discarded, and the VLM is trained to align its predictive representations with the full, uncompressed feature maps of the foundation models, identical to the alignment strategy in WoG w/o Future Enc. WoG w. Future Enc.: The standard training paradigm of our method, where the VLM is supervised to align with the low-dimensional condition representations Oc queried and compressed by the Future Encoder. Table 5 SimplerEnv evaluation across different Future Encoder ablation variants on Google Robot tasks. Mv Near: Move Near, Drawer: Open/Close Drawer. Model Visual Matching Pick Coke Mv Near Drawer Avg. Variant Aggregation Pick Coke Mv Near Drawer Avg. Overall Avg. WoG w/o Future Enc. WoG w/o Future Enc. in Stage-II WoG w. Future Enc. 93.3% 91.3% 97.7% 79.2% 87.1% 87.1% 52.8% 75.1% 46.3% 74.9% 61.1% 82.0% 87.3% 88.0% 89.7% 72.9% 77.5% 77.5% 58.3% 66.7% 58.4% 66.7% 14.8% 9.8% 12.4% 59.9% 70.9% Table 6 SimplerEnv evaluation across different Future Encoder ablation variants on WidowX Robot tasks. Model Put Spoon on Towel Stack Green on Yellow Put Carrot on Plate Put Eggplant in Basket Overall Average Grasp Spoon Success Grasp Block Success Grasp Carrot Success Grasp Eggplant Success Grasp Avg. Success Avg. WoG w/o Future Enc. WoG w/o Future Enc. in Stage-II WoG w. Future Enc. 75.0% 83.3% 95.8% 54.2% 70.8% 54.2% 58.3% 58.3% 66.7% 29.2% 29.2% 29.2% 75.0% 50.0% 83.3% 54.2% 37.5% 50.0% 91.7% 95.8% 100.0% 91.7% 91.7% 100.0% 75.0% 71.8% 86.4% 57.3% 57.3% 58.4% The experimental results, detailed in Table 5 and Table 6, demonstrate that the proposed WoG, which utilizes the Future Encoder to query conditions, consistently outperforms variants that lack this component across the majority of tasks. This advantage is particularly pronounced in the success rates of grasping and picking phases, suggesting that the condition extraction mechanism effectively leverages the potential of the underlying vision foundation models and keep generalizable during various setups, by amplifying the trajectory planning capabilities inherent to the dino-vae configuration during action execution. We also observe that the method does not yield distinct advantage in spatially sensitive placement tasks, such as Stack. This reveals limitation of the current paradigm: while WoG effectively magnifies the inherent strengths of foundation models for action generation, it cannot inject the additional semantic information required to compensate for their intrinsic deficiencies in specific capabilities."
        },
        {
            "title": "5.1 Setup\nIn-Distribution (ID) Setup. We design the following 3 tasks for evaluation and collect corresponding expert\ndemonstrations, forming our In-Distribution (ID) setup:",
            "content": "Pick and Place (P&P) is rigid-body manipulation, where the robot picks and places green cup into plate following the given instruction. Successful execution requires avoiding table-top obstacles and collisions with objects already in the plate, which evaluates the models ability to predict action-relevant dynamics for collision-aware trajectory planning. We collect 100 expert demonstrations for this task. Close the Microwave is an articulated-object manipulation task, where the robot is required to close the microwave door. This task evaluates the policys ability to predict and control articulated rotational dynamics. We collect 100 expert demonstrations for this task. Fold the Towel is deformable manipulation task, where the robot grasps one bottom corner of towel folded into an isosceles triangle and aligns it with the other bottom corner. The task is considered successful if the distance between the two corners is less than 5 cm. We collect 200 expert demonstrations for this task. 8 Figure 3 Overview of our real-world experiment setup. The figure shows our robotic platform and sensors (left), the execution of the three tasks under the in-distribution setup (middle), and the modifications applied for the out-of-distribution setup (right). Out-of-Distribution (OOD) Setup. To evaluate generalization, we construct 3 conditions unseen in the expert demonstrations, forming the Out-of-Distribution (OOD) setup: Background Change: Expert demonstrations are collected using single tablecloth, while evaluation is conducted with different tablecloth to introduce background variation. Light Change: During evaluation, numerically controlled light source with fixed intensity is projected onto the workspace from fixed position, introducing lighting conditions not covered by the expert demonstrations. Novel Object: For Fold the Towel, evaluation is conducted using unseen towel. For Pick and Place, we replace the original cup with cups of different colors and shapes and modify the corresponding instructions accordingly. Platform. We use UR5 robotic arm equipped with Robotiq 2F-85 gripper for manipulation. top-down Intel RealSense D435 and an L515 camera are mounted to provide global observations, while only the D435 is used in this work. All devices are connected to workstation with an NVIDIA RTX 4090 GPU for model inference and control. Baselines. We select UniVLA [12], Latent Action Model, and VPP [24], which leverages future video prediction, both of which have been widely validated in real-world robotic settings, as baselines. In addition, we compare against variants that ablate each training stage, as well as models trained on different data sources, to comprehensively assess the contribution of our approach. Protocols. For the real-world evaluation, 20 trials are performed per method for each task. All methods are evaluated under closely matched randomized initial scene configurations for each trial. The above experiment setups are shown in the Figure 3. Detailed training and evaluation settings of our method can be found in Appendix C."
        },
        {
            "title": "5.2 Effective and Generalizable Manipulation",
            "content": "The performance results of WoG and baselines are summarized in Table 7. Through these evaluations, we aim to assess whether WoG can effectively model future condition and leverage its prediction to facilitate action generation in dynamic interactions. We further examine whether incorporating future observation prediction leads to overfitting to ID scenarios, potentially degrading generalization performance under OOD setup. Fine-grained action prediction in dynamic interactions. Our tasks challenges the model to handle rigid, articulated, and deformable objects, requiring precise low-level control under complex dynamics. In the P&P task, which necessitates simultaneous obstacle avoidance and precise end-effector placement, UniVLA falters due to the coarse resolution of its high-level latent planning. While VPP captures dynamics via video prediction, it is consistently outperformed by WoG. This comparison underscores the advantage of 9 Table 7 Detailed performance comparison across all tasks. We report the success rate (%) for In-Distribution (ID) and various Out-of-Distribution scenarios. For OOD settings, scores are formatted as ID OOD. Best results are highlighted in bold. Model UniVLA [12] VPP [24] WoG Microwave Pick and Place Fold the Towel ID 80% 90% 100% ID Background Novel Object ID Background Light Change Novel Object 25% 25%20% 25%10% 20% 20%20% 55% 55%30% 55%15% 45% 45%30% 60%40% 60% 60%50% 60% 60%55% 20%10% 45%20% 60%35% 20%10% 45%30% 60%50% explicitly modeling predictive information in dedicated action condition space, rather than relying on high-dimensional visual predictions. This advantage is further amplified in the Fold task. Controlling deformable objects requires accurate trajectory planning and precise release timing to achieve the target geometry. WoG significantly widens the performance gap over VPP, as its condition space effectively distills manipulation-relevant dynamics (e.g., cloth deformation) while discarding the redundant perceptual signals inherent in video generation. Similarly, in the Close task, WoG achieves near-perfect performance, confirming that the predicted conditions are sufficient to guide fine-grained interaction with articulated dynamics. Generalization ability in OOD scenes. key aspect of generalization from upstream pretraining to downstream finetuning is the ability to transfer manipulation knowledge across heterogeneous embodiments, while preserving reliance on high-level visual representations rather than overfitting to the finetuning environment. Under background and novel object variations, baseline methods exhibit pronounced degradation. Latent action models tend to implicitly overfit to object-specific dynamics coupled with training appearances, limiting transferability. Similarly, VPP is constrained by the visual distribution of expert demonstrations, leading to artifacts when inputs deviate from the training domain. In contrast, WoG maintains superior performance with minimal degradation across all OOD scenarios. We attribute this robustness to the design of the condition space: by querying and compressing informative features from frozen, pretrained visual encoders, WoG constructs conditions that are highly distinctive for manipulation yet invariant to visual nuisances. This design ensures that the generalization power of upstream visual priors is preserved rather than distorted during finetuning. Notably, even under severe lighting changes, as the most challenging shift, WoG exhibits the smallest relative performance drop. This confirms that our framework learns stable, action-centric representations, striking an optimal balance between expressiveness for control and compactness for generalization."
        },
        {
            "title": "5.3 Efficient Training Strategy\nWe pretrain WoG and its variants on the OXE dataset under different training strategies as ablations. vanilla\nVLA follows standard VLA pretraining with the same VLM backbone and DiT head, supervising actions\nfrom current observations only. WoG w/o cotrain is trained using the first-stage future-observation-guided\nobjective and second-stage action supervision without condition supervision. All models are fine-tuned with\nidentical expert demonstrations on downstream tasks. Details are shown in Appendix C.3.",
            "content": "Table 8 Ablation of training stages in WoG. Vanilla VLA predicts actions solely from current observations. WoG w/o cotrain removes the condition supervision in the second stage. We report success rates (%); for OOD settings, scores are formatted as ID OOD. Best results are highlighted in bold. Model Vanilla VLA WoG w/o cotrain WoG (Ours) Microwave Pick and Place Fold the Towel ID 90% 95% 100% ID Background Novel Object ID Background Light Change Novel Object 45% 45%45% 45%40% 40% 40%25% 45% 45%45% 45%35% 30% 30%30% 60%40% 60% 60%50% 60% 60%55% 40%10% 30%10% 60%35% 40%30% 30%30% 60%50% As shown in Table 8, WoG trained with the two-stage scheme outperforms both variants. Compared to 10 the vanilla VLA, WoG achieves improvements across all tasks under the ID setup, demonstrating its ability to extract effective conditions from observations. Moreover, under the OOD setup, WoG maintains strong generalization performance, indicating the robustness of the predictive condition space. For WoG w/o cotrain, we observe performance comparable to the vanilla VLA across tasks. This suggests that introducing future conditions does not significantly degrade the action generation capability of the VLM backbone. However, this variant still falls notably behind the full WoG, highlighting the necessity of the co-training. These results confirm that explicitly supervising the alignment between future conditions and the VLM backbone is crucial for distilling future action-relevant knowledge into the VLA model."
        },
        {
            "title": "5.4 Learning from Human Data",
            "content": "We evaluate the two human data utilization strategies introduced in Section 3.4, with results summarized in Table 9. w. human v. denotes leveraging unannotated human videos only in the second stage for future condition supervision. w. human v./a. further incorporates small subset of action-annotated human videos, which are supervised for action prediction in both training stages. Specific dataset information and training strategies are introduced in Appendix C.4. Table 9 Performance under different human data integration strategies. w/o human data: trained solely on robot data. w. human v.: trained with unannotated human videos. w. human v./a.: trained with mix of annotated and unannotated human videos. Best results are bold. Strategy Pick and Place Fold the Towel ID Background Novel Object ID Background Light Change Novel Object w/o human data (Base) w. human v. w. human v./a. 60% 60%55% 70% 70%70% 70% 70%70% 60%40% 70%35% 70%45% 60% 60%50% 50% 50%45% 65% 65%60% 60%35% 50%30% 65%45% 60%50% 50%45% 65%50% As shown in Table 9, we observe that even when human videos are used solely for condition prediction supervision in the second stage, our model still benefits on the P&P task and exhibits smaller relative performance drop under OOD settings. However, performance degrades on the deformable object manipulation task. We attribute this behavior to the task-dependent similarity between human and robotic manipulation. For pick-and-place, task execution patterns in human demonstrations closely resemble robotic behaviors, leading to more aligned action-relevant conditions. In contrast, deformable object manipulation induces larger mismatch in the condition space for many conditions from more flexible human manipulations are not modeled during first-stage robot training, which limits the transferability of human manipulation knowledge and results in degraded performance. Nevertheless, even with only 220h of action-annotated human data introduced in the first training stage, WoG is able to rapidly acquire human-aligned conditioning representations and effectively transfer them to robotic manipulation. The resulting model consistently outperforms its robot-only counterpart across all ID and OOD settings, demonstrating substantially improved generalization. These results indicate the strong potential of our framework to scale with larger and more diverse human datasets."
        },
        {
            "title": "5.5 Learning from UMI Data\nWe collected an additional 120 UMI trajectories for both the P&P and Fold tasks to augment the training\nprocess detailed in Appendix C.5. Crucially, this data was introduced exclusively during the second-stage\nfine-tuning alongside our expert demonstrations, ensuring that the condition space learned in the first stage\nremained unaltered. Despite the significant domain gaps characterized by UMI’s egocentric observations,\ndistinct action representations, and a completely different embodiment configuration: the fact that the\ncondition space was established solely on OXE pretraining without prior exposure to such inputs, WoG\nachieved remarkable performance gains.",
            "content": "Specifically, success rates surged from 60% to 85% on P&P and from 60% to 80% on Fold as shown in Figure 4. These results demonstrate that even when pretrained strictly on standard robot data, WoG 11 acquires highly robust and generalizable condition encoding capability. We attribute this success to the models proficiency in capturing embodiment-agnostic dynamics, such as intrinsic object motion, which facilitates the seamless integration of UMI data and highlights the broad scalability of our approach."
        },
        {
            "title": "6 Conclusion",
            "content": "We propose WoG, novel world modeling framework for action generation that compresses future observations into low-dimensional condition space via guidance. By jointly predicting these compact future conditions within VLA, WoG strikes an effective balance between efficient future forecasting and the acquisition of rich manipulation knowledge. Experiments demonstrate the efficiency and strong generalization capability of our approach, while validating the effectiveness of the proposed training strategy and the scalability of WoG to large-scale human manipulation data. Future work may focus on designing more expressive and efficient condition representations to better handle scenarios with strong spatial or action constraints, as well as exploring improved knowledge distillation and more generalizable condition learning from human videos. Figure 4 Performance after training with UMI data. Compared to training solely with robot data, the WoG showed 42% improvement in performance on the P&P task and 33% on the Fold task."
        },
        {
            "title": "A Detailed Architecture",
            "content": "Our method samples future visual observations at temporal frequency of one-quarter relative to the action sequence. Concretely, for the default prediction horizon of 16 action steps, we uniformly sample 4 frames to serve as the future observation sequence. Regarding feature encoding, the frozen DINOv2 model processes each sampled image to extract semantic features. In parallel, for the Wan VAE, we utilize the current observation as the initiating frame, encoding it jointly with the sampled future frames to capture temporal and spatial features. Both the DINOv2 features and the VAE features (with their last two spatial dimensions flattened) are projected into unified embedding space matching the hidden dimension of the Q-Former and are subsequently stacked. Q-Former, instantiated with = 16 learnable query tokens, aggregates the necessary dynamic representations from this combined feature set via cross-attention mechanisms, ultimately compressing the information into compact condition space with dimensionality of = 32 by default. The query mechanism of the Q-Former-based Encoder remains consistent across both training stages, with the sole distinction being that the encoder transitions from trainable state in the first stage to frozen state in the second. In this second stage, the VLM processes the current observation to yield the sequence of last hidden states. Capitalizing on the causal nature of the architecture, the trailing tokens effectively distill the most comprehensive visual and linguistic contextanalogous to our strategy of utilizing the final learnable token as the action token. Consequently, we select the last 4 tokens from the VLM output to align with the target future representations extracted by the frozen vision foundation models. To implement this alignment, we instantiate 16 learnable query embeddings, matching the token count of the condition representation. These embeddings perform cross-attention over the selected VLM hidden states and are subsequently projected into the 32-dimensional space to compute the alignment loss against the ground-truth future conditions. We detailed above query mechanisms in Figure 5. Figure 5 Detailed illustration of the query mechanisms within WoG. The left panel depicts the future encoder, which maintains set of learnable queries to extract low-dimensional, action-relevant conditions from the features of pretrained vision models. The right panel illustrates the query mechanism for condition prediction during the second training stage. Here, set of learnable query embeddings performs cross-attention with the last hidden states of the VLM, producing predictive representations that are supervised to align with the target conditions generated by the now-frozen future encoder."
        },
        {
            "title": "B Simulation Experiments",
            "content": "B.1 Training Settings For simulation evaluation, Our model is pretrained on the Open X-Embodiment (OXE) dataset [38] for 100k training steps with global batch size of 1024 in the first stage, the dataset sampling ratios are shown in Table 10. In the second stage, the model is further trained on the Bridge and Fractal datasets for 50k steps 13 using the same batch size with individual ratios as about 51% and 49%. No additional fine-tuning is needed on individual Bridge or Fractal datasets. Table 10 Dataset Sampling Ratios during the OXE pretraining. Dataset Ratio Dataset Ratio Dataset Fractal Taco Play Roboturk Toto NYU Franka Play Dataset Austin Sailor Dataset IAMLab CMU Pickup Insert CMU Stretch DobbE Jaco Play 0.140639 Kuka 0.032661 0.025753 Viola Stanford Hydra Dataset 0.022367 0.009245 Furniture Bench Dataset 0.024248 Austin Sirius Dataset 0.010043 UTAustin Mutex 0.001718 BC-Z 0.015656 DROID 0.140674 Bridge 0.005354 Berkeley Cable Routing 0.010483 Berkeley Autolab UR5 0.049202 Austin Buds Dataset 0.027112 UCSD Kitchen Dataset 0.019224 DLR EDAN Shared Control 0.024852 Berkeley Fanuc Manipulation 0.082621 FMB Dataset 0.111433 Ratio 0.146649 0.002907 0.013452 0.002343 0.000545 0.000613 0.008600 0.023224 B.2 Evaluation Settings The SIMPLER simulator [30] distinguishes itself by functioning as rigorous real-to-sim evaluation proxy for the Fractal and Bridge datasets, thereby compelling models to tackle cross-domain transfer challenges. Notably, the simulator offers two evaluation protocols characterized by distinct degrees of domain shift for Google Robot tasks: Visual Matching, which aims to closely replicate real-world tasks by minimizing visual discrepancies between simulated and physical environments; and Variant Aggregation, which introduces more severe distributional shifts by modifying environmental elements such as backgrounds, lighting conditions, distractors, and table textures. Real-World Experiments C.1 Training Settings For real-world experiments, our model is pretrained on the Open X-Embodiment (OXE) dataset for 100k training steps with global batch size of 1024 in the first stage and 50k steps on OXE in the second stage. We also use the OXE pretrained checkpoints provided by the baselines. All the models are then finetuned on our real-world episodes for 30 epochs with global batch size of 512 (During the fine-tuning phase, WoG mirrors the training protocol of only the second stage, maintaining the co-training objective that jointly optimizes for both action prediction and future condition forecasting). The oxe pretraining dataset sampling ratios are shared with Table 10. For our self-collected finetuning dataset, all the tasks share equal sampling weights, Therefore, the sampling probability is only positively correlated with the training sample amount for each task. C.2 Evaluation Settings We adopt Maniunicon [33] control strategy with the same evaluation settings. For all methods, the policy predicts future action sequence of length 16 conditioned on the current RGB observation from the D435 camera, of which the first 8 steps are executed. As mentioned above, we continue to use the default DINOv2 and Wan VAE as the pre-trained visual encoder combination for our real-world experiments. C.3 Ablation Details For the Vanilla VLA, we adhere to standard training protocol where the model is supervised solely on action prediction given current observations. To ensure fair comparison, we align the training budget with the total duration of the two-stage WoG: the model is pretrained on the OXE dataset for 150k steps with global batch size of 1024, followed by fine-tuning on expert demonstrations using batch size of 512 for 30 epochs. Regarding the WoG w/o cotrain variant, the first stage setup remains identical to that of the full WoG, involving 100k steps of pretraining with batch size of 1024 under future-observation-guided action supervision. 14 In the second stage, the model continues training for an additional 50k steps with the same batch size. However, crucially, we exclude the co-training objective in this phase; the model receives supervision exclusively for action prediction, without the auxiliary supervision for predicting future conditions. Subsequently, the model undergoes fine-tuning on expert demonstrations for 30 epochs with global batch size of 512. Consistent with the second pretraining stage, supervision during this fine-tuning phase remains restricted exclusively to action prediction. C.4 Human Data Learning Settings Figure 6 Process of human manipulation process. We keep the same human manipulation data collection strategy as in [15, 49]. Human trajectories can be efficiently collected with VR devices at rate of approximately 450 trajectories per hour, substantially outpacing the teleoperated robot trajectory collection. Nevertheless, as discussed earlier, we use action annotations for only 11% of the data, while the remaining 89% are leveraged solely as unlabeled videos. We argue that this setting more faithfully reflects real-world conditions, where action-labeled human manipulation data are scarce but large-scale unlabeled videos are abundant, and is therefore essential for evaluating the scalability of WoG. Dataset Information. We use human manipulation dataset collected by ourselves with PICO 4 Ultra Enterprise as in [15], consisting of 650k trajectories with total duration of approximately 1,920 hours, among which 220-hour subset is annotated with actions. The human arm state is parameterized by the pose (translation and rotation) of the wrist joint relative to the PICO coordinate system. Additionally, the hand aperture (palm opening and closing) is mapped to the continuous/discrete state of the robot gripper. To facilitate unified action representation, we formulate actions as the state deltas between consecutive frames, expressed in the reference frame of the preceding time step. Training Strategy. We explain the specific implementation of the two methods in section 3.4 in our experiment. For the w. human v. variant: WoG is pretrained on the Open X-Embodiment (OXE) dataset for 100k training steps with global batch size of 1024. At the second stage, all 1,920 hours of unannotated human videos are used with the OXE dataset. For robot data, the model receives supervision from both condition prediction and action prediction. For human video data, it receives supervision only from condition prediction. 50k steps of the same batch size is also used in the second stage. For w. human v./a. variant: action supervision is applied to the 220-hour annotated subset in both training stages. Specifically, in the first stage, the model utilizes both the OXE dataset and the 220-hour annotated human subset to perform action prediction conditioned on future observations. In the second stage, the training set is expanded to include the full 1,920-hour corpus of human manipulation videos alongside the 15 Table 11 Dataset Sampling Ratios for the Human Data Learning. Dataset Ratio Dataset Ratio Dataset Fractal Taco Play Roboturk Toto NYU Franka Play Dataset Austin Sailor Dataset IAMLab CMU Pickup Insert CMU Stretch DobbE Jaco Play 0.117016 Kuka 0.027175 0.021427 Viola 0.018610 Stanford Hydra Dataset 0.007692 Furniture Bench Dataset 0.020175 Austin Sirius Dataset 0.008356 UTAustin Mutex 0.001429 BC-Z 0.013026 DROID 0.117045 Bridge 0.004455 Berkeley Cable Routing 0.008722 Berkeley Autolab UR5 0.040937 Austin Buds Dataset 0.022558 UCSD Kitchen Dataset 0.015995 DLR EDAN Shared Control 0.020677 Berkeley Fanuc Manipulation 0.068743 FMB Dataset 0.092715 Human Manip. Data Ratio 0.122016 0.002419 0.011192 0.001949 0.000454 0.000510 0.007155 0.019323 0.167972 OXE dataset. During this phase, action supervision is applied exclusively to the OXE data and the 220-hour annotated human subset, while the supervision for future condition prediction is applied to all data samples. The dataset sampling ratios incorporated human dataset are shown in Table 11. We also show our human manipulation video samples in Figure 6. C.5 UMI Data Learning Settings Our UMI data collection consists exclusively of egocentric observations, where actions are defined as egocentric motions relative to the headset coordinate system. Since our objective is to evaluate the performance improvement on our target tasks, we incorporate UMI data solely as data source during the final fine-tuning stage alongside expert demonstrations. Consistent with the sampling strategy described earlier, it is assigned sampling weight equivalent to that of the other expert demonstration tasks."
        },
        {
            "title": "References",
            "content": "[1] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. [2] AgiBot-World-Contributors. Agibot world colosseo: large-scale manipulation platform for scalable and intelligent embodied systems, 2025. [3] Ali Amin et al. π 0.6: vla that learns from experience. arXiv preprint arXiv:2511.14759, 2025. [4] Theodore Wilbur Anderson. Asymptotic theory for principal component analysis. The Annals of Mathematical Statistics, 34(1):122148, 1963. [5] Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with joint-embedding predictive architecture, 2023. [6] Lucas Beyer, Andreas Steiner, André Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, et al. Paligemma: versatile 3b vlm for transfer. arXiv preprint arXiv:2407.07726, 2024. [7] Hongzhe Bi, Hengkai Tan, Shenghao Xie, Zeyuan Wang, Shuhe Huang, Haitian Liu, Ruowen Zhao, Yao Feng, Chendong Xiang, Yinze Rong, Hongyan Zhao, Hanyu Liu, Zhizhong Su, Lei Ma, Hang Su, and Jun Zhu. Motus: unified latent action world model, 2025. [8] Johan Bjorck, Fernando Castañeda, et al. Gr00t n1: An open foundation model for generalist humanoid robots, 2025. [9] Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, et al. π0.5: vision-language-action model with open-world generalization. arXiv preprint arXiv:2504.16054, 2025. [10] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. π0: vision-language-action flow model for general robot control, 2024. arXiv preprint arXiv:2410.24164, 2025. [11] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. Stable video diffusion: Scaling latent video diffusion models to large datasets, 2023. [12] Qingwen Bu, Yanting Yang, Jisong Cai, Shenyuan Gao, Guanghui Ren, Maoqing Yao, Ping Luo, and Hongyang Li. Univla: Learning to act anywhere with task-centric latent actions. In Proceedings of Robotics: Science and Systems, 2025. [13] Jun Cen, Siteng Huang, Yuqian Yuan, Kehan Li, Hangjie Yuan, Chaohui Yu, Yuming Jiang, Jiayan Guo, Xin Li, Hao Luo, Fan Wang, Fan Wang, and Deli Zhao. Rynnvla-002: unified vision-language-action and world model. arXiv preprint arXiv:2511.17502, 2025. [14] Jun Cen, Chaohui Yu, Hangjie Yuan, Yuming Jiang, Siteng Huang, Jiayan Guo, Xin Li, Yibing Song, Hao Luo, Fan Wang, Deli Zhao, and Hao Chen. Worldvla: Towards autoregressive action world model. arXiv preprint arXiv:, 2025. [15] Chilam Cheang, Sijin Chen, Zhongren Cui, Yingdong Hu, Liqun Huang, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Xiao Ma, Hao Niu, Wenxuan Ou, Wanli Peng, Zeyu Ren, Haixin Shi, Jiawen Tian, Hongtao Wu, Xin Xiao, Yuyang Xiao, Jiafeng Xu, and Yichu Yang. Gr-3 technical report, 2025. [16] Tianxing Chen, Yao Mu, Zhixuan Liang, Zanxin Chen, Shijia Peng, Qiangyu Chen, Mingkun Xu, Ruizhen Hu, Hongyuan Zhang, Xuelong Li, and Ping Luo. G3flow: Generative 3d semantic flow for pose-aware and generalizable object manipulation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 17351744, June 2025. [17] Xinyi Chen, Yilun Chen, Yanwei Fu, Ning Gao, Jiaya Jia, Weiyang Jin, Hao Li, Yao Mu, Jiangmiao Pang, Yu Qiao, Yang Tian, Bin Wang, Bolun Wang, Fangjing Wang, Hanqing Wang, Tai Wang, Ziqin Wang, Xueyuan Wei, Chao Wu, Shuai Yang, Jinhui Ye, Junqiu Yu, Jia Zeng, Jingjing Zhang, Jinyu Zhang, Shi Zhang, Feng Zheng, Bowen Zhou, and Yangkun Zhu. Internvla-m1: spatially guided vision-language-action framework for generalist robot policy, 2025. 17 [18] Yi Chen, Yuying Ge, Yizhuo Li, Yixiao Ge, Mingyu Ding, Ying Shan, and Xihui Liu. Moto: Latent motion token as the bridging language for robot manipulation. 2025. [19] Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. In Proceedings of Robotics: Science and Systems, 2023. [20] Cheng Chi, Zhenjia Xu, Chuer Pan, Eric Cousineau, Benjamin Burchfiel, Siyuan Feng, Russ Tedrake, and Shuran Song. Universal manipulation interface: In-the-wild robot teaching without in-the-wild robots. In Proceedings of Robotics: Science and Systems (RSS), 2024. [21] Patrick Esser, Robin Rombach, and Björn Ommer. Taming transformers for high-resolution image synthesis, 2020. [22] Shichao Fan, Kun Wu, Zhengping Che, Xinhua Wang, Di Wu, Fei Liao, Ning Liu, Yixue Zhang, Zhen Zhao, Zhiyuan Xu, Meng Li, Qingjie Liu, Shanghang Zhang, Min Wan, and Jian Tang. Xr-1: Towards versatile vision-language-action models via learning unified vision-motion representations, 2025. [23] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. In International Conference on Machine Learning, pages 25552565, 2019. [24] Yucheng Hu, Yanjiang Guo, Pengchao Wang, Xiaoyu Chen, Yen-Jen Wang, Jianke Zhang, Koushil Sreenath, Chaochao Lu, and Jianyu Chen. Video prediction policy: generalist robot policy with predictive visual representations. In Forty-second International Conference on Machine Learning, 2025. [25] Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, and Dorsa Sadigh. Prismatic vlms: Investigating the design space of visually-conditioned language models. In International Conference on Machine Learning, 2024. [26] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. [27] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross Girshick. Segment anything, 2023. [28] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, 2023. [29] Qixiu Li, Yaobo Liang, Zeyu Wang, Lin Luo, Xi Chen, Mozheng Liao, Fangyun Wei, Yu Deng, Sicheng Xu, Yizhong Zhang, et al. Cogact: foundational vision-language-action model for synergizing cognition and action in robotic manipulation. arXiv preprint arXiv:2411.19650, 2024. [30] Xuanlin Li, Kyle Hsu, Jiayuan Gu, Karl Pertsch, Oier Mees, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, et al. Evaluating real-world robot manipulation policies in simulation. 2024. [31] Yue Liao, Pengfei Zhou, Siyuan Huang, Donglin Yang, Shengcong Chen, Yuxin Jiang, Yue Hu, Jingbin Cai, Si Liu, Jianlan Luo, Liliang Chen, Shuicheng Yan, Maoqing Yao, and Guanghui Ren. Genie envisioner: unified world foundation platform for robotic manipulation, 2025. [32] Minghui Lin, Pengxiang Ding, Shu Wang, Zifeng Zhuang, Yang Liu, Xinyang Tong, Wenxuan Song, Shangke Lyu, Siteng Huang, and Donglin Wang. Hif-vla: Hindsight, insight and foresight through motion representation for vision-language-action models. arXiv preprint arXiv:2512.09928, 2025. [33] Minghuan Liu, Zhengbang Zhu, Xiaoshen Han, Peng Hu, Haotong Lin, Xinyao Li, Jingxiao Chen, Jiafeng Xu, Yichu Yang, Yunfeng Lin, Xinghang Li, Yong Yu, Weinan Zhang, Tao Kong, and Bingyi Kang. Manipulation as in simulation: Enabling accurate geometry perception in robots, 2025. [34] Mingyu Liu, Jiuhe Shu, Hui Chen, Zeju Li, Canyu Zhao, Jiange Yang, Shenyuan Gao, Hao Chen, and Chunhua Shen. Stamo: Unsupervised learning of generalizable robot motion from compact state representation, 2025. [35] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. 2022. [36] Xiangkai Ma, Lekai Xing, Han Zhang, Wenzhong Li, and Sanglu Lu. Unifying perception and action: hybrid-modality pipeline with implicit visual chain-of-thought for robotic action generation, 2025. [37] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. Transactions on Machine Learning Research Journal (TMLR), 2024. [38] Abby ONeill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, et al. Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0. In International Conference on Robotics and Automation, 2024. [39] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023. [40] Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, and Sergey Levine. Fast: Efficient action tokenization for vision-language-action models. arXiv preprint arXiv:2501.09747, 2025. [41] Sandeep Routray, Hengkai Pan, Unnat Jain, Shikhar Bahl, and Deepak Pathak. Vipra: Video prediction for robot actions, 2025. [42] Dominik Schmidt and Minqi Jiang. Learning to act without actions. In The Twelfth International Conference on Learning Representations, 2024. [43] Yue Su, Xinyu Zhan, Hongjie Fang, Yong-Lu Li, Cewu Lu, and Lixin Yang. Motion before action: Diffusing object motion as manipulation condition. IEEE Robotics and Automation Letters, 10(7):74287435, 2025. [44] Yue Su, Xinyu Zhan, Hongjie Fang, Han Xue, Hao-Shu Fang, Yong-Lu Li, Cewu Lu, and Lixin Yang. Dense policy: Bidirectional autoregressive learning of actions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1448614495, October 2025. [45] Yue Su, Chubin Zhang, Sijin Chen, Liufan Tan, Yansong Tang, Jianan Wang, and Xihui Liu. Dspv2: Improved dense policy for effective and generalizable whole-body mobile manipulation. arXiv preprint arXiv:2509.16063, 2025. [46] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in Neural Information Processing Systems, 30, 2017. [47] Ang Wang et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [48] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer, 2025. [49] Ruoshi Wen, Guangzeng Chen, Zhongren Cui, Min Du, Yang Gou, Zhigang Han, Liqun Huang, Mingyu Lei, Yunfei Li, Zhuohang Li, Wenlei Liu, Yuxiao Liu, Xiao Ma, Hao Niu, Yutao Ouyang, Zeyu Ren, Haixin Shi, Wei Xu, Haoxiang Zhang, Jiajun Zhang, Xiao Zhang, Liwei Zheng, Weiheng Zhong, Yifei Zhou, Zhengming Zhu, and Hang Li. Gr-dexter technical report, 2026. [50] Mengda Xu, Zhenjia Xu, Yinghao Xu, Cheng Chi, Gordon Wetzstein, Manuela Veloso, and Shuran Song. Flow as the cross-domain manipulation interface. In 8th Annual Conference on Robot Learning, 2024. [51] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [52] Jiange Yang, Yansong Shi, Haoyi Zhu, Mingyu Liu, Kaijing Ma, Yating Wang, Gangshan Wu, Tong He, and Limin Wang. Como: Learning continuous latent motion from internet videos for scalable robot learning. arXiv preprint arXiv:2505.17006, 2025. [53] Seonghyeon Ye, Joel Jang, Byeongguk Jeon, Sejune Joo, Jianwei Yang, Baolin Peng, Ajay Mandlekar, Reuben Tan, Yu-Wei Chao, Bill Yuchen Lin, et al. Latent action pretraining from videos. arXiv preprint arXiv:2410.11758, 2024. [54] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023. [55] Chubin Zhang, Jianan Wang, Zifeng Gao, Yue Su, Tianru Dai, Cai Zhou, Jiwen Lu, and Yansong Tang. Clap: Contrastive latent action pretraining for learning vision-language-action models from human videos, 2026. 19 [56] Chuheng Zhang, Tim Pearce, Pushi Zhang, Kaixin Wang, Xiaoyu Chen, Wei Shen, Li Zhao, and Jiang Bian. What do latent action models actually learn?, 2025. [57] Jianke Zhang, Yanjiang Guo, Yucheng Hu, Xiaoyu Chen, Xiang Zhu, and Jianyu Chen. Up-vla: unified understanding and prediction model for embodied agent, 2025. [58] Wenyao Zhang, Hongsi Liu, Zekun Qi, Yunnan Wang, Xinqiang Yu, Jiazhao Zhang, Runpei Dong, Jiawei He, Fan Lu, He Wang, Zhizheng Zhang, Li Yi, Wenjun Zeng, and Xin Jin. Dreamvla: vision-language-action model dreamed with comprehensive world knowledge, 2025. [59] Wenyao Zhang, Bozhou Zhang, Zekun Qi, Wenjun Zeng, Xin Jin, and Li Zhang. Disentangled robot learning via separate forward and inverse dynamics pretraining. In The Fourteenth International Conference on Learning Representations, 2026. [60] Zhengshen Zhang, Lei Zhou, Chenchen Liu, Zhiyang Liu, Chengran Yuan, Sheng Guo, Ruiteng Zhao, Marcelo H. Ang Jr., and Francis EH Tay. Dexgrasp-diffusion: Diffusion-based unified functional grasp synthesis method for multi-dexterous robotic hands, 2024. [61] Zhengshen Zhang, Hao Li, Yalun Dai, Zhengbang Zhu, Lei Zhou, Chenchen Liu, Dong Wang, Francis E. H. Tay, Sijin Chen, Ziwei Liu, Yuxiao Liu, Xinghang Li, and Pan Zhou. From spatial to actions: Grounding vision-language-action model in spatial foundation priors, 2025. [62] Tony Z. Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning fine-grained bimanual manipulation with low-cost hardware. In Proceedings of Robotics: Science and Systems, 2023. [63] Ruijie Zheng, Jing Wang, Scott Reed, Johan Bjorck, Yu Fang, Fengyuan Hu, Joel Jang, Kaushil Kundalia, Zongyu Lin, Loic Magne, Avnish Narayan, You Liang Tan, Guanzhi Wang, Qi Wang, Jiannan Xiang, Yinzhen Xu, Seonghyeon Ye, Jan Kautz, Furong Huang, Yuke Zhu, and Linxi Fan. Flare: Robot learning with implicit world modeling, 2025. [64] Zhide Zhong, Haodong Yan, Junfeng Li, Xiangchen Liu, Xin Gong, Tianran Zhang, Wenxuan Song, Jiayi Chen, Xinhu Zheng, Hesheng Wang, and Haoang Li. Flowvla: Visual chain of thought-based motion reasoning for vision-language-action models, 2025. [65] Fangqi Zhu, Hongtao Wu, Song Guo, Yuxiao Liu, Chilam Cheang, and Tao Kong. Irasim: fine-grained world model for robot manipulation, 2025. [66] Dong Zhuo, Wenzhao Zheng, Jiahe Guo, Yuqi Wu, Jie Zhou, and Jiwen Lu. Streaming 4d visual geometry transformer, 2025."
        }
    ],
    "affiliations": [
        "ByteDance Seed",
        "The University of Hong Kong"
    ]
}