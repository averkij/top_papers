{
    "paper_title": "Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of Experts",
    "authors": [
        "Xiaoming Shi",
        "Shiyu Wang",
        "Yuqi Nie",
        "Dianqi Li",
        "Zhou Ye",
        "Qingsong Wen",
        "Ming Jin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Deep learning for time series forecasting has seen significant advancements over the past decades. However, despite the success of large-scale pre-training in language and vision domains, pre-trained time series models remain limited in scale and operate at a high cost, hindering the development of larger capable forecasting models in real-world applications. In response, we introduce Time-MoE, a scalable and unified architecture designed to pre-train larger, more capable forecasting foundation models while reducing inference costs. By leveraging a sparse mixture-of-experts (MoE) design, Time-MoE enhances computational efficiency by activating only a subset of networks for each prediction, reducing computational load while maintaining high model capacity. This allows Time-MoE to scale effectively without a corresponding increase in inference costs. Time-MoE comprises a family of decoder-only transformer models that operate in an auto-regressive manner and support flexible forecasting horizons with varying input context lengths. We pre-trained these models on our newly introduced large-scale data Time-300B, which spans over 9 domains and encompassing over 300 billion time points. For the first time, we scaled a time series foundation model up to 2.4 billion parameters, achieving significantly improved forecasting precision. Our results validate the applicability of scaling laws for training tokens and model size in the context of time series forecasting. Compared to dense models with the same number of activated parameters or equivalent computation budgets, our models consistently outperform them by large margin. These advancements position Time-MoE as a state-of-the-art solution for tackling real-world time series forecasting challenges with superior capability, efficiency, and flexibility."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 2 ] . [ 2 0 4 0 6 1 . 9 0 4 2 : r TIME-MOE: BILLION-SCALE TIME SERIES FOUN-"
        },
        {
            "title": "DATION MODELS WITH MIXTURE OF EXPERTS",
            "content": "Xiaoming Shi, Shiyu Wang, Yuqi Nie1, Dianqi Li, Zhou Ye, Qingsong Wen2, Ming Jin3 1Princeton University 2Squirrel Ai Learning 3Griffith University sxm728@hotmail.com, kwuking@gmail.com, ynie@princeton.edu {dianqili77, yezhou199032, qingsongedu, mingjinedu}@gmail.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Deep learning for time series forecasting has seen significant advancements over the past decades. However, despite the success of large-scale pre-training in language and vision domains, pre-trained time series models remain limited in scale and operate at high cost, hindering the development of larger capable forecasting models in real-world applications. In response, we introduce TIME-MOE, scalable and unified architecture designed to pre-train larger, more capable forecasting foundation models while reducing inference costs. By leveraging sparse mixture-of-experts (MoE) design, TIME-MOE enhances computational efficiency by activating only subset of networks for each prediction, reducing computational load while maintaining high model capacity. This allows TIME-MOE to scale effectively without corresponding increase in inference costs. TIME-MOE comprises family of decoder-only transformer models that operate in an autoregressive manner and support flexible forecasting horizons with varying input context lengths. We pre-trained these models on our newly introduced large-scale data Time-300B, which spans over 9 domains and encompassing over 300 billion time points. For the first time, we scaled time series foundation model up to 2.4 billion parameters, achieving significantly improved forecasting precision. Our results validate the applicability of scaling laws for training tokens and model size in the context of time series forecasting. Compared to dense models with the same number of activated parameters or equivalent computation budgets, our models consistently outperform them by large margin. These advancements position TIME-MOE as state-of-the-art solution for tackling real-world time series forecasting challenges with superior capability, efficiency, and flexibility. Resources: https://github.com/Time-MoE/Time-MoE"
        },
        {
            "title": "INTRODUCTION",
            "content": "Time series data is major modality in real-world dynamic systems and applications across various domains (Box et al., 2015; Zhang et al., 2024; Liang et al., 2024). Analyzing time series data is challenging due to its inherent complexity and distribution shifts, yet it is crucial for unlocking insights that enhance predictive analytics and decision-making. As key task in high demand, time series forecasting has long been studied and is vital for driving various use cases in fields such as energy, climate, education, quantitative finance, and urban computing (Jin et al., 2023; Nie et al., 2024; Mao et al., 2024). Traditionally, forecasting has been performed in task-specific, end-toend manner using either statistical or deep learning models. Despite their competitive performance, the field has not converged on building unified, general-purpose forecasting models until recently, with the emergence of few foundation models (FMs) for universal forecasting (Das et al., 2024; Woo et al., 2024; Ansari et al., 2024). Although promising, they are generally small in scale and have limited task-solving capabilities compared to domain-specific models, limiting their real-world impact when balancing forecasting precision against computational budget. Equal contribution Project lead Corresponding author 1 Figure 1: Performance overview. (Left) Comparison between TIME-MOE models and state-of-theart time series foundation models, reporting the average zero-shot performance across six benchmark datasets. (Right) Comparison of fewand zero-shot performance between TIME-MOE and dense variants, with similar effective FLOPs per time series token, across the same six benchmarks. Increasing model size and training tokens typically leads to performance improvements, as known as scaling laws, which have been extensively explored in the language and vision domains (Kaplan et al., 2020; Alabdulmohsin et al., 2022). However, such properties have not been thoroughly investigated in the time series domain. Assuming that scaling forecasting models with high-quality training data follows similar principles, several challenges remain: Dense versus sparse training. Most time series forecasting models compose of dense layers, which means each input time series tokens requires computations with all model parameters. While effective, this is computationally intensive. In contrast, sparse training with mixture-of-experts (MoE) is more flop-efficient per parameter and allows for scaling up model size with fixed inference budget while giving better performance, as showcased on the right of Figure 1. However, optimizing sparse, large-scale time series model faces another challenge of stability and convergency. Time series are highly heterogeneous (Woo et al., 2024; Dong et al., 2024), and selecting the appropriate model design and routing algorithm often involves trade-off between performance and computational efficiency. Sparse solutions for time series foundation models have yet to be explored, leaving significant gap in addressing these two challenges. While time series pre-training datasets are no longer major bottleneck, most existing works (Das et al., 2024; Woo et al., 2024; Ansari et al., 2024) have not extensively discussed their in-model data processing pipelines or mixing strategies. Answering this is particularly important, given that existing data archives are often noisy and largely imbalanced across domains. On the other hand, most time series FMs face limitations in flexibility and generalizability. Generalpurpose forecasting is fundamental capability, requiring model to handle any forecasting problems, regardless of context lengths, forecasting horizons, input variables, and other properties such as frequencies and distributions. Meanwhile, achieving strong generalizability pushes the boundaries further that existing works often fail to meet simultaneously. For instance, Timer (Liu et al., 2024b) has limited native support for arbitrary output lengths, which may lead to truncated outputs, while Moment (Goswami et al., 2024) operates with fixed input context length. Although Moirai (Woo et al., 2024) achieves universal forecasting, it depends on hardcoded heuristics in both the input and output layers. The recognition of the above challenges naturally raises pivotal question: How to scale time series foundation models to achieve universal forecasting while balancing model capability and computational overhead, mirroring the success of foundation models in other domains? Answering this question drives the design of TIME-MOE, scalable and unified architecture for pre-training larger, more capable forecasting FMs while reducing computational costs. TIME-MOE consists of family of decoder-only transformer models with mixture-of-experts architecture, operating in an auto-regressive manner to support any forecasting horizon and accommodate context lengths of up to 4096. With its sparsely activated design, TIME-MOE enhances computational efficiency by activating only subset of networks for each prediction, reducing computational load 2 while maintaining high model capacity. This allows TIME-MOE to scale effectively without significantly increasing inference costs. Our proposal is built on minimalist design, where the input time series is point-wise tokenized and encoded before being processed by sparse transformer decoder, activating only small subset of parameters. Pre-trained on large-scale time series data across 9 domains and over 300 billion time points, TIME-MOE is optimized through multi-task learning to forecast at multiple resolutions. During inference, different forecasting heads are utilized to enable forecasts across diverse scales, enabling flexible forecast horizons. For the first time, we scale time series FM up to 2.4 billion parameters, achieving substantial improvements in forecasting precision compared to existing models, as shown on the left of Figure 1. Compared to dense models with the same number of activated parameters or equivalent computational budgets, our models consistently outperform them by large margin. Our contributions lie in three aspects: 1. We present TIME-MOE, universal decoder-only time series forecasting foundation model architecture with mixture-of-experts. To the best of our knowledge, this is the first work to scale time series foundation models up to 2.4 billion parameters. TIME-MOE achieves substantial improvements in forecasting accuracy and consistently outperforms dense models with comparable computational resources, while maintaining high efficiency. 2. We introduce Time-300B, the largest open-access time series data collection, comprising over 300 billion time points spanning more than nine domains, accompanied by well-designed datacleaning pipeline. Our TIME-MOE models and Time-300B data collection are open-sourced. 3. Trained on Time-300B, TIME-MOE models outperform other time series foundation models with similar number of activated parameters across six real-world benchmarks, achieving reductions in forecasting errors by an average of 20% and 24% in zero-shot and in-distribution scenarios, respectively."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Time Series Forecasting. Deep learning models have become powerful tools for time series forecasting over the past decade, which can be broadly categorized into two types: (1) univariate models, such as DeepState (Rangapuram et al., 2018), DeepAR (Salinas et al., 2020), and N-BEATS (Oreshkin et al., 2020), which focus on modeling individual time series, and (2) multivariate models, which include both transformer-based approaches (Wen et al., 2023; Zhou et al., 2021; Nie et al., 2023; Liu et al., 2024a; Wang et al., 2024c; Chen et al., 2024) and non-transformer models (Sen et al., 2019; Jin et al., 2022; Wang et al., 2024b; Hu et al., 2024; Qi et al., 2024), designed to handle multiple time series simultaneously. While these models achieve competitive in-domain performance, many are task-specific and fall short in generalizability when applied to cross-domain data in few-shot or zero-shot scenarios. Large Time Series Models. Pre-training on large-scale sequence data has significantly advanced modality understanding in language and vision domains (Dong et al., 2019; Selva et al., 2023). Building on this progress, self-supervised learning has been extensively developed for time series (Zhang et al., 2024), employing masked reconstruction (Zerveas et al., 2021; Nie et al., 2023) or contrastive learning (Zhang et al., 2022; Yue et al., 2022; Yang et al., 2023). However, these methods are limited in both data and model scale, with many focused on in-domain learning and transfer. Recently, general pre-training of time series models on large-scale data has emerged, though still in its early stages with insufficient exploration into sparse solutions. We discuss the development more in Appendix A. Unlike these dense models, TIME-MOE introduces scalable and unified architecture for pre-training larger forecasting foundation models, which is also more capable while maintaining the same scale of activated parameters or computational budget as dense models. Sparse Deep Learning for Time Series. Deep learning models are often dense and overparameterized (Hoefler et al., 2021), leading to increased memory and computational demands during both training and inference. However, sparse networks, such as mixture-of-experts models (Jacobs et al., 1991), which dynamically route inputs to specialized expert networks, have shown comparable or even superior generalization to dense models while being more efficient (Fedus et al., 2022; Riquelme et al., 2021). In time series research, model sparsification has received less attention, as time series models have traditionally been small in scale, with simple models like DLinear (Zeng et al., 2023) and SparseTSF (Lin et al., 2024) excelling in specific tasks prior to the advent of largescale, general pre-training. The most relevant works on this topic include Pathformer (Chen et al., 3 Figure 2: The architecture of TIME-MOE, which is decoder-only model. Given an input time series of arbitrary length, 1 we first tokenize it into sequence of data points, 2 which are then encoded. These tokens are processed through -stacked backbone layers, primarily consisting of causal multi-head self-attention and 3 sparse temporal mixture-of-expert layers. During training, 4 we optimize forecasting heads at multiple resolutions. For model inference, TIME-MOE provides forecasts of flexible length by 5 dynamically scheduling these heads. 2024), MoLE (Ni et al., 2024), and IME (Ismail et al., 2023). However, none of them delve into the scalability of foundation models with sparse structures. Besides, MoLE and IME are not sparse models, as input data is passed to all heads and then combined to make predictions."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "Our proposed TIME-MOE, illustrated in Figure 2, adopts mixture-of-experts-based, decoder-only transformer architecture, comprising three key components: (1) input token embedding, (2) MoE transformer block, and (3) multi-resolution forecasting. For the first time, we scale sparselyactivated time series model to 2.4 billion parameters, achieving significantly better zero-shot performance with the same computation. This marks major step forward in developing large time series models for universal forecasting. Problem Statement. We address the problem of predicting future values in time series: given sequence of historical observations X1:T = (x1, x2, . . . , xT ) RT spanning time steps, our objective is to forecast the next time steps, i.e., ˆXT +1:T +H = fθ (X1:T ) RH . Here, fθ represents time series model, where is the context length and is the forecasting horizon. Notably, both and can be flexible during TIME-MOE inference, distinguishing it from taskspecific models with fixed horizons. Additionally, channel independence (Nie et al., 2023) is adopted to transform multivariate input into univariate series, allowing TIME-MOE to handle any-variate forecasting problems in real-world applications."
        },
        {
            "title": "3.1 TIME-MOE OVERVIEW",
            "content": "Input Token Embedding. We utilize point-wise tokenization for time series embedding to ensure the completeness of temporal information. This enhances our models flexibility and broad applicability in handling variable-length sequences. Then, we employ SwiGLU (Shazeer, 2020) to embed each time series point: h0 = SwiGLU(xt) = Swish (W xt) (V xt) , (1) 4 where RD1 and RD1 are learnable parameters, and denotes the hidden dimension. MoE Transformer Block. Our approach builds upon decoder-only transformer (Vaswani, 2017) and integrates recent advancements from large language models (Bai et al., 2023; Touvron et al., 2023). We employ RMSNorm (Zhang & Sennrich, 2019) to normalize the input of each transformer sub-layer, thereby enhancing training stability. Instead of using absolute positional encoding, we adopt rotary positional embeddings (Su et al., 2024), which provide greater flexibility in sequence length and improved extrapolation capabilities. In line with (Chowdhery et al., 2023), we remove biases from most layers but retain them in the QKV layer of self-attention to improve extrapolation. To introduce sparsity, we replace feed-forward network (FFN) with mixture-of-experts layer, incorporating shared pool of experts that are sparsely activated. = SA (cid:0)RMSNorm (cid:0)hl1 ul = RMSNorm (cid:0)ul ul = Mixture (cid:0)ul hl (cid:1) , (cid:1) + ul t. (cid:1)(cid:1) + hl , (2) (3) (4) Here, SA denotes self-attention with causal mask, and Mixture refers to the mixture-of-experts layer. In practice, Mixture comprises several expert networks, each mirroring the architecture of standard FFN. An individual time series point can be routed to either single expert (Fedus et al., 2022) or multiple experts (Lepikhin et al., 2020). One expert is designated as shared expert to capture and consolidate common knowledge across different contexts. Mixture (cid:0)ul (cid:1) = gN +1,t FFNN +1 (cid:0)ul (cid:1) + (cid:88) (cid:0)gi,t FFNi (cid:0)ul (cid:1)(cid:1) , (5) gi,t = i=1 si,t Topk({sj,t1 }, K), otherwise, +1ul (cid:1) , iul (8) R1D denotes the trainable parameters, and and respectively denote the numbers (cid:26)si,t, 0, gN +1,t = Sigmoid (cid:0)Wl (cid:0)Wl si,t = Softmaxi (cid:1) , (7) (6) where Wl of non-shared experts and activated non-shared experts per MoE layer. Multi-resolution Forecasting. We introduce novel multi-resolution forecasting head, which allows for forecasting at multiple scales simultaneously, in contrast to existing foundation models that are limited to single fixed scale. This capability enhances TIME-MOE flexibility by enabling forecasting across various horizons. The model employs multiple output projections from single-layer FFNs, each designed for different prediction horizons. During training, TIME-MOE aggregates forecasting errors from different horizons to compute composite loss (Section 3.2.2), thereby improving the model generalization. By incorporating simple greedy scheduling algorithm (see Appendix B), TIME-MOE efficiently handles predictions across arbitrary horizons. This design also boosts prediction robustness through multi-resolution ensemble learning during inference."
        },
        {
            "title": "3.2.1 TI M E-300B DATASET",
            "content": "Training time series foundation models require extensive, high-quality data. However, wellprocessed large-scale datasets are still relatively scarce. Recent advancements have facilitated the collection of numerous time series datasets from various sources (Godahewa et al., 2021; Ansari et al., 2024; Woo et al., 2024; Liu et al., 2024b). Nonetheless, data quality still remains challenge, with prevalent issues such as missing values and invalid observations (Wang et al., 2024a) that can impair model performance and destabilize training. To mitigate these issues, we developed streamlined data-cleaning pipeline (Appendix C) to filter and refine raw data, and constructed the largest open-access, high-quality time series data collection named Time-300B for foundation model pre-training. Time-300B is composed of diverse range of publicly available datasets, spanning multiple domains such as energy, retail, healthcare, weather, finance, transportation, and web, as well as portion of synthetic data to enhance the data quantity and diversity. Time-300B covers wide spectrum of sampling frequencies from seconds to yearly intervals, and has over 300 billion time points after being processed by our data-cleaning pipeline, as summarized in Table 1. 5 Table 1: Key statistics of the pre-training dataset Time-300B from various domains. Energy Finance Healthcare Nature Sales Synthetic Transport Web Other Total # Seqs. # Obs. % 2,875,335 15.981 5.17 % 1,715 1,752 413.696 471.040 0.0001% 0.0001% 31,621,183 279.724 90.50 % 110,210 26.382 0.008 % 11,968,625 9.222 2.98% 622,414 2.130 0.69 % 972,158 48,220,929 40,265 1.804 20.32 309.09 0.58 % 0.006 % 100% Table 2: high-level summary of TIME-MOE model configurations. TIME-MOEbase 12 TIME-MOElarge 12 TIME-MOEultra 36 Layers Heads Experts dmodel 384 768 1024 12 12 16 2 2 2 8 8 8 dff 1536 3072 dexpert Activated Params Total Params 192"
        },
        {
            "title": "3.2.2 LOSS FUNCTION",
            "content": "Pre-training time series foundation models in large scale presents significant challenges in training stability due to the massive datasets and the vast number of parameters involved. To address this, we use the Huber loss (Huber, 1992; Wen et al., 2019), which provides greater robustness to outliers and improves training stability: Lar (xt, ˆxt) = 2 δ(cid:1) , otherwise, where δ is hyperparameter that balances the L1 and L2 loss components. (cid:40) 1 2 (xt ˆxt)2 , δ (cid:0)xt ˆxt 1 if xt ˆxt δ, (9) When training the model with MoE architecture, focusing solely on optimizing prediction error often leads to load imbalance issues among the experts. common problem is routing collapse (Shazeer et al., 2017), where the model predominantly selects only few experts, limiting training opportunities for others. To mitigate this, following the approaches of (Dai et al., 2024; Fedus et al., 2022), we achieve expert-level balancing with an auxiliary loss to reduce routing collapse: Laux = (cid:88) i=1 firi, fi ="
        },
        {
            "title": "1\nKT",
            "content": "T (cid:88) t=1 (Time point selects Expert i) , ri ="
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 si,t, (10) where fi represents the fraction of tokens assigned to expert i, and ri denotes the proportion of router probability allocated to expert i. is the indicator function. Finally, we combine the auto-regressive losses across all multi-resolution projections with the auxiliary balance loss to form the final loss: ="
        },
        {
            "title": "1\nP",
            "content": "P (cid:88) j=1 (cid:16) Xt+1:t+pj , ˆXt+1:t+pj (cid:17)"
        },
        {
            "title": "Lar",
            "content": "+ αLaux, (11) where is the number of multi-resolution projections and pj is the horizon of the j-th projection."
        },
        {
            "title": "3.2.3 MODEL CONFIGURATIONS AND TRAINING DETAILS",
            "content": "Informed by the scaling laws demonstrated by Llama (Dubey et al., 2024; Touvron et al., 2023), which show that 7or 8-billion parameter model continues to improve performance even after training on over one trillion tokens, we chose to scale TIME-MOE up to 2.4 billion parameters with around 1 billion of them activated. This model, TIME-MOEultra, supports inference on consumergrade GPUs with less than 8GB of VRAM. We have also developed two smaller models: TIMEMOEbase, with 50 million activated parameters, and TIME-MOElarge, with 200 million activated parameters, both specifically designed for fast inference on CPU architectures. These streamlined models are strategically developed to ensure broader accessibility and applicability in resourceconstrained environments. The detailed model configurations are in Table 2. Each model undergoes training for 100, 000 steps with batch size of 1024, where the maximum sequence length is capped at 4096. This setup results in the consumption of 4 million time points per iteration. We choose {1, 8, 32, 64} as different forecast horizons in the output projection and set the factor of the auxiliary loss α to 0.02. For optimization, we employ the AdamW optimizer, configured with the following hyperparameters: lr = 1e-3, weight decay = 0.1, β1 = 0.9, β2 = 0.95. learning rate scheduler with linear warmup over the initial 10, 000 steps followed by cosine annealing is also utilized. Training is conducted on 128 NVIDIA A100-80G GPUs using BF16 precision."
        },
        {
            "title": "4 MAIN RESULTS",
            "content": "TIME-MOE consistently outperforms state-of-the-art forecasting models by large margins across six well-established benchmarks and settings (Appendix B). To ensure fair comparison, we adhered to the experimental configurations from (Woo et al., 2024) for out-of-distribution forecasting and (Wu et al., 2023a) for in-distribution forecasting with unified evaluation pipeline we developed. Specifically, we evaluate TIME-MOE against 16 different baselines, representing state-of-the-art models in long-term forecasting. They are categorized into two groups: 1. zero-shot forecasting evaluation group, includes pre-trained foundation models such as Moirai (2024), TimesFM (2024), Moment (2024), and Chronos (2024); 2. in-distribution (full-shot) forecasting evaluation group, consists of modern time series models such as iTransformer (2024a), TimeMixer (2024b), TimesNet (2023a), PatchTST (2023), Crossformer (2023), TiDE (2023), DLinear (2023), and FEDformer (2022b)."
        },
        {
            "title": "4.1 ZERO-SHOT FORECASTING",
            "content": "Table 3: Full results of zero-shot forecasting experiments. lower MSE or MAE indicates better prediction. TimesFM, due to its use of Weather datasets in pretraining, is not evaluated on these two datasets and is denoted by dash (). Red: the best, Blue: the 2nd best. Models Metrics TIME-MOE (Ours) Zero-shot Time Series Models TIME-MOEbase TIME-MOElarge TIME-MOEultra Moiraismall Moiraibase Moirailarge TimesFM Moment Chronossmall Chronosbase Chronoslarge MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE ETTh1 ETTh2 ETTm ETTm2 Weather Global Temp 96 0.357 192 0.384 336 0.411 720 0.449 AVG 0.400 96 0.305 192 0.351 336 0.391 720 0.419 AVG 0.366 96 0.338 192 0.353 336 0.381 720 0.504 AVG 0. 96 0.201 192 0.258 336 0.324 720 0.488 AVG 0.317 96 0.160 192 0.210 336 0.274 720 0.418 AVG 0.265 96 0.211 192 0.257 336 0.281 720 0.354 AVG 0.275 Average 1st Count 0.336 0.381 0.404 0.434 0.477 0. 0.359 0.386 0.418 0.454 0.404 0.368 0.388 0.413 0.493 0.415 0.291 0.334 0.373 0.464 0.365 0.214 0.260 0.309 0.405 0.297 0.343 0.386 0.405 0.465 0.400 0. 0.350 0.388 0.411 0.427 0.394 0.302 0.364 0.417 0.537 0.405 0.309 0.346 0.373 0.475 0.376 0.197 0.250 0.337 0.480 0.316 0.159 0.215 0.291 0.415 0.270 0.210 0.254 0.267 0.289 0. 0.336 0.382 0.412 0.430 0.455 0.419 0.354 0.385 0.425 0.496 0.415 0.357 0.381 0.408 0.477 0.405 0.286 0.322 0.375 0.461 0.361 0.213 0.266 0.322 0.400 0. 0.342 0.385 0.395 0.420 0.385 0.380 0.349 0.395 0.447 0.457 0.412 0.292 0.347 0.406 0.439 0.371 0.281 0.305 0.369 0.469 0.356 0.198 0.235 0.293 0.427 0. 0.157 0.208 0.255 0.405 0.256 0.214 0.246 0.266 0.288 0.253 0.322 0.379 0.413 0.453 0.462 0.426 0.352 0.379 0.419 0.447 0.399 0.341 0.358 0.395 0.472 0. 0.288 0.312 0.348 0.428 0.344 0.211 0.256 0.290 0.397 0.288 0.345 0.379 0.398 0.421 0.385 0.372 0.401 0.402 0.376 0.392 0.381 0.388 0.414 0.404 0.688 0.557 0.466 0.409 0.440 0.393 0.441 0.390 0.435 0.421 0.412 0.413 0.434 0.415 0.465 0.434 0.688 0.560 0.530 0.450 0.492 0.426 0.502 0.424 0.438 0.434 0.433 0.428 0.495 0.445 0.503 0.456 0.675 0.563 0.570 0.486 0.550 0.462 0.576 0.467 0.439 0.454 0.447 0.444 0.611 0.510 0.511 0.481 0.683 0.585 0.615 0.543 0.882 0.591 0.835 0.583 0.428 0.427 0.417 0.419 0.480 0.439 0.473 0.443 0.683 0.566 0.545 0.472 0.591 0.468 0.588 0.466 0.297 0.336 0.294 0.330 0.296 0.330 0.315 0.349 0.342 0.396 0.307 0.356 0.308 0.343 0.320 0.345 0.368 0.381 0.365 0.375 0.361 0.371 0.388 0.395 0.354 0.402 0.376 0.401 0.384 0.392 0.406 0.399 0.370 0.393 0.376 0.390 0.390 0.390 0.422 0.427 0.356 0.407 0.408 0.431 0.429 0.430 0.492 0.453 0.411 0.426 0.416 0.433 0.423 0.418 0.443 0.454 0.395 0.434 0.604 0.533 0.501 0.477 0.603 0.511 0.361 0.384 0.362 0.382 0.367 0.377 0.392 0.406 0.361 0.409 0.424 0.430 0.405 0.410 0.455 0. 0.418 0.392 0.363 0.356 0.380 0.361 0.361 0.370 0.654 0.527 0.511 0.423 0.454 0.408 0.457 0.403 0.431 0.405 0.388 0.375 0.412 0.383 0.414 0.405 0.662 0.532 0.618 0.485 0.567 0.477 0.530 0.450 0.433 0.412 0.416 0.392 0.436 0.400 0.445 0.429 0.672 0.537 0.683 0.524 0.662 0.525 0.577 0.481 0.462 0.432 0.460 0.418 0.462 0.420 0.512 0.471 0.692 0.551 0.748 0.566 0.900 0.591 0.660 0.526 0.436 0.410 0.406 0.385 0.422 0.391 0.433 0.418 0.670 0.536 0.640 0.499 0.645 0.500 0.555 0.465 0.214 0.288 0.205 0.273 0.211 0.274 0.202 0.270 0.260 0.335 0.209 0.291 0.199 0.274 0.197 0.271 0.284 0.332 0.275 0.316 0.281 0.318 0.289 0.321 0.289 0.350 0.280 0.341 0.261 0.322 0.254 0.314 0.331 0.362 0.329 0.350 0.341 0.355 0.360 0.366 0.324 0.369 0.354 0.390 0.326 0.366 0.313 0.353 0.402 0.408 0.437 0.411 0.485 0.428 0.462 0.430 0.394 0.409 0.553 0.499 0.455 0.439 0.416 0.415 0.307 0.347 0.311 0.337 0.329 0.343 0.328 0.346 0.316 0.365 0.349 0.380 0.310 0.350 0.295 0.338 0.198 0.222 0.220 0.217 0.199 0.211 0.247 0.265 0.271 0.259 0.246 0.251 0.283 0.303 0.286 0.297 0.274 0.291 0.373 0.354 0.373 0.354 0.337 0.340 0.275 0.286 0.287 0.281 0.264 0.273 - - - - - - - - - - 0.243 0.255 0.211 0.243 0.203 0.238 0.194 0.235 0.278 0.329 0.263 0.294 0.256 0.290 0.249 0.285 0.306 0.346 0.321 0.339 0.314 0.336 0.302 0.327 0.350 0.374 0.404 0.397 0.397 0.396 0.372 0.378 0.294 0.326 0.300 0.318 0.292 0.315 0.279 0. 0.227 0.354 0.224 0.351 0.224 0.351 0.255 0.375 0.363 0.472 0.234 0.361 0.230 0.355 0.228 0.354 0.269 0.396 0.266 0.394 0.267 0.395 0.313 0.423 0.387 0.489 0.276 0.400 0.273 0.395 0.276 0.398 0.292 0.419 0.296 0.420 0.291 0.417 0.362 0.460 0.430 0.517 0.314 0.431 0.324 0.434 0.327 0.437 0.351 0.437 0.403 0.498 0.387 0.488 0.486 0.545 0.582 0.617 0.418 0.504 0.505 0.542 0.472 0.535 0.285 0.409 0.297 0.416 0.292 0.413 0.354 0.451 0.440 0.524 0.311 0.424 0.333 0.431 0.326 0.431 0.349 0.377 0.347 0.370 0.359 0.373 0.396 0.413 0.461 0.454 0.428 0.420 0.429 0.412 0.416 0.405 3 10 28 11 10 1 4 0 1 Setup. Time series foundation models have recently demonstrated impressive zero-shot learning capabilities (Liang et al., 2024). In this section, we conducted experiments on the six well-known long-term forecasting benchmarks for which datasets were not included in the pre-training corpora. We use four different prediction horizons, which are {96, 192, 336, 720}, with the corresponding input time series lengths {512, 1024, 2048, 3072}. The evaluation metrics adopt mean square error (MSE) and mean absolute error (MAE). Results. Detailed results of zero-shot forecasting are in Table 3. TIME-MOE achieves consistent state-of-the-art performances, improving large margin as MSE reduction in average exceeding 20% over the other most competitive baselines. Importantly, as the model size scales (e.g., base ultra), it continuously exhibits enhanced performance across all datasets, affirming the efficacy of scaling laws within our time series foundation models. Furthermore, in comparisons with robust baselines that have similar number of activated parameters, TIME-MOE demonstrates significantly superior performance. The largest models among the state-of-the-art baselines are Chronoslarge, Moment and Moirailarge. Compared to those models, TIME-MOE achieved average MSE reductions of 23%, 30% and 11% respectively. 7 Table 4: Full results of in-domain forecasting experiments. lower MSE or MAE indicates better prediction. Full-shot results besides Global Temp are obtained from (Liu et al., 2024a). Red: the best, Blue: the 2nd best. Models Metrics TIME-MOE (Ours) Full-shot Time Series Models TIME-MOEbase TIME-MOElarge TIME-MOEultra iTransformer TimeMixer TimesNet PatchTST Crossformer TiDE DLinear FEDformer MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE ETTh1 ETTh ETTm1 ETTm2 Weather Global Temp 96 0.345 192 0.372 336 0.389 720 0.410 AVG 0.379 96 0.276 192 0.331 336 0.373 720 0.404 AVG 0. 96 0.286 192 0.307 336 0.354 720 0.433 AVG 0.345 96 0.172 192 0.228 336 0.281 720 0.403 AVG 0.271 96 0.151 192 0.195 336 0.247 720 0.352 AVG 0.236 96 0.192 192 0.238 336 0.259 720 0.345 AVG 0.258 Average 1st Count 0. 0.373 0.396 0.412 0.443 0.406 0.340 0.371 0.402 0.431 0.386 0.334 0.358 0.390 0.445 0.381 0.265 0.306 0.345 0.424 0.335 0.203 0.246 0.288 0.366 0.275 0.328 0.375 0.397 0.465 0. 0.362 0.335 0.374 0.390 0.402 0.375 0.278 0.345 0.384 0.437 0.361 0.264 0.295 0.323 0.409 0.322 0.169 0.223 0.293 0.451 0.284 0.149 0.192 0.245 0.352 0. 0.192 0.236 0.256 0.322 0.251 0.304 0.371 0.400 0.412 0.433 0.404 0.335 0.373 0.402 0.437 0.386 0.325 0.350 0.376 0.435 0.371 0.259 0.295 0.341 0.433 0. 0.201 0.244 0.285 0.365 0.273 0.329 0.375 0.397 0.451 0.388 0.359 0.323 0.359 0.388 0.425 0.373 0.274 0.330 0.362 0.370 0.334 0.256 0.281 0.326 0.454 0. 0.183 0.223 0.278 0.425 0.277 0.154 0.202 0.252 0.392 0.250 0.189 0.234 0.253 0.292 0.242 0.301 0.365 0.391 0.418 0.450 0.406 0.338 0.370 0.396 0.417 0. 0.323 0.343 0.374 0.452 0.373 0.273 0.301 0.339 0.424 0.334 0.208 0.251 0.287 0.376 0.280 0.322 0.376 0.399 0.426 0.380 0.358 0.386 0.405 0.375 0.400 0.384 0.402 0.414 0.419 0.423 0.448 0.479 0.464 0.386 0.400 0.376 0.419 0.441 0.436 0.436 0.429 0.421 0.429 0.460 0.445 0.471 0.474 0.525 0.492 0.437 0.432 0.420 0.448 0.487 0.458 0.484 0.458 0.491 0.469 0.501 0.466 0.570 0.546 0.565 0.515 0.481 0.459 0.459 0.465 0.503 0.491 0.498 0.482 0.521 0.500 0.500 0.488 0.653 0.621 0.594 0.558 0.519 0.516 0.506 0.507 0.454 0.447 0.448 0.442 0.454 0.450 0.468 0.454 0.529 0.522 0.540 0.507 0.455 0.451 0.440 0. 0.297 0.349 0.289 0.341 0.340 0.374 0.302 0.348 0.745 0.584 0.400 0.440 0.333 0.387 0.358 0.397 0.380 0.400 0.372 0.392 0.402 0.414 0.388 0.400 0.877 0.656 0.528 0.509 0.477 0.476 0.429 0.439 0.428 0.432 0.386 0.414 0.452 0.541 0.426 0.433 1.043 0.731 0.643 0.571 0.594 0.541 0.496 0.487 0.427 0.445 0.412 0.434 0.462 0.657 0.431 0.446 1.104 0.763 0.874 0.679 0.831 0.657 0.463 0.474 0.383 0.406 0.364 0.395 0.414 0.496 0.386 0.406 0.942 0.683 0.611 0.549 0.558 0.515 0.436 0.449 0.334 0.368 0.320 0.357 0.338 0.375 0.329 0.367 0.404 0.426 0.364 0.387 0.345 0.372 0.379 0.419 0.377 0.391 0.361 0.381 0.374 0.387 0.367 0.385 0.450 0.451 0.398 0.404 0.380 0.389 0.426 0.441 0.426 0.420 0.390 0.404 0.410 0.411 0.399 0.410 0.532 0.515 0.428 0.425 0.413 0.413 0.445 0.459 0.491 0.459 0.454 0.441 0.478 0.450 0.454 0.439 0.666 0.589 0.487 0.461 0.474 0.453 0.543 0.490 0.407 0.409 0.381 0.395 0.400 0.405 0.387 0.400 0.513 0.495 0.419 0.419 0.403 0.406 0.448 0.452 0.180 0.264 0.175 0.258 0.187 0.267 0.175 0.259 0.287 0.366 0.207 0.305 0.193 0.292 0.203 0.287 0.250 0.309 0.237 0.299 0.249 0.309 0.241 0.302 0.414 0.492 0.290 0.364 0.284 0.362 0.269 0.328 0.311 0.348 0.298 0.340 0.321 0.351 0.305 0.343 0.597 0.542 0.377 0.422 0.369 0.427 0.325 0.366 0.412 0.407 0.391 0.396 0.408 0.403 0.402 0.400 1.730 1.042 0.558 0.524 0.554 0.522 0.421 0.415 0.288 0.332 0.275 0.323 0.291 0.332 0.280 0.326 0.757 0.610 0.358 0.403 0.350 0.400 0.304 0.349 0.174 0.214 0.163 0.209 0.172 0.220 0.177 0.218 0.158 0.230 0.202 0.261 0.196 0.255 0.217 0.296 0.221 0.254 0.208 0.250 0.219 0.261 0.225 0.259 0.206 0.277 0.242 0.298 0.237 0.296 0.276 0.336 0.278 0.296 0.251 0.287 0.280 0.306 0.278 0.297 0.272 0.335 0.287 0.335 0.283 0.335 0.339 0.380 0.358 0.349 0.339 0.341 0.365 0.359 0.354 0.348 0.398 0.418 0.351 0.386 0.345 0.381 0.403 0.428 0.257 0.278 0.240 0.271 0.259 0.286 0.258 0.280 0.258 0.315 0.270 0.320 0.265 0.316 0.308 0.360 0.223 0.351 0.215 0.346 0.250 0.381 0.219 0.349 0.272 0.406 0.223 0.352 0.221 0.354 0.261 0.392 0.282 0.404 0.266 0.393 0.298 0.418 0.269 0.395 0.305 0.435 0.278 0.401 0.257 0.388 0.299 0.423 0.313 0.431 0.313 0.430 0.315 0.434 0.319 0.435 0.352 0.468 0.330 0.440 0.294 0.418 0.341 0.454 0.393 0.488 0.468 0.536 0.407 0.497 0.452 0.526 0.508 0.562 0.485 0.544 0.380 0.479 0.359 0.469 0.303 0.419 0.316 0.426 0.318 0.433 0.315 0.426 0.359 0.468 0.329 0.434 0.288 0.410 0.315 0.435 0.349 0.382 0.337 0.375 0.356 0.400 0.349 0.382 0.560 0.516 0.421 0.439 0.387 0.416 0.375 0. 4 21 33 0 7 0 0 0 0 0 4. IN-DISTRIBUTION FORECASTING Setup. We fine-tune the pre-trained TIME-MOE models on the train split of the above-mentioned six benchmarks and set the number of finetuning epochs to only one. Results. The full results are in Table 4. TIME-MOE exhibits remarkable capabilities, comprehensively surpassing advanced deep time series models from recent years, achieving MSE reduction of 24% in average. Fine-tuning on downstream data with only one epoch significantly improves predictive performance, showcasing the remarkable potential of large time series models built on the MoE architecture. Similar to zero-shot forecasting, as the model size increases, the scaling law continues to be effective, leading to continuous improvements in the performance of the TIME-MOE."
        },
        {
            "title": "4.3 ABLATION STUDY",
            "content": "Table 5: Ablation studies. (Left) Average MSE for horizon-96 forecasting across six benchmarks, evaluated with different model components. (Right) Analysis of various multi-resolution forecasting configurations. Further details in Appendix D.1. Average MSE Average MSE Inference Speed TIME-MOEbase w/o Huber loss w/o multi-resolution layer w/o mixture-of-experts w/o auxiliary loss 0.262 0.267 0.269 0.272 0. TIME-MOEbase TIME-MOEbase w/ {1,8,32} TIME-MOEbase w/ {1,8} TIME-MOEbase w/ {1} 0.262 0.273 0.320 1.382 0.095 s/iter 0.130 s/iter 0.411 s/iter 2.834 s/iter To validate our designs in TIME-MOE, we conducted detailed ablation studies on key architectural components and loss functions across all experimental benchmarks, as shown in Table 5. Model Architecture. Replacing the MoE layers with standard FFNs (w/o mixture-of-experts) led to an average performance drop from 0.262 to 0.272, highlighting the performance boost provided by the sparse architecture. detailed comparison of dense and sparse models is presented in Section 4.4. We retained only the horizon-32 output layer by eliminating the other multi-resolution output layers from the TIME-MOEbase, excluding the multi-task optimization (w/o multi-resolution layer). Consequently, we observed that the performance of this modified model was slightly inferior compared to that of the TIME-MOEbase. Additionally, as shown in the right side of Table 5, our Figure 3: Scalability analysis. (Left) Comparison of dense and sparse models in terms of training and inference costs. (Right) Average MSE for 96-horizon forecasting across six benchmarks, comparing TIME-MOE and dense models, both trained from scratch with varying data sizes. default selection of four multi-resolution output projections with receptive horizons of {1, 8, 32, 64} results in optimal predictive performance and inference speed. As we reduce the number of multiresolution output projections, performance consistently declines, and inference speed significantly increases. This demonstrates the rationality of our multi-resolution output projection design. Training Loss. Models trained with Huber loss outperformed those using MSE loss (w/o Huber loss), due to Huber losss superior robustness in handling outlier time points. We also removed the auxiliary loss from the objective function, retaining only the auto-regressive loss (w/o auxiliary loss) while still using the MoE architecture. This adjustment caused the expert layers to collapse into smaller FFN during training, as the activation score of the most effective expert became disproportionately stronger without the load balance loss. Consequently, the models performance was significantly worse than the TIME-MOEbase."
        },
        {
            "title": "4.4 SCALABILITY ANALYSIS",
            "content": "Dense versus Sparse Models. To assess the performance and efficiency benefits of sparse architectures in time series forecasting, we replaced the MoE layer with dense layer containing an equivalent number of parameters as the activated parameters in the MoE layer. Using identical training setup and data, we trained three dense models corresponding to the sizes of the three TIMEMOE models. zero-shot performance comparison between the dense and sparse models is shown in Figure 3. Our approach reduced training costs by an average of 78% and inference costs by 39% compared to dense variants. This clearly demonstrates the advantages of TIME-MOE, particularly in maintaining exceptional performance while significantly reducing costs. Model and Data Scaling. We save model checkpoints at intervals of every 20 billion time points during training, allowing us to plot performance traces for models of different sizes trained on various data scales. The right side of Figure 3 shows that models trained on larger datasets consistently outperform those trained on smaller datasets, regardless of model size. Our empirical results confirm that as both data volume and model parameters scale, sparse models demonstrate continuous and substantial improvements in performance, as well as achieve better forecasting accuracy compared to the dense counterparts under the same scales. Training Precision. We trained new model, TIME-MOEbase (FP32), using identical configurations but with float32 precision instead of bfloat16. As shown in Table 6, the forecasting performance of both models is comparable. However, the bfloat16 model achieves 12% improvement in training speed and reduces memory consumption by 20% compared to the float32 model. Moreover, the bfloat16 model can seamlessly integrate with flash-attention (Dao, 2024), further boosting training and inference speed by 23% and 19% respectively. 9 Table 6: Comparison of BF16 and FP32 in terms of training and inference efficiency. Further details are provided in Table 13 of Appendix D.2. FA denotes flash-attention. Average MSE Training Speed Inference Speed Training Memory Inference Memory TIME-MOEbase TIME-MOEbase w/o FA TIME-MOEbase w/ FP32 0.262 0.262 0.261 0.84 s/iter 1.09 s/iter 1.24 s/iter 0.095 s/iter 0.118 s/iter 0.133 s/iter 1.77 GB 1.77 GB 2.21 GB 226.70 MB 226.70 MB 453.41 MB ."
        },
        {
            "title": "4.5 SPARSIFICATION ANALYSIS",
            "content": "Activation Visualization. As shown in Figure 4, TIME-MOE dynamically activates different experts across various datasets, with each expert specializing in learning distinct knowledge. This leads to diverse activation patterns across datasets from different domains, showcasing TIME-MOEs strong generalization capabilities. The heterogeneous activations indicate that the model adapts its learned representations to the specific characteristics of each dataset, contributing to its great transferability and generalization as large-scale time series foundation model. Figure 4: Gating scores for experts across different layers in the six benchmarks. Table 7: Performance and inference speed across different topk setups. Average MSE for horizon-96 forecasting evaluated across six benchmarks. Lower values of inference speed (s/iter) indicate better performance. Number of Experts. We performed sensitivity analysis on the number of experts, represented as topk, within the TIME-MOE architecture, as shown in Table 7. As increases, performance shows only marginal changes, with minimal improvements in average MSE. However, inference time increases noticeably as more experts are utilized. This indicates that increasing sparsity within the MoE architecture does not compromise performance but significantly enhances computational efficiency. This balance is critical for scaling time series foundation models, where optimizing performance and computational cost is essential. Sparse MoE architectures inherently offer advantages in these areas. TIME-MOEbase Average MSE Inference Speed w/ {Top1} 0.264 w/ {Top2} 0.262 w/ {Top4} 0.262 w/ {Top6} 0.265 w/ {Top8} 0.269 0.082 s/iter 0.095 s/iter 0.109 s/iter 0.120 s/iter 0.129 s/iter"
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this paper, we introduced TIME-MOE, scalable and unified architecture for time series foundation models that leverages sparse design with mixture-of-experts to enhance computational efficiency without compromising model capacity. Pre-trained on our newly introduced large-scale time series dataset, Time-300B, TIME-MOE was scaled to 2.4 billion parameters, with 1.1 billion activated, demonstrating significant improvements in forecasting accuracy. Our results validate the scaling properties in time series forecasting, showing that TIME-MOE consistently outperforms dense models with equivalent computational budgets across multiple benchmarks. With its ability to perform universal forecasting and superior performance in both zero-shot and fine-tuned scenarios, TIME-MOE establishes itself as state-of-the-art solution for real-world forecasting challenges. This work paves the way for future advancements in scaling and enhancing the efficiency of time series foundation models."
        },
        {
            "title": "REFERENCES",
            "content": "Ibrahim Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai. Revisiting neural scaling laws in language and vision. Advances in Neural Information Processing Systems, 35:2230022312, 2022. Alexander Alexandrov, Konstantinos Benidis, Michael Bohlke-Schneider, Valentin Flunkert, Jan Gasthaus, Tim Januschowski, Danielle C. Maddix, Syama Rangapuram, David Salinas, Jasper Schulz, Lorenzo Stella, Ali Caner A¼rkmen, and Yuyang Wang. Gluonts: Probabilistic and neural time series modeling in python. Journal of Machine Learning Research, 21(116):16, 2020. Abdul Fatir Ansari, Lorenzo Stella, Caner Turkmen, Xiyuan Zhang, Pedro Mercado, Huibin Shen, Oleksandr Shchur, Syama Sundar Rangapuram, Sebastian Pineda Arango, Shubham Kapoor, et al. Chronos: Learning the language of time series. arXiv preprint arXiv:2403.07815, 2024. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Christoph Bergmeir, Quang Bui, Frits de Nijs, and Peter Stuckey. Residential power and battery data, August 2023. URL https://doi.org/10.5281/zenodo.8219786. George EP Box, Gwilym Jenkins, Gregory Reinsel, and Greta Ljung. Time series analysis: forecasting and control. John Wiley & Sons, 2015. CDC. Flu portal dashboard, 2017. URL https://gis.cdc.gov/grasp/fluview/ fluportaldashboard.html. Peng Chen, Yingying Zhang, Yunyao Cheng, Yang Shu, Yihang Wang, Qingsong Wen, Bin Yang, and Chenjuan Guo. Pathformer: Multi-scale transformers with adaptive pathways for time series forecasting. In International Conference on Learning Representations, 2024. Song Chen. Beijing Multi-Site Air-Quality Data. UCI Machine Learning Repository, 2019. DOI: https://doi.org/10.24432/C5RK5G. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240): 1113, 2023. Together Computer. Redpajama: an open dataset for training large language models, 2023. URL https://github.com/togethercomputer/RedPajama-Data. Damai Dai, Chengqi Deng, Chenggang Zhao, RX Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Wu, et al. Deepseekmoe: Towards ultimate expert specialization in mixtureof-experts language models. arXiv preprint arXiv:2401.06066, 2024. Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR), 2024. Abhimanyu Das, Weihao Kong, Andrew Leach, Shaan Mathur, Rajat Sen, and Rose Yu. Longterm forecasting with tide: Time-series dense encoder. Transactions on Machine Learning Research, 2023. Abhimanyu Das, Weihao Kong, Rajat Sen, and Yichen Zhou. decoder-only foundation model for time-series forecasting. In Forty-first International Conference on Machine Learning, 2024. Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. Unified language model pre-training for natural language understanding and generation. Advances in neural information processing systems, 32, 2019. Zheng Dong, Renhe Jiang, Haotian Gao, Hangchen Liu, Jinliang Deng, Qingsong Wen, and Xuan Song. Heterogeneity-informed meta-parameter learning for spatiotemporal time series forecasting. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 631641, 2024. 11 Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Patrick Emami, Abhijeet Sahu, and Peter Graf. Buildingsbench: large-scale dataset of 900k buildings and benchmark for short-term load forecasting. Advances in Neural Information Processing Systems, 36:1982319857, 2023. William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139, 2022. Azul Garza, Cristian Challu, and Max Mergenthaler-Canseco. Timegpt-1. arXiv preprint arXiv:2310.03589, 2023. Rakshitha Wathsadini Godahewa, Christoph Bergmeir, Geoffrey I. Webb, Rob Hyndman, and Pablo Montero-Manso. Monash time series forecasting archive. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. URL https: //openreview.net/forum?id=wEc1mgAjU-. Georg Goerg. Forecastable component analysis. ICML, 2013. Mononito Goswami, Konrad Szafer, Arjun Choudhry, Yifu Cai, Shuo Li, and Artur Dubrawski. Moment: family of open time-series foundation models. In Forty-first International Conference on Machine Learning, 2024. Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks. Journal of Machine Learning Research, 22(241):1124, 2021. Jiaxi Hu, Yuehong Hu, Wei Chen, Ming Jin, Shirui Pan, Qingsong Wen, and Yuxuan Liang. Attractor memory for long-term time series forecasting: chaos perspective. arXiv preprint arXiv:2402.11463, 2024. Peter Huber. Robust estimation of location parameter. In Breakthroughs in statistics: Methodology and distribution, pp. 492518. Springer, 1992. Aya Abdelsalam Ismail, Sercan Arik, Jinsung Yoon, Ankur Taly, Soheil Feizi, and Tomas Pfister. Interpretable mixture of experts. Transactions on Machine Learning Research, 2023. ISSN 28358856. Robert Jacobs, Michael Jordan, Steven Nowlan, and Geoffrey Hinton. Adaptive mixtures of local experts. Neural computation, 3(1):7987, 1991. Ming Jin, Yu Zheng, Yuan-Fang Li, Siheng Chen, Bin Yang, and Shirui Pan. Multivariate time series forecasting with dynamic graph neural odes. IEEE Transactions on Knowledge and Data Engineering, 35(9):91689180, 2022. Ming Jin, Qingsong Wen, Yuxuan Liang, Chaoli Zhang, Siqiao Xue, Xue Wang, James Zhang, Yi Wang, Haifeng Chen, Xiaoli Li, et al. Large models for time series and spatio-temporal data: survey and outlook. arXiv preprint arXiv:2310.10196, 2023. Ming Jin, Yifan Zhang, Wei Chen, Kexin Zhang, Yuxuan Liang, Bin Yang, Jindong Wang, Shirui Pan, and Qingsong Wen. Position: What can large language models tell us about time series analysis. In Forty-first International Conference on Machine Learning, 2024. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020. 12 Yuxuan Liang, Haomin Wen, Yuqi Nie, Yushan Jiang, Ming Jin, Dongjin Song, Shirui Pan, and Qingsong Wen. Foundation models for time series analysis: tutorial and survey. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 6555 6565, 2024. Shengsheng Lin, Weiwei Lin, Wentai Wu, Haojun Chen, and Junjie Yang. Sparsetsf: Modeling long-term time series forecasting with 1k parameters. In Forty-first International Conference on Machine Learning, 2024. Xu Liu, Yutong Xia, Yuxuan Liang, Junfeng Hu, Yiwei Wang, Lei Bai, Chao Huang, Zhenguang Liu, Bryan Hooi, and Roger Zimmermann. Largest: benchmark dataset for large-scale traffic forecasting. arXiv preprint arXiv:2306.08259, 2023. Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long. itransformer: Inverted transformers are effective for time series forecasting. In The Twelfth International Conference on Learning Representations, 2024a. Yong Liu, Haoran Zhang, Chenyu Li, Xiangdong Huang, Jianmin Wang, and Mingsheng Long. Timer: Generative pre-trained transformers are large time series models. In Forty-first International Conference on Machine Learning, 2024b. Paolo Mancuso, Veronica Piccialli, and Antonio Sudoso. machine learning approach for forecasting hierarchical time series. Expert Systems with Applications, 182:115102, 2021. Shengzhong Mao, Chaoli Zhang, Yichi Song, Jindong Wang, Xiao-Jun Zeng, Zenglin Xu, and Qingsong Wen. Time series analysis for education: Methods, applications, and future directions. arXiv preprint arXiv:2408.13960, 2024. Soukayna Mouatadid, Paulo Orenstein, Genevieve Elaine Flaspohler, Miruna Oprescu, Judah Cohen, Franklyn Wang, Sean Edward Knight, Maria Geogdzhayeva, Samuel James Levang, Ernest Fraenkel, and Lester Mackey. SubseasonalclimateUSA: dataset for subseasonal forecasting and benchmarking. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. Tung Nguyen, Jason Kyle Jewik, Hritik Bansal, Prakhar Sharma, and Aditya Grover. Climatelearn: Benchmarking machine learning for weather and climate modeling. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. Ronghao Ni, Zinan Lin, Shuaiqi Wang, and Giulia Fanti. Mixture-of-linear-experts for long-term time series forecasting. In International Conference on Artificial Intelligence and Statistics, pp. 46724680. PMLR, 2024. Yuqi Nie, Nam Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. time series is worth 64 words: Long-term forecasting with transformers. In The Eleventh International Conference on Learning Representations, 2023. Yuqi Nie, Yaxuan Kong, Xiaowen Dong, John Mulvey, Vincent Poor, Qingsong Wen, and Stefan Zohren. survey of large language models for financial applications: Progress, prospects and challenges. arXiv preprint arXiv:2406.11903, 2024. Boris Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-beats: Neural basis expansion analysis for interpretable time series forecasting. In International Conference on Learning Representations, 2020. ourownstory. Neuralprophet datasets, 2023. URL https://github.com/ourownstory/ neuralprophet-data. Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023. 13 Shiyi Qi, Zenglin Xu, Yiduo Li, Liangjian Wen, Qingsong Wen, Qifan Wang, and Yuan Qi. Pdetime: Rethinking long-term multivariate time series forecasting from the perspective of partial differential equations. arXiv preprint arXiv:2402.16913, 2024. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. Syama Sundar Rangapuram, Matthias Seeger, Jan Gasthaus, Lorenzo Stella, Yuyang Wang, and Tim Januschowski. Deep state space models for time series forecasting. Advances in neural information processing systems, 31, 2018. Stephan Rasp, Peter Dueben, Sebastian Scher, Jonathan Weyn, Soukayna Mouatadid, and Nils Thuerey. Weatherbench: benchmark data set for data-driven weather forecasting. Journal of Advances in Modeling Earth Systems, 12(11):e2020MS002203, 2020. Kashif Rasul, Arjun Ashok, Andrew Robert Williams, Arian Khorasani, George Adamopoulos, Rishika Bhagwatkar, Marin Biloˇs, Hena Ghonia, Nadhir Vincent Hassen, Anderson Schneider, Sahil Garg, Alexandre Drouin, Nicolas Chapados, Yuriy Nevmyvaka, and Irina Rish. Lag-llama: Towards foundation models for time series forecasting, 2023. Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andre Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts. Advances in Neural Information Processing Systems, 34:85838595, 2021. David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. Deepar: Probabilistic forecasting with autoregressive recurrent networks. International journal of forecasting, 36(3):1181 1191, 2020. Javier Selva, Anders Johansen, Sergio Escalera, Kamal Nasrollahi, Thomas Moeslund, and Albert Clapes. Video transformers: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(11):1292212943, 2023. Rajat Sen, Hsiang-Fu Yu, and Inderjit Dhillon. Think globally, act locally: deep neural network approach to high-dimensional time series forecasting. Advances in neural information processing systems, 32, 2019. Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton, and Dean. The sparsely-gated mixture-of-experts layer. Outrageously large neural networks, 2017. Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Willem van Panhuis, Anne Cross, and Donald Burke. Project tycho 2.0: repository to improve the integration and reuse of data for global population health. Journal of the American Medical Informatics Association, 25:16081617, 2018. Ashish Vaswani. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017. Jingyuan Wang, Jiawei Jiang, Wenjun Jiang, Chengkai Han, and Wayne Xin Zhao. Towards efficient and comprehensive urban spatial-temporal prediction: unified library and performance benchmark. arXiv preprint arXiv:2304.14343, 2023a. Jun Wang, Wenjie Du, Wei Cao, Keli Zhang, Wenjia Wang, Yuxuan Liang, and Qingsong Wen. Deep learning for multivariate time series imputation: survey. arXiv preprint arXiv:2402.04059, 2024a. 14 Shiyu Wang, Haixu Wu, Xiaoming Shi, Tengge Hu, Huakun Luo, Lintao Ma, James Zhang, and In The Jun Zhou. Timemixer: Decomposable multiscale mixing for time series forecasting. Twelfth International Conference on Learning Representations, 2024b. Xue Wang, Tian Zhou, Qingsong Wen, Jinyang Gao, Bolin Ding, and Rong Jin. Card: Channel aligned robust blend transformer for time series forecasting. In The Twelfth International Conference on Learning Representations (ICLR), 2024c. Zhixian Wang, Qingsong Wen, Chaoli Zhang, Liang Sun, Leandro Von Krannichfeldt, and Yi Wang. Benchmarks and custom package for electrical load forecasting. arXiv preprint arXiv:2307.07191, 2023b. Qingsong Wen, Jingkun Gao, Xiaomin Song, Liang Sun, and Jian Tan. RobustTrend: huber loss with combined first and second order difference regularization for time series trend filtering. In Proceedings of the 28th International Joint Conference on Artificial Intelligence, pp. 38563862, 2019. Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing Ma, Junchi Yan, and Liang Sun. Transformers in time series: survey. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence (IJCAI), pp. 67786786, 2023. Gerald Woo, Chenghao Liu, Akshat Kumar, and Doyen Sahoo. Pushing the limits of pre-training for time series forecasting in the cloudops domain. arXiv preprint arXiv:2310.05063, 2023. Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, and Doyen Sahoo. In Forty-first International Unified training of universal time series forecasting transformers. Conference on Machine Learning, 2024. Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. Advances in Neural Information Processing Systems, 34:2241922430, 2021. Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet: Temporal 2d-variation modeling for general time series analysis. In International Conference on Learning Representations, 2023a. Haixu Wu, Hang Zhou, Mingsheng Long, and Jianmin Wang. Interpretable weather forecasting for worldwide stations with unified deep model. Nature Machine Intelligence, 2023b. Yiyuan Yang, Chaoli Zhang, Tian Zhou, Qingsong Wen, and Liang Sun. Dcdetector: Dual attention contrastive representation learning for time series anomaly detection. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 30333045, 2023. Zhihan Yue, Yujing Wang, Juanyong Duan, Tianmeng Yang, Congrui Huang, Yunhai Tong, and Bixiong Xu. Ts2vec: Towards universal representation of time series. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 89808987, 2022. Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series In Proceedings of the AAAI conference on artificial intelligence, volume 37, pp. forecasting? 1112111128, 2023. George Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha Bhamidipaty, and Carsten Eickhoff. transformer-based framework for multivariate time series representation learning. In Proceedings of the 27th ACM SIGKDD conference on knowledge discovery & data mining, pp. 21142124, 2021. Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. Kexin Zhang, Qingsong Wen, Chaoli Zhang, Rongyao Cai, Ming Jin, Yong Liu, James Zhang, Yuxuan Liang, Guansong Pang, Dongjin Song, et al. Self-supervised learning for time series analysis: Taxonomy, progress, and prospects. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. 15 Xiang Zhang, Ziyuan Zhao, Theodoros Tsiligkaridis, and Marinka Zitnik. Self-supervised contrastive pre-training for time series via time-frequency consistency. Advances in Neural Information Processing Systems, 35:39884003, 2022. Yunhao Zhang and Junchi Yan. Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting. In International Conference on Learning Representations, 2023. Yu Zheng, Xiuwen Yi, Ming Li, Ruiyuan Li, Zhangqing Shan, Eric Chang, and Tianrui Li. ForeIn Proceedings of the 21th ACM SIGKDD casting fine-grained air quality based on big data. international conference on knowledge discovery and data mining, pp. 22672276, 2015. Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pp. 1110611115, 2021. Jingbo Zhou, Xinjiang Lu, Yixiong Xiao, Jiantao Su, Junfu Lyu, Yanjun Ma, and Dejing Dou. Sdwpf: dataset for spatial dynamic wind power forecasting challenge at kdd cup 2022. arXiv preprint arXiv:2208.04360, 2022a. Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. FEDformer: Frequency enhanced decomposed transformer for long-term series forecasting. In Proc. 39th International Conference on Machine Learning (ICML 2022), 2022b."
        },
        {
            "title": "A FURTHER RELATED WORK",
            "content": "In this section, we delve deeper into the related work on large time series models. Current research efforts in universal forecasting with time series foundation models can be broadly classified into three categories, as summarized in Table 8: (1) encoder-only models, such as Moirai (Woo et al., 2024) and Moment (Goswami et al., 2024), which employ masked reconstruction and have been pre-trained on datasets containing 27B and 1B time points, respectively, with model sizes reaching up to 385M parameters; (2) encoder-decoder models, exemplified by Chronos (Ansari et al., 2024), which offers pre-trained models at four scales, with up to 710M parameters; and (3) decoder-only models, including TimesFM (Das et al., 2024), Lag-Llama (Rasul et al., 2023), and Timer (Liu et al., 2024b), with the largest models containing up to 200M parameters. In contrast to these dense models, TIME-MOE introduces scalable, unified architecture with sparse mixture-of-experts design, optimized for larger time series forecasting models while reducing inference costs. Trained on our Time-300B dataset, comprising over 300B time points, TIME-MOE is scaled to 2.4B parameters for the first time. It outperforms existing models with the same number of activated parameters, significantly enhancing both model efficiency and forecasting precision, while avoiding limitations such as fixed context lengths or hardcoded heuristics. Table 8: Comparison between large time series models."
        },
        {
            "title": "Method",
            "content": "Time-MoE Moirai"
        },
        {
            "title": "Chronos",
            "content": "Timer Lag-Llama TimeGPT"
        },
        {
            "title": "Architecture",
            "content": "DecoderOnly EncoderDecoder-"
        },
        {
            "title": "Only",
            "content": "EncoderOnly EncoderDecoderDecoderEncoderDecoder Decoder"
        },
        {
            "title": "Only",
            "content": "(Max) Model Size"
        },
        {
            "title": "Dataset Scale",
            "content": "2.4B"
        },
        {
            "title": "Point",
            "content": "309B 311M 200M"
        },
        {
            "title": "Patch",
            "content": "27B/231B* 100B"
        },
        {
            "title": "Max Context Length",
            "content": "4096 5000 512 385M"
        },
        {
            "title": "Patch",
            "content": "1.13B 512 710M"
        },
        {
            "title": "Point",
            "content": "84B 512 67M"
        },
        {
            "title": "Patch",
            "content": "28B 1440 200M Unknown"
        },
        {
            "title": "Point",
            "content": "0.36B"
        },
        {
            "title": "Patch",
            "content": "100B"
        },
        {
            "title": "Dense",
            "content": "Open-source Data"
        },
        {
            "title": "Source",
            "content": ""
        },
        {
            "title": "Ours",
            "content": "Woo et al. Das et al. Goswami et al. Ansari et al. Liu et al. Rasul et al. Garza et al. * Depend on the way of calculation according to the original paper."
        },
        {
            "title": "B IMPLEMENTATION DETAILS",
            "content": "Training Configuration. Each model is trained for 100,000 steps with batch size of 1,024, and maximum sequence length capped at 4,096. This setup processes 4 million time points per iteration. We use forecast horizons of {1, 8, 32, 64} in the output projection and set the auxiliary loss factor α to 0.02. For optimization, we apply the AdamW optimizer with the following hyperparameters: lr = 1e-3, weight decay = 1e-1, β1 = 0.9, and β2 = 0.95. learning rate scheduler with linear warmup for the first 10,000 steps, followed by cosine annealing, is used. Training is performed on 128 NVIDIA A100-80G GPUs with BF16 precision. To improve batch processing efficiency and handle varying sequence lengths, we employ sequence packing (Raffel et al., 2020), which reduces padding requirements. Benchmark Details. We evaluate the performance of various models for long-term forecasting across eight well-established datasets, including the Weather (Wu et al., 2021), Global Temp (Wu et al., 2023b), and ETT datasets (ETTh1, ETTh2, ETTm1, ETTm2) (Zhou et al., 2021). detailed description of each dataset is provided in Table 9. Metrics. We use mean square error (MSE) and mean absolute error (MAE) as evaluation metrics for time-series forecasting. These metrics are calculated as follows: MSE ="
        },
        {
            "title": "1\nH",
            "content": "H (cid:88) i=1 (xi (cid:98)xi)2, MAE ="
        },
        {
            "title": "1\nH",
            "content": "H (cid:88) i=1 xi (cid:98)xi, 17 Table 9: Detailed dataset descriptions. Dataset sizes are listed as (Train, Validation, Test)."
        },
        {
            "title": "Dataset",
            "content": "ETTm1 ETTm2 Long-term ETTh1 Forecasting ETTh2 7"
        },
        {
            "title": "Dataset Size",
            "content": "{96, 192, 336, 720} (34465, 11521, 11521) Frequency Forecastability Information 0.46 Temperature 15min {96, 192, 336, 720} (34465, 11521, 11521) 15min {96, 192, 336, 720} (8545, 2881, 2881) {96, 192, 336, 720} (8545, 2881, 2881)"
        },
        {
            "title": "Weather",
            "content": "21 {96, 192, 336, 720} (36792, 5271, 10540) 10 min Global Temp 1000 {96, 192, 336, 720} (12280, 1755, 3509)"
        },
        {
            "title": "Hourly",
            "content": "0.55 0.38 0.45 0.75 0.78 Temperature Temperature Temperature Weather Temperature The forecastability is calculated by one minus the entropy of Fourier decomposition of time series (Goerg, 2013). larger value indicates better predictability. where xi, (cid:98)xi are the ground truth and predictions of the i-th future time point. Multi-resolution Forecasting. To construct the multi-resolution forecasting head, we define output projections, each corresponding to distinct forecasting horizon, denoted as (p1, p2, . . . , pP ). The output projection for horizon pj is used to forecast the subsequent pj time steps, as follows: ˆXt+1:t+pj = Wpj hL , (12) where Wpj Rpj is the learnable parameter matrix for that horizon, and hL represents the output hidden state from the last MoE Transformer block. All output projections are optimized simultaneously during model training. During inference, we apply greedy scheduling algorithm for arbitrary target output lengths H, as outlined in Algorithm 1. For each forecast operation in the auto-regressive process, we select projection pj with the closest forecasting horizon that does not exceed the remaining forecast duration. This approach allows TIME-MOE to extend predictions beyond the next immediate time step or fixed horizon, significantly improving both the models utility and overall forecasting accuracy. Algorithm 1 Scheduling for the Multi-resolution Forecasting Require: Target output length H, forecast horizon of each output projection {p1, p2, . . . , pP } in ascending order Ensure: Combined output length ˆH = H, p1 = 1 1: ˆH 0 2: {} 3: while ˆH < do 4: 5: 6: 7: 8: end if 9: end for 10: 11: end while 12: return for = down to 1 do if ˆH + pi then ˆH ˆH + pj add pj to break"
        },
        {
            "title": "C PROCESSED DATA ARCHIVE",
            "content": "Going beyond previous work (Ansari et al., 2024; Woo et al., 2024; Liu et al., 2024b), we organized comprehensive large-scale time series dataset from vast collection of complex raw data. To ensure data quality, we addressed issues by either imputing missing values or discarding malformed time series. Inspired by data processing techniques from large language models (Penedo et al., 2023; Computer, 2023; Jin et al., 2024), we developed fine-grained data-cleaning pipeline specifically designed for time series data: 18 Missing Value Processing. In time series data, missing values often appear as NaN (Not Number) or Inf (Infinity). While previous studies commonly address this by replacing missing values with the mean, this may distort the original time series pattern. Instead, we employ method that splits the original sequence into multiple sub-sequences at points where missing values occur, effectively removing those segments while preserving the integrity of the original time series pattern. Invalid Observation Processing. In some data collection systems, missing values are often filled with 0 or another constant, leading to sequences with constant values that do not represent valid patterns for the model. To address this, we developed filtering method that uses fixed-length window to scan the entire sequence. For each window, we calculate the ratio of first-order and second-order differences, discarding the window if this ratio exceeds pre-specified threshold (set to 0.2 in our case). The remaining valid continuous window sequences are then concatenated into single sequence. This process transforms the original sequence into multiple sub-sequences, effectively removing segments with invalid patterns. Following the processing steps described above, we compiled high-quality time series dataset named Time-300B, which spans range of sampling frequencies from seconds to yearly intervals, encompassing total of 309.09 billion time points. To optimize memory efficiency and loading speed, each dataset is split into multiple binary files, with metafile providing details such as the start and end positions of each sequence. This setup allows us to load the data using fixed amount of memory during training, preventing memory shortages. Datasets like Weatherbench, CMIP6, and ERA5 are particularly large, often leading to data imbalance and homogenization. To mitigate these issues, we apply down-sampling to these datasets. During training, we utilized approximately 117 billion time points in Time-300B, sampling each batch according to fixed proportions of domains and distributions of observation values. Below, we outline the key properties of the datasets after processing, including their domain, sampling frequency, number of time series, total number of observations, and data source. Also, we present the KEY COMPONENTS SOURCE CODE of the data-cleaning pipeline in Algorithm 2. Table 10: Datasets and key properties from Time-300B. For frequency: = second, = minute, = hour, = day, = business day, = week, = month, = quarter, = year. Dataset Domain Freq. # Time Series # Obs. Source Electricity (15 min) Electricity (Weekly) ERCOT Load Australian Electricity Solar Power Wind Farms BDG-2 Bear BDG-2 Fox BDG-2 Panther BDG-2 Rat Borealis Buildings900K BDG-2 Bull BDG-2 Cockatoo Covid19 Energy Elecdemand GEF GEF17 BDG-2 Hog"
        },
        {
            "title": "IDEAL",
            "content": "KDD Cup 2018 Energy Energy Energy Energy Energy Energy Energy Energy Energy Energy Energy Energy Energy Energy Energy Energy"
        },
        {
            "title": "Energy",
            "content": "15T 30T 4S H H H 30T H 347 318 5 26 39,708,170 Godahewa et al. (2021) 49,608 1,238, 1,153,584 Godahewa et al. (2021) ourownstory (2023) Godahewa et al. (2021) 5,248 Godahewa et al. (2021) 43,246 39,705,317 Godahewa et al. (2021) 215 179 455 17 1,422,320 2,285,288 893,840 4,596, 82,757 Emami et al. (2023) Emami et al. (2023) Emami et al. (2023) Emami et al. (2023) Emami et al. (2023) 2,464,188 15,124,358,211 Emami et al. (2023) 501,832 17032 31, 17,520 788,280 140,352 365,304 1,253,088 922, Wang et al. (2023b) Wang et al. (2023b) Wang et al. (2023b) Godahewa et al. (2021) Wang et al. (2023b) Wang et al. (2023b) Wang et al. (2023b) Emami et al. (2023) Godahewa et al. (2021) 464 4 1 20 8 152 225 3, 19 Dataset Domain Freq. # Time Series # Obs. Source Table 10 continued from previous page KDD Cup 2022 London Smart Meters PDB Residential Load Power Residential PV Power Sceaux SMART Spanish Exchange Rate CIF Bitcoin FRED MD NN5 Daily Tourism Monthly Tourism Quarterly Tourism Yearly COVID Deaths Hospital CDC Fluview ILINet Energy Energy Energy Energy Energy Energy Energy Energy Finance Finance Finance Finance Finance Finance Finance Finance Healthcare Healthcare Healthcare CDC Fluview WHO NREVSS Healthcare Project Tycho US Births Healthcare Healthcare Weatherbench (Hourly) Weatherbench (Daily) Weatherbench (Weekly) Beijing Air Quality China Air Quality CMIP6 ERA5 Oikolab Weather Saugeen Subseasonal Subseasonal Precipitation Sunspot Temperature Rain Weather Dominick Car Parts Favorita Sales Favorita Transactions Hierarchical Sales Restaurant M"
        },
        {
            "title": "Traffic",
            "content": "Taxi (Hourly)"
        },
        {
            "title": "Covid Mobility",
            "content": "Nature Nature Nature Nature Nature Nature Nature Nature Nature Nature Nature Nature Nature Nature Sales Sales Sales Sales Sales Sales Sales"
        },
        {
            "title": "Transport",
            "content": "10T 30T H B M Q M W D H 8,554 24,132 79,508 248,888 2,332,874 Zhou et al. (2022a) 160,041,727 Godahewa et al. (2021) 17,520 Wang et al. (2023b) 404,832,695 Bergmeir et al. (2023) 184,238,228 Bergmeir et al. (2023) 1 5 1 13 72 104 220 359 427 419 727 286 108 588 1 34, 95,709 35,064 56,096 7,108 68927 71, 35,303 98,867 39,128 11,198 364 55, 220,144 56,407 120,183 7,275 Emami et al. (2023) Emami et al. (2023) Wang et al. (2023b) Ansari et al. (2024) Godahewa et al. (2021) Godahewa et al. (2021) Godahewa et al. (2021) Godahewa et al. (2021) Godahewa et al. (2021) Godahewa et al. (2021) Godahewa et al. (2021) Godahewa et al. (2021) Godahewa et al. (2021) CDC (2017) CDC (2017) van Panhuis et al. (2018) Godahewa et al. (2021) 3,984,029 74,630,250,518 Rasp et al. (2020) 301,229 226,533 4,262 17,686 3,223,513,345 Rasp et al. (2020) 462,956,049 Rasp et al. (2020) 2,932,657 4,217,605 Chen (2019) Zheng et al. (2015) 6H 14,327,808 104,592,998,400 Nguyen et al. (2023) D D D D D 30T 11,940,789 93,768,721,472 Nguyen et al. (2023) 309 38 17, 13,467 19 13,226 9,525 3,712 91,513 258 215 155 615,574 17, Godahewa et al. (2021) Godahewa et al. (2021) 51,968,498 Mouatadid et al. (2023) 4,830,284 Mouatadid et al. (2023) 45,312 Godahewa et al. (2021) 3,368,098 Godahewa et al. (2021) 26,036,234 Ansari et al. (2024) 759,817 Godahewa et al. (2021) 816 Godahewa et al. (2021) 20,371,303 Woo et al. (2024) 81,196 114,372 30,289 Woo et al. (2024) Mancuso et al. (2021) Woo et al. (2024) 14,341 5,011,077 Alexandrov et al. (2020) 78,848 Ansari et al. (2024) 14,993, Godahewa et al. (2021) 1,762,024 Ansari et al. (2024) 19,872 120,950 Wang et al. (2023a) Godahewa et al. (2021) 556 1,371 2,433 552 20 Dataset HZMetro LargeST Loop Seattle Los-Loop Pedestrian Counts PEMS Bay PEMS03 PEMS04 PEMS07 PEMS Q-Traffic SHMetro SZ-Taxi Rideshare Taxi Traffic Hourly Traffic Weekly Uber TLC Daily Uber TLC Hourly Vehicle Trips Wiki Daily (100k) Alibaba Cluster Trace Azure VM Traces 2017 Borg Cluster Data 2011 Kaggle Web Traffic Weekly Extended Web Traffic Wiki-Rolling TSMixup 10M KernelSynth 1M M1 Monthly M1 Quarterly M1 Yearly M3 Monthly M3 Quarterly M3 Yearly M4 Daily M4 Hourly M4 Monthly M4 Quarterly M4 Weekly M4 Yearly Table 10 continued from previous page Domain Freq. # Time Series Transport 15T 160 # Obs. 11,680 Source Wang et al. (2023a) Transport Transport Transport Transport Transport Transport Transport Transport Transport Transport Transport Transport Transport Transport Transport Transport Transport Transport Transport Web Web Web Web Web Web Web Synthetic Synthetic Other Other Other Other Other Other Other Other Other Other Other Other 5T 5T 5T 5T 5T 5T 5T 5T 15T 15T 15T 30T D D 5T 5T 5T D - - 3M 3M M 3M 1,208,997 4,175,062,621 Liu et al. (2023) 1, 3,381 80 3,980 1,651 6,634 3, 2,612 33,700,832 Wang et al. (2023a) 6,231,168 3,125,914 15,975, 9,210,432 14,638,784 23,789,760 8,684,480 Wang et al. (2023a) Godahewa et al. (2021) Wang et al. (2023a) Wang et al. (2023a) Wang et al. (2023a) Wang et al. (2023a) Wang et al. (2023a) 46, 257,200,384 Wang et al. (2023a) 574 156 1,352 96, 1,363 821 235 344 10 100, 48,640 263,928 216,636 133,388 161,890 47, 41,902 464,256 192,949 Wang et al. (2023a) Wang et al. (2023a) Godahewa et al. (2021) 40,584,636 Alexandrov et al. (2020) 14,858,016 Godahewa et al. (2021) 78,816 42, 510,284 1,626 Godahewa et al. (2021) Alexandrov et al. (2020) Alexandrov et al. (2020) Godahewa et al. (2021) 274,099,872 Ansari et al. (2024) 83,776,950 880,648,165 176,650,715 Woo et al. (2023) Woo et al. (2023) Woo et al. (2023) 15,206,232 Godahewa et al. (2021) 332,586,145 Godahewa et al. (2021) 40,619,100 Alexandrov et al. (2020) 10,968,625 8,198,358,952 Ansari et al. (2024) 1,000, 1,024,000,000 Ansari et al. (2024) 8 195 106 755 645 4,134 415 30,126 2, 293 106 1,047 9,628 3136 109, 36,960 18,319 Godahewa et al. (2021) Godahewa et al. (2021) Godahewa et al. (2021) Godahewa et al. (2021) Godahewa et al. (2021) Godahewa et al. (2021) 9,903,554 Godahewa et al. (2021) 352,988 Godahewa et al. (2021) 8,480,953 Godahewa et al. (2021) 491,632 348,224 3,136 Godahewa et al. (2021) Godahewa et al. (2021) Godahewa et al. (2021) 21 Algorithm 2 Sample code of Data-cleaning Pipline # Missing Value Processing def split_seq_by_nan_inf(seq, minimum_seq_length: int = 1): output = [] sublist = [] for num in seq: if num is None or np.isnan(num) or np.isinf(num): if len(sublist) >= minimum_seq_length: output.append(sublist) sublist = [] else: sublist.append(num) if len(sublist) >= minimum_seq_length: output.append(sublist) return output # Invalid Observation Processing def split_seq_by_window_quality(seq, window_size: int = 128, zero_threshold, minimum_seq_length: int = 256): if len(seq) <= window_size: flag, info = check_sequence(seq, zero_threshold=zero_threshold) if flag: return [seq] else: return [] = window_size sub_seq = [] out_list = [] while True: if + window_size > len(seq): window_seq = seq[i - window_size: len(seq)] = len(seq) window_seq = seq[i - window_size: i] flag, info = check_sequence(window_seq, zero_threshold=zero_threshold) if flag: sub_seq.extend(window_seq) else: else: if len(sub_seq) >= minimum_seq_length: out_list.append(sub_seq) sub_seq = [] if >= len(seq): break += window_size if len(sub_seq) >= minimum_seq_length: out_list.append(sub_seq) return out_list def check_sequence(seq, zero_threshold: float): import numpy as np if not isinstance(seq, np.ndarray): seq = np.array(seq) if len(seq.shape) > 1: raise RuntimeError(fDimension of the seq is not equal to 1: {seq.shape}) flag = True info = {} nan_count = np.sum(np.isnan(seq)) info[nan_count] = nan_count if nan_count > 0: flag = False return flag, info inf_count = np.sum(np.isinf(seq)) info[inf_count] = inf_count if inf_count > 0: flag = False return flag, info zero_ratio = np.sum(seq == 0) / len(seq) info[zero_ratio] = zero_ratio if zero_ratio > zero_threshold: flag = False first_diff = seq[1:] - seq[:-1] first_diff_zero_ratio = np.sum(first_diff == 0) / len(first_diff) info[first_diff_zero_ratio] = first_diff_zero_ratio if first_diff_zero_ratio > zero_threshold: flag = False second_diff = seq[2:] - seq[:-2] second_diff_zero_ratio = np.sum(second_diff == 0) / len(second_diff) info[second_diff_zero_ratio] = second_diff_zero_ratio if second_diff_zero_ratio > zero_threshold: flag = False return flag, info"
        },
        {
            "title": "D ADDITIONAL RESULTS",
            "content": "D.1 ABLATION STUDY Table 11: MSE for horizon-96 forecasting across six benchmarks, evaluated with different model components. ETTh1 ETTh2 ETTm1 ETTm2 Weather Global Temp Average TIME-MOEbase w/o Huber loss w/o multi-resolution layer w/o mixture-of-experts w/o auxiliary loss 0.357 0.365 0.358 0.370 0. 0.305 0.309 0.313 0.317 0.325 0.338 0.344 0.348 0.347 0.350 0.201 0.205 0.212 0.212 0.219 0.160 0.163 0.164 0.163 0.164 0.211 0.217 0.217 0.223 0.226 0.262 0.267 0.269 0.272 0. As shown in Table 11, replacing the MoE layers with standard FFNs (denoted as w/o mixture-ofexperts ) led to noticeable performance decline, with the MSE worsening from 0.262 to 0.272. This highlights the significant contribution of the sparse architecture to the models overall performance, as its dynamic routing enables more specialized processing of diverse input patterns. We also conducted experiments by retaining only the horizon-32 forecasting head from the TIMEMOEbase (denoted as w/o multi-resolution layer), excluding the multi-task optimization. The performance of this modified model was slightly inferior to the complete TIME-MOEbase. Table 12: Full ablation results for different multi-resolution forecasting configurations. ETTh1 ETTh2 ETTm1 ETTm2 Weather Global Temp Average MSE Inference Speed"
        },
        {
            "title": "0.357\nTIME-MOEbase\nTIME-MOEbase w/ {1,8,32} 0.353\nTIME-MOEbase w/ {1,8}\n0.389\nTIME-MOEbase w/ {1}\n1.071",
            "content": "0.305 0.316 0.391 0.920 0.338 0.370 0.441 2.098 0.201 0.225 0.304 2.320 0.160 0.161 0.174 1.500 0.211 0.213 0.222 0.383 0.262 0.273 0.320 1. 0.095 s/iter 0.130 s/iter 0.411 s/iter 2.834 s/iter As shown in Table 12, the default configuration of four multi-resolution forecasting heads with receptive horizons of 1, 8, 32, 64 delivers optimal predictive performance and inference speed. Reducing the number of heads consistently resulted in decreased performance and longer inference time. This inverse relationship highlights the effectiveness of our multi-resolution forecasting design, striking balance between accuracy and computational efficiency in decoder-only forecasting foundation model. These findings highlight the importance of key architectural components in TIME-MOE, such as the mixture-of-experts, multi-task optimization, and multi-resolution forecasting, in delivering state-ofthe-art performance in universal time series forecasting. D.2 TRAINING PRECISION ANALYSIS To optimize model performance and efficiency, we conducted comparative study examining the impact of numerical precision during training. We trained two versions of our model under identical configurations, with the only difference being the precision: one using bfloat16 and the other using float32. The model trained with float32 precision is referred to as TIME-MOEbase w/ FP32. Table 13: Full results of the comparison between BF16 and FP32 in terms of training and inference efficiency. FA denotes flash-attention. ETTh1 ETTh2 ETTm1 ETTm2 Weather Global Temp Average MSE Training Speed Inference Speed Training Memory Inference Memory TIME-MOEbase 0.357 TIME-MOEbase w/o FA 0.357 TIME-MOEbase w/ FP32 0.358 0.305 0.305 0.303 0.338 0.338 0.342 0.201 0.201 0.198 0.160 0.160 0. 0.211 0.211 0.208 0.262 0.262 0.261 0.84 s/iter 1.09 s/iter 1.24 s/iter 0.095 s/iter 0.118 s/iter 0.133 s/iter 1.77 GB 1.77 GB 2.21 GB 226.70 MB 226.70 MB 453.41 MB As detailed in Table 6, our analysis reveals that the forecasting performances of these two models are remarkably comparable. This finding is significant as it demonstrates that the use of reduced precision (e.g., bfloat16) does not compromise the predictive capabilities of our model. 23 However, the similarities in performance belie the substantial differences in computational efficiency and resource utilization: Training Speed: Notably, the bfloat16 model demonstrates 12% improvement in training speed compared to its float32 counterpart. This considerable acceleration in the training process can significantly reduce the time-to-deployment for large-scale models and facilitate more rapid experimentation and iteration. Memory Consumption: In terms of memory usage, the bfloat16 model exhibits superior efficiency, consuming substantially less memory than the float32 model. Specifically, we observed reduction of 20% in memory usage. This memory optimization is crucial for scaling models to larger sizes or deploying them on memory-constrained hardware. Compatibility with Advanced Techniques: key advantage of the bfloat16 model is its seamless integration with advanced optimization techniques. In particular, it can easily be combined with flash-attention (Dao, 2024), state-of-the-art attention mechanism designed for better efficiency. This integration results in an additional 23% increase in training speed and 19% boost in inference speed, further enhancing the already significant performance gains. The implications of these findings are far-reaching: Resource Efficiency: The reduced memory footprint and increased training speed of the bfloat16 model translate to more efficient utilization of computational resources, potentially lowering infrastructure costs and energy consumption. Scalability: The memory savings offered by bfloat16 precision enable the training of larger, more complex models on the same hardware, potentially leading to improved model capabilities without increasing computational requirements. Faster Development Cycles: The substantial improvements in training speed can accelerate the research and development process, allowing for more rapid prototyping and experimentation. Inference Optimization: The compatibility with flash-attention not only benefits training but also enhances inference speed, which is crucial for real-time applications and large-scale deployments. Our experiments show that adopting bfloat16 precision, combined with advanced techniques like flash-attention, provides compelling balance between model performance, computational efficiency, and resource utilization. These optimizations enable the scalable and efficient deployment of large-scale time series forecasting models without sacrificing predictive accuracy."
        },
        {
            "title": "E FORECAST SHOWCASES",
            "content": "To visualize the performance differences among various large-scale time series models, we present the forecasting results of our model, TIME-MOE, in comparison to the ground truth across six realworld benchmarks. These benchmarks include ETTh1, ETTh2, ETTm1, ETTm2, Weather, and Global Temp datasets. Alongside TIME-MOEs results, we also show the performance of other large-scale baseline models at different scales, providing comprehensive view of their comparative capabilities (Figures 5 10). In all figures, the context length is set to 512, and the forecast horizon is 96. To enhance clarity and aesthetics, we display the full forecast output, complemented by portion of the preceding historical input data, ensuring more intuitive comparison. The results clearly demonstrate the superiority of TIME-MOE over the other foundational models. Its ability to consistently produce more accurate forecasts across range of datasets underscores the effectiveness of its architecture and design. The performance gains are especially noticeable in long-term prediction scenarios, where TIME-MOEs handling of temporal dependencies proves more robust than its counterparts. These visual comparisons highlight the practical advantages of TIME-MOE in large-scale time series forecasting, reinforcing its status as state-of-the-art model. 24 Figure 5: Zero-shot forecasting cases from ETTh1 by different models, with forecast horizon 96. Blue lines are the ground truths and read lines are the model predictions. 25 Figure 6: Zero-shot forecasting cases from ETTh2 by different models, with forecast horizon 96. 26 Figure 7: Zero-shot forecasting cases from ETTm1 by different models, with forecast horizon 96. 27 Figure 8: Zero-shot forecasting cases from ETTm2 by different models, with forecast horizon 96. 28 Figure 9: Zero-shot forecasting cases from Weather by different models, with forecast horizon 96. 29 Figure 10: Zero-shot forecasting cases from Global Temp by different models, with forecast horizon 96."
        }
    ],
    "affiliations": [
        "Griffith University",
        "Princeton University",
        "Squirrel Ai Learning"
    ]
}