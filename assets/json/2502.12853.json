{
    "paper_title": "S$^2$R: Teaching LLMs to Self-verify and Self-correct via Reinforcement Learning",
    "authors": [
        "Ruotian Ma",
        "Peisong Wang",
        "Cheng Liu",
        "Xingyan Liu",
        "Jiaqi Chen",
        "Bang Zhang",
        "Xin Zhou",
        "Nan Du",
        "Jia Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent studies have demonstrated the effectiveness of LLM test-time scaling. However, existing approaches to incentivize LLMs' deep thinking abilities generally require large-scale data or significant training efforts. Meanwhile, it remains unclear how to improve the thinking abilities of less powerful base models. In this work, we introduce S$^2$R, an efficient framework that enhances LLM reasoning by teaching models to self-verify and self-correct during inference. Specifically, we first initialize LLMs with iterative self-verification and self-correction behaviors through supervised fine-tuning on carefully curated data. The self-verification and self-correction skills are then further strengthened by both outcome-level and process-level reinforcement learning, with minimized resource requirements, enabling the model to adaptively refine its reasoning process during inference. Our results demonstrate that, with only 3.1k self-verifying and self-correcting behavior initialization samples, Qwen2.5-math-7B achieves an accuracy improvement from 51.0\\% to 81.6\\%, outperforming models trained on an equivalent amount of long-CoT distilled data. Extensive experiments and analysis based on three base models across both in-domain and out-of-domain benchmarks validate the effectiveness of S$^2$R. Our code and data are available at https://github.com/NineAbyss/S2R."
        },
        {
            "title": "Start",
            "content": "S2R: Teaching LLMs to Self-verify and Self-correct via Reinforcement Learning Ruotian Ma1*, Peisong Wang2*, Cheng Liu1, Xingyan Liu1, Jiaqi Chen3, Bang Zhang1, Xin Zhou4, Nan Du1 , Jia Li 5 1Tencent 2Tsinghua University 3The University of Hong Kong 4Fudan University 5The Hong Kong University of Science and Technology (Guangzhou) ruotianma@tencent.com, wps22@mails.tsinghua.edu.cn 5 2 0 F 8 1 ] . [ 1 3 5 8 2 1 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent studies have demonstrated the effectiveness of LLM test-time scaling. However, existing approaches to incentivize LLMs deep thinking abilities generally require large-scale data or significant training efforts. Meanwhile, it remains unclear how to improve the thinking abilities of less powerful base models. In this work, we introduce S2R, an efficient framework that enhances LLM reasoning by teaching models to self-verify and self-correct during inference. Specifically, we first initialize LLMs with iterative self-verification and self-correction behaviors through supervised fine-tuning on carefully curated data. The self-verification and self-correction skills are then further strengthened by both outcomelevel and process-level reinforcement learning, with minimized resource requirements, enabling the model to adaptively refine its reasoning process during inference. Our results demonstrate that, with only 3.1k selfverifying and self-correcting behavior initialization samples, Qwen2.5-math-7B achieves an accuracy improvement from 51.0% to 81.6%, outperforming models trained on an equivalent amount of long-CoT distilled data. Extensive experiments and analysis based on three base models across both in-domain and out-ofdomain benchmarks validate the effectiveness of S2R. Our code and data are available at https://github.com/NineAbyss/S2R."
        },
        {
            "title": "Introduction",
            "content": "Recent advancements in Large Language Models (LLMs) have demonstrated paradigm shift from scaling up training-time efforts to test-time compute (Snell et al., 2024a; Kumar et al., 2024; Qi et al., 2024; Yang et al., 2024). The effectiveness of scaling test-time compute is illustrated by OpenAI o1 (OpenAI, 2024), which shows strong rea- * Equal contribution. This work was done during Peisong, Cheng, Jiaqi and Bang were interning at Tencent. Corresponding authors. 1 Figure 1: The data efficiency of S2R compared to competitive methods, with all models initialized from Qwen2.5-Math-7B. soning abilities by performing deep and thorough thinking, incorporating essential skills like selfchecking, self-verifying, self-correcting and selfexploring during the models reasoning process. This paradigm not only enhances reasoning in domains like mathematics and science but also offers new insights into improving the generalizability, helpfulness and safety of LLMs across various general tasks (OpenAI, 2024; Guo et al., 2025). Recent studies have made various attempts to replicate the success of o1. These efforts include using large-scale Monte Carlo Tree Search (MCTS) to construct long-chain-of-thought (longCoT) training data, or to scale test-time reasoning to improve the performance of current models (Guan et al., 2025; Zhao et al., 2024; Snell et al., 2024b); constructing high-quality long-CoT data for effective behavior cloning with substantial human effort (Qin et al., 2024); and exploring reinforcement learning to enhance LLM thinking abilities on large-scale training data and models (Guo et al., 2025; Team et al., 2025; Cui et al., 2025; Yuan et al., 2024). Recently, DeepSeek R1 (Guo et al., 2025) demonstrated that large-scale reinforcement learning can incentivize LLMs deep thinking abilities, with the R1 series showcasing the promising potential of long-thought reasoning. However, these approaches generally requires significant resources to enhance LLMs thinking abilities, including large datasets, substantial trainingtime compute, and considerable human effort and time costs. Meanwhile, it remains unclear how to incentivize valid thinking in smaller or less powerful LLMs beyond distilling knowledge from more powerful models. In this work, we propose S2R, an efficient alternative to enhance the thinking abilities of LLMs, particularly for smaller or less powerful LLMs. Instead of having LLMs imitate the thinking process of larger, more powerful models, S2R focus on teaching LLMs to think deeply by iteratively adopting two critical thinking skills: self-verifying and self-correcting. By acquiring these two capabilities, LLMs can continuously reassess their solutions, identify mistakes during solution exploration, and refine previous solutions after self-checking. Such paradigm also enables flexible allocation of test-time compute to different levels of problems. Our results show that, with only 3.1k training samples, Qwen2.5-math-7B significantly benefits from learning self-verifying and self-correcting behaviors, achieving 51.0% to 81.6% accuracy improvement on the Math500 test set. This performance outperforms the same base model distilled from an equivalent amount of long-CoT data (accuracy 80.2%) from QwQ-32B-Preview (Team, 2024a). More importantly, S2R employs both outcomelevel and process-level reinforcement learning (RL) to further enhance the LLMs self-verifying and self-correcting capabilities. Using only rule-based reward models, RL improves the validity of both the self-verification and self-correction process, allowing the models to perform more flexible and effective test-time scaling through self-directed trial-and-error process. By comparing outcomelevel and process-level RL for our task, we found that process-level supervision is particularly effective in boosting accuracy of the thinking skills at intermediate steps, which might benefit base models with limited reasoning abilities. In contrast, outcome-level supervision enables models explore more flexible trial-and-error paths towards the correct final answer, leading to consistent improvement in the reasoning abilities of more capable base models. Additionally, we further show the potential of offline reinforcement learning as more efficient alternative to the online RL training. We conducted extensive experiments across LLMs on 7 math reasoning benchmarks. Experimental results demonstrate that S2R outperforms competitive baselines in math reasoning, including recently-released advanced o1-like models Eurus2-7B-PRIME (Cui et al., 2025), rStar-Math-7B (Guan et al., 2025) and Qwen2.5-7B-SimpleRL (Zeng et al., 2025). We also found that S2R is generalizable to out-of-domain general tasks, such as MMLU-PRO, highlighting the validity of the learned self-verifying and self-correcting abilities. Additionally, we conducted series of analytical experiments to better demonstrate the reasoning mechanisms of the obtained models, and provide insights into performing online and offline RL training for enhancing LLM reasoning."
        },
        {
            "title": "2 Methodology",
            "content": "The main idea behind teaching LLMs selfverification and self-correction abilities is to streamline deep thinking into critical paradigm: selfdirected trial-and-error with self-verification and self-correction. Specifically: (1) LLMs are allowed to explore any potential (though possibly incorrect) solutions, especially when tackling difficult problems; (2) during the process, self-verification is essential for detecting mistakes on-the-fly; (3) selfcorrection enables the model to fix detected mistakes. This paradigm forms an effective test-time scaling approach that is more accessible for less powerful base models and is generalizable across various tasks. In this section, we first formally define the problem (2.1). Next, we present the two-stage training framework of S2R, as described in Figure 2: Stage 1: Behavior Initialization: We first construct dynamic self-verifying and self-correcting trial-and-error trajectories to initialize the desired behavior. Then, we apply supervised fine-tuning (SFT) to the initial policy models using these trajectories, resulting in behavior-initialized policy models (2.2); Stage 2: Reinforcement Learning: Following behavior initialization, we employ reinforcement learning to further enhance the self-verifying and self-correcting capabilities of the policy models. We explore both outcome-level and process-level RL methods, as well as their offline versions (2.3). 2.1 Problem Setup We formulate the desired LLM reasoning paradigm as sequential decision-making process under 2 Figure 2: Overview of S2R. reinforcement learning framework. Given problem x, the language model policy π is expected to generate sequence of interleaved reasoning actions = (a1, a2, , aT ) until reaching the termination action <end>. We represent the series of actions before an action at as y:at, i.e., y:at = (a1, a2, , ati), where at is excluded. The number of tokens in is denoted as y, and the total number of actions in is denoted as ya. We restrict the action space to three types: solve, verify, and <end>, where solve actions represent direct attempts to solve the problem, verify actions correspond to selfassessments of the preceding solution, and <end> actions signal the completion of the reasoning process. We denote the type of action ai as ype(), where ype(ai) {verify, solve, <end>}. We expect the policy to learn to explore new solutions by generating solve actions, to self-verify the correctness of preceding solutions with verify actions, and to correct the detected mistakes with new solve actions if necessary. Therefore, for each action ai, the type of the next action ai+1 is determined by the following rules: ype(ai+1) = verify, ype(ai) = solve ype(ai) = verify solve, and Parser(ai) = INCORRECT ype(ai) = verify and Parser(ai) = CORRECT <end>, Here, arser(a) {CORRECT, INCORRECT} (for any action where ype(a) = verify ) is 3 function (e.g., regex) that converts the models free-form verification text into binary judgments. For simplicity, we denote the j-th solve action as sj and the j-th verify action as vj. Then we have = (s1, v1, s2, v2, , sk, vk, <end>). 2.2 Initializing Self-verification and Self-correction Behaviors 2.2.1 Learning Valid Self-verification Learning to perform valid self-verification is the most crucial part in S2R, as models can make mistakes during trial-and-error, and recognizing intermediate mistakes is critical for reaching the correct answer. In this work, we explore two methods for constructing self-verification behavior. Problem-Solving Verification The most intuitive method for verification construction is to directly query existing models to generate verifications on the policy models responses, and then filter for valid verifications. By querying existing models using different prompts, we found that existing models tend to perform verification in Problem-Solving manner, i.e., by re-solving the problem and checking whether the answer matches the given one. We refer to this kind of verification as Problem-Solving Verification. Confirmative Verification \"Problem-solving\" verification is intuitively not the ideal verification behavior we seek. Ideally, we expect the model to think outside the box and re-examine the solution from new perspective, rather than thinking from the same problem-solving view for verification. We refer to this type of verification behavior as Confirmative Verification. Specifically, we construct Confirmative Verification by prompting existing LLMs to \"verify the correctness of the answer without re-solving the problem\", and filtering out invalid verifications using LLM-as-ajudge. The detail implementation can be found in Appendix A.1. 2.2.2 Learning Self-correction Another critical part of S2R is enabling the model to learn self-correction. Inspired by Kumar et al. (2024) and Snell et al. (2024b), we initialize the self-correcting behavior by concatenating series of incorrect solutions (each followed by verification recognizing the mistakes) with final correct solution. As demonstrated by Kumar et al. (2024), LLMs typically fail to learn valid selfcorrection behavior through SFT, but the validity of self-correction can be enhanced through reinforcement learning. Therefore, we only initialize the self-correcting behavior at this stage, leaving further enhancement of the self-correcting capabilities to the RL stage. 2.2.3 Constructing Dynamic Trial-and-Error Trajectory We next construct the complete trial-and-error trajectories for behavior initialization SFT, following three principles: To ensure the diversity of the trajectories, we construct trajectories of various lengths. Specifically, we cover {1, 2, 3, 4} for = (a1, , a2k) = (s1, v1, , sk, vk) in the trajectories. To ensure that the LLMs learn to verify and correct their own errors, we construct the failed trials in each trajectory by sampling and filtering from the LLMs own responses. As plausible test-time scaling method allocates reasonable effort to varying levels of problems, it is important to ensure the trial-and-error trajectories align with the difficulty level of problems. Specifically, more difficult problems will require more trial-and-error iterations before reaching the correct answer. Thus, we determine the length of each trajectory based on the accuracy of the sampled responses for each base model. 2.2.4 Supervised Fine-tuning for Thinking Behavior Initialization Once the dynamic self-verifying and selfcorrecting training data DSF is ready, we optimize the policy π for thinking behavior initialization by minimizing the following objective: = E(x,y)DSF δmask(at) log π(at x, y:at ) (1) where the mask function δmask(at) for action at in = (a1, , aT ) is defined as: aty (cid:88) δmask(at) = if ype(at) = verify if ype(at) = solve and = if ype(at) = <end> and = 1, 1, 1, 0, otherwise 1 That is, we optimize the probability of all verifications and only the last correct solution sN by using masks during training. 2.3 Boosting Thinking Capabilities via Reinforcement Learning After Stage 1, we initialized the policy model π with self-verification and self-correction behavior, obtaining πSF . We then explore further enhancing these thinking capabilities of πSF via reinforcement learning. Specifically, we explore two simple RL algorithms: the outcome-level REINFORCE Leave-One-Out (RLOO) algorithm and proces-level group-based RL algorithm. 2.3.1 Outcome-level RLOO We first introduce the outcome-level REINFORCE Leave-One-Out (RLOO) algorithm (Ahmadian et al., 2024; Kool et al., 2019) to further enhance the self-verification and self-correction capabilities of πSF . Given problem and the response = (s1, v1, ..., sT , vT ), we define the reward function Ro(x, y) based on the correctness of the last solution sT : Ro(x, y) = (cid:40) 1, 1, Vgolden(sT ) = correct otherwise Here Vgolden() {correct, incorrect} represents ground-truth validation by matching the golden answer with the given solution. We calculate the advantage of each response using an estimated baseline and KL reward shaping as follows: A(x, y) = Ro(x, y) ˆb β log πθold(yx) πref (yx) (2) 4 where β is the KL divergence regularization coefficient, and πref is the reference policy (in our case, πSF ). ˆb(x, y(m)) = 1 .Ro(x, y(j)) (cid:80) 1 j=1,...,M j=m is the baseline estimation of RLOO, which represents the leave-one-out mean of sampled outputs {y(1), ...y(M )} for each input x, serving as baseline estimation for each y(m). Then, we optimize the policy πθ by minimizing the following objective after each sampling episode based on πθold: L(θ) = xD yπθold (x) (cid:20) min (cid:0)r(θ)A(x, y), clip(cid:0)r(θ), 1 ϵ, 1 + ϵ(cid:1)A(x, y)(cid:1) (cid:21) (3) where r(θ) = πθ(yx) πθold (yx) is the probability ratio. When implementing the above loss function, we treat as complete trajectory sampled with an input problem x, meaning we optimize the entire trajectory with outcome-level supervision. With this approach, we aim to incentivize the policy model to explore more dynamic self-verification and self-correcting trajectories on its own, which has been demonstrated as an effective practice in recent work (Guo et al., 2025; Team et al., 2025). 2.3.2 Process-level Group-based RL Process-level supervision has demonstrated effectiveness in math reasoning (Lightman et al., 2023a; Wang et al., 2024c). Since the trajectory of S2R thinking is naturally divided into self-verification and self-correction processes, it is intuitive to adopt process-level supervision for RL training. Inspired by RLOO and process-level GRPO (Shao et al., 2024), we designed group-based process-level optimization method. Specifically, we regard each action in the output trajectory as sub-process and define the action level reward function Ra(a x, y:a) based on the action type. For each solve action sj, we expect the policy to generate the correct solution; for each verify action vj, we expect the verification to align with the actual solution validity. The corresponding rewards are defined as follows: Ra(sj x, y:sj ) = (cid:26) 1, 1, Vgolden(sj) = correct otherwise Ra(vj x, y:vj ) = (cid:26) 1, 1, arser(vj) = Vgolden(sj) otherwise To calculate the advantage of each action at, we estimate the baseline as the average reward of the group of actions sharing the same reward context: R(at x, y) = (Ra(ai x, y:ai))t1 i=1 which is defined as the reward sequence of the previous actions y:at of each action at. We denote the set of actions sharing the same reward context R(at x, y) as G(R(at x, y)). Then the baseline can be estimated as follows: ˆb(at x, y) = 1 G(R(atx, y)) (cid:88) Ra(ax(a), y(a) :a ) aG(R(atx,y)) And the advantage of each action at is: A(at x, y) =Ra(at x, y:at) ˆb(at x, y) β log πθold(at x, y) πref(at x, y) (4) (5) The main idea of the group-based baseline estimation is that the actions sharing the same reward context are provided with similar amounts of information before the action is taken. For instance, all actions sharing reward context consisting of one failed attempt and one successful verification (i.e., R(atx, y) = (1, 1)) are provided with the information about the problem, failed attempt, and the reassessment on the failure. Given the same amount of information, it is reasonable to estimate baseline using the average reward of these actions. Putting it all together, we minimize the following surrogate loss function to update the policy parameters θ, using trajectories collected from πold: (θ) = xD yπθold (x)(cid:20) 1 clip ra(θ), 1 ϵ, 1 + ϵ (cid:0) (cid:1) ay (cid:88) A(a min ra(θ)A(a x, y:a), (cid:0) x, y:a) (cid:21) (cid:1) (6) where ra(θ) = πθ(ax,y:a) (ax,y:a) is the importance ratio. πθold 2.4 More Efficient Training with Offline RL While online RL is known for its high resource requirements, offline RL, which does not require real-time sampling during training, offers more efficient alternative for RL training. Additionally, offline sampling allows for more accurate baseline Stage 1: Behavior Initialization Base Model Llama-3.1-8B-Instruct Qwen2-7B-Instruct Qwen2.5-Math-7B Source MATH MATH MATH # Training Data 4614 4366 3111 Stage 2: Reinforcement Learning Base Model Llama-3.1-8B-Instruct Qwen2-7B-Instruct Qwen2.5-Math-7B Source MATH+GSM8K MATH+GSM8K MATH+OpenMath2.0 # Training Data 9601 9601 10000 Table 1: Training data statistics. calculations with better trajectories grouping for each policy. As part of our exploration into more efficient RL training in S2R framework, we also experimented with offline RL to assess its potential in further enhancing the models thinking abilities. In Appendix D.2, we include more details and formal definition for offline RL training."
        },
        {
            "title": "3 Experiment",
            "content": "To verify the effectiveness of the proposed method, we conducted extensive experiments across 3 different base policy models on various benchmarks. 3.1 Experiment Setup Base Models To evaluate the general applicability of our method across different LLMs, we conducted experiments using three distinct base models: Llama-3.1-8B-Instruct (Dubey et al., 2024), Qwen2-7B-Instruct (qwe, 2024), and Qwen2.5Math-7B (Qwen, 2024). Llama-3.1-8B-Instruct and Qwen2-7B-Instruct are versatile generalpurpose models trained on diverse domains without specialized focus on mathematical reasoning. In contrast, Qwen2.5-Math-7B is state-of-theart model specifically tailored for mathematical problem-solving and has been widely adopted in recent research on math reasoning (Guan et al., 2025; Cui et al., 2025; Zeng et al., 2025). Training Data Setup For Stage 1: Behavior Initialization, we used the widely adopted MATH (Hendrycks et al., 2021a) training set for dynamic trial-and-error data collection 1. For each base model, we sampled 5 responses per problem in the training data. After data filtering and sampling, we constructed dynamic trial-and-error training set consisting of 3k-4k instances for each base model. Detailed statistics of the training set are shown 1We use the MATH split from Lightman et al. (2023a), i.e., 12000 problems for training and 500 problems for testing. in Table 1. For Stage 2: Reinforcement Learning, we used the MATH+GSM8K (Cobbe et al., 2021a) training data for RL training on the policy πSF initialized from Llama-3.1-8B-Instruct and Qwen2-7B-Instruct. Since Qwen2.5-math-7b already achieves high accuracy on the GSM8K training data after Stage 1, we additionally include training data randomly sampled from the OpenMath2 dataset (Toshniwal et al., 2024). Following (Cui et al., 2025), we filter out excessively easy or difficult problems based on each πSF from Stage 1 to enhance the efficiency and stability of RL training, resulting in RL training sets consisting of approximately 10000 instances. Detailed statistics of the final training data can be found in Table 1. Additional details on training data construction can be found in in Appendix A.1. Baselines We benchmark our proposed method against four categories of strong baselines: Frontier LLMs includes cutting-edge proprietary models such as GPT-4o, the latest Claude, and OpenAIs o1-preview and o1-mini. Top-tier open-source reasoning models covers state-of-the-art open-source models known for their strong reasoning capabilities, including Mathstral-7B-v0.1 (Team, 2024b), NuminaMath72B (LI et al., 2024), LLaMA3.1-70B-Instruct (Dubey et al., 2024), and Qwen2.5-Math-72BInstruct (Yang et al., 2024). Enhanced models built on Qwen2.5-Math-7B: Given the recent popularity of Qwen2.5-Math-7B as base policy model, we evaluate S2R against three competitive baselines that have demonstrated superior performance based on Qwen2.5Math-7B: Eurus-2-7B-PRIME (Cui et al., 2025), rStar-Math-7B (Guan et al., 2025), and Qwen2.57B-SimpleRL (Zeng et al., 2025). These models serve as direct and strong baseline for our Qwen2.5-Math-7B-based variants. SFT with different CoT constructions: We also compare with training on competitive types of CoT reasoning, including the original CoT solution in the training datasets, and Long-CoT solutions distilled from QwQ-32B-Preview (Team, 2024a), widely adopted open-source o1-like model (Chen et al., 2024c; Guan et al., 2025; Zheng et al., 2024). Specifically, to ensure fair comparison between behavior initialization with long-CoT and S2R, we use long-CoT data of the same size as our behavior initialization data. We 6 Datasets MATH 500 AIME 2024 AMC 2023 College Math Olympiad Bench GSM8K GaokaoEn 2023 Average Model Frontier LLMs GPT-4o Claude3.5-Sonnet GPT-o1-preview GPT-o1-mini Top-tier Open-source Reasoning LLMs Mathstral-7B-v0.1 NuminaMath-72B-CoT LLaMA3.1-70B-Instruct Qwen2.5-Math-72B-Instruct 76.6 78.3 85.5 90.0 57.8 64.0 65.4 85.6 General Model: Llama-3.1-8B-Instruct 48.0 Llama-3.1-8B-Instruct Llama-3.1-8B-Instruct + Original Solution SFT 31.0 51.4 Llama-3.1-8B-Instruct + Long CoT SFT Llama-3.1-8B-S2 R-BI (ours) 49.6 Llama-3.1-8B-S2 R-PRL (ours) 53.6 Llama-3.1-8B-S2 R-ORL (ours) 55.0 General Model: Qwen2-7B-Instruct Qwen2-7B-Instruct Qwen2-7B-Instruct + Original Solution SFT Qwen2-7B-Instruct + Long CoT SFT Qwen2-7B-S2 R-BI (ours) Qwen2-7B-S2 R-PRL (ours) Qwen2-7B-S2 R-ORL (ours) Math-Specialized Model: Qwen2.5-Math-7B Qwen2.5-Math-7B Qwen2.5-Math-7B-Instruct Eurus-2-7B-PRIME(Cui et al., 2025) rStar-Math-7B2(Guan et al., 2025) Qwen2.5-7B-SimpleRL(Zeng et al., 2025) Qwen2.5-Math-7B + Original Solution SFT Qwen2.5-Math-7B + Long CoT SFT Qwen2.5-Math-7B-S2 R-BI (ours) Qwen2.5-Math-7B-S2 R-PRL (ours) Qwen2.5-Math-7B-S2 R-ORL (ours) 51.2 41.2 60.4 61.2 65.4 64. 51.0 83.2 79.2 78.4 82.4 58.0 80.2 81.6 83.4 84.4 9.3 16.0 44.6 56.7 0.0 3.3 23.3 30.0 6.7 3.3 6.7 10.0 6.7 6.7 3.3 0.0 6.7 3.3 6.7 3.3 16.7 13.3 26.7 26.7 26.7 6.7 16.7 23.3 26.7 23. 47.5 - 90.0 95.0 37.5 70.0 50.0 70.0 30.0 7.5 27.5 20.0 25.0 32.5 30.0 25.0 32.5 27.5 35.0 42.5 45.0 72.5 57.8 47.5 62.5 42.5 60.0 60.0 70.0 77.5 48.5 - - 57. 33.7 39.7 42.5 49.5 30.8 22.0 36.3 33.3 33.7 34.7 18.2 30.1 36.3 41.1 36.7 34.7 21.5 47.0 45.0 52.5 - 35.8 49.6 43.9 43.8 43.8 43.3 - - 65.3 21.5 32.6 27.7 49. 15.6 8.0 19.0 17.6 18.5 20.7 19.1 10.2 23.4 27.1 27.0 26.2 16.7 40.4 42.1 47.1 43.3 20.0 42.1 44.4 46.4 44.9 92.9 96.4 - 94.8 84.9 90.8 94.1 95.9 84.4 58.7 87.0 85.3 86.7 87. 86.4 74.5 81.2 87.4 89.0 86.4 58.3 95.6 88.0 89.7 - 79.5 91.4 91.9 93.2 92.9 67.5 - - 78.4 46.0 58.4 54.0 71.9 41.0 28.3 48.3 41.0 43.1 45.2 39.0 34.8 53.5 49.1 49.9 50. 39.7 67.5 57.1 65.7 - 51.9 69.1 70.1 70.4 70.1 55.1 - - 76.9 40.2 51.3 51.0 64.6 36.6 22.7 39.5 36.7 38.2 40.3 35.3 30.8 42.0 42.4 44.2 44.1 35.6 59.9 56.6 58.2 - 42.1 58.4 59.3 62.0 62. Table 2: The performance of S2R and other strong baselines on the most challenging math benchmarks is presented. BI refers to the behavior-initialized models through supervised fine-tuning, ORL denotes models trained with outcome-level RL, and PRL refers to models trained with process-level RL. The highest results are highlighted in bold and the second-best results are marked with underline. For some baselines, we use the results from their original reports or from Guan et al. (2025), denoted by . provide more details on the baseline data construction in Appendix A.2.3. More details on the baselines are included in Appendix A.2. Evaluation Datasets We evaluate the proposed method on 7 diverse mathematical benchmarks. To ensure comprehensive evaluation, in addition to the in-distribution GSM8K (Cobbe et al., 2021b) and MATH500 (Lightman et al., 2023a) test sets, we include challenging out-of-distribution benchmarks covering various difficulty levels and mathematical domains, including the AIME 2024 competition problems (AI-MO, 2024a), the AMC 2023 exam (AI-MO, 2024b), the advanced reasoning tasks from Olympiad Bench (He et al., 2024), and college-level problem sets from College Math (Tang et al., 2024a). Additionally, we assess performance on real-world standardized tests, the GaoKao (Chinese College Entrance Exam) En 2023 (Liao et al., 2024). detailed description of these datasets is provided in Appendix B.1. Evaluation Metrics We report Pass@1 accuracy for all baselines. For inference, we employ vLLM (Kwon et al., 2023) and develop evaluation scripts based on Qwen Maths codebase. All evaluations are performed using greedy decoding. Details of the prompts used during inference are provided in 2To ensure fair comparison, we report the Pass@1 (greedy) accuracy obtained without the process preference model of rStar, rather than the result obtained with increased test-time computation using 64 trajectories. 7 Appendix A.3. All implementation details, including hyperparameter settings, can be found in Appendix B.2. 3.2 Main Results Table 2 shows the main results of S2R compared with baseline methods. We can observe that: (1) S2R consistently improves the reasoning abilities of models across all base models. Notably, on Qwen2.5-Math-7B, the proposed method improves the base model by 32.2% on MATH500 and by 34.3% on GSM8K. (2) Generally, S2R outperforms the baseline methods derived from the same base models across most benchmarks. Specifically, on Qwen2.5-Math-7B, S2R surpasses several recently proposed competitive baselines, such as Eurus2-7B-PRIME, rStar-Math-7B and Qwen2.5-7BSimpleRL. While Eurus-2-7B-PRIME and rStarMath-7B rely on larger training datasets (Figure 1) and require more data construction and reward modeling efforts, S2R only needs linear sampling efforts for data construction, 10k RL training data and rule-based reward modeling. These results highlight the efficiency of S2R. (3) With the same scale of SFT data, S2R also outperforms the long-CoT models distilled from QwQ-32BPreview, demonstrating that learning to self-verify and self-correct is an effective alternative to longCoT for test-time scaling in smaller LLMs. Comparing process-level and outcome-level RL, we find that outcome-level RL generally outperforms process-level RL across the three models. This is likely because outcome-level RL allows models to explore trajectories without emphasizing intermediate accuracy, which may benefit enhancing long-thought reasoning in stronger base models like Qwen2.5-Math-7B. In contrast, process-level RL, which provides guidance for each intermediate verification and correction step, may be effective for models with lower initial capabilities, such as Qwen2-7B-Instruct. As shown in Figure 3, processlevel RL can notably enhance the verification and correction abilities of Qwen2-7B-S2R-BI. 3.3 Generalizing to Cross-domain Tasks Despite training on math reasoning tasks, we found that the learned self-verifying and self-correcting capability can also generalize to out-of-distribution In Table 3, we evaluate the general domains. SFT model and the outcome-level RL model based on Qwen2.5-Math-7B on four cross-domain tasks: FOLIO (Han et al., 2022) on logical reasoning, Model Qwen2.5-Math-72B-Instruct Llama-3.1-70B-Instruct OpenMath2-Llama3.1-70B QwQ-32B-Preview Eurus-2-7B-PRIME Qwen2.5-Math-7B-Instruct Qwen2.5-Math-7B Qwen2.5-Math-7B-S2R-BI (ours) Qwen2.5-Math-7B-S2R-ORL (ours) FOLIO CRUXEval StrategyQA MMLUProSTEM 69.5 65.0 68.5 84.2 56.7 61.6 37.9 58.1 61.6 68.6 59.6 35.1 65.2 50.0 28.0 40.8 48.0 50. 94.3 88.8 95.6 88.2 79.0 81.2 61.1 88.7 90.8 66.0 61.7 55.0 71.9 53.7 44.7 46.0 49.8 50.0 Table 3: Performance of the proposed method and the baseline methods on 4 cross-domain tasks. The results with are reported by Shen et al. (2025). CRUXEval (Gu et al., 2024) on code reasoning, StrategyQA (Geva et al., 2021) on multi-hop reasoning and MMLUPro-STEM on multi-task complex understanding (Wang et al., 2024d; Shen et al., 2025), with details of these datasets provided in Appendix B.1. The results show that after learning to self-verify and self-correct, the proposed method effectively boosts the base models performance across all tasks and achieves comparative results to the baseline models. These findings indicate that the learned self-verifying and self-correcting capabilities are general thinking skills, which can also benefit reasoning in general domains. Additionally, we expect that the performance in specific domains can be further improved by applying S2R training on domain data with minimal reward model requirements (e.g., rule-based or LLM-asa-judge). For better illustration, we show cases on how the trained models perform self-verifying and self-correcting on general tasks in Appendix E. 3.4 Analyzing Self-verification and Self-correction Abilities In this section, we conduct analytical experiments on the models self-verification and self-correction capabilities from various perspectives. 3.4.1 Problem-solving v.s. Confirmative Verification We first compare the Problem-solving and Confirmative Verification methods described in 2.2.1. In Table 4, we present the verification results of different methods on the Math500 test set. We report the overall verification accuracy, as well as the initial verification accuracy when the initial answer is correct (Vgolden(s0) = correct) and incorrect (Vgolden(s0) = incorrect), respectively. We observe from the table that: (1) Generally, problem-solving verification achieves superior overall accuracy compared to confirmative verification. This result is intuitive, as existing models 8 Base Model Methods Overall Verification Acc. Initial Verification Acc. Vgolden(s0) = correct Vgolden(s0) = incorrect Llama3.1-8B-Instruct Qwen2-7B-Instruct Qwen2.5-Math-7B Problem-solving Confirmative Problem-solving Confirmative Problem-solving Confirmative 80.10 65. 73.28 58.31 77.25 61.58 87.28 77. 90.24 76.16 91.21 82.80 66.96 78. 67.37 70.05 56.67 68.04 Table 4: Comparison of problem-solving and confirmative verification. are trained for problem-solving, and recent studies have highlighted the difficulty of existing LLMs in performing reverse thinking (Berglund et al., 2023; Chen et al., 2024b). During data collection, we also found that existing models tend to verify through problem-solving, even when prompted to verify without re-solving (see Table 6 in Appendix A.1). (2) In practice, accuracy alone does not fully reflect the validity of method. For example, when answer accuracy is sufficiently high, predicting all answers as correct will naturally lead to high verification accuracy, but this is not desired behavior. By further examining the initial verification accuracy for both correct and incorrect answers, we found that problem-solving verification exhibits notable bias toward predicting answers as correct, while the predictions from confirmative verification are more balanced. We deduce that this bias arises might be because problem-solving verification is more heavily influenced by the preceding solution, aligning with previous studies showing that LLMs struggle to identify their own errors (Huang et al., 2023; Tyen et al., 2023). In contrast, confirmative verification performs verification from different perspectives, making it less influenced by the LLMs preceding solution. In all experiments, we used confirmative verification for behavior initialization. 3.4.2 Boosting Self-verifying and Self-correcting with RL In this experiment, we investigate the effect of RL training on the models self-verifying and selfcorrecting capabilities. We assess self-verification using the following metrics: (1) Verification Accuracy: The overall accuracy of verification predictions, as described in 3.4.1. (2) Error Recall: The recall of verification when the preceding answers are incorrect. (3) Correct Precision: The precision of verification when it predicts the answers as correct. Both Error Figure 3: Evaluation on verification and correction. Recall and Correct Precision directly affect the final answer accuracy: if verification fails to detect an incorrect answer, or if it incorrectly predicts an answer as correct, the final answer will be wrong. For self-correction, we use the following metrics: (1) Incorrect to Correct Rate: the rate at which the model successfully corrects an incorrect initial answer to correct final answer. (2) Correct to Incorrect Rate: the rate at which the model incorrectly changes correct initial answer to an incorrect final answer. We provide the formal definitions of the metrics used in Appendix C. In Figure 3, we present the results of the behavior-initialized model (SFT) and different RL models obtained from Qwen2.5-Math-7B. We observe that: (1) Both RL methods effectively enhance self-verification accuracy. The process-level RL shows larger improvement on accuracy, while the outcome-level RL consistently improves Error Recall and Correct Precision. This might be because process-level supervision indiscriminately promotes verification accuracy in intermediate steps, while outcome-level supervision allows the policy model to explore freely in intermediate steps and only boosts the final answer accuracy, thus mainly enhancing Error Recall and Correct Precision (which directly relate to final answer accuracy). (2) Both RL methods can successfully enhance the models self-correction capability. Notably, the models ability to correct incorrect answers is significantly improved after RL training. The rate of model mistakenly altering correct answers is also notably reduced. This comparison demonstrates that S2R can substantially enhance the validity of models self-correction ability. 9 Model Datasets MATH 500 AIME 2024 AMC 2023 College Math Olympiad Bench GSM8K GaokaoEn 2023 Average General Model: Qwen2-7B-Instruct Qwen2-7B-Instruct Qwen2-7B-S2 R-BI (ours) Qwen2-7B-S2 R-PRL (ours) Qwen2-7B-S2 R-ORL (ours) Qwen2-7BInstruct-S2 R-PRL-offline (ours) Qwen2-7B-Instruct-S2 R-ORL-offline (ours) Math-Specialized Model: Qwen2.5-Math-7B Qwen2.5-Math-7B Qwen2.5-Math-7B-S2 R-BI (ours) Qwen2.5-Math-7B-S2 R-PRL (ours) Qwen2.5-Math-7B-S2 R-ORL (ours) Qwen2.5-Math-7B-S2 R-PRL-offline (ours) Qwen2.5-Math-7B-S2 R-ORL-offline (ours) 51.2 61.2 65.4 64.8 61.6 61.0 51.0 81.6 83.4 84.4 83.4 82.0 3.3 3.3 6.7 3.3 10.0 6.7 16.7 23.3 26.7 23.3 23.3 20.0 30.0 27.5 35.0 42.5 32.5 37. 45.0 60.0 70.0 77.5 62.5 67.5 18.2 41.1 36.7 34.7 40.2 40.5 21.5 43.9 43.8 43.8 50.0 49.8 19.1 27.1 27.0 26.2 26.5 27.3 16.7 44.4 46.4 44.9 46.7 45.8 86.4 87.4 89.0 86.4 87.6 87. 58.3 91.9 93.2 92.9 92.9 92.6 39.0 49.1 49.9 50.9 50.4 49.6 39.7 70.1 70.4 70.1 72.2 70.4 35.3 42.4 44.2 44.1 44.1 44.3 35.6 59.3 62.0 62.4 61.6 61.2 Table 5: Comparison of S2R using online and offline RL training. 3.5 Exploring Offline RL As described in 2.4, we explore offline RL as more efficient alternative to online RL training, given the effectiveness of offline RL has been demonstrated in recent studies (Baheti et al., 2023; Cheng et al., 2025; Wang et al., 2024b). Table 5 presents the results of offline RL with process-level and outcome-level supervision, compared to online RL. We can observe that: (1) Different from online RL, process-level supervision outperforms outcome-level supervision in offline RL training. This interesting phenomenon may be due to: a) Outcome-level RL, which excels at allowing models to freely explore dynamic trajectories, is more suitable for on-the-fly sampling during online parameter updating. b) In contrast, process-level RL, which requires accurate baseline estimation for intermediate steps, benefits from offline trajectory sampling, which can provide more accurate baseline estimates with larger scale data sampling. (2) Offline RL consistently improves performance over the behavior-initialized models across most benchmarks and achieves comparable results to online RL. These results highlight the potential of offline RL as more efficient alternative for enhancing LLMs deep reasoning."
        },
        {
            "title": "4 Related Work",
            "content": "4.1 Scaling Test-time Compute Scaling test-time compute recently garners wide attention in LLM reasoning (Snell et al., 2024b; Wu et al., 2024; Brown et al., 2024). Existing studies have explored various methods for scaling up test-time compute, including: (1) AggregationFigure 4: The accuracy and average trial number of different models across difficulty levels. Evaluated on MATH500 test set. Improvement across Difficulty Levels 3.4.3 To further illustrate the effect of S2R training, Figure 4 shows the answer accuracy and average number of trials (i.e., the average value of \"K\" across all = (s1, v1, , sK, vK) under each difficulty level) for the SFT and SFT+RL models. We observe that: (1) By learning to self-verify and selfcorrect during reasoning, the models learn to dynamically allocate test-time effort. For easier problems, the models can reach confident answer with fewer trials, while for more difficult problems, they require more trials to achieve confident answer. (2) RL further improves test-time effort allocation, particularly for less capable model (e.g., Llama3.18B-Instruct). (3) After RL training, the answer accuracy for more difficult problems is notably improved, demonstrating the effectiveness of the self-verifying and self-correcting paradigm in enhancing the models reasoning abilities. based methods that samples multiple responses for each question and obtains the final answer with self-consistency (Wang et al., 2023) or by selecting best-of-N answer using verifier or reward model (Wang et al., 2024c; Zhang et al., 2024b; Lightman et al., 2023b; Havrilla et al., 2024b); (2) Searchbased methods that apply search algorithms such as Monte Carlo Tree Search (Tian et al., 2024; Wang et al., 2024a; Zhang et al., 2024a; Qi et al., 2024), beam search (Snell et al., 2024b), or other effective algorithms (Feng et al., 2023; Yao et al., 2023) to search for correct trajectories; (3) Iterative-refinebased methods that iteratively improve test performance through self-refinement (Madaan et al., 2024a; Shinn et al., 2024; Chen et al., 2024a, 2025). Recently, there has been growing focus on training LLMs to perform test-time search on their own, typically by conducting longer and deeper thinking (OpenAI, 2024; Guo et al., 2025). These test-time scaling efforts not only directly benefit LLM reasoning, but can also be integrated back into training time, enabling iterative improvement for LLM reasoning (Qin et al., 2024; Feng et al., 2023; Snell et al., 2024b; Luong et al., 2024). In this work, we also present an efficient framework for training LLMs to perform effective test-time scaling through self-verification and self-correction iterations. This approach is achieved without extensive efforts, and the performance of S2R can also be consistently promoted via iterative training. 4.2 Self-verification and Self-correction to perform effective Enabling LLMs selfverification and self-correction is promising solution for achieving robust reasoning for LLMs (Madaan et al., 2024b; Shinn et al., 2023; Paul et al., 2023; Lightman et al., 2023a), and these abilities are also critical for performing deep reasoning. Previous studies have shown that direct prompting of LLMs for self-verification or self-correction is suboptimal in most scenarios (Huang et al., 2023; Tyen et al., 2023; Ma et al., 2024; Zhang et al., 2024c). As result, recent studies have explored various approaches to enhance these capabilities during post-training (Saunders et al., 2022; Rosset et al., 2024; Kumar et al., 2024). These methods highlight the potential of using human-annotated or LLM-generated data to equip LLMs with self-verification or self-correction capabilities (Zhang et al., 2024d; Jiang et al., 2024), while also indicating that behavior imitation via supervised fine-tuning alone is insufficient for achieving valid self-verification or self-correction (Kumar et al., 2024; Qu et al., 2025; Kamoi et al., 2024). In this work, we propose effective methods to enhance LLMs self-verification and self-correction abilities through principled imitation data construction and RL training, and demonstrate the effectiveness of our approach with in-depth analysis. 4.3 RL for LLM Reasoning Reinforcement learning has proven effective in enhancing LLM performance across various tasks (Ziegler et al., 2019; Stiennon et al., 2020; Bai et al., 2022; Ouyang et al., 2022; Setlur et al., 2025). In LLM reasoning, previous studies typically employ RL in an actor-critic framework (Lightman et al., 2024; Tajwar et al., 2024; Havrilla et al., 2024a), and research on developing accurate reward models for RL training has been long-standing focus, particularly in reward modeling for Processlevel RL (Lightman et al., 2024; Setlur et al., 2024, 2025; Luo et al., 2024). Recently, several studies have demonstrate that simplified reward modeling and advantage estimation (Ahmadian et al., 2024; Shao et al., 2024; Team et al., 2025; Guo et al., 2025) in RL training can also effectively enhance LLM reasoning. Recent advances in improving LLMs deep thinking (Guo et al., 2025; Team et al., 2025) further highlight the effectiveness of utilizing unhackable rewards (Gao et al., 2023; Everitt et al., 2021) to consistently enhance LLM reasoning. In this work, we also show that simplified advantage estimation and RL framework enable effective improvements on LLM reasoning. Additionally, we conducted an analysis on process-level RL, outcome-level RL and offline RL, providing insights for future work in RL for LLM reasoning."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we propose S2R, an efficient framework for enhancing LLM reasoning by teaching LLMs to iteratively self-verify and self-correct during reasoning. We introduce principled approach for behavior initialization, and explore both outcome-level and process-level RL to further strengthen the models thinking abilities. Experimental results across three different base models on seven math reasoning benchmarks demonstrate that S2R significantly enhances LLM reasoning with minimal resource requirements. Since selfverification and self-correction are two crucial abil11 ities for LLMs deep reasoning, S2R offers an interpretable framework for understanding how SFT and RL enhance LLMs deep reasoning. It also offers insights into the selection of RL strategies for enhancing LLMs long-CoT reasoning."
        },
        {
            "title": "References",
            "content": "2024. Qwen2 technical report. Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, and Sara Hooker. 2024. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint arXiv:2402.14740. AI-MO. 2024a. Aime 2024. AI-MO. 2024b. Amc 2023. Ashutosh Baheti, Ximing Lu, Faeze Brahman, Ronan Le Bras, Maarten Sap, and Mark Riedl. 2023. Leftover lunch: Advantage-based offline reinforcement learning for language models. arXiv preprint arXiv:2305.14718. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 2022. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862. Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans. 2023. The reversal curse: Llms trained on\" is b\" fail to learn\" is a\". arXiv preprint arXiv:2309.12288. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc Le, Christopher Ré, and Azalia Mirhoseini. 2024. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787. Jiefeng Chen, Jie Ren, Xinyun Chen, Chengrun Yang, Ruoxi Sun, and Sercan Ö Arık. 2025. Sets: Leveraging self-verification and self-correction for improved test-time scaling. arXiv preprint arXiv:2501.19306. Justin Chih-Yao Chen, Archiki Prasad, Swarnadeep Saha, Elias Stengel-Eskin, and Mohit Bansal. 2024a. Magicore: Multi-agent, iterative, coarseto-fine refinement for reasoning. arXiv preprint arXiv:2409.12147. Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, et al. 2024c. Do not think that much for 2+ 3=? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187. Pengyu Cheng, Tianhao Hu, Han Xu, Zhisong Zhang, Yong Dai, Lei Han, Xiaolong Li, et al. 2025. Selfplaying adversarial language game enhances llm reasoning. Advances in Neural Information Processing Systems, 37:126515126543. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021a. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021b. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al. 2025. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Tom Everitt, Marcus Hutter, Ramana Kumar, and Victoria Krakovna. 2021. Reward tampering problems and solutions in reinforcement learning: causal influence diagram perspective. Synthese, 198(Suppl 27):64356467. Xidong Feng, Ziyu Wan, Muning Wen, Ying Wen, Weinan Zhang, and Jun Wang. 2023. Alphazero-like tree-search can guide large language model decoding and training. arXiv preprint arXiv:2309.17179. Leo Gao, John Schulman, and Jacob Hilton. 2023. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pages 1083510866. PMLR. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did Aristotle Use Laptop? Question Answering Benchmark with Implicit Reasoning Strategies. Transactions of the Association for Computational Linguistics (TACL). Justin Chih-Yao Chen, Zifeng Wang, Hamid Palangi, Rujun Han, Sayna Ebrahimi, Long Le, Vincent Perot, Swaroop Mishra, Mohit Bansal, Chen-Yu Lee, et al. 2024b. Reverse thinking makes llms stronger reasoners. arXiv preprint arXiv:2411.19865. Alex Gu, Baptiste Rozière, Hugh Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida I. Wang. 2024. Cruxeval: benchmark for code reasoning, understanding and execution. arXiv preprint arXiv:2401.03065. 12 Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. 2025. rstar-math: Small llms can master math reasoning with self-evolved deep thinking. arXiv preprint arXiv:2501.04519. Ryo Kamoi, Yusen Zhang, Nan Zhang, Jiawei Han, and Rui Zhang. 2024. When can llms actually correct their own mistakes? critical survey of selfcorrection of llms. Transactions of the Association for Computational Linguistics, 12:14171440. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, et al. 2022. Folio: Natural language reasoning with firstorder logic. arXiv preprint arXiv:2209.00840. Alex Havrilla, Yuqing Du, Sharath Chandra Raparthy, Christoforos Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, Sainbayar Sukhbaatar, and Roberta Raileanu. 2024a. Teaching large language models to reason with reinforcement learning. arXiv preprint arXiv:2403.04642. Alex Havrilla, Sharath Raparthy, Christoforus Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, and Roberta Raileanu. 2024b. Glore: When, where, and how to improve llm reasoning via global and local refinements. arXiv preprint arXiv:2402.10963. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. 2024. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021a. Measuring mathematical problem solving with the math dataset. In Thirtyfifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2). Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021b. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874. Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. 2023. Large language models cannot self-correct reasoning yet. arXiv preprint arXiv:2310.01798. Huchen Jiang, Yangyang Ma, Chaofan Ding, Kexin Luan, and Xinhan Di. 2024. Towards intrinsic selfcorrection enhancement in monte carlo tree search boosted reasoning via iterative preference learning. arXiv preprint arXiv:2412.17397. Wouter Kool, Herke van Hoof, and Max Welling. 2019. Buy 4 reinforce samples, get baseline for free! Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, et al. 2024. Training language models to selfcorrect via reinforcement learning. arXiv preprint arXiv:2409.12917. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. 2024. Numina- [https://github.com/project-numina/ math. aimo-progress-prize](https://github.com/ project-numina/aimo-progress-prize/blob/ main/report/numina_dataset.pdf). Minpeng Liao, Wei Luo, Chengxi Li, Jing Wu, and Kai Fan. 2024. Mario: Math reasoning with code interpreter outputa reproducible pipeline. arXiv preprint arXiv:2401.08190. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023a. Lets verify step by step. arXiv preprint arXiv:2305.20050. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023b. Lets verify step by step. arXiv preprint arXiv:2305.20050. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2024. Lets verify step by step. In The Twelfth International Conference on Learning Representations. Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, et al. 2024. Improve mathematical reasoning in language models by automated process supervision. arXiv preprint arXiv:2406.06592. Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. 2024. Reft: Reasoning with reinforced fine-tuning. arXiv preprint arXiv:2401.08967. 13 Ruotian Ma, Xiaolei Wang, Xin Zhou, Jian Li, Nan Du, Tao Gui, Qi Zhang, and Xuanjing Huang. 2024. Are large language models good prompt optimizers? arXiv preprint arXiv:2402.02101. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. 2024a. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. 2024b. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36. OpenAI. 2024. Openai o1 system card. preprint. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744. Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and Boi Faltings. 2023. Refiner: Reasoning feedback arXiv preprint on intermediate representations. arXiv:2304.01904. Zhenting Qi, Mingyuan Ma, Jiahang Xu, Li Lyna Zhang, Fan Yang, and Mao Yang. 2024. Mutual reasoning makes smaller llms stronger problem-solvers. arXiv preprint arXiv:2408.06195. Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi Li, et al. 2024. O1 replication journey: strategic progress reportpart 1. arXiv preprint arXiv:2410.18982. Yuxiao Qu, Tianjun Zhang, Naman Garg, and Aviral Kumar. 2025. Recursive introspection: Teaching language model agents how to self-improve. Advances in Neural Information Processing Systems, 37:55249 55285. Qwen. 2024. Qwen2.5-math-7b. Amrith Setlur, Saurabh Garg, Xinyang Geng, Naman Garg, Virginia Smith, and Aviral Kumar. 2025. Rl on incorrect synthetic data scales the efficiency of llm math reasoning by eight-fold. Advances in Neural Information Processing Systems, 37:4300043031. Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, and Aviral Kumar. 2024. Rewarding progress: Scaling automated proarXiv preprint cess verifiers for llm reasoning. arXiv:2410.08146. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. Maohao Shen, Guangtao Zeng, Zhenting Qi, Zhang-Wei Hong, Zhenfang Chen, Wei Lu, Gregory Wornell, Subhro Das, David Cox, and Chuang Gan. 2025. Satori: Reinforcement learning with chain-of-actionthought enhances llm reasoning via autoregressive search. arXiv preprint arXiv:2502.02508. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2024. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36. Noah Shinn, Beck Labash, and Ashwin Gopinath. 2023. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2024a. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2024b. Scaling llm test-time compute optimally can be more effective than scaling model parameters. Preprint, arXiv:2408.03314. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. 2020. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008 3021. Corby Rosset, Ching-An Cheng, Arindam Mitra, Michael Santacroce, Ahmed Awadallah, and Tengyang Xie. 2024. Direct nash optimization: Teaching language models to self-improve with general preferences. arXiv preprint arXiv:2404.03715. Fahim Tajwar, Anikait Singh, Archit Sharma, Rafael Rafailov, Jeff Schneider, Tengyang Xie, Stefano Ermon, Chelsea Finn, and Aviral Kumar. 2024. Preference fine-tuning of llms should leverage suboptimal, on-policy data. arXiv preprint arXiv:2404.14367. William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. 2022. Self-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802. Zhengyang Tang, Xingxing Zhang, Benyou Wan, and Furu Wei. 2024a. Mathscale: Scaling instruction tuning for mathematical reasoning. arXiv preprint arXiv:2403.02884. 14 Zhengyang Tang, Xingxing Zhang, Benyou Wang, and Furu Wei. 2024b. Mathscale: Scaling instruction tuning for mathematical reasoning. arXiv preprint arXiv:2403.02884. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. 2025. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599. Qwen Team. 2024a. Qwq: Reflect deeply on the boundaries of the unknown. The Mistral AI Team. 2024b. Mathstral-7b-v0.1. Ye Tian, Baolin Peng, Linfeng Song, Lifeng Jin, Dian Yu, Haitao Mi, and Dong Yu. 2024. Toward selfimprovement of llms via imagination, searching, and criticizing. arXiv preprint arXiv:2404.12253. Shubham Toshniwal, Wei Du, Ivan Moshkov, Branislav Kisacanin, Alexan Ayrapetyan, and Igor Gitman. 2024. Openmathinstruct-2: Accelerating ai for math with massive open-source instruction data. arXiv preprint arXiv:2410.01560. Gladys Tyen, Hassan Mansoor, Peter Chen, Tony Mak, and Victor Carbune. 2023. Llms cannot find reasoning errors, but can correct them! arXiv preprint arXiv:2311.08516. Chaojie Wang, Yanchen Deng, Zhiyi Lv, Shuicheng Yan, and An Bo. 2024a. Q*: Improving multi-step reasoning for llms with deliberative planning. Preprint, arXiv:2406.14283. Huaijie Wang, Shibo Hao, Hanze Dong, Shenao Zhang, Yilin Bao, Ziran Yang, and Yi Wu. 2024b. Offline reinforcement learning for llm multi-step reasoning. arXiv preprint arXiv:2412.16145. Peiyi Wang, Lei Li, Zhihong Shao, R. X. Xu, Damai Dai, Yifei Li, Deli Chen, Y. Wu, and Zhifang Sui. 2024c. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. Preprint, arXiv:2312.08935. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. 2024d. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. 2024. An empirical analysis of compute-optimal inference for problem-solving with language models. arXiv preprint arXiv:2408.00724. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. 2024. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822. Lifan Yuan, Wendi Li, Huayu Chen, Ganqu Cui, Ning Ding, Kaiyan Zhang, Bowen Zhou, Zhiyuan Liu, and Hao Peng. 2024. Free process rewards without process labels. arXiv preprint arXiv:2412.01981. Weihao Zeng, Yuzhen Huang, Wei Liu, Keqing He, Qian Liu, Zejun Ma, and Junxian He. 2025. 7b model and 8k examples: Emerging reasoning with reinforcement learning is both effective and efficient. https://hkust-nlp.notion.site/ simplerl-reason. Notion Blog. Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang. 2024a. Rest-mcts*: Llm self-training via process reward guided tree search. arXiv preprint arXiv:2406.03816. Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. 2024b. Generative verifiers: Reward modeling as next-token prediction. In The 4th Workshop on Mathematical Reasoning and AI at NeurIPS24. Qingjie Zhang, Han Qiu, Di Wang, Haoting Qian, Yiming Li, Tianwei Zhang, and Minlie Huang. 2024c. Understanding the dark side of llms intrinsic selfcorrection. arXiv preprint arXiv:2412.14959. Yunxiang Zhang, Muhammad Khalifa, Lajanugen Logeswaran, Jaekyeom Kim, Moontae Lee, Honglak Lee, and Lu Wang. 2024d. Small language models need strong verifiers to self-correct reasoning. arXiv preprint arXiv:2404.17140. Yu Zhao, Huifeng Yin, Bo Zeng, Hao Wang, Tianqi Shi, Chenyang Lyu, Longyue Wang, Weihua Luo, and Kaifu Zhang. 2024. Marco-o1: Towards open reasoning models for open-ended solutions. arXiv preprint arXiv:2411.14405. Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. 2024. Processbench: Identifying process errors in mathematical reasoning. arXiv preprint arXiv:2412.06559. Daniel Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2019. Fine-tuning lanarXiv guage models from human preferences. preprint arXiv:1909.08593."
        },
        {
            "title": "A Implementation Details",
            "content": "A.2 Baseline Details A.1 Verification Processing and SFT Data Construction Given the responses sampled from the original LLM policy, we prompt frontier LLMs for initial verifications. In order to construct more valid verification, we force the LLMs to verify without resolving the problem and filter out invalid verifications during data processing. We found that despite being instructed to \"verify without re-solving the problem\", most existing LLMs still biased to solve the problem again, as shown in Table 6. Finally, we collected the verification data by querying gpt4-preview-11061 , which shows strong instructionfollowing ability to \"verify without re-solving the problem\" and can perform plausible verification such as adopting reverse thinking, inductive reasoning and other methods. For these collected prompts, we refine the remaining verifications using gpt-4o to improve fluency and clarity. During this refinement, we instruct gpt-4o to append conclusion at the end of each verification based on its stancefor example: Therefore, the answer is correct/incorrect/cannot verify. Finally, we discard any verifications where the judgment does not align with the actual correctness of the answer. The prompts we used during the whole process are provided in Appendix A.3. With the refined and filtered verifications, we construct the SFT data as follows. For each problem, we determine the number of answer attempts required to eventually obtain correct answer based on the accuracy from the initial sampling. The lower the accuracy, the more rounds of responses are generated. In our implementation, we categorize all problems into four difficulty levels and construct answer sequences with 1, 2, 3, or 4 rounds, according to descending accuracy. Then, after an incorrect answer, we append Wait, let me recheck my solution along with the corresponding verification. If that answer is not the final attempt, we further append Let me try again. We ensure that the last answer in the sequence is correct. Additionally, we ensure that the answers in each round for given problem are distinct. Figure 5 is an example of SFT data constructed with 4 rounds of responses. 1https://openai.com/api/ A.2.1 Baseline Implementations In Table 2, the reported results for Frontier LLMs and Top-tier Open-source Reasoning LLMs are sourced from the original reports and Guan et al. (2025). We evaluate Llama-3.1-8B-Instruct (Dubey et al., 2024), Qwen2-7B-Instruct (qwe, 2024), Qwen2.5-Math-7B, Qwen2.5-Math-7BInstruct and Qwen2.5-Math-72B-Instruct(Yang et al., 2024) using the same process described in Section 3.1. For Eurus-7B-PRIME (Cui et al., 2025), rStar-Math-7B (Guan et al., 2025), and Qwen2.5-7B-SimpleRL (Zeng et al., 2025), we report results directly from the original papers. In Table 3, the results for Llama-3.1-70BInstruct and QwQ-32B-Preview are taken from Shen et al. (2025). For the remaining baselines, we follow the official evaluation protocol of the dataset project2. A.2.2 Baseline License In this work, we utilize the Llama-3.1-8BInstruct model, whose license can be reviewed https://huggingface.co/meta-llama/ at Llama-3.1-8B-Instruct/blob/main/LICENSE. the models Qwen2-7B-Instruct, In addition, Qwen2.5-Math-7B, Eurus-2-7B-PRIME, and project vLLM are distributed under the Apache License 2.0. We gratefully acknowledge the the open-source community contributions of and strictly adhere to the terms of the respective licenses. A.2.3 Baseline SFT Data Construction Original Solution SFT Data In this setting, we use the solution from the original dataset as sft data. To ensure fair comparison, we maintain the same training data volume as our behavior initialization approaches. Long CoT SFT Data We also introduce baseline by fine-tuning on Long CoT responses generated by QwQ-32B-Preview (Team, 2024a). Specifically, we instruct QwQ to generate responses to given problems and filter out those with incorrect answers. The remaining high-quality responses are then used for supervised fine-tuning. Importantly, we ensure that the total training data volume 2https://github.com/Yale-LILY/FOLIO https://github.com/facebookresearch/cruxeval https://github.com/eladsegal/strategyqa https://github.com/TIGER-AI-Lab/MMLU-Pro Without Asking for Confirmative Verification Model Confirmative out of 100 GPT-4o GPT-4-Preview-1106 QwQ-32B-preview Llama-3.1-70B-Instruct 26 32 37 28 Asking for Confirmative Verification Model Confirmative out of 100 GPT-4o GPT-4-Preview-1106 QwQ-32B-preview Llama-3.1-70B-Instruct 44 61 58 50 Table 6 remains consistent with that used in our behavior initialization approach. The prompt we use for QwQ is provided in Appendix A.3. A.3 Prompts The prompts we use in all experiments are as follows: Sampling Responses During Training/Inference Please reason step by step, and put your final answer within boxed{}. Problem: {problem} Verification Refinement You are math teacher. will give you math problem and an answer. Verify the answer's correctness without stepbystep solving. Use alternative verification methods. Question: {problem} Answer: {answer} Verification: Verification Collection Refine this verification text to read as natural selfcheck within solution. Maintain logical flow and professionalism. Key Requirements: 1. Avoid phrases like \"without solving stepbystep\" or \"as math teacher\". 2. Treat the answer as your own prior solution. 3. Conclude with EXACTLY one of: Therefore, the answer is correct. Therefore, the answer is incorrect. Therefore, the answer cannot be verified. Original text: {verification}"
        },
        {
            "title": "B Detailed Experiment Settings",
            "content": "B.1 Datasets In-domain Datasets B.1.1 MATH500 (Lightman et al., 2023b) offers streamlined slice of the broader MATH (Hendrycks et al., 2021b) dataset, comprising 500 test problems selected through uniform sampling. Despite its smaller scope, it maintains distribution of topics and difficulty levels that mirrors the larger MATH corpus. GSM8K (Cobbe et al., 2021a) features around 8,500 grade-school math word problems. The dataset focuses on simple arithmetic through early algebra and includes 1,319 distinct tasks in its test set. OlympiadBench (He et al., 2024) collects 8,476 advanced math and physics questions drawn from Olympiad contexts, with some originating from the Chinese college entrance exam. We use the subset of 674 text-only competition questions, providing open-ended math challenges. AMC2023 (AI-MO, 2024b) and AIME (AI-MO, 2024a) each supply set of challenging exam-style problems: 40 questions from AMC 2023 and 30 from AIME 2024, all in text-only format. CollegeMath (Tang et al., 2024b) is dataset targeting advanced college-level mathematics, drawn from nine textbooks spanning seven major fieldsalgebra, pre-calculus, calculus, vector calculus, probability, linear algebra, and differential equations. The final collection comprises 1,281 training examples and 2,818 test examples. Gaokao2023en (Liao et al., 2024) is dataset consisting of 385 mathematics problems sourced from the 2023 Chinese higher education entrance examination, which have been professionally translated into English. B.1.2 Cross-domain Datasets FOLIO (Han et al., 2022) is meticulously annotated to assess intricate logical reasoning in natural language. It pairs 1,430 conclusions with 487 sets of premiseseach verified using first-order logic (FOL)and contains 203 unique problems in its test portion. CRUXEval (Gu et al., 2024) tests code comprehension and reasoning through 800 concise Python functions (spanning 313 lines). Each function is accompanied by one or more input-output examples. The goal is to predict the correct outputs given the function body and specific input. The test partition encompasses all 800 problems. Details of each test dataset we used as benchmark are as follows: StrategyQA (Geva et al., 2021) targets multihop reasoning questions where the necessary inter17 Model Learning Rate Batch Size KL Coefficient Max Length Training Epochs Llama-3.1-8B-Instruct Qwen2-7B-Instruct Qwen2.5-Math-7B 5e-6 5e-6 5e-6 32 32 0.1 0.1 0.01 8000 6000 8000 3 3 3 Table 7: Model Training Hyperparameter Settings (SFT) Model Learning Rate Training Batch Size Forward Batch Size KL Coefficient Max Length Sampling Temperature Clip Range Training Steps Llama-3.1 Qwen2-7B-Instruct Qwen2.5-Math-7B 5e-7 5e-7 5e-7 64 64 64 256 256 256 0.05 0.05 0.01 8000 6000 8000 0.7 0.7 0. 0.2 0.2 0.2 500 500 500 Table 8: Model Training Hyperparameter Settings (RL) mediate steps are not explicit. Each of its 2,780 items includes strategic query, breakdown of the reasoning steps, and supporting evidence drawn from Wikipedia. MMLUProSTEM is extracted from MMLUPro (Wang et al., 2024d). Following Satori (Shen et al., 2025), we conduct evaluations on six STEM subsetsphysics, chemistry, computer science, engineering, biology, and economics. training code is built upon Hugging Face TRL3. For inference, we use single NVIDIA A100 (40GB) GPU with vLLM-0.5.44. We utilize transformers version 4.39.3 for fine-tuning Qwen2-7B-Instruct and Qwen2.5-Math-7B, version 4.44.0 for finetuning Llama-3.1-8B, and version 4.46.3 for reinforcement learning. We use PyTorch 2.1.1 across our training pipeline. Our evaluation code is built upon Qwen Maths evaluation codebase5. B.2 Hyperparameters Setting"
        },
        {
            "title": "C Metrics Definition",
            "content": "During behavior initialization with SFT, we use batch size of 32 and adopt learning rate of 5e6. We set the maximum sequence length 8000 to accommodate long responses and verifications. To balance stability and convergence during training, we add KL punishment to the training loss, and the KL coefficient is set to 0.1. During reinforcement learning, for each training batch, we use training batch size of 64, and sample responses for each question in batch, resulting forward batch size of 64n. For each forward batch, we update the model for step with the training batch size 64. Specifically, for both process-level and outcome-level RL, we adopt = 4 (i.e., for RLOO, the sample number is also 4). More hyperparameters of the RL training are presented in Table 8. We use the BF16 model precision in all experiments. Main hyperparameters used in the experiments are illustrated in Table 7 and 8. B.3 Experiment Environment We include the formal definition of metrics we use for analyzing self-verification and self-correction behaviors of the post-trained models as follows. C.1 Notations We first present the main notations used in our formulation in Table 9. C.2 Self-Verification Metrics C.2.1 Verification Accuracy (VA) Verification Accuracy measures how often the verification prediction matches the ground-truth correctness (N is the total number of verifications in the responses to the test set): VA = 1 (cid:80)N t=1 (cid:16) Parser(vt) = Vgolden(st) (cid:17) . (7) C.2.2 Error Recall (ER) Error Recall measures the recall of detecting incorrect answers (i.e., the fraction of actually incorrect All experiments are implemented using the PyTorch framework on 32 NVIDIA H20 (96GB) GPUs or 32 NVIDIA A100Pro (40GB) GPUs. Our 3https://github.com/huggingface/trl 4https://github.com/vllm-project/vllm 5https://github.com/QwenLM/Qwen2.5-Math 18 Variable π Description The policy Problem instance ai Series of predefined actions: = {a1, a2, . . . , an} The i-th action in the response y, and let ype(ai) {verify, solve, <end>} jth attempt to solve the problem jth self-verification for the jth attempt arser() arser(vj) {correct, incorrect} sj vj The text parser to get the self-verification result indicating the correctness of action sj Vgolden(ai) {correct, incorrect} The rule based reward function R() {1, 1} problem in each y. Formally: (cid:80) ICR = I(cid:0)R(s1) = 1(cid:1) I(cid:0)R(sTy ) = 1(cid:1) I(cid:0)R(s1) = 1(cid:1) (cid:80) . (10) C.3.2 Correct to Incorrect Rate (CIR) The rate at which the model incorrectly alters an initially correct answer (R(s1) = 1) into an incorrect final answer (R(sTy ) = 1), where Ty = ya/2 is the total number of attempts to solve the problem in each y. Formally: R(sj) = R(vj) = (cid:26) 1, 1, (cid:26) 1, 1, Vgolden(sj) = correct otherwise arser(vj) = Vgolden(sj) otherwise (cid:80) CIR = I(cid:0)R(s1) = 1(cid:1) I(cid:0)R(sTy ) = 1(cid:1) I(cid:0)R(s1) = 1(cid:1) (cid:80) . (11) Vgolden() R() <end> I() End of action series The indicator function, I() {0, 1}. I() = 1 if the condition inside holds true, and I() = 0 otherwise. Table 9: Variable Lookup Table answers that are successfully identified as incorrect): (cid:80) (cid:80) ya 2 t=1 (cid:16) ER = (cid:17) (cid:16) (cid:16) R(st)=1 Parser(vt)=incorrect (cid:80) (cid:80) ya 2 t= (cid:17) R(st)=1 (cid:17) . (8) where ya is the total number of actions in and ya is the total number of attempts to 2 solve the problem (y = {a1, a2, , aya} = {s1, v1, , ya }). , ya 2 C.2.3 Correct Precision (CP) Correct Precision measures the precision when the verification model predicts an answer to be correct (i.e., among all correct predictions, how many are truly correct): CP = (cid:80) (cid:80) ya 2 t=1 (cid:16) Parser(vt)=correct (cid:17) (cid:16) (cid:17) R(st)=1 (cid:80) (cid:80) ya 2 t= (cid:16) Parser(vt)=correct (cid:17) . (9) C.3 Self-Correction Metrics C.3.1 Incorrect to Correct Rate (ICR) The rate at which the model successfully corrects an initially incorrect answer (R(s1) = 1) into correct final answer (R(sTy ) = 1), where Ty = ya/2 is the total number of attempts to solve the"
        },
        {
            "title": "D Offline RL Training Details",
            "content": "In this section, we provide additional details on the offline reinforcement learning training process, including formal definition, ablation studies, and implementation details. D.1 Accuracy-Grouped Baseline Definition To fully leverage the advantages of offline RL, which does not require real-time sampling, we explore more appropriate baseline selection by further grouping trajectories based on problem difficulty. Intuitively, for two trajectories y(1) and y(2) sampled under questions of different difficulty levels, and their corresponding actions a(1) at the same position, even if they share identical reward contexts, their expected returns (baselines) should differ, i.e., the expected return is typically lower for more challenging problems. and a(2) We measure problems difficulty by estimating how often it is solved correctly under the current sampling policy. Concretely, we sample multiple trajectories in parallel for each problem. The fraction of these trajectories that yield correct final answer serves as the problems accuracy. We then discretize this accuracy into separate bins, effectively grouping the problems according to their estimated difficulty. All trajectories belonging to problems within the same accuracy bin form common subset. Compared to using direct reward contexts alone, this accuracy-based grouping offers more robust estimate of expected returns, problems in the same bin share similar success rates. Moreover, unlike pre-defined difficulty grouping, these Accuracy Range Retained Questions MATH500 AIME2024 AMC2023 College Math Olympiad Bench GSM8K GaokaoEn2023 Average 0.7] 0.8] 0.9] 1] [0.1 [0.2 [0.3 [ 1805 2516 4448 Full 83.4 82.6 81.6 80.6 23.3 23.3 23.3 26.7 62.5 70.0 70.0 67.5 50.0 49.8 49.4 50. 46.7 45.3 44.7 43.0 92.9 92.4 92.0 91.4 72.2 70.1 68.1 67.0 61.6 61.9 61.3 60.9 Table 10: Comparison of question filtering accuracy selection. bins adjust dynamically as the models capabilities evolve. Building on this approach, we propose two accuracy-based baseline estimation methods for offline RL as follows. D.1.1 Accuracy-Grouped Baseline With Position Group Within each accuracy bin, we further split actions based on their position in the trajectory. Concretely, we consider all actions occurring at the same step index across trajectories in the same bin to be comparable, and we compute their average return to serve as the baseline. Thus, when we look up the baseline for particular action at given step in trajectory, we use the average return of all actions taken at that same step index in all trajectories belonging to the same accuracy bin. D.1.2 Accuracy-Grouped Baseline With Reward Context We also propose combining accuracy-based grouping with reward-context grouping. The underlying assumption is that even if two actions share the same immediate reward context, their expected returns can differ if they originate from different difficulty bins. Generally, problems that are harder to solve exhibit lower expected returns. Consequently, we first bin the trajectories by accuracy, then further group them by common reward context. Within each sub-group, we average the returns of all relevant actions to obtain the baseline. D.2 Offline RL Implementation Details In each iteration of offline RL training, we generate multiple trajectories (e.g., eight) per prompt in parallel. We then apply prompt filtering, rejection sampling, accuracy-based baseline estimation, advantage computation, and policy updates. Implementation details follow. Our ablation study on Qwen2.5-Math-7B shown in Table 10 confirms that filtering improves performance. The most stable results are obtained with an accuracy range of [0.1, 0.7], suggesting that including moderately difficult samples enhances the models reasoning capabilities. D.2.2 Rejection Sampling We discard any trajectory that does not follow the alternation pattern of solution and verification: = (s1, v1, . . . , sk, vk). Additionally, we remove malformed trajectories such as = (s1, s2, v1). To mitigate reward hacking due to excessively long outputs, we eliminate trajectories where R(st) = 1 and R(vt) = 1 at timestep t, but further actions are taken at + 1. Moreover, we discard trajectories containing more than 20 actions, as excessive action sequences can introduce instability and deviate from expected solution structures. D.2.3 Loss Function To determine the best offline baseline method, we conducted ablation studies on Qwen2.5-Math7B shown in Table 11. We found that using the accuracy-grouped baseline with an additional division by position provides the most stable results. When computing advantages, we subtract both the baseline and scaled relative policy term like Equation 5. Notably, we fix πref as the reference policy instead of being updated at each iteration. D.2.4 Training Hyperparameter Settings We use batch size of 64, maximum learning rate of 5 107, and KL penalty coefficient of 0.1. The maximum training sequence length is set to 8192. We apply warm-up phase of 5 steps and clipping range parameter of 0.2. We use BF16 model precision in all experiments. D.2.1 Prompt Filtering"
        },
        {
            "title": "E Demo Cases",
            "content": "As we sample multiple trajectories for each prompt, we compute the accuracy of each prompt. We retain prompts whose accuracy falls within predefined range. To intuitively demonstrate the effectiveness of our proposed method, we present the models inference examples after RL on the MATH500 and StrategyQA datasets in the Figure 6 and Figure 7. 20 Baseline Method MATH500 AIME2024 AMC2023 College Math Olympiad Bench GSM8K GaokaoEn2023 Based on reward context Based on accuracy group with position Based on accuracy group with reward context 82.4 83.4 82.4 26.7 23.3 23.3 65.0 62.5 67.5 50.1 50.0 49.3 46.1 46.7 45.8 92.9 92.9 93. 71.2 72.2 71.2 Average 62.1 61.6 61.8 Datasets Table 11: The performance of different baselines Figure 5: SFT data example."
        },
        {
            "title": "F Other Discussion",
            "content": "F.1 Discussion on Potential Risk We have carefully considered potential risks associated with our work and found no significant concerns. Our approach, focused on enhancing LLM reasoning through self-verification and selfcorrection, does not introduce malicious or harmful effects, privacy issues, or security threats. Additionally, it does not contribute to biases, fairness concerns, or environmental impact. We believe our work is safe for responsible use in research. F.2 Use of AI Assistant In this work, we utilized an AI assistant solely for the purpose of refining and polishing the language of the manuscript. The AI assistant was employed to improve clarity, flow, and overall readability, ensuring the text adhered to academic writing standards. It was not involved in any data analysis, experimentation, or formulation of ideas. All research design, methodology, results, and conclusions were developed independently by the authors. The use of the AI assistant was limited to language enhancement and did not influence the content or scientific integrity of the work. 21 Figure 6: MATH500 Case. 22 Figure 7: StrategyQA Case."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Tencent",
        "The Hong Kong University of Science and Technology (Guangzhou)",
        "The University of Hong Kong",
        "Tsinghua University"
    ]
}