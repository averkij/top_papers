{
    "paper_title": "LM2: Large Memory Models",
    "authors": [
        "Jikun Kang",
        "Wenqi Wu",
        "Filippos Christianos",
        "Alex J. Chan",
        "Fraser Greenlee",
        "George Thomas",
        "Marvin Purtorab",
        "Andy Toulis"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper introduces the Large Memory Model (LM2), a decoder-only Transformer architecture enhanced with an auxiliary memory module that aims to address the limitations of standard Transformers in multi-step reasoning, relational argumentation, and synthesizing information distributed over long contexts. The proposed LM2 incorporates a memory module that acts as a contextual representation repository, interacting with input tokens via cross attention and updating through gating mechanisms. To preserve the Transformers general-purpose capabilities, LM2 maintains the original information flow while integrating a complementary memory pathway. Experimental results on the BABILong benchmark demonstrate that the LM2model outperforms both the memory-augmented RMT model by 37.1% and the baseline Llama-3.2 model by 86.3% on average across tasks. LM2 exhibits exceptional capabilities in multi-hop inference, numerical reasoning, and large-context question-answering. On the MMLU dataset, it achieves a 5.0% improvement over a pre-trained vanilla model, demonstrating that its memory module does not degrade performance on general tasks. Further, in our analysis, we explore the memory interpretability, effectiveness of memory modules, and test-time behavior. Our findings emphasize the importance of explicit memory in enhancing Transformer architectures."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 9 4 0 6 0 . 2 0 5 2 : r LM2: Large Memory Models Jikun Kang1, Wenqi Wu1, Filippos Christianos1, Alex J. Chan1, Fraser Greenlee1, George Thomas1, Marvin Purtorab1, and Andy Toulis.1 1Convergence Labs Ltd. This paper introduces the Large Memory Model (LM2), decoder-only Transformer architecture enhanced with an auxiliary memory module that aims to address the limitations of standard Transformers in multi-step reasoning, relational argumentation, and synthesizing information distributed over long contexts. The proposed LM2 incorporates memory module that acts as contextual representation repository, interacting with input tokens via cross attention and updating through gating mechanisms. To preserve the Transformers general-purpose capabilities, LM2 maintains the original information flow while integrating complementary memory pathway. Experimental results on the BABILong benchmark demonstrate that the LM2model outperforms both the memory-augmented RMT model by 37.1% and the baseline Llama-3.2 model by 86.3% on average across tasks. LM2 exhibits exceptional capabilities in multi-hop inference, numerical reasoning, and large-context question-answering. On the MMLU dataset, it achieves 5.0% improvement over pre-trained vanilla model, demonstrating that its memory module does not degrade performance on general tasks. Further, in our analysis, we explore the memory interpretability, effectiveness of memory modules, and test-time behavior. Our findings emphasize the importance of explicit memory in enhancing Transformer architectures. Correspondence: Jikun Kang at jikun@convergence.ai Website: https://convergence.ai Code: https://github.com/convergence-ai/lm"
        },
        {
            "title": "1 Introduction",
            "content": "Transformer-based models have achieved remarkable success. Landmark architectures such as GPT-3 Brown et al. (2020), BERT Kenton and Toutanova (2019), and Vision Transformers Dosovitskiy (2020) have established state-of-the-art performance across wide array of applications, including machine translation Zhu et al. (2020), text summarization Liu and Lapata (2019), question-answering Li et al. (2023), and image recognition Dosovitskiy (2020). As demonstrated by studies on large-scale models, their generalization capabilities improve significantly with increased data and model size, leading to emergent behaviors that extend beyond their original training objectives Kaplan et al. (2020); Kang et al. (2024). Despite their significant contributions, current Transformer models encounter critical limitations when applied to long context reasoning tasks Kuratov et al. (2024). For instance, in the needle-in-a-haystack problem, models must answer questions that require reasoning across facts scattered throughout exceedingly long documents. Effectively addressing tasks with extensive context demands the models ability to discern essential information from vast amounts of irrelevant data. Recent memory-augmented architectures (e.g., Bulatov et al., 2022; Ko et al., 2024) attempt to tackle these challenges by using recurrent prompts to track long context information. However, these architectures primarily summarize previous answers into prompts without fully integrating long-term information, leading to performance degradation over long contexts. For example, on Task 2 (see appendix A), MemReasoner Ko LM2: Large Memory Models et al. (2024) achieves performance score of 60.6 for context lengths under 8K, but drops significantly to 18.5 when the context length exceeds 16K. Additionally, these models are specifically tailored for memory-based tasks, thereby sacrificing the generalization capabilities inherent to large language models (LLMs). To address these limitations, we propose the Large Memory Model (LM2), novel architecture that enhances the Transformer framework with dedicated memory module. This module functions as an auxiliary storage and retrieval mechanism, dynamically interacting with input embeddings to improve performance. The memory module follows structured process: initializing with memory bank, leveraging cross attention for efficient interaction with sequence embeddings, and using gating mechanisms, such as forget and input gates, to selectively update stored information. By decoupling memory storage and retrieval from immediate processing, LM2 provides robust solution for modeling long-term dependencies, overcoming the shortcomings of existing methods while maintaining computational efficiency. This architecture is particularly well-suited for tasks requiring long context and complex reasoning, offering practical and scalable alternative to current approaches. Moreover, as illustrated in Figure 1, we maintain the original information flow-namely, the output embeddings passed from one block to the nextwhile introducing an additional, complementary memory information flow represented by the memory embeddings. The memory information flow is controlled by learnable output gate, which uses cross attention to dynamically regulate the amount of memory information passed to subsequent layers. This design ensures that the original attention information flow remains intact while dynamically incorporating relevant memory information as needed. Figure 1 Illustration of LM2 overall architecture. It consists of separate memory bank, which updates the main information flow through cross attention, and is updated using the input (I), output (O), and forget (F) gates. For the information flow from one block to another, the gray curve shows the normal attention flow and the pink curve shows the extra memory flow. We first evaluate the effectiveness of LM2 on the BABILong dataset Kuratov et al. (2024), challenging benchmark specifically designed to test memory-intensive reasoning capabilities. To verify that our memorybased approach does not undermine general performance, we also assess LM2 on the MMLU benchmark Hendrycks et al. (2021), which spans broad array of academic subjects and difficulty levels. Across both evaluations, LM2 outperforms state-of-the-art (SOTA) memory model Recurrent Memory Transformer (RMT) Bulatov et al. (2022) by up to 80.4%, illustrating enhanced proficiency in multi-hop inference, numerical reasoning, and relational argumentation. These improvements underscore the value of incorporating our explicit memory mechanisms within Transformer architectures, enabling more robust handling of extended contexts. The contributions of this work are summarized as follows: We propose novel memory-augmented Transformer architecture that incorporates dynamic memory module capable of capturing and leveraging long-term dependencies in sequential data. We introduce an additional memory information flow within the decoder block that complements the existing attention mechanism, enabling the integration of enriched memory information while preserving the original attention information. Through extensive experiments on long context reasoning tasks (up to context of 128K tokens), LM2 2 LM2: Large Memory Models outperforms SOTA memory-augmented model RMT and non-memory baseline Llama-3.2 on average 37.1% and 86.3%, respectively, demonstrating the practical benefits of our approach."
        },
        {
            "title": "2 Large Memory Model (LM2)",
            "content": "We present Large Memory Model (LM2), memory-augmented Transformer model designed to enhance its long-term memory capabilities. LM2 consists of multiple Transformer decoder blocks, augmented with memory module that dynamically stores and updates intermediate sequences of representations. The decoder block processes input sequences using positional embeddings, while the memory module interacts with these embeddings via cross attention mechanisms. We use skip connection between the multi-head attention and the memory modules to facilitate learning and maintain the original intermediate embeddings of the Transformer. The memory updates are controlled by learnable control gates, denoted as F, I, and O, which correspond to the forget, input, and output gates, respectively. The memory module operates through two primary stages: memory information flow, and memory updates. Each of these stages is elaborated on in the following sections."
        },
        {
            "title": "2.1 Memory Information Flow",
            "content": "As depicted in Figure 1, we introduce an explicit memory module, named the memory bank RN dd, designed to store long-term memory. Here, denotes the number of memory slots, while represents the hidden dimension of each slot. For simplicity, each memory slot is initialized as an identity matrix: Mr = Idd, where {1, . . . , } and Idd is the identity matrix. We use cross attention-based mechanism between the memory bank and input embeddings to locate memory slots that contain relevant information. This approach is based on the idea that humans tend to store and group related information together (e.g., in Documentation Science and Archival Science (Dooley, 2007)). Note that the input embeddings are encoded by the positional encoder, which embeds the input tokens and persists the temporal correlations between states and actions. Concretely, each input embedding acts as the query, while the memory bank serves as both the key and the value store. Intuitively, this means we look up where (via the key) in to find relevant information and then retrieve it (via the value). To enable cross attention, the input embeddings RT (where is the sequence length) and memory bank RN are projected into query (Q), key (K), and value (V) spaces: = EtWQ, = MtWK, = MtWV , (1) where WQ, WK, WV Rdd are learnable projection matrices, and stands for decoder block t. (cid:17) (cid:16) QK The attention scores are computed as the scaled dot product of the query and key matrices: = , where RT represents the alignment between the input sequence and memory softmax slots. The resultant attention output is Emem = AV, where Emem RT integrates information from the input and memory. To ensure temporal consistency, causal masking is applied, and optionally, top-k attention is used to retain only the most relevant memory interactions. To regulate the influence of the memory information (gray path in Figure 1) on the existing attention information flow (pink path in Figure 1), an output gate is introduced. The output gate dynamically controls the contribution of the memory retrieval based on the cross attention output Emem: gout = σ (EmemWout) , (2) where Wout Rdd is learnable parameter matrix, and σ is the sigmoid activation function. The gated memory output is then computed as: Egated = gout Mt. (3) The gated memory output is integrated into the standard attention flow of the Transformer decoder through skip connection. Specifically, the output of the self-attention mechanism, Eattn, is combined with the gated 3 LM2: Large Memory Models memory output as Enext = Eattn + Egated. This skip connection ensures that the standard attention output and the memory-augmented features jointly contribute to the next decoder layer. By dynamically gating the memory retrieval and integrating it with the attention flow, LM2 effectively balances the use of memory and contextual information, enhancing its ability to model long-term dependencies while preserving the core Transformer operations."
        },
        {
            "title": "2.2 Memory updates\nAs illustrated in Figure 2, the update process is divided into three distinct phases: the input, forget, and output\n(previously described). By gating how much new information is introduced and how much old information is\ndiscarded, the memory module avoids overwriting crucial long-term facts while also eliminating irrelevant or\noutdated content when processing long context sequences.",
            "content": "Input Phase During the input phase, the model decides how much of the newly computed embeddings (Emem) to incorporate into the memory. To achieve this, first an input gate is computed: gin = σ(cid:0)EtWin (cid:1), (4) where Win Rdd is learnable parameter matrix, Et is the current input representation, and σ is the sigmoid activation function. This gating mechanism serves as filter, deciding which relevant information should be written into memory, while also preventing the influx of noise or redundant details. Figure 2 Illustration of how memory module works inside of each decoding block, where blue, green, and red box corresponds to forget, input, and output phase. Forgetting Phase Once new information is made available during the input phase, the memory must also decide which parts of its existing content to discard. This is governed by the forget gate: gforget = σ(cid:0)EmemWforget (cid:1), (5) where Wforget Rdd. By outputting values less than one, the forget gate selectively erases memory slots that are no longer relevant, allowing the model to focus on more recent or salient information. Memory Update Combining these two gating mechanisms leads to the updated memory state: Mt+1 = gin tanh(Emem) + gforget Mt, (6) where tanh function is applied to keep the new memory content bounded. Through these regulated phases, the memory module memorizes the most relevant information and removes outdated details, ensuring that it remains both concise and informative over time."
        },
        {
            "title": "3 Pre-training LM2",
            "content": "We base our work on the Llama-3 model framework Dubey et al. (2024), employing it as the foundation for our Transformer architecture. Its architecture comprises 16 decoder blocks, each with model dimension of 2,048. The feed-forward networks within these blocks have an inner dimension of 8,192. The model utilizes 32 attention heads, with 8 dedicated key/value heads. Our memory module extends this architecture, consisting of 2,048 memory slots, each with dimension of 2,048. Memory modules are integrated into all 16 decoder blocks, as this configuration empirically achieves the best performance (see Section 4.3 for detailed results). The Llama-3 framework comprises approximately 4 LM2: Large Memory Models Table 1 Performance on the BABILong dataset: All models are evaluated on various context lengths ranging from 0K, 1K, 2K, and 4K to an aggregated average length of 8K. Qa stands for various subsets. Due to page limits, we aggregate the results for 8K, 16K, 32K, 64K, and 128K into single metric, with detailed results provided in Appendix B. model qa1 qa2 qa3 qa4 qa5 qa qa7 qa8 qa9 qa10 Avg. Llama-3.2-1.2B vanilla-Llama-1.7B RMT-1.7B LM2-1.7B 54.0 86.0 85.0 99. Llama-3.2-1.2B 48.0 Llama-3.2-1.2B-RAG 51.0 31.0 vanilla-Llama-1.7B 35.0 RMT-1.7B LM2-1.7B 85.0 Llama-3.2-1.2B 44.0 Llama-3.2-1.2B-RAG 52.0 25.0 vanilla-Llama-1.7B RMT-1.7B 44.0 LM2-1.7B 58.0 Llama-3.2-1.2B 37.0 Llama-3.2-1.2B-RAG 47.0 21.0 vanilla-Llama-1.7B 24.0 RMT-1.7B LM2-1.7B 46. Llama-3.2-1.2B 19.0 Llama-3.2-1.2B-RAG 29.3 vanilla-Llama-1.7B 11.3 17.5 RMT-1.7B LM2-1.7B 23.8 25.0 57.0 49.0 89.0 22.0 14.0 21.0 26.0 59.0 18.0 11.0 22.0 21.0 43.0 16.0 3.0 18.0 20.0 37. 8.0 1.0 15.0 14.5 15.0 0K 62.0 59.0 81.0 88.0 1K 55.0 59.0 43.0 33.0 68.0 2K 50.0 49.0 34.0 41.0 43.0 4K 56.0 58.0 28.0 24.0 34.0 59.0 85.0 95.0 98.0 69.0 80.0 71.0 61.0 91.0 64.0 75.0 58.0 79.0 87. 56.0 68.0 55.0 28.0 78.0 29.0 46.0 49.0 70.0 24.0 19.0 44.0 29.0 72.0 19.0 12.0 37.0 43.0 64.0 25.0 16.0 38.0 22.0 48.0 AVG. Length 8K 17.8 5.0 21.3 20.5 24.5 27.3 55.8 14.5 22.5 24.0 36.5 72.0 31.0 20.3 38.8 49.0 83.0 84.0 95.0 49.0 49.0 60.0 50.0 84.0 52.0 48.0 60.0 47.0 73. 50.0 58.0 61.0 46.0 66.0 49 49.8 44.0 47.0 47.3 14.0 95.0 82.0 96.0 9.0 10.0 71.0 83.0 96.0 18.0 5.0 65.0 78.0 93.0 14.0 3.0 64.0 75.0 93. 21.3 4.8 63.0 73.3 92.8 52.0 79.0 78.0 97.0 31.0 38.0 40.0 41.0 69.0 24.0 33.0 38.0 41.0 53.0 27.0 36.0 35.0 35.0 45.0 12.8 22.8 33.5 34.5 37. 41.0 83.0 85.0 99.0 55.0 40.0 67.0 68.0 82.0 55.0 50.0 66.0 69.0 75.0 55.0 45.0 49.0 65.0 62.0 48.0 46.3 42.0 62.5 53.8 22.0 77.0 76.0 94. 33.0 46.0 58.0 53.0 77.0 42.0 43.0 58.0 51.0 69.0 32.0 39.0 53.0 45.0 50.0 41.8 36.8 36.3 43.0 42.0 40.7 75.0 76.4 92.5 39.5 40.6 50.6 47.9 78. 38.6 37.8 46.3 51.4 65.8 36.8 37.3 42.2 38.4 55.9 28.2 32.3 31.2 35.5 39.9 1.2 billion parameters, with an additional 0.5 billion parameters introduced by the memory module, resulting in total of 1.7 billion parameters for the LM2 model. For pre-training, we leverage high-quality dataset sourced from the SmolLM-Corpus Loubna et al. (2023). The dataset is structured into three distinct sections: synthetic test-books and stories, educational web content, and python codes. To ensure focused evaluation on language tasks, we exclude Python sample training data from this process. The specific details of the training dataset are outlined as follows: Synthetic Textbooks and Stories: Generated using advanced language models to cover wide range of topics, providing 28 billion tokens of diverse educational content. Educational Web Content: Filtered and deduplicated web pages from FineWeb-Edu Penedo et al. (2024), contributing 220 billion tokens of high-quality educational material."
        },
        {
            "title": "4 Experiments",
            "content": "We design our experiments to answer the following questions: Q1: How does LM2 perform in memory tasks? Q2: Does LM2 harm the performance in general tasks? Q3: Do we need to include the memory module in all decoder blocks? Q4: What is stored in the memory bank? Q5: How is the memory module updated at test-time? 5 LM2: Large Memory Models To evaluate LM2, we compare its performance against the following baselines: vanilla-Llama-1.7B: The Llama 3.2 architecture, pre-trained from scratch on the same datasets as LM2. We scale this model to 1.7 billion parameters for fair comparison. RMT-1.7B: Recurrent Memory Transformer (RMT) Bulatov et al. (2022) is memory-augmented framework that generates memory tokens, serving as an additional module built on top of existing LLMs. We use the LLaMA-1.7B model as the backbone and fine-tune it on the bAbI training dataset Weston et al. (2016), following the methodology outlined in Kuratov et al. (2024) and Ko et al. (2024). Llama-3.2-1.2B: To show case the effectiveness of LM2 we also compared the model against the original model trained by Meta, with the same total number of pure Transformer parameters (1.2B), but trained on far more high-quality tokens. Llama-3.2-1.2B-RAG: Lastly, we compare with version of Llama with retrieval-augmented generation (RAG) to better handle long context problems."
        },
        {
            "title": "4.1 Performance on Memory Tasks\nBABILong The BABILong dataset Kuratov et al. (2024) extends bAbI benchmark Weston et al. (2016) by\nincorporating significantly longer contexts and more intricate queries, thus demanding advanced memory\ncapabilities and multi-step reasoning. By increasing both contextual and computational challenges, BABILong\noffers a rigorous evaluation benchmark for testing memory-augmented models.",
            "content": "Table 1 presents comparison of our model against the baselines on the BABILong dataset. We report results across multiple context lengths, from 0K context-length, which is identical to bAbI dataset, to the maximum context length of 128K, which is the target context-length of the backbone Llama-3.2 model. From this table, we observe several key findings as follows: Performance at bAbI benchmark (0K). Without additional context, LM2-1.7B achieves the highest average accuracy of 92.5%, surpassing Llama-3.21.2B, vanilla-LLama-1.7B, and RMT-1.7B, which average results are 40.7%, 75.0% and 76.4%, respectively. Because Llama-3.2-1.2B-RAG is designed for retrieval-augmented generation and evaluated only at longer contexts, it is not included in the 0K setting. This suggests that LM2s underlying modeling improvements enhance its core reasoning ability. Performance at Long Context Lengths (1K4K). As context length increases, performance generally degrades for all models, but LM2-1.7B maintains noticeable improvement over both standard and retrieval-augmented Llama variants and RMT. For instance, at 4K, LM2-1.7Bs average accuracy (55.9%) is higher than Llama-3.2-1.2B, vanilla-LLama-1.7B, and RMT-1.7B, which average results are 36.8%, 42.2% and 48.4%, respectively. This gap underscores LM2s effectiveness for longterm memory ranging from 1K to 4K. Figure 3 Performance on BABILong benchmark with different capabilities. Performance at Long Contexts (8K128K). Although all models exhibit some accuracy decline at these extreme long context lengths, LM2-1.7B remains robust. RMT-1.7B shows reasonable robustness, yet still falls short of LM2-1.7B on most tasks. RAG methods demonstrate some improvements over the baseline Llama, but still falls behind memory-based methods. These results highlight LM2s ability to handle long context problems where Transformer-based models struggle. LM2: Large Memory Models Performance at Different Reasoning Types To further understand how LM2 performs in different reasoning scenarios, we group the BABILong dataset into five categories: (1) Single-step Reasoning (qa1), (2) Multi-step Reasoning (qa23), (3) Relation Tracking (qa45), (iv) Basic Queries (qa68), and (v) Negation & Uncertainty (qa910). Figure 3 depicts the results in radar chart, where higher values indicate better performance. Across nearly all task categories except for Relation Tracking, LM2-1.7B demonstrates the best performance. Notably, LM2 outperforms the other methods on both single and multi-step reasoning, indicating that it can handle more complex, multi-hop inferences and direct fact retrieval with fewer errors. The improvement margin is larger for Basic Queries, Single-Step Reasoning, and Multi-step Reasoning, suggesting that LM2 has strong abilities to retrieve long-term facts and apply them in complex reasoning tasks. The marginally lower performance on Relation Tracking can be attributed to RAGs approach of chunking the context into smaller, more focused documents and retrieving only the most relevant pieces at inference time. RAG makes it much easier to precisely identify which facts are associated with the queried relationship, thus serving as an extremely strong baseline for this task category."
        },
        {
            "title": "4.2 Performance on General Benchmarks\nTo further evaluate if introducing an extra memory module affects LLMs’ general performance, we evaluate\nthe proposed memory-based model, LM2, on the MMLU benchmark Hendrycks et al. (2021), which tests a\nbroad spectrum of subject areas—STEM, Humanities, Social Sciences, and Others—as well as varied difficulty\nlevels—High School, College, Professional, and General Knowledge. Table 2 presents the results of LM2 in\ncomparison to vanilla-Llama and RMT.",
            "content": "Overall, LM2 demonstrates clear performance gain, improving the average accuracy of vanillaLlama from 28.0% to 29.4%. On the contrary, despite sharing the same pre-trained model, RMT degrades the performance of vanilla-Llama to 26.5%. Notably, LM2 achieves substantial gains in Humanities and Social Sciences, where LM2 surpasses vanilla-Llama by 3.5% and 2.4%, respectively. These categories often involve context-rich questions, suggesting that LM2s memory-based approach is advantageous for retaining and leveraging more nuanced and interconnected information. Meanwhile, LM2 also sustains competitive performance in STEM and Others, indicating its robustness beyond highly specialized domains. Table 2 Performance on MMLU dataset. For better visualization, the dataset is categorized on two criteria - subject and difficulty. Subject Category Difficulty Level STEM Humanities Social Sciences Others High School College Professional General Knowledge vanilla Llama 27.2 28.7 29.2 27.7 28.8 27.7 27. 27.2 RMT LM2 25.7 26.7 27.0 27.1 26.5 27.1 26.6 25.6 28.1 32.2 31.6 28.0 30.4 29.0 27.6 28. These results illustrate that LM2 overcomes the drawback associated with memory-augmented models: performance degradation on more general tasks. Current memory-based architectures are carefully designed for memory tasks, weakening their ability to general LLM tasks. However, LM2s performance on all categories of MMLU dataset indicates that the proposed memory mechanism does not impede its general applicability. Average 29.4 28.0 26."
        },
        {
            "title": "4.3 Impact of memory modules\nWe evaluate the effectiveness of proposed memory modules using perplexity as the primary metric across\nvarying numbers of training tokens (measured in billions). Figure 5 illustrates the perplexity trends for the\nbaseline vanilla-Llama and LM2 with varying degrees of memory integration (i.e., 1, 6, 12, and 16 blocks),\nwhere 16 is the maximum number of blocks used in Llama-3.2-1B.",
            "content": "The results demonstrate that integrating memory information more extensively throughout the decoder leads to improved model performance. Specifically, implementing the memory module in only the first block achieves similar results to the vanilla Llama, but with slower convergence. This suggests that introducing single memory 7 LM2: Large Memory Models Figure 5 We evaluate variations of integrating memory within the decoder blocks. The number indicates how many of the initial decoder blocks include the memory module, as we found that the order of implementing memory modules does not affect performance. flow does not degrade overall performance but may slow down training because of extra memory optimization. In contrast, incorporating more memory flows, such as in the 6-block configuration, leads to lower perplexity, highlighting the effectiveness of the proposed memory flow design. The 16-block configuration significantly outperforms the limited 1-block integration, validating that the proposed memory flow is highly advantageous for reducing perplexity and enhancing the overall capabilities of the model."
        },
        {
            "title": "4.4 Analysis of Memory Representa-\ntions\nTo gain deeper insights into the information en-\ncoded within the memory module, we utilize the\nNeuron Explainer method Bills et al. (2023). It\ngenerates natural language explanations of neuron\nbehavior, simulates activations using these descrip-\ntions, and evaluates their accuracy through predic-\ntive scoring. We utilize this approach to explain\nthe latent representations of specific memory slots,\nwhich helps understand how these slots process\nand retain task-relevant information. By analyz-\ning activations within the memory module, the\nNeuron Explainer identifies patterns in latent rep-\nresentations of each memory slot, mapping them\nto specific elements of the input text.",
            "content": "Figure 4 We sample question from MMLU to test the LM2 in few-shot fashion. To study how the memory module focuses on relevant information, we place useful information inside one of the few-shot examples. We evaluate LM2 using the input text illustrated in Figure 4. Subsequently, we identify and rank the most relevant memory slots, selecting two for sampling (slots 1679 and 1684) along with one of the least relevant memory slot (slot 1). Utilizing the neuron explainer, we investigate the relevance rationales. 8 LM2: Large Memory Models Explanation 4.1: Memory Slot 1679 This memory slots representations for this specific input text suggest that the memory modules focus is likely on detecting factual information, question and answer structures. These observations suggest that Memory Slot 1679 specializes in retrieving and synthesizing factual information for the target question, functioning as repository for domain-specific knowledge and structured reasoning. Explanation 4.2: Memory Slot 1684 This representations in this memory slot is designed to focus on specific elements within the input text, as evidenced by the pattern in the memory bank. Memory Slot 1684, in contrast, demonstrated focus on structural elements within the input text. Its activations aligned closely with linguistic markers and contextual cues, such as Options: or Answer:. This behavior implies that Memory Slot 1684 facilitates the models comprehension of input organization, enabling effective parsing of complex instruction formats and multi-part structures. Explanation 4.3: Memory Slot 1 The representations in this memory slot for the provided input text are primarily negative. This suggests that the module is not detecting the specific aspects it was designed to recognize in the input text. Memory Slot 1, on the other hand, showed predominantly negative activations across the input text, indicating minimal engagement with the task-specific content. These findings underline the importance of memory modules in gathering information for the generation tasks."
        },
        {
            "title": "4.5 Test-time memory adaptations\nWe further investigate how memory updates influence model generation during test time. To explore this, we\nanalyze the example illustrated in Figure 4. Cross attention heatmaps, presented in Figure 6, provide key\ninsights into these memory updates.",
            "content": "Figure 6a shows the cross attention heatmap prior to memory updates. In this figure, tokens such as France and Paris strongly engage with the memory. These tokens do not pertain specifically to the target question about photosynthesis. Instead, on the first pass, memory initially focuses on the structure of question as well as identifying factual information. Next, we examine the memory heatmap after various inference update steps (one inference step corresponds to single forward pass for one token). As depicted in Figure 6b, the tokens attended to by the memory slots shift toward those relevant to the target question. Since cross attention exclusively computes the relationships between input tokens and memory, this shift reflects the influence of test-time memory updates. These changes highlight the adaptive nature of memory during inference."
        },
        {
            "title": "5 Related Work",
            "content": "Memory augmented Transformers Various methods have been proposed to augment Transformers with memory. One direction is to optimize the attention mechanisms and use some global representations acting as memory points to ensure input coverage. Models like Longformer Beltagy et al. (2020), Big Bird Zaheer et al. (2020), GMAT Gupta and Berant (2020) and Extended Transformer Construction Ainslie et al. (2020) all proposed some sparse attention mechanisms to reduce the quadratic dependency of self-attention to linear and introduced global tokens to encode the information from the entire sequence. Another line of work introduces memorization capabilities to Transformers through recurrence. TransformerXL Dai et al. (2019) addresses the limitation of fixed-length context by introducing segment-level recurrence 9 LM2: Large Memory Models (a) Cross-attention heatmaps before memory update. (b) Cross-attention heatmaps after memory update. Figure 6 Cross-attention heatmaps between input tokens and memory. The x-axis shows the memory slots sorted by slot number. The y-axis shows the most attended tokens. Diagonal attentions are marked with +. and relative position encodings. However, during training, gradients are restricted to individual segments, limiting the models ability to capture long-term temporal dependencies. Recurrent Memory Transformer (RMT) Bulatov et al. (2022) mitigates these limitations by introducing more efficient memory mechanism. It adds recurrence to Transformers via small number of special overlapping memory tokens between segments of long sequences, enabling gradients to propagate across them while significantly reducing memory usage. RMT outperforms Transformer-XL for sequence processing tasks and is on par with Transformer-XL on language modeling, but requires less memory. Associative RMT (ARMT) Rodkin et al. (2024) is follow-up to RMT that addresses its time complexity issues. Similarly, MemReasoner Ko et al. (2024) introduces memory-augmented LLM architecture designed for temporal reasoning. However, as demonstrated by Kuratov et al. (2024) and Ko et al. (2024), RMT continues to outperform these subsequent models, maintaining its status as the state-of-the-art (SOTA) method. Therefore, we primarily consider RMT as the SOTA memory-based model and compare LM2 against it. Retrieval-Augmented Generation (RAG) Retrieval-Augmented Generation (RAG) Lewis et al. (2020) is popular solution for language models to handle large amounts of text. The core architecture of RAG comprises retriever module that identifies relevant information from knowledge base, ensuring that the input to the generative model remains within the token limit while filtering out irrelevant noise, thereby improving efficiency and response quality. While Retrieval-Augmented Generation (RAG) has proven effective for many tasks, it struggles with some complicated tasks like multi-hop question-answering Mavi et al. (2024), which require retrieving and reasoning over multiple interconnected pieces of evidence."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduced Large Memory Model (LM2), memory-augmented Transformer architecture designed to address long context reasoning challenges. The key innovation is the memory module, integrated inside the decoder blocks, which augments the model with additional memory information while also updating itself. Empirical results on the BABILong benchmark highlights LM2s advantages on various long context tasks. On average across tasks, LM2 outperforms the SOTA memory-augmented RMT model by 37.1%, and non-memory baseline Llama-3.2 model by 86.3%. Furthermore, LM2 achieves improvement over baselines on the MMLU benchmark, evidencing that its memory module does not degrade performance on general tasks. Overall, these findings underscore the importance of explicit memory mechanisms, and lay foundation for further research on integrating long-term memory into large language models. 10 LM2: Large Memory Models"
        },
        {
            "title": "References",
            "content": "Joshua Ainslie, Santiago Ontañón, Chris Alberti, Philip Pham, Anirudh Ravula, and Sumit Sanghai. ETC: encoding long and structured data in transformers. CoRR, abs/2004.08483, 2020. URL https://arxiv.org/abs/2004.08483. 9 Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer. CoRR, abs/2004.05150, 2020. URL https://arxiv.org/abs/2004.05150. 9 Steven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever, Jan Leike, Jeff Wu, and William Saunders. Language models can explain neurons in language models. https://openaipublic.blob.core. windows.net/neuron-explainer/paper/index.html, 2023. 8 Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. 1 Aydar Bulatov, Yuri Kuratov, and Mikhail S. Burtsev. Recurrent memory transformer, 2022. URL https://arxiv.org/ abs/2207.06881. 1, 2, 6, 10 Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc V. Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond fixed-length context. CoRR, abs/1901.02860, 2019. URL http://arxiv.org/ abs/1901.02860. 9 Jackie Dooley. The archival advantage: Integrating archival expertise into management of born-digital library materials. Archival Science Special Issue on Archiving Research Data, 7(1), March 2007. 3 Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 4 Ankit Gupta and Jonathan Berant. GMAT: global memory augmentation for transformers. CoRR, abs/2006.03274, 2020. URL https://arxiv.org/abs/2006.03274. 9 Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021. 2, 7 Jikun Kang, Romain Laroche, Xingdi Yuan, Adam Trischler, Xue Liu, and Jie Fu. Think before you act: Decision transformers with working memory. In ICML. OpenReview.net, 2024. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. 1 Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of naacL-HLT, volume 1, page 2. Minneapolis, Minnesota, 2019. 1 Ching-Yun Ko, Sihui Dai, Payel Das, Georgios Kollias, Subhajit Chaudhury, and Aurelie Lozano. Memreasoner: memory-augmented llm architecture for multi-hop reasoning. In The First Workshop on System-2 Reasoning at Scale, NeurIPS24, 2024. 1, 6, 10 Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Ivan Rodkin, Dmitry Sorokin, Artyom Sorokin, and Mikhail Burtsev. Babilong: Testing the limits of llms with long context reasoning-in-a-haystack, 2024. 1, 2, 6, 10 Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive NLP tasks. CoRR, abs/2005.11401, 2020. URL https://arxiv.org/abs/2005.11401. Qianxi Li, Yingyue Cao, Jikun Kang, Tianpei Yang, Xi Chen, Jun Jin, and Matthew Taylor. Laffi: Leveraging hybrid natural language feedback for fine-tuning language models. arXiv preprint arXiv:2401.00907, 2023. 1 11 LM2: Large Memory Models Yang Liu and Mirella Lapata. Hierarchical transformers for multi-document summarization. In Anna Korhonen, David Traum, and Lluís Màrquez, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 50705081, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/ v1/P19-1500. URL https://aclanthology.org/P19-1500/. 1 Allal Loubna, Ben, Lozhkov Anton, and Bakouch Elie. Small language models: Efficient, accessible, and effective. https://huggingface.co/blog/smollm, 2023. Accessed: 2025-01-16. 5 Vaibhav Mavi, Anubhav Jangra, and Adam Jatowt. Multi-hop question answering, 2024. URL https://arxiv.org/abs/ 2204.09140. 10 Guilherme Penedo, Hynek Kydlíček, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf, et al. The fineweb datasets: Decanting the web for the finest text data at scale. arXiv preprint arXiv:2406.17557, 2024. 5 Ivan Rodkin, Yuri Kuratov, Aydar Bulatov, and Mikhail Burtsev. Associative recurrent memory transformer, 2024. URL https://arxiv.org/abs/2407.04841. Jason Weston, Antoine Bordes, Sumit Chopra, and Tomás Mikolov. Towards ai-complete question answering: set of prerequisite toy tasks. In ICLR (Poster), 2016. 6 Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontañón, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird: Transformers for longer sequences. CoRR, abs/2007.14062, 2020. URL https://arxiv.org/abs/2007.14062. 9 Jinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin, Wengang Zhou, Houqiang Li, and Tie-Yan Liu. Incorporating bert into neural machine translation. arXiv preprint arXiv:2002.06823, 2020. 1 LM2: Large Memory Models"
        },
        {
            "title": "A BABILong Dataset",
            "content": "This section provides an overview of the tasks in BABILong. Each task targets specific aspect of language understanding and reasoning, forming core benchmark for assessing model performance on retrieve factors from long context. Task 1: Single Supporting Fact Goal : Identify and use exactly one piece of relevant information from the text to answer question. Key Challenge: Pinpointing the specific sentence or fact that directly yields the correct answer. Task 2: Two Supporting Facts Goal : Answer questions using two pieces of interconnected information. Key Challenge: Linking separate facts and understanding how they combine to produce the correct answer. Task 3: Three Supporting Facts Goal : Extend the reasoning chain to three distinct pieces of information. Key Challenge: Maintaining accuracy over longer inference chains and managing multiple pieces of related text. Task 4: Two Argument Relations Goal : Understand relationships involving two entities (arguments) to answer questions. Key Challenge: Correctly interpreting and manipulating relational information (e.g., who gave what to whom) with two entities. Task 5: Three Argument Relations Goal : Similar to Task 4 but introduces third entity in the relationship. Key Challenge: Tracking more complex interactions among three entities while maintaining clarity and correctness. Task 6: Yes/No Questions Goal : Provide binary (yes/no) answers based on the facts. Key Challenge: Determining whether sufficient evidence exists in the text to affirm or deny the query. Task 7: Counting Goal : Count the number of times or entities that meet certain conditions. Key Challenge: Performing numerical reasoning and accurately tracking quantities within the text. Task 8: Lists/Sets Goal : Gather all items satisfying specific criteria into list or set. Key Challenge: Aggregating multiple elements from different parts of the text into cohesive list/set. Task 9: Simple Negation Goal : Handle statements containing negation. Key Challenge: Understanding how negative statements (e.g., John did not pick up the apple) alter the truth value and impact the answer. Task 10: Indefinite Knowledge Goal : Work with statements that contain incomplete or uncertain information. Key Challenge: Managing and expressing knowledge not explicitly stated (e.g., Someone picked up the apple, but we dont know who)."
        },
        {
            "title": "B BABILong Benchmark Results",
            "content": "In Table 3, we present the whole expeirments of compared models on BABILong benchmark. 13 LM2: Large Memory Models Table 3 Detailed performance of BABILong benchmark model qa qa2 qa3 Llama-3.2-1.2B Llama-3.2-3.2B vanilla-Llama-1.7B RMT-1.7B LM2-1.7B 54.0 62.0 86.0 85.0 99.0 Llama-3.2-1.2B 48.0 Llama-3.2-1.2B-RAG 51.0 31.0 35.0 85. vanilla-Llama-1.7B RMT-1.7B LM2-1.7B Llama-3.2-1.2B 44.0 Llama-3.2-1.2B-RAG 52.0 58.0 44.0 25.0 LM2-1.7B RMT-1.7B vanilla-Llama-1.7B Llama-3.2-1.2B 37.0 Llama-3.2-1.2B-RAG 47.0 46.0 24.0 21. LM2-1.7B RMT-1.7B vanilla-Llama-1.7B Llama-3.2-1.2B 26.0 Llama-3.2-1.2B-RAG 36.0 34.0 14.0 17.0 LM2-1.7B RMT-1.7B vanilla-Llama-1.7B Llama-3.2-1.2B 24.0 Llama-3.2-1.2B-RAG 26.0 23.0 23.0 10. LM2-1.7B RMT-1.7B vanilla-Llama-1.7B Llama-3.2-1.2B 15.0 Llama-3.2-1.2B-RAG 28.0 19.0 12.0 10.0 LM2-1.7B RMT-1.7B vanilla-Llama-1.7B Llama-3.2-1.2B 11.0 Llama-3.2-1.2B-RAG 27.0 19.0 21.0 8. LM2-1.7B RMT-1.7B vanilla-Llama-1.7B Llama-3.2-1.2B-RAG 17.0 15.0 17.0 7.0 LM2-1.7B RMT-1.7B vanilla-Llama-1.7B 25.0 37.0 57.0 49.0 89.0 22.0 14.0 21.0 26.0 59.0 18.0 11.0 43.0 21.0 22. 16.0 3.0 37.0 20.0 18.0 11.0 1.0 12.0 15.0 19.0 6.0 2.0 17.0 9.0 11.0 7.0 1.0 13.0 16.0 17.0 8.0 0.0 18.0 18.0 13.0 0.0 16.0 13.0 14. 29.0 29.0 46.0 49.0 70.0 24.0 19.0 44.0 29.0 72.0 19.0 12.0 64.0 43.0 37.0 25.0 16.0 48.0 22.0 38.0 24.0 5.0 31.0 25.0 26.0 19.0 9.0 28.0 18.0 21. 15.0 2.0 20.0 20.0 24.0 13.0 4.0 19.0 19.0 14.0 3.0 12.0 20.0 19.0 qa5 qa6 qa qa8 qa9 qa10 59.0 82.0 85.0 95.0 98.0 69.0 80.0 71.0 61.0 91.0 64.0 75.0 87.0 79.0 58. 56.0 68.0 78.0 28.0 55.0 52.0 72.0 63.0 25.0 41.0 46.0 76.0 39.0 19.0 37.0 46.0 74.0 31.0 22.0 30.0 42.0 66.0 22.0 15.0 16.0 73.0 23.0 18.0 13. 49.0 53.0 83.0 84.0 95.0 49.0 49.0 60.0 50.0 84.0 52.0 48.0 73.0 47.0 60.0 50.0 58.0 66.0 46.0 61.0 44.0 49.0 53.0 47.0 51.0 55.0 45.0 44.0 47.0 59. 54.0 51.0 50.0 46.0 54.0 43.0 54.0 42.0 48.0 52.0 49.0 48.0 47.0 52.0 14.0 25.0 95.0 82.0 96.0 9.0 10.0 71.0 83.0 96.0 18.0 5.0 93.0 78.0 65. 14.0 3.0 93.0 75.0 64.0 25.0 8.0 95.0 74.0 60.0 20.0 5.0 93.0 75.0 61.0 23.0 2.0 92.0 74.0 71.0 17.0 4.0 91.0 70.0 60.0 5.0 91.0 72.0 63. 52.0 53.0 79.0 78.0 97.0 31.0 38.0 40.0 41.0 69.0 24.0 33.0 53.0 41.0 38.0 27.0 36.0 45.0 35.0 35.0 19.0 28.0 40.0 38.0 37.0 13.0 29.0 38.0 33.0 34. 13.0 19.0 35.0 34.0 33.0 6.0 15.0 35.0 33.0 30.0 11.0 34.0 35.0 28.0 41.0 65.0 83.0 85.0 99.0 55.0 40.0 67.0 68.0 82.0 55.0 50.0 75.0 69.0 66. 55.0 45.0 62.0 65.0 49.0 44.0 44.0 57.0 65.0 42.0 47.0 52.0 48.0 62.0 46.0 53.0 41.0 59.0 62.0 39.0 48.0 48.0 51.0 61.0 41.0 46.0 54.0 64.0 46. 22.0 56.0 77.0 76.0 94.0 33.0 46.0 58.0 53.0 77.0 42.0 43.0 69.0 51.0 58.0 32.0 39.0 50.0 45.0 53.0 40.0 35.0 49.0 46.0 45.0 48.0 36.0 42.0 42.0 46. 46.0 32.0 39.0 43.0 53.0 33.0 44.0 38.0 41.0 41.0 39.0 38.0 42.0 42.0 qa4 0K 62.0 64.0 59.0 81.0 88.0 1K 55.0 59.0 43.0 33.0 68.0 2K 50.0 49.0 43.0 41.0 34.0 4K 56.0 58.0 34.0 24.0 28.0 8K 40.0 57.0 26.0 28.0 20.0 16K 33.0 59.0 28.0 23.0 11. 32K 24.0 51.0 23.0 18.0 13.0 64K 12.0 56.0 19.0 21.0 14.0 128K 51.0 19.0 21.0 12."
        }
    ],
    "affiliations": [
        "Convergence Labs Ltd."
    ]
}