{
    "paper_title": "Repair-R1: Better Test Before Repair",
    "authors": [
        "Haichuan Hu",
        "Xiaochen Xie",
        "Quanjun Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "APR (Automated Program Repair) aims to automatically locate program defects, generate patches and validate the repairs. Existing techniques for APR are often combined with LLMs (Large Language Models), which leverages the code-related knowledge of LLMs to improve repair effectiveness. Current LLM-based APR methods typically utilize test cases only during the inference stage, adopting an iterative approach that performs repair first and validates it through test execution afterward. This conventional paradigm neglects two important aspects: the potential contribution of test cases in the training phase, and the possibility of leveraging testing prior to repair. To address this, we propose Repair-R1, which introduces test cases into the model's training phase and shifts test generation to precede repair. The model is required to first generate discriminative test cases that can distinguish defective behaviors, and then perform repair based on these tests. This enables the model to better locate defects and understand the underlying causes of defects, thereby improving repair effectiveness. We implement Repair-R1 with three different backbone models, using RL (reinforcement learning) to co-optimize test generation and bug repair. Experimental results on four widely adopted benchmarks demonstrate the superiority of Repair-R1. Specially, compared to vanilla models, Repair-R1 improves repair success rate by 2.68\\% to 48.29\\%, test generation success rate by 16.38\\% to 53.28\\%, and test coverage by 0.78\\% to 53.96\\%. We publish the code and weights at https://github.com/Tomsawyerhu/APR-RL and https://huggingface.co/tomhu/Qwen3-4B-RL-5000-step."
        },
        {
            "title": "Start",
            "content": "Repair-R1: Better Test Before Repair Haichuan Hu, Alibaba Cloud, 522022320050@smail.nju.edu.cn Xiaochen Xie, Zhejiang University, xcxie@zju.edu.cn Quanjun Zhang, Nanjing University of Science and Technology, quanjunzhang@njust.edu.cn 1 5 2 0 J 0 3 ] . [ 1 3 5 8 2 2 . 7 0 5 2 : r Abstract APR (Automated Program Repair) aims to automatically locate program defects, generate patches and validate the repairs. Existing techniques for APR are often combined with LLMs (Large Language Models), which leverages the code-related knowledge of LLMs to improve repair effectiveness. Current LLM-based APR methods typically utilize test cases only during the inference stage, adopting an iterative approach that performs repair first and validates it through test execution afterward. This conventional paradigm neglects two important aspects: the potential contribution of test cases in the training phase, and the possibility of leveraging testing prior to repair. To address this, we propose Repair-R1, which introduces test cases into the models training phase and shifts test generation to precede repair. The model is required to first generate discriminative test cases that can distinguish defective behaviors, and then perform repair based on these tests. This enables the model to better locate defects and understand the underlying causes of defects, thereby improving repair effectiveness. We implement Repair-R1 with three different backbone models, using RL (reinforcement learning) to co-optimize test generation and bug repair. Experimental results on four widely adopted benchmarks demonstrate the superiority of Repair-R1. Specially, compared to vanilla models, Repair-R1 improves repair success rate by 2.68% to 48.29%, test generation success rate by 16.38% to 53.28%, and test coverage by 0.78% to 53.96%. We publish the code and weights at Github and HuggingFace. I. INTRODUCTION APR (Automated Program Repair) aims to automatically locate and fix potential code-related bugs, preventing unexpected behavior under specific inputs, thereby improving the reliability of the program. Since the emergence of LLMs (Large Language Models), they have been widely applied to APR. LLMs learn typical error patterns and repair strategies from existing defect datasets, enabling the generalization of repair capabilities to extensive scenarios. However, existing LLM-based APR approaches primarily use buggy programs and patches during training, and largely ignore test cases, treating them merely as tools for patch validation. This can lead to two main problems. First, the model becomes overly dependent on similar bugs. Through pre-training and fine-tuning, the model tends to match syntactically and semantically similar bugs from its parameterized knowledge when encountering new ones, attempting to resolve current issues based on past experience. Although different bugs may appear very similar in text and functionality, their root causes are often quite distinct. As result, the model may generate superficially similar patches without truly understanding or deeply analyzing the underlying defect. This is akin to relying on rote memorization and doing large volumes of exercises to improve exam performancean approach that often backfires. The second issue is the underutilization of test data. Test cases are among the most effective tools for identifying bugs, and software projects are often accompanied by large number of unit tests to ensure code quality. 2 Moreover, during the inference phase, developers typically use error messages to guide the model in performing bug repair. These facts sufficiently highlight the critical role that test cases play in the repair task. Therefore, ignoring test information during the training phase of repair models represents significant waste. Simply using test cases for test-time scaling is insufficient; instead, test cases should be incorporated into the training phase to enable scaling at the training stage of the repair model. To address the above issues, we propose Repair-R1, which organically combines the tasks of test generation and APR during the training phase. The core idea of Repair-R1 is illustrated in Figure 1, with the aim of enabling the model to first generate test cases that help locate the bug before generating patches. Specifically, such discriminative test cases have two characteristics: first, they must pass the correct code, ensuring the correctness of the test cases themselves; second, they should fail the buggy code, demonstrating their ability to distinguish between correct and defective implementations. To implement Repair-R1, we design novel reinforcement learning (RL) framework, as shown in Figure 2. Each optimization step of Repair-RL is divided into two stages: first, the test generation phase produces discriminative test cases based on the bug, and then the test generation reward is computed according to the effectiveness ratio of the generated test cases. Next, patches are generated based on the given bug and the discriminative test cases, and its correctness is evaluated by running oracle tests, from which the code repair reward is calculated. Finally, the policy model is jointly optimized by combining both rewards. To enhance the effectiveness of the oracle tests, we also perform test augmentation. Although the correct code is used as the ground-truth patch to validate the correctness of generated test cases, this utilization is implicit. The model does not directly learn the correct patches in supervised manner, but instead explores the space of possible solutions through self-play. Analogous to solving problems, the supervised learning approach is like teacher directly revealing the answers, whereas Repair-R1s training is more like solving the problem on ones own and then being told whether the answer is correct without being shown the details of the solution. Compared to APR models trained via supervised learning, Repair-R1 demonstrates better generalization ability. The result curves in Figure 1 also indicate that as the testing capability improves, the repair capability of Repair-R1 is further enhanced. We summarize our contributions as follows: (1) We propose Repair-R1, novel method that utilizes reinforcement learning to train models to generate discriminative tests prior to code repair. This approach helps in locating and understanding bugs, while simultaneously improving the models abilities in both test generation and bug repair. To the best of our knowledge, Repair-R1 is the first work that employs reinforcement learning to leverage test generation as guiding mechanism for optimizing the code repair process. (2) We validate the effectiveness of Repair-R1 under configuration involving three models, four benchmarks, and five different methods (a total of 60 experiments). Experimental results show that compared to the untrained model, Repair-R1 improves repair success rate by 2.68% to 48.29%, test generation success rate by 16.38% to 53.28%, and test coverage by 0.78% to 53.96%. Through comparative experiments and ablation studies, we also demonstrate the superiority of Repair-R1 over SFT and single-objective optimization. (3) Repair-R1 points out new direction for LLM-based APR, breaking away from the conventional test-driven repair paradigm of repairing first and testing later, and offers novel perspective on the repair process. 3 Figure 1: This is real-world example of bug fixing related to email format validation. To fix the bug, the APR model possibly generates four types of test cases. The first test case passes both the buggy code and the correct code, and thus does not have the ability to expose the bug. The second test case fails in both the buggy and correct code, making it completely incorrect and having no reference value. The third test case passes the buggy code but fails in the correct one which is exactly the opposite of what is expected. The fourth test case passes the correct code but fails in the buggy code, precisely covering the scenario where the bug is triggered. For the purpose of repair, the fourth type of test case has the capability to distinguish between buggy and correct code, and can help the model understand the bug, thereby improving the success rate of the repair. We expect that during the repair process, the model will generate as many of the fourth-type test cases as possible. To this end, we train the model using RL to jointly optimize test generation and bug repair capabilities. Figures (a)-(d) reflect the simultaneous improvement in test generation and bug repair performance during the models (use Qwen2.5-Coder-1.5B-Instruct as an example) training and testing phases. A. Automated Program Repair II. BACKGROUND AND RELATED WORKS APR aims to assist developers in localizing and fixing program bugs automatically. Traditional APR techniques can be classified as heuristic-based [1, 2, 3], constraint-based [4, 5, 6] and template-based [7, 8]. Although effective, traditional APR approaches are plagued by issues such as high computational cost and limited generalization ability. Modern APR methods, primarily based on deep learning, have improved upon the shortcomings of previous APR methods. As part of learning-based methods, Neural Machine Translation (NMT) techniques have been extensively studied in recent years, e.g., TENURE [9], Tare [10], SelfAPR [11], RewardRepair [12]. They share the same insight 4 that APR can be viewed as an NMT problem that aims to translate buggy code into correct code. One downside of NMT-based methods is that they relatively sensitive to noise in the training dataset. To improve this, LLM-based methods [13, 14, 15] leverages the general capabilities of LLMs in code-related tasks, enabling them to fix bugs through zero-shot or few-shot methods, thereby reducing the dependence on high-quality training datasets. However, most existing APR models are trained only on buggy programs and patches, ignoring the importance of test information. Test cases are used merely as tools to validate patch correctness during the inference phase."
        },
        {
            "title": "The insufficient utilization of test cases in APR inspires us to incorporate test information into the training phase",
            "content": "of APR models. We combine the test generation task with the APR task and leverage RL to jointly optimize the models ability in both test generation and bug repair. Consequently, the model can understand and fix bugs better by generating discriminative test cases that reveal the root cause of bugs. B. Reinforcement Learning Proximal Policy Optimization (PPO) [16] is one of the most popular and representative RL algorithm, which uses an actor-critic setup with clipped updates for stability. Different from PPO, Direct Preference Optimization (DPO) and its variants [17, 18, 19, 20] skip the critic and directly optimize from preferences using closed-form rewards, improving efficiency. Recent efficient Group Relative Policy Optimization (GRPO) [21] scales well with large-scale reinforcement learning [22] by optimizing policies through relative preferences among groups of actions, offering improved stability and performance over traditional methods. Reinforcement learning applied specifically to coding tasks has also gained traction [23, 24]. In this paper, we adopt GRPO to optimize Repair-R1 not only for performance considerations. In addition, through mathematical derivation, we reformulate the co-optimization problem as an ELBO (Evidence Lower Bound) maximization problem. We further find that, with appropriate reward design, the optimization objective of GRPO aligns with that of ELBO, making GRPO well-suited choice for Repair-R1. III. APPROACH Figure 2 illustrates the overall optimization process of Repair-R1. Before starting the optimization, we copy the original model and refer to it as the reference model, and then perform optimization on the current policy model. At each optimization step, we input the buggy code into the model, and first ask the policy model to generate test cases that can expose the bug. Based on these test cases and the bug, we then ask the policy model to generate corresponding patches. The prompt is given in Figure 3. We collect the test cases and patches produced by the policy model and evaluate their effectiveness. Specifically, we adopt reinforcement learning approach to jointly optimize the policy models test generation capability and code repair capability. Separate rewards are computed for the policy model in terms of test generation and code repair, and format reward is also introduced to standardize the output format and syntax. Subsequently, advantages are calculated for each sample using the obtained rewards, and the KL divergence between the policy and reference models is computed. The policy model is then updated using both the advantage and the KL divergence. Details on the reinforcement learning algorithm design and the reward computation can be found in Sections III-C and III-D, respectively. 5 Figure 2: The Optimization Process of Repair-R A. Motivation Given bug, the bug repair model will attempt to fix it based on the code-related knowledge it has acquired from pre-trained datasets. The success of repairs may stem from one of two possibilities. (1) The model accurately identifies the underlying cause of the bug and applies targeted fix. (2) The model recovers similar solution by leveraging memorized patterns from the training data. Clearly, the former case is preferable, as it reflects the models capacity for generalization and its ability to address novel and more complex bugs. Therefore, our motivation is to enhance the models ability to understand and identify bugs, thereby enabling it to achieve stronger code repair capabilities. B. Joint Optimization of Repair and Test Traditional LLM-based APR is formalized as: π(p, b) = (cid:89) i=1 π(pi p<i, b) (1) Where π represents the probability distribution of the repair model, represents the bug under repair, and represents n-token patch output by the model π. As discussed in the motivation section, we expect the model π to comprehend the underlying cause of the bug before generating the corresponding patch p. Thus, we introduce 6 Figure 3: The prompt used for optimizing Repair-R1. latent variable to represent the underlying cause of the bug. After the introduction of z, the code repair task can be further formalized as follows. (cid:90) π(p, b) = π(p z, b) π(z b) dz Our goal is to optimize the model parameter θ. θ = max θ EbD [log πθ(p, b)] (2) (3) Since πθ(p, b) involves an intractable integral over the latent variable z, direct computation or maximization is not feasible. To address this, we introduce an approximate posterior distribution qϕ(z p, b) to approximate the true posterior πθ(z p, b). Based on this approximation, we derive lower bound of the log-likelihood known as ELBO, which serves as the optimization objective. ELBO is defined as follows. ELBO = Eqϕ(zp,b) [log πθ(p z, b)] KL (qϕ(z p, b) πθ(z b)) (4) 7 In the field of code repair, as unit tests are commonly employed to evaluate patch correctness, identifying test case that discriminates between the correct and buggy versions effectively corresponds to pinpointing the underlying bug cause. Therefore, πθ(z b) is converted into test generation task, which is optimized by both the correctness of generated test cases and their capabilities to discriminate between correct and buggy code. Reward design for test generation is detailed in Section III-D. Before generating patches, the model is first asked to generate such test cases, and then output patches based on the generated test cases and the original bug. Eqϕ(zp,b) [log πθ(p z, b)] reflects the correctness of the generated patches, we optimize this objective by enhancing the pass rate of the patches on the oracle test cases. Meanwhile, the ELBO optimization objective encourages KL (qϕ(z p, b) πθ(z b)) to be as small as possible, which ensures that the optimized model parameters remain close to the original ones. C. Reinforcement Learning Algorithm By incorporating the latest model optimization techniques, we find that the GRPO (Group Relative Policy Optimization) algorithm aligns well with our task objectives. GRPO is used by DeepSeek-R1 and offers strong advantages in balancing cost and performance. On one hand, through reward design, the objective function of GRPO can encompass the optimization goals within the ELBO Formula 4 , including minimizing KL divergence and maximizing patch correctness. On the other hand, GRPO is well-suited for unsupervised training, where we rely solely on the test pass rate as the optimization signal. By avoiding the use of any supervised learning algorithms, we prevent the model from memorizing patches in the training data, leading to improved generalization performance. In this study, we adopt GRPO to optimize the code repair model. GRPO aims to maximize the following objective function. JGRPO(θ) = ExD, y1:Gπold(x;C) (cid:34) 1 (cid:88) i= 1 yi yi (cid:88) t=1 min (cid:16) πθ πref (cid:0)yi,t x, yi,<t; C(cid:1) (cid:0)yi,t x, yi,<t; C(cid:1) (cid:35) ˆAi,t, (5) (cid:0)yi,t x, yi,<t; C(cid:1) (cid:0)yi,t x, yi,<t; C(cid:1) , 1 ϵ, 1 + ϵ Where ϵ and β are hyper-parameters, ˆAi,t is the advantage, computed from the relative rewards of responses (cid:2)πθ πref (cid:16) πθ πref β DKL (cid:17) ˆAi,t clip (cid:3), (cid:17) within each group, and yi,t is tth token generated by the policy model. GRPO incorporates the KL divergence term DKL between the policy model and the reference model directly into the loss. During each step of training, GRPO first samples group of responses from the policy model, then calculate rewards, advantages and KL divergence term for further optimization. D. Reward Modeling In Repair-R1, we employ rule-based rewards instead of using reward model for the following reasons. First, rule-based rewards have been proven to be effective on code-related tasks. Since the correctness of patch is generally evaluated by oracle test cases, pass rate can naturally serve as reliable and interpretable signal for measuring patch quality. Second, rule-based rewards are more straightforward to design and debug compared to learned reward models, which often require large amounts of annotated data and extensive training. 8 Specifically, we design three types of rule-based rewards: format reward, code repair reward, and test generation reward. The format reward ensures that the generated outputs are syntactically correct and follow the desired formatting conventions. The code repair reward encourages the model to produce successful and semantically accurate code repairs. The test generation reward promotes the generation of valid and discriminative test cases that are useful for evaluating the quality of patches. Equal weights are assigned to all three types of rewards. Format Reward. The format reward is designed to enforce specific output structure for the model. Specifically, we introduce text-based format reward to encourage the model to generate responses in the < test > ... < /test >< patch > ... < /patch > format, ensuring structured outputs for both code repair and test generation. In contrast to DeepSeek-R1, we do not require the use of < think > ... < /think >, as we observe that excessive thinking steps can lead to performance degradation in code-related tasks. In addition to the text-based reward, we incorporate syntax-based format reward to ensure that the models outputs adhere to syntactically valid formats. For code, we check syntactic correctness, while for test cases, it requires structured JSON list with multiple supporting specifications (e.g., assertions, input/output-style). The format reward is formalized as: Rf(a) = α (a) (cid:124) (cid:123)(cid:122) (cid:125) Text-based reward + β [C(acode) + (atest)] (cid:125) (cid:123)(cid:122) Syntax-based reward (cid:124) (6) Where represents the response of the model, acode represents the patch extracted from a, atest represents the test cases extracted from a, represents the result of format validation, and represents the result of code compiling. Code Repair Reward. The code repair reward assesses the effectiveness of the repair process and reflects both the correctness and quality of the generated patches. Rather than treating patch correctness as binary outcome (correct/incorrect), we model it as continuous value in the range [0, 1], determined by the pass rate of oracle test cases. pass rate of 100% implies full correctness, whereas 0% pass rate indicates complete failure. Although the original datasets typically contain sufficient number of oracle test cases (often ten or more), to improve the robustness of test-based evaluation, we conduct test case augmentation. The augmented test cases are validated using manually written reference patches to ensure their correctness and exclude invalid ones. The code repair reward is formalized as: Rr(a) = 1 (cid:88) tT (t, acode) (7) Where represents the response of the model, acode represents the patch extracted from a, represent the augmented set of test cases. For each test case in , we evaluate acode on t, and (t, acode) {0, 1} depends on whether acode passes t. Then, the total pass rate of is calculated as the code repair reward Rr. Test Generation Reward. First, we categorize the generated test cases into two categories: valid and invalid, based on their execution outcomes on both the buggy and fixed versions of the code. valid test case satisfies two conditions. First, it is correct, with input-output behavior that matches the expected specification. Second, it is discriminative, meaning it can distinguish between the buggy and correct versions of the code, thereby helping the model locate the defect. Formally, the validity of generated test case is defined as: Vt = ft(t, G) (1 ft(t, B)) 9 (8) Where the validity Vt of test is equal to 1 if and only if the ground-truth code passes (ft(t, G) = 1) and the buggy code fails (ft(t, B) = 0). all other cases, Vt is set to 0. Building upon this, the test generation reward further assesses the models capability of generating multiple test cases and is formally defined as: Rt = 1 (cid:88) i=1 Vi = 1 (1 F)P (9) Where vector = [f (t1, G), (t2, G), . . . , (tn, G)] indicates whether each test case passes on the ground-truth code G, vector = [f (t1, B), (t2, B), . . . , (tn, B)] indicates whether each test case passes on the buggy code B, and is the number of generated tests. If no tests is generated and = 0, Rt is set to 0. A. Dataset Construction IV. EXPERIMENTAL SETUP Dataset construction involves four main steps: collecting normal samples, generating defective variants, performing defect validation and filtering, and train-test split. Normal Samples Collection. We select four widely used benchmarks for code generation, including HumanEval, MBPP, CodeForces, CodeContests. For HumanEval and MBPP, we included all available samples. In the case of CodeContests and CodeForces, we execute the ground truth solutions on the oracle tests and filter out samples with runtime of less than 3 seconds. Defective Variants Generation. For all collected samples, we use GPT-4o as the mutation model and require it to generate at least 10 defective versions for each sample. Defect Validation and Filtering. To ensure the validity and distinctiveness of the generated mutants, we first run the oracle test on each mutant to verify that the defect could be detected (i.e., killed by the test). Based on the results, we further remove semantically redundant mutants to maintain the quality and diversity of the dataset. Train-test Split. Finally, the filtered defect dataset was partitioned into training and test sets at 4:1 ratio, with care taken to ensure that no original sample appeared in both sets, thereby preventing data contamination. Specifically, the training set contains 5,421 samples, and the test set contains 1,358 samples. B. Evaluation Metrics We evaluate Repair-R1 on the following three metrics: Bugfix: Bug fix rate. Test: Success rate of generating test cases that can both pass the ground truth patch and fail the buggy code. Tcov: The proportion of bugs covered by at least one test that can pass the ground truth patch and fail the buggy code. 10 V. EXPERIMENT RESULTS As shown in Table I, we evaluate three Qwen models with varying sizes and architectures across four defect benchmarks. For each benchmark, we conduct five experimental configurations: (1) using the vanilla model without any adaptation, marked as Vanilla; (2) fine-tuning on defect-repair pairs, marked as SFT; (3) applying reinforcement learning solely on the repair task, marked as RL-Repair; (4) applying reinforcement learning solely on the test generation task, marked as RL-Test; and (5) employing collaborative reinforcement learning on both tasks in joint manner, marked as RL-Both. In the following sections, we present an analysis of the experimental results. Table I: The performance of Repair-R1 (RL-Both) is assessed across four widely-used benchmarks, including HumanEval, MBPP, CodeForces and CodeContests. Model HumanEval (112 bugs) MBPP (257 bugs) CodeForces (585 bugs) CodeContests (404 bugs) Bugfix Test Tcov Bugfix Test Tcov Bugfix Test Tcov Bugfix Test Tcov Qwen2.5-Coder-1.5B-Instruct (Vanilla) 33.04% 17.34% 37.50% 23.74% Qwen2.5-Coder-1.5B-Instruct (SFT) 23.21% 3.93% 8.04% 9.34% 9.58% 2.09% 23.74% 4.79% 5.84% 16.75% 7.99% 7.08% 14.36% 5.45% 9.57% 24.01% 1.67% 1.69% 2.72% 2.48% Qwen2.5-Coder-1.5B-Instruct (RL-Test) 33.93% 48.23% 73.21% 28.40% 28.99% 58.37% 3.25% 39.57% 51.45% 3.71% 50.84% 52.48% Qwen2.5-Coder-1.5B-Instruct (RL-Repair) 75.00% 37.54% 52.71% 62.65% 25.48% 32.37% 44.10% 31.59% 38.72% 41.83% 35.85% 45.43% Qwen2.5-Coder-1.5B-Instruct (RL-Both) 81.25% 48.07% 71.43% 66.15% 29.00% 43.58% 46.67% 40.76% 50.09% 39.85% 54.95% 56.68% Qwen2.5-Coder-3B-Instruct (Vanilla) 51.79% 24.45% 56.25% 38.91% 18.54% 44.36% 10.26% Qwen2.5-Coder-3B-Instruct (SFT) 19.53% 6.43% 14.06% 22.57% 6.68% 13.54% 29.02% 9.89% 6.22% 29.91% 13.12% 15.03% 33.05% 5.64% 6.10% 16.09% 12.53% Qwen2.5-Coder-3B-Instruct (RL-Test) 56.25% 48.59% 59.82% 46.30% 40.70% 46.69% 14.70% 40.77% 61.54% 12.87% 55.24% 58.42% Qwen2.5-Coder-3B-Instruct (RL-Code) 79.46% 26.74% 58.93% 68.87% 20.04% 48.25% 52.31% 16.68% 41.37% 47.77% 8.71% 23.02% Qwen2.5-Coder-3B-Instruct (RL-Both) 85.71% 50.10% 60.71% 69.65% 36.58% 45.14% 53.85% 41.74% 63.76% 50.00% 55.22% 59.16% Qwen3-4B (Vanilla) Qwen3-4B (SFT) Qwen3-4B (RL-Test) Qwen3-4B (RL-Code) Qwen3-4B (RL-Both) 83.93% 40.00% 81.25% 48.64% 26.18% 53.31% 35.21% 27.52% 57.26% 34.16% 11.93% 31.44% 66.07% 8.22% 16.96% 67.70% 12.69% 28.40% 54.70% 4.00% 5.47% 45.30% 1.79% 3.47% 71.43% 57.70% 77.68% 58.37% 50.11% 70.04% 31.28% 44.55% 68.38% 30.94% 56.86% 62.38% 84.82% 40.50% 86.61% 70.82% 26.03% 61.87% 51.79% 30.90% 64.62% 51.49% 14.99% 40.10% 86.61% 59.49% 73.21% 71.98% 55.27% 73.15% 54.19% 43.90% 66.67% 51.73% 58.11% 61.39% A. Test Generation Helps Repair Better Through RL training, the repair capability of Repair-R1 is significantly improved compared to the vanilla model, and the test generation ability is also enhanced accordingly. As shown in Table I, all three models demonstrate consistent improvements in both repair and test generation across the four benchmarks. The repair success rate increases by 2.68% to 48.29%, the test generation success rate improves by 16.38% to 53.28%, and the test coverage is enhanced by 0.78% to 53.96%. Figure 4: comparison of test generation and repair correlation before and after RL 11 To further explore whether enhancements in test generation can facilitate improvements in repair capability, we classify the combinations of repair outcomes and test generation results into four categories: (1) Successful repair with effective test case generation, marked as fix w/ test. (2) Successful repair without effective test case generation, marked as fix w/o test. (3) Failed repair with effective test case generation, marked as fail w/ test. (4) Failed repair without effective test case generation, marked as fail w/o test. As shown in Figure 4, we compare the distribution of four types of samples across the three models before and after RL training. We observe that for the vanilla models, the ability to generate tests is highly limited, which in turn constrains the effectiveness of repair. Most successful repairs are accompanied by failed test cases and generally exhibit low success rates, indicating that the model merely outputs syntactically correct code without understanding the underlying cause of the defect. In contrast, after RL training, both test generation and repair capabilities are significantly improved. Successful repairs are often associated with correctly generated test cases, suggesting that the model can now perform more effective repairs based on better understanding of the defect location and its root cause. B. Repair-R1 Outperforms SFT on Imbalanced Datasets To broadly evaluate the generalization capability of Repair-R1 across different types of bugs, we pay special attention to the type of samples when selecting benchmarks. Specifically, HumanEval and MBPP consist of functionlevel bugs without entry points, while CodeContests and CodeForces use standard input and output formats. Moreover, the dataset exhibits an imbalanced distribution, with HumanEval and MBPP contributing only small portion (approximately one-third) of the total samples. The results in Table indicate that the disparities in data types and the imbalance in distribution have notably impacted the effectiveness of SFT. For the more prevalent benchmarks, CodeForces and CodeContests, the model shows improved repair performance after SFT. Specifically, Qwen2.5-Coder-1.5B-Instruct improves the repair success rate by 11.96% and 18.56%, Qwen2.5-Coder-3B-Instruct by 18.76% and 19.93%, and Qwen-4B by 19.49% and 11.14% on CodeForces and CodeContests, respectively. However, for the less-represented benchmarks, HumanEval and MBPP, the model exhibits forgetting effect after fine-tuning, with significant drop in repair performance compared to before training. Specifically, Qwen2.5-Coder-1.5B-Instruct decrease repair success rate by 9.83% and 14.4% and Qwen2.5-Coder-3B-Instruct by 32.26% and 16.34% on HumanEval and MBPP, respectively. In contrast, RL avoids this issue. The model trained with RL demonstrates consistently improved repair performance across all benchmarks. The improvement in repair success rate across the four benchmarks ranges from 2.68% to 48.29% for the three models. By comparing SFT and RL, we find that SFT merely fits the data distribution during training. Since pre-training tasks typically do not involve code repair, significant parameter updates occur when the model is fine-tuned on defect datasets, leading it to overfit to the dominant distributions (e.g., CodeContests and CodeForces), while suffering from forgetting on underrepresented datasets (e.g., HumanEval). In contrast, RL builds upon the models foundational coding capabilities and incrementally learns repair patterns, thereby enhancing repair performance without causing catastrophic forgetting. 12 C. Ablation Study We conduct an ablation study on the tasks of test generation and code repair. In the RL training process, we retain only the Code Repair Reward or the Test Generation Reward separately, and evaluate the performance of the trained model on both test generation and code repair tasks. As illustrated in Table I, training the model exclusively on test generation appears to result in marginal improvement in repair capability. This observation is supported by certain results. For instance, Qwen2.5-Coder-3BInstruct (Test) achieves an increase in repair success rates of 4.46%, 7.39%, and 4.44% on HumanEval, MBPP, and CodeForces, respectively, compared to the vanilla model. Nevertheless, this improvement is not stable, with Qwen-4B (Test) exhibiting decrease in repair effectiveness on HumanEval, CodeForces, and CodeContests. On the other hand, models trained solely on the repair task also show improvements in test generation. This trend is stable, with all three models achieving varying degrees of improvement in test coverage and test effectiveness across four benchmarks. Building upon these findings, we compare the two ablation models with Repair-R1 and observe that Repair-R1 demonstrates the most favorable performance in terms of both repair and test generation. Specifically, compared to RL-Repair, Repair-R1 achieves repair success rate improvement ranging from 0.24% to 6.25%, with improvements observed in 11 out of 12 comparison settings. Regarding test generation, Repair-R1 achieves comparable level of test coverage as RL-Test, while demonstrating higher success rate in test generation. D. Test-time Scaling Performance of Repair-R1 We analyze the test-time scaling ability of Repair-R1 across different benchmarks and models. As shown in Figure 5, we present the repair success rate of Repair-R1 under different sampling configurations, with sample sizes varying from 1 to 8. In general, Qwen-4B exhibits the best scaling capability among the three models, achieving superior performance on HumanEval and CodeForces, comparable results with Qwen2.5-Coder-3B-Instruct on CodeContests, and slightly lower performance on MBPP. We believe this is not entirely due to the difference in parameter scale. We observe that when the sampling size is less than or equal to 4, Qwen-4B performs slightly worse than Qwen2.5-Coder-3B-Instruct overall, as the latter is code-specific language model (CodeLM) and thus has direct advantage on code-related tasks. However, as the sampling size increases, the repair performance of Qwen-4B gradually surpasses that of Qwen2.5-Coder-3B-Instruct. This can be attributed to the fact that Qwen-4B is reasoning model, which benefits more from larger sampling sizes by leveraging diverse reasoning paths to generate higher-quality patches. This trend also suggests that while specialized models like Qwen2.5-Coder-3B-Instruct and Qwen2.5-Coder-1.5B-Instruct may have an edge in low-sampling scenarios due to their task-specific training, general-purpose reasoning models such as Qwen-4B can catch up and even outperform them when given more samples, thanks to their broader knowledge base and stronger reasoning capabilities. 13 (a) HumanEval (b) MBPP (c) CodeForces (d) CodeContests Figure 5: Bugfix@K performance of Repair-R1 on different benchmarks. VI. CONCLUSION This paper presents Repair-R1, method that leverages the generation of discriminative test cases to guide the model in performing bug repair, thus jointly improving its abilities in both test generation and bug repair. We design reward functions for both test generation and code repair, and employ the GRPO algorithm to optimize the model. Experimental results show that compared to vanilla models, Repair-R1 improves repair success rate by 2.68% to 48.29%, test generation success rate by 16.38% to 53.28%, and test coverage by 0.78% to 53.96%. Comparative experiments and ablation studies also demonstrate the superiority of Repair-R1 over SFT and single-objective optimization. REFERENCES [1] C. Le Goues, T. Nguyen, S. Forrest, and W. Weimer, Genprog: generic method for automatic software repair, Ieee transactions on software engineering, vol. 38, no. 1, pp. 5472, 2011. [2] M. Martinez and M. Monperrus, Astor: program repair library for java, in Proceedings of the 25th international symposium on software testing and analysis, 2016, pp. 441444. [3] Y. Yuan and W. Banzhaf, Arja: Automated repair of java programs via multi-objective genetic programming, IEEE Transactions on software engineering, vol. 46, no. 10, pp. 10401067, 2018. [4] T. Durieux and M. Monperrus, Dynamoth: dynamic code synthesis for automatic program repair, in Proceedings of the 11th International Workshop on Automation of Software Test, 2016, pp. 8591. [5] M. Martinez and M. Monperrus, Ultra-large repair search space with automatically mined templates: The cardumen mode of astor, in International symposium on search based software engineering. Springer, 2018, pp. 6586. [6] S. Mechtaev, J. Yi, and A. Roychoudhury, Angelix: Scalable multiline program patch synthesis via symbolic analysis, in Proceedings of the 38th international conference on software engineering, 2016, pp. 691701. [7] K. Liu, A. Koyuncu, D. Kim, and T. F. Bissyande, Tbar: Revisiting template-based automated program repair, in Proceedings of the 28th ACM SIGSOFT international symposium on software testing and analysis, 2019, pp. 3142. 14 [8] Q. Zhang, C. Fang, T. Zhang, B. Yu, W. Sun, and Z. Chen, Gamma: Revisiting template-based automated program repair via mask prediction, in 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE, 2023, pp. 535547. [9] X. Meng, X. Wang, H. Zhang, H. Sun, X. Liu, and C. Hu, Template-based neural program repair, in 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE, 2023, pp. 14561468. [10] Q. Zhu, Z. Sun, W. Zhang, Y. Xiong, and L. Zhang, Tare: Type-aware neural program repair, in 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE, 2023, pp. 14431455. [11] H. Ye, M. Martinez, X. Luo, T. Zhang, and M. Monperrus, Selfapr: Self-supervised program repair with test execution diagnostics, in Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering, 2022, pp. 113. [12] H. Ye, M. Martinez, and M. Monperrus, Neural program repair with execution-based backpropagation, in Proceedings of the 44th international conference on software engineering, 2022, pp. 15061518. [13] I. Bouzenia, P. Devanbu, and M. Pradel, Repairagent: An autonomous, llm-based agent for program repair, arXiv preprint arXiv:2403.17134, 2024. [14] F. Zubair, M. Al-Hitmi, and C. Catal, The use of large language models for program repair, Computer Standards & Interfaces, vol. 93, p. 103951, 2025. [15] B. Yang, H. Tian, W. Pian, H. Yu, H. Wang, J. Klein, T. F. Bissyande, and S. Jin, Cref: An llm-based conversational software repair framework for programming tutors, in Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis, 2024, pp. 882894. [16] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, Proximal policy optimization algorithms, 2017. [Online]. Available: https://arxiv.org/abs/1707.06347 [17] R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn, Direct preference optimization: Your language model is secretly reward model, Advances in neural information processing systems, vol. 36, pp. 53 72853 741, 2023. [18] Y. Liu, J. Ding, and X. Liu, Ipo: Interior-point policy optimization under constraints, in Proceedings of the AAAI conference on artificial intelligence, vol. 34, no. 04, 2020, pp. 49404947. [19] S. Cen, J. Mei, K. Goshvadi, H. Dai, T. Yang, S. Yang, D. Schuurmans, Y. Chi, and B. Dai, Value-incentivized preference optimization: unified approach to online and offline rlhf, arXiv preprint arXiv:2405.19320, 2024. [20] Y. Meng, M. Xia, and D. Chen, Simpo: Simple preference optimization with reference-free reward, Advances in Neural Information Processing Systems, vol. 37, pp. 124 198124 235, 2024. [21] Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. Li, Y. Wu et al., Deepseekmath: Pushing the limits of mathematical reasoning in open language models, arXiv preprint arXiv:2402.03300, 2024. [22] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi et al., Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, arXiv preprint arXiv:2501.12948, 2025. [23] S. Dou, Y. Liu, H. Jia, L. Xiong, E. Zhou, W. Shen, J. Shan, C. Huang, X. Wang, X. Fan et al., Stepcoder: Improve code generation with reinforcement learning from compiler feedback, arXiv preprint arXiv:2402.01391, 2024. [24] J. Li, Y. Zhao, Y. Li, G. Li, and Z. Jin, Acecoder: An effective prompting technique specialized in code generation, ACM Transactions on Software Engineering and Methodology, vol. 33, no. 8, pp. 126, 2024."
        }
    ],
    "affiliations": [
        "Alibaba Cloud",
        "Nanjing University of Science and Technology",
        "Zhejiang University"
    ]
}