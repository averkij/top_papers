{
    "paper_title": "Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust Text-based Person Retrieval",
    "authors": [
        "Tianlu Zheng",
        "Yifan Zhang",
        "Xiang An",
        "Ziyong Feng",
        "Kaicheng Yang",
        "Qichuan Ding"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Although Contrastive Language-Image Pre-training (CLIP) exhibits strong performance across diverse vision tasks, its application to person representation learning faces two critical challenges: (i) the scarcity of large-scale annotated vision-language data focused on person-centric images, and (ii) the inherent limitations of global contrastive learning, which struggles to maintain discriminative local features crucial for fine-grained matching while remaining vulnerable to noisy text tokens. This work advances CLIP for person representation learning through synergistic improvements in data curation and model architecture. First, we develop a noise-resistant data construction pipeline that leverages the in-context learning capabilities of MLLMs to automatically filter and caption web-sourced images. This yields WebPerson, a large-scale dataset of 5M high-quality person-centric image-text pairs. Second, we introduce the GA-DMS (Gradient-Attention Guided Dual-Masking Synergetic) framework, which improves cross-modal alignment by adaptively masking noisy textual tokens based on the gradient-attention similarity score. Additionally, we incorporate masked token prediction objectives that compel the model to predict informative text tokens, enhancing fine-grained semantic representation learning. Extensive experiments show that GA-DMS achieves state-of-the-art performance across multiple benchmarks."
        },
        {
            "title": "Start",
            "content": "Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust Text-based Person Retrieval Tianlu Zheng*, Yifan Zhang*, Xiang An, Ziyong Feng Kaicheng Yang, Qichuan Ding Northeastern University South China University of Technology DeepGlint 2302190@stu.neu.edu.cn, dingqichuan@mail.neu.edu.cn, kaichengyang@deepglint.com Code: https://github.com/Multimodal-Representation-Learning-MRL/GA-DMS Data: https://huggingface.co/datasets/Kaichengalex/WebPerson-5M 5 2 0 2 1 1 ] . [ 1 8 1 1 9 0 . 9 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Although Contrastive Language-Image Pretraining (CLIP) exhibits strong performance across diverse vision tasks, its application to person representation learning faces two critical challenges: (i) the scarcity of large-scale annotated vision-language data focused on person-centric images, and (ii) the inherent limitations of global contrastive learning, which struggles to maintain discriminative local features crucial for fine-grained matching while remaining vulnerable to noisy text tokens. This work advances CLIP for person representation learning through synergistic improvements in data curation and model architecture. First, we develop noise-resistant data construction pipeline that leverages the in-context learning capabilities of MLLMs to automatically filter and caption web-sourced images. This yields WebPerson, large-scale dataset of 5M highquality person-centric image-text pairs. Second, we introduce the GA-DMS (GradientAttention Guided Dual-Masking Synergetic) framework, which improves cross-modal alignment by adaptively masking noisy textual tokens based on the gradient-attention similarity score. Additionally, we incorporate masked token prediction objectives that compel the model to predict informative text tokens, enhancing fine-grained semantic representation learning. Extensive experiments show that GA-DMS achieves state-of-the-art performance across multiple benchmarks."
        },
        {
            "title": "Introduction",
            "content": "The rapid advancement of large-scale visionlanguage pre-training (Chen et al., 2023; Yang et al., 2023a; Gu et al., 2024; Wu et al., 2023) has been driven by the unprecedented availability of web-sourced image-text pairs. As milestone in vision-language representation learning, Contrastive LanguageImage Pre-training (CLIP) *Equal contribution. Corresponding author. 1 (a) Current work exhibits several deficiencies (b) Our method for robust person representation learning Figure 1: Current human-centric datasets are limited in diversity and scale, complicating model training due to noise interference and hindering the effective learning of fine-grained semantics. (Radford et al., 2021b) employs dual encoders for visual and textual modalities and leverages contrastive loss mechanism (Wang and Liu, 2021) to learn joint representations. Trained on 400 million noisy web-curated image-text pairs, CLIP exhibits strong zero-shot generalization and has been widely adopted for tasks including image classification (Abdelfattah et al., 2023; Peng et al., 2023; An et al., 2023), retrieval (Sain et al., 2023; Shao et al., 2023; Hu et al., 2025), and grounding (Xiao et al., 2023; An et al., 2024; Xie et al., 2025). However, CLIP shows suboptimal performance in text-based person retrieval, as evidenced by recent studies (Shao et al., 2023; Yan et al., 2023b; Li et al., 2023; Han et al., 2024; Zhao et al., 2025). CLIPs suboptimal performance in text-based person retrieval stems from two key limitations. First, the scarcity and noise levels in person-centric image-text data pose significant challenges. Existing datasets such as CUHK-PEDES (Li et al., 2017), ICFG-PEDES (Ding et al., 2021), and RSTPReid (Zhu et al., 2021) are constrained in scale due to their reliance on extensive manual annotations. Although large-scale person-centric datasets like LUPerson (Fu et al., 2021) comprise approximately 200K identities and 4 million images, they lack corresponding textual descriptions. Recent efforts (Tan et al., 2024) have employed Multimodal Large Language Models (MLLMs) to address data scarcity by generating synthetic captions. However, these automatically produced annotations frequently contain inaccuracies and semantic misalignments, thereby introducing noise into the training process and requiring the implementation of corrective strategies (Zhao et al., 2024). Second, CLIPs global contrastive learning paradigm fails to effectively capture fine-grained visual semantics crucial for distinguishing similar individuals (Yan et al., 2023b; Liu et al., 2024). This is particularly problematic as person retrieval often relies on localized attributes (e.g., clothing patterns or accessories) that require precise visual-semantic alignment. In this work, we advance CLIP for person representation learning through synergistic improvements in data curation and model architecture  (Fig.1)  . We initially introduce WebPerson, large-scale person-centric dataset consisting of 5 million high-quality text-image pairs derived from web-sourced images. After that, we propose the GA-DMS (Gradient-Attention Guided DualMasking Synergetic) framework, which enhances cross-modal alignment by masking noisy textual tokens based on gradient-attention similarity score. Meanwhile, we incorporate masked token prediction objectives to enforce the model to predict informative text tokens, thereby enhancing fine-grained semantic representation learning. Extensive experiments demonstrate that GA-DMS establishes new state-of-the-art performance across multiple benchmarks. The main contributions of this paper are summarized as follows: We design novel person-centric data construction pipeline that automatically filters and annotates web-sourced images, yielding the WebPerson dataset with 5 million highquality image-text pairs. We propose the GA-DMS (GradientAttention Guided Dual-Masking Synergetic) framework to improve cross-modal alignment through gradient-attention guided noisy text token masking while enhancing fine-grained visual-semantic correspondence via masked informative token prediction objectives. We conduct comprehensive experiments and demonstrate that GA-DMS achieves new state-of-the-art performance across multiple text-based person retrieval datasets."
        },
        {
            "title": "2.1 Person Representation Learning",
            "content": "Early approaches to text-based person retrieval typically employ separate vision and language encoders with custom alignment losses (Zheng et al., 2020; Si et al., 2018). These methods often exhibit suboptimal modality alignment and require extensive manual annotation. The introduction of CLIP (Radford et al., 2021b) establishes unified vision-language embedding space, significantly advancing cross-modal matching. Recent works extend CLIP with specialized modules for textbased person retrieval. IRRA (Jiang and Ye, 2023) merges visual cues into textual tokens via crossmodal transformer and aligns global similarity distributions. MDRL (Yang et al., 2024b) designs cross-modality global feature learning architecture to learn the global features from the two modalities and meet the demand of the task. UniPT (Shao et al., 2023) utilizes simple vision-and-language pre-training framework to explicitly align the feature space of the image and text modalities during pre-training. However, these approaches largely ignore data noise, which critically influences crossmodal alignment in feature space. RDE (Qin et al., 2024) mitigates the adverse impact of noisy through the proposed confident consensus division and novel triplet alignment loss. ProPOT (Yan et al., 2024) transforms the identity-level matching problem into prototype learning problem, aiming to learn identity-enriched prototypes. However, prototype aggregation compromises fine-grained semantic learning."
        },
        {
            "title": "2.2 Person-centric Dataset",
            "content": "High-quality image-text paired datasets are essential for learning discriminative person representations. However, existing manually annotated datasets (e.g., CUHK-PEDES (Li et al., 2017), ICFG-PEDES (Ding et al., 2021), RSTPReid (Zhu et al., 2021)) face severe scalability limitations due to labor-intensive annotation processes. This scalability bottleneck ultimately constrains models capacity to acquire diverse semantic information 2 and learn discriminative features. Recent efforts to mitigate this issue focus on constructing largescale datasets, such as LUPerson (Fu et al., 2021), LUPerson-T (Shao et al., 2023), LUPerson-MLLM (Tan et al., 2024), and SYNTH-PEDES (Zuo et al., 2024) demonstrate that increased data volume improves general pedestrian representation learning. Nevertheless, these datasets primarily derive from video sources, inheriting inherent scalability constraints from computationally intensive video processing pipelines. The success of multimodal large language models in cross-modal understanding (Yu et al., 2024) has inspired their application to synthetic data generation. For instance, LUPersonMLLM (Tan et al., 2024) employs template-guided MLLMs to generate diverse textual descriptions, significantly enhancing text-to-image ReID performance. However, this approach remains limited by its dependence on existing LUPerson image collections."
        },
        {
            "title": "3.1 Person-Centric Image Filtering",
            "content": "In this study, we utilize the COYO700M dataset (Byeon et al., 2022), large-scale dataset that contains 747M image-text pairs collected from CommonCrawl, as our web-crawled images source. To filter high-quality person-centric images, we initially deploy the YOLOv11 model (Jocher and Qiu, 2024) to detect humans and extract bounding box coordinates. The specific workflow is illustrated in Fig. 2, where images are retained based on the following criteria: (i) shorter dimension exceeds 90 pixels, (ii) aspect ratio between 1:2 and 1:4, and (iii) human detection confidence above 85%. Subsequently, YOLOv11-Pose (Jocher and Qiu, 2024) verifies pose integrity by requiring: (i) visibility of at least eight keypoints, (ii) presence of at least one hip keypoints and two head keypoints. This process yields 5 million high-quality human-centric images filtered from the COYO700M dataset."
        },
        {
            "title": "3.2 Synthetic Caption Generation",
            "content": "Following the selection of 5 million high-quality human-centric images, we develop synthetic caption generation pipeline to create diverse and precise textual descriptions. Our approach transforms existing captions from CUHK-PEDES (Li et al., 2017), ICFG-PEDES (Ding et al., 2021), and RSTPReid (Zhu et al., 2021) into structured templates using Qwen2.5-72B-Instruct (Yang et al., Figure 2: The details of person-centric image filtering and synthetic caption generation pipeline for constructing our WebPerson dataset. 2024a). The model systematically replaces finegrained attributes (e.g., black jacket, ponytail) with standardized placeholders (e.g., [colored top], [hairstyle]). To reduce redundancy and cluster semantically similar templates, inspired by the previous works (Yang et al., 2025; Gu et al., 2025b), we employ the OPENCLIP ViT-bigG/14 (Radford et al., 2021a) to extract text embeddings of the template texts, then we utilize the standard k-means algorithm to partition all the templates embeddings into distinct clusters based on the nearest neighbor criterion. Within each cluster, we select the most representative template (highest cosine similarity to the centroid) along with five randomly sampled templates. To further enhance template diversity, we employ Qwen2.5-72B-Instruct (Yang et al., 2024a) to synthesize new templates from this refined set. All generated templates undergo rigorous review to eliminate biases and stereotypes, yielding curated collection of one thousand highquality templates. To generate diverse, high-quality captions, we leverage the in-context learning capabilities of MLLMs (Li et al., 2025; Gu et al., 2025a). Specifically, we randomly assign template to each image and use Qwen2.5-VL-7B-Instruct and Qwen2.5-VL-32B-Instruct (Bai et al., 2025) to produce captions that follow the given template. We adopt vLLM (Kwon et al., 2023) to accelerate largescale inference. The details of the prompt are pro3 Figure 3: Overview of our proposed method. (a) The architecture of our proposed Gradient-Attention Guided Dual-Masking Synergic (GA-DMS) framework. (b) The details of the Gradient-Attention Similarity Score (GASS). vided in the Appendix A.2. for each transformer layer is then computed as:"
        },
        {
            "title": "4 GA-DMS Framework",
            "content": "This section presents our GA-DMS (GradientAttention Guided Dual-Masking Synergetic) framework  (Fig. 3)  . In Sec. 4.1, we introduce the Gradient-Attention Similarity Score, which dynamically differentiates noise tokens and informative tokens during the training process. In Sec. 4.2, we present the dual-masking synergetic learning details and the training objective."
        },
        {
            "title": "4.1 Gradient-Attention Similarity Score",
            "content": "Existing interpretability research (Selvaraju et al., 2017) on CLIP-based models has shown that intermediate layer gradients retain fine-grained imagetext alignment information. Motivated by prior work (Zhao et al., 2025), we introduce gradientattention similarity score that quantifies each textual tokens contribution to the imagetext alignment. We denote the embeddings of the text tokens and image tokens as and . We first calculate the global cosine similarity SIM = TeosV cls. The gradient importance for the l-th transformer layers text token eos is then derived as gl = SIM . To capture fine-grained semantics, we integrate Multi-Scale Pooling (MSP) layer within the transformer architecture. The MSP layer aggregates local contexts at multiple scales through average pooling of adjacent tokens, followed by bilinear interpolation to restore original dimensions. This process yields features enriched with multiscale local information. The spatial importance wl eos wl = Φ(MSP(ql eos) MSP(kl)T) (1) where Φ is the normalization function, ql eos is the query embedding for the [eos] token at layer l, kl represent the key embedding at layer l. The gradient-based score Sl of the l-th transformer layer is defined as: = gl wl vl Sl (2) where vl is the value embedding at layer l. Simultaneously, we compute attention-based semantic scores Sl for each token based on the attention maps Ml from the l-th transformer layer. We denote the attention score for the [eos] token as l, the attention-based semantic score Sl is computed as: = Sl (cid:80)N j=1 (3) The final gradient-attention similarity score for the effective textual tokens is defined as: = ReLU ( 1 Sl Sl a) (cid:88) lL (4) where represents the number the final layers of the transformer, RBN , and is the number of tokens. This score integrates information from both gradients and attention maps to weight text tokens for masking probability computation."
        },
        {
            "title": "4.2.1 Noise Token Masking\nWhile Multimodal Large Language Models\n(MLLMs) inevitably introduce noise during large-\nscale data generation due to inherent hallucination",
            "content": "4 effects. To mitigate this issue, we employ noise token masking strategy to reduce the influence of noise tokens based on the gradient-attention similarity score S. We calculate the masking probability for the i-th text token Ti as: p(Ti) = αn 1 + eλ[(1si)γ] (5) where si is the gradient-attention similarity score for the i-th token, αn is hyperparameter to set the upper limit of the masking probability for noise tokens. λ and γ respectively modulate the slope and midpoint of the probability distribution, thereby sharpening the differentiation between noisy and semantically relevant tokens. During training, we dynamically mask textual tokens using [mask] according to these computed probabilities. eos)}B Given the embeddings of image-text pairs {(vi cls, ti i=1, we define the ground-truth matching distribution as qi,j and compute the predicted distribution as: pi,j = cls, tj exp(sim(vi b=1 exp(sim(vi eos)/τ ) cls, tb eos)/τ ) (cid:80)B (6) where τ is temperature parameter. Following (Jiang and Ye, 2023), we adopt the Similarity Distribution Matching (SDM) loss to align the distribution. The Li2t is defined as: Li2t ="
        },
        {
            "title": "1\nB",
            "content": "B (cid:88) (cid:88) i=1 j=1 pi,j log (cid:18) pi,j (cid:19) qi,j + ε (7) where ε is small number to avoid numerical problems. We compute symmetric loss Lt2i by swapping {(vi eos)}, and the SDM loss is: cls, ti Lsdm = Li2t + Lt2i (8)"
        },
        {
            "title": "4.2.2 Masked Informative Token Prediction\nTo improve fine-grained semantic representation,\nwe selectively mask tokens with strong image-\nsemantic correlations and introduce a masked token\nprediction task to enhance local semantic learning.\nSimilar to the Equation 5, the masking probability\nfor the informative text tokens is defined as:",
            "content": "p(Ti) = αi 1 + eλ[siγ] (9) where αi bounds the maximum masking probability for informative tokens. For effective finegrained visual-textual fusion during token prediction, we integrate cross-modal interaction module (Jiang and Ye, 2023) as decoder. This module 5 consists of multi-head cross-attention followed by four Transformer layers to align modalities in shared embedding space. final MLP layer predicts original tokens from the fused representations. Given hidden states hm , and denotes the masked text token set, the distribution of the output token is mi = MLP(hm ). The Masked Token Prediction (MTP) loss Lmtp is defined as: Lmtp = 1 MV (cid:88) (cid:88) yi log iM jV exp(mi j) k=1 exp(mi k) (cid:80)V , (10) where is the size of vocabulary V, and yj is one-hot vocabulary distribution. Finally, the total loss is define as: = Lsdm + βLmtp (11) where β is loss weight."
        },
        {
            "title": "5 Experiments",
            "content": "Implementation Details. Consistent with previous works (Tan et al., 2024; Jiang and Ye, 2023), we utilize the CLIP ViT-B/16 model as our backbone. Following IRRA (Jiang and Ye, 2023), we incorporate randomly initialized multimodal interaction encoder to facilitate masked token prediction. Our implementation processes 384128 resolution images with maximum length of = 77 text sequences. We employ Adam (Kingma, 2014) as the optimizer, initialized with learning rate of 1e 4 and weight decay of 4e 5. The parameters β1 and β2 are set to 0.9 and 0.999, respectively. The temperature parameter τ in SDM loss is set to 0.02. We train GA-DMS for 30 epochs with batch size of 512 on 8 NVIDIA A100 (80G) GPUs. For generating synthetic templates and captions, we utilize Qwen2.5-72B-Instruct (Yang et al., 2024a), Qwen2.5-VL-7B-Instruct, and Qwen2.5VL-32B-Instruct (Bai et al., 2025). Additionally, vLLM (Kwon et al., 2023) is leveraged to accelerate large-scale inference. Please refer to the Appendix A.1 for more detailed hyperparameters. Evaluation. Following previous works (Tan et al., 2024; Qin et al., 2024), we conduct comprehensive evaluation of our method across three challenging text-to-image person retrieval datasets: CUHKPEDES (Li et al., 2017), ICFG-PEDES (Ding et al., 2021), and RSTPReid (Zhu et al., 2021). We employ Rank-k (k=1, 5, 10) and mean Average Precision (mAP) as evaluation metrics for all datasets. Method Image Enc. Text Enc. CUHK-PEDES ICFG-PEDES R1 R5 R10 mAP R1 R5 R10 mAP R1 ViTAA (Wang et al., 2020) SSAN (Ding et al., 2021) LBUL (Wang et al., 2022b) SAF (Li et al., 2022b) TIPCB (Chen et al., 2022) CAIBC (Wang et al., 2022a) AXM-Net (Farooq et al., 2022) LGUR (Shao et al., 2022) IVT (Shu et al., 2022) LCR²S (Yan et al., 2023a) UniPT (Shao et al., 2023) RN50 RN50 RN50 ViT-Base RN50 RN50 RN50 DeiT-Small ViT-Base RN50 ViT-Base with ALBEF (Li et al., 2021) backbone: LSTM LSTM BERT BERT BERT BERT BERT BERT BERT 55.97 61.37 64.04 64.13 64.26 64.43 64.44 65.25 65.69 TextCNN+BERT 67.36 68. BERT 75.84 80.15 82.66 82.62 83.19 82.87 80.52 83.12 85.93 84.19 84.67 83.52 86.73 87.22 88.4 89.1 88.37 86.77 89.00 91.15 89.62 90.38 - - - - - - 58.70 - - 59.20 - 50.98 54.23 - - 54.96 - - 59.02 56.04 57.93 60.09 68.79 72.63 - - 74.72 - - 75.32 73.60 76.08 76. 75.78 79.53 - - 81.89 - - 81.56 80.22 82.40 82.46 - - - - - - - - - 38.21 - - 43.50 45.55 - - 47.35 - 47.95 46.70 54.95 51.85 RSTPReid R5 R10 mAP - 67.80 68.2 - - 69.55 - 71.85 70.00 76.65 74. - 77.15 77.85 - - 79.00 - 80.25 78.80 84.70 82.85 - - - - - - - - - 40.92 - RaSa (Bai et al., 2023) APTM (Yang et al., 2023b) CLIP-ViT Swin-B BERT-base BERT-base 76.51 76. 90.29 90.04 94.25 94.15 69.38 66.91 65.28 68.51 80.40 82.99 85.12 87. 41.29 41.22 66.90 67.50 86.50 85.70 91.35 91.45 52.31 52.56 with CLIP (Radford et al., 2021b) backbone: Han et al. (Han et al., 2021) IRRA (Jiang and Ye, 2023) FSRL (Wang et al., 2024) Propot (Yan et al., 2024) SAP-SAM (Wang et al., 2024) PLOT (Park et al., 2024) RDE (Qin et al., 2024) NAM (Tan et al., 2024) Ours (1.0 M) Ours (5.0 M) CLIP-RN101 CLIP-ViT CLIP-ViT CLIP-ViT CLIP-ViT CLIP-ViT CLIP-ViT CLIP-ViT CLIP-ViT CLIP-ViT CLIP-Xformer CLIP-Xformer CLIP-Xformer CLIP-Xformer CLIP-Xformer CLIP-Xformer CLIP-Xformer CLIP-Xformer CLIP-Xformer CLIP-Xformer 64.08 73.38 74.65 74.89 75.05 75.28 75.94 76.82 77.02 77.60 81.73 89.93 89.77 89.90 89.93 90.42 90.14 91.16 91.28 91.40 88.19 93.71 94.03 94.17 93.73 94.12 94.12 94.46 94.58 94. 60.08 66.10 67.49 67.12 - - 67.56 69.55 69.65 69.82 - 63.46 64.01 65.12 63.97 65.76 67.68 67.05 69.07 69.51 - 80.25 80.42 81.57 80.84 81.39 82.47 82.16 83.26 83.47 - 85.82 85.56 86.97 86.17 86.73 87.36 87.33 87.64 87.67 - 38.06 39.64 42.93 - - 40.06 41.51 41.91 42.30 - 60.20 60.20 61.87 62.85 61.80 65.35 68.50 70.30 71. - 81.30 81.40 83.63 82.65 82.85 83.95 87.15 88.00 87.25 - 88.20 88.60 89.70 89.85 89.45 89.90 92.10 92.85 92.90 - 47.17 47.38 47.82 - - 50.88 53.02 54.89 55.43 Table 1: Comparisons with state-of-the-art methods in the traditional evaluation settings. The best results are marked in bold, and the second-best results are underlined. Pre-training Dataset CUHK-PEDES R1 mAP ICFG-PEDES mAP R1 RSTPReid R1 mAP None MALS (1.5M) LUPerson-T (0.95M) SYNTH-PEDES (1.0M) LUPerson-MLLM (1.0M) Ours (0.1 M) Web-Person (1.0 M) Ours (5.0 M) 12.65 20.47 21.55 57.29 56.01 58.95 66.26 68.34 11.15 18.46 18.76 51.86 50.34 52.77 58.54 60.22 6.67 11.71 11.20 57.13 37. 38.18 51.99 54.64 2.51 4.57 4.53 31.36 20.21 19.70 28.81 30.68 13.45 21.50 22.15 42.20 50.60 47.10 55.35 57.60 10.31 16.95 17.29 32.28 37. 36.68 40.57 42.00 Table 2: Comparisons with existing pre-training datasets in the direct transfer setting. The best results are marked in bold, and the second-best results are underlined. the CUHK-PEDES, ICFG-PEDES, and RSTPReid datasets, respectively. The primary reason for this improvement is that our proposed GA-DMS framework effectively distinguishes between noise and informative tokens based on the gradient-attention similarity score. As shown in Fig. 4, compared with NAM (Tan et al., 2024), our GA-DMS can better allocate weights to text tokens while concentrating attention on human-centric regions. This capability not only reduces the effect of noise on model training but also improves the models capacity to learn fine-grained semantic information. Moreover, upon scaling the WebPerson dataset from 1.0 to 5.0 M, GA-DMS achieves new state-of-the-art Rank-1 accuracies of 77.6%, 69.51%, and 71.25% across three downstream datasets. Figure 4: Visualization of token-wise weight scores and attention maps generated by NAM (Tan et al., 2024) and our GA-DMS."
        },
        {
            "title": "5.1 Comparison with Existing Methods.",
            "content": "We evaluate GA-DMS against state-of-the-art methods on three benchmarks: CUHK-PEDES, ICFGPEDES, and RSTPReid. As shown in Tab. 1, our pre-trained model achieves superior performance after fine-tuning with the IRRA (Jiang and Ye, 2023), which significantly improves Rank-1 accuracy and mAP on the RSTPReid dataset by 10.10% and 7.72% over the baseline of IRRA. Compared with the NAM (Tan et al., 2024), GA-DMS obtains 0.2%, 2.02%, and 1.8% improvement in Rank-1 on 6 Pre-training Dataset Source CUHK-PEDES R1 mAP Target ICFG-PEDES R1 mAP RSTPReid R1 mAP Masking Method Components CUHK-PEDES ICFG-PEDES RSTPReid CSS GASS SDM MTP R1 mAP R1 mAP R1 mAP None MALS(1.5 M) LUPerson-T(0.95 M) SYNTH-PEDES(1.0 M) LUPerson-MLLM(1.0 M) Ours(0.1 M) Ours(1.0 M) Ours(5.0 M) CUHK-PEDES ICFG-PEDES RSTPReid CUHK-PEDES ICFG-PEDES RSTPReid CUHK-PEDES ICFG-PEDES RSTPReid CUHK-PEDES ICFG-PEDES RSTPReid CUHK-PEDES ICFG-PEDES RSTPReid CUHK-PEDES ICFG-PEDES RSTPReid CUHK-PEDES ICFG-PEDES RSTPReid CUHK-PEDES ICFG-PEDES RSTPReid 73.48 33.90 35.25 73.67 43.11 44.51 74.28 34.66 39.26 74.12 60.49 57.75 76.59 60.75 60. 75.53 58.67 58.49 77.02 68.16 68.41 77.60 69.83 69.19 66.21 31.65 32.35 65.23 38.93 39.99 66.52 32.51 34. 65.82 54.61 53.01 68.06 54.42 53.85 67.92 52.66 52.50 69.65 60.79 61.28 69.82 62.06 62.00 43.04 63.83 33. 46.02 65.21 40.78 44.83 65.33 34.95 57.14 66.63 53.88 47.17 67.18 46.39 47.79 66.35 44.41 57.24 69.07 56. 58.91 69.52 57.13 22.45 38.37 19.58 24.06 38.52 25.42 22.72 38.45 22.25 32.12 39.32 30.88 25.41 40.27 27. 25.14 39.95 25.98 32.13 41.91 34.64 33.70 42.30 35.76 52.55 47.45 60.40 55.05 48.45 64.05 54.25 48.30 61. 55.85 49.80 66.75 59.35 55.65 69.45 56.75 52.93 65.90 61.10 59.15 70.30 61.80 60.05 71.25 39.97 36.83 47. 41.29 37.29 50.08 39.26 38.51 48.28 40.85 37.34 52.18 43.76 44.05 53.30 41.01 39.84 49.28 45.27 44.94 54. 46.81 45.46 55.43 Table 3: Comparisons with existing pre-training datasets in the fine-tuning setting. The best results are marked in bold, and the second-best results are underlined. Gray indicates that the source and target are homologous."
        },
        {
            "title": "5.2 Comparison with Existing Datasets.",
            "content": "We conduct comprehensive comparisons between our WebPerson dataset and four existing largescale pre-training datasets: MALS (Yang et al., 2023b), LUPerson-T (Shao et al., 2023), SYNTHPEDES (Zuo et al., 2024), and LUPersonMLLM (Tan et al., 2024). MALS consists of 1.5 million synthetic images generated using commercial diffusion models, with textual descriptions automatically produced by BLIP (Li et al., 2022a). LUPerson-T includes 0.95 million images, each enhanced by one of 456 templates to maximize caption diversity. SYNTH-PEDES provides 4.8 million images, each annotated with an average of 2.53 textual descriptions, generated through hybrid architecture that combines ResNet101-FPN (He et al., 2016) visual encoder with GPT-2 (Radford et al., 2019) text generator for detailed person attribute modeling. Notably, LUPerson-MLLM utilizes two multimodal large language models for caption generation, supplemented by 46 ChatGPToptimized templates obtained through iterative dialogues to enhance linguistic variation. This dataset comprises 1.0 million images, each paired with two MLLM-generated captions. Tab. 2 presents comparative results under direct transfer setting, where models pre-trained on WebPerson exhibit superior cross-dataset generalization across three benchmarks. Specifically, under the 56.75 50. 34.63 17.59 45.50 34.51 56.35 63.29 62.74 57.29 63.87 64.25 50.21 57.42 57. 52.28 57.56 58.27 34.72 43.39 42.96 36.24 44.02 44.39 17.66 24.12 23.88 18.96 24.18 24.67 44.60 33.28 51.95 39.41 50.80 38. 47.90 35.97 52.30 39.61 52.70 40.12 Table 4: Ablation on different components and masking methods. CSS: Cosine Similarity Score. GASS: Gradient-Attention Similarity Score. SDM: Similarity Distribution Matching. MTP: Masked Token Prediction. comparable 1M dataset, our constructed WebPerson dataset demonstrates superior performance on CUHK-PEDES and RSTPReid, and shows suboptimal performance on ICFG-PEDES. Notably, the WebPerson dataset demonstrates comparable performance to the full-scale LUPerson-MLLM even when trained on mere 0.1M samples. These experimental results demonstrate that our proposed WebPerson dataset exhibits strong robustness and can learn person representations with enhanced transferability. As shown in Tab. 3, we also evaluate the fine-tuning performance following LUPersonMLLM (Tan et al., 2024), utilizing the IRRA with models pretrained on different datasets. Results indicate that WebPerson pretraining yields stateof-the-art performance across both in-domain and cross-domain scenarios. At the 1M data scale, WebPerson achieves consistent improvements over LUPerson-MLLM, with Rank-1 accuracy gains of 0.43%, 1.89%, and 0.85% on CUHK-PEDES, ICFG-PEDES, and RSTPReid respectively. The cross-domain evaluations reveal particularly significant performance enhancements, highlighting WebPersons exceptional representation transferability through fine-tuning."
        },
        {
            "title": "5.3 Ablation Study",
            "content": "Ablation on Different Components and Masking Methods. To substantiate the efficacy of various components and the effectiveness of our proposed Gradient-Attention Similarity Score (GASS), we perform comprehensive ablation study with 0.5M data sample from our WebPerson dataset. As shown in Tab. 4, the integrating Masked Token Prediction (MTP) with GASS improves performance across all evaluation metrics, as predicting semantically rich tokens enhances fine-grained learning. The Similarity Distribution Matching (SDM) component alone enhances image-text alignment by 7 (a) Ablation on αn for masking noise tokens (b) Ablation on αi for masking informative tokens Figure 5: Ablation experiment results for αn and αi, which can directly influence the upper limit of the masking probability for noise and informative tokens. replacing noisy tokens with learnable embeddings, achieving Rank-1 accuracy gains of 7.12%, 9.39%, and 6.8% on CUHK-PEDES, ICFG-PEDES, and RSTPReid respectively. By combining MTP with SDM, we observe enhancements across all metrics, further substantiating the efficacy of the components within our method. When comparing Cosine Similarity Score (CSS) with Gradient-Attention Similarity Score (GASS), GASS consistently exhibits superior performance. This advantage primarily stems from GASSs capacity to precisely weight textual tokens during training by incorporating gradient and attention information. As illustrated in Fig. 4, our method accurately allocates weights to noise textual tokens (e.g., \"white lace top\"), thereby effectively mitigating the influence of noise on the models representation learning. Ablation on αn and αi. In this work, our dual-masking synergetic learning method dynamically masks textual tokens according to gradientattention similarity scores. We introduce parameters αn and αi to regulate the maximum masking probabilities for noise and informative tokens. Fig. 5 presents an ablation study on αn and αi to determine the optimal settings. For enhanced performance on three downstream datasets, we set αn = 0.2 and αi = 0.3. Additionally, our method consistently outperforms random masking baselines, confirming its effectiveness. Data Scaling Analysis. To explore the impact of pretraining data scale on person representation learning, we systematically augmented the dataset size from 0.1M to 1M, 3M, and 5M samples for pertaining. Fig. 6 illustrates the direct transfer performance evaluation across three benchmarks at 8 Figure 6: Data scaling analysis of WebPerson dataset.The performance of our GA-DMS method in direct transfer settings. different data scales. The outcomes consistently reveal performance enhancements as the data volume increases. At the maximum scale of 5.0M samples, the model demonstrates Rank-1 accuracy improvements of 9.39%, 16.46%, and 10.50% across the three benchmarks in comparison to the 0.1M baseline, indicating clear upward trajectory. These findings conclusively demonstrate that scaling highquality pretraining data substantially enhances textbased person retrieval capability."
        },
        {
            "title": "Conclusion",
            "content": "In this paper, we enhance CLIP for person representation learning by synergistically improving data acquisition and model architecture. First, we devise noise-resistant data construction pipeline that leverages the in-context learning capabilities of MLLMs for automatic filtering and captioning of web-crawled images. This results in the WebPerson dataset, which comprises 5M highquality person-centric image-text pairs. Second, we propose the GA-DMS framework, which improves cross-modal alignment by masking semantically irrelevant tokens based on gradient-attention similarity score. Concurrently, we implement masked token prediction objectives that force the model to reconstruct informative text tokens, facilitating discriminative fine-grained feature learning for visualsemantic correspondence. Comprehensive experiments demonstrate that GA-DMS achieves state-ofthe-art performance in several downstream datasets. We hope our work provides insights for the person representation learning task."
        },
        {
            "title": "Limitations",
            "content": "In this work, we demonstrate the exceptional textbased person retrieval performance of the personcentric dataset constructed solely from internet images. Limited by computational resources, this paper constructs 5M-scale WebPerson dataset, with further scaling left for community exploration."
        },
        {
            "title": "Acknowledgement",
            "content": "This work was supported in part by the National Natural Science Foundation of China under Grant 62373086, by the Liaoning Province Applied Basic Research Program (2025JH2/101330131), by the Guangdong Basic and Applied Basic Research under 2023A1515140014, and by the State Key Laboratory of Robotics under Grant 2024-O14."
        },
        {
            "title": "Ethics Statement",
            "content": "We abide by the ACL Code of Ethics. The data resources used in this study are publicly available."
        },
        {
            "title": "References",
            "content": "Rabab Abdelfattah, Qing Guo, Xiaoguang Li, Xiaofeng Wang, and Song Wang. 2023. Cdul: Clip-driven unsupervised learning for multi-label image classification. In ICCV, pages 13481357. Xiang An, Jiankang Deng, Kaicheng Yang, Jaiwei Li, Ziyong Feng, Jia Guo, Jing Yang, and Tongliang Liu. 2023. Unicom: Universal and compact representation learning for image retrieval. arXiv preprint arXiv:2304.05884. Xiang An, Kaicheng Yang, Xiangzi Dai, Ziyong Feng, and Jiankang Deng. 2024. Multi-label cluster discrimination for visual representation learning. In ECCV, pages 428444. Springer. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, and 1 others. 2025. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923. Yang Bai, Min Cao, Daming Gao, Ziqiang Cao, Chen Chen, Zhenfeng Fan, Liqiang Nie, and Min Zhang. 2023. Rasa: Relation and sensitivity aware representation learning for text-based person search. IJCAI. Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. 2022. Coyo-700m: Image-text pair dataset. https: //github.com/kakaobrain/coyo-dataset. Fei-Long Chen, Du-Zhen Zhang, Ming-Lun Han, XiuYi Chen, Jing Shi, Shuang Xu, and Bo Xu. 2023. Vlp: survey on vision-language pre-training. Machine Intelligence Research, 20(1):3856. Yuhao Chen, Guoqing Zhang, Yujiang Lu, Zhenxing Wang, and Yuhui Zheng. 2022. Tipcb: simple but effective part-based convolutional baseline for text-based person search. Neurocomputing. Zefeng Ding, Changxing Ding, Zhiyin Shao, and Dacheng Tao. 2021. Semantically self-aligned network for text-to-image part-aware person reidentification. arXiv preprint arXiv:2107.12666. Ammarah Farooq, Muhammad Awais, Josef Kittler, and Syed Safwan Khalid. 2022. Axm-net: Implicit crossmodal feature alignment for person re-identification. In AAAI. Dengpan Fu, Dongdong Chen, Jianmin Bao, Hao Yang, Lu Yuan, Lei Zhang, Houqiang Li, and Dong Chen. 2021. Unsupervised pre-training for person reidentification. In CVPR, pages 1475014759. Dengpan Fu, Dongdong Chen, Hao Yang, Jianmin Bao, Lu Yuan, Lei Zhang, Houqiang Li, Fang Wen, and Dong Chen. 2022. Large-scale pre-training for perIn CVPR, son re-identification with noisy labels. pages 24762486. Tiancheng Gu, Kaicheng Yang, Xiang An, Ziyong Feng, Dongnan Liu, Weidong Cai, and Jiankang Deng. 2024. Rwkv-clip: robust vision-language representation learner. arXiv preprint arXiv:2406.06973. Tiancheng Gu, Kaicheng Yang, Ziyong Feng, Xingjun Wang, Yanzhao Zhang, Dingkun Long, Yingda Chen, Weidong Cai, and Jiankang Deng. 2025a. Breaking the modality barrier: Universal embedding learning with multimodal llms. arXiv preprint arXiv:2504.17432. Tiancheng Gu, Kaicheng Yang, Chaoyi Zhang, Yin Xie, Xiang An, Ziyong Feng, Dongnan Liu, Weidong Cai, and Jiankang Deng. 2025b. Realsyn: An effective and scalable multimodal interleaved document transformation paradigm. arXiv preprint arXiv:2502.12513. Qianru Han, Xinwei He, Zhi Liu, Sannyuya Liu, Ying Zhang, and Jinhai Xiang. 2024. Clip-scgi: Synthesized caption-guided inversion for person reidentification. arXiv preprint arXiv:2410.09382. Xiao Han, Sen He, Li Zhang, and Tao Xiang. 2021. Text-based person search with limited data. arXiv preprint arXiv:2110.10807. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In CVPR, pages 770778. Xiaoxing Hu, Kaicheng Yang, Jun Wang, Haoran Xu, Ziyong Feng, and Yupei Wang. 2025. Decoupled global-local alignment for improving compositional understanding. arXiv preprint arXiv:2504.16801. Ding Jiang and Mang Ye. 2023. Cross-modal implicit relation reasoning and aligning for text-to-image person retrieval. In CVPR, pages 27872797. Glenn Jocher and Jing Qiu. 2024. Ultralytics yolo11. Diederik Kingma. 2014. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In ACM SIGOPS. Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022a. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In ICML, pages 1288812900. PMLR. Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. 2021. Align before fuse: Vision and language representation learning with momentum distillation. Advances in neural information processing systems, 34:96949705. Shiping Li, Min Cao, and Min Zhang. 2022b. Learning semantic-aligned feature representation for textbased person search. In ICASSP. Shuang Li, Tong Xiao, Hongsheng Li, Bolei Zhou, Dayu Yue, and Xiaogang Wang. 2017. Person search with natural language description. In CVPR, pages 1970 1979. Siyuan Li, Li Sun, and Qingli Li. 2023. Clipreid: exploiting vision-language model for image reidentification without concrete text labels. In AAAI, volume 37, pages 14051413. Yanshu Li, Tian Yun, Jianjiang Yang, Pinyuan Feng, Jinfa Huang, and Ruixiang Tang. 2025. Taco: Enhancing multimodal in-context learning via task arXiv mapping-guided sequence configuration. preprint arXiv:2505.17098. Feng Liu, Minchul Kim, Zhiyuan Ren, and Xiaoming Liu. 2024. Distilling clip with dual guidance for learning discriminative human body shape representation. In CVPR, pages 256266. Jicheol Park, Dongwon Kim, Boseung Jeong, and Suha Kwak. 2024. Plot: Text-based person search with part slot attention for corresponding part discovery. In ECCV. Springer. Fang Peng, Xiaoshan Yang, Linhui Xiao, Yaowei Wang, and Changsheng Xu. 2023. Sgva-clip: Semanticguided visual adapting of vision-language models for few-shot image classification. TMM, 26:34693480. Yang Qin, Yingke Chen, Dezhong Peng, Xi Peng, Joey Tianyi Zhou, and Peng Hu. 2024. Noisycorrespondence learning for text-to-image person reidentification. In CVPR. Alec Radford, Jong Wook Kim, Chris Hallacy, A. Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021a. Learning transferable visual models from natural language supervision. In ICML. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, and 1 others. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9. Aneeshan Sain, Ayan Kumar Bhunia, Pinaki Nath Chowdhury, Subhadeep Koley, Tao Xiang, and YiZhe Song. 2023. Clip for all things zero-shot sketchbased image retrieval, fine-grained or not. In CVPR, pages 27652775. Ramprasaath Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. 2017. Grad-cam: Visual explanations from deep networks via gradient-based localization. In ICCV, pages 618626. Zhiyin Shao, Xinyu Zhang, Changxing Ding, Jian Wang, and Jingdong Wang. 2023. Unified pretraining with pseudo texts for text-to-image person re-identification. In ICCV, pages 1117411184. Zhiyin Shao, Xinyu Zhang, Meng Fang, Zhifeng Lin, Jian Wang, and Changxing Ding. 2022. Learning granularity-unified representations for text-to-image person re-identification. In ACMMM. Xiujun Shu, Wei Wen, Haoqian Wu, Keyu Chen, Yiran Song, Ruizhi Qiao, Bo Ren, and Xiao Wang. 2022. See finer, see more: Implicit modality alignment for text-based person retrieval. In ECCV, pages 624641. Springer. Jianlou Si, Honggang Zhang, Chun-Guang Li, Jason Kuen, Xiangfei Kong, Alex Kot, and Gang Wang. 2018. Dual attention matching network for contextaware feature sequence based person re-identification. In CVPR, pages 53635372. Guanglu Song, Biao Leng, Yu Liu, Congrui Hetang, and Shaofan Cai. 2018. Region-based quality estimation network for large-scale person re-identification. In AAAI, volume 32. Wentan Tan, Changxing Ding, Jiayu Jiang, Fei Wang, Yibing Zhan, and Dapeng Tao. 2024. Harnessing the power of mllms for transferable text-to-image person reid. In CVPR, pages 1712717137. Di Wang, Feng Yan, Yifeng Wang, Lin Zhao, Xiao Liang, Haodi Zhong, and Ronghua Zhang. 2024. Fine-grained semantics-aware representation learning for text-based person retrieval. In ICMR. Feng Wang and Huaping Liu. 2021. Understanding the behaviour of contrastive loss. In CVPR, pages 24952504. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, and 1 others. 2021b. Learning transferable visual models from natural language supervision. In ICML, pages 87488763. PmLR. Zhe Wang, Zhiyuan Fang, Jun Wang, and Yezhou Yang. 2020. Vitaa: Visual-textual attributes alignment in In Computer person search by natural language. visionECCV 2020: 16th European conference, glasgow, UK, August 2328, 2020, proceedings, part XII 16, pages 402420. Springer. 10 Shuyu Yang, Yinan Zhou, Zhedong Zheng, Yaxiong Wang, Li Zhu, and Yujiao Wu. 2023b. Towards unified text-based person retrieval: large-scale multi-attribute and language search benchmark. In ACMMM, pages 44924501. Qiying Yu, Quan Sun, Xiaosong Zhang, Yufeng Cui, Fan Zhang, Yue Cao, Xinlong Wang, and Jingjing Liu. 2024. Capsfusion: Rethinking image-text data at scale. In CVPR, pages 1402214032. Chenyang Zhao, Kun Wang, Janet H. Hsiao, and Antoni B. Chan. 2025. Grad-eclip: Gradient-based visual and textual explanations for clip. Preprint, arXiv:2502.18816. Henry Hengyuan Zhao, Pan Zhou, and Mike Zheng Shou. 2024. Genixer: Empowering multimodal large language model as powerful data generator. In ECCV, pages 129147. Springer. Zhedong Zheng, Liang Zheng, Michael Garrett, Yi Yang, Mingliang Xu, and Yi-Dong Shen. 2020. Dual-path convolutional image-text embeddings with instance loss. TOMM, 16(2):123. Aichun Zhu, Zijie Wang, Yifeng Li, Xili Wan, Jing Jin, Tian Wang, Fangqiang Hu, and Gang Hua. 2021. Dssl: Deep surroundings-person separation learning for text-based person retrieval. In ACMMM, pages 209217. Jialong Zuo, Jiahao Hong, Feng Zhang, Changqian Yu, Hanyu Zhou, Changxin Gao, Nong Sang, and Jingdong Wang. 2024. Plip: Language-image pretraining for person representation learning. NIPS, 37:4566645702. Zijie Wang, Aichun Zhu, Jingyi Xue, Xili Wan, Chao Liu, Tian Wang, and Yifeng Li. 2022a. Caibc: Capturing all-round information beyond color for textbased person retrieval. In ACM MM. Zijie Wang, Aichun Zhu, Jingyi Xue, Xili Wan, Chao Liu, Tian Wang, and Yifeng Li. 2022b. Look before you leap: Improving text-based person retrieval by learning consistent cross-modal common manifold. In ACM MM, pages 19841992. Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian. 2018. Person transfer gan to bridge domain gap for person re-identification. In CVPR, pages 7988. Yu Wu, Yana Wei, Haozhe Wang, Yongfei Liu, Sibei Yang, and Xuming He. 2023. Grounded image text matching with mismatched relation reasoning. In ICCV, pages 29762987. Linhui Xiao, Xiaoshan Yang, Fang Peng, Ming Yan, Yaowei Wang, and Changsheng Xu. 2023. Clipvg: Self-paced curriculum adapting of clip for visual grounding. TMM, 26:43344347. Yin Xie, Kaicheng Yang, Xiang An, Kun Wu, Yongle Zhao, Weimo Deng, Zimin Ran, Yumeng Wang, Ziyong Feng, Roy Miles, and 1 others. 2025. Regionbased cluster discrimination for visual representation learning. arXiv preprint arXiv:2507.20025. Shuanglin Yan, Neng Dong, Jun Liu, Liyan Zhang, and Jinhui Tang. 2023a. Learning comprehensive representations with richer self for text-to-image person re-identification. In ACMMM. Shuanglin Yan, Neng Dong, Liyan Zhang, and Jinhui Tang. 2023b. Clip-driven fine-grained text-image person re-identification. TIP, 32:60326046. Shuanglin Yan, Jun Liu, Neng Dong, Liyan Zhang, and Jinhui Tang. 2024. Prototypical prompting for text-to-image person re-identification. arXiv preprint arXiv:2409.09427. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, and 1 others. 2024a. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115. Fan Yang, Wei Li, Menglong Yang, Binbin Liang, and Jianwei Zhang. 2024b. Multi-modal disordered representation learning network for description-based person search. In AAAI. Kaicheng Yang, Jiankang Deng, Xiang An, Jiawei Li, Ziyong Feng, Jia Guo, Jing Yang, and Tongliang Liu. 2023a. Alip: Adaptive language-image pre-training with synthetic caption. In ICCV, pages 29222931. Kaicheng Yang, Tiancheng Gu, Xiang An, Haiqiang Jiang, Xiangzi Dai, Ziyong Feng, Weidong Cai, and Jiankang Deng. 2025. Clip-cid: Efficient clip distillation via cluster-instance discrimination. In AAAI, volume 39, pages 2197421982."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Detail Experimental Settings We present the settings used in the training GADMS in Tab. 5."
        },
        {
            "title": "Value",
            "content": "Temperature Loss weight β Multiple scales Adam β1 Adam β2 Adam ϵ Warm-up epochs Weight decay Batch size Learning rate Learning rate scheduler CosineAnnealingLR Training epochs GPU 0.02 0.4 [1,2] 0.9 0.999 103 5 4 105 512 104 30 8A100(80G) Table 5: Hyperparameters used for GA-DMS pretraining. A.2 Detail Instruction Prompt The prompt used to input Qwen2.5-72BInstruct (Yang et al., 2024a) for the generation of structured templates is as follows: First, identify the words in the title that describe pedestrian attributes, such as tops, pants, footwear, head features, accessories, age, gender, actions, etc. Then replace these words with crossidentity generic terms like colored top, colored bottom,hairstyle etc. Complete examples are as follows: \"A man wearing orange jersey with yellow stripes, pair of black shorts and pair of green shoes.\" [man] wearing [color top] with [color pattern], pair of [colored bottom] and pair of [colored shoes]. This lady is wearing glasses, and she has her hair in yellow ponytail. She is wearing striped shirt and is carrying bag over her right shoulder.\" This [person] is wearing an [accessory], and [he/she] has [colored hairstyle]. [He/She] is wearing patterned top and is carrying an object over [his/her] [body part]. women is wearing light colored sweater and black pants. She has long dark hair in pony tail. \" \"A [person] is wearing [colored top] and [colored bottom]. [He/She] has long [colored hair] in [hairstyle].\" The prompt used for inputting Qwen2.5-VLInstruct (Bai et al., 2025) to generate pedestrian descriptions is as follows: \"Please generate concise caption for the pedestrian image based on the following principles: Core Subject Focus: Only describe the dominant pedestrian elements in the frame (e.g., gender, clothing, footwear, head features, accessories, actions),focusing on the color of each part.\" Description restriction: 1.Use vague color terms (e.g., dark, light) only when the color is uncertain. 2.Use generic terms like \"top\" or \"bottom\" only when the clothing type is unclear, otherwise, use specific terms like \"shirt\" or \"shorts.\" Background Suppression Rule: Do not mention background information or abstract atmospheres (e.g., cozy). Certainty Principle: Only output visually confirmed details omit descriptions of unclear/lowresolution areas. Invisible elements do not need be described in the sentence(e.g., items are not visible). Avoid speculative terms (\"possibly\", \"seems\", \"appears to be\"), do not interpret potential relationships (e.g., inferring identity or emotions), and exclude artistic style critiques (e.g., \"impressionist style\"). Sentence Structure Reference: \"<Structured Template>\",First output the most significant pedestrian elements, the sentence length is less than <sequence length> English words. Use common words and phrasing from social media or daily life, ensuring correct grammar and logic. Provide only the caption sentence without any additional output.\" A.3 The Influence of Layers. We calculate the Gradient-Attention Similarity Score (GASS) between each text token and the image using the final layers of the text encoder. This study examines how the number of layers involved in gradient-based similarity computation influences performance. As depicted in Fig. 7, the model consistently outperforms the baseline, which lacks gradient-based masking, across all tested layer depths. Notably, employing the last 8 layers of the text encoder achieves the highest overall performance, underscoring their effectiveness in optimizing masking outcomes. A.4 Dataset analysis Do not add any extra features not included in the original description. Output only the final description without any explanation. Current text-based person retrieval datasets predominantly consist of manually annotated pedestrian images from re-identification benchmarks, Datasets CUHK-PEDES (Li et al., 2017) LPW (Song et al., 2018) MSMT-17 (Wei et al., 2018) RSTPReid (Zhu et al., 2021) ICFG-PEDES (Ding et al., 2021) LUPerson (Fu et al., 2021) LUPerson-NL (Fu et al., 2022) MALS (Yang et al., 2023b) LuPerson-T (Shao et al., 2023) Luperson-MLLM (Tan et al., 2024) SYNTH-PEDES (Zuo et al., 2024) WebPerson Year 2017 2018 2018 2021 2021 2021 2022 2023 2023 2024 2024 2025 #Images 40,206 592,438 126,441 20,505 54,522 4,180,243 10,683,716 1,510,330 957,606 1,020,022 4,791,771 5,002,723 #Descriptions 80,412 - - 41,010 54,522 - - 1,510,330 1,277,991 2,037,239 12,138,157 10,005,446 Data Source Market, Duke, etc. Surveillance Video Manual Collection MSMT-17 MSMT-17 YouTube YouTube Automatic Synthesis LUPerson LUPerson LUPerson-NL& LPW COYO-700M #Vocabulary Size 12,517 - - 6,331 5,848 - - 4,772 459 39,566 8,598 96, Label Method Manual Manual+Detector+NN FasterRCNN Manual Manual YOLOv5 FairMOT ImaginAIry CLIP MLLM SPAC MLLM Table 6: Statistical comparison of different datasets. WebPerson stands as the largest automatically-generated text-described person dataset, offering inherent scalability without manual annotation requirements. Figure 7: Results of different layers to compute S. The encoders contain 12 layers in total. fundamentally limited in scale and diversity by the substantial costs of human annotation. While generative methods have shown promise for dataset augmentation, they fail to achieve the necessary scale and fidelity for practical deployment. The emergence of Multimodal Large Language Models (MLLMs) and the availability of web-scale image resources now enable new paradigm for automated dataset construction. Our WebPerson dataset leverages novel image filtering and text generation techniques to create comprehensive pedestrian image library with accurate textual descriptions across diverse scenarios. Compared to existing datasets, WebPerson offers three key advantages: High-quality WebPerson surpasses existing datasets containing single-style synthetic images or low-quality surveillance footage by providing superior texture details and diverse scene variations. Our rigorous image filtering pipeline ensures exceptional visual fidelity, while the MLLMpowered text generation framework produces highly accurate and detailed descriptions. Fig. 8 showcases representative examples demonstrating precise textual characterization of pedestrian attributes. Diversity Sourced from web data, WebPerson exhibits rich variations in images, including but not limited to scene diversity, viewpoint changes, occlusions, clothing variations, and body poses. Our caption generation strategy further ensures corresponding textual descriptions maintain sufficient diversity. This dual-modality diversity enables 13 Figure 8: Visualization of some examples in our WebPerson dataset. WebPerson to serve as an effective training corpus for developing robust models that generalize well to novel and unseen data across visual tasks, language tasks, and vision-language tasks. Large-scale As illustrated in Tab. 6, we compare the attributes of WebPerson with other prominent person datasets. WebPerson emerges as the most extensive real-world dataset, featuring high-quality image-text pairs, encompassing 5 million images and 10 million textual descriptions. Moreover, our efficient data collection and caption generation strategies enable seamless scalability in data volume. A.5 Broader Impact This work introduces novel pedestrian representation learning framework that achieves finegrained cross-modal alignment through gradientbased token-wise similarity scoring while effectively suppressing noise interference. Complementing this framework, we construct WebPerson, large-scale human-centric dataset with diverse websourced image-text pairs. Together, these contributions demonstrate robust performance in humanoriented applications, including intelligent surveillance and autonomous retail systems."
        }
    ],
    "affiliations": [
        "DeepGlint",
        "Northeastern University",
        "South China University of Technology"
    ]
}