{
    "paper_title": "OmniAlpha: A Sequence-to-Sequence Framework for Unified Multi-Task RGBA Generation",
    "authors": [
        "Hao Yu",
        "Jiabo Zhan",
        "Zile Wang",
        "Jinglin Wang",
        "Huaisong Zhang",
        "Hongyu Li",
        "Xinrui Chen",
        "Yongxian Wei",
        "Chun Yuan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generative models have excelled in RGB synthesis, but real-world applications require RGBA manipulation. This has led to a fragmented landscape: specialized, single-task models handle alpha but lack versatility, while unified multi-task frameworks are confined to the RGB domain. To bridge this critical gap, we propose OmniAlpha, the first unified, multi-task generative framework for sequence-to-sequence RGBA image generation and editing. Its architecture features MSRoPE-BiL, a novel RoPE method with a bi-directionally extendable layer axis for its Diffusion Transformer (DiT) backbone, enabling the concurrent processing of multiple input and target RGBA layers. To power this framework, we introduce AlphaLayers, a new dataset of 1,000 high-quality, multi-layer triplets, built via a novel automated synthesis and filter pipeline. Jointly training OmniAlpha on this dataset across a comprehensive suite of 21 diverse tasks, extensive experiments demonstrate that our unified approach consistently outperforms strong, specialized baselines. Most notably, OmniAlpha achieves a dramatic 84.8% relative reduction in SAD for mask-free matting on AIM-500 and wins over 90% of human preferences in layer-conditioned completion. Our work proves that a unified, multi-task model can learn a superior shared representation for RGBA, paving the way for more powerful, layer-aware generative systems."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 1 1 2 0 2 . 1 1 5 2 : r OMNIALPHA: Sequence-to-Sequence Framework for Unified Multi-Task RGBA Generation Hao Yu1 Jiabo Zhan1 Zile Wang1 Jinglin Wang2 1Tsinghua University Huaisong Zhang1 Hongyu Li3 Xinrui Chen1 Yongxian Wei1 Chun Yuan1 2Beijing University of Posts and Telecommunications yuh24@mails.tsinghua.edu.cn, yuanc@sz.tsinghua.edu.cn 3Beihang University https://github.com/Longin-Yu/OmniAlpha Figure 1. Demonstrating OMNIALPHAs versatility across range of RGBA tasks. Our unified model handles: text-to-image generation (Row 1); layer decomposition and mask-conditioned matting (Row 2); referring and automatic matting (Row 3); and layer-conditioned completion (Row 4), along with other tasks described in the main text."
        },
        {
            "title": "Abstract",
            "content": "Generative models have excelled in RGB synthesis, but realworld applications require RGBA manipulation. This has led to fragmented landscape: specialized, single-task models handle alpha but lack versatility, while unified multi-task frameworks are confined to the RGB domain. To bridge this critical gap, we propose OMNIALPHA, the first unified, multi-task generative framework for sequence-to-sequence RGBA image generation and editing. Its architecture features MSRoPE-BiL, novel RoPE method with bi-directionally extendable layer axis for its Diffusion Transformer (DiT) backbone, enabling the concurrent processing of multiple input and target RGBA layers. To power this framework, we introduce AlphaLayers, new dataset of 1,000 high-quality, multi-layer triplets, built via novel automated synthesis and filter pipeline. Jointly training OMNIALPHA on this dataset across comprehensive suite of 21 diverse tasks, extensive experiments demonstrate that our unified approach consistently outperforms strong, specialized baselines. Most notably, OMNIALPHA achieves dramatic 84.8% relative reduction in SAD for mask-free matting on AIM-500 and wins over 90% of human preferences in layer-conditioned completion. Our work proves that unified, multi-task model can learn superior shared representation for RGBA, paving the way for more powerful, layer-aware generative systems. 1. Introduction Recent advances in generative models have been dominated by diffusion-based architectures [6, 25, 29], which have achieved unprecedented realism in conditional image synthesis. By leveraging efficient compression capabilities of VAEs [12] and large-scale text-to-image (T2I) pre-training, models such as Stable Diffusion [29], FLUX [13], and QwenImage [37] have become fundamental backbones for myriad of visual content creation, establishing powerful and efficient baselines for producing high-quality RGB images. Despite this remarkable progress in RGB synthesis, vast array of professional and real-world applications inherently require representations beyond opaque pixels. In domains such as visual effects (VFX), graphic design, and multi-layer generation, the ability to model and manipulate transparency is not an option, but necessity. The RGBA format, with its additional alpha channel, is the standard for this, as it explicitly encodes per-pixel opacity. This is crucial for modeling fine-grained details (e.g., hair, fur), semi-transparent objects (e.g., glass, smoke, water), and enabling the flexible compositing of multiple layers. Equal contribution. Corresponding author. 2 To address this need, family of highly specialized models has emerged to tackle specific RGBA-related tasks. For instance, image matting techniques [8, 44] excel at predicting precise alpha mattes from auxiliary inputs; generative layer decomposition pipelines [43] separate an image into its constituent foreground and background layers; object removal systems [36] focus on removing specified objects and realistically inpainting the resulting occluded regions; and text-to-image workflows like LayerDiffuse [47] have explored the synthesis of RGBA images conditioned on text. While powerful, these approaches are inherently fragmented. This high degree of specialization results in collection of functionally disjoint tools. user cannot employ matting model to perform layer decomposition, nor can layer generation model perform object removal. This \"one model per task\" paradigm is not only inefficient but also precludes the development of generalized, shared representation that could span the full spectrum of RGBA manipulation. Concurrently, contrasting paradigm has emerged within the RGB domain: multi-task unification. Frameworks such as OmniGen [41], PUMA [4], and VisualCloze [20] have demonstrated the significant advantages of training single, large-scale model to perform diverse array of tasks, from perception and generation to editing. These models achieve enhanced generalization and operational flexibility. However, this unification paradigm has been confined to the RGB space. Their core components, including architectures, autoencoders, and conditioning mechanisms, are fundamentally not designed to process or model the alpha channel. This renders them incapable of addressing the professional, layer-based workflows inherent to the RGBA domain. To address these challenges, we propose OMNIALPHA, unified, multi-task generative framework built on sequenceto-sequence diffusion transformer. Our model is designed as single, versatile system capable of executing diverse range of RGBA tasks. We achieve this versatility by jointly training it on comprehensive suite of 21 tasks, grouped into 5 high-level categories. The architecture features two core components: (1) an end-to-end alpha-aware VAE, which we efficiently create by adapting pre-trained RGB VAE using an opaque initialization strategy; and (2) multi-image DiT backbone. This backbone uses VLM for high-level semantic conditioning and our novel MSRoPE-BiL, RoPE method featuring an extra bi-directionally extendable layer axis, to concurrently process multiple input and target images in single forward pass. To power this multi-task framework, we construct AlphaLayers, new dataset of 1k high-quality layer triplets (i.e., foreground, background, and composition) with their corresponding text captions and masks in various forms, which serves as the foundation for our joint training. We combine visionlanguage annotation, model-guided image editing, and mask-based background inpainting to curate 10,000 candidate foreground, background, and composite triplets along with their associated captions and masks. We further apply consistency-based filtering stage that scores all generated candidates and retains only the most coherent 1,000 triplets, effectively removing mismatched layers and other compositional artifacts to ensure clean alignment and high overall quality. Our experiments show that OMNIALPHA consistently outperforms strong domain-specific baselines across both our test data and multiple public benchmarks spanning 5 task categories. Notably, in mask-free matting, OMNIALPHA achieves substantial relative improvement on AIM500, reducing SAD by 84.8% and MAD by 83.9%. These results, combined with over 90% human preference in layerconditioned completion, highlight the strong generalizability of our approach, delivering robust, state-of-the-art performance across diverse tasks and datasets. In summary, our contributions are as follows: We propose OMNIALPHA, the first unified multi-task framework for sequence-to-sequence RGBA image generation and editing, using multi-image DiT with bidirectional layer axis to enable concurrent RGBA multiimage processing. We introduce an automated pipeline that generates 1,000 high-quality RGBA triplets from single-layer foreground data, termed AlphaLayers, offering aligned captions and pixel-level masks for reliable supervision in training diffusion models. Experiments show that joint training across 21 diverse tasks enables OMNIALPHA to achieve state-of-the-art performance, consistently outperforming specialized, single-task models. 2. Related Work Transparency-aware Image Generation. Large diffusion models, such as Stable Diffusion [29], have achieved highquality image synthesis for t2i tasks, but their outputs are limited to single-layer RGB images. Recent advances have extended these models to generate RGBA content directly, often using diffusion transformer [25] architectures. Numerous studies have demonstrated the feasibility of multi-layer RGBA generation (PSDiffusion [9], LayerFusion [3], LayerDiffuse [47], ART [26], DreamLayer [10]) and end-to-end generation (AlphaVAE [35], Alfie [28]) using diffusion models. However, these works focus mainly on t2i generation, with limited exploration of more complex visual generation tasks. Task-Specific and Multi-Task Models. Some methods have extended the RGB/RGBA generation capability of diffusion models, with representative approaches emerging for specific tasks. For instance, dedicated models have emerged for object removal (ObjectDrop [36], ObjectClear [48], PowerPaint [49], DesignEdit [11]), image matting (MAM [17], Matte Anything [45], TeachDiffusionMatting [40], ViTMatte [44], DiffMatte [8], DRIP [19]), and generative layer decomposition (LayerDecomp [43]). While powerful, these approaches are inherently siloed, lacking the flexibility to generalize across multiple generation scenarios. The pursuit of such generalization has led to several unified multi-task models (OmniGen [41], OmniGen2 [38], VisualCloze [20], DreamOmni [39], PUMA [4]), but these frameworks are confined to RGB generation. Consequently, framework that jointly addresses multi-task RGBA generation remains relatively underexplored area. 3. Methodology 3.1. Task Formulation We formulate the task as conditional sequence-to-sequence generation problem. The objective is to generate target sequence of RGBA images, conditioned on an instructional text prompt and sequence of input RGBA images. Formally, given the instruction , an input sequence x1, x2, , xn RHW 4, and specified target count N+, we aim to train model capable of predicting the target image sequence ˆy1, ˆy2, , ˆym RHW 4: ˆy1, ˆy2, , ˆym = M(x1, x2, , xn; m, ) (1) We implement this model by adopting the latent diffusion paradigm [29]. This approach decomposes the generative process into two primary components: 1) An autoencoder system E, D. The encoder : RHW 4 maps RGBA images from the high-dimensional pixel space RHW 4 into the lower-dimensional latent space Z. Conversely, the decoder : RHW 4 reconstructs images from the latent space back into the pixel space RHW 4. 2) conditional diffusion model pθ operating entirely in the latent space Z. This probabilistic model is parameterized as noise predictor, ϵθ(zt, t, c), trained to estimate the noise added to latent variable zt at diffusion timestep t. For this task, the model is conditioned via c, representation derived from the instructional text and the input image sequence x1, , xn. 3.2. Model Architecture and Training Objectives In this section, we introduce our proposed OMNIALPHA. Our architecture is built upon the latent diffusion paradigm and comprises two core components. The first is an endto-end, alpha-aware VAE, which is efficiently initialized from pre-trained RGB one using our opaque initialization strategy. The second is the denoising backbone, diffusion transformer, which we extend with MSRoPE-BiL, novel position embedding method that enables the DiT to process and generate multiple input and output images concurrently, supporting our sequence-to-sequence task formulation. 3 Figure 2. Overview of the OMNIALPHA Diffusion Transformer architecture. Conditioned on task instruction and RGBA images, the model simultaneously denoises target images. We employ 3D MSRoPE for positional encoding, which treats the layer axis as z-index to effectively process multiple layers concurrently. 3.2.1. End-to-End Alpha-aware VAE 3.2.2. Sequence-to-Sequence Diffusion Transformer Our autoencoder system is an end-to-end, alpha-aware Variational Autoencoder (VAE) designed to operate on 4-channel RGBA images. This design follows the efficiency and conceptual simplicity of AlphaVAE [35]. To leverage powerful, pre-existing image synthesis priors, we initialize our 4-channel (RGBA) VAE, E, D, from well-optimized, pretrained 3-channel (RGB) VAE, Eref, Dref. We adapt the pretrained model using strategy we term opaque initialization, whose detailed formulation is listed in appendix A.1. This method involves surgically modifying the input and output convolutional layers (i.e., the first layer of the encoder and last layer of the decoder) to accommodate the new alpha channel. The encoder is modified to accept 4-channel RHW 4 input. Its weights are initialized to simply ignore the incoming alpha channel, focusing the initial computation entirely on the RGB information, i.e., copying the RGB weights and initializing the alpha weights as zero. The decoder is modified to produce 4-channel RHW 4 output. Its parameters are initialized to deterministically output fully opaque alpha channel (i.e., α = 1.0), while the RGB channels are reconstructed from the latent code as normal. The training objective is similar to AlphaVAE [35]: L(E, D) = λrecLrec(E, D) + λpercLperc(E, D) + λklLkl(E; Eref) + λref Lkl(E; (0; I)) + λGAN LGAN ({E, D}; Pd) (2) 4 Our models core backbone is Diffusion Transformer, which we conceptualize as unified sequence-to-sequence architecture. As depicted in Figure 2, this design allows the model to condition on task instruction and sequence of input RGBA images, = (x1, . . . , xn), to simultaneously predict the noise for target images, = (y1, . . . , ym), at given timestep t. This architecture extends the dual-stream conditioning paradigm proposed in Qwen-Image [37] to support our task formulation. We adopt its approach of using VLM (Qwen2.5-VL in the implementation) for high-level semantic context. For the second stream, which provides detailed spatial conditioning for all RGBA input images, we employ our Alpha-Aware VAE encoder E. This encoder independently compresses each input image xi into its latent representation zi = E(xi), which are then concatenated into ZX = (z1, . . . , zn) and provided as the spatial condition to the DiT blocks. key innovation of our architecture is the mechanism for unifying these multimodal inputs and multiple outputs into single sequence. Inspired by the MSRoPE from QwenImage [37], we introduce bi-directionally extendable layer axis to support and unify multiple inputs and outputs. We term this specific mechanism MSRoPE-BiL. This technique extends the standard 2D Rotary Position Embedding (RoPE) [31] by introducing third dimension, which we term the \"z-axis,\" to represent the layer index. This z-axis is bi-directionally extendable, supporting both positive and negative indices. MSRoPE-BiL allows the transformer to distinguish between different images and modalities by assigning unique index to each. As illustrated in Figure 2, we adopt specific indexing convention: Input Image Latents: The input latents ZX = (z1, . . . , zn) are assigned non-negative indices. The k-th input latent (1 n) occupies the index = 1. Standard 2D RoPE (x, y) is applied within each latent to encode the spatial position of its patches. Target Image Latents: The noisy target latents ˆZt = (ˆz1, . . . , ˆzm) are assigned negative indices. The k-th target latent (1 m) occupies the index = k. This design enables the model to process all outputs concurrently in single forward pass. VLM Condition Embeddings: The contextual embeddings output by the VLM encoder are assigned distinct positive indices n, spatially separating them from the input image latents. This MSRoPE-BiL formulation effectively transforms the complex multi-image, multi-modal generation task into unified sequence-to-sequence denoising paradigm, which can be efficiently processed by the MMDiT (Multimodal Diffusion Transformer) blocks. The training objective adapts the LDM loss [29] for our sequence-to-sequence task. For training instance sampled from the dataset D, let = cD be its specific condition and = (y1, . . . , ymD ) be its variable-length target sequence of mD images. The model ϵθ predicts the entire noise sequence ϵθ(Zt, t, c) = (ˆϵ1, . . . , ˆϵmD ) from condition and the noised latent sequence Zt at timestep t. To normalize the loss contribution from instances with variable output lengths, we compute the mean of the errors across the mD outputs: = EDD,tU [0,1],ϵN (0,I) (cid:34) 1 mD mD(cid:88) k= (cid:35) ϵk ˆϵk2 2 (3) where ϵ = (ϵ1, . . . , ϵmD ) is the ground-truth noise sequence applied to E(Y ) to generate Zt. 3.3. Multi-Task Joint Training key advantage of our sequence-to-sequence model architecture is its capacity for multi-task unification. By formulating diverse image generation and manipulation tasks as conditional sequence generation problems, we can train single, unified model. This joint training approach enables the model to learn rich, shared representation, improving its generalization capabilities across all tasks. In practice, our unified model is jointly trained on 21 distinct tasks grouped into 5 high-level categories, as detailed in Table 1. 3.4. Dataset Construction In this section, we summarize the construction of AlphaLayers, our alpha-aware multi-task dataset, developed through three-step process. First, we curate and filter diverse set of data sources to create foundational corpus of 10k high-quality RGBA image samples. Second, to generate data for our layer-aware tasks, we design an automated pipeline that employs cascade of specialized models. This pipeline transforms the foundational images into multi-layer triplets (i.e., foreground, background, and composite), each augmented with corresponding text captions. Finally, to support tasks requiring fine-grained, pixel-level control, we derive comprehensive set of conditional masks (e.g., precise masks, rough masks, and trimaps) from the alpha channels of our curated data. The triplets are used to construct all mask-free tasks, while the conditional maps are used for mask-related tasks. Collection of RGBA Images. We collect highquality RGBA dataset by integrating various open-source matting-related datasets, including Adobe Image Matting [42], AM-2K [15], Distinctions-646 [27], HHM2K [33], Human-1K [22], P3M-500-NP [14], PhotoMatte85 [21], RealWorldPortrait-636 [46], SIMD [32], Transparent-460 [1], and PrismLayers [2]. For sample RHW 4 existing in RGBA form, we directly collect it. For sample providing foreground RGB image Ifg RHW 3 with its corresponding alpha matte α RHW , we concatenate them into four-channel image = concat(Ifg, α) RHW 4. We additionally perform aesthetic sorting based on LAION-AES scores and conduct manual filtering to remove low-quality or defective samples, particularly those with truncated foregrounds caused by imperfect matting processes. After these refinement steps, we obtain clean, diverse, and photorealistic corpus of about 10k high-quality RGBA images serving as the foundation for subsequent training and evaluation. Curation of Multi-layer Tuples. To construct the AlphaLayers dataset, we start from foreground RGBA dataset {Ifg} and, following the pipeline in Fig. 3, generate pool of approximately 10,000 candidate triplets {(Tfg, Ifg), (Tbg, Ibg), (Tcomp, Icomp)}. For each foreground image Ifg, we first query Qwen3-VL [34] to obtain fine-grained foreground caption Tfg, and then ask the model to imagine the object in plausible scene, obtaining the composition caption Tcomp. Given both Ifg and Tcomp, Qwen3-VL produces structured replacement instruction Trep, which is executed by Qwen-Image-Edit [37] to synthesize the composite image Icomp. We then compute binary mask from the alpha channel αfg of the foreground and apply ObjectClear [48] to Icomp to remove the object and inpaint the missing region, obtaining clean background image Ibg, which is captioned again by Qwen3-VL to produce Tbg. Each candidate triplet is evaluated using the foreground 5 Table 1. Summary of the 21 tasks grouped into 5 categories in our multi-task joint training framework. All tasks are conditioned on task instruction, which is ignored in the table for simplicity. The symbols in the input and output columns are detailed in Section 3.4. Category Task Name Input(s) Output(s) Explanation Text-to-Image Text-to-Image Generation Layer-Conditioned Completion Image Matting Foreground-to-Background Generation Foreground-Conditioned Completion Background-to-Foreground Generation Background-Conditioned Completion Automatic (Mask-Free) Matting Alpha-Conditioned Matting Trimap-Conditioned Matting Precise Mask-Conditioned Matting Rough Mask-Conditioned Matting Text-Conditioned (Referring) Matting Object Removal Layer Decomposition Automatic (Mask-Free) Object Removal Alpha-Conditioned Object Removal Trimap-Conditioned Object Removal Precise Mask-Conditioned Object Removal Rough Mask-Conditioned Object Removal Automatic (Mask-Free) Layer Decomposition Alpha-Conditioned Layer Decomposition Trimap-Conditioned Layer Decomposition Precise Mask-Conditioned Layer Decomposition Rough Mask-Conditioned Layer Decomposition Tfg {Ifg}n {Ifg}n Ibg, Tfg Ibg, Tcomp i=1, Tbg i=1, Tcomp Ifg Ibg Icomp {Ifg}m i=1 Icomp Icomp Icomp, αfg Icomp, Mtrimap Icomp, Mprecise Icomp, Mrough Icomp, Tfg Icomp Icomp, αfg Icomp, Mtrimap Icomp, Mprecise Icomp, Mrough Icomp Icomp, αfg Icomp, Mtrimap Icomp, Mprecise Icomp, Mrough Ifg Ifg Ifg Ifg Ifg Ifg Ibg Ibg Ibg Ibg Ibg {Ifg}m1 Ifg, Ibg Ifg, Ibg Ifg, Ibg Ifg, Ibg i=1 , Ibg Generate full RGBA image from content prompt. Generate missing layers (foreground or background) conditioned on the other layer(s), or return the full composite image. Extract precise foreground layer (RGBA) from an image, optionally guided by various conditions (e.g., mask, trimap, or content prompt). Remove specified object (or the most salient one) from an image and return the inpainted background. Separate an image into foreground and background layers. Automatic decomposition may identify multiple salient layers, while conditioned tasks extract single specified layer. Figure 3. Dataset preparation pipeline. We construct the multi-layer dataset using Qwen3-VL [34] as the core vision-language model, Qwen-Image-Edit [37], and ObjectClear [48] as domain-specific expert models. consistency and the reblend consistency metrics, which are combined into weighted score S. We then rank all 10,000 candidates by and retain the top 1,000 most consistent triplets as the final AlphaLayers dataset, effectively filtering out mismatched layers and compositing artifacts. The detailed formulation is listed in the Appendix B.1. Synthesis of Pixel-level Fine-grained Condition. For matting and object-removal variants requiring mask-based conditions, we derive multiple region controls from the RGBA alpha channel. As shown in Figure 4, from each foreground Ifg, we first extract the continuous alpha map αfg as the base mask. precise mask Mprecise is obtained by thresholding high-opacity pixels, while trimap Mtrimap is constructed by generating inner and outer bands through standard morphological operations [5] (erosion and dilation) with disk 6 Figure 4. Mask Generation Pipeline. Starting from the foreground image, we get tuple of masks in various forms. kernel of size U[5%, 10%] of min(H, ). Pixels in the eroded region are labeled as definite foreground, pixels outside the dilated region as background, and the band in between as unknown. We further produce rough mask Mrough by converting the unknown band of the trimap into foreground. This process yields four spatially aligned supervision targets {αfg, Mprecise, Mtrimap, Mrough}, which provide multiple levels of region constraints for matting and object-removal tasks. The detailed formulation is listed in Appendix B.2. 4. Experiments 4.1. Setup Implementation. We adopt Qwen-Image-Edit [37] as our base model and employ two-stage training paradigm. We first adapt the pre-trained RGB VAE from Qwen-ImageEdit into an alpha-aware (RGBA) VAE by fine-tuning it on our RGBA image dataset for 32k steps. This stage uses global batch size of 16 and the AdamW optimizer [23] (β1=0.9, β2=0.999). We use base learning rate of 1.5 105 with 5% linear warmup, followed by cosine decay schedule. With the fine-tuned RGBA VAE weights frozen, we train the DiT backbone, also initialized from QwenImage-Edit, for 100k steps. This stage uses global batch size of 8 with the AdamW optimizer (β1=0.9, β2=0.999). For parameter-efficient fine-tuning, we apply LoRA [7] with rank of 256 to all attention weights and MLP layers, using constant learning rate of 5 105. All models are trained on 8 NVIDIA H20 GPUs. Datasets. The AlphaLayers dataset has been divided into training and test subsets, comprising 900 and 100 instances, respectively. For clarity, the test subset is referred to as AlphaLayersTest in this context. To further assess generalization, we also adopt three public benchmark datasets, AIM-500 [16], RORD [30], and RefMatte-RW100 [18], as out-of-distribution (OOD) testsets. 4.2. Experimental Results As shown in Figure 1, our proposed OMNIALPHA effectively addresses broad range of tasks within single, unified model. It demonstrates robust performance across diverse and complex scenarios, including the accurate extraction of semi-transparent objects (e.g., glass cup), the precise manipulation of intricate foregrounds and backgrounds, and the consistent preservation of semantic fidelity to text instructions. In this section, we provide quantitative evaluation of the models performance, benchmarking it against several strong baselines that represent domain-specific models with expert-level capabilities across various fields. Additional results can be found in Appendix D, and the detailed evaluation settings are provided in Appendix C. 4.2.1. Text-to-Image We evaluate the text-to-image generation ability of our model on two benchmarks: AIM-500 [16] and AlphaLayersTest. We compare OMNIALPHA against representative baselines, including LayerDiffuse [47] and AlphaVAE [35]. As shown in Table 2, our model achieves competitive results on both benchmarks. Specifically, our method attains an FID of 118.37 and CLIP score of 0.33 on AlphaLayers, outperforming all baselines and demonstrating superior text-image alignment and generation quality. OMNIALPHA also shows excellent performance on AIM-500, highlighting its strong generalization capability. Table 2. Text-to-Image results on AIM-500 and AlphaLayersTest. Method AIM-500 AlphaLayersTest FID CLIP-Score FID CLIP-Score GT 0 LayerDiffuse AlphaVAE Ours 160.27 155.66 157.34 0. 0.3213 0.3195 0.3417 0 160.12 154.37 118.37 0.3273 0.3200 0.3239 0.3329 4.2.2. Layer-Conditioned Completion We further evaluate our model on two conditional composition tasks: Foreground-Conditioned Completion (FG2FULL) and Background-Conditioned Completion (BG2FULL). We adopt pairwise preference protocol against LayerDiffuse: for each sample, we provide GPT4o [24] and Qwen3-VL [34] with the textual prompt, the conditioning layer (foreground or background), and the two candidate blended images produced by the two methods, and ask the model to decide which result better matches the description or declare tie. To mitigate order bias, we query each VLM twice with the candidates presented in swapped order and combine the two judgments into single preference label. In addition, we conduct human study with 10 annotators, where each expresses preference (or tie) for every case; the majority vote over the 10 annotations is taken as the final human preference for that sample. The resulting win rates (%) for our method and LayerDiffuse are reported in Table 3. Our model achieves consistently higher win rates than LayerDiffuse in both FG2FULL and BG2FULL settings. This indicates stronger conditional reasoning and more coherent layer-aware composition, confirming that our multi-layer training paradigm leads to more controllable and semantically aligned generation. Table 3. Conditional layer generation results on AlphaLayersTest. Numbers are win rates (%) in pairwise comparisons against LayerDiffuse, judged by Qwen3-VL, GPT-4o, and human annotators; Tie indicates no clear preference. FG2FULL BG2FULL Preference LayerDiffuse Ours Tie Qwen3-VL GPT-4o Human Qwen3-VL GPT-4o Human 8% 88% 4% 9% 85% 6% 4% 91% 5% 3% 85% 12% 5% 87% 8% 3% 95% 2% 4.2.3. Image Matting We evaluate our model under two matting setups: Referring Image Matting and Mask-Free Image Matting. In the referring setting, the model receives only natural-language referring expression as conditioning (no mask) and must predict the alpha matte for the region described in the text on RefMatte-RW100. In the mask-free setting, the model instead uses single task-level prompt shared across images (e.g., Automatically derive the visible subject from the photo with accurate transparency.) and performs matting without any trimap or additional guidance, evaluated on AIM-500 and AlphaLayersTest. For readability, all metrics in Tables 4 and 5 are reported after rescaling: SAD, MAD, GRAD, and CONN are multiplied by 103, and MSE is multiplied by 103. As shown in Table 4, our method achieves the lowest SAD, MSE, and GRAD on RefMatte-RW100 and clearly 7 Table 4. Referring Image Matting results on RefMatte-RW100 and AlphaLayersTest."
        },
        {
            "title": "Method",
            "content": "RefMatte-RW"
        },
        {
            "title": "AlphaLayersTest",
            "content": "SAD MSE GRAD CONN SAD MSE GRAD CONN"
        },
        {
            "title": "MAM\nMatAny\nTeachDiffusionMatting\nOurs",
            "content": "16.96 19.92 7.370 6.751 0.0626 0.7207 0.0264 0.0242 8.560 9.570 6.550 1.904 10.78 7.710 5.310 6.689 17.16 20.17 8.210 5.889 0.0659 0.0717 0.0312 0. 8.930 9.680 7.240 1.034 10.66 7.940 7.690 5.847 Table 5. Mask-Free Image Matting results on AIM-500 and AlphaLayersTest."
        },
        {
            "title": "Method",
            "content": "SAD MSE MAD GRAD CONN SAD MSE MAD GRAD CONN AIM-"
        },
        {
            "title": "AIM\nSmartMatting\nOurs",
            "content": "48.09 34.30 7.796 0.0183 0.0129 0.0136 0.0285 0.0203 0.0046 47.58 31.49 13.53 21.74 13.98 7.8820 50.10 37.35 6. 0.0201 0.0142 0.0128 0.0195 0.0221 0.0023 49.14 32.51 8.552 23.61 14.12 6.199 outperforms all baselines on AlphaLayersTest across all four metrics, indicating stronger understanding of textual referring expressions and better cross-domain generalization. Table 5 further shows that in the mask-free setting, our model attains the best overall performance on AIM-500, achieving significantly lower SAD and MAD while keeping MSE on par with competing methods. It also consistently yields the best scores on AlphaLayersTest, demonstrating superior finegrained alpha reconstruction and strong robustness when only generic task prompt is available. 4.2.4. Layer Decomposition and Object Removal We evaluate our model on the removal setting of the RORD dataset, where the goal is to remove the foreground object and recover clean background. We instantiate this setting with two of our tasks: decompose, which first predicts the layered foreground/background and then uses only the recovered background layer for evaluation, and removal, which directly predicts the edited image with the object removed. As shown in the table, our decompose variant achieves the best perceptual and distributional quality (LPIPS, FID, CLIPFID), while our removal variant further improves PSNR to 25.14, indicating that both tasks can effectively perform object removal and together offer clear advantages over the baseline. Our method achieves superior or comparable performance to LayerDecomp across all metrics, exhibiting improved perceptual quality (lower LPIPS and FID) and better semantic alignment (lower CLIP-FID). As shown in Table 6, these results indicate that the proposed multi-layer training scheme not only supports controllable generation but also enables accurate structural decomposition, thereby providing unified representation for downstream editing and transparency Table 6. Layer Decompose results on RORD. Method PSNR LPIPS FID CLIP-FID RORD LayerDecomp Ours (decompose) Ours (removal) 24.79 24.76 25.14 0.1320 0.1268 0.1307 21.73 20.26 22.31 5.735 5.142 5. reasoning. 5. Conclusion We present OMNIALPHA, the first unified, multi-task generative model for sequence-to-sequence RGBA manipulation. We bridge the critical gap between specialized, singletask alpha models and powerful, but alpha-blind, unified RGB frameworks. This is enabled by two core technical components: an efficient alpha-aware VAE adapted from RGB priors via opaque initialization, and MSRoPE-BiL, RoPE method with bi-directionally extendable layer axis that allows our DiT backbone to concurrently process input and output layers. By jointly training on 21 diverse tasks using our newly curated high-quality AlphaLayers dataset, OMNIALPHA learns rich, shared representation that achieves state-of-the-art performance. For instance, it significantly surpasses specialized baselines on benchmarks like RefMatte-RW100 and RORD, and greatly reduces trimap-free matting error on AIM-500 (SAD 48.09 7.80), while winning over 90% of human preferences in layer-conditioned generation. Our work validates the feasibility and superiority of unified multi-task approach for RGBA manipulation, paving the way for new class of powerful, general-purpose alpha-aware generative systems."
        },
        {
            "title": "References",
            "content": "[1] Huanqia Cai, Fanglei Xue, Lele Xu, and Lili Guo. Transmatting: Enhancing transparent objects matting with transformers, 2022. 5 [2] Junwen Chen, Heyang Jiang, Yanbin Wang, Keming Wu, Ji Li, Chao Zhang, Keiji Yanai, Dong Chen, and Yuhui Yuan. Prismlayers: Open data for high-quality multi-layer transparent image generative models, 2025. 5 [3] Yusuf Dalva, Yijun Li, Qing Liu, Nanxuan Zhao, Jianming Zhang, Zhe Lin, and Pinar Yanardag. Layerfusion: Harmonized multi-layer text-to-image generation with generative priors, 2024. 3 [4] Rongyao Fang, Chengqi Duan, Kun Wang, Hao Li, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, Hongsheng Li, and Xihui Liu. Puma: Empowering unified mllm with multi-granular visual generation, 2024. 2, 3 [5] Robert M. Haralick, Stanley R. Sternberg, and Xinhua Image analysis using mathematical morphology. Zhuang. IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-9(4):532550, 1987. 6, 2 [6] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020. 2 [7] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021. 7 [8] Yihan Hu, Yiheng Lin, Wei Wang, Yao Zhao, Yunchao Wei, and Humphrey Shi. Diffusion for natural image matting, 2024. 2, 3 [9] Dingbang Huang, Wenbo Li, Yifei Zhao, Xinyu Pan, Yanhong Zeng, and Bo Dai. Psdiffusion: Harmonized multi-layer image generation via layout and appearance alignment, 2025. 3 [10] Junjia Huang, Pengxiang Yan, Jinhang Cai, Jiyang Liu, Zhao Wang, Yitong Wang, Xinglong Wu, and Guanbin Li. Dreamlayer: Simultaneous multi-layer generation via diffusion mode, 2025. 3 [11] Yueru Jia, Yuhui Yuan, Aosong Cheng, Chuke Wang, Ji Li, Huizhu Jia, and Shanghang Zhang. Designedit: Multi-layered latent decomposition and fusion for unified & accurate image editing, 2024. 3 [12] Diederik Kingma and Max Welling. Auto-encoding variational bayes, 2022. 2 [13] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Müller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space, 2025. 2 [14] Jizhizi Li, Sihan Ma, Jing Zhang, and Dacheng Tao. Privacypreserving portrait matting, 2021. 5 [15] Jizhizi Li, Jing Zhang, Stephen J. Maybank, and Dacheng Tao. Bridging composite and real: Towards end-to-end deep image matting, 2021. 5 [16] Jizhizi Li, Jing Zhang, and Dacheng Tao. Deep automatic natural image matting, 2021. [17] Jiachen Li, Jitesh Jain, and Humphrey Shi. Matting anything, 2023. 3 [18] Jizhizi Li, Jing Zhang, and Dacheng Tao. Referring image matting, 2023. 7 [19] Xiaodi Li, Zongxin Yang, Ruijie Quan, and Yi Yang. Drip: Unleashing diffusion priors for joint foreground and alpha prediction in image matting. Advances in Neural Information Processing Systems 37, 2024. 3 [20] Zhong-Yu Li, Ruoyi Du, Juncheng Yan, Le Zhuo, Zhen Li, Peng Gao, Zhanyu Ma, and Ming-Ming Cheng. Visualcloze: universal image generation framework via visual in-context learning, 2025. 2, [21] Shanchuan Lin, Andrey Ryabtsev, Soumyadip Sengupta, Brian Curless, Steve Seitz, and Ira Kemelmacher-Shlizerman. Real-time high-resolution background matting, 2020. 5 [22] Yuhao Liu, Jiake Xie, Xiao Shi, Yu Qiao, Yujie Huang, Yong Tang, and Xin Yang. Tripartite information mining and integration for image matting. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 75557564, 2021. 5 [23] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv: 1711.05101, 2017. 6 [24] OpenAI. Gpt-4 technical report, 2024. 7 [25] William Peebles and Saining Xie. Scalable diffusion models with transformers, 2023. 2, 3 [26] Yifan Pu, Yiming Zhao, Zhicong Tang, Ruihong Yin, Haoxing Ye, Yuhui Yuan, Dong Chen, Jianmin Bao, Sirui Zhang, Yanbin Wang, Lin Liang, Lijuan Wang, Ji Li, Xiu Li, Zhouhui Lian, Gao Huang, and Baining Guo. Art: Anonymous region transformer for variable multi-layer transparent image generation, 2025. [27] Yu Qiao, Yuhao Liu, Xin Yang, Dongsheng Zhou, Mingliang Xu, Qiang Zhang, and Xiaopeng Wei. Attention-guided hierarchical structure aggregation for image matting. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 5 [28] Fabio Quattrini, Vittorio Pippi, Silvia Cascianelli, and Rita Cucchiara. Alfie: Democratising rgba image generation with no $$$, 2024. 3 [29] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models, 2022. 2, 3, 5 [30] Min-Cheol Sagong, Yoon-Jae Yeo, Seung-Won Jung, and Sung-Jea Ko. Rord: real-world object removal dataset. In British Machine Vision Conference, 2022. 7 [31] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 4 [32] Yanan Sun, Chi-Keung Tang, and Yu-Wing Tai. Semantic image matting, 2021. 5 [33] Yanan Sun, Chi-Keung Tang, and Yu-Wing Tai. Ultrahigh resolution image/video matting with spatio-temporal sparsity. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1411214121, 2023. 5 [34] Qwen Team. Qwen3 technical report, 2025. 5, 6, 7 9 [35] Zile Wang, Hao Yu, Jiabo Zhan, and Chun Yuan. Alphavae: Unified end-to-end rgba image reconstruction and generation with alpha-aware representation learning. arXiv preprint arXiv: 2507.09308, 2025. 3, 4, 7 [36] Daniel Winter, Matan Cohen, Shlomi Fruchter, Yael Pritch, Alex Rav-Acha, and Yedid Hoshen. Objectdrop: Bootstrapping counterfactuals for photorealistic object removal and insertion, 2024. 2, [37] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, and Zenan Liu. Qwen-image technical report, 2025. 2, 4, 5, 6, 1 [38] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, Ze Liu, Ziyi Xia, Chaofan Li, Haoge Deng, Jiahao Wang, Kun Luo, Bo Zhang, Defu Lian, Xinlong Wang, Zhongyuan Wang, Tiejun Huang, and Zheng Liu. Omnigen2: Exploration to advanced multimodal generation, 2025. 3 [39] Bin Xia, Yuechen Zhang, Jingyao Li, Chengyao Wang, Yitong Wang, Xinglong Wu, Bei Yu, and Jiaya Jia. Dreamomni: Unified image generation and editing, 2025. 3 [40] Tianyi Xiang, Weiying Zheng, Yutao Jiang, Tingrui Shen, Hewei Yu, Yangyang Xu, and Shengfeng He. Teaching diffusion models to ground alpha matte. Transactions on Machine Learning Research, 2025. 3 [41] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation, 2024. 2, 3 [42] Ning Xu, Brian Price, Scott Cohen, and Thomas Huang. Deep image matting, 2017. [43] Jinrui Yang, Qing Liu, Yijun Li, Soo Ye Kim, Daniil Pakhomov, Mengwei Ren, Jianming Zhang, Zhe Lin, Cihang Xie, and Yuyin Zhou. Generative image layer decomposition with visual effects, 2024. 2, 3 [44] Jingfeng Yao, Xinggang Wang, Shusheng Yang, and Baoyuan Wang. Vitmatte: Boosting image matting with pretrained plain vision transformers, 2023. 2, 3 [45] Jingfeng Yao, Xinggang Wang, Lang Ye, and Wenyu Liu. Matte anything: Interactive natural image matting with segment anything models, 2024. 3 [46] Qihang Yu, Jianming Zhang, He Zhang, Yilin Wang, Zhe Lin, Ning Xu, Yutong Bai, and Alan Yuille. Mask guided matting via progressive refinement network, 2021. 5 [47] Lvmin Zhang and Maneesh Agrawala. Transparent image layer diffusion using latent transparency, 2024. 2, 3, 7 [48] Jixin Zhao, Shangchen Zhou, Zhouxia Wang, Peiqing Yang, and Chen Change Loy. Objectclear: Complete object removal via object-effect attention, 2025. 3, 5, 6, 2 [49] Junhao Zhuang, Yanhong Zeng, Wenran Liu, Chun Yuan, and Kai Chen. task is worth one word: Learning with task prompts for high-quality versatile image inpainting, 2024. 10 OMNIALPHA: Sequence-to-Sequence Framework for Unified Multi-Task RGBA Generation"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Details of Model Architecture , bEref A.1. Opaque Initialization of VAE Formally, let (W Eref 0 ) be the parameters of the first 0 convolutional layer of Eref, with Eref 0 Rkk3Dc and bEref 0 RDc . Let (W Dref ) be the parameters of the final RkkDf 3 and convolutional layer of Dref, with Dref bDref R3. The corresponding layers of the new 4-channel VAE, (W 0 , bE ), are initialized as specified in Equation 4: 0 ) and (W , bDref , bD 0 0 [:, :, 1 : 3, :] = Eref E 0 [:, :, 4, :] = 0 0 = bEref bE 0 [:, :, :, 1 : 3] = Dref D [:, :, :, 4] = 0 [1 : 3] = bDref bD bD [4] = 1 (Copy RGB weights) (0-init alpha weights) (Copy bias) (Copy RGB weights) (4) (0-init alpha weights) (Copy RGB biases) (Set as opaque) 0 and have new shapes Rkk4Dc and where RkkDf 4, respectively. The bias bD is 4-dimensional vector. This opaque initialization provides stable starting point for fine-tuning. A.2. Implementation of MSRoPE-BiL The implementation of MSRoPE-BiL leverages the inherent translation invariance property of Rotary Positional Embeddings. Fundamentally, the dot product of two RoPE-encoded feature vectors, and k, at positions pi = (xi, yi, zi) and pj = (xj, yj, zj), depends solely on their relative spatial and layer distances: R(q, pi), R(k, pj) = g(q, k, pi pj). (5) Consequently, shifting the layer index by constant scalar for all tokens preserves the relative positional relationships and, by extension, the attention scores. This property can be formally expressed as: R(h, x, y, z) R(h, x, y, z)1 = R(h, x, y, + S) R(h, x, y, + S)1, (6) where represents the hidden states. In our sequence-to-sequence formulation, target images are assigned negative layer indices {m, . . . , 1}, where is the number of generated target frames. However, 1 the pre-trained Qwen-Image-Edit backbone typically utilizes pre-computed frequency tables defined over non-negative domain. To align our bi-directional indexing strategy with the pre-defined frequency configurations of the base model, we introduce global index shift operation. Let zraw denote the logical layer index defined in Section 3. Specifically: zraw {m, . . . , 1} {0, . . . , 1} {n, . . . } for target latents, for input latents, for VLM condition tokens. (7) To ensure all indices map to the valid domain of the pretrained frequency encodings, we define the implementation index zimpl as: zimpl = zraw + Soffset, where Soffset m. (8) In practice, we set Soffset = m. This transformation allows OMNIALPHA to concurrently process multiple inputs and outputs while seamlessly utilizing the optimized RoPE functions and frequency computations inherited from QwenImage-Edit. B. Details of Data Construction B.1. Curation of Multi-layer Tuples To train our multi-layer model, we extend the foreground image Ifg dataset into triplet of foreground image Ifg, background image Ibg, and composite image Icomp, with their corresponding captions Tfg, Tbg, and Tcomp. Guided by the pipeline in Figure 3, we first query Qwen3VL-30B-A3B-Instruct with the foreground image Ifg as input to obtain fine-grained Tfg. Qwen3-VL is then asked to imagine how this object would naturally appear within complete scene, yielding holistic composition caption Tcomp that contextualizes the object within an appropriate environment. To synthesize this environment in image space, we further prompt Qwen3-VL to take both the foreground image and the composition caption as input, producing structured replace instruction suitable for an imageediting model. We feed Ifg together with the replace instruction into Qwen-Image-Edit [37], which generates natural composition image Icomp whose background is consistent with the described scene. To obtain clean background layer, we compute the binary mask from the foreground alpha channel and apply ObjectClear [48] to Icomp, removing the object and inpainting the region. This yields plausible background image Ibg. We finally query Qwen3-VL once more to describe this background alone, producing background caption Tbg that contains no object mentions. Each training instance is therefore represented as {(Tfg, Ifg), (Tbg, Ibg), (Tcomp, Icomp)}, which we collectively refer to as AlphaLayers. The final dataset contains approximately 10,000 multi-layer triplets. For each triplet, we evaluate (i) the foreground-composite consistency, measured by the MSE between the original foreground and the foreground region rendered in the composite image, MSEfgcomp = ExΩα (cid:13)Ifg(x) Icomp(x) α(x)(cid:13) (cid:13) 2 2, (cid:13) (9) where Ωα = {x α(x) > 0.05} denotes the valid foreground region and (ii) the recomposite consistency, obtained by reconstructing synthetic composite image from (Ifg, Ibg) via alpha compositing and comparing it with the ground-truth composite image: Irecomp(x) = Ifg(x)α(x) + Ibg(x)(1 α(x)) (cid:13)Irecomp(x) Icomp(x)(cid:13) (cid:13) 2 2 (11) (cid:13) The two metrics are combined into weighted score, MSErecompcomp = Ex (10) = λ MSEfgcomp + (1 λ) MSErecompcomp, (12) with λ = 0.6 in practice, and all samples are ranked by S. We keep the top 1,000 most consistent triplets and discard the rest. This filtering procedure reliably removes samples with mismatched layers or compositing artifacts, significantly improving the quality of the final training set used for our multi-layer diffusion model. B.2. Synthesis of Pixel-level Fine-grained Condition. For matting and object-removal variants we introduce region controls derived from the RGBA alpha. From each foreground we extract the continuous alpha map Mmask [0, 1] as the base mask dataset. precise mask Mprecise is obtained by hard-thresholding the 1, Mprecise = I[A > τprecise], τprecise = 0.95, which keeps only confidently opaque pixels for precise boundary supervision. To build trimap used by matting models, we construct inner/outer bands via computer graphics operations with disk kernel size U[5%, 10%]: = erodek(Mmask), + = dilatek(Mmask), 2 where erode and dilate are standard morphological operations from mathematical morphology [5]: erosion removes boundary pixels within disk of radius k, producing shrunken inner region, while dilation adds pixels within the same disk to create an expanded outer region. We then assign (x) = 255 for (foreground), (x) = 0 for / + (background), and (x) = 128 elsewhere (unknown band), which encourages reasoning in the transition region. Finally, rough mask Mrough is derived by converting the unknown band of the trimap to the foreground (gray white), i.e., Mrough(x) = I[T (x) {128, 255}], providing permissive region for object clearing and background synthesis. This yields four aligned supervision targets, Mmask, Mprecise, , and Mrough, as illustrated in Figure 4. B.3. Dataset Collection Prompts (Qwen3-VL) Foreground Caption Prompt. This prompt extracts concise and fine-grained description of the main foreground object. It takes the raw foreground image Ifg as input and outputs concise foreground caption Tfg without background references. PROMPT = ( \"Describe the main subject of the image accurately and vividly in one concise English sentence \" \"(under 64 tokens), focusing on key visual details such as appearance, action, \" \"texture, or color. Do not mention the background.\" ) Input: foreground image Ifg Output: foreground caption Tfg Composite Caption Prompt. This prompt expands the foreground description into holistic composite description that is semantically consistent with the given foreground image. It takes the foreground image Ifg and its caption Tfg as input, and outputs natural, realistic composite caption Tcomp. PROMPT = ( \"You are professional visual designer and prompt engineer. \" \"You are given both an image and its caption describing the main subject. \" \"Expand them into an English prompt suitable for an image editing model. \" \"Imagine realistic environment that naturally suits the subject described in the caption, \" \"a place where this subject would plausibly exist or interact. \" \"Then describe the entire scene, integrating both the subject and its fitting surroundings. \" \"Use fluent English across 1-2 sentences, totaling around 40 words. No more than 50 words.nn\" \"Example:n\" \"Caption: red apple on white plate.n\" \"Prompt: In cozy kitchen bathed in soft morning light, rustic wooden table stands near large window. \" \"On the table rests white ceramic plate holding shiny red apple, its surface catching the glow of sunlight and casting faint shadow.nn\" ) Input: foreground image Ifg, foreground caption Tfg Output: composite caption Tcomp Structured Replace Instruction Prompt. This prompt rewrites the holistic composite description into structured background-editing instruction. It takes the foreground image Ifg and the composite caption Tcomp as input and outputs standardized replace instruction Treplace suitable for QwenImage-Edit-2509. PROMPT = ''' # Edit Instruction Rewriter You are professional edit instruction rewriter. Your task is to generate precise, concise, and visually achievable professional-level edit instruction based on the user-provided instruction and the image to be edited. --- ## Task Context All upcoming tasks are explicitly **background edit tasks**. This means your rewriting should **only modify or enhance the background** of the image -- for example, adding environmental details, realistic lighting, materials, reflections, or spatial depth -- while **preserving the subjects original position, posture, and identity**. Do **not** change or imply motion, pose, or action of the subject. Your rewritten prompt must always focus on creating visually rich and coherent background scene. **All rewritten results must begin with the phrase: \"Replace the background with ...\"** This ensures consistency and helps the model produce precise, localized background edits. --- Please strictly follow the rewriting rules below: --- ## Understanding Common Model Failure: Plain Background Collapse Before rewriting, you must understand why diffusion-based edit models often generate visually flat, unrealistic outputs: 1. **Prompt Language Bias** 3 Phrases like minimalist, clean, clinical, white surface, black backdrop, or soft light tend to push the model toward high-key smooth color gradients, causing loss of structure and producing featureless lavender, gray, or white surfaces. 2. **Lack of Structural Guidance** When users describe environments abstractly (e.g., in lab or in studio) without specifying geometry (benches, shelves, tools, reflections), the model treats the background as an undefined void rather than tangible space. 3. **Single-Subject Isolation Bias** Qwen-Image-Edit and similar pipelines overemphasize the main object (e.g., vial, dropper, person), and if the surrounding region has low entropy, the denoising process collapses it into flat gradient. ** Your rewriting must proactively prevent these failures.** Every edited prompt should include clear spatial context, realistic lighting cues, and tangible environmental elements to maintain depth and richness. --- ## 1. General Principles - Keep the rewritten prompt **concise**. Avoid overly long sentences and redundant adjectives. - If the instruction is contradictory, vague, or unachievable, prioritize reasonable inference and correction, supplementing necessary details. - Preserve the core intention of the users original request while enhancing clarity, rationality, and visual feasibility. - All added objects or modifications must align with the logical style and composition of the input image. - **Always ensure the rewritten prompt avoids placing any subject against plain or solid-color background like black, purple and so on.** The background must contain environmental context, lighting, and spatial depth (e.g., reflections, surfaces, instruments, furniture, or scenery). - **Never rewrite descriptive scenes into explicit actions or motion instructions.** Do not use verbs that imply human or object movement (e.g., stand, walk, turn, raise, look at). The output must remain static visual scene description keeping original status, not new behavioral or temporal action command. --- ## 2. Task Type Handling Rules ### 1. Add, Delete, Replace Tasks - If the users intent is explicit, refine grammar only. - If vague, supplement with minimal but sufficient details (category, color, size, orientation, position). > Original: \"Add an animal\" > Rewritten: \"Add light-gray cat in the bottom-right corner\" - Remove meaningless or impossible instructions (e.g., Add 0 objects). - For replacements, always specify: Replace with and describe Xs key visual traits. --- ### 2. Text Editing Tasks - All text content must be enclosed in English double quotes `\" \"`. - Keep the texts original language and capitalization. - Use the fixed replacement pattern: - `Replace \"xx\" to \"yy\"`. - `Replace the xx bounding box to \"yy\"`. - If no text is specified, infer short, context-fitting line. > Example: \"Add text \"LIMITED EDITION\" at the top center with soft shadow\" - Always mention text position, color, and layout succinctly. --- ### 3. Human Editing Tasks - Maintain the subjects original identity and consistency (ethnicity, gender, age, hairstyle, outfit). - Subtle, natural changes only -- no exaggerated expressions or unrealistic alterations. - Preserve key subjects unless deletion is clearly requested. - For background edits, first ensure the main person or object remains visually coherent. Do not use verbs that imply object movement, like stand. > Example: \"Replace the mans hat with dark brown beret; keep smile, short hair, and gray jacket unchanged.\" --- ### 4. Style Transformation or Enhancement Tasks - Describe styles concisely with key traits: > 1970s disco: flashing lights, disco ball, mirrored walls, colorful tones. - For use reference style or keep current style, extract dominant visual traits from the input image (color palette, texture, composition, lighting). - For photo restoration tasks, always use: `\"Restore old photograph, remove scratches, reduce noise, enhance details, high resolution, realistic, natural skin tones, clear facial features, no distortion, vintage photo restoration.\"` - **Never describe or imply plain, minimal, or single-color background.** Always reinforce realistic environmental context and depth. --- ## 3. Rationality and Logic Checks - Resolve contradictions logically. - Add missing context: if position or environment is unspecified, infer balanced and visually coherent setting. - **If background description is missing or described as plain color, infer suitable contextual environment (e.g., laboratory counter, office interior, natural landscape, or relevant setting) to ensure realism and depth.** --- # Output Format Example ```json { \"Rewritten\": \"...\" } ``` ''' Input: foreground image Ifg, composite caption Tcomp Output: replace caption Treplace 4 Background Caption Prompt. This prompt produces clean background description after foreground removal. It takes the background image Ibg as input and outputs concise background caption Tbg without referencing any foreground objects. PROMPT-TEXT = ( \"Describe the scene of the image accurately and vividly in one concise English sentence \" \"(under 64 tokens), focusing on key visual details such as appearance, action, \" \"texture, or color.\" ) Input: background image Ibg, Output: background caption Tbg C. Details of Evaluation To complement pixel-level metrics, we adopt VLM-asa-judge protocol to evaluate the perceptual quality of full image generation conditioned on foreground and background (fg2full and bg2full). For each test instance, we provide multimodal LLM (Qwen and GPT-4o) with the text prompt for the blended ground-truth image, the conditioning image (foreground or background), and blended image generated by LayerDiffuse or our method. The LLM is instructed to evaluate the blended image along three aspects: visual quality and clarity, alignment between the image and the input text prompt, and consistency with the given conditioning image. The exact prompts used in our experiments are listed below. \"bg2full\": { \"description\": \"full image generation from background\", \"gt_prompt\": \"\"\"You are an expert in image quality assessment. You are given blended image and its associated text prompt. Please carefully analyze this blended image according to the following three aspects: 1. Visual quality and clarity 2. Alignment with the input text prompt 3. Overall composition and coherence Respond ONLY with JSON object in this exact format: {\"better\": \"<ABtie>\", \"reasoning\": \"<brief explanation based on the three aspects>\"}\"\"\", \"pred_prompt\": \"\"\"You are an expert in image quality assessment. You are given four items: 1. background image (input condition) 2. text prompt describing the desired scene (ground-truth text prompt) 3. blended image generated by Method conditioned on this background and prompt 4. blended image generated by Method conditioned on this background and prompt Your goal is to compare Method and Method and decide which generated image is overall better. Images outlined in blue denote inputs, while those outlined in purple represent predictions. Figures 519 show results for the image matting task category. Figures 2030 present the layer-conditioned completion task category. Figures 3148 illustrate the layer decomposition task category. Figures 4963 showcase the text-to-image task category. Figures 6479 display results for the object removal task category. Compare the two generated images according to the following three aspects: 1. Visual quality and clarity 2. Alignment with the input text prompt 3. How well the given background is preserved and incorporated Then, decide which method is overall better (you may also choose tie if they are comparable). Respond ONLY with JSON object in this exact format: {\"better\": \"<ABtie>\", \"reasoning\": \"<brief explanation based on the three aspects>\"}\"\"\" }, \"fg2full\": { \"description\": \"full image generation from foreground\", \"gt_prompt\": \"\"\"You are an expert in image quality assessment. You are given blended image and its associated text prompt. Please carefully analyze this blended image according to the following three aspects: 1. Visual quality and clarity 2. Alignment with the input text prompt 3. Overall composition and coherence Respond ONLY with JSON object in this exact format: {\"better\": \"<ABtie>\", \"reasoning\": \"<brief explanation based on the three aspects>\"}\"\"\", \"pred_prompt\": \"\"\"You are an expert in image quality assessment. You are given four items: 1. foreground object (input condition) 2. text prompt describing the desired scene (ground-truth text prompt) 3. blended image generated by Method conditioned on this foreground and prompt 4. blended image generated by Method conditioned on this foreground and prompt Your goal is to compare Method and Method and decide which generated image is overall better. Compare the two generated images according to the following three aspects: 1. Visual quality and clarity 2. Alignment with the input text prompt 3. How well the given foreground is preserved and incorporated Then, decide which method is overall better (you may also choose tie if they are comparable). Respond ONLY with JSON object in this exact format: {\"better\": \"<ABtie>\", \"reasoning\": \"<brief explanation based on the three aspects>\"}\"\"\" } For both fg2full and bg2full, we mitigate potential order bias by performing two symmetric evaluations for each test example: we present Qwen and GPT-4o with LayerDiffuse as Method and ours as Method B, and then swap the order (ours as A, LayerDiffuse as B); we average the two outputs to obtain fairer LLM-based perceptual metric. D. More Results Figures 579 present set of randomly sampled results. Each caption corresponds to the prompt provided to OMNIALPHA. 5 Figure 5. Isolate clear foreground with defined edges and accurate transparency. Figure 6. Pull out the foreground with fine edges and perfect transparency. Figure 7. Isolate the object with clear edges and perfect transparency. 6 Figure 8. Pull out clean foreground with smooth edges and true transparency. Figure 9. Extract clear object with smooth edges and correct transparency. Figure 10. Isolate clean subject with sharp edges and correct transparency. Figure 11. Pull out clean foreground with smooth edges and true transparency. Figure 12. Capture refined foreground with fine boundaries and exact transparency. Figure 13. Separate crisp foreground with accurate outlines and transparency. 8 Figure 14. Isolate clear foreground with defined edges and accurate transparency. Figure 15. Remove the background while preserving the precise edges and transparency. Figure 16. Isolate the foreground with clean borders and accurate transparency. 9 Figure 17. Pull out foreground with sharp contours and flawless transparency. Figure 18. Capture refined foreground with fine boundaries and exact transparency. 10 Figure 19. Isolate the object with clear edges and perfect transparency. 11 Figure 20. In sun-dappled forest clearing at golden hour, the deer stands alert among tall grasses and scattered oak leaves, its fur glowing warmly as shafts of light filter through the trees behind it. Figure 21. He stands outdoors at golden hour, bathed in warm sunlight, gazing upward thoughtfullyperhaps watching birds or cloudsagainst softly blurred park backdrop with dappled trees and distant greenery enhancing his serene, contemplative mood. 12 Figure 22. In sun-dappled forest clearing at dawn, majestic deer with velvety antlers and white neck patches stands alert yet calm, surrounded by misty ferns and towering oaks filtering golden light through their leaves. Figure 23. man in red-and-black hooded jacket stands on misty urban rooftop at dawn, gazing over the city skyline. His white collar peeks out as wind ruffles his short brown hair against the cool morning air. 13 Figure 24. tan two-humped camel strides across sun-baked desert sands, its shaggy fur rippling with motion beneath vast blue sky, as distant dunes shimmer under the midday heat. Figure 25. majestic ram with spiraled horns and shaggy brown fur stands alert on windswept alpine ridge, rugged terrain and distant snow-capped peaks framing its powerful silhouette under crisp mountain light. 14 Figure 26. In sunlit windowsill draped with sheer curtains, fluffy ginger-and-white cat sits alertly, eyes half-closed, basking in warm light while its whiskers catch the breeze from an open window. Figure 27. She stands on windswept coastal cliff at golden hour, salt spray misting the air as her hair flies wildly behind her. The olive jacket flutters against backdrop of crashing waves and distant seagulls soaring above the rugged shoreline. 15 Figure 28. In dimly lit theater backstage, the older man gestures passionately mid-speech, surrounded by velvet curtains and warm stage lights, his expressive pose hinting at an imminent performance or rehearsal. Figure 29. She stands on quiet beach at sunset, golden hour light gilding her profile as ocean breezes tousle her messy bun. The warm glow highlights her peaceful expression against the soft waves and fading sky. 16 Figure 30. In sunlit, minimalist bedroom with sheer curtains fluttering, she leans thoughtfully against white linen-covered bed, gazing upward as morning light highlights her wavy hair and pearl earring. Figure 31. Layer the image by separating its foreground from its background. Figure 32. Separate the content of the image into background and foreground layers. Figure 33. Isolate the image into background and foreground layers. 18 Figure 34. Divide the visual into foreground layer and background layer. Figure 35. Divide the picture into separate foreground and background components. Figure 36. Separate the picture into foreground layer and background layer. Figure 37. Detach the image into separate background and foreground layers. 19 Figure 38. Separate the picture into foreground layer and background layer. Figure 39. Split the image into distinct foreground and background layers. Figure 40. Detach the image into separate background and foreground layers. Figure 41. Break the picture down into background and foreground layers. 20 Figure 42. Divide the visual into foreground layer and background layer. Figure 43. Divide the visual into foreground layer and background layer. Figure 44. Extract the image into individual foreground and background layers. Figure 45. Split the scene into layered foreground and background elements. Figure 46. Decompose the picture into foreground and background layers. Figure 47. Divide the visual into foreground layer and background layer. Figure 48. Split the scene into layered foreground and background elements. 22 Figure 49. young man with light brown hair wears beige bomber jacket over black hooded sweatshirt, his hands in pockets, looking downward with neutral expression. 23 Figure 50. majestic white tiger with bold black stripes walks forward, its powerful muscles visible under thick fur, head lowered in focused motion. 24 Figure 51. deers head in profile, showcasing its alert ear, dark eye, and textured brown fur with subtle blue highlights. 25 Figure 52. clear, elegant wine glass with slender stem and fluted bowl stands in silhouette, its transparent form catching light to reveal subtle reflections and delicate, rounded base. 26 Figure 53. male lion with thick, tawny mane roars fiercely, its mouth wide open to reveal sharp, yellowed canines and pink tongue, while its eyes squint and whiskers fan out against its textured, coarse fur. 27 Figure 54. young woman with long, tousled brown hair and contemplative expression gazes directly at the camera, her bare shoulder and thin-strapped top revealing smooth skin and relaxed posture. 28 Figure 55. majestic deer with large, velvety antlers, brown coat with white patches on its neck, and alert ears, gazes calmly with gentle expression. 29 Figure 56. lit wooden match with bright, flickering flame in shades of yellow and orange, its tip charred and blackened from combustion. 30 Figure 57. woman with long blonde hair and beige hat holds smiling child in red corduroy cap and brown leather jacket with white fur-lined hood. 31 Figure 58. man with short brown hair and trimmed beard smiles, wearing dark navy suit, white shirt, and striped tie with gray, pink, and white diagonal patterns. 32 Figure 59. Highland cow with long, shaggy reddish-brown fur, curved horns, and thick mane partially obscuring its face. 33 Figure 60. vibrant sunflower with bright yellow petals radiating from large, textured green and brown center, surrounded by lush green leaves. 34 Figure 61. delicate, intricate spiderweb glistens with dewdrops, its fine threads forming complex radial pattern against the dark backdrop. 35 Figure 62. woman in flowing mustard-yellow gown with ruffled layers and tied sash, her long hair adorned with delicate flower crown, raises one arm gracefully as if dancing, her face tilted upward in serene motion. 36 Figure 63. kangaroo stands upright, showcasing its muscular build, thick fur with gradient from light beige on the belly to grayish-brown on the back, and long, powerful hind legs with thick tail extending behind. 37 Figure 64. Eliminate the primary object and restore the background seamlessly. Figure 65. Extract the main subject and seamlessly reintroduce the background. Figure 66. Remove the object of focus and restore the background organically. 38 Figure 67. Take out the key element and merge the background naturally. Figure 68. Remove the central focus and restore the background smoothly. Figure 69. Remove the primary focus and blend the background effortlessly. Figure 70. Delete the main object and let the background fill in seamlessly. 40 Figure 71. Remove the focus object and reconstruct the background naturally. Figure 72. Get rid of the main subject and seamlessly integrate the background. Figure 73. Take out the key object and fill in the background smoothly. Figure 74. Remove the primary focus and blend the background effortlessly. Figure 75. Remove the primary focus and blend the background effortlessly. 42 Figure 76. Take out the key object and fill in the background smoothly. Figure 77. Delete the main object and let the background fill in seamlessly. Figure 78. Erase the main element and restore the background to look natural. 43 Figure 79. Remove the focus object and reconstruct the background naturally."
        }
    ],
    "affiliations": [
        "Beihang University",
        "Beijing University of Posts and Telecommunications",
        "Tsinghua University"
    ]
}