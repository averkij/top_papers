{
    "paper_title": "SMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context Learning",
    "authors": [
        "Melanie Rieff",
        "Maya Varma",
        "Ossian Rabow",
        "Subathra Adithan",
        "Julie Kim",
        "Ken Chang",
        "Hannah Lee",
        "Nidhi Rohatgi",
        "Christian Bluethgen",
        "Mohamed S. Muneer",
        "Jean-Benoit Delbrouck",
        "Michael Moor"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal in-context learning (ICL) remains underexplored despite significant potential for domains such as medicine. Clinicians routinely encounter diverse, specialized tasks requiring adaptation from limited examples, such as drawing insights from a few relevant prior cases or considering a constrained set of differential diagnoses. While multimodal large language models (MLLMs) have shown advances in medical visual question answering (VQA), their ability to learn multimodal tasks from context is largely unknown. We introduce SMMILE, the first expert-driven multimodal ICL benchmark for medical tasks. Eleven medical experts curated problems, each including a multimodal query and multimodal in-context examples as task demonstrations. SMMILE encompasses 111 problems (517 question-image-answer triplets) covering 6 medical specialties and 13 imaging modalities. We further introduce SMMILE++, an augmented variant with 1038 permuted problems. A comprehensive evaluation of 15 MLLMs demonstrates that most models exhibit moderate to poor multimodal ICL ability in medical tasks. In open-ended evaluations, ICL contributes only 8% average improvement over zero-shot on SMMILE and 9.4% on SMMILE++. We observe a susceptibility for irrelevant in-context examples: even a single noisy or irrelevant example can degrade performance by up to 9.5%. Moreover, example ordering exhibits a recency bias, i.e., placing the most relevant example last can lead to substantial performance improvements by up to 71%. Our findings highlight critical limitations and biases in current MLLMs when learning multimodal medical tasks from context."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 5 5 3 1 2 . 6 0 5 2 : r SMMILE: AN EXPERT-DRIVEN BENCHMARK FOR MULTIMODAL MEDICAL IN-CONTEXT LEARNING PREPRINT Melanie Rieff1 Maya Varma2 Ossian Rabow2,3 Subathra Adithan4 Julie Kim2 Ken Chang2 Hannah Lee5 Nidhi Rohatgi2 Christian Bluethgen2,6,7 Jean-Benoit Delbrouck2,8, Michael Moor1, Mohamed S. Muneer2 2Stanford University 1ETH Zurich 3Lund University 4Jawaharlal Institute of Postgraduate Medical Education and Research 5UCSF 6University of Zurich 7University Hospital Zurich 8HOPPR June 27,"
        },
        {
            "title": "ABSTRACT",
            "content": "Multimodal in-context learning (ICL) remains underexplored despite significant potential for domains such as medicine. Clinicians routinely encounter diverse, specialized tasks requiring adaptation from limited examples, such as drawing insights from few relevant prior cases or considering constrained set of differential diagnoses. While multimodal large language models (MLLMs) have shown advances in medical visual question answering (VQA), their ability to learn multimodal tasks from context is largely unknown. We introduce SMMILE, the first expert-driven multimodal ICL benchmark for medical tasks. Eleven medical experts curated problems, each including multimodal query and multimodal in-context examples as task demonstrations. SMMILE encompasses 111 problems (517 question-image-answer triplets) covering 6 medical specialties and 13 imaging modalities. We further introduce SMMILE++, an augmented variant with 1038 permuted problems. comprehensive evaluation of 15 MLLMs demonstrates that most models exhibit moderate to poor multimodal ICL ability in medical tasks. In open-ended evaluations, ICL contributes only 8% average improvement over zero-shot on SMMILE and 9.4% on SMMILE++. We observe susceptibility for irrelevant in-context examples: even single noisy or irrelevant example can degrade performance by up to 9.5%. Moreover, example ordering exhibits recency bias, i.e., placing the most relevant example last can lead to substantial performance improvements by up to 71%. Our findings highlight critical limitations and biases in current MLLMs when learning multimodal medical tasks from context. SMMILE is available at https://smmile-benchmark.github.io."
        },
        {
            "title": "Introduction",
            "content": "In-context learning (ICL) has been widely studied as the striking ability of large language models (LLMs) to learn and generalize to new tasks at inference time when provided with few demonstration examples in their input context, without requiring any parameter updates [5]. Given set of relevant labeled examples in the input prompt, ICL enables models to flexibly adapt to provided contexts, contributing to applications like retrieval-augmented generation [13, 18, 32, 29] and model personalization. Although ICL has been predominantly studied in the context of LLMs, recent works have explored extensions of ICL to multimodal settings [1, 16, 15, 17]. Multimodal ICL holds particular promise for the domain of medicine due to the close parallels between ICL and clinical workflows; in real-world medical settings, clinicians are routinely asked to address specialized tasks given knowledge of limited prior examples, such as few relevant prior cases or constrained set of differential diagnoses. Models capable of performing multimodal ICL in high-stakes medical settings must be carefully assessed for reliability. Although some prior works have proposed strategies for evaluating the ICL capabilities *These authors contributed equally to this work. Co-senior authors. SMMILE PREPRINT Figure 1: Overview of the SMMILE benchmark. In order to test the ability of MLLMs to perform multimodal in-context learning in the medical domain, we curate an expert-annotated dataset consisting of multimodal queries paired with two or more task-specific in-context examples. In contrast to prior few-shot evaluations, our in-context examples are expert-designed demonstrations of the task at hand, rather than randomly retrieved examples. of multimodal LLMs (MLLMs) in the general domain [33, 4, 6, 36], no benchmarks have been previously developed to systematically evaluate multimodal ICL in the medical domain. Additionally, existing few-shot evaluations in medical settings often randomly select examples rather than focusing on specific task demonstrations [25, 31], which may partially explain why minimal improvements over zero-shot evaluations are often observed. In this work, we aim to address these challenges by introducing the Stanford Multimodal Medical In-context Learning (SMMILE) benchmark. Notably, SMMILE is an expert-driven benchmark, developed in collaboration with an international team of 11 medical experts. Our contributions are summarized below: We present SMMILE, the first expert-driven multimodal ICL benchmark for the medical domain. Medical experts contributed problems, each consisting of (1) multimodal query to be posed to MLLM and (2) two or more multimodal in-context examples designed to serve as relevant task demonstrations. In total, SMMILE includes 111 problems encompassing 517 question-image-answer triplets across 6 medical specialties and 13 imaging modalities. We also introduce SMMILE++, an augmented benchmark with 1038 problems designed by permuting the order of in-context examples in SMMILE. Our benchmarks support both open-ended and closed-ended evaluations. We evaluate 15 MLLMs on our benchmarks, including both open-source and closed-source models with diverse architectures and model sizes. Our results show that existing MLLMs struggle to effectively learn from multimodal in-context examples in the medical setting, with ICL contributing to minimal performance boosts over zero-shot evaluations across most evaluated models. In open-ended settings, even the best-performing models (GPT-4o and Qwen2.5-VL-72B) are only capable of answering approximately half of the questions accurately. These results expose significant shortcoming of current MLLMs: although in-context examples are manually designed to serve as effective task demonstrations, MLLMs are unable to accurately learn the task at hand. The manually-curated and high-quality nature of the SMMILE benchmark can help reveal insights into how effective in-context examples can be selected for MLLMs. To this end, we perform an in-depth analysis of two critial factors associated with selecting in-context examples. First, we demonstrate that the quality of in-context examples is SMMILE PREPRINT important: the inclusion of just one irrelevant sample in the in-context example list can impair performance. Second, we demonstrate that the order of in-context examples matters: all evaluated MLLMs suffer from recency bias, where placing the most relevant in-context examples later in the example list can lead to improved performance. By highlighting the limited ICL capabilities of current MLLMs, we hope that our benchmark will be valuable asset for monitoring this critical ability in future MLLMs. Our benchmark can help drive the development of medical MLLMs capable of efficiently learning to perform novel tasks at inference time. SMMILE is available at https://smmile-benchmark.github.io."
        },
        {
            "title": "2 SMMILE: Benchmarking Multimodal Medical In-Context Learning",
            "content": "In this section, we describe our expert-guided process for curating data (Section 2.1) as well as provide quantitative analysis of the final SMMILE benchmark (Section 2.2). 2.1 Dataset Curation In order to collect data, we first recruited clinical experts to contribute multimodal ICL problems. In this setting, each problem consists of (1) query to be posed to MLLM, including question, an associated non-text media item (e.g. an image), and ground-truth answer; and (2) two or more in-context examples, each including question, an associated non-text media item, and an answer. Examples are designed to serve as relevant task demonstrations that support model in learning the task at hand. We provide sample queries and examples from SMMILE in Figure 1. Experts were given access to web interface and instructed to create ten problems. Initial recruiting via direct contacting yielded set of 21 clinical domain experts. Out of this initial set, 11 experts sucessfully complied with the instructions and created problems for the SMMILE dataset. The final set of domain experts includes nine medical doctors and two medical students. The doctors report an average of 6.4 years of clinical experience with specialty expertise in radiology, general medicine, and pathology. For maximal flexibility, we instructed the clinical contributors to create problems by means of writing text and providing URLs to publicly available non-text media. While SMMILE currently focuses solely on non-text media in the form of images to easily benchmark various VLMs, this abstract format will enable us to extend to additional modalities in the future. Figure 2: Web interface for data collection. The problem creation pipeline for SMMILE involves guided, step-by-step workflow. First, the clinical expert is presented with set of detailed instructions, which cover topic scope, data sourcing, and answer formatting (Appendix A.1). Next, the expert is directed to the homepage interface, where they initialize new problem (Appendix A.2). Then, the problem creation tool is loaded (Appendix A.3), which enables the expert to select the relevant medical specialty as well as add, remove, or reorder panels for in-context examples and the final query. Finally, upon clicking Submit, the expert is shown an overview of the completed problem for validation or further editing (Appendix A.4). Our pipeline is designed to ensure consistency, adherence to guidelines, and ease of use through the problem creation process. We then performed manual quality control, which involved the following three steps. First, each problem was manually inspected by two different authors to check for errors, irregularities, or other inaccuracies. Second, each problem was annotated and categorized as shown in Figure 3 (A-F). Third, all text was put through spell check software and manually adjusted when needed. This resulted in 15 grammar and spelling changes to questions and 6 grammar and spelling changes to answers. Additionally, 8 problems had to be modified to make exact match (EM) evaluations possible, including 1 phrasing change, 5 insertions of additional in-context examples, and 2 modifications to queries. 3 SMMILE PREPRINT 2.2 Benchmark Details The SMMILE dataset includes 111 problems, with each problem consisting of single query and an average of 3.65 in-context examples (with spread of 2 to 19 examples per problem). In total, SMMILE encompasses 517 question-image-answer triplets. Figure 3 analyzes the composition of SMMILE with several descriptive statistics. As shown in Graphs A-D, the dataset is primarily composed of diagnostic and classification problems, with about three-quarters requiring free response formats. While many cases are common in clinical practice, over one-third represent uncommon presentations. The majority are rated as difficult for current LLMs. Graphs and demonstrate diversity of image types, covering 6 medical specialties and 13 imaging modalities. Graph summarizes the distribution of in-context examples per problem, and Graph details the distribution of question and answer lengths across the dataset. We leverage the SMMILE dataset to design two benchmark tasks: (1) open-ended generation, where MLLM is presented with query question and image and tasked with generating free-text response, and (2) closed-ended generation, where the MLLM selects an answer from closed set of possible choices obtained from the in-context example set. Additionally, we introduce large-scale, augmented dataset called SMMILE++ by permuting in-context examples from subset of problems in the original dataset. To find permutable problems, we excluded all reasoning problems. An upper limit of 4! = 24 permutations per problem was used, implying that problems with more than 4 in-context examples were shuffled until 24 unique permutations had been created. The final SMMILE++ dataset includes 1038 problems. Descriptive statistics for SMMILE++ are presented in Figure 9. Figure 3: Dataset characteristics. (AD) Distribution of four key categorical annotations across the unique problems: (A) answer format, (B) rarity of the clinical case based on how often clinicians would experience the medical concepts included in each problem, (C) primary cognitive process required (where reasoning classification is defined by final problem not having direct support in its in-context example set), and (D) rated difficulty for state-of-the-art (SOTA) LLMs based on model performance. Hard difficulty is assigned when tested SOTA LLMS are unable to answer the problem, otherwise medium difficulty is assigned to the problem. (EF) Horizontal barplots showing the breakdown of each problem by its main medical specialty (E) and by main image type used (F). (G) Histogram of the number of in-context examples provided per problem. (H) Overlaid histograms of the character-length distributions for questions versus answers. All panels are based on the 111 problems included in SMMILE."
        },
        {
            "title": "3 Experiments",
            "content": "We now utilize the SMMILE benchmark to evaluate the extent to which existing MLLMs can learn relevant medical knowledge when presented with multimodal in-context examples. We describe our experimental setup in Section 3.1, and we provide quantitative analyses of 15 open-source and closed-source MLLMs in Sections 3.2 and 3.3. Our results show that existing MLLMs struggle to effectively learn from multimodal in-context examples in the medical setting, demonstrating that SMMILE is challenging and practically-useful benchmark for future MLLM development. 4 SMMILE PREPRINT 3.1 Experimental setup Models We evaluate total of 15 state-of-the-art MLLMs encompassing range of model sizes (0.5B to 90B parameters for the open-source models), pretraining domains (general-purpose MLLMs and domain-specific medical MLLMs), access types (open-source and closed-source), and model architectures. Open-source models considered in this work include: LLaVA-v1.5 (7B and 13B) [22], LLaVA-NeXT-7B [23], LLaVA-OneVision (0.5B and 7B) [19], LLaVA-Med-7B [20], Llama-3.2-Vision-90B [9], MedVLM-R1 [27], MedGemma 4B [8], and Qwen2.5-VL (3B, 7B, 32B, and 72B) [3]. In particular, LLaVA-Med-7B, MedGemma 4B, and MedVLM-R1 are domain-specific MLLMs designed specifically for medical tasks. Closed-source models considered in this work include GPT-4o [26] and Claude 3.7 Sonnet [2]. For all models, we use standard input prompt consisting of system message, in-context examples, and the query image and question. To ensure fair comparison, we set the maximum generation length to 512 tokens for all models across all open-ended tasks. All experiments with local models were conducted on single-node compute server equipped with 8 NVIDIA H200 (141 GB) GPUs. For more details on computational resources, please refer to Appendix Section C. Baselines In addition to the 15 MLLMs evaluated above, we consider three baselines: (1) Random, where random answer from the in-context example set is selected as the response, (2) Majority, where the most frequent answer from the in-context example set is selected as the response, and (3) Text-Only, where text-only LLM (Llama3.3 70B) [9] is evaluated using only the textual components (questions and answers) from the problems, without any image inputs. For the ICL evaluation, this text-only model receives the questions and answers from the in-context examples, while for the 0-shot evaluation, it receives only the query question. Evaluation Metrics For open-ended evaluations, we evaluate MLLM-generated outputs using two primary metrics. First, the Exact Match (EM) metric counts model generation as correct (score of 100) if it exactly matches the ground-truth answer, and incorrect (score of 0) otherwise. During evaluation, answers are normalized to account for minor variations in formatting, punctuation, and capitalization before comparison. Second, the LLM-as-a-Judge approach provides text-only LLM (Llama3.3 70B) with both the model generation and the ground-truth answer; the model is then prompted to evaluate accuracy. This metric captures semantic correctness beyond exact string matching, making it particularly valuable for medical reasoning tasks where multiple phrasings might convey the same diagnosis or finding. The LLM provides binary judgment (0 for incorrect, 1 for correct) for each generated output, and the final score represents the percentage of outputs judged as correct. For closed-ended (multiple-choice) evaluations, we simply measure the accuracy of selecting the correct option. To estimate variability in our metrics, we employ bootstrap resampling approach with Nbootstrap = 1000 bootstrap iterations. For each iteration, we randomly sample with replacement from the original results to create simulated dataset of the same size as the original dataset, then calculate the accuracy for this bootstrap sample. We report the mean accuracy and standard deviation across all 1000 bootstrap samples. We use fixed random seed for reproducibility. 3.2 Benchmarking MLLMs with SMMILE In Table 1, we report performance metrics resulting from evaluations of 15 state-of-the-art MLLMs on SMMILE. Several trends are observable from these results. ICL shows mixed results with concerning baseline failures despite average improvements. While the average performance improvement across the 15 models in the open-ended LLM-as-a-Judge setting is 8.01% (absolute) and 31.2% (relative), this masks troubling heterogeneity in ICL effectiveness. Notably, 7 out of 15 models perform worse than even Random baseline (randomly selecting an answer from in-context examples, 27.86%): MedVLMR1 (26.74%), LLaVA-OneVision-7B (24.25%), LLaVA-NeXT-7B (23.66%), LLaVA-OneVision-0.5B (21.63%), LLaVA-v1.5-13B (20.91%), LLaVA-v1.5-7B (18.727%), and LLaVA-Med-7B (10.19%). Some models even show performance degradation with ICL, such as LLaVA-Med-7B dropping by more than half from 21.65% to 10.19%. The average improvement is driven primarily by few models showing substantial gains: LLaVA-NeXT-7B with 107.2% (11.42%23.66%), Qwen2.5-VL-32B with 65.4% (25.27%41.79%), and Qwen2.5-VL-72B with 42.4% (29.90%42.59%) relative improvement, respectively. This highly variable performance reveals that ICL benefits remain model-specific and unreliable. GPT-4o is the overall leader. With ICL, GPT-4o delivers the best open-ended score (49.88%) and the best closedended MCQA accuracy (58.85%), demonstrating the most effective multimodal reasoning capabilities across task formats. Domain-specific medical models do not perform significantly better than general-purpose baselines of comparable size (1-10B parameters). Across evaluations against size-matched general-purpose models, medical models 5 SMMILE PREPRINT demonstrate highly variable performance. MedGemma 4B achieves similar zero-shot and ICL LLM-as-a-Judge performance when compared to similarly-sized general-purpose models such as Qwen2.5-VL-3B. However, in some instances, ICL leads to performance drops; in particular, LLaVA-Med-7B exhibits severe degradation with ICL when evaluated via LLM-as-a-Judge, dropping to fraction of its zero-shot performance (21.65% to 10.19%). Such trends are not observed for size-matched general-purpose models like LLaVA-NeXT-7B and LLaVA-v1.5-7B. These findings indicate that domain-specific fine-tuning does not consistently improve the ICL capabilities of models when compared to general-purpose models of similar scale. Qwen2.5-VL-32B achieves the highest performance when evaluated with exact match, yet exact match remains challenging. Qwen2.5-VL-32B achieves the highest EM accuracy (31.84%), followed closely by Llama-3.2-Vision90B (30.53%). This shows that large-scale models are capable of translating ICL into substantially better literal answer matching. However, EM scores trail far behind closed-ended performance and LLM-as-a-Judge performance, underscoring the difficulty of word-for-word answer generation. Closed-ended questions are easier for MLLMs. Five models achieve an accuracy greater than 50% on MCQA (GPT-4o, Claude 3.7 Sonnet, Llama-3.2-Vision-90B, Qwen2.5-VL-32B, Qwen2.5-VL-72B). text-only Llama 3.3 baseline still achieves an accuracy of 38.6%, suggesting that multiple-choice items rely less on precise visual grounding than open-ended generation. Model scale is not the sole determinant of success. Smaller Qwen (3-7B) and LLaVA (0.5-7B) variants lag well behind larger models. However, the Qwen2.5-VL-32B model outperforms its 72B counterpart on EM and approximately matches it on MCQA. Table 1: We benchmark 15 MLLMs on SMMILE, reporting both zero-shot performance as well as performance with in-context examples. The best result is bolded for each task and evaluation metric. Text-only baseline used Llama 3.3 70B. Llava-Med-7B refers to LLaVA-Med-v1.5-Mistral-7B. Model Majority Random Text only Claude 3.7 Sonnet GPT-4o Llama-3.2-Vision-90B LLaVA-v1.5-7B LLaVA-v1.5-13B LLaVA-NeXT-7B LLaVA-OneVision-0.5B LLaVA-OneVision-7B LLaVA-Med-7B MedGemma-4B-Multimodal MedVLM-R1 Qwen2.5-VL-3B Qwen2.5-VL-7B Qwen2.5-VL-32B Qwen2.5-VL-72B LLM-as-a-Judge EM Open-ended Closed-ended MCQA 0-shot ICL 0-shot ICL 0-shot ICL 5.32 2. 37.18 4.39 32.56 4.60 31.84 4.38 14.61 3.57 19.58 3.64 11.42 3.04 18.26 3.93 16.41 3.65 21.65 4.18 27.73 4.70 25.20 4.09 27.62 4.26 17.90 3.20 25.27 3.86 29.90 4.08 26.30 3.88 27.86 4.43 16.53 3.94 36.17 4.44 49.88 4.69 40.66 4.99 18.72 3.40 20.91 3.49 23.66 3.90 21.63 4.00 24.25 3.81 10.19 3.06 36.86 4.81 26.74 4.44 33.58 4.09 29.58 4.63 41.79 4.73 42.59 4.55 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 2.61 1.59 3.44 2.00 0.00 0.00 0.00 0.00 3.65 1.50 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 27.26 4.03 23.16 3.88 5.22 1. 2.63 1.67 8.94 2.54 30.53 4.07 16.37 3.32 19.54 3.95 2.69 1.31 13.46 3.11 22.43 4.33 0.00 0.00 12.14 3.07 15.26 2.91 26.18 4.52 22.45 3.81 31.84 4.37 15.71 3.30 38.62 4.61 56.10 4.31 49.74 4.48 55.04 4.93 40.34 5.35 38.96 4.83 38.11 4.26 44.03 4.47 40.15 4.59 0.00 0.00 41.21 5.15 36.54 4.63 37.58 4.41 38.24 4.75 51.76 4.46 52.33 4.58 24.15 3.70 36.30 4.66 28.03 4.28 42.01 4.83 58.85 4.62 30.30 5.20 22.30 3.92 24.92 4.25 29.01 4.05 32.11 4.44 27.17 4.28 0.00 0.00 40.67 4.93 26.22 4.31 27.35 4.23 45.01 4.39 49.97 5.07 54.71 4.89 In Table 2, we report performance metrics from evaluations of 14 state-of-the-art MLLMs on SMMILE++, the augmented variant of the SMMILE dataset consisting of 1038 problems1. There are notable changes in model rankings. Unlike Table 1 where GPT-4o was dominant, Qwen2.5-VL-72B now takes over the lead (53.8% LLM-as-a-Judge accuracy, 63.2% on ICL-MCQA). Broader benefits from in-context learning are visible. Broader benefits from in-context learning are visible. The largest relative performance improvements are observed when in-context examples are presented to models, such as: LLaVA-v1.5-7B (99.4% relative improvement in LLM-as-a-Judge, 12.31% 24.55%), Qwen2.5-VL-7B (94.4% relative improvement in LLM-as-a-Judge, 21.10% 41.01%), and Qwen2.5-VL-3B (79.9% relative improvement in 1Claude 3.7 Sonnet was excluded from evaluations on SMMILE++ due to limited access and API usage fees. 6 SMMILE PREPRINT LLM-as-a-Judge, 17.53% 31.54%). In the open-ended setting, all models (with the notable exception of LLaVAMed-7B) demonstrate higher ICL performance than zero-shot performance when evaluated with LLM-as-a-Judge, with an average relative improvement of 44.7%. Exact-match is still challenging, but the ceiling rises. The best ICL accuracy with EM evaluation increases from 31.84% (Qwen2.5-VL-32B in Table 1) to 35.34% (Qwen2.5-VL-7B in Table 2). Closed-ended tasks remain easier. Top MCQA performance climbs from 58.9% (GPT-4o in Table 1) to 63.2% (Qwen2.5-VL-72B in Table 2), and four models (GPT-4o, Qwen2.5-VL-7B, Qwen2.5-VL-32B, and Qwen2.5-VL72B) surpass the 50% mark. These results are consistent with the trend that multiple-choice questions are less challenging than open-ended generation. Table 2: We benchmark 14 state-of-the-art MLLMs on SMMILE++, the augmented variant of the SMMILE dataset with 1038 samples. We report both zero-shot performance as well as performance with in-context examples. The best result is bolded for each task and evaluation metric. Text-only baseline used Llama 3.3 70B.Llava-Med-7B refers to LLaVA-Med-v1.5-Mistral-7B. Model Majority Random Text only GPT-4o LLama-3.2-Vision-90B LLaVA-v1.5-7B LLaVA-v1.5-13B LLaVa-NeXT-7B LLaVA-OneVision-0.5B LLaVA-OneVision-7B LLaVA-Med-7B MedGemma-4B-Multimodal MedVLM-R1 Qwen2.5-VL-3B Qwen2.5-VL-7B Qwen2.5-VL-32B Qwen2.5-VL-72B LLM-as-a-Judge EM Open-ended Closed-ended MCQA 0-shot ICL 0-shot ICL 0-shot ICL - - 7.37 0.84 38.41 1.51 25.23 1.38 12.31 1.07 14.23 1.07 16.39 1.14 17.75 1.14 20.41 1.25 24.84 1.31 24.72 1.35 28.82 1.37 17.53 1.22 21.10 1.24 28.92 1.40 34.79 1.44 17.70 1.19 25.35 1.34 14.55 1.10 46.45 1.54 29.56 1.43 24.55 1.35 23.13 1.30 17.57 1.16 20.16 1.22 27.72 1.37 4.62 0.64 38.66 1.54 33.54 1.46 31.54 1.49 41.01 1.56 35.37 1.48 53.80 1.54 - - 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 6.92 0.78 2.91 0.54 0.00 0.00 0.00 0.00 2.91 0.54 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 17.59 1.19 25.35 1.32 3.59 0.58 7.79 0.81 27.51 1.38 20.47 1.22 21.12 1.28 3.53 0.55 14.28 1.09 25.70 1.31 0.19 0.14 13.30 1.06 24.32 1.33 28.09 1.38 35.34 1.45 25.26 1.33 24.44 1.33 - - 41.45 1.52 56.70 1.48 49.27 1.50 48.32 1.55 41.33 1.51 42.26 1.46 35.51 1.43 41.64 1.47 0.29 0.17 40.36 1.55 37.68 1.54 41.72 1.50 54.38 1.55 46.56 1.57 60.62 1.55 16.65 1.13 33.77 1.46 22.41 1. 55.76 1.56 30.04 1.40 23.83 1.34 25.55 1.27 26.15 1.35 27.78 1.41 27.45 1.37 0.00 0.00 44.78 1.52 24.09 1.34 38.10 1.55 49.79 1.60 53.25 1.53 63.22 1.51 3.3 Fine-Grained Analysis We now perform fine-grained breakdown of MLLM performance across the SMMILE benchmark. We specifically focus on five reproducible MLLMs for this analysis: LLaVA-OneVision-0.5B, LLaVA-Med-7B, LLaVA-v1.5-13B, Qwen2.5-VL-32B, and Qwen2.5-VL-72B. We first evaluate MLLM performance stratified by answer format. Each ground-truth answer in the SMMILE dataset was annotated by an expert with one of the following four categorical labels: \"multiple choice\", \"free response\", \"binary (yes/no)\", or \"numerical\". As shown in Figure 4 (Panel A), MLLMs display substantial variations in performance across the four categories, with all five evaluated models demonstrating the strongest performance on binary (yes/no) answers. Notably, we find that all evaluated models fail to correctly answer questions with numerical answers, which is critical limitation since the ability to provide quantitative responses is vital for effective decision-making in medical settings. This finding is corroborated by analysis in Figure 4 (Panel B), which again finds that all evaluated MLLMs struggle when quantitative reasoning is necessary to answer medical question. In Figure 4 (Panel C), we stratify SMMILE by the number of in-context examples included with each question. We report the effect of the number of ICL examples on MLLM performance, and we also include results from zero-shot evaluations (where no examples are provided to the model). We find that for all evaluated models, providing two ICL examples leads to substantial improvements in performance over the zero-shot setting. However, trends become more variable as the number of ICL examples increases. In particular, we observe that increasing the number of ICL examples is not consistently correlated with stronger performance; in particular, all models exhibit substantial performance degradations that often dip below zero-shot performance. These results suggest that existing MLLMs may be unable to perform ICL tasks when provided with lengthy inputs consisting of multiple interleaved image-text pairs. 7 SMMILE PREPRINT Figure 4: We provide fine-grained breakdown of MLLM performance on the SMMILE benchmark. We report performance stratified by answer format (Panel A), cognitive process necessary to obtain the answer (Panel B), number of in-context examples provided to the model (Panel C), and image type (Panel D). Here, we focus on open-ended evaluations, and the y-axis represents prediction accuracy as computed by the LLM-as-a-Judge approach. The acronym MG refers to Mammograms. Finally, in Figure 4 (Panel D), we evaluate MLLM performance stratified by image type. Each query image the SMMILE benchmark points to was annotated by an expert with one of the following categorical labels indicating modality: \"X-Ray\", \"CT\", \"Staining\", \"Photograph\", \"ECG\", \"Ultrasound\", \"Fundus Photograph\", \"Text\", \"MRI\", \"Mammogram\", \"Illustration\", \"EEG\", and \"Other\". We observe that MLLMs exhibit highly variable performance across the 13 considered modalities. No MLLM consistently achieves strong performance across all modalities, suggesting that the MLLMs are unable to consistently glean relevant information from provided in-context examples. We particularly note that when query images come from the MRI and Illustration modalities, all evaluated models fail to generate any correct answers. Modalities like \"Text\", \"Mammogram\", \"Fundus Photograph\", and \"EEG\" also prove to be challenging, with at least two MLLMs failing to generate any correct answers. In summary, our results demonstrate that SMMILE is comprehensive and challenging benchmark for evaluating in-context learning abilities of MLLMs in medical settings. We hope that SMMILE can serve as valuable resource for driving forward the development of future MLLMs."
        },
        {
            "title": "4 Analyzing In-Context Example Construction",
            "content": "The manually-curated and high-quality nature of the SMMILE benchmark can help reveal insights into how effective in-context examples can be selected for MLLMs. In this section, we analyze two critical factors associated with selecting in-context examples: (1) quality of in-context examples (Section 4.1) and (2) the order of examples provided to the MLLM (Section 4.2). 4.1 Analyzing Example Quality The role of in-context example quality on MLLM performance is not well-understood, and the high-quality nature of SMMILE provides unique opportunity for addressing this question. Here, we create two perturbed versions of the SMMILE dataset as follows. (1) SMMILE-Random-Noise: For each sample in SMMILE, we add random image-question-answer triplet from the dataset to the in-context example set. (2) SMMILE-Targeted-Noise: For each 8 SMMILE PREPRINT Table 3: We create two perturbed verions of the SMMILE dataset (SMMILE-Random-Noise and SMMILE-TargetedNoise) in order to evaluate the role of in-context example quality on MLLM performance. Here, we report performance across nine open-source MLLMs (ordered by model size) in the open-ended setting with LLM-as-a-Judge evaluation. The best result per row is bolded. Model SMMILE SMMILE-Random-Noise SMMILE-Targeted-Noise LLaVA-OneVision-0.5B 21.63 4.00 33.58 4.09 Qwen2.5-VL-3B 18.72 3.40 LLaVA-v1.5-7B 24.25 3.81 LLaVA-OneVision-7B 23.66 3.90 LLaVA-NeXT-7B 10.19 3.06 LLaVA-Med-7B 29.58 4.63 Qwen2.5-VL-7B 20.91 3.49 LLaVA-v1.5-13B 41.79 4.73 Qwen2.5-VL-32B Average 24.92 19.41 3.50 30.40 4.65 17.95 3.87 21.90 3.86 17.77 3.26 4.88 2.07 33.11 3.92 18.87 3.80 39.60 4.60 22.65 21.35 3.83 30.37 4.73 14.80 3.31 23.04 3.82 24.38 3.99 1.88 1.32 31.92 4.01 16.14 3.40 39.10 4.48 22. sample in SMMILE, we select an image-question-answer triplet from the dataset that shares the same specialty as the sample. In Table 3, we report performance of 9 MLLMs across these perturbed variants of SMMILE. We observe that the inclusion of just one noisy sample in the in-context example list can impair performance, with most models exhibiting performance degradations on both SMMILE-Random-Noise (9.1% relative decrease from SMMILE on average) and SMMILE-Targeted-Noise (9.5% relative decrease from SMMILE on average). Targeted noise contributes to slightly lower performance than random noise on average, suggesting that even targeted, specialty-based selection of in-context examples can impair performance if the selected examples are not effective demonstrations of the task at hand. Importantly, the effects of noise are model-specific; the presence of noisy in-context examples affects each model in differing ways, leading to substantial changes in the final rankings. Our results demonstrate the critical need for high-quality, manually-curated benchmarks for evaluating in-context abilities of MLLMs in the medical setting, as the presence of noisy or irrelevant samples in the in-context example set can prevent developers from accurately understanding the capabilities of trained models. 4.2 Analyzing Example Order Prior works have suggested that models may be sensitive to the order of in-context examples [35, 10, 30]. Here, we investigate the extent to which (a) the first in-context example and (b) the last in-context example influence MLLM predictions. To this end, we filter the SMMILE dataset to subset of 69 problems where at least one in-context example has an identical answer to the query question; then, we modify the ordering of the in-context example list such that the placement of examples with identical answers can be explicitly controlled. In Figure 5 (left), we compare performance when the first in-context example contains an identical answer to the query question (\"Yes\") with performance when examples with identical answers occur later in the in-context example list (\"No\"). We observe substantial performance degradations (absolute decrease of up to 47%) when the answer to the first in-context example matches the answer to the query question. This trend holds for all nine MLLMs evaluated in this setting, which consist of varied architectures and parameter counts. Importantly, our finding suggests that MLLMs are affected by recency bias, where placing the most relevant in-context examples (i.e. those that share answers with query question) later in the list can lead to improved performance. This finding is further corrobrated by results in Figure 5 (right), where we compare performance when the last in-context example contains an identical answer to the query question (\"Yes\") with performance when examples with identical answers occur earlier in the in-context example list (\"No\"). We observe substantial performance improvements (absolute improvement of up to 71%) when the answer to the last in-context example matches the answer to the query question."
        },
        {
            "title": "5 Related Work",
            "content": "In recent years, Large Language Models (LLMs) and Multimodal LLMs (MLLMs) have demonstrated advanced capabilities on medical reasoning tasks. In this section, we provide an overview of key prior works on in-context learning, medical MLLMs, and benchmarking efforts. 9 SMMILE PREPRINT Figure 5: We analyze the effect of example order on MLLM performance. Here, we report performance across nine MLLMs (ordered by model size) in the open-ended setting with LLM-as-a-Judge evaluation. In-Context Learning: In-context learning (ICL) was popularized by [5] in their paper introducing GPT-3, demonstrating that LLMs can learn to solve tasks at inference time by merely conditioning on few labeled examples in the input prompt without any gradient-based fine-tuning. This paradigm shift has enabled models to generalize to new tasks at inference time simply from natural language instructions and exemplars alone. The extension of ICL to the visionlanguage domain was pioneered by Flamingo, powerful model trained on interleaved sequences of images and text [1]. Flamingo showcased strong few-shot performance on wide range of visual question answering (VQA) and image captioning tasks by learning entirely from prompts composed of image-text pairs, thereby introducing the first stepping stone towards multimodal ICL. MLLMs in Medicine: Inspired by general-purpose MLLMs like Flamingo [1] and Llava [24], recent works have proposed medical MLLMs capable of handling tasks such as radiology report generation, visual question answering, and medical diagnosis. This includes works like Med-PaLM [28], Med-Flamingo [25], Llava-Med [20], ChexAgent [7], and BiomedGPT [34]. The ability of these models to perform multimodal in-context learning has not been well studied due to lack of available benchmarks. Benchmarking Multimodal ICL: Evaluating the ability of MLLMs to effectively learn from multimodal in-context examples at inference time is challenging. In the general domain, several works have presented approaches for evaluating the ICL capabilities of MLLMs [33, 4, 6]; in particular, [36] recently introduced VL-ICL Bench. The domain of medicine serves as an optimal application domain for multimodal ICL due to the presence of highly-specialized concepts and complex imagery as well as the potential for real-world clinical impact. However, to the best of our knowledge, no benchmarks have been previously developed to evaluate multimodal ICL capabilities in the medical domain. Our benchmark SMMILE is designed to bridge this research gap. Prior works in the medical domain evaluated few-shot visual question answering or radiology report generation with benchmarks such as VQA-RAD [14], PathVQA [11], SLAKE [21], or MIMIC-CXR [12]; typically, few-shot examplars in these settings are chosen in an automated fashion via random selection [25, 31]. In contrast, in-context examples included in SMMILE are carefully curated by experts in order to serve as relevant task demonstrations that support the learning of the task at hand."
        },
        {
            "title": "6 Discussion",
            "content": "Key findings. In this work, we introduced SMMILE, multimodal medical in-context learning benchmark designed in collaboration with team of international clinical experts. Even the best-performing models, such as GPT-4o on SMMILE and Qwen2.5-VL-72B on SMMILE++, are only capable of answering approximately half of the questions accurately. Applying ICL results in substantial performance boosts for only few models. Our results demonstrate substantial gap between current MLLMs and the generalizability required for clinical use. Impact. SMMILE is the first benchmark to (i) evaluate multimodal in-context learning in medicine, (ii) release expert-annotated problems with graded task difficulty for supporting medical ICL, and (iii) supply fine-grained 10 SMMILE PREPRINT analysis toolkit with open datasets, evaluation code, and baselines so that researchers can reproduce our pipeline and measure progress with minimal friction. Limitations and future work. We note several key directions for future work. (1) Scale. Crowdsourcing or synthetic augmentation could help expand coverage across the thirteen data modalities included in SMMILE. (2) Modalities. Time-series signals, volumetric scans, genomics, and structured EHR fields are not currently included. (3) Expert diversity. Current contributors may not capture all specialties or practice settings; future work can look at recruiting larger diversity of expert contributors. (4) Task scope. The benchmark centers on diagnosis; extensions to treatment planning, prognosis, and longitudinal reasoning would help cover other aspects of clinical workflows. Nonetheless, SMMILE exposes concrete weaknesses in todays MLLMs when applied to medical scenarios and supplies the community with rigorous, extensible framework for further research. 11 SMMILE PREPRINT"
        },
        {
            "title": "Acknowledgments",
            "content": "MV is supported by graduate fellowship awards from the Department of Defense (NDSEG), the Knight-Hennessy Scholars program at Stanford University, and the Quad program. JBD was supported in part by the Medical Imaging and Data Resource Center (MIDRC), funded by the National Institute of Biomedical Imaging and Bioengineering (NIBIB) of the National Institutes of Health under contract 75N92020D00021 and through The Advanced Research Projects Agency for Health (ARPA-H). We thank Dr. Tina Chen (Stanford Radiology Resident) for her contribution to the SMMILE dataset curation."
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. [2] Anthropic. Claude 3.7 sonnet system card, 2025. [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025. [4] Folco Bertini Baldassini, Mustafa Shukor, Matthieu Cord, Laure Soulier, and Benjamin Piwowarski. What makes multimodal in-context learning work? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pages 15391550, June 2024. [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [6] Shuo Chen, Zhen Han, Bailan He, Jianzhe Liu, Mark Buckley, Yao Qin, Philip Torr, Volker Tresp, and Jindong Gu. Can multimodal large language models truly perform multimodal in-context learning?, 2024. [7] Zhihong Chen, Maya Varma, Jean-Benoit Delbrouck, Magdalini Paschali, Louis Blankemeier, Dave Van Veen, Jeya Maria Jose Valanarasu, Alaa Youssef, Joseph Paul Cohen, Eduardo Pontes Reis, et al. Chexagent: Towards foundation model for chest x-ray interpretation. arXiv preprint arXiv:2401.12208, 2024. [8] Google. Medgemma, 2025. [9] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, et al. The llama 3 herd of models, 2024. [10] Qi Guo, Leiyu Wang, Yidong Wang, Wei Ye, and Shikun Zhang. What makes good order of examples in in-context learning. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics: ACL 2024, pages 1489214904, Bangkok, Thailand, August 2024. Association for Computational Linguistics. [11] Xuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie. Pathvqa: 30000+ questions for medical visual question answering. arXiv preprint arXiv:2003.10286, 2020. [12] Alistair EW Johnson, Tom Pollard, Seth Berkowitz, Nathaniel Greenbaum, Matthew Lungren, Chihying Deng, Roger Mark, and Steven Horng. Mimic-cxr, de-identified publicly available database of chest radiographs with free-text reports. Scientific data, 6(1):317, 2019. [13] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick SH Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In EMNLP (1), pages 67696781, 2020. [14] Jason Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman. dataset of clinically generated visual questions and answers about radiology images. Scientific data, 5(1):110, 2018. [15] Hugo Laurençon, Andrés Marafioti, Victor Sanh, and Léo Tronchon. Building and better understanding visionlanguage models: insights and future directions. In Workshop on Responsibly Building the Next Generation of Multimodal Foundational Models, 2024. [16] Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander Rush, Douwe Kiela, et al. Obelics: An open web-scale filtered dataset of interleaved image-text documents. Advances in Neural Information Processing Systems, 36:7168371702, 2023. [17] Hugo Laurençon, Léo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models? Advances in Neural Information Processing Systems, 37:8787487907, 2024. SMMILE PREPRINT [18] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledgeintensive nlp tasks. Advances in neural information processing systems, 33:94599474, 2020. [19] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer, 2024. [20] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training large language-and-vision assistant for biomedicine in one day. Advances in Neural Information Processing Systems, 36:2854128564, 2023. [21] Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu. Slake: semantically-labeled knowledgeenhanced dataset for medical visual question answering. In 2021 IEEE 18th international symposium on biomedical imaging (ISBI), pages 16501654. IEEE, 2021. [22] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2024. [23] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. [24] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [25] Michael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga, Yash Dalmia, Jure Leskovec, Cyril Zakka, Eduardo Pontes Reis, and Pranav Rajpurkar. Med-flamingo: multimodal medical few-shot learner. In Machine Learning for Health (ML4H), pages 353367. PMLR, 2023. [26] OpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander adry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, et al. Gpt-4o system card, 2024. [27] Jiazhen Pan, Che Liu, Junde Wu, Fenglin Liu, Jiayuan Zhu, Hongwei Bran Li, Chen Chen, Cheng Ouyang, and Daniel Rueckert. Medvlm-r1: Incentivizing medical reasoning capability of vision-language models (vlms) via reinforcement learning, 2025. [28] Tao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaekermann, Mohamed Amin, Pi-Chuan Chang, Andrew Carroll, Charles Lau, Ryutaro Tanno, Ira Ktena, et al. Towards generalist biomedical ai. Nejm Ai, 1(3):AIoa2300138, 2024. [29] Guangzhi Xiong, Qiao Jin, Zhiyong Lu, and Aidong Zhang. Benchmarking retrieval-augmented generation for medicine. In Findings of the Association for Computational Linguistics ACL 2024, pages 62336251, 2024. [30] Sonnet Xu, Joseph Janizek, Yixing Jiang, and Roxana Daneshjou. Biasicl: In-context learning and demographic biases of vision language models, 2025. [31] Benjamin Yan, Ruochen Liu, David Kuo, Subathra Adithan, Eduardo Reis, Stephen Kwak, Vasantha Venugopal, Chloe OConnell, Agustina Saenz, Pranav Rajpurkar, and Michael Moor. Style-aware radiology report generation with RadGraph and few-shot prompting. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1467614688, Singapore, December 2023. Association for Computational Linguistics. [32] Cyril Zakka, Rohan Shad, Akash Chaurasia, Alex Dalal, Jennifer Kim, Michael Moor, Robyn Fong, Curran Phillips, Kevin Alexander, Euan Ashley, et al. Almanacretrieval-augmented language models for clinical medicine. Nejm ai, 1(2):AIoa2300068, 2024. [33] Yuchen Zeng, Wonjun Kang, Yicong Chen, Hyung Il Koo, and Kangwook Lee. Can mllms perform text-to-image in-context learning?, 2024. [34] Kai Zhang, Rong Zhou, Eashan Adhikarla, Zhiling Yan, Yixin Liu, Jun Yu, Zhengliang Liu, Xun Chen, Brian Davison, Hui Ren, et al. generalist visionlanguage foundation model for diverse biomedical tasks. Nature Medicine, pages 113, 2024. [35] Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. What makes good examples for visual in-context learning? In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 1777317794. Curran Associates, Inc., 2023. [36] Yongshuo Zong, Ondrej Bohdal, and Timothy Hospedales. Vl-icl bench: The devil in the details of multimodal in-context learning. arXiv preprint arXiv:2403.13164, 2024. 13 SMMILE PREPRINT"
        },
        {
            "title": "Contents",
            "content": "A Dataset Curation A.1 Step 1: Instructions for Clinical Experts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Step 2: Homepage Interface A.3 Step 3: Problem Creation . A.4 Step 4: Final Submission . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Descriptive Statistics for SMMILE++ Additional Experimental Details Extended Fine-Grained Analysis for SMMILE Extended Fine-Grained Analysis for SMMILE++ Licensing Considerations"
        },
        {
            "title": "A Dataset Curation",
            "content": "14 14 15 16 17 17 18 18 18 In this section, we provide extended details on our four-step expert-guided data curation procedure. A.1 Step 1: Instructions for Clinical Experts Clinical experts are provided with detailed instructions covering topic scope, data sourcing, and answer formatting, as shown below. We re excited that you are participating in this research project to create medical visual question - answering ( VQA ) benchmark for multimodal AI models ! We focus on challenging tasks for which we provide model with few multimodal context examples , to be followed by final problem ( see below in the visual examples ) which on its own is not easy to solve for existing vision - language models ( . . , those of us with access can check with GPT4 - ) . Topics : The problems can range across all medical specialties , including radiology images , photographs , pathology slices , ophthalmology imaging etc . Try to focus mostly on 2 images ( . . , slice of CT , Chest - ray , Photograph etc .) , but links to further modalities are welcome ( audio , video , sequencing etc .) - as long as they can be referred to via URL . Data : Do NOT upload any media ( . . , images , videos , audio ) . Instead , please provide URL ( link ) to publicly available media resource . Do not display any identifiable patient information . Guidelines : Try to follow consistent answer format within given problem - if the problem allows for it . Most importantly , answers must follow consistent format : \" Epidural hematoma , left .\" , \" Subdural hematoma , right .\" etc . Two in - context examples minimum - 10 maximum . 14 SMMILE PREPRINT A.2 Step 2: Homepage Interface The expert is directed to the homepage interface (Figure 6), where they can initialize new problem. Figure 6: Experts are directed to the homepage interface, visualized here. 15 SMMILE PREPRINT A.3 Step 3: Problem Creation The problem creation tool is then loaded (Figure 7), where the expert can select the relevant medical specialty as well as add, remove, or reorder panels for in-context examples and the final query. Figure 7: The expert first selects the medical associated with the problem. Then, the expert adds or removes panels corresponding to in-context examples. The expert can also reorder panels to sort the in-context examples and final problem. 16 SMMILE PREPRINT A.4 Step 4: Final Submission Upon clicking \"Submit\", the expert is shown an overview of the completed problem for validation or further editing (Figure 8). Figure 8: After the expert clicks \"Submit\", they are presented with an overview of their newly created problem. The expert can then validate the problem or return to the previous screen for additional edits. Descriptive Statistics for SMMILE++ Figure 9 analyzes the composition of SMMILE++ with several descriptive statistics."
        },
        {
            "title": "C Additional Experimental Details",
            "content": "All experiments with local models were conducted on research cluster equipped with 8 NVIDIA H200 (141 GB) GPUs. For the larger models (>30B parameters), we used 2-4 GPUs with model parallelism to accommodate memory requirements. The average inference time per sample varied from 3 seconds for the smaller models (0.5B-7B) to 15 seconds for the largest open-source models (70B-90B). Evaluating the entire SMMILE benchmark (111 problems) took between 10 minutes and two hours for single model configuration, while evaluating the augmented SMMILE++ benchmark (1063 problems) required around 5-10 hours per model. For API-based models (GPT-4o and Claude 3.7 Sonnet), we used their respective APIs with rate limiting considerations, resulting in longer evaluation times. LLM-as-a-Judge evaluations were performed with Llama3.3 70B accessed via the Ollama software package2. The input prompt is provided below. The output is binary value, which we multiply by 100 to achieve final score of either 0 (incorrect) or 100 (correct) for each generated output. medical AI model is provided with an image and asked the question \"{ question }\". The correct answer to this question is : \"{ answer }\". The AI model outputs \"{ response }\" as its response . Is the AI model correct ? Please output your answer as single digit , where 1 indicates that the AI model is correct and 0 indicates that the AI model is incorrect with respect to the correct answer . Do not provide anything other than the digit in your response . 2Ollama can be accessed at https://github.com/ollama/ollama. 17 SMMILE PREPRINT Figure 9: Dataset characteristics. (AD) Distribution of four key categorical annotations across the unique problems: (A) answer format, (B) rarity of the clinical case based on how often clinicians would experience the medical concepts included in each problem, (C) primary cognitive process required (where reasoning classification is defined by final problem not having direct support in its in-context example set), and (D) rated difficulty for state-of-the-art LLMs based on model performance. (EF) Horizontal barplots showing the breakdown of each problem by its main medical specialty (E) and by main image type used (F). (G) Histogram of the number of in-context examples provided per problem. (H) Overlaid histograms of the character-length distributions for questions versus answers. All panels are based on the 1038 problems included in SMMILE++. Extended Fine-Grained Analysis for SMMILE Figure 10 provides an extended fine-grained analysis of MLLM performance on the SMMILE benchmark. Figure 11 analyzes MLLM performance on the SMMILE benchmark stratified by number of in-context examples provided to the model. Extended Fine-Grained Analysis for SMMILE++ Figure 12 provides fine-grained analysis of MLLM performance on the SMMILE++ benchmark. Figure 13 analyzes MLLM performance on the SMMILE++ benchmark stratified by number of in-context examples provided to the model."
        },
        {
            "title": "F Licensing Considerations",
            "content": "This benchmark includes question-answer pairs generated by medical experts, licensed under CC BY 4.0. SMMILE is available at https://smmile-benchmark.github.io. 18 SMMILE PREPRINT Figure 10: We provide fine-grained breakdown of MLLM performance on the SMMILE benchmark. We report performance stratified by answer format (Panel A), rarity (Panel B), cognitive process (Panel C), difficulty (Panel D), medical specialty (Panel E), and image type (Panel F). Here, we focus on open-ended evaluations, and the y-axis represents prediction accuracy as computed by the LLM-as-a-Judge approach. The acronym MG refers to Mammograms. SMMILE PREPRINT Figure 11: We analyze MLLM performance on the SMMILE benchmark stratified by number of in-context examples provided to the model. Here, we focus on open-ended evaluations, and the y-axis represents prediction accuracy as computed by the LLM-as-a-Judge approach. 20 SMMILE PREPRINT Figure 12: We provide fine-grained breakdown of MLLM performance on the SMMILE++ benchmark. We report performance stratified by answer format (Panel A), rarity (Panel B), cognitive process (Panel C), difficulty (Panel D), medical specialty (Panel E), and image type (Panel F). Here, we focus on open-ended evaluations, and the y-axis represents prediction accuracy as computed by the LLM-as-a-Judge approach. The acronym MG refers to Mammograms. 21 SMMILE PREPRINT Figure 13: We analyze MLLM performance on the SMMILE++ benchmark stratified by number of in-context examples provided to the model. Here, we focus on open-ended evaluations, and the y-axis represents prediction accuracy as computed by the LLM-as-a-Judge approach."
        }
    ],
    "affiliations": [
        "ETH Zurich",
        "HOPPR",
        "Jawaharlal Institute of Postgraduate Medical Education and Research",
        "Lund University",
        "Stanford University",
        "UCSF",
        "University Hospital Zurich",
        "University of Zurich"
    ]
}