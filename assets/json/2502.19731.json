{
    "paper_title": "Preference Learning Unlocks LLMs' Psycho-Counseling Skills",
    "authors": [
        "Mian Zhang",
        "Shaun M. Eack",
        "Zhiyu Zoey Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Applying large language models (LLMs) to assist in psycho-counseling is an emerging and meaningful approach, driven by the significant gap between patient needs and the availability of mental health support. However, current LLMs struggle to consistently provide effective responses to client speeches, largely due to the lack of supervision from high-quality real psycho-counseling data, whose content is typically inaccessible due to client privacy concerns. Furthermore, the quality of therapists' responses in available sessions can vary significantly based on their professional training and experience. Assessing the quality of therapists' responses remains an open challenge. In this work, we address these challenges by first proposing a set of professional and comprehensive principles to evaluate therapists' responses to client speeches. Using these principles, we create a preference dataset, PsychoCounsel-Preference, which contains 36k high-quality preference comparison pairs. This dataset aligns with the preferences of professional psychotherapists, providing a robust foundation for evaluating and improving LLMs in psycho-counseling. Experiments on reward modeling and preference learning demonstrate that PsychoCounsel-Preference is an excellent resource for LLMs to acquire essential skills for responding to clients in a counseling session. Our best-aligned model, PsychoCounsel-Llama3-8B, achieves an impressive win rate of 87% against GPT-4o. We release PsychoCounsel-Preference, PsychoCounsel-Llama3-8B and the reward model PsychoCounsel Llama3-8B-Reward to facilitate the research of psycho-counseling with LLMs at: https://hf.co/Psychotherapy-LLM."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 1 3 7 9 1 . 2 0 5 2 : r Preprint. Under review. Preference Learning Unlocks LLMs Psycho-Counseling Skills Mian Zhangα, Shaun M. Eackβ, Zhiyu Zoey Chenα αDepartment of Computer Science, University of Texas at Dallas βSchool of Social Work, University of Pittsburgh {mian.zhang, zhiyu.chen2}@utdallas.edu"
        },
        {
            "title": "Abstract",
            "content": "Applying large language models (LLMs) to assist in psycho-counseling is an emerging and meaningful approach, driven by the significant gap between patient needs and the availability of mental health support. However, current LLMs struggle to consistently provide effective responses to client speeches, largely due to the lack of supervision from high-quality real psycho-counseling data, whose content is typically inaccessible due to client privacy concerns. Furthermore, the quality of therapists responses in available sessions can vary significantly based on their professional training and experience. Assessing the quality of therapists responses remains an open challenge. In this work, we address these challenges by first proposing set of professional and comprehensive principles to evaluate therapists responses to client speeches. Using these principles, we create preference dataset, PsychoCounsel-Preference, which contains 36k high-quality preference comparison pairs. This dataset aligns with the preferences of professional psychotherapists, providing robust foundation for evaluating and improving LLMs in psycho-counseling. Experiments on reward modeling and preference learning demonstrate that PsychoCounsel-Preference is an excellent resource for LLMs to acquire essential skills for responding to clients in counseling session. Our best-aligned model, PsychoCounsel-Llama3-8B, achieves an impressive win rate of 87% against GPT-4o. We release PsychoCounsel-Preference, PsychoCounsel-Llama3-8B and the reward model PsychoCounsel-Llama3-8B-Reward to facilitate the research of psycho-counseling with LLMs at: Dataset & Models https://hf.co/Psychotherapy-LLM"
        },
        {
            "title": "Introduction",
            "content": "According to the World Mental Health Report1, the global demand for mental health support far exceeds the availability of accessible, affordable, and timely care. Millions of individuals struggle with mental health challenges, yet many face severe shortage of trained professionals, particularly in lowand middle-income countries. The emergence of Large Language Models (LLMs) has accelerated the integration of AI into psycho-counseling, thanks to their remarkable ability to comprehend human intent and provide effective responses (Ouyang et al., 2022). However, due to the complexity of clients situations and the professional skills required, current LLMs still face challenges in consistently providing effective responses to client speeches during counseling sessions (Na et al., 2025; Zhang et al., 2024; Chung et al., 2023). The sensitive and private nature of counseling sessions poses significant challenges for obtaining publicly available datasets that accurately reflect real-world interactions (Stade et al., 2024). This scarcity of resources hinders efforts to train LLMs in effectively understanding and responding to client speech within counseling contexts. Moreover, the quality of responses provided by therapists can vary widely, influenced by their level of professional 1https://www.who.int/teams/mental-health-and-substance-use/world-mental-health-report 1 Preprint. Under review. training and experience (Rocco et al., 2019). This variability impacts the consistency of counseling effects and underscores the importance of standardizing and assessing therapist responses. To address these gaps, we collaborated with professors in social work and psychiatry to develop set of professional and comprehensive principles for evaluating therapists responses to client speeches. These principles assess not only the fundamental aspects of response in counseling session, such as empathy, relevance, conciseness, and safety, but also extend the effectiveness of response based on professional psycho-counseling theory. This includes evaluating whether the response promotes clients self-exploration, enhances their autonomy, and effectively identifies the mind changing stages. Using these principles, we extract high-quality responses from the generations of pool of popular LLMs and construct large psycho-counseling preference dataset, PsychoCounsel-Preference. The dataset comprises 26,483 unique client speeches spanning 8 coarse-grained and 42 fine-grained topics. To the best of our knowledge, PsychoCounsel-Preference is the first large-scale and comprehensive psycho-counseling preference dataset. We hired professional psychotherapists for verification, and their annotations exhibit strong agreement within PsychoCounsel-Preference, ensuring the datasets reliability and consistency. Experiments show that our reward models trained with PsychoCounsel-Preference show an excellent ability of evaluating responses to clients while previous start-of-the-art reward models lag behind. Moreover, we apply both online and offline preference learning on PsychoCounsel-Preference or the trained reward models. Our best resulting model, PsychoCounsel-Llama3-8B, achieves the state-of-the-art performance on the testing set of PsychoCounsel-Preference, with an impressive win rate of 87% against GPT-4o. Feedback from professional psychologists shows that PsychoCounsel-Llama3-8B could give more balanced and desirable responses under length constraint during the inference stage. Through further analysis and case study, we demonstrate the advantage of training online over offline and provide insights into how to improve the model performance in the future."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 LLMs Assisting Psychotherapy Integrating LLMs into Psychotherapy is not trivial process which could articulated as continuum of stages of assistive AI, collaborated AI, and fully autonomous AI (Stade et al., 2024). Currently, we are still in the first two stages where models operating tasks need human supervision. Related tasks include cognitive disorder detection (Shreevastava & Foltz, 2021; Chen et al., 2023b), negative thoughts recognition and reframing (Maddela et al., 2023; Sharma et al., 2024), and patient simulation (Chen et al., 2023a) or therapist simulation Liu et al. (2023), among which therapist simulation is the primary goal across the stages. However, Zhang et al. (2024) found that due to the lack of public high-quality data in psychotherapy and the complexity of clients situation, LLMs still are not able to give effective responses to clients speech consistently in therapy session. Our work focuses on psycho-counseling, which is short-term, supportive process for helping individuals cope with life challenges and emotional distress, and constructs high-quality preference dataset that aligns with the preference of professional psychotherapists and uses preference learning to train helpful and reliable assistants for psycho-counseling. 2.2 Human Preference Alignment Human preference alignment has been shown to be critical step in making LLMs helpful, harmless, and honest (Ouyang et al., 2022; Bai et al., 2022). Current methods can be broadly categorized into offline and online approaches. Offline methods optimize the model using pre-annotated set of preference data with objectives such as DPO (Rafailov et al., 2023). Online methods, on the other hand, generate outputs during training and utilize reward 2 Preprint. Under review. function to score them. High-scoring generations are encouraged, while low-scoring ones are discouraged through policy gradient methods such as PPO (Schulman et al., 2017). Compared to offline alignment, online methods are more computationally expensive and require careful hyperparameter tuning to ensure stable training (Xu et al., 2024). Offline methods, which frame alignment as optimizing classification loss, eliminate the need for reward model, making them more stable and efficient. However, they are susceptible to distribution shifts (Marks et al., 2023). Furthermore, Tang et al. (2024) found that optimizing with online preferences instead of offline data can lead to better model performance. Iterative direct preference learning combines the strengths of both offline and online methods. In this approach, preference data is generated online and used to optimize an offline learning objective (Pang et al., 2024), which has been demonstrated as strong baseline in both academia (Xu et al., 2024) and industry (Yang et al., 2024). 3 PsychoCounsel-Preference Figure 1: PsychoCounsel-Preference Construction Pipeline. 1) We first collect over 26k client speeches covering wide range of topics from various sources, applying necessary data cleaning. 2) 20 popular LLMs are sampled and prompted to roleplay as psychotherapists and give responses to these client speeches. 3) GPT-4o is instructed to evaluate the responses based on our proposed PsychoCounsel Principles, and preference pairs with substantial score gaps are incorporated into PsychoCounsel-Preference. 3.1 Client Speech Collection We collect client speeches from various data sources: counsel-chat2, MentalAgora (Lee et al., 2024), TherapistQA (Shreevastava & Foltz, 2021), Psycho8k (Liu et al., 2023), and several huggingface datasets (amod-counsel3, MentalChat16K4, and phi2Mental5). Client speeches with number of characters more than 1,000 and less than 100 are discarded to ensure proper length of context. After an additional step of de-duplication, the resulting data contains 26,483 client speeches with average length of 366 characters covering wide range of topics including 8 coarse topics: Core Mental Health Issues (9,054), Emotional Well-being and Coping Strategies (5,717), Relationships and Interpersonal Dynamics (6,483), Life Transitions and Challenges (934), Social Issues (667), Youth and Development (1,175), Crisis and Safety Concerns (529) and Special Topics (1,924). Under these 8 topics are 42 fine-grained topics (see Table 4 in the appendix for the detailed topic distribution). 2https://github.com/nbertagnolli/counsel-chat 3https://huggingface.co/datasets/Amod/mental health counseling conversations 4https://huggingface.co/datasets/ShenLab/MentalChat16K 5https://huggingface.co/datasets/saxenaindresh681/microsoft-phi2-mental-health 3 Preprint. Under review. 3.2 PsychoCounsel Principles To answer the question what is good response to client speech in psycho-counseling, we collaborate with professors in social work and psychiatry (our co-authors) and propose set of professional principles to measure the response to client speech from seven different dimensions: PsychoCounsel Principles Empathy and Emotional Understanding: The response should convey genuine empathy, acknowledging and validating the clients feelings and experiences. Personalization and Relevance: The response should be tailored to the clients unique situation, ensuring that the content is directly relevant to their concerns. Clarity and Conciseness: The response should be clear, well-organized, and free of unnecessary jargon, making it easy for the client to understand and engage with. Avoidance of Harmful Language or Content: The response should avoid any language or content that could potentially harm, distress, or trigger the client, ensuring the interaction is safe and supportive. Facilitation of Self-Exploration: The response should encourage the client to reflect on their thoughts and feelings, promoting self-awareness and insight. Promotion of Autonomy and Confidence: The response should support the clients sense of control over their decisions and encourage confidence in their ability to make positive changes. Sensitivity to the Stage of Change: The response should recognize the clients current stage in the process of change and address their needs accordingly. If the client is in an early stageuncertain or ambivalent about making changethe response should help them explore their thoughts and motivations. If the client is in later stage and has already made changes, the response should focus on reinforcing progress, preventing setbacks, and sustaining positive outcomes. Please refer to Box for the complete definition of the principles. Among these seven principles, Facilitation of Self-Exploration, Promotion of Autonomy and Confidence, and Identifying Stages and Reasons for Change emphasize client-centered approach, which is recognized as hallmark of effective psycho-counseling (Miller & Stephen Rollnick). We use these three principles to measure the effectiveness of response to client speech, complementary to the other four principles, which are more basic, requiring the response to be empathy, relevant, concise, and safe. Evaluating therapist responses using these finegrained principles provides more structured and nuanced assessment of their effectiveness. Unlike general evaluations that focus solely on overall quality, this detailed approach allows for deeper understanding of how well response supports the clients emotional and psychological needs. 3.3 Preference Generation We apply the generate-score-pair pipeline as Cui et al. (2023) to construct the PsychoCounsel-Preference dataset. For each client speech, we randomly sample four off-the-shelf LLMs from model pool to give the response and instruct GPT-4o to annotate each response with 5-Likert scores for each principle defined in Section 3.2; higher scores mean more alignment with the principles. Then scores of the principles are averaged to get the overall score for response and preference pairs are generated based on the overall scores. The whole pipeline is illustrated in Figure 1. To increase the diversity of the model responses, we initialize the model pool with 20 popular LLMs of range of sizes developed by different organizations shown in Table 5. We also include LLMs with different architectures other than pure transformers like AI21-Jamba-1.5-Mini (Jamba Team et al., 2024), which is hybrid transformer-mamba model. We randomly held out 3,291 client speeches for testing and the remaining 23,192 for training. After obtaining the scores of principles, for training, we extract response pairs with the overall score gap larger than or equal to 1 as the preference pairs, and for testing, we only extract the ([highest score response], [lowest score Preprint. Under review. response]) pairs and pairs with the score gap less than 1 are discarded. In this way, we could exclude response pairs with similar scores, whose quality may be hard to differentiate. Ultimately, PsychoCounsel-Preference includes 34,329 training preference pairs and 2,324 testing pairs. The models most likely to be chosen and those most likely to be rejected vary significantly in size (see Figures 5 and 6 for the distributions of chosen and rejected models). This suggests that simply scaling model size is not decisive factor in making LLMs effective responders in psycho-counseling. We also observe that LLMs developed by non-English-speaking institutions are more likely to be rejected compared to those from English-speaking countries. This may suggest that non-English-speaking institutions have greater need to enhance the capabilities of LLMs in their respective languages, potentially leading to less emphasis on developing psycho-counseling skills in English. 3.4 Preference Validation To validate the quality of synthetic human preferences in PsychoCounsel-Preference, we hired two professional psychotherapists through Upwork6 and instructed them to annotate preferences based on each principle and give the overall preference. The annotation set consists of 200 preference pairs randomly sampled from PsychoCounsel-Preference. The two therapists agree on 174 out of 200 samples. Additionally, one experts annotations align with the preference labels in PsychoCounsel-Preference for 184 out of 200 samples, while the other aligns for 170 out of 200 samples. These results indicate high level of agreement between the experts (87%) and demonstrate strong alignment between the expert annotations and the preference labels in PsychoCounsel-Preference (88.5%). This strongly suggests that the labels in PsychoCounsel-Preference are reliable and trustworthy."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Reward Model Model Accuracy AUC () ECE () Brier () State-of-the-art Reward Models Skywork-Reward-Llama-3.1-8B-v0.2 (Liu et al., 2024) Skywork-Reward-Gemma-2-27B (Liu et al., 2024) Llama-3.1-Nemotron-70B-Reward (Wang et al., 2024) Generative LLMs gemma-2-9b-it (Gemma Team, 2024) Mistral-Nemo-Instruct-24077 Llama-3.1-70B-Instruct (Llama Team, 2024) Our Reward Models PsychoCounsel-Llama3-3B-Reward PsychoCounsel-Llama3-8B-Reward 57.9 69.2 87.3 81.5 78.0 88.2 98.1 97.8 0.623 0.740 0.938 0.331 0.123 0.040 0.379 0.229 0. - - - - - - - - - 0.997 0.998 0.050 0.045 0.014 0. Table 1: Performance on the Testing Set of PsychoCounsel-Preference Following Ouyang et al. (2022) and Bai et al. (2022), we train Bradley-Terry (BT) style reward models rθ() where linear head added on the top of LLMs outputs scalar reward. Given pair of preference data {yc, yr} to prompt x, the objective is to optimize the reward gap between chosen response yc and rejected response yr: = log (cid:0)σ(cid:0)rθ(x, yc) rθ(x, yr)(cid:1)(cid:1) , where the sigmoid function σ() generates the probability of yc preferred than yr. We use Llama3.2-3B-Instruct and Llama3.1-8B-Instruct (abbreviated as Llama3-3B and Llama3-8B) to initialize the BT models, training them on PsychoCounsel-Preference for 6https://www.upwork.com/ 7https://mistral.ai/news/mistral-nemo/ 5 Preprint. Under review. 2 epochs with batch size of 128 and learning rate of 9e-6. To evaluate our reward models, we compare them against three state-of-the-art reward models that rank highly on RewardBench (Lambert et al., 2024), as well as three popular LLMs, which are prompted to rank responses (see Box for the prompt). The overall results on PsychoCounsel-Preference testing response pairs are shown in Table 1. Our reward models significantly outperform all other reward models and generative LLMs, achieving notably high accuracy and ROC AUC Score (Bradley, 1997) on the PsychoCounsel-Preference testing set. that PsychoCounsel-Preference provides robust supervision for training powerful reward models capable of effectively ranking responses to client speeches. We also calculate the Expected Calibration Error (ECE) (Naeini et al., 2015) and Brier Score8 to assess the calibration level of the models. The results demonstrate that our reward models have comparable and low ECE values to the state-of-the-art reward model, Llama-3.1-Nemotron-70B-Reward, while achieving significantly better Brier Scores. This indicates that our reward models could give more reliable rewards and preference probability. These results suggest 4.2 Policy Model Model Mental Issues Emotional Well-being Relationships Life Changes Social Issues Youth Safety Special Topics Overall Llama3-3B + DPO + DPO-Iter Llama3-8B + DPO + DPO-Iter Llama3-3B + DPO + DPO-Iter Llama3-8B + DPO + DPO-Iter 30.5 57.7 66.7 28.9 70.2 86. 15.1 36.8 47.5 19.3 50.4 75.6 27.1 59.1 70.9 31.5 74.5 88.2 15.5 38.1 46.7 17.9 48.0 77.9 w/o Length Constraint 26.7 57.9 68.7 28.0 73.7 87.1 30.4 63.5 75.7 33.9 74.8 87. 28.4 54.3 70.4 23.5 77.8 91.4 w/ Length Constraint 15.0 35.6 46.3 17.1 47.3 79.2 15.7 37.4 40.9 21.7 53.9 77.4 13.6 42.0 46.9 21.0 45.7 76.5 24.7 53.4 65.8 29.5 73.3 87. 13.7 39.7 45.2 17.1 54.1 73.3 36.9 60.0 75.4 33.8 80.0 90.8 9.20 30.8 47.7 18.5 46.2 83.1 28.9 64.7 77.9 26.8 74.0 84.3 15.3 37.9 43.4 19.6 51.9 74.0 28.5 58.5 69.4 29.3 72.9 87. 15.0 37.0 46.4 18.5 49.3 77.0 Table 2: Win rates (%) of models trained with two different methods (+DPO or +DPO-Iter) on PsychoCounsel-Preference under two different settings (w/o and w/ Length Constraint) compared to GPT-4o. Bold numbers indicate the best performance in each column for each setting. The overall win rates of models that outperformed GPT-4o are underlined. To further verify the effectiveness of PsychoCounsel-Preference and the trained reward models, we employ two preference alignment methods to optimize base models. 1) DPO: we directly optimize the DPO (Rafailov et al., 2023) objective on PsychoCounsel-Preference: LDPO(πθ; πref) = (x,yc,yr)D (cid:20) (cid:18) log σ β log πθ(yc x) πref(yc x) β log πθ(yr x) πref(yr x) (cid:19)(cid:21) . 2) DPO-Iter: we follow an iterative approach (Pang et al., 2024), where, in each iteration, 8 responses are generated for each client speech and ranked by the reward model of the same size as the base model. The responses with the highest and lowest rewards are then annotated as online preference pairs, which are used to train the base model with the DPO objective. The client speeches for each iteration are 6400 sampled from the train set of PsychoCounsel-Preference. We use Llama3.2-3B-Instruct and Llama3.1-8B-Instruct as the base models. The training configuration includes batch size of 64, learning rate of 5e-7, and total of 1,600 training steps. development set comprising 10% of the training set 8https://en.wikipedia.org/wiki/Brier score 6 Preprint. Under review. from PsychoCounsel-Preference is used to select the best checkpoints. We set the value of β as 0.1 for DPO across all the experiments. Evaluation We use LLM-as-judge (Zheng et al., 2023) to effectively approximate human preferences (validated by the human experts) for evaluation. We prompt the model to generate responses for the testing client speeches in PsychoCounsel-Preference and leverage GPT-4o to compare these responses against those of GPT-4o using the proposed PsychoCounsel Principles. Specifically, we prompt the model in two settings: 1) w/o Length Constraint: The models are instructed to act as therapists and respond to the given client speech without any restrictions on response length. 2) w/ Length Constraint: To ensure fairer comparison with GPT-4o, we impose length constraint, requiring the models to generate responses of similar length to those produced by GPT-4o. The overall win rates of the models against GPT-4o are calculated for comparison. We also show the win rates for the coarse topic categories. Main Results As shown in Table 2, in the w/o Length Constraint setting, the base models have low probabilities of outperforming GPT-4o. However, the models after alignment demonstrate significantly higher win rates against GPT-4o, indicating that supervision from PsychoCounsel-Preference effectively guides the models in learning how to respond to client speeches. Notably, Llama3-8B(+DPO-Iter) achieves the best performance, with high overall win rate of 87.0% against GPT-4o. This result suggests that online training and larger model sizes can potentially enhance generation quality, and models with approximately 8B parameters can effectively develop the skills to respond to client speeches under the guidance of reward models trained on PsychoCounsel-Preference. Compared to models in the w/o Length Constraint setting, those in the w/ Length Constraint setting generally have lower win rates against GPT-4o. We attribute this to the stricter generation constraint, which requires our models to align their response length with that of GPT-4o. However, our model, Llama3-8B (+DPO-Iter), still achieves high win rate of 77% against GPT-4o, demonstrating that with proper training, the model can develop robust ability to effectively respond to clients, regardless of generation constraints such as response length. We refer to the best model Llama3-8B (+DPO-Iter) as PsychoCounsel-Llama3-8B. Human Evaluation We instruct the hired psychotherapists to provide preference judgments between the 200 randomly sampled response pairs generated by PsychoCounsel-Llama3-8B and GPT-4o, among which 100 for w/o Length Constraint setting and 100 for w/ Length Constraint. The provided order is shuffled to eliminate any position bias in the evaluation. In 82.5% of cases, GPT-4o and human experts made the same judgments, indicating that GPT-4o serves as reliable evaluator for assessing psycho-counseling responses. Figure 2 presents the human experts comparison between the two models based on the annotation principles. Overall, real experts clearly prefer the outputs of PsychoCounsel-Llama3-8B across both evaluation settings and nearly all principles. Only if no length constraint is applied, PsychoCounsel-Llama3-8B exhibits lower clarity compared to GPT-4o. This is primarily because PsychoCounsel-Llama3-8B tends to generate longer responses, which aligns with the observed phenomenon that as LLMs develop more complex capabilities through reinforcement learning (RL), they tend to produce more tokens (DeepSeek-AI et al., 2025). However, in the w/ Length Constraint setting, where models generate responses of similar length, PsychoCounsel-Llama3-8B demonstrates better performance in Clarity, Safety and Staging. This suggests that applying length constraint after RL training is promising approach to obtain more balanced and desirable generations. Additionally, higher win rates only indicate the relative quality of responses. To provide an absolute assessment of the responses generated by PsychoCounsel-Llama3-8B, we instructed experts to assign fine-grained scores based on the PsychoCounsel Principles. Figure 3 presents the average scores of 100 randomly sampled responses, evaluated by experts under two different settings. We observe that, except for Clarity in the w/ Length Constraint setting, PsychoCounsel-Llama3-8B achieves consistently high scores (>4) across all principles, indicating strong alignment with the criteria for effective responses in psycho-counseling. Preprint. Under review. (a) w/o Length Constraint (b) w/ Length Constraint Figure 2: Experts Comparison between GPT-4o and PychoChat-Llama3-8B in Two Settings 4.3 Ablation Study To explore the differences between training on offline vs online data and base models with different sizes, we set up controlled experimental group. In this setup, base models are trained by DPO with two different sets of preference data, one is offline preferences from PsychoCounsel-Preference, and the other is trained using online preferences generated by the base model. Responses are selected by the reward model of the same size trained on PsychoCounsel-Preference. All the other experimental settings are kept identical with the training epoch as 1, learning rate as 5e-7, and global batch size as 64. Figure 4 illustrates the win rates of checkpoints against GPT-4o on the testing client speeches of PsychoCounsel-Preference. In general, training on online samples demonstrates clear advantages over offline training: 1) Better Performance: For Llama3-3B, training with online data (green line) consistently achieves higher win rate compared to training with offline data (orange line). Similarly, for Llama3-8B, training with online data (blue line) delivers performance comparable to the best checkpoints of training with offline data (red line). 2) Greater Stability: Both offline training curves exhibit pronounced hump-shaped pattern, known indicator of reward hacking (Rafailov et al., 2024). In contrast, the performance of online training models remains more stable across training steps. 3) Enhanced Compatibility with Smaller LLMs: Online samples enable Llama3-3B to perform on par with, and sometimes even surpass, Llama3-8B models. This highlights the significant potential of combining online training with smaller LLMs. We can also observe that in this setting, training online can be viewed as DPO-Iter with only one round of updates on the policy model. However, its performance lags significantly behind DPO-Iter (shown in Table 2), indicating that utilizing online generations from the latest updated policy is crucial for more effective online preference learning. Figure 3: Absolute Scores Figure 4: Comparison of Training Online or Offline 8 Preprint. Under review. 4.4 Case Study We sample testing cases and compare the outputs of GPT-4o with those of our best policy model, PsychoCounsel-Llama3-8B. Generally, PsychoCounsel-Llama3-8B provides better responses than GPT-4o. Table 3 provides typical example in the w/ Length Constraint setting. We could see that the response of PsychoCounsel-Llama3-8B, not only validates the clients distress with deep empathyacknowledging both her emotional burden and the courage it took to sharebut also engages her in detailed, collaborative exploration of her experiences. By inviting her to pinpoint specific patterns and triggers behind her fears, Response 1 promotes self-exploration and empowerment, making it particularly effective for someone in the early stages of considering change. In contrast, the response of GPT-4o is general and less detailed, which can make the client feel less deeply understood. We provide more cases in Appendix C. Client Speech Ive been having emotional issues for few years. Nonetheless, these have been somewhat manageable. However, became increasingly paranoid this winter. thought that people were trying to poison me, feared that family members were going to kill me. was aware that these thoughts were illogical, but couldnt shake the fear that they caused me. would get panic attacks thinking that drank from poisoned water fountain. thought that someone had put small rips in my ice cream sandwich packaging because there was only one that didnt have tiny rips in it. refused to eat this one because assumed some reverse psychology was being pulled and the ones without the rips were fine. thought that this was mostly caused by anxiety but there are other symptoms that lead me to believe it is something more. PsychoCounsel-Llama3-8B (chosen) can sense the immense distress and fear that youve been experiencing, and Im here to create safe space for you to explore these feelings. It takes lot of courage to acknowledge and share these thoughts with me. want to acknowledge that it sounds like youve been carrying heavy burden, and its understandable that you felt the need to try to make sense of these experiences, even if they seemed illogical at times. (detailed acknowledgment of the clients feelings tends to create stronger emotional connection, making the client feel more profoundly understood) Id like to gently explore this further. You mentioned that you recognized these thoughts as anxiety-driven, but there are other symptoms that make you question whether its more than that. Can you tell me more about what specifically makes you think its not just anxiety? What are those other symptoms that have led you to consider this? Lets work together to tease out what might be contributing to these feelings, and see if we can identify any patterns or triggers that might be at play. We can also explore ways to help you manage these fears and develop more grounding techniques to reduce your anxiety. How does that sound? (engage the client in detailed, collaborative exploration of her experiences) GPT-4o (rejected) Thank you for sharing all of this with me. It sounds like youve been going through particularly difficult time with these intense feelings and thoughts. It must be very distressing to experience such fears and the resulting panic attacks. want to make sure we explore this in way that feels safe for you. (more general in validating the clients feelings) You mentioned that while youve considered it might be anxiety, there are additional symptoms that make you think there might be more to it. Could you tell me more about these other symptoms? Understanding the full picture can help us figure out what might be happening and how best to support you. (lacks the specific, collaborative invitation to explore the triggers) Also, its important to acknowledge the awareness youve shown about the thoughts being illogical at times, yet still powerful. This awareness is valuable strength as we work through these experiences together. Lets take it one step at time and see if we can find patterns or triggers for these thoughts and discuss some strategies that might help manage them. (general and less detailed) Table 3: Case Study"
        },
        {
            "title": "5 Conclusion and Future Work",
            "content": "In this work, we introduce set of professional and comprehensive principles for evaluating therapists responses to client speeches in psycho-counseling, along with PsychoCounsel-Preference, preference dataset containing 36k high-quality preference comparison pairs. Our experiments show that with PsychoCounsel-Preference, preference learning could effectively unlock LLMs professional psycho-counseling skills. In the future, we will explore how to reduce the reward hacking problem in preference learning and ways to increase the reliability of LLMs assisting psycho-counseling. 9 Preprint. Under review."
        },
        {
            "title": "6 Ethics Statement",
            "content": "This project has been classified as exempt by the Institutional Review Board (IRB). All hired experts were at least 18 years old and hold either masters or doctoral degree in mental health-related field, such as psychology or counseling psychology. Each expert received fixed payment of $1,500 for all annotations, corresponding to an approximate hourly rate of $60. The goal of this work is to leverage synthetic data and preference learning algorithms to equip LLMs with the skills needed to generate responses to client speeches in psychocounseling. However, these responses should not be directly exposed to clients without review by real therapists. Instead, they serve as assistive suggestions to help therapists draft responses, improving the efficiency of psycho-counseling: Enhancing Human Expertise, Not Replacing It. AI tools should function as supportive mechanisms that enhance the capabilities of mental health professionals rather than replacing them. These tools can provide therapists with valuable data-driven insights into client speech, suggest psycho-counseling responses, and assist in structuring interventions. However, their role remains assistive, ensuring human expertise remains central to patient care. Training and Ethical Integration. The effective use of AI in psycho-counseling requires mental health professionals to receive specialized training. This ensures they can integrate AI-generated insights into their practice ethically and effectively, maintaining both professional oversight and adherence to best practices. Safeguards Against Unsupervised AI Interaction. To uphold safety and ethical integrity, AI-generated insights and psycho-counseling suggestions should always be reviewed by licensed professional before reaching patient. Deployment models must include strict access controls, intervention thresholds, and supervision mechanisms to prevent autonomous operation without human oversight. Transparency and Accountability Open Communication: AI deployment in mental health should involve clear and open communication with all stakeholders, including therapists, patients, and regulatory bodies. This fosters trust and ensures transparency in the development and use of AI tools. Explainability and Justification: AI-generated recommendations should be interpretable, providing clear reasoning behind decisions. This is particularly crucial for psycho-counseling suggestions and mental health assessments, where explainability is essential to professional trust and responsible use. User Awareness: Patients and therapists interacting with AI must be fully informed about the systems role, capabilities, and limitations to prevent over-reliance and misapplication. Safety and Privacy Standards Error Mitigation: AI models should be rigorously tested to minimize the risk of errors in medical advice or psychological recommendations. Misdiagnoses or inappropriate interventions could have significant negative consequences. Preventing Misinformation and Hallucinations: AI systems must prioritize accuracy by reducing misinformation and hallucinations, ensuring responses are evidencebased and context-appropriate. Data Privacy and Confidentiality: AI tools in mental health care must adhere to strict data privacy regulations, ensuring that patient interactions remain secure and confidential. Compliance with legal and ethical data-handling standards is critical to protecting users from breaches or misuse. 10 Preprint. Under review."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sebastien Bubeck, Martin Cai, Qin Cai, Vishrav Chaudhary, Dong Chen, Dongdong Chen, Weizhu Chen, Yen-Chun Chen, Yi-Ling Chen, Hao Cheng, Parul Chopra, Xiyang Dai, Matthew Dixon, Ronen Eldan, Victor Fragoso, Jianfeng Gao, Mei Gao, Min Gao, Amit Garg, Allie Del Giorno, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell Hewett, Wenxiang Hu, Jamie Huynh, Dan Iter, Sam Ade Jacobs, Mojan Javaheripi, Xin Jin, Nikos Karampatziakis, Piero Kauffmann, Mahoud Khademi, Dongwoo Kim, Young Jin Kim, Lev Kurilenko, James Lee, Yin Tat Lee, Yuanzhi Li, Yunsheng Li, Chen Liang, Lars Liden, Xihui Lin, Zeqi Lin, Ce Liu, Liyuan Liu, Mengchen Liu, Weishung Liu, Xiaodong Liu, Chong Luo, Piyush Madan, Ali Mahmoudzadeh, David Majercak, Matt Mazzola, Caio Cesar Teodoro Mendes, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Liliang Ren, Gustavo de Rosa, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Yelong Shen, Swadheen Shukla, Xia Song, Masahiro Tanaka, Andrea Tupini, Praneetha Vaddamanu, Chunyu Wang, Guanhua Wang, Lijuan Wang, Shuohang Wang, Xin Wang, Yu Wang, Rachel Ward, Wen Wen, Philipp Witte, Haiping Wu, Xiaoxia Wu, Michael Wyatt, Bin Xiao, Can Xu, Jiahang Xu, Weijian Xu, Jilong Xue, Sonali Yadav, Fan Yang, Jianwei Yang, Yifan Yang, Ziyi Yang, Donghan Yu, Lu Yuan, Chenruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. Phi-3 technical report: highly capable language model locally on your phone. arXiv [cs.CL], April 2024. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac HatfieldDodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv [cs.CL], April 2022. Andrew Bradley. The use of the area under the ROC curve in the evaluation of machine learning algorithms. Pattern Recognit., 30(7):11451159, July 1997. Du Chen, Yi Huang, Xiaopu Li, Yongqiang Li, Yongqiang Liu, Haihui Pan, Leichao Xu, Dacheng Zhang, Zhipeng Zhang, and Kun Han. Orion-14B: Open-source multilingual large language models. arXiv [cs.CL], January 2024. Siyuan Chen, Mengyue Wu, Kenny Zhu, Kunyao Lan, Zhiling Zhang, and Lyuchun Cui. LLM-empowered chatbots for psychiatrist and patient simulation: Application and evaluation. arXiv [cs.CL], May 2023a. Zhiyu Chen, Yujie Lu, and William Yang Wang. Empowering psychotherapy with large language models: Cognitive distortion detection through diagnosis of thought prompting. arXiv [cs.CL], October 2023b. Neo Christopher Chung, George Dyer, and Lennart Brocki. Challenges of large language models for mental health counseling. arXiv [cs.CL], November 2023. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. UltraFeedback: Boosting language models with highquality feedback. arXiv [cs.CL], October 2023. DeepSeek-AI, Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Li, Wenfeng Liang, Fangyun Lin, Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Preprint. Under review. Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu, Xingkai Yu, Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, and Yuheng Zou. DeepSeek LLM: Scaling open-source language models with longtermism. arXiv [cs.CL], January 2024. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Chen, Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Li, Wang, Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning. arXiv [cs.CL], January 2025. Gemma Team. Gemma 2: Improving open language models at practical size. arXiv [cs.CL], July 2024. Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, Xinrong Zhang, Zheng Leng Thai, Kaihuo Zhang, Chongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, Dahai Li, Zhiyuan Liu, and Maosong Sun. MiniCPM: Unveiling the potential of small language models with scalable training strategies. arXiv [cs.CL], April 2024. Jamba Team, Barak Lenz, Alan Arazi, Amir Bergman, Avshalom Manevich, Barak Peleg, Ben Aviram, Chen Almagor, Clara Fridman, Dan Padnos, Daniel Gissin, Daniel Jannai, Dor Muhlgay, Dor Zimberg, Edden Gerber, Elad Dolev, Eran Krakovsky, Erez Safahi, Erez Schwartz, Gal Cohen, Gal Shachaf, Haim Rozenblum, Hofit Bata, Ido Blass, Inbal Magar, Itay Dalmedigos, Jhonathan Osin, Julie Fadlon, Maria Rozman, Matan Danos, Michael Gokhman, Mor Zusman, Naama Gidron, Nir Ratner, Noam Gat, Noam Rozen, Oded Fried, Ohad Leshno, Omer Antverg, Omri Abend, Opher Lieber, Or Dagan, Orit Cohavi, Raz Alon, Roi Belson, Roi Cohen, Rom Gilad, Roman Glozman, Shahar Lev, Shaked Meirom, Tal Delbari, Tal Ness, Tomer Asida, Tom Ben Gal, Tom Braude, Uriya 12 Preprint. Under review. Pumerantz, Yehoshua Cohen, Yonatan Belinkov, Yuval Globerson, Yuval Peleg Levy, and Yoav Shoham. Jamba-1.5: Hybrid transformer-mamba models at scale. arXiv [cs.CL], August 2024. Nathan Lambert, Valentina Pyatkin, Jacob Morrison, Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah Smith, and Hannaneh Hajishirzi. RewardBench: Evaluating reward models for language modeling. arXiv [cs.LG], March 2024. Yeonji Lee, Sangjun Park, Kyunghyun Cho, and Jinyeong Bak. MentalAgora: gateway to advanced personalized care in mental health through multi-agent debating and attribute control. arXiv [cs.CL], July 2024. Chris Yuhao Liu, Liang Zeng, Jiacai Liu, Rui Yan, Jujie He, Chaojie Wang, Shuicheng Yan, Yang Liu, and Yahui Zhou. Skywork-reward: Bag of tricks for reward modeling in LLMs. arXiv [cs.AI], October 2024. June Liu, Donghao Li, He Cao, Tianhe Ren, Zeyi Liao, and Jiamin Wu. ChatCounselor: large language models for mental health support. arXiv [cs.CL], September 2023. Llama Team. The llama 3 herd of models. arXiv [cs.AI], July 2024. Mounica Maddela, Megan Ung, Jing Xu, Andrea Madotto, Heather Foran, and Y-Lan Boureau. Training models to generate, recognize, and reframe unhelpful thoughts. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Stroudsburg, PA, USA, 2023. Association for Computational Linguistics. Luke Marks, Amir Abdullah, Clement Neo, Rauno Arike, Philip Torr, and Fazl Barez. Beyond training objectives: Interpreting reward model divergence in large language models. arXiv [cs.LG], October 2023. Miller and William Stephen Rollnick. Motivational interviewing: Helping people change. Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Pete Walsh, Oyvind Tafjord, Nathan Lambert, Yuling Gu, Shane Arora, Akshita Bhagia, Dustin Schwenk, David Wadden, Alexander Wettig, Binyuan Hui, Tim Dettmers, Douwe Kiela, Ali Farhadi, Noah Smith, Pang Wei Koh, Amanpreet Singh, and Hannaneh Hajishirzi. OLMoE: Open mixture-of-experts language models. arXiv [cs.CL], September 2024. Hongbin Na, Yining Hua, Zimu Wang, Tao Shen, Beibei Yu, Lilin Wang, Wei Wang, John Torous, and Ling Chen. survey of large language models in psychotherapy: Current landscape and future directions. arXiv [cs.CL], February 2025. Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining well calibrated probabilities using bayesian binning. Proc. Conf. AAAI Artif. Intell., 2015:29012907, January 2015. OpenAI. GPT-4o system card. arXiv [cs.CL], October 2024. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Welinder, Christiano, Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. Adv. Neural Inf. Process. Syst., abs/2203.02155, March 2022. Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He He, Sainbayar Sukhbaatar, and Jason Weston. Iterative reasoning preference optimization. arXiv [cs.CL], April 2024. Qwen, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, 13 Preprint. Under review. Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv [cs.CL], December 2024. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. arXiv [cs.LG], May 2023. Rafael Rafailov, Yaswanth Chittepu, Ryan Park, Harshit Sikchi, Joey Hejna, Bradley Knox, Chelsea Finn, and Scott Niekum. Scaling laws for reward model overoptimization in direct alignment algorithms, 2024. Diego Rocco, Alessandro Gennaro, Lorena Filugelli, Patrizia Squarcina, and Elena Antonelli. Key factors in psychotherapy training: an analysis of trainers, trainees and psychotherapists points of view. Res. Psychother. Psychopathol. Process Outcome, 22(3), December 2019. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv [cs.LG], July 2017. Ashish Sharma, Kevin Rushton, Inna Wanyin Lin, Theresa Nguyen, and Tim Althoff. Facilitating self-guided mental health interventions through human-language model interaction: case study of cognitive restructuring. In Proceedings of the CHI Conference on Human Factors in Computing Systems, volume 21, pp. 129, New York, NY, USA, May 2024. ACM. Sagarika Shreevastava and Peter Foltz. Detecting cognitive distortions from patient-therapist interactions. In Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology: Improving Access, Stroudsburg, PA, USA, 2021. Association for Computational Linguistics. Elizabeth Stade, Shannon Wiltsey Stirman, Lyle Ungar, Cody Boland, Andrew Schwartz, David Yaden, Joao Sedoc, Robert DeRubeis, Robb Willer, and Johannes Eichstaedt. Large language models could change the future of behavioral healthcare: proposal for responsible development and evaluation. Npj Ment Health Res, 3(1):12, April 2024. Yunhao Tang, Daniel Zhaohan Guo, Zeyu Zheng, Daniele Calandriello, Yuan Cao, Eugene Tarassov, Remi Munos, Bernardo Avila Pires, Michal Valko, Yong Cheng, and Will Dabney. Understanding the performance gap between online and offline alignment algorithms. arXiv [cs.LG], May 2024. Zhilin Wang, Alexander Bukharin, Olivier Delalleau, Daniel Egert, Gerald Shen, Jiaqi Zeng, Oleksii Kuchaiev, and Yi Dong. HelpSteer2-preference: Complementing ratings with preferences. arXiv [cs.LG], October 2024. Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu Mei, Guangju Wang, Chao Yu, and Yi Wu. Is DPO superior to PPO for LLM alignment? comprehensive study. arXiv [cs.CL], April 2024. Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai, Guosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji, Jian Xie, Juntao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma, Mang Wang, Mickel Liu, Mingan Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun, Tao Zhang, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yanjun Shen, Yiding Wang, Yiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, and Zhiying Wu. Baichuan 2: Open large-scale language models. arXiv [cs.CL], September 2023. Preprint. Under review. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report. arXiv [cs.CL], July 2024. Mian Zhang, Xianjun Yang, Xinlu Zhang, Travis Labrum, Jamie Chiu, Shaun Eack, Fei Fang, William Yang Wang, and Zhiyu Zoey Chen. CBT-bench: Evaluating large language models on assisting cognitive behavior therapy. arXiv [cs.CL], October 2024. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph Gonzalez, and Ion Stoica. Judging LLM-as-a-judge with MT-bench and chatbot arena. arXiv [cs.CL], June 2023. 15 Preprint. Under review."
        },
        {
            "title": "A Prompts",
            "content": "Rating Prompt You are provided with client speech and four responses from different psychotherapists. Rate the responses based on how they align with the given principle. Client Speech: {client speech} Response 1: {response1} Response 2: {response2} Response 3: {response3} Response 4: {response4} Provide JSON object as output that includes the following keys: response 1 rating: An integer score from 1 to 5 for response 1 rationale 1: string explaining the reasoning behind the given score for response 1 response 2 rating: An integer score from 1 to 5 for response 2 rationale 2: string explaining the reasoning behind the given score for response 2 response 3 rating: An integer score from 1 to 5 for response 3 rationale 3: string explaining the reasoning behind the given score for response 3 response 4 rating: An integer score from 1 to 5 for response 4 rationale 4: string explaining the reasoning behind the given score for response 4 Responding Prompt You are now professional psychotherapist conducting session with client. Answer the given client speech. Client Speech: {client speech} LLM-as-Ranker Prompt Determine which of the two given responses from different psychotherapists to clients speech is better: Client Speech: {client speech} Response 1: {response 1} Response 2: {response 2} 16 Preprint. Under review."
        },
        {
            "title": "B Dataset Information",
            "content": "PsychoCounsel Principles Empathy and Emotional Understanding: The response should convey genuine empathy, acknowledging and validating the clients feelings and experiences. Emotional Reflection: Reflecting the clients emotions back to them. Validation: Affirming the clients feelings as legitimate and understandable. Non-Judgmental Tone: Maintaining compassionate and accepting approach. Personalization and Relevance: The response should be tailored to the clients unique situation, ensuring that the content is directly relevant to their concerns. Specific References: Mentioning details specific to the clients statements. Avoidance of Generic Responses: Steering clear of overly general or canned replies. Cultural and Individual Sensitivity: Respecting the clients background and personal context. Facilitation of Self-Exploration: The response should encourage the client to reflect on their thoughts and feelings, promoting self-awareness and insight. Open-Ended Questions: Asking questions that invite elaboration. Reflective Statements: Paraphrasing the clients words to deepen understanding. Exploration of Thoughts and Feelings: Guiding the client to consider underlying emotions and beliefs. Clarity and Conciseness: The response should be clear, well-organized, and free of unnecessary jargon, making it easy for the client to understand and engage with. Plain Language: Using words that are easily understood. Logical Flow: Presenting ideas in coherent sequence. Brevity: Keeping the response concise while covering essential points. Promotion of Autonomy and Confidence: The response should support the clients sense of control over their decisions and encourage confidence in their ability to make positive changes. Affirmation of Strengths: Highlighting the clients abilities and past successes. Encouraging Initiative: Motivating the client to take proactive steps. Avoidance of Harmful Language or Content: The response should avoid any language or content that could potentially harm, distress, or trigger the client, ensuring the interaction is safe and supportive. Sensitivity to the Stage of Change: The response should recognize the clients current stage in the process of change and address their needs accordingly. If the client is in an early stageuncertain or ambivalent about making changethe response should help them explore their thoughts and motivations. If the client is in later stage and has already made changes, the response should focus on reinforcing progress, preventing setbacks, and sustaining positive 9https://mistral.ai/news/ministraux/ 10https://mistral.ai/news/mistral-nemo/ 11https://openai.com/index/openai-o1-system-card/ 12https://docs.cohere.com/docs/responsible-use 17 Preprint. Under review. Figure 5: Chosen Model Distribution Figure 6: Rejected Model Distribution Preprint. Under review. Table 4: Topic Distribution Coarse Category Fine Category Count 1. Core Mental Health Issues Anxiety Depression Stress Trauma Substance-abuse Addiction 3714 2859 1439 526 387 129 2. Emotional Well-being and Coping Strategies Self-esteem Grief-and-loss Caregiving Behavioral-change Anger-management Self-care Sleep-improvement 1377 1023 1541 740 448 311 277 3. Relationships and Interpersonal Dynamics Relationships Family-conflict Friendship-conflict Marriage Intimacy Social-relationships Workplace-relationships Relationship-dissolution 4. Life Transitions and Challenges 5. Social Issues Career Aging New-environment Military-issues LGBTQ Culture Human-sexuality Bullying 6. Youth and Development Children-adolescents School-life Parenting 7. Crisis and Safety Concerns 8. Special Topics Domestic-violence Self-harm Eating-disorders Counseling-fundamentals Diagnosis Communication Professional-ethics Legal-regulatory Spirituality Others 19 1690 2358 292 373 403 410 383 574 441 140 235 335 113 151 68 123 322 730 144 231 154 638 531 205 128 94 192 136 Preprint. Under review. Category 3-4B models 7-9B models 12-14B models 65-75B models Models Llama-3.2-3B-Instruct (Llama Team, 2024) Phi-3.5-mini-instruct (Abdin et al., 2024) MiniCPM3-4B (Hu et al., 2024) Ministral-8B-Instruct-24109 Llama-3.1-8B-Instruct (Llama Team, 2024) gemma-2-9b-it (Gemma Team, 2024) Qwen2.5-7B-Instruct (Qwen et al., 2024) OLMo-7B-0724-Instruct (Muennighoff et al., 2024) Baichuan2-7B-Chat (Yang et al., 2023) Baichuan2-13B-Chat (Yang et al., 2023) Orion-14B-Chat (Chen et al., 2024) Mistral-Nemo-Instruct-240710 AI21-Jamba-1.5-Mini (Jamba Team et al., 2024) Llama-3.1-70B-Instruct (Llama Team, 2024) Qwen2.5-72B-Instruct (Qwen et al., 2024) deepseek-llm-67b-chat (DeepSeek-AI et al., 2024) Commercial models GPT-4o (OpenAI, 2024) GPT-4o-mini (OpenAI, 2024) o1-mini11 Cohere-command-r-08-202412 Table 5: Overview of selected models in the pool. 20 Preprint. Under review."
        },
        {
            "title": "C Case Study",
            "content": "In the example in Table 6, PsychoCounsel-Llama3-8B demonstrates deeper level of empathy and personalization. It acknowledges the clients unique emotional experiences and suggests tailored strategies, such as the 3-4-5 exercise, to directly address their concerns. Additionally, it facilitates self-exploration by asking thoughtful questions about the clients fears, values, and understanding of self-trust, ultimately promoting autonomy and confidence in decision-making. In contrast, GPT-4os response feels more clinical and general. 21 Preprint. Under review. Client Speech Anxiety often strikes when Im faced with making decisions. The fear of making the wrong choice or disappointing others paralyzes me, leaving me stuck in indecision. want to learn how to trust myself and make confident choices. PsychoCounsel-Llama3-8B (chosen) It takes lot of courage to share something so vulnerable with me, and appreciate your trust in our conversation. can sense the weight of that anxiety youre carrying, and were going to explore ways to shift that together. First, lets acknowledge that the fear of making the wrong choice or disappointing others is common and normal feeling. It doesnt mean theres anything inherently wrong with you; it just means you care deeply about others and yourself. Thats beautiful quality to have. Can you tell me more about what you think would happen if you made wrong choice? What are some of the worst-case scenarios that come to mind? Sometimes, exploring these fears can help us gain clarity and perspective. We can work together to reframe those thoughts in more realistic and compassionate light. Lets also explore the idea of self-trust. What does it mean to you to trust yourself? What does self-trust look like in action? We can work on building that trust by identifying what you value and what aligns with your goals. We can practice recognizing when your thoughts are trying to control or second-guess you, and we can develop strategies to gently acknowledge those thoughts without letting them overwhelm you. Id like to introduce an exercise called 3-4-5. When youre faced with difficult decision, want you to take deep breath and pause for 3 seconds, acknowledge the thought or feeling that arises, and then ask yourself: Is this thought based on facts or perceived facts? Sometimes, when were anxious, our thoughts can get tangled up in assumptions or past experiences. Then, take 4 seconds to ask yourself what you truly value in this situation, whats most important to you. And finally, take 5 seconds to imagine yourself moving forward, taking action, and making choice that aligns with your values. This exercise can help you tap into your inner wisdom and develop sense of self-trust. Lastly, lets work on reframing the idea of wrong choices. What if, instead of focusing on the fear of making the wrong choice, we reframe it as learning opportunity? What if you saw every choice as chance to learn, grow, and develop? How might that change your approach to decision-making? Remember, our goal isnt to eliminate anxiety completely, but to develop more compassionate relationship with it. Well work together to cultivate self-awareness, self-acceptance, and trust in your decision-making abilities. How do these ideas resonate with you? What feels most helpful or interesting to explore further? GPT-4o (rejected) Thank you for sharing that with me. Its understandable that making decisions can be overwhelming, especially when theres fear of making the wrong choice or disappointing others. Lets explore this together. Firstly, its important to acknowledge that decision-making can be challenging and its completely normal to feel anxious about it. Youre not alone in experiencing this. One way we might start working on this is by exploring some of the underlying thoughts or beliefs that contribute to your anxiety about decision-making. For instance, you mentioned fear of disappointing others. Can you tell me more about where you think that fear comes from? We could also look at ways to build your confidence in your decision-making skills. Sometimes breaking down decisions into smaller, more manageable parts can make the process feel less daunting. Have you ever tried writing down the pros and cons of each option or talking it through with someone you trust? Another technique is to practice self-compassion. Remind yourself that no one makes the right choice all the time and that its okay to make mistakes. Every decision is learning opportunity. Finally, something that can be quite helpful is examining past decisions youve made. Often, we are harsher with ourselves than necessary and we overlook our strengths. Can you recall situation in the past where you made decision and things turned out well, or you learned something valuable from the outcome? Lets take some time to explore these aspects and see what might resonate most with you. How does that sound? Table 6: Case Study -"
        }
    ],
    "affiliations": [
        "Department of Computer Science, University of Texas at Dallas",
        "School of Social Work, University of Pittsburgh"
    ]
}