{
    "paper_title": "Re-ttention: Ultra Sparse Visual Generation via Attention Statistical Reshape",
    "authors": [
        "Ruichen Chen",
        "Keith G. Mills",
        "Liyao Jiang",
        "Chao Gao",
        "Di Niu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion Transformers (DiT) have become the de-facto model for generating high-quality visual content like videos and images. A huge bottleneck is the attention mechanism where complexity scales quadratically with resolution and video length. One logical way to lessen this burden is sparse attention, where only a subset of tokens or patches are included in the calculation. However, existing techniques fail to preserve visual quality at extremely high sparsity levels and might even incur non-negligible compute overheads. % To address this concern, we propose Re-ttention, which implements very high sparse attention for visual generation models by leveraging the temporal redundancy of Diffusion Models to overcome the probabilistic normalization shift within the attention mechanism. Specifically, Re-ttention reshapes attention scores based on the prior softmax distribution history in order to preserve the visual quality of the full quadratic attention at very high sparsity levels. % Experimental results on T2V/T2I models such as CogVideoX and the PixArt DiTs demonstrate that Re-ttention requires as few as 3.1\\% of the tokens during inference, outperforming contemporary methods like FastDiTAttn, Sparse VideoGen and MInference. Further, we measure latency to show that our method can attain over 45\\% end-to-end % and over 92\\% self-attention latency reduction on an H100 GPU at negligible overhead cost. Code available online here: \\href{https://github.com/cccrrrccc/Re-ttention}{https://github.com/cccrrrccc/Re-ttention}"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 8 1 9 2 2 . 5 0 5 2 : r Re-ttention: Ultra Sparse Visual Generation via Attention Statistical Reshape Ruichen Chen ECE Department University of Alberta ruichen1@ualberta.ca Keith G. Mills ECE Department University of Alberta kgmills@ualberta.ca Liyao Jiang ECE Department University of Alberta liyao1@ualberta.ca Chao Gao Huawei Technologies Edmonton, Alberta, Canada chao.gao4@huawei.com Di Niu ECE Department University of Alberta dniu@ualberta.ca"
        },
        {
            "title": "Abstract",
            "content": "Diffusion Transformers (DiT) have become the de-facto model for generating highquality visual content like videos and images. huge bottleneck is the attention mechanism where complexity scales quadratically with resolution and video length. One logical way to lessen this burden is sparse attention, where only subset of tokens or patches are included in the calculation. However, existing techniques fail to preserve visual quality at extremely high sparsity levels and might even incur non-negligible compute overheads. To address this concern, we propose Re-ttention, which implements very high sparse attention for visual generation models by leveraging the temporal redundancy of Diffusion Models to overcome the probabilistic normalization shift within the attention mechanism. Specifically, Re-ttention reshapes attention scores based on the prior softmax distribution history in order to preserve the visual quality of the full quadratic attention at very high sparsity levels. Experimental results on T2V/T2I models such as CogVideoX and the PixArt DiTs demonstrate that Re-ttention requires as few as 3.1% of the tokens during inference, outperforming contemporary methods like FastDiTAttn, Sparse VideoGen and MInference. Further, we measure latency to show that our method can attain over 45% end-to-end and over 92% self-attention latency reduction on an H100 GPU at negligible overhead cost. Code available online here: https://github.com/cccrrrccc/Re-ttention"
        },
        {
            "title": "Introduction",
            "content": "Diffusion Transformers (DiT) [31, 3, 2, 22, 9] combine the attention [36] mechanism with the iterative denoising of Diffusion Models [33] to generate high-quality visual content such as videos [45, 52] and images [19, 42, 39]. However, key bottleneck to generating longer videos and higher resolution content is the global properties of the self-attention module, whose compute cost scales quadratically with sequence size, i.e., resolution and video length. Sparse attention techniques [5] aim to lower the computational burden by reducing the number of sequence tokens/patches that the attention mechanism considers during inference. Contemporary techniques like MInference [17] and Sparge Attention [49], as well as XAttention [44] achieve 50% sparsity (i.e., reducing only 50% of the attention computations) by relying on downsampling the attention map or anti-diagonal scoring, respectively. In parallel, several recent methods [40, 46, 48] have been proposed specifically for DiTs, increasing the attention sparsity to 70% by exploiting Preprint. Under review. Figure 1: Visual comparison using CogVideoX-2B [45] T2V model. Columns correspond to different frames. Rows correspond to to different sparse attention methods (sparsity degree in paranthesis; higher is better). Prompt: colorful butterfly perching on bud. More examples in the Appendix. the inherent characteristics of diffusion process such as the progressively denoising structure and the spatial/temporal locality of attention. While these methods can reduce more than half of the attention computation, their effectiveness remains limited for the growing computational demands of high-resolution image and video generation. Previous researches like LongFormer [1] and BigBird [47] can achieve >95% sparse attention. However, their reliance on retraining and fine-tuning introduces significant computation burdens, limiting their applicability to modern large-scale, pretrained generative models. Thus, the development of sparse attention techniques that achieve >95% sparsity with minimal visual quality loss remains an open challenge. In this paper, we propose an effective method to statistically reshape the attention distribution distorted by the deployment of sparse attention, which we call Re-ttention. Re-ttention overcomes the high sparsity challenge faced by the training-free sparse attention method. Moreover, it is simple to implement and incurs negligible overhead compared to standard sparse attention at the same sparsity level. Figure 1 provides sample content from our technique compared to other sparse attention methods. Our detailed contributions are as follows: 1. We relate the failure to achieve degradation-free >95% sparsity without training from scratch to the distributional shift in attention scores caused by the reduced softmax denominator term, i.e., the row-wise sum of the exponentials of involved elements. We design an experiment to illustrate the importance of preserving this term and the impact on visual generation. 2. We discover the softmax distribution redundancy among neighboring denoising steps. Although the actual value of denominator changes unpredictably, the ratio between the sparse and full denominator is relatively stable. 3. We propose that the attention scores shifted by sparse attention are viable to be recovered by approximating the real softmax denominator. 4. The recovered attention scores deviate from valid probability distribution, as their sum is less than one, violating the normalization property of softmax. We leverage the redundancy among neighboring denoising steps to compensate the missing probability with residual. We apply Re-ttention to T2V models such as CogVideoX [45] in order to outperform contemporary methods like FastDiTAttn [46], Sparse VideoGen [40] and MInference [17] on relevant tasks such as VBench [15]. Furthermore, we apply Re-ttention to T2I models like the PixArt series [3, 2] and 2 others [22] to maximize performance on Human Preference Score v2 (HPSv2) [39] and GenEval [11] while achieving high sparsity of 96.9%. On an H100 GPU, this translates to over 92% self-attention latency reduction. This further tails end-to-end gains ranging from over 30% on PixArt-α/Σ to over 45% latency reduction on CogVideoX-2B at negligible overhead cost."
        },
        {
            "title": "2 Related Work",
            "content": "Diffusion Models (DM) [13] dominate visual generation tasks. Early DMs [33, 32] use convolutional U-Net structures [34] as their backbones. Later, Diffusion Transformers (DiT) [31] adopt the attentionbased [36] of Vision Transformers (ViT) [8] to increase scalability and visual generation quality. In addition to being the favored backbone structure for text-to-image (T2I) DMs [3, 2, 22, 9, 19, 42], the DiT structure is extensible to video generation [38, 18] as well. Specifically, Latte [29] proposes 2D+1D attention block for video generation, which performs spatial and temporal attention separately. Subsequent works like CogVideoX [45] and OpenSora [23] adopt 3D attention structure which processes the spatial and temporal dimensions simultaneously, yielding improved generation quality. However, this enhancement comes at the cost of significantly increased computation due to the quadratic complexity of attention, highlighting the pressing need for more efficient and sparse attention, which we explore in this work. Sparse Attention denotes class of techniques that aim to alleviate the hardware cost of the attention mechanism by omitting computation for unnecessary query-key pairs. Specifically, it is well documented that the attention mechanism produces sparse results [7, 27], yet suffers from burdensome quadratic complexity and wasted computation by default. LongFormer [1] proposes sliding window attention that restricts attention to local region. BigBird [47] and Mistal-7B [16] extend this idea to fine-grained attention masks, while SwinFormer [28] use local attention for efficient ViT design. Although these methods can reduce the attention computation by factor or 8 or more, they often necessitate training or fine-tuning the model, thus restricting the scope of deployment. There are also training-free sparse attention methods. MInference [17] downsamples the attention probability matrix (QK ) into blocks then dynamically select the top-k blocks to perform sparse attention. Subsequent research like FlexPrefill [20], Sparge Attention [49] and XAttention [44] rely on the block selection idea and propose dynamic block sorting algorithms. Further methods like StreamingLLM [41], DiTFastAttn [46] and Sparse VideoGen [40] identify the special attention patterns in LLM and DiT and propose efficient attention masking based on those patterns. However, the sparsity achievable by these methods is limited to < 70%, which is much higher compared to prior works that require re-training or fine-tuning. We aim to address this gap and provide training-free sparse attention method that can achieve > 95% sparsity on visual generation tasks. Caching is technique used in computer systems to temporarily store data or computations, thereby reducing redundant processing and improving overall efficiency. In DiT, the lengthy denoising process makes it well-suited for the application of caching techniques. Recent methods [4, 26, 51] re-use the attention outputs or the intermediate features at different denoising timesteps to skip the attention computation. Methods like DiTFastAttn [46, 48] leverage the caching mechanism to improve the visual quality."
        },
        {
            "title": "3 Background: The Attention Mechanism and Sparsity",
            "content": "The attention mechanism [36] is the foundation of transformer architectures like DiTs. Let RT be an input token/patch sequence, where is the sequence length, dependent on the input size (e.g., image resolution or video length) and is the embedding dimension, hyperparameter of the ; dh Z+. We first transformer model. The attention mechanism contains heads such that dh = map into three representations, Query (Q), Key (K) and Value (V ) of identical size RhT dh , then compute the attention as follows, Attention(Q, K, ) = Softmax( QK dh )V. (1) Figure 2: Illusion of attention map computed by full attention, contemporary sparse attention (window-based) and our proposed Re-ttention. Sparse attention shifts the distribution of attention scores, resulting in degraded performance as sparsity increases. In contrast, Re-ttention re-uses the denominator ratio cached from the previous denoising steps to scale the sparse attention score to the full attention level. Then, we apply residual caching to accurately restore the full attention scores. We can decompose this mechanism into several intermittent matrices, specifically the product of and before (Apre) and after (A) the softmax operation: Apre = QK dh RhT , (2) = Softmax(Apre) [0, 1)hT . (3) The computation of these matrices is very expensive [10]. To make matter worse, their size scales quadratically with , which depends on the image/video resolution and video length. However, the softmax operation is computed row-wise and produces probability distribution (cid:80)T j=1 A:,:,j = 1, which empirically produces sparse in practice [7, 27]. Therefore, one way to alleviate this computational burden is to use sparse attention mechanism. The key idea is to omit less relevant values of Apre, that are likely to be 0 or close to 0 in A, from the softmax computation altogether. Formally, we express the sparse attention calculation using mask {0, 1}hT where 1 means an index of Apre will be included in the softmax, while the rest are excluded. The indexes of the included values in Apre form set = (cid:83)h i=1 Sk,i, where Sk,i = {(k, i, j)Mk,i,j = 1, 0 }. Given an arbitrary element of the pre-softmax matrix Apre tion are given by k,i,j, the normal and sparse softmax computa- (cid:83)T k=1 Ak,i,j = exp(Apre k,i,j) t=1 exp(Apre k,i,t) (cid:80)T , (4) Ak,i,j = (cid:80) exp(Apre k,i,j) exp(Apre k,i,t) tSk,i if Sk,i, otherwise, (5) respectively. Ultimately, the mask matrix determines the potential amount of computational savings. can be computed statically [6] prior to inference or dynamically [17, 20, 49, 44] at runtime. Static techniques make more assumptions about the sparse regions of while dynamic techniques impose additional inference overhead to compute . Regardless of technique, we can quantify the attention sparsity as percentage, e.g., 10%, 50%, 90%, etc., simply by computing the ratio of values in that are 0 as follows: Sparsity = (1 hT 2 ) 100%, (6) where higher value for sparsity corresponds to lower computational burden. Therefore, sparse corresponds to an overall sparse attention. However, high sparsification can cause significant shifts in the softmax calculation statistics [41] and lead to detrimental performance. As we will next show, our proposed method, Re-ttention, aims to identify these statistical issues and address them."
        },
        {
            "title": "4 Proposed Method: Re-ttention",
            "content": "In this section we form hypothesis regarding how distributional shift in softmax statistics prevents current training-free sparse attention methods from satisfactorily operating at high sparsity, e.g., > 95%. We then elaborate on our proposed Re-ttention technique, which overcomes this burden by 4 re-using and caching softmax statistics at high sparsity. Figure 2 provides high-level overview of our proposed technique in comparison to full and sparse attention. 4.1 Importance of the Softmax Denominator As preliminary investigation, we gauge the performance of several existing sparse attention techniques [17, 46, 40]. We consider the GenEval [11] benchmark and evaluate performance across spectrum of sparsity values, i.e., starting at the highest sparsity these techniques consider in their original manuscript and then further increasing the sparsity. Figure 3 illustrates our findings. We observe that existing approaches suffer monotonic performance drop when the sparsity is further increased beyond their proposed value (denoted with ). Per Eq. 5, higher value of sparsity corresponds the inclusion of fewer tokens in the softmax denominator as Sk,i shrinks. Thus, the further removal of tokens, i.e, increasing sparsity closer to 100%, has larger impact on the overall denominator value [41]. This phenomenon, introduces detrimental distribution shift in the overall attention scores. We design toy experiment to test this hypothesis and showcase the significance of the softmax denominator term. Specifically, we define post-softmax masking operation as = M, (7) Figure 3: Quality-sparsity comparison of Re-ttention, Sparse VideoGen (SVG), MInference and DiTFastAttn. denotes the sparsity level that prior methods operate under non-degraded conditions. where is the output of the original full softmax attention via Eq. 4, denotes element-wise multiplication and is masked attention. We emphasize that Equation 7 is not proper sparse attention calculation and does not entail speedup. However, it mimics the output of sparse attention as still zeroes out the same indices of A, yet preserves the denominator of the full softmax. We calculate using sliding window attention [1]. We then generate visual content using both the formal sparse attention from Equation 5 and our post-softmax Equation 7 for comparison. Figure 4 provides comparison, though we provide additional examples and prompts in the supplementary. We observe how the post-softmax attention preserves the guitar-playing panda, chair and background while the pre-softmax attention creates noisy frame with jumbled contents where the panda appears to eat the guitar. Thus, these visual results validate our assumption regarding the importance of maintaining the softmax denominator. The challenge now becomes how to preserve this information in an efficient sparse attention setup. 4.2 Leveraging Denoising Properties for Statistical Reshape Figure 4: Visual comparison of preSoftmax and post-Softmax masking on CogVideoX-2B with 66% sparsity, using sliding-window attention [1]. One way to mitigate the distribution shift is to maintain the softmax denominator from the full attention calculation. We achieve this by exploiting the sequential nature of the DM denoising process and taking inspiration from DiT caching [4, 26, 51] and redundancy [35] methods. Denominator Approximation. Figure 5 tracks the magnitude of the softmax denominator for single token in the 9th head of the 12th DiT block in PixArt-α Specifically, we calculate the denominator using both the full and sparse attention (with 87.5% sparsity) as well as the ratio ρ between these statistics, (cid:80) ρ = exp(Apre tSk,i t=1 exp(Apre k,i,t) k,i,t) (cid:80)T . (8) This yields an insightful observation: While the actual value of the denominators may change unpredictably and non-monotonically over the denoising process, given statically computed mask , the ratio ρ follows predictable trend. Therefore, we propose simple method to recover the attention distribution by modifying the output of the sparse softmax operation Eq. 5 as follows: k,i,j = ρAk,i,j, (9) where we cache 0 < ρ 1 from previous step. Multiplying the Softmax output of sparse attention by ρ approximates the full attention value, mitigating the distribution shift. In practice, since ρ is not constant per Fig. 5, we empirically find that slightly increasing the cached ρ after each denoising step achieves better performance. Thus, we parameterize ρt = ρt1 + λ after each denoising step, where λ is ramp-up hyperparameter. Residual Caching. Although we can modify the softmax to make the sparse attention more closely match that of full attention, Equation 8 does not yield proper probability distributions as (cid:80)T j=1 A:,:.j < 1. Practically, this reduces the magnitude of overall attention outputs per Equation 1, which can negatively impact performance. To address this issue, we first define the residual as the difference between the full attention computed via Equation 4 and our ρ-reshaped attention via Equations 5 and 9: Figure 5: Plotting softmax denominators for full and sparse attention as well as the ratio ρ per Eq. 8 across 20 steps. = FullAttention(Q, K, ) ReshapeAttention(Q, K, V, ρ). (10) We can later add to our sparse attention output. In fact, is mathematically equivalent to the attention output of the masked tokens at the caching timestep. Overall, the idea behind Re-ttention is to compute sparse attention for important regions, while re-using previously cached statistics from previous steps in less important regions of the attention map. This involves caching necessary statistics from the full attention in some steps, which is common in DiT sparse attention methods [46, 48]."
        },
        {
            "title": "5 Experimental Setup and Results",
            "content": "We evaluate Re-ttention on both the text-to-video (T2V) and text-to-image (T2I) tasks using number of DiT models, such as CogVideoX (2B) [45], PixArt-α/Σ (0.6B) [3, 2] and Hunyuan-DiT (1.6B) [22]. We generate 720 480 resolution, 6 second videos at 8 fps and 1024 1024 pixel images throughout this paper. We compare to several existing sparse attention methods for visual content in the literature like Sparse VideoGen (SVG) [40], MInference [17] and DiTFastAttn [46, 48] to demonstrate both qualitative and quantitative performance gains and computational cost savings. Implementation Details. Specifically, we use the HuggingFace Diffusers library [37] to instantiate the base DiT models and consider the default values for inference parameters like the classifier-free guidance (CFG) scale and number of denoising steps - 50 for CogVideoX/Hunyuan and 20 for the PixArt DiTs. Following prior literature on DiT acceleration [40, 51, 21, 26], we apply the full attention during the first 5, 10 or 15 steps for the PixArt DiTs, Hunyuan and CogVideoX models, respectively, and then apply the sparse attention mechanism for the remainder of the denoising process. Further, we set caching period of 5 steps for DiTFastAttn and Re-ttenion, where we perform full attention to cache required statistics. For fair comparison, we apply this caching to SVG 6 Table 1: Quantitative evaluation results for T2V model CogVideoX-2B [45] on VBench [15] and other metrics. Arrows indicate if higher or lower value of metric is preferred. Best and second-best results in bold and italics, respectively. Attention Sparsity PSNR SSIM LPIPS ImageQual SubConsist Full-Attention SVG SVG MInference MInference DiTFastAttn Re-ttention 0.0% 87.5% 96.9% 87.5% 96.9% 96.9% 96.9% Reference Reference Reference 14.48 10.50 14.99 9.25 27.93 27.96 0.548 0.418 0.558 0.325 0.865 0. 0.501 0.898 0.480 0.818 0.098 0.059 65.72% 54.48% 51.82% 53.78% 34.36% 64.86% 64.87% 94.97% 89.26% 96.73% 83.71% 75.84% 94.32% 94.80% and MInference as well. To perform T2I using SVG, we treat the image as video containing single frame. We provide further baseline experimental details in the supplementary. The rest of this section is organized as follows: We enumerate our T2V and T2I evaluation setup and results in Sections 5.1 and 5.2, respectively. Next, we provide hardware efficiency metrics in Section 5.3 and provide ablation studies in Section 5.4. 5.1 Text-to-Video Evaluation We perform quantitative T2V evaluation using the Animal and Architecture categories of VBench [15], which consist of 100 videos each. For video quality, we use VBench score to evaluate standalone video quality. Specifically, we follow previous literature [40] and report the Image Quality and Subject Consistency metrics in VBench. Additionally, we compute the Peak Signal-to-Noise Ratio (PSNR) [14], Structural Similarity Index Measure (SSIM) [30] and Learned Perceptual Image Patch Simularity (LPIPS) [50]. These metrics evaluate the similarity and quality of videos generated by sparse attention methods relative to those generated using the full attention mechanism. We evaluate all methods at 96.9% sparsity to provide an apples-to-apples performance investigation. However, some methods exhibit substantial degradation at this level and produce very noisy/black frames, so we additionally report results at less aggressive setting of 87.5% sparsity. Table 1 presents our findings. Results demonstrate that Re-ttention consistently outperforms all other baselines in terms of video quality and similarity metrics. Notably, Re-ttention not only outperforms both SVG and MInference at the strict sparsity of 96.9%, but also at 87.5% sparsity. The one exception is SVG at 96.9% sparsity, which achieves the highest SubConsist performance. However, this result is an outlier, as it even exceeds the SubConsist performance of full attention significantly while SVG substantially underperforms on all other metrics at this sparsity level. Furthermore, Re-ttention also outperforms DiTFastAttn, which also involves caching additional statistics at the high sparsity level of 96.9%. Therefore, overall, these results demonstrate the robustness, competitive performance of Re-ttention at > 95% sparsity in T2V applications. We provide some sample frames from videos generated by Re-ttention, baseline sparse attention methods and full attention. Specifically, recall Figure 1 in the introduction. The video generated by Re-ttention shows the best clarity and temporal consistency across frames for the main subject, and it has no artifacts in the background. Moreover, the video generated by Re-ttention is most similar to the reference video generated with full-attention. In contrast, the video generated by DiTFastAttn has noisy texture artifacts both in the background and the subject. For SVG and MInference, the subject is inconsistent and deformed despite using much lower sparsity. We provide more T2V visual comparisons in the supplementary materials. 5.2 Text-to-Image Results We evaluate T2I performance on comprehensive set benchmark metrics: GenEval [11], HPSv2 [39], and MS-COCO 2014 [24]. GenEval consists of 553 unique prompts. For each prompt, the DiT generates 4 images. HPSv2 consists of four image categories: Animation, Concept-art, Painting and Photos. Each category consists of 800 images for 3.2k generations in total. Finally, we generate 10k images using the MS-COCO 2014 validation set and measure the LPIPS score [50], ImageReward (IR) [43] and CLIP score [12] using the ViT-B/16 backbone. 7 Table 2: Quantitative evaluation results for PixArt-α [3], PixArt-Σ [2] and Hunyuan-DiT [22] across the GenEval [11], HPSv2 [39], and MS-COCO 2014 [24] benchmarks. Best and second best results in bold and italics, respectively. Model Attention Sparsity GenEval HPSv2 LPIPS IR CLIP PixArt-α PixArt-Σ Hunyuan Full-Attention SVG MInference DiTFastAttn DiTFastAttn Re-ttention Re-ttention Full-Attention SVG MInference DiTFastAttn DiTFastAttn Re-ttention Re-ttention Full-Attention SVG MInference DiTFastAttn DiTFastAttn Re-ttention Re-ttention 0.0% 75.0% 75.0% 93.8% 96.9% 93.8% 96.9% 0.0% 75.0% 75.0% 93.8% 96.9% 93.8% 96.9% 0.0% 75.0% 75.0% 93.8% 96.9% 93.8% 96.9% 0.480 0.368 0.433 0.431 0.364 0.456 0.448 0.544 0.172 0.429 0.411 0.233 0.513 0.512 0.610 0.317 0.450 0.024 0.002 0.585 0. 30.79 25.24 28.04 27.26 26.71 28.29 27.57 30.70 18.48 27.09 27.64 22.79 28.37 27.72 30.41 24.73 23.94 14.77 12.28 29.03 28. Reference 0.864 0.655 0.458 0.506 0.590 0.354 0.372 -0.141 0.549 0.688 0.314 0.688 0.646 Reference 0. 0.742 0.536 0.591 0.734 0.417 0.435 -1.315 0.457 0.507 -0.600 0.808 0.784 Reference 1.027 0.854 0.720 0.896 0.923 0.598 0.606 -0.574 -0.063 -2.074 -2.237 0.911 0. 31.28 29.43 30.93 30.72 29.63 31.21 31.20 31.54 26.09 30.76 30.08 27.37 31.59 31.59 31.77 27.92 30.15 22.47 22.10 31.63 31. FullAttention SVG (75%) MInference (75%) DiTFastAttn (93.8%) DiTFastAttn (96.9%) Re-ttention (93.8%) Re-ttention (96.9%) Prompt: cat on city street with people. Prompt: teddy bear wearing white shirt and green apron Prompt: view of Big Ben from over the water, during the day. Figure 6: Visual comparison on MS-COCO 2014 [24] prompts using PixArt-α (row 1), PixArt-Σ (row 2), and Hunyuan (row 3). We show images generated by Re-ttention (our method) and by other attention methods in different columns. We provide further examples in the appendix. Table 2 lists our results on the T2I task. Re-ttention outperforms all other sparse attention methods across models and metrics, showing consistently better performance. Additionally, Re-ttention achieves this while operating under an extremely high sparsity of 96.9%, which reduces the token/patch sequence to less than one twentieth of its original size, whereas other baseline methods underperform at 75% sparsity, which only reduces sequence length down to one fourth. Additionally, our performance on the IR metric is consistently positive, feat that no other sparse attention method attains. Moreover, while DiTFastAttn attains similar T2V performance to Re-ttention, it fails to generalize to he T2I task on Hunyuan in terms of GenEval and HPSv2 performance, even at reduced sparsity of 93.8%. In contrast, Re-ttention performance neither suffers at 93.8% nor 96.9% sparsity, underscoring the effectiveness of our technique. Next, we present the visual (qualitative) comparisons on PixArt-α [3], PixArt-Σ [2] and Hunyuan [22] T2I models in Figures 6. More visual comparisons can be found in the Appendix. Overall, Re8 ttention generates images with better image quality than other sparse attention methods and has higher similarity to the reference images generated by full-attention, even when using an extreme sparsity of 96.9%. For PixArt-α [3] and PixArt-Σ [2], Re-ttention generates clean, high-quality images that are well aligned to the prompts. Whereas the other methods often generate colored noise artifacts, distorted subjects, and lower quality images. For Hunyuan [22], we observe that the other sparse attention methods generate severely degraded images, while Re-ttention can generate images that are similar to images generated by full-attention. 5.3 Hardware Efficiency We measure the inference latency on an NVIDIA H100 80GB HBM3 GPU. Specifically, we showcase end-to-end DiT speedup and then zoom in on the operation-level for the self-attention block. Model Table 3: Average end-to-end generation latency time to generate one video/image in seconds (% change in parenthesis) using DiTFastAttn and Re-ttention. End-to-end. We report latency in Table 3. We consider DiTFastAttn as similar cachingbased baseline while SVG and MInference perform costlier online computation to compute the mask matrix . We measure latency at the default sparsity level of 87.5% for DiTFastAttn and at 87.5%/96.9% for both methods. Latency for Re-ttention and DiTFastAttn at 96.9% is similar with the latter costing slightly more time over the entire denoising process while also showing noticable improvement over 87.5% sparsity and the full attention, especially for the costlier CogVideoX T2V and Hunyuan T2I models. Overall, sparse attention achieves the biggest gains (> 40% reduction) on CogVideoX, while extreme sparsity (> 95%) makes bigger impact on the smaller models: > 4% more speedup on PixArt-α/Σ compared to 87.5% sparsity. PixArt-α 1.92 PixArt-Σ 2.05 Hunyuan 8.80 CogVideoX 54.43 1.31 (-31.6%) 1.44 (-30.1%) 7.62 (-13.4%) 29.78 (-45.3%) 1.40 (-27.2%) 1.52 (-25.8%) 7.77 (-11.6%) 31.35 (-42.4%) 1.31 (-31.8%) 1.43 (-30.3%) 7.60 (-13.6%) 29.71 (-45.4%) DiTFastAttn (87.5%) DiTFastAttn (96.9%) Re-ttention (96.9%) Full Full Attn. Model (Seq_len ) Table 4: Average latency (ms; % change in parenthesis) of attention block for common sparse attention with 96.9% sparsity and Re-ttention with 96.9% sparsity. Operation-level. Table 4 reports latency measurements on the selfattention block. We evaluate and report the latency of self-attention block using Re-ttention and common sparse attention, as depicted in Table 4. Note that this calculation excludes external caching and the calculation of , so MInference, SVG and DiTFastAttn behave similarly, while Re-ttention executes Eqs. 9 and 10 for multiplying ρ and adding R, respectively. consistent finding is that 96.9% sparsity roughly translates to 92% latency reduction on an NVIDIA H100, showing that our contributions do not impose significant overhead. PixArt (4k) Hunyuan (4k) CogVideoX (17k) 0.186 (-92.8%) 0.204 (-92.2%) 1.273 (-92.0%) 0.186 (-92.8%) 0.206 (-92.1%) 1.277 (-92.0%) 2.59 2.61 15. Sparse Attn. Re-ttention 5.4 Ablation Studies Finally, we ablate the effect of the ramp-up hyperparameter λ on PixArt-Σ [2] on overall performance. Specifically, we evaluate on HPSv2 overall as well as the animation and concept-art categories. Table 5 reports our findings. These findings demonstrate the robustness of Re-ttention as it is possible to forgo the λ parameter, yet it is better to select moderate value. Table 5: HPVs2 score under different ramp-up hyperparameter λ with 96.9% sparsity on PixArt-Σ [2]. λ 0 0.01 0.02 0.04 Anime ConceptArt HPSv2 29.40 28.89 28.88 29.60 26.94 26.21 26.19 27.23 27.46 26.83 26.82 27."
        },
        {
            "title": "6 Future Work, Limitations and Impacts",
            "content": "We design Re-ttention around achieving high sparsity for the non-autoregressive self-attention mechanism utilized by visual generation DiTs, rather than the autoregressive, causally-masked attention of LLMs which may more often feature different attention patterns such as columns [6, 41]. 9 Additionally, Re-ttention exploits the sequential nature of DMs and is inspired by DiT caching techniques [4, 26, 51]. Our method may not be readily generalizable to autoregressive LLMs or autoregressive visual content generation models, though this is direction for future work. Furthermore, Re-ttention is currently designed for statically computed attention masks, which offer speedup advantages yet make stronger assumptions about the layout of the attention map. Therefore, potential direction for further investigation is to look into how Re-ttention can be applied in the case of dynamically generated attention masks. 6.1 Societal Impacts Re-ttention improves the efficiency of image and video generation by enabling extremely sparse attention. This makes high-quality generative models more accessible and environmentally sustainable by reducing computational and energy demands. By lowering resource barriers, Re-ttention can benefit creators, educators, and researchers in low-resource settings. While any generative model carries risk of misuse, Re-ttention does not introduce new risks beyond existing systems. Responsible deployment and continued dialogue on ethical use remain important."
        },
        {
            "title": "7 Conclusion",
            "content": "We propose Re-ttention, training-free sparse attention method for Diffusion Transformers, which achieves 96.9% sparsity and 92% self-attention latency reduction on an H100, without performance loss on DiTs like CogVideoX and Hunyuan. We attain these gains by identifying the distribution shift of attention scores incurred by sparse attention methods that prevents extreme sparsity (> 95%) without significant performance degradation and resolve this issue using combination of caching and statistical re-use. We evaluate Re-ttention on T2V and T2I tasks, outperforming contemporary baselines like SVG, MInference and DiTFastAttn while achieving significant inference speedup."
        },
        {
            "title": "References",
            "content": "[1] Iz Beltagy, Matthew Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. [2] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-Σ: Weak-to-strong training of diffusion transformer for 4k text-to-image generation, 2024. [3] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Zhongdao Wang, James T. Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id= eAKmQPe3m1. [4] Pengtao Chen, Mingzhu Shen, Peng Ye, Jianjian Cao, Chongjun Tu, Christos-Savvas Bouganis, Yiren Zhao, and Tao Chen. Delta-dit: training-free acceleration method tailored for diffusion transformers. arXiv preprint arXiv:2406.01125, 2024. [5] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. [6] Steve Dai, Hasan Genc, Rangharajan Venkatesan, and Brucek Khailany. Efficient transformer inference with statically structured sparse attention. In 2023 60th ACM/IEEE Design Automation Conference (DAC), pages 16. IEEE, 2023. [7] Yichuan Deng, Zhao Song, and Chiwun Yang. Attention is naturally sparse with gaussian distributed input. arXiv preprint arXiv:2404.02690, 2024. [8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. [9] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 10 [10] Yao Fu. Challenges in deploying long-context transformers: theoretical peak performance analysis. arXiv preprint arXiv:2405.08944, 2024. [11] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36:5213252152, 2023. [12] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021. [13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [14] Alain Horé and Djemel Ziou. Image quality metrics: Psnr vs. ssim. In 2010 20th International Conference on Pattern Recognition, pages 23662369, 2010. doi: 10.1109/ICPR.2010.579. [15] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. [16] AQ Jiang, Sablayrolles, Mensch, Bamford, DS Chaplot, Ddl Casas, Bressand, Lengyel, Lample, Saulnier, et al. Mistral 7b. arxiv 2023. arXiv preprint arXiv:2310.06825, 2024. [17] Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir Abdi, Dongsheng Li, Chin-Yew Lin, et al. Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention. Advances in Neural Information Processing Systems, 37:5248152515, 2024. [18] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [19] Black Forest Labs. flux, 2024. URL https://github.com/black-forest-labs/flux. [20] Xunhao Lai, Jianqiao Lu, Yao Luo, Yiyuan Ma, and Xun Zhou. Flexprefill: context-aware sparse attention mechanism for efficient long-sequence inference. arXiv preprint arXiv:2502.20766, 2025. [21] Muyang Li, Tianle Cai, Jiaxin Cao, Qinsheng Zhang, Han Cai, Junjie Bai, Yangqing Jia, Kai Li, and Song Han. Distrifusion: Distributed parallel inference for high-resolution diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 71837193, 2024. [22] Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, Dayou Chen, Jiajun He, Jiahao Li, Wenyue Li, Chen Zhang, Rongwei Quan, Jianxiang Lu, Jiabin Huang, Xiaoyan Yuan, Xiaoxiao Zheng, Yixuan Li, Jihong Zhang, Chao Zhang, Meng Chen, Jie Liu, Zheng Fang, Weiyan Wang, Jinbao Xue, Yangyu Tao, Jianchen Zhu, Kai Liu, Sihuan Lin, Yifu Sun, Yun Li, Dongdong Wang, Mingtao Chen, Zhichao Hu, Xiao Xiao, Yan Chen, Yuhong Liu, Wei Liu, Di Wang, Yong Yang, Jie Jiang, and Qinglin Lu. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding, 2024. [23] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024. [24] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár. Microsoft coco: Common objects in context. In Computer Vision ECCV 2014, pages 740755, Cham, 2014. Springer International Publishing. [25] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. [26] Feng Liu, Shiwei Zhang, Xiaofeng Wang, Yujie Wei, Haonan Qiu, Yuzhong Zhao, Yingya Zhang, Qixiang Ye, and Fang Wan. Timestep embedding tells: Its time to cache for video diffusion model. arXiv preprint arXiv:2411.19108, 2024. [27] Liu Liu, Zheng Qu, Zhaodong Chen, Fengbin Tu, Yufei Ding, and Yuan Xie. Dynamic sparse attention for scalable transformer acceleration. IEEE Transactions on Computers, 71(12):31653178, 2022. 11 [28] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1001210022, 2021. [29] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. [30] Jim Nilsson and Tomas Akenine-Möller. Understanding ssim. arXiv preprint arXiv:2006.13846, 2020. [31] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. [32] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. SDXL: improving latent diffusion models for high-resolution image synthesis. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=di52zR8xgf. [33] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [34] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted InterventionMICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pages 234241. Springer, 2015. [35] Xibo Sun, Jiarui Fang, Aoyu Li, and Jinzhe Pan. Unveiling redundancy in diffusion transformers (dits): systematic study. arXiv preprint arXiv:2411.13588, 2024. [36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/ paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf. [37] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj, Dhruv Nair, Sayak Paul, William Berman, Yiyi Xu, Steven Liu, and Thomas Wolf. Diffusers: State-of-the-art diffusion models. https://github.com/huggingface/diffusers, 2022. [38] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [39] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. [40] Haocheng Xi, Shuo Yang, Yilong Zhao, Chenfeng Xu, Muyang Li, Xiuyu Li, Yujun Lin, Han Cai, Jintao Zhang, Dacheng Li, et al. Sparse videogen: Accelerating video diffusion transformers with spatial-temporal sparsity. arXiv preprint arXiv:2502.01776, 2025. [41] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. [42] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, and Song Han. Sana: Efficient high-resolution image synthesis with linear diffusion transformer, 2024. URL https://arxiv.org/abs/2410.10629. [43] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: learning and evaluating human preferences for text-to-image generation. In Proceedings of the 37th International Conference on Neural Information Processing Systems, pages 1590315935, 2023. [44] Ruyi Xu, Guangxuan Xiao, Haofeng Huang, Junxian Guo, and Song Han. Xattention: Block sparse attention with antidiagonal scoring. arXiv preprint arXiv:2503.16428, 2025. [45] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 12 [46] Zhihang Yuan, Hanling Zhang, Lu Pu, Xuefei Ning, Linfeng Zhang, Tianchen Zhao, Shengen Yan, Guohao Dai, and Yu Wang. Ditfastattn: Attention compression for diffusion transformer models. Advances in Neural Information Processing Systems, 37:11961219, 2024. [47] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33:1728317297, 2020. [48] Hanling Zhang, Rundong Su, Zhihang Yuan, Pengtao Chen, Mingzhu Shen Yibo Fan, Shengen Yan, Guohao Dai, and Yu Wang. Ditfastattnv2: Head-wise attention compression for multi-modality diffusion transformers. arXiv preprint arXiv:2503.22796, 2025. [49] Jintao Zhang, Chendong Xiang, Haofeng Huang, Jia Wei, Haocheng Xi, Jun Zhu, and Jianfei Chen. Spargeattn: Accurate sparse attention accelerating any model inference. arXiv preprint arXiv:2502.18137, 2025. [50] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. [51] Xuanlei Zhao, Xiaolong Jin, Kai Wang, and Yang You. Real-time video generation with pyramid attention broadcast. arXiv preprint arXiv:2408.12588, 2024. [52] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024."
        },
        {
            "title": "A Appendix",
            "content": "We provide additional details on our methodology in Sec. A.1 and baselines in Sec. A.2. Section A.3 provides elaborates on our pre vs. post-softmax example from Figure 4. Finally, Sections A.4 and A.5 provide additional T2V and T2I results, respectively. A.1 Explanation of Re-ttention In Section 4 we claim that the residual in Re-ttention is mathematically equivalent to the attention output of the masked tokens at the caching timestep. We now further elaborate on this claim: Recall the definition of in Eq. 3 and the set that contains the included values (by sparse attention) in A. Hence, the can be decomposed into two parts: = AS + /S, AS = 1(k,i,j)S, /S = 1(k,i,j) /S, (11) where 1S is the indicator matrix that is 1 where (k, i, j) S, and 0 elsewhere. Conversely, 1 /S is 1 where (k, i, j) / and 0 elsewhere. At the caching timestep, we have the ratio ρ between the denominator of full and sparse attention according to Eq. 8. Because we compute full attention in the caching step, the ratio ρ is not an approximation but an accurate value. Hence, we have: ReshapeAttention(Q, K, V, ρ) = ρA = AS Therefore, the residual in Eq. 10 is: = AS = /S V, (12) (13) which is mathematically equivalent to the attention output of the masked tokens at the caching timestep. A.2 Details of Baseline Implementation We compare Re-ttention to three different baseline methods: Sparse VideoGen (SVG) [40], MInference [17], and DiTFastAttn [46]. We enumerate the experiment implementation details for Text-to-Video (T2V) and Text-to-Image (T2I) generation tasks, respectively. Text-to-Video For DiTs, MInference classifies all attention heads into block sparse format [17] to generate . We use the SVG official implementation for CogVideoX series to generate videos. As for Re-ttention and DiTFastAttn, we use sliding window attention, which restricts each tokens attention to local neighborhood and will repeat the same mask at each frame of the video. Text-to-Image Since T2I generation lacks temporal dimension, we apply only the spatial attention heads in SVG and adjust the window size to match the target sparsity. Re-ttention and DiTFastAttn use the same sliding window attention as SVG. A.3 Additional Examples for Post-Softmax Masking Operation Figure 7 expands on Fig. 4 by providing additional video frame comparisons and the lengthy textual prompt. Post-Softmax masking not only better preserves the objects (panda, stool, guitar, etc.), but also consistently maintains the main part of the video over time, while pre-softmax causes the large panda to vanish. This example further validates our assumption regarding the importance of maintaining the softmax denominator. Further, Figure 8 provides an additional comparison with different prompt. In the pre-softmax video, the ship becomes increasingly distorted over time, whereas in the post-softmax video, it remains consistent throughout. Notably, the reduced texture detail in the post-Softmax output reveals an issue caused by denormalized attention probabilitiesspecifically, information loss due to the sum of softmax probabilities being less than one, leading to shrinkage effect in the features. 14 Figure 7: Visual comparison of pre-softmax and post-softmax masking on CogVideoX-2B with 66% sparsity. Prompt: panda, dressed in small, red jacket and tiny hat, sits on wooden stool in serene bamboo forest. The pandas fluffy paws strum miniature acoustic guitar, producing soft, melodic tunes. Nearby, few other pandas gather, watching curiously and some clapping in rhythm. Sunlight filters through the tall bamboo, casting gentle glow on the scene. The pandas face is expressive, showing concentration and joy as it plays. The background includes small, flowing stream and vibrant green foliage, enhancing the peaceful and magical atmosphere of this unique musical performance. Figure 8: Visual comparison of pre-softmax and post-softmax masking on CogVideoX-2B with 66% sparsity. Prompt: detailed wooden toy ship with intricately carved masts and sails is seen gliding smoothly over plush, blue carpet that mimics the waves of the sea. The ships hull is painted rich brown, with tiny windows. The carpet, soft and textured, provides perfect backdrop, resembling an oceanic expanse. Surrounding the ship are various other toys and childrens items, hinting at playful environment. The scene captures the innocence and imagination of childhood, with the toy ships journey symbolizing endless adventures in whimsical, indoor setting. A.4 Visual Comparison for Video Generation We show additional visual (qualitative) comparisons on video generation using the CogVideoX2B [45] model in Figures 9, 10, 11 and 12. For example, in Figure 10, Re-ttention has the best looking otter as well as the food with the most similar shape as the reference video. Besides, while other baseline methods have artifacts like blurry textures and distortions in the background, Re-ttention preserves background fidelity, closely matching the reference video. Those additional comparisons match the experiment result in the main paper: The videos generated by Re-ttention are the most similar to the reference video generated by full-attention; also, it has the best clarity and consistency and no artifacts in the background. A.5 Visual Comparison for Image Generation We show additional visual (qualitative) comparisons on image generation using the PixArt-α [3], PixArt-Σ [2], and Hunyuan [22] models in Figures 13, 14 and 15, respectively. The main object generated by the dynamic sparse attention baseline MInference deviates significantly from the full-attention reference, often resulting in unnatural or distorted appearances. For static baseline methods like SVG and DiTFastAttn, although the main objects in their images are more similar to the full-attention reference images, there are artifacts in the background which degrade the image quality. In comparison, Re-ttention not only preserves the fidelity of the main object but also mitigates background artifacts, demonstrating superior performance in T2I generation and strong generalization across different DiT architectures. 15 Figure 9: T2V visual comparison using CogVideoX-2B [45] T2V model. Each row corresponds to video frames generated by different methods. Prompt: curious sloth hanging from tree branch. Figure 10: T2V visual comparison using CogVideoX-2B [45] T2V model. Each row corresponds to video frames generated by different methods. Prompt: otter on branch while eating. 16 Figure 11: T2V visual comparison using CogVideoX-2B [45] T2V model. Each row corresponds to video frames generated by different methods. Prompt: church interior. Figure 12: T2V visual comparison using CogVideoX-2B [45] T2V model. Each row corresponds to video frames generated by different methods. Prompt: the georgian building. 17 FullAttention SVG (75%) MInference (75%) DiTFastAttn (93.8%) DiTFastAttn (96.9%) Re-ttention (93.8%) Re-ttention (96.9%) Prompt: cat on city street with people. Prompt: small dog sitting on wooden chair. Prompt: man with white shirt and lose tie with messed up hair. Prompt: girl with blue hair is taking self portrait. Prompt: The water the boat is in is reflecting the sun. Prompt: train engine carrying carts down track. Prompt: duck floating on top of body of water. Prompt: there is large book shelf in this living room Figure 13: T2I visual comparison on MS-COCO 2014 [25] dataset using PixArt-α [3] model. Each row corresponds to one prompt, we show images generated by Re-ttention (our method) and by other attention methods in different columns. 18 FullAttention SVG (75%) MInference (75%) DiTFastAttn (93.8%) DiTFastAttn (96.9%) Re-ttention (93.8%) Re-ttention (96.9%) Prompt: black and brown dog sits on white rug with toy donut in front of him. Prompt: couple of zebras that are running through some grass Prompt: teddy bear wearing white shirt and green apron Prompt: person walking across street in the rain. Prompt: an aerial view from planes window of clouds and sunset Prompt: steak topped with an egg and peppers. Prompt: Two asian people pose for picture while sharing drinks at table. Prompt: pretty young lady holding black umbrella. Figure 14: T2I visual comparison on MS-COCO 2014 [25] dataset using PixArt-Σ [2] model. Each row corresponds to one prompt, we show images generated by Re-ttention (our method) and by other attention methods in different columns. 19 FullAttention SVG (75%) MInference (75%) DiTFastAttn (93.8%) DiTFastAttn (96.9%) Re-ttention (93.8%) Re-ttention (96.9%) Prompt: very rusty old car near some pretty flowers. Prompt: An adult polar bear is swimming in the icy water Prompt: man holding baseball bat during baseball game. Prompt: girl with pale skin wearing hoodie holds up toothbrush. Prompt: mountain area with rocks and grass, and large ram standing in the grass. Prompt: small clock sitting in the middle of side walk. Prompt: view of Big Ben from over the water, during the day. Prompt: Light breaks through cloudy day at the pier. Figure 15: T2I visual comparison on MS-COCO 2014 [25] dataset using Hunyuan-DiT [22] model. Each row corresponds to one prompt, we show images generated by Re-ttention (our method) and by other attention methods in different columns."
        }
    ],
    "affiliations": [
        "ECE Department University of Alberta",
        "Huawei Technologies Edmonton, Alberta, Canada"
    ]
}