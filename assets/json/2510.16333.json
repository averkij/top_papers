{
    "paper_title": "RL makes MLLMs see better than SFT",
    "authors": [
        "Junha Song",
        "Sangdoo Yun",
        "Dongyoon Han",
        "Jaegul Choo",
        "Byeongho Heo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "A dominant assumption in Multimodal Language Model (MLLM) research is that its performance is largely inherited from the LLM backbone, given its immense parameter scale and remarkable capabilities. This has created a void in the understanding of the vision encoder, which determines how MLLMs perceive images. The recent shift in MLLM training paradigms, from Supervised Finetuning (SFT) to Reinforcement Learning (RL), magnifies this oversight-namely, the significant lack of analysis on how such training reshapes the vision encoder as well as the MLLM. To address this, we first investigate the impact of training strategies on MLLMs, where RL shows a clear advantage over SFT in strongly vision-related VQA benchmarks. Motivated by this, we conduct a critical yet under-explored analysis of the vision encoder of MLLMs through diverse and in-depth experiments, ranging from ImageNet classification and segmentation to gradient visualization. Our results demonstrate that MLLM's post-training strategy (i.e., SFT or RL) not only leads to distinct outcomes on MLLM downstream tasks, but also fundamentally reshapes MLLM's underlying visual representations. Specifically, the key finding of our study is that RL produces stronger and precisely localized visual representations compared to SFT, boosting the ability of the vision encoder for MLLM. We then reframe our findings into a simple recipe for building strong vision encoders for MLLMs, Preference-Instructed Vision OpTimization (PIVOT). When integrated into MLLMs, a PIVOT-trained vision encoder outperforms even larger and more heavily-trained counterparts, despite requiring less than 1% of the computational cost of standard vision pretraining. This result opens an effective and efficient path for advancing the vision backbones of MLLMs. Project page available at https://june-page.github.io/pivot/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 3 3 3 6 1 . 0 1 5 2 : r a"
        },
        {
            "title": "RL makes MLLMs see better than SFT",
            "content": "Junha Song1,2, Sangdoo Yun2, Dongyoon Han2, Jaegul Choo1, Byeongho Heo2 1KAIST, 2NAVER AI Lab Corresponding author dominant assumption in Multimodal Language Model (MLLM) research is that its performance is largely inherited from the LLM backbone, given its immense parameter scale and remarkable capabilities. This has created void in the understanding of the vision encoder, which determines how MLLMs perceive images. The recent shift in MLLM training paradigms, from Supervised Finetuning (SFT) to Reinforcement Learning (RL), magnifies this oversightnamely, the significant lack of analysis on how such training reshapes the vision encoder as well as the MLLM. To address this, we first investigate the impact of training strategies on MLLMs, where RL shows clear advantage over SFT in strongly vision-related VQA benchmarks. Motivated by this, we conduct critical yet under-explored analysis of the vision encoder of MLLMs through diverse and in-depth experiments, ranging from ImageNet classification and segmentation to gradient visualization. Our results demonstrate that MLLMs post-training strategy (i.e., SFT or RL) not only leads to distinct outcomes on MLLM downstream tasks, but also fundamentally reshapes MLLMs underlying visual representations. Specifically, the key finding of our study is that RL produces stronger and precisely localized visual representations compared to SFT, boosting the ability of the vision encoder for MLLM. We then reframe our findings into simple recipe for building strong vision encoders for MLLMs, Preference-Instructed Vision OpTimization (PIVOT). When integrated into MLLMs, PIVOT-trained vision encoder outperforms even larger and more heavily-trained counterparts, despite requiring less than 1% of the computational cost of standard vision pretraining. This result opens an effective and efficient path for advancing the vision backbones of MLLMs. Date: October 21, 2025 Project page: https://june-page.github.io/pivot/"
        },
        {
            "title": "1 Introduction",
            "content": "Human knowledge is acquired through multiple sensory experiences, with vision playing dominant role in understanding the environment and accumulating knowledge, beyond finding food and avoiding predators (Piaget et al., 1952; Tong et al., 2024a). Inspired by this principle, recent advances in Large Language Models (LLMs) (Dubey et al., 2024; Yang et al., 2025b; Brown et al., 2020) naturally extend toward Multimodal LLMs (MLLMs) (Achiam et al., 2023; Team et al., 2023, 2024a). Especially, large vision language models1 have been recently and preferentially investigated as pathway to foster visual intelligence in LLMs (Liu et al., 2023a; Li et al., 2025a; Chen et al., 2024). The combination of independently pretrained LLMs and vision models enabled MLLMs to reach strong initial capabilities (Mokady et al., 2021; Li et al., 2023). Further advances have been driven by larger and stronger architectures, along with higher-quality datasets, as shown in LLaVA (Liu et al., 2024a; Li et al., 2025a), QwenVL (Bai et al., 2023b), and DINO-MLLM (Fan et al., 2025). Building on this, current research now seeks improvements via reinforcement learning (RL), moving beyond the standard supervised 1Following recent works (Tong et al., 2024a,b; Fan et al., 2025), we refer to LLMs with visual capabilities as MLLMs. 1 finetuning (SFT), paralleling the shift that RL brought to LLMs (Christiano et al., 2017; Ouyang et al., 2022). For instance, several studies demonstrate that incorporating human preference data via RL enhances MLLM performance (Sun et al., 2024a; Wang et al., 2024b) and mitigates hallucination (Yang et al., 2025c; Yu et al., 2024; Fu et al., 2025b). Other research has expanded the scope of RL to include contrastive image pairs (Wang et al., 2024a; Fu et al., 2025a; Xie et al., 2024). Despite the efficacy of RL in the MLLM, comprehensive understanding of its effects compared to SFTand critically, its influence on the vision encoderremains largely absent from the literature. Specifically, the field lacks systematic comparison within MLLMs between SFT for instruction-following and RL for preference alignment, including an analysis of model scaling in common benchmarks. The lack of understanding is especially notable for another under-investigated dimension: the vision encoder. Indeed, research has progressed little beyond the preliminary finding that fine-tuning the vision encoder (Tong et al., 2024a; Li et al., 2024) yields better outcomes than keeping it frozen (Liu et al., 2023a; Li et al., 2023; Driess et al., 2023; Karamcheti et al., 2024). Such oversight can be attributed to an implicit, LLM-centric assumption about the source of MLLM capabilities, leaving significant void in our understanding of how SFT and RL differ in reshaping visual representations. We present timely exploration of both the MLLM and its vision encoder under different training strategies. We focus our RL analysis on Direct Preference Optimization (DPO) for simplicity, which is common recipe for recent MLLMs (Yu et al., 2024; Yang et al., 2025c; Fu et al., 2025a). We begin with fundamental analysis in Section 3, comparing the effects of SFT and RL on MLLMs across broad vision-language (VL) benchmarks. Our analysis reveals that RL yields significant gains on vision-centric tasks, finding that motivates deeper investigation into the vision encoder itself. Subsequently, in Section 4, we conduct unique and critical analysis of the vision encoder, providing key insights for the visual encoder development. Our results reveal that MLLM post-training rewrites the visual representations, with RL driving stronger representation than SFT. The finding is supported by gradient visualizations that trace how optimization signals propagate to the vision encoder. Figure 1 TD;LR. We study how SFT and RL (e.g., DPO) affect not only MLLMs but also their vision encoders, and formulate simple recipe, PIVOT, for evolving vision models for use in MLLM. The foregoing analysis establishes that RL reshapes visual representations, motivating critical question we explore in Section 5: Can RL-trained models surpass SOTA vision models for MLLM?. To this end, we re-formalize RL training as an auxiliary training process for vision encoder, termed PreferenceInstructed Vision OpTimization (PIVOT), and evaluate its efficacy on diverse set of encoders, including CLIP (Radford et al., 2021), DINO (Oquab et al., 2024), and MAE (He et al., 2022). The results reveal remarkable impact of PIVOT when the enhanced encoders are used within MLLMs; vision model trained with PIVOT not only outperforms its original counterpart but also surpasses substantially larger model (e.g., SigLIP2-So/16 + PIVOT > SigLIP2-g/16) and even subsequent-generation encoder (e.g., SigLIP1-So/142 + PIVOT > SigLIP2-So/16). Notably, this enhancement is achieved with just 18 hours of training on 8 H100 GPUs using Qwen2.5-1.5B LLM-head. This amounts to fewer than 1% of GPUs of standard vision pre-training, with SigLIP2 trained on up to 2K TPUv5e chips. Taken together, the evidence indicates that even state-of-the-art encoders have substantial room for MLLM evolution, and PIVOT is promising direction for future exploration."
        },
        {
            "title": "2 MLLMs on RL: Where do we stand?",
            "content": "The initial paradigm for training LLMs involves auto-regressive pre-training followed by SFT to promote instruction-following capabilities (Radford et al., 2018; Dai et al., 2019; Yang et al., 2019; Brown et al., 2020). subsequent breakthrough occurs with Reinforcement Learning from Human Feedback (RLHF), 2We use SigLIP1-So/14, as the weights for SigLIP1-So/16 are not publicly available. 2 which demonstrates that utilizing RL to align LLM outputs with human preferences enables chat-oriented LLMs (Christiano et al., 2017; Ouyang et al., 2022; Touvron et al., 2023b). The use of RL has become cornerstone of modern LLM development, with advanced methods like DPO (Rafailov et al., 2023) and GRPO (Shao et al., 2024) being widely implemented in recent models such as LLaMA-3 (Dubey et al., 2024) and Qwen-2.5 (Yang et al., 2025a). MLLMs have adopted the LLM training advances to leverage prior experiences. Early MLLMs such as LLaVA-Next (Li et al., 2024) and Cambrian (Tong et al., 2024a) combine pre-trained LLM with pre-trained vision model, then align the LLM to vision representation through SFT on vision-language data like captioning and visual question answering. Recent works, as summarized in Table A, demonstrated that applying RL as an auxiliary process can further boost MLLMs downstream performance (Yu et al., 2025; Wang et al., 2024b; Sun et al., 2024a). Other studies have proposed advanced DPO variants for multimodal contexts, for instance by incorporating visual preference data (Fu et al., 2025a; Wang et al., 2024a) or modifying the objective to mitigate hallucinations (Yu et al., 2024; Yang et al., 2025c). Further studies highlight RLs advantages over SFT in adapting an MLLMs knowledge to specialized environments, such as map navigation (Chu et al., 2025) and robot action planning (Li et al., 2025b). These studies reveal clear trend in the application of RLHF to MLLMs. They rely on RL using either PPO (Sun et al., 2024a) or DPO, with the predominant choice becoming DPO (Yu et al., 2024, 2025; Wang et al., 2024a; Yang et al., 2025c; Fu et al., 2025a), as shown in Table A. In line with this trend, our work focuses on DPO as the representative RL method for MLLM research."
        },
        {
            "title": "3 How do SFT and RL affect MLLMs?",
            "content": "Despite the advances of RL described in Section 2, existing studies lack comprehensive analysis, offering limited insight and intuition into following questions: How do SFT and DPO affect MLLM on diverse VQA tasks?, Is DPO actually superior to SFT?, And does this trend hold with model scaling? To address them, we establish controlled training setup and conduct deep investigation."
        },
        {
            "title": "3.1 Experimental setup & prerequisite",
            "content": "Model scaling. The standard MLLM architecture, which integrates an LLM with vision encoder via multimodal projector, has proven effective, achieving superior performance on VL tasks (Lei et al., 2025; Shukor et al., 2025). Our model is implemented using the popular open-source MLLM repository, LLaVA-OneVision (Li et al., 2025a). Following their setup, we conduct study across various cases by adopting four scales of the Qwen2.5 LLM (0.5B, 1.5B, 3B, 7B) (Yang et al., 2025a) and four SigLIP2 384px sizes (B/16, L/16, So/16, g/16) (Tschannen et al., 2025), with 2-layer MLP serving as the projector. Training procedure. Our MLLM development process consists of two stages: Stage 1 pre-training and Stage 2 post-training. In Stage 1, we first align the visual and language embedding spaces by conducting multimodal projector-only training. And then, base MLLM is established by training all model parameters on diverse VL datasets, including Visual Question Answering (VQA), vision-grounded dialogue, and image captioning (Tong et al., 2024a; Li et al., 2025a). Stage 2 indicates post-training, which involves full-parameter update of the base model according to SFT or DPO, detailed below. Post-training strategies. Our analysis compares two post-training approaches: SFT and DPO. Prior works like MPO (Wang et al., 2024b) typically focus on comparing pre-trained model (Stage 1 ) against the same model further trained with DPO, which does not provide fair evaluation of DPO versus SFT. On the other hand, we conduct controlled comparison in Stage 2, using the same number of image-query-response pairs across the two algorithms. Specifically, we define the post-training dataset as XPT = {x0, x1, . . . , xT }, with each element xi = {Ii, qi, yc } representing an image Ii, query qi, and the corresponding chosen and rejected responses yc . The optimization objectives using this dataset is defined as follows: LSFT = EiXPT log πθ(yc Ii, qi); LDPO = EiXPT log σ , yr and yr , (1) Ii,qi) Ii,qi) log πθ(yr log πθ(yc πref(yc Ii,qi) Ii,qi) πref(yr (cid:17)(cid:17) (cid:16) (cid:16) β 3 Figure 2 Scaling the vision encoder in MLLMs. We analyze the impact of the vision encoder sizes, ranging from 86M (B/16) to 1B (g/16) parameters, in Qwen2.5-3B combined with SigLIP2 on visionlanguage benchmarks. Interestingly, DPO yields particularly stronger gains over SFT in vision-intensive VQA. Figure 3 Scaling the language model in MLLMs. Using SigLIP2-So/16 as the vision encoder, we vary the language model size (Qwen2.5) and evaluate performance. Consistent with Figure 2, DPO substantially outperforms SFT on vision-related tasks, while they show comparable results in the Knowledge VQA. where πθ represents the MLLM; πref is the reference model; and β is the temperature controlling the strength of preference alignment. In short, we compare SFT (Stage 2 ) with DPO (Stage 2 ) with the same number of training samples. more detailed description is given in Section D.1. Data & Evaluation. To ensure reproducibility, we utilize publicly available datasets provided in the LLaVAOneVision and MPO repositories. To be more specific, in Stage 1, we apply projector-only pre-training on the LAION/CC/SBU-558K dataset (Liu et al., 2024a) and perform end-to-end pre-training on the LLaVA-OneVision-3.2M dataset (Li et al., 2025a). As the post-training dataset Xpt in Stage 2, we utilize the MPO (Wang et al., 2024b) data and randomly sample 20K instances, scale comparable to recent DPO studies for MLLMs (Yu et al., 2024, 2025; Yang et al., 2025c). It is worth noting that this two-stage strategy and the proportion of training data closely resemble the training paradigm of LLMs such as InstructGPT (Ouyang et al., 2022), where RLHF is applied after instruction-following pre-training. For evaluation, we adapt the benchmark suite introduced in Cambrian (Tong et al., 2024a), which covers 16 tasks across four categories of VQA: General, Knowledge, OCR & Chart, and Vision-Centric. This provides broader and more common comparison than prior studies that mainly focus only on vision (Yang et al., 2025c; Fu et al., 2025b) or specialized tasks (Chu et al., 2025; Shenfeld et al., 2025)."
        },
        {
            "title": "3.2 Analysis and findings",
            "content": "We compare the performance of MLLMs trained with two post-training approaches, SFT and DPO, across different model scales. First, Figure 2 reports results as the vision model, SigLIP2, scales from 86M to 1B, with the language model fixed to Qwen2.5-3B. Next, Figure 3 shows performances as the language model size increases from 0.5B to 7B, while keeping the vision encoder fixed to SigLIP2-So/16. Before comparing SFT and DPO, we analyze the impacts of model scaling on MLLM benchmarks. As shown in Figure 2, the performance improves with the size of the vision encoder, confirming the importance of the visual representation capacity within MLLMs. Replacing SigLIP2-B/16 with SigLIP2-g/16 encoder yields significantly better performance on strongly vision-related tasks. For the DPO-tune MLLM, the gap between the B/16 and g/16 models reaches +4.5%p in Vision-Centric and strikingly +10.6%p in OCR & Chart VQA. In contrast, the improvement is relatively minor at +1.9%p in the weakly vision-related 4 task, Knowledge VQA. These results show that the vision model plays crucial role in vision-related tasks, even though the language model scaling in Figure 3 exhibits large performance gap. Finding 1: Increasing the capacity of the vision encoder in MLLMs is particularly important for tasks requiring fine-grained visual understanding. central focus of our analysis is the comparative efficacy of DPO and SFT for MLLM post-training. The results in Figure 2 show that DPO achieves superior performance compared to SFT, particularly on tasks that require deep visual comprehension rather than those primarily relying on the LLMs knowledge. For instance, on Knowledge VQA benchmarks such as ScienceQA (Lu et al., 2022) and MathVista (Lu et al., 2023), where models rely on scientific or mathematical backgrounds in LLMs, the improvement is only marginal (e.g., +0.3%p). On the other hand, DPOs superiority becomes evident in strongly vision-related benchmarks like OCR & Chart VQA and Vision Centric VQA, including ChartQA (Masry et al., 2022), DocVQA (Mathew et al., 2021), MMVP (Tong et al., 2024b), and CV-bench (Tong et al., 2024a). Quantitatively, with the SigLIP2-L/16, DPO builds model with +4.2%p and +2.4%p higher performance on OCR & Chart VQA and Vision-Centric VQA, respectively. The trend of DPOs superiority holds firm even when scaling the language model, as shown in Figure 3. Even as the language models size increases, the DPO-tuned MLLM consistently surpasses the SFT model, maintaining significant gaps of +3.1%p in OCR & Chart VQA and +4.2%p in Vision-Centric VQA with SigLIP2-g/16. It highlights the superiority of DPO, particularly on tasks requiring detailed visual understanding, and further implies that preference alignment impacts the models visual processing capabilities, beyond the language model. This observation motivates an in-depth analysis of visual representation in MLLMs. Finding 2: Preference alignment (DPO) produces MLLMs with superior performance to SFT, especially on strongly vision-related tasks. As final analysis, we investigate the effect of data scaling on the Stage 2 post-training. The training data is scaled from 3K to 40K, whereas the model sizes are fixed to Qwen2.5-1.5B and SigLIP2-So/16. The results are shown in Figure 4. While SFTs performance improves gradually with more data, DPO achieves high performance rapidly, even with small number of samples (35K). We also observe that DPO-trained model outperforms an SFT-trained counterpart even with data disadvantage. For example, DPO with 3K samples achieves score of 60.4%p, surpassing the 59.5%p score of an SFT model trained on 40K samples. Additional results, including performance on distinct domains, are in Section C.1. Figure 4 Impact of data scales on MLLM tasks."
        },
        {
            "title": "4 How does MLLM training affect Visual representations?",
            "content": "The previous section demonstrates DPOs superiority over SFT on MLLM benchmarks, with impressive gains on vision-related tasks. The finding suggests that DPO impacts not only the language module but also the models visual processing capabilities. Several studies have investigated the vision encoder in MLLMs, focusing primarily on architectural adjustments such as enabling vision encoder updates (Bai et al., 2025; Li et al., 2024), applying all image grids (Li et al., 2025a; Marafioti et al., 2025), and utilizing multiple vision encoders (Tong et al., 2024b,a). In this section, we move beyond these approaches to conduct deeper analysis of the vision encoder within MLLMs. To the best of our knowledge, this is the first work to conduct an in-depth analysis of the vision encoder in MLLMs. 5 Figure 6 ImageNet accuracy of vision encoder. MLLM post-training is conducted with either SFT or DPO, then the vision encoder is detached from LLM and its vision-only performance evaluated via linear probing. We scale the LLM with fixed SigLIP2-So/16 (left), or the vision encoder with fixed Qwen2.5-1.5B (right)."
        },
        {
            "title": "4.1 Experimental setup",
            "content": "We begin with the MLLMs used in Section 3, which are trained with Stage 1 pre-training and either SFT or DPO Stage 2 post-training. After separating the vision components from the MLLM (i.e., detach the vision encoder and projector), we assess their standalone performance on classic vision tasks, including ImageNet classification and semantic segmentation. Performance is measured using image features generated from the vision encoder, or from the combined encoder-projector. In this analysis, we disentangle the impact on the visual representations by isolating the vision encoder from the LLM. More details are available in Section and the source code."
        },
        {
            "title": "4.2 Evaluating vision encoders beyond MLLM Benchmarks",
            "content": "ImageNet Classification. We conduct model scaling experiments on ImageNet classification, performing linear-probe evaluation with the features extracted from the visual components. Note that the features are originally used as the visual token inputs in the MLLM. As shown in Figure 6, our investigation highlights the following key points. (i ) The MLLM posttraining actually reshapes the visual representations. (ii ) DPO consistently outperforms SFT in the vision-only benchmark. DPO outperforms SFT in ImageNet Top-1 accuracy by +1.83%p for SigLIP2-So/16 coupled with Qwen-3B head, and by +1.96%p for SigLIP2-L/16 with Qwen-1.5B head. We claim this as novel finding: DPOa prevalent RL method in the LLM community (Yang et al., 2025b; Dubey et al., 2024)is more effective than SFT, not only for aligning LLMs but also for learning visual representations. (iii ) MLLM training with larger LLMs yields high-performing vision encoder that retains its effectiveness even when separated from the LLM. For instance, when trained on DPO, the SigLIP2-So/16 coupled with 7B LLM exhibits +4.4%p increase in ImageNet accuracy compared to when coupled with 0.5B LLM. It supports the hypothesis that larger-capacity LLMs provide more informative optimization signals to the vision encoder. Figure 5 Impact of data scales on ImageNet. Additionally, we investigate how the data scale of Stage 2 post-training affects visual representations, using the MLLM architecture described in Section 3 (Qwen2.5-1.5B and SigLIP2-So/16). The results in Figure 5 show notable difference from those observed in Figure 4. While performance on MLLM benchmarks improves for both SFT and DPO with more data, only DPO benefits from data scaling in the quality of visual representation. This finding suggests that the choice of MLLM training strategy fundamentally alters how the model sees an image. Finding 3: MLLM training not only adapts the language model but also reshapes the visual representations that determine how the model sees an image. Gradient Visualization. To understand DPOs effectiveness on vision, we investigate how DPO differs in the gradient signals to the vision encoder compared to SFT in the post-training stage. We use Figure 7 Gradient visualization for DPO and SFT. Using Grad-CAM (Selvaraju et al., 2017), we visualize the gradient signals received by the vision encoder features (A := ΦV E(I)) under MLLM post-training. We observe that the gradient signals from DPO align more strongly with question-relevant regions than those from SFT. Figure 8 Segmentation probing results. We evaluate segmentation performance via two-layer MLP probing across 6 encoders, each MLLM-trained with Qwen2.5-1.5B LLM head. The y-axis shows the mean patch-level recall over six random seeds. DPO consistently outperforms over SFT, with the gain shown above the DPO bar. Grad-CAM (Selvaraju et al., 2017): we compute the loss for specific sample xi as defined in Equation (1) and perform backward pass with the sample loss. During the backward pass, we obtain the gradients with respect to the feature activations of the vision encoder, measure the gradient magnitude of each token, and visualize the results. Interestingly, as shown in Figure 7, large gradients primarily occur in question-relevant regions, supporting Finding 3. Moreover, the SFT signal tends to be scattered, while the signal from DPO is precisely focused on semantically relevant regions. We hypothesize that the contrastive nature of the DPO objective enables fine-grained gradient signals for the visual representations when differentiating between chosen and rejected responses. Additional results are available in Section C.4. Image Segmentation. Assuming that DPO enhances the fine-grained training of visual representations, we expect it to be connected with improved localization ability. To measure the localization ability, we perform segmentation probing evaluation with the ADE20K (Zhou et al., 2017) dataset, following the protocol of Covert et al. (2025). First, we utilize MLLM-tuned vision encoders from Section 3. Then, we freeze the vision encoder and attach two-layer MLP, training it as patch-level classifier for segmentation. We utilize various vision encoders, based on CLIP (Radford et al., 2021), SigLIP1 (Zhai et al., 2023), and SigLIP2 (Tschannen et al., 2025), all of which are tuned with either SFT or DPO using Qwen-1.5B LLM. The results in Figure 8 show that the MLLM-tuned vision encoder with DPO consistently outperforms those with SFT on segmentation task; for example, DPO-tuned yields 1.08%p increase in patch-level recall when using CLIP-L/14 336px encoder. The superiority of DPO is also supported by the qualitative results in Figure 9 and Figure F, showing DPO-tuned vision encoders generate segmentation maps with closer alignment with the ground truth. Finding 4: DPO steers the vision encoder toward more fine-grained analysis of visual information, improving its object localization capabilities. Vision & Language alignment. Huh et al. (2024) proposed representation alignment metric to evaluate representation similarity between models trained on different modalities, such as vision and language; 7 Figure 9 Qualitative results of segmentation. We visualize results from probing on the CLIP-L/14 336px encoder, post-trained with SFT and DPO in MLLMs. The DPO-trained vision encoder (VE) yields more accurate segmentation maps that closely align with the ground truth. More results are in Figure F. Figure 10 representational alignment. We measure alignment (Huh et al., 2024) between reference LLMs and vision encoders trained within MLLMs. SigLIP2-So/16, paired with three different LLM scales (x-axis), is trained with DPO or SFT and then used to compute alignment scores against five reference LLMs. typically, larger and stronger vision models show higher alignment with LLMs. We adopt this metric to evaluate the representations of vision encoder. As shown in Figure 10, vision encoders trained with DPO show stronger alignment scores. Additionally, pairing with larger LLM leads to consistently higher alignment scores, which supports our aforementioned hypothesis that larger LLMs transmit more useful signals to the vision encoder during backpropagation. Finding 5: The vision encoder benefits from larger LLM, which provides more informative backward signals for visual representation within an MLLM."
        },
        {
            "title": "5 What’s next: Unlocking vision model potential via RL",
            "content": "Our analysis has shown that training vision model with an LLM via DPO builds more fine-grained visual representations than SFT. We now reframe this training process into an effective strategy for evolving vision models, which we term Preference-Instructed Vision OpTimization (PIVOT). In this section, we apply PIVOT to existing vision models that are widely adopted as vision encoders in MLLMs. These include encoders pretrained with image-language supervision3 (e.g., CLIP and SigLIP) or with vision-only self-supervision (e.g., MAE (He et al., 2022) and DINOv2 (Oquab et al., 2024)). Our objective is to investigate how much these vision models can be improved by PIVOT for use in MLLM. 3Following Cambrian (Tong et al., 2024a), we consider CLIP training as strongly supervised, as language provides richer supervision than class labels."
        },
        {
            "title": "5.1 Experimental setup",
            "content": "The process begins with vision encoder commonly used in MLLMs, such as CLIP and SigLIP1. The encoder is attached to an LLM and optimized through both pre-training and post-training with DPO or SFTon 3M instruction-following samples and 20K preference pairs, as described in Section 3.1. We refer to this training procedure as PIVOT. Afterward, the vision encoder is detached from the LLM, its weights are frozen, and the resulting model is termed the PIVOT-enhanced encoder. We evaluate the performance of PIVOT-enhanced encoder by combining it with Qwen2.5-1.5B and build an MLLM. The combined model is optimized with projector-only pretraining on LAION/CC/SBU-558K (Liu et al., 2024a), followed by instruction finetuning of the projector and LLM on Cambrians 737K dataset. This design allows us to isolate the encoders capability and assess the effectiveness of PIVOT representations within MLLMs. Note that we follow the same evaluation protocol as prior works such as Cambrian (Tong et al., 2024a), DINO-MLLM (Fan et al., 2025), and MLLM-data (Han et al., 2025), which has been demonstrated to allows us to study visual representations efficiently. More details are in Figure D. The idea of PIVOT is simple yet effective: training vision models with LLM-head using DPO. We highlight the contributions of PIVOT: (i ) positioning PIVOT not as new method, but as an under-explored training regime. (ii ) showing that it can develop significantly better MLLMs than those using original vision models, revealing substantial room for improvement in state-of-the-art vision models. (iii ) presenting the first evidence that DPO reshapes visual features with more positive effects than SFT on standard vision benchmarks as well as on multimodal tasks."
        },
        {
            "title": "5.2 Results",
            "content": "Figure 11 Comparing CLIP (Radford et al., 2021) and PIVOT training. They both utilize language models and vision-language data. The results are presented in Table 1. In the following, we describe the main comparisons in detail. SigLIP1 SigLIP2. We compare an MLLM using the original SigLIP2 encoder against PIVOT-enhanced SigLIP1. SigLIP2 is more recent model, developed with substantially larger datasets and an advanced training scheme compared to its predecessor. An MLLM leveraging the SigLIP2-So/16 encoder achieves an average VQA score of 52.4%p. However, by enhancing SigLIP1-So/14 with the PIVOT process, we obtain an MLLM that achieves an average VQA score of 53.2%p, surpassing those with SigLIP2-So/16. SigLIP2So/16 SigLIP2-g/16. SigLIP2-g/16 is considered to have the strongest representations in its family due to its large scale. We compare its MLLM performance against PIVOT-enhanced SigLIP2So/16. Despite having 2.5 times fewer parameters, the So/16 model outperforms the g/16 model, achieving score of 55.6%p versus 53.9%p. This shows the considerable potential for enhancing popular vision backbones for optimal performance within MLLMs. DPO vs. SFT on PIVOT. In Section 4, we show that DPO during post-training benefits even vision encoders within MLLM. Similarly, vision encoder enhanced by DPO (i.e., PIVOT) provides 1.3%p advantage over one enhanced with SFT (56.7%p vs. 55.4%p) in the MLLM application when using SigLIP2-g/16. Here, SFT can be seen as similar to the language alignment of (Bolya et al., 2025). This result indicates that DPOs advantage over SFT continues in the context of PIVOT. Thus, we adopt DPO as the default choice for PIVOT. Classic vision encoders + PIVOT. We investigate the effect of PIVOT on diverse vision encoders and find that all five models improve MLLM performance. An interesting observation is that this improvement holds not only for vision-only self-supervised models such as MAE (He et al., 2022) and MOCO (He et al., 2020), but also for the supervised encoder (Dosovitskiy et al., 2021) trained solely with an image classification loss on the ImageNet dataset. Model ensemble. The idea of model ensemble utilizing multiple vision encoders for single MLLM has been explored in prior works (Tong et al., 2024b,a). The experiments show that combining SigLIP19 Evolving vision encoder for MLLM applications MLLM combining the vision encoder with Qwen2.5-1.5B Model SigLIP1-So/14 (2023) + SFT + PIVOT SigLIP2-So/16 (2025) + SFT + PIVOT SigLIP2-g/16 (2025) + SFT + PIVOT # Params # Samples seen Average (All) General OCR&Chart Vision-Cent. Knowledge 400M 400M 1000M 30B 30B + 0.003B 30B + 0.003B 40B 40B + 0.003B 40B + 0.003B 40B 40B + 0.003B 40B + 0.003B 50.9 52.2 53.2 52.4 54.6 55.6 53.9 55.4 56. 65.4 66.5 67.7 66.2 66.9 68.1 66.5 67.4 68.5 42.3 45.2 46.8 46.6 52.2 53.9 50.8 52.8 54.7 49.8 50.8 51.7 50.6 51.7 52.4 51.9 53.1 54.2 46.0 46.3 46.6 46.1 47.7 48.1 46.4 48.5 49.3 Classical vision encoders Model CLIP-L/14 336px (2021) + PIVOT DINOv2-g/14 378px (2024) + PIVOT MAE-H/14 224px (2022) + PIVOT MOCO-B/16 224px (2020) + PIVOT SupViT-H/14 224px (2021) + PIVOT # Params # Samples seen Average (All) General OCR&Chart Vision-Cent. Knowledge 303M 1000M 632M 86M 632M 32B 32B + 0.003B 2B 2B + 0.003B 2B 2B + 0.003B 1.4B 1.4B + 0.003B N/A N/A 46.3 49.5 40.9 43.6 36.8 39.7 35.3 37.5 35.5 37.7 62.1 64.6 58.4 62.1 47.6 52.5 42.5 47.4 44.6 47.3 35.1 37.8 17.6 18.7 17.3 18.2 17.1 17.6 17.2 18.1 43.0 48.6 45.1 49.2 40.2 43.3 39.6 41.0 38.2 40.3 45.0 47.1 42.6 44.3 42.0 44.6 42.1 44.1 42.1 45. Model ensemble (Tong et al., 2024b) Model SigLIP1-So/14 + DINOv2-g/14 378px SigLIP1-So/14 + ConvNeXt-XXL SigLIP1-So/14+PIVOT + ConvNeXt-XXL # Params 700M 1.25B 1.25B Average (All) General OCR&Chart Vision-Cent. Knowledge 41.5 44.6 48.5 46.5 49.1 52.5 45.1 45.9 46. 64.5 65.9 67.3 49.4 51.4 53.6 Table 1 Influence of PIVOT on existing vision models. We apply PIVOT to reveal the potential for improving existing vision models for MLLMs. Following the setup in Section 3.1, vision model is trained with Qwen2.5-1.5B LLM-head on 3M samples, and then finetuned with either SFT (+ SFT) or DPO (+ PIVOT) on 20K data. # samples seen refers number samples used for whole training as in Cherti et al. (2023); Zhai et al. (2023). So/14 and ConvNeXt-XXL increases the average score from 50.9%p to 51.4%p (+0.5%p), although it requires greater number of parameters. We show that SigLIP1-So/14+ PIVOT alone achieves superior score of 53.2%p without increasing parameters. Furthermore, combining this SigLIP1+ PIVOT with ConvNeXt-XXL results in an additional performance gain, reaching score of 53.6%p. Finding 6: Existing vision models possess substantial potential for improvement within MLLMs, which can be unlocked by PIVOT. We provide additional experimental results in Section B.2, including the impact of training data scale and different usage strategies for the PIVOT-enhanced projector."
        },
        {
            "title": "6 Conclusion & Broader impact",
            "content": "In this work, we investigated the differential impacts of SFT and RL on both MLLMs and their vision encoders. Our experiments first demonstrated that DPO, form of RL, achieves superior MLLM performance over SFT, particularly on tasks requiring detailed visual comprehension. subsequent, focused analysis of the vision encoder revealed that DPO induces stronger and more localized visual features. We then consolidated these findings into PIVOT, practical recipe, and validated its efficacy across diverse range of vision encoders. We hope this research contributes to the broader goal of enabling MLLMs to better perceive and interpret visual information. Broader Impact. Various future directions can be explored based on our study. One extension is to investigate how alternative MLLM training techniques, such as the modified or image-wise DPO variants in Section 2, distinctly impact the vision encoder. Moreover, examining how other RL algorithms, like PPO (Schulman et al., 2017) and GRPO (Shao et al., 2024), alter visual representations would be an insightful follow-up."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1, 18 Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko, and Daniele Calandriello. general theoretical paradigm to understand learning from human preferences. In International Conference on Artificial Intelligence and Statistics. PMLR, 2024. 18 Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023a. 18, 24 Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023b. 1, 18 Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 5, 18 Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. In ICLR, 2022. 18 Daniel Bolya, Po-Yao Huang, Peize Sun, Jang Hyun Cho, Andrea Madotto, Chen Wei, Tengyu Ma, Jiale Zhi, Jathushan Rajasegaran, Hanoona Rasheed, Junke Wang, Marco Monteiro, Hu Xu, Shiyu Dong, Nikhila Ravi, Daniel Li, Piotr Dollár, and Christoph Feichtenhofer. Perception encoder: The best visual embeddings are not at the output of the network, 2025. URL https://arxiv.org/abs/2504.13181. 9, 19 Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. NeurIPS, 2020. 1, 2, 18 Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In NeurIPS, 2020. 18 Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021. 18 Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In ICML, 2020. 18 Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In CVPR, 2021. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In CVPR, 2024. 1, 24 Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. In CVPR, 2023. 10 Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. In NeuIPS, 2017. 2, 3, 18 Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training. In ICML, 2025. 3, 4, 18, 19, 20 Ian Covert, Tony Sun, James Zou, and Tatsunori Hashimoto. Locality alignment improves vision-language models. In ICLR, 2025. 7, 26 Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond fixed-length context. In ACL, 2019. 2, 18 Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter 11 of the association for computational linguistics: human language technologies, volume 1 (long and short papers), 2019. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 9, 10 Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, et al. Palm-e: An embodied multimodal language model. In ICML, 2023. 2 Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pp. arXiv2407, 2024. 1, 3, 6, 24, 27 Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization. In ICML, 2024. 18 David Fan, Shengbang Tong, Jiachen Zhu, Koustuv Sinha, Zhuang Liu, Xinlei Chen, Michael Rabbat, Nicolas Ballas, Yann LeCun, Amir Bar, et al. Scaling language-free visual representation learning. In ICCV, 2025. 1, 9, 21, Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. corr abs/2306.13394 (2023), 2023. 24 Jinlan Fu, Shenzhen Huangfu, Hao Fei, Xiaoyu Shen, Bryan Hooi, Xipeng Qiu, and See-Kiong Ng. Chip: Cross-modal hierarchical direct preference optimization for multimodal llms. In ICLR, 2025a. 2, 3, 18, 19 Yuhan Fu, Ruobing Xie, Xingwu Sun, Zhanhui Kang, and Xirong Li. Mitigating hallucination in multimodal large language model via hallucination-targeted direct preference optimization. In ACL, 2025b. 2, 4, 18, 19 Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying Shan. Planting seed of vision in large language model. arXiv preprint arXiv:2307.08041, 2023. 24 Junlin Han, Shengbang Tong, David Fan, Yufan Ren, Koustuv Sinha, Philip Torr, and Filippos Kokkinos. Learning to see before seeing: Demystifying llm visual priors from language pre-training. arXiv preprint arXiv:2509.26625, 2025. 9, 18, 21, Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, 2020. 9, 10, 18 Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, 2022. 2, 8, 9, 10, 18 Tuomo Hiippala, Malihe Alikhani, Jonas Haverinen, Timo Kalliokoski, Evanfiya Logacheva, Serafina Orekhova, Aino Tuomainen, Matthew Stone, and John Bateman. Ai2d-rst: multimodal corpus of 1000 primary school science diagrams. Language Resources and Evaluation, 55:661688, 2021. 24 Jiwoo Hong, Noah Lee, and James Thorne. Orpo: Monolithic preference optimization without reference model. EMNLP, 2024. 18 Drew A. Hudson and Christopher D. Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In CVPR, 2019. 24 Minyoung Huh, Brian Cheung, Tongzhou Wang, and Phillip Isola. The platonic representation hypothesis. In ICML, 2024. 7, 8 Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, et al. Perceiver io: general architecture for structured inputs & outputs. In ICLR, 2022. Gueyoung Jung, Matti Hiltunen, Kaustubh Joshi, Richard Schlichting, and Calton Pu. Mistral: Dynamically managing power, performance, and adaptation cost in cloud infrastructures. In 2010 IEEE 30th International Conference on Distributed Computing Systems, pp. 6273. IEEE, 2010. 27 12 Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, and Dorsa Sadigh. Prismatic vlms: Investigating the design space of visually-conditioned language models. In ICML, 2024. 2 Weixian Lei, Jiacong Wang, Haochen Wang, Xiangtai Li, Jun Hao Liew, Jiashi Feng, and Zilong Huang. The scalability of simplicity: Empirical analysis of vision-language learning with single transformer. arXiv preprint arXiv:2504.10462, 2025. 3 Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In ACL, 2020. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. TMLR, 2025a. 1, 3, 4, 5, 18, 19, 21, 24 Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-nextinterleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024. 2, 3, 5, 18 Haozhan Li, Yuxin Zuo, Jiale Yu, Yuhao Zhang, Zhaohui Yang, Kaiyan Zhang, Xuekai Zhu, Yuchen Zhang, Tianxing Chen, Ganqu Cui, Dehui Wang, Dingxiang Luo, Yuchen Fan, Youbang Sun, Jia Zeng, Jiangmiao Pang, Shanghang Zhang, Yu Wang, Yao Mu, Bowen Zhou, and Ning Ding. Simplevla-rl: Scaling vla training via reinforcement learning. arXiv preprint arXiv:2509.09674, 2025b. 3, 18, 19, 20 Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, 2023. 1, 2, Yuan-Hong Liao, Sven Elflein, Liu He, Laura Leal-Taixé, Yejin Choi, Sanja Fidler, and David Acuna. Longperceptualthoughts: Distilling system-2 reasoning for system-1 perception. arXiv preprint arXiv:2504.15362, 2025. 19 Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023a. 1, 2, 18 Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR, 2024a. 1, 4, 9 Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In ECCV, 2024b. Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin Huang, Dezhi Peng, Mingyu Liu, Mingrui Chen, Chunyuan Li, Lianwen Jin, et al. On the hidden mystery of ocr in large multimodal models. arXiv preprint arXiv:2305.07895, 2023b. 24 Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In NeurIPS, 2022. 5, 24 Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In ICLR, 2023. 5, 24 Andrés Marafioti, Orr Zohar, Miquel Farré, Merve Noyan, Elie Bakouch, Pedro Cuenca, Cyril Zakka, Loubna Ben Allal, Anton Lozhkov, Nouamane Tazi, et al. Smolvlm: Redefining small and efficient multimodal models. arXiv preprint arXiv:2504.05299, 2025. 5 Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. In ACL, 2022. 5, Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In WACV, 2021. 5, 23, 24 Kohsei Matsutani, Shota Takashiro, Gouki Minegishi, Takeshi Kojima, Yusuke Iwasawa, and Yutaka Matsuo. Rl squeezes, sft expands: comparative study of reasoning llms. arXiv preprint arXiv:2509.21128, 2025. 18 Ron Mokady, Amir Hertz, and Amit Bermano. Clipcap: Clip prefix for image captioning. arXiv preprint arXiv:2111.09734, 2021. 1 Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, TL Scao, Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. Crosslingual generalization through multitask finetuning (2022). arXiv preprint arXiv:2211.01786, 1786. 27 Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. DINOv2: Learning robust visual features without supervision. TMLR, 2024. 2, 8, 10 Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. In NeuIPS, 2022. 2, 3, 4, 18 Jean Piaget, Margaret Cook, et al. The origins of intelligence in children, volume 8. International Universities Press New York, 1952. 1 Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018. 2, 18, 23 Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 2, 7, 9, 10, 18 Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In NeuIPS, 2023. 3, 18, 19, 23 Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 2020. 18 John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 10, Ramprasaath Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In ICCV, 2017. 7, 28 Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. arXiv preprint arXiv:2402.03300, 2024. 3, 10, 18 Idan Shenfeld, Jyothish Pari, and Pulkit Agrawal. Rls razor: Why online reinforcement learning forgets less. arXiv preprint arXiv:2509.04259, 2025. 4, 18, 19, 20 Mustafa Shukor, Enrico Fini, Victor Guilherme Turrisi da Costa, Matthieu Cord, Joshua Susskind, and Alaaeldin El-Nouby. Scaling laws for native multimodal models. In ICCV, 2025. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In CVPR, 2019. 24 Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale. arXiv preprint arXiv:2303.15389, 2023. 19 Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. In ACL, 2024a. 2, 3, 18, 19 Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. In ACL, 2024b. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 1 Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024a. 1, 18 14 Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024b. 24, 27 Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. NeurIPS, 2024a. 1, 2, 3, 4, 5, 8, 9, 18, 21, 23, 24, 27 Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In CVPR, 2024b. 1, 5, 9, 10, 24 Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. 18, 23 Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. 3, 18 Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilingual visionlanguage encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. 3, 7, 10, 19 Fei Wang, Wenxuan Zhou, James Huang, Nan Xu, Sheng Zhang, Hoifung Poon, and Muhao Chen. mdpo: Conditional preference optimization for multimodal large language models. In EMNLP, 2024a. 2, 3, 18, 19 Junyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang, Yukai Gu, Haitao Jia, Jiaqi Wang, Haiyang Xu, Ming Yan, Ji Zhang, et al. Amber: An llm-free multi-dimensional benchmark for mllms hallucination evaluation. arXiv preprint arXiv:2311.07397, 2023. Weiyun Wang, Zhe Chen, Wenhai Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Jinguo Zhu, Xizhou Zhu, Lewei Lu, Yu Qiao, et al. Enhancing the reasoning ability of multimodal large language models via mixed preference optimization. arXiv preprint arXiv:2411.10442, 2024b. 2, 3, 4, 18, 19, 25 xAI. grok, 2024. URL https://x.ai/blog/grok-1.5v. 24 Yuxi Xie, Guanzhen Li, Xiao Xu, and Min-Yen Kan. V-dpo: Mitigating hallucination in large vision language models via vision-guided direct preference optimization. In EMNLP, 2024. 2, 18, 19 Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, and Chunyuan Li. Llava-critic: Learning to evaluate multimodal models. In CVPR, 2025. 18, An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2025a. 3, 24 An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025b. 1, 6 Zhihe Yang, Xufang Luo, Dongqi Han, Yunjian Xu, and Dongsheng Li. Mitigating hallucinations in large vision-language models via dpo: On-policy data hold the key. In CVPR, 2025c. 2, 3, 4, 18, Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ Salakhutdinov, and Quoc Le. Xlnet: Generalized autoregressive pretraining for language understanding. In NeurIPS, 2019. 2 Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, et al. Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback. In CVPR, 2024. 2, 3, 4, 18, 19 Tianyu Yu, Haoye Zhang, Qiming Li, Qixin Xu, Yuan Yao, Da Chen, Xiaoman Lu, Ganqu Cui, Yunkai Dang, Taiwen He, et al. Rlaif-v: Open-source ai feedback leads to super gpt-4v trustworthiness. In CVPR, 2025. 3, 4, 18, 19 Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In CVPR, 2024. 24 Fatemeh Pesaran Zadeh, Yoojin Oh, and Gunhee Kim. Lpoi: Listwise preference optimization for vision language models. In ACL, 2025. 18, 19 Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In ICCV, 2023. 7, 10, 19 Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Lmms-eval: Reality check on the evaluation of large multimodal models, 2024. URL https://arxiv.org/abs/2407.12772. 23 Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In CVPR, 2017. 7,"
        },
        {
            "title": "A Related Work",
            "content": "A.1 MLLMs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 LLMs with RL. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 MLLMs with RL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.4 Vision-centric pre-trainings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "B Additional Analysis and Findings",
            "content": "B.1 DPO and SFT under new data distributions . . . . . . . . . . . . . . . . . . . . . . . . B.2 Ablation study on PIVOT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Additional Experiment Results C.1 Scaling training data in MLLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Performance of MLLMs on all benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . C.3 ImageNet classification with vision encoder . . . . . . . . . . . . . . . . . . . . . . . C.4 Gradient visualization of vision encoder . . . . . . . . . . . . . . . . . . . . . . . . . C.5 Segmentation probing with vision encoder . . . . . . . . . . . . . . . . . . . . . . . . Additional Explanations D.1 SFT and DPO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Evaluation benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Discussions E.1 Low performance gap on Knowledge VQA . . . . . . . . . . . . . . . . . . . . . . . . . E.2 Future work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "F Experimental Details",
            "content": "F.1 Pre-training & Post-training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.2 ImageNet classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.3 Grad-CAM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.4 Semantic segmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.5 Representation alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.6 PIVOT-enhanced vision model evaluation . . . . . . . . . . . . . . . . . . . . . . . . . 17 18 18 18 18 19 19 20 22 22 22 23 23 23 23 24 24 24 24 26 26 26"
        },
        {
            "title": "A Related Work",
            "content": "A.1 MLLMs. Building on the success of LLMs, the development of MLLMs has become prominent research direction for equipping LLMs with visual understanding (Team et al., 2024a; Achiam et al., 2023; Bai et al., 2023b). The standard paradigm involves connecting pretrained vision encoder to an LLM via multimodal projector, creating strong baseline (Liu et al., 2023a; Li et al., 2023; Jaegle et al., 2022). Subsequent advancements have been achieved by employing larger components (Li et al., 2024) or by training on higher-quality conversational data (Li et al., 2025a; Bai et al., 2025; Han et al., 2025). The dominant training strategy for these models has been SFT (Touvron et al., 2023a; Brown et al., 2020; Dai et al., 2019), where the model learns to generate ground-truth response for given visual input and query. As noted in Cambrian (Tong et al., 2024a), while SFT has been effective, RL is emerging as promising alternative to potentially surpass the performance ceilings of current methods. A.2 LLMs with RL. Following the development of various Transformer-based language models (Raffel et al., 2020; Lewis et al., 2020; Radford et al., 2018; Touvron et al., 2023a; Bai et al., 2023a), trained with objectives such as masked modeling (Devlin et al., 2019) and SFT, major breakthrough was achieved by aligning LLMs with human preferences through RLHF (Christiano et al., 2017; Ouyang et al., 2022; Touvron et al., 2023b). The foundational method involved using PPO (Schulman et al., 2017) to optimize an SFT model against reward model trained on preference data. This paradigm has since evolved: DPO (Rafailov et al., 2023) directly instills preference alignment by optimizing on pairwise preferences, and GRPO (Shao et al., 2024) updates the policy using group-wise relative rankings of candidate responses. This line of research, which also includes methods like IPO (Azar et al., 2024), KTO (Ethayarajh et al., 2024), and ORPO (Hong et al., 2024), has consistently demonstrated the power of RL. Whereas prior works, RLs Razor (Shenfeld et al., 2025) and RL-Squeezes (Matsutani et al., 2025), compared RL and SFT in the context of LLM adaptation to new tasks, we conduct parallel investigation into how these distinct trainings impact MLLMs. A.3 MLLMs with RL The MLLM field is increasingly adopting RL to push beyond the limitations of SFT, mirroring the evolution of LLMs. We provide comprehensive list in Table A. Several studies (Yu et al., 2025; Xiong et al., 2025), including LLaVA-RLHF (Sun et al., 2024a) and MPO (Wang et al., 2024b), have reported that applying additional preference alignment to an SFT-trained MLLM can boost its performance. In parallel, other works have proposed DPO extensions for MLLMs: RLHF-V (Yu et al., 2024), OPA-DPO (Yang et al., 2025c), and HDPO (Fu et al., 2025b). These approaches reweight token-level losses on disagreement tokens between the chosen and rejected responses, or combine SFT with DPO for joint training. Some studies (Zadeh et al., 2025; Xie et al., 2024), such as CHiP (Fu et al., 2025a) and mDPO (Wang et al., 2024a), have shown that incorporating visual preference data reduces perceptual errors in MLLMs. Finally, Chu et al. (2025) and Li et al. (2025b) have indicated that RL is advantageous for adapting MLLMs inherent knowledge to special environments, like card games, map navigation, or robot action planning. Our work conducts controlled comparison between SFT and DPO (Section 3.1) and, unlike RL-vs.-SFT studies, evaluates on common benchmarks rather than specialized settings. A.4 Vision-centric pre-trainings The pretraining of vision models has largely followed two paths: image-only self-supervised learning and image-language supervised learning. The former, encompassing contrastive (He et al., 2020; Chen et al., 2020; Caron et al., 2020; Chen & He, 2021; Caron et al., 2021) learning and masked-imagemodeling (Bao et al., 2022; He et al., 2022), has proven effective for creating visual representation models for classic vision tasks like image classification and segmentation. The latter, as in CLIP (Radford et al., 18 Abbreviation RLHF-V (Yu et al., 2024) RLAIF-V (Yu et al., 2025) LLaVA-RLHF (Sun et al., 2024a) LLaVA-Critic (Xiong et al., 2025) OPA-DPO (Yang et al., 2025c) HDPO (Fu et al., 2025b) CHiP (Fu et al., 2025a) mDPO (Wang et al., 2024a) LPOI (Zadeh et al., 2025) V-DPO (Xie et al., 2024) MPO (Wang et al., 2024b) RL Generalizes (Chu et al., 2025) SimpleVLA-RL (Li et al., 2025b) Title Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback Open-Source AI Feedback Leads to Super GPT-4V Trustworthiness Aligning Large Multimodal Models with Factually Augmented RLHF Learning to Evaluate Multimodal Models Mitigating Hallucinations in Large Vision-Language Models via DPO: On-Policy Data Hold the Key Mitigating Hallucination in Multimodal Large Language Model via Hallucination-targeted Direct Preference Optimization Cross-modal Hierarchical Direct Preference Optimization for Multimodal LLMs Conditional Preference Optimization for Multimodal Large Language Models Listwise Preference Optimization for Vision Language Models Mitigating Hallucination in Large Vision Language Models via Vision-Guided Direct Preference Optimization Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization SFT Memorizes, RL Generalizes: Comparative Study of Foundation Model Post-training SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning Venue CVPR Year RL DPO CVPR 2025 DPO ACL PPO CVPR CVPR 2025 2025 DPO DPO ACL DPO ICLR 2025 DPO EMNLP DPO ACL 2025 DPO EMNLP DPO arXiv 2024 DPO ICML PPO arXiv 2025 GRPO LongPerceptualThoughts (Liao et al., 2025) LongPerceptualThoughts: Distilling System-2 arXiv DPO Reasoning for System-1 Perception Table List of RL-based MLLM works. We provide an overview of methods with their venues, years, and RL optimization strategies, and note that most of the previous studies have adopted DPO (Rafailov et al., 2023) as one of their RL baselines. 2021), SigLIP (Zhai et al., 2023; Tschannen et al., 2025), and EvaCLIP (Sun et al., 2023), aligns vision and language, enabling strong zero-shot recognition and making these models popular backbones for MLLMs (Li et al., 2025a). Our PIVOT is CLIP-style alternative for training vision encoders, as both use language-aligned supervision (Figure 11). Applied to existing encoders, it evolve into MLLM-ready encoders with <1% of the GPUs and data relative to SigLIP2 training. Recently, Perception Encoder (Bolya et al., 2025) explored improved recipes for building powerful vision encoders through vision-language pre-training. Its language alignment stage follows strategy similar to the + SFT setting in Table 1. Unlike their focus on SFT-driven representation changes, we investigate how RL training influences vision representations."
        },
        {
            "title": "B Additional Analysis and Findings",
            "content": "B.1 DPO and SFT under new data distributions Motivation. Previous studies, including RLgeneralize (Chu et al., 2025), SimpleVLA-RL (Li et al., 2025b), and RL-Razor (Shenfeld et al., 2025), have posited that RL is beneficial for adapting to new data distributions, mitigating performance degradation and catastrophic forgetting. Unfortunately, they either focused on specialized environments, such as card gaming and robot action planning, or conducted evaluations confined to the knowledge domain like mathematics. Hence, we examine how our MLLMs behave on more common VQA benchmarks when the Stage 2 post-training data distribution differs from that of Stage 1 pre-training. Experimental setup. The LLaVA-OneVision samples predominantly contain short answers with fewer than 50 tokens and lack special tokens such as <think>. . . </think> and <review>. . . </review>. In contrast, the MMPR samples occasionally include longer responses and diverse annotation patterns. We 19 Figure MLLM training under new data distribution. We post-train (i.e., Stage 2 ) an MLLM (Qwen2.5-3B + SigLIP2-So/16) under varying proportions of samples from shifted distribution (0%, 50%, 80%). DPO remains stable, while SFT shows substantial declines, particularly on general and vision-centric VQA benchmarks. Evolving vision encoder MLLM combining the vision encoder with Qwen2.5 Vision encoder SigLIP2-So/16+PIVOT SigLIP2-So/16+PIVOT SigLIP2-So/16+PIVOT SigLIP2-So/16+PIVOT SigLIP2-So/16+PIVOT SigLIP2-So/16+PIVOT # Params PIVOT-proj. 400M 400M 400M 400M 400M 400M 0 2 1 0 2 1 LLM Qwen2.5-0.5B Qwen2.5-0.5B Qwen2.5-0.5B Qwen2.5-1.5B Qwen2.5-1.5B Qwen2.5-1.5B Add. layer Total layer Avg. (All) General. OCR&Chart. Vision-Cen. Knowledge. 2 2 1 2 2 2 4 2 2 4 2 42.9 44.3 45.2 52.4 54.3 54.6 56.3 56.5 57.8 66.2 66.4 66.7 39.1 39.8 39.5 46.1 48.2 47.1 37.9 41.4 43.4 46.6 49.7 50.8 38.3 39.4 40.3 50.6 52.9 54. PIVOT-projector 0, 1, 2 denote configurations that Table Ablation on reusing the PIVOT-trained projector. reuse none, only the first layer, or two layers of the frozen PIVOT-trained projector, respectively. Additional trainable layers (Stage 3 -projector) are appended before the LLM to match dimensionality. Among these, the 1+1 setupreusing the first frozen layer with one new layerachieves the best downstream MLLM performance during the final Stage 3 in Figure D. exploit this discrepancy by constructing new Stage 2 post-training dataset based on MMPR. Specifically, we sample 20K instances from MMPR, where fraction r% (0%, 50%, or 80%) consists of samples that either exceed 100 tokens or contain special tokens. The remaining 100 r% of the dataset is randomly sampled from the rest of MMPR following our original setup. Results. The results, shown in Figure A, reveal that DPO maintains robust performance even as the proportion of new-distribution samples increases. In contrast, SFT-trained MLLMs experience sharp decline: while achieving 62.2%p with 0% of new-distribution samples, their performance drops to 53.2%p when the ratio increases to 80%. The degradation is especially pronounced in vision-centric VQA tasks, where the accuracy gap between DPO and SFT reaches 17.9%p with 80% new-distribution samples. It demonstrates that the trends observed in earlier RL studies (Chu et al., 2025; Li et al., 2025b; Shenfeld et al., 2025) also generalize across the broad set of 16 benchmarks considered in our evaluation. B.2 Ablation study on PIVOT Beyond the results in Section 5, we perform further experiments to gain deeper insights into PIVOT. PIVOT-enhanced projector. (setup) There are two components responsible for visual representation in an MLLM: the vision encoder and the projector. In Section 5, we examined how the vision encoder trained through Stage 1 and Stage 2 operates in Stage 3 of Figure D. In this section, we extend the analysis to the projector, investigating whether reusing the PIVOT-tuned projector benefits the model under the same setting. The PIVOT-tuned projector obtained after Stage 2 follows our standard MLLM architecture, consisting of two-layer MLP. In Stage 3, we vary which part of this projector is reused, and denote the configurations as PIVOT-tuned 0, 1, and 2, corresponding to using none, only the first linear layer, or the entire two-layer MLP, respectively. In Stage 3, the PIVOT-tuned vision encoder and projector must be connected to new LLM. To enable this, we introduce new set of MLP layers, referred to as the Stage 3-projector. We vary the number of layers in this module, and define the total number of multimodal linear layers as the sum of those from the PIVOT-tuned projector and the Stage 3-projector. For example, PIVOT projector 2, Add. layer 2 indicates that the two frozen layers from the PIVOT-tuned projector are reused, while two additional randomly initialized layers are appended in the Stage 3-projector, resulting in total of four layers. Evolving vision encoder Vision encoder # Params SigLIP2-So/16 +PIVOT LLM Data 400M Qwen2.5-1.5B Cambrian-737K 400M Qwen2.5-1.5B Cambrian-737K SigLIP2-So/16 +PIVOT 400M Qwen2.5-1.5B LLaVA-OV-3M 400M Qwen2.5-1.5B LLaVA-OV-3M SigLIP2-So/16 +PIVOT 400M Qwen2.5-0.5B LLaVA-OV-3M 400M Qwen2.5-0.5B LLaVA-OV-3M MLLM combining the vision encoder with Qwen2.5 Avg. (All) 52.4 55.6 +3.2 56.9 59.2 +2.3 49.0 50.6 +1.6 General. OCR&Chart. Vision-Cen. Knowledge. 66.2 68.1 +1.9 67.9 68.9 +1.0 58.8 59.9 +1. 46.6 53.9 +7.3 56.4 59.8 +3.4 47.2 51.0 +3.8 50.6 52.4 +1.8 51.3 54.6 +3.3 45.2 46.5 +1.3 46.1 48.1 +2.0 52.0 53.5 +1.5 44.6 45.1 +0.5 Table Effect of larger Stage 3 data on PIVOT performance. Comparison between basic and PIVOT-enhanced SigLIP2-So/16 encoders paired with Qwen2.5, trained on either Cambrian-737K (Tong et al., 2024a) or LLaVAOV-3M (Li et al., 2025a). PIVOT consistently improves MLLM performance, and its advantage remains robust even with larger-scale training data. Evolving vision encoder Vision encoder # Params SigLIP2-So/16 + PIVOT 400M 400M LLM Qwen2.5-0.5B Qwen2.5-0.5B SigLIP2-So/16 SigLIP2-So/16 + PIVOT 400M 400M 400M Qwen2.5-1.5B Qwen2.5-1.5B Qwen2.5-1.5B MLLM combining the vision encoder with Qwen2.5 Full train? Avg. (All) General. OCR&Chart. Vision-Cen. Knowledge. 45.1 46.0 +0.9 52.4 54.5 55.2 +0.7 57.1 58.2 +1.1 66.2 67.1 67.5 +0.4 44.6 41.5 +0.4 46.6 51.1 52.1 +1. 39.6 39.4 +1.9 50.6 53.1 54.1 +1.0 39.2 45.0 +0.2 46.1 46.9 47.2 +0.3 Table Effect of full-parameter training. Although updating all model parameters inevitably alters the intrinsic representations of the vision encoder, we aim to understand its overall effect. MLLMs incorporating PIVOTenhanced encoders exhibit clear and consistent gains over their baseline counterparts, highlighting the robustness of PIVOT beyond the controlled evaluation protocol. (results) We observe that using the first linear layer from the PIVOT-tuned projector together with one additional layer (i.e., two layers in total) yields the best downstream MLLM performance. Contrary to the expectation that increasing the depth (e.g., 2+2 layers) would leverage more parameters and improve results, this configuration instead leads to inferior performance. Based on this finding, we adopt the 1+1 setting for all subsequent experiments in Section 5. Training with more data. In Section 5.1, our PIVOT-enhanced vision encoder is paired with new LLM and finetuned on the Cambrian-737K dataset. We examine whether the same trend holds with larger-scale data during Stage 3 of Figure by using the LLaVA-OV-3M dataset. The results are reported in Table C. Our findings confirm that the advantage of PIVOT persists even with more data. For example, when combined with Qwen2.5-1.5B, the PIVOT-enhanced SigLIP2-So/16 achieves an average gain of +2.3%p over the base SigLIP2-So/16 encoder, demonstrating that the benefits of PIVOT are robust to data scale in the Stage 3. Training all parameters. We adopt the same evaluation protocol of Cambrian (Tong et al., 2024a), DINOMLLM (Fan et al., 2025), and MLLM-data (Han et al., 2025) in order to directly assess how useful the visual representations of vision encoder itself are for an MLLM. As noted by Cambrian, it allows us to study visual representations efficiently. Obviously, training all parameters of the model during the stage 3 of Figure alters the intrinsic representations of the encoder. Nevertheless, we were interested in understanding how the MLLM performance changes when all parametersincluding the vision encoder, LLM, and projectorare updated during Stage 3. The results, reported in Table D, show that MLLMs built upon PIVOT-enhanced encoders consistently outperform their counterparts. 21 Figure Scaling the amount of post-training data for MLLM. We vary the size of training data for an MLLM built with Qwen2.5-1.5B and SigLIP2-So/16 and measure its performance. Figure ImageNet classification. We train MLLMs with different post-training strategies while scaling either the vision encoder (with fixed Qwen2.5-1.5B) or the LLM (with fixed SigLIP2-So/16). We utilize features extracted from the MLLMs vision encoder (i.e., SigLIP2-So/16). Note that the features used in Figure 6 are the outputs of the vision encoder and multimodal projector, which are directly used as the LLMs visual embeddings."
        },
        {
            "title": "C Additional Experiment Results",
            "content": "C.1 Scaling training data in MLLMs In Figure 4, we analyze the effect of post-training data scale on MLLM performance, focusing on the average scores across all benchmarks. To complement this, we provide the results for each specific domain in Figure B. The results consistently show that the DPO-tuned MLLM outperforms its SFT counterpart as the amount of training data increases. For Knowledge VQA, interestingly, we observe that the performance gap between the two models diminishes from +4.2 to +0.7 as the data size increases from 3K to 10K samples. C.2 Performance of MLLMs on all benchmarks Full results on MLLM performance are reported in Table F, Table G, and Table H. Within our controlled setup, DPO-trained MLLMs consistently surpass their SFT-trained models across different scales of data, vision encoders, and language models. The advantage is evident on strongly vision-related tasks and appears more modest on weakly vision-related tasks. C.3 ImageNet classification with vision encoder We present additional experimental results for ImageNet classification in Figure C. For this analysis, we conduct linear-probe evaluation using features obtained from the MLLMs vision encoder. This setup differs from the main papers experiment in Figure 6, which utilizes visual embeddings that have passed through both the vision encoder and the projector. The results reinforce our primary findings: DPO consistently enhances visual representations more effectively than SFT, and the vision encoders performance improves as the size of the LLM it is trained with increases. 22 C.4 Gradient visualization of vision encoder We provide additional Grad-CAM results in Figure E, where we visualize the gradients on the visual features induced by the SFT and DPO losses. The results show that MLLM post-training yields larger gradients on question-relevant image regions, with DPO providing more concentrated signals than the diffuse gradients from SFT. Furthermore, the bottom two examples in Figure correspond to global query (like \"Describe the photo in detail.\"). For this type of query, both DPO and SFT generate similarly distributed gradients across the entire image, different outcome from the localized queries. As will be further discussed in Section E.2, this supports our hypothesis that the nature of the post-training data can determine how DPO enhances visual representations. C.5 Segmentation probing with vision encoder We provide additional qualitative results for segmentation probing in Figure F. For this experiment, CLIP-L/14 336px vision encoder is post-trained in an MLLM with either SFT or DPO, using Qwen2.5-3B as the base LLM. The qualitative results indicate that the DPO-trained vision encoder yields segmentation maps more consistent with the ground truth."
        },
        {
            "title": "D Additional Explanations",
            "content": "D.1 SFT and DPO We elaborate on the post-training techniques discussed in Section 3.1. SFT is standard approach for equipping LLMs with instruction-following abilities (Radford et al., 2018; Touvron et al., 2023a). In our work, this involves training the MLLM πθ using maximum likelihood objective on the post-training dataset XPT. Specifically, for each given image Ii and query qi, the model is optimized to maximize the probability of generating the chosen response yc , as formulated in Equation (1). In contrast, DPO (Rafailov et al., 2023) is prominent RL method that directly aligns the model with human preferences without requiring an explicit reward model. DPO leverages the full preference pair, including both the chosen response yc . Its objective, also formulated in Equation (1), is to increase the likelihood of the chosen response while simultaneously decreasing that of the rejected one, relative to reference policy πref, which is typically the initial model before preference alignment. and the rejected response yr D.2 Evaluation benchmarks As stated in Section 3.1, we adopt the evaluation suite from Cambrian (Tong et al., 2024a) for comprehensive assessment of MLLM performance. This suite consists of 16 benchmarks categorized into four domains: General, Knowledge, OCR & Chart, and Vision-Centric VQA. list of these benchmarks, along with their domain assignments and citations, is provided in Table E. Unlike other benchmarks whose scores generally range from 0 to 100, MME produces values on 02000 scale. To ensure comparability within the overall MLLM evaluation, when computing the average score, we rescale the MME results by factor of 20. We utilize the Cambrian source code, except in the case of DocVQA (Mathew et al., 2021). The Cambrian implementation of DocVQA does not yield numeric outputs automatically; rather, it requires manual submission of result CSV files to the evaluation website. To streamline this process, we employ the lmms-eval (Zhang et al., 2024) source code to obtain DocVQA scores. Benchmark GQA SEED MME MMBench AI2D ScienceQA MathVista MMMU TextVQA DocVQA ChartQA OCRBench MMVP RealWorldQA CVBench-2D CVBench-3D Task all image-based perception all all image-based math vision all all all all all all all all Domain General VQA General VQA General VQA General VQA Knowledge VQA Knowledge VQA Knowledge VQA Knowledge VQA OCR & Chart VQA OCR & Chart VQA OCR & Chart VQA OCR & Chart VQA Vision-Centric VQA Vision-Centric VQA Vision-Centric VQA Vision-Centric VQA Citation Hudson & Manning (2019) Ge et al. (2023) Fu et al. (2023) Liu et al. (2024b) Hiippala et al. (2021) Lu et al. (2022) Lu et al. (2023) Yue et al. (2024) Singh et al. (2019) Mathew et al. (2021) Masry et al. (2022) Liu et al. (2023b) Tong et al. (2024b) xAI (2024) Tong et al. (2024a) Tong et al. (2024a) Table List of benchmarks used. To evaluate MLLLMs, we used 16 benchmarks that are assigned to each of the domains proposed in Cambrian (Tong et al., 2024a)."
        },
        {
            "title": "E Discussions",
            "content": "E.1 Low performance gap on Knowledge VQA As we observe in Section 3, DPO shows clear advantage over SFT on strongly vision-related tasks, but this performance gap diminishes for Knowledge VQA. This suggests that for knowledge-intensive tasks, leveraging the rejected responses yr provides less significant benefit compared to the standard SFT approach. We hypothesize that for problems in domains like science and math, the chosen responses yc may already contain sufficient factual knowledge, making the comparative signal from yr less critical. The interplay between preference data characteristics and task domains is valuable direction for future research. E.2 Future work Beyond the broader impact discussed in Section 6, our study opens several additional avenues for research. While our work primarily utilizes the LLaVA Li et al. (2025a) framework with Qwen2.5 Yang et al. (2025a) backbone, natural extension is to investigate if our findings generalize to other MLLM architectures, such as InternVL Chen et al. (2024) and Qwen-VL Bai et al. (2023a), or when using different LLM backbones like LLaMA Dubey et al. (2024) and Gemma Team et al. (2024b). Another promising direction involves exploring whether novel dataset formats could be designed to better leverage DPO for learning stronger visual representations. We have particular interest in this direction and plan to actively pursue it as part of our future work. Furthermore, expanding the evaluation beyond the 16 benchmarks from Cambrian to include traditional hallucination benchmarks Wang et al. (2023); Sun et al. (2024b) could provide deeper insights into the comparative performance of DPO and SFT."
        },
        {
            "title": "F Experimental Details",
            "content": "F.1 Pre-training & Post-training We describe in detail the training strategies of the models used in Section 3. We build our models using the LLaVA-OneVision code4. Our experiments utilize four scales of the SigLIP2 vision encoder (google/SigLIP2-B/16-patch16-384, google/SigLIP2-L/16-patch16-384, google/siglip2-So/16patch16-384, google/SigLIP2-g/16-opt-patch16-384) and four versions of the Qwen2.5-Instruct LLM 4https://github.com/LLaVA-VL/LLaVA-NeXT 24 General OCR & Chart Vision-Centric Knowledge l 2 e D 3 e M i a M i a n C n C B D P e e Train V Model Qwen2.5-3B+SigLIP2-B/16 DPO 59.7 1438.4 72.4 72.7 60.1 83.7 44.3 15.7 73.3 45.3 56.9 64.5 65.5 60.5 49.3 59.2 59.3 Qwen2.5-3B+SigLIP2-B/16 SFT 58.3 1509.4 72.2 71.8 57.1 83.6 42.8 15.9 73.5 42.7 55.3 63.7 60.0 57.2 47.0 57.1 57.2 Qwen2.5-3B+SigLIP2-L/16 DPO 62.6 1498.1 75.8 73.6 61.3 85.6 42.3 15.8 74.8 48.7 58.0 66.7 69.3 65.4 58.8 64.2 66.8 Qwen2.5-3B+SigLIP2-L/16 SFT 60.8 1547.8 74.5 73.0 58.3 85.5 42.0 15.5 74.5 43.3 57.6 66.7 65.3 55.1 56.8 62.4 64.1 Qwen2.5-3B+SigLIP2-So/16 DPO 63.9 1553.1 76.2 74.4 61.5 87.0 42.6 15.7 75.9 52.0 59.0 67.2 69.3 67.6 62.2 64.4 69.9 Qwen2.5-3B+SigLIP2-So/16 SFT 62.3 1550.0 75.2 73.7 59.3 86.8 42.3 15.9 75.9 47.5 59.0 66.3 65.8 58.5 60.1 65.1 67.1 Qwen2.5-3B+SigLIP2-g/16 DPO 64.8 1548.6 77.6 75.0 62.5 87.9 43.1 17.2 75.4 52.0 59.1 68.8 70.2 66.0 65.2 67.0 72.3 SFT 62.9 1558.0 75.7 74.4 60.4 86.9 43.8 17.3 75.3 48.0 59.9 67.5 65.1 57.6 63.8 65.6 67.0 Qwen2.5-3B+SigLIP2-g/16 M 2 A Q e - - M Q h V T Table Scaling the vision encoder in MLLMs. We analyze the impact of the vision encoder sizes, ranging from 86M (SigLIP-B/16) to 1B (SigLIP-g/16) parameters, in Qwen2.5-3B combined with SigLIP2. General OCR & Chart Vision-Centric Knowledge l 2 e D 3 e I M E M a Train V o Model Qwen2.5-0.5B+SigLIP2-So/16 DPO 51.5 1167.7 58.1 65.7 55.9 76.2 34.1 10.8 59.7 28.0 52.5 52.3 56.5 59.1 56.2 54.8 46.1 Qwen2.5-0.5B+SigLIP2-So/16 SFT 49.5 1170.9 55.2 63.1 55.2 75.1 33.7 10.2 59.1 25.3 50.5 45.3 49.5 57.1 54.3 54.5 45.3 Qwen2.5-1.5B+SigLIP2-So/16 DPO 61.0 1478.7 71.9 72.5 60.9 86.7 40.1 14.7 71.5 50.0 57.4 63.6 59.9 65.4 62.7 64.7 59.5 Qwen2.5-1.5B+SigLIP2-So/16 SFT 59.1 1442.1 70.6 71.9 58.7 86.6 41.0 12.6 70.9 44.0 56.0 63.1 56.1 62.1 58.8 63.4 57.7 Qwen2.5-3B+SigLIP2-So/16 DPO 63.9 1553.1 76.2 74.4 61.5 87.0 42.6 15.7 75.9 52.0 59.0 67.2 69.3 67.6 62.2 64.4 69.9 Qwen2.5-3B+SigLIP2-So/16 SFT 62.3 1550.0 75.2 73.7 59.3 86.8 42.3 15.9 75.9 47.5 59.0 66.3 65.8 58.5 60.1 65.1 67.1 Qwen2.5-7B+SigLIP2-So/16 DPO 68.9 1664.0 80.3 76.0 64.0 92.4 50.2 20.4 80.7 59.3 62.2 73.0 75.8 74.2 65.6 71.1 73.5 SFT 66.2 1627.7 78.6 74.9 59.9 91.8 48.7 18.7 79.9 46.0 63.5 71.9 72.3 68.8 64.0 69.0 70.0 Qwen2.5-7B+SigLIP2-So/ M 2 Q e - - M t C t Table Scaling the language model in MLLMs. Using SigLIP2-So/16 as the vision encoder, we vary the size of the language model (Qwen2.5) and evaluate performance across multiple benchmarks. (Qwen/Qwen2.5-0.5B-Instruct, Qwen/Qwen2.5-1.5B-Instruct, Qwen/Qwen2.5-3B-Instruct, Qwen/ Qwen2.5-7B-Instruct), which are connected by 2-layer MLP projector. For the training data, we use the BLIP_LAION_CC_SBU_558k dataset5 for projector-only pretraining and the LLaVA-OneVision-Data-Single 3.2M dataset6 for Stage 1 pretraining. For Stage 2 post-training, we use 20K subset randomly sampled from the MMPR-1.2 dataset7. The hyperparameters for Stage 1 are adopted from the standard LLaVA-OneVision finetuning script8, including learning rate of 1 105 and batch size of 256. For Stage 2 DPO post-training, we largely follow the corresponding script9 but adjust the learning rate (LR) to 1 106 and use batch size of 256 for our data scale. To ensure controlled comparison for Stage 2 SFT post-training, we use the same finetuning script with learning rate of 1 105, but remove the vision-encoder-specific LR, mirroring the DPO setup. Since SFT and DPO rely on fundamentally different loss formulations, their optimal learning rates naturally diverge. In practice, we observe that DPO requires substantially smaller LRs than SFT, partly because DPO accounts for both chosen and rejected responses, effectively doubling the batch size per iteration compared to SFT. This observation aligns with prior settings, such as those in InternVL2.5 (Wang et al., 2024b), where an LR of 2 107 is used for DPO and 4 105 for SFT. 5https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain/blob/main/blip_laion_cc_sbu_558k.json 6https://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-Data 7https://huggingface.co/datasets/OpenGVLab/MMPR-v1.2 8https://github.com/LLaVA-VL/LLaVA-NeXT/blob/main/scripts/train/finetune_si.sh 9https://github.com/LLaVA-VL/LLaVA-NeXT/blob/main/scripts/train/dpo.sh 25 r Data size Train General OCR & Chart Vision-Centric Knowledge D 3 e C - 2 e - W o A 2 M E P P V M A V c D DPO 60.4 1490.4 72.0 72.5 60.7 86.2 40.4 15.3 71.3 46.7 56.2 62.6 57.2 65.2 61.7 64.6 58.8 SFT 56.4 1431.0 70.0 70.9 58.0 86.8 33.8 8.5 67.2 40.7 58.8 52.7 41.7 63.2 59.5 63.1 56.5 DPO 60.6 1486.3 72.2 72.4 60.7 86.0 40.6 15.5 71.0 47.3 57.1 63.2 59.0 65.6 61.9 64.4 59.0 SFT 57.6 1409.4 70.5 71.3 58.2 86.6 39.6 8.7 70.7 38.7 59.1 62.6 42.7 63.2 59.3 63.1 56.6 DPO 60.8 1480.1 72.0 72.7 60.3 86.1 40.6 14.8 71.0 49.0 57.0 63.2 60.2 65.6 62.2 64.5 59.5 SFT 58.9 1431.5 70.9 72.1 58.9 86.0 40.2 12.5 71.1 44.6 57.9 63.6 49.3 62.4 58.1 64.5 58.0 DPO 61.0 1478.7 71.9 72.5 60.9 86.7 40.1 14.7 71.5 50.0 57.4 63.6 59.9 65.4 62.7 64.7 59.5 SFT 59.1 1442.1 70.6 71.9 58.7 86.6 41.0 12.6 70.9 44.0 56.0 63.1 56.1 62.1 58.8 63.4 57.7 DPO 61.3 1495.4 72.2 73.2 60.9 86.4 39.0 15.6 71.7 51.9 59.0 63.0 61.3 64.5 62.6 64.8 59.7 SFT 59.5 1423.7 70.5 72.1 58.3 85.9 40.0 13.5 71.3 45.3 57.9 64.1 59.1 63.2 57.4 64.3 58.3 e O t T t C R 3K 3K 5K 5K 10K 10K 20K 20K 40K 40K Table Scaling data on MLLM performance. We vary the size of training data for an MLLM built with Qwen2.5-1.5B and SigLIP2-So/16 and measure its performance. F.2 ImageNet classification This section details the protocol for the ImageNet classification experiment presented in Section 4. Our approach is based on the linear probe evaluation from the official OpenAI-CLIP repository10. As recommended in their public issue11, we freeze the feature extractor and train scikit-learn Logistic Regression model with L2 regularization, sweeping over lambda values for maximum of 1000 iterations. Since evaluating on the full 1M ImageNet dataset is time-consuming, we follow the practice discussed in the community12 and perform validation on 50k random subset of the ImageNet data for early-stage validation. In addition, we implement prototype-based linear classifier for more rapid validation; this is achieved by averaging the features of each class to form the weights of linear layer. We verify that this faster method yields similar performance trends to the standard Logistic Regression approach. F.3 Grad-CAM We present here the experimental details for the gradient visualization in Section 4. We construct training pipeline using single sample and visualize the gradients around the 20th step. This setup alleviates the issue where the cosine learning rate scheduler sets the initial learning rate near zero and produces uninformative gradients at very early steps in the original LLaVA-OneVision code. By focusing on this step range, we obtain meaningful gradient patterns. F.4 Semantic segmentation We describe here the experimental details for the semantic segmentation study in Section 4. The setup follows the implementation referenced in the codebase of prior work (Covert et al., 2025)13. Specifically, we freeze the vision encoder and attach two-layer MLP head, which is trained on the ADE20K dataset (Zhou et al., 2017). Evaluation is conducted on the validation set, where segmentation is performed at the patch level and recall is used as the primary metric. The training procedure follows the default configuration of the referenced repository, including 5 training epochs and learning rate of 1 103. 10https://github.com/openai/CLIP?tab=readme-ov-file#linear-probe-evaluation 11https://github.com/openai/CLIP/issues/39#issuecomment-778034767 12https://github.com/openai/CLIP/issues/64#issuecomment-804444364 13https://github.com/iancovert/patch-seg/tree/main?tab=readme-ov-file Figure We illustrate the experimental setup in Section 5. More details can be found in Section 5.1. The process begins with contrastive pretraining of the vision encoder using CLIP or SigLIP on large-scale imagetext data. Next, the vision encoder is paired with an LLM head and optimized through preference-instructed finetuning (SFT and DPO) with 3M+20K samples (i.e., post-training in Section 3.1). Finally, the tuned vision encoder is frozen and evaluated in an MLLM setting by coupling it with Qwen-1.5B and finetuning on the Cambrian 737K dataset. This setup parallels prior evaluation protocols such as Cambrian (Tong et al., 2024a), DINO-MLLM (Fan et al., 2025), and MLLM-data (Han et al., 2025) and allows direct assessment of the standalone usefulness of vision representations within MLLMs. F.5 Representation alignment In Section 4, we present results measured against five reference LLMs, including Gemma-2B/7B (Team et al., 2024b), LLaMA-3-8B (Dubey et al., 2024), and Mixtral-8x7B (Jung et al., 2010) and Bloomz7B (Muennighoff et al., 1786). The vision models under analysis are vision encoders trained within MLLM frameworks alongside three different sizes of LLMs. We evaluate alignment between our vision encoders and the reference LLMs using the implementation provided in the Platonic Representation repository14. Scores are computed on the minhuh/prh dataset distributed with the repository. Since this dataset contains only 1,024 examples, the results exhibit variability. To address this, we evaluate vision encoders trained with three different random seeds and report the averaged performance F.6 PIVOT-enhanced vision model evaluation To evaluate the effectiveness of PIVOT-enhanced vision models within MLLMs, we follow the pipeline illustrated in Figure D. The middle part of the figure corresponds to the training strategy described in Section 3.1 and Section F.1. In the rightmost part of the figure (i.e., Stage 3 ), the model is finetuned on new dataset using the configuration provided in the LLaVA-NeXT repository15. This setup includes batch size of 256, learning rate of 1 105, and other default hyperparameters. 14https://github.com/minyoungg/platonic-rep 15https://github.com/LLaVA-VL/LLaVA-NeXT/blob/main/scripts/train/finetune_si.sh 27 Figure Additional results of Grad-CAM. We provide additional experimental results of Figure 7, where we illustrate the gradients received by the vision encoder under MLLM post-training approaches, DPO and SFT, using Grad-CAM (Selvaraju et al., 2017). 28 Figure Qualitative results on segmentation probing. We study segmentation probing of vision encoder (VE), CLIP-L/14 336px, post-trained in an MLLM with SFT and DPO, where Qwen2.5-3B is base LLM. The DPO-trained ViT yields segmentation maps consistent with the ground truth, unlike the broader maps from the SFT-trained model."
        }
    ],
    "affiliations": [
        "KAIST",
        "NAVER AI Lab"
    ]
}