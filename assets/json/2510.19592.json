{
    "paper_title": "Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation",
    "authors": [
        "Su Ho Han",
        "Jeongseok Hyun",
        "Pilhyeon Lee",
        "Minho Shim",
        "Dongyoon Wee",
        "Seon Joo Kim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal large language models (MLLMs) demonstrate strong video understanding by attending to visual tokens relevant to textual queries. To directly adapt this for localization in a training-free manner, we cast video reasoning segmentation as a video QA task and extract attention maps via rollout mechanism. However, raw attention maps are noisy and poorly aligned with object regions. We propose Decomposed Attention Fusion (DecAF), which refines these maps through two mechanisms: (1) contrastive object-background fusion and (2) complementary video-frame fusion. This method suppresses irrelevant activations and enhances object-focused cues, enabling direct conversion of attention maps into coarse segmentation masks. In addition, we introduce attention-guided SAM2 prompting for obtaining fine-grained masks. Unlike existing methods that jointly train MLLMs with SAM, our method operates entirely without retraining. DecAF outperforms training-free methods and achieves performance comparable to training-based methods on both referring and reasoning VOS benchmarks. The code will be available at https://github.com/HYUNJS/DecAF."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 2 9 5 9 1 . 0 1 5 2 : r Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation DECOMPOSED ATTENTION FUSION IN MLLMS FOR TRAINING-FREE VIDEO REASONING SEGMENTATION Su Ho Han1 Minho Shim3 Jeongseok Hyun1 Dongyoon Wee3 Pilhyeon Lee2 Seon Joo Kim1 1Yonsei University 2Inha University 3NAVER Cloud"
        },
        {
            "title": "ABSTRACT",
            "content": "Multimodal large language models (MLLMs) demonstrate strong video understanding by attending to visual tokens relevant to textual queries. To directly adapt this for localization in training-free manner, we cast video reasoning segmentation as video QA task and extract attention maps via rollout mechanism. However, raw attention maps are noisy and poorly aligned with object regions. We propose Decomposed Attention Fusion (DecAF), which refines these maps through two mechanisms: (1) contrastive object-background fusion and (2) complementary video-frame fusion. This method suppresses irrelevant activations and enhances object-focused cues, enabling direct conversion of attention maps into In addition, we introduce attention-guided SAM2 coarse segmentation masks. prompting for obtaining fine-grained masks. Unlike existing methods that jointly train MLLMs with SAM, our method operates entirely without retraining. DecAF outperforms training-free methods and achieves performance comparable to training-based methods on both referring and reasoning VOS benchmarks. The code will be available at https://github.com/HYUNJS/DecAF."
        },
        {
            "title": "INTRODUCTION",
            "content": "In recent years, Multimodal Large Language Models (MLLMs) (Lin et al., 2023; Chen et al., 2024; Zhang et al., 2024; Bai et al., 2025; Wang et al., 2024a) have rapidly advanced, demonstrating strong performance on challenging video QA benchmarks (Mangalam et al., 2023; Fu et al., 2025). These advances reveal their ability to process temporal visual cues and perform complex reasoning over natural language queries. Such capabilities suggest that MLLMs may also possess inherent localization ability in videos, which can be leveraged to perform training-free video reasoning segmentation, task that localizes objects corresponding to text-based queries requiring complex reasoning. Recent studies (Yan et al., 2024; Bai et al., 2024; Gong et al., 2025b; Lin et al., 2025) have attempted to adapt MLLMs for video segmentation via efficient fine-tuning methods, such as LoRA (Hu et al., 2022). However, such approaches inevitably require model-specific training and additional optimization efforts. In this work, we instead investigate the intrinsic localization ability of MLLMs and introduce training-free framework for video reasoning segmentation based on decomposed attention fusion. Loc-Head (Kang et al., 2025a) explores the localization ability of MLLMs in the image domain by selecting attention heads responsible for grounding. However, we find it does not generalize well to video reasoning segmentation, as its head selection algorithm relies heavily on heuristics. First, it assumes the presence of single referring object and selects heads based on spatial entropy, which makes extension to multi-object and temporal video data difficult. Second, it is vulnerable to the visual attention sink phenomenon (Kang et al., 2025b), where certain regions consistently receive dominant attention scores regardless of the instruction. For instance, Loc-Head excludes heads that strongly attend to the bottom row in LLaVA, but this rule does not transfer to Qwen2VL, where excluding the right-most column is instead required. To obtain attention maps for object localization without relying on modelor task-specific design, we start with attention rollout (Abnar & Zuidema, 2020). Rollout aggregates attention weights across *Equal contribution. Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation Figure 1: Visualization of our method. (a) Noise in irrelevant regions is suppressed by contrastive fusion with the background attention map. As shown in the first frame, background activations are removed, and the target object is emphasized. (b) Video attention map captures temporal cues, while frame attention map highlights object-centric details. Their fusion resolves conflicts (e.g., identifying the server vs. the hitting player) and produces more consistent localization. The attention mask is obtained directly from the attention map while the SAM mask is generated by SAM2. layers, revealing visual cues to which the MLLM attends when producing answers. Its applicability across attention-based MLLMs makes it plausible approach to probing localization ability. However, since the rollout integrates signals from all heads, irrelevant regions and visual attention sinks often dominate, reducing the relative strength of object cues. To overcome these limitations, we introduce Decomposed Attention Fusion (DecAF). DecAF is designed to suppress noise and enhance object attention signals by decomposing and fusing attention maps in two key ways. First, Contrastive Object-Background Fusion combines the object and background attention maps through simple subtraction. The object attention map is obtained with prompt focusing on the target object, while the background attention map is derived from contrastive prompt that excludes this object. This design effectively suppresses irrelevant activations and highlights the target object signal, as illustrated in Fig. 1 (a). Second, Complementary VideoFrame Fusion leverages the distinct strengths of video and frame attention in multi-scale manner. Video attention captures temporal context, which is essential when the object is temporarily absent or requires temporal reasoning, but its coarse granularity limits performance on small objects. In contrast, frame attention provides object-centric, fine-grained cues but lacks temporal coherence. By combining these two attentions, this fusion maintains clearer object focus while also leveraging temporal context, resulting in more robust attention maps that accurately localize the target object across the video. 2 Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation With the object localization attention map obtained from our two fusion methods, we first generate video object masks through simple thresholding, which provides reliable localization of the target object but remains coarse due to the low granularity of attention. To obtain denser masks, we extract point prompts from the attention map and apply SAM2 (Ravi et al., 2024). However, these coarse prompts, derived from spurious activations in the attention map, often produce false positives. To address this issue, we propose an attention consistency score that evaluates the alignment between the predicted mask and the underlying attention map, enabling unreliable segmentation masks to be filtered out. As shown in Fig. 1, this process transforms noisy attention map into precise and reliable segmentation mask. We evaluate DecAF across three MLLM families and five datasets, including three referring VOS datasets (Khoreva et al., 2018; Seo et al., 2020; Ding et al., 2023) and two reasoning VOS datasets (Yan et al., 2024; Bai et al., 2024). DecAF consistently outperforms prior training-free approaches (Li et al., 2025; Kang et al., 2025a), both with and without SAM. In addition, the dense video object masks achieve performance comparable to training-based methods (Lai et al., 2024; Yan et al., 2024; Bai et al., 2024; Lin et al., 2025; Gong et al., 2025b;a). These results highlight that decomposed attention fusion offers simple and effective framework for training-free video reasoning segmentation."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Multimodal Large Language Models. LLMs demonstrate powerful reasoning and cognition capabilities (Brown et al., 2020; Dubey et al., 2024; Yang et al., 2024), leading to the development of MLLMs (Wang et al., 2024b; Google, 2024; Team, 2024; Liu et al., 2024). These models, built on the transformer architecture (Vaswani et al., 2017), rely on the attention mechanism. Due to the quadratic cost of attention, some MLLMs firstly compress video tokens into fixed number of tokens via lightweight modules (Jin et al., 2024; Song et al., 2024; Maaz et al., 2024). However, this token compression inevitably sacrifices fine-grained spatial information, unlike LLaVA-style models (Liu et al., 2023), which use linear projector to preserve dense spatial features. More recently, Qwen2VL (Wang et al., 2024b) further advances this line by supporting native-resolution video inputs, maintaining both aspect ratio and fine-grained visual details. In this work, we build on such models and focus on exploring the inherent localization ability of MLLMs. Text-conditioned Video Object Segmentation. Early research on referring VOS (RVOS) focuses on localizing the target object from simple textual expressions, typically describing appearance. Datasets such as Ref-DAVIS (Khoreva et al., 2018) and Ref-YouTube-VOS (Seo et al., 2020) were designed for this setting and only cover single-object cases. More recently, MeViS (Ding et al., 2023) introduces motion-centric and more challenging scenarios, including cases where the referred object is absent or where multiple candidates match the expression. With the advent of powerful MLLMs (Liu et al., 2023), video reasoning segmentation has emerged, targeting complex expressions that extend beyond appearance or motion cues and require reasoning over world knowledge and temporal context (Yan et al., 2024; Bai et al., 2024). To address this, existing approaches adapt pretrained MLLMs to RVOS via lightweight finetuning strategies such as LoRA (Hu et al., 2022), and integrate them with segmentation model such as SAM (Kirillov et al., 2023) for precise mask generation, often requiring full finetuning of the mask decoder (Gong et al., 2025b; Lin et al., 2025). In contrast, we leverage MLLMs and SAM in training-free manner. Training-free Text-to-Visual Grounding with MLLMs. Recently, MLLMs have been studied for training-free visual grounding tasks (Lin et al., 2024; Li et al., 2025; Kang et al., 2025a). VLSAM (Lin et al., 2024) and TAM (Li et al., 2025) leverage the attention rollout mechanism (Abnar & Zuidema, 2020) to localize objects in images, with VL-SAM further refining the masks using SAM. Both methods identify all objects by enumerating categories during MLLM decoding. In contrast, Kang et al. (2025a) proposed method that selects specific attention heads responsible for localization, enabling direct grounding of the object referred to by the given expression. However, this head-selection method shows poor generalization: attention heads identified on referring datasets transfer poorly and yield low accuracy on reasoning-intensive datasets. 3 Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation Figure 2: Overview of DecAF. (a) Attention rollout with our V-Max normalization produces rollout matrix that accumulates attention across layers, from which visual-token scores for the final query token are extracted as attention maps for grounding. (b) Contrastive fusion suppresses attention scores on background regions. (c) Complementary fusion integrates videoand frame-level cues. (d) These fusion methods are combined into the full pipeline to refine noisy attention maps."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "3.1 OVERVIEW Given video and text instruction referring to an object, our framework produces segmentation masks of the target object(s). The pipeline consists of two stages. First, coarse segmentation masks are obtained from attention score maps computed in an MLLM. Second, fine-grained dense segmentation masks are generated using SAM conditioned on these attention maps. In the first stage, we propose Decoupled Attention Fusion (DecAF), illustrated in Fig. 2, which integrates contrastive and complementary fusion strategies with tailored prompting methods. To obtain the attention scores, we adopt attention rollout (Abnar & Zuidema, 2020) with new normalization technique designed for MLLMs. In the second stage, we introduce training-free SAM2 prompting pipeline guided by attention maps  (Fig. 3)  . Point queries are first selected by thresholding the attention maps, and SAM2 generates mask tracklets for each query. These tracklets are then evaluated with the proposed attention consistency score, which measures whether the predicted masks consistently overlap with high-attention regions across frames. The resulting scores are used to rank and tracklet candidates. 3.2 ATTENTION ROLLOUT WITH VISION-AWARE NORMALIZATION We trace the influence of visual tokens on the models output by propagating attention scores through the transformer layers of MLLMs. To better capture language-conditioned grounding, we modify the standard attention rollout (Abnar & Zuidema, 2020) with vision-aware normalization scheme. Standard rollout. Given the attention tensor A(l) RhN from the l-th transformer layer, where is the number of heads and is the total number of tokens, the head-wise averaged attention matrix is computed as Eq. 1, and the residual connection is incorporated by adding the identity matrix as Eq. 2. This reflects that token can either propagate its own representation through the skip connection or attend to other tokens via the attention mechanism. The rollout matrix is then recursively accumulated across layers as Eq. 3, starting from the initialization R(1) = ˆA(1), and producing R(L), which encodes how information flows from each token to every other token throughout the network. A(l) = 1 h (cid:88) i=1 A(l) . (1) ˆA(l) = ( A(l) + I)/2. (2) R(l) = ˆA(l)R(l1). (3) Head-wise weighted aggregation. To reduce the effect of noisy heads, we assign weight to each head based on the strength of its vision attention. For each layer l, let the original attention tensor before aggregation be denoted as A(l) RhN (Nv+Nt), where Nv and Nt indicate the number of 4 Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation visual and textual tokens, respectively. From A(l), the vision block is extracted: A(l) The maximum value over the visual token dimension is then computed as: RhN Nv . m(l) = Nvmax j=1 A(l) [:, :, j], m(l) RhN . (4) Averaging m(l) over the token dimension finally produces the head-wise weight vector, w(l) Rh. The weights are normalized so that maxh(w(l) ) = 1, and these normalized weights are used to aggregate the heads, resulting in the final attention weights ˆA(l) RN (Nv+Nt)."
        },
        {
            "title": "3.3 DECOMPOSED ATTENTION FUSION",
            "content": "The attention rollout mechanism quantifies token-to-token influence. To perform text-conditioned video reasoning segmentation, we cast the task as video question answering, where the goal is to identify category of the object in video referred to by the text instruction. We then exploit the rollout matrix values with the last token as query and visual tokens as keys, using them as attention scores that indicate how visual tokens contribute to answering the video QA, as shown in Fig. 2 (a). However, the rollout matrix aggregates signals across all heads and layers and is too noisy to serve directly as segmentation score map. In addition to pervasive noise, we observe strong activations in irrelevant regions, known as the visual attention sink phenomenon. To address this, we introduce Decomposed Attention Fusion (DecAF) to obtain cleaner, object-focused attention maps. As shown in Fig. 2 (d), DecAF applies contrastive fusion within each modality (video and frame) in parallel, followed by complementary fusion after upscaling the video-level attention maps to match the frame-level size. The resulting attention maps are then converted into coarse segmentation masks via thresholding. Here, we explain with shortened prompts, but the full prompts are in the Appendix. Contrastive ObjectBackground Attention Fusion. key challenge of using attention maps for segmentation is that irrelevant regions often receive very high scores, which cannot be suppressed by simple thresholding. Such visual attention sinks frequently appear regardless of the given instruction. To address this issue, we introduce contrastive fusion, which contrasts attention maps obtained from object-focused and background-focused prompts. Subtracting background from object attention effectively highlights the target region while suppressing spurious responses. The specific process follows Fig. 2 (b). The object attention map is obtained by prompting the model to identify the target object category from the referring expression using an object-focused prompt template, What is the main object referred to in the given expression? The rollout attention weights from this response form the positive map. For the background attention map, we first use background-focused prompt such as Describe the background scene of the video. However, this may cause the target object to be mistakenly attended when it is not the main salient object but still appears in the background. To mitigate this, we additionally insert the identified category oname into the template, to explicitly exclude the target object from the background attention map. The rollout attention map from this response serves as the negative map. Both object and background attention maps are reshaped into (T, Hp, Wp), where is the number of frames and (Hp, Wp) is the patch grid. Before fusion, Gaussian smoothing is applied to both maps to mitigate the sparsity of raw attention weights. The contrastive map, Vctr, is then computed by subtracting the background map from the object map, clamped to remove negative values. Finally, minmax normalization is applied to scale the values into the [0, 1] range. Complementary VideoFrame Attention Fusion. The softmax operation in attention enforces that all token scores sum to one. With video inputs, this constraint spreads attention across large number of tokens, yielding maps that are relatively sparse and shaped by temporal context. In contrast, with image inputs, attention is concentrated on fewer tokens and tends to emphasize object-centric spatial details. We therefore exploit these complementary properties of videoand frame-level attention maps to achieve more robust localization. As shown in Fig. 2 (c), we apply the identical attention rollout pipeline individually to the video and frame modalities, where each frame in the image modality is processed along the batch axis. This mixed-modality design introduces two modifications in the contrastive fusion step. (1) Since background prompting requires an object category, we select single prediction by aggregating outputs from both videoand frame-level inputs with object category choice prompt. (2) For minmax 5 Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation Figure 3: Overview of our SAM prompting pipeline with attention maps. (1) Point queries for SAM2 are obtained from attention maps via thresholding (τpq). (2) During mask propagation, highly overlapping masks are removed. (3) Spurious mask tracklets are removed using our scoring method. normalization, we normalize frame-level maps independently per frame, while video-level maps are normalized globally across all frames. Finally, the two sets of maps are fused by simple averaging, combining the global temporal context of video attention with the spatial precision of frame attention. Our videoframe decoupled prompting enables multi-scale processing, allowing higher-resolution inputs to be used for frame attention. Recent MLLMs, such as InternVL and LLaVA-NeXT, support dynamic image resolutions via tiling, whereas video inputs remain constrained to lower resolutions (e.g., 448). In contrast, QwenVL supports native resolutions for both video and image; in this case, we simply double the width and height for image inputs. To align modalities, low-res video attention maps are upsampled to match the frame-level resolution before fusion. 3.4 SAM2 PROMPTING WITH ATTENTION MAPS After DecAF process, we obtain the spatio-temporal attention maps, RTsHpWp , where Ts is the number of sampled frames and (Hp, Wp) is spatial resolution of visual token grid. Since this resolution is coarse, we introduce method to prompt SAM2 using the attention maps to produce fine-grained object masks, ˆM RT HW . Here, we use full frames at high-resolution, rather than the sampled frames used in the MLLM. The overall pipeline is illustrated in Fig. 3. Point Query Generation. Since SAM requires spatial prompts, we generate point queries directly from attention maps to guide object mask prediction. We select visual tokens with attention scores above threshold τpq and use their center coordinates as point queries. The set of point queries is defined as in Eq. 5, where ox and oy denote half the token width and height, respectively, ensuring that each point corresponds to the token center. = {p = (t, + oy, + ox) Vt,y,x τpq}. (5) Frame-wise Prompting and Propagation. Starting from the first frame, the point queries are fed sequentially to SAM2 which produces frame-level masks and propagates them through subsequent frames. This process generates video mask for each point query (pi), denoted as Mi RTsHW , together with its confidence score, sSAM , predicted by SAM2. = Vpi +sSAM Naive thresholding may generate large number of redundant masks. To reduce computation, we assign an object score sobj for each predicted mask, where Vpi is attention score of pi. We then apply non-maximum suppression (NMS) using this object score. Two masks are considered overlapping if their IoU exceeds threshold (e.g., 0.7), and the one with the lower score is removed. If propagated mask from previous frames highly overlaps with new mask in the current frame, we retain only the one with the higher object score. Through this process, we obtain video mask tracklets, where << P, effectively reducing redundancy while keeping high-quality candidates. Mask Tracklet Scoring and Selection. Since attention maps are at low resolution, point queries are not spatially precise and may fall on background regions. Nevertheless, SAM often produces high-confidence masks from such queries (e.g., walls), leading to false positives. To suppress these cases, we evaluate each mask tracklet using an attention consistency score (sac), which measures whether the mask consistently overlaps with high-attention regions across frames. Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation Table 1: Comparison of MLLM-based text-conditioned VOS methods that directly compute masks from attention maps (Attn Mask). All methods are training-free and grouped by MLLM. Method MLLM Ref-DAVIS ReasonVOS ReVOS (Overall) ReVOS (Referring) ReVOS (Reasoning) &F &F &F &F F &F Loc-Head [CVPR25] LLaVA-7B DecAF [Ours] 18.9 23.2 14.5 12.1 14.0 10.2 12.6 15.0 10.2 14.1 17.4 10.8 11.1 12.7 9.5 LLaVA-OV-7B 21.6 24.0 19.1 17.6 19.8 15.4 15.6 17.6 13.7 16.9 19.5 14.4 14.3 15.6 12.9 Loc-Head [CVPR25] InternVL3-8B InternVL3-8B DecAF [Ours] 19.0 24.1 14.0 14.4 15.9 12.9 14.6 16.8 12.4 16.5 19.3 13.7 12.7 14.2 11.1 20.5 25.3 15.8 18.3 21.4 15.2 16.6 19.9 13.4 18.1 22.0 14.2 15.1 17.8 12. TAM [ICCV25] Qwen2VL-7B Loc-Head [CVPR25] Qwen2VL-7B Qwen2VL-7B DecAF [Ours] 2.9 3.0 2.5 3.3 1.8 11.1 7.0 18.8 23.8 13.8 20.0 24.8 15.2 13.6 17.3 9.8 2.9 9.1 2.9 2.6 2.8 2.6 2.9 16.5 22.2 10.7 10.0 12.9 7.2 13.2 17.5 8.9 15.2 19.8 10.7 17.5 23.2 11.8 13.0 16.4 9.6 2.7 2. 2.7 3.1 TAM [ICCV25] 3.9 Loc-Head [CVPR25] Qwen2.5VL-7B 19.1 24.2 14.0 11.0 13.4 8.5 DecAF [Ours] 4.1 3.8 16.9 22.7 11.0 11.4 14.5 8.3 Qwen2.5VL-7B 25.3 32.0 18.6 20.6 25.9 15.2 20.2 26.0 14.5 22.1 28.8 15.4 18.3 23.1 13.5 4.0 4.0 4.0 14.1 18.6 9.6 Qwen2.5VL-7B 3. 2.8 4.3 3.4 3.6 3.8 4. 3.8 4.1 = Avg(Vpi, sSAM For each tracklet i, we then compute combined tracklet score, strk ). Tracklets with strk τtrk are retained and propagated across all video frames via SAM2 to generate the final dense segmentation masks. This procedure naturally supports both single-object and multiobject localization by treating each high-confidence query as an independent object hypothesis. The computation of sac is as follows. First, we obtain binary mask for each frame by thresholding the attention map at its mean score µt (Eq. 6). Second, we assign the negative maximum attention score per frame, δt = max(Vt,:,:), to regions below µt, (Eq. 7), penalizing low-attention areas. Finally, each mask tracklet is downampled to the attention map resolution, Mi RTsHpWp , and sac is computed as ratio of inner products (Eq. 8), where , denotes the tensor inner product. , sac MAttn t,y,x = (cid:26)1, Vt,y,x µt, 0, otherwise. (6) ˆVt,y,x = (cid:26)Vt,y,x, Vt,y,x µt, δt, otherwise. (7) sac = Mi, ˆV MAttn, ˆV (8)"
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EVALUATION SETTING Datasets and Evaluation Metrics. We evaluate our method on three referring VOS datasets: RefDAVIS (Khoreva et al., 2018), Ref-YouTube-VOS (Seo et al., 2020), and MeViS (Ding et al., 2023). In addition, we validate it on two reasoning VOS datasets: ReasonVOS (Bai et al., 2024) and ReVOS (Yan et al., 2024). Note that ReasonVOS provides only test set and is used for zeroshot evaluation, whereas the other datasets include training data. For evaluation, we employ the standard VOS metrics: region similarity (J ), contour accuracy (F), and their mean (J &F). Implementation Details. For mask generation directly from attention maps, we apply Otsus adaptive thresholding method (Otsu et al., 1975). By default, attention rollout starts from the middle LLM layer (e.g., 14 for 28 layers of Qwen2.5VL-7B), and SAM prompting threshold values of τtrk = 0.8 and τpq = 0.8. We use publicly released MLLM checkpoints and the SAM2-hiera-large. 4.2 COMPARISON WITH EXISTING METHODS USING MLLMS Mask without SAM. We evaluate segmentation masks obtained directly from MLLM attention maps using simple upscaling and thresholding, and compare them with existing methods in Tab. 1. Uniformly sampled 16 frames are used here. TAM (Li et al., 2025) performs poorly due to its strong dependence on predicted word tokens, making it unable to reliably ground expressions under our object-focused prompt. Further analysis of TAMs failure cases is provided in the Appendix. Loc-Head (Kang et al., 2025a) is also designed for text-conditioned segmentation, but operates in the image domain. Our method consistently outperforms Loc-Head across different MLLMs and datasets, with especially large margins on datasets require complex reasoning. This suggests that our method generalizes more effectively to reasoning-intensive scenarios, whereas localization heads rely on heuristic head selection and thus exhibit limited robustness. Despite these relative improvements, attention maps remain very low resolution, and the resulting scores are still below those of conventional segmentation models. In particular, contour accuracy (F) 7 Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation Table 2: Comparison of MLLM-based text-conditioned VOS methods. The upper gray rows correspond to training-based methods, while the lower colored rows correspond to training-free methods. ReasonVOS ReVOS (Referring) ReVOS (Reasoning) ReVOS (Overall) Ref-DAVIS Method MLLM &F &F &F &F F &F LLaVA-7B ChatUniVi-7B LISA [CVPR24] VISA [ECCV24] VideoLISA [NeurIPS24] LLaVA-Phi-3-V 68.8 64.9 72.7 47.5 45.1 49.9 GLUS [CVPR25] VRS-HQ [CVPR25] Veason-R1 [arxiv25.08] Qwen2.5VL-7B - 76.0 72.6 79.4 - LLaVA-7B ChatUniVi-7B - - - - - - - - - - 64.8 62.2 67.3 31.1 29.1 33.1 40.9 39.1 42.7 45.7 44.3 47.1 36.1 33.8 38.4 46.9 44.9 49.0 50.9 49.2 52.6 43.0 40.6 45.4 69.4 66.3 72.5 - - - - - - - - - 49.9 47.5 52.4 54.9 52.4 57.3 58.3 56.0 60.7 51.4 48.8 53.9 59.1 56.6 61.6 62.1 59.8 64.5 56.1 53.5 58.7 59.9 56.0 63.8 61.3 58.2 64.4 63.6 60.7 66.5 59.0 55.8 62.2 Loc-Head [CVPR25] DecAF [Ours] Loc-Head [CVPR25] DecAF [Ours] Loc-Head [CVPR25] DecAF [Ours] Loc-Head [CVPR25] DecAF [Ours] 56.3 52.1 60.5 33.6 29.3 38.0 32.5 28.2 36.9 36.9 32.5 41.3 28.1 23.8 32.5 LLaVA-7B 59.6 54.9 64.4 48.8 44.7 52.9 37.0 32.8 41.2 40.4 36.1 44.8 33.6 29.5 37.7 LLaVA-OV-7B 65.2 61.3 69.2 39.6 47.0 43.3 40.4 36.4 44.4 44.2 40.2 48.1 36.6 32.7 40.6 InternVL3-8B 61.7 56.3 67.2 56.0 52.1 59.9 43.8 39.9 47.6 47.8 44.0 51.6 39.7 35.9 43.5 InternVL3-8B 61.8 57.7 65.9 31.7 29.0 34.5 40.7 37.3 44.0 49.2 45.6 52.9 32.1 29.1 35.0 Qwen2VL-7B Qwen2VL-7B 63.2 58.3 68.1 49.1 45.4 52.9 42.0 38.3 45.7 49.4 45.7 53.2 34.5 30.9 38.1 Qwen2.5VL-7B 64.0 59.4 68.6 41.6 38.2 45.0 43.8 39.9 47.6 50.2 46.3 54.0 37.3 33.6 41.1 Qwen2.5VL-7B 75.2 70.7 79.7 59.7 56.1 63.4 50.3 46.2 54.4 54.8 50.8 58.7 45.9 41.6 50.1 Table 3: Comparison on additional datasets. Method MLLM MeViS Ref-YTVOS &F J &F Chat-UniVi-7B VISA [ECCV24] 44.5 VideoLISA [NeurIPS24] LLaVA-Phi-3-V 44.4 51.3 LLaVA-7B GLUS [CVPR25] 50.6 VRS-HQ [CVPR25] Chat-UniVi-7B 52.2 Veason-R1 [arxiv25.08] Qwen2.5VL-7B 41.8 47.1 41.3 47.6 48.5 54.2 47.6 53.7 48.4 56. Loc-Head [CVPR25] DecAF [Ours] Qwen2.5VL-7B Qwen2.5VL-7B 34.7 43.5 30.3 39.2 39.4 47.6 61.5 63.7 67.3 70.4 - 48.2 57. 59.8 61.7 65.5 68.3 - 44.0 53.3 63.2 65.7 69.0 72.5 - 52.5 60.6 is much lower than region similarity (J ), reflecting the inability of low-resolution attention maps to capture fine-grained boundaries opposite to the trend observed in segmentation-specialized models. These findings suggest that attention masks alone are too coarse for precise segmentation, but they provide sufficient coarse localization signal to guide SAM prompting (Ravi et al., 2024). Mask with SAM. We evaluate dense segmentation masks for all video frames using SAM2, and report the results in Tabs. 2 and 3, including both training-based and training-free methods. Loc-Head proposes its own SAM prompting method, but it is developed under single-object assumption: the largest bounding box (bbox) is obtained using the convex hull algorithm. Also, prompting with an imprecise bbox may result in segmenting non-target objects. On Ref-DAVIS with LLaVA-7B, Loc-Heads bbox prompting achieves 30.3, whereas our prompting achieves 56.3. This large gap highlights the advantage of our prompting method; thus, we adopt for all subsequent comparisons. In regards to training-free methods, our method outperforms Loc-Head across different MLLMs and datasets, including the additionally presented MeViS and Ref-YTVOS (Tab. 3). Although Loc-Head achieves slightly higher scores on Ref-DAVIS with InternVL3-8B, its performance drops substantially on ReasonVOS, which requires handling more complex expressions. Compared with training-based methods, our method achieves comparable or even superior performance. On Ref-DAVIS, our method with Qwen2.5VL-7B outperforms VISA and VideoLISA by 5.8 and 5.4 &F , respectively. On MeViS, our method achieves 43.5, which is very close to VISA (44.5) and VideoLISA (44.4). It is worth noting that recent state-of-the-art models (GLUS, VRSHQ, Veason-R1) leverage trained keyframe selection modules, whereas our method simply employs uniform sampling. Even with this difference, our approach surpasses GLUS on ReasonVOS and achieves only 0.2 lower than Veason-R1, despite Veason-R1 training the same MLLM (Qwen2.5VL) with an additional RL-based algorithm. This clearly manifests the effectiveness of our method. 4.3 ABLATION STUDY We use Qwen2.5-VL-7B (QVL2.5) and InternVL3-8B (IVL3) as models, and Ref-DAVIS (Ref-D) and ReasonVOS (ReasV) as datasets. By default, results are reported with QVL2.5 and &F. Decoupled Attention Fusion. We evaluate the effectiveness of DecAF. First, we examine objectbackground contrastive fusion (Tab. 4a), which substantially improves attention mask accuracy on Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation Table 4: Ablation study of decomposed attention fusion. (a) Object-background contrasting MLLM Obj Bg Attn Mask SAM Mask Ref-D ReasV Ref-D ReasV IVL3 QVL2.5 12.7 20.5 14.5 25.3 13.4 18.3 13.7 20. 48.7 61.7 62.0 75.2 50.0 56.0 53.8 59.7 (b) Video-frame complementing MLLM Vid Frm Ref-D ReasV IVL QVL2.5 45.6 56.3 61.7 66.2 68.2 75.2 46.2 45.0 56.0 53.4 54.4 59. Table 5: Ablation study of attention rollout. (a) Rollout method Table 6: SAM prompting threshold values. Method Ref-D ReasV Rollout (Abnar & Zuidema, 2020) Rollout Max (Lin et al., 2024) Rollout V-Max (Ours) 67.7 72.5 75.2 52.9 56.4 59.7 (b) Starting LLM layer for rollout Layer index 7 (1/4) 14 (2/4) 21 (3/4) Qwen2.5VL-7B InternVL3-8B Ref-D ReasV Ref-D ReasV 69.7 75.2 72. 58.4 59.8 59.9 61.8 61.7 55.1 53.8 56.0 56.5 τtrk τpq Ref-D ReasV 0.7 0.7 70.9 0.7 0.8 74.4 0.7 0.9 73.5 0.8 0.7 71.4 0.8 0.8 75.2 0.8 0.9 73.9 0.9 0.7 71.3 0.9 0.8 73.8 0.9 0.9 73.6 57.4 59.1 61.3 57.6 59.8 61.5 57.9 60.6 61. (c) Multi-scale complementing MLLM MS Ref-D ReasV IVL3 QVL2.5 54.1 61.7 72.6 75. 50.3 56.0 56.6 59.7 Table 7: Ablation study of computing attention consistency score (sac). Thresh (µ) Penalty (δ) Ref-D ReasV Not Use Otsu Otsu Mean Mean 58.6 68.2 67.3 64.6 75.2 49.1 55.4 57.5 51.4 59.7 Table 8: Evaluation on other sizes of MLLMs. MLLM Size Ref-DAVIS ReasonVOS ReVOS (Overall) ReVOS (Referring) ReVOS (Reasoning) &F &F J &F &F J &F InternVL3 Qwen2VL Qwen2.5VL 2B 8B 14B 2B 7B 3B 7B 52.0 61.7 62.6 41.5 63.2 58.3 75. 46.6 56.3 57.4 36.7 58.3 53.7 70.7 57.5 67.2 67.7 46.3 68.1 62.9 79. 46.8 56.0 59.0 45.2 49.1 50.8 59.7 43.0 52.1 55.3 41.8 45.4 47.1 56. 50.6 59.9 62.8 48.6 52.9 54.4 63.4 35.7 43.8 43.4 28.5 42.0 37.8 50. 31.5 39.9 39.3 24.6 38.3 33.6 46.2 39.8 47.6 47.5 32.3 45.7 41.9 54. 39.5 47.8 47.6 35.2 49.4 44.3 54.8 35.3 44.0 43.4 31.1 45.7 39.8 50. 43.6 51.6 51.8 39.3 53.2 48.7 58.7 31.9 39.7 39.2 21.7 34.5 31.2 45. 27.7 35.9 35.2 18.2 30.9 27.4 41.6 36.1 43.5 43.2 25.2 38.1 35.1 50. both referring and reasoning VOS datasets (e.g., 12.7 20.5 and 14.5 25.3 on Ref-D) by suppressing the irrelevant regions. Similar improvements are also observed for SAM mask accuracy. Second, video-frame complementary fusion (Tab. 4b) further enhances accuracy. For Qwen2.5VL, video-only and frame-only inputs yield 66.3 and 68.2 on Ref-D, respectively, whereas combining both achieves 75.2. Consistent gains are also observed on ReasV and with InternVL3. For video-frame fusion, we adopt multi-scale scheme that leverages higher resolution inputs at the frame level. While Qwen2.5VL supports native resolutions, InternVL and LLaVA-OV models require fixed input size but can handle dynamic high resolution image inputs through tiling. As shown in Tab. 4c, this multi-scale fusion brings additional improvements, particularly for InternVL, whose attention map resolution is very low without tiling. Attention Rollout. Tab. 5a compares our method with previous attention rollout methods. Our vision-aware head-weighted normalization further improves accuracy over the method of Lin et al. (2024). We also evaluate different LLM layers for rollout (Tab. 5b), and observe that selecting middle layer yields the best overall performance. SAM Prompting. Tab. 6 reports the results with different threshold values to filter point queries (τpq) and tracklets (τtrk) used in the SAM prompting process. Increasing τpq helps filter out nontarget objects or background regions, but too high value may result in missing points. While the optimal threshold combination varies across datasets, our method remains substantially robust to threshold choices, and we use τpq = 0.8 and τtrk = 0.8 across all datasets and models. Mask Tracklet Scoring. As shown in Tab. 7, we ablate the attention consistency score (sac), which contributes to the mask tracklet score (strk). Omitting sac and relying only on the object score sobj leads to significant accuracy drop. For MAttn, we compare Otsu thresholding and simple averaging to obtain µ, and for ˆV, we evaluate both with and without the penalty term (δ). Without δ, Otsu thresholding yields higher accuracy than mean thresholding, as it produces tighter object Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation masks. In contrast, with δ, mean thresholding performs better, as it tends to cover the entire object region, while any included background has low attention scores and low sac. MLLM Scalability. Tab. 8 shows that larger MLLMs generally yield better performance. InternVL3 improves from 52.0 to 62.6 on Ref-D and 46.8 59.0 on ReasV while Qwen2.5VL also scales effectively, with its 7B model achieving the best results across all datasets. Qualitative Results. Due to space limitations, qualitative results are provided in the Appendix."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We explore the intrinsic localization ability of MLLMs by casting video reasoning segmentation as video QA. Based on the attention rollout, we materialize the influence of visual tokens as attention maps, which can be converted into coarse segmentation masks via thresholding. To suppress noise, we propose decoupled attention fusion method and introduce an attention-guided SAM2 prompting pipeline that produces fine-grained masks in training-free manner."
        },
        {
            "title": "REFERENCES",
            "content": "Samira Abnar and Willem Zuidema. Quantifying attention flow in transformers. In Proceedings of the Association for Computational Linguistics (ACL), 2020. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Zechen Bai, Tong He, Haiyang Mei, Pichao Wang, Ziteng Gao, Joya Chen, Lei Liu, Zheng Zhang, and Mike Zheng Shou. One token to seg them all: Language instructed reasoning segmentation in videos. In Proceedings of Advances in Neural Information Processing Systems (NeurIPS), 2024. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Proceedings of Advances in Neural Information Processing Systems (NeurIPS), 33:18771901, 2020. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2418524198, 2024. Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, and Chen Change Loy. Mevis: large-scale In Proceedings of International benchmark for video segmentation with motion expressions. Conference on Computer Vision (ICCV), pp. 26942703, 2023. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2410824118, 2025. Sitong Gong, Lu Zhang, Yunzhi Zhuge, Xu Jia, Pingping Zhang, and Huchuan Lu. Reinforcing video reasoning segmentation to think before it segments, 2025a. URL https://arxiv. org/abs/2508.11538. Sitong Gong, Yunzhi Zhuge, Lu Zhang, Zongxin Yang, Pingping Zhang, and Huchuan Lu. The devil is in temporal token: High quality video reasoning segmentation. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2025b. 10 Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation Gemini Team Google. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, In Proceedand Weizhu Chen. LoRA: Low-rank adaptation of large language models. ings of International Conference on Learning Representations (ICLR), 2022. URL https: //openreview.net/forum?id=nZeVKeeFYf9. Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video understanding. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1370013710, 2024. Seil Kang, Jinyeong Kim, Junhyeok Kim, and Seong Jae Hwang. Your large vision-language model only needs few attention heads for visual grounding. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 93399350, June 2025a. Seil Kang, Jinyeong Kim, Junhyeok Kim, and Seong Jae Hwang. See what you are told: Visual attention sink in large multimodal models. arXiv preprint arXiv:2503.03321, 2025b. Anna Khoreva, Anna Rohrbach, and Bernt Schiele. Video object segmentation with language referring expressions. In Proceedings of Asian Conference on Computer Vision (ACCV), pp. 123141. Springer, 2018. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of International Conference on Computer Vision (ICCV), pp. 40154026, 2023. Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 95799589, June 2024. Yi Li, Hualiang Wang, Xinpeng Ding, Haonan Wang, and Xiaomeng Li. Token activation map to visually explain multimodal llms. In Proceedings of International Conference on Computer Vision (ICCV), 2025. Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023. Lang Lin, Xueyang Yu, Ziqi Pang, and Yu-Xiong Wang. Glus: Global-local reasoning unified into single large language model for video segmentation. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2025. Zhiwei Lin, Yongtao Wang, and Zhi Tang. Training-free open-ended object detection and segmentation via attention as prompts. In Proceedings of Advances in Neural Information Processing Systems (NeurIPS), 2024. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Proceedings of Advances in Neural Information Processing Systems (NeurIPS), 36, 2023. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2629626306, 2024. Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. In Proceedings of the Association for Computational Linguistics (ACL), 2024. Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very long-form video language understanding. Proceedings of Advances in Neural Information Processing Systems (NeurIPS), 36:4621246244, 2023. Nobuyuki Otsu et al. threshold selection method from gray-level histograms. Automatica, 11 (285-296):2327, 1975. Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. URL https://arxiv.org/abs/2408.00714. Seonguk Seo, Joon-Young Lee, and Bohyung Han. Urvos: Unified referring video object segmentation network with large-scale benchmark. In Proceedings of European Conference on Computer Vision (ECCV), pp. 208223. Springer, 2020. Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, et al. Moviechat: From dense token to sparse memory for long video understanding. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1822118232, 2024. OpenAI Team. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Proceedings of Advances in Neural Information Processing Systems (NeurIPS), 30, 2017. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024a. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024b. Cilin Yan, Haochen Wang, Shilin Yan, Xiaolong Jiang, Yao Hu, Guoliang Kang, Weidi Xie, and Efstratios Gavves. Visa: Reasoning video object segmentation via large language models. In Proceedings of European Conference on Computer Vision (ECCV), 2024. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, arXiv preprint Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv:2407.10671, 2024. Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llava-next: strong zero-shot video understanding model, April 2024. URL https://llava-vl.github.io/blog/2024-04-30-llava-next-video/. 12 Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation"
        },
        {
            "title": "A USE OF LARGE LANGUAGE MODELS",
            "content": "We utilized GPT for polishing our manuscript. Our usage is only limited to refining and grammar check of our own written draft."
        },
        {
            "title": "B PROMPT TEMPLATES",
            "content": "We describe here the prompts used to obtain object and background attention maps in Contrastive Object-Background Fusion. Object-focused Prompt. The object attention map is obtained from the attention weights produced when the MLLM answers prompt about the target object category referred to in the given expression. The prompt template is shown below: {Expression} What is the main object (or objects) referred to in the given expression or question? Focus on the **primary subject or agent** involved in the described action or behavior. Respond with single word (e.g., cat, person, dog) that best describes the target object(s). Background-focused Prompt. In contrast, the background attention map is derived from the attention weights produced when the MLLM responds to prompt that asks it to describe the background, excluding the target object category, in single word or short phrase. The prompt template is shown below: Describe the background scene of the video, excluding any {Object category}. Answer the question using single word or phrase. Object Category Choice Prompt. The quality of this contrastive fusion relies on the correctness of the object category. To ensure robust category selection, we first gather category predictions from both video-level and frame-level inputs and then confirm the final target category through an explicit query. The prompt template is shown below: {Expression} {Object category list} Identify the object class referred to by the expression. Given: - Expression: - Candidate object class list: Goal: Instructions: 1. If the expression is **clear**, rely on it directly (e.g., person driving car person). 2. support your decision (e.g., check frequency and plausibility). 3. lacks clarity. Output the most likely referred object class - just the label. If the expression is **vague**, use the object class list to Avoid defaulting to the most frequent class unless the expression The final object category is used to construct the background-focused prompts when obtaining videoand frame-level background attention maps. Importantly, the same set of prompt templates was applied across all MLLMs and datasets without any dataset-specific or model-specific modifications. 13 Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation Figure 4: Analysis of TAMs failure cases"
        },
        {
            "title": "C MORE DETAILS ABOUT PREVIOUS METHODS",
            "content": "TAM. TAM (Li et al., 2025) exhibits strong sensitivity to the predicted word tokens. As shown in Fig. 4 (a), when the model predicts the token bike, the resulting attention map is largely misaligned with the target object. In Fig. 4 (b), when the expression is changed to black bicycle, the word bicycle is split into two tokens, and the first token again shows severe misalignment. In contrast, Fig. 4 (c) displays the attention map for the second token icycle, which provides relatively better alignment with the target object. These examples demonstrate that TAMs localization is highly unstable and depends heavily on how object words are generated and tokenized. Moreover, decoding object or background categories typically spans multiple tokens, and the original evaluation protocol reports the best-performing token (i.e., the highest IoU among the predicted tokens) for each class. Such an evaluation overstates performance, underscoring TAMs lack of robustness in practical scenarios. Loc-Head. Loc-Head (Kang et al., 2025a) was originally proposed in the image domain, where attention maps from MLLMs are used to segment the target object referred to by given expression in training-free manner. The method consists of two stages: first, identifying localization heads and then generating object masks using the attention weights from these heads. In reproducing this method, we observed two major limitations. First, the procedure for discovering localization heads relies on sampling 1,000 imagetext pairs from RefCOCO. While heads discovered from RefCOCO yielded reasonable performance when evaluated on video datasets, re-discovering heads from samples drawn directly from video datasets led to substantial drop in performance. For example, the Attn-mask (J ) score decreased from 24.2 19.2 on RefDAVIS (Khoreva et al., 2018) and from 18.6 4.2 on ReVOS (Yan et al., 2024). Consequently, all experiments in our reproduction used the RefCOCO-discovered heads across datasets. Second, the head-selection process includes heuristic that excludes heads strongly attending to the bottom row to prevent the visual attention sink phenomenon. We found that this heuristic does not generalize across all models. For example, on Qwen2VL, applying the original heuristic resulted in score of 0.0 because the attention tended to concentrate in the right-most column rather than the bottom 14 Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation row. After adapting the rule to exclude heads that strongly attend to the right-most column, the Attnmask improved to 23.8. Similarly, for InternVL3, enabling tiling during head discovery degraded performance, indicating further sensitivity to preprocessing choices. These results suggest that the Loc-Head procedure does not generalize reliably across either models or datasets. second issue arises in producing dense segmentation masks. In Loc-Head, the attention map is first binarized using the mean attention score as threshold, after which the largest convex hull algorithm is applied to extract bounding box. This bounding box is then used as prompt to SAM for generating dense mask. However, because the attention map is coarse, the resulting bounding boxes are often inaccurate, leading to large degradation in the quality of the SAM masks. When we reproduced this procedure, the performance dropped significantly compared to the papers reported numbers; for instance, on RefCOCO validation the score decreased from 74.2 34.4. To ensure fair comparison, we therefore applied our SAM prompting process consistently across all video datasets. Overall, these findings highlight that Loc-Head approach depends heavily on dataset-specific sampling, model-specific heuristics. These issues make it difficult to obtain consistent results across models and datasets. In contrast, our proposed DecAF framework works reliably across different MLLMs and datasets, providing more consistent and generalizable performance compared to LocHead approach."
        },
        {
            "title": "D QUALITATIVE RESULTS",
            "content": "We provide qualitative results to demonstrate the effectiveness of our proposed Decomposed Attention Fusion (DecAF) and SAM prompting. Fig. 5, 6, 7, 8, 9 present diverse cases, including single-object, multi-object, small-object, temporal reasoning, and world knowledge scenarios. Each example shows the attention maps obtained through DecAF, the attention masks directly generated from the fused attention maps, and the dense masks obtained via SAM prompting. Across these scenarios, DecAF consistently produces attention maps that align with instructionreferred target objects, and both the attention masks and SAM masks accurately capture the object regions. Even in challenging settings involving multiple objects or small targets, our approach maintains robust localization and segmentation quality. Moreover, for cases requiring temporal reasoning or world knowledge, DecAF effectively leverages the capabilities of MLLMs to generate accurate masks without additional training. We also report several failure cases  (Fig. 10)  . As shown in Fig. 10 (a), our proposed attention consistency scoring method may underperform when the target object occupies large area in certain frames but the attention weights cover only small portion of that region. In such cases, the method assigns strong penalty, leading to low scores even when the mask tracklet is correctly generated. Similarly (Fig. 10 (c)), when the target object is small and appears only briefly in the video, it occupies only small fraction of the overall attention weights in the video, which results in low attention consistency scores and false filtering. Finally, Fig. 10 (b) shows that when the target object is extremely thin or elongated (e.g., paraglider lines), the attention maps fail to capture its structure, resulting in poor masks. 15 Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation Figure 5: Qualitative results for the single object case. Figure 6: Qualitative results for the multiple objects case. Figure 7: Qualitative results for the small object case. Figure 8: Qualitative results for the temporal reasoning case. 16 Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation Figure 9: Qualitative results for the world knowledge cases. 17 Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation Figure 10: Qualitative examples of failure cases."
        }
    ],
    "affiliations": [
        "Inha University",
        "NAVER Cloud",
        "Yonsei University"
    ]
}