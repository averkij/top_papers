{
    "paper_title": "Does Your Reasoning Model Implicitly Know When to Stop Thinking?",
    "authors": [
        "Zixuan Huang",
        "Xin Xia",
        "Yuxi Ren",
        "Jianbin Zheng",
        "Xuanda Wang",
        "Zhixia Zhang",
        "Hongyan Xie",
        "Songshi Liang",
        "Zehao Chen",
        "Xuefeng Xiao",
        "Fuzhen Zhuang",
        "Jianxin Li",
        "Yikun Ban",
        "Deqing Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in large reasoning models (LRMs) have greatly improved their capabilities on complex reasoning tasks through Long Chains of Thought (CoTs). However, this approach often results in substantial redundancy, impairing computational efficiency and causing significant delays in real-time applications. Recent studies show that longer reasoning chains are frequently uncorrelated with correctness and can even be detrimental to accuracy. In a further in-depth analysis of this phenomenon, we surprisingly uncover and empirically verify that LRMs implicitly know the appropriate time to stop thinking, while this capability is obscured by current sampling paradigms. Motivated by this, we introduce SAGE (Self-Aware Guided Efficient Reasoning), a novel sampling paradigm that unleashes this efficient reasoning potential. Furthermore, integrating SAGE as mixed sampling into group-based reinforcement learning (SAGE-RL) enables SAGE-RL to effectively incorporate SAGE-discovered efficient reasoning patterns into standard pass@1 inference, markedly enhancing both the reasoning accuracy and efficiency of LRMs across multiple challenging mathematical benchmarks."
        },
        {
            "title": "Start",
            "content": "Does Your Reasoning Model Implicitly Know When to Stop Thinking? Zixuan Huang 1 2 Xin Xia 2 Yuxi Ren 2 Jianbin Zheng 2 Xuanda Wang 2 Zhixia Zhang 1 Hongyan Xie 1 Songshi Liang 3 Zehao Chen 1 Xuefeng Xiao 2 Fuzhen Zhuang 1 Jianxin Li 1 Yikun Ban 1 Deqing Wang 1 6 2 0 2 9 ] . [ 1 4 5 3 8 0 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in large reasoning models (LRMs) have greatly improved their capabilities on complex reasoning tasks through long Chains of Thought (CoTs). However, this approach often results in substantial redundancy, impairing computational efficiency and causing significant delays in real-time applications. Recent studies show that longer reasoning chains are frequently uncorrelated with correctness and can even be detrimental to accuracy. In further in-depth analysis of this phenomenon, we surprisingly uncover and empirically verify that LRMs implicitly know the appropriate time to stop thinking, while this capability is obscured by current sampling paradigms. Motivated by this, we introduce SAGE (Self-Aware Guided Efficient Reasoning), novel sampling paradigm that unleashes this efficient reasoning potential. Furthermore, integrating SAGE as mixed sampling into groupbased reinforcement learning (SAGE-RL) effectively incorporates SAGE-discovered efficient reasoning patterns into standard pass@1 inference, markedly enhancing both the reasoning accuracy and efficiency of LRMs across multiple challenging mathematical benchmarks. 1. Introduction Reinforcement learning from verifiable rewards (RLVR) algorithms, such as GRPO (Shao et al., 2024; Yang et al., 2026) and GSPO (Zheng et al., 2025a), have played pivotal role in enabling test-time scaling. This capability allows large reasoning models (LRMs) like o3 (OpenAI, 2025a) and DeepSeek-R1 (Guo et al., 2025) to think longer. Longer CoTs enable LRMs to explore intermediate steps in greater depth and reduce abrupt logical leaps, thereby achieving unprecedented performance on challenging rea1Beihang University 2Bytedance China 3Renmin University of China. Correspondence to: Yikun Ban <yikunb@buaa.edu.cn>, Deqing Wang <dqwang@buaa.edu.cn>. Preprint. February 10, 2026. Figure 1. SAGE unleashes the efficient reasoning potential of LRMs obscured by pass@1 and identifies the optimal completions within the models capability hidden in pass@k. By enabling LRMs to learn these efficient reasoning patterns, SAGE-RL-tuned models simultaneously enhance reasoning capacity and conciseness on multiple challenging mathematical benchmarks. soning benchmarks such as AIME (Art of Problem Solving, 2024), OlympiadBench (Chaoqun et al., 2024) and IMO (Luong et al., 2025). While longer reasoning chains are expected for solving harder problems, prior work shows that length inflation can be uncorrelated with correctness, and that shorter chains may in fact yield better accuracy. For example, Balachandran et al. (2025) observe that on AIME 2025, DeepSeek-R1 produces responses nearly 5 longer than Claude 3.7 Sonnet while achieving comparable accuracy; Hassid et al. (2025) show that on AIME and HMMT, the shortest responses from QwQ-32B outperform randomly sampled ones by 2 percentage points using 31% fewer tokens. These findings collectively reveal that current CoT outputs often contain substantial redundancy and irrelevant tokens that do not Submission and Formatting Instructions for ICML 2026 contribute to the final solution. These unnecessary tokens dramatically reduce reasoning efficiency. This naturally raises pertinent question: do LRMs know the appropriate time to terminate thinking? We find that, during the exploration of multiple reasoning chains, LRMs consistently assign high confidence to concise yet effective reasoning paths. However, current samplingbased inference strategies typically overlook or fail to select these short and effective chains. Moreover, this phenomenon exhibits clear convergence behavior and becomes increasingly pronounced as the exploration space expands. Taken together, these results strongly indicate that reasoning models implicitly know the appropriate moment to terminate their reasoning process, but this capability is obscured by current pass@1 training and inference paradigms. Motivated by this insight, we introduce SAGE (Self-Aware Guided Efficient Reasoning), simple yet effective decoding strategy that leverages the reasoning models selfconfidence to discover relatively precise reasoning chains. By incorporating SAGE as mixed sampling into groupbased reinforcement learning (SAGE-RL), we enable the reasoning model to learn concise yet effective thinking patterns without altering its original reasoning paradigm. In summary, our contributions in this work are as follows: We uncover and demonstrate that LRMs implicitly know the appropriate time to stop thinking, but this capability is obscured by current sampling paradigms. We propose SAGE, novel sampling paradigm that unleashes the efficient reasoning potential of LRMs, simultaneously improving both accuracy and conciseness of reasoning chains. We propose SAGE-RL, simple modification to RLVR frameworks that integrates SAGE into the rollout process. As shown in Figure 1, SAGE-RL-tuned models achieve consistent gains across six challenging reasoning benchmarks, including MATH-500, AIME 2024, AIME 2025, AMC23, OlympiaBench and Minerva. 2. Dilemmas of Reasoning Models under"
        },
        {
            "title": "Current Sampling Paradigms",
            "content": "To investigate whether reasoning models possess the ability to recognize the appropriate moment to terminate thinking, we first need to re-examine the dilemmas faced by these models under current sampling paradigms. Pass@k: Scaling CoT length does not lead to correct answers. Assuming that LRMs using current sampling paradigms can reliably stop thinking at the appropriate moment, longer CoTs should outperform shorter ones in leading to correct solutions. However, extensive experiments involving multiple samplings of the same problem refute this assumption. Balachandran et al. (2025) observes that on AIME 2025, DeepSeek-R1 produces responses nearly 5 longer than Claude 3.7 Sonnet while achieving comparable accuracy; Hassid et al. (2025) also shows that on AIME and HMMT, the shortest responses from QwQ-32B outperform randomly sampled ones by 2 percentage points using 31% fewer tokens. Shrivastava et al. (2025) found that, on AIME 2025, in 72% of problems where both correct and incorrect answers were generated, the longer response was more likely to be incorrect than the shorter one. These findings collectively reveal that: once the chain-ofthought length reaches certain threshold, simply scaling the length further does not lead to corresponding improvement in the models reasoning capability. Furthermore, the optimal response within the models capability is obscured by existing sampling paradigms and can currently only be retrieved post hoc through test-time scaling methods. Pass@1: Existing sampling strategies fail to enable timely termination of thinking. To gain finer-grained understanding of these findings and precisely locate its root cause, we build upon this observation and take step further. Reasoning tasks, particularly in mathematical reasoning and code generation, typically require step-by-step answers. Leveraging this observation, we introduce simple metric to quantify the efficient reasoning capability of models: the Ratio of the First Correct Step (RFCS), defined as the step index at which the correct answer first appears divided by the total number of reasoning steps. Specifically, we utilize DeepSeek-distilled-Qwen-1.5B (DS1.5B), (DeepSeek-AI, 2025), DeepScaleR (Luo et al., 2025b) and Qwen3-8B (Yang et al., 2025a) to generate answers for MATH-500 (Lightman et al., 2023) problems. For each response, we segment it into distinct reasoning steps by nn (Chen et al., 2025a) and compute RFCS for each problem. As illustrated in Figure 2, the model correctly derives the answer using only 500 tokens, yet under the current sampling strategy, it continues with an additional 452 redundant tokens before terminating the reasoning process. This clearly demonstrates that the LRM fails to end its thinking at the appropriate moment on this problem. Such cases are not isolated in our study. More statistical results are summarized in Figure 3, where RFCS(< 1) and RFCS(avg) respectively denote the number of correct responses where RFCS is not equal to 1 and the average RFCS value across all correct responses. From the statistical results, all models exhibit significant ineffective steps in over half of the samples. Moreover, compared to DS-1.5B, models with higher post-training extent (DeepScaleR), or more advanced reasoning capabilities (Qwen3-8B) show no substantial improvement on this metric. This indicates that, in general scenarios, existing reasoning models struggle to 2 Submission and Formatting Instructions for ICML 2026 ϕ(yi; y<i) = log πθ(yi y<i, x). (2) Token-Wise Reasoning Path Exploration. We first propose token-wise reasoning path expansion algorithm until maximum step budget Tmax is reached. With exploration width (denoted as EW), we maintain the top-m candidate sequences according to the scoring function Φ, and expands them in subsequent decoding steps (Meister et al., 2020). Formally, given set of candidate sequences Yi1 = {y(1) i1 Yi1, [1, m], we select the top 2m most probable tokens i1} at timestep 1, for y(j) i1, . . . , y(m) i1, y(2) (j) = Top2m ({yi yi V}; ϕ(; y(j) i1)) (3) where Topm(; ϕ) denotes an operator that ranks all candidate elements in descending order according to their ϕscores and returns the subset consisting of the top 2m elements with the highest scores. This yields candidate group of size 2m = 2m2, formally written as Figure 2. Illustration of the step-by-step answering process. ˆYi = {y(j,k) [m], [2m]}, (4) Figure 3. Statistics of RFCS on MATH500 across LRMs. terminate their thinking process at the appropriate moment under the current inference paradigm (i.e., pass@1). In summary, the surprising performance of relatively shorter responses in pass@k reveals the inherent potential of the model for efficient reasoning. The pervasive redundancy of reasoning steps in pass@1 indicates that current sampling paradigms obscure this potential. Therefore, we attempt to adopt sampling strategy with larger exploration space built upon pass@1 to intentionally uncover the precise reasoning chains that are hidden within the broader pass@k distribution. 3. Intentionally Exploring Shorter CoTs Notations. Given query and prefix y<k = (y1, y2, ...yk1) previously generated by the language model πθ, We define Φ as the average cumulative logprobability up to generation step k, where ϕ(yi; y<i) is the (next-token) log-probability of the i-th token in πθ: Φ(yk) = 1 i=1 ϕ(yi; y<i). (1) where each candidate sequence is constructed by appending the k-th best token to the j-th beam: = y(j) y(j,k) i1 y(j,k) , y(j,k) (j). (5) We retain the top-m highest-scoring candidate sequences for next iteration: Yi = Topm ({y(j,k) [m], [2m]}; Φ). (6) Exploration Termination. We denote the Tolerance accept rank Ratio of </think> h/2m as TR, where {1, 2, . . . , 2m} is hyperparameter representing the tolerance for the rank of </think> . Given the required number of CoTs {1, 2, . . . , m}, once we have reached can- , where y(j,k) didate sequence y(j,k) is </think> and within the top-h probable tokens toph (j), we add it as completion to the candidate sequence set O. Otherwise, we discard this candidate sequence, as the models confidence in terminating the thinking process is low at this point (Liu et al., 2025). When r, we terminate the entire process. If when = Tmax and < r, we also add </think>}; Φ) to to ensure = r. toprO ({YTmax Greedy Sampling of the Answers. Through the above process, we generate reasoning chains ti for each question x. Next, we derive the answer πθ(aix, ti) greedily based on the query and the internal reasoning chains. Ultimately, for each question x, our decoding strategy generates completions {ti, ai}, [r]. 3 Submission and Formatting Instructions for ICML 2026 Notably, although our algorithm is built upon vanilla beam search, it exhibits significant differences. We provide detailed comparative analysis in Appendix B. 4. Your Reasoning Model Implicitly Knows"
        },
        {
            "title": "When to Stop Thinking",
            "content": "Built upon the observations of Section 2 and take step further, we conduct analytical experiments involving the following related algorithms: TSearch(m, r) w/ Φ denotes the algorithm from Section 3 with exploration width and returned completions. TSearch (m, r) w/ ϕ is TSearch variant used to ablate the role of Φ, which greedily retains the top-m candidate sequences with the most probable new token at each step according to the following equation instead of Equation 6: Yi = {y(j,k) (j, k) (cid:187)(cid:187)(cid:187)(cid:187)(cid:187)(cid:187) arg Topm ({y(j,k) [m], [2m]}; ϕ(; y(j) i1))}. EW = 0 denotes greedy sampling, which essentially represents degeneration of TSearch with no exploration. Li et al. (2023) found that its performance is comparable to the average results obtained from random sampling. Random refers to standard random sampling with temperature and top-p both set to 1.0. 4.1. High-Confidence Paths Lead to Efficient Reasoning We use reasoning chains retained by Φ to represent the highconfidence paths generated during TSearch. To assess the role of the Φ during this process, we compare TSearch w/ Φ with TSearch w/ ϕ across increasing exploration width m. is used for </think> detection. With TR = 1, termination occurs immediately upon </think> appearance, probably leading to significant length collapse. The results in Figure 4 clearly demonstrate the pivotal role of Φ. Observation 1 (Figure 4). In TSearch w/ Φ, increasing leads to consistent reduction in response length accompanied by steady improvement in accuracy. By contrast, TSearch w/ ϕ suffers rapid degradation in accuracy that closely tracks the sharp decline in response length. Furthermore, enlarging the exploration space represents an opportunity to enhance reasoning chain quality when Φ is present, whereas its absence makes length collapse and performance deterioration an inevitable consequence. These results indicate that the high-confidence branches preserved by Φ are not only markedly shorter, but also substantially more effective. (7) 4.2. High-Confidence Paths Lead to Confident Ends To further investigate the length collapse problem in TSearch w/ ϕ illustrated in Section 4.1, we apply TR to drop branches concluded with low confidence. The experimental results are shown in Table 1. Table 1. Comparison of TSearch (4,1) variants under different TR with the same settings of Figure 4. When TR < 1, TSearch prunes candidate sequences where the rank ratio of </think> within is lower than TR. ACC denotes the accuracy, LEN refers to the average response length, T-LEN represents the average number of think tokens. TR ACC T-LEN LEN Method Random TSearch (4,1) w/ ϕ TSearch (4,1) w/ ϕ TSearch (4,1) w/ ϕ - 1.00 0.75 0.50 TSearch (4,1) w/ Φ 1.00 TSearch (4,1) w/ Φ 0.75 TSearch (4,1) w/ Φ 0.50 0. 0.79 0.82 0.89 0.92 0.92 0.91 3126 1712 2022 2176 2213 2221 2212 2129 2333 2609 2609 2621 2632 Figure 4. Comparison of TSearch variants with increasing EW on DS-7B and randomly selected subset of MATH-500 (size = 100) under 10k token budget. To directly investigate the influence of Φ, we uniformly set TR = 1. As for TSearch w/ Φ, varying the TR has virtually no impact on performance. By comparison, it exerts strong influence on TSearch w/ ϕ. This indicates strong correlation between the presence of Φ and the ranking of </think> within . To further study the correlation between them, we record the average rank ratio at which </think> appears during TSearch and illustrate them in in Figure 5. Enlarging the exploration width influences TSearch in two contrasting ways. On the positive side, broader candidate token window facilitates the discovery of more varied reasoning paths and improves the probability of identifying optimal solutions among pass@k samples (Shrivastava et al., 2025; Hassid et al., 2025). On the negative side, larger We observe that as EW increases, the </think> token identified by TSearch w/ Φ consistently ranks first within the candidate set at the moment it appears when evaluated by Φ. This behavior indicates that the policy is highly confident in terminating the reasoning process once </think> enters . In contrast, for TSearch w/ ϕ, the rank ratio of 4 Submission and Formatting Instructions for ICML 2026 length), as illustrated in Figure 7. (1) At EW = 0, the model operates in completely nonexploratory regime and exhibits limited reasoning efficiency. This indicates that standard non-exploratory greedy or random sampling constrains the models inherent ability, which is fully consistent with the observations in Section 2. (2) As shown in Figure 15, enlarging the exploration width leads to consistent improvements in pass@1 while simultaneously reducing response length, with both metrics exhibiting trend toward gradual convergence. This trend further verifies reasoning models inherent efficient reasoning capability. From Figure 7, we can clearly find that this capability is progressively unleashed as the exploration width grows. (3) LRMs gradually approach the boundary of their inherent efficient reasoning capability as the degree of exploration increases, and this phenomenon is not an isolated occurrence but universal pattern observed across models and datasets. Figure 7. Token efficiency comparison on each run in Figure 15. Observation 3 (Figure 7). As the exploration space expands during reasoning, LRM is increasingly capable of identifying precise and compact reasoning paths with high confidence. Furthermore, with the continued growth of the exploration space, this behavior demonstrates an obvious convergence trend. Furthermore, as post-trained version of DS-1.5B, DeepScaleR exhibits steeper token efficiency improvement on both MATH-500 and AMC23. This suggests that greater post-training enhances the models ability to leverage increased exploration space for unleashing its intrinsic efficient reasoning potential. In summary, when provided with adequate exploration space, LRMs can identify precise and concise reasoning chains with high confidence and appropriately terminate the reasoning process, indicating that these models possess an inherent sense of when to stop reasoning. By contrast, current purely sampling-based strategies implicitly limit this capability of LRMs by relying solely on the next-token probability distribution. Figure 5. The average rank ratio of </think> in upon appearance. the </think> token gradually increases as measured by ϕ, suggesting increasing uncertainty about whether the next token should be </think> . This discrepancy explains the significant differences in the role of TR between TSearch w/ Φ and TSearch w/ ϕ, as reported in Table 1. Observation 2 (Figure 6). The policy implicitly exhibits high confidence in terminating high-confidence reasoning chain, as supported by TSEARCH with the cumulative probability Φ. However, the final </think> token may have relatively low next-token probability, which is revealed by TSEARCH w/ ϕ. This discrepancy indicates that many short yet high-quality reasoning chains are likely to be overlooked by greedy or random sampling strategies. Figure 6. Illustration of Observation 2. When reasoning branches are retained according to the models confidence at each expansion step, the model is able to conclude them with strong confidence. 4.3. Scaling Exploration Drives Capability Convergence In this section, we conduct further experiments to probe the upper boundary of the efficient reasoning capability illustrated in Section 4.1. Specifically, under sufficient token budget Tmax = 32,768, we adopt TSearch (m, 1) w/ Φ as the sampling strategy, and compare the pass@1 and response length of DS-1.5B and DeepScaleR on MATH-500 and AMC23 as the exploration width increases. An increase in corresponds to larger exploration space during the generation of reasoning chains. The results are shown in Figure 15. For clearer visualization of the models performance trends, we measure reasoning efficiency for each run in Figure 15 using token efficiency (pass@1 / response 5 Submission and Formatting Instructions for ICML Figure 8. Performance comparison with SAGE and Degrade-SAGE on MATH-500 and AMC23 under different generation step budgets. 5. Self-Aware Guided Efficient Reasoning 5.1. Methodology While Section 4.3 demonstrates that TSearch w/ Φ effectively unleashes the efficient reasoning potential of LRMs as the exploration space expands, the method remains inherently greedy. Our goal, however, is to translate this insight into random samplingbased inference paradigms. Fortunately, from prior analysis in Section 4.2, when Φ is present, </think> consistently achieves the top rank upon appearance. This observation implies that TSearch w/ Φ is effectively equivalent to directly identifying reasoning steps that terminate with </think> , rendering token-level reasoning chain expansion unnecessary. Based on this observation and built upon TSearch w/ Φ, we introduce Self-Aware Guided Efficient Reasoning (SAGE), simple yet effective sampling paradigm that performs step-wise reasoning chain expansion. SAGE differs from TSearch w/ Φ in only the following two respects: Step-Wise Reasoning Chain Exploration. At step i, each candidate sequence is extended by one full reasoning step until the maximum reasoning step limit Tmax is reached: , r(j,k) R(j), r(j,k) = y(j) y(j,k) i1 where R(j) {r(j,1) } denotes the set of 2m reasoning steps independently sampled from the policy πθ conditioned on the query and prefix y(j) i1 using vanilla random sampling. This process replaces the token-level expansion in Equation 5. , . . . , r(j,2m) , r(j,2) (8) Exploration Termination. Based on the conclusions in Section 4.2, we no longer need to manually set the tolerance rank ratio TR as the high-confidence reasoning branches consistently lead to confident ends. Our termination condition can be simply defined as : If we have reached candidate sequence y(j,k) ends with </think> , we add it as completion to the candidate sequence set O. , where r(j,k) k 5.2. SAGE Inference Scaling Trends with Step Budget We introduce step-wise alternative to random sampling namely Degrade SAGE to ablate the exploration space of 6 SAGE. Degrade SAGE directly samples one reasoning step at each iteration until </think> appears or Tmax is reached. To balance computational efficiency and performance (discussed in Appendix D.4), we adopt SAGE (2,1) as the representative of our algorithm. We scale the maximum reasoning step budget gradually and compare the pass@1 and response length of SAGE and Degraded SAGE on MATH-500 (mean@4) and AMC23 (mean@16) respectively. We mark Random results for DeepScaleR and DS-1.5B at 32,768 token budget with red and blue dashed lines, respectively. (1) The inference scaling trends of SAGE demonstrate the models capability to terminate thinking at appropriate timings. Under constrained step budgets, SAGE outperforms Degraded SAGE in pass@1 with similar sequence lengths. This advantage stems from SAGE stopping thinking earlier, leading to more complete CoTs. When step budgets are ample, relatively stable performance gap emerges between SAGE and Degraded SAGE. Here, with token count no longer bottleneck, the difference stem solely from reasoning chain exploration. These results clearly show that SAGE effectively identifies reasoning chains superior to those of Degraded SAGE, as they are both shorter and more likely to lead to correct answers. (2) SAGE prioritizes performance for strong models and hard datasets, and efficiency for weaker models and simple datasets. On stronger DeepScale and harder AMC23, we observe greater pass@1 gains. In contrast, on weaker DS-1.5B and simpler MATH-500, we note larger response length reductions. From the models perspective, stronger models have higher capability ceiling, enabling SAGE to deliver larger accuracy gains with more necessary tokens. In contrast, weaker models suffer from more severe overthinking, creating more chances for token redundancy reduction. From the datasets perspective, LRMs can solve most problems on easier datasets, making response length the key optimization goal. By exploiting the models inherent sense of when to stop thinking, SAGE identifies shorter reasoning chains to reduce response length significantly. Conversely, harder datasets contain more challenging problems requiring more tokens to solve, and SAGE boosts accuracy notably on them, confirming its efficacy on uncovering correct reasoning chains with minimal necessary tokens. Submission and Formatting Instructions for ICML 2026 Table 2. Pass@1, response length (LEN) and token efficiency (TE) results on four complex mathematical benchmarks. TE is calculated as Pass@1 / LEN. Bold and underlined denote the best and second-best results. Method DS-1.5B + LC-R1 + ThinkPrune-2k + AdaptThink + Efficient Reasoning + GRPO + SAGE-GRPO + GSPO + SAGE-GSPO DeepScaleR + ThinkPrune-2k + GRPO + SAGE-GRPO DS-7B + LC-R1 + AdaptThink + Efficient Reasoning + GRPO-LEAD + GRPO + SAGE-GRPO Qwen3-8B + GRPO + SAGE-GRPO + GSPO + SAGE-GSPO MATH-500 AIME 2024 AIME 2025 OlympiadBench Pass@1(%) LEN TE(10 3) Pass@1(%) LEN TE( 3) Pass@1(%) LEN TE(10 3) Pass@1(%) LEN TE(10 3) 83.2 80.4 (2.8) 81.7 (1.5) 80.4 (2.8) 82.0 (1.2) 83.6 (0.4) 84.8 (1.6) 83.4 (0.2) 85.2 (2.0) 86.0 82.5 (3.5) 87.6 (1.6) 88.8 (2.8) 91.6 87.3 (4.3) 88.9 (2.7) 89.8 (1.8) 89.5 (2.1) 92.0 (0.4) 93.0 (1.4) 94.4 93.6 (0.8) 95.0 (0.6) 94.6 (0.2) 94.4 (0.0) 4882 2973 (1909) 2826 (2056) 2563 (2319) 2821 (2061) 3907 (975) 2915 (1967) 3898 (984) 2921 (1961) 3805 2946 (859) 3482 (323) 3117 (688) 3871 2076 (1795) 2199 (1672) 2408 (1463) 2752 (1119) 3219 (652) 2141 (1730) 5640 4470 (1170) 3015 (2625) 4342 (1298) 2753 (2887) 17.0 27.0 (58.8%) 28.9 (70.0%) 31.4 (84.1%) 29.1 (70.6%) 21.4 (25.6%) 29.1 (70.7%) 25.3 (21.4%) 29.2 (71.6%) 22.6 28.0 (23.9%) 25.2 (11.3%) 28.4 (25.7%) 23.7 42.1 (77.7%) 40.4 (70.9%) 37.3 (57.6%) 32.5 (37.1%) 28.5 (20.2%) 43.4 (83.1%) 16.7 20.9 (25.1%) 31.5 (88.2%) 22.2 (32.9%) 34.3 (105.3%) 25.1 23.3 (1.8) 23.7 (1.4) 25.7 (0.6) 26.2 (1.1) 28.3 (3.2) 28.8 (3.7) 28.3 (3.2) 28.5 (3.4) 31.4 33.5 (2.1) 35.6 (4.2) 36.1 (4.7) 51.9 51.7 (0.2) 52.1 (0.2) 51.9 (0.0) 53.1 (1.2) 52.5 (0.6) 55.3 (3.4) 73.2 72.8 (0.4) 73.5 (0.3) 73.0 (0.2) 73.7 (0.5) 12300 7098 (5202) 7085 (5215) 8055 (4245) 9189 (3111) 8767 (3533) 7243 (5057) 8604 (3696) 6889 (5411) 9370 8108 (1262) 8592 (778) 8094 (1276) 11305 6820 (4485) 6679 (4626) 6667 (4638) 7023 (4282) 8424 (2881) 6422 (4883) 15920 10573 (5347) 8975 (6945) 10544 (5376) 8547 (7373) 2.04 3.28 (60.8%) 3.35 (64.2%) 3.19 (56.4%) 2.85 (39.7%) 3.23 (58.3%) 3.98 (95.1%) 3.29 (61.3%) 4.14 (102.9%) 3.35 4.13 (23.3%) 4.14 (23.6%) 4.46 (33.1%) 4.59 7.58 (65.1%) 7.80 (69.9%) 7.78 (69.5%) 7.56 (64.7%) 6.23 (35.7%) 8.61 (87.6%) 4.60 6.89 (49.8%) 8.19 (78.0%) 6.92 (50.4%) 8.62 (87.4%) 20.9 20.9 (0.0) 19.7 (1.2) 21.8 (0.9) 22.9 (2.0) 24.1 (3.2) 26.5 (5.6) 25.1 (4.2) 27.1 (6.2) 25.4 26.0 (0.6) 27.4 (2.0) 27.2 (1.8) 37.1 35.7 (1.4) 35.0 (2.1) 36.2 (0.9) 36.1 (1.0) 38.4 (1.3) 38.0 (0.9) 67.3 66.6 (0.7) 66.6 (0.7) 66.2 (1.1) 66.0 (1.3) 11669 6942 (4727) 6918 (4751) 8155 (3514) 8590 (3079) 8263 (3406) 7479 (4190) 8227 (3442) 7167 (4502) 9310 7486 (1824) 8185 (1125) 7704 (1606) 12540 7458 (5082) 7807 (4733) 7501 (5039) 7842 (4698) 10123 (2417) 6583 (5957) 18342 13981 (4361) 10052 (8290) 14082 (4260) 9183 (9159) 1.79 3.01 (68.2%) 2.85 (59.2%) 2.67 (49.2%) 2.67 (49.2%) 2.92 (63.1%) 3.54 (97.8%) 3.05 (70.4%) 3.78 (111.1%) 2.73 3.47 (27.1%) 3.35 (22.7%) 3.53 (29.3%) 2.96 4.79 (61.8%) 4.48 (72.3%) 4.82 (62.8%) 4.60 (55.4%) 3.79 (28.0%) 5.77 (94.9%) 3.67 4.76 (29.7%) 6.58 (79.3%) 4.70 (30.2%) 7.19 (95.9%) 33.4 32.0 (1.4) 32.9 (0.5) 32.6 (0.8) 33.8 (0.4) 34.2 (0.8) 36.9 (3.5) 34.6 (1.2) 37.3 (3.9) 35.9 35.1 (0.8) 36.2 (0.3) 36.5 (0.6) 39.8 41.4 (1.6) 38.9 (0.9) 40.1 (0.3) 40.6 (0.8) 41.2 (1.4) 41.8 (2.0) 46.6 45.1 (1.5) 45.4 (1.2) 46.6 (0.0) 46.7 (0.1) 8954 4632 (4322) 4752 (4202) 4563 (4391) 5755 (3199) 6323 (2631) 5050 (3904) 6410 (2544) 5172 (3782) 5972 4723 (1249) 5443 (529) 4890 (1082) 7839 4193 (3646) 4915 (2924) 4599 (3240) 4972 (2867) 5498 (2341) 4435 (3404) 11707 7512 (4195) 5972 (5735) 7964 (3743) 5436 (6271) 3.73 6.91 (85.3%) 6.92 (85.5%) 7.14 (91.4%) 5.87 (57.4%) 5.41 (45.0%) 7.31 (96.0%) 5.40 (44.8%) 7.21 (93.3%) 6.01 7.43 (23.6%) 6.65 (10.6%) 7.46 (24.1%) 5.08 9.87 (94.3%) 7.91 (55.7%) 8.72 (71.7%) 8.17 (60.8%) 7.50 (47.6%) 9.42 (85.4%) 4.00 6.00 (50.0%) 7.60 (90.0%) 5.85 (46.2%) 8.59 (114.7%) 6. SAGE-RL: Integrating Efficient Reasoning Patterns into Current Inference Paradigms As shown in Section 5, SAGE effectively unleashes reasoning models implicit capacity for efficient reasoning. An appealing extension is to incorporate the efficient reasoning pattern uncovered by SAGE into standard pass@1 inference. Thus, we introduce SAGE-RL, simple modification to RLVR, to achieve this goal. Given question q, RLVR typically samples group of responses = {o1, . . . , oG} from the current policy. The sole difference between SAGE-RL and RLVR lies in the rollout phase, where SAGE-RL employs hybrid sampling strategy. SAGE-RL employs SAGE (m,r) to generate responses {oS 2 , . . . , oS } and uses standard random sampling for the remaining Gr responses {oR Gr}. Ultimately, the rollout phase in SAGE-RL yields the set of , oR responses = {oS Gr} for each q. 1 , . . . , oR 2 , . . . , oR 1 , . . . , oS 1 , oR 1 , oS 7. Experiments We apply both RLVR (GRPO (Shao et al., 2024), GSPO (Zheng et al., 2025a) ) and corresponding SAGE-RL method (SAGE-GRPO, SAGE-GSPO) to tune four widely adopted LRMs with group size of = 8. The training objectives of these algorithms can be found in Appendix C.1. Within each group, SAGE-RL employs SAGE (2,2) to search for two completions with precise reasoning chains, while the remaining six completions are obtained through default random sampling in verl. We also compare with existing open-source methods, including LC-R1 (Cheng et al., 7 2025), ThinkPrune (Hou et al., 2025), AdaptThink (Zhang et al., 2025), Efficient-Reasoning (Arora et al., 2025), and GRPO-LEAD (Zhang et al., 2025). Additional implementation details are provided in Appendix C.2 due to space constraints. 7.1. Main Results Table 2 presents performance comparison among SAGERL and baselines. Due to space constraints, we present results from only four out of the six evaluated datasets. The complete experimental results and additional analysis are provided in Appendix D. (1) SAGE-RL achieves comprehensive improvements in both reasoning capability and token efficiency. As shown in Table 2, most baselines achieve token compression at the cost of reduced reasoning capability. For instance, on MATH-500, AdaptThink compresses the token count of DS-1.5B from 4,882 to 2,563, but at the expense of 2.8% drop in pass@1. Similar performance degradation is also widely observed across AIME 2024, AIME 2025 and OlympiaBench. RLVR was initially proposed to improve reasoning performance through extended reasoning lengths (DeepSeek-AI, 2025), yet existing baselines compromise this capability to different extents. In contrast, SAGE-RL consistently achieve the best or second-best token efficiency across all benchmarks, while effectively improving the base models capabilities on these complex reasoning tasks. This is because SAGERL achieves efficient reasoning by enabling LRMs to learn more precise reasoning chains, simultaneously shortening Submission and Formatting Instructions for ICML 2026 Figure 9. Training Dynamics comparison between RLVR and SAGE-RL. The left two figures present results evaluated every 10 steps on MATH-500 under an 8,192 token budget. The right two figures illustrate the entropy and KL divergence of the policy for every step. the inference trajectories while enhancing reasoning capability. As illustrated in Figure 8, the reasoning chains sampled by SAGE are shorter than those from standard sampling and more effectively guide the model toward correct solutions. In group-based comparison processes similar to GRPO, this advantage is amplified by the baselines regularization. Since SAGE more frequently yields high-reward outcomes, the policy model naturally shifts its reasoning patterns toward the efficient modes discovered by SAGE. (2) SAGE-RL effectively enables LRMs to learn efficient reasoning patterns. As shown in Table 2, although vanilla GRPO and GSPO moderately improve the reasoning capability of LRMs compared to other baselines, the inference trajectories learned by LRMs from standard random sampling still contain substantial token redundancy. Consequently, the overall token efficiency remains significantly lower than that of efficient reasoning baselines. In contrast, SAGE-RL achieves substantial improvements in both reasoning capability and token efficiency. Since the only difference lies in the sampling strategy for 2 out of 8 samples per group, the results demonstrate that SAGE-RL effectively enables the policy model to learn shorter yet more accurate reasoning patterns. Figure 9 clearly illustrates this process. As training progresses, deploying SAGE-RL on both GRPO and GSPO leads to more pronounced improvements in pass@1 and greater reductions in response length. In contrast to standard RLVR, SAGE-RL shows more significant entropy reduction, suggesting that the policy model gradually acquires the precise reasoning chains identified by SAGE, resulting in greater confidence during inference as training progresses. In terms of KL divergence, SAGE-RL also exhibits more pronounced increasing trend. This indicates that the policy model deviates more significantly from the original probability distribution as training progresses. Such behavior suggests that the reasoning chains generated by SAGE, compared to those from random sampling, induce larger updates in the model. This is primarily because unleashing the models efficient reasoning capability requires more substantial updates to learn reasoning patterns that Figure 10. Statistics of RFCS on MATH-500 across different SAGE-RL-tuned models. differ markedly from the original ones. As SAGE-RLs improvement solely stems from the rollout phase, the direct comparison with RLVR in this section serves as an effective ablation study of our approach. 7.2. Analysis on Reasoning Behavior We computed the RFCS metric on MATH-500 for SAGEGRPO-tuned models, with results shown in Figure 10. Across all models, the proportion of samples with RFCS(<1) decreases substantially compared to Figure 3, indicating significant reduction in redundant reasoning steps. Simultaneously, the RFCS(avg) increases markedly, suggesting that the reasoning models more frequently terminate thinking immediately after producing the correct answer. As shown in Figure 16 and Figure 17, SAGE-GRPO-tuned models effectively avoid generating large number of ineffective reasoning steps. These findings strongly confirm that SAGERL effectively teaches LRMs precise reasoning patterns. 8. Conclusion In this work, we uncover and demonstrate that LRMs implicitly know the appropriate time to stop thinking, but this potential is obscured by current sampling paradigms. Built on this observation, We propose SAGE, sampling paradigm that unleash this capability to uncover precise reasoning chains, yielding significantly CoT length reduction and accuracy improvement. By simply integrating SAGE into the rollout process of RLVR, SAGE-RL achieves lasting gains in inference-time reasoning efficiency. 8 Submission and Formatting Instructions for ICML"
        },
        {
            "title": "Impact statement",
            "content": "This paper uncovers and demonstrates the inherent efficient reasoning potential of LRMs, contributing to the broader field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Aggarwal, P. and Welleck, S. L1: Controlling how long reasoning model thinks with reinforcement learning. arXiv preprint arXiv:2503.04697, 2025. Aggarwal, P., Madaan, A., Yang, Y., et al. Lets sample step by step: Adaptive-consistency for efficient reasoning and coding with llms. arXiv preprint arXiv:2305.11860, 2023. Arora, D. and Zanette, A. Training language models to reason efficiently. arXiv preprint arXiv:2502.04463, 2025. Arora et al. Training language models to reason efficiently, 2025. URL https://arxiv.org/abs/2502.04463. Art of Problem Solving. American examination. tional mathematics //artofproblemsolving.com/wiki/index.php/ American Invitational Mathematics Examination, 2024. Accessed: 2025-03-28. invitahttps: Aytes, S. A., Baek, J., and Hwang, S. J. Sketch-of-thought: Efficient llm reasoning with adaptive cognitive-inspired sketching. arXiv preprint arXiv:2503.05179, 2025. Balachandran, V., Chen, J., Chen, L., Garg, S., Joshi, N., Lara, Y., Langford, J., Nushi, B., Vineet, V., Wu, Y., et al. Inference-time scaling for complex tasks: Where we stand and what lies ahead. arXiv preprint arXiv:2504.00294, 2025. Chaoqun, H., Renjie, L., Yuzhuo, B., Shengding, H., Zhen, T., Junhao, S., Jinyi, H., Xu, H., Yujie, H., Yuxiang, Z., Jie, L., Lei, Q., Zhiyuan, L., and Maosong, S. OlympiadBench: challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 38283850, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.211. URL https: //aclanthology.org/2024.acl-long.211/. Chen, Q., Qin, L., Wang, J., Zhou, J., and Che, W. Unlocking the capabilities of thought: reasoning boundary framework to quantify and optimize chain-of-thought. 9 Advances in Neural Information Processing Systems, 37: 5487254904, 2024. Chen, R., Zhang, Z., Hong, J., Kundu, S., and Wang, Z. Seal: Steerable reasoning calibration of large language models for free. arXiv preprint arXiv:2504.07986, 2025a. Chen, X., Xu, J., Liang, T., He, Z., Pang, J., Yu, D., Song, L., Liu, Q., Zhou, M., Zhang, Z., Wang, R., Tu, Z., Mi, H., and Yu, D. Do NOT think that much for 2+3=? on the overthinking of long reasoning models. In Forty-second International Conference on Machine Learning, 2025b. URL https://openreview.net/ forum?id=MSbU3L7V00. Chen, Z., Ai, T., Li, Y., Li, G., Wei, Y., Zhou, W., Li, G., Yu, B., Chen, Z., Sun, H., Zhuang, F., Li, J., Wang, D., and Ban, Y. Llmboost: Make large language models stronger with boosting, 2025c. URL https://arxiv.org/abs/ 2512.22309. Cheng, Z., Chen, D., Fu, M., and Zhou, T. Optimizing length compression in large reasoning models, 2025. URL https://arxiv.org/abs/2506.14755. Chuang, Y.-N., Zhou, H., Sarma, P., Gopalan, P., Boccio, J., Bolouki, S., and Hu, X. Learning to route llms with confidence tokens. arXiv preprint arXiv, 2410, 2024. Cui, Y., He, P., Zeng, J., Liu, H., Tang, X., Dai, Z., Han, Y., Luo, C., Huang, J., Li, Z., et al. Stepwise perplexity-guided refinement for efficient chainof-thought reasoning in large language models. arXiv preprint arXiv:2502.13260, 2025. Dai, M., Yang, C., and Si, Q. S-grpo: Early exit via reinforcement learning in reasoning models. arXiv preprint arXiv:2505.07686, 2025. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Fan, C., Zhang, Y., Jia, J., Hero, A., and Liu, S. Cyclicreflex: Improving large reasoning models via cyclical reflection token scheduling, 2025. URL https: //arxiv.org/abs/2506.11077. Gao, J., Xu, S., Ye, W., Liu, W., He, C., Fu, W., Mei, Z., Wang, G., and Wu, Y. On designing effective rl reward at training time for llm reasoning, 2024. URL https://arxiv.org/abs/2410.15115. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Submission and Formatting Instructions for ICML 2026 Han, T., Wang, Z., Fang, C., Zhao, S., Ma, S., and Chen, Z. Token-budget-aware llm reasoning, 2024. Han, T., Wang, Z., Fang, C., Zhao, S., Ma, S., and Chen, Z. Token-budget-aware llm reasoning, 2025. URL https: //arxiv.org/abs/2412.18547. Hassid, M., Synnaeve, G., Adi, Y., and Schwartz, R. Dont overthink it. preferring shorter thinking chains for improved llm reasoning. arXiv preprint arXiv:2505.17813, 2025. He, X., Ban, Y., Zou, J., Wei, T., Cook, C., and He, J. Llm-forest: Ensemble learning of llms with graphIn Findings augmented prompts for data imputation. of the Association for Computational Linguistics: ACL 2025, pp. 69216936, 2025. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Hou, B., Zhang, Y., Ji, J., Liu, Y., Qian, K., Andreas, J., and Chang, S. Thinkprune: Pruning long chain-of-thought of llms via reinforcement learning, 2025. URL https: //arxiv.org/abs/2504.01296. Huang, S., Wang, H., Zhong, W., Su, Z., Feng, J., Cao, B., and Fung, Y. R. Adactrl: Towards adaptive and controllable reasoning via difficulty-aware budgeting, 2025a. URL https://arxiv.org/abs/2505.18822. Kimi Team. Kimi k1.5: Scaling reinforcement learning with llms, 2025b. URL https://arxiv.org/abs/2501. 12599. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Lambert, N., Morrison, J., Pyatkin, V., Huang, S., Ivison, H., Brahman, F., Miranda, L. J. V., Liu, A., Dziri, N., Lyu, S., Gu, Y., Malik, S., Graf, V., Hwang, J. D., Yang, J., Bras, R. L., Tafjord, O., Wilhelm, C., Soldaini, L., Smith, N. A., Wang, Y., Dasigi, P., and Hajishirzi, H. Tulu 3: Pushing frontiers in open language model post-training, 2025. URL https://arxiv.org/abs/2411.15124. Lee, A., Che, E., and Peng, T. How well do llms compress their own chain-of-thought? token complexity approach. arXiv preprint arXiv:2503.01141, 2025. Lewkowycz, A., Andreassen, A. J., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V. V., Slone, A., Anil, C., Schlag, I., Gutman-Solo, T., Wu, Y., Neyshabur, B., GurAri, G., and Misra, V. Solving quantitative reasoning problems with language models. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. URL https:// openreview.net/forum?id=IFXTZERXdM7. Huang, X., Vangani, T. K., Liu, Z., Zou, B., and Aw, A. T. Adacot: Rethinking cross-lingual factual reasoning through adaptive chain-of-thought, 2025b. URL https://arxiv.org/abs/2501.16154. Li, Y., Yuan, P., Feng, S., Pan, B., Wang, X., Sun, B., Wang, H., and Li, K. Escape sky-high cost: Early-stopping self-consistency for multi-step reasoning. arXiv preprint arXiv:2401.10480, 2024. Huang, Z., Ban, Y., Fu, L., Li, X., Dai, Z., Li, J., and Wang, D. Adaptive sample scheduling for direct preference optimization. arXiv preprint arXiv:2506.17252, 2025c. Huang, Z., Xia, X., Ren, Y., Zheng, J., Xiao, X., Xie, H., Li, H., Liang, S., Dai, Z., Zhuang, F., Li, J., Ban, Y., and Wang, D. Real-time aligned reward model beyond semantics. 2026. URL https://api.semanticscholar. org/CorpusID:285240754. Kang, Y., Sun, X., Chen, L., and Zou, W. C3ot: Generating shorter chain-of-thought without compromising effectiveness. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 2431224320, 2025. Kimi Team. Kimi k2: Open agentic intelligence. https: //moonshotai.github.io/Kimi-K2/, 2025a. Li, Z., Xu, T., Zhang, Y., Lin, Z., Yu, Y., Sun, R., and Luo, Z.-Q. Remax: simple, effective, and efficient reinforcement learning method for aligning large language models. arXiv preprint arXiv:2310.10505, 2023. Liao, B., Xu, Y., Dong, H., Li, J., Monz, C., Savarese, S., Sahoo, D., and Xiong, C. Reward-guided speculative decoding for efficient llm reasoning. arXiv preprint arXiv:2501.19324, 2025. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Lets verify step by step, 2023. URL https: //arxiv.org/abs/2305.20050. Liu, T., Guo, Q., Hu, X., Jiayang, C., Zhang, Y., Qiu, X., and Zhang, Z. Can language models learn to skip steps? arXiv preprint arXiv:2411.01855, 2024. Submission and Formatting Instructions for ICML 2026 Liu, Y., Zheng, J., Sun, Z., Peng, Z., Dong, W., Sha, Z., Cui, S., Wang, W., and He, X. Thought manipulation: External thought can be efficient for large reasoning models. arXiv preprint arXiv:2504.13626, 2025. Luo, H., Shen, L., He, H., Wang, Y., Liu, S., Li, W., Tan, N., Cao, X., and Tao, D. O1-pruner: Length-harmonizing fine-tuning for o1-like reasoning pruning. arXiv preprint arXiv:2501.12570, 2025a. Luo, M., Tan, S., Wong, J., Shi, X., Tang, W. Y., Roongta, M., Cai, C., Luo, J., Li, L. E., Popa, R. A., and Stoica, I. DeepScaleR: Surpassing O1-Preview with 1.5B Model by Scaling RL, 2025b. Luong, M.-T., Hwang, D., Nguyen, H. H., Ghiasi, G., Chervonyi, Y., Seo, I., Kim, J., Bingham, G., Lee, J., Mishra, S., et al. Towards robust mathematical reasoning. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pp. 35406 35430, 2025. Ma, W., He, J., Snell, C., Griggs, T., Min, S., and Zaharia, M. Reasoning models can be effective without thinking. arXiv preprint arXiv:2504.09858, 2025a. Ma, X., Wan, G., Yu, R., Fang, G., and Wang, X. Cot-valve: arXiv Length-compressible chain-of-thought tuning. preprint arXiv:2502.09601, 2025b. Manvi, R., Singh, A., and Ermon, S. Adaptive inferencetime compute: Llms can predict if they can do better, even mid-generation. arXiv preprint arXiv:2410.02725, 2024. Mathematical Association of America. Amc conhttps://maa.org/student-programs/amc/, tests. 2023. Accessed: 2025-03-28. Meister, C., Vieira, T., and Cotterell, R. Best-first beam search. TACL, 2020. Meng, Y., Xia, M., and Chen, D. Simpo: Simple preference optimization with reference-free reward, 2024. URL https://arxiv.org/abs/2405.14734. Munkhbat, T., Ho, N., Kim, S. H., Yang, Y., Kim, Y., and Yun, S.-Y. Self-training elicits concise reasoning in large language models. arXiv preprint arXiv:2502.20122, 2025. Ong, I., Almahairi, A., Wu, V., Chiang, W.-L., Wu, T., Gonzalez, J. E., Kadous, M. W., and Stoica, I. Routellm: Learning to route llms with preference data, 2024. URL https://arxiv. org/abs/2406.18665, 2024. OpenAI. Learning to reason with llms. https://openai. com/research/learning-to-reason-with-llms, 2025. Accessed: 15 March 2025. 11 OpenAI. Introducing openai o3 and o4-mini. https:// openai.com/index/introducing-o3-and-o4-mini/, April 2025a. OpenAI. OpenAI o3: Most advanced reasonhttps://openai.com/index/ ing model. introducing-o3-and-o4-mini/, April 2025b. Qi, P., Liu, Z., Pang, T., Du, C., Lee, W. S., and Lin, M. Optimizing anytime reasoning via budget relative policy optimization, 2025. URL https://arxiv.org/abs/ 2505.13438. Qiao, Z., Deng, Y., Zeng, J., Wang, D., Wei, L., Meng, F., Zhou, J., Ren, J., and Zhang, Y. Concise: Confidenceguided compression in step-by-step efficient reasoning, 2025. URL https://arxiv.org/abs/2505.04881. Qu, Y., Yang, M. Y., Setlur, A., Tunstall, L., Beeching, E. E., Salakhutdinov, R., and Kumar, A. Optimizing testtime compute via meta reinforcement fine-tuning. arXiv preprint arXiv:2503.07572, 2025. Qwen Team. Qwq-32b: Embracing the power of rehttps://qwenlm.github.io/ inforcement learning. blog/qwq-32b/, 2025. Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., and Finn, C. Direct preference optimization: Your language model is secretly reward model, 2024. URL https://arxiv.org/abs/2305.18290. Renze, M. and Guven, E. The benefits of concise chain of thought on problem-solving in large language models. In 2024 2nd International Conference on Foundation and Large Language Models (FLLM), pp. 476483. IEEE, 2024. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms, 2017. URL https://arxiv.org/abs/1707.06347. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Shen, Y., Zhang, J., Huang, J., Shi, S., Zhang, W., Yan, J., Wang, N., Wang, K., Liu, Z., and Lian, S. Dast: Difficultyadaptive slow-thinking for large reasoning models. arXiv preprint arXiv:2503.04472, 2025a. Shen, Y., Zhang, J., Huang, J., Shi, S., Zhang, W., Yan, J., Wang, N., Wang, K., Liu, Z., and Lian, S. Dast: Difficultyadaptive slow-thinking for large reasoning models, 2025b. URL https://arxiv.org/abs/2503.04472. Submission and Formatting Instructions for ICML 2026 Sheng, G., Zhang, C., Ye, Z., Wu, X., Zhang, W., Zhang, R., Peng, Y., Lin, H., and Wu, C. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Yang, C., Si, Q., Duan, Y., Zhu, Z., Zhu, C., Li, Q., Lin, Z., Cao, L., and Wang, W. Dynamic early exit in reasoning models, 2025b. URL https://arxiv.org/abs/2504. 15895. Shrivastava, V., Awadallah, A., Balachandran, V., Garg, S., Behl, H., and Papailiopoulos, D. Sample more to think less: Group filtered policy optimization for concise reasoning. arXiv preprint arXiv:2508.09726, 2025. Yang, F., Chen, Z., Wang, X., Lu, X., Chai, J., Yin, G., Lin, W., Ma, S., Zhuang, F., Wang, D., Yang, Y., Li, J., and Ban, Y. Your group-relative advantage is biased, 2026. URL https://arxiv.org/abs/2601.08521. Song, M., Zheng, M., Li, Z., Yang, W., Luo, X., Pan, Y., and Zhang, F. Fastcurl: Curriculum reinforcement learning with progressive context extension for efficient training r1-like reasoning models, 2025. URL https://arxiv. org/abs/2503.17287. Yeo, E., Tong, Y., Niu, X., Neubig, G., and Yue, X. Demystifying long chain-of-thought reasoning in LLMs. In ICLR 2025 Workshop on Navigating and Addressing Data Problems for Foundation Models, 2025. URL https://openreview.net/forum?id=AgtQlhMQ0V. Team, K., Du, A., Gao, B., Xing, B., Jiang, C., Chen, C., Li, C., Xiao, C., Du, C., Liao, C., et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. Wen, L., Cai, Y., Xiao, F., He, X., An, Q., Duan, Z., Du, Y., Liu, J., Tang, L., Lv, X., Zou, H., Deng, Y., Jia, S., and Zhang, X. Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond, 2025a. URL https: //arxiv.org/abs/2503.10460. Wen, X., Liu, Z., Zheng, S., Ye, S., Wu, Z., Wang, Y., Xu, Z., Liang, X., Li, J., Miao, Z., et al. Reinforcement learning with verifiable rewards implicitly incentivizes correct reasoning in base llms. arXiv preprint arXiv:2506.14245, 2025b. Wu, S., Xie, J., Zhang, Y., Chen, A., Zhang, K., Su, Y., and Xiao, Y. Arm: Adaptive reasoning model, 2025. URL https://arxiv.org/abs/2505.20258. Xia, H., Leong, C. T., Wang, W., Li, Y., and Li, W. Tokenskip: Controllable chain-of-thought compression in llms. arXiv preprint arXiv:2502.12067, 2025. Xie, Y., Kawaguchi, K., Zhao, Y., Zhao, J. X., Kan, M.- Y., He, J., and Xie, M. Self-evaluation guided beam search for reasoning. Advances in Neural Information Processing Systems, 36:4161841650, 2023. Xu, S., Xie, W., Zhao, L., and He, P. Chain of draft: Thinking faster by writing less. arXiv preprint arXiv:2502.18600, 2025a. Xu, S., Xie, W., Zhao, L., and He, P. Chain of draft: Thinking faster by writing less, 2025b. URL https: //arxiv.org/abs/2502.18600. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a. Yi et al. Shorterbetter: Guiding reasoning models to find optimal inference length for efficient reasoning, 2025. URL https://arxiv.org/abs/2504.21370. Yu, P., Xu, J., Weston, J., and Kulikov, I. Distilling system 2 into system 1. arXiv preprint arXiv:2407.06023, 2024. Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Dai, W., Fan, T., Liu, G., Liu, L., et al. Dapo: An opensource llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Yue, C., Dong, C., Gao, Y., He, H., Chai, J., Yin, G., and Lin, W. Promoting efficient reasoning with verifiable stepwise reward. arXiv preprint arXiv:2508.10293, 2025. Zeng, W., Huang, Y., Liu, Q., Liu, W., He, K., Ma, Z., and He, J. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild, 2025. URL https://arxiv.org/abs/2503.18892. Zhang, J., Lin, N., Hou, L., Feng, L., and Li, J. Adaptthink: Reasoning models can learn when to think, 2025. URL https://arxiv.org/abs/2505.13417. Zhang et al. Grpo-lead: difficulty-aware reinforcement learning approach for concise mathematical reasoning in language models, 2025. URL https://arxiv.org/ abs/2504.09696. Zheng, C., Liu, S., Li, M., Chen, X.-H., Yu, B., Gao, C., Dang, K., Liu, Y., Men, R., Yang, A., et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025a. Zheng, H., Zhou, Y., Bartoldson, B. R., Kailkhura, B., Lai, F., Zhao, J., and Chen, B. Act only when it pays: Efficient reinforcement learning for llm reasoning via selective rollouts. arXiv preprint arXiv:2506.02177, 2025b. Zou, J., Ban, Y., Li, Z., Qi, Y., Qiu, R., Yang, L., and He, J. Transformer copilot: Learning from the mistake 12 Submission and Formatting Instructions for ICML 2026 log in LLM fine-tuning. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, URL https://openreview.net/forum?id= 2025. MRvxlTlkNQ. 13 Submission and Formatting Instructions for ICML A. Related Work A.1. Stimulating Reasoning Capabilities through Reinforcement Learning The introduction of OpenAI o1 (OpenAI, 2025) marks major advance in reasoning performance and the beginning of the LRM era, inspiring efforts to replicate such strong reasoning abilities(Zou et al., 2025; Chen et al., 2025c; He et al., 2025). DeepSeek-R1, for example, achieves comparable results using simple rule-based reward with the group relative policy optimization (GRPO)(Shao et al., 2024) algorithm, and its open-source release has established RLVR (DeepSeek-AI, 2025; Kimi Team, 2025b; Gao et al., 2024; Lambert et al., 2025; Zeng et al., 2025; Wen et al., 2025a; Song et al., 2025) as an effective paradigm for improving LLM reasoning, and Yang et al. (2026) provides principled theoretical analysis of its advantage estimation. This paradigm simplifies reward design by employing binary 0/1 rewards determined through rule-based correctness evaluation, eliminating the need for separate reward models as required in original GRPO (Shao et al., 2024; DeepSeek-AI, 2025; Schulman et al., 2017; Huang et al., 2026) implementations, thereby substantially reducing memory and computational overhead during RL training. Subsequent models, including the Kimi series (Kimi Team, 2025b;a), QwQ (Qwen Team, 2025), and O3 (OpenAI, 2025b), further advance these capabilities. RLVR assigns scores to trajectories based on pre-designed rules, rewarding desirable behaviors and penalizing undesirable ones. This encourages models to generate long CoTs to maximize correctness, fostering advanced reasoning behaviors such as search and backtracking. However, this also engenders bias toward redundancy over the risk of error, which results in overthinkingwasting computational resources, impairing model performance, and ultimately limiting the practical applicability of LRMs. A.2. Explorations in Efficient Reasoning Overthinking issue is first identified and analyzed by Chen et al. (2025b), who observe that LRMs generate lengthy outputs that neither improve accuracy nor introduce new solution strategies especially for easy prompt. To address this, various works explore efficient reasoning from different angles. Training-Free Methods typically improve reasoning efficiency through prompting engineering (Han et al., 2024; Xu et al., 2025a; Lee et al., 2025; Renze & Guven, 2024; Chen et al., 2024; Aytes et al., 2025; Chuang et al., 2024; Ong et al., 2024; Xu et al., 2025b; Huang et al., 2025b; Han et al., 2025), Best-of-N sampling pruning (Xie et al., 2023; Liao et al., 2025) and optimizations (Li et al., 2024; Manvi et al., 2024; Aggarwal et al., 2023) , and early-exit (Ma et al., 2025a; Yang et al., 2025b; Fan et al., 2025) mechanisms during reasoning. These approaches cannot fundamentally resolve the issue of redundant reasoning in models, and their effectiveness is often heavily contingent upon the models instruction-following capability. In practice, the observed improvements in experiments are typically modest or insignificant. While SAGE itself is also training-free algorithm, it essentially serves to unleash the models inherent potential for efficient reasoning. This allows the LRMs to select the currently optimal candidate sequence based on its self-aware at each inference iteration step. Offline Training Methods primarily supervised fine-tuning models with variable-length CoT data (Yu et al., 2024; Kang et al., 2025; Xia et al., 2025; Ma et al., 2025b; Munkhbat et al., 2025; Liu et al., 2024; Han et al., 2024). Recently, ConCISE (Qiao et al., 2025) constructs concise CoT data by inserting prompt tokens and employing early-exit during inference, then enhances the models reasoning conciseness through SFT/SimPO (Rafailov et al., 2024; Meng et al., 2024). The primary challenge of this line of work lies in the difficulty of obtaining high-quality short chains of thought, and the offline training paradigm tends to limit the models exploration ability on difficult problems. For similar reasons, we do not choose offline distillation to learn trajectories sampled by SAGE in this work. Since distillation depends on strong teacher model, we are concerned that self-distillation will limit the upper boundary of the models reasoning capability. Online Training Methods mainly adopt reinforcement learning for better generalization. (Kimi Team, 2025b; Shen et al., 2025b; Yeo et al., 2025; Cheng et al., 2025; Team et al., 2025; Luo et al., 2025a; Aggarwal & Welleck, 2025; Arora & Zanette, 2025; Yeo et al., 2025; Shen et al., 2025a; Qu et al., 2025; Cui et al., 2025) introduce length penalties in the reward function to suppress overly long reasoning traces. Yi et al. (2025), Hou et al. (2025), and Qi et al. (2025) optimize performance under fixed token budget to balance efficiency and effectiveness. GFPO (Shrivastava et al., 2025) attains Submission and Formatting Instructions for ICML 2026 sampling outputs aligned with the optimization objective via oversampling. S-GRPO (Dai et al., 2025) and VSRM (Yue et al., 2025) truncate reasoning steps and perform repeated rollouts to evaluate the rewards of reasoning subchains, which are then leveraged for RL training. Zhang et al. (2025), Huang et al. (2025a), and Wu et al. (2025) assign predefined thinking patterns based on task difficulty, which essentially reflects length budget. All the aforementioned methods are heavily rely on sophisticated reward design, which can easily lead to training instability or even reward hacking during the RL training process. Moreover, explicit or implicit integration of length compression into the optimization objective may impair the models reasoning capabilities. In this work, instead of modifying the optimization objective, we optimize the sampling process to enable the policy model to directly learn the efficient reasoning chains uncovered by SAGE via the advantage estimation of RLVR. This design yields the following two key advantages: (1) Low Computational Cost: We eliminate the need for extra oversampling as in GFPO (Shrivastava et al., 2025), where single parallel sampling step suffices to generate high-quality reasoning chains. Additionally, we do not require repeated rollouts for reward value estimation, step essential to methods such as S-GRPO (Dai et al., 2025) and VSRM (Yue et al., 2025). (2) Stable Training Dynamics: By preserving all components of RLVR except for the rollout procedure, SAGE-RL exhibits no significant difference in training stability compared with vanilla RLVR. B. Significant Differences from Beam Search In this section, we highlight the significant distinctions between TSearch w/ Φ and Beam Search from two perspectives: experimental results and underlying principles. Table 3. Performance Comparison of different sampling strategies on different models (Max Tokens=10,086). Due to the inherent characteristic of Beam Search that it returns multiple responses by default, when calculating the ACC of Beam Search and TSearch, we consider result correct if it contains at least one correct answer. Model DS-1.5B Qwen3-8B Sampling Strategy Greedy Random Beam Search (4, 4) TSearch w/ Φ (4, 4) Greedy Random Beam Search (4, 4) TSearch w/ Φ (4, 4) ACC LEN 4216 0.81 4142 0.81 4472 0.82 2972 0.84 4505 0.82 4526 0.82 4655 0.84 2946 0.89 We compared the performance of vanilla beam search with TSearch w/ Φ on randomly selected subset of MATH-500 (size=100). For the fairness of comparison, we uniformly set the exploration width to = 4. Since Beam Search directly returns the final set of candidate sequences, i.e., the number of returned sequences = m, we therefore uniformly set = 4. As shown in Table 3, Even though Beam Search generates four responses for each question, its final ACC is only comparable to those of random sampling and greedy sampling. Conversely, our algorithm achieves markedly higher accuracy while significantly reducing average response length. We analyze and illustrate the root causes of these differences in Figure 11. In Case A, although </think> appears within the log-probability window, the corresponding candidate sequence is discarded because its overall confidence score Φ does not rank first. In Case B, candidate sequence containing </think> is initially retained but is subsequently pruned during further expansion. In contrast, our algorithm directly accepts the sequence upon detecting </think> . These results indicate that our algorithm prevents the premature discarding of precise reasoning branches in later steps and significantly enhancing reasoning efficiency. 15 Submission and Formatting Instructions for ICML 2026 Figure 11. Two distinctions between TSearch w/ Φ and vanilla beam search. C. Experimental Details C.1. Objectives and Training Hyperparameters The objectives of GRPO and SAGE-GRPO are as follows: JGRPO(θ) = ExD, {yi}G i=1 πθold (x) [ 1 i= 1 yi yi t=1 min (wi,t(θ) Ai,t, clip (wi,t(θ), 1 ε, 1 + ε) Ai,t)] , (9) JSAGE-GRPO(θ) = xD, {yi} i=1 πθold (x)[ 1 ( yi t=1 1 yi i=1 (cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:209)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207) SAGE (m, r) min (wi,t(θ) Ai,t, clip(wi,t(θ), 1 ε, 1 + ε) Ai,t) + (10) yi t=1 1 yi i=r+1 (cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:209)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207) Random Sampling min (wi,t(θ) Ai,t, clip(wi,t(θ), 1 ε, 1 + ε) Ai,t) )] where is the number of generated responses to each query (i.e., the group size), and the importance ratio wi,t(θ) and advantage Ai,t of token yi,t are: wi,t(θ) = πθ(yi,tx, yi,<t) πθold(yi,tx, yi,<t) , Ai,t = Ai = r(x, yi) mean ({r(x, yi)} std ({r(x, yi)}G i=1) i=1) , (11) The objectives of GSPO and SAGE-GSPO are as follows: JGSPO(θ) = ExD, {yi}G i=1 πθold (x) [ 1 i=1 min (si(θ) Ai, clip (si(θ), 1 ε, 1 + ε) Ai)] , (12) 16 Submission and Formatting Instructions for ICML 2026 JSAGE-GSPO(θ) = xD, {yi} i= πθold (x)[ 1 ( min (si(θ) Ai, clip(si(θ), 1 ε, 1 + ε) Ai) i=1 (cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:209)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207) SAGE (m, r) + min (si(θ) Ai, clip(si(θ), 1 ε, 1 + ε) Ai) i=r+1 (cid:205)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:209)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:210)(cid:207) Random Sampling )] where we adopt the group-based advantage estimation: Ai = r(x, yi) mean ({r(x, yi)} i=1) std ({r(x, yi)}G i=1) , and define the importance ratio si(θ) based on sequence likelihood: si(θ) = ( 1 yi πθ(yix) πθold(yix) ) = exp ( 1 yi yi t=1 log πθ(yi,tx, yi,<t) πθold(yi,tx, yi,<t) ) . C.2. Experimental Setup (13) (14) (15) To thoroughly evaluate the effectiveness of SAGE-RL, we conduct experiments using several widely adopted LRMs as base models, including DeepSeek-R1-Distill-Qwen-1.5B (DS-1.5B), DeepSeek-R1-Distill-Qwen-7B (DS-7B) (DeepSeek-AI, 2025), DeepScaleR (Luo et al., 2025b), and Qwen3-8B (Yang et al., 2025a). Training Data Considering the importance of training data quality (Huang et al., 2025c), we use the English subset of DAPO (Yu et al., 2025) as well as MATH (Hendrycks et al., 2021) problems with difficulty from level 3 to level 5 (Zheng et al., 2025b). This collection consists of approximately 20,000 carefully curated problems covering wide range of difficulty levels. Training Configuration We use the verl (Sheng et al., 2024) framework for SAGE-RL training using the rule based reward function. To ensure completely fair comparison that highlights the role of SAGE in the rollout phase, we adopt identical hyperparameter settings for the same base model across SAGE-RL and all its baselines and variants. We tune the base models with global batch size of 32 across 8 GPUs for 600 steps with the Adam optimizer with learning rate of 1e-6, cosine warmup for the first 50 steps, and sampling temperature = 1.0. We apply KL regularization with β = 0.001 and an entropy coefficient of γ = 0.001. Our models are trained with 9,216 maximum context length, with 1,024 tokens reserved for the prompt. Sampling Strategy We tune all models with group size of = 8. Within each group, SAGE-RL employs SAGE (2,2) to search for two completions with precise reasoning chains, while the remaining six completions are obtained through the default random sampling in verl. Evaluation We follow previous work (Yue et al., 2025; Liu et al., 2025; Dai et al., 2025) and select comprehensive set of benchmarks, AIME24, AIME25(Art of Problem Solving, 2024),OlympiadBench(Chaoqun et al., 2024), MATH-500, Minerva(Lewkowycz et al., 2022), and AMC23(Mathematical Association of America, 2023), providing broader coverage than previous studies. During evaluation, we set the maximum generation length at 32768 tokens, consistent with Hou et al. (2025)s work and DeepSeek-R1. The temperature and top-p are set to 1.0 and 0.95, respectively. For all benchmarks, we report the average pass@1, response length(LEN) and token efficiency(TE) over runs. Specifically, for OlympiadBench, Minerva and MATH-500 where the benchmark sizes are relatively large, we set to 8; for the other benchmarks, we set to 32 to reduce randomness. D. Additional Experimental Results D.1. Comparison with Extended Datasets and Additional Analysis In this section, we present the complete evaluation results on six mathematical datasets. We divide the benchmarks into two groups of equal size. The three datasets in the upper part of Table 4 are more challenging than those in the lower part. 17 Submission and Formatting Instructions for ICML 2026 Table 4. Pass@1, response length(LEN) and TE results on six benchmarks and four base models before and after LC-R1, ThinkPrune-2k, AdaptThink, Efficient Reasoning, GRPO-LEAD, GRPO, GSPO, SAGE-GRPO and SAGE-GSPO. TE is calculated as Pass@1/LEN. Bold and underlined numbers denote the best and second-best results. The percentage in parentheses after TE indicates the improvement compared with the base model. Method DS-1.5B + LC-R1 + ThinkPrune-2k + AdaptThink + Efficient Reasoning + GRPO + SAGE-GRPO + GSPO + SAGE-GSPO DeepScaleR + ThinkPrune-2k + GRPO + SAGE-GRPO DS-7B + LC-R1 + AdaptThink + Efficient Reasoning + GRPO-LEAD + GRPO + SAGE-GRPO Qwen3-8B + GRPO + SAGE-GRPO + GSPO + SAGE-GSPO Method DS-1.5B + LC-R1 + ThinkPrune-2k + AdaptThink + Efficient Reasoning + GRPO + SAGE-GRPO + GSPO + SAGE-GSPO DeepScaleR + ThinkPrune-2k + GRPO + SAGE-GRPO DS-7B + LC-R1 + AdaptThink + Efficient Reasoning + GRPO-LEAD + GRPO + SAGE-GRPO Qwen3-8B + GRPO + SAGE-GRPO + GSPO + SAGE-GSPO AIME 2024 AIME 2025 OlympiadBench Pass@1(%) LEN TE(10 3) Pass@1(%) LEN TE(10 3) Pass@1(%) LEN TE(10 3) 25.1 23.3 (1.8) 23.7 (1.4) 25.7 (0.6) 26.2 (1.1) 28.3 (3.2) 28.8 (3.7) 28.3 (3.2) 28.5 (3.4) 31.4 33.5 (2.1) 35.6 (4.2) 36.1 (4.7) 51.9 51.7 (0.2) 52.1 (0.2) 51.9 (0.0) 53.1 (1.2) 52.5 (0.6) 55.3 (3.4) 73.2 72.8 (0.4) 73.5 (0.3) 73.0 (0.2) 73.7 (0.5) 12300 7098 (5202) 7085 (5215) 8055 (4245) 9189 (3111) 8767 (3533) 7243 (5057) 8604 (3696) 6889 (5411) 9370 8108 (1262) 8592 (778) 8094 (1276) 11305 6820 (4485) 6679 (4626) 6667 (4638) 7023 (4282) 8424 (2881) 6422 (4883) 15920 10573 (5347) 8975 (6945) 10544 (5376) 8547 (7373) MATH-500 2.04 3.28 (60.8%) 3.35 (64.2%) 3.19 (56.4%) 2.85 (39.7%) 3.23 (58.3%) 3.98 (95.1%) 3.29 (61.3%) 4.14 (102.9%) 3.35 4.13 (23.3%) 4.14 (23.6%) 4.46 (33.1%) 4.59 7.58 (65.1%) 7.80 (69.9%) 7.78 (69.5%) 7.56 (64.7%) 6.23 (35.7%) 8.61 (87.6%) 4.60 6.89 (49.8%) 8.19 (78.0%) 6.92 (50.4%) 8.62 (87.4%) 20.9 20.9 (0.0) 19.7 (1.2) 21.8 (0.9) 22.9 (2.0) 24.1 (3.2) 26.5 (5.6) 25.1 (4.2) 27.1 (6.2) 25.4 26.0 (0.6) 27.4 (2.0) 27.2 (1.8) 37.1 35.7 (1.4) 35.0 (2.1) 36.2 (0.9) 36.1 (1.0) 38.4 (1.3) 38.0 (0.9) 67.3 66.6 (0.7) 66.6 (0.7) 66.2 (1.1) 66.0 (1.3) 11669 6942 (4727) 6918 (4751) 8155 (3514) 8590 (3079) 8263 (3406) 7479 (4190) 8227 (3442) 7167 (4502) 9310 7486 (1824) 8185 (1125) 7704 (1606) 12540 7458 (5082) 7807 (4733) 7501 (5039) 7842 (4698) 10123 (2417) 6583 (5957) 18342 13981 (4361) 10052 (8290) 14082 (4260) 9183 (9159) Minerva 1.79 3.01 (68.2%) 2.85 (59.2%) 2.67 (49.2%) 2.67 (49.2%) 2.92 (63.1%) 3.54 (97.8%) 3.05 (70.4%) 3.78 (111.1%) 2.73 3.47 (27.1%) 3.35 (22.7%) 3.53 (29.3%) 2.96 4.79 (61.8%) 4.48 (72.3%) 4.82 (62.8%) 4.60 (55.4%) 3.79 (28.0%) 5.77 (94.9%) 3.67 4.76 (29.7%) 6.58 (79.3%) 4.70 (30.2%) 7.19 (95.9%) 33.4 32.0 (1.4) 32.9 (0.5) 32.6 (0.8) 33.8 (0.4) 34.2 (0.8) 36.9 (3.5) 34.6 (1.2) 37.3 (3.9) 35.9 35.1 (0.8) 36.2 (0.3) 36.5 (0.6) 39.8 41.4 (1.6) 38.9 (0.9) 40.1 (0.3) 40.6 (0.8) 41.2 (1.4) 41.8 (2.0) 46.6 45.1 (1.5) 45.4 (1.2) 46.6 (0.0) 46.7 (0.1) 8954 4632 (4322) 4752 (4202) 4563 (4391) 5755 (3199) 6323 (2631) 5050 (3904) 6410 (2544) 5172 (3782) 5972 4723 (1249) 5443 (529) 4890 (1082) 7839 4193 (3646) 4915 (2924) 4599 (3240) 4972 (2867) 5498 (2341) 4435 (3404) 11707 7512 (4195) 5972 (5735) 7964 (3743) 5436 (6271) AMC23 3.73 6.91 (85.3%) 6.92 (85.5%) 7.14 (91.4%) 5.87 (57.4%) 5.41 (45.0%) 7.31 (96.0%) 5.40 (44.8%) 7.21 (93.3%) 6.01 7.43 (23.6%) 6.65 (10.6%) 7.46 (24.1%) 5.08 9.87 (94.3%) 7.91 (55.7%) 8.72 (71.7%) 8.17 (60.8%) 7.50 (47.6%) 9.42 (85.4%) 4.00 6.00 (50.0%) 7.60 (90.0%) 5.85 (46.2%) 8.59 (114.7%) Pass@1(%) LEN TE(10 3) Pass@1(%) LEN TE(10 3) Pass@1(%) LEN TE( 3) 6210 3512(2698) 3667 (2543) 2912 (3298) 3530 (2680) 4806 (1404) 3735 (2475) 4454 (1756) 3647 (2563) 5184 3188 (1996) 4386 (798) 3817 (1367) 5490 2834 (2656) 2869 (2621) 2903 (2587) 2990 (2500) 3510 (1980) 2692 (2798) 7358 4964 (2394) 3390 (3968) 3962 (3396) 3363 (3995) 4.85 9.06 (86.8%) 8.97 (85.0%) 11.1 (128.7%) 8.90 (83.5%) 6.66 (37.3%) 9.05 (86.6%) 7.18 (48.0%) 9.21 (89.9%) 7.45 11.9 (59.6%) 9.21 (23.6%) 10.9 (45.6%) 7.83 15.7 (100.0%) 15.8 (101.2%) 15.7 (101.0%) 16.0 (104.3%) 13.1 (67.4%) 16.8 (114.4%) 7.04 10.6 (50.6%) 15.8 (124.2%) 12.5 (77.8%) 16.0 (126.8%) 60.1 61.8 (1.7) 60.8 (0.7) 62.3 (2.2) 64.7 (4.6) 65.4 (5.3) 66.3 (6.2) 66.1 (6.0) 68.3 (8.2) 64.2 65.8 (1.6) 69.3 (5.1) 70.9 (6.7) 81.9 79.1 (2.8) 80.7 (1.2) 80.7 (1.2) 82.7 (0.8) 83.0 (1.1) 84.9 (3.0) 90.5 88.6 (1.9) 90.7 (0.2) 87.7 (2.8) 90.9 (0.4) 8250 4889 (3361) 5224 (3026) 4969 (3281) 5202 (3048) 5771 (2479) 5091 (3159) 6095 (2191) 5278 (2972) 6683 5046 (1637) 5872 (811) 5438 (1245) 7170 3686 (3484) 5130 (2040) 4933 (2237) 4384 (2786) 4880 (2290) 3953 (3217) 10852 7079 (3773) 5563 (5289) 6464 (4388) 5041 (5811) 7.28 12.6 (73.6%) 11.6 (59.9%) 12.5 (71.7%) 12.4 (70.9%) 11.3 (55.6%) 13.0 (78.9%) 10.9 (63.1%) 12.9 (77.7%) 9.61 13.0 (35.6%) 11.8 (22.8%) 13.0 (35.7%) 11.4 21.5 (87.9%) 15.7 (37.7%) 16.4 (43.3%) 18.9 (65.8%) 17.0 (49.0%) 21.5 (88.1%) 8.34 12.5 (50.1%) 16.3 (95.4%) 13.6 (63.1%) 18.0 (106.7%) 83.2 80.4 (2.8) 81.7 (1.5) 80.4 (2.8) 82.0 (1.2) 83.6 (0.4) 84.8 (1.6) 83.4 (0.2) 85.2 (2.0) 86.0 82.5 (3.5) 87.6 (1.6) 88.8 (2.8) 91.6 87.3 (4.3) 88.9 (2.7) 89.8 (1.8) 89.5 (2.1) 92.0 (0.4) 93.0 (1.4) 94.4 93.6 (0.8) 95.0 (0.6) 94.6 (0.2) 94.4 (0.0) 4882 2973 (1909) 2826 (2056) 2563 (2319) 2821 (2061) 3907 (975) 2915 (1967) 3898 (984) 2921 (1961) 3805 2946 (859) 3482 (323) 3117 (688) 3871 2076 (1795) 2199 (1672) 2408 (1463) 2752 (1119) 3219 (652) 2141 (1730) 5640 4470 (1170) 3015 (2625) 4342 (1298) 2753 (2887) 17.0 27.0 (58.8%) 28.9 (70.0%) 31.4 (84.1%) 29.1 (70.6%) 21.4 (25.6%) 29.1 (70.7%) 25.3 (21.4%) 29.2 (71.6%) 22.6 28.0 (23.9%) 25.2 (11.3%) 28.4 (25.7%) 23.7 42.1 (77.7%) 40.4 (70.9%) 37.3 (57.6%) 32.5 (37.1%) 28.5 (20.2%) 43.4 (83.1%) 16.7 20.9 (25.1%) 31.5 (88.2%) 22.2 (32.9%) 34.3 (105.3%) 30.1 31.8 (1.7) 32.9 (2.8) 32.3 (2.2) 31.4 (1.3) 32.0 (1.9) 33.8 (3.7) 32.0 (1.9) 33.6 (3.5) 38.6 37.9 (0.7) 40.4 (1.8) 41.4 (2.8) 43.0 44.4 (1.4) 45.2 (2.2) 45.7 (2.7) 46.3 (3.3) 46.0 (3.0) 45.2 (2.2) 51.8 52.6 (0.8) 53.5 (1.7) 49.6 (2.2) 53.7 (1.9) 18 Submission and Formatting Instructions for ICML 2026 Performance on DS-1.5B and DS-7B For experiments with DS-1.5B as the base model, SAGE-RL consistently achieve the best or second-best performance across all benchmarks, while effectively improving the original models capabilities across all six mathematical reasoning benchmarks. Notably, SAGE-GSPO yields significant pass@1 gains of 6.2 % on AIME 2025 and 8.2 % on AMC23. AdaptThink stands out as powerful baseline, attaining the highest token efficiency on MATH-500 and Minerva, while exhibiting the most pronounced reasoning simplification on OlympiadBench, MATH-500, and Minerva, but this high level of conciseness restrict the models ability to explore different solution strategies. As result, AdaptThink struggle in terms of overall performance and consistently lag behind our method. As for the other baselines, both their performance and efficiency are generally less competitive compared to SAGE-RL. similar trend is observed when using DS-7B as the base model. Both GRPO-LEAD and Efficient-Reasoning adopt strategy of sacrificing less compression in exchange for improved performance; however, the reasoning capability gains they achieve remain substantially smaller than those of SAGE-RL. For instance, on AIME 2024, SAGE-GRPO not only outperforms GRPO-LEAD by 2.2 % in pass@1, but also produces noticeably shorter responses. These two sets of experiments together indicate that, on distilled models, SAGE-RL not only effectively alleviates the overthinking problem but also substantially enhances the models reasoning capability on complex mathematical problems. Since our method simultaneously improves the base models reasoning capability and the precision of its thinking process, it achieves more consistent and substantially larger gains in token efficiency compared to other approaches. Performance on DeepScaleR DeepScaleR has undergone systematic and comprehensive reinforcement learning (Luo et al., 2025b). Consequently, additional fine-tuning on this model typically yields only marginal performance gains, which accounts for the scarcity of related works that adopt DeepScaleR as base model. Nevertheless, SAGE-RL still achieves relatively significant token efficiency improvements on DeepScaleR. Particularly on OlympiaBench, Math-500 and Minerva, SAGE-RL delivers roughly twice the token efficiency gains of GRPO, demonstrating that SAGE-RL yields relatively substantial benefits even for models with extensive post-training. Performance on Qwen3-8B As one of the strongest reasoning models under the same parameter scale, Qwen3-8B achieves excellent performance across various mathematical reasoning tasks. Even on the highly challenging AIME 2025, it attains an impressive pass@1 of 67.3%. However, as illustrated in Figure 3, the overthinking problem remains largely unresolved in this model. For instance, on MATH-500, despite comparable pass@1 performance, the average response length is more than 2.5 times that of SAGE-GRPO-tuned DS-7B. Notably, vanilla RLVR is capable of moderately reducing the response length of the base model. This effect stems from the training procedure, where sequences must be padded to fixed batch length, causing the token budget in evaluation to be significantly smaller than that used in inference. As result, the model tends to receive positive rewards more readily for short answers, encouraging shorter generations. Nevertheless, this mechanism limits the models ability to improve or may even cause declines on reasoning tasks of varying difficulty, particularly on datasets such as AIME 2024 and AMC 23. In contrast, SAGE-RL still achieves moderate improvements in the reasoning capability of Qwen3-8B under limited training token budgets, while effectively reducing the redundancy in the thinking process. For example, SAGE-GSPO attains 1.9% increase in pass@1 on Minerva, while compressing the average response length to only 45.7% of the original. These results strongly demonstrate that SAGE-RL remains highly effective even on state-of-the-art reasoning models. Comparison of SAGE-GRPO and SAGE-GSPO As shown in Figure 9, across both GRPO and GSPO, the key variations in pass@1, response length, and KL loss are driven by the incorporation of SAGE-RL rather than fundamental RLVR algorithms. This underscores the robust positive impact of our approach across different RLVR implementations. In terms of entropy, GSPO-tuned models show elevated values, largely due to sequence-level importance sampling disregarding fine-grained token-level variations, thereby resulting in higher inference uncertainty. From the experimental results shown in Table 4, SAGE-GSPO exhibits particularly strong performance in reducing response length and slightly outperforms SAGE-GRPO in overall metrics. We hypothesize that this advantage stems from the greater stability of GSPOs sequence-level importance sampling compared to GRPOs token-level importance sampling, which is especially beneficial in more unstable scenarios such as MoE models (Zheng et al., 2025a). similar issue arises in SAGE-RL due to the hybrid sampling used in the rollout phase. In SAGE-GRPO, some of the rollouts are generated by selecting sequences at every reasoning step based on the full-sequence confidence score Φ, rather than greedly choosing the highest log-probability token as in Equation 7. Consequently, as indicated in Equation 11, the 19 Submission and Formatting Instructions for ICML 2026 probability πθold(yi,t x, yi,<t) under the old policy may be lower than that of random sampling, increasing the likelihood of clipping during importance sampling. In contrast, GSPO treats the entire sequence as the basic unit for importance sampling, thereby avoiding this issue entirely. Overall, Table 4 demonstrates that SAGE-RL achieves substantially superior performance compared to all baseline methods across six challenging mathematical reasoning tasks. Meanwhile, Figure 9 reveals higher pass@1 scores and increased KL divergence, accompanied by reduced response entropy and shorter response lengths. These results indicate that SAGE successfully unleashes the models implicit capacity for timely thinking termination. Consequently, the model learns efficient reasoning with increased confidence, confirming the viability of leveraging RLVR to instill effective reasoning patterns. This is consistent with the results of Wen et al. (2025b), which demonstrate that RLVR effectively promotes correct reasoning chains in base LLMs. D.2. Hyperparameters Sensitivity Analysis This section examines the influence of the two primary hyperparameters influencing SAGE-RL: the SAGE exploration width and the total number of rollouts produced by SAGE per group. We evaluate SAGE-RL under various combinations of these parameters and denote each setting as SAGE(m, r)-RL. Figure 12 illustrates the training dynamics of DS-1.5B with SAGE-GRPO under different hyperparameter combinations and GRPO. The corresponding evaluation results on four mathematical datasets are reported in Table 5. Table 5. comparison of experimental results for DS-1.5B under different SAGE-GRPO parameter settings. Here, SAGE (m, r) denotes an exploration width of m, with the final retention of different trajectories. Method MATH-500 AIME 2024 AIME OlympiadBench Pass@1(%) LEN TE(10 3) Pass@1(%) LEN TE(10 3) Pass@1(%) LEN TE( 3) Pass@1(%) LEN TE(10 3) DS-1.5B + GRPO + SAGE (1,1)-GRPO + SAGE (2,1)-GRPO + SAGE (2,2)-GRPO 83.2 83.6 (0.4) 84.0 (0.8) 84.2 (1.0) 84.8 (1.6) 4882 3907 (975) 3416 (1466) 2952 (1930) 2915 (1967) 17.0 21.4 (25.6%) 24.6 (44.7%) 28.5 (67.8%) 29.1 (70.7%) 25.1 28.3 (3.2) 28.3 (3.2) 28.5 (3.4) 28.8 (3.7) 12300 8767 (3533) 7979 (4321) 7308 (4992) 7243 (5057) 2.04 3.23 (58.3%) 3.55 (74.0%) 3.90 (91.2%) 3.98 (95.1%) 20.9 24.1 (3.2) 24.8 (3.9) 25.7 (4.8) 26.5 (5.6) 11669 8263 (3406) 7730 (3939) 7603 (4066) 7479 (4190) 1.79 2.92 (63.1%) 3.21 (79.3%) 3.38 (88.8%) 3.54 (97.8%) 33.4 34.2 (0.8) 34.5 (1.1) 35.2 (1.8) 36.9 (3.5) 8954 6323 (2631) 5857 (3097) 5267 (3687) 5050 (3904) 3.73 5.41 (45.0%) 5.89(57.9%) 6.68 (79.1%) 7.31 (96.0%) Figure 12. Training dynamics comparison for SAGE-GRPO with distinct hyperparameter combination: average response length when tested on MATH500, average SAGE-produced trajectory length during training, entropy, and KL divergence. The Impact of SAGE Rollout Quantity As shown in Table 5, the transition of from 1 to 2 has limited effect on the results. From policy optimization perspective, larger allows the policy model to learn from more efficient reasoning samples; however, the advantage estimate per sample becomes less sharp compared to = 1, leading to similar overall updates. Figure 12 shows that SAGE (2,1)-GRPO and SAGE (2,2)-GRPO display very similar trends in entropy and KL divergence, markedly different from those of SAGE (1,1)-GRPO and vanilla GRPO. This indicates that enlarging has little impact on policy updates, as rollouts with similar reasoning trajectories offer minimal additional information. The Impact of Exploration Width On the other hand, enlarging from 1 to 2 yields substantial performance gains. According to the results shown in Table 5, while SAGE (1,1)-GRPO yields moderate improvements over the vanilla GRPO baseline, its performance is markedly inferior to that of SAGE (2,1)-GRPO. 20 Submission and Formatting Instructions for ICML This indicates that exploration width significantly influences the activation of the models efficient reasoning capability, consistent with the findings in Figure 11. As illustrated in Figure 12, SAGE (1,1)-GRPO exhibits significantly milder entropy reduction and KL divergence increase relative to SAGE (2,1)-GRPO, and its training dynamics remain much closer to the vanilla GRPO. More directly, both the average length of SAGE-produced rollouts in SAGE(2,1)-GRPO and the average response length of the model at test time are significantly shorter than those observed in SAGE(1,1)-GRPO. These results indicate that limited exploration width causes SAGE-RL to largely collapse to the standard GRPO optimization behavior. D.3. SAGE-RL shows Promising Potential in Difficult Reasoning Tasks To more clearly elucidate the operational mechanism behind SAGE-RL, we compare the training dynamics of SAGE-GRPODS-1.5B (Ours) and GRPO-DS-1.5B (GRPO) on MATH-500 across five difficulty levels as training steps scales. The level 1-5 ranges from low to high, reflecting increasing levels of difficulty. Figure 13. The training dynamics of SAGE-GRPODS-1.5B (Ours) and GRPO-DS-1.5B (GRPO) on MATH-500 across level 1-5. As illustrated in Figure 13, both GRPO and SAGE-GRPO show steady performance gains across all difficulty levels of MATH500 as training progresses. SAGE-GRPO converges markedly faster than GRPO at every level and eventually attains performance comparable to GRPO on level 1-3 problems. clear divergence appears on level 4-5 problems, where SAGE-GRPO achieves substantially superior pass@1 and lower response length. Remarkably, the downward trend in response length for SAGE-GRPO continues even after GRPO has converged. These observations suggest that SAGE-RL primarily improves overall performance by dramatically increasing reasoning efficiency on difficult problems. This is consistent with the results in Table 4, which reveal significantly larger gains from SAGE-RL fine-tuning on more challenging benchmarks such as AIME 2024, AIME 2025, OlympiadBench, and Minerva than on relatively easier ones such as MATH-500 and AMC23. Collectively, these findings highlight the considerable potential of SAGE-RL in overcoming the reasoning performance bottlenecks faced by current LRMs on highly difficult tasks. D.4. Time Complexity Analysis Time Complexity Analysis of SAGE SAGE generates 2m reasoning steps in parallel with fixed exploration width at each expansion step. Therefore, it theoretically achieves the same time complexity as Degrade SAGE, meanwhile, its space complexity is approximately 2m times higher. However, as we adopt vLLM (Kwon et al., 2023) as the inference engine, whose core design philosophy centers on space-for-time tradeoff: it maximizes GPU memory utilization to minimize inference latency. Nevertheless, our implementation is constrained to the use of only 8 GPUs. Under this memory-limited setting, SAGE incurs higher inference-time cost compared to Degrade SAGE. We report the average per-sample runtime of SAGE (m, 1) under different EW in this constrained hardware environment. Here, denotes the exploration width EW. When = 0, SAGE degenerates to Degrade SAGE. As shown in Figure 14(a), the inference time of DS-1.5B remains consistently higher than that of DeepScaleR. Moreover, the average inference time 21 Submission and Formatting Instructions for ICML Figure 14. (a) Average inference time of SAGE on each question; (b) Comparison of Normalized inference time between the base models and the SAGE-GRPO tuned models on each question, approximated and normalized by the average response length. per response increases significantly with larger exploration widths. This primarily arises from the trade-off adopted by vLLM: elevated space complexity is exchanged for reduced time complexity in the context of limited computational resources. In particular, once the exploration width exceeds 2, the growth rate of inference time accelerates further. Therefore, we primarily set exploration width = 2, which represents the transition point between the slow-growth and fast-growth regions, to achieve balanced trade-off between efficiency and performance. Time Complexity Analysis of SAGE-RL Tuned Models In the standard pass@1 inference setting, the KV cache is prefilled during the initial prompt processing phase, which ensures that the generation latency per subsequent token remains approximately constant. Consequently, for short queries, the total inference time of each completion scales nearly linearly with the number of generated tokens. However, vLLM aggressively optimizes inference speed through techniques such as KV cache reuse and continuous batching, which compromises the fairness of direct wall-clock time comparisons. Given the approximately linear relationship between inference time and the number of generated tokens, we adopt the proxy metric 0.0001 (average response length) to reflect the average inference latency. We compare this normalized metric between the base models and our SAGE-GRPO-tuned models. As shown in Figure 14(b), although SAGE incurs increasing inference-time cost with larger exploration widths under constrained hardware, SAGE-RL-tuned models can significantly reduce the average inference time in the standard pass@1 inference paradigm. Specifically, even on the relatively easier MATH-500 and AMC23 subsets among the six datasets we evaluated, our approach still achieves 28.7% reduction in inference latency. When approximating average inference time using the average response length, Table 4 clearly shows that, compared to the baseline, SAGE-RL-tuned models reduce inference latency by more than 40% across the majority of models and benchmarks. Submission and Formatting Instructions for ICML 2026 Figure 15. Performance on DeepSacleR and DS-1.5B with different exploration width on MATH500 and AMC23. Under all settings, both pass@1 and response length gradually converge. 23 Submission and Formatting Instructions for ICML 2026 Figure 16. Case Study 1 Submission and Formatting Instructions for ICML 2026 Figure 17. Case Study"
        }
    ],
    "affiliations": [
        "Beihang University",
        "Bytedance China",
        "Renmin University of China"
    ]
}