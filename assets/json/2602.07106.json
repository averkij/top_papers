{
    "paper_title": "Ex-Omni: Enabling 3D Facial Animation Generation for Omni-modal Large Language Models",
    "authors": [
        "Haoyu Zhang",
        "Zhipeng Li",
        "Yiwen Guo",
        "Tianshu Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Omni-modal large language models (OLLMs) aim to unify multimodal understanding and generation, yet incorporating speech with 3D facial animation remains largely unexplored despite its importance for natural interaction. A key challenge arises from the representation mismatch between discrete, token-level semantic reasoning in LLMs and the dense, fine-grained temporal dynamics required for 3D facial motion, which makes direct modeling difficult to optimize under limited data. We propose Expressive Omni (Ex-Omni), an open-source omni-modal framework that augments OLLMs with speech-accompanied 3D facial animation. Ex-Omni reduces learning difficulty by decoupling semantic reasoning from temporal generation, leveraging speech units as temporal scaffolding and a unified token-as-query gated fusion (TQGF) mechanism for controlled semantic injection. We further introduce InstructEx, a dataset aims to facilitate augment OLLMs with speech-accompanied 3D facial animation. Extensive experiments demonstrate that Ex-Omni performs competitively against existing open-source OLLMs while enabling stable aligned speech and facial animation generation."
        },
        {
            "title": "Start",
            "content": "Ex-Omni: Enabling 3D Facial Animation Generation for Omni-modal Large Language Models Haoyu Zhang 1 Zhipeng Li2 Yiwen Guo 3 Tianshu Yu 1 1The Chinese University of Hong Kong, Shenzhen 2LIGHTSPEED 3Independent Researcher 1{haoyuzhang3@link.cuhk.edu.cn, yutianshu@cuhk.edu.cn} 2zhipengxli@tencent.com 3guoyiwen89@gmail.com 6 2 0 2 6 ] . [ 1 6 0 1 7 0 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Omni-modal large language models (OLLMs) aim to unify multimodal understanding and generation, yet incorporating speech with 3D facial animation remains largely unexplored despite its importance for natural interaction. key challenge arises from the representation mismatch between discrete, token-level semantic reasoning in LLMs and the dense, fine-grained temporal dynamics required for 3D facial motion, which makes direct modeling difficult to optimize under limited data. We propose Expressive Omni (Ex-Omni), an opensource omni-modal framework that augments OLLMs with speech-accompanied 3D facial animation. Ex-Omni reduces learning difficulty by decoupling semantic reasoning from temporal generation, leveraging speech units as temporal scaffolding and unified token-as-query gated fusion (TQGF) mechanism for controlled semantic injection. We further introduce InstructEx, dataset aims to facilitate augment OLLMs with speech-accompanied 3D facial animation. Extensive experiments demonstrate that Ex-Omni performs competitively against existing open-source OLLMs while enabling stable aligned speech and facial animation generation."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have achieved remarkable progress across wide range of tasks (Minaee et al., 2024), demonstrating impressive generalization and reasoning capabilities. As research continues to expand from unimodal understanding toward multimodal understanding and generation, growing attention has been devoted to unifying these tasks within single framework, commonly This work was done during internship at LIGHTSPEED. Corresponding Author. Project Leader. aWe plan to release the code and data after completing the internal compliance review, subject to the companys open source policy. Figure 1: Overview of the motivation behind Ex-Omni. It supports any combinations of textual and speech inputs, and is capable of unified generation of multimodal outputs, including text, speech, and 3D facial animation. referred to as omni-modal large language models (OLLMs). With the continued development of such omni models (Fu et al., 2025; Xie and Wu, 2024b; Team, 2025; Li et al., 2025; Xie and Wu, 2024a; Luo et al., 2025; Xu et al., 2025; AI et al., 2025), further breakthroughs are expected in areas such as human-computer interaction and embodied intelligence. In the area of human-computer interaction, there is growing demand for OLLMs that can engage in natural and expressive interactions with humans. Human communication is inherently multimodal and further extends beyond verbal content alone. In face-to-face interaction, temporally coherent 3D facial animation synchronized with speech plays crucial role in conveying non-verbal cues and enhancing interaction naturalness, particularly in applications such as virtual characters, digital avatars, and embodied agents. However, existing opensource OLLMs primarily focus on linguistic, acoustic, or pixel-level visual outputs, while expressive non-verbal modalities such as 3D facial animation remain largely underexplored. Motivated by this gap, we investigate integrating 3D facial animation generation into OLLMs. natural idea is to 1 directly attach facial decoder to an LLM and predict animation from its hidden representations. In practice, we found this design exposes challenge: LLM hidden states are optimized for sparse, tokenlevel semantics with weakly constrained temporal structure, whereas 3D facial animation requires dense and temporally smooth motion at much finer time scale. Bridging these representations forces the decoder to infer fine-grained dynamics from coarse semantic features, resulting in an illconditioned mapping that typically demands substantially larger model capacity and more paired speechface supervision for stable generation. In this paper, we propose Expressive Omni (ExOmni), an open-source omni-modal framework that augments OLLMs with speech-accompanied 3D facial animation, where facial motion is represented using ARKit-52 blendshape coefficients (Lewis et al., 2014) and generated in non-autoregressive manner. Ex-Omni follows text or speech instructions to generate synchronized speech paired with facial animation in an end-to-end manner. To facilitate stable and temporally coherent facial animation learning from LLM semantics, Ex-Omni adopts two complementary design choices. First, instead of directly predicting facial motion from LLM hidden states, Ex-Omni leverages discrete speech units as structured intermediate representation, providing explicit temporal scaffolding for facial generation. Second, unified token-as-query gated fusion (TQGF) mechanism is introduced to selectively regulate how and when semantic information from the LLM is injected into the speech and facial generation processes, simplifying optimization and improving temporal alignment. In addition, we construct InstructEx, dataset designed for enabling 3D facial animation generation for OLLMs. InstructEx spans diverse type of data, including Automatic Speech Recognition (ASR), Text-to-Speech (TTS), Text-to-Text (T2T), Speech-to-Speech (S2S) question answering, and large-scale synthetic Speech-to-Face (S2F) corpus specifically curated to bridge the gap between limited real-world recordings and open-domain generalization. This design enables joint learning of language understanding, speech generation, and 3D facial generation within unified framework. Extensive experimental results show that Ex-Omni performs competitively compared to existing opensource OLLMs. Overall, the key contributions are: We propose Expressive Omni (Ex-Omni), enabling unified instruction following and generation across text, speech, and speechaccompanied 3D facial animation. To the best of our knowledge, Ex-Omni is among the first open-source OLLM to natively support speech-aligned 3D facial animation generation. To reduce the difficulty of learning temporally coherent 3D facial animation from LLM semantics. Ex-Omni leverages discrete speech units as temporal scaffolding and employs unified token-as-query gated fusion (TQGF) to regulate how and when semantic information is injected into temporal generation. We construct InstructEx, dataset aims to facilitate augment OLLMs with speechaccompanied 3D facial animation. Extensive experiments show that Ex-Omni achieves competitive performance in multiple tasks with limited data."
        },
        {
            "title": "2 Related Work",
            "content": "Omni-modal Large Language Models. OLLMs (Fu et al., 2025; Xie and Wu, 2024b; Team, 2025; Li et al., 2025) represent significant advancement in multimodal large language models, as they integrate the capabilities of understanding and generating across multiple modalities, such as text, speech, and vision, within unified framework. For example, Mini-Omni (Xie and Wu, 2024a) utilizes text-instructed speech generation and batchparallel strategies to achieve seamless speech synthesis while preserving the models text capabilities. OpenOmni (Luo et al., 2025) introduces two-stage training framework to achieve zeroshot cross-modal alignment from vision-language tasks to speech-language tasks. Qwen2.5-Omni (Xu et al., 2025) introduce the Thinker-Talker architecture to integrate text, speech, and vision modalities into unified end-to-end model. Ming-Omni (AI et al., 2025) is the first to integrate visual generation capabilities into unified omni-modal model, utilizing modality-specific routers to achieve understanding and generation across multiple modalities. Facial Animation Generation. Facial animation generation is an important research area for improving system interactivity. Earlier methods (Chen et al., 2019; Mittal and Wang, 2020; Zhang et al., 2 Figure 2: Model architecture of EX-Omni. 2021; Hong et al., 2022) mainly focus on 2D facial animation generation, field that has become mature after years of research. In recent years, 3D facial animation generation (Richard et al., 2021; Xing et al., 2023; Peng et al., 2023b,a; Fan et al., 2024; Peng et al., 2025) has gradually received more attention. These methods have generally focused on predictions based on mesh representations or parameterized models to enhance realism. For mesh-based methods, FaceFormer (Fan et al., 2022) introduces transformer-based approach for generating 3D facial animations, using autoregressive modeling to capture long-term audio context. CodeTalker (Xing et al., 2023) utilizes discrete motion priors learned from real facial movements, applying vector quantized autoencoder to reduce uncertainty in the audio-tomotion mapping process. For parameterized methods, ARKit-like blendshape models (Lewis et al., 2014) are commonly used. For example, Peng et al. (2023b) propose EmoTalk which disentangles emotion and content from speech to generate expressive facial movements. Their approach uses an emotiondisentangling encoder and an emotion-guided feature fusion decoder to improve emotional expression in 3D facial animations. Peng et al. (2025) propose DuelTalker which supports multi-round, dualspeaker interactions in 3D talking head generation, aiming to capture dynamic interactions between speakers. Additionally, Fan et al. (2024) introduce Unitalker, which combines both mesh-based and parameterized annotation styles, enabling scalable generation by leveraging the strengths of both approaches to produce more realistic and expressive 3D facial animations."
        },
        {
            "title": "3.1 Overview",
            "content": "Figure 2 shows the overall pipeline of Ex-Omni. Given text input and speech waveform a, Ex-Omni performs instruction understanding and multimodal generation within an LLM-centered unified framework, where the LLM focuses on semantic reasoning rather than direct temporal generation. The model produces discrete speech unit sequence u, which is decoded into waveform speech, as well as sequence of 3D facial animation parameters (i.e., blendshape coefficients). At high level, Ex-Omni adopts structured decomposition that reduces the difficulty of learning temporally coherent generation by decoupling semantic reasoning from modality-specific temporal synthesis. Specifically, the LLM is responsible for instruction understanding and semantic reasoning, while speech units are used as an explicit temporal scaffolding to guide downstream speech and facial animation generation. We formulate the overall model as (u, y) = F(x, a; θ), θ = {θE, θP , θL, θU , θF }, (1) where θE, θP , and θL correspond to the speech encoder, speech projector and LLM, respectively. θU denotes the speech unit generator while θF represents the facial decoder."
        },
        {
            "title": "3.2 Unified Speech-Text Representation",
            "content": "Given speech waveform and text input x, we map both modalities into shared LLM token embedding space. Specifically, speech is first encoded into high-level representations and then projected as Xs = PθP (EθE (a)) RTsd, while text tokens are embedded as Xl = EmbθL(x) RTld, (2) (3) where denotes the LLM embedding dimension. The unified input is constructed by concatenation: = [Xl; Xs] R(Tl+Ts)d, (4) with positional encodings omitted for clarity."
        },
        {
            "title": "3.3 LLM-Centered Reasoning",
            "content": "The LLM serves as semantic reasoner that focuses exclusively on instruction understanding and high-level reasoning. Given the unified input representation X, the LLM performs autoregressive generation to produce the text response t1:Tlr . During this process, we extract the sequence of last hidden states corresponding to the generated response tokens: = [h1, h2, . . . , hTlr ] RTlrd, (5) where hi represents the features at step containing high-level semantic reasoning information. The probability of the next token is predicted based on these states: pθL(ti+1 t1:i, X) = Softmax(Wohi). (6) 3.4 Joint Speech and 3D Facial Animation Generation Ex-Omni jointly generates speech and 3D facial animation, aiming to maintain semantic consistency and temporal alignment across modalities. We adopt token-as-query gated fusion (TQGF) mechanism, which applies an asymmetric fusion rule where the token sequence always serves as the query, while upstream semantic representations act as contextual key/value. This design explicitly assigns temporal responsibility to the target token sequence, and selectively injects semantic cues from the LLM via gated cross-attention. As result, TQGF decouples high-level semantic reasoning from modality-specific temporal modeling, thereby simplifying temporal learning under limited multimodal supervision. Formally, let RM denote query tokens and RN denote context tokens. The gated fusion operation is Fuse(Q, C) = + σ(cid:0)G(Q)(cid:1) Attn(Q, C), (7) where Attn(Q, C) denotes cross-attention from to C, G() is head-specific element-wise gating factors, and σ() is the sigmoid function. For speech generation, we model speech synthesis as an autoregressive prediction of discrete speech units. Given the generated text tokens t1:Tlr , the speech generator enriches the explicit token embeddings with the semantic hidden states through: = FuseθU (EmbθU (t1:Tlr ), H) , (8) where RTlrd serves as the conditioning signal. Then, the speech generator predicts sequence of speech units u1:Tu: pθU (u1:Tu H) = Tu(cid:89) j=1 pθU (uj u<j, H). (9) For 3D facial animation generation, we parameterize facial motion using ARKit-52 blendshape coefficients and formulate S2F generation as nonautoregressive sequence prediction problem, where the model outputs the full blendshape coefficients ˆy1:Ty RTy52 in parallel. Given discrete speech units u1:Tu, we first obtain unit embeddings and align them to the target video frame rate by temporally resampling features to length Ty (e.g., via linear interpolation), yielding frame-level query representations Qy RTyd. In parallel, we project the hidden states of the speech generator into the same space to obtain contextual key/value representations RTud, which carry semantically rich information. We then inject speech semantics into the frame-level queries using the TQGF module: Hf = FuseθF (Qy, S) RTyd. (10) We then apply periodic rotary positional embeddings (see Appendix A.2) to Hf and refine it via Transformer encoder, thus obtaining the predicted sequence of 3D facial parameters: ˆy1:Ty = EθF (Hf ), ˆyt R52. (11)"
        },
        {
            "title": "3.5 Training Strategy",
            "content": "Stage (Speech-Text Alignment). We train the speech projector using ASR data while freezing all other components. This stage aligns speech representations with the semantic space of the base LLM. Stage II (Speech Generation Pre-training). We train the speech generator on TTS data while freezing the base LLM and other modules. This stage focuses on learning autoregressive speech unit prediction. Stage III (Speech-Face Co-training). We further train the speech generator on TTS data paired with blendshape parameters annotations. This stage introduces explicit supervision for speechface alignment while keeping unrelated components frozen. Stage IV (Joint Fine-tuning). All components are unfrozen and jointly optimized on mixture of ASR, TTS, S2S, and T2T data. The T2T data include code and mathematical reasoning data, which help preserve the general reasoning capability of the base LLM."
        },
        {
            "title": "3.6 Training Objectives",
            "content": "Autoregressive Objectives for Text and Speech. For text tokens and discrete speech units, we adopt standard autoregressive modeling. Given token sequence = (z1, . . . , zT ), the objective is Lar = (cid:88) t=1 log p(zt z<t), (12) where zt denotes either text token or speech unit, depending on the training data. 3D Facial Animation Generation. For 3D facial animation, the face generator is trained with hybrid regression loss that captures both spatial accuracy and temporal smoothness. Let ˆyt RK and yt RK denote the predicted and facial blendshape annotations at frame t, respectively. We define the frame-wise loss as Lbs ="
        },
        {
            "title": "1\nB",
            "content": "B (cid:88) i=1 1 Ti (cid:88) tTi (cid:13) (cid:13)ˆy(i) (cid:13) y(i) (cid:13) 2 (cid:13) (cid:13) 2 , (13) where Ti denotes the valid temporal range determined by the target sequence length. 5 To encourage temporally smooth facial motion, we further introduce velocity consistency term: i=1 (cid:80) (cid:80)B tTi{1} 1 Ti1 (cid:13) (cid:13)(ˆy(i) (cid:13) Lvel = 1 ˆy(i) t1) (y(i) (cid:13) 2 (cid:13) t1) (cid:13) 2 (14) Therfore, the final facial loss is Lface = Lbs + λvelLvel, where λvel controls the contribution of temporal smoothness and is empirically set to 0.3. y(i) Overall objective. The objective is = Lar + Lface."
        },
        {
            "title": "4 Dataset Construction",
            "content": "Table 1: Datasets introduction. Stage Stage I"
        },
        {
            "title": "Stage II",
            "content": "Data 1317.37K ASR 2371.94K TTS"
        },
        {
            "title": "Stage III",
            "content": "99.14K TTS & Face"
        },
        {
            "title": "Stage IV",
            "content": "59.34K S2S & Face 20.00K ASR 20.00K TTS & Face 30.00K T2T Hours 3554.15 6370.99 257.04 713.03 52.59 52.79 N/A The data statistics of InstructEx are summarized in Table 1. To support evaluation across different downstream tasks within unified framework, we construct multi-stage training corpus with controlled linguistic balance and data quality. Notably, to overcome the limited vocabulary in existing 3D facial capture datasets, we strategically incorporate high-quality synthetic data for 3D facial animation generation, ensuring robust generalization in opendomain scenarios. In Stage I, approximately 1,000K ASR samples are primarily sampled from the Emilia corpus with balanced ChineseEnglish ratio of approximately 1:1. In addition, we incorporate the train-clean100, train-clean-360, and train-other-500 subsets from LibriSpeech (Panayotov et al., 2015), as well as training data from WenetSpeech (Zhang et al., 2022) with confidence scores higher than 0.95. In Stage II, approximately 2.17 million text samples from Emilia (He et al., 2024, 2025), evenly split between English and Chinese (about 1:1), are converted into speech using CosyVoice 2 (Du et al., 2024b) with single, unified speaker identity. To explicitly control speaker variability, fixed reference utterance is adopted for each language. The English reference text is This is reference voice from OpenAI that is used for generating training data., and its Chinese counterpart is direct translation of the same sentence, ensuring semantic equivalence across languages. The corresponding reference audios are synthesized using the OpenAITTS (model=tts-1, voice=alloy) and are used solely for speaker conditioning, while all speech generation is performed by CosyVoice 2 (Du et al., 2024b). In Stage III, high-quality motion-capture facial animation data are scarce, and existing public datasets typically contain only few thousand samples with limited coverage of speech content and expressive variation. In contrast, NVIDIA Audio2Face-3D (an open-weights model) is trained on large-scale, professionally captured motion data using commercial-grade multi-view capture systems, enabling temporally coherent and anatomically realistic facial animation. Therefore, we adopt the state-of-the-art Audio2Face-3D model (Chung et al., 2025) as high-quality teacher to generate blendshape annotations as structured temporal supervision based on the Stage II speech data. Specifically, we uniformly sample 10K TTS samples from the Stage II samples, with an approximately balanced ChineseEnglish ratio of 1:1, and augment them with aligned facial blendshape sequences generated by Audio2Face-3D. In Stage IV, we mainly reuse the TTS QA data released by OpenOmni (Luo et al., 2025) (from training stage 3-1). We filter out mathematical and code-related queries, yielding 59.34k samples for S2S learning. To overcome the semantic limitations of real-world capture data, we construct corresponding synthetic S2F subset by pairing these same 59.34k samples with high-fidelity blendshape annotations generated by the Audio2Face-3D model (Chung et al., 2025). Meanwhile, the 30,000 excluded samples (mathematical and code-related queries) are repurposed as T2T data. To preserve capabilities acquired in earlier phases, we further incorporate 20,000 ASR samples randomly sampled from Stage and 20,000 TTS & Face pairs from Stage III. Together, these diverse sources constitute the joint fine-tuning dataset for Stage IV. Regarding S2S data synthesis, fixed speaker identity is enforced on the response side, consistent with the strategy in Stage II. In contrast, reference audios are randomly sampled from the Emilia dataset, and CosyVoice 2 (Du et al., 2024b) is employed to synthesize question utterances with diverse speaker characteristics."
        },
        {
            "title": "5.1 Evaluation",
            "content": "Speech-to-Face Evaluation. We evaluate S2F performance under two settings, depending on whether the model natively supports speech-toface generation. Ex-Omni is evaluated end-to-end, while models without native facial animation generation capability are assessed using cascaded scheme, where an OLLM first generates speech responses that are subsequently fed into downstream S2F model. Evaluations are conducted on A2F-Bench (Fan et al., 2024), the translated ExA2F-EN set, and the CommonEval QA dataset. Since dialogue responses are open-ended, facial motion annotations are non-unique, making direct comparison against single ground-truth sequence ambiguous. Therefore, we adopt reference-based evaluation protocol using NVIDIA Audio2Face-3D as fixed external reference. Following prior work (Peng et al., 2023b; Fan et al., 2024), we evaluate facial animation quality using the lip vertex error (LVE). Specifically, LVE computes the ℓ2 distance between predicted and reference lip vertices, and for each frame is defined as the maximum ℓ2 error among all lip vertices. The final LVE score is obtained by averaging this per-frame error over all frames and all samples in the test set, where lower values indicate better 3D facial animation generation performance. Further details of the evaluation protocol and human study are provided in Appendix A.3. Text-to-Face Evaluation. T2F evaluation follows the same protocol as S2F evaluation, except that the input is text rather than speech. Human Evaluation of Facial Animation Generation. To complement automatic evaluation, we conduct human A/B preference study on speechdriven facial animation. We recruit 8 evaluators, each of whom reviews 20 randomized pairs of rendered videos (i.e., 40 videos per evaluator), where each pair contains one baseline result and one generated by Ex-Omni. Evaluators are asked to select the animation with better audio-visual consistency, with an emphasis on lipspeech synchronization and temporal alignment; tie option is provided. We report the proportions of samples where Ex-Omni is preferred (Win) and ties occur (Tie), as well as an Overall score defined as Win+0.5Tie. Importantly, to assess inter-rater reliability, we additionally report the majority match 6 Table 2: Performance comparison of 3D facial animation generation in dialogue scenes. indicates lower is better. Note: Ex-Omni+Task-specific S2F model adopt two-stage pipeline, where Ex-Omni generates speech responses and the output audio is subsequently used as input to S2F model. In contrast, Native Ex-Omni directly generates facial animation within unified framework. Method Speech-to-Face Text-to-Face CommonEval Ex-A2F-EN A2F-Bench CommonEval Ex-A2F-EN A2F-Bench Qwen2.5-Omni+EmoTalk Qwen2.5-Omni+Unitalker-B-D3 Qwen2.5-Omni+Unitalker-B-D6 Ex-Omni+EmoTalk Ex-Omni+Unitalker-B-D3 Ex-Omni+Unitalker-B-D6 Ex-Omni 8.020 6.618 7.334 7.433 6.527 7. 4.754 Cascaded 7.548 7.811 8.560 9.782 7.725 8.785 Native 8.497 7.634 6.530 7.282 6.879 6.601 7. 3.866 8.014 6.555 7.245 7.399 6.424 7.035 5.430 7.509 8.322 8.886 7.194 6.591 7.256 3.377 7.390 6.983 7.531 6.868 6.534 7. 3.667 Table 3: Human A/B preference study on 3D facial animation. D3 and D6 denotes UniTalker-B-D3 and UniTalker-B-D3, respectively. nese speech is transcribed using Paraformer-zh and evaluated using Character Error Rate (CER). Native vs. Cascaded"
        },
        {
            "title": "Win Tie Overall MMF",
            "content": "Ex-Omni vs. Ex-Omni+EmoTalk 55.0 10.0 70.0 5.0 Ex-Omni vs. Ex-Omni+D3 80.0 5.0 Ex-Omni vs. Ex-Omni+D6 60.0 72.5 82.5 70.0 73.8 70.6 fraction (MMF), which measures the fraction of individual ratings that agree with the per-pair majority vote, thereby quantifying annotation consistency. Evaluations are conducted in both Chinese and English under the same protocol. For English, we translate the A2F-Bench text content into English using GPT-4o (Fu et al., 2025), synthesize speech from the translated text, and apply the same S2F evaluation procedure. Speech-to-Text Evaluation. We evaluate S2T capability using VoiceBench (Chen et al., 2024), which covers diverse set of speech-based tasks, including open-ended question answering, referencebased QA, multiple-choice QA, reasoning, instruction following and safety. Open-ended QA is evaluated using GPT-based scores, while other tasks are evaluated using accuracy-based metrics. All the evaluations were conducted using the open-source code of VoiceBench to ensure consistency. Further details of the evaluation protocol are provided in Appendix A.3. Text-to-Speech Evaluation. For TTS evaluation, generated speech is transcribed into text by ASR model on Seed-TTS-Eval (Anastassiou et al., 2024). Specifically, English speech is transcribed using Whisper-Large-V3 (Radford et al., 2023) and evaluated using Word Error Rate (WER), while ChiBaselines For S2T evaluation, we compare ExOmni with several representative OLLMs and speech large language models. Specifically, the baselines include GPT-4o-Audio (Hurst et al., 2024), Kimi-Audio (KimiTeam et al., 2025), Qwen2.5-Omni (Xu et al., 2025), VITA-1.0 (Fu et al., 2024), VITA-1.5 (Fu et al., 2025), MiniOmni (Chen et al., 2025b), Mini-Omni2 (Xie and Wu, 2024b), Moshi (Défossez et al., 2024), and LLaMA-Omni (Fang et al., 2025a). For S2F evaluation, we compare Ex-Omni with two recent S2F methods, EmoTalk (Peng et al., 2023b) and UniTalker (Fan et al., 2024), both of which support direct prediction of facial blendshape coefficients. For TTS evaluation, we compare Ex-Omni with several advanced TTS systems and OLLMs, including Seed-TTS (Anastassiou et al., 2024), FireRedTTS (Guo et al., 2024), CosyVoice (Du et al., 2024a), CosyVoice 2 (Du et al., 2024b), Spark-TTS (Wang et al., 2025), and Qwen2.5-Omni (Xu et al., 2025)."
        },
        {
            "title": "5.2 Experiments Results and Analysis",
            "content": "3D Facial Animation Generation Results. As shown in Table 2, compared with cascaded baselines that combine Omni backbones with external facial decoders (e.g., EmoTalk and UniTalker), Ex-Omni produces facial animation results that are more closely aligned with the Audio2Face-3D reference, demonstrating the effectiveness of directly generating facial animation within unified framework. We further observe that under cascaded settings with identical task-specific facial decoders, pipelines built upon different OLLMs 7 Table 4: Speech-to-Text performance comparison on VoiceBench. means higher is better. Model AlpacaEval CommonEval WildVoice SD-QA MMSU OBQA BBH IFEval AdvBench Overall GPT-4o-Audio Kimi-Audio Qwen2.5-Omni Moshi VITA-1.0 VITA-1.5 LLaMA-Omni Mini-Omni Mini-Omni2 Ex-Omni 4.78 4.46 4.49 2.01 3.38 4.21 3.70 1.95 2.32 2.57 4.49 3.97 3.93 1.60 2.15 3.66 3.46 2.02 2.18 2.87 Proprietary 75.50 63.12 Open-Source 55.71 15.64 27.94 38.88 39.69 13.92 9.31 40. 4.58 4.20 - 1.30 1.87 3.48 2.92 1.61 1.79 2.19 80.25 62.17 89.23 83.52 84.10 69.70 76.02 61. 98.65 100.00 61.32 24.04 25.70 52.15 25.93 24.69 24.27 24.24 81.10 25.93 29.01 71.65 27.47 26.59 26.59 25.49 - 47.40 47.70 55.30 49.20 46.30 46.40 50.40 52.87 10.12 22.82 38.14 14.87 13.58 11.56 16.22 99.42 44.23 26.73 97.69 11.35 37.12 57.50 83. 86.75 76.91 - 29.51 36.43 64.53 41.12 30.42 33.49 43.57 exhibit relatively similar performance, indicating that in cascaded schemes the overall S2F quality is primarily determined by the cascaded downstream task-specific model rather than the choice of the OLLMs. In contrast, Ex-Omni benefits from native S2F generation, where facial animation and speech are generated jointly within single framework. This design avoids potential information loss introduced by intermediate speech generation and leads to more natural facial animation generation. We also note that on the Ex-A2F-EN benchmark, Ex-Omni exhibits relatively higher error compared to other settings. This is mainly because Ex-Omni tends to generate longer speech responses, which increases the temporal length and complexity of the corresponding facial animation sequences, thereby imposing higher demands on the facial decoder. We further evaluate T2F generation using the same evaluation protocol, with textual input as the only difference, and observe consistent trends across all benchmarks. Finally, we note that Ex-Omni is trained using blendshape annotations generated by Audio2Face-3D, which may introduce bias when using the same model as reference. However, Audio2Face-3D itself is trained on professionally captured motion-capture data and is widely regarded as strong proxy for high-quality 3D facial motion. Using its predictions as supervision and as reference for open-ended dialogue responses therefore provides scalable and consistent approximation in open-domain settings. Human Evaluation Results. We conduct human A/B preference study to complement the automatic evaluation in Table 2 and to reduce potential bias introduced by reference-based metrics. Since the quantitative protocol measures similarity to Audio2Face-3D rather than perceptual quality, S2F model that are optimized differently may be unfairly penalized despite generating plausible facial motions. As shown in Table 3, Ex-Omni consistently achieves strong human preference when evaluators focus on mouth-speech synchronization. Across all comparisons, Ex-Omni wins in 55%-80% of the samples, with only 5%-10% ties, yielding overall preference scores of 60.0%-82.5%. Moreover, the inter-rater consistency measured by MMF remains high (70.0%-73.8%), indicating that evaluators typically form clear majority preference for each sample and that the observed advantages of Ex-Omni are reproducible rather than driven by noisy individual judgments. These results provide direct perceptual evidence that Ex-Omni produces more accurate and stable facial motion. Speech-to-Text Results. As shown in Table 4, proprietary models consistently outperform opensource models across most benchmarks, largely due to their substantially larger training data scale. Despite being trained on only 713.03 hours of S2S QA data, Ex-Omni achieves competitive performance among open-source models. Notably, ExOmni ranks second on SD-QA (40.14%), indicating strong robustness in reference-based speech QA, and performs competitively on AdvBench. On AlpacaEval, CommonEval, and WildVoice, ExOmni attains reasonable GPT scores given its limited training data. In contrast, performance on MMSU, OBQA, BBH, and IFEval remains low across most models, suggesting that speech-based multiple-choice reasoning and instruction following are still challenging. Overall, these results highlight Ex-Omnis effectiveness under performancedata efficiency trade-off. Text-to-Speech Results. Table 5 reports the T2S results on Seed-TTS-Eval. Except for Qwen2.5Omni, all compared methods are dedicated TTS 8 Figure 3: Case study on 3D facial animation generation. The figure highlights mouth-opening behaviors aligned with phonemes that require large lip movements. (a) Results generated from English speech; (b) Results generated from Chinese speech. [...] indicates omitted content for brevity, and parenthetical annotations denote dominant articulation cues. model. Many recent open-source OLLMs fail to follow explicit TTS instructions, showing limitation in controllable speech generation for generalpurpose OLLMs. As an OLLMs, Ex-Omni is not designed to compete with task-specific TTS model in terms of absolute synthesis quality. Nevertheless, Ex-Omni achieves reasonable performance across all test splits, demonstrating its effectiveness in the unified framework. Table 5: Text-to-Speech performance comparison on Seed-TTS-Eval. means lower is better."
        },
        {
            "title": "Human",
            "content": "Seed-TTS Spark TTS CosyVoice CosyVoice 2 FireRedTTS Qwen2.5-Omni Ex-Omni test-zh test-en test-hard"
        },
        {
            "title": "1.25\nProprietary\n1.12\nOpen-Source\n1.20\n3.63\n1.45\n1.51\n1.70\n3.37",
            "content": "2.14 2.25 1.98 4.29 2.57 3.82 2.72 2.67 - 7.59 - 11.55 6.83 17.45 7.97 13. Ablation Study on Facial Animation Generation. Table 6 presents the impact of regularization terms and components on 3D facial animation generation, evaluated using the LVE. Removing the Lvel leads to noticeable degradation on A2FBench (from 3.667 to 3.751), demonstrating that velocity regularization is useful for constraining abrupt lip motion and improving temporal stability. Replacing the speech generators last layer contextual representations with LLM last layer features (w/o speech context) results in performance drop, particularly on A2F-Bench (from 3.667 to 5.079), indicating that generator-level representations provide more suitable semantic-temporal interface for fine-grained prediction. Further removing all contextual conditioning and relying solely on speech units (w/o context) leads to consistently higher LVE, showing the importance of contextual information. Notably, w/o speech context performs worse than w/o context, indicating that directly injecting high-level LLM semantic representations may introduce additional instability rather than providing useful guidance. This observation is consistent with our discussion on representation mismatch in section introduction. When remove the proposed TQGF (w/o TQGF), we concatenate the context and tokens and then fuse them using self-attention. We can see that the performance on English benchmarks is slightly improved, while performance on Chinese benchmarks degrades. Therefore, we think that TQGF may help balance performance across different languages by modulating semantic conditioning, leading to robust speech-to-face generation behavior. In addition, we observe that this setting incurs higher computational and memory overhead, leading to slower training. This further demonstrates the effectiveness of TQGF. Table 6: Effect of each regularization & component on facial animation generation. means lower is better."
        },
        {
            "title": "Method",
            "content": "Ex-A2F-EN A2F-Bench Ex-Omni w/o Lvel w/o speech context w/o context w/o TQGF 3.377 3.437 3.509 3.414 3.184 3.667 3.751 5.079 4.510 3.682 Case Study of 3D Facial Animation Generation. By analyzing the cases and evaluation results from the human A/B preference study, we observe that differences in facial animation quality become more pronounced under long-form speech outputs. Figure 3 presents representative examples selected from the A/B test. The predicted 9 blendshape parameters are applied to 3D facial template and rendered using Blender for visualization. As shown, all compared methods produce temporally aligned mouth movements corresponding to the spoken content, indicating generally correct audio-visual synchronization. However, under long speech sequences, task-specific speech-to-face models tend to generate conservative mouth motions with limited opening amplitude, while ExOmni maintains stable temporal alignment and exhibits more expressive mouth opening dynamics at semantically emphasized regions. Notably, in our evaluation, this increased expressiveness appears to be more aligned with human preferences in long-form speech cases."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduced Ex-Omni that extends OLLMs with the capability to generate speechaccompanied 3D facial animation. By addressing the mismatch between token-level reasoning and fine-grained facial motion, Ex-Omni decouples high-level semantic understanding from modalityspecific temporal synthesis through discrete speechunit scaffolding and unified token-query-guided fusion mechanism. Extensive experiments demonstrate that Ex-Omni achieves competitive performance on speech understanding and generation benchmarks, while enabling aligned and joint generation of speech and 3D facial animation."
        },
        {
            "title": "References",
            "content": "Inclusion AI, Biao Gong, Cheng Zou, Chuanyang Zheng, Chunluan Zhou, Canxiang Yan, Chunxiang Jin, Chunjie Shen, Dandan Zheng, Fudong Wang, Furong Xu, Guangming Yao, Jun Zhou, Jingdong Chen, Jianxin Sun, Jiajia Liu, Jianjiang Zhu, Jun Peng, Kaixiang Ji, and 39 others. 2025. Ming-omni: unified multimodal model for perception and generation. CoRR, abs/2506.09344. Philip Anastassiou, Jiawei Chen, Jitong Chen, Yuanzhe Chen, Zhuo Chen, Ziyi Chen, Jian Cong, Lelai Deng, Chuang Ding, Lu Gao, Mingqing Gong, Peisong Huang, Qingqing Huang, Zhiying Huang, Yuanyuan Huo, Dongya Jia, Chumin Li, Feiya Li, Hui Li, and 27 others. 2024. Seed-tts: family of highquality versatile speech generation models. CoRR, abs/2406.02430. Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, and Furu Wei. 2022. Speecht5: Unified-modal encoder-decoder pretraining for spoken language processing. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, pages 57235738. Lele Chen, Ross K. Maddox, Zhiyao Duan, and Chenliang Xu. 2019. Hierarchical cross-modal talking In face generation with dynamic pixel-wise loss. IEEE Conference on Computer Vision and Pattern Recognition, pages 78327841. Qian Chen, Yafeng Chen, Yanni Chen, Mengzhe Chen, Yingda Chen, Chong Deng, Zhihao Du, Ruize Gao, Changfeng Gao, Zhifu Gao, Yabin Li, Xiang Lv, Jiaqing Liu, Haoneng Luo, Bin Ma, Chongjia Ni, Xian Shi, Jialong Tang, Hui Wang, and 17 others. 2025a. Minmo: multimodal large language model for seamless voice interaction. CoRR, abs/2501.06282. Wenxi Chen, Ziyang Ma, Ruiqi Yan, Yuzhe Liang, Xiquan Li, Ruiyang Xu, Zhikang Niu, Yanqiao Zhu, Yifan Yang, Zhanxun Liu, Kai Yu, Yuxuan Hu, Jinyu Li, Yan Lu, Shujie Liu, and Xie Chen. 2025b. Slamomni: Timbre-controllable voice interaction system with single-stage training. In Findings of the Association for Computational Linguistics, pages 2262 2282. Yiming Chen, Xianghu Yue, Chen Zhang, Xiaoxue Gao, Robby T. Tan, and Haizhou Li. 2024. Voicebench: Benchmarking llm-based voice assistants. CoRR, abs/2410.17196. Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, Chang Zhou, and Jingren Zhou. 2024. Qwen2-audio technical report. CoRR, abs/2407.10759. Chaeyeon Chung, Ilya Fedorov, Michael Huang, Aleksey Karmanov, Dmitry Korobchenko, Roger Blanco Ribera, and Yeongho Seol. 2025. Audio2face-3d: Audio-driven realistic facial animation for digital avatars. CoRR, abs/2508.16401. Alexandre Défossez, Laurent Mazaré, Manu Orsini, Amélie Royer, Patrick Pérez, Hervé Jégou, Edouard Grave, and Neil Zeghidour. 2024. Moshi: speechtext foundation model for real-time dialogue. CoRR, abs/2410.00037. Zhihao Du, Qian Chen, Shiliang Zhang, Kai Hu, Heng Lu, Yexin Yang, Hangrui Hu, Siqi Zheng, Yue Gu, Ziyang Ma, Zhifu Gao, and Zhijie Yan. 2024a. Cosyvoice: scalable multilingual zero-shot textto-speech synthesizer based on supervised semantic tokens. CoRR, abs/2407.05407. Zhihao Du, Yuxuan Wang, Qian Chen, Xian Shi, Xiang Lv, Tianyu Zhao, Zhifu Gao, Yexin Yang, Changfeng Gao, Hui Wang, Fan Yu, Huadai Liu, Zhengyan Sheng, Yue Gu, Chong Deng, Wen Wang, Shiliang Zhang, Zhijie Yan, and Jingren Zhou. 2024b. Cosyvoice 2: Scalable streaming speech synthesis with large language models. CoRR, abs/2412.10117. Xiangyu Fan, Jiaqi Li, Zhiqian Lin, Weiye Xiao, and Lei Yang. 2024. Unitalker: Scaling up audio-driven 10 In 3d facial animation through unified model. Computer Vision - ECCV 2024 - 18th European Conference, volume 15099, pages 204221. Yingruo Fan, Zhaojiang Lin, Jun Saito, Wenping Wang, and Taku Komura. 2022. Faceformer: Speech-driven 3d facial animation with transformers. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1874918758. Qingkai Fang, Shoutao Guo, Yan Zhou, Zhengrui Ma, Shaolei Zhang, and Yang Feng. 2025a. Llama-omni: Seamless speech interaction with large language models. In The Thirteenth International Conference on Learning Representations. OpenReview.net. Qingkai Fang, Yan Zhou, Shoutao Guo, Shaolei Zhang, and Yang Feng. 2025b. Llama-omni 2: Llm-based real-time spoken chatbot with autoregressive streamIn Proceedings of the 63rd ing speech synthesis. Annual Meeting of the Association for Computational Linguistics, pages 1861718629. Association for Computational Linguistics. Chaoyou Fu, Haojia Lin, Zuwei Long, Yunhang Shen, Meng Zhao, Yifan Zhang, Xiong Wang, Di Yin, Long Ma, Xiawu Zheng, Ran He, Rongrong Ji, Yunsheng Wu, Caifeng Shan, and Xing Sun. 2024. VITA: towards open-source interactive omni multimodal LLM. CoRR, abs/2408.05211. Chaoyou Fu, Haojia Lin, Xiong Wang, Yifan Zhang, Yunhang Shen, Xiaoyu Liu, Haoyu Cao, Zuwei Long, Heting Gao, Ke Li, Long Ma, Xiawu Zheng, Rongrong Ji, Xing Sun, Caifeng Shan, and Ran He. 2025. VITA-1.5: towards gpt-4o level real-time vision and speech interaction. CoRR, abs/2501.01957. Haohan Guo, Kun Liu, Feiyu Shen, Yi-Chen Wu, FengLong Xie, Kun Xie, and Kaituo Xu. 2024. Fireredtts: foundation text-to-speech framework for industry-level generative speech applications. CoRR, abs/2409.03283. Michael Hassid, Tal Remez, Tu Anh Nguyen, Itai Gat, Alexis Conneau, Felix Kreuk, Jade Copet, Alexandre Défossez, Gabriel Synnaeve, Emmanuel Dupoux, Roy Schwartz, and Yossi Adi. 2023. Textually preIn The Thirtytrained speech language models. Seventh Annual Conference on Neural Information Processing Systems. Haorui He, Zengqiang Shang, Chaoren Wang, Xuyuan Li, Yicheng Gu, Hua Hua, Liwei Liu, Chen Yang, Jiaqi Li, Peiyang Shi, Yuancheng Wang, Kai Chen, Pengyuan Zhang, and Zhizheng Wu. 2024. Emilia: An extensive, multilingual, and diverse speech dataset for large-scale speech generation. In Proc. of SLT. Fa-Ting Hong, Longhao Zhang, Li Shen, and Dan Xu. 2022. Depth-aware generative adversarial network for talking head video generation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 33873396. Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander Madry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex Paino, and 79 others. 2024. Gpt-4o system card. CoRR, abs/2410.21276. KimiTeam, Ding Ding, Zeqian Ju, Yichong Leng, Songxiang Liu, Tong Liu, Zeyu Shang, Kai Shen, Wei Song, Xu Tan, Heyi Tang, Zhengtao Wang, Chu Wei, Yifei Xin, Xinran Xu, Jianwei Yu, Yutao Zhang, Xinyu Zhou, Y. Charles, and 21 others. 2025. Kimiaudio technical report. CoRR, abs/2504.18425. John P. Lewis, Ken Anjyo, Taehyun Rhee, Mengjie Zhang, Frédéric H. Pighin, and Zhigang Deng. 2014. Practice and theory of blendshape facial models. In 35th Annual Conference of the European Association for Computer Graphics, pages 199218. Yunxin Li, Xinyu Chen, Shenyuan Jiang, Haoyuan Shi, Zhenyu Liu, Xuanyu Zhang, Nanhao Deng, Zhenran Xu, Yicheng Ma, Meishan Zhang, and 1 others. 2025. Uni-moe-2.0-omni: Scaling language-centric omnimodal large model with advanced moe, training and data. arXiv preprint arXiv:2511.12609. Run Luo, Ting-En Lin, Haonan Zhang, Yuchuan Wu, Xiong Liu, Yongbin Li, Longze Chen, Jiaming Li, Lei Zhang, Xiaobo Xia, Hamid Alinejad-Rokny, Fei Huang, and Min Yang. 2025. Openomni: Advancing open-source omnimodal large language models with progressive multimodal alignment and real-time emotional speech synthesis. In The Thirty-ninth Annual Conference on Neural Information Processing Systems. Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, and Jianfeng Gao. 2024. Large language models: survey. arXiv preprint arXiv:2402.06196. Gaurav Mittal and Baoyuan Wang. 2020. Animating face using disentangled audio representations. In IEEE Winter Conference on Applications of Computer Vision, pages 32793287. Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015. Librispeech: An ASR corpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2015, pages 5206 5210. Haorui He, Zengqiang Shang, Chaoren Wang, Xuyuan Li, Yicheng Gu, Hua Hua, Liwei Liu, Chen Yang, Jiaqi Li, Peiyang Shi, Yuancheng Wang, Kai Chen, Pengyuan Zhang, and Zhizheng Wu. 2025. Emilia: large-scale, extensive, multilingual, and diverse dataset for speech generation. In arXiv:2501.15907. Ziqiao Peng, Yanbo Fan, Haoyu Wu, Xuan Wang, Hongyan Liu, Jun He, and Zhaoxin Fan. 2025. Dualtalk: Dual-speaker interaction for 3d talking head conversations. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21055 21064. Junyang Lin. 2025. Qwen2.5-omni technical report. CoRR, abs/2503.20215. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, and 40 others. 2025. Qwen3 technical report. CoRR, abs/2505.09388. Aohan Zeng, Zhengxiao Du, Mingdao Liu, Kedong Wang, Shengmin Jiang, Lei Zhao, Yuxiao Dong, and Jie Tang. 2024. Glm-4-voice: Towards intelligent and human-like end-to-end spoken chatbot. CoRR, abs/2412.02612. Binbin Zhang, Hang Lv, Pengcheng Guo, Qijie Shao, Chao Yang, Lei Xie, Xin Xu, Hui Bu, Xiaoyu Chen, Chenchen Zeng, Di Wu, and Zhendong Peng. 2022. WENETSPEECH: 10000+ hours multi-domain mandarin corpus for speech recognition. In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2022, pages 61826186. Chenxu Zhang, Yifan Zhao, Yifei Huang, Ming Zeng, Saifeng Ni, Madhukar Budagavi, and Xiaohu Guo. 2021. FACIAL: synthesizing dynamic talking face with implicit attribute learning. In 2021 IEEE/CVF International Conference on Computer Vision, pages 38473856. Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu. 2023. Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities. In Findings of the Association for Computational Linguistics, pages 1575715773. Dong Zhang, Xin Zhang, Jun Zhan, Shimin Li, Yaqian Zhou, and Xipeng Qiu. 2024. Speechgpt-gen: Scaling chain-of-information speech generation. CoRR, abs/2401.13527. Ziqiao Peng, Yihao Luo, Yue Shi, Hao Xu, Xiangyu Zhu, Hongyan Liu, Jun He, and Zhaoxin Fan. 2023a. Selftalk: self-supervised commutative training diagram to comprehend 3d talking faces. In Proceedings of the 31st ACM International Conference on Multimedia, pages 52925301. Ziqiao Peng, Haoyu Wu, Zhenbo Song, Hao Xu, Xiangyu Zhu, Jun He, Hongyan Liu, and Zhaoxin Fan. 2023b. Emotalk: Speech-driven emotional disenIn IEEE/CVF tanglement for 3d face animation. International Conference on Computer Vision, pages 2063020640. Zihan Qiu, Zekun Wang, Bo Zheng, Zeyu Huang, Kaiyue Wen, Songlin Yang, Rui Men, Le Yu, Fei Huang, Suozhi Huang, Dayiheng Liu, Jingren Zhou, and Junyang Lin. 2025. Gated attention for large language models: Non-linearity, sparsity, and attentionsink-free. In The Thirty-Ninth Annual Conference on Neural Information Processing Systems. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2023. Robust speech recognition via large-scale weak supervision. In International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 2849228518. Alexander Richard, Michael Zollhöfer, Yandong Wen, Fernando De la Torre, and Yaser Sheikh. 2021. Meshtalk: 3d face animation from speech using crossmodality disentanglement. In 2021 IEEE/CVF International Conference on Computer Vision, pages 11531162. Meituan LongCat Team. 2025. Longcat-flash-omni technical report. CoRR, abs/2511.00279. Xinsheng Wang, Mingqi Jiang, Ziyang Ma, Ziyu Zhang, Songxiang Liu, Linqin Li, Zheng Liang, Qixi Zheng, Rui Wang, Xiaoqin Feng, Weizhen Bian, Zhen Ye, Sitong Cheng, Ruibin Yuan, Zhixian Zhao, Xinfa Zhu, Jiahao Pan, Liumeng Xue, Pengcheng Zhu, and 6 others. 2025. Spark-tts: An efficient llm-based text-to-speech model with single-stream decoupled speech tokens. CoRR, abs/2503.01710. Zhifei Xie and Changqiao Wu. 2024a. Mini-omni: Language models can hear, talk while thinking in streaming. CoRR, abs/2408.16725. Zhifei Xie and Changqiao Wu. 2024b. Mini-omni2: Towards open-source gpt-4o with vision, speech and duplex capabilities. CoRR, abs/2410.11190. Jinbo Xing, Menghan Xia, Yuechen Zhang, Xiaodong Cun, Jue Wang, and Tien-Tsin Wong. 2023. Codetalker: Speech-driven 3d facial animation with In IEEE/CVF Conference discrete motion prior. on Computer Vision and Pattern Recognition, pages 1278012790. Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, Bin Zhang, Xiong Wang, Yunfei Chu, and"
        },
        {
            "title": "A Appendix",
            "content": "A.1 Supplementary of Related Work Speech Language Models. Recent advancements in speech language models (Fang et al., 2025a,b; Chen et al., 2025a; Zhang et al., 2023; Hassid et al., 2023; Chu et al., 2024; Chen et al., 2025b; Xie and Wu, 2024a) have significantly improved speech understanding and generation in an end-to-end manner, eliminating the need for cascaded ASR and TTS models. For example, SpeechT5 (Ao et al., 2022) aligns text and speech representations into shared semantic space using unified encoder-decoder structure and crossmodal vector quantization methods. Moshi (Défossez et al., 2024) addresses the issues of latency and information bottlenecks in traditional speech dialogue systems through its full-duplex speech-tospeech generation framework and the Inner Monologue design. SpeechGPT-Gen (Zhang et al., 2024) introduces the Chain-of-Information Generation to decouples the modeling of semantic and perceptual information,thus making the speech generation process more efficient and precise. GLM-4Voice (Zeng et al., 2024) addresses the delay and error accumulation problems by adopting 12.5Hz speech segmenter, streaming reasoning and largescale speech-to-text pre-training. A.2 Experimental Settings and"
        },
        {
            "title": "Implementation Details",
            "content": "Architecture Details. For speech encoding, we adopt Whisper-Large-V3 (Radford et al., 2023) as the speech encoder, which is kept frozen throughout training. The speech representations produced by the encoder are mapped into the LLM embedding space via two-layer MLP-based speech projector. Follow OpenOmni (Luo et al., 2025), the speech projector first temporally downsamples the encoder features by concatenating every 5 consecutive frames, followed by two fully connected layers with ReLU activation in between, enabling more expressive and structured alignment between speech and language representations. The LLM backbone is instantiated with Qwen3-8B (Yang et al., 2025), which serves as the central semantic reasoner. For speech generation, we employ Qwen3-0.6B (Yang et al., 2025) as lightweight speech generator responsible for autoregressive discrete speech unit prediction. The speech decoder is implemented using GLM4-Voice-Decoder (Zeng et al., 2024) and is kept frozen during training. More details of the speech decoder can refer to (Zeng et al., 2024). Semantic information from the LLM is injected into downstream temporal generation modules via gated attention. It follows the same design as in prior work (Qiu et al., 2025), where standard cross-attention operation is augmented with learnable gating function. Concretely, the attention output is modulated by sigmoid gate predicted from the query representations, allowing the model to adaptively control the strength of semantic conditioning at each time step. For 3D facial animation, dedicated face decoder is used to predict ARKit-52 blendshape coefficients. Its detailed architecture is described in Section 3.4. Specifically, the TQGF module used for feature fusion consists of two layers, while the encoder used to further refine the fused representations is composed of six layers. Discrete Speech Units. In addition, we follow GLM-4-Voice (Zeng et al., 2024) for the definition and usage of discrete speech units. Specifically, the speech waveform is represented as sequence of discrete tokens (with the same tokenizer and temporal resolution as GLM-4-Voice), and the speech generator is trained to autoregressively predict these unit tokens. At inference, the predicted unit sequence is converted back to waveform by the frozen GLM4-Voice-Decoder, using the same decoding configuration (e.g., sampling/streaming setup) as in (Zeng et al., 2024). Unless otherwise specified, all hyperparameters and implementation details for unit tokenization and waveform decoding are kept identical to GLM-4-Voice to ensure reproducible speech generation pipeline. Render Settings. For quantitative evaluation, we uniformly adopt the rendering template provided by EmoTalk (Peng et al., 2023b) to ensure fair and consistent visual comparisons across methods. In addition, for demonstration purposes, we render the generated facial animations using realistic human-style head template to provide clearer and more intuitive visualizations of our methods capabilities. It is worth noting that the ARKit-52 blendshape coefficients is independent of facial identity, allowing the same predicted coefficients to be applied to different facial templates. This identity-agnostic property and its relatively low parameter dimensionality make ARKit-52 particularly suitable for robust and transferable 3D facial animation generation. 13 Table 7: The detailed training setup for Ex-Omni and the hyperparameters used across different training stages. All experiments are conducted on 8 NVIDIA H20 GPUs."
        },
        {
            "title": "Hyperparameter",
            "content": "I II"
        },
        {
            "title": "III",
            "content": "IV epoch batch size optimizer warmup ratio Gradient Accumulation lr of Speech Encoder lr of Speech Projector lr of LLM lr of Speech Generator lr of Facial Decoder lr of Speech Decoder freeze Speech Encoder freeze Speech Projector freeze LLM freeze Speech Generator freeze Facial Decoder freeze Speech Decoder 1 128 AdamW 0.3 1 0 1 103 0 0 0 0 3 128 AdamW 0.1 1 0 0 0 1 104 0 0 10 128 AdamW 0.1 1 0 0 0 0 1 103 0 3 8 AdamW 0.1 4 0 0 2 106 5 105 5 105 0 Training Details and Hyperparameters. All experiments are conducted on machine equipped with 8 NVIDIA H20 GPUs, each with 96 GB of memory. We use CUDA 12.6, PyTorch 2.7.0 and Python 3.10 for model training and evaluation. The detailed hyperparameters are shown in Table 7. Periodic Positional Encoding in Facial Decoder. Although Text-to-Text generation does not incur speech or facial animation losses, the textual inputs are still routed through the speech-related temporal modules in Ex-Omni during training. As result, the model must handle long-form instruction reasoning together with dense temporal representations, even when the supervision is purely textual. Under this setting, standard sinusoidal positional encoding becomes insufficient for stable and efficient temporal modeling, especially when long sequences are processed in parallel. To address this issue, we adopt periodic variant of rotary positional encoding (RoPE) for facial animation generation, following the practice of UniTalks periodic sinusoidal positional encoding. Unlike standard RoPE, the positional index is defined to be strictly periodic. Given frame index and predefined period , we compute the periodic position as = mod α , (15) where α is scaling factor. This formulation enforces that frames separated by integer multiples of share the same positional encoding, introducing periodic inductive bias that aligns well with rhythmic facial and lip motions. The periodic position is then used to compute rotation angles following the standard RoPE formulation with base 10,000, and the resulting embeddings are applied via the usual block-wise rotation. In practice, we empirically set = 25 and α = 1.0. A.3 Evaluation and Experiment Details Reference-based Evaluation Protocol. In speech-to-face generation, facial motion annotations are inherently non-unique. Even for real captured data, multiple plausible facial animation sequences can correspond to the same speech content, especially in dialogue settings where expressive style and articulation vary across speakers. As result, directly evaluating against single ground-truth facial motion sequence is fundamentally ambiguous. To address this issue, we adopt reference-based evaluation protocol using NVIDIA Audio2Face-3D as fixed external reference model. Importantly, Audio2Face-3D is not treated as unique ground truth, but rather as strong and consistent proxy that enables relative comparison across different methods. We choose Audio2Face-3D for two main reasons. First, it is trained on large-scale, professionally captured real facial motion data, providing strong prior for speech-driven facial dynamics under data-scarce conditions. Second, as state-of-the-art S2F model with high expressiveness and temporal stability, it has been widely adopted in recent literature. Using such fixed reference allows us to assess lipspeech synchronization and temporal consistency in controlled and reproducible 14 manner, without assuming the uniqueness of facial motion annotations. Human A/B Preference Study. To assess perceptual quality beyond reference-based metrics, we conduct human A/B preference study using our best available resources. Both evaluators and test samples are selected randomly rather than being curated or filtered to favor any specific method. Specifically, evaluators are recruited independently and are not involved in model development, and test samples are randomly sampled without manual screening. This human evaluation complements reference-based metrics and mitigates potential bias introduced by using NVIDIA Audio2Face3D both as teacher model for supervision generation and as reference during evaluation. We emphasize that Audio2Face-3D is trained on largescale, professionally captured real facial motion data and is widely regarded as strong proxy for high-quality 3D facial animation. To quantify interrater agreement, we report the mean majority fraction (MMF), defined as the average proportion of assessors selecting the majority label (A/B/Tie) for each sample, which reflects the consistency of human judgments. Speech-to-Text Evaluation We adopt speech-totext question answering (S2T QA) instead of conventional ASR-based evaluation to assess speech understanding. This choice is motivated by the fact that question answering better reflects realworld interactive scenarios for omni-modal models. Unlike cascaded ASR-based pipelines, which are susceptible to error accumulation from transcription mistakes, S2T QA evaluates the models end-to-end semantic reasoning capability directly from speech inputs. By operating on speech signals holistically, OLLMs can leverage semantic and contextual cues beyond exact lexical transcription, resulting in more robust evaluation of speech understanding. A.4 Supplementary of Experiments Results and Analysis Ablation Study on Speech Generation. Table 8 analyzes the effect of the token-as-query gated fusion (TQGF) on speech generation using WER/CER. Removing TQGF leads to performance drop on the more challenging test-hard split (from 13.67 to 14.68), indicating that explicitly regulated semantic injection is important for modeling complex linguistic content and long-range dependencies. On test-en, the performance remains comparable, while on test-zh, removing TQGF improves the error rate. This reflects that TQGF acts as stabilizing mechanism which is consistent with our observations in facial animation generation (Section 5.2). In additional, Table 8: Ablation Study of TQGF on Speech Generation. means lower is better. Method test-zh test-en test-hard Ex-Omni w/o TQGF 3.37 2.24 2.67 2.73 13.67 14.68 Training Dynamics under Different Model Scales. As shown in Figure 4, Stage II and Stage III exhibit nearly identical loss and gradient norm trajectories across the 1.7B, 4B, and 8B models, indicating that the optimization of speech-unit autoregression and speechface alignment is largely insensitive to model scale under these settings. In contrast, in Stage and Stage IV, the 4B model shows higher loss and larger gradient fluctuations than both the smaller 1.7B model and the larger 8B model. In addition, our experiments (not shown for brevity) with 0.5B model reveal unstable gradients, including gradient explosion, in Stage I. This behavior may be related to differences in the pretrained parameter distributions of Qwen3 models at different scales. When speech representations are projected into the LLM semantic space, such differences can increase optimization difficulty, particularly in stages that require direct alignment or joint fine-tuning of high-level semantic representations. Overall, from the perspective of training dynamics, larger-scale LLMs tend to exhibit more stable optimization behavior in these stages. Table 9: Speech-Text consistency analysis results. means lower is better."
        },
        {
            "title": "Model",
            "content": "Avg Audio Dur. WER (%) 0-20 20-40 40-60 0-60 Qwen2.5-Omni LLaMA-Omni2 Ex-Omni 29.72 11.02 25.64 9.19 5.51 14.35 27.56 6.89 9.11 N/A 3.44 12.87 6.48 10.66 11. Speech-Text Consistency Analysis. As shown in Table 9, Figure 5 and Figure 6, we analyze speechtext consistency on the CommonEval dataset by comparing the generated speech and text outputs from each model. Specifically, we 15 Figure 4: Loss curves on different stages with different parameters LLMs. dio duration, indicating that errors tend to accumulate during long-form speech generation. Although LLaMA-Omni2 achieves lower overall WER, this behavior is largely associated with its tendency to generate short audio responses, with most outputs constrained within 20 seconds, which substantially reduces the difficulty of speech synthesis. In contrast, for longer audio durations (4060 seconds), both Ex-Omni and Qwen2.5-Omni exhibit speechtext inconsistency, where the textual responses continue while the generated speech is prematurely truncated. This phenomenon may be influenced by multiple factors, including the limited autoregressive capacity of the relatively small speech generator and token budget constraints, as speech units have much higher temporal resolution than text tokens (e.g., approximately 12 speech tokens per second in Ex-Omni), making long-form speech generation more susceptible to maximum token limits. We further observe that Ex-Omni exhibits higher WER in the 020 second range, which is primarily driven by subset of samples in the 1520 second interval. Further analysis suggests that this behavior may be related to an imbalance in the training data distribution for speech responses of this duration. We leave improving data balance and robustness for both shortand long-form speech generation as future work. Overall, despite these challenges, Ex-Omni demonstrates competitive speechtext consistency compared to Qwen2.5Omni, particularly under longer and more challengFigure 5: Response audio duration distribution. Figure 6: Average WER distribution across different audio durations. transcribe the synthesized speech using WhisperV3-Large and compute the WER between the ASR output and the corresponding text response. The analysis is conducted on 200 CommonEval samples, and only audio segments with durations of up to 60 seconds are included in the analysis. Table 9 reports WER statistics across different audio duration ranges. Across models, we observe consistent trend that speechtext inconsistency increases with au16 to speech-only OLLMs, potentially affecting realtime interaction scenarios. These limitations point to several promising directions for future work. We plan to extend ExOmni to support emotion-aware and more expressive facial animation, as well as to improve the realism and controllability of speech generation, particularly with respect to speaker identity and vocal timbre. Moreover, we will explore more efficient modeling and inference strategies to reduce latency and enable more responsive joint speechface generation for interactive applications. ing generation scenarios. Table 10: Latency ana of Ex-Omni. The experiment is conducted on NVIDIA H20 GPU. means lower is better. Model RTF Speech TTFT (s) Face Latency (s) Ex-Omni 2.158 0. 0.012 Latency Analysis. As shown in Table 10, we evaluate Ex-Omni using three latency metrics: Overall RTF, Avg Speech TTFT, and Avg Face Latency. Overall RTF is defined as RTF = te2e , tspeech where te2e denotes the end-to-end generation time and tspeech is the duration of the generated speech. Avg Speech TTFT measures the time until the first speech unit is generated, reflecting the responsiveness of speech generation. Avg Face Latency measures the additional time required to generate facial animation once speech units become available. We randomly sample 100 instances from the CommonEval dataset for evaluation. As shown in Table 10, Ex-Omni achieves very low Avg Speech TTFT (0.029s) and Avg Face Latency (0.012s), indicating fast component-level responses. However, the overall RTF reaches 2.158 under our current evaluation setup, showing that the model does not yet operate in real time under the tested hardware configuration. All latency measurements are conducted on an NVIDIA H20 GPU, which is the most capable inference hardware available to us at the time of evaluation. The increased end-toend latency mainly stems from the relatively large Qwen3-8B backbone used for semantic reasoning. While optimizing real-time efficiency is not the primary focus of this work, we believe that with more powerful inference hardware or further systemlevel optimizations, the proposed framework has the potential to approach real-time performance. Reducing overall latency while maintaining strong instruction-following and reasoning capabilities remains promising direction for future research. A.5 Limitations and Future Work Despite its effectiveness, Ex-Omni has several limitations. First, the current framework focuses primarily on mouth articulation and lipspeech synchronization, without explicitly modeling higherlevel facial expressions or emotional states, which limits the expressiveness of generated animations. Second, incorporating 3D facial animation inevitably increases generation latency compared"
        }
    ],
    "affiliations": [
        "Independent Researcher",
        "LIGHTSPEED",
        "The Chinese University of Hong Kong, Shenzhen"
    ]
}