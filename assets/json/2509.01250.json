{
    "paper_title": "Towards More Diverse and Challenging Pre-training for Point Cloud Learning: Self-Supervised Cross Reconstruction with Decoupled Views",
    "authors": [
        "Xiangdong Zhang",
        "Shaofeng Zhang",
        "Junchi Yan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Point cloud learning, especially in a self-supervised way without manual labels, has gained growing attention in both vision and learning communities due to its potential utility in a wide range of applications. Most existing generative approaches for point cloud self-supervised learning focus on recovering masked points from visible ones within a single view. Recognizing that a two-view pre-training paradigm inherently introduces greater diversity and variance, it may thus enable more challenging and informative pre-training. Inspired by this, we explore the potential of two-view learning in this domain. In this paper, we propose Point-PQAE, a cross-reconstruction generative paradigm that first generates two decoupled point clouds/views and then reconstructs one from the other. To achieve this goal, we develop a crop mechanism for point cloud view generation for the first time and further propose a novel positional encoding to represent the 3D relative position between the two decoupled views. The cross-reconstruction significantly increases the difficulty of pre-training compared to self-reconstruction, which enables our method to surpass previous single-modal self-reconstruction methods in 3D self-supervised learning. Specifically, it outperforms the self-reconstruction baseline (Point-MAE) by 6.5%, 7.0%, and 6.7% in three variants of ScanObjectNN with the Mlp-Linear evaluation protocol. The code is available at https://github.com/aHapBean/Point-PQAE."
        },
        {
            "title": "Start",
            "content": "Towards More Diverse and Challenging Pre-training for Point Cloud Learning: Self-Supervised Cross Reconstruction with Decoupled Views Xiangdong Zhang, Shaofeng Zhang, Junchi Yan School of AI, Shanghai Jiao Tong University {zhangxiangdong, sherrylone, yanjunchi}@sjtu.edu.cn 5 2 0 2 1 ] . [ 1 0 5 2 1 0 . 9 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Point cloud learning, especially in self-supervised way without manual labels, has gained growing attention in both vision and learning communities due to its potential utility in wide range of applications. Most existing generative approaches for point cloud self-supervised learning focus on recovering masked points from visible ones within single view. Recognizing that two-view pre-training paradigm inherently introduces greater diversity and variance, it may thus enable more challenging and informative pre-training. Inspired by this, we explore the potential of two-view learning in this domain. In this paper, we propose Point-PQAE, cross-reconstruction generative paradigm that first generates two decoupled point clouds/views and then reconstructs one from the other. To achieve this goal, we develop crop mechanism for point cloud view generation for the first time and further propose novel positional encoding to represent the 3D relative position between the two decoupled views. The cross-reconstruction significantly increases the difficulty of pre-training compared to selfreconstruction, which enables our method to surpass previous single-modal self-reconstruction methods in 3D selfsupervised learning. Specifically, it outperforms the selfreconstruction baseline (Point-MAE) by 6.5%, 7.0%, and 6.7% in three variants of ScanObjectNN with the MLPLINEAR evaluation protocol. The code is available at https://github.com/aHapBean/Point-PQAE . 1. Introduction 3D vision is gaining increasing attention for its wide applications such as autonomous driving [44] and robotics [12, 48], owing to its ability to understand the human world. The point cloud is the most popular representation form of data in 3D vision and many analyses based on point cloud are Equal contribution. Corresponding author. This paper is in part supported by Shanghai Municipal Science and Technology Major Project, China, under grant No. 2021SHZDZX0102. Figure 1. Comparison between self-reconstruction (PointMAE [39] and other methods based on it) and cross-reconstruction (Ours) paradigms. In the self-reconstruction paradigm, part of the input data is masked, and the autoencoder is trained to recover the missing patches. In contrast, the cross-reconstruction paradigm generates decoupled views first, with one view leveraging crossview information to reconstruct the other, using an autoencoder backbone. explored to solve various tasks such as object classification [17, 30, 40, 41, 63, 72], object detection [36, 42, 90] and segmentation [17, 30, 40, 63, 68], they typically necessitate fully-supervised training from scratch. Compared to 2D data, point cloud generally requires more expensive and labor-intensive efforts to collect and annotate, which hinders the development of the fully supervised 3D representation learning method. Self-supervised learning is one of the predominant approaches to address this issue and has been proven to be effective in 2D vision [5, 9, 16, 22, 23, 80 83, 89]. Inspired by this, self-supervised learning has been widely studied in the 3D field in recent years. Self-supervised learning in 3D can be mainly divided into two categories, i.e., generative methods [1, 8, 28, 31, 39, 61, 78, 87] and contrastive methods [2, 10, 11, 25, 43, 47, 70]. Similar to 2D, contrastive methods in 3D aim to learn global-discriminative information by maximizing the mutual information across views [2, 10, 70] with intra-/intermodal information [5258, 65, 67]. PointContrast [70] is the first to learn transformation invariance across views through the extension of InfoNCE ob1 jective [37, 70]. Different from contrastive methods, the core idea of the generative methods [31, 39, 78] in 3D is inspired by MAE [23], which masks the input point cloud and uses the visible part to reconstruct the masked part. Point-MAE [39] is one of the conventional methods, which learns point-wise information through the selfreconstruction mechanism. Specifically, Point-MAE masks high ratio of the point cloud and reconstructs the masked points through the visible part. Point-M2AE [78] proposes hierarchical Transformer [33] with corresponding masking strategy based on it, capturing both fine-grained and high-level semantics of 3D shapes. Existing generative methods mostly focus on the masked reconstruction in single view, where single-view learning has been proven less difficult and informative than two views [9, 16] since two views would bring more variance than single view. To bring the benefits of the twoview learning paradigm to the generative learning field, one straightforward assumption is adopting the two-view learning paradigm in the 3D generative pertaining tasks to increase the difficulty of point-wise reconstruction. However, unlike 2D vision data, constructing two views in the 3D point cloud is more challenging, as point cloud data is less structured. Besides, cross-reconstruction (reconstructing one view from the other view) is much more difficult than self-reconstruction (reconstructing the masked single view), which involves learning both the inter-positional relation between two views and the inner spatial information within the view. The differences between these two paradigms can be seen in Fig. 1. Aimed at solving the above issues, we propose Point-PQAE (Position Query based AutoEncoder for Point Cloud), novel framework aimed at addressing two primary challenges: i) how to construct two views in point cloud data, and ii) how to perform cross-reconstruction between two views. As shown in Fig. 2, for each point cloud, we first apply custom-designed point cloud crop mechanism, which randomly selects two points as the central points of two views and records their positions. Following this, for each central point, the nearest points according to specified ratio are incorporated into the view. Subsequently, we normalize the cropped point cloud and apply random augmentation (e.g., rotation) to each point cloud to achieve view decoupling. Then we calculate the view-relative positional embedding (VRPE) for these two decoupled views. Finally, we take the VRPE and one view of the point cloud as input to predict the point-wise input of the other view. The highlights of this paper are: i) New generative framework: We propose PointPQAE, the first framework that successfully brings crossreconstruction to 3D generative field for point cloud selfsupervised learning, with three modules: 1) decoupled views generation, 2) VRPE generation, and 3) positional query block. To our knowledge, we are the first to design and apply crop mechanism to point cloud self-supervised learning. Our framework, Point-PQAE, enables more informative and challenging self-supervised pre-training compared to existing self-reconstruction methods. ii) The first relative position-aware query module for point cloud: We introduce positional query block after the encoder to capture relative position information between decoupled views. The module applies cross-attention between the hidden representations of one view and the VRPE, enabling it to predict the other view. Our module can be easily plugged into various related tasks, e.g., distillation as stated in Appendix Sec. 4. iii) Strong performance: Point-PQAE achieves new stateof-the-art performance on several benchmarks, e.g., outperforming previous methods on few-shot learning and achieving average improvements of 1.8%, 6.7%, and 4.4% over the baseline (Point-MAE) on ScanObjectNN classification across three evaluation protocols (FULL, MLP-LINEAR, MLP-3), respectively. 2. Related works SSL in 2D vision. Self-supervised learning (SSL) in 2D vision is broadly categorized into contrastive-based and masked-image-modeling (MIM) approaches. Contrastive methods learn discriminative features by creating two augmented views of an image and enforcing invariance between them. For instance, SimCLR [9] treats augmented views from the same image as positive pairs and those from different images as negatives. MoCo [22] utilizes memory bank to store these negative samples and further proposes momentum-updated mechanism to avoid model collapse. MIM-based methods mask portions of the input and task models with reconstructing the masked regions, encouraging fine-grained understanding. MAE [23] employs an encoder-decoder architecture, discarding masked patches before reconstruction, while SimMIM [71] replaces masked regions with learnable parameter. Subsequent work explores variations such as masking strategies [27, 85] and feature-level reconstruction [89]. Representation learning for point clouds. It is challenging task due to its irregular and sparse nature when compared to other modalities such as images which are wellstructured and uniform. PointContrast [70] firstly borrows the idea of contrastive learning in 2D and performs pointlevel invariant mapping on two transformed views of the given point cloud. On the basis of PointContrast, CrossPoint [2] further proposes inter and intra-modal contrastive objectives to enhance the global-discriminative capability. To better model multi-modal data, ACT [10] employs crossmodal auto-encoders as teacher models to acquire knowledge from other modalities. In contrast to contrastive objectives, Point-MAE [39] proposes to randomly mask points, and reconstruct the masked part through the visible part. 2 Figure 2. Pipeline of Point-PQAE. The input point cloud is randomly cropped followed by the rotation to generate views. Then, we feed the views to the patch embedding layer and the transformer encoder, followed by the proposed positional query block. The View-Relative Positional Embedding (VRPE) is obtained through the VRPE module by extracting relative geometric relations, which is taken as Query in the cross-attention mechanism. The queried hidden embeddings are then fed to the decoder to predict the inputs of the other view. Following Point-MAE and CrossPoint, ReCon [43] harmoniously incorporates the generative method Point-MAE and the contrastive method. Almost all the existing generative methods in 3D rely on self-reconstruction. To this end, we propose new cross-reconstruction generative framework, named Point-PQAE, which increases pre-training difficulty by introducing cross-view reconstruction, bringing more variance to the training samples compared to singleview reconstruction. This requires the model to effectively learn both intra-view and inter-view knowledge, thereby enabling the model to learn more semantic representations. 3. The proposed Point-PQAE 3.1. Views and embeddings generation Decoupled views generation. Random Image Crop has been widely used in both the supervised domain [21, 26, 50] and the self-supervised domain [4, 9, 22], and it has been proved to be effective in practice. This method typically involves randomly selecting rectangular region from an image and cropping it. However, to the best of our knowledge, no corresponding crop mechanism has been proposed for 3D self-supervised representation learning. Instead of directly applying the 2D crop algorithm to 3D (by randomly selecting cube in space), which may lead to inconsistency, as the points within cubes of the same size can vary significantly due to the diverse distribution of points in 3D, we design in smarter way: For randomly selected crop ratio, we first select center point randomly and then choose the corresponding number of points in the point cloud that are closest to the center point. Formally, given an input point cloud with points Rp3 and pre-defined minimum crop ratio rm where 0 < rm < 1, we first randomly select the crop ratios r1 and r2 uniformly from [rm, 1]. Then, we randomly select two center points C1, C2 R13 and get X1 and X2 by expanding nearest r1 and r2 points to the center point C1 and C2 respectively in X. Meanwhile, we record the absolute coordinates of cropped point clouds geometric centers, which are denoted as L1 R13 and L2 R13, respectively. The process can be formulated as: r1, r2 = UniformSample([rm, 1]) C1, C2 = RandomSelect(X) X1 = N-Nearest(C1, r1, X) X2 = N-Nearest(C2, r2, X) (1) (2) (3) (4) where X1 Rr1p3 and X2 Rr2p3 mean the two cropped views. The X1 and X2 are normalized using the min-max normalization, centered on L1 and L2, respectively. Subsequently, we apply additional augmentation to further enhance the variance between two views. We apply random rotations to the cropped views. The distinct normalization centers, along with random rotation, effectively isolate the coordinate systems of the two views and alter their fixed relative relationships. This process decouples the two point clouds, resulting in two independent views. Point patches generation. We divide the input point cloud into irregular point patches (may overlap) via the Far3 thest Point Sampling(FPS) and K-Nearest Neighborhood (KNN) algorithm following Point-BERT [76]. Formally, given two point clouds (views) X1 and X2, we first use the FPS algorithm to sample pre-defined points as group centers, which we called G1 Rn3 and G2 Rn3, respectively. Then, based on the center points, we use KNN to choose the pre-defined k-nearest points for each group center G(i) (0 n) and finally get groups (each group contains points). We apply the FPS and KNN on X1 and X2, respectively: G1 = FPS(X1), G2 = FPS(X2) P1 = KNN(X1, G1), P2 = KNN(X2, G2) (5) (6) where G1, G2 Rn3 and P1, P2 Rn(k3). Then, the P1 and P2 both are treated as sequence of length and dimension 3 as the input. 3.2. Backbone with positional query Following [39], we adopt lightweight PointNet [40], followed by an asymmetric encoder-decoder structure consisting of Transformer blocks in the Point-PQAE. After the encoder projects the input embedding into the latent space, we use the latent representation of one view and view-relative positional embedding (VRPE) between two views to implement the position query through cross-attention [60]. We directly feed the output of the position query block to the decoder to reconstruct another view without using extra positional embedding. The last layer of the model adopts simple prediction head to achieve the cross-reconstruction. Encoding. The encoder consists of the lightweight PointNet and standard Transformer blocks [60], which projects P1 and P2 to latent space, respectively: H1 = Encoder(P1, G1), H2 = Encoder(P2, G2), (7) where H1 RnD and H2 RnD mean the latent representations of the view 1 and view 2. Note that is the hidden dimension of the networks. Positional query block. After obtaining latent representations H1 and H2, we feed them into the proposed positional query block to extract cross-information. The module can be mainly divided into 3 sub-stages: attain relative information, VRPE generation, and cross-attention. Attain relative information. Take reconstructing view 2 from view 1 as an example. To reconstruct view 2, we have to know the latent representations of view 1 and the relative positions of view 2 centered on view 1, where the latent representations of view 1 can be obtained through the encoder network. Therefore, the key to reconstructing view 2 is to obtain the relative positions of view 2 centered on view 1. Back to the crop operation, we have recorded the geometric centers of views, denoted as L1 and L2, respectively. We define the relative position of view 2 centered on view 1 as RL12 = L1 L2 where RL12 R13. To better align the dimension in the latter operation, we expand the RL12 to Rn3 by repeating the content for times. By obtaining the relative positions between the two views, we further add the group center location to help the PQ modules reconstruct each group of the inputs. Specifically, we define the group-/patchwise relative positions as: RP12 = ConCat(G2, RL12), RP21 = ConCat(G1, RL21) where RP Rn6 (as Rn3 and RL Rn3) and ConCat(, ) is the concatenate operation. (8) (9) View-relative positional embedding (VRPE). Instead of simply using learnable positional encoding, which could hurt the expression of the relative positions between the two views (see ablation in Sec. 4.3), we use the fixed positional encoding of relative information to cut the uncertainty. Specifically, for elements in the second dimension of RP12, we use sinusoid form [23] in Transformers to generate view-relative positional embedding VRPE12: VRPEi 12 = (cid:20) sin (cid:18) RPi e2 1 12 D/12 (cid:19) , cos (cid:18) RPi e2 1 12 D/12 (cid:19) , sin sin D/12 12 (cid:18) RPi e2 2 (cid:18) RPi 12 (cid:19) (cid:19) , cos , cos (cid:18) RPi e2 2 12 D/ (cid:18) RPi 12 (cid:19) , . . . , (cid:19)(cid:21) (10) where RPi 12 Rn1, VRPEi 12 Rn(D/6) (1 6). = 10000 is pre-defined, which is also used in MAE [23]. Finally, we concatenate VRPEi 12 for each dimension to obtain VRPE12 RnD. Cross-attention. Empirically, assume the latent representations of view 1, denoted as H1, contains intrinsic features and the global characteristics of its source point cloud, then with the combination of the view-relative positional embeddings VRPE12, the view 2 can be reconstructed. Specifically, we employ VRPE12 (as Q) and H1 (as K, V) in cross-attention mechanisms, as can be formalized: T2 = Attn(QVRPE12 , KH1 , VH1) (cid:19) (cid:18) QVRPE12KH1 = Softmax VH1 (11) where QVRPE12 = VRPE12WQ, KH1 = H1WK, and VH1 = H1WV, where WQ, WK, and WV are learnable parameters. Reconstructing view 1 from view 2 is siamese condition of this, through the same way as stated above, we can obtain RL21, RP21, VRPE21 and yield T1. Decoding. The obtained T1 and T2 are fed into decoder composed of few transformer blocks: Z1 = Decoder(T1), Z2 = Decoder(T2) (12) 4 Finally, similar to [39], we adopt projection head (composed of fully connected layer) to predict the input point pred, P2 cloud as follows, where P1 pred Rnk3: P1 pred = Reshape(Linear(Z1)), pred = Reshape(Linear(Z2)) (13) (14) 3.3. Objective function The overall objective is to perform cross-reconstruction, which involves predicting view 1 from view 2 and predicting view 2 from view 1. Given the predicted point patches P1 pred and ground truth P1 and P2, we compute the cross-reconstruction loss using the l2 Chamfer Distance loss function [14], written as Lcross = L21 +L12, where: pred and P2 L21 = 1 (cid:12) (cid:12) (cid:12)P1 + (cid:12) (cid:12) (cid:12) pred 1 P1 (cid:88) aP pred min bP1 b2 2 (cid:88) bP1 min aP pred b2 2 (15) where is the cardinality of the set and 2 is the l2 distance. The L12 follows similarly. 4. Experiments This section is organized as follows: First, we present the model architecture and pre-training details. Second, we validate the effectiveness of our pre-trained model on wide range of downstream tasks, including object classification, few-shot learning, and part segmentation. Finally, ablation studies are conducted to demonstrate the properties and robustness of our proposed Point-PQAE. 4.1. Pre-training setups Pre-trained dataset. We use the dataset ShapeNet [6] for pre-training of Point-PQAE following previous studies [10, 39, 43]. ShapeNet [6] consists of about 51,300 clean 3D models, covering 55 common object categories. Model structure. We adopt standard Transformer blocks [60] in the autoencoders backbone where the encoder has 12 Transformer blocks and the decoder has 4 blocks. MLP ratio in Transformer blocks is set to 4. Each Transformer block has 384 hidden dimensions and 6 heads. The positional query module between encoder and decoder only performs cross-attention mechanism once as stated in Sec. 3.2, and the number of heads is set to 6 and remains 384 hidden dimensions. Experiment detail. For each input point cloud, we only apply random rotation for pre-training data augmentation. After sampling 1024 points via FPS from the input point cloud, we generate two different partial point clouds by randomly cropping and rotating as illustrated in Sec. 3.1 with 5 rm = 0.6. Both of them are divided into 64 patches with 32 points via FPS and KNN. The Point-PQAE model undergoes pre-training for 300 epochs using an AdamW [35] optimizer with batch size of 128. The initial learning rate is set to 0.0005, with weight decay of 0.05, with cosine learning rate decay [34] utilized. We visualize the pretraining results in Fig. 3. It shows Point-PQAE learns crossknowledge well and is able to generalize excellently to other crop ratios, even though it was pre-trained with rm = 0.6. 4.2. Transfer learning on downstream tasks Transfer protocol. We use three transfer learning protocols for classification tasks following [10]: FULL, MLPLINEAR, MLP-3, which are detailed in Appendix Sec. 3. 3D real-world object classification. The transferability of models can be well demonstrated through testing on 3D real-world object dataset. Therefore, we transfer our pre-trained model to ScanObjectNN [59] for classification, which is one of the most challenging 3D datasets, covering approximately 15,000 real-world objects across 15 categories. We conduct experiments on three variants: OBJBG, OBJ-ONLY, and PB-T50-RS. The results are reported in Tab. 1. Our Point-PQAE surpasses previous single-modal self-supervised methods by margin under all protocols. In contrast to the most relevant baseline, Point-MAE, PointPQAE significantly outperforms it, showing improvements of 2.4%, 1.7%, and 1.2%. Especially for the MLP-LINEAR and MLP-3 protocols, our Point-PQAE outperforms PointMAE by 6.7% and 4.4% on average, respectively. The excellent frozen representation evaluation results indicate the superior quality of the representation learned by our Point-PQAE during pre-training. This shows that the crossreconstruction paradigm learns more robust representations and generalizes better on real-world datasets. 3D clean object classification. We evaluate the performance of our pre-trained model for object classification using the ModelNet40 dataset [69]. ModelNet40 comprises 12,311 meticulously crafted 3D CAD models, representing 40 distinct object categories. During testing, we adhere to the standard voting method [32] to ensure fair comparisons with previous work such as [8, 39]. Standard random scaling and random translation are applied for data augmentation during training, and the input point clouds exclusively contain coordinate information, without additional normal information provided. The results are presented in Tab. 1. Our Point-PQAE yields better or comparable results compared to previous approaches. Few-shot learning. We conduct the few-shot learning classification on the ModelNet40 dataset to show the generalization ability of our method following the protocols in previous studies [39, 49, 62, 76]. The few-shot learning experiments are conducted in the form of w-way, s-shot including four parts where {5, 10} represents the number of randomly selected classes and {10, 20} means Figure 3. Cross-reconstruction results on ShapeNet. The arrow points from source point clouds to cross-reconstruction results. PointPQAE generalizes well to other crop ratios though with minimum crop ratio rm = 0.6 when pre-training. the number of randomly selected samples for each w. Each part is conducted with 10 independent trials. The mean accuracy and standard deviation are reported in Tab. 2. Our Point-PQAE demonstrates superior performance compared to previous methods in few-shot learning, even surpassing cross-modal methods that utilize strong pre-trained teachers such as ACT and ReCon under FULL protocol. Specifically, Point-PQAE achieves 2.7%, 1.1%, 4.5%, 1.9% improvement under MLP-LINEAR protocol and shows enhanced performance over the self-construction method Point-MAE. Part segmentation. We conduct part segmentation experiments on the ShapeNetPart [75] to validate the effectiveness of our Point-PQAE. The ShapeNetPart contains 16,881 objects covering 16 categories. We sample 2,048 points from each input point cloud, following previous work [39, 76] and divide each cloud into 128 point patches We use the same segmentation head and utilize learned features from the 4th, 8th, and 12th layers of the Transformer block as in Point-MAE [39]. We concatenate three levels of features. Then average pooling, max pooling, and upsampling are utilized to generate features for each point and an MLP is applied for label prediction. The results are reported in Tab. 3a, The results are reported in Tab. 3a, which show that Point-PQAE achieves comparable Inst.mIoU to the previous method, achieving 84.6% in Cls.mIoU and improving the from-scratch baseline by 1.4%. 3D scene segmentation. Semantic segmentation on large-scale 3D scenes is challenging, requiring models possessing deep comprehension of semantics and intricate local geometric relationships. We report semantic segmentation results on the S3DIS dataset [3] in Tab. 3b. Our Point-PQAE improves the from-scratch baseline by 2.0% and 1.4%, and outperforms the self-reconstruction method Point-MAE by 0.7% and 0.6% in mAcc and mIoU. 4.3. Ablation study Experiments are conducted to show the properties of our Point-PQAE. We report the classification results on three variants of ScanObjectNN. The experiment settings are aligned with Sec. 4.2. View-relative positional embedding. To analyze the effect of view-relative positional embedding in the positional query block, we conduct series of experiments using different types of positional embeddings. We primarily consider two types of embeddings: Sinusoid (as discussed in Sec. 3.2) and Learnable, along with None group. Learnable embedding refers to utilizing the relative position information RP12 and RP21 as input and employing an MLP composed of two linear layers with an activation function to map the 6-dimensional input to Ddimensional output. The None group involves using randomly assigned embeddings to assess the effectiveness of VRPE design. Tab. 4a displays the results of different positional embeddings. In the None group, the model cannot learn the relative positional information across two views, In addition to leading to significant drop in accuracy. the type of VRPE, we also experiment with Absolute Positional Embedding (APE) to compare its performance with our proposed VRPE. The APE is unsuitable for our framework since it requires shared coordinate system for both views, making it incompatible with the normalization and rotation used in our method. We incorporate APE into our Point-PQAE by removing these operations. The results in Tab. 4a show APE performs worse and support our claim. Data augmentation. After applying the random crop mechanism to the point cloud, we further perform data augmentation on the cropped views to generate new views. 6 Table 1. Classification accuracy (%) on ScanObjectNN and ModelNet40. The inference model parameters #P (M) are reported. Three variants are evaluated on ScanObjectNN and the accuracy obtained on ModelNet40 is reported for both 1K and 8K points. We compare methods using the plain Transformer architectures, e.g. Point-MAE [39], hierarchical Transformer architectures and dedicated architectures for 3D. The dagger() denotes the baseline results reported from ReCon [43] which aligns augmentation with us and ReCon. Methods #P ScanObjectNN OBJ BG OBJ ONLY PB T50 RS ModelNet40 1K 8K Supervised Learning Only PointNet [40] PointNet++ [41] DGCNN [63] PointCNN [30] SimpleView [15] PCT [17] 3.5 1.5 1.8 0.6 - 2.88 73.3 82.3 82.8 86.1 - - 79.2 84.3 86.2 85.5 - - 68.0 77.9 78.1 78.5 80.50.3 - 89.2 90.7 92.9 92.2 93.9 93.2 with Self -Supervised Representation Learning (FULL) Transformer [60] Point-BERT [76] MaskPoint [31] Point-MAE [39] Point-M2AE [78] PointDif [88] Point-MAE [39] Point-PQAE 22.1 22.1 - 22.1 15.3 22.1 22.1 22.1 83.0 87.4 89.3 90.0 91.2 93.3 92.6 95.0 84.0 88.1 88.1 88.3 88.9 91.9 91.9 93. Methods using cross-modal information and teacher models Joint-MAE [18] TAP [64] ACT [10] PointMLP+ULIP [73] I2P-MAE [79] ReCon [43] 88.9 89.5 91.9 - 91.6 93.6 90.9 90.4 93.3 - 94.2 95.2 - 22.1 22.1 - 15.3 43.6 79.1 83.0 84.3 85.2 86.4 87.6 88.4 89.6 86.1 85.7 88.2 89.4 90.1 90. 91.4 93.2 93.8 93.8 94.0 - 93.8 94.0 94.0 94.0 93.7 94.5 94.1 94.5 90.8 91.9 - - - - 91.8 93.8 - 94.0 - - 94.0 94.3 - - 94.0 94.7 94. with Self -Supervised Representation Learning (MLP-LINEAR) Point-MAE [39] Point-PQAE 22.1 22.1 82.80.3 89.30.3 83.20.2 90.20.4 74.10.2 80.80. 90.20.1 92.00.2 90.70.1 92.20.1 Methods using cross-modal information and teacher models ACT [10] ReCon [43] 85.20.8 89.50.2 85.80.2 89.70.2 22.1 43. 76.30.3 81.40.1 91.40.2 92.50.2 91.80.2 92.70.1 with Self -Supervised Representation Learning (MLP-3) Point-MAE [39] Point-PQAE 22.1 22. 85.80.3 90.70.2 85.50.2 90.90.2 80.40.2 83.30.1 91.30.2 92.80.1 91.70.2 92.90.1 Methods using cross-modal information and teacher models ACT [10] ReCon [43] 87.10.2 90.60.2 87.90.4 90.70.3 22.1 43.6 81.50.2 83.80.4 92.70.2 93.00.1 93.00.1 93.40. Data augmentation is crucial for generating diverse and decoupled views, so we conducted ablation studies on different augmentation methods. The results in Tab. 4b indicate that rotation performs best. Effectiveness of the crop mechanism. Random crop has been shown to be highly effective in 2D self-supervised learning (SSL), particularly for contrastive learning [4, 9, 16], and is also vital for generative learning [23]. However, its potential in 3D SSL remains largely unexplored. Even contrastive learning methods for point cloud understanding have not explored the power of it [2, 38]. To address this gap, we first introduce custom-designed random crop mechanism for point cloud data, using it as the foundation for generating decoupled views. Without the crop augmentation, the two views will contain exactly the same shape though with normalization and rotation applied. The model would only need to infer the augmentation rather than the inter-view relationship, which means the views are not decoupled. Conversely, given two isolated cropped point clouds, followed by min-max normalization and rotation for further decoupling, there remain overlapping and non-overlapping parts, requiring the model to effectively encode the intra-view parts in order to infer the unknown inter-view points in our Point-PQAE. However, directly applying random crop as an additional augmentation to the self-reconstruction method (Point-MAE) does 7 Table 2. Few-shot classification results on ModelNet40. 10 independent trials are conducted in each experimental setting. The mean accuracy (%) and standard deviation are reported for each setting. The dagger() denotes the baseline results reported from ReCon [43] which aligns augmentation with us and ReCon. Methods 5-way 10-way 10-shot 20-shot 10-shot 20-shot DGCNN [63] OcCo [61] 31.62.8 90.62.8 40.84.6 92.51.9 19.92.1 82.91.3 16.91.5 86.52.2 with Self -Supervised Representation Learning (FULL) Transformer [60] Point-BERT [76] MaskPoint [31] Point-MAE [39] Point-M2AE [78] Point-MAE [78] Point-PQAE 87.85.2 94.63.1 95.03.7 96.32.5 96.81.8 96.42.8 96.93.2 93.34.3 96.32.7 97.21.7 97.81.8 98.31.4 97.82.0 98.91.0 84.65.5 91.05.4 91.44.0 92.64.1 92.34.5 92.54.4 94.14.2 89.46.3 92.75.1 93.43.5 95.03.0 95.03.0 95.23.9 96.32.7 Methods using cross-modal information and teacher models Joint-MAE [18] TAP [64] ACT [10] I2P-MAE [79] ReCon [43] 96.72.2 97.31.8 96.82.3 97.01.8 97.31.9 97.91.9 97.81.9 98.01.4 98.31.3 98.91.2 92.63.7 93.12.6 93.34.0 92.65.0 93.33.9 95.12.6 95.81.0 95.62.8 95.53.0 95.83.0 with Self -Supervised Representation Learning (MLP-LINEAR) Point-MAE [78] Point-PQAE 89.74.1 93.53. 91.15.6 93.04.6 91.74.0 96.81.9 83.56.1 89.05.2 with Self -Supervised Representation Learning (MLP-3) Point-MAE [78] Point-PQAE 95.02.8 95.33. 96.72.4 98.21.8 90.64.7 92.03.8 93.85.0 94.73.5 Table 3. Segmentation results. Cls.mIoU (%) and Inst.mIoU (%) refer to Mean intersection over union for all classes and all instances, respectively. mAcc (%) refers to mean accuracy. (a) Part segmentation on ShapeNetPart. (b) Semantic segmentation results on S3DIS Area 5. Method Supervised PointNet [40] PointNet++ [41] DGCNN [63] Self-supervised Transformer [60] CrossPoint [2] Point-BERT [76] Point-MAE [39] Point-PQAE (Ours) Cross-modal ACT [10] ReCon [43] Cls.mIoU (%) Inst.mIoU (%) 80.4 81.9 82.3 83.4 - 84.1 84.2 84.6 84.7 84.8 83.7 85.1 85. 84.7 85.5 85.6 86.1 86.1 86.1 86.4 Method Supervised PointNet [40] PointNet++ [41] Self-supervised Transformer [60] Point-MAE [39] Point-PQAE (Ours) Cross-modal ACT [10] mAcc (%) mIoU (%) 49.0 67.1 68.6 69.9 70. 71.1 41.1 53.5 60.0 60.8 61.4 61.2 not improve the performance much, as shown in Tab. 4c. In other words, it means our Point-PQAE fits crop mechanism better by harmoniously incorporating it into the decoupled views generation process. The experiment results in Tab. 4c demonstrate the critical role of random crop in our method. We set minimum crop ratio rm = 1.0 when removing the crop in our Point-PQAE. Analysis on decoupled views generation. While crop plays an important role in our method, relying solely on the random crop mechanism to generate new views would degenerate cross-reconstruction to simply reconstructing one part from another. This is similar to the block mask self-reconstruction used in Point-MAE [39], where visible blocks are used to reconstruct masked blocks, which is also trivial for pre-training. The only difference between them 8 Table 4. Ablation study with Point-PQAE pre-training on ShapeNet. The classification results by accuracy (%) on three variants of ScanObjectNN are reported. Default settings are marked in gray . (a) View-Relative positional embedding Positional Embedding OBJ BG OBJ ONLY PB T50 RS None APE (sinusoid) VRPE (learnable) VRPE (sinusoid Eq. (10)) 84.5 92.3 93.4 95.0 85.9 91.0 93.1 93.6 79.3 87.7 89.1 89.6 (b) Data augmentation Data augmentation OBJ BG OBJ ONLY PB T50 RS jitter scale rotation scale&translate rotation+scale&translate 93.3 93.3 95.0 92.8 93.8 91.4 91.7 93.6 91.7 92.9 87.3 88.0 89.6 88.1 89.1 (c) 3D random crop mechanism Method crop OBJ BG OBJ ONLY PB T50 RS Point-MAE Point-MAE Point-PQAE Point-PQAE 92.6 92.9 92.9 95.0 91.9 92.1 92.3 93.6 88.4 88.8 87.7 89.6 (d) Effects of augmentations after random crop norm. rotation OBJ BG OBJ ONLY PB T50 RS 92.8 93.3 93.5 95.0 92.1 92.6 92.6 93.6 88.2 87.9 88.8 89.6 is that the cropped parts can overlap somewhat, whereas the block mask does not. What makes the cross-reconstruction different from block mask self-reconstruction are the subsequent augmentations including normalization and rotation that decouple the cropped parts. By normalizing cropped point clouds centered on the geometric centers, the coordinate systems of the two point clouds become isolated and independent from each other, with rotation further amplifying the variance between the two views, thus enabling the generation of two decoupled views. The importance of the augmentations applied after the random crop can be validated through experiments, as shown in Tab. 4d. 5. Conclusion In this paper, we propose novel cross-reconstruction generative framework for self-supervised learning on 3D point clouds, called Point-PQAE. In contrast to well-studied self-reconstruction schemes, Point-PQAE reconstructs one cropped point cloud from another decoupled point cloud. To supply sufficient information for cross-reconstruction, we further propose 3D view-relative positional embedding and corresponding position-aware query module. Compared to self-reconstruction, the cross-reconstruction task, carefully designed by us, is much more challenging for pre-training, promoting the learning of richer semantic representations during pre-training. This enables our proposed Point-PQAE to outperform previous single-modal self-reconstruction methods by margin and to perform on par with, or better than cross-modal methods."
        },
        {
            "title": "Appendix",
            "content": "A. Comparisons to more peer methods Apart from the self-reconstruction methods in the main paper (including baseline Point-MAE [39] and others), there are some other peer methods, including Point-FEMAE [77], PCP-MAE [87], I2P-MAE [79], Joint-MAE [18], CrossBERT [29], and TAP [64]. Here, we discuss the relation of Point-PQAE with these approaches and compare its performance against them to better position our work. brief comparison of peer methods with our Point-PQAE can be seen in Tab. 5, and the performance of these methods on downstream tasks is reported in Tab. 6. Point-PQAE achieves the best or comparable performance when compared with them. Relation to Point-FEMAE [77]. Connection. Both of them are reconstruction-based methods. Differences. 1) Pre-Training Efficiency. Point-FEMAE performs mask reconstruction in both the global and local branches and introduces Local Enhancement Module (LEM) which consists of some convolution layers and MLP layers to each transformer block. To achieve local patch convolution with coordinate-based nearest neighbors, when tokens are input to LEM, it duplicates nearest neighboring patches (k = 20) for each input token and aggregates nearest information for each token which brings an extra calculation burden to each block in the encoder. Point-PQAE utilizes original transformer blocks, making it more efficient during pretraining. 2) Backbone. Point-FEMAE reserves the LEM modules in the encoder for fine-tuning which means adding convolution and MLP layers for each transformer block to the backbone for fine-tuning, while our Point-PQAE utilizes an encoder consisting of pure transformer blocks for fine-tuning, which remains simple and is aligned to previous work. Relation to PCP-MAE [87]. Connection. Both of them are reconstruction-based methods. Differences. Targeted at alleviating information leakage of centers in point cloud, PCP-MAE proposes new module called Predicting Center Module (PCM) and novel loss for better utility of centers based on Point-MAE, which is still self-reconstruction method. Our Point-PQAE differs from it as it is pioneering cross-reconstruction method; it uses VRPE to perform cross-view point cloud reconstruction, which overcomes the limitations of self-reconstruction methods. Relation to I2P-MAE [79]. Connection. Both of them are reconstruction-based methods. Differences. I2P-MAE heavily relies on strong pre-trained 2D models as guide to achieve multi-task cross-modal learning while our PointPQAE utilizes single-modal data without relying on any pre-trained model. Besides, the utilization of 2D data in I2P-MAE brings heavy computation burden to the pretraining process. Relation to Joint-MAE [18]. Connection. Both of them are reconstruction-based methods. Differences. JointMAE utilizes shared weight encoder but 3 different decoders for pre-training, and its multi-task pre-training method including 2D / 3D / 2D-3D reconstruction, which makes it more computational while Point-PQAE utilizes one encoder and one decoder only for per-training and is single task method which focuses on 3D data crossreconstruction. Relation to Cross-BERT [29]. Connection. Both are point cloud self-supervised methods. Differences. CrossBERT is method that utilizes two isolated encoders for cross-modal learning of point clouds and rendered images. To prevent the collapse of its intra-/crossmodal contrastive learning, it further uses another two momentum encoders that perform EMA-update, which makes pre-training of Cross-BERT much more complex than Point-PQAE. Additionally, Cross-BERT requires pre-training dVAE as the tokenizer and includes mask cross-modal learning task alongside contrastive learning. In contrast, Point-PQAE focuses solely on point clouds, utilizing single encoder for single-task self-supervised pre-training. Connection. Relation to TAP [64]. Both are reconstruction-based methods. Differences. TAP makes cross-modal reconstruction, which renders images of point clouds with different poses and after getting the latent representation of the 3D point cloud, it uses the pose information of the rendered images to query cross-modal information from the latent representation by cross-attention. And then using the queried information to rebuild the rendered image. Point-PQAE uses view-relative position embedding (VRPE) to make cross-view information interaction by cross-attention to achieve cross-reconstruction which uses 3D data only. Extra online operation on rendering and processing 2D data in TAP will raise the computational needs compared to Point-PQAE. B. Related cross-reconstruction works Our Point-PQAE pioneers the cross-reconstruction paradigm in 3D point cloud self-supervised learning (SSL). There are two essential components in our proposed cross-reconstruction framework: 1) Two isolated/decoupled views, rather than two parts of the same instance that maintain fixed relative relationship. 2) model that achieves cross-view reconstruction using relative position information. To our knowledge, there are no similar previous methods in this domain. To better position our methodology, we compare it to similar SSL methods, including SiamMAE [19] and CropMAE [13], proposed in the image domain. SiamMAE operates on pairs of randomly sampled video frames and asymmetrically masks them, utilizing the past frame to predict the masked future frame. CropMAE relies on image augmentations to gener9 Table 5. Methodology comparisons between our Point-PQAE and other peer methods. The Extra in the table means extra parts in contrast to Point-MAE which adopts standard transformer blocks as backbone. Methods Point-MAE [39] Point-FEMAE [77] PCP-MAE [87] I2P-MAE [79] Joint-MAE [18] Cross-BERT [29] TAP [64] Point-PQAE Single-/CrossModal Pre-trained Model Needed Single-/MultiTask Extra Transformer Blocks (Pre-training) Extra Modules (Fine-tuning) Single Single Single Cross Cross Cross Cross Single Single Multi Multi Multi Multi Multi Single Single Table 6. Performance of peer methods. The classification results on ScanObjectNN and ModelNet40 and few-shot learning results on ModelNet40 are reported by accuracy (%). We term OBJ BG, OBJ ONLY, PB T50 RS as BG, OY, RS respectively. We compare methods using the plain Transformer architectures, e.g. Point-MAE[39], Point-PQAE (ours), hierarchical Transformer architectures and methods with extra modules during fine-tuning. Methods Point-MAE [39] Point-FEMAE [77] PCP-MAE [87] I2P-MAE [79] Joint-MAE [18] Cross-BERT [29] TAP [64] #P 22.1 27.4 22.1 - - 22.1 22.1 ScanObjectNN ModelNet40 ModelNet40 few-shot BG OY RS 1K 8K 5-way 10-way 10-shot 20-shot 10-shot 20-shot 90.0 95.2 95.5 94.2 90.9 93.7 90.4 88.3 93.3 94.3 91.6 88.9 92.1 89. 85.2 90.2 90.4 90.1 86.1 89.0 85.7 93.8 94.5 94.2 94.1 94.0 94.2 - 93.9 94.0 - - - - 94.4 - 94.3 96.32.5 97.21.9 97.42.3 97.01.8 96.72.2 97.02.1 97.31. 97.81.8 98.61.3 99.10.8 98.31.3 97.71.8 98.21.3 97.81.7 92.64.1 94.03.3 93.53.7 92.65.0 92.63.7 93.03.4 93.12.6 95.03.0 95.82.8 95.92.7 95.53.0 95.12.6 95.63.0 95.81.0 96.93.0 99.01.0 94.04. 96.12.8 Point-PQAE 22.1 95.0 93.6 89. ate two views, using one to reconstruct the other. Relation of SiamMAE and CropMAE to our PointPQAE: All are cross-reconstruction methods. Both SiamMAE and CropMAE have two essential components for cross-reconstruction-framework including two-view (different frames sampled from one video for SiamMAE and isolated augmented images for CropMAE) and crossview reconstruction model. They can be treated as crossreconstruction methods, similar to our Point-PQAE. Difference of SiamMAE and CropMAE to our PointPQAE: 1) Different domain: SiamMAE and CropMAE focus on the 2D SSL domain. Our Point-PQAE is the first method for cross-reconstruction in the 3D SSL domain. 2) Asymmetric/symmetric reconstruction: SiamMAE uses the past to predict the future, which is asymmetric. CropMAE performs asymmetric reconstruction and doesnt explore siamese cross-reconstruction. In contrast, our PointPQAE is inherently symmetric, and the siamese loss brings performance gain. 3) No relative information utilized: SiamMAE and CropMAE do not incorporate relative information into training but rely on non-fully masking to guide the cross-reconstruction. The VRPE adopted by our PointPQAE provides explicit guidance, making training more stable and improving explainability. 4) No tuned-needed mask ratio exists in our framework. There is hyperparameter mask ratio that needs to be tuned in both SiamMAE and CropMAE, but this is not the case in our Point-PQAE framework. Relation and differences between Joint-MAE [18] and PiMAE [7]. We discuss the differences between our framework and two seemingly similar methods in the point cloud domain: Joint-MAE [18] and PiMAE [7]. Joint-MAE and PiMAE adopt similar strategy, utilizing paired point clouds and images to perform cross-modal masked autoencoding. Our framework, however, differs significantly from these two methods. The relation of these methods to our Point-PQAE is that all three are self-supervised approaches that focus on the point cloud domain. Differences: 1) Different motivations: Joint-MAE and PiMAE aim to explore the semantic correlation between 2D and 3D data by performing 3D-2D interactions and achieving crossmodal self-reconstruction through cross-modal knowledge. 10 inspired by the success of two-view preIn contrast, training paradigms, we propose Point-PQAE, the first crossreconstruction framework for point cloud self-supervised learning (SSL). 2) Different modalities: Both Joint-MAE and PiMAE rely on paired image-point cloud data, making them crossmodal methods. Our Point-PQAE, on the other hand, only consumes unlabeled point cloud data, making it more easily extendable. Additionally, incorporating image data could increase computational requirements. 3) Joint-MAE and PiMAE cannot be called crossreconstruction methods, unlike our Point-PQAE, because: Recall that cross-reconstruction methods require two components: decoupled views and cross-reconstruction framework. In cross-reconstruction, decoupled views are obtained through independent augmentations, achieving significant diversity between views, and the crossreconstruction framework relies on information from view 1 to mandatorily reconstruct view 2. The paired 3D and 2D views used by Joint-MAE and PiMAE cannot be considered isolated or decoupled views. Take PiMAE as an example: the image is merely render from specific camera pose of the point cloud. No augmentations can be applied to either of these views (as discussed in Section 4 of the PiMAE paper), so diversity between views cannot be achieved. Cross-view knowledge is used as auxiliary, not mandatory, in these two methods. If either the 3D or 2D data is removed, reconstruction can still be achieved, which turns into the case in MAE [23] or Point-MAE [39]. However, cross-reconstruction framework should mandatorily rely on view 1 to reconstruct view 2, as in our Point-PQAE, SiamMAE [19], and CropMAE [13]. For instance, in Joint-MAE, 3D information is used as auxiliary for 2D MAE (or vice versa), and cross-reconstruction loss (specifically, cross-modal reconstruction loss) is added to the 2D-3D output. Thus, it is more appropriate to refer to these methods as cross-modality self-reconstruction methods. C. Discussion on the view-relative positional embedding and positional query Relative Positional Embedding (RPE) methods. To better position the View-Relative Positional Embedding (VRPE) proposed by us for point cloud cross-view reconstruction, we discuss the difference between our VRPE and existing RPE methods. In the fields of Natural Language Processing (NLP) and 2D vision, RPE techniques have been widely adopted [46, 66, 74]. For instance, Rotary Positional Embedding (RoPE) [51] is an emerging RPE technique gaining traction in the realm of large language models (LLMs). RoPE integrates rotational transformations to encode relative token positions, enabling more efficient extrapolation over unseen sequences. iRPE [66] first reviews existing relative position encoding methods, and then proposes new RPE methods dedicated to 2D images. The work [45] investigates the potential problems in Shaw-RPE and XL-RPE, which are the most representative and prevalent RPEs, and proposes two novel RPEs called LRHC-RPE and GCDFRPE. Generally, in NLP and 2D vision, RPE captures the relative distances or orientations between tokens or pixels to enhance the models capacity to understand relationships between paired elements. This approach often leads to improved generalization, especially when handling outof-distribution data. In contrast, our proposed VRPE is designed with viewor instance-based focus, rather than token-based one. Rather than capturing relationships between individual tokens or pixels, our VRPE encodes the positional relationships between two decoupled views. Our approach is not aimed at improving extrapolation or generalization. Instead, it is tailored to model the geometric and contextual information between different views to facilitate accurate crossview reconstruction. This shift in focus makes our VRPE fundamentally distinct from existing RPE methods, highlighting the importance of carefully distinguishing our approach from existing RPE techniques. Related Positional Query (PQ) methods. The positional query is also used in 2D self-supervised learning (SSL) and AI-generated content (AIGC). PQCL [84] pioneered the introduction of positional query, aiming to represent geometric relationships between multiple cropped views. PQDiff [86] advanced this concept by devising contiguous relative positional query module, applying it to image outpainting to achieve arbitrary location and contiguous expansion factor outpainting. Positional query has also found applications in 2D segmentation tasks. For example, DFPQ [20] generates positional queries dynamically by leveraging cross-attention scores from the previous decoder block and the positional encodings of the image features, which together enhance the effectiveness of semantic segmentation. Our method, however, distinguishes itself from these existing positional query approaches by focusing on the 3D world, which presents significantly greater complexity (one more dimension) and challenges compared to 2D image domains. By leveraging the obtained VRPE to query the target view from the source view, our PQ technique successfully achieves decoupled view reconstruction. D. Additional experimental details Training details. We utilize ShapeNet [6] as our pretraining dataset, which comprises curated collection of 3D CAD object models, featuring 51K unique models across 55 common categories. The pre-training process spans 300 epochs, employing cosine learning rate schedule [34] starting at 5e-4, with warm-up period of 10 epochs. We use the AdamW optimizer [35] and batch size of 128. All experiments are conducted on single GPU i.e., RTX 3090 (24GB). For further training details including pre-training and finetuning, refer to Tab. 7. During the pre-training of our Point-PQAE on ShapeNet, we apply rotation to the input point cloud following ReCon [43], followed by generating decoupled views from the augmented point cloud. Finetuning evaluation protocol. For classification tasks on ScanObjectNN and ModelNet40, as well as fewshot learning on ModelNet40, we adopt three evaluation protocols, following [10, 43], to assess both the transferability of learned representations (FULL) and the quality of frozen features (MLP-LINEAR, MLP-3). The protocols are as follows: (a) FULL: Fine-tuning the pre-trained model by updating both the backbone and the classification head. (b) MLP-LINEAR: Fine-tuning by updating only the classification head, which consists of single-layer linear MLP. (c) MLP-3: Fine-tuning by updating only the parameters of three-layer non-linear MLP classification head (which is structured the same as in FULL). E. Additional ablation study Integrate Positional Query (PQ) scheme into knowledge distillation. The knowledge distillation [24] typically involves inputting the same instance into both the student model and the frozen teacher model, then maximizing the mutual agreement between their outputs to distill knowledge from the teacher to the student. Our positional query block can be seamlessly integrated into knowledge distillation, allowing for cross-view distillation rather than being confined to distillation within the same view. For example, view 1 is fed to the student, view 2 is fed to the teacher, and positional query block is added after the backbone to model relative relations and recover the latent representation of view 2. We conduct experiments on distilling the pre-trained model ReCon [43], and the results are reported in Tab. 8, indicating that our PQ scheme successfully learns knowledge from the ReCon teacher and performs much better than the baseline. It shows that the PQ scheme can be easily utilized as plug-in tool for knowledge distillation. Reconstruction loss function. Tab. 9 shows the performance of Point-PQAE using different reconstruction loss functions: cosine similarity loss (cos), l1-form Chamfer distance [14] (CD-l1), and the l2-form Chamfer distance (CD-l2). The results show the CD-l2 is more suitable for Point-PQAE. Siamese loss function. The generative pre-training task designed by us is naturally siamese structure and we get the form of Lcross = L21 + L12 as stated in Sec. 3.3. We analyze the benefit of the siamese loss function by doing an ablation study with loss functions Lcross = L21 + L12 or L21 only. The Tab. 10 presents the experiment results. It shows this siamese loss function contributes to the performance of our Point-PQAE and brings accuracy gain. Minimum crop ratio. The minimum crop ratio rm is important for the proposed point cloud crop mechanism. We conduct experiments to analyze the effect of minimum random crop ratios on the performance. The results are reported in Fig. 4. The results show that 0.6 is the best crop ratio for our Point-PQAE. When the ratio is too low, the model struggles to extract sufficient relevant information from the cropped view for effective cross-reconstruction. Conversely, excessively high ratios make the task too straightforward, hindering the model from learning robust representations. Definition of views and parts in our work. We emphasize the importance of distinguishing between parts and views to understand the significance of decoupled view generation and our cross-view reconstruction method. We define the following: Without independently applying augmentations after cropping, the relative relationships between the cropped parts remain fixed. However, by performing view decoupling, the relative relationships between parts become more diverse, and we define these as views. Existing self-reconstruction methods generally focus on cross-part reconstruction (e.g., block masking in PointIn contrast, cross-view reconstruction (ours) MAE [39]). significantly outperforms cross-part reconstruction, as demonstrated in the main paper Table 4, where line 4 outperforms line 1. F. Limitations and future work Point-PQAE is novel cross-reconstruction generative learning paradigm that differs significantly from previous self-reconstruction methods, enabling more diverse and challenging pre-training. Point-MAE [39] pioneered the self-reconstruction paradigm in the point cloud selfsupervised (SSL) learning field and variant optimizations are well explored, e.g., cross-modal [10, 18, 43], masking strategy [79], and hierarchical architecture [78, 79]. Compared to the well-studied self-reconstruction, crossreconstruction remains significantly under-explored. As the initial venture into cross-reconstruction, our PointPQAE opens new avenue for advancement in point cloud SSL. However, the model employs vanilla transformer architecture and is constrained to single-modality knowledge. This architecture may not be optimally suited for crossreconstruction tasks. Furthermore, the limited size of the available 3D point cloud datasetsdue to the challenges 12 Table 7. Training details for pretraining and downstream fine-tuning."
        },
        {
            "title": "ShapeNetPart",
            "content": "S3DIS optimizer learning rate weight decay learning rate scheduler training epochs warmup epochs batch size drop path rate number of points number of point patches point patch size augmentation AdamW 5e-4 5e-2 cosine 300 10 128 0.1 1024 64 32 Rotation AdamW 2e-5 5e-2 cosine 300 10 32 0.2 2048 128 32 Rotation AdamW 1e-5 5e-2 cosine 300 10 32 0.2 1024 64 32 Scale&Trans AdamW 2e-4 5e-2 cosine 300 10 16 0.1 2048 128 32 - AdamW 2e-4 5e-2 cosine 60 10 32 0.1 2048 128 32 -"
        },
        {
            "title": "GPU device",
            "content": "RTX 3090 RTX 3090 RTX 3090 RTX 3090 RTX 3090 Figure 4. Ablation study on different minimum crop ratios rm, where the results (%) of three variants: OBJ BG, OBJ ONLY, PT T50 RS on ScanObjectNN are reported. Table 8. Integrate PQ into distillation. Results on ScanobjectNN (%) are reported. Type OBJ BG OBJ ONLY PB T50 RS Train from scratch PQ distillation 83.0 93.5 84.0 91. 79.1 88.5 in data collectionrestricts the broader applicability of our single-modality approach. Future work could explore the integration of knowledge from additional modalities or the development of more efficient and appropriate architectures for the cross-reconstruction paradigm. Table 9. Reconstruction loss function. The default setting is marked in gray."
        },
        {
            "title": "References",
            "content": "Loss Function OBJ BG OBJ ONLY PB T50 RS cos CD-l1 CD-l2 90.5 93.1 95.0 89.8 91.7 93.6 85.2 89.4 89.6 Table 10. Siamese loss. The default setting is marked in gray. Loss Function OBJ BG OBJ ONLY PB T50 RS L21 L21 + L12 93.4 95.0 92.4 93.6 89.2 89.6 [1] Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and Leonidas Guibas. Learning representations and generative models for 3d point clouds. In International conference on machine learning, pages 4049. PMLR, 2018. [2] Mohamed Afham, Isuru Dissanayake, Dinithi Dissanayake, Amaya Dharmasiri, Kanchana Thilakarathna, and Ranga Rodrigo. Crosspoint: Self-supervised cross-modal contrastive learning for 3d point cloud understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 99029912, 2022. 1, 2, 7, 8 [3] Iro Armeni, Ozan Sener, Amir Zamir, Helen Jiang, Ioannis Brilakis, Martin Fischer, and Silvio Savarese. 3d semantic parsing of large-scale indoor spaces. In Proceedings of 13 the IEEE conference on computer vision and pattern recognition, pages 15341543, 2016. 6 [4] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: arXiv preprint Bert pre-training of image transformers. arXiv:2106.08254, 2021. 3, [5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. 1 [6] Angel Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015. 5, 11 [7] Anthony Chen, Kevin Zhang, Renrui Zhang, Zihan Wang, Yuheng Lu, Yandong Guo, and Shanghang Zhang. Pimae: Point cloud and image interactive masked autoencoders for 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 52915301, 2023. 10 [8] Guangyan Chen, Meiling Wang, Yi Yang, Kai Yu, Li Yuan, and Yufeng Yue. Pointgpt: Auto-regressively generative pretraining from point clouds. Advances in Neural Information Processing Systems, 36, 2024. 1, 5 [9] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 15971607. PMLR, 2020. 1, 2, 3, 7 [10] Runpei Dong, Zekun Qi, Linfeng Zhang, Junbo Zhang, Jianjian Sun, Zheng Ge, Li Yi, and Kaisheng Ma. Autoencoders as cross-modal teachers: Can pretrained 2d image transformers help 3d representation learning? arXiv preprint arXiv:2212.08320, 2022. 1, 2, 5, 7, 8, [11] Bian Du, Xiang Gao, Wei Hu, and Xin Li. Self-contrastive learning with hard negative sampling for self-supervised In Proceedings of the 29th ACM Inpoint cloud learning. ternational Conference on Multimedia, pages 31333142, 2021. 1 [12] Ikenna Enebuse, Mathias Foo, Babul Salam Ksm Kader Ibrahim, Hafiz Ahmed, Fhon Supmak, and Odongo Steven Eyobu. comparative review of hand-eye calibration techIEEE Access, 9:113143 niques for vision guided robots. 113155, 2021. 1 [13] Alexandre Eymael, Renaud Vandeghen, Anthony Cioppa, Silvio Giancola, Bernard Ghanem, and Marc Van Droogenbroeck. Efficient image pre-training with siamese cropped masked autoencoders. In European Conference on Computer Vision, pages 348366. Springer, 2025. 9, 11 [14] Haoqiang Fan, Hao Su, and Leonidas Guibas. point set generation network for 3d object reconstruction from single image. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 605613, 2017. 5, 12 [15] Ankit Goyal, Hei Law, Bowei Liu, Alejandro Newell, and Jia Deng. Revisiting point cloud shape classification with simple and effective baseline. In International Conference on Machine Learning, pages 38093820. PMLR, 2021. 7 [16] Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:2127121284, 2020. 1, 2, 7 [17] Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang Mu, Ralph Martin, and Shi-Min Hu. Pct: Point cloud transformer. Computational Visual Media, 7:187199, 2021. 1, [18] Ziyu Guo, Renrui Zhang, Longtian Qiu, Xianzhi Li, and Pheng-Ann Heng. Joint-mae: 2d-3d joint masked autoencoders for 3d point cloud pre-training. arXiv preprint arXiv:2302.14007, 2023. 7, 8, 9, 10, 12 [19] Agrim Gupta, Jiajun Wu, Jia Deng, and Fei-Fei Li. Siamese masked autoencoders. Advances in Neural Information Processing Systems, 36:4067640693, 2023. 9, 11 [20] Haoyu He, Jianfei Cai, Zizheng Pan, Jing Liu, Jing Zhang, Dacheng Tao, and Bohan Zhuang. Dynamic focus-aware positional queries for semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1129911308, 2023. 11 [21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. 3 [22] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 97299738, 2020. 1, 2, 3 [23] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000 16009, 2022. 1, 2, 4, 7, [24] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. DistillarXiv preprint ing the knowledge in neural network. arXiv:1503.02531, 2015. 12 [25] Siyuan Huang, Yichen Xie, Song-Chun Zhu, and Yixin Zhu. Spatio-temporal self-supervised representation learning for In Proceedings of the IEEE/CVF Inter3d point clouds. national Conference on Computer Vision, pages 65356545, 2021. 1 [26] Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012. 3 [27] Gang Li, Heliang Zheng, Daqing Liu, Chaoyue Wang, Bing Su, and Changwen Zheng. Semmae: Semantic-guided masking for learning masked autoencoders. NeurIPS, 2022. 2 [28] Jiaxin Li, Ben Chen, and Gim Hee Lee. So-net: SelfIn Proceedorganizing network for point cloud analysis. ings of the IEEE conference on computer vision and pattern recognition, pages 93979406, 2018. 1 [29] Xin Li, Peng Li, Zeyong Wei, Zhe Zhu, Mingqiang Wei, Junhui Hou, Liangliang Nan, Jing Qin, Haoran Xie, and Fu Lee 14 Wang. Cross-bert for point cloud pretraining. arXiv preprint arXiv:2312.04891, 2023. 9, 10 [30] Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, and Baoquan Chen. Pointcnn: Convolution on x-transformed points. Advances in neural information processing systems, 31, 2018. 1, 7 [31] Haotian Liu, Mu Cai, and Yong Jae Lee. Masked discrimination for self-supervised learning on point clouds. In European Conference on Computer Vision, pages 657675. Springer, 2022. 1, 2, 7, 8 [32] Yongcheng Liu, Bin Fan, Shiming Xiang, and Chunhong Pan. Relation-shape convolutional neural network for point cloud analysis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8895 8904, 2019. 5 [33] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1001210022, 2021. [34] Ilya Loshchilov and Frank Hutter. tic gradient descent with warm restarts. arXiv:1608.03983, 2016. 5, 12 Sgdr: StochasarXiv preprint [35] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 5, 12 [36] Ishan Misra, Rohit Girdhar, and Armand Joulin. An end-toend transformer model for 3d object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 29062917, 2021. [37] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. 2 [38] Bo Pang, Hongchi Xia, and Cewu Lu. Unsupervised 3d point cloud representation learning by triangle constrained In Proceedings of the contrast for autonomous driving. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 52295239, 2023. 7 [39] Yatian Pang, Wenxiao Wang, Francis EH Tay, Wei Liu, Yonghong Tian, and Li Yuan. Masked autoencoders for point cloud self-supervised learning. In European conference on computer vision, pages 604621. Springer, 2022. 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12 [40] Charles Qi, Hao Su, Kaichun Mo, and Leonidas Guibas. Pointnet: Deep learning on point sets for 3d classification In Proceedings of the IEEE conference and segmentation. on computer vision and pattern recognition, pages 652660, 2017. 1, 4, 7, 8 [41] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas Guibas. Pointnet++: Deep hierarchical feature learning on point sets in metric space. Advances in neural information processing systems, 30, 2017. 1, 7, 8 [42] Charles Qi, Or Litany, Kaiming He, and Leonidas Guibas. Deep hough voting for 3d object detection in point clouds. In proceedings of the IEEE/CVF International Conference on Computer Vision, pages 92779286, 2019. 1 [43] Zekun Qi, Runpei Dong, Guofan Fan, Zheng Ge, Xiangyu Zhang, Kaisheng Ma, and Li Yi. Contrast with reconstruct: Contrastive 3d representation learning guided by generative pretraining. arXiv preprint arXiv:2302.02318, 2023. 1, 3, 5, 7, 8, 12 [44] Rui Qian, Xin Lai, and Xirong Li. 3d object detection for autonomous driving: survey. Pattern Recognition, 130: 108796, 2022. 1 [45] Anlin Qu, Jianwei Niu, and Shasha Mo. Explore better relative position embeddings from encoding perspective for In Proceedings of the 2021 Confertransformer models. ence on Empirical Methods in Natural Language Processing, pages 29892997, 2021. 11 [46] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. 11 [47] Aditya Sanghi. Info3d: Representation learning on 3d objects using mutual information maximization and contrastive learning. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XXIX 16, pages 626642. Springer, 2020. 1 [48] Oleg Yu Sergiyenko and Vera Tyrsa. 3d optical machine vision sensors with intelligent data management for robotic swarm navigation improvement. IEEE Sensors Journal, 21 (10):1126211274, 2020. 1 [49] Charu Sharma and Manohar Kaul. Self-supervised few-shot learning on point clouds. Advances in Neural Information Processing Systems, 33:72127221, 2020. [50] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. 3 [51] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 11 [52] Haoru Tan, Chuang Wang, Sitong Wu, Tieqiang Wang, XuYao Zhang, and Cheng-Lin Liu. Proxy graph matching with proximal matching networks. In The Annual AAAI Conference on Artificial Intelligence (AAAI), 2021. 1 [53] Haoru Tan, Sitong Wu, Fei Du, Yukang Chen, Zhibin Wang, Fan Wang, and Xiaojuan Qi. Data pruning via movingone-sample-out. In Neural Information Processing Systems (NeurIPS), 2023. [54] Haoru Tan, Sitong Wu, and Jimin Pi. Semantic diffusion network for semantic segmentation. In Neural Information Processing Systems (NeurIPS), 2023. [55] Haoru Tan, Chuang Wang, Sitong Wu, Xu-Yao Zhang, Fei Yin, and Cheng-Lin Liu. Ensemble quadratic assignment network for graph matching. International Journal of Computer Vision (IJCV), 2024. [56] Haoru Tan, Sitong Wu, Zhuotao Tian, Yukang Chen, Xiaojuan Qi, and Jiaya Jia. Saco loss: Sample-wise affinity consistency for vision-language pre-training. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [57] Haoru Tan, Sitong Wu, Wei Huang, Shizhen Zhao, and Xiaojuan Qi. Data pruning by information maximization. In In15 ternational Conference on Learning Representations (ICLR), 2025. [58] Haoru Tan, Sitong Wu, Bo Zhao, Zeke Xie, and XIAOJUAN QI. Diff-in: Data influence estimation with differential approximation, 2025. 1 [59] Mikaela Angelina Uy, Quang-Hieu Pham, Binh-Son Hua, Thanh Nguyen, and Sai-Kit Yeung. Revisiting point cloud classification: new benchmark dataset and classification model on real-world data. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1588 1597, 2019. [60] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 4, 5, 7, 8 [61] Hanchen Wang, Qi Liu, Xiangyu Yue, Joan Lasenby, and Matt Kusner. Unsupervised point cloud pre-training via occlusion completion. In Proceedings of the IEEE/CVF international conference on computer vision, pages 97829792, 2021. 1, 8 [62] Hanchen Wang, Qi Liu, Xiangyu Yue, Joan Lasenby, and Matt Kusner. Unsupervised point cloud pre-training via occlusion completion. In Proceedings of the IEEE/CVF international conference on computer vision, pages 97829792, 2021. 5 [63] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay Sarma, Michael Bronstein, and Justin Solomon. Dynamic graph cnn for learning on point clouds. ACM Transactions on Graphics (tog), 38(5):112, 2019. 1, 7, 8 [64] Ziyi Wang, Xumin Yu, Yongming Rao, Jie Zhou, and Jiwen Lu. Take-a-photo: 3d-to-2d generative pre-training of point cloud models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 56405650, 2023. 7, 8, 9, 10 [65] Hai Wu, Ruifei He, Haoru Tan, Xiaojuan Qi, and Kaibin Huang. Vertical layering of quantized neural networks for IEEE Transactions on Pattern heterogeneous inference. Analysis and Machine Intelligence, 2023. [66] Kan Wu, Houwen Peng, Minghao Chen, Jianlong Fu, and Hongyang Chao. Rethinking and improving relative position encoding for vision transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1003310041, 2021. 11 [67] Sitong Wu, Haoru Tan, Yukang Chen, Shaofeng Zhang, Jingyao Li, Bei Yu, Xiaojuan Qi, and Jiaya Jia. Mixture-ofscores: Robust image-text data quality score via three lines In International Conference on Computer Vision of code. (ICCV), 2025. 1 [68] Xiaoyang Wu, Li Jiang, Peng-Shuai Wang, Zhijian Liu, Xihui Liu, Yu Qiao, Wanli Ouyang, Tong He, and Hengshuang Zhao. Point transformer v3: Simpler, faster, stronger. arXiv preprint arXiv:2312.10035, 2023. 1 [69] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: deep representation for volumetric shapes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 19121920, 2015. 5 [70] Saining Xie, Jiatao Gu, Demi Guo, Charles Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised preIn Computer training for 3d point cloud understanding. VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part III 16, pages 574591. Springer, 2020. 1, 2 [71] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: simple framework for masked image modeling. In CVPR, 2022. [72] Mingye Xu, Zhipeng Zhou, Yali Wang, and Yu Qiao. Towards robustness and generalization of point cloud representation: geometry coding method and large-scale objectlevel dataset. Computational Visual Media, 10(1):2743, 2024. 1 [73] Le Xue, Mingfei Gao, Chen Xing, Roberto Martn-Martn, Jiajun Wu, Caiming Xiong, Ran Xu, Juan Carlos Niebles, and Silvio Savarese. Ulip: Learning unified representation of language, images, and point clouds for 3d understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11791189, 2023. 7 [74] Zhilin Yang. training for arXiv:1906.08237, 2019. 11 Xlnet: Generalized autoregressive prearXiv preprint language understanding. [75] Li Yi, Vladimir Kim, Duygu Ceylan, I-Chao Shen, Mengyan Yan, Hao Su, Cewu Lu, Qixing Huang, Alla Sheffer, and Leonidas Guibas. scalable active framework for region annotation in 3d shape collections. ACM Transactions on Graphics (ToG), 35(6):112, 2016. [76] Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie Zhou, and Jiwen Lu. Point-bert: Pre-training 3d point cloud transformers with masked point modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1931319322, 2022. 4, 5, 6, 7, 8 [77] Yaohua Zha, Huizhen Ji, Jinmin Li, Rongsheng Li, Tao Dai, Bin Chen, Zhi Wang, and Shu-Tao Xia. Towards compact 3d representations via point feature enhancement masked autoencoders. arXiv preprint arXiv:2312.10726, 2023. 9, 10 [78] Renrui Zhang, Ziyu Guo, Peng Gao, Rongyao Fang, Bin Zhao, Dong Wang, Yu Qiao, and Hongsheng Li. Point-m2ae: multi-scale masked autoencoders for hierarchical point cloud pre-training. Advances in neural information processing systems, 35:2706127074, 2022. 1, 2, 7, 8, 12 [79] Renrui Zhang, Liuhui Wang, Yu Qiao, Peng Gao, and Hongsheng Li. Learning 3d representations from 2d pre-trained In Promodels via image-to-point masked autoencoders. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2176921780, 2023. 7, 8, 9, 10, 12 [80] Shaofeng Zhang, Qiang Zhou, Sitong Wu, Haoru Tan, Zhibin Wang, Jinfa Huang, and Junchi Yan. Cr2pq: Continuous relative rotary positional query for dense visual representation learning. In The Thirteenth International Conference on Learning Representations. 1 [81] Shaofeng Zhang, Meng Liu, and Junchi Yan. The diversified ensemble neural network. Advances in Neural Information Processing Systems, 33:1600116011, 2020. [82] Shaofeng Zhang, Feng Zhu, Junchi Yan, Rui Zhao, and Xiaokang Yang. Zero-cl: Instance and feature decorrelation 16 for negative-free symmetric contrastive learning. In International Conference on Learning Representations, 2021. [83] Shaofeng Zhang, Meng Liu, Junchi Yan, Hengrui Zhang, Lingxiao Huang, Xiaokang Yang, and Pinyan Lu. M-mix: Generating hard negatives via multi-sample mixing for contrastive learning. In Proceedings of the 28th ACM SIGKDD conference on knowledge discovery and data mining, pages 24612470, 2022. 1 [84] Shaofeng Zhang, Qiang Zhou, Zhibin Wang, Fan Wang, and Junchi Yan. Patch-level contrastive learning via positional query for visual pre-training. In ICML, 2023. 11 [85] Shaofeng Zhang, Feng Zhu, Rui Zhao, and Junchi Yan. Contextual image masking modeling via synergized contrasting without view augmentation for faster and better visual pretraining. In ICLR, 2023. 2 [86] Shaofeng Zhang, Jinfa Huang, Qiang Zhou, Fan Wang, Jiebo Luo, Junchi Yan, et al. Continuous-multiple image outpainting in one-step via positional query and diffusion-based approach. In ICLR, 2024. 11 [87] Xiangdong Zhang, Shaofeng Zhang, and Junchi Yan. Pcpmae: Learning to predict centers for point masked autoencoders. Advances in Neural Information Processing Systems, 37:8030380327, 2024. 1, 9, [88] Xiao Zheng, Xiaoshui Huang, Guofeng Mei, Yuenan Hou, Zhaoyang Lyu, Bo Dai, Wanli Ouyang, and Yongshun Gong. Point cloud pre-training with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2293522945, 2024. 7 [89] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot: Image bert pre-training arXiv preprint arXiv:2111.07832, with online tokenizer. 2021. 1, 2 [90] Yin Zhou and Oncel Tuzel. Voxelnet: End-to-end learning for point cloud based 3d object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 44904499, 2018."
        }
    ],
    "affiliations": [
        "School of AI, Shanghai Jiao Tong University"
    ]
}