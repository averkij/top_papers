{
    "paper_title": "FitDiT: Advancing the Authentic Garment Details for High-fidelity Virtual Try-on",
    "authors": [
        "Boyuan Jiang",
        "Xiaobin Hu",
        "Donghao Luo",
        "Qingdong He",
        "Chengming Xu",
        "Jinlong Peng",
        "Jiangning Zhang",
        "Chengjie Wang",
        "Yunsheng Wu",
        "Yanwei Fu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Although image-based virtual try-on has made considerable progress, emerging approaches still encounter challenges in producing high-fidelity and robust fitting images across diverse scenarios. These methods often struggle with issues such as texture-aware maintenance and size-aware fitting, which hinder their overall effectiveness. To address these limitations, we propose a novel garment perception enhancement technique, termed FitDiT, designed for high-fidelity virtual try-on using Diffusion Transformers (DiT) allocating more parameters and attention to high-resolution features. First, to further improve texture-aware maintenance, we introduce a garment texture extractor that incorporates garment priors evolution to fine-tune garment feature, facilitating to better capture rich details such as stripes, patterns, and text. Additionally, we introduce frequency-domain learning by customizing a frequency distance loss to enhance high-frequency garment details. To tackle the size-aware fitting issue, we employ a dilated-relaxed mask strategy that adapts to the correct length of garments, preventing the generation of garments that fill the entire mask area during cross-category try-on. Equipped with the above design, FitDiT surpasses all baselines in both qualitative and quantitative evaluations. It excels in producing well-fitting garments with photorealistic and intricate details, while also achieving competitive inference times of 4.57 seconds for a single 1024x768 image after DiT structure slimming, outperforming existing methods."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 5 1 ] . [ 1 9 9 4 0 1 . 1 1 4 2 : r FitDiT: Advancing the Authentic Garment Details for High-fidelity Virtual Try-on Boyuan Jiang1* Xiaobin Hu1* Donghao Luo1 Qingdong He1 Chengming Xu1 Jinlong Peng1 Jiangning Zhang1 Chengjie Wang1 Yunsheng Wu1 Yanwei Fu2 1 Tencent 2 Fudan University Figure 1. FitDiT demonstrates exceptional performance in virtual try-on, addressing challenges related to texture-aware preservation and size-aware fitting across various scenarios."
        },
        {
            "title": "Abstract",
            "content": "Although image-based virtual try-on has made considerable progress, emerging approaches still encounter challenges in producing high-fidelity and robust fitting images across diverse scenarios. These methods often struggle with issues such as texture-aware maintenance and sizeaware fitting, which hinder their overall effectiveness. To address these limitations, we propose novel garment perception enhancement technique, termed FitDiT, designed for high-fidelity virtual try-on using Diffusion Transformers (DiT) allocating more parameters and attention to highresolution features. First, to further improve texture-aware maintenance, we introduce garment texture extractor that incorporates garment priors evolution to fine-tune garment feature, facilitating to better capture rich details such *Equal Contribution Project page: https://byjiang.com/FitDiT/. as stripes, patterns, and text. Additionally, we introduce frequency-domain learning by customizing frequency distance loss to enhance high-frequency garment details. To tackle the size-aware fitting issue, we employ dilatedrelaxed mask strategy that adapts to the correct length of garments, preventing the generation of garments that fill the entire mask area during cross-category try-on. Equipped with the above design, FitDiT surpasses all baselines in both qualitative and quantitative evaluations. It excels in producing well-fitting garments with photorealistic and intricate details, while also achieving competitive inference times of 4.57 seconds for single 1024 768 image after DiT structure slimming, outperforming existing methods. The code and dataset will be made publicly available. 1. Introduction The remarkable growth of e-commerce has created consistent demand for more convenient and personalized shopping experience. Image-based virtual try-on (VTON) has emerged as widely adopted technique aimed at generating realistic images of human models wearing specific garments, thereby enhancing the shopping experience for consumers. In recent years, multitude of researchers [9, 11, 1517, 20, 21, 28, 41, 43, 45] have dedicated significant efforts towards achieving more realistic and photorealistic virtual try-on results. Most recent studies in this domain have predominantly utilized generative adversarial networks (GANs) [13] or latent diffusion models (LDMs) [36] for image and video synthesis. However, traditional GAN-based approaches [1517] often struggle to faithfully reproduce complex garment textures, realistic lighting and shadows, and lifelike depictions of the human body. As result, recent research has shifted focus toward LDM U-Net-based methods [7, 44, 49], which enhance the authenticity of clothed images. Despite these advancements, virtual try-on still faces significant garment-fitting challenges, particularly in two areas: (1) Rich texture-aware maintenance, where the transformation of intricate textures (e.g., patterns, text, stripes, trademarks) to the target model is hindered by the limitations of U-Net-based diffusion structures, which allocate less attention to high-resolution latent features; and (2) Size-aware fitting, where clothing information leakage occurs in cross-category or size-mismatched try-on scenarios, leading to generated garments that cover the entire agnostic mask region. To address these challenges and ensure robustness across diverse scenarios, we propose FitDiT, novel approach to enhance authentic garment perception for high-fidelity virtual try-on. First, to tackle the rich texture-aware challenge, we introduce DiT-based LDM that leverages the strengths of the Diffusion Transformer architecture, focusing more attention on high-resolution latent features related to garment patterns. We also provide an analysis of the structural differences between U-Net and DiT-based LDMs. Given that existing DiT methods currently available suffer from redundancy and sub-optimal structures for virtual try-on applications, we propose reevaluation of the try-on DiT approach. The customization design of our FitDiT mainly focuses on three key aspects: structure slimming, garment condition modulation, and garment feature injection. Besides, unlike other try-on methods that directly use existing Diffusion U-Net as the Garment Net without adequately fitting the garment texture, we introduce garment priors evolution strategy. This strategy fine-tunes the garment-dedicated extractor using garment data to render exceptional and rich details. Furthermore, from the observation of the frequency spectra discrepancies between synthesized and real images, frequency-spectra distance loss in pixel space is proposed to refine the high-frequency knowledge in the frequency domain. This high-frequency knowledge involves complex textures and boosts authentic garment perception. Lastly, to improve size-aware fitting, we propose dilated-relaxed mask augmentation that employs coarse rectangular mask with random adjustments to its length and width. This masking strategy can effectively avoid the leakage of garment shape information during training. The random adjustments enable the model to autonomously perceive the overall shape of the garment, promoting the generation of well-fitted try-ons, particularly for mismatched garments. Our contributions are summarized as follows: To the best of our knowledge, our FitDiT is the first attempt to customize the Diffusion Transformer (DiT) for virtual try-on applications, overcoming the limitations of current U-Net LDMs in complex texture maintenance by assigning greater attention to high-resolution features. For rich texture-aware maintenance, we propose garment priors evolution strategy to better exact the pattern knowledge of garments and frequency-spectra distance loss in pixel space to retain complex patterns. For size-aware fitting, we propose dilated-relaxed mask augmentation with the coarse rectangular mask to lower the leakage of garment shape, and enable the model to adaptively learn the overall shape of garments. Extensive qualitative and quantitative evaluations have clearly demonstrated FitDiTs superiority over state-ofthe-art virtual try-on models, especially in handling richly textured garments with size mismatches. Additionally, it achieves competitive inference times of 4.57 seconds for single 1024 768 image, surpassing existing methods. These findings serve as significant milestone in advancing the field of virtual try-on, enabling more intricate applications in real-world settings. 2. Related Works 2.1. Image-based Virtual Try-on Image-based virtual try-on has been extensively researched over the years, emerging as promising and formidable task. series of studies based on the Generative Adversarial Networks (GANs) [22, 27, 43, 46] have been carried out for more natural generation. However, GAN-based methods often struggle to generate outfitted images in high-fidelity and realism due to the fact that the GAN-based framework uses numerous efforts for explicit the warping process and overlooks realistic garment texture. Meanwhile, GANbased methods [12, 20, 22] lack the ability to generalize well to diverse person images, leading to significant performance degradation when applied to out-of-distribution images. As remarkable advancements have been observed Figure 2. FitDiT employs two-stage training strategy. In the first stage, Garment Priors Evolution is utilized to fine-tune GarmentDiT for enhanced clothing feature extraction. In the second stage, we customize the DiT blocks through structure slimming, garment condition modulation, and high-resolution garment feature injection, resulting in DenoisingDiT for the try-on. DenoisingDiT is trained jointly using frequency loss and denoising loss. in Text-to-Image diffusion models [19, 38, 39] in recent years, some studies [4, 24, 40] have been encouraged to integrate pre-trained diffusion models [36] as generative priors into the virtual try-on task. For instance, TryOnDiffusion [49] introduces parallel U-Nets Diffusion to enhance garment details and warp the garment for try-on. Subsequent studies have regarded virtual try-on as an exemplarbased image inpainting problem, focusing on fine-tuning inpainting diffusion models using virtual try-on datasets to produce high-quality try-on images. For example, LADIVTON [30]and DCI-VTON [14] have been proposed that treat clothing as pseudo-words or utilize warping networks to seamlessly integrate garments into pre-trained diffusion models. IDM-VTON [7] designs advanced attention modules to encode high-level garment semantics while extracting low-level features to preserve fine-grained details. Anyfit [23] employs diffusion model and introduces Hydra Block for attire combinations, producing harmonized upper and lower styles. Although these methods have garnered attention regarding the nature and realism of synthesized images, they still face challenges in addressing the hard occasions of rich texture-aware maintenance and size-aware fitting, which are prevalent in real-world virtual try-on. 2.2. Diffusion Models U-Net [37] has been the established backbone for diffusionbased image generation and is extensively employed in textto-image models such as LDM [36], SDXL [33], DALL-E [2], and Imagen [39]. U-ViT [1] utilizes ViT with long skip connections between shallow and deep layers, highlighting the significance of these long skip connections while suggesting that the downsampling and upsampling operators in CNN-based U-Net are not always essential. DiT [32] uses the Transformers to replace U-Net for class-conditioned image generation and proves the significant correlation existing between network complexity and sample quality. Stable Diffusion 3 [10] and Flux introduces transformer-based architecture that utilizes separate weights for text and image modalities, achieving an amazing text-to-image effect. In this paper, we introduce the DiT structure and analyze its advantages for virtual try-on, highlighting the limitations of U-Net-based LDMs. The downsampling and upsampling operations in U-Net are not essential for our purposes, as they tend to focus more attention on low-resolution latent features. This focus undermines the maintenance of highresolution, rich textures in garments, which is crucial for achieving realistic virtual try-on. 3. Method 3.1. Model Overview An overall framework of FitDiT is presented in Fig. 2. Given person image xp and garment image xg, FitDiT aims to generate an image xtr that visualizes the person wearing the provided garment. It is common practice to treat virtual try-on as specific case of exemplar-based image inpainting task, which involves filling the masked person image xp using the garment xg as reference. FitDiT employs parallel-branch architecture, where the GarmentDiT extracts detailed garment features from the input garment image. These features are then injected into DenoisingDiT through hybrid attention mechanism. We utilize customized Stable Diffusion 3 [10] for both GarmentDiT and DenoisingDiT, which will be described in detail in Sec. 3.2. Additionally, we incorporate Pose Guider [18], composed of 4-layer convolutional network (with 4 4 kernels, 2 2 strides, and 32, 64, 256, and 512 channels), which uses DWPose [47] as input to ensure the coherence of the generated human body in the inpainting area. 3.2. Customization of DiT for Virtual Try-on The original SD3 is text-to-image model composed of series of stacked MM-DiT blocks. We analyze the differences between the text-to-image and virtual try-on tasks and customize the model specifically for the virtual try-on. Structure slimming. The original SD3 uses OpenCLIP bigG/14 [5], CLIP-ViT/L [34] and T5-xxl [35] as text encoders to process the text prompt, which serves as control condition for image generation. However, for virtual try-on, the generated image is primarily determined by the given garment, with the text prompt having limited impact [8]. Therefore, we remove the text encoder from SD3, resulting in approximately 72% parameter savings. This modification also improves the speed of model training and inference while reducing memory usage. Garment condition modulation. In virtual try-on tasks, various types of garments (e.g., upper body, lower body, dresses) are typically trained using unified model, which may lead to confusion during training. OOTDiffusion [44] utilizes text embedding of the garment label {upperbody, lowerbody, dress} as an auxiliary conditioning input to differentiate between garment types. However, this condition is considered coarsegrained, as garments within the same category can vary significantly. To this end, we propose utilizing the image encoders of OpenCLIP bigG/14 and CLIP-ViT/L to encode the given garment into garment image embedding. This embedding is then combined with the timestep embedding to produce scale and shift parameters that modulate features in the DiT block in garment-aware manner. Garment feature injection. To extract garment features, we first input the garment into the GarmentDiT and forward it with timestep = 0, saving the key and value features {kr, vr} from the GarmentDiT attention module, which contains rich clothing texture information. Then during each denoising step, we inject the saved garment features into DenoisingDiT using the hybrid attention mechanism with the following q, k, v, = qd, = kdkr, = vdvr, (1) where denotes feature concatenation across the channel dimension, and qd, kd, vd represent the query, key, and value from DenoisingDiT, respectively. 3.3. Dilated-relaxed Mask Strategy Figure 3. Previous works tend to fill the entire inpainting area due to strict mask strategy. In contrast, FitDiT can accurately restore the shape of the garment with the dilated-relaxed mask strategy. Existing works on cross-category try-on often encounters inaccuracies in shape rendering. This issue arises because previous approaches typically construct agnostic masks strictly based on human parsing contours, as illustrated in Fig. 3. Such mask construction strategy can lead to the leakage of garment shape information during training, resulting in the models tendency to fill the entire mask area during inference. To mitigate this problem, we propose dilated-relaxed mask strategy that allows the model to automatically learn the optimal length of the target garment during training. Specifically, for upper-body try-on, we derive the smallest enclosing rectangle that can simultaneously encompass the hand keypoints and the human parsing of the upper garment. For lower-body try-on, we construct agnostic masks using foot keypoints and human parsing of the lower garment. For dresses, we consider both hand and foot keypoints, along with human parsing of the dresses. Furthermore, to enhance the models perception of garment shapes, we randomly expand the edges of the masks by few pixels to cover parts of the non-changing areas. 3.4. Garment Texture Enhancement To maintain rich texture during try-on, we propose twostage training strategy. First, we conduct garment priors evolution stage to fine-tune the GarmentDiT using rich garment data, enabling it to render with exceptional detail. This is followed by DenoisingDiT training, which incorporates frequency loss and denoising loss. Garment priors evolution The garment feature extractor plays crucial role in preserving texture details during the try-on task. In previous works [7, 44], its weights are initialized using models trained on large-scale image-text paired datasets without fine-tuning. However, due to the discrepancy between text-to-image and try-on tasks, directly applying text-to-image models as feature extractors for the tryon task may lead to suboptimal performance. To address this issue, we propose simple yet effective garment priors evolution strategy to enhance our GarmentDiT. Specifically, given garment latent z0, we fine-tune the GarmentDiT usFigure 4. Frequency domain gaps between the real and the generated images by different algorithms. ing the following denoising loss function: Lg = EϵN (0,1),tU (t)[w(t)ϵθ(zt; Ivec, t) ϵ2], (2) where zt = (1 t)z0 + tϵ is the noisy garment latent, and w(t) is weighting function at each timestep t. The model trained with Lg loss will serve as the garment feature extractor and will remain frozen during subsequent training. Frequency learning The Discrete Fourier Transform (DFT) is mathematical method that changes discrete, finite signal into its frequency components, which are shown as complex exponential waves. An image is type of twodimensional discrete signal made up of real numbers. In Fig. 4, we visualize the spectrum of real image alongside the try-on results generated by different algorithms using DFT. It is evident that significant discrepancies in the details of the generated images, such as text and patterns, compared to the real images result in noticeable gaps in their spectral representations. This observation leads us to hypothesize that minimizing the differences between the generated images and the real images in the frequency domain could enhance the fidelity of clothing detail reconstruction in the try-on results. To this end, we propose frequency-spectra distance loss in pixel space, which enables the model to focus more on the components with significant gaps in the frequency domain during optimization. To represent an image in the frequency domain, we use the 2D DFT: (u, v) = 1 (cid:88) 1 (cid:88) x=0 y=0 (x, y) ei2π( ux + vy ), (3) ei2π( ux + vy ) = cos 2π( ux + vy ) sin 2π( ux + vy ), where is the image size, (x, y) represents the pixel at coordinate (x, y) in the spatial domain, and (u, v) is the complex frequency value that corresponds to spatial frequency at coordinate (u, v) in the frequency spectrum. and are Eulers number and the imaginary unit. Different frequencies in the frequency spectrum correspond to various patterns within the image. Therefore, by minimizing Figure 5. Attention-related parameter ratios at various resolutions. the spectral error, we can enhance the overall structural and textural accuracy of the generated results. Specifically, to compute the frequency domain error, we first need to convert the noise latent zt back to pixel space. Following the forward process of Rectified Flows (RFs) [25], the model input zt at the current timestep is straight path between the data distribution z0 and standard normal distribution ϵ (0, 1), i.e., zt = (1 t)z0 + tϵ. (4) Based on Eq. 4, we can estimate ˆz0 through single denoising step, utilizing the noise ϵt predicted by the network at the current timestep t, ˆz0 = zt tϵt 1 . (5) The latent ˆz0 is converted back to pixel space by the VAE decoder, resulting in the predicted try-on image ˆxtr. We can calculate the frequency gap between ˆxtr and xp, Lf = 1 (cid:88) 1 (cid:88) u= v=0 Fˆxtrmg (u, v) Fxpmg (u, v)2, (6) where mg represents the garment segmentation mask of xp obtained from human parsing model. Ultimately, the overall training loss function of DenoisingDiT is combination of the imaged-based frequency loss and the latent-based diffusion noise prediction loss. 3.5. Advantages of using DiT for Try-on Existing try-on works are typically implemented based on Stable Diffusion v1.5 [36] or Stable Diffusion XL [33], both of which utilize U-Net backbone architecture. In the context of the virtual try-on task, garment features from the reference branch are injected into the denoising branch using hybrid attention mechanism. Consequently, the injection of high-resolution features is crucial for detail preservation. In Fig. 5, we illustrate the attention-related parameter ratios of different models at various latent resolutions, all using the same input resolution of (1024 768). SDXL assigns 92% of its parameters to the 32 24 resolution. For SD v1.5, the attention-related parameter ratios at latent resolutions Figure 6. Visual results on CVDD with complex garment texture, cross-categories, and in-the-wild try-on. Best viewed when zoomed in. higher than 64 48 only account 16%. In contrast, SD3 allocates over 99% of its parameters to the 64 48 resolution, providing more opportunities for high-resolution feature fusion. This makes DiT particularly suitable for tasks such as virtual try-on, which requires detail preservation. It is clear that SD3 allocates over 99% of its parameters to the 64 48 resolution, while SDXL assigns 92% of its parameters to the 32 24 resolution. For SD v1.5, the attentionrelated parameter at latent resolution higher than 64 48 only accounts 16%. Therefore, for tasks such as virtual tryon, which demand high fidelity in detail preservation, the DiT architecture emerges as superior choice. 4. Experiments 4.1. Datasets and Implementation Our experiments are conducted on two publicly available fashion datasets: VITON-HD [6] and DressCode [29], along with self-collected dataset called the Complex Virtual Dressing Dataset (CVDD), which consists of 516 highFigure 7. Visual results on DressCode and VTON-HD test set. Best viewed when zoomed in."
        },
        {
            "title": "DressCode",
            "content": "VITON-HD Paired Unpaired Paired Unpaired SSIM LPIPS FID KID FID KID SSIM LPIPS FID KID FID KID LaDI-VTON (2023) StableVTON (2024) IDM-VTON (2024) OOTDiffusion (2024) CatVTON (2024) 0.9025 - 0.9228 0.8975 0.9011 0.0719 - 0.0478 0.0725 0. 4.8636 - 3.8001 3.9497 3.2755 1.5580 - 1.2012 0.7198 0.6696 6.8421 - 5.6159 6.7019 5.4220 2.3345 - 1.5536 1.8630 1.5490 0.8763 0.8665 0.8806 0.8513 0.8694 0.0911 0.0835 0.0789 0.0964 0. 6.6044 6.8581 6.3381 6.5186 6.1394 1.0672 1.2553 1.3224 0.8961 0.9639 9.4095 9.5868 9.6114 9.6733 9.1434 1.6866 1.4508 1.6387 1.2061 1.2666 FitDiT (Ours) 0. 0.0431 2.6383 0.4990 4.7324 0.9011 0. 0.0661 4.7309 0.1895 8.2042 0.3421 Table 1. Quantitative results on VITON-HD and DressCode datasets. We compare the metrics under both paired (models clothing is the same as the given cloth image) and unpaired settings (models clothing differs) with other methods. resolution image pairs featuring complex garment textures and human poses. For quantitative comparison on two public datasets, we train two variants of models on the VITONHD and DressCode separately using the official splits at resolution of 1024 768. For quantitative comparison on CVDD, we first train model with the combination of VITON-HD and DressCode at resolution of 1024 768 and then fine-tune it at higher resolution of 1536 1152 with only the parameters of the attention layers being trainable. This model is also utilized for qualitative comparisons. All experiments are conducted on 8 NVIDIA H20 GPUs, utilizing total batch size of 32. We employ the AdamW optimizer [26] with learning rate of 3 105. 4.2. Evaluation Metrics In paired try-on settings with ground truth in test datasets, we assess reconstruction accuracy using LPIPS [48] and SSIM [42], as well as the generation authenticity using FID [31] and KID [3]. In unpaired settings, we report only FID and KID due to the absence of ground truth. 4.3. Qualitative Results For qualitative results, we compare our method with OOTDiffusion [44], CatVTON [8], IDM [7] and Kolors-VirtualTry-On [40]. The results are presented in Fig. 6 and Fig. 7. Fig. 7 illustrates the results from VITON-HD and DressCode datasets, while Fig. 6 showcases results from our Complex Virtual Dressing Dataset (CVDD). Due to the superior design of the DiT architecture, which incorporates high-resolution feature injection, frequency loss, and garment priors evolution, FitDiT excels in retaining complex textures and small text, as demonstrated in Fig. 6(a). This capability is crucial for real-world try-on applications. For cross-category try-on, FitDiT employs dilated-relaxed mask strategy that effectively maintains the shape of the clothing during outfit changes, as shown in Fig. 6(b). In contrast, other methods tend to fill the entire mask area, resulting in incorrect appearances. Additionally, we present virtual try-on in-the-wild, featuring complex backgrounds and human poses in Fig. 6(c). 4.4. Quantitative Results We conduct extensive experiments on VITON-HD, DressCode, and CVDD, as indicated in Tab. 1 and Tab. 2. FitDiT significantly outperforms all baselines across all datasets. Notably, on the CVDD, which features garments with complex textures and intricate patterns, FitDiT demonstrates substantial progress, highlighting the models strengths in texture preservation. As significant performance milestone, FitDiT achieves remarkable reduction in the KID error by 71.6% compared to the second-best method, OOTDiffusion, on the unpaired VITON-HD dataset. Methods Paired Unpaired SSIM LPIPS FID KID FID KID LaDI-VTON (2023) IDM-VTON (2024) OOTDiffusion (2024) CatVTON (2024) 0.8431 0.8529 0.8397 0.8457 0.1432 0.1399 0.1485 0.1494 26.4509 24.9510 26.2757 27.7435 1.024 0.7931 1.1137 1.7160 39.4821 35.8422 40.7213 38. 3.0239 1.1313 4.3277 3.4777 FitDiT (Ours) 0.8636 0.1130 20.7543 0. 33.4937 0.7434 Table 2. Quantitative results on CVDD. 4.5. Ablation Study Dilated-relaxed mask. We demonstrate the effectiveness of dilated-relaxed mask in Figs. 3, 6 and 8. In the context of cross-category try-on, IDM, which employs strict mask strategy, suffers from leakage of clothing shape, causing the model to fill the entire mask area. AnyFit, utilizing parsing-free strategy, can somewhat determine the correct length of the garment. However, its masks are constructed solely based on keypoints, which is inadequate for generating appropriate agnostic masks. Given the significant length variations among different clothing styles, such as long down jackets and athletic vests, relying exclusively on keypoints may not suffice for generating suitable agnostic masks. As result, the results of AnyFit remain slightly inferior to those of our FitDiT, which employs dilatedrelaxed mask strategy. Figure 8. Visual validation of the role of dilated-relaxed mask. Frequency loss. We demonstrate the effectiveness of frequency loss in Fig. 9 and Tab. 3. By incorporating frequency loss during training, the resulting images exhibit sharper details and more accurately preserved textures (e.g., drawstring on the sweater). Garment priors evolution. The effectiveness of garment priors evolution is illustrated in Fig. 9 and Tab. 3. By fine-tuning the garment-dedicated extractor using garmentspecific data, we achieve exceptional detail and richness. It effectively mitigates issues related to artifacts and texture loss in the generated results (e.g., font and pattern). 4.6. Computational Analysis We analyze the models computation cost in terms of inference time and GPU memory usage. All models operFigure 9. Visual validation of the role of garment priors evolution and frequency loss. Method SSIM LPIPS FID KID - w/o Frequency loss - w/o garment priors evolution Full FitDiT 0.8593 0.8578 0.8636 0.1239 0.1269 0.1130 22.6325 23.1786 20. 0.2960 0.5214 0.1602 Table 3. Ablation study results on CVDD. ate with FP16 precision, and the input resolutions are set to 1024768, with 25 denoising steps. As shown in Tab. 4, although FitDiT performs garment feature injection on highresolution latent features, we customize the DiT structure specifically for the try-on task by removing the text encoder and text feature injection. This customization significantly reduces the models computational complexity and parameter count. Consequently, FitDiT achieved an inference time of 4.57 seconds, which is 27% faster than StableVITON and 54% faster than IDM, while maintaining GPU memory usage consistent with IDM. Furthermore, by incorporating sequential CPU offload technique, FitDiT can operate with less than 6GB of memory on consumer GPU. Method StableVITON OOTDiffusion IDM CatVTON FitDiT Inference time (s) GPU memory (M) 6.23 10,978 8.51 8, 9.99 19,504 7.87 8,384 4.57 19,550 Table 4. Computational analysis of different methods. 5. Conclusion We propose FitDiT to customize the Diffusion Transformer (DiT) structure specifically designed for virtual try-on via enhancing the high-resolution texture, and also provide insight analysis of DiT structure attention. Besides, our technical cornerstone lies in garment priors evolution, frequency domain learning, and dilated-relaxed mask augmentation, which effectively support texture-aware maintenance and size-aware fitting of garments. Through extensive qualitative and quantitative evaluations, our research demonstrates superiority over state-of-the-art virtual try-on models, particularly in handling rich-textured garments and addressing cases of mismatched sizes. These findings mark significant milestone in advancing the field of virtual try-on, paving the way for more intricate applications in real-world settings."
        },
        {
            "title": "References",
            "content": "[1] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2266922679, 2023. 3 [2] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. 3 [3] Mikołaj Binkowski, Danica Sutherland, Michael Arbel, and Arthur Gretton. Demystifying mmd gans. arXiv preprint arXiv:1801.01401, 2018. 7 [4] Mengting Chen, Xi Chen, Zhonghua Zhai, Chen Ju, Xuewen Hong, Jinsong Lan, and Shuai Xiao. Wear-any-way: Manipulable virtual try-on via sparse correspondence alignment. arXiv preprint arXiv:2403.12965, 2024. 3 [5] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scalIn Proing laws for contrastive language-image learning. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 28182829, 2023. 4 [6] Seunghwan Choi, Sunghyun Park, Minsoo Lee, and Jaegul try-on via Viton-hd: High-resolution virtual Choo. In Proceedings of the misalignment-aware normalization. IEEE/CVF conference on computer vision and pattern recognition, pages 1413114140, 2021. [7] Yisol Choi, Sangkyung Kwak, Kyungmin Lee, Hyungwon Choi, and Jinwoo Shin. Improving diffusion models for virtual try-on. ECCV, 2024. 2, 3, 4, 7 [8] Zheng Chong, Xiao Dong, Haoxiang Li, Shiyue Zhang, Wenqing Zhang, Xujie Zhang, Hanqing Zhao, and Xiaodan Liang. Catvton: Concatenation is all you need for virtual tryon with diffusion models. arXiv preprint arXiv:2407.15886, 2024. 4, 7 [9] Haoye Dong, Xiaodan Liang, Xiaohui Shen, Bochao Wang, Hanjiang Lai, Jia Zhu, Zhiting Hu, and Jian Yin. Towards multi-pose guided virtual try-on network. In Proceedings of the IEEE/CVF international conference on computer vision, pages 90269035, 2019. 2 [10] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 3, 4 [11] Chongjian Ge, Yibing Song, Yuying Ge, Han Yang, Wei Liu, and Ping Luo. Disentangled cycle consistency for highlyrealistic virtual try-on. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1692816937, 2021. 2 [12] Yuying Ge, Yibing Song, Ruimao Zhang, Chongjian Ge, Wei Liu, and Ping Luo. Parser-free virtual try-on via distilling In Proceedings of the IEEE/CVF conappearance flows. ference on computer vision and pattern recognition, pages 84858493, 2021. 2 [13] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139144, 2020. 2 [14] Junhong Gou, Siyu Sun, Jianfu Zhang, Jianlou Si, Chen Qian, and Liqing Zhang. Taming the power of diffusion models for high-quality virtual try-on with appearance flow. In Proceedings of the 31st ACM International Conference on Multimedia, pages 75997607, 2023. 3 [15] Xintong Han, Zuxuan Wu, Zhe Wu, Ruichi Yu, and Larry Davis. Viton: An image-based virtual try-on network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 75437552, 2018. 2 [16] Xintong Han, Xiaojun Hu, Weilin Huang, and Matthew Scott. Clothflow: flow-based model for clothed person generation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1047110480, 2019. [17] Sen He, Yi-Zhe Song, and Tao Xiang. Style-based global In Proceedings of the appearance flow for virtual try-on. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 34703479, 2022. [18] Li Hu. Animate anyone: Consistent and controllable imageto-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81538163, 2024. 4 [19] Xiaobin Hu, Xu Peng, Donghao Luo, Xiaozhong Ji, Jinlong Peng, Zhengkai Jiang, Jiangning Zhang, Taisong Jin, Chengjie Wang, and Rongrong Ji. Diffumatting: Synthesizing arbitrary objects with matting-level annotation. arXiv preprint arXiv:2403.06168, 2024. 3 [20] Thibaut Issenhuth, Jeremie Mary, and Clement Calauzenes. Do not mask what you do not need to mask: parser-free virtual try-on. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XX 16, pages 619635. Springer, 2020. 2 [21] Jeongho Kim, Guojung Gu, Minho Park, Sunghyun Park, and Jaegul Choo. Stableviton: Learning semantic correspondence with latent diffusion model for virtual try-on. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81768185, 2024. 2 [22] Sangyun Lee, Gyojung Gu, Sunghyun Park, Seunghwan Choi, and Jaegul Choo. High-resolution virtual try-on with In Eumisalignment and occlusion-handled conditions. ropean Conference on Computer Vision, pages 204219. Springer, 2022. 2 [23] Yuhan Li, Hao Zhou, Wenxiang Shang, Ran Lin, Xuanhong Chen, and Bingbing Ni. Anyfit: Controllable virtual tryon for any combination of attire across any scenario. arXiv preprint arXiv:2405.18172, 2024. 3 [24] Yujie Liang, Xiaobin Hu, Boyuan Jiang, Donghao Luo, Kai Wu, Wenhui Han, Taisong Jin, and Chengjie Wang. Vtonhandfit: Virtual try-on for arbitrary hand pose guided by hand priors embedding. arXiv preprint arXiv:2408.12340, 2024. [25] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 5 [26] Ilya Loshchilov, Frank Hutter, et al. Fixing weight decay regularization in adam. arXiv preprint arXiv:1711.05101, 5, 2017. 7 [27] Yifang Men, Yiming Mao, Yuning Jiang, Wei-Ying Ma, and Zhouhui Lian. Controllable person image synthesis with attribute-decomposed gan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 50845093, 2020. 2 [28] Matiur Rahman Minar, Thai Thanh Tuan, Heejune Ahn, Paul Rosin, and Yu-Kun Lai. Cp-vton+: Clothing shape and texture preserving image-based virtual try-on. In CVPR workshops, pages 1014, 2020. 2 [29] Davide Morelli, Matteo Fincato, Marcella Cornia, Federico Landi, Fabio Cesari, and Rita Cucchiara. Dress code: Highresolution multi-category virtual try-on. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 22312235, 2022. 6 [30] Davide Morelli, Alberto Baldrati, Giuseppe Cartella, Marcella Cornia, Marco Bertini, and Rita Cucchiara. Ladi-vton: Latent diffusion textual-inversion enhanced virtual try-on. In Proceedings of the 31st ACM International Conference on Multimedia, pages 85808589, 2023. [31] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On aliased resizing and surprising subtleties in gan evaluation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1141011420, 2022. 7 [32] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. 3 [33] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 3, 5 [34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 4 [35] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. 4 [36] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2, 3, [37] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241. Springer, 2015. 3 [38] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In Proceedings of the IEEE/CVF conference generation. on computer vision and pattern recognition, pages 22500 22510, 2023. 3 [39] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 3 [40] Kolors Team. Kolors: Effective training of diffusion model for photorealistic text-to-image synthesis. arXiv preprint, 2024. 3, 7 [41] Bochao Wang, Huabin Zheng, Xiaodan Liang, Yimin Chen, Liang Lin, and Meng Yang. Toward characteristicpreserving image-based virtual try-on network. In Proceedings of the European conference on computer vision (ECCV), pages 589604, 2018. [42] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. 7 [43] Zhenyu Xie, Zaiyu Huang, Xin Dong, Fuwei Zhao, Haoye Dong, Xijin Zhang, Feida Zhu, and Xiaodan Liang. Gpvton: Towards general purpose virtual try-on via collaboraIn Proceedings of tive local-flow global-parsing learning. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2355023559, 2023. 2 [44] Yuhao Xu, Tao Gu, Weifeng Chen, and Chengcai Chen. Ootdiffusion: Outfitting fusion based latent diffusion for controllable virtual try-on. arXiv preprint arXiv:2403.01779, 2024. 2, 4, 7 [45] Han Yang, Ruimao Zhang, Xiaobao Guo, Wei Liu, Wangmeng Zuo, and Ping Luo. Towards photo-realistic virtual try-on by adaptively generating-preserving image content. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 78507859, 2020. 2 [46] Zhijing Yang, Junyang Chen, Yukai Shi, Hao Li, Tianshui Chen, and Liang Lin. Occlumix: Towards de-occlusion virIEEE Transactual try-on by semantically-guided mixup. tions on Multimedia, 25:14771488, 2023. 2 [47] Zhendong Yang, Ailing Zeng, Chun Yuan, and Yu Li. Effective whole-body pose estimation with two-stages distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 42104220, 2023. 4 [48] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. [49] Luyang Zhu, Dawei Yang, Tyler Zhu, Fitsum Reda, William Chan, Chitwan Saharia, Mohammad Norouzi, and Ira Kemelmacher-Shlizerman. Tryondiffusion: tale of two unets. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 46064615, 2023. 2,"
        }
    ],
    "affiliations": [
        "Fudan University",
        "Tencent"
    ]
}