{
    "paper_title": "MCIF: Multimodal Crosslingual Instruction-Following Benchmark from Scientific Talks",
    "authors": [
        "Sara Papi",
        "Maike Züfle",
        "Marco Gaido",
        "Beatrice Savoldi",
        "Danni Liu",
        "Ioannis Douros",
        "Luisa Bentivogli",
        "Jan Niehues"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in large language models have catalyzed the development of multimodal LLMs (MLLMs) that integrate text, speech, and vision within unified frameworks. As MLLMs evolve from narrow, monolingual, task-specific systems to general-purpose instruction-following models, a key frontier lies in evaluating their multilingual and multimodal capabilities over both long and short contexts. However, existing benchmarks fall short in evaluating these dimensions jointly: they are often limited to English, mostly focus on one single modality at a time, rely on short-form contexts, or lack human annotations -- hindering comprehensive assessment of model performance across languages, modalities, and task complexity. To address these gaps, we introduce MCIF (Multimodal Crosslingual Instruction Following), the first multilingual human-annotated benchmark based on scientific talks that is designed to evaluate instruction-following in crosslingual, multimodal settings over both short- and long-form inputs. MCIF spans three core modalities -- speech, vision, and text -- and four diverse languages (English, German, Italian, and Chinese), enabling a comprehensive evaluation of MLLMs' abilities to interpret instructions across languages and combine them with multimodal contextual information. MCIF is released under a CC-BY 4.0 license to encourage open research and progress in MLLMs development."
        },
        {
            "title": "Start",
            "content": "MCIF: MULTIMODAL CROSSLINGUAL INSTRUCTIONFOLLOWING BENCHMARK FROM SCIENTIFIC TALKS Sara Papi1, Maike ufle2, Marco Gaido1, Beatrice Savoldi1, Danni Liu2, Ioannis Douros3, Luisa Bentivogli1, Jan Niehues2 1Fondazione Bruno Kessler (Italy), 2Karlsruhe Institute of Technology (Germany), 3Translated (Italy) {spapi,mgaido,bsavoldi,bentivo}@fbk.eu, {maike.zuefle, danni.liu,jan.niehues}@kit.edu, ioannis@translated.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Recent advances in large language models have catalyzed the development of multimodal LLMs (MLLMs) that integrate text, speech, and vision within unified frameworks. As MLLMs evolve from narrow, monolingual, task-specific systems to general-purpose instruction-following models, key frontier lies in evaluating their multilingual and multimodal capabilities over both long and short contexts. However, existing benchmarks fall short in evaluating these dimensions jointly: they are often limited to English, mostly focus on one single modality at the time, rely on short-form contexts, or lack human annotationshindering comprehensive assessment of model performance across languages, modalities, and task complexity. To address these gaps, we introduce MCIF (Multimodal Crosslingual Instruction Following), the first multilingual human-annotated benchmark based on scientific talks that is designed to evaluate instruction-following in crosslingual, multimodal settings over both shortand long-form inputs. MCIF spans three core modalitiesspeech, vision, and textand four diverse languages (English, German, Italian, and Chinese), enabling comprehensive evaluation of MLLMs abilities to interpret instructions across languages and combine them with multimodal contextual information. MCIF is released under CC-BY 4.0 license to encourage open research and progress in MLLMs development. 5 2 0 2 5 2 ] . [ 1 4 3 6 9 1 . 7 0 5 2 : r"
        },
        {
            "title": "Preprint",
            "content": ""
        },
        {
            "title": "INTRODUCTION",
            "content": "In recent years, large language models (LLMs) have achieved remarkable progress across wide range of tasks (Brown et al., 2020; Grattafiori et al., 2024), leading to growing interest on extending their capabilities beyond text to embrace multiple modalities such as speech (Rubenstein et al., 2023; Chu et al., 2023) and vision (Alayrac et al., 2022; Achiam et al., 2023; Huang et al., 2023). This shift has given rise to multimodal LLMs (MLLMs), which aim to unify language, audio, and visual understanding within single framework (Liang et al., 2024). Initially tailored for specific tasks (Li et al., 2023b; Tang et al., 2023), MLLMs are now evolving towards more flexible and generalized usage, where they are expected to follow natural language instructions and perform diverse, complex tasks (Hendrycks et al., 2020). This paradigm, widely known as instruction following, requires models to interpret user instruction with the provided context and generate an appropriate response across one or more modalities (Ouyang et al., 2022; Su et al., 2023; Caffagni et al., 2024). parallel and equally important challenge lies in extending these capabilities to multilingual and crosslingual settings (Hu et al., 2020). General-purpose MLLMs must not only handle inputs and outputs in the same language by supporting the highest possible number of diverse languages, but also process cross-lingual multimodal inputs, such as speech in language paired with an instruction in another language (Zeng et al., 2025). Despite rapid advances in both instruction-following and multilingual modeling (Wei et al., 2021; Sanh et al., 2021; Barrault et al., 2023), current benchmarks fall short of comprehensively analyzing these aspects. Recent work either focuses exclusively on two modalities, such as vision-text (Fu et al., 2023; Zhang et al., 2023; Qian et al., 2024; Das et al., 2024) or speech-text (Yan et al., 2025; Pandey et al., 2025), or restricts their scope to English (Li et al., 2023a; Chen et al., 2024a), thereby overlooking the complexities of multilingual and crosslingual interactions (Duan et al., 2021; Wang et al., 2022; Gao et al., 2023; Pernes et al., 2024). Adding to these limitations, current multimodal benchmarks predominantly focus on short-form inputs, neglecting the evaluation of models capabilities with long dependencies (Fu et al., 2024), and only few are human-annotated (Li et al., 2023a), raising concerns about data quality, potential biases, and the overall reliability of model evaluations (Zhang et al., 2024a). To fill this gap, we introduce MCIF, the first manually-curated benchmark explicitly designed to evaluate crosslingual, multimodal instruction-following abilities over both shortand long-form inputs. MCIF covers three core modalitiesspeech, video, and textand spans four typologically diverse languages: English, German, Italian, and Chinese. MCIF supports wide range of tasks, enabling systematic evaluation of MLLMs abilities to follow instructions across different languages and combinations of modalities. The benchmark is released under permissive CC-BY 4.0 license to facilitate broad adoption and further research in this area."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Speech-Text Benchmarks. Recent efforts to benchmark instruction-following capabilities in speech-based models have led to the development of several speech-text evaluation datasets. Most of them, such as Speech-ifeval (Lu et al., 2025), SAKURA (Yang et al., 2025b), and MMSU (Wang et al., 2025b), restrict their scope to instruction-following monolingual tasks, predominantly covering the English language. AIR-Bench (Yang et al., 2024), VoiceBench (Chen et al., 2024a), ADUBench (Gao et al., 2024), URO (Yan et al., 2025), and SpeechInstructBench (Wang et al., 2025c) are more dialogue-oriented and introduce open-ended tasks that are limited to English and Chinese, with the latter three benchmarks relying entirely on synthetic TTS speech. SD-Eval (Ao et al., 2024), Dynamic-SUPERB (Huang et al., 2025), AudioBench (Wang et al., 2025a) , MSTEB (Beyene et al., 2025), and SIFT-50M (Pandey et al., 2025) offer multilingual speech-text evaluation but rely on preexisting benchmarks, such as CommonVoice (Ardila et al., 2020), and FLEURS (Conneau et al., 2023), potentially suffering from data contamination (Sainz et al., 2023; Balloccu et al., 2024; Kocyigit et al., 2025), reducing their utility for current models evaluation. Overall, while interest in speech-language evaluation is growing, existing benchmarks do not support multimodal, crosslingual, and long-form instruction-following in unified setting as MCIF. Vision-Text Benchmarks. Similarly to the speech-text domain, the vision-text domain has seen huge increase in the number of benchmarks designed to assess MLLMs across diverse capabilities. MMMU (Yue et al., 2024) and MIA-Bench (Qian et al., 2024) evaluate MLLMs with image-textual"
        },
        {
            "title": "Preprint",
            "content": "task name acronym in mod out mod src lang tgt lang context type cross TQA Textual Question Answering TSUM Text Summarization MT Machine Translation ASR Automatic Speech Recognition SQA Spoken Question Answering SSUM Speech Summarization ST Speech Translation VQA Video Question Answering VSUM Video Summarization Audio-Video Recognition AVR Audio-Video Question Answering AVQA AVSUM Audio-Video Summarization AVT Audio-Video Translation - - - SHORT LONG SHORT LONG LONG SHORT LONG SHORT LONG LONG SHORT LONG SHORT LONG LONG SHORT LONG Table 1: Tasks covered by MCIF with their input and output modalities (in mod and out mod), input type supported (context type) that can be long-form ( LONG ) or short-form ( SHORT ), source and target languages (src lang and tgt lang) covered among English , German , Italian , and Chinese . cross indicates whether the task can be crosslingual, i.e., if it involves target language different from the source language. inputs across several domains, but cover English only. MME (Fu et al., 2023) extends the evaluation to English and Chinese translation, and M3Exam (Zhang et al., 2023) to 9 diverse languages, while EXAMS-V (Das et al., 2024) further widens the coverage to 7 language families. Despite their complexity, these vision-text benchmarks are all constrained to benchmark the models abilities when dealing with single images rather than videossequences of images. Video-based benchmarks such as Video-Bench (Ning et al., 2023), InfiniBench (Ataallah et al., 2024), VITATECS (Li et al., 2024b), TempCompass (Liu et al., 2024), LVBench (Wang et al., 2024b), MVBench (Li et al., 2024a), and MMBench-Video (Fang et al., 2024) focus on bimodal interactions (video and text), cover English only, and rarely incorporate human-authored multilingual instructions. VideoMME (Fu et al., 2024) and MF2 (Zaranis et al., 2025) represent the first benchmarks comprising the three modalities, but restrict their scope to video understanding only, rather than broader crosslingual instruction-following tasks. As result, there is currently no benchmark that enables systematic evaluation across speech, vision, and text modalities in crosslingual instruction-following framework."
        },
        {
            "title": "BENCHMARK",
            "content": "We create MCIF from English video of scientific presentations, collecting the related audio, transcripts and translations of their content, summaries (abstracts), and set of manually created questions and open-ended answers. It results in highly multitask, natural, and expert-vetted benchmark characterized by: 3 modalities: text, speech, and video; 4 languages: English, German, Italian, and Chinese; 2 context types: short-form and long-form text, speech, and video contents; 13 tasks: crosslingual and multimodal tasks divided into 4 macro-areas (recognition, translation, question answering, and summarization); manual curation and annotations: professionally created transcripts, translations, summaries, and question-answer pairs."
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Breakdown of MCIF statistics. The total length is obtained by counting the number of space-separated words for English, German, and Italian, and the number of characters for Chinese. Questions and answers statistics in the inner circle refer to the type of question, while those in the outer circle refer to the input modality required to answer the question (see Section 3.2). MCIF spans 13 different tasks, clustered by the three input modalities1 and their interplay, as shown in Table 1. The detailed description of each task is provided in Appendix A. The data selection is described in Section 3.1, the human-annotation process adopted for building the benchmark in Section 3.2, and the instruction-following prompt composition in Section 3.3."
        },
        {
            "title": "3.1 DATA SELECTION AND COLLECTION",
            "content": "Data Selection. To develop multimodal, cross-lingual, multi-task benchmark for scientific communication, we collect scientific talks from the ACL Anthology2, the main reference repository for research in the Natural Language Processing (NLP) and language technologies community. This source is particularly well-suited to our objectives. First, it is openly available under CC-BY 4.0 License, permitting unrestricted use and redistribution. Second, it offers naturally multimodal and challenging material: video presentations self-recorded by speakers from various linguistic backgrounds and accents, accompanied by slides, spoken audio, and corresponding research papers. To avoid data contamination issues of testing models on material that has been used for training (Balloccu et al., 2024; Kocyigit et al., 2025), we selected the most recent available material at the time of collection,3 namely, the ACL 2023 paper presentations.4 Data Collection. The presentations were randomly selected across the Long (912) and Short (165) ACL 2023 main conference papers,5 covering different topics in the context of NLP. The resulting benchmark includes 21 presentations, with total of 2 hours of video content and approximately 15.5k words. The video, in mp4 format, is converted with ffmpeg6 into wav audio, mono-channel, 16 kHz. To support both the exploration of how models handle long versus short context and to maximize usability for models with limited context capacity, we provide both the full-length video/speech context and an automatically segmented version generated with SHAS (Tsiamas et al., 2022), with segments averaging 16 seconds in duration. Together with the video, we collect the abstracts, which serve as summaries for the presentations. To improve test set representativeness for summarization tasks, we further collect 79 additional videos, yielding total of 100 samples about 10 hours of content with summaries totaling 17k words. For these additional samples, audio and textual transcripts are also available, ensuring alignment across all three modalities. detailed breakdown of MCIF statistics is provided in Fig. 1. 1As we are dealing with instruction-following models, all tasks involve at least the text modality for the presence of the prompt. Therefore, we include text as modality if the task involves text only. 2https://aclanthology.org/ 3As of April 18th, 2025, the most recent conference having the video presentation available is the one chosen for the benchmark. 4https://aclanthology.org/events/acl-2023/ 5The resulting collection was manually inspected and validated to discard presentations with i) repeated speakers (i.e., each sample represents unique speaker), ii) inaudible or low-quality speech (e.g., presentations with excessive background noise or featuring speaker distant from the microphone), and iii) automatically generated speech (e.g., text-to-speech synthesis is used to produce the audio). 6https://ffmpeg.org/"
        },
        {
            "title": "3.2 DATASET CREATION AND MANUAL ANNOTATIONS",
            "content": "We detail the dataset curation process, including the creation and design of human gold data across tasks and languages. Key steps are outlined below, with additional information on professional costs, annotation procedures, and task guidelines available in Appendix B. Recognition and Summarization. For each talk, we tasked professional linguists with producing high-quality gold transcripts in US English, following detailed guidelines.7. This enabled the creation of aligned English video, speech, and transcript data. For summarization, instead, we used the abstract from the associated paper as the textual summary, as prior work shows abstracts provide reasonably accurate representations of the scientific talks (Zufle et al., 2025). Question-Answering. To evaluate model understanding, we design QA task intended to probe different aspects of contextual dependency and task realism. First, each talk was paired with at least 10 QA pairs, which followed structured distribution: i) General common questions, which are generic and applicable to any talk (e.g., What are the affiliations of the authors?); ii) Transcript questions, created after watching the full talk and targeting narrow, context-dependent information retrieval; and iii) Abstract questions, generated after reading only the abstract, simulating scenario where user queries about talk without having watched it in its full length. All QA pairs were created and verified by 16 expert annotators8 with background in machine learning and NLP. Each QA pair was also distinguished according to the input modality required to answer the question, explicitly including cases where no answer is available. Labels were assigned as follows: NA if the information was not present in either the video or audio, AV if the information was available in both the audio and video modalities, with either modality alone being sufficient to answer the question, if the answer could be obtained from the audio only, and if it was available in the video only. breakdown of the QA distribution among categories is illustrated in Fig. 1. Overall, this setup enables systematic evaluation of model performance across modality conditions and unanswerable cases, as detailed in Appendix B.2. All QA pairs are created in English and, as described in the following, then translated into three additional languages. Translation and Multilinguality. To enable the creation of cross-lingual benchmark for speech and video-to-text translation, as well as cross-lingual question answering and summarization, all English textual datatranscripts, summaries, and QA pairswere translated into three additional languages: Italian, German, and Chinese.9 These languages were selected as they are well-resourced, allow for comparable analyses, and represent diversity of language (sub)families and writing systems.10 All translations were carried out by professional translators with expertise in scientific content. As translators reviewed the original QA pairs, summaries, and transcripts during this process, translation also served as secondary verification step, further ensuring the quality and consistency of the source material. 3.3 INSTRUCTION-FOLLOWING PROMPTS For each sample in the benchmark, there is no information about the specific task to be performed (e.g., ASR), the input modality (e.g., audio, video, or text), or the target language pair to support (e.g., English to German); rather, the model has to correctly interpret and fulfill diverse instructions across the supported language pairs (e.g., Traduci il contenuto inglese in italiano[it], Translate the English content into Italian[en]). Following previous work, we always specify the source language in the prompt, which is written in the target language (Xu et al., 2024; Lu et al., 2024a). We create two subsets of the MCIF benchmark, MCIFfix and MCIFmix, based on the set of prompts. MCIFfix include fixed prompt for each of the task macro-areas (recognition, translation, question answering, and summarization), which maps into the 13 tasks. The full set is reported in Appendix C. Instead, the MCIFmix comprises prompts sampled from list of 10 different prompts for each of the 13 tasks, including the original prompts of MCIFfix. The latter subset is intended to measure the generalization capabilities of the models, as they have to cope with different instructions but 7Linguists are hired via professional agency; details on the process and costs in Appendix B.2. 8They are all students and authors of this paper, with high English proficiency. 9Specifically, Mandarin Chinese, and language variants as spoken in Germany and Italy. 10The selection of these three languages was partly guided by budget constraints and the availability of native-speaking co-authors to supervise the work."
        },
        {
            "title": "Preprint",
            "content": "with the same meaning; hence, MCIFmix is more difficult than MCIFfix. In line with the rest of the benchmark, the 10 different prompts, for each of the 4 languages and task macrotypes, have been manually devised and reported in Appendix C."
        },
        {
            "title": "4.1 MODELS",
            "content": "We evaluate range of models across modalities: LLMs on text-only tasks, SpeechLLMs on speech tasks, VideoLLMs on video tasks, and MLLMs on all tasks (text, speech, and video). To ensure compatibility within unified evaluation framework, we select publicly available open-weight models hosted on HuggingFace that can be run using the HuggingFace Transformers library. Due to computational constraints, we restrict our selection to models with fewer than 20 billion parameters. The list is presented below, and the reference to the HuggingFace model weights and full generation settings are detailed in Appendix D. LLMs: Aya Expanse (Dang et al., 2024), Gemma 3 (Team Gemma et al., 2025), Llama 3.1 (Grattafiori et al., 2024), Phi4 (Abdin et al., 2024a), Qwen3 (Yang et al., 2025a), Tower+ (Rei et al., 2025). SpeechLLMs: DeSTA2 (Lu et al., 2024b), Granite-Speech 3.3 (Saon et al., 2025), Phi4-Multimodal (Abdin et al., 2024b), Qwen2-Audio (Chu et al., 2024), UltraVox v0.511. VideoLLMs: InternVL3 (Chen et al., 2024b), LLaVA-Next (Zhang et al., 2024b), Qwen2.5-VL (Wang et al., 2024a), VideoLLaMA 3 (Zhang et al., 2025), Video-XL 2 (Shu et al., 2024). MLLMs: Gemma 3n12, Ming-Lite-Omni (AI, 2025), MiniCPM-o 2.6 (Yao et al., 2024), Ola (Liu et al., 2025), Qwen2.5-Omni (Xu et al., 2025)."
        },
        {
            "title": "4.2 METRICS",
            "content": "The evaluation is carried out by computing separate scores for each of the tasks involved by using commonly adopted metrics in the community. Namely, for recognition tasks (ASR, AVR), we compute WER using the jiWER library13 after normalizing the test using the Whisper normalizer (Radford et al., 2023), version 0.0.10. For translation tasks (MT, ST, AVT), we use COMET (Rei et al., 2020), with the standard model Unbabel/wmt22-comet-da, after concatenating all (speech or video) segments belonging to the same talk in the case of the short context and resegmenting the text with mwerSegmenter (Matusov et al., 2005) to pair them with the reference sentences. Lastly, for question answering (TQA, SQA, VQA, AVQA) and summarization (TSUM, SSUM, VSUM, AVSUM), we compute BERTScore (Zhang et al., 2020) rescaling the scores with baselines to obtain more interpretable scores in wider range (typically, in the [0, 1] range).14 The reported scores are all multiplied by 100 to enhance readability."
        },
        {
            "title": "5 RESULTS",
            "content": "WARNING! This section contains intermediate and incomplete results In this section, we report the results on MCIF from several perspectives. First, in Section 5.1, we analyze the overall performance achieved by all the models presented in Section 4.1 on both MCIFfix and MCIFmix. Subsequently, in Section 5.2, we focus on MLLMs and investigate the impact of modalities integration on the models abilities to perform each task. Lastly, in Section 5.3, we isolate the question answering task and break down the results of the best-performing model for each category (LLM, SpeechLLM, VideoLLM, and MLLM), considering the type of question and the input modality required to answer the question. 11https://www.ultravox.ai/ 12https://deepmind.google/models/gemma/gemma-3n/ 13https://github.com/jitsi/jiwer 14See https://github.com/Tiiiger/bert_score/blob/master/journal/rescale_ baseline.md"
        },
        {
            "title": "Preprint",
            "content": "t n R Modekonl Input Modality DeSTA2 GraniteSpeech Phi4-Multimodal Qwen2-Audio UltraVox v0.5 InternVL3 LLaVA-NeXT Qwen2.5-VL VideoLLaMA3 Video-XL Gemma 3n Ming-Lite-Omni MiniCPM-o-2 Ola Qwen2.5-Omni Aya Expanse Gemma 3 Llama 3.1 Phi4 Qwen3 Tower+ DeSTA2 GraniteSpeech Phi4-Multimodal Qwen2-Audio UltraVox v0.5 L InternVL3 LLaVA-NeXT Qwen2.5-VL VideoLLaMA3 Video-XL2 MiniCPM-o-2 Ola Qwen2.5-Omni SUMMARIZATION BERTSCORE REC. TRANSLATION QUESTION ANSWER. WER BERTSCORE COMET 72.7 76.3 77.3 45.1 42.0 52.2 62.1 9.4 77.7 81.2 81.6 6.8 31.3 71.5 73.6 79.1 146.2 46.1 52.5 34.4 22.9 20.9 19.1 0.7 -11.3 0.4 1.3 11.9 42.4 36.8 37.4 34.4 34.5 33.5 32.5 36.5 16.2 19.7 17.1 20.8 5.7 27.3 26.8 27.2 38.0 11.2 19.2 8.8 26.3 30.3 27.3 39.0 27.1 33.3 28.3 25.9 9.1 20.0 15.4 13.3 7.3 8. 9.4 -9.6 11.0 3.7 13.4 33.1 12.3 21.1 10.9 17.0 35.9 38.9 37.0 37.9 34.7 31.6 32.8 31.2 17.4 17.9 16.8 22.5 8.5 27.6 25.3 23.6 29.2 27.8 28.4 26.9 30.5 31.3 31.2 20.4 30.1 27.7 24.6 27.4 18.7 17.9 19.2 16.6 93.7 36.7 38.5 29.9 117.5 55.8 55.9 47.4 100.7 35.0 41.3 42.9 72.3 76.8 80.4 54.5 74.2 76.4 81.2 43. 63.3 68.8 73.9 82.3 87.9 86.3 79.2 83.2 77.5 82.9 85.7 84.8 82.9 86.2 85.5 83.6 87.3 85.9 39.4 44.3 39.4 35.4 40.3 32.3 56.3 66.4 56.5 39.7 43.1 41.3 36.7 40.5 35.5 93.0 99.9 39.2 93.1 88.1 7.3 -3.0 15.5 15.8 14.4 -11.6 -22.6 -26.3 -25.9 -16.0 -7.0 -12.2 -12.4 -17.1 7.1 37.8 34.1 33.3 38.5 18.8 12.4 14.0 9.5 27.8 28.1 27.3 30.6 3.7 -2.0 -13.2 10.5 19.9 12.2 4.3 -5.1 3.0 6.2 4. 5.9 7.1 -6.0 20.8 21.3 23.5 34.8 15.4 5.8 -6.9 4.6 38.4 31.8 32.0 37.7 19.2 26.1 31.4 27.7 29.3 -46.6 9.6 24.4 19.8 17.9 15.1 16.4 9.7 -6.4 8.0 3.8 4. 9.2 8.0 7.4 12.0 21.7 -5.4 -13.1 17.8 23.9 -16.1 -32.0 -9.2 3.6 6.1 8.2 8.1 -9.2 -2.3 -8.4 98.8 9.9 98.5 37.3 40.7 39.5 64.0 63.0 54.9 37.2 42.3 40. 13.9 21.4 11.4 18.3 -15.0 36.2 39.3 35.6 33.6 17.6 46.6 38.3 35.9 44.5 17.3 Table 2: Results on MCIFfix of the 21 models for each input modality, context, task (divided into the 4 macro-areas: RECOGNITION, TRANSLATION, QUESTION-ANSWERING, and SUMMARIZATION), and target language ( for Chinese, while source language, being always English, is omitted). The best result by context is marked in bold, and the best overall result is underlined. Gemma 3n and Ming-Lite-Omni are removed from LONG scores as they are not able to process long-form speech. for Italian, and for German, for English,"
        },
        {
            "title": "5.1 MCIFFIX AND MCIFMIX",
            "content": "Table 2 and Table 3 report the model results on MCIFfix and MCIFmix, respectively, for each model, context (long or short), and task divided by macro-area. We also report the non-normalized BERTScore results for question answering and summarization tasks in Appendix E. RECOGNITION. In recognition, SpeechLLMs show strong performance on short-form audio, confirming their specialization in speech transcription tasks. However, performance degrades significantly in the long-form scenario, where the WER increases substantially across all models. Notably, Ola, an MLLM, achieves the best long-form performance by wide margin, outperforming SpeechLLMs and even Phi-4 Multimodal, which still reports WERs above 20. This suggests that Olas architectureparticularly its strategy of chunking and concatenating long speech segments using Whisper-based encoderenables it to maintain transcription quality over extended inputs. Despite this, long-form recognition remains major challenge for most models."
        },
        {
            "title": "Preprint",
            "content": "t n R Model Input Modality DeSTA2 GraniteSpeech Phi4-Multimodal Qwen2-Audio UltraVox v0.5 InternVL3 LLaVA-NeXT Qwen2.5-VL VideoLLaMA3 Video-XL Gemma 3n Ming-Lite-Omni MiniCPM-o-2 Ola Qwen2.5-Omni Aya Expanse Gemma 3 Llama 3.1 Phi4 Qwen3 Tower+ SUMMARIZATION BERTSCORE REC. TRANSLATION QUESTION ANSW. WER BERTSCORE COMET 72.1 76.4 76.2 72.8 42.9 49.4 47.5 9.5 77.7 81.3 81.3 6.7 32.9 70.9 74.2 78.4 192.7 44.5 43.4 44.9 24.6 21.8 21.5 -0.5 -11.8 0.7 8.4 10.8 41.4 36.4 36.3 28.4 34.8 34.2 31.9 35.5 17.2 21.4 17.2 17.2 94.1 37.0 38.7 38.7 128.2 57.3 55.1 47.5 212.5 34.8 40.4 42.3 72.4 76.7 79.6 49.0 74.0 74.4 81.0 48.0 8.2 7.7 28.5 28.1 30.2 39.2 14.6 7.8 24.7 28.9 30.2 37.2 27.9 33.4 27.1 27.8 5.6 18.6 18.8 14.4 8.4 6. 11.7 8.9 9.4 2.6 7.9 28.5 14.1 19.2 10.7 22.6 39.0 38.9 37.7 36.6 34.8 32.7 33.2 31.4 62.3 69.2 74.1 81.6 86.0 82.7 80.3 83.6 79.2 82.0 86.9 85.1 82.4 86.1 85.1 81.5 83.5 86.0 14.5 14.2 14.5 22.4 6.0 27.4 25.8 23.0 29.8 27.9 28.2 28.0 30.1 31.0 30.7 19.2 25.1 23.0 22.9 23.3 2.6 15.5 14.9 18.1 DeSTA2 GraniteSpeech Phi4-Multimodal Qwen2-Audio UltraVox v0. O 132.7 39.9 43.9 39.1 35.1 39.4 29.4 80.4 29.8 60.7 65.4 52.4 39.7 43.4 41.3 93.0 36.8 40.6 35.5 91.6 4.5 -5.1 15.9 14.9 15.2 -10.9 -22.5 -23.8 -26.1 -17.2 -6.8 -12.2 -11.8 -17.1 9.0 37.1 33.6 33.3 37.4 17.6 12.6 13.5 9.1 27.2 28.3 28.1 29.5 1.6 -3.3 -12.0 6.8 20.6 5.5 -4. 2.0 7.2 4.9 9.2 7.3 7.0 InternVL3 LLaVA-NeXT Qwen2.5-VL VideoLLaMA3 Video-XL MiniCPM-o-2 Ola Qwen2.5-Omni 4.0 -3.3 19.8 24.9 23.1 32.1 13.5 11.4 13.3 21.6 -2.3 -7.1 35.5 31.8 34.2 34.4 17.2 12.1 15.2 14.0 28.7 31.9 24.5 27.0 -21.1 -29.6 -26.9 -72.2 2.1 25.7 24.0 19.6 14. 15.2 -9.6 -7.6 -5.2 7.5 1. 0.4 98.5 36.8 94.9 37.2 40.8 39.2 47.1 59.1 48.5 37.5 41.6 41.6 14.3 18.0 10.8 22.5 -18.5 36.1 33.5 34.8 32.4 12.3 46.3 39.3 37.1 37.6 8.8 4.5 7.0 7.2 1.9 8.7 7. -4.6 9.3 0.6 Table 3: Results on MCIFmix of the 21 models for each input modality, context, task (divided into the 4 macro-areas: RECOGNITION, TRANSLATION, QUESTION ANSWERING, and SUMMARIZATION), and target language ( for Chinese, while source language, being always English, is omitted). The best result by context is marked in bold, and the best overall result is underlined. Gemma 3n and Ming-Lite-Omni are removed from LONG scores as they are not able to process long-form speech. for Italian, and for German, for English, TRANSLATION. As expected, LLMs dominate in the translation task, benefiting from their strong language modeling and multilingual training. While some SpeechLLMs and MLLMs are competitive on short-form segments, performance deteriorates markedly in the long-form setting. This is especially evident for SpeechLLMs, where we observe drops of up to 30 COMET points in certain languages. The degradation is frequently attributable to undertranslation, where models fail to translate the entire context, which is more pronounced with long-form inputs. QUESTION ANSWERING. This macro-area shows more mixed results, especially in the shortform condition, where best-performing models come from across SpeechLLMs, VideoLLMs, and MLLMs. In particular, SpeechLLMs like Phi4-Multimodal and Qwen2-Audio, as well as MLLMs like Ola and Qwen2.5-Omni, show BERTScores above 30, indicating competitive performance in all languages while Qwen2.5-VL stands out in Chinese. In the long-form setting, Ola and Qwen2.5Omni remain highly competitive, with the Qwen2.5-Omni achieving the best scores in both English and Chinese, likely due to the language distribution in the training set. In contrast to recognition and translation, MLLMs generally improve or maintain performance on long-form question answering, while SpeechLLMs and VideoLLMs experience significant drops, mirroring trends seen in the"
        },
        {
            "title": "Preprint",
            "content": "other tasks. Surprisingly, LLMs see an overall underperformance in this task, suggesting marked limitations in crosslingual information extraction. SUMMARIZATION. LONG Across tasks, short-form performance is generally stronger, with long-form inSHORT puts introducing clear degradation in most models, particularly SpeechLLMs and VideoLLMs. An exception is Ola in the recognition task, which is the only model showing significant improvement from shortto long-form context inputs of more than 45 WER. manual inspection of the models outputs revealed that Ola occasionally misinterprets transcription prompts in the short-form, opting instead to perform image captioning of accompanying slides, while this is not the case for long-form inputs. For other models, the primary cause of long-form degradation appears to be undertranslation, where only part of the context is translated. Additional failure cases include persistent use of the wrong language (e.g., GraniteSpeech defaulting to English) or refusal to answer the users requests (UltraVox v0.5), independent of context type. MCIFfix MCIFmix. When comparing results between MCIFfix and MCIFmix, we observe consistent degradation across tasks, highlighting the limited robustness of current models to prompt variations. In recognition, the impact is especially pronounced: DeSTA2 and UltraVox degrade by up to 46.6 WER, and even Ola, previously the strongest on long-form, drops by 27 WER. Among MLLMs, MiniCPM-o-2 nearly doubles its WER on short-form, while Qwen2.5-Omni and MingLite-Omni also decline. In translation, LLMs remain relatively stable, though some drops (up to 4 COMET) are observed. SpeechLLMs are less consistent, with models like UltraVox fluctuating by over 10 COMET across languages. Question answering is similarly unstable, with drops up to 6 BERTScore for Phi4-Multimodal and LLaVA-NeXT, though some MLLMs show isolated gains. Summarization . Despite these shifts, Phi4-Multimodal remains the top performer in recognition and translation on short-form and becomes the best model in long-form recognition as Olathe best model on MCIFfixsuffers significant performance drop. In question answering, Qwen2.5-Omni demonstrates strong robustness to prompt variation, especially on long-form, securing two of the four best overall scores. These results reveal that many current models lack robustness to prompt reformulation of MCIFmix, with some tasks amplifying this vulnerability (e.g., recognition)."
        },
        {
            "title": "Open",
            "content": "Closed Models."
        },
        {
            "title": "5.2 EFFECT OF MODALITIES INTEGRATION",
            "content": "Work in progress..."
        },
        {
            "title": "5.3 BREAKDOWN ON QUESTION ANSWERING",
            "content": "Work in progress..."
        },
        {
            "title": "6 CONCLUSIONS",
            "content": "In this work, we introduced MCIF, the first human-annotated, multimodal, crosslingual instructionfollowing benchmark from the scientific domain. MCIF covers three core modalities (text, speech, and video) across four typologically diverse languages (English, German, Italian, and Chinese). It includes both shortand long-form contexts, spanning 13 tasks grouped into four macro-areas: recognition, translation, question answering, and summarization. Beyond providing comprehensive evaluation framework for current models abilities, MCIF includes the MCIFmix variant, which introduces distinct prompts across macro-areas, enabling multifaceted assessment of models instruction generalization and robustness. We further present an extensive benchmarking study of state-of-the-art LLMs, SpeechLLMs, VideoLLMs, and MLLMs, highlighting both their current capabilities and significant performance gaps. We believe MCIF lays the groundwork for future progress toward truly general-purpose, multimodal, and crosslingual instruction-following systems."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "The work presented in this paper is funded by the European Unions Horizon research and innovation programme under grant agreement No 101135798, project Meetween (My Personal AI Mediator for Virtual MEETings BetWEEN People) and the PNRR project FAIR - Future AI Research (PE00000013), under the NRRP MUR program funded by the NextGenerationEU."
        },
        {
            "title": "REFERENCES",
            "content": "Marah Abdin, Jyoti Aneja, Harkirat Behl, Sebastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell Hewett, Mojan Javaheripi, Piero Kauffmann, et al. Phi-4 technical report. arXiv preprint arXiv:2412.08905, 2024a. Marah Abdin, Jyoti Aneja, Harkirat Behl, Sebastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell J. Hewett, Mojan Javaheripi, Piero Kauffmann, James R. Lee, Yin Tat Lee, Yuanzhi Li, Weishung Liu, Caio C. T. Mendes, Anh Nguyen, Eric Price, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Xin Wang, Rachel Ward, Yue Wu, Dingli Yu, Cyril Zhang, and Yi Zhang. Phi-4 technical report, 2024b. URL https://arxiv.org/abs/ 2412.08905. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Inclusion AI. Ming-omni: unified multimodal model for perception and generation, 2025. URL https://arxiv.org/abs/2506.09344. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:23716 23736, 2022. Junyi Ao, Yuancheng Wang, Xiaohai Tian, Dekun Chen, Jun Zhang, Lu Lu, Yuxuan Wang, Haizhou Li, and Zhizheng Wu. Sd-eval: benchmark dataset for spoken dialogue understanding beyond words. arXiv preprint arXiv:2406.13340, 2024. Rosana Ardila, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael Henretty, Reuben Morais, Lindsay Saunders, Francis Tyers, and Gregor Weber. Common voice: massively-multilingual speech corpus. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pp. 42184222, Marseille, France, May 2020. ISBN 979-10-95546-34-4. URL https://aclanthology.org/2020.lrec-1.520/. Kirolos Ataallah, Chenhui Gou, Eslam Abdelrahman, Khushbu Pahwa, Jian Ding, and Mohamed Elhoseiny. Infinibench: comprehensive benchmark for large multimodal models in very long video understanding. arXiv preprint arXiv:2406.19875, 2024. Simone Balloccu, Patrıcia Schmidtova, Mateusz Lango, and Ondrej Dusek. Leak, cheat, repeat: In Proceedings of Data contamination and evaluation malpractices in closed-source LLMs. the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 6793, St. Julians, Malta, March 2024. URL https: //aclanthology.org/2024.eacl-long.5. Loıc Barrault, Yu-An Chung, Mariano Cora Meglioli, David Dale, Ning Dong, Paul-Ambroise Duquenne, Hady Elsahar, Hongyu Gong, Kevin Heffernan, John Hoffman, et al. Seamlessm4t: Massively multilingual & multimodal machine translation. arXiv preprint arXiv:2308.11596, 2023. Luel Hagos Beyene, Vivek Verma, Min Ma, Jesujoba O. Alabi, Fabian David Schmidt, Joyce Nakatumba-Nabende, and David Ifeoluwa Adelani. msteb: Massively multilingual evaluation of llms on speech and text tasks, 2025. URL https://arxiv.org/abs/2506.08400."
        },
        {
            "title": "Preprint",
            "content": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 18771901, 2020. URL https://proceedings.neurips.cc/paper_ files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf. Davide Caffagni, Federico Cocchi, Luca Barsellotti, Nicholas Moratelli, Sara Sarto, Lorenzo Baraldi, Lorenzo Baraldi, Marcella Cornia, and Rita Cucchiara. The revolution of multimodal large language models: survey. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics: ACL 2024, pp. 13590 13618, Bangkok, Thailand, August 2024. doi: 10.18653/v1/2024.findings-acl.807. URL https://aclanthology.org/2024.findings-acl.807/. Yiming Chen, Xianghu Yue, Chen Zhang, Xiaoxue Gao, Robby Tan, and Haizhou Li. Voicebench: Benchmarking llm-based voice assistants. arXiv preprint arXiv:2410.17196, 2024a. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024b. Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou. Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models. arXiv preprint arXiv:2311.07919, 2023. Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen2-audio technical report. arXiv preprint arXiv:2407.10759, 2024. Alexis Conneau, Min Ma, Simran Khanuja, Yu Zhang, Vera Axelrod, Siddharth Dalmia, Jason Riesa, Clara Rivera, and Ankur Bapna. Fleurs: Few-shot learning evaluation of universal representations of speech. In 2022 IEEE Spoken Language Technology Workshop (SLT), pp. 798805. IEEE, 2023. John Dang, Shivalika Singh, Daniel Dsouza, Arash Ahmadian, Alejandro Salamanca, Madeline Smith, Aidan Peppin, Sungjin Hong, Manoj Govindassamy, Terrence Zhao, Sandra Kublik, Meor Amer, Viraat Aryabumi, Jon Ander Campos, Yi-Chern Tan, Tom Kocmi, Florian Strub, Nathan Grinsztajn, Yannis Flet-Berliac, Acyr Locatelli, Hangyu Lin, Dwarak Talupuru, Bharat Venkitesh, David Cairuz, Bowen Yang, Tim Chung, Wei-Yin Ko, Sylvie Shang Shi, Amir Shukayev, Sammie Bae, Aleksandra Piktus, Roman Castagne, Felipe Cruz-Salinas, Eddie Kim, Lucas Crawhall-Stein, Adrien Morisot, Sudip Roy, Phil Blunsom, Ivan Zhang, Aidan Gomez, Nick Frosst, Marzieh Fadaee, Beyza Ermis, Ahmet Ustun, and Sara Hooker. Aya expanse: Combining research breakthroughs for new multilingual frontier, 2024. URL https: //arxiv.org/abs/2412.04261. Rocktim Das, Simeon Hristov, Haonan Li, Dimitar Dimitrov, Ivan Koychev, and Preslav Nakov. EXAMS-V: multi-discipline multilingual multimodal exam benchmark for evaluating vision language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 77687791, Bangkok, Thailand, August 2024. doi: 10.18653/v1/2024.acl-long.420. URL https://aclanthology.org/2024.acl-long.420/. Zhichao Duan, Xiuxing Li, Zhengyan Zhang, Zhenyu Li, Ning Liu, and Jianyong Wang. Bridging In 2021 IEEE Interthe language gap: Knowledge injected multilingual question answering. national Conference on Big Knowledge (ICBK), pp. 339346, 2021. doi: 10.1109/ICKG52313. 2021.00052."
        },
        {
            "title": "Preprint",
            "content": "Xinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao, Yining Li, Dahua Lin, and Kai Chen. Mmbench-video: long-form multi-shot benchmark for holistic video understanding. Advances in Neural Information Processing Systems, 37:8909889124, 2024. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Rongrong Ji. Mme: comprehensive evaluation benchmark for multimodal large language models. ArXiv, abs/2306.13394, 2023. Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. Kuofeng Gao, Shu-Tao Xia, Ke Xu, Philip Torr, and Jindong Gu. Benchmarking open-ended audio dialogue understanding for large audio-language models. arXiv preprint arXiv:2412.05167, 2024. Mingqi Gao, Wenqing Wang, Xiaojun Wan, and Yuemei Xu. Evaluating factuality in crossIn Findings of the Association for Computational Linguistics: ACL lingual summarization. 2023, pp. 1241512431, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.786. URL https://aclanthology.org/2023. findings-acl.786/. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzman, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur elebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vıtor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning"
        },
        {
            "title": "Preprint",
            "content": "Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and arXiv preprint Jacob Steinhardt. Measuring massive multitask language understanding. arXiv:2009.03300, 2020."
        },
        {
            "title": "Preprint",
            "content": "Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson. Xtreme: massively multilingual multi-task benchmark for evaluating cross-lingual generalisation. In International conference on machine learning, pp. 44114421. PMLR, 2020. Chien-yu Huang, Wei-Chih Chen, Shu-Wen Yang, Andy T. Liu, Chen-An Li, Yu-Xiang Lin, WeiCheng Tseng, Anuj Diwan, Yi-Jen Shih, Jiatong Shi, William Chen, Chih-Kai Yang, Xuanjun Chen, Chi-Yuan Hsiao, Puyuan Peng, Shih-Heng Wang, Chun-Yi Kuan, Ke-Han Lu, Kai-Wei Chang, Fabian Alejandro Ritter Gutierrez, and et al. Dynamic-superb phase-2: collaboratively expanding benchmark for measuring the capabilities of spoken language models with 180 tasks. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id= s7lzZpAW7T. Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, et al. Language is not all you need: Aligning perception with language models. Advances in Neural Information Processing Systems, 36: 7209672109, 2023. Muhammed Yusuf Kocyigit, Eleftheria Briakou, Daniel Deutsch, Jiaming Luo, Colin Cherry, and Markus Freitag. Overestimation in llm evaluation: controlled large-scale study on data contaminations impact on machine translation. arXiv preprint arXiv:2501.18771, 2025. Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023a. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: bootstrapping language-image pretraining with frozen image encoders and large language models. ICML23. JMLR.org, 2023b. Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2219522206, 2024a. Shicheng Li, Lei Li, Yi Liu, Shuhuai Ren, Yuanxin Liu, Rundong Gao, Xu Sun, and Lu Hou. Vitatecs: diagnostic dataset for temporal concept understanding of video-language models. In European Conference on Computer Vision, pp. 331348. Springer, 2024b. Zijing Liang, Yanjie Xu, Yifan Hong, Penghui Shang, Qi Wang, Qiang Fu, and Ke Liu. survey In Proceedings of the 3rd International Conference on of multimodel large language models. Computer, Artificial Intelligence and Control Engineering, pp. 405409, 2024. Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, In Findings of the and Lu Hou. TempCompass: Do video LLMs really understand videos? Association for Computational Linguistics: ACL 2024, pp. 87318772, Bangkok, Thailand, August 2024. doi: 10.18653/v1/2024.findings-acl.517. URL https://aclanthology.org/ 2024.findings-acl.517/. Zuyan Liu, Yuhao Dong, Jiahui Wang, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Ola: Pushing the frontiers of omni-modal language model with progressive modality alignment. arXiv preprint arXiv:2502.04328, 2025. Hongyuan Lu, Haoran Yang, Haoyang Huang, Dongdong Zhang, Wai Lam, and Furu Wei. Chainof-dictionary prompting elicits translation in large language models. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 958976, Miami, Florida, USA, November 2024a. doi: 10.18653/v1/2024.emnlp-main.55. URL https://aclanthology.org/ 2024.emnlp-main.55/. Ke-Han Lu, Zhehuai Chen, Szu-Wei Fu, Chao-Han Huck Yang, Jagadeesh Balam, Boris Ginsburg, Yu-Chiang Frank Wang, and Hung-yi Lee. Developing instruction-following speech language model without speech instruction-tuning data. arXiv preprint arXiv:2409.20007, 2024b."
        },
        {
            "title": "Preprint",
            "content": "Ke-Han Lu, Chun-Yi Kuan, and Hung-yi Lee. Speech-ifeval: Evaluating instruction-following arXiv preprint and quantifying catastrophic forgetting in speech-aware language models. arXiv:2505.19037, 2025. Evgeny Matusov, Gregor Leusch, Oliver Bender, and Hermann Ney. Evaluating machine translation output with automatic sentence segmentation. In Proceedings of the Second International Workshop on Spoken Language Translation, Pittsburgh, Pennsylvania, USA, October 24-25 2005. URL https://aclanthology.org/2005.iwslt-1.19/. Munan Ning, Bin Zhu, Yujia Xie, Bin Lin, Jiaxi Cui, Lu Yuan, Dongdong Chen, and Li Yuan. Video-bench: comprehensive benchmark and toolkit for evaluating video-based large language models. arXiv preprint arXiv:2311.16103, 2023. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35: 2773027744, 2022. Prabhat Pandey, Rupak Vignesh Swaminathan, KV Girish, Arunasish Sen, Jian Xie, Grant Strimel, and Andreas Schwarz. Sift-50m: large-scale multilingual dataset for speech instruction finetuning. arXiv preprint arXiv:2504.09081, 2025. Diogo Pernes, Goncalo M. Correia, and Afonso Mendes. Multi-target cross-lingual summarization: novel task and language-neutral approach. In Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 1290812924, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-emnlp.755. URL https://aclanthology.org/2024.findings-emnlp.755/. Yusu Qian, Hanrong Ye, Jean-Philippe Fauconnier, Peter Grasch, Yinfei Yang, and Zhe Gan. Miabench: Towards better instruction following evaluation of multimodal llms. arXiv preprint arXiv:2407.01509, 2024. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In Proceedings of the 40th International Conference on Machine Learning, ICML23. JMLR.org, 2023. Ricardo Rei, Craig Stewart, Ana Farinha, and Alon Lavie. COMET: neural framework In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Profor MT evaluation. ceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 26852702, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.213. URL https://aclanthology.org/2020. emnlp-main.213/. Ricardo Rei, Nuno M. Guerreiro, Jose Pombal, Joao Alves, Pedro Teixeirinha, Amin Farajian, and Andre F. T. Martins. Tower+: Bridging generality and translation specialization in multilingual llms, 2025. URL https://arxiv.org/abs/2506.17080. Paul Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur Bapna, Zalan Borsos, Felix de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, et al. Audiopalm: large language model that can speak and listen. arXiv preprint arXiv:2306.12925, 2023. Oscar Sainz, Jon Campos, Iker Garcıa-Ferrero, Julen Etxaniz, Oier Lopez de Lacalle, and Eneko Agirre. NLP evaluation in trouble: On the need to measure LLM data contamination for each In Findings of the Association for Computational Linguistics: EMNLP 2023, pp. benchmark. 1077610787, Singapore, December 2023. doi: 10.18653/v1/2023.findings-emnlp.722. URL https://aclanthology.org/2023.findings-emnlp.722. Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021."
        },
        {
            "title": "Preprint",
            "content": "George Saon, Avihu Dekel, Alexander Brooks, Tohru Nagano, Abraham Daniels, Aharon Satt, Ashish Mittal, Brian Kingsbury, David Haws, Edmilson Morais, et al. Granite-speech: opensource speech-aware llms with strong english asr capabilities. arXiv preprint arXiv:2505.08699, 2025. Yan Shu, Peitian Zhang, Zheng Liu, Minghao Qin, Junjie Zhou, Tiejun Huang, and Bo Zhao. Video-xl: Extra-long vision language model for hour-scale video understanding. arXiv preprint arXiv:2409.14485, 2024. Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. Pandagpt: One model to instruction-follow them all. In Proceedings of the 1st Workshop on Taming Large Language Models: Controllability in the era of Interactive Assistants!, pp. 1123, 2023. Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, and Chao Zhang. Salmonn: Towards generic hearing abilities for large language models. arXiv preprint arXiv:2310.13289, 2023. Team Gemma, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Rame, Morgane Rivi`ere, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. Ioannis Tsiamas, Gerard I. Gallego, Jose A. R. Fonollosa, and Marta R. Costa-juss`a. SHAS: Approaching optimal Segmentation for End-to-End Speech Translation. In Proc. Interspeech 2022, pp. 106110, 2022. doi: 10.21437/Interspeech.2022-59. Bin Wang, Xunlong Zou, Geyu Lin, Shuo Sun, Zhuohan Liu, Wenyu Zhang, Zhengyuan Liu, AiTi Aw, and Nancy F. Chen. AudioBench: universal benchmark for audio large language models. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 4297 4316, Albuquerque, New Mexico, April 2025a. ISBN 979-8-89176-189-6. doi: 10.18653/v1/ 2025.naacl-long.218. URL https://aclanthology.org/2025.naacl-long.218/. Dingdong Wang, Jincenzi Wu, Junan Li, Dongchao Yang, Xueyuan Chen, Tianhua Zhang, and Helen Meng. Mmsu: massive multi-task spoken language understanding and reasoning benchmark. arXiv preprint arXiv:2506.04779, 2025b. Dingdong Wang, Jin Xu, Ruihang Chu, Zhifang Guo, Xiong Wang, Jincenzi Wu, Dongchao Yang, Shengpeng Ji, and Junyang Lin. Inserter: Speech instruction following with unsupervised interleaved pre-training, 2025c. URL https://arxiv.org/abs/2503.02769. Jiaan Wang, Fandong Meng, Duo Zheng, Yunlong Liang, Zhixu Li, Jianfeng Qu, and Jie Zhou. survey on cross-lingual summarization. Transactions of the Association for Computational Linguistics, 10:13041323, 11 2022. ISSN 2307-387X. doi: 10.1162/tacl 00520. URL https://doi.org/10.1162/tacl_a_00520. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024a. Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Xiaotao Gu, Shiyu Huang, Bin Xu, Yuxiao Dong, et al. Lvbench: An extreme long video understanding benchmark. arXiv preprint arXiv:2406.08035, 2024b. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Dai, and Quoc Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021. Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Hassan Awadalla. paradigm shift in maIn The Twelfth chine translation: Boosting translation performance of large language models. International Conference on Learning Representations, 2024."
        },
        {
            "title": "Preprint",
            "content": "Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, Bin Zhang, Xiong Wang, Yunfei Chu, and Junyang Lin. Qwen2.5-omni technical report, 2025. URL https://arxiv.org/abs/2503.20215. Ruiqi Yan, Xiquan Li, Wenxi Chen, Zhikang Niu, Chen Yang, Ziyang Ma, Kai Yu, and Xie Chen. Uro-bench: comprehensive benchmark for end-to-end spoken dialogue models. arXiv preprint arXiv:2502.17810, 2025. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, arXiv preprint Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv:2505.09388, 2025a. Chih-Kai Yang, Neo Ho, Yen-Ting Piao, and Hung-yi Lee. Sakura: On the multi-hop reasoning of large audio-language models based on speech and audio information. arXiv preprint arXiv:2505.13237, 2025b. Qian Yang, Jin Xu, Wenrui Liu, Yunfei Chu, Ziyue Jiang, Xiaohuan Zhou, Yichong Leng, Yuanjun Lv, Zhou Zhao, Chang Zhou, and Jingren Zhou. AIR-bench: Benchmarking large audio-language models via generative comprehension. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 19791998, Bangkok, Thailand, August 2024. doi: 10.18653/v1/2024. acl-long.109. URL https://aclanthology.org/2024.acl-long.109/. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95569567, 2024. Emmanouil Zaranis, Antonio Farinhas, Saul Santos, Beatriz Canaverde, Miguel Moura Ramos, Aditya Surikuchi, Andre Viveiros, Baohao Liao, Elena Bueno-Benito, Nithin Sivakumaran, et al. Movie facts and fibs (mf2): benchmark for long movie understanding. arXiv preprint arXiv:2506.06275, 2025. Bo Zeng, Chenyang Lyu, Sinuo Liu, Mingyan Zeng, Minghao Wu, Xuanfan Ni, Tianqi Shi, Yu Zhao, Yefeng Liu, Chenyu Zhu, Ruizhe Li, Jiahui Geng, Qing Li, Yu Tong, Longyue Wang, Weihua Luo, and Kaifu Zhang. Marco-bench-mif: On multilingual instruction-following capability of large language models, 2025. URL https://arxiv.org/abs/2507.11882. Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, et al. Videollama 3: Frontier multimodal foundation models for image and video understanding. arXiv preprint arXiv:2501.13106, 2025. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=SkeHuCVFDr. Wenxuan Zhang, Mahani Aljunied, Chang Gao, Yew Ken Chia, and Lidong Bing. M3exam: multilingual, multimodal, multilevel benchmark for examining large language models. Advances in Neural Information Processing Systems, 36:54845505, 2023. Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li, Kun Wang, Qingsong Wen, Zhang Zhang, et al. Mme-realworld: Could your multimodal llm challenge high-resolution real-world scenarios that are difficult for humans? arXiv preprint arXiv:2408.13257, 2024a. Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llava-next: strong zero-shot video understanding model, April 2024b. URL https://llava-vl.github.io/blog/2024-04-30-llava-next-video/."
        },
        {
            "title": "Preprint",
            "content": "Maike Zufle, Sara Papi, Beatrice Savoldi, Marco Gaido, Luisa Bentivogli, and Jan Niehues. Nutshell: dataset for abstract generation from scientific talks, 2025. URL https://arxiv. org/abs/2502.16942."
        },
        {
            "title": "A TASKS DESCRIPTION",
            "content": "The description for each task is provided in Table 4. Task Name Acronym Textual Question Answering TQA Text Summarization TSUM Machine Translation MT Automatic Speech Recognition Spoken Question Answering ASR SQA Speech Summarization SSUM Speech Translation ST Video Question Answering VQA Video Summarization VSUM Audio-Video Recognition Audio-Video Question Answering AVR AVQA Audio-Video Summarization AVSUM Description Given textual context in the source language and question in the target language, the task involves generating an accurate openended answer in the target language based on the provided context. Given textual context in the source language, the task involves generating shorter version in the target language that retains the most important information. Given textual context, the task involves translating the text from the source language into different target language, preserving the meaning while adapting to linguistic and grammatical norms. The task involves converting the spoken language into written text in the same language, focusing on accurate transcription of the speech signal. Given speech context in the source language and textual question in the target language, the task involves generating an accurate open-ended answer in the target language based on the provided context. Given speech context in the source language, the task involves generating shorter version of the spoken content in the target language that retains the most important information. The task involves translating speech in the source language into text in the target language, combining speech recognition and machine translation in single task. Given video context in the source language and textual question in the target language, the task involves generating an accurate open-ended answer in the target language based on the provided context. Given video context in the source language, the task involves generating summary in the target language based on the provided context. Given both video and speech contexts, the task involves generating an accurate content transcript. Given both video and speech contexts in the source language and textual question in the target language, the task involves generating an accurate open-ended answer in the target language based on the provided audio-visual context. Given both video and speech contexts in the source language, the task involves generating shorter version of the content in the target language that retains the most important information. Audio-Video Translation AVT Given both video and speech contexts, the task involves generating an accurate content translation. Table 4: Extended description of the tasks supported by the MCIF benchmark."
        },
        {
            "title": "B DATA CREATION PROCESS AND GUIDELINES",
            "content": "B.1 TRANSCRIPTION AND TRANSLATION To produce the talk transcripts, their translations, and the translations of QA and summaries, we hired professional linguists and translators and via language service agency. For each language and language pair, two professionals were assigned (2 for English, German, Italian, and Mandarin Chinese), for total of 8 professionals. Transcriptions were paid per audio runtime (C3min, in line"
        },
        {
            "title": "Preprint",
            "content": "with market rates). Translations were paid per weighted word count (accounting for repetitions, translation memory matches, and MT suggestions) at an average rate of 0.04C/source word. Professionals were task Ea9lJzU5GD5MisejRH2p20oByhxd9uUq80o639Qwb8Eyow?e=Ht6yXA). perform the on https://fbk.sharepoint.com/:b:/s/MTUnit/ provided with guidelines (available how to detailed at: Transcription began from ASR English outputs (model details are internal to the agency and thus confidential), with professionals revising and correcting the output using MateDub,15 CAT-assisted tool that integrates video playback for context-aware transcription. Professionals were instructed to produce clean, fluent transcripts closely aligned with the original audio while respecting technical jargon, US spelling, and proper punctuation. Disfluencies and background noises were omitted. Translation started from an internal MT system (also internal to the agency and thus confidential), with translators working in the CAT tool MateCat.16 They were free to entirely modify or disregard automatic suggestions to ensure adequacy and fluency. Translators adhered to the transcript formatting and respected language variants (Italian for Italy, German for Germany, Mandarin Chinese). They were instructed not to translate i) original English paper titles, if any, ii) non-English words or expressions that were provided as multilingual examples during the presentation, if any. Native-speaking authors of each target language supervised translations to ensure terminological consistency. B.2 QUESTION-ANSWERING The creation of the QA pairs was carried out by students and authors of this paper, who are all experts in NLP and machine learning. All contributors followed set of detailed instructions, fully available at https://fbk.sharepoint.com/:b:/s/MTUnit/EZKUhrURd_ xOm6V3CvcFmDwBRxb0Tc-44AHQpJJBNwLtMw?e=vWqJhd, which outlined the process and quality criteria for creating and annotating the QA pairs. QA types and distribution. For each talk, annotators were asked to produce at least 10 QA pairs, divided by both the type of question and the type of answer. For question types, each talk required: i) 3 general questions (pre-assigned, the same for all papers), ii) 3 realistic, abstract-based questions, created after reading the abstract, and iii) 4 paper-specific, transcript-based questions. We enforced this distribution to ensure balanced representation of different question types. In all cases, contributors had to annotate each QA pair with timestamp indicating where the answer appeared in the video, and assign label reflecting its source of information: (answerable from audio only), (from video only), AV (from both), or NA (not answerable). target distribution of answer labels was also required for each talk: minimum of 7 A/AV pairs (with at least 5 AV), 2 pairs, and 1 NA pair. If this distribution was not achieved through the existing QAs, additional pairs were created to meet the target. The guidelines, linked above, provided detailed recommendations on how to formulate clear, specific, and concise questions, avoiding vague or multi-part formulations, and ensuring answers directly addressed the question in no more than two sentences. They also included conventions for US spelling and number formatting."
        },
        {
            "title": "C LIST OF PROMPTS",
            "content": "The fixed prompts used for MCIFfix are reported in Table 5."
        },
        {
            "title": "D MODELS",
            "content": "The models used for the analyses are listed in Table 6 and run using the HuggingFace Transformer version indicated for each model, as some of them require specific version. 15https://matedub.com/ 16https://www.matecat.com/"
        },
        {
            "title": "Preprint",
            "content": "Lang."
        },
        {
            "title": "Prompt",
            "content": "en de it zh en de it zh en de it zh Transcribe the English content."
        },
        {
            "title": "Translation",
            "content": "Ubersetze den englischem Inhalt nach Deutsch. Traduci il contenuto inglese in italiano. 将英文内容翻译成中文"
        },
        {
            "title": "Question Answering",
            "content": "Answer the following question given the English content: {QUESTION} Beantworte die folgende Frage basierend auf dem englischen Inhalt: {QUESTION} Rispondi alla seguente domanda dato il contenuto inglese: {QUESTION} 根据所给的英文内容回答以下问题{QUESTION} Summarization Summarize the English content in an abstract of approximately 200 words. Fasse den englischen Inhalt in einem Abstract mit maximal 200 Wortern zusammen. Riassumi il contenuto inglese in un abstract di circa 200 parole. 用400个字左右概括所给的英语内容 Table 5: Fixed prompts used for MCIFfix. For all models, we use the default generation parameters and apply greedy search, following the usage instructions reported in the model cards. The adopted prompts for each model are the suggested prompts,17 when available. The code used for inference is released upon paper acceptance. The inference is performed using single NVIDIA GH200 120GB GPU. RESULTS NON-NORMALIZED WITH BASELINES The scores of MCIFfix and MCIFmix with non-normalized BERTScore are provided in, respectively, Table 7 and Table 8. 17For Qwen2.5-Omni, the suggested prompt leads the model to often reply that is not able to perform the task, and has been replaced with more generic prompt."
        },
        {
            "title": "Preprint",
            "content": "Model Param. In.Mod. Weights Gemma 3 Phi 12B 14.7B Aya Expanse 8B Qwen3 14B Tower-Plus Llama 3.1 DeSTA2 GraniteSpeech 3.3 Qwen2-Audio 9B 8B 8B 8B 7B Phi4Multimodal 5.6B UltraVox 0.5 8.07B InternVL3 14B Qwen2.5-VL VideoLLaMA Video-XL-2 LLaVA-NeXTVideo Qwen2.5Omni Ming-LiteOmni MiniCPM-o-2 Ola Gemma 3n 7B 7B 8B 7B 7B 2.8B 8B 7B 4B https://hf.co/google/gemma-3-12b-it https://hf.co/microsoft/phihttps://huggingface.co/CohereLabs/ aya-expanse-8b https://huggingface.co/Qwen/ Qwen3-14B https://huggingface.co/Unbabel/ Tower-Plus-9B https://hf.co/meta-llama/Llama-3. 1-8B-Instruct https://hf.co/DeSTA-ntu/ DeSTA2-8B-beta https://hf.co/ibm-granite/ granite-speech-3.3-8b https://hf.co/Qwen/ Qwen2-Audio-7B-Instruct https://hf.co/microsoft/ Phi-4-multimodal-instruct https://hf.co/fixie-ai/ultravox-v0_ 5-llama-3_2-1b https://huggingface.co/OpenGVLab/ InternVL3-14B https://huggingface.co/Qwen/Qwen2. 5-VL-7B-Instruct https://huggingface.co/DAMO-NLP-SG/ VideoLLaMA3-7B https://huggingface.co/BAAI/ Video-XL-2 https://huggingface.co/llava-hf/ LLaVA-NeXT-Video-7B-hf https://hf.co/microsoft/ Phi-4-multimodal-instruct https://huggingface.co/inclusionAI/ Ming-Lite-Omni https://huggingface.co/openbmb/ MiniCPM-o-2_6 https://huggingface.co/THUdyh/Ola-7b https://huggingface.co/google/ gemma-3n-E4B-it HFv 4.51.3 4.51.3 4.51.3 4.51. 4.51.3 4.51.3 4.51.3 4.52.4 4.51.3 4.48. 4.51.3 4.51.3 4.51.3 4.51.3 4.51.3 4.51. 4.51.3 4.45.0 4.44.2 4.43.4 4.53.0 Table 6: Details of the models, including the number of parameters (Param.), input modalities analyzed in this paper (In.Mod.), their public weights release (Weights), and the HuggingFace Transformer version (HFv) used for the experiments on the MCIF benchmark."
        },
        {
            "title": "Preprint",
            "content": "t n R Model Input Modality DeSTA2 GraniteSpeech Phi4-Multimodal Qwen2-Audio UltraVox v0.5 InternVL3 LLaVA-NeXT Qwen2.5-VL VideoLLaMA3 Video-XL Gemma 3n Ming-Lite-Omni MiniCPM-o-2 Ola Qwen2.5-Omni Aya Expanse Gemma 3 Llama 3.1 Phi4 Qwen3 Tower+ DeSTA2 GraniteSpeech Phi4-Multimodal Qwen2-Audio UltraVox v0.5 L InternVL3 LLaVA-NeXT Qwen2.5-VL VideoLLaMA3 Video-XL2 MiniCPM-o-2 Ola Qwen2.5-Omni REC. TRANSLATION QUESTION ANSW. SUMMARIZATION WER BERTSCORE BERTSCORE COMET 45.1 72.7 76.3 77.3 9.4 42.0 52.2 62.1 6.8 77.7 81.2 81.6 71.5 73.6 79.1 31.3 146.2 46.1 52.5 34.4 93.7 36.7 38.5 29.9 117.5 55.8 55.9 47.4 100.7 35.0 41.3 42.9 72.3 76.8 80.4 54.5 74.2 76.4 81.2 43.5 63.3 68.8 73.9 82.3 87.9 86.3 79.2 83.2 77.5 82.9 85.7 84.8 82.9 86.2 85.5 86.3 87.3 85.9 39.4 44.3 39.4 35.4 40.3 32.3 56.3 66.4 56.5 39.7 43.1 41.3 36.7 40.5 35.5 93.0 99.9 39.2 93.1 88. 87.0 69.5 69.3 55.1 85.1 62.0 62.2 49.6 90.3 75.7 76.2 70.3 89.0 74.4 74.4 71.3 85.9 69.1 68.5 64. 87.7 71.8 72.3 71.9 84.8 63.7 66.3 63.4 87.6 73.2 72.4 72.4 87.7 74.3 72.8 66.5 86.5 67.4 67.0 58.9 85.0 65.1 64.8 56.4 88.7 57.8 65.1 60.8 85.2 69.6 66.1 62.5 89.2 76.5 76.1 71.9 89.0 73.6 74.5 68.9 86.1 68.4 68.4 64.9 87.8 71.2 71.0 58.6 88.1 72.2 72.8 66.9 88.3 73.5 73.8 64.0 88.2 72.2 71.3 67.2 86.3 68.4 69.3 62.3 85.7 67.6 67.5 53.4 84.4 63.8 64.7 49.5 79.3 51.4 52.2 47.5 81.9 56.8 57.3 47.0 89.5 74.6 74.7 72.2 86.2 66.2 67.3 58.0 87.8 72.3 72.4 68.6 83.6 63.1 65.6 56.4 86.5 66.2 63.8 59.5 84.2 59.5 61.3 48.8 86.6 69.7 70.9 70.5 85.7 65.2 66.6 64.6 84.1 59.2 63.8 62.2 82.0 59.0 59.9 48.9 89.6 73.7 74.2 71.8 86.4 64.5 68.8 65.6 87.5 73.6 72.5 68.0 75.3 62.9 55.9 40.3 87.3 69.1 68.8 61.6 84.8 63.2 63.4 50.6 98.8 9.9 98. 37.3 40.7 39.5 64.0 63.0 54.9 37.2 42.3 40.8 85.5 69.7 66.3 63.0 80.6 35.0 41.3 42.9 89.2 76.6 75.5 69.9 86.1 64.5 65.1 53.7 91.0 76.2 75.6 74.9 43.5 74.2 76.4 81.2 Table 7: Results on MCIFfix of the 21 models for each input modality, context, task (divided into the 4 macro-areas: RECOGNITION, TRANSLATION, QUESTION ANSWERING, and SUMMARIZATION), and target language ( for Chinese, while source language, being always English, is omitted). The best result by context is marked in bold, and the best overall result is underlined. Gemma 3n and Ming-Lite-Omni are removed from LONG scores as they are not able to process long-form speech. BERTScore is not normalized with the baseline. for Italian, and for German, for English,"
        },
        {
            "title": "Preprint",
            "content": "t n R Model Input Modality REC. TRANSLATION QUESTION ANSW. SUMMARIZATION WER BERTSCORE BERTSCORE COMET DeSTA2 GraniteSpeech Phi4-Multimodal Qwen2-Audio UltraVox v0.5 InternVL3 LLaVA-NeXT Qwen2.5-VL VideoLLaMA3 Video-XL2 Gemma 3n Ming-Lite-Omni MiniCPM-o-2 Ola Qwen2.5-Omni Aya Expanse Gemma 3 Llama 3.1 Phi4 Qwen3 Tower+ 72.8 72.1 76.4 76.2 9.5 42.9 49.4 47.5 6.7 77.7 81.3 81.3 70.9 74.2 78.4 32.9 192.7 44.5 43.4 44.9 94.1 37.0 38.7 38.7 128.2 57.3 55.1 47.5 212.5 34.8 40.4 42.3 72.4 76.7 79.6 49.0 74.0 74.4 81.0 48.0 87.3 69.9 70.2 54.5 84.9 61.8 62.3 49.4 90.1 75.5 75.8 67.6 89.0 74.7 74.1 70.8 86.0 69.7 68.5 62.5 87.9 72.3 73.5 72.5 84.4 64.5 65.1 61.4 87.3 72.6 73.5 71.6 87.8 74.4 72.3 67.3 86.3 68.7 67.5 57. 84.7 66.0 65.2 55.9 87.9 64.9 64.6 58.3 85.5 68.9 66.1 65.0 89.7 76.4 76.3 71.3 89.0 74.1 74.6 69.0 62.3 69.2 74.1 81.6 86.0 82.7 80.3 83.6 79.2 82.0 86.9 85.1 82.4 86.1 85.1 81.5 83.5 86.0 85.6 66.9 67.5 64.9 87.7 71.4 70.8 57.4 88.2 72.2 72.7 67.4 88.2 73.4 73.7 63.5 87.4 70.3 70.7 65.3 86.3 68.4 69.3 55.9 DeSTA2 GraniteSpeech Phi4-Multimodal Qwen2-Audio UltraVox v0.5 L 132.7 39.9 43.9 39.1 35.1 39.4 29.4 80.4 60.7 65.4 52.4 29.8 39.7 43.4 41.3 93.0 36.8 40.6 35.5 91.6 85.8 67.2 67.8 52.5 83.9 63.3 64.7 49.8 79.3 52.3 52.1 47.0 82.0 56.8 57.5 47.0 89.4 74.4 74.7 71.7 86.1 66.3 67.1 58.8 87.7 72.4 72.7 68.1 83.5 63.6 65.5 55.5 86.6 65.0 64.8 57.8 84.3 59.9 60.8 49.3 InternVL3 LLaVA-NeXT Qwen2.5-VL VideoLLaMA3 Video-XL MiniCPM-o-2 Ola Qwen2.5-Omni 86.5 71.1 70.8 69.3 85.4 65.9 67.1 64.5 82.7 60.2 63.5 61.6 81.5 58.6 60.0 51.5 89.1 73.7 75.0 70.3 86.0 66.1 67.8 61.1 88.0 73.7 71.3 67.0 79.6 50.1 51.8 22.1 87.5 70.7 69.5 61.4 84.4 62.0 62.1 55.7 98.5 36.8 94.9 37.2 40.8 39.2 47.1 59.1 48.5 37.5 41.6 41. 85.5 68.4 66.1 64.9 80.0 63.2 62.7 52.7 89.2 74.4 75.2 69.4 85.2 64.2 65.3 59.0 90.9 76.6 76.1 71.8 84.6 64.3 64.8 55.0 Table 8: Results on MCIFfix of the 21 models for each input modality, context, task (divided into the 4 macro-areas: RECOGNITION, TRANSLATION, QUESTION ANSWERING, and SUMMARIZATION), and target language ( for Chinese, while source language, being always English, is omitted). The best result by context is marked in bold, and the best overall result is underlined. Gemma 3n and Ming-Lite-Omni are removed from LONG scores as they are not able to process long-form speech. BERTScore is not normalized with the baseline. for Italian, and for German, for English,"
        }
    ],
    "affiliations": [
        "Fondazione Bruno Kessler (Italy)",
        "Karlsruhe Institute of Technology (Germany)",
        "Translated (Italy)"
    ]
}