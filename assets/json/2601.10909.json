{
    "paper_title": "FrankenMotion: Part-level Human Motion Generation and Composition",
    "authors": [
        "Chuqiao Li",
        "Xianghui Xie",
        "Yong Cao",
        "Andreas Geiger",
        "Gerard Pons-Moll"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Human motion generation from text prompts has made remarkable progress in recent years. However, existing methods primarily rely on either sequence-level or action-level descriptions due to the absence of fine-grained, part-level motion annotations. This limits their controllability over individual body parts. In this work, we construct a high-quality motion dataset with atomic, temporally-aware part-level text annotations, leveraging the reasoning capabilities of large language models (LLMs). Unlike prior datasets that either provide synchronized part captions with fixed time segments or rely solely on global sequence labels, our dataset captures asynchronous and semantically distinct part movements at fine temporal resolution. Based on this dataset, we introduce a diffusion-based part-aware motion generation framework, namely FrankenMotion, where each body part is guided by its own temporally-structured textual prompt. This is, to our knowledge, the first work to provide atomic, temporally-aware part-level motion annotations and have a model that allows motion generation with both spatial (body part) and temporal (atomic action) control. Experiments demonstrate that FrankenMotion outperforms all previous baseline models adapted and retrained for our setting, and our model can compose motions unseen during training. Our code and dataset will be publicly available upon publication."
        },
        {
            "title": "Start",
            "content": "FrankenMotion: Part-level Human Motion Generation and Composition Chuqiao Li1 Xianghui Xie1,2 Yong Cao1 Andreas Geiger1 1Tubingen AI Center, University of Tubingen, Germany 2Max Planck Institute for Informatics, Saarland Informatics Campus, Germany https://coral79.github.io/frankenmotion/ Gerard Pons-Moll1, 6 2 0 2 5 1 ] . [ 1 9 0 9 0 1 . 1 0 6 2 : r Figure 1. Overview of our FrankenMotion framework. Left: Body-Part Control, where users specify fine-grained movements of individual body parts; Middle: Body-Part + Action Control, enabling coordinated whole-body actions with part-specific constraints; Right: Body-Part + Action + Sequence Control, supporting complex multi-stage motion sequences involving interactions and transitions. In all cases, FrankenAgent translates natural-language instructions into structured control signals for precise motion generation."
        },
        {
            "title": "Abstract",
            "content": "Human motion generation from text prompts has made remarkable progress in recent years. However, existing methods primarily rely on either sequence-level or action-level descriptions due to the absence of fine-grained, part-level motion annotations. This limits their controllability over individual body parts. In this work, we construct highquality motion dataset with atomic, temporally-aware partlevel text annotations, leveraging the reasoning capabilities of large language models (LLMs). Unlike prior datasets that either provide synchronized part captions with fixed time segments or rely solely on global sequence labels, our dataset captures asynchronous and semantically distinct part movements at fine temporal resolution. Based on this dataset, we introduce diffusion-based part-aware motion generation framework, namely FrankenMotion, where each body part is guided by its own temporally-structured textual prompt. This is, to our knowledge, the first work to provide atomic, temporally-aware part-level motion annotations and have model that allows motion generation with both spatial (body part) and temporal (atomic action) control. Experiments demonstrate that FrankenMotion outperforms all previous baseline models adapted and retrained for our setting, and our model can compose motions unseen during training. Our code and dataset will be publicly available upon publication. 1. Introduction Human motion generation is fundamental task with broad applications in augmented reality (AR) and virtual reality (VR), gaming, entertainment, and embodied AI [67, 71]. In recent years, significant progress has been achieved in this field, largely driven by the growing availability of motion capture (mocap) datasets and their corresponding textual annotations. [12, 17, 31, 38]. These advancements have enabled motion generation models conditioned on various modalities, including text, scene layouts, and object interactions [8, 19, 20, 52]. Concurrently, various extensions of the base task [4, 64], including motion editing, physicsaware generation, and style transfer, have been introduced. However, existing methods struggle to achieve both temporal and part-level control, primarily due to limitations in both dataset annotation and model design. For example, models trained on mocap datasets can generate realistic motions [13, 17, 18, 46], yet they still lack finegrained control over temporal dynamics and body parts. This is because most mocap datasets [30] lack temporally and part-level aligned annotations. To address this, several approaches [24, 62] have incorporated part-level labels to enhance model performance. Despite leveraging such information, these methods still fail to capture temporally coherent part-level features, which are crucial for fine-grained motion generation and controllable editing. In this paper, we tackle motion generation from the perspective of composition. We argue that complex motions can be decomposed into simpler atomic motion elements. The key to allowing fine-grained control and complex motion generation lies in designing models that can learn these fundamental motion elements and their compositional relationships. We consider body parts as the basic elements in motion composition and design model that maps prompts of body parts into complete temporal segments, the atoms of motion. We also input the high level action descriptions so that the model understands how different body parts compose into semantically meaningful complex motions. Training such model requires detailed per frame body part annotation, which is prohibitive to obtain. There exist datasets with textual annotations yet they lack structured part annotations. HumanML3D [12] and KITML [38] feature high level summary of full motion sequence. Babel [39] contains action level annotations such as walk, stand, and knock. Some actions, like raising arms, do describe what one part is doing yet the annotations are not structured, and the part movement of most actions, such as knock, remains unannotated. However, if person sits down, the knees are probably bending; if person is tying their shoes, their spine is bending, and the arms are tying the shoelaces. In other words, we can infer what the body parts are doing from high-level descriptions. Remarkably, we find that LLMs are powerful in inferring such relationships. Thus, we instantiate FrankenAgent, an LLM agent that consumes existing datasets and outputs coherent perframe body part annotations together with high level annotations. Using this, we automatically annotate the largest part-level human motion dataset to this date, which we call FrankenStein dataset. Using FrankenStein, we train FrankenMotion, method that can be controlled at the sequence, action and part level as illustrated in Fig. 1. By constitutionality, we can create novel movements not seen during training, such as person sitting while raising the left arm. Experiments show that our FrankenAgent is highly reliable, with 93.08% annotations considered correct by human experts. Results on our annotated FrankenStein also show that our model consistently outperforms state-of-the-art methods on part based motion generation in terms of both semantic correctness and realism. In summary, our main contributions are: We present FrankenMotion, text-to-motion model that learns to compose complex motions through hierarchical conditioning on part-, action-, and sequence-level text, enabling fine-grained control over body parts and timing. We introduce FrankenStein, new dataset with structured and temporally aligned body part annotations, leveraging existing datasets and LLM agent namely FrankenAgent. With hierarchical conditioning, our model allows flexible controlling at part, action or sequence level and generating novel motion compositions. 2. Related Work Motion generation with control. Controllable motion generation has become increasingly important as it enables models to synthesize realistic motions aligned with user intent and environmental context. Among various modalities, text-conditioned motion generation [7, 8, 12, 13, 32, 35, 44, 46, 60, 61, 68] has gained popularity for its flexibility and expressiveness. Other forms of control include inter-person conditioning [29, 42, 43, 48] for modeling humanhuman interactions, key-pose or trajectory guidance [2, 66] for structural control, and audio-driven synthesis [22, 23, 65] for cross-modal alignment. Humanobject interaction modeling [21, 50, 5355, 5759, 63] further introduces affordance reasoning and contact-based control, while sceneaware methods [19, 34, 47, 49] incorporate spatial constraints to ensure physical plausibility. Despite these advances, achieving fine-grained spatial and temporal control in motion generation remains challenging open problem. Fine-grained Spatio-Temporal Motion Generation. Achieving precise control in text-to-motion generation requires modeling both temporal and spatial structures. Existing approaches largely emphasize temporal control, regulating the duration and sequencing of motion segments at the frame level. Methods such as TEACH [1] compose motion segments using text and sequence-level conditioning, PriorMDM [44] refines transitions between short clips via two-stage inference process, and FlowMDM [3] enhances temporal coherence by modeling smooth local transitions. DART [68] employs latent diffusion model for real-time, autoregressive motion generation, while UniMotion [20] introduces hierarchical control through both frameand sequence-level conditioning. Beyond temporal modeling, spatial control has also gained attention. FineMoGen [62] supports diffusion-based motion generation and editing for fine-grained, per-body-part control, but its stage-based annotations enforce synchronized temporal intervals across parts, limiting flexibility. Besides, STMC [37] achieves spatial composition by assembling body-part motions from pretrained diffusion models, yet it remains post-hoc method lacking end-to-end spatialtemporal reasoning. While these methods produce impressive results, none provide unified, fine-grained multi-level control over atomic body parts, atomic actions, and sequence-level semantics along the temporal axis. Motion understanding and annotation. Contrary to motion generation, motion understanding with natural language is also an important task for human behavior analysis and motion data annotation. Earlier work PoseScript [9] generates text descriptions from pose parameters based on predefined rules. Follow up work MotionScript [56] extends PoseScript to generate texts for motions programmatically. Despite detailed descriptions, the generated texts are complex and less similar to natural human language. Recent line of works focus more on using LLMs to understand human poses [11, 25] or motions [5, 18, 33, 70]. common practice is to fine tune the pre-trained language models to align text and motion in the latent space. These methods differ in the formulation of motion understanding as language translation [18] or interactive question answering [5, 33]. Some works can also handle raw videos [10, 51] in addition to 3D motions. In general, rule based annotation [9, 56] lacks human readability, while fine-tuned models [18] lose the generalization ability of original language models. Furthermore, these annotations focus on high-level action descriptions, lacking detailed body part decomposition to learn the essential motion elements. Our proposed annotation pipeline uses existing data and LLMs to generate atomic body-part level text prompts with temporal structure. 3. FrankenStein Dataset Construction The quality and granularity of motion annotations directly constrain the upper bound of learning in motion understanding tasks. Existing datasets typically provide only coarsegrained labels, while lacking fine-grained temporal decomposition and body-part-specific annotations. For example, as shown in Fig. 2a, existing datasets only include sequence and action labels. To address this limitation, we propose new annotation paradigm that explicitly operates at three (1) sequence level: global delevels of granularity: scription of the full motion sequence, consistent with existing annotation formats; (2) action level: temporally localized coarse atomic actions; and (3) body-part level: finegrained annotations for individual body parts (e.g., head, arms, legs, spine, trajectory) over time. This hierarchical annotation design enables richer and more structured representation of human motion. As illustrated in Fig. 2b, the entire annotation process is automated, leveraging the reasoning capabilities of LLMs. 3.1. Data Source Before presenting our automatic annotation pipeline, we first introduce several widely used motionlanguage datasets to which our approach can be applied: KIT-ML [38], BABEL [39], AMASS [30] and HumanML3D [12]. KIT-ML is one of the earliest datasets linking motion capture data with natural language. It combines sequences from the CMU and KIT mocap datasets, providing motiontext pairs that enabled early research on motionlanguage alignment. However, it remains small in scale and lacks temporal segmentation or part-level annotations. AMASS later unified motion capture data from 15 publicly available mocap datasets into consistent parameterization of human motion, forming large-scale foundation for subsequent motionlanguage datasets. Building on this resource, BABEL annotates subset of AMASS with atomic action and coarse sequence-level labels, covering diverse activities but offering mainly categorical and short temporal annotations. Similarly, HumanML3D extends AMASS with crowdsourced natural language descriptions, providing richer semantics but limited temporal resolution and no part-level detail. Although these datasets have propelled progress at the intersection of motion understanding and natural language, they share two critical shortcomings: (1) lack of hierarchical structure: existing annotations provide only coarse action or sequence labels without temporally decomposing motions into sub-actions or body-part levels; and (2) absence of body-part granularity: the datasets do not specify which body parts are responsible for each motion, limiting fine-grained understanding. 3.2. LLM-based Annotation Framework To address these limitations, we aim at building motion dataset with structured text annotations that describe the motion at three different granularity levels and are temporally aligned. Specifically, given one motion sequence starts from = 0 and ends at = , we define one basic annotation element using text label that describes the motion segment starts from ts and ends at te, formally: = (L, ts, te). The goal is to obtain structured collection of annotation elements = {As, Aa, Ap} that covers sequence level annotation As, atomic actions Aa, and body part annotations Ap. The sequence level annotation summarizes the full motion sequence in one text hence the annotation is simply one element: As = {(Ls, ts = 0, te = )}. Figure 2. LLM-assisted fine-grained motion annotation pipeline, compared with existing dataset. Given motions with high level action descriptions, we instruct LLM to decompose the actions into part level descriptions and align with corresponding temporal windows. This gives the most important body part text and corresponding motions needed to learn essential motion elements. s, ti = ti1 i=1, with e = T, and ti Atomic actions are list of non-overlapping annotations, where each one summarizes motion segment covering e)}N temporal window: Aa = {(Li, ti = 0, tN . The part annotations, unique in our dataset, contain more fine-grained and structured annotations for body parts: Ap = {Ak}K k=1, here Ak, similar to atomic actions Aa, contains atomic motion annotations but with description specific for body part k. Formally: Ak = {(Lj j=1. Note that annotating every single element in this collection is too expensive and unnecessary. Hence we allow the text labels to be unknown for atomic actions Aa and part annotations Ap, see example annotation in Fig. 2c. e)}M k, tj s, tj Leveraging the powerful body part reasoning capability of LLMs, we instantiate FrankenAgent to construct body part annotations Ap from sequence annotation ˆAs and atomic actions ˆAa provided by existing datasets. To ensure consistency with part annotations, we also further refine the existing sequence and part annotations: Ap, Aa, As = FrankenAgent( ˆAa, ˆAs) (1) We adopt Deepseek-R1 as our primary FrankenAgent due to its strong reasoning ability and robust long-context understanding1. We carefully design prompts to ensure high-quality decomposition. The LLMs are instructed to provide temporally aligned annotations with explicit bodypart coverage, to decompose complex actions into inter1We use Deepseek-R1-0528: https://www.deepseek.com/. Attributes BABEL HumanML3D KITML Ours Annotation Type Sequence Label Atomic Action Label Body-part Label Statistics Dataset Size Vocabulary Total Label Unseen Label Part Label 43.5h 2,162 91.4k 28.6h 6,995 44.9k N/A N/A 11.2h 1,577 6.3k 39.1h 4,117 138.5k 28.8k 46.1k Table 1. Comparison of source motionlanguage datasets and our extended dataset. Our dataset builds upon KIT-ML [38], BABEL [40], and HumanML3D [12], using LLM-based reasoning to produce multi-level, part-aware, and unseen annotations. pretable segments, and to avoid vague expressions by outputting unknown when uncertain. BABEL serves as the primary reference, with KIT-ML and HumanML3D used for augmentation when available. To avoid hallucination, we explicitly instruct the agent to produce unknown for the motion segments that it is unsure. Detailed prompt templates can be found in Supp. 3.3. Comparison with Existing Datasets Based on our pipeline, we obtain large-scale, high-quality dataset that substantially improves over existing datasets, Figure 3. Overview of our FrankenMotion. Our model is transformer-based diffusion model that can be input conditioned on a) sequence level prompt, b) action-level prompt and c) part-level prompt. After training with our paired data of motion and structured multigranularity text annotations, it learns the essential motion elements and how to compose them into complex motions. see comparison in Tab. 1. Our dataset offers two major advantages: (1) Hierarchical: it provides multi-level annotations that span from the overall sequence to individual body parts; and (2) Atomic labels: each body part and atomic actions are annotated. Statistically, our dataset spans 39 hours of motion data and includes three levels of labels with around 15.7k, 31.5k, and 46.1k annotations respectively, forming approximately 16k sequences and 265k atomic motion segments with an average duration of 4.8 seconds. We split the dataset into 80% training, 10% validation, and 10% testing subsets. Beyond human-provided annotations, our system infers 28.8k new annotations through reasoning based on existing ones. FrankenStein offers precise spatialtemporal grounding at the atomic level, making it both challenging and valuable for advancing fine-grained motion understanding and generation. Further details are provided in the Supp. 4. FrankenMotion: Part-Based Spatiotemporal Composition We aim to build model that learns the spatial and temporal relationship between different parts and high level action semantics, allowing complex motion composition and fine-grained input control and editing. In the following, we start with problem setup (Sec. 4.1) and then discuss our network details (Sec. 4.2) followed by robust training strategy Sec. 4.3). An overview can be found in Fig. 3. 4.1. Preliminaries Pose representation. We follow STMC [37] to represent single frame pose Rd using SMPL [27] pose parameters, joint positions, velocities and angular velocities: = [rz, rx, ry, α, θ, j], (2) where rz is the () coordinate of the pelvis, rx and ry are the linear velocities of the pelvis in the xy plane, and α is the angular velocity around the vertical (Z) axis. θ denotes the SMPL pose parameters (encoded using the 6D representation [69]), and are the 3D joint positions computed from the SMPL layer, forming rotation-invariant representation by defining in local coordinate frame aligned with the body. To make θ local to the body, the rotation is removed from the SMPL global orientation. Multi-granularity control. We adopt transformer-based diffusion model as the framework for our text conditioned motion generation. To generate one motion sequence of frames, our model allows text control at three different levels of granularity: 1). Ls = {Ls}: single text description for the full sequence. 2). La = {Lj aj {1...W }}: action labels of different non-overlapping temporal windows for the full sequence. 3). Body part prompts Lp = {Li ki {1, ...T }, {1, ..., K}}: text prompt for body part (among predefined parts) at each frame i. This is the most fine-grained input condition and allows part-based motion composition and editing. At inference time, users can conveniently provide only sequence-level description, specify sparse part-level prompts, or edit existing control signals to guide the generated motion. For the output, we adopt the pose representation defined in Eq. (2) aim at generating sequence of realistic human motion: x[1...T ]. We adopt the sample prediction mode in diffusion models. Specifically, let x[1...T ] be the clean motion sequence and x[1...T ] be the noisy motion at diffusion step σ, our model fθ predicts the clean motion from three levels of text prompts Ls, La, Lp and diffusion timestep σ: σ 0 ˆx[1...T ] = fθ(x[1...T ] σ , σ, Ls, La, Lp) (3) To effectively learn the complex structured information of sequence, action and part level input, we design network that extracts the features from different granularity levels into joint latent space which we discuss next. 4.2. Spatio-temporal Embedding To effectively learn the atomic text-to-motion mapping and composition, we use joint embedding for sequence, action, part-level text and motion, see Fig. 3. Action-part-motion embedding: we use CLIP [41] to extract text features for all input prompts. For action and part labels, we apply PCA to reduce the embedding dimension to = 50, yielding Fa RW and Fp RT (KD). Each action embedding in Fa corresponds to one of the temporal windows.To align these with the per-frame features, we expand each window embedding to its corresponding frame range, producing Fa RT D. Thus, every frame is associated with both detailed part-level and highlevel action text features, where Dm+t denotes the dimension of the fused motiontext feature space. We then concatenate both matrices, leading to matrix of text features Fa+p RN (K+1)D. The combined text feature Fa+p is then further concatenated with the noisy motion input x[1...T ] to form the full conditioning input for the model. σ After passing through an MLP, this fusion produces motiontext embeddings Fa+p+m RT Dm+t. Sequence level context: To incorporate sequence-level context, we encode the sequence text Cs using the CLIP text encoder followed by an MLP, obtaining global feature vector Fs RDm+t. We append this global feature as an additional token to the fused sequence representation. The diffusion timestep embedding, after an MLP projection, is also added as separate token. Together, these yield final input representation of size R(T +2)Dm+t, which jointly encodes motion, part-level, action-level, sequencelevel, and diffusion timestep information. as our body part annotations are sparse. To improve robustness, we introduce random masking with stochastic masking probability for each labelled text condition. We adopt Beta distribution to randomly decide the zero out probability of body part text label Li k: Beta(5r, 5(1 r)), where is the desired masking rate. At each training step, we sample different for body part labels Li that are not unknown. This stochastic masking enhances robustness to incomplete conditioning and improves generalization under sparse supervision [26]. We train our diffusion model fθ Training loss. parametrized by θ using the standard DDPM objective [16]: = x[1...T ] 0 (cid:104) fθ(x[1...T ] σ , σ,ϵ , σ, c) x02 2 (cid:105) (4) where ϵ (0, I) is the diffusion noise and = (Ls, La, Lp) is our hierarchical text condition. Implementation details. Following [36], we employ cosine noise schedule with 100 diffusion steps, as introduced by [6]. We use the AdamW optimizer [28] with learning rate of 2104 and batch size of 32. For text encoding, we adopt the frozen text encoder from CLIP (ViT-B/32) [41]. Our model trains for approximately 47.5 hours on single NVIDIA H100 GPU and each evaluation model trains for around 16 hours on single NVIDIA A100 GPU. 5. Experiments In this section, we first evaluate our newly introduced motion dataset, and then compare our model with baselines for hierarchical text to motion generation. We further ablate the design choices of our model and showcase various applications due to the flexibility of our model design. Results validate the high quality of our data and demonstrate that our model outperforms all previous methods. 5.1. Dataset Quality To evaluate the quality of our FrankenAgent, we conduct human evaluation on the annotated FrankenStein dataset. We randomly sampled 50 motion sequences and asked three human experts to assess whether the generated part, action, and sequence labels are consistent with the corresponding motion segments. Each expert assigns binary correctness score for every label, and we compute the average as the annotation accuracy. The overall accuracy of our FrankenAgent generated annotation is 93.08%, demonstrating the high accuracy of our method. We also report inter-annotator agreement using Gwets AC1 coefficient (AC1) [15] to assess the reliability of human evaluation, where we obtain score of AC1 = 0.91 (0.8 AC1 1.0), indicating high reliability. Please refer to the Supp for more details. 4.3. Robust FrankenMotion Training 5.2. Fine-grained Motion Generation Masking strategy. We set zero to our text feature when text label is unknown. This creates sparse input text condition Our FrankenMotion allows motion generation from hierarchical texts of sequence, atomic action, and body-part laMethod GT STMC DartControl UniMotion Avg-part semantic correctness Per-action semantic correctness Per-seq semantic correctness Per-action realism Per-seq realism R@1 R@3 M2T R@1 R@3 M2T R@1 R@3 M2T FID Div. FID Div. 52.040.16 64.880.18 0.710.00 54.830.12 72.420.11 0.770.00 72.660.19 91.470.17 0.780.00 0.000.00 53.560.05 0.000.00 48.810.04 40.670.29 51.380.30 0.660.00 40.960.61 56.320.66 0.700.00 43.580.35 62.320.45 0.670.00 0.100.00 51.790.10 0.200.00 46.820.17 38.670.70 50.230.46 0.650.00 39.770.77 57.550.46 0.690.00 54.280.60 76.950.68 0.700.00 0.140.00 52.660.21 0.280.00 46.580.03 62.660.30 82.080.35 0.740.00 0.050.00 53.120.23 0.080.00 48.360.03 45.720.24 57.360. 47.580.12 65.620.33 0.750.00 0.690.00 FrankenMotion 47.210.19 58.970. 0.690.00 48.100.13 65.790.14 0.750.00 65.270.22 85.620. 0.760.00 0.040.00 53.820.10 0.060.00 48.600.05 Table 2. Evaluating text to motion generation. We report the semantic correctness and realism of parts (averaged), action and sequence level motion, with 95% confidence interval () after 20 repeated evaluations. Across all settings, our FrankenMotion achieves the best performance, outperforming all prior baselines in both correctness and realism. Figure 4. Qualitative comparison with baselines. Prior methods cannot compose parts into realistic motion (STMC [37]), generates repetitive motions (DART [68]), or do not follow the intricate details like turn around (UniMotion [20]). Our method faithfully composes the complex parts into one realistic motion while also following precisely the detailed body part prompts and high level semantics. bels. To the best of our knowledge, none of the prior methods are able to accomplish this complex task. To be able to compare, we adapt prior methods to include part control and train on our proposed FrankenStein dataset. We outline the baseline setup below and provide the details in Supp. Baselines. We adapt UniMotion [20], STMC [37], and DART [68] to our setup as the baselines. For fair comparison, all methods are modified to use the same pose state representation (Eq. (2)) defined in STMC [37] that includes SMPL [27] pose parameters, velocity, and joint positions. 1). STMC is post-hoc, test-time composition method that stitches body-part motions predicted by MDM [46] at each frame. We retrain the base MDM on all possible motion-text pairs from our FrankenStein dataset to ensure that MDM understands our text labels. At inference time, STMC receives part labels as conditioning and stitches the part motions generated by the retrained MDM. For the frames where no part labels are given, we use atomic action labels or sequence level text. 2). UniMotion is hierarchical model that supports sequence-level and framelevel (atomic action) text conditioning with temporal alignment but lacks body part control. We merge all part and action labels into one long text for each frame as the framelevel text input for UniMotion. 3). DART is an autoregressive model that predicts future motion frames conditioned on motion history and high-level text prompt. We merge our three levels of text into one long text for each frame and train DART with these pairs of text-motion data. Evaluation metrics. Following [20], we evaluate generated motions along two axes: semantic correctness, consisting of R-Precision [12] and M2T [37], and realism, consisting of Frechet Inception Distance (FID) and Diversity [12]. All these metrics require pretrained text to motion generation model to measure the text-motion alignment. To assess the performance in all three levels of control, we train separate evaluation models for each input modality following TMR [36]: seven for each body part, one for actions, and one for full sequences. Each model is trained with paired data of text and corresponding full body motion. For semantic correctness metrics, we follow [36] and use MPNet [45] embeddings to remove false negative pairs due to paraphrased text labels (e.g., hold arm lateral vs. hold arm horizontally). Due to space constraints, we report semantic alignment scores for sequence-, action-, and avMethod Inputs Avg-part semantic correctness Per-action semantic correctness Per-seq semantic correctness Realism Part Atomic Seq. R@3 M2T M2M R@3 M2T M2M R@3 M2T M2M FID Div. GT Ours 64.880.18 0.710.00 1.000.00 72.420.11 0.770.00 1.000.00 91.470.17 0.780.00 1.000.00 0.000.00 46.840. 56.340.42 0.690.00 0.720.00 57.740.42 0.690.00 0.730.00 65.390.17 0.750.00 0.750.00 0.760.00 58.970.18 65.790.14 0.750.00 0.690.00 0.750. 85.620.18 0.760.00 0.750. 0.080.00 46.090.09 0.070.00 46.190.07 46.530.03 0.050.00 Table 3. Importance of hierarchical input condition. We train models that consume input of part only, or additionally with atomic action, or all part, action and sequence level (Seq.) texts. Even with part-level inputs alone, our model attains strong performance, achieving an M2T score close to the upper-bound GT reference. Incorporating action and sequence texts introduces high-level semantics for the desired motion, which further enhances both the correctness and realism of part-level motion generation. eraged part-level evaluations, while realism metrics are reported for all actionand sequence-level crops. Additional numbers for part-level realism can be found in Supp. For all metrics, we repeat the evaluation 20 times and report the 95% confidence interval, indicated by the variance score. Results. We compare our method against baselines quantitatively in Tab. 2 and qualitatively in Fig. 4. STMC [37] follows part instructions thanks to the well trained MDM model, but it struggles to compose different parts into realistic motion. This often leads to unsmooth transitions between atomic actions or fine-grained control, such as turning around, being ignored, as can be seen in Fig. 4 column 2. UniMotion [20] generates more realistic motion due to its frame-level control, as can be seen from the FID score in Tab. 2. However, it lacks an explicit structure of the body part features, leading to less precise text control, and the tiny detailed motions are ignored, as shown in the R-1 score (Tab. 3) and Fig. 4, where it does not turn around. DART primarily follows the sequence level text; however, it cannot control motion precisely at each frame. Due to its autoregressive design, error can accumulate, and it often produces repeated motion segments representing only partial input prompts (see repeated sitting and standing in Fig. 4). In contrast, our model has structured body part conditioning, together with hierarchical control of atomic action and sequence level text. It generates fine grained motions that are controlled precisely by body parts while also maintaining coherence with the high level semantics introduced by atomic action and sequence level text. Notice in Fig. 4 column one how precisely our model follows the text prompt of body parts and actions at every frame. In Tab. 2, our method clearly outperforms all baselines in both motion quality and consistency with the input text. 5.3. Ablation Study We propose model that consumes text at three levels of granularity, as we find that these different texts are important for generating coherent motions. To evaluate this, we train separate models that take only part text, part and atomic action, and all text as input conditions, and we evaluate the motion generation performance in Tab. 3. We also show the scores of our evaluation model, which serve as an upper bound. Notably, the model conditioned only on part texts already achieves competitive scores, demonstrating the strong capability of our model for understanding fine-grained part texts. With additional high level semantics of atomic action and sequence level text, our model generates motions that more precisely follow the part labels. Furthermore, such motions are more meaningful due to the guidance of high level text semantics, illustrating the benefits of our hierarchical design. We show some qualitative examples in Supp. 5.4. Flexible Input Control Applications Thanks to its modular design and the sparse structure of our dataset, our model supports flexible conditioning during inference. Users can control the motion at different granularitiessuch as dominant body part, an action-level phrase, or single sequence-level descriptionallowing adaptive control depending on the available text or user preference. We demonstrate the flexibility of our input in Fig. 1 and Fig. 4. Additional qualitative examples and more application cases are provided in the supplementary video. 6. Conclusion and Limitation We introduced multi-level spatiotemporal motion conditioning, enabling controllable generation at the sequence, atomic action, and atomic part levels. To support this task, we built FrankenStein, fine-grained, temporally aligned dataset with atomic part-level annotations derived via LLM reasoning. Based on it, FrankenMotion learns to compose motion from atomic elements, achieving finegrained and flexible spatialtemporal control. Experiments show that it outperforms adapted baselines and establishes strong foundation for compositional motion generation. Limitation While FrankenMotion enables fine-grained spatiotemporal text-to-motion generation, it cannot yet generate minute-long motion sequences within single pass. Extending its ability to model long-term temporal structure will be an important direction for future work. Acknowledgments: Special thanks RVH and AVG members for the help and discussion. Prof. Gerard Pons-Moll and Prof. Andreas Geiger are members of the Machine Learning Cluster of Excellence, EXC number 2064/1 - Project number 390727645. Gerard Pons-moll is endowed by the Carl Zeiss Foundation. Andreas Geiger was supported by the ERC Starting Grant LEGO-3D (850533)."
        },
        {
            "title": "References",
            "content": "[1] Nikos Athanasiou, Mathis Petrovich, Michael J. Black, and Gul Varol. TEACH: Temporal Action Compositions for 3D Humans. In International Conference on 3D Vision (3DV), 2022. 2 [2] Jinseok Bae, Inwoo Hwang, Young-Yoon Lee, Ziyu Guo, Joseph Liu, Yizhak Ben-Shabat, Young Min Kim, and MubImproving motion diffubasir Kapadia. Less is more: In Proceedings of the sion models with sparse keyframes. IEEE/CVF International Conference on Computer Vision, pages 1106911078, 2025. 2 [3] German Barquero, Sergio Escalera, and Cristina Palmero. Flowmdm: Seamless human motion composition with blended positional encodings. arXiv preprint arXiv:2402.15509, 2024. 2 [4] Jona Braun, Sammy Christen, Muhammed Kocabas, Emre Aksan, and Otmar Hilliges. Physically plausible full-body In 2024 International hand-object interaction synthesis. Conference on 3D Vision (3DV), pages 464473. IEEE, 2024. [5] Ling-Hao Chen, Shunlin Lu, Ailing Zeng, Hao Zhang, Benyou Wang, Ruimao Zhang, and Lei Zhang. Motionllm: Understanding human behaviors from human motions and videos. arxiv:2405.20340, 2024. 3 [6] Ting Chen. On the importance of noise scheduling for diffusion models. arXiv preprint arXiv:2301.10972, 2023. 6 [7] Jungbin Cho, Junwan Kim, Jisoo Kim, Minseo Kim, Mingu Kang, Sungeun Hong, Tae-Hyun Oh, and Youngjae Yu. Discord: Discrete tokens to continuous motion via rectiIn Proceedings of the IEEE/CVF Infied flow decoding. ternational Conference on Computer Vision (ICCV), pages 1460214612, 2025. 2 [8] Wenxun Dai, Ling-Hao Chen, Jingbo Wang, Jinpeng Liu, Bo Dai, and Yansong Tang. Motionlcm: Real-time controllable motion generation via latent consistency model. arXiv preprint arXiv:2404.19759, 2024. 2 [9] Ginger Delmas, Philippe Weinzaepfel, Thomas Lucas, Francesc Moreno-Noguer, and Gregory Rogez. Posescript: 3d human poses from natural language. In European Conference on Computer Vision, pages 346362. Springer, 2022. 3 [10] Qihang Fang, Chengcheng Tang, Bugra Tekin, Shugao Ma, and Yanchao Yang. Humocon: Concept discovery for human motion understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. 3 [11] Yao Feng, Jing Lin, Sai Kumar Dwivedi, Yu Sun, Priyanka Patel, and Michael J. Black. Chatpose: Chatting about 3d human pose. In CVPR, 2024. [12] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generating diverse and natural 3d human motions from text. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 2, 3, 4, 7 [13] Chuan Guo, Yuxuan Mu, Muhammad Gohar Javed, Sen Wang, and Li Cheng. Momask: Generative masked modeling of 3d human motions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19001910, 2024. 2 [14] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [15] Kilem Gwet. Handbook of inter-rater reliability. Gaithersburg, MD: STATAXIS Publishing Company, pages 223246, 2001. 6 [16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. 6 [17] Yiming Huang, Weilin Wan, Yue Yang, Chris CallisonBurch, Mark Yatskar, and Lingjie Liu. Como: Controllable motion generation through language guided pose code editing, 2024. [18] Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen. Motiongpt: Human motion as foreign language. Advances in Neural Information Processing Systems, 36, 2024. 2, 3 [19] Nan Jiang, Zhiyuan Zhang, Hongjie Li, Xiaoxuan Ma, Zan Wang, Yixin Chen, Tengyu Liu, Yixin Zhu, and Siyuan Huang. Scaling up dynamic human-scene interaction modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17371747, 2024. 2 [20] Chuqiao Li, Julian Chibane, Yannan He, Naama Pearl, Andreas Geiger, and Gerard Pons-Moll. Unimotion: Unifying 3d human motion synthesis and understanding. arXiv preprint arXiv:2409.15904, 2024. 2, 3, 7, 8 [21] Jiaman Li, Jiajun Wu, and Karen Liu. Object motion guided human motion synthesis. ACM Transactions on Graphics (TOG), 42(6):111, 2023. 2 [22] Jiefeng Li, Jinkun Cao, Haotian Zhang, Davis Rempe, Jan Kautz, Umar Iqbal, and Ye Yuan. Genmo: Generative models for human motion synthesis. arXiv preprint arXiv:2505.01425, 2025. 2 [23] Xiaojie Li, Ronghui Li, Shukai Fang, Shuzhao Xie, Xiaoyang Guo, Jiaqing Zhou, Junkun Peng, and Zhi Wang. Music-aligned holistic 3d dance generation via hierarchical motion modeling. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1442014430, 2025. [24] Jing Lin, Ailing Zeng, Shunlin Lu, Yuanhao Cai, Ruimao Zhang, Haoqian Wang, and Lei Zhang. Motion-x: largescale 3d expressive whole-body human motion dataset. Advances in Neural Information Processing Systems, 2023. 2 [25] Jing Lin, Yao Feng, Weiyang Liu, and Michael J. Black. ChatHuman: Chatting about 3d humans with tools. In CVPR, 2025. 3 [26] Lei Liu, Yuhao Luo, Xu Shen, Mingzhai Sun, and Bin Li. βdropout: unified dropout. IEEE Access, 7:3614036153, 2019. 6 [27] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black. SMPL: skinned multi-person linear model. ACM Trans. Graphics (Proc. SIGGRAPH Asia), 34, 2015. 5, 7 [28] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [29] Yiyi Ma, Yuanzhi Liang, Xiu Li, Chi Zhang, and Xuelong Li. Intersyn: Interleaved learning for dynamic motion synIn Proceedings of the IEEE/CVF Inthesis in the wild. ternational Conference on Computer Vision (ICCV), pages 1283212841, 2025. 2 [30] Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Gerard Pons-Moll, and Michael J. Black. AMASS: archive of motion capture as surface shapes. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, 2019. 2, 3 [31] Naureen Mahmood, Nima Ghorbani, Nikolaus Troje, Gerard Pons-Moll, and Michael Black. Amass: Archive In Proceedings of of motion capture as surface shapes. the IEEE/CVF international conference on computer vision, pages 54425451, 2019. 2 [32] Zichong Meng, Yiming Xie, Xiaogang Peng, Zeyu Han, and Huaizu Jiang. Rethinking diffusion for text-driven human motion generation. arXiv preprint arXiv:2411.16575, 2024. 2 [33] Jiawei Mo, Yixuan Chen, Rifen Lin, Yongkang Ni, Min Zeng, Xiping Hu, and Min Li. Mochat: Joints-grouped spatio-temporal grounding llm for multi-turn motion comprehension and description, 2024. 3 [34] Liang Pan, Zeshi Yang, Zhiyang Dou, Wenjia Wang, Buzhen Huang, Bo Dai, Taku Komura, and Jingbo Wang. Tokenhsi: Unified synthesis of physical human-scene interIn Proceedings of the actions through task tokenization. Computer Vision and Pattern Recognition Conference, pages 53795391, 2025. [35] Mathis Petrovich, Michael J. Black, and Gul Varol. TEMOS: Generating diverse human motions from textual descriptions. In European Conference on Computer Vision (ECCV), 2022. 2 [36] Mathis Petrovich, Michael J. Black, and Gul Varol. TMR: Text-to-motion retrieval using contrastive 3D human motion synthesis. In International Conference on Computer Vision (ICCV), 2023. 6, 7 [37] Mathis Petrovich, Or Litany, Umar Iqbal, Michael Black, Gul Varol, Xue Bin Peng, and Davis Rempe. Stmc: Multitrack timeline control for text-driven 3d human motion generation. arXiv preprint arXiv:2401.08559, 2024. 3, 5, 7, 8 [38] Matthias Plappert, Christian Mandery, and Tamim Asfour. The kit motion-language dataset. Big data, 4(4):236252, 2016. 2, 3, 4 [39] Abhinanda R. Punnakkal, Arjun Chandrasekaran, Nikos Athanasiou, Alejandra Quiros-Ramirez, and Michael J. Black. BABEL: Bodies, action and behavior with english labels. In Proceedings IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), pages 722731, 2021. 2, 3 [40] Abhinanda Punnakkal, Arjun Chandrasekaran, Nikos Athanasiou, Alejandra Quiros-Ramirez, and Michael Black. Babel: Bodies, action and behavior with english labels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 722731, 2021. [41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 6 [42] Pablo Ruiz-Ponce, German Barquero, Cristina Palmero, Sergio Escalera, and Jose Garcıa-Rodrıguez. in2in: Leveraging individual information to generate human interactions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19411951, 2024. 2 [43] Pablo Ruiz-Ponce, German Barquero, Cristina Palmero, Sergio Escalera, and Jose Garcıa-Rodrıguez. Mixermdm: Learnable composition of human motion diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1238012390, 2025. 2 [44] Yoni Shafir, Guy Tevet, Roy Kapon, and Amit Haim Bermano. Priormdm: Human motion diffusion as generative prior. In ICLR, 2023. 2 [45] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mpnet: Masked and permuted pre-training for language understanding. Advances in neural information processing systems, 33:1685716867, 2020. 7 [46] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel Cohen-or, and Amit Haim Bermano. Human motion diffuIn The Eleventh International Conference on sion model. Learning Representations, 2023. 2, 7 [47] Yuan Wang, Yali Li, Xiang Li, and Shengjin Wang. Hsi-gpt: general-purpose large scene-motion-language model for human scene interaction. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 71477157, 2025. [48] Yabiao Wang, Shuo Wang, Jiangning Zhang, Ke Fan, Jiafu Wu, Zhucun Xue, and Yong Liu. Timotion: Temporal and interactive framework for efficient human-human moIn Proceedings of the IEEE/CVF Confertion generation. ence on Computer Vision and Pattern Recognition (CVPR), pages 71697178, 2025. 2 [49] Zan Wang, Yixin Chen, Baoxiong Jia, Puhao Li, Jinlu Zhang, Jingze Zhang, Tengyu Liu, Yixin Zhu, Wei Liang, and Siyuan Huang. Move as you say interact as you can: Language-guided human motion generation with scene affordance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 433444, 2024. 2 [50] Zhen Wu, Jiaman Li, Pei Xu, and Karen Liu. HumanIn Prothe IEEE/CVF International Conference on object interaction from human-level instructions. ceedings of Computer Vision, pages 1117611186, 2025. 2 [63] Xiaohan Zhang, Bharat Lal Bhatnagar, Sebastian Starke, Vladimir Guzov, and Gerard Pons-Moll. Couch: Towards controllable human-chair interactions. In European Conference on Computer Vision (ECCV), 2022. 2 [64] Xiaohan Zhang, Bharat Lal Bhatnagar, Sebastian Starke, Ilya Petrov, Vladimir Guzov, Helisa Dhamo, Eduardo PerezPellitero, and Gerard Pons-Moll. Force: Dataset and method for intuitive physics guided human-object interaction. CoRR, 2024. [65] Xiangyue Zhang, Jianfang Li, Jiaxu Zhang, Ziqiang Dang, Jianqiang Ren, Liefeng Bo, and Zhigang Tu. Semtalk: Holistic co-speech motion generation with frame-level semantic In Proceedings of the IEEE/CVF International emphasis. Conference on Computer Vision, pages 1376113771, 2025. 2 [66] Yaqi Zhang, Di Huang, Bin Liu, Shixiang Tang, Yan Lu, Lu Chen, Lei Bai, Qi Chu, Nenghai Yu, and Wanli Ouyang. Motiongpt: Finetuned llms are general-purpose motion generators. arXiv preprint arXiv:2306.10900, 2023. 2 [67] Zeyu Zhang, Akide Liu, Ian Reid, Richard Hartley, Bohan Zhuang, and Hao Tang. Motion mamba: Efficient and long sequence motion generation with hierarchical and bidirectional selective ssm. arXiv preprint arXiv:2403.07487, 2024. 2 [68] Kaifeng Zhao, Gen Li, and Siyu Tang. DartControl: diffusion-based autoregressive motion model for real-time text-driven motion control. In The Thirteenth International Conference on Learning Representations (ICLR), 2025. 2, 3, 7 [69] Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao Li. On the continuity of rotation representations in neural In Proceedings of the IEEE/CVF conference on networks. computer vision and pattern recognition, pages 57455753, 2019. 5 [70] Zixiang Zhou, Yu Wan, and Baoyuan Wang. Avatargpt: Allin-one framework for motion understanding, planning, generation and beyond. arXiv preprint arXiv:2311.16468, 2023. [71] Wentao Zhu, Xiaoxuan Ma, Dongwoo Ro, Hai Ci, Jinlu Zhang, Jiaxin Shi, Feng Gao, Qi Tian, and Yizhou Wang. IEEE Transactions Human motion generation: survey. on Pattern Analysis and Machine Intelligence, 46(4):2430 2449, 2023. 2 [51] Liang Xu, Shaoyang Hua, Zili Lin, Yifan Liu, Feipeng Ma, Yichao Yan, Xin Jin, Xiaokang Yang, and Wenjun Zeng. Motionbank: large-scale video motion benchmark arXiv preprint with disentangled rule-based annotations. arXiv:2410.13790, 2024. 3 [52] Sirui Xu, Zhengyuan Li, Yu-Xiong Wang, and Liang-Yan Gui. Interdiff: Generating 3d human-object interactions with physics-informed diffusion. In ICCV, 2023. 2 [53] Sirui Xu, Yu-Xiong Wang, Liangyan Gui, et al. Interdreamer: Zero-shot text to 3d dynamic human-object interaction. Advances in Neural Information Processing Systems, 37:5285852890, 2024. 2 [54] Sirui Xu, Dongting Li, Yucheng Zhang, Xiyan Xu, Qi Long, Ziyin Wang, Yunzhi Lu, Shuchang Dong, Hezi Jiang, Akshat Interact: Advancing large-scale versatile 3d Gupta, et al. human-object interaction generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 70487060, 2025. [55] Sirui Xu, Hung Yu Ling, Yu-Xiong Wang, and Liang-Yan Gui. Intermimic: Towards universal whole-body control for physics-based human-object interactions. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1226612277, 2025. [56] Payam Jome Yazdian, Eric Liu, Rachel Lagasse, Hamid Mohammadi, Li Cheng, and Angelica Lim. Motionscript: Natural language descriptions for expressive 3d human motions. arXiv preprint arXiv:2312.12634, 2024. 3 [57] Ling-An Zeng, Guohong Huang, Yi-Lin Wei, Shengbo Gu, Yu-Ming Tang, Jingke Meng, and Wei-Shi Zheng. Chainhoi: Joint-based kinematic chain modeling for human-object In Proceedings of the IEEE/CVF interaction generation. Conference on Computer Vision and Pattern Recognition (CVPR), pages 1235812369, 2025. 2 [58] Ling-An Zeng, Guohong Huang, Yi-Lin Wei, Shengbo Gu, Yu-Ming Tang, Jingke Meng, and Wei-Shi Zheng. Chainhoi: Joint-based kinematic chain modeling for human-object interaction generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1235812369, 2025. [59] Juze Zhang, Jingyan Zhang, Zining Song, Zhanhe Shi, Chengfeng Zhao, Ye Shi, Jingyi Yu, Lan Xu, and Jingya Wang. Hoi-mˆ 3: Capture multiple humans and objects interaction within contextual environment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 516526, 2024. 2 [60] Jianrong Zhang, Hehe Fan, and Yi Yang. Energymogen: Compositional human motion generation with energyIn Proceedings of based diffusion model in latent space. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1759217602, 2025. [61] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondiffuse: Text-driven human motion generation with diffusion model. arXiv preprint arXiv:2208.15001, 2022. 2 [62] Mingyuan Zhang, Huirong Li, Zhongang Cai, Jiawei Ren, Lei Yang, and Ziwei Liu. Finemogen: Fine-grained spatiotemporal motion generation and editing. NeurIPS, 36, 2024. 2,"
        }
    ],
    "affiliations": [
        "Max Planck Institute for Informatics, Saarland Informatics Campus, Germany",
        "Tubingen AI Center, University of Tubingen, Germany"
    ]
}