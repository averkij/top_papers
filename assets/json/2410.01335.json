{
    "paper_title": "Layer Swapping for Zero-Shot Cross-Lingual Transfer in Large Language Models",
    "authors": [
        "Lucas Bandarkar",
        "Benjamin Muller",
        "Pritish Yuvraj",
        "Rui Hou",
        "Nayan Singhal",
        "Hongjiang Lv",
        "Bing Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Model merging, such as model souping, is the practice of combining different models with the same architecture together without further training. In this work, we present a model merging methodology that addresses the difficulty of fine-tuning Large Language Models (LLMs) for target tasks in non-English languages, where task-specific data is often unavailable. We focus on mathematical reasoning and without in-language math data, facilitate cross-lingual transfer by composing language and math capabilities. Starting from the same pretrained model, we fine-tune separate \"experts\" on math instruction data in English and on generic instruction data in the target language. We then replace the top and bottom transformer layers of the math expert directly with layers from the language expert, which consequently enhances math performance in the target language. The resulting merged models outperform the individual experts and other merging methods on the math benchmark, MGSM, by 10% across four major languages where math instruction data is scarce. In addition, this layer swapping is simple, inexpensive, and intuitive, as it is based on an interpretative analysis of the most important parameter changes during the fine-tuning of each expert. The ability to successfully re-compose LLMs for cross-lingual transfer in this manner opens up future possibilities to combine model expertise, create modular solutions, and transfer reasoning capabilities across languages all post hoc."
        },
        {
            "title": "Start",
            "content": "LAYER SWAPPING FOR ZERO-SHOT CROSS-LINGUAL TRANSFER IN LARGE LANGUAGE MODELS Lucas Bandarkar Benjamin Muller Rui Hou Nayan Singhal Hongjiang Lv Pritish Yuvraj Bing Liu 4 2 0 2 ] . [ 1 5 3 3 1 0 . 0 1 4 2 : r Meta GenAI University of California, Los Angeles"
        },
        {
            "title": "ABSTRACT",
            "content": "Model merging, such as model souping, is the practice of combining different models with the same architecture together without further training. In this work, we present model merging methodology that addresses the difficulty of finetuning Large Language Models (LLMs) for target tasks in non-English languages, where task-specific data is often unavailable. We focus on mathematical reasoning and without in-language math data, facilitate cross-lingual transfer by composing language and math capabilities. Starting from the same pretrained model, we finetune separate experts on math instruction data in English and on generic instruction data in the target language. We then replace the top and bottom transformer layers of the math expert directly with layers from the language expert, which consequently enhances math performance in the target language. The resulting merged models outperform the individual experts and other merging methods on the math benchmark, MGSM, by 10% across four major languages where math instruction data is scarce. In addition, this layer swapping is simple, inexpensive, and intuitive, as it is based on an interpretative analysis of the most important parameter changes during the fine-tuning of each expert. The ability to successfully re-compose LLMs for cross-lingual transfer in this manner opens up future possibilities to combine model expertise, create modular solutions, and transfer reasoning capabilities across languages all post hoc."
        },
        {
            "title": "INTRODUCTION",
            "content": "Instruction fine-tuning Large Language Models (LLMs) is necessary to customize pre-trained models for real-world applications. This fine-tuning is especially critical in multilingual settings because most popular open-source LLMs have been pretrained on highly English-centric data (Llama et al., 2024; Yang et al., 2024a; Jiang et al., 2023). While recent LLMs like LLAMA 3 and QWEN2 have seen over trillion non-English tokens due to the sheer scale of their pretraining datasets, these tokens are still heavily concentrated in just few languages, resulting in limited capabilities for most other non-English languages (Llama et al., 2024; Yang et al., 2024a). Furthermore, the scarcity of high-quality labeled data available for post-training in non-English languages and the tremendous cost of procuring it further exacerbates the inequality. Even machine-translating English posttraining data to target languagesa typical solutionhas significant computational overhead and often leads to datasets of unreliable quality (Khanuja et al., 2024). Numerous efforts have set out to annotate or assemble high-quality datasets for lower-resource languages (Khan et al., 2024; Singh et al., 2024b; Tonja et al., 2024), but massive gap still exists in many tasks and domains, such as math. As result, developers are forced to largely rely on cross-lingual transferthe generalization of learned capacities from high-resource languages to lower onesbut the rate of such transfer is low for most languages (Philippy et al., 2023). Correspondence to lucasbandarkar@cs.ucla.edu 1 Figure 1: Our merging method which swaps in top and bottom transformer layers from language expert into math expert, buffered by transition zone. In this paper, we present novel solution that merges two LLMs together in order to transfer math reasoning capabilities to lower-resource languages during supervised fine-tuning (SFT). In the absence of in-language math data, we fine-tune two variants of the same pretrained model: one with English math samples and the other with generic instruction data in the target language. Provided these variants, which we refer to as experts, we combine their learned language and task capabilities by re-composing an LLM with mix of parameters from each. Notably, the top and bottom few transformer layers are selected from the language expert and the middle transformer layers are selected from the math expert. In addition, we build 1or 2-layer transition zones in between, which consists of weighted averages of the parameter values of both. This simple, yet strategic, recomposition leads to high performance in math in the target language and avoids negative interference. The intuition behind this layer swapping method is informed by our analysis of the SFT updates that led to the experts. We find that the learned math capabilities are concentrated in the middle transformer layers, especially in the second half of the model. Meanwhile, we find the enhanced language skills to have come from parameters closest to the input and output, in line with previous literature stating this is where the most language-specific representations are concentrated (Chang et al., 2022; Choenni et al., 2024). Therefore, our methodology takes the most important layers from each expert and transfers math capabilities to the target language. The resulting layer-swapped models deliver strong performance across numerous target languages (Swahili, Telugu, Bengali, Japanese) on MGSM (Shi et al., 2023), the manually-translated version of the Grade School Math benchmark (Cobbe et al., 2021). In all these major languages, where math SFT data is not readily available, layer swapping outperforms baselines, the individual experts and model souping (Wortsman et al., 2022) by 10% on average. For Swahili, layer swapping exceeds MGSM performance of models on fine-tuned mixed Swahili and math SFT dataset when evaluated. Therefore, this methodology provides simple and effective way to improve the capabilities of LLMs without the need for any task-specific data in the target language. In addition, it is inexpensive and fully post hoc, meaning it has the potential to be practical in many settings. Fundamentally, the success of this method also provides empirical evidence for cross-lingual patterns in the latent structures of LLMs that can be further interpreted and exploited. We discuss relevant literature in the following section. We then explain the analysis that led to our methodology and present layer swapping in detail in Sections 3 and 4. Next, we show empirical results and discuss findings in Sections 5 and 6. Finally, we propose future work prompted by the success of this methodology in Section 7."
        },
        {
            "title": "2.1 MODEL MERGING",
            "content": "While the use of weight averaging to stabilize training through noise reduction well pre-dates instability challenges in deep neural networks (Breiman, 1996), the use of model merging to combine trained model checkpoints is an emerging research space in deep learning. Similar to ensembling model outputs (Dietterich, 2000), aggregating model weights also improves model robustness and generalization Izmailov et al. (2018), even when combining models trained on the same data (Rame et al., 2022). Numerous studies seek to improve the pre-merging conditions, either via linearizing fine-tuning (Ortiz-Jimenez et al., 2023) or aligning weights (Ainsworth et al., 2023). Wortsman et al. (2022) develops an empirical method to average, or soup, subset of available model checkpoints together, in order to increase the model search space during hyperparameter tuning. Simple weight averaging, however, is at risk of negative transfer, or interference, between trained checkpoints. To address this, methods have been proposed to selectively combine models at the individual weight level, either using the magnitude and direction of the fine-tuning deltas (Ilharco et al., 2023; Yadav et al., 2023; Davari & Belilovsky, 2024; Yu et al., 2024) or leveraging information theory (Matena & Raffel, 2022). In parallel, sparse fine-tuning methods have been developed to create fine-tuned models with small proportion of weights changed (Guo et al., 2021; Sung et al., 2021; Xu et al., 2021), which then allows adding together fine-tuning updates with less overlap. Overall, model merging, and notably inexpensive model souping, is very common in practice because it improves training stability, model robustness and generalization, and performance by increasing the search space or combining expertise in multi-task settings (Yang et al., 2024b). 2.2 LLM MULTILINGUALITY The inability for language model to learn more languages without undermining other capabilitiesthe curse of multilinguality (Conneau et al., 2020; Pfeiffer et al., 2022)was heavily studied problem in encoder models. Numerous investigations attempted to understand the quantity and location of parameter sharing versus language-specific parameters (Wang et al., 2020; Choenni et al., 2023) in massively multilingual encoder models (e.g. mBERT (Devlin et al., 2019)). While higher rates of language-specific parameters were found in the top and bottom layers, largely because of the proximity to token representations (Chang et al., 2022; Choenni et al., 2024), language specialization occurs across the model. In addition, increasing the model vocabulary has proven effective in increasing models language capacity (Zheng et al., 2021; Liang et al., 2023; Llama et al., 2024). And while recently model scaling has mitigated the limitation from parameter quantity, the massive amount of labeled data needed for LLM training presents new challenge. In encoder models, crosslingual transfer was enhanced by aligning cross-lingual representations (Patra et al., 2023; Ouyang et al., 2021; Gaschi et al., 2023), but this is difficult in decoder-only models. To boost transfer in post-training, several data synthesis or augmentation solutions have been proposed, both for SFT (Zhang et al., 2023b; Qin et al., 2023; Chai et al., 2024) or reinforcement learning from human feedback (RLHF) (Dang et al., 2024; She et al., 2024; Lai et al., 2024). 2.3 MODEL MERGING FOR CROSS-LINGUAL TRANSFER To address the limited representational capacity of multilingual models, many solutions have been proposed to strategically share or splits parts of the model. This could be major blocks, as in mixtureof-experts (Fedus et al., 2022; NLLB et al., 2022), or few parameters in each layer, as in crosslingual adapters (Pfeiffer et al., 2020; 2022). Ansell et al. (2022) combines modular and sparse fine-tuning approaches in two-stage SFT, where first task and language experts are fine-tuned and all weights who did not change more than threshold are masked. Next, the experts are fine-tuned again from the pretrained model with this mask, creating sparse task vectors that can be composed together with lower rates of parameter overlap. This composable sparse fine-tuning was also adapted for larger decoder-only LLMs (Ansell et al., 2024). In this work, we develop simpler and more flexible merging method for cross-lingual transfer that does not require fine-tuning more than once."
        },
        {
            "title": "3.1 SETUP",
            "content": "We start by training numerous math and language experts by fine-tuning LLAMA 3.1 8B (Llama et al., 2024). We perform SFT runs with 30-40k labeled samples with varying hyperparameters1 and for each type of expert, select the three best checkpoints. The math experts were fine-tuned on English math world problems from the Orca-Math synthetic dataset (Mitra et al., 2024). In order to select the three experts, we use results on the English splits of MGSM, as well as the average across languages. For the language experts, we select Swahili, Bengali, Telugu, and Japanese as the target languages. These are languages present in MGSM and other benchmarks discussed below, but where LLAMA 3.1s performance lags behind the top languages like Spanish. In addition, boosting cross-lingual transfer to these languages is important because of the lack of in-language math instruction data. For each of the four, we mix together samples from available instruction datasets in that language, to create experts with enhanced general language and instruction-following capabilities. The resulting datasets contain many different types of taskssuch as translation, NER, and question-answeringbut no math2. After numerous SFT runs on these generic, multi-task datasets, the three checkpoints for each language are primarily selected based off their performance on the target language splits on BELEBELE (Bandarkar et al., 2024) and FLORES (NLLB et al., 2022). These simple tasks, reading comprehension and translation, are strong indicators of basic language understanding and generation capabilities. We, however, also ensure slight improvement on MBPP (Austin et al., 2021), MMLU (Hendrycks et al., 2021), and MGSM as secondary measures of language improvement. 3.2 PARAMETER ANALYSIS OF SFT (2A) Japanese expert #1 (2B) Math expert #1 Figure 2: This visualization displays the location with more significant magnitude of change during SFT for two representative experts. In detail, this shows the percentage of rows for each 2dimensional parameter in the 32 transformer layers of LLAMA 3.1 8B where the mean absolute value is above threshold (1.9 105 and 1.0 105, respectively). The darker the green represents parameters changing more significantly, relative to the others. Larger versions of these images, as well as for more experts, can be found in Appendix A.4 1Training specifics such as hyperparameters are provided in Appendix A.2 2Dataset details are provided in Appendix A. 4 For these experts, we first investigate where the parameters are being updated the most and where they remain unchanged during fine-tuning. Using similar notation to Ilharco et al. (2023), let θpre, θf be the set of weights for the pretrained model and fine-tuned expert, respectively. We generalize the delta of fine-tuning as = Wf Wpre for all oneor two-dimensional parameters θ. To compare model parameter deltas across tensors with different shapes, we represent the magnitude of change at the row level using mean absolute value (MAV) of the difference (for one-dimensional W, this is simply the MAV across the vector). We observe high consistency in the patterns of these magnitudes across our math experts, and separately all our language experts, despite the latter being trained on different datasets and mixes of tasks. In Fig. 2, we show visualization for representative language expert and representative math expert. For the language expert, we find that the attention parameters (left four columns in the visualization) are getting updated most significantly, notably in the first couple layers (bottom) and the last couple layers (top). The feed-forward layers (right three columns) do not change at all until the last few layers. In comparison, the math experts follow different patterns. In these runs, the first half (bottom 16 layers) remain largely unchanged. In this second half, the attention parameters are being updated significantly, similar to language experts, except the feed-forward layers are also getting changed quite bit. To demonstrate the consistency across training runs and languages, we show the same visualization for more experts in Appendix A.4. 3.3 SPARSIFYING UPDATES We then attempt to create sparse model updates for increased composability similar to Ansell et al. (2022), but without retraining. We attempted leveraging the magnitude of the deltas to selectively update the model without undermining performance gains. Concretely, we use thresholds to determine whether to apply the update or revert the value to the original value in the pretrained LLAMA 3.1. This is done at row-level granularity, in order to not partially modify linear transformations. However, in our analysis across math experts, we find that more than 70-80% of model parameters are required to be updated for the increase in performance to remain equivalent. Such small degree of sparsification would not significantly reduce interference between math and language experts. We next attempted location-based sparsification. This means leveraging our intuition of what patterns occur during fine-tuning to select specific parameter tensors to merge or not. breakthrough was made when we revert the first five and last two transformer layers of our math experts to their original values. After this mitigation, we find that the math experts perform equally well. This is significant because our intuitionbased on our analysis and previous multilingual interpretability research on both encoder-decoder models (Chang et al., 2022; Choenni et al., 2024) and decoderonly models (Tang et al., 2024; Zhang et al., 2024b)is that the first few and last few transformer layers contain the most important language-specific parameters. Meanwhile, our discovery implies the first few and last few transformer layers were not very important for the math experts. This contrast prompted the swapping of these layers from the language expert into the math expert."
        },
        {
            "title": "4 METHODOLOGY",
            "content": "4.1 LAYER SWAPPING The layer swapping methodology takes two fine-tuned experts, one on the target language and the other on the target task in English, and re-composes joint model with the top and bottom transformer layers from the language expert and the middle from the math expert. As displayed in Fig. 1, we additionally design transition zone in between the off-the-shelf layers from each expert. These buffer layers are weighted averages of the respective layers from each expert that ensure that the outputs of one layer does not directly input into layer fine-tuned separately. While intuitively these transition zones seem necessary, we do not find empirical evidence that they provide statistically significant benefit over replacing them with the math experts layers. However, as discussed in Section 3.1, our experts were not fine-tuned for very long and therefore the latent representation spaces after each layer would not have diverged very much from each other. This explains, in theory, why transition zones were not helpful in our setting. We conjecture that if the experts had been trained further (or simply with higher learning rates), such buffer zone would be necessary. We therefore still present it as central component of our layer swapping methodology. 5 The implementation of layer swapping is as simple as iterating through the state dictionary of the math expert and for each parameter, either: (1) keeping it as is, (2) replacing its value with that of the language expert, or (3) averaging that of the math and language expert (See Algorithm 1). Algorithm 1 Layer Swapping Input: task expert θtask, language expert θlang, lower layers to swap b, upper layers to swap u, lower transition layers tb, upper transition layers tu, weight of each expert wtask, wlang, number of model layers Output: Merged model θmerged layer number of n, N/A if not attention or feedforward parameters if < or > 1 then θmerged{n} θlang{n} else if > 1 + tb or < tu then 1: for parameter name in models parameters do 2: 3: 4: 5: 6: 7: 8: end if 9: 10: end for 11: Return θmerged θmerged{n} θtask{n} else θmerged{n} (wtask θtask{n} + wlang θlang{n})/(wtask + wlang) 4.2 CONFIGURATION Layer swapping has several components that can be configured in various ways. Most notably, the number of layers to swap at the top and bottom and the number of layers to include in the respective transition zones. We tested how to configure these swapped layers and transition zones to most effectively merge these models, empirically. We find, however, that there is wide range of possible configurations in which this methodology is still very effective. Note that all these experiments were conducted on 32-layer LLAMA 3.1 transformer model. For model of this size, we find that the desired configuration of each component is in the ranges listed below, with our default values underlined: 1. The number of bottom layers to swap {3, 4, 5} 2. The number of top layers to swap {0, 1, 2} 3. The number of layers in the lower transition zone tb {0, 1, 2} 4. The number of layers in the upper transition zone tu {0, 1, 2} 5. The transition zones are averages of the layers from both (i.e. soups) that can be unweighted or magnitude-adjusted weighted averages. 6. The non-transformer parameters (input token embeddings, output layers, etc) work best if they are also averages of the two experts, as opposed to simply from the language expert. While the results were largely equivalent within this range, some patterns did exist. We find that for languages where performance was lower (e.g. Telugu), the higher performing configurations typically had more layers from language expert. This is perhaps because the boost from improving the language capabilities was more important relative to languages LLAMA 3.1 was already better at. For example for languages such as Japanese, the best configuration has close to the minimum layers from language expert, similar to the minimum swap illustrated in Fig. 3. Amongst the math experts, the math expert with the highest scores on all four target languages (named math expert #2) performed understandably better with fewer layers from the language expert swapped in. Generally, we conjecture that lower-resource languages tend to require more layers from the language expert. 6 (3A) Possible config. with more layers swapped in. (3B) Possible config. with less layers swapped in. Figure 3: The comparison of the maximum (left) and minimum (right) swapping setups that we find effective empirically. Note on the right, there are no upper layers directly from the language expert."
        },
        {
            "title": "5 EMPIRICAL RESULTS",
            "content": "5.1 EXPERIMENTS ON SWAHILI, TELUGU, JAPANESE, AND BENGALI As discussed in Section 3.1, we launched SFT runs with different hyperparameters and on five different datasets: math data in English and generic instruction data in Bengali, Swahili, Telugu, and Japanese. For each dataset, we selected three checkpoints, striking balance between top performance and sufficient diversity to ensure robust experimental results. For each language, the three language experts could pair with any of the three math experts which gives nine merged models to evaluate. To understand the effectiveness of layer swapping, we present the MGSM scores of the base model, LLAMA 3.1, and the individual experts selected to determine whether layer swapping constructively combines the learned task and capabilities. In addition, we present an additional baseline; the classic model soup (Wortsman et al., 2022). Model souping is the most common model merging method in practice, used in preand post-training for purposes such as boosting model robustness and generalization but also to combine capabilities of experts (Yu et al., 2024). In vanilla model souping, the merged parameters are simple averages of the parameters of the input experts. For each language, we additionally evaluate adjusting for the magnitude of change from the base model to the expert using weighted average. Using notation from Section 3.2, the resulting weight of each expert is the inverse of the average MAV for all rows in all for that expert. For all model soup results presented, we select the highest MGSM score amongst these possible configurations. Given the number of experts and merged pairs in our evaluations, we present two aggregate metrics: the mean and the maximum of evaluations. While the mean demonstrates the consistency of each method, the maximum demonstrates the ceiling of the method, often more desirable given the ability to iterate through numerous settings during model training. As displayed in Table 1, we find that layer swapping consistently outperforms these baselines on MGSM. For Swahili and Telugu, the default layer swapping configuration scores higher than both the individual experts across all 9 pairs. This provides tremendous evidence that this merging method prevents negative interference. For Bengali, where the math expert already performs significantly better than for Swahili and Telugu, numerous layer-swapping configurations consistently surpass the math experts average score. In addition, the best layer swapped model performs higher than the best math expert on its own. For Japanese, we find here lower average performance compared to the individual math experts. However, we note that our Japanese experts were perhaps the weakest; the increase in performance across BELEBELE, FLORES, MBPP, and MMLU after SFT were minimal and with the data and hyperparameters tested, we were unable to do better. Furthermore, MGSM performance of both the base LLAMA 3.1 and the math experts were already decent in Japanese, prior to merging. In comparison to model souping, layer swapping consistently outperforms it in terms of both maximum and average performance in all languages. Model soup results typically lie in between the individual results of the experts. 7 Table 1: MGSM 8-shot results of layer swapping across four languages compared to the individual experts and model souping. Note that for aggregate statistics of the individual SFT runs, we select the 3 best checkpoints from numerous training runs with periodic checkpointing. The merging methods are aggregated over the 9 pairs (3 language experts 3 math experts), which means the min, avg, and max measures are not perfectly comparable. Setting Details LLAMA 3.1 8B language expert math expert model soup layer swap layer swap top 3 training runs top 3 training runs best config, 9 pairs default config, 9 pairs best config, 9 pairs Swahili avg Swahili max Telugu avg Telugu max Bengali avg Bengali max Japanese avg Japanese max 24.8 24.8 12.0 12.0 29.2 29.2 33.6 33.6 24.7 25.6 20.0 22.4 33.5 35.2 35.9 36.8 29.5 32.8 20.1 24.0 38.3 44.4 42.7 44.8 29.3 32.0 20.9 26.4 36.8 38.4 38.7 40. 32.4 36.4 22.7 27.6 37.1 40.4 38.5 40.8 32.8 37.2 23.0 27.6 38.7 45.2 40.1 43.2 We further evaluated these layer-swapped models on BELEBELE, FLORES, MBPP, and MMLU to ensure no over-optimization to this benchmark and find that the results are on par, usually even higher, than the base model and the individual experts. In addition, we manually checked small number of text generations to further ensure the quality of the resulting model. In addition, we also evaluate combining layer swapping with model souping given that there are several language and math experts. We experiment souping with souping the three experts for each of the languages before layer swapping with soup of the three math experts and present results in Table 5 in the Appendix. We find that these language and math soups perform on par with the individual pairs when layer swapped together. This feasibility in this settings shows that layer swapping can further extend the search space of fine-tuned models. Given these results, we conjecture that perhaps layer swapping provides the biggest benefit for lower-resource languages (e.g. Swahili, Telugu), although such conclusion would require evaluating more than four languages. This conjecture would be explained by the difficulty to improve language capabilities with low-data SFT when LLAMA 3.1 was already pretrained on more tokens in that language. Additionally, for higher-resource languages, cross-lingual transfer occurs more naturally when fine-tuning on English math data. 5.2 FURTHER EVALUATIONS ON SWAHILI Given that here,model souping appears to be susceptible to negative interference when merging, we additionally evaluate TIES-merging (Yadav et al., 2023) for Swahili. This merging method resolves interference between experts by sequentially trimming conflicting values, determining the sign of In addition, we present an alternative for each weight, and then merging the resulting weights. Swahili where instead of post hoc model merging, we mix the language and math datasets and do single fine-tuning run with samples from both. Instead of training two experts, each on 30-40k samples, the joint SFT is over the union (80k samples). Identical to the training of experts, we launch many SFT runs with different hyperparameters and select the best three checkpoints. Table 2 displays these more detailed results for Swahili. We find that TIES-merging was similarly performing as model souping in our setup, which means consistently worse than layer swapping. For the Swahili and math data mixing prior to fine-tuning, we are able to achieve comparable results to layer swapping on average, with the average MGSM score for this joint SFT being less than one point lower. However, the maximum performance of these checkpoints lagged the best layerswapped pairs by 4.4 percentage points. This means that the ceiling for cross-lingual transfer is significantly higher with this methodology than simply mixing datasets together. This is significant because our method of merging two variants fine-tuned on separate datasets proves more effective than an extended fine-tuning on the combined datasets. 8 Table 2: MGSM 8-shot results of layer swapping for Swahili in more detail and with two additional comparisons, TIES-merging and dataset merging. We display the minimum performance in Swahili, as well as the average across all 9 languages in MGSM and in English. Setting Details LLAMA 3.1 8B Swahili expert math expert swh&math joint SFT model soup TIESmerging layer swap layer swap top 3 training runs top 3 training runs top 3 training runs best config, 9 pairs best config, 9 pairs default config, 9 pairs best config, 9 pairs Swahili min Swahili avg Swahili max English avg All langs avg 24.8 24.8 24.8 56.0 37.7 23.6 24.7 25.6 55.7 37.5 27.2 29.5 32.8 66.2 45.4 31.6 32.1 32.8 64.3 46.0 25.6 29.3 32.0 62.0 43. 25.2 29.5 32.4 60.1 41.8 29.6 32.4 36.4 64.7 44.1 29.2 32.8 37.2 64.4 44.4 We note that by swapping in layers from the Swahili expert, MGSM performance in English and other non-Swahili languages decreases from the math expert. This is expected given we optimize for different language, yet the decrease is relatively small nonetheless. For comparison, we see that performance drops much more for the other merging methods on these non-target languages."
        },
        {
            "title": "6 DISCUSSION",
            "content": "Layer swapping is simple, yet effective method for merging together models for cross-lingual transfer. The success of this post hoc method prompts numerous key insights. In settings where in-language task data is unavailable or rare, such as labeled math samples in Telugu, layer swapping provides very effective way to create capable model with simply English task data and general target language data. For lower-resource languages, such constrained scenario is extremely common in practiceespecially when it comes to instruction fine-tuning data. All other baselines we evaluated in such scenario were not able to combine language and math capabilities as consistently and as effectively, as discussed in Section 5. Model souping is popular because of its flexibility, convenience, and ability to expand model search In this regard, layer swapping provides more effective alternative for the multilingual space. setting with the same advantages. This method can be implemented fully post hoc between any number of checkpoints using simple parameter arithmetic, with no further fine-tuning. Because of how inexpensive it is to implement, it enables the ability to quickly iterate through and test many configurations (e.g. the number of swapped transformer layers). Similar to souping, the simplicity of this method means that it has the potential to work at any stage of training (pretraining, finetuning, preference tuning, etc.). In multi-task fine-tuning scenarios, this could allow for non-English capabilities being boosted on the side, separately from other reasoning capabilitiessuch as math, multi-hop reasoning, coding, safety, etc.and then models are merged back together post hoc to combine the learned skills. The effectiveness of this method indicates that there are meaningful multilingual patterns occurring in these LLMs that researchers can uncover and exploit. Potentially, we can reinterpret the functionality of the top and bottom transformer layers as, to certain degree, natural language interfaces to broader model intelligence. Accordingly, this can potentially give way to newer generation of modular solutions for multilingualism in decoder-only LLMs, similar to the MoE or adapter solutions that were designed for encoder models (Pfeiffer et al., 2020; 2022; Liu et al., 2023). Regardless, this work suggests that further interpretability research into LLM multilinguality could lead to more cost-effective solutions for boosting non-English capabilities. In terms of model merging, our analysis indicates that weight-level techniques are perhaps not as effective for reducing interference when merging parameters together, as compared to parameter-level or even layer-level merges. possible explanation is that modifying individual weight values may disrupt the linear dependence within the transformations defined by the weight tensors. Conversely, the recent success of self-speculative decoding in LLMs (Zhang et al., 2024a; Elhoushi et al., 2024) suggests that the output embedding space of most layers across the model are very similar. If so, keeping entire layers in tact may alleviate the risk of undermining newfound learnings. Another potential reason is that using very granular magnitude measures may be too noisy to properly indicate which weight updates are important and which are not. By the nature of backpropagation, weight magnitude changes may be more indicative of the magnitude of the original value (larger weights will have larger gradients) or simply the weights proximity to the output layer. This is perhaps especially the case for small-scale training runs, where there is more noise."
        },
        {
            "title": "7 LIMITATIONS AND FUTURE WORK",
            "content": "The positive results of this layer swapping methodology largely raise many questions about the extent to which it is effective and in what settings. We propose here many further investigations that will build broader understanding of the methods practical scope. Freezing parameters before training experts: Instead of re-composing model with layers from separate models, logical next evaluation would be to freeze model layers from the beginning. Intuitively, this would only be more effective from performance standpoint as there is no ad hoc merging of experts from disparate training jobs. That being said, part of the benefit of this solution is that it is convenient and flexible because its all post hoc and required no prior planning. Parameter freezing would require knowing which configuration would work prior to training. LLMs with different pretraining: Any transformer model should work similarly, but it is unclear if the English-centric nature of LLAMA 3.1 enabled the success of layer swapping (or conversely inhibited it). It is also possible that in weaker models (e.g. LLAMA 1), the non-English representations are not developed enough to enable this cheap transfer. Similarly, our analysis may not extend to LLMs with more balanced multilingual pretraining, who may have very different representation spaces. Different model sizes: LLMs with more parameters, from either more transformer layers or larger hidden dimensions, would clearly require new merging configurations. In addition, it remains to be seen how related attributes such as sparsity, redundancy, and the quantity of cross-lingual shared parameters, would influence the performance of layer swapping. Lower-resource langs: Among the limited languages evaluated, the lower-resource ones benefited most from layer swapping. However, the model needs minimum capabilities in the target language for zero-shot cross-lingual transfer. It is unclear how well the LLM needs to understand the target language for this to be effective. Other reasoning tasks: This work only tackles mathematical reasoning, but its conceivable that other complex reasoning capabilities are concentrated in parameters separate from language capabilities in similar manner. That being said, the configuration required for layer swapping may be completely different and may require preliminary analysis as in Section 3. Parameter-efficient fine-tuning: Since layer swapping treats transformer layers as unified blocks, it would be equivalent if the model were fine-tuned using parameter-efficient SFT methods such as LoRA, which consists of inserting adapter weights into the transformer layer (Hu et al., 2022). The modification of other model merging methods for such adapters is simple (Zhang et al., 2023a), and therefore layer swapping has the potential to be effective in parameter-efficient fine-tuning settings. Different training stage: We limit the focus of this work on low-data SFT. Model souping, itself, is implemented in practice at all stages (e.g. pretraining, CPT). This raises the question of in which settings layer swapping would be effective for model merging. Similarly, model souping is effective even when the checkpoints have been fine-tuned significantly. It is unclear what would be the point for our methodology where the experts have diverged from each other too significantly that they would no longer recombine well. If it is only effective in small training runs, it could still enable multi-step training where experts are iteratively fine-tuned and merged successively, analogous to gradient aggregation in data-distributed training."
        },
        {
            "title": "8 CONCLUSION",
            "content": "In this paper, we address the difficulty of training LLMs to perform tasks in languages where labeled task-specific data does not exist. We first trained separate experts on English math data and generic instruction data in numerous target languages. An analysis of the importance of different parameter 10 updates during fine-tuning led to the development of the layer swapping method which swaps in the top and bottom layers from language experts into the math expert. Surprisingly, this simple model merging provides one of the highest performing methods to fine-tune an LLM for math in target language without the presence of target language data. The strong intuition behind this solution and its flexibility, simplicity, and effectiveness provides vast potential for being practical in many other settings. This method can be potentially adapted for new pretrained models, target tasks, target languages, training stages, training setups, and more. In addition, this method indicates better model interpretability of multilinguality can lead to more efficient methods for transferring English capabilities to lower resource languages."
        },
        {
            "title": "REPRODUCIBILITY",
            "content": "For reproducing the fine-tuning of our expert models, we describe the process in Section 3.1 and provide further details on the data and fine-tuning hyperparameters in Appendix A.1 and A.2. Further details for our analysis described in Section 3.2 can be found in Appendix A.4. We provide pseudocode of the layer swapping algorithm defined in Section 4 in Algorithm 1. Our experiments for which results are presented are all explained thoroughly in Section 5. ACKNOWLEDGMENTS The authors acknowledge the crucial support provided by Mostafa Elhoushi, Tanmay Parekh, Jiabao Ji, and Chloe Bi that made this project possible."
        },
        {
            "title": "REFERENCES",
            "content": "Samuel Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. Git re-basin: Merging models modulo permutation symmetries. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=CQsmMYmlP5T. Alan Ansell, Edoardo Ponti, Anna Korhonen, and Ivan Vulic. Composable sparse fine-tuning for cross-lingual transfer. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 17781796, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.125. URL https://aclanthology.org/2022. acl-long.125. Alan Ansell, Ivan Vulic, Hannah Sterz, Anna Korhonen, and Edoardo M. Ponti. Scaling sparse finetuning to large language models, 2024. URL https://arxiv.org/abs/2401.16405. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Lucas Bandarkar, Davis Liang, Benjamin Muller, Mikel Artetxe, Satya Narayan Shukla, Donald Husa, Naman Goyal, Abhinandan Krishnan, Luke Zettlemoyer, and Madian Khabsa. The belebele benchmark: parallel reading comprehension dataset in 122 language variants. In LunWei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 749775, Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL https: //aclanthology.org/2024.acl-long.44. L. Breiman. Bagging predictors. Machine Learning, 24:123140, 1996. URL https://api. semanticscholar.org/CorpusID:47328136. Linzheng Chai, Jian Yang, Tao Sun, Hongcheng Guo, Jiaheng Liu, Bing Wang, Xiannian Liang, Jiaqi Bai, Tongliang Li, Qiyao Peng, and Zhoujun Li. xcot: Cross-lingual instruction tuning for cross-lingual chain-of-thought reasoning, 2024. URL https://arxiv.org/abs/2401. 07037. Tyler Chang, Zhuowen Tu, and Benjamin Bergen. The geometry of multilingual language model In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of representations. 11 the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 119136, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.9. URL https://aclanthology.org/2022. emnlp-main.9. Rochelle Choenni, Dan Garrette, and Ekaterina Shutova. How do languages influence each other? studying cross-lingual data sharing during LM fine-tuning. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1324413257, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.818. URL https://aclanthology.org/ 2023.emnlp-main.818. Rochelle Choenni, Ekaterina Shutova, and Dan Garrette. Examining modularity in multilingual LMs via language-specialized subnetworks. In Kevin Duh, Helena Gomez, and Steven Bethard (eds.), Findings of the Association for Computational Linguistics: NAACL 2024, pp. 287301, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024. findings-naacl.21. URL https://aclanthology.org/2024.findings-naacl.21. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv: 2110.14168, 2021. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzman, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 84408451, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.747. URL https://aclanthology.org/ 2020.acl-main.747. John Dang, Arash Ahmadian, Kelly Marchisio, Julia Kreutzer, Ahmet Ustun, and Sara Hooker. Rlhf can speak many languages: Unlocking multilingual preference optimization for llms, 2024. URL https://arxiv.org/abs/2407.02552. MohammadReza Davari and Eugene Belilovsky. Model breadcrumbs: Scalable upcycling of finetuned foundation models via sparse task vectors merging. In ICML 2024 Workshop on Foundation Models in the Wild, 2024. URL https://openreview.net/forum?id=vuyP3tupig. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 41714186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/ N19-1423. Thomas G. Dietterich. Multiple classifier systems. In Lecture Notes in Computer Science, 2000. URL https://api.semanticscholar.org/CorpusID:56776745. Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich, Basil Hosmer, Bram Wasti, Liangzhen Lai, Anas Mahmoud, Bilge Acun, Saurabh Agarwal, Ahmed Roman, Ahmed Aly, Beidi Chen, and Carole-Jean Wu. Layerskip: Enabling early exit inference and self-speculative decoding. Meta Research, 2024. URL https://ai.meta.com/research/publications/ layerskip-enabling-early-exit-inference-and-self-speculative-decoding/. William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: scaling to trillion parameter models with simple and efficient sparsity. The Journal of Machine Learning Research, 23(1), January 2022. ISSN 1532-4435. Felix Gaschi, Patricio Cerda, Parisa Rastin, and Yannick Toussaint. Exploring the relationship between alignment and cross-lingual transfer in multilingual transformers. In Anna Rogers, Jordan 12 Boyd-Graber, and Naoaki Okazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, pp. 30203042, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.189. URL https://aclanthology.org/ 2023.findings-acl.189. Demi Guo, Alexander Rush, and Yoon Kim. Parameter-efficient transfer learning with diff pruning. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 48844896, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.378. URL https://aclanthology.org/2021.acl-long.378. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id= d7KBjmI3GmQ. Masanori Hirano, Masahiro Suzuki, and Hiroki Sakaji. llm-japanese-dataset v0: Construction of japanese chat dataset for large language models and its methodology, 2023. URL https:// arxiv.org/abs/2305.12720. Edward Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=nZeVKeeFYf9. Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Ludwig Schmidt, Hannaneh Hajishirzi, In The Eleventh International Conferand Ali Farhadi. Editing models with task arithmetic. ence on Learning Representations, 2023. URL https://openreview.net/forum?id= 6t0Kwf8-jrj. Pavel Izmailov, Dmitrii Podoprikhin, T. Garipov, Dmitry P. Vetrov, and Andrew Gordon Wilson. Averaging weights leads to wider optima and better generalization. In Conference on Uncertainty in Artificial Intelligence, 2018. URL https://api.semanticscholar.org/CorpusID: 3833416. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7b, 2023. URL https: //arxiv.org/abs/2310.06825. Mohammed Safi Ur Rahman Khan, Priyam Mehta, Ananth Sankar, Umashankar Kumaravelan, Sumanth Doddapaneni, Suriyaprasaad G, Varun Balan G, Sparsh Jain, Anoop Kunchukuttan, Pratyush Kumar, Raj Dabre, and Mitesh M. Khapra. Indicllmsuite: blueprint for creating pretraining and fine-tuning datasets for indian languages. arXiv preprint arXiv: 2403.06350, 2024. Simran Khanuja, Srinivas Gowriraj, Lucio Dery, and Graham Neubig. DeMuX: Data-efficient In Kevin Duh, Helena Gomez, and Steven Bethard (eds.), Proceedings multilingual learning. of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 74237436, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024. naacl-long.412. URL https://aclanthology.org/2024.naacl-long.412. Wen Lai, Mohsen Mesgar, and Alexander Fraser. LLMs beyond English: Scaling the multilingual capability of LLMs with cross-lingual feedback. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics ACL 2024, pp. 81868213, Bangkok, Thailand and virtual meeting, August 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.findings-acl.488. Davis Liang, Hila Gonen, Yuning Mao, Rui Hou, Naman Goyal, Marjan Ghazvininejad, Luke Zettlemoyer, and Madian Khabsa. XLM-V: Overcoming the vocabulary bottleneck in multilingual 13 masked language models. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1314213152, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. emnlp-main.813. URL https://aclanthology.org/2023.emnlp-main.813. Meizhen Liu, Xu Guo, He Jiakai, Jianye Chen, Fengyu Zhou, and Siu Hui. InteMATs: Integrating granularity-specific multilingual adapters for cross-lingual transfer. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 50355049, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.335. URL https://aclanthology.org/2023. findings-emnlp.335. Team Llama, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian andAhmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, and et al. The llama 3 herd of models. Meta Research, 2024. Michael Matena and Colin Raffel. Merging models with fisher-weighted averaging. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 1770317716. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2022/ 2022. file/70c26937fbf3d4600b69a129031b66ec-Paper-Conference.pdf. Arindam Mitra, Hamed Khanpour, Corby Rosset, and Ahmed Awadallah. Orca-math: Unlocking the potential of slms in grade school math, 2024. Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. Crosslingual generalization through multitask finetuning, 2022. Team NLLB, Marta R. Costa-juss`a, James Cross, Onur elebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzman, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. No language left behind: Scaling human-centered machine translation. Meta Research, 2022. Guillermo Ortiz-Jimenez, Alessandro Favero, and Pascal Frossard. Task arithmetic in the tanIn Thirty-seventh Conference on Neural gent space: Improved editing of pre-trained models. Information Processing Systems, 2023. URL https://openreview.net/forum?id= 0A9f2jZDGW. Xuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. ERNIE-M: Enhanced multilingual representation by aligning cross-lingual semantics with monolingual corpora. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 2738, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.3. URL https: //aclanthology.org/2021.emnlp-main.3. Barun Patra, Saksham Singhal, Shaohan Huang, Zewen Chi, Li Dong, Furu Wei, Vishrav Chaudhary, and Xia Song. Beyond English-centric bitexts for better multilingual language representation learning. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1535415373, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.856. URL https://aclanthology.org/2023. acl-long.856. Jonas Pfeiffer, Ivan Vulic, Iryna Gurevych, and Sebastian Ruder. MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer. In Bonnie Webber, Trevor Cohn, Yulan He, 14 and Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 76547673, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.617. URL https://aclanthology. org/2020.emnlp-main.617. Jonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James Cross, Sebastian Riedel, and Mikel Artetxe. Lifting the curse of multilinguality by pre-training modular transformers. In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz (eds.), Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 34793495, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.255. URL https: //aclanthology.org/2022.naacl-main.255. Fred Philippy, Siwen Guo, and Shohreh Haddadan. Towards common understanding of contributing factors for cross-lingual transfer in multilingual language models: review. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 58775891, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long. 323. URL https://aclanthology.org/2023.acl-long.323. Libo Qin, Qiguang Chen, Fuxuan Wei, Shijue Huang, and Wanxiang Che. Cross-lingual prompting: Improving zero-shot chain-of-thought reasoning across languages. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 26952709, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.163. URL https: //aclanthology.org/2023.emnlp-main.163. Alexandre Rame, Kartik Ahuja, Jianyu Zhang, Matthieu Cord, Leon Bottou, and David Lopez-Paz. Model ratatouille: Recycling diverse models for out-of-distribution generalization. In International Conference on Machine Learning, 2022. URL https://api.semanticscholar. org/CorpusID:254877458. Shuaijie She, Wei Zou, Shujian Huang, Wenhao Zhu, Xiang Liu, Xiang Geng, and Jiajun Chen. MAPO: Advancing multilingual reasoning through multilingual-alignment-as-preference optimization. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1001510027, Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.acl-long.539. Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. Language models are multilingual chain-of-thought reasoners. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id= fR3wGCk-IXp. Shivalika Singh, Freddie Vargus, Daniel Dsouza, Borje Karlsson, Abinaya Mahendiran, Wei-Yin Ko, Herumb Shandilya, Jay Patel, Deividas Mataciunas, Laura OMahony, Mike Zhang, Ramith Hettiarachchi, Joseph Wilson, Marina Machado, Luisa Moura, Dominik Krzeminski, Hakimeh Fadaei, Irem Ergun, Ifeoma Okoh, Aisha Alaagib, Oshan Mudannayake, Zaid Alyafeai, Vu Chien, Sebastian Ruder, Surya Guthikonda, Emad Alghamdi, Sebastian Gehrmann, Niklas Muennighoff, Max Bartolo, Julia Kreutzer, Ahmet Ustun, Marzieh Fadaee, and Sara Hooker. Aya dataset: An open-access collection for multilingual instruction tuning. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1152111567, Bangkok, Thailand, August 2024a. Association for Computational Linguistics. URL https://aclanthology.org/ 2024.acl-long.620. Shivalika Singh, Freddie Vargus, Daniel Dsouza, Borje F. Karlsson, Abinaya Mahendiran, Wei-Yin Ko, Herumb Shandilya, Jay Patel, Deividas Mataciunas, Laura OMahony, Mike Zhang, Ramith Hettiarachchi, Joseph Wilson, Marina Machado, Luisa Souza Moura, Dominik Krzeminski, Hakimeh Fadaei, Irem Ergun, Ifeoma Okoh, Aisha Alaagib, Oshan Mudannayake, Zaid Alyafeai, 15 Vu Minh Chien, Sebastian Ruder, Surya Guthikonda, Emad A. Alghamdi, Sebastian Gehrmann, Niklas Muennighoff, Max Bartolo, Julia Kreutzer, Ahmet Ustun, Marzieh Fadaee, and Sara Hooker. Aya dataset: An open-access collection for multilingual instruction tuning, 2024b. Yi-Lin Sung, Varun Nair, and Colin Raffel. Training neural networks with fixed sparse masks. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id= Uwh-v1HSw-x. Tianyi Tang, Wenyang Luo, Haoyang Huang, Dongdong Zhang, Xiaolei Wang, Xin Zhao, Furu Wei, and Ji-Rong Wen. Language-specific neurons: The key to multilingual capabilities in large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 57015715, Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.acl-long.309. Atnafu Lambebo Tonja, Bonaventure FP Dossou, Jessica Ojo, Jenalea Rajab, Fadel Thior, Eric Peter Wairagala, Aremu Anuoluwapo, Pelonomi Moiloa, Jade Abbott, Vukosi Marivate, et al. arXiv preprint Inkubalm: small arXiv:2408.17024, 2024. language model for low-resource african languages. Zirui Wang, Zachary C. Lipton, and Yulia Tsvetkov. On negative interference in multilingual models: Findings and meta-learning treatment. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 44384450, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.359. URL https://aclanthology.org/ 2020.emnlp-main.359. Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162, pp. 2396523998. PMLR, 9 2022. URL https: //proceedings.mlr.press/v162/wortsman22a.html. Runxin Xu, Fuli Luo, Zhiyuan Zhang, Chuanqi Tan, Baobao Chang, Songfang Huang, and Fei Huang. Raise child in large language model: Towards effective and generalizable finetuning. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 95149528, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.749. URL https: //aclanthology.org/2021.emnlp-main.749. Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. Ties-merging: Resolving interference when merging models. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=xtaX3WyCj1. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024a. Enneng Yang, Li Shen, Guibing Guo, Xingwei Wang, Xiaochun Cao, Jie Zhang, and Dacheng Tao. Model merging in llms, mllms, and beyond: Methods, theories, applications and opportunities, 2024b. URL https://arxiv.org/abs/2408.07666. 16 Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. Language models are super mario: In International Conference on Absorbing abilities from homologous models as free lunch. Machine Learning. PMLR, 2024. Jinghan Zhang, Shiqi Chen, Junteng Liu, and Junxian He. Composing parameter-efficient modules with arithmetic operation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023a. URL https://openreview.net/forum?id=5r3e27I9Gy. Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Gang Chen, and Sharad Mehrotra. Draft& In Lunverify: Lossless large language model acceleration via self-speculative decoding. Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 11263 11282, Bangkok, Thailand, August 2024a. Association for Computational Linguistics. URL https://aclanthology.org/2024.acl-long.607. Zhihan Zhang, Dong-Ho Lee, Yuwei Fang, Wenhao Yu, Mengzhao Jia, Meng Jiang, and Francesco Barbieri. Plug: Leveraging pivot language in cross-lingual instruction tuning, 2023b. Zhihao Zhang, Jun Zhao, Qi Zhang, Tao Gui, and Xuanjing Huang. Unveiling linguistic regions in large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 62286247, Bangkok, Thailand, August 2024b. Association for Computational Linguistics. URL https://aclanthology.org/2024.acl-long.338. Bo Zheng, Li Dong, Shaohan Huang, Saksham Singhal, Wanxiang Che, Ting Liu, Xia Song, and Furu Wei. Allocating large vocabulary capacity for cross-lingual language model pretraining. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 32033215, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.257. URL https: //aclanthology.org/2021.emnlp-main.257."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 FINE-TUNING DATASETS Table 3: Datasets used for supervised-fine-tuning (SFT) in this project Category Datasets URL"
        },
        {
            "title": "Math",
            "content": "Orca Math word problems dataset from Microsoft (Mitra et al., 2024) https://huggingface. co/datasets/microsoft/ orca-math-word-problems-200k"
        },
        {
            "title": "Telugu",
            "content": "Aya Dataset from Cohere for AI (Singh et al., 2024a) https://huggingface.co/datasets/ CohereForAI/aya_dataset NLLB English-Telugu translation data from FAIR (NLLB et al., 2022) https://huggingface.co/datasets/ allenai/nllb English instruction dataset, machine translated to Telugu Aya Dataset by Cohere for AI (Singh et al., 2024a) https://huggingface.co/datasets/ CohereForAI/aya_dataset Bengali English-Bengali NLLB (NLLB et al., 2022) translation data from https://huggingface.co/datasets/ allenai/nllb IndicShareLlama dataset from AI4Bharat (Khan et al., 2024) https://huggingface.co/datasets/ ai4bharat/indic-align BongChat dataset from Lumatic AI https://huggingface.co/datasets/ lumatic-ai/BongChat-v1-253k Aya Dataset by Cohere for AI (Singh et al., 2024a) https://huggingface.co/datasets/ CohereForAI/aya_dataset Swahili English-Swahili NLLB (NLLB et al., 2022) translation data from https://huggingface.co/datasets/ allenai/nllb Inkuba dataset from Lelapa (Tonja et al., 2024) https://huggingface.co/datasets/ lelapa/Inkuba-instruct xP3 MT dataset from BigScience, with FLORES samples removed (Muennighoff et al., 2022) https://huggingface.co/datasets/ bigscience/xP3mt Aya Dataset by Cohere for AI (Singh et al., 2024a) https://huggingface.co/datasets/ CohereForAI/aya_dataset Japanese English-Japanese translation data from NLLB (NLLB et al., 2022) https://huggingface.co/datasets/ allenai/nllb LLM-Japanese dataset from Izumi Lab (Hirano et al., 2023) https://huggingface.co/datasets/ izumi-lab/llm-japanese-dataset Ichikara dataset from RIKEN AIP https://huggingface.co/datasets/ p1atdev/ichikara-instruction Dolly dataset from Databricks, machine translated to Japanese https://huggingface.co/datasets/ kunishou/databricks-dolly-15k-ja 18 A.2 SUPERVISED FINE-TUNING DETAILS While many hyperparameters were tried, below is listed the hyperparameter configurations that led to the three best checkpoints (the experts) for each category. Note that in all runs, we do checkpointing every 5000 samples and use different random seed for data sampling. Table 4: Hyperparameters for the training runs that led to each of our experts Expert Learn Rate Batch Size Seq. Length weight decay clip, max norm sched. warmup β2 math #1 math #2 math #3 jpn #1 jpn # jpn #3 swh #1 swh #2 swh #3 tel #1 tel # tel #3 ben #1 ben #2 ben #3 2.0 108 1.0 10 4.0 108 1.0 107 1.0 107 2.0 107 7.0 108 2.0 10 1.0 107 7.0 108 5.0 108 1.0 107 7.0 108 5.0 10 1.0 107 4 4 4 8 8 8 8 8 8 4 8 8 4 2048 0. 1."
        },
        {
            "title": "Linear",
            "content": "1000 0.99 2048 0.01 0."
        },
        {
            "title": "Linear",
            "content": "1000 0.99 2048 0.1 1024 0. 1024 0.1 1.0 0.5 0."
        },
        {
            "title": "Linear",
            "content": "500 0.999 WSD 1000 0.995 WSD 1000 0.99 1024 0.01 0.5 WSD 1000 0.99 1024 0.1 0.5 WSD 1000 0.99 1024 0.05 0.5 WSD 1000 0.99 1024 0.1 1024 0. 1.0 0.5 WSD 1000 0.999 WSD 1000 0.99 1024 0.05 0.5 WSD 300 0.99 2048 0.1 1024 0. 0.5 0.5 WSD 1000 0.99 WSD 1000 0.99 1024 0.05 0.5 WSD 300 0.99 2048 0.1 0.5 WSD 1000 0.99 19 A.3 RESULTS FROM COMBINING WITH SOUPING Table 5: MGSM 8-shot results when combining layer swapping with model souping of the 3 experts for each category. Multilingual soup refers to the uniform soup of all 12 language experts."
        },
        {
            "title": "Swahili\nTelugu\nBengali\nJapanese",
            "content": "model soup, pairwise layer swap, pairwise layer swap, language soup w/ math soup layer swap, multilingual soup w/ math soup default avg of 9 pairs config, default avg of 9 pairs config, default config, version 1 default config, version 29.3 20.9 36.8 38.7 32.4 22.7 37.1 38.5 32.8 22.0 40.4 39.2 29.6 21.2 36.0 41.6 A.4 PARAMETER-LEVEL VISUALIZATIONS OF THE EXPERT VECTORS Larger parameter-level visualizations for number of the experts, each visualization is the same as described in Fig. Figure 4: Visualization of the magnitude of change during SFT for math expert #1 20 Figure 5: Visualization of the magnitude of change during SFT for math expert #2 Figure 6: Visualization of the magnitude of change during SFT for Swahili expert #1 21 Figure 7: Visualization of the magnitude of change during SFT for Japanese expert #1 Figure 8: Visualization of the magnitude of change during SFT for Telugu expert #1 22 Figure 9: Visualization of the magnitude of change during SFT for Bengali expert #"
        }
    ],
    "affiliations": [
        "GenAI",
        "Meta",
        "University of California, Los Angeles"
    ]
}