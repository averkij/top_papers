{
    "paper_title": "Data Valuation using Neural Networks for Efficient Instruction Fine-Tuning",
    "authors": [
        "Ishika Agarwal",
        "Dilek Hakkani-Tür"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Influence functions provide crucial insights into model training, but existing methods suffer from large computational costs and limited generalization. Particularly, recent works have proposed various metrics and algorithms to calculate the influence of data using language models, which do not scale well with large models and datasets. This is because of the expensive forward and backward passes required for computation, substantial memory requirements to store large models, and poor generalization of influence estimates to new data. In this paper, we explore the use of small neural networks -- which we refer to as the InfluenceNetwork -- to estimate influence values, achieving up to 99% cost reduction. Our evaluation demonstrates that influence values can be estimated with models just 0.0027% the size of full language models (we use 7B and 8B versions). We apply our algorithm of estimating influence values (called NN-CIFT: Neural Networks for effiCient Instruction Fine-Tuning) to the downstream task of subset selection for general instruction fine-tuning. In our study, we include four state-of-the-art influence functions and show no compromise in performance, despite large speedups, between NN-CIFT and the original influence functions. We provide an in-depth hyperparameter analyses of NN-CIFT. The code for our method can be found here: https://github.com/agarwalishika/NN-CIFT."
        },
        {
            "title": "Start",
            "content": "Data Valuation using Neural Networks for Efficient Instruction Fine-Tuning Ishika Agarwal UIUC ishikaa2@illinois.edu Dilek Hakkani-Tür UIUC dilek@illinois.edu"
        },
        {
            "title": "Abstract",
            "content": "Influence functions provide crucial insights into model training, but existing methods suffer from large computational costs and limited generalization. Particularly, recent works have proposed various metrics and algorithms to calculate the influence of data using language models, which do not scale well with large models and datasets. This is because of the expensive forward and backward passes required for computation, substantial memory requirements to store large models, and poor generalization of influence estimates to new data. In this paper, we explore the use of small neural networks which we refer to as the InfluenceNetwork to estimate influence values, achieving up to 99% cost reduction. Our evaluation demonstrates that influence values can be estimated with models just 0.0027% the size of full language models (we use 7B and 8B versions). We apply our algorithm of estimating influence values (called NN-CIFT: Neural Networks for effiCient Instruction Fine-Tuning) to the downstream task of subset selection for general instruction fine-tuning. In our study, we include four state-of-the-art influence functions and show no compromise in performance, despite large speedups, between NN-CIFT and the original influence functions. We provide an indepth hyperparameter analyses of NN-CIFT. The code for our method can be found here: https://github.com/agarwalishika/NN-CIFT. 5 2 0 2 7 1 ] . [ 2 9 6 9 9 0 . 2 0 5 2 : r"
        },
        {
            "title": "Introduction",
            "content": "The strong instruction-following abilities of large language models (LLMs) can be attributed to instruction fine-tuning (IFT) (Zhang et al., 2024). IFT builds on top of current language modeling capabilities and strengthens the instruction following abilities of models. Recent works have taken data efficient approaches for IFT. The goal is to select small subset of samples on which to finetune model (Agarwal et al., 2025; Mirzasoleiman"
        },
        {
            "title": "Pairwise",
            "content": "DELIFT (Agarwal et al., 2025) DELIFT (SE) (Agarwal et al., 2025) LESS (Xia et al., 2024) NN-CIFT (ours)"
        },
        {
            "title": "Size",
            "content": "O(M ) O(M ) 7-8B 355M O(M + ) 7-8B 205K O(M ) SelectIT (Liu et al., 2024a) NN-CIFT (ours) O(M ) O(M ) 7-8B 205K Table 1: Approximate computational complexity of data valuation in previous works measured by the cost of forward passes (F ) or the cost of backward passes (B) through model. and are the cardinality of DF and DT , fine-tuning and target dataset respectively, we use for subset selection. See Appendix B.1 for more details. Size denotes the number of parameters of the corresponding model. Note: larger models have higher cost for forward and back passes. et al., 2020; Das and Khetan, 2024; Xia et al., 2024; Renduchintala et al., 2024; Liu et al., 2024c) that emulates the full dataset. Data efficient pipelines typically consist of two stages: (1) data valuation: designing functions to estimate the influence of data points, and (2) data selection: using influence estimates to choose balanced set of influential data. Usually, data selection is cheaper than valuation for instance, DELIFT (SE)1 (Agarwal et al., 2025) computes the similarity of sentence embeddings between pairs of data (expensive) for valuation and selects representative data using submodular function (cheap). Formally, influence functions estimate the value of data. For instance, brute force influence functions use leave-one-out (LOO) training to measure impact by omitting each data point and evaluating performance (Scanlon, 1982). More recent influence functions use LLMs to estimate influence. Table 1 outlines the expenses of state-of-the-art (SOTA) influence functions, which comes from 1Short for \"Sentence Embedding\". Figure 1: Overview of NN-CIFT. The first step consists of using established influence functions to collect data for training the InfluenceNetwork. Next, the data from Step (1) is used to train the InfluenceNetwork and, subsequently, estimate the influence values for the rest of the data. Finally, the data selection algorithm corresponding to the original influence function is used to select subset of IFT data to fine-tune model on. the large amount of forward and backward passes through highly parameterized models. In this paper, we introduce NN-CIFT: Neural Networks for effiCient Instruction Fine-Tuning and explore how to train influence functions efficiently. We improve efficiency by using compact neural networks which we coin as the InfluenceNetwork that are 0.0027% the size of LLMs, to estimate influence. Figure 1 outlines our methodology with pairwise influence function (more details about pairwise influence functions in Appendix B.1). As depicted, NN-CIFT is three-step algorithm. The neural network must be trained to estimate influence values effectively. Hence, we first use the influence function (with LLMs) to output influence values for very small subset of data. This becomes our training data for the InfluenceNetwork. We find that small neural network can sufficiently learn to estimate influence with very few data (covered in Section 4). Second, we train the InfluenceNetwork, and use it to estimate the influence values for the rest of the data points. Finally, we apply data selection algorithm on the influence values. This helps to obtain small subset of IFT data to enhance language models. After fine-tuning language models on the chosen subsets, we find that NN-CIFT achieves comparable performance to the original influence functions (covered in Section 5). Our contributions and findings are listed as follows. NN-CIFT: 1. alleviates the cost of using expensive LLMs during data valuation by using smaller and cheaper neural networks, without affecting the performance on downstream tasks (Tables 2 and 3); 2. achieves competitive performance to previous data valuation methods, despite using only 0.25%-5% of the data. The average mean square error in influence values between NN-CIFT and the original influence functions is merely 0.067 (Figure 2); 3. is shown to be effective for new data points, circumventing the need to retrain an influence function for new data previous works incur this cost (Figure 2). 4. reduces costs by 77-99% time during data valuation  (Table 4)  . Section 2 outlines the current state of research in data valuation and data selection. Section 3 explains the problem setting. Section 4 presents the 2 main methodology for NN-CIFT and motivating results. Finally, Section 5 reports results on the downstream task of subset selection after the data valuation stage. In our evaluation, we find that using small LLM with the original influence functions results in degraded performance. Our hyperparameter studies are in Section 4.4, Figure 3 and Section 5.2, Figure 4. Lastly, the SOTA influence functions are detailed in Appendix B."
        },
        {
            "title": "2.1 Data Valuation",
            "content": "Wei et al. (2023) hint that different models extract different information from the same data. Hence, effective fine-tuning requires datasets to be specific to each model. Not all data points affect the model equally - models learn more from certain data points than others. Therefore, data valuation methods prune out such low-influence data for efficient fine-tuning (Xia et al., 2024; Agarwal et al., 2025). Current research is divided into model-independent and model-dependent valuation metrics. Model-independent methods, such as distance or clustering-based methods (Das and Khetan, 2024; Liu et al., 2024c; Renduchintala et al., 2024) are faster and less computationally expensive. Distance-based methods assign more \"influence\" to data points that are further from each other, optimizing for diverse subset. Clustering-based methods assign more \"influence\" to data points that are representative (i.e., the centroids of clusters). On the other hand, model-dependent methods such as inference-based and gradient-based are more resource intensive. Inference-based methods (Liu et al., 2024a; Agarwal et al., 2025) use model inference signals (e.g., token distributions) to evaluate the performance or confidence of models, and valuate data based on how performative/confident they are. Gradient based methods (Xia et al., 2024; Mirzasoleiman et al., 2020; Killamsetty et al., 2021; Koh and Liang, 2020), on the other hand, can assign higher influence to data points with (1) higher magnitudes of gradients, or (2) gradients that match domain-specific data (for domainspecific fine-tuning, for example). While they are expensive to calculate, when paired with data selection algorithms, modeldependent data valuation metrics can be used to select subsets of data that are specific to models capabilities. Model-dependent data valuation metrics help to select data that will maximize certain objective for each model, rendering fine-tuning more effective."
        },
        {
            "title": "2.2 Data Selection",
            "content": "Data selection aims to prune redundant and noisy data samples from large datasets to produce small, information-rich subset (Agarwal et al., 2025; Xia et al., 2024). This subset should be representative of the larger dataset while performing comparably, if not better, than using the full dataset. Data selection methods usually have objectives for selecting data: (1) instruction tuning (Liu et al., 2024a), (2) task-specific fine-tuning (Liu et al., 2024c), (3) continual learning (Agarwal et al., 2025), (4) preference alignment (Liu et al., 2024b), etc. While certain objectives are subsets of others (e.g. (2) is subset of (1)), the data selected for each purpose may not necessarily overlap. For instance, (1) requires data that is representative of particular dataset, whereas (2) focuses on samples that reflect specific tasks like math reasoning, question answering, or summarization. Similarly, (3)s samples are specifically chosen to introduce new information to model without overriding or repeating previously learned information."
        },
        {
            "title": "3 Problem Formulation",
            "content": "Given model and fine-tuning data DF , the goal is to select small subset SF DF that maximizes the performance of after fine-tuning on SF . SF is the optimal subset if it can be used to train model that is comparable to model trained on DF . However, more recent works jointly optimize other objectives during subset selection. Examples of objectives include not only representation, but also task-specific refinement and continual learning. For such joint optimization, the subset SF is aligned with another target domain dataset DT . The choice of DT can guide the subset selection towards various objectives. For example, if the objective is representation or task-specific refinement, SF will contain points from DF that are similar to DT (Liu et al., 2024c; Xia et al., 2024; Das and Khetan, 2024). Alternatively, if the objective is continual learning, SF will contain points from DF that would allow the model to learn new information that is present in DT (Agarwal et al., 2025; Tiwari et al., 2022). As mentioned before, computing influence functions can be very expensive process. There are 3 Figure 2: MSE versus InfluenceNetwork training data size (u) plotted for 8 different training sizes, broken down by the quadrant. These results are for learning DELIFT influence values. Error rates on each quadrant correspond to losses across different sets: Q1 for training, Q2/Q3 for validation, and Q4 for testing. As shown, the InfluenceNetwork achieves MSE of merely 0.05% starting from = 0.05 and always outperforms the baselines. two kinds of influence functions: pairwise and pointwise both require forward/backward passes through language models, but the costs slightly differ. Pairwise influence functions compute the influence between every pair of points in dataset. We study three SOTA pairwise functions, whose formulations are details in Appendix B.1. This paper also studies one pointwise influence functions that simply compute the influence of each data point individually, formally outlined in Appendix B.2. While pointwise influence functions are more efficient than pairwise, they are not as performant during subset selection (Xia et al., 2024; Agarwal et al., 2025)."
        },
        {
            "title": "4.1 Defining the InfluenceNetwork.",
            "content": "For estimating the influence values of data samples, we call our neural network the InfluenceNetwork. It is 2-layer neural network with hidden size of 100 neurons, and an output size of 1 neuron. For activation, we use ReLU in between the layers. The function INθ represents the neural network with parameters θ. As input, INθ takes two data points and and outputs the estimated influence of on j. Specifically, embeddings for and are computed (denoted as emb() below) using the BAAI General Embedding model (bge-large-en-v1.5, in particular) (Xiao et al., 2023) and are concatenated:"
        },
        {
            "title": "3.1 Our motivation",
            "content": "Overall, our aim is to reduce the total number of forward or back propagations through models with millions and billions of parameters by replacing large portion with forward propagations through small neural networks with (merely) hundreds of thousands of parameters. Pairwise influence functions calculate the similarity between two data points (denoted as sim(i, j)). Because influence values are usually not learned, they need to be recomputed for any data beyond the training data. In other words, as data is constantly being collected, influence values for new data must be recomputed. However, NN-CIFT is learned. Hence, our method does not require any extra computation to estimate influence values, unlike previous work."
        },
        {
            "title": "4 Learning Influence Estimation",
            "content": "This section describes in detail Steps 1 and 2 in Figure 1. It outlines the structure and initial experimentation of the InfluenceNetwork. 0 INθ(i, j) 1, 0 θ(concat(emb(i), emb(j))) 1, (i, j) DF DT The bge-large-en-v1.5 model generates embeddings of size 1,024, which means the input has total length of 2,048. Hence, the InfluenceNetwork has exactly 204,900 parameters. For training, we use 20 epochs and learning rate η = 0.0001."
        },
        {
            "title": "4.2 Training the InfluenceNetwork.",
            "content": "Below is an illustration of the quadratic similarity matrix that is computed during the data valuation stages. Previous influence compute the entire matrix for data valuation we only use Q1. DF Q1 Q2 Q3 Q4 DT Using the predefined influence functions in Appendix B, small fraction of influence values are 4 Figure 3: MSE versus InfluenceNetwork sizes (measured by the number of parameters). We try 1-5 layers with 46 different combinations of hidden layer sizes from {5, 10, 20, 50, 100, 200, 500, 1000, 2000, 3000, 4000, 5000}. computed we call this fraction u. We use u% of data from DF and u% of data from DT to compute the training set for the InfluenceNetwork. As mentioned above, this training set is represented by Q1 in the illustration. The quadrants Q1 to Q4 represent the subset of influence values between combination of in-distribution (ID) data and out-of-distribution (OOD) data. ID and OOD data is determined by whether the InfluenceNetwork was trained on the data (ID) or not (OOD): Q1: Fully ID data from DF and DT Q2: ID data from DF and OOD data from DT Q3: OOD data from DF and ID data from DT Q4: Fully OOD data from DF and DT The InfluenceNetwork is able to predict influence values with low error rates. After just = 0.05, it is consistently better than random influence values and predicting only 0. The average MSE between the InfluenceNetworks influence scores and DELIFTs influence scores is 0.072, 0.072, 0.062, 0.063 for Q1 to Q4, respectively (averaging to 0.067). Furthermore, the error rate stays consistent across all four quadrants, showing that NN-CIFT does not need to be retrained to estimate the influence of new data points that are collected after the training data. One thing to note is that although = 0.05, with pairwise influence functions, we end up using only 0.25% of the data to train the InfluenceNetwork because we use 5% of DF and 5% of DT ."
        },
        {
            "title": "4.3 Evaluating the InfluenceNetwork.",
            "content": "To ensure our InfluenceNetwork is able to output influence values correctly, we compute the average mean squared error (MSE) between the ground truth influence values (from Appendix B) and the predicted influence values: We vary the number of layers and dimensions of each layer. For simplicity, we plot the number of parameters in the InfluenceNetwork versus the MSE. The results can be found in Figure 3. This figure shows that small InfluenceNetworks perform comparatively well as larger InfluenceNetworks. 1 DF DT (cid:88) (IFθ(i, j) sim(i, j))2 (i,j)DF DT We separate the evaluation between the four quadrants of data to study the performance with ID and OOD data. To train the InfluenceNetwork, we use DELIFTs influence values on the MixInstruct dataset (Jiang et al., 2023) to train our InfluenceNetwork (more dataset details in Section 5). We report the results from InfluenceNetwork and two other baselines: (1) Randomly generating number between 0 and 1, and (2) only Predicting 0 influence. These results can be found in Figure 2."
        },
        {
            "title": "5 Subset Selection Evaluation",
            "content": "Motivated by the results in Figure 2, we apply the InfluenceNetwork to the downstream task of subset selection: can we achieve the same performance when using the InfluenceNetwork instead of the original influence function? Thus, this section corresponds to Step 3 in Figure 1. Datasets and models. We use MixInstruct as well as Alpaca (Taori et al., 2023) to evaluate NNCIFT. These are both instruction tuning datasets where we use 15k for training, 5k for validation, and 5k for testing. We evaluate using two models: microsoft/Phi-3-small-8k-instruct (Abdin et al., 2024) and meta-llama/Llama-3.1-8B 5 Dataset Method Metric Initial Random SelectIT DistilGPT2 + SelectIT NN-CIFT + SelectIT LESS DistilGPT2 + LESS NN-CIFT + LESS DELIFT (SE) DistilGPT2 + DELIFT(SE) NN-CIFT + DELIFT (SE) DELIFT DistilGPT2 + DELIFT NN-CIFT + DELIFT Full Data MixInstruct Alpaca ICL QLoRA ICL QLoRA ROUGE BGE 37.87 39.00 43.08 40.21 43.71 42.08 40.33 42.84 47.43 46.74 47. 48.46 42.49 48.57 58.65 78.92 80.66 84.50 79.37 81.95 83.24 79.25 83.74 84.40 84.36 82. 85.77 79.19 83.90 88.72 LAJ 2.98 3.12 3.18 3.05 3.16 3.26 3.19 3. 3.28 3.23 3.23 3.35 3.19 3.41 3.45 ROUGE BGE 36.36 44. 45.14 41.60 46.09 45.16 42.57 45.18 48.22 45.50 46.49 52.79 48.34 53.30 65.51 82.55 85. 85.88 81.75 86.13 84.95 79.48 84.63 86.50 84.06 84.68 88.04 84.60 81.34 92.24 LAJ 3.02 3.12 3.21 3.08 3.19 3.28 3.17 3.26 3.28 3.29 3.29 3.37 3.28 3.54 3. ROUGE BGE 25.79 34.93 33.56 31.68 34.85 35.78 32.91 36.12 37.53 35.99 37. 38.36 32.50 38.99 35.27 67.82 73.50 77.10 74.86 77.79 76.84 74.19 77.11 80.76 79.38 80. 81.13 74.49 80.29 77.85 LAJ 2.56 3.07 3.12 3.04 3.13 3.16 3.09 3. 3.25 3.20 3.26 3.36 3.25 3.49 3.31 ROUGE BGE 27.29 35. 34.04 32.30 34.07 35.28 35.85 36.49 42.66 40.15 42.52 43.43 38.26 44.64 39.29 71.57 75. 78.10 75.75 78.11 76.49 76.64 75.75 84.26 83.89 84.58 85.05 79.58 85.23 78.85 LAJ 2.62 2.96 3.21 3.14 3.16 3.15 3.15 3.16 3.18 3.09 3.29 3.56 3.46 3.57 3. Table 2: Results on the Phi-3 model with = 0.3, = 0.05. NN-CIFT + Method indicates using NN-CIFT to estimate influence values computed from the corresponding methods influence function. DistilGPT2 + Method indicates using the DistilGPT2 model as the language model in the corresponding methods influence function. The average performance difference between NN-CIFT and the original influence function is merely 1.40%. (Grattafiori et al., 2024). Note: we use Phi-3 and Llama-8B as shorthand for these models, respectively. Phi-3 has 7.39B parameters and LLama-8B has 8.03B parameters. Metrics. To evaluate the instruction following capabilities of our fine-tuned model M, we employ variety of metrics to capture the similarity between ground truth answers and predicted answers from M: (1) ROUGE (Lin, 2004): n-gram word overlap (specifically, rouge-1), (2) BGE: semantic similarity of embeddings using bge-large-en-v1.5, and (3) LAJ: an LLM-as-a-Judge, namely the prometheus-7b-v2.0 model (Kim et al., 2023). Prometheus grading rubric is borrowed from Agarwal et al. (2025) in Appendix B. Next, to evaluate the costs of each method, we use time (in seconds) took on 2 Nvidia A40 GPUs. influence Baselines. Besides functions the DELIFT, DELIFT (SE), LESS, and SelectIT, we include three other baselines: Initial, DistilGPT2, and Full Data. Initial is the setting where = 0.0. This is the base models performance on the dataset. Next, we use small language model DistilGPT2 (distilbert/distilgpt2) (Sanh et al., 2020) which has 88.2M parameters as in the underlying language/embedding model the influence functions. Finally, Full Data is the setting where = 1.0, the models performance when the full dataset is used. i.e., Setup. We use = 0.05 for training the InfluenceNetwork. We also use small fraction of DF to fine-tune the language model we call this fraction v. We evaluate with = 0.3. Our evaluation framework includes two different settings to fine-tune the language model: using the selected subset of data points as (1) PEFT data for QLoRA (Dettmers et al., 2023) on M, or (2) in-context learning (ICL) examples. To elaborate on the ICL set up, we choose the top-5 most semantically similar samples from the chosen subset to add incontext. To measure semantic similarity, we again use bge-large-en-v1.5. Table 2 reports results for Phi-3 on both datasets with = 0.3; Table 3 reports results for Llama-8B on both datasets with = 0.3; Table 4 reports the cost in time for each method. All tables report the results for one run."
        },
        {
            "title": "5.1 Analysis",
            "content": "Table 4 reports the costs for each method, in seconds. It shows that data valuation can be performed at 77-99% faster than the original influence functions. This is because the number of parameters in NN-CIFT is 0.0026-0.0029% the size of the language model in the original influence function. Also, when using the DistilGPT2 model, which is near 1% the size of the language model, the costs are reduced by 54-91%. While these results are promising, the results on the downstream task of subset selection clearly differentiate NN-CIFT and the DistilGPT2 baseline. To elaborate, despite the significant speedups, NN-CIFT 6 Dataset Method Metric Initial Random SelectIT DistilGPT2 + SelectIT NN-CIFT + SelectIT LESS DistilGPT2 + LESS NN-CIFT + LESS DELIFT (SE) DistilGPT2 + DELIFT (SE) NN-CIFT + DELIFT (SE) DELIFT DistilGPT2 + DELIFT NN-CIFT + DELIFT Full Data MixInstruct Alpaca ICL QLoRA ICL QLoRA ROUGE BGE 28.53 40.07 46.51 41.26 46.48 48.21 42.18 48.20 48.36 47.21 48.59 51.66 47.09 52. 54.43 74.05 84.04 86.18 80.33 85.86 86.19 78.34 86.31 85.91 84.24 85.01 88.02 84.74 88. 92.55 LAJ 2.94 3.26 3.25 3.20 2.28 3.34 3.23 3.36 3.38 3.28 3. 3.43 3.26 3.41 3.40 ROUGE BGE 34.42 41.68 50.31 44.86 50. 51.24 48.64 51.56 51.43 49.37 50.53 55.58 48.21 55.85 59.47 78.54 84.26 87.38 84.72 87. 86.07 79.09 86.39 86.20 84.24 86.10 91.81 84.24 91.96 94.12 LAJ 3.00 3. 3.25 3.23 3.26 3.37 3.27 3.41 3.34 3.29 3.33 3.50 3.28 3.51 3.58 ROUGE BGE 24.85 36.95 41.42 39.18 42.07 43.34 42.02 44.42 44.30 43.51 45.49 46.49 45.08 46. 48.53 72.45 80.47 83.25 80.99 83.67 84.19 80.89 84.69 85.52 85.45 86.27 87.60 81.45 87. 91.21 LAJ 2.26 3.12 3.27 2.99 3.27 3.38 3.29 3.32 3.41 3.41 3. 3.50 3.41 3.55 3.63 ROUGE BGE 34.29 38.64 44.51 41.72 44. 44.73 42.51 46.40 45.35 44.89 45.75 49.16 41.07 49.15 48.29 80.82 80.46 84.18 81.50 85. 84.04 82.35 85.44 86.34 79.81 86.45 87.74 83.22 87.74 90.82 LAJ 3.03 3. 3.34 3.14 3.37 3.32 3.29 3.36 3.48 3.36 3.47 3.54 3.44 3.50 3.66 Table 3: Results on the Llama-8B model with = 0.3, = 0.05. NN-CIFT + Method and DistilGPT2 + Method follow the same definitions as in Table 2. The average performance difference between NN-CIFT and the original influence function is merely 1.39%. Model Dataset Initial Random SelectIT DistilGPT2 + SelectIT NN-CIFT + SelectIT LESS DistilGPT2 + LESS NN-CIFT + LESS DELIFT (SE) DistilGPT2 + DELIFT(SE) NN-CIFT + DELIFT (SE) DELIFT DistilGPT2 + DELIFT NN-CIFT + DELIFT Full Data Phi-3 Llama-8B MixInstruct Alpaca MixInstruct Alpaca - 12.4 7,047 144 65 12,338 1,291 78 216 98 48 67,379 8,058 - - 12.3 6,594 139 63 11,217 1,278 75 218 99 48 68,117 7,790 - - 12.9 6,671 144 64 10,843 1,291 74 218 98 48 68,076 8,058 - - 12.3 6,470 139 63 14,819 1,278 84 219 99 48 65,711 7,790 - Table 4: Costs (in seconds) of data valuation. Following are the specifications on each method. Random: choosing random subset of points as subset. SelectIT: calculating the ranking scores for each data point according to Appendix B.2. LESS: computing the cosine similarity between pairs of projected gradients for DF and DT , according to Equation 3. DELIFT (SE): computing the distance between each pair of embeddings (i, j) : DF , DT , according to Equation 2. DELIFT: computing the inference-based utility metric for each pair of embeddings (i, j), according to Equation 1. NN-CIFT: Steps 1 and 2 in Figure 1. Note, the costs of DistilGPT2 are the same across both models because they use the same data valuation (Phi-3 or Llama-8B are used for data selection/evaluation). shows no compromise to performance. Table 2 reports the results for Phi-3 and Table 3 reports the results for Llama-8B for = 0.3. To begin, the pairwise functions outperform the pointwise function (SelectIT) because they are able to capture more fine-grained effects of the data point on models learning. Next, DELIFT and DELIFT (SE) are able to outperform LESS because the theoretical guarantees of using submodular functions yields improved empirical performance. Finally, DELIFT uses model dependent information, tailoring the subset to the models weaknesses, allowing it to outperform DELIFT (SE). Keeping these in mind, NN-CIFT is able to achieve performance comparable to the original data valuation methods, even across models and datasets. However, DistilGPT2 shows performance degradations, which is more pronounced in the model-dependent methods (DELIFT, LESS, and SelectIT). This is because the model-dependent methods experience significant performance gains when the data valuation model is the same as the fine-tuning model. The absolute average performance difference across metrics between the original influence functions and NN-CIFT is only 1.40%2. Because the neural network is able to estimate the influence values with great accuracy, the selected subsets of data would be mostly the same between the original influence function and NN-CIFT. Hence, the performance difference of 1.40% can be attributed as the variability in the language models performance between two runs. Additionally, this trend is consistent across datasets and models, which shows 2The average performance difference is calculated by taking the absolute difference in performance, dividing it by the original performance, and then averaging this ratio across all settings (datasets, methods, metrics, baselines). 7 Figure 4: Hyperparameter study for and on MixInstruct with DELIFTs influence function. Lighter colors indicate better BGE performance. the wide applicability of our method."
        },
        {
            "title": "5.2 Hyperparameter study #2: Trade-off",
            "content": "between and We perform hyperparameter study between and on MixInstruct using DELIFTs influence function (Equation 1). We perform grid search where = = {0, 0.01, 0.05, 0.1, 0.15, 0.20, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8}, amounting to 169 experiments. Figure 4 shows the results using the BGE metric from each of these experiments. As shown, the two figures in each row follow the same general trend, showcasing that NN-CIFT can effectively replace the expensive influence function estimation. As expected, we notice few trends. (1) QLoRA generally has better performance than ICL. This is because fine-tuning has more impact on the model than simply adding examples to the prompt (i.e., prompt engineering). (2) The bottom right tends to be darker as fewer IFT data lead to insufficient training. (3) Larger IFT subsets, especially in the ICL setting, lead to poorer performance. During ICL, the top-5 semantically similar samples are chosen from the subset to add as in-context examples. However, semantic similarity does not always translate to performance enhancement as these samples can be harmful to the models performance. Finally, follow-up to (3), the highest performance regions tend to be around = 0.2 - 0.4. Appendix contains results on smaller subsets of IFT data (v = 0.1 and 0.2)."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduce NN-CIFT: Neural Networks for effiCient Instruction Fine-Tuning to distill highly parameterized models used in modern influence functions into small neural networks. We empirically show the effectiveness of our InfluenceNetwork design through low prediction error rates, and competitive performance on the downstream task of subset selection for IFT. We use four different influence functions to test with NNCIFT; our experimentation shows that NN-CIFT can lower costs for expensive data valuation, is adaptive to all kinds of influence functions (modeldependent or -independent; pairwise or pointwise), and does not require retraining for new data."
        },
        {
            "title": "7 Limitations",
            "content": "While NN-CIFT is effective, it is heavily dependent on the influence function. The influence func8 tions that were studied require large datasets to be annotated, which can be infeasible. Furthermore, NN-CIFT cannot yet be used for areas such as taskspecific dataset selection or continual learning. In these cases, the objectives of data selection are beyond representation. Finally, even though costs were shown to be much smaller, NN-CIFT still incurs quadratic cost. Although we show results for SelectIT, which runs in linear time, SelectIT is not able to outperform the pairwise methods. Future work will involve finding solution that can estimate influence of data point to data set (or models training dynamics) while also incurring linear time cost."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai, Qin Cai, Vishrav Chaudhary, Dong Chen, Dongdong Chen, and 110 others. 2024. Phi-3 technical report: highly capable language model locally on your phone. Preprint, arXiv:2404.14219. Ishika Agarwal, Krishnateja Killamsetty, Lucian Popa, and Marina Danilevsky. 2025. DELIFT: Data efficient language model instruction fine-tuning. In The Thirteenth International Conference on Learning Representations. Jeff Bilmes. 2022. Submodularity in machine learning and artificial intelligence. Devleena Das and Vivek Khetan. 2024. Deft: Data efficient fine-tuning for pre-trained language models via unsupervised core-set selection. Preprint, arXiv:2310.16776. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms. Preprint, arXiv:2305.14314. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, and 542 others. 2024. The llama 3 herd of models. Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. 2023. Llm-blender: Ensembling large language models with pairwise ranking and generative fusion. arXiv preprint arXiv:2306.02561. Krishnateja Killamsetty, Durga Sivasubramanian, Ganesh Ramakrishnan, Abir De, and Rishabh Iyer. 2021. Grad-match: Gradient matching based data subset selection for efficient deep model training. Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, and 1 others. 2023. Prometheus: Inducing fine-grained evaluation capability in language models. arXiv preprint arXiv:2310.08491. Pang Wei Koh and Percy Liang. 2020. Understanding black-box predictions via influence functions. Preprint, arXiv:1703.04730. Chin-Yew Lin. 2004. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain. Association for Computational Linguistics. Liangxin Liu, Xuebo Liu, Derek F. Wong, Dongfang Li, Ziyi Wang, Baotian Hu, and Min Zhang. 2024a. Selectit: Selective instruction tuning for large language models via uncertainty-aware self-reflection. Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He. 2024b. What makes good data for alignment? comprehensive study of automatic data selection in instruction tuning. Zifan Liu, Amin Karbasi, and Theodoros Rekatsinas. 2024c. Tsds: Data selection for task-specific model finetuning. Preprint, arXiv:2410.11303. Baharan Mirzasoleiman, Jeff Bilmes, and Jure Leskovec. 2020. Coresets for data-efficient training of machine learning models. Preprint, arXiv:1906.01827. N Kowndinya Renduchintala, Sumit Bhatia, and Ganesh Ramakrishnan. 2024. Smart: Submodular data mixture strategy for instruction tuning. Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2020. Distilbert, distilled version of bert: smaller, faster, cheaper and lighter. Preprint, arXiv:1910.01108. Edmund Scanlon. 1982. Residuals and influence in regression. New York: Chapman and Hall. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https:// github.com/tatsu-lab/stanford_alpaca. Rishabh Tiwari, Krishnateja Killamsetty, Rishabh Iyer, and Pradeep Shenoy. 2022. Gcr: Gradient coreset based replay buffer selection for continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 99108. Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, and Tengyu Ma. 2023. Larger language models do in-context learning differently. 9 Dataset Method Metric Initial Random SelectIT NN-CIFT + SelectIT LESS NN-CIFT + LESS DELIFT (SE) NN-CIFT + DELIFT (SE) DELIFT NN-CIFT + DELIFT Full Data MixInstruct Alpaca ICL QLoRA ICL QLoRA ROUGE BGE 37.87 37.51 33.20 33.55 32.57 33.19 35.71 36.34 36.45 36.17 58. 78.92 78.01 72.12 72.15 72.07 72.94 78.09 78.02 78.11 78.16 88. LAJ 2.98 3.05 3.12 3.07 3.05 3.02 3.22 3.22 3.23 3. 3.45 ROUGE BGE 36.36 35.55 37.00 35.38 34.61 35. 39.63 39.75 39.83 38.08 65.51 82.55 82.13 73.45 72.45 72.82 72. 78.36 78.76 78.83 78.25 92.24 LAJ 3.02 3.04 3.13 3. 3.18 3.18 3.28 3.33 3.29 3.28 3.51 ROUGE BGE 25.79 24.33 24.48 26.41 26.15 24.63 29.17 29.22 30.15 31.95 35. 67.82 67.37 67.48 65.57 69.83 70.11 70.69 72.28 74.01 74.84 77. LAJ 2.56 2.84 2.86 2.81 2.81 2.84 3.01 3.03 3.18 3. 3.31 ROUGE BGE 27.29 29.34 30.06 28.78 28.53 27. 30.60 30.23 37.81 37.26 39.29 71.57 70.86 68.06 67.83 67.17 67. 71.50 71.01 78.49 78.36 78.85 LAJ 2.62 3.06 3.04 2. 2.99 2.51 3.14 3.16 3.31 3.28 3.29 Table 5: Results on the Phi-3 model with = 0.1, = 0.05. NN-CIFT + Method and DistilGPT2 + Method follow the same definitions as in Table 2. The average performance difference between NN-CIFT and the original influence function is merely 1.91%. Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. 2024. Less: Selecting influential data for targeted instruction tuning. arXiv preprint arXiv:2402.04333. Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. 2023. C-pack: Packaged resources to advance general chinese embedding. Preprint, arXiv:2309.07597. Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, and Guoyin Wang. 2024. Instruction tuning for large language models: survey. Preprint, arXiv:2308.10792."
        },
        {
            "title": "A Evaluation on smaller subsets",
            "content": "Tables 5 and 7 report extra results for the Phi-3 model on = 0.1 and = 0.2, respectively. Similarly, Tables 6 and 8 report results for Llama-8B on = 0.1 and = 0.2, respectively. With Tables 2 and 3 in the main text, these results show an increasing trend in performance with higher subset of IFT data (i.e., higher v). They also show similar trends where NN-CIFT performs similarly to the original influence function."
        },
        {
            "title": "B Influence Functions",
            "content": "Following the problem formulation, we formally define the influence functions we used throughout our evaluation. model performance are chosen to represent DT . This can be calculated by comparing the performance with and without (ix, iy) as an in-context example (where D(, ) [0, 1] is function to measure distance between two probability distributions, and (qθ) is language model with parameters θ and input query q): sim(i, j) = D(jy, (ix, iy, jxθ))D(jy, (jxθ)) (1) After data valuation, the data selection stage consists of using submodular functions (Bilmes, 2022). In particular, we use the Facility Location submodular function. It takes as input similarity kernel that will optimize the maximum similarity between the chosen subset and the overall dataset while also minimizing the size of the chosen subset. To minimize the subset size, the Facility Location and submodular functions, in general employ diminishing gains property. This property states that samples added to smaller subset have more value than samples added to larger subset. Hence, we rely on our influence function to capture the informativeness of samples, and submodular functions to choose set of representative samples, resulting in small, information-rich subset on which to fine-tune model. B.1 Pairwise Influence Functions DELIFT (Agarwal et al., 2025) is modeldependent, Samples inference-based metric. (ix, iy) DF are used as in-context examples for evaluating (jx, jy) DT , and those with improved DELIFT (SE) (Agarwal et al., 2025) is modelindependent metric, and chooses samples from DF which are semantically closest to the samples from DT . Semantic distance is calculated by the cosine distance between embeddings of samples: 10 Dataset Method Metric Initial Random SelectIT NN-CIFT + SelectIT LESS NN-CIFT + LESS DELIFT (SE) NN-CIFT + DELIFT (SE) DELIFT NN-CIFT + DELIFT Full Data MixInstruct Alpaca ICL QLoRA ICL QLoRA ROUGE BGE 28.53 35.67 36.53 35.57 35.31 35.16 35.13 35.12 37.82 37. 54.43 74.05 76.30 78.69 78.86 77.07 78.11 77.71 78.69 80.55 81. 92.55 LAJ 2.94 3.18 3.14 3.17 3.19 3.16 3.12 3. 3.18 3.15 3.40 ROUGE BGE 34.42 37.20 36.95 37. 37.46 37.93 36.78 37.33 37.61 37.88 59.47 78.54 80.63 81.51 80. 80.86 81.36 79.69 80.34 82.63 82.01 94.12 LAJ 3.00 3. 3.20 3.21 3.23 3.20 3.15 3.08 3.20 3.19 3.58 ROUGE BGE 24.85 30.82 31.52 30.52 31.31 32.16 30.14 31.12 31.82 31. 48.53 72.45 75.38 75.69 74.86 75.07 76.11 73.71 74.69 75.62 75. 91.21 LAJ 2.26 2.82 2.84 2.88 2.71 2.75 2.61 2. 2.83 2.79 3.63 ROUGE BGE 34.29 36.95 38.06 37. 37.45 37.93 36.80 37.33 37.61 37.88 48.29 80.82 80.48 81.51 80. 80.85 81.35 79.69 80.34 80.55 81.16 90.82 LAJ 3.03 3. 3.19 3.13 3.23 3.21 3.15 3.08 3.29 3.29 3.66 Table 6: Results on the Llama-8b model with = 0.1, = 0.05. NN-CIFT + Method and DistilGPT2 + Method follow the same definitions as in Table 2. The average performance difference between NN-CIFT and the original influence function is merely 1.14%. Dataset Method Metric Initial Random SelectIT NN-CIFT + SelectIT LESS NN-CIFT + LESS DELIFT (SE) NN-CIFT + DELIFT (SE) DELIFT NN-CIFT + DELIFT Full Data MixInstruct Alpaca ICL QLoRA ICL QLoRA ROUGE BGE 37.87 37. 35.39 35.71 37.61 37.87 39.56 39.62 45.55 46.44 58.65 78.92 78. 78.14 78.23 79.55 77.96 81.25 81.47 82.32 82.47 88.72 LAJ 2.98 3.06 3.02 3.04 3.07 3.04 3.17 3.16 3.36 3.38 3. ROUGE BGE 36.36 38.89 37.71 37.36 37.43 38.96 39.77 39. 43.74 43.76 65.51 82.55 81.88 78.26 78.24 78.93 78.93 82.74 82. 82.35 82.72 92.24 LAJ 3.02 3.05 3.06 3.05 3.09 3. 3.15 3.14 3.50 3.52 3.51 ROUGE BGE 25.79 29. 30.31 31.03 32.57 33.20 34.06 33.01 35.02 34.44 35.27 67.82 76. 74.26 75.79 74.07 74.94 77.31 76.67 77.89 77.39 77.85 LAJ 2.56 3.12 3.13 3.09 3.02 3.05 3.23 3.27 3.40 3.36 3. ROUGE BGE 27.29 30.27 37.10 36.67 34.61 35.42 39.48 38. 39.32 38.30 39.29 71.57 76.21 77.66 77.98 76.68 78.02 80.95 80. 80.89 80.32 78.85 LAJ 2.62 3.15 3.10 3.04 3.08 3. 3.25 3.20 3.35 3.31 3.29 Table 7: Results on the Llama-8b model with = 0.2, = 0.05. NN-CIFT + Method and DistilGPT2 + Method follow the same definitions as in Table 2. The average performance difference between NN-CIFT and the original influence function is merely 1.08%. sim(i, j) = < emb((ix, iy)), emb((jx, jy)) > emb((ix, iy)) emb((jx, jy)) (2) , where emb(q) is an embedding model with input data q. Similar to DELIFT, DELIFT (SE) also uses the Facility Location function to select small, information-rich subset of samples. LESS (Xia et al., 2024) is model-dependent, gradient-based metric. Here, gradients between samples in DF and DT are matched by cosine similarity, and those that match the highest are chosen to represent DT (where (q; θ) is the gradient of data point from model with parameters θ): sim(i, j) = < ((ix, iy); θ), ((jx, jy); θ) > ((ix, iy); θ) ((jx, jy); θ) (3) During the data selection stage, the top-k matching gradients are chosen to be part of the subset. One thing to notice is that the above equation implies quadratic computation while Table 1 in the main text denotes linear computation this is because the gradients for each data point only need to be computed once, while the cosine similarity can be computed many times inexpensively. B.2 Pointwise Influence Functions Finally, SelectIT (Liu et al., 2024a) is another model-dependent metric that uses performance signals for data valuation, but incurs linear cost as it uses models uncertainty to rank data samples. Still, as mentioned in Table 1 from the main text, the linear time operations are forward propagations through LLMs. SelectIT ranks data points based on their tokenDataset Method Metric Initial Random SelectIT NN-CIFT + SelectIT LESS NN-CIFT + LESS DELIFT (SE) NN-CIFT + DELIFT (SE) DELIFT NN-CIFT + DELIFT Full Data MixInstruct Alpaca ICL QLoRA ICL QLoRA ROUGE BGE 28.53 39.55 39.20 40.02 40.33 43. 44.57 45.03 45.55 46.40 54.43 74.05 82.79 82.84 82.63 82.17 82. 82.63 83.69 83.69 84.73 92.55 LAJ 2.94 3.25 3.29 3. 3.26 3.27 3.31 3.30 3.37 3.34 3.40 ROUGE BGE 34.42 39.05 40.44 39.92 40.34 40.21 45.97 45.97 48.21 47.81 59. 78.54 82.64 82.55 82.22 82.87 82.89 83.87 83.95 86.81 86.83 94. LAJ 3.00 3.26 3.30 3.29 3.26 3.26 3.33 3.40 3.36 3. 3.58 ROUGE BGE 24.85 31.49 35.98 38.84 36.11 37. 38.52 38.57 39.16 40.16 48.53 72.45 76.96 81.82 84.09 79.82 80. 82.37 82.18 82.30 82.37 91.21 LAJ 2.26 3.06 2.95 3. 3.06 3.07 3.18 3.17 3.26 3.28 3.63 ROUGE BGE 34.29 41.67 42.62 44.62 43.48 43.48 45.73 45.20 45.24 45.67 48. 80.82 79.77 83.17 84.63 82.94 82.80 83.33 82.79 83.38 83.49 90. LAJ 3.03 3.14 3.21 3.23 3.32 3.34 3.35 3.39 3.39 3. 3.66 Table 8: Results on the Llama-8b model with = 0.2, = 0.05. NN-CIFT + Method and DistilGPT2 + Method follow the same definitions as in Table 2. The average performance difference between NN-CIFT and the original influence function is merely 1.26%. level, sentence-level, and model-level uncertainty expressed via token distribution. The token-level uncertainty is represented as the maximum probability of token during next-token prediction. The sentence-level uncertainty is computed based on the token-level uncertainties of all the tokens in sentence, for each prompt in pool of prompts. Finally, the model-level uncertainty is calculated by taking weighted average of the sentence-level uncertainty scores for multiple model sizes (the weights are determined by model size). This threestage process provides ranking process thus, during data selection, the points with the top-k scores are chosen."
        },
        {
            "title": "C License",
            "content": "All the code of this project is under the Apache 2.0 License. The datasets MixInstruct and Alpaca are under the MIT and Creative Commons Attribution Non Commercial 4.0 International Licenses, respectively. The code for the baselines are under the MIT and Apache 2.0 Licenses. Our use of existing artifact(s) is consistent with their intended use. The artifacts are all in English, and do not contain data with personally identifiable information."
        }
    ],
    "affiliations": [
        "UIUC"
    ]
}