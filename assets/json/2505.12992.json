{
    "paper_title": "Fractured Chain-of-Thought Reasoning",
    "authors": [
        "Baohao Liao",
        "Hanze Dong",
        "Yuhui Xu",
        "Doyen Sahoo",
        "Christof Monz",
        "Junnan Li",
        "Caiming Xiong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Inference-time scaling techniques have significantly bolstered the reasoning capabilities of large language models (LLMs) by harnessing additional computational effort at inference without retraining. Similarly, Chain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy by generating rich intermediate reasoning trajectories, but these approaches incur substantial token costs that impede their deployment in latency-sensitive settings. In this work, we first show that truncated CoT, which stops reasoning before completion and directly generates the final answer, often matches full CoT sampling while using dramatically fewer tokens. Building on this insight, we introduce Fractured Sampling, a unified inference-time strategy that interpolates between full CoT and solution-only sampling along three orthogonal axes: (1) the number of reasoning trajectories, (2) the number of final solutions per trajectory, and (3) the depth at which reasoning traces are truncated. Through extensive experiments on five diverse reasoning benchmarks and several model scales, we demonstrate that Fractured Sampling consistently achieves superior accuracy-cost trade-offs, yielding steep log-linear scaling gains in Pass@k versus token budget. Our analysis reveals how to allocate computation across these dimensions to maximize performance, paving the way for more efficient and scalable LLM reasoning."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 2 9 9 2 1 . 5 0 5 2 : r Fractured Chain-of-Thought Reasoning Baohao Liao Hanze Dong Yuhui Xu Doyen Sahoo Christof Monz Junnan Li Caiming Xiong University of Amsterdam Salesforce AI Research Abstract Inference-time scaling techniques have significantly bolstered the reasoning capabilities of large language models (LLMs) by harnessing additional computational effort at inference without retraining. Similarly, Chain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy by generating rich intermediate reasoning trajectories, but these approaches incur substantial token costs that impede their deployment in latency-sensitive settings. In this work, we first show that truncated CoT, which stops reasoning before completion and directly generates the final answer, often matches full CoT sampling while using dramatically fewer tokens. Building on this insight, we introduce Fractured Sampling, unified inference-time strategy that interpolates between full CoT and solution-only sampling along three orthogonal axes: (1) the number of reasoning trajectories, (2) the number of final solutions per trajectory, and (3) the depth at which reasoning traces are truncated. Through extensive experiments on five diverse reasoning benchmarks and several model scales, we demonstrate that Fractured Sampling consistently achieves superior accuracy-cost trade-offs, yielding steep log-linear scaling gains in Pass@k versus token budget. Our analysis reveals how to allocate computation across these dimensions to maximize performance, paving the way for more efficient and scalable LLM reasoning."
        },
        {
            "title": "1 Introduction",
            "content": "Recent advances in large language models (LLMs) have enabled impressive capabilities in complex reasoning and problem solving (Guo et al., 2025; Kojima et al., 2022; Jaech et al., 2024; Brown et al., 2020; Hurst et al., 2024; Anthropic, 2024; Team et al., 2024). While much progress has been driven by scaling model size and training data Hestness et al. (2017); Kaplan et al. (2020); Hoffmann et al. (2022), complementary direction, inference-time scaling, has gained traction (Wang et al., 2023). This approach enhances performance by increasing computational effort at inference, without altering model parameters. Techniques such as selfconsistency decoding (majority voting) (Wang et al., 2022), best-of-n sampling (Stiennon et al., 2020; Brown et al., 2024; Cobbe et al., 2021; Dong et al., 2023), and ensemble-style methods (Yao et al., 2023; Zhou et al., 2022; Liao et al., 2025) leverage multiple forward passes to produce more accurate and robust predictions from instructed models. In parallel with these inference-time scaling methods, another line of work has focused on improving the quality of individual reasoning paths. Chain-of-Thought (CoT) prompting (Wei et al., 2022) has emerged as particularly effective technique by encouraging models to articulate intermediate reasoning steps before arriving at final answer. Recently, Long Chain-of-Thought (Long-CoT) reasoning (Guo et al., 2025; Jaech et al., 2024) introduces longer and more diverse reasoning trajectories, often incorporating mechanisms like self-reflection and self-correction (Kumar et al., 2024). These extended CoTs explore broader solution space *BL, HD, YX contributed equally to this work. Correspondence to HD and YX: {hanze.dong,yuhui.xu}@salesforce.com. 1 and aggregate diverse intermediate steps into single response. This has been shown to significantly improve accuracy and robustness, especially for tasks that require multi-step or logical reasoning. The downside is that they also dramatically increase token usage, resulting in higher inference costs. Combining inference-time scaling with Long-CoT methods (e.g., using Long-CoT with self-consistency decoding) further amplifies this computational burden. Each technique alone may require thousands of additional tokens per input; together, they often push token budgets to impractical levels, making such methods unsuitable for latency-sensitive or resource-constrained applications. This raises central question: Can we retain the benefits of Long-CoT reasoning without incurring the full cost? To address this, we revisit the common assumption that complete Long-CoT traces are essential for accurate reasoning. Surprisingly, we find that incomplete CoT trajectories, i.e., traces truncated before the final answer, can still yield highly accurate results. As shown in Figure 1, across five reasoning benchmarks, simply truncating the CoT prefix and generating the answer (dashed orange) matches or even exceeds the accuracy of full CoT sampling (solid blue) given max token constraint. This result challenges the notion that more reasoning always leads to better outcomes and suggests new frontier for efficiency: partial reasoning traces. To systematically trade off between cost and performance, we propose Fractured Sampling, unified inferencetime strategy that interpolates between full CoT and solution-only sampling. As illustrated in Figure 2(a), Fractured Sampling explores three orthogonal dimensions: 1. Thinking trajectories: the number of distinct CoT prefixes sampled; 2. Solution diversity: the number of final answers generated per prefix; 3. Thinking prefix length: the depth at which each CoT is truncated. Figure 2(b) further reveals that thinking steps (blue) dominate the overall token count, while final solutions (orange) contribute minimally, highlighting ample opportunities to optimize reasoning depth and breadth. Contributions. Our key contributions are as follows: (1) We show that truncated CoT trajectories often achieve comparable or better performance than full CoT, at fraction of the inference cost. (2) We propose Fractured Sampling, unified inference-time framework that jointly controls reasoning depth, diversity, and token efficiency. (3) We provide comprehensive analysis of the scaling behavior of Fractured Sampling across multiple reasoning benchmarks, offering practical insights into efficient inference strategies for LLMs."
        },
        {
            "title": "2 Preliminary",
            "content": "Notations. Let denote the input prompt and ε be random seed used to introduce stochasticity. The instruct LLM generates an initial response as follows: = (x, ε), and parser extracts the final answer: = g(z). Baseline sampling schemes. Before introducing our method, we review common sampling-based inference techniques widely used to enhance output quality from LLMs. Vanilla Sampling. This approach generates independent completions by sampling with different random seeds: Fn(x, ε1:n) = {g (x, εi) = 1, . . . , n} . 2 Figure 1: Pass@1 accuracy versus maximum token budget for DeepSeek-R1-Distill-Qwen-1.5B (1st row) and 7B (2nd row) on reasoning benchmarks. Solid blue lines show the original full chain-of-thought (CoT) sampling, while dashed orange lines show our truncated CoT + response approach. Across all benchmarks, truncating the CoT (and generating the final answer) achieves equal or better accuracy with substantially fewer tokens, demonstrating that full CoT is unnecessary and that truncating CoT can save large amount of computation without sacrificing performance. Pass@k. The pass@k metric estimates the probability that at least one of the samples is correct: pass@k = ( yi Fk(x, ε1:k) s.t. yi is correct) . Best-of-n. This strategy selects the most confident response among candidates. If s(z) denotes scoring function (e.g., reward model), the best-of-n output is: (cid:32) (cid:33) ybest = argmax zif (x,ε1:n) s(zi) . These inference-time strategies serve as foundations for improving reliability and robustness in model predictions, especially for reasoning-intensive tasks. Our approach builds on the sampling method by explicitly leveraging internal reasoning traces to enhance sample efficiency and answer diversity. Reasoning LLMs and long-CoT thinking process. To better capture intermediate reasoning steps, reasoning-augmented LLMs use CoT mechanism. Instead of producing direct answer, the model first generates reasoning trace: = [hε 1, , hε ] = fh(x, ε), where denotes the total number of reasoning steps. The final response is then generated conditioned on the full thought process: = fo(x, h, ε). This CoT formulation provides richer supervision and enables more structured sampling strategies, which our approach builds upon to enhance efficiency and performance. To better reflect the internal reasoning process, we enhance diversity by sampling additional random seeds for each of thinking processes: Fn,m(x, ε1:n, ε1:n,1:m) = {g fo(x, fh(x, εi), εi,j) = 1, , n; = 1, , m} . 3 (a) Sampling strategies for reasoning LLMs. (b) Token statistics. Figure 2: (a) Comparison of sampling strategies for reasoning LLMs. Top: Sampling Trajectoriesmultiple complete reasoning chains are sampled independently from the model. Middle: Sampling Responsesa single reasoning chain is used to generate diverse final responses. Bottom: Fractured Samplingour proposed method samples across both multiple reasoning trajectories and intermediate reasoning steps, enabling finegrained control over diversity and computation. (b) Token statistics across tasks and models. Bars represent the average token count per sample, broken down into reasoning steps (blue) and final solutions (orange). The thinking process dominates the overall cost. However, standard sampling methods only operate on the reasoning trajectory or the final outputs, overlooking the models intermediate reasoning dynamics. To fully exploit the internal structure of CoT reasoning, we propose sampling not just across independent trajectories, but also across intermediate reasoning stages."
        },
        {
            "title": "3 Fractured sampling for long chain-of-thought reasoning",
            "content": "To formalize the intermediate reasoning process, we denote the partial reasoning trace up to step as: 1:t = [hε hε 1, , hε ] = h(x, ε). Our approach leverages intermediate reasoning traces to aggregate predictions, thereby enhancing both efficiency and diversity. The key idea is to decompose the response generation into multiple stages and perform aggregation not only over independent final responses but also across intermediate reasoning steps. Fractured sampling for reasoning LLMs. Fractured sampling extends this idea by incorporating intermediate reasoning stages directly into the sampling process. Specifically, we sample responses at each step of the reasoning chain: Fn,m,H (x, ε1:n, ε1:H 1:n,1:m) = (cid:8)g fo (cid:0)x, h(x, εi), εt i,j (cid:1) (cid:12) (cid:12) = 1, , n; = 1, , m; = 1, , H(cid:9) . h(x, εi) denotes the partial reasoning trace up to step t, and εt Here, i,j is the random seed used for generating the response at that stage. By aggregating responses across all intermediate steps, fractured sampling captures the evolving thought process and synthesizes diverse insights into more robust final answer. Fractured sampling offers two primary advantages: (1) Granular Aggregation: Integrating intermediate reasoning steps enables early detection of conclusions and avoid overthinking, improving the consistency of final predictions. (2) Enhanced Diversity: The multi-level sampling mechanism encourages wide range of 4 reasoning trajectories. Aggregating these paths produces consensus that is more resilient to individual failures. Three orthogonal dimensions of sampling. Fractured Sampling unifies and extends existing sampling strategies by operating along three orthogonal axes: m: Solution Diversity sampling multiple final outputs from single reasoning trace. n: Trajectory Diversity sampling multiple independent reasoning traces with different seeds (vanilla CoT sampling). H: Reasoning Depth Diversity sampling at different intermediate stages of single reasoning trace (unique to fractured sampling). This tri-dimensional framework enables fine-grained exploration of the costperformance landscape. While and offer diversity at the output or full-trajectory level, the dimension uniquely captures the temporal evolution of reasoning, offering early, diverse, and efficient decision points. Together, they provide powerful toolkit for scalable and reliable inference-time reasoning. Empirical results (see Section 4) show that fractured sampling is strong methods to produce diverse and meaningful solutions."
        },
        {
            "title": "3.1 Analysis of fractured sampling",
            "content": "Fractured sampling benefits from diverse solutions. By distributing samples across both trajectories and intermediate steps, fractured sampling capitalizes on diverse error modes to boost overall success. The following proposition provides an analysis about our phenomenon. Proposition 1 (Diversity Lower Bound, informal). Let Fk be the indicator of failure for branch sample and qk = (Fk = 1). Then the fractured-sampling success probability satisfies pseg = 1 Pr(cid:0)K k=1Fk = 1(cid:1) = 1 (cid:104) (cid:89) (cid:105) , Fk and by inclusionexclusion k=1 (cid:104) (cid:89) (cid:105) Fk = k=1 (cid:89) k= qk + (cid:88) i<j Cov(Fi, Fj) + . That is to say, negative covariance Cov(Fi, Fj) 0 means that failures at two different samples i, tend not to coincide, i.e. the two sampling locations provide diverse error modes. If we only consider the second order expansion, we have pseg 1 (cid:81)K t=1(1 pt)m. Fractured sampling spreads samples across intermediate steps to maximize this diversity: because failures are unlikely to all happen together, the probability that every sample fails is strictly less than the naıve product of their marginal failure rates. Consequently, the overall success probability pseg is boosted above the independent-baseline 1 (cid:81)(1 pt)m. k=1 qk = 1 (cid:81)H To understand the limits of fractured sampling, we examine two extreme correlation regimes among the = mH branch samples. 5 Figure 3: Correlation matrices of binary failure indicators across intermediate reasoning depths (positions H) under fractured sampling for five benchmarks. Each cell shows the Pearson correlation coefficient between failure events at two depth positions; green denotes positively correlated failures (synchronized error modes), while pink denotes negatively correlated failures (diverse error modes) that fractured sampling exploits to boost overall success. Almost perfect correlation. When every sample fails or succeeds in unison (Fi = Fj almost surely), the entire set of trials collapses to single Bernoulli event. In this case, Pr(F1 = = FK = 1) = q, pseg = 1 q, so the sampling reduces to plain single-step sampling and yields no extra benefit. Sampling only along the m-axis (multiple outputs per trace) behaves similar to this. Full independence. If all Fk are mutually independent with Pr(Fk = 1) = qk, then Pr(F1 = = FK = 1) = (cid:89) k= (cid:89) qk = (1 pt)m, pseg = 1 t=1 (cid:89) (1 pt)m. t=1 The sampling achieves the product-of-marginals bound: diversity arises purely from geometric averaging of each steps success rate. Standard trajectory sampling (n-axis) behaves similar to this regime with single successful rate p. Intermediate regimes. Between these extremes, negative pairwise covariances (Cov(Fi, Fj) < 0) drive the all-fail probability below the independent baseline, delivering gains beyond simple marginal aggregation. By contrast, positive correlations (Cov(Fi, Fj) > 0) erode this advantage, interpolating smoothly between full independence and perfect correlation. Sampling along the depth dimension exploits these intermediate correlations to maximize diversity and overall success. As illustrated in Figure 3, the correlation matrices shows how failure events at different reasoning depths co-occur across five diverse benchmarks (MATH500 L5, AIME24, AIME25, AIMO2 and GPQA). Dark green cells along the diagonal indicate that failures at the same depth are, by definition, almost perfectly correlated. More interestingly, the off-diagonal pattern varies by task: many entries are light or even pink (negative), signalling that failures at two distinct depths tend not to happen simultaneously. This negative covariance across depths is precisely what fractured sampling exploits, by spreading samples over intermediate stages, it decorrelates error modes and thus markedly reduces the probability that all branch samples fail together. Benchmarks with stronger negative off-diagonal structure (e.g. GPQA) exhibit the largest gains from fractured sampling, confirming our theoretical diversity lower-bound analysis."
        },
        {
            "title": "3.2 Scaling Laws Along the Trajectory Dimension",
            "content": "In fractured sampling, we allocate computation across three orthogonal axes (n, m, H). Here, we hold the branching factor and fracturing depth constant, and investigate how increasing the number of independent trajectories affects performance under fixed token budget. Denote the total tokens consumed as B(n, m, H) = Cthinking + Csolution = n(cid:2)Cthinking + mH Csolution where Cthinking is the average tokens per trajectory spent on thinking (the reasoning prefix), and Csolution is the per-step cost of generating each candidate solution. (cid:3), Log-linear scaling behavior. Empirical studies across diverse benchmarks reveal remarkably consistent log-linear relationship between computational budget and success rate along each axis: pass@k(cid:0)Bn pass@k(cid:0)Bm pass@k(cid:0)BH (cid:1) Cn log Bn + cn, (cid:1) Cm log Bm + cm, (cid:1) CH log BH + cH , Bn = B(n, 1, 1), Bm = B(1, m, 1), BH = B(1, 1, H). Here, the constants Cn, Cm, CH measure the marginal gain in log-budget per unit improvement in pass rate, while cn, cm, cH capture dataset-specific offsets. Depth yields the steepest slope. Across range of tasks, we consistently find CH max{Cn, Cm}, indicating that allocating tokens to deeper intermediate sampling (the axis) produces the largest incremental improvements per token. Intuitively, early-stage branching captures coarse but high-signal glimpses of the solution space, allowing the model to course-correct before committing to full trajectories and thus yielding higher gains for each additional intermediate sample. Beyond single-axis scaling. While single-axis laws offer valuable intuition, actual performance often improves when (n, m, H) are tuned jointly. Since the n-axis contributes additively and independently, we condition on fixed (m, H) and model pass@k(cid:0)B(cid:1) Cm,H log B(cid:0)n m, H(cid:1) + cm,H , where the coefficient Cm,H encapsulates the combined effect of branching factor and depth. These crossterms reveal synergistic gains or trade-offs between axes, guiding more nuanced budget allocations. We explore these interactions and derive dataset-specific strategies in Section 4."
        },
        {
            "title": "4 Empirical results",
            "content": "Settings. All inference experiments are conducted using NVIDIA A100-80GB GPUs, leveraging the vLLM framework (Kwon et al., 2023). Following the sampling configuration recommended by Guo et al. (2025), we set temperature=0.6, top p=0.95, and max tokens=32768. Our primary focus is on models from the DeepSeek-R1 family (Guo et al., 2025), and we further validate our findings using reasoning models from Qwen3 (Team, 2025), Skywork-OR1 (He et al., 2025), and DeepScaler (Luo et al., 2025). 7 Evaluation is performed on five challenging math and scientific reasoning benchmarks: MATH500 Level 5 (L5) (Lightman et al., 2023), AIME24, AIME25-I (MAA Committees, 2025), AIMO2 reference questions (Frieder et al., 2024), and the GPQA Diamond set (Rein et al., 2024), containing 134, 30, 15, 10, and 198 questions, respectively. Unless otherwise specified, we set = 16, = 16 and = 4. = 16 indicates that the original thinking CoT is divided into 16 equally sized segments based on token count. For instance, the third fractured CoT consists of the first three segments of the full thinking trajectory."
        },
        {
            "title": "4.1 Scaling law for each dimension",
            "content": "Figure 4 plots pass@k versus total tokens for the three sampling schemes under matched cost. Across all benchmarks and model sizes, fractured sampling exhibits the steepest log-linear gains per token. In particular, we fit pass@k(cid:0)B (cid:1) log + c, {n, m, H}, and consistently observe CH max{Cn, Cm}. This confirms that allocating budget to intermediate-step branching yields higher marginal returns than either sampling more independent traces or more final answers alone. Interpreting the gains. Fractured sampling captures rich, underutilized variation in intermediate reasoning states, allowing the model to course-correct early and avoid committing to error-prone trajectories. This leads to: (1) Higher early returns: At small budgets, Hsampling yields much steeper rise in pass rate than or m, since few intermediate samples can quickly pinpoint correct partial reasoning. (2) Consistent dominance: The gap between the H-curve and the others persists across all budgets, demonstrating robustness to scale. (3) Task-dependent effect: Benchmarks with less positive error correlations across depths (e.g. GPQA) show the largest absolute improvement from fractured sampling, in line with our diversity-bound analysis. These results empirically validate that, under the same compute budget, fractured sampling shifts the inference-time scaling curve upward, achieving higher accuracy at lower cost by leveraging the temporal structure of chain-of-thought."
        },
        {
            "title": "4.2 Scaling law across dimensions",
            "content": "Thus far we have examined each sampling axis in isolation. Figure 5 extends this analysis by comparing four representative schemes that allocate budget across the solution (m) and depth (H) dimensions simultaneously, with the trajectory axis (n) swept to 16. We have: (1) (H=1, m=1): standard singlepath CoT sampling (baseline). (2)(H=1, m=4): augment baseline with 4 final answers per trajectory. (3) (H=16, m=1): fractured sampling across 16 depths, one answer each. (4) (H=16, m=4): full three-axis sampling (both deep fracturing and multiple final answers). Across every task and model, non-baseline schemes (excl. (H=1, m=4)) outperform (H=1, m=1) at fixed budget. More importantly, expanding is usually more effective than expanding m. These multi-axis scaling laws reveal that, under the same token budget, the most efficient use of compute is to allocate tokens for temporal branches H. Final-solution replicates may also work in some cases."
        },
        {
            "title": "4.3 Best-of-N across dimensions",
            "content": "In prior experiments, we reported the metric pass@k, which indicates whether correct prediction is present among set of generated samples. In this section, we further examine whether correct solution can be 8 Figure 4: Pass@k performance versus total token budget for three sampling schemes on five benchmarks. In each subplot, we compare: (green dotted)sampling only the final solution; (blue solid)sampling full reasoning trajectories; (orange dashed)fractured sampling across all intermediate steps. Rows correspond to DeepSeek-R1-Distill-Qwen-1.5B, 7B and DeepSeek-R1 models. Fractured sampling (H) consistently yields higher pass@k at given token budget. Please refer to Figure B.1 for DeepScaleR and Qwen3 with similar pattern. identified by reward model from among the predictions generated across the three sampling axes. To this end, we employ the process reward model (PRM), specifically Qwen2.5-Math-PRM-72B (Zhang et al., 2025), which has demonstrated strong performance across range of PRM benchmarks. Due to its limited context window (4K tokens), we score only the final solution rather than the intermediate reasoning steps. The reward assigned to the final step is used as the overall score for the entire solution. Table 1: Best-of-N accuracy with different dimensional settings. = 16 here for all settings. = 1, = 1 denotes the standard sampling setting. = 4 here means that we take the last 4 solutions among all 16 predictions in the dimension. As shown in Table 1, sampling with = 1, = 4 yields modest improvement in average accuracy compared to the standard sampling setting of = 1, = 1 (61.6% vs. 60.4%). Interestingly, increasing only the dimension to = 16, = 1 also leads to slight improvement (61.4% vs. 60.4%), which contrasts with our earlier observation that varying is typically more effective than varying in terms of pass@k. We hypothesize that incorporating all = 16 generated solutions introduces excessive noise, making it challenging for PRM to correctly identify the optimal solution. This may be due to two factors: (1) the Long-CoT model tends to generate coherent and logically consistent solutions, which are difficult for the PRM to differentiate; (2) the PRM is trained predominantly on simpler and short-CoT data and may struggle to evaluate responses to more complex and long ones. DS-R1-Qwen-7B 1 1 1 16 1 -4 4 1 4 16 4 -4 DS-R1-Qwen-14B 1 1 53.3 53.3 60.0 53.3 60.0 60.0 63.3 70.0 73.3 70.0 73.3 73.3 40.0 40.0 60.0 40.0 50.0 70.0 90.3 90.3 93.3 90.3 92.5 94. 55.1 53.5 53.5 54.6 57.6 56.1 MATH500 L5 AIME25 AIME24 AIMO2 91. 60.0 80.0 50.0 59.6 GPQA Avg. 60.4 61.4 68.0 61.6 66.7 70.8 68.3 Motivated by the trend observed in Figure 6where later reasoning positions (i.e., higher indices) are associated with improved accuracywe apply simple denoising strategy by discarding earlier solutions 9 Figure 5: Pass@k performance versus total token budget for four sampling schemes on five benchmarks. In each subplot, we compare: H=1, m=1only sampling full reasoning trajectory; H=1, m=4sampling both full reasoning trajectories and the final solution; H=16, m=1both full reasoning trajectory sampling and fractured sampling across all intermediate steps; H=16, m=4sampling all three dimensions. is in [1, 2, 4, 8, 16] for the five points (from left to right) on each line. Rows correspond to DeepSeek-R1-DistillQwen-1.5B and DeepSeek-R1 models. Please refer to Figure B.2 for DeepScaleR and Qwen3 with similar pattern. (H = 1 to = 11) and retaining only the last four (H = 4). This simple adjustment significantly enhances performance, raising the accuracy from 61.4% (H = 16, = 1) to 68.0% (H = 4, = 1). Further combining both dimensions (H = 4, = 4) yields an accuracy of 70.8%, 10.4% improvement over the baseline setting (H = 1, = 1). Notably, this configuration even outperforms standard sampling with larger model that has twice the number of parameters (70.8% vs. 68.3%). We hypothesize that welltrained PRM could even further push the limit, since Qwen2.5-Math-PRM-72B is trained with short-CoT data. Figure 6: Accuracy versus the position of fractured CoT. We split the whole reasoning trajectory into 16 intermediate steps. We can observe: (1) Even with 1 16 reasoning trajectory, the accuracy is still decent, especially for GPQA; (2) More reasoning tokens lead to higher accuracy. 10 Table 2: The relative performance for early stop compared to vanilla sampling (DeepSeek-R1-Distill-Qwen1.5B, DeepScaleR-1.5B-Preview and Skywork-OR1-7B-Preview). Overall, early stopping significantly saves inference budget while preserving the accuracy. Model Method Accuracy (%) Number of Tokens per Question (K) MATH500 AIME25 AIMO2 GPQA Avg. MATH500 AIME25 AIMO2 GPQA Avg. DS-R1-1.5B DSR-1.5B SW-OR1-7B Vanilla Early Stop Vanilla Early Stop Vanilla Early Stop 70.8 +1.2 76.5 -0.9 89.0 -0.4 27.5 -0.0 41.7 -0.0 45.0 -0. 15.0 +10.6 20.0 -0.0 47.5 -0.0 34.1 -0.3 19.2 +0.4 48.6 +1. 36.9 +2.9 39.4 -0.1 57.5 +0.2 8.8 -1.4 5.1 -0.9 6.7 -0. 17.4 -2.3 8.3 -1.1 13.4 -2.5 21.1 -7.1 10.6 -3.6 14.9 -3. 10.0 -0.6 7.8 -0.2 8.5 -2.2 14.3 -2.9 8.0 -1.5 10.9 -2."
        },
        {
            "title": "4.4 Early stopping for efficient generation",
            "content": "From the perspective of inference efficiency, we explore whether the consistency of predictions across the dimension can be leveraged for early stopping. Specifically, if particular prediction appears with high frequency (i.e., exceeds predefined threshold) across multiple positions, we consider this as signal to terminate the generation early, thereby reducing computational cost. As illustrated in Figure 6, prediction accuracy tends to be low at earlier positions. When the reasoning trace is divided into too many intermediate steps (i.e., larger H), the model must generate correspondingly large number of partial solutions, each requiring additional tokens. To balance computational efficiency and accuracy, we empirically initialize the first position at token index of 6144 and evaluate predictions at every subsequent 2048-token interval. For example, given question, the model first generates 6144 reasoning tokens. Based on these tokens, solution is generated and prediction is extracted. Then, conditioned on the original question and the previously generated 6144 reasoning tokens, the model continues generating another 2048 tokens to produce the next prediction. Generation terminates once the same prediction occurs more than once or when the maximum token limit (max tokens) is reached. In the latter case, we adopt the final prediction, as later predictions tend to benefit from more extensive reasoning. As shown in Table 2, this early stopping strategy preserves model accuracy and, in some cases, improves itachieving 2.9% increase for DeepScaleR-1.5B-Preview. In terms of computational efficiency, early stopping reduces the number of generated tokens by approximately 20% compared to standard generation. Notably, this method is simple to implement and requires no additional training. Related Work. Due to the page limit, please refer to the Appendix 5."
        },
        {
            "title": "5 Related work",
            "content": "Test-time scaling law. Scaling laws have traditionally described how model performance improves with increased training compute (Hestness et al., 2017; Kaplan et al., 2020; Hoffmann et al., 2022), e.g., through more supervised fine-tuning or reinforcement learning steps. However, complementary class of test-time scaling laws has emerged (Snell et al., 2024; Jaech et al., 2024), which characterizes performance gains obtained purely by increasing inference-time budget, without modifying model parameters. This includes techniques such as self-consistency decoding (Wang et al., 2022), best-of-n sampling (Brown et al., 2024; Cobbe et al., 2021; Dong et al., 2023). On the other hand, CoT prompting, where performance improves with more samples or longer reasoning traces (Wei et al., 2022). Recent work, including the O1 and R1 series (Jaech et al., 2024; Guo et al., 2025), further demonstrates that extended trajectories (e.g., Long CoT) with multiple rollouts yields predictable improvements under test-time scaling curves. On the other hand, Process Reward Models (PRMs) (Lightman et al., 2023; Zhang et al., 2024a; Wang 11 et al., 2023) further enable fine-grained control by assigning dense, step-level rewards, which can guide search methods like Monte Carlo Tree Search (Luo et al., 2024). However, most approaches scale only along coarse dimensions, such as sample count or token length, or require external supervision via PRMs for finer control. In this work, we propose more fine-grained view through Fractured Sampling without relying on PRMs, which explicitly decomposes generation into multi-stage reasoning traces and enables aggregation at intermediate steps. This design reveals richer scaling behaviors across trajectory depth, diversity, and stage-wise composition, and offers more nuanced understanding of inference-time compute allocation. Efficient sampling for LLMs. As large language models grow in size and capability, their inference cost becomes significant bottleneck (Wan et al., 2023), especially when relying on multi-sample or multi-turn decoding strategies in reinforcement learning (Ouyang et al., 2022; Xiong et al., 2023; Dong et al., 2024; Xiong et al., 2025; Shao et al., 2024) or large-scale serving (Ainslie et al., 2023). This has motivated line of work on efficient sampling, which aims to reduce compute without sacrificing performance. Approaches such as speculative decoding (Stern et al., 2018; Leviathan et al., 2023; Xia et al., 2024; Chen et al., 2023a; Zhang et al., 2023; Sun et al., 2024; Chen et al., 2023b; Li et al., 2024b; Liao et al., 2025), KV cache pruning (Xu et al., 2024; Xiao et al., 2023; Zhang et al., 2024c; Li et al., 2024a; Ge et al., 2023; Zhang et al., 2024b; Yang et al., 2024; Liu et al., 2024), are widely used in real-world LLM services. While these methods achieve notable efficiency gains, they largely operate within fixed test-time scaling curve: improving the efficiency of given point on the curve without fundamentally changing its shape. In contrast, we argue that the most principled path forward lies in reshaping the scaling law itself: by rethinking how inference budget is allocated across reasoning stages and sampling axes, one can unlock qualitatively different computeperformance tradeoffs. Our proposed Fractured Sampling method embodies this principle, revealing richer scaling dynamics and enabling more cost-effective reasoning through staged aggregation."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we introduce Fractured Sampling, new Long-CoT inference paradigm that seamlessly unifies partial-trace and final-answer sampling by jointly controlling reasoning depth, trajectory diversity, and solution diversity. We uncover consistent loglinear scaling trends along each axis and offer theoretical insights into how sampling across intermediate reasoning steps maximizes diversity and per-token gains. Fractured Sampling redefines the costperformance frontier of chain-of-thought inference, enabling powerful reasoning in LLMs with lower computational overhead."
        },
        {
            "title": "References",
            "content": "Ainslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y., Lebron, F., and Sanghai, S. (2023). Gqa: arXiv preprint Training generalized multi-query transformer models from multi-head checkpoints. arXiv:2305.13245. Anthropic, A. (2024). Claude 3.5 sonnet model card addendum. Claude-3.5 Model Card, 3. Brown, B., Juravsky, J., Ehrlich, R., Clark, R., Le, Q. V., Re, C., and Mirhoseini, A. (2024). Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances in neural information processing systems, 33:18771901. Chen, C., Borgeaud, S., Irving, G., Lespiau, J.-B., Sifre, L., and Jumper, J. (2023a). Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318. Chen, Z., Yang, X., Lin, J., Sun, C., Chang, K. C.-C., and Huang, J. (2023b). Cascade speculative drafting for even faster llm inference. arXiv preprint arXiv:2312.11462. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. (2021). Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Dong, H., Xiong, W., Goyal, D., Zhang, Y., Chow, W., Pan, R., Diao, S., Zhang, J., SHUM, K., and Zhang, T. (2023). RAFT: Reward ranked finetuning for generative foundation model alignment. Transactions on Machine Learning Research. Dong, H., Xiong, W., Pang, B., Wang, H., Zhao, H., Zhou, Y., Jiang, N., Sahoo, D., Xiong, C., and Zhang, T. (2024). Rlhf workflow: From reward modeling to online rlhf. arXiv preprint arXiv:2405.07863. Frieder, S., Bealing, S., Nikolaiev, A., Smith, G. C., Buzzard, K., Gowers, T., Liu, P. J., Loh, P.-S., Mackey, L., de Moura, L., Roberts, D., Sculley, D., Tao, T., Balduzzi, D., Coyle, S., Gerko, A., Holbrook, R., Howard, A., and Markets, X. (2024). Ai mathematical olympiad - progress prize 2. https://kaggle. com/competitions/ai-mathematical-olympiad-progress-prize-2. Kaggle. Ge, S., Zhang, Y., Liu, L., Zhang, M., Han, J., and Gao, J. (2023). Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. (2025). Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. He, J., Liu, J., J., Liu, C. Y., Yan, R., Wang, C., Cheng, P., Zhang, X., Zhang, F., and Zhou, Y. https://capricious-hydrogen-41c.notion.site/ Xu, (2025). series. Skywork-Open-Reaonser-Series-1d0bc9ae823a80459b46c149e4f51680. Notion Blog. S., Zeng, L., Wei, T., Cheng, C., Liu, Y., Skywork open reasoner Shen, W., Li, Hestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H., Kianinejad, H., Patwary, M. M. A., Yang, Y., and Zhou, Y. (2017). Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409. Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A., et al. (2022). Training compute-optimal large language models. arXiv preprint arXiv:2203.15556. Hurst, A., Lerer, A., Goucher, A. P., Perelman, A., Ramesh, A., Clark, A., Ostrow, A., Welihinda, A., Hayes, A., Radford, A., et al. (2024). Gpt-4o system card. arXiv preprint arXiv:2410.21276. Jaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., et al. (2024). Openai o1 system card. arXiv preprint arXiv:2412.16720. Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. (2020). Scaling laws for neural language models. arXiv preprint arXiv:2001.08361. Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. (2022). Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213. Kumar, A., Zhuang, V., Agarwal, R., Su, Y., Co-Reyes, J. D., Singh, A., Baumli, K., Iqbal, S., Bishop, C., Roelofs, R., et al. (2024). Training language models to self-correct via reinforcement learning. arXiv preprint arXiv:2409.12917. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J. E., Zhang, H., and Stoica, I. (2023). Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. Leviathan, Y., Kalman, M., and Matias, Y. (2023). Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pages 1927419286. PMLR. Li, Y., Huang, Y., Yang, B., Venkitesh, B., Locatelli, A., Ye, H., Cai, T., Lewis, P., and Chen, D. (2024a). Snapkv: Llm knows what you are looking for before generation. arXiv preprint arXiv:2404.14469. Li, Y., Wei, F., Zhang, C., and Zhang, H. (2024b). Eagle: Speculative sampling requires rethinking feature uncertainty. arXiv preprint arXiv:2401.15077. Liao, B., Xu, Y., Dong, H., Li, J., Monz, C., Savarese, S., Sahoo, D., and Xiong, C. (2025). Reward-guided speculative decoding for efficient llm reasoning. arXiv preprint arXiv:2501.19324. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. (2023). Lets verify step by step. arXiv preprint arXiv:2305.20050. Liu, Z., Yuan, J., Jin, H., Zhong, S., Xu, Z., Braverman, V., Chen, B., and Hu, X. (2024). Kivi: tuning-free asymmetric 2bit quantization for kv cache. arXiv preprint arXiv:2402.02750. Luo, L., Liu, Y., Liu, R., Phatale, S., Guo, M., Lara, H., Li, Y., Shu, L., Zhu, Y., Meng, L., et al. (2024). Improve mathematical reasoning in language models by automated process supervision. arXiv preprint arXiv:2406.06592. S., Wong, L. E., Popa, R. A., Luo, M., Tan, J., Li, preview with DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2. Notion Blog. Shi, X., Tang, W. Y., Roongta, M., Cai, C., Luo, (2025). o1https://pretty-radio-b75.notion.site/ and Stoica, rl. 1.5b model Deepscaler: Surpassing scaling J., by I. MAA Committees (2025). AIME Problems and Solutions. https://artofproblemsolving.com/wiki/ index.php/AIME_Problems_and_Solutions. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. (2022). Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:2773027744. Rein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y., Dirani, J., Michael, J., and Bowman, S. R. (2024). Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. (2024). Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. 14 Snell, C., Lee, J., Xu, K., and Kumar, A. (2024). Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314. Stern, M., Shazeer, N., and Uszkoreit, J. (2018). Blockwise parallel decoding for deep autoregressive models. Advances in Neural Information Processing Systems, 31. Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, P. F. (2020). Learning to summarize with human feedback. Advances in neural information processing systems, 33:30083021. Sun, H., Chen, Z., Yang, X., Tian, Y., and Chen, B. (2024). Triforce: Lossless acceleration of long sequence generation with hierarchical speculative decoding. arXiv preprint arXiv:2404.11912. Team, G., Georgiev, P., Lei, V. I., Burnell, R., Bai, L., Gulati, A., Tanzer, G., Vincent, D., Pan, Z., Wang, S., et al. (2024). Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530. Team, Q. (2025). Qwen3. Wan, Z., Wang, X., Liu, C., Alam, S., Zheng, Y., Qu, Z., Yan, S., Zhu, Y., Zhang, Q., Chowdhury, M., et al. (2023). Efficient large language models: survey. arXiv preprint arXiv:2312.03863, 1. Wang, P., Li, L., Shao, Z., Xu, R., Dai, D., Li, Y., Chen, D., Wu, Y., and Sui, Z. (2023). Math-shepherd: Verify and reinforce llms step-by-step without human annotations. arXiv preprint arXiv:2312.08935. Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., and Zhou, D. (2022). Selfconsistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. (2022). Chain-ofthought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837. Xia, H., Yang, Z., Dong, Q., Wang, P., Li, Y., Ge, T., Liu, T., Li, W., and Sui, Z. (2024). Unlocking efficiency in large language model inference: comprehensive survey of speculative decoding. arXiv preprint arXiv:2401.07851. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. (2023). Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453. Xiong, W., Dong, H., Ye, C., Wang, Z., Zhong, H., Ji, H., Jiang, N., and Zhang, T. (2023). Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint. arXiv preprint arXiv:2312.11456. Xiong, W., Yao, J., Xu, Y., Pang, B., Wang, L., Sahoo, D., Li, J., Jiang, N., Zhang, T., Xiong, C., et al. from rejection sampling to reinforce. arXiv preprint (2025). minimalist approach to llm reasoning: arXiv:2504.11343. Xu, Y., Jie, Z., Dong, H., Wang, L., Lu, X., Zhou, A., Saha, A., Xiong, C., and Sahoo, D. (2024). Think: Thinner key cache by query-driven pruning. arXiv preprint arXiv:2407.21018. Yang, D., Han, X., Gao, Y., Hu, Y., Zhang, S., and Zhao, H. (2024). Pyramidinfer: Pyramid kv cache compression for high-throughput llm inference. arXiv preprint arXiv:2405.12532. Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y. (2023). React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR). 15 Zhang, H., Wang, P., Diao, S., Lin, Y., Pan, R., Dong, H., Zhang, D., Molchanov, P., and Zhang, T. (2024a). Entropy-regularized process reward model. arXiv preprint arXiv:2412.11006. Zhang, J., Wang, J., Li, H., Shou, L., Chen, K., Chen, G., and Mehrotra, S. (2023). Draft & verify: Lossless large language model acceleration via self-speculative decoding. arXiv preprint arXiv:2309.08168. Zhang, Y., Gao, B., Liu, T., Lu, K., Xiong, W., Dong, Y., Chang, B., Hu, J., Xiao, W., et al. (2024b). Pyramidkv: Dynamic kv cache compression based on pyramidal information funneling. arXiv preprint arXiv:2406.02069. Zhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai, R., Song, Z., Tian, Y., Re, C., Barrett, C., et al. (2024c). H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36. Zhang, Z., Zheng, C., Wu, Y., Zhang, B., Lin, R., Yu, B., Liu, D., Zhou, J., and Lin, J. (2025). The lessons of developing process reward models in mathematical reasoning. arXiv preprint arXiv:2501.07301. Zhou, D., Scharli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans, D., Cui, C., Bousquet, O., Le, Q., et al. (2022). Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625."
        },
        {
            "title": "A Proof of the Diversity Lower Bound",
            "content": "Proof. Let qi = Pr(Fi = 1) = E[Fi] and denote µij = E[FiFj], µijk = E[FiFjFk], µijkl = E[FiFjFkFℓ]."
        },
        {
            "title": "Define the joint cumulants",
            "content": "κij = µij qiqj, κijk = µijk µijqk µikqj µjkqi + 2qiqjqk. Using the inclusion-exclusion identity (cid:81) yields the exact expansion: Fk = (cid:80) I[K](1)I (cid:81) iI (1 Fi) and collecting equal-order terms (cid:104)(cid:81)K k=1 Fk (cid:105) = (cid:81)K k=1 qk + (cid:80) i<j κij + (cid:80) i<j<k κijk + (cid:80) i<j<k<ℓ κijkl + + κ1,2,...,K (1) The dots represent cumulants of order four and higher. Equation (1) can be written compactly as where κI is the joint cumulant on the index set (with κ{i} = qi and κ = 1). (cid:104) (cid:89) (cid:105) Fk = (cid:88) κI , k=1 I[K]"
        },
        {
            "title": "B More results",
            "content": "Figure B.1: Pass@k performance versus total token budget for three sampling schemes on five benchmarks. In each subplot, we compare: (green dotted)sampling only the final solution; (blue solid)sampling full reasoning trajectories; (orange dashed)fractured sampling across all intermediate steps. Rows correspond to DeepScaleR-1.5B-Preview and Qwen3-1.7B models models. Fractured sampling (H) consistently yields higher pass@k at given token budget. 17 Figure B.2: Pass@k performance versus total token budget for four sampling schemes on five benchmarks. In each subplot, we compare: H=1, m=1only sampling full reasoning trajectory; H=1, m=4sampling both full reasoning trajectories and the final solution; H=16, m=1both full reasoning trajectory sampling and fractured sampling across all intermediate steps; H=16, m=4sampling all three dimensions. is in [1, 2, 4, 8, 16] for the five points (from left to right) on each line. Rows correspond to DeepScaleR-1.5B-Preview and Qwen3-1.7B models. MATH500 L5 is saturated here, resulting in less efficient gain from dimensions and m."
        }
    ],
    "affiliations": [
        "Salesforce AI Research",
        "University of Amsterdam"
    ]
}