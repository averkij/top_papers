{
    "paper_title": "Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents",
    "authors": [
        "Changdae Oh",
        "Seongheon Park",
        "To Eun Kim",
        "Jiatong Li",
        "Wendi Li",
        "Samuel Yeh",
        "Xuefeng Du",
        "Hamed Hassani",
        "Paul Bogdan",
        "Dawn Song",
        "Sharon Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Uncertainty quantification (UQ) for large language models (LLMs) is a key building block for safety guardrails of daily LLM applications. Yet, even as LLM agents are increasingly deployed in highly complex tasks, most UQ research still centers on single-turn question-answering. We argue that UQ research must shift to realistic settings with interactive agents, and that a new principled framework for agent UQ is needed. This paper presents the first general formulation of agent UQ that subsumes broad classes of existing UQ setups. Under this formulation, we show that prior works implicitly treat LLM UQ as an uncertainty accumulation process, a viewpoint that breaks down for interactive agents in an open world. In contrast, we propose a novel perspective, a conditional uncertainty reduction process, that explicitly models reducible uncertainty over an agent's trajectory by highlighting \"interactivity\" of actions. From this perspective, we outline a conceptual framework to provide actionable guidance for designing UQ in LLM agent setups. Finally, we conclude with practical implications of the agent UQ in frontier LLM development and domain-specific applications, as well as open remaining problems."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 4 ] . [ 1 3 7 0 5 0 . 2 0 6 2 : r Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents Changdae Oh1, Seongheon Park1, To Eun Kim2, Jiatong Li1, Wendi Li1, Samuel Yeh1, Xuefeng Du3, Hamed Hassani4, Paul Bogdan5, Dawn Song6 and Sharon Li1 1University of WisconsinMadison, 2Carnegie Mellon University, 3Nanyang Technological University, 4University of Pennsylvania, 5University of Southern California, 6University of California, Berkeley Abstract Uncertainty quantification (UQ) for large language models (LLMs) is key building block for safety guardrails of daily LLM applications. Yet, even as LLM agents are increasingly deployed in highly complex tasks, most UQ research still centers on single-turn question-answering. We argue that UQ research must shift to realistic settings with interactive agents, and that new principled framework for agent UQ is needed. This paper presents the first general formulation of agent UQ that subsumes broad classes of existing UQ setups. Under this formulation, we show that prior works implicitly treat LLM UQ as an uncertainty accumulation process, viewpoint that breaks down for interactive agents in an open world. In contrast, we propose novel perspective, conditional uncertainty reduction process, that explicitly models reducible uncertainty over an agents trajectory by highlighting interactivity of actions. From this perspective, we outline conceptual framework to provide actionable guidance for designing UQ in LLM agent setups. Finally, we conclude with practical implications of the agent UQ in frontier LLM development and domain-specific applications, as well as open remaining problems. Date: February 4, 2026 Contact: {changdae, sharonli}@cs.wisc.edu 1. Introduction LLM-based agents operating in open-world environments take actions that have real consequences: making costly bookings, modifying databases, or issuing irreversible commands (Google, 2025; OpenAI, 2025) at the next level of autonomy (Steinberger, 2026). In such settings, failures are no longer limited to incorrect text generation: agents may act prematurely under unresolved ambiguity, propagate errors across long interaction trajectories, or commit to outcomes that are costly or difficult to undo. To be deployed responsibly, agents must be able to assess and act upon the likelihood of failure (Kochenderfer, 2015). This makes reliable uncertainty quantification (UQ) an urgent and central requirement for agentic systems. Despite the importance, most existing UQ research treats LLMs as static oracles: system examined in isolation, prompted once, and evaluated by the uncertainty of single response or chain of responses (Figure 1, left). These methods implicitly assume static system in which no new information is acquired after the initial prompt (Farquhar et al., 2024; Kadavath et al., 2022; Malinin and Gales, 2021). Under this assumption, uncertainty is treated as point-wise estimate or uni-directional propagation. While appropriate for single-turn inference and non-interactive multi-step reasoning, they do not meet the growing needs of UQ in agentic AI settings featured by its long-horizon interactive environment. Interactive agents operate in multi-turn interactive environments with feedback from users, tools, and external systems (Figure 1, right). For example, flight-booking agent may need to decide whether to finalize Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents Figure 1 Comparison between UQ setups. Traditional LLM UQ (a) measures the uncertainty of an answer given question, whereas the UQ for LLM reasoning (b) expands the problem by considering multi-step responses rather than single response. Agent UQ (c) goes further by considering continual interactions between agent and user/environment across the trajectory, making it multi-turn, interactive inference setup (example sourced from ğœ -bench Airline; Barres et al. (2025)). reservation or ask follow-up questions about dates, budget, or layover preferences. Early uncertainty should prompt information-seeking actions, while subsequent dialogue and database queries can resolve ambiguities and reduce uncertainty, enabling confident execution. Here, uncertainty does not strictly accumulateit may also decrease as direct consequence of interaction, even as the trajectory grows longer. In this paper, we argue that making LLM agents trustworthy requires shift in focus: from pointwise uncertainty of answers to structured uncertainty flow in open, interactive decision processes; and new framework is necessary for this. We present the following perspective: Uncertainty quantification for an LLM agent must incorporate interactivity to model reducible uncertainty: in non-isolated, agentic systems, total uncertainty should be allowed to decrease as new information is acquired through interaction. To ground this perspective in formal discussion, we first provide concrete definition of agent UQ and general formulation that captures broad categories of existing UQ setups (Sec. 3). Specifically, we model the agents problem-solving trajectory as stochastic process over actions, observations, and environment states, represented as simple graphical model. Then, we define both turn-level and trajectory-level uncertainty in simple, probabilistic expression, where we show that many prior UQ approaches arise as special cases. This unified view also exposes fundamental limitation of existing methods: uncertainty is implicitly treated as accumulating monotonically along the trajectory, regardless of the role played by different actions; thus fails to relate uncertainty to task reward. To address this limitation, we introduce in Sec. 4 conceptual framework that models agent UQ as conditional uncertainty reduction process, through an information-gating mechanism that operationalizes the distinction between uncertainty-increasing and uncertainty-reducing actions by considering their interactivity. We further derive the analytic bounds that improve the interpretability of the resulting uncertainty estimates. 2 Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents Figure 2 Graphical model for an agent problem-solving trajectory with examples. Given task specification ğ¸0 and an initial user query ğ‘‚0, an agent spans multi-turn trajectory characterized by chain of action ğ´, observation ğ‘‚, and environment state ğ¸. This simple abstraction describes some representative agentic prompting methods such as ReAct (Yao et al., 2022). See Appendix A.1 for details. In addition, Sec. 5 discusses practical implications of agent UQ across diverse domains, including healthcare, software engineering, and robotics, as well as frontier LLM research on adaptive reasoning and post-training. Finally, we outline open problems unique to uncertainty modeling in agentic systems (Sec. 6). Together, we view this work as step toward providing the field with necessary foundation and clearer direction for future research on uncertainty-aware agentic systems. We formalize agent UQ as distinct problem setup and show why existing LLM UQ formulations are insufficient for interactive agent setups. To tackle that limitation, we present conceptual framework for agent UQ by viewing it as conditional uncertainty reduction process. To give actionable insights and promote discussion, we describe promising examples of agent UQ applications in various domains and also remaining open problems. 2. Related Work In contrast to classic UQ (Gal and Ghahramani, 2016; MacKay, 1992; Neal, 1992), LLM UQ in LLMs. UQ brings extra challenges from its computational cost and free-form outputs. Common approaches include aggregating output token probability (Aichberger et al., 2024; Duan et al., 2024; Fadeeva et al., 2024; Fomicheva et al., 2020; Malinin and Gales, 2021; Zhang et al., 2023, 2025b), measuring consistency over samples (Farquhar et al., 2024; Manakul et al., 2023; Nikitin et al., 2024), verbalizing confidence (Kadavath et al., 2022; Lin et al., 2022; Yona et al., 2024), or conformal prediction (Cherian et al., 2024; Kumar et al., 2023; Quach et al., 2024). While promising, they mostly use single-step question-answering as testbed, which limits their practical applicability to agentic setups. UQ in reasoning and agent setups. Beyond single-step generation, researchers show growing interest in measuring the confidence of LLMs during their reasoning. This kind of (un)certainty information has been used to improve the quality of response (Fu et al., 2025; Kang et al., 2025b; Wang et al., 2023) or to guide model during inference or training (Cheng et al., 2025; Hu et al., 2024; Lightman et al., 2024; Wang et al., 2025a,b; Zhu et al., 2025). Recently, there have also been attempts to model uncertainty in interactive and/or agentic setups (Chan et al., 2025; Duan et al., 2025; Frankel et al., 2024; Han et al., 2024; Zhao et al., 2025). Although UQ here aims to model uncertainty dynamics over long-horizon generation, existing methods do not capture the reducible nature of uncertainty in open environments (Kirchhof et al., 2025b), highlighting the need for new framework for agent UQ. 3 Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents 3. General Formulation for Agent UQ 3.1. Problem Statement and Definitions Imagine flight booking agent depicted in Figure 1 right. Here, the types of action ğ´ and observation ğ‘‚ are diverse: for ğ´, there is information-seeking, questioning for clarification, calling an API function, and thinking or updating without interactive behaviors. The observation ğ‘‚, on the other hand, can be database status report, tool-calling results, or the users clarification. Both action and observation can be displayed in flexible forms of natural language messages or structured strings, e.g., code. Concretely, given task specification ğ¸0, e.g., flight booking, user makes stochastic initial query ğ‘‚0, and the agent first requests the users ID to get relevant information. Then, it calls programmed function to get privileged information from the database. Not only does the agent interact with the user or tool, but it also thinks about better future actions (i.e., planning) or makes system update without external observation ğ‘‚ = {}. All interactions and other actions are stored in environment state ğ¸ for context-grounded actions over long horizon inference. This makes up trajectory â„±ğ‘¡ (chain of ğ´, ğ¸, and ğ‘‚) up to ğ‘¡-turn which continues until the task-specific termination rule is met, e.g., when the agent submits an answer (Jimenez et al., 2024; Wei et al., 2025; Yang et al., 2018), the user generates stop sign (Barres et al., 2025; Yao et al., 2024), or goal completion (Shridhar et al., 2021) with multiple constraints (Xie et al., 2024). Now, to formalize UQ in this real-world agent context, we begin with concrete definition of the agent system in Definition 1, and introduce graphical model for the agent trajectory in Figure 2. This can generalize many existing agent benchmark setups (Jimenez et al., 2024; Liu et al., 2024; Mialon et al., 2023; Song et al., 2023; Wei et al., 2025; Xie et al., 2024; Yang et al., 2018; Yao et al., 2024). Definition 1 (Stochastic Agent System). Let ğ¸ğ‘– be an environment state, mixture of context memory of agent-user-tool interaction logs up to the ğ‘–-th turn that is fully accessible to the agent, and state of the system database, which is not directly accessible to the agent (only partially observable via toolcalling). Let ğ‘‚ğ‘– and ğ´ğ‘– be the ğ‘–-th turn observation and action derived from distributions ğ‘ƒ and ğ‘ƒğœ‹,ğ’¯ , respectively, where ğ’¯ indicates tool set and ğœ‹ is an LLM policy. Then, under task specification ğ¸0 and the users initial query ğ‘‚0, we define the trajectory as sequence of turns â„±ğ‘‡ = {(ğ´ğ‘¡, ğ¸ğ‘¡, ğ‘‚ğ‘¡)}ğ‘‡ ğ‘¡=0 where â„±0 = (ğ¸0, ğ‘‚0). Given deterministic update function â„(), the generative process for each step ğ‘¡ is defined as: ğ´ğ‘– ğ‘ƒğœ‹,ğ’¯ (ğ¸ğ‘–1, ğ‘‚ğ‘–1), ğ‘‚ğ‘– ğ‘ƒ (ğ´ğ‘–, ğ¸ğ‘–), ğ¸ğ‘– = â„(ğ¸ğ‘–1, ğ‘‚ğ‘–1, ğ´ğ‘–). To express this trajectory with simple mathematical representation, we introduce dynamic Bayesian network (Murphy, 2002) in Figure 2. With this graphical model, dependency structures of ğ´, ğ¸, and ğ‘‚ can be explained as follows: (1) since the environment state ğ¸ğ‘–1 contains the entire conversation history of ğ´ğ‘–1 and ğ‘‚<ğ‘–1 up to the ğ‘– 1 step1, the current action ğ´ğ‘– depends only on ğ¸ğ‘–1 and ğ‘‚ğ‘–1; (2) the current observation ğ‘‚ğ‘– from the environment also depends only on ğ´ğ‘– and ğ¸ğ‘–; (3) the transition of the environment state (ğ¸ğ‘–1 ğ¸ğ‘–) can be defined as deterministic function2 â„() which stacks the conversation logs up in memory module and conducts rule-based modification (write, update, or delete) on previous system state given the current action. Under this Bayesian network, the joint probability for the trajectory can be factorized as below, ğ‘ƒ (â„±ğ‘‡ ) = ğ‘ƒ (ğ¸0, ğ‘‚0)Î ğ‘‡ = ğ‘ƒ (ğ¸0, ğ‘‚0)Î ğ‘‡ ğ‘–=1ğ‘ƒ (â„±ğ‘–â„±ğ‘–1) ğ‘–=1ğ‘ƒğœ‹,ğ’¯ (ğ´ğ‘–ğ¸ğ‘–1, ğ‘‚ğ‘–1)ğ‘ƒ (ğ‘‚ğ‘–ğ´ğ‘–, ğ¸ğ‘–), 1An adaptive memory structure (Xiong et al., 2025; Xu et al., 2025a) can replace this ever-growing memory. 2In general, this system transition can be also stochastic, but we consider simplified yet still realistic scenario. 4 Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents where â„±0 = (ğ¸0, ğ‘‚0), and ğ‘ƒ (ğ¸ğ‘–) = 1 as it is determistic. Now, we define the UQ problem for this stochastic agent system as follows. Definition 2 (Agent UQ). Let ğ‘ˆ (ğ‘‹) 0 be an uncertainty function that takes random variable ğ‘‹ or its distribution to produce non-negative real value. Given ğ‘‡ -turn trajectory, agent UQ aims to estimate both turn-level uncertainty ğ‘ˆ (â„±ğ‘¡â„±ğ‘¡1) for ğ‘¡ = 1, ..., ğ‘‡ and trajectory-level uncertainty ğ‘ˆ (â„±ğ‘‡ ) = ğ‘ˆ (â„±0, ..., â„±ğ‘‡ ), as joint total uncertainty. In the machine learning field, the most popular choices of the uncertainty function ğ‘ˆ (ğ‘‹) are Shannon entropy (Shannon, 1948) E[ log ğ‘ƒ (ğ‘‹)], negative log probability log ğ‘ƒ (ğ‘‹ = ğ‘¥), and their variants. These information-theoretic measures of uncertainty have nice property, chain-rule (Cover, 1999), making them suit for sequential modeling. For example, given ğ‘ˆ (â„±ğ‘¡) := E[ log ğ‘ƒ (â„±ğ‘¡)], we have the following uncertainty expansion, allowing us to model the agents total uncertainty as simple arithmetic of component-level uncertainties 3: ğ‘ˆ (â„±ğ‘‡ ) = ğ‘ˆ (ğ¸0, ğ‘‚0) + = ğ‘ˆ (ğ¸0, ğ‘‚0) + ğ‘‡ ğ‘–=1 ğ‘‡ ğ‘–=1 ğ‘ˆ (â„±ğ‘–â„±ğ‘–1) [ğ‘ˆ (ğ´ğ‘–ğ¸ğ‘–1, ğ‘‚ğ‘–1) + ğ‘ˆ (ğ‘‚ğ‘–ğ´ğ‘–, ğ¸ğ‘–)], where ğ‘ˆ (ğ¸0, ğ‘‚0) = ğ‘ˆ (ğ‘‚0ğ¸0) + ğ‘ˆ (ğ¸0) denotes an initial query uncertainty plus task volatility. Desideratum. While previous LLM UQ setups usually focus on conditional uncertainty of final answer, agent UQ aims at estimating joint uncertainty over the full trajectory, where the estimate from calibrated agent should be predictive of the reward on that trajectory. Formally, for all ğ‘Ÿ1 > ğ‘Ÿ2, E[ğ‘ˆ (â„±ğ‘‡ )ğ‘Ÿ(â„±ğ‘‡ ) = ğ‘Ÿ1 ] < E[ğ‘ˆ (â„±ğ‘‡ )ğ‘Ÿ(â„±ğ‘‡ ) = ğ‘Ÿ2 ], (1) where ğ‘Ÿ() denotes real-value reward function commonly set as success-failure binary annotator. We draw analogies between agent UQ and probabilistic Turing machines (Gill III, 1974), as well as belief tracking in Partially Observable Markov Decision Processes (Kaelbling et al., 1998), highlighting that agent UQ is grounded while still having its unique edge in contrast to these existing theoretical models in terms of setups and focus. See Appendix for discussion. 3.2. Unified View on Existing UQ Setups We now show that our agent UQ formulation captures various existing UQs as special cases. Reduction to single-step LLM UQ. When ğ‘¡ = 1 and the action space is restricted to responses, LLM UQ can be cast as special case ğ‘ˆ (â„±ğ‘‡ ) = ğ‘ˆ (ğ‘‚0) + ğ‘ˆ (ğ´1ğ‘‚0) ğ‘ˆ (ğ´1ğ‘‚0), where ğ‘‚0 and ğ´1 denote the given question and the corresponding model answer, respectively. Most current LLM UQ literature addresses only the conditional answer uncertainty ğ‘ˆ (ğ´1ğ‘‚0) (Fadeeva et al., 2023; Farquhar et al., 2024; Malinin and Gales, 2021). few also consider the question ambiguity ğ‘ˆ (ğ‘‚0) (Hou et al., 2024; Tomov et al., 2025) to reflect the reality of wild user query (Min et al., 2020). 3See Appendix and for derivation and alternative measures, e.g., informational energy (Pardo and Taneja, 1991) and nonextensive entropy (Gell-Mann and Tsallis, 2004). 5 Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents Reduction to multi-step reasoning UQ. When the action space involves multi-step reasoning (e.g., chain-of-thought, Wei et al. (2022)), we have ğ‘ˆ (â„±ğ‘‡ ) = ğ‘ˆ (ğ‘‚0) + ğ‘‡ ğ‘–=1 ğ‘ˆ (ğ´ğ‘–ğ´<ğ‘–, ğ‘‚0) ğ‘‡ ğ‘ˆ (ğ´ğ‘–ğ´<ğ‘–, ğ‘‚0) ğ‘–=1 max ğ‘–{1,...,ğ‘‡ } ğ‘ˆ (ğ´ğ‘–ğ´<ğ‘–, ğ‘‚0) ğ‘‡ ğ‘–=1 ğ‘¤ğ‘–ğ‘ˆ (ğ´ğ‘–ğ´<ğ‘–, ğ‘‚0), (2) (3) (4) (5) where ğ‘¤ğ‘– 0 and ğ‘‡ ğ‘–=1 ğ‘¤ğ‘– = 1. Here, ğ‘‚0 and ğ´ğ‘‡ denote the given initial query and the models final response, and ğ´ğ‘–, ..., ğ´ğ‘‡ 1 are intermediate outputs. Similar to the single-step setup, most existing multistep UQs (Tanneru et al., 2024; Tao et al., 2025) do not consider the question ambiguity ğ‘ˆ (ğ‘‚0), so that reduces Eq. 2 to Eq. 3, while few consider it (Leang et al., 2025). Meanwhile, rather than summing up all the step-wise uncertainties, some approaches consider the most uncertain one, i.e., the lowest confidence used in Fu et al. (2025), which can be represented by Eq. 4. Finally, Eq. 5 expresses various weighted average methods, such as standard length-normalized uncertainty (Kang et al., 2025b) by setting ğ‘¤ğ‘– = 1 , and tail ğ‘‡ confidence (Fu et al., 2025) by setting ğ‘¤ğ‘‡ = 1, and other clever weighting strategies, i.e., top-K mean (Fu et al., 2025) or model-based estimates (Zhao et al., 2025). Connection to process reward modeling. We further draw connection between this trajectory-level UQ problem and the process reward modeling problem (Li and Li, 2025; Lightman et al., 2024; Uesato et al., 2022) where we assign credit (reward) for each intermediate reasoning step, and then it can be utilized to evaluate the reasoning trajectory by aggregating the rewards across steps to derive trajectory-level reward. For example, Lightman et al. (2024) proposed the product of the step-wise probabilities and the minimum probability between steps as the aggregation method. These are analogs to Eq. 3 and Eq. 4, respectively. ğ‘‡ From this unified view, we see that they all model the multi-step Limitation of existing frameworks. uncertainty as uni-directional propagation, i.e., every step-level uncertainty positively contributes to the total uncertainty, without distinguishing action types at each turn. One might claim that the average uncertainty 1 ğ‘¡=1 ğ‘ˆ (â„±ğ‘¡â„±ğ‘¡1) can already be fine, as it might produce lower value for the chain of ğ‘‡ certain actions and vice versa. However, it does not consider the type of action which is critical in agentic setups, where an agents interactivity is key to earning reward. See Fig. 3 (a), scenario wherein user does not fully specify their goal upfront. After searching for options, the agent either reasons itself or interacts with the user to delegate decisions. Although both trajectories are evidential, interaction-oriented ones tend to achieve higher rewards, despite reasoning-oriented ones often yielding greater confidence. Yet, existing methods can produce misleading uncertainty estimates, fail to meet the desiderata (Eq 1). 4. How to Model Uncertainty Dynamics of Interactive Agents? 4.1. Unique Challenge In contrast to the LLM UQ, agent UQ spans multi-turn trajectory in an open environment. This presents unique challenge: modeling cascade uncertainty in an interactive system. Here, the desired UQ method should be able to precisely aggregate uncertainties across the entire trajectory and account for additional Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents Figure 3 Limitation of existing UQ (a) and our suggestion (b). Prior works just concern the evidentiality when designing or evaluating UQ while neglecting interactivity, so they may fail to reliably capture the agents failure. We urge expanding dimension of interest, interactivity, and moving on to reducible uncertainty modeling of agents. information gained through interactions. While there is emerging research on step-wise uncertainty aggregation (Fu et al., 2025; Kang et al., 2025b), they examine an isolated, noninteractive setting where an agent does not receive any external feedback during inference (e.g., Fig. 1, b). Meanwhile, agent UQ considers setup full of interactions with user and an environment, making it non-isolated, interactive system. Since an agent in this open world can gather additional information, make clarifications, and request confirmations at each step by interacting with the user and external databases through conversations and tool calls, the uncertainty dynamics in this interactive system includes not only propagations of turnlevel uncertainty, but also reductions. As shown in Fig. 3, the existing perspective does not consider interactivity in agent UQ and prohibits reducible uncertainty modeling. In this paper, we argue that we must introduce interactivity in agent UQ to model bi-directional uncertainty flow. 4.2. Agent UQ as Conditional Uncertainty Reduction Process We need new perspective to model an agents uncertainty dynamics that accounts for both reductions and increases, depending on the agents interactivity at each step. While there are multiple ways to realize this concept, we propose one possible implementation as prototype: information gating, which goes hand in hand with the information-theoretic measure of uncertainty noted in Sec. 3. Specifically, we derive lower bound of total uncertainty ğ‘ˆ (â„±ğ‘‡ ) that admits selective reduction or increase in uncertainty throughout the trajectory: ğ‘ˆ (â„±ğ‘‡ ) = ğ‘ˆ (ğ¸0, ğ‘‚0) + ğ‘‡ ğ‘–=1 ğ‘ˆ (â„±ğ‘–â„±ğ‘–1) = ğ‘ˆ (ğ¸0, ğ‘‚0) + = ğ‘ˆ (ğ¸0, ğ‘‚0) + ğ‘ˆ (ğ¸0, ğ‘‚0) + ğ‘‡ ğ‘–=1 ğ‘‡ ğ‘–= ğ‘‡ ğ‘–=1 [ğ‘ˆ (ğ´ğ‘–ğ¸ğ‘–1, ğ‘‚ğ‘–1) + ğ‘ˆ (ğ‘‚ğ‘–ğ´ğ‘–, ğ¸ğ‘–)] ğ‘ˆ (ğ´ğ‘–ğ¸ğ‘–1, ğ‘‚ğ‘–1)[1 + ğ‘ˆ (ğ‘‚ğ‘–ğ´ğ‘–, ğ¸ğ‘–) ğ‘ˆ (ğ´ğ‘–ğ¸ğ‘–1, ğ‘‚ğ‘–1) ] ğ‘ˆ (ğ´ğ‘–ğ¸ğ‘–1, ğ‘‚ğ‘–1)ğ‘”(â„±ğ‘–). (6) 7 Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents Figure 4 Illustration on the proposed agent UQ paradigm. We propose the conditional uncertainty reduction process for LLM agents by discerning interactive and evidential actions from others. Here, we introduce conditional gating function ğ‘”() that reduces or increases current turn uncertainty depending on the action. That is, for set of valid uncertainty reduction actions, ğ’œ, we define: ğ‘”(â„±ğ‘–) = { Info(ğ‘‚ğ‘–;ğ‘‚0ğ¸ğ‘–ğ‘‚0) ğ‘ˆ (ğ´ğ‘–ğ¸ğ‘–1,ğ‘‚ğ‘–1) 1 + ğ‘ˆ (ğ‘‚ğ‘–ğ´ğ‘–,ğ¸ğ‘–) ğ‘ˆ (ğ´ğ‘–ğ¸ğ‘–1,ğ‘‚ğ‘–1) if ğ´ğ‘– ğ’œ, otherwise. where Info(; ) denotes mutual information or pointwise mutual information that measures the amount of information gain by having the current observation ğ‘‚ğ‘– w.r.t. initial query ğ‘‚0 given conversation history ğ¸ğ‘–ğ‘‚0. For each turn, if the agents action is classified as an interactive (inviting user or tool) and evidential one (factual or not in conflict with stored data), then ğ‘”() produces the amount of information gain with negative sign to realize uncertainty reduction; otherwise, it propagates uncertainty (See Fig. 4). Importantly, the signed gating function handles uncertainty in more interpretable manner than traditional estimates by letting not only quantitative but also directional interpretations. Besides, this uncertainty model allows us to derive the analytic extrema, i.e., maximum and minimum, of total uncertainty in closed forms as shown in Lemma 1. Lemma 1 (Extrema of Information Gating). Let the lower bound of agent total uncertainty in Eq. 6 be ğ‘ˆ (â„±ğ‘‡ ), denote ğ‘ˆ (ğ‘‹) := ğ»(ğ‘‹) = E[ log ğ‘ƒ (ğ‘‹)] and Info(ğ‘‹; ğ‘Œ ) := ğ¼(ğ‘‹; ğ‘Œ ) = E[log ğ‘ƒ (ğ‘‹,ğ‘Œ ) ğ‘ƒ (ğ‘‹)ğ‘ƒ (ğ‘Œ ) ], then, we have: ğ‘ˆ (â„±ğ‘‡ ) ğ»(ğ¸0, ğ‘‚0) ğ‘ˆ (â„±ğ‘‡ ) ğ»(ğ¸0, ğ‘‚0) + ğ‘‡ 1 ğ‘–=1 ğ‘‡ ğ‘–=1 ğ¼(ğ‘‚ğ‘–, ğ‘‚0ğ¸ğ‘–ğ‘‚0) + ğ»(ğ´ğ‘‡ ğ¸ğ‘‡ 1, ğ‘‚ğ‘‡ 1), ğ»(ğ´ğ‘–, ğ‘‚ğ‘–ğ¸ğ‘–1, ğ‘‚ğ‘–1). Proof. If ğ´ğ‘– ğ’œ (resp. ğ´ğ‘– / ğ’œ) for all 1 ğ‘– ğ‘‡ 1, then ğ‘ˆ (â„±ğ‘‡ ) becomes monotonic uncertainty reduction (resp. propagation) process, deriving above inequalities (See Appendix for details). Interpretation. Lemma 1 states that an agents total uncertainty can be expressed as an ever-decreasing process starting from task-specific initial uncertainty reduced by the amount of extra information delivered by current observation, given the previous trajectory till its final behavior in the ideal case; whereas it is continually stacked up by the amount of joint entropy between action and observation per turn in worst case. Together with the directional interpretation turn-level uncertainty (increase or decrease), these analytic extrema help us better understand the numerical uncertainty, which is usually hard to interpret (BelÃ©m et al., 2024; Kirchhof et al., 2025a; Van Der Bles et al., 2019). Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents Action Category Interactivity Example Informationgathering interactive Agent uses read tool (e.g., get_reservation_details or search_flights) to retrieve data or asks the user for missing information, e.g., Could you provide your reservation number and last name?. interactive Agent asks the user preferences or confirmation on decision, such as, Do you want me to proceed with booking FQ8APE?. Asking clarification or confirmation to user Thinking State-changing tool call (writing) noninteractive noninteractive Providing final information to user noninteractive Agent plans the future action sequence based on the previous trajectory. Agent calls the tools that modify the database (e.g., cancel_reservation, book_reservation, update_reservation_flights) commit to an outcome. Agent reports the result of an action, such as Your reservation has been cancelled; your refund will be processed or Your flight has been rebooked to SFO departing at 8 a.m.. Table 1 Example action classes in Airline booking scenario in ğœ -bench. number of the agents actions can be categorized into five classes, which may reduce or increase the total uncertainty. See Appendix for extended analyses. Sketch of implementation. To operationalize this idea, we first need an action classifier to determine whether ğ´ğ‘– ğ’œ or not. One can adopt model-based evaluator (Kim et al., 2025) and rule-based verifier (Zeng et al., 2025) to assess the interactivity and evidentiality of actions, respectively, where ğ´ğ‘– ğ’œ if and only if ğ´ğ‘– is both evidential and interactive. An example categorization of actions in an airline assistant task is provided in Table 1. After that, four different quantities need to be estimated: (1) initial query uncertainty, (2) action uncertainty, (3) observation uncertainty, and (4) mutual information between the query and observation given the previous trajectory. For these estimations, we can leverage the output probability (Aichberger et al., 2024; Malinin and Gales, 2021), consistency in multiple trials (Farquhar et al., 2024), world modeling (Anonymous, 2026; Chae et al., 2025), and learning neural estimator (Gritsai et al., 2025; Molavipour et al., 2020). We leave the actual implementation for future work. See Appendix for additional notes. 5. Practical Implications Developing an agent UQ framework is not merely theoretical exercise but prerequisite for deploying LLM agents in non-deterministic real environments. We outline implications for frontier LLM research and three specialized domains: healthcare, coding, and robotics, in which agent UQ may have profound downstream effects, thereby incentivizing policymakers, practitioners, and researchers. 5.1. Advancing Frontier LLMs Test-time scaling through reasoning is now de facto standard technique to enhance the quality of LLM responses on challenging tasks (Muennighoff et al., 2025; Wei et al., 2022). However, there is still huge room for improvement, e.g., mitigating overthinking and/or pursuing efficiency (Aggarwal et al., 2025; Sui et al., 2025). Agent UQ provides concrete guidance to develop an adaptive reasoning method, i.e., early stopping 9 Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents or interaction invoking, via uncertainty budgeting. In addition, multi-turn reinforcement learning (RL) has emerged as promising yet challenging research theme in eliciting the agentic behavior and reasoning of LLMs (Shani et al., 2024; Zeng et al., 2025; Zhou et al., 2024). Agent UQ helps tackle the main bottlenecks, credit assignment and when-to-explore problems (Choudhury, 2025; Pecka and Svoboda, 2014; Thrun, 1992), limited positive feedback (Lee et al., 2025, 2026), and inspires an information pursuit policy (Chattopadhyay et al., 2022) by leveraging directional and quantitative uncertainty information as feedback. 5.2. Reliable Clinical Decision Supporting Agents Modern LLM agents offer promising vision for healthcare, moving beyond static knowledge retrieval to autonomous systems with complex reasoning and tool-execution capabilities (Wang et al., 2025c). However, the transition from promising prototypes to reliable clinical agents faces critical obstacles stemming from the systems inability to recognize and manage its own limitations. We believe that agent UQ can fill this gap. For example, although Ferber et al. (2025) observed huge potential of tool-calling LLM agents in oncology diagnoses, the remaining error rate underscores the necessity of human-in-the-loop workflow. Here, agent UQ can serve as gatekeeper, e.g., flagging highly accumulated uncertainty moments to invite humans, automating during low-uncertainty periods, while pursuing total uncertainty reduction for the right final diagnosis. This aligns with the concept: adaptive healthcare that envisions paradigm shift from passive prediction to active, risk-aware inquiry (Hinostroza Fuentes et al., 2025). 5.3. Reliable Software Engineering Agents Although advanced coding agents (Anthropic, 2025; OpenAI, 2025) have pushed performance on standard benchmarks (Jimenez et al., 2024) to an impressive degree, the disparity between passing controlled test and safely modifying production-level codebase stresses the urgent need for an agent UQ framework. Agent UQ enables more controllable exploration and commitment mechanism, wherein the agentic system knows uncertainty over bug patch candidates and decides whether to gather more evidence, e.g., check more files, or commit to fix. Besides, considering uncertainty-triggered rollback and branching algorithm, in which an agent treats increasing uncertainty after an action as signal to revert or branch by mirroring the checkpoint-rollback workflows of human engineers. These highlight the potential of UQ for reliable agentic coders. 5.4. Relible Embodied Agents in Cyber-Physical Systems Embodied agents in cyber-physical systems face heterogeneous uncertainty from sensing, dynamics, and human intent (Fung et al., 2025; Gemini Robotics, 2025; Li et al., 2024), and they actively reduce uncertainty through interaction by re-sensing the environment or interacting with humans before committing to irreversible physical actions (Xu et al., 2025b). For example, robot instructed to retrieve fragile object may explore or seek clarification rather than execute grasp under high uncertainty. This makes embodied autonomy canonical instance of conditional uncertainty reduction process, where information-gathering actions reduce uncertainty and state-changing actions increase commitment and risk. By explicitly quantifying and managing this uncertainty dynamic, our agent UQ solution offers safer action selection (Zhang et al., 2025a), principled deferral (Hagenow and Shah, 2025), and robust execution (RÃ¶mer et al., 2025). 6. Open Problems Although we have so far presented conceptual roadmap and the practical implications of agent UQ, significant open problems remain as follows. 10 Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents Developing rigid benchmarks at scale. As in the case of process reward modeling mentioned in Section 3.2, annotating across long-horizon trajectories introduces non-trivial efforts and costs (Choudhury, 2025). Moreover, as true uncertainty is unmeasurable, i.e., only proxy-based evaluation is possible, establishing benchmarks for agent UQ stands on the next level of difficulty than the static LLM evaluation (Ielanskyi et al., 2025). Investigation on defining proper agent performance measures (Hendrycks et al., 2025; Kapoor et al., 2025) with scalable evaluation and annotation methods for agent rollouts (Chang et al., 2024; Kim et al., 2025; Yang et al., 2024) would be crucial stepping stone. Long-horizon black-box uncertainty estimation. LLM agents are mostly offered by proprietary services, where we cannot access the output probability of the agent. While model owners and providers can leverage low-cost white-box UQ approaches, others may have to rely on expensive black-box UQ that requires heavy sampling per action over long trajectory (Zhang et al., 2024). This poses great challenge for low-resource users in developing UQ with state-of-the-art agents. Is high uncertainty per step due to the agents lack of knowledge about the right Solution multiplicity. action for that step? Or, is it due to the intrinsic multiplicity of valid actions in that step? We cannot identify which of them is the right source of high uncertainty! Besides, in contrast to the classic question answering, the task bid to an agent is too complex to be perfectly specified at once. Thus, each intermediate step is intrinsically ambiguous and allows multiple valid actions, making it impossible to specify the source of the high uncertainty. UQ in multi-agent or self-evolving agent systems. While we confine our setup to single-agent, nonevolving agent systems, there has been remarkable advancement in multi-agent systems (Guo et al., 2024; Wu et al., 2024) and self-evolving agents (Jiang et al., 2025; Wang et al., 2024a). Modeling the joint uncertainty dynamics of multiple agents or considering the non-stationary toolkit and agent parameters over time brings extra challenges that may require deliberation and other formal toolkits (Cheng et al., 2021; JÃ¸ sang, 2016; Tsallis and Brigatti, 2004). 7. Conclusion We argue that LLM UQ research should move towards more realistic setup, i.e., agentic inference, featuring the interactive long-horizon task episode, which requires paradigm shift from point-wise estimate to the bi-directional dynamics model of uncertainty. We laid the foundation for this by providing concrete definition and general formulation of agent UQ that abstracts wide variety of existing UQ approaches. To tackle the limitations of existing framing, we presented novel perspective, conditional uncertainty reduction process, with actionable guidelines and theoretical analysis. As concluding remark, we quote SchrÃ¶dingers note on living in physics (SchrÃ¶dinger, 1974): Every living means an increase in entropy of the part of the world, and it can only survive by continually drawing negative entropy from its environment."
        },
        {
            "title": "Limitations",
            "content": "Although this paper provided novel high-level vision for agent UQ with crystallized formulation, we built this vision on relatively simple setup with some assumptions: reliable observation ğ‘‚ğ‘– from the environment and user, and deterministic environment update ğ¸ğ‘– ğ¸ğ‘–+1. However, agentic systems in the wild may encounter unreliable (even adversarial; Kang et al. (2025a)) observations and stochastically evolving environment, where our current proposal does not ensure its effectiveness. In that scenario, we may 11 Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents need to explicitly model uncertainty in the agents evidence stream as well, e.g., trust/ignorance about tool outputs, retrieval, and user-provided facts, by leveraging formalisms, such as subjective logic (Cheng et al., 2020a; Jsang, 2018), imprecise probability theory (Walley, 1991), and formal belief representation (Genin and Huber, 2022), to safely manage memory and act under unreliable observations in stochastic world."
        },
        {
            "title": "Ethical Considerations",
            "content": "This work aims to establish foundation for uncertainty quantification of LLM agents, which can directly contribute to society, science, and human well-being (Han et al., 2023; Hendrycks et al., 2025; Oh et al., 2025c; Ren et al., 2025). We believe that the realization of our proposed framework can significantly boost the reliability of the LLM agent in dynamic, interactive inference setup, thereby facilitating risk mitigation in high-stakes decision-making. However, we also acknowledge that the proposed framework can be misused by allowing adversaries to efficiently steer the agent towards its blind spots or manipulate uncertainty thresholds to bypass safety filters (Wang et al., 2025d). Therefore, we recommend alleviations such as limiting exposure to attack-enabling signals, red-teaming, and monitoring for abuse, which avoid overpromising what low uncertainty implies."
        },
        {
            "title": "Acknowledgments",
            "content": "We sincerely thank Artem Shelmanov, Shawn Im, Hyeong Kyu Choi, Jongwon Jeong, Eunsu Kim, Mingyu Kim, Ayoung Lee, JungEun Kim, Hayun Lee, and Kyungwoo Song for their sharp feedback on the draft. This work is supported in part by the AFOSR Young Investigator Program under award number FA9550-231-0184, National Science Foundation under awards IIS-2237037 and IIS-2331669, Office of Naval Research under grant number N00014-231-2643, Schmidt Sciences Foundation, Open Philanthropy, Alfred P. Sloan Fellowship, and gifts from Google and Amazon. Paul Bogdan acknowledges the support by the National Science Foundation (NSF) under the NSF Award 2243104 under the Center for Complex Particle Systems (COMPASS), NSF Mid-Career Advancement Award BCS-2527046, U.S. Army Research Office (ARO) under Grant No. W911NF-23-1-0111, Defense Advanced Research Projects Agency (DARPA) Young Faculty Award and DARPA Director Fellowship Award, Okawa foundation award, National Institute of Health (NIH) under R01 AG 079957, and Intel faculty awards. 12 Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents"
        },
        {
            "title": "References",
            "content": "P. Aggarwal, S. Kim, J. Lanchantin, S. Welleck, J. Weston, I. Kulikov, and S. Saha. Optimalthinkingbench: Evaluating over and underthinking in llms. arXiv preprint arXiv:2508.13141, 2025. M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, C. Fu, K. Gopalakrishnan, K. Hausman, et al. Do as can, not as say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022. L. Aichberger, K. Schweighofer, and S. Hochreiter. Rethinking uncertainty estimation in natural language generation. arXiv preprint arXiv:2412.15176, 2024. Anonymous. Dreamphase: Offline imagination and uncertainty-guided planning for large-language-model agents. In International Conference on Learning Representations, 2026. Anthropic. Claude sonnet 4.5 system card. Technical report, Anthropic PBC, 2025. URL https://www. anthropic.com/claude-sonnet-4-5-system-card. V. Barres, H. Dong, S. Ray, X. Si, and K. Narasimhan. ğœ 2-bench: Evaluating conversational agents in dual-control environment. arXiv preprint arXiv:2506.07982, 2025. C. BelÃ©m, M. Kelly, M. Steyvers, S. Singh, and P. Smyth. Perceptions of linguistic uncertainty by language models and humans. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 84678502, 2024. M. I. Belghazi, A. Baratin, S. Rajeshwar, S. Ozair, Y. Bengio, A. Courville, and D. Hjelm. Mutual information neural estimation. In International conference on machine learning, pages 531540. PMLR, 2018. O. Calin and C. UdriÅŸte. Informational Energy, pages 133163. Springer International Publishing, 2014. ISBN 978-3-319-07779-6. doi: 10.1007/978-3-319-07779-6_5. H. Chae, N. Kim, K. T. iunn Ong, M. Gwak, G. Song, J. Kim, S. Kim, D. Lee, and J. Yeo. Web agents with world models: Learning and leveraging environment dynamics in web navigation. In International Conference on Learning Representations, 2025. K. H. R. Chan, Y. Ge, E. Dobriban, H. Hassani, and R. Vidal. Conformal information pursuit for interactively guiding large language models. arXiv preprint arXiv:2507.03279, 2025. M. Chang, J. Zhang, Z. Zhu, C. Yang, Y. Yang, Y. Jin, Z. Lan, L. Kong, and J. He. Agentboard: An analytical evaluation board of multi-turn llm agents. Advances in neural information processing systems, 37:74325 74362, 2024. A. Chattopadhyay, S. Slocum, B. D. Haeffele, R. Vidal, and D. Geman. Interpretable by design: Learning IEEE Transactions on Pattern Analysis and Machine predictors by composing interpretable queries. Intelligence, 45(6):74307443, 2022. C. Chen, Z. Zhang, B. Guo, S. Ma, I. Khalilov, S. A. Gebreegziabher, Y. Ye, Z. Xiao, Y. Yao, T. Li, et al. The obvious invisible threat: Llm-powered gui agents vulnerability to fine-print injections. arXiv preprint arXiv:2504.11281, 2025. D. Cheng, S. Huang, X. Zhu, B. Dai, W. X. Zhao, Z. Zhang, and F. Wei. Reasoning with exploration: An entropy perspective. arXiv preprint arXiv:2506.14758, 2025. M. Cheng, S. Nazarian, and P. Bogdan. There is hope after all: Quantifying opinion and trustworthiness in neural networks. Frontiers in artificial intelligence, 3:54, 2020a. 13 Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents M. Cheng, C. Yin, J. Zhang, S. Nazarian, J. Deshmukh, and P. Bogdan. general trust framework for In Proceedings of the 20th International Conference on Autonomous Agents and multi-agent systems. MultiAgent Systems, pages 332340, 2021. P. Cheng, W. Hao, S. Dai, J. Liu, Z. Gan, and L. Carin. Club: contrastive log-ratio upper bound of mutual information. In International conference on machine learning, pages 17791788. PMLR, 2020b. J. Cherian, I. Gibbs, and E. Candes. Large language model validity via enhanced conformal prediction methods. Advances in Neural Information Processing Systems, 37:114812114842, 2024. S. Choudhury. Process reward models for llm agents: Practical framework and directions. arXiv preprint arXiv:2502.10325, 2025. S. Chun. Multiplicity is an inevitable and inherent challenge in multimodal learning. arXiv preprint arXiv:2505.19614, 2025. T. M. Cover. Elements of information theory. John Wiley & Sons, 1999. J. Duan, H. Cheng, S. Wang, A. Zavalny, C. Wang, R. Xu, B. Kailkhura, and K. Xu. Shifting attention to relevance: Towards the predictive uncertainty quantification of free-form large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 50505063, 2024. J. Duan, J. Diffenderfer, S. Madireddy, T. Chen, B. Kailkhura, and K. Xu. Uprop: Investigating the uncertainty propagation of llms in multi-step agentic decision-making. arXiv preprint arXiv:2506.17419, 2025. E. Fadeeva, R. Vashurin, A. Tsvigun, A. Vazhentsev, S. Petrakov, K. Fedyanin, D. Vasilev, E. Goncharova, A. Panchenko, M. Panov, et al. Lm-polygraph: Uncertainty estimation for language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 446461, 2023. E. Fadeeva, A. Rubashevskii, A. Shelmanov, S. Petrakov, H. Li, H. Mubarak, E. Tsymbalov, G. Kuzmin, A. Panchenko, T. Baldwin, et al. Fact-checking the output of large language models via token-level uncertainty quantification. In Findings of the Association for Computational Linguistics ACL 2024, pages 93679385, 2024. S. Farquhar, J. Kossen, L. Kuhn, and Y. Gal. Detecting hallucinations in large language models using semantic entropy. Nature, 630(8017):625630, 2024. D. Ferber, O. S. El Nahhas, G. WÃ¶lflein, I. C. Wiest, J. Clusmann, M.-E. LeÃŸmann, S. Foersch, J. Lammert, M. Tschochohei, D. JÃ¤ger, et al. Development and validation of an autonomous artificial intelligence agent for clinical decision-making in oncology. Nature cancer, pages 113, 2025. M. Fomicheva, S. Sun, L. Yankovskaya, F. Blain, F. GuzmÃ¡n, M. Fishel, N. Aletras, V. Chaudhary, and L. Specia. Unsupervised quality estimation for neural machine translation. Transactions of the Association for Computational Linguistics, 8:539555, 2020. E. Frankel, S. S. Li, L. J. Ratliff, Y. Tsvetkov, S. Oh, and P. W. Koh. Conformal reasoning: Uncertainty estimation in interactive environments, 2024. Y. Fu, X. Wang, Y. Tian, and J. Zhao. Deep think with confidence. arXiv preprint arXiv:2508.15260, 2025. P. Fung, Y. Bachrach, A. Celikyilmaz, K. Chaudhuri, D. Chen, W. Chung, E. Dupoux, H. Gong, H. JÃ©gou, A. Lazaric, et al. Embodied ai agents: Modeling the world. arXiv preprint arXiv:2506.22355, 2025. 14 Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents Y. Gal and Z. Ghahramani. Dropout as bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning, pages 10501059. PMLR, 2016. M. Gell-Mann and C. Tsallis. Nonextensive Entropy: Interdisciplinary Applications. Oxford University Press, 04 2004. Gemini Robotics. Gemini robotics 1.5: Pushing the frontier of generalist robots with advanced embodied reasoning, thinking, and motion transfer. arXiv preprint arXiv:2510.03342, 2025. K. Genin and F. Huber. Formal Representations of Belief. In E. N. Zalta and U. Nodelman, editors, The Stanford Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, Fall 2022 edition, 2022. J. Gill. Computational complexity of probabilistic turing machines. SIAM Journal on Computing, 6(4):675695, 1977. J. T. Gill III. Computational complexity of probabilistic turing machines. In Proceedings of the sixth annual ACM symposium on Theory of computing, pages 9195, 1974. Google. Gemini agent. https://gemini.google/overview/agent/, 2025. Accessed: 2025-12-11. G. Gritsai, M. Richards, M. MÃ©loux, K. Cho, and M. Peyrard. Mist: Mutual information via supervised training. arXiv preprint arXiv:2511.18945, 2025. T. Guo, X. Chen, Y. Wang, R. Chang, S. Pei, N. V. Chawla, O. Wiest, and X. Zhang. Large language model based multi-agents: survey of progress and challenges. arXiv preprint arXiv:2402.01680, 2024. M. Hagenow and J. A. Shah. Realm: Real-time estimates of assistance for learned models in human-robot interaction. IEEE Robotics and Automation Letters, 2025. J. Han, H. Yoo, Y. Kim, J. Myung, M. Kim, H. Lim, J. Kim, T. Y. Lee, H. Hong, S.-Y. Ahn, et al. Recipe: How to integrate chatgpt into efl writing education. In Proceedings of the tenth ACM conference on learning@ scale, pages 416420, 2023. J. Han, W. Buntine, and E. Shareghi. Towards uncertainty-aware language agent. arXiv preprint arXiv:2401.14016, 2024. D. Hendrycks, D. Song, C. Szegedy, H. Lee, Y. Gal, E. Brynjolfsson, S. Li, A. Zou, L. Levine, B. Han, et al. definition of agi. arXiv preprint arXiv:2510.18212, 2025. V. G. Hinostroza Fuentes, H. A. Karim, M. J. T. Tan, and N. AlDahoul. Ai with agency: vision for adaptive, efficient, and ethical healthcare. Frontiers in Digital Health, 7:1600216, 2025. B. Hou, Y. Liu, K. Qian, J. Andreas, S. Chang, and Y. Zhang. Decomposing uncertainty for large language models through input clarification ensembling. In R. Salakhutdinov, Z. Kolter, K. Heller, A. Weller, N. Oliver, J. Scarlett, and F. Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 1902319042. PMLR, 2127 Jul 2024. Z. Hu, C. Liu, X. Feng, Y. Zhao, S.-K. Ng, A. T. Luu, J. He, P. W. W. Koh, and B. Hooi. Uncertainty of thoughts: Uncertainty-aware planning enhances information seeking in llms. Advances in Neural Information Processing Systems, 37:2418124215, 2024. M. Ielanskyi, K. Schweighofer, L. Aichberger, and S. Hochreiter. Addressing pitfalls in the evaluation of uncertainty estimation methods for natural language generation. In ICLR Workshop: Quantify Uncertainty and Hallucination in Foundation Models: The Next Frontier in Reliable AI, 2025. 15 Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents P. Jiang, J. Lin, Z. Shi, Z. Wang, L. He, Y. Wu, M. Zhong, P. Song, Q. Zhang, H. Wang, et al. Adaptation of agentic ai. arXiv preprint arXiv:2512.16301, 2025. C. E. Jimenez, J. Yang, A. Wettig, S. Yao, K. Pei, O. Press, and K. R. Narasimhan. SWE-bench: Can language models resolve real-world github issues? In International Conference on Learning Representations, 2024. A. JÃ¸ sang. Generalising bayes theorem in subjective logic. In 2016 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI), pages 462469. IEEE, 2016. A. Jsang. Subjective Logic: formalism for reasoning under uncertainty. Springer Publishing Company, Incorporated, 2018. S. Kadavath, T. Conerly, A. Askell, T. Henighan, D. Drain, E. Perez, N. Schiefer, Z. Hatfield-Dodds, N. DasSarma, E. Tran-Johnson, et al. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221, 2022. L. P. Kaelbling, M. L. Littman, and A. R. Cassandra. Planning and acting in partially observable stochastic domains. Artificial intelligence, 101(1-2):99134, 1998. H. Kang, J. Yeon, and G. Singh. Trap: Targeted redirecting of agentic preferences. arXiv preprint arXiv:2505.23518, 2025a. Z. Kang, X. Zhao, and D. Song. Scalable best-of-n selection for large language models via self-certainty. In Advances in Neural Information Processing Systems, 2025b. S. Kapoor, B. Stroebl, P. Kirgis, N. Nadgir, Z. S. Siegel, B. Wei, T. Xue, Z. Chen, F. Chen, S. Utpala, et al. Holistic agent leaderboard: The missing infrastructure for ai agent evaluation. arXiv preprint arXiv:2510.11977, 2025. W. Kim, S. Park, Y. In, S. Kim, D. Lee, and C. Park. Beyond the final answer: Evaluating the reasoning trajectories of tool-augmented agents. arXiv preprint arXiv:2510.02837, 2025. M. Kirchhof, L. FÃ¼ger, A. Golinski, E. G. Dhekane, A. Blaas, and S. Williamson. Self-reflective uncertainties: Do llms know their internal answer distribution? In ICML 2025 Workshop on Reliable and Responsible Foundation Models, 2025a. M. Kirchhof, G. Kasneci, and E. Kasneci. Position: Uncertainty quantification needs reassessment for large language model agents. In Forty-second International Conference on Machine Learning Position Paper Track, 2025b. M. J. Kochenderfer. Decision making under uncertainty: theory and application. MIT press, 2015. N. Kotelevskii, A. Artemenkov, K. Fedyanin, F. Noskov, A. Fishkov, A. Shelmanov, A. Vazhentsev, A. Petiushko, and M. Panov. Nonparametric uncertainty quantification for single deterministic neural network. Advances in Neural Information Processing Systems, 35:3630836323, 2022. L. Kuhn, Y. Gal, and S. Farquhar. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. In The Eleventh International Conference on Learning Representations, 2023. B. Kumar, C. Lu, G. Gupta, A. Palepu, D. Bellamy, R. Raskar, and A. Beam. Conformal prediction with large language models for multi-choice question answering. arXiv preprint arXiv:2305.18404, 2023. J. O. J. Leang, Z. Zhao, A. P. Gema, S. Yang, W.-C. Kwan, X. He, W. Li, P. Minervini, E. Giunchiglia, and S. B. Cohen. Picsar: Probabilistic confidence selection and ranking for reasoning chains. arXiv preprint arXiv:2508.21787, 2025. Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents S. Lee, B. Amos, and G. Fanti. Banel: Exploration posteriors for generative modeling using only negative rewards. arXiv preprint arXiv:2510.09596, 2025. S. Lee, S. Lim, S. Park, S. Cheon, and K. Song. Semi-supervised preference optimization with limited feedback. In International Conference on Learning Representations, 2026. M. Li, S. Zhao, Q. Wang, K. Wang, Y. Zhou, S. Srivastava, C. Gokmen, T. Lee, E. L. Li, R. Zhang, et al. Embodied agent interface: Benchmarking llms for embodied decision making. Advances in Neural Information Processing Systems, 37:100428100534, 2024. W. Li and Y. Li. Process reward model with q-value rankings. In Proceedings of the International Conference on Learning Representations, 2025. H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2024. S. Lin, J. Hilton, and O. Evans. Teaching models to express their uncertainty in words. Transactions on Machine Learning Research, 2022. ISSN 2835-8856. S. Liu, H. Liu, J. Liu, L. Xiao, S. Gao, C. Lyu, Y. Gu, W. Zhang, D. F. Wong, S. Zhang, et al. Compassverifier: unified and robust verifier for llms evaluation and outcome reward. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 3345433482, 2025. X. Liu, H. Yu, H. Zhang, Y. Xu, X. Lei, H. Lai, Y. Gu, H. Ding, K. Men, K. Yang, S. Zhang, X. Deng, A. Zeng, Z. Du, C. Zhang, S. Shen, T. Zhang, Y. Su, H. Sun, M. Huang, Y. Dong, and J. Tang. Agentbench: Evaluating llms as agents. In International Conference on Learning Representations, 2024. J. Lu, T. Holleis, Y. Zhang, B. Aumayer, F. Nan, H. Bai, S. Ma, S. Ma, M. Li, G. Yin, Z. Wang, and R. Pang. ToolSandbox: stateful, conversational, interactive evaluation benchmark for LLM tool use capabilities. In L. Chiruzzo, A. Ritter, and L. Wang, editors, Findings of the Association for Computational Linguistics: NAACL 2025, pages 11601183, Albuquerque, New Mexico, Apr. 2025. Association for Computational Linguistics. D. J. C. MacKay. practical bayesian framework for backpropagation networks. Neural Computation, 4(3): 448472, 1992. A. Malinin and M. Gales. Uncertainty estimation in autoregressive structured prediction. In International Conference on Learning Representations, 2021. P. Manakul, A. Liusie, and M. Gales. Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. In Proceedings of the 2023 conference on empirical methods in natural language processing, pages 90049017, 2023. G. Mialon, C. Fourrier, T. Wolf, Y. LeCun, and T. Scialom. Gaia: benchmark for general ai assistants. In International Conference on Learning Representations, 2023. S. Min, J. Michael, H. Hajishirzi, and L. Zettlemoyer. Ambigqa: Answering ambiguous open-domain questions. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 57835797, 2020. S. Molavipour, G. Bassi, and M. Skoglund. Conditional mutual information neural estimator."
        },
        {
            "title": "In IEEE",
            "content": "International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 50255029, 2020. 17 Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents T. Mu, A. Helyar, J. Heidecke, J. Achiam, A. Vallone, I. Kivlichan, M. Lin, A. Beutel, J. Schulman, and L. Weng. Rule based rewards for language model safety. Advances in Neural Information Processing Systems, 37: 108877108901, 2024. N. Muennighoff, Z. Yang, W. Shi, X. L. Li, L. Fei-Fei, H. Hajishirzi, L. Zettlemoyer, P. Liang, E. CandÃ¨s, and T. B. Hashimoto. s1: Simple test-time scaling. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 2028620332, 2025. S. Mukherjee, H. Asnani, and S. Kannan. Ccmi: Classifier based conditional mutual information estimation. In Uncertainty in artificial intelligence, pages 10831093. PMLR, 2020. K. P. Murphy. Dynamic bayesian networks: representation, inference and learning. University of California, Berkeley, 2002. R. Neal. Bayesian learning via stochastic dynamics. Advances in neural information processing systems, 5, 1992. D. Nguyen, J. Chen, Y. Wang, G. Wu, N. Park, Z. Hu, H. Lyu, J. Wu, R. Aponte, Y. Xia, X. Li, J. Shi, H. Chen, V. D. Lai, Z. Xie, S. Kim, R. Zhang, T. Yu, M. Tanjim, N. K. Ahmed, P. Mathur, S. Yoon, L. Yao, B. Kveton, J. Kil, T. H. Nguyen, T. Bui, T. Zhou, R. A. Rossi, and F. Dernoncourt. GUI agents: survey. In W. Che, J. Nabende, E. Shutova, and M. T. Pilehvar, editors, Findings of the Association for Computational Linguistics: ACL 2025, pages 2252222538, Vienna, Austria, July 2025. Association for Computational Linguistics. A. Nikitin, J. Kossen, Y. Gal, and P. Marttinen. Kernel language entropy: Fine-grained uncertainty quantification for llms from semantic similarities. Advances in Neural Information Processing Systems, 37:89018929, 2024. C. Oh, H. Lim, M. Kim, D. Han, S. Yun, J. Choo, A. Hauptmann, Z.-Q. Cheng, and K. Song. Towards calibrated robust fine-tuning of vision-language models. Advances in Neural Information Processing Systems, 37: 1267712707, 2024. C. Oh, Z. Fang, S. Im, X. Du, and Y. Li. Understanding multimodal LLMs under distribution shifts: An information-theoretic approach. In International Conference on Machine Learning, 2025a. C. Oh, J. Li, S. Im, and S. Li. Visual instruction bottleneck tuning."
        },
        {
            "title": "In Advances in Neural Information",
            "content": "Processing Systems, 2025b. J. Oh, E. Kim, and A. Oh. Flex-travelplanner: benchmark for flexible planning with language agents. arXiv preprint arXiv:2506.04649, 2025c. OpenAI. Chatgpt agent. https://chatgpt.com/features/agent/, 2025. Accessed: 2025-12-11. OpenAI."
        },
        {
            "title": "Introducing",
            "content": "gpt-5.2-codex. introducing-gpt-5-2-codex/, 2025. https://openai.com/index/ L. Pardo and I. Taneja. Information energy and its aplications. In Advances in electronics and electron physics, volume 80, pages 165241. Elsevier, 1991. S. Park, O. Bastani, J. Weimer, and I. Lee. Calibrated prediction with covariate shift via unsupervised domain adaptation. In International Conference on Artificial Intelligence and Statistics, pages 32193229. PMLR, 2020. S. G. Patil, H. Mao, F. Yan, C. C.-J. Ji, V. Suresh, I. Stoica, and J. E. Gonzalez. The berkeley function calling leaderboard (bfcl): From tool use to agentic evaluation of large language models. In International Conference on Machine Learning, 2025. 18 Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents M. Pecka and T. Svoboda. Safe exploration techniques for reinforcement learningan overview. In International workshop on modelling and simulation for autonomous systems, pages 357375. Springer, 2014. H. Peng, Y. Qi, X. Wang, B. Xu, L. Hou, and J. Li. Verif: Verification engineering for reinforcement learning in instruction following. arXiv preprint arXiv:2506.09942, 2025. V. Quach, A. Fisch, T. Schuster, A. Yala, J. H. Sohn, T. S. Jaakkola, and R. Barzilay. Conformal language modeling. In International Conference on Learning Representations, 2024. J. Ren, J. Luo, Y. Zhao, K. Krishna, M. Saleh, B. Lakshminarayanan, and P. J. Liu. Out-of-distribution detection and selective generation for conditional language models. In The Eleventh International Conference on Learning Representations, 2023. S. Ren, P. Jian, Z. Ren, C. Leng, C. Xie, and J. Zhang. Towards scientific intelligence: survey of llm-based scientific agents. arXiv preprint arXiv:2503.24047, 2025. A. RÃ©nyi. On measures of entropy and information. In Proceedings of the fourth Berkeley symposium on mathematical statistics and probability, volume 1: contributions to the theory of statistics, volume 4, pages 547562. University of California Press, 1961. R. RÃ¶mer, A. Kobras, L. Worbis, and A. P. Schoellig. Failure prediction at runtime for generative robot policies. In Advances in Neural Information Processing Systems, 2025. E. SchrÃ¶dinger. What is Life? & Mind and Matter: The Physical Aspect of the Living Cell. Cambridge University Press, 1974. ISBN 9780521093972. L. Shani, A. Rosenberg, A. Cassel, O. Lang, D. Calandriello, A. Zipori, H. Noga, O. Keller, B. Piot, I. Szpektor, et al. Multi-turn reinforcement learning with preference human feedback. Advances in Neural Information Processing Systems, 37:118953118993, 2024. C. E. Shannon. mathematical theory of communication. The Bell system technical journal, 27(3):379423, 1948. Y. Shi, W. Yu, W. Yao, W. Chen, and N. Liu. Towards trustworthy gui agents: survey. arXiv preprint arXiv:2503.23434, 2025. N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan, and S. Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652, 2023. M. Shridhar, X. Yuan, M.-A. Cote, Y. Bisk, A. Trischler, and M. Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. In International Conference on Learning Representations, 2021. C. H. Song, J. Wu, C. Washington, B. M. Sadler, W.-L. Chao, and Y. Su. Llm-planner: Few-shot grounded planning for embodied agents with large language models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 29983009, 2023. P. Steinberger. January 2026. Introducing openclaw. https://openclaw.ai/blog/introducing-openclaw, Y. Sui, Y.-N. Chuang, G. Wang, J. Zhang, T. Zhang, J. Yuan, H. Liu, A. Wen, S. Zhong, N. Zou, et al. Stop overthinking: survey on efficient reasoning for large language models. arXiv preprint arXiv:2503.16419, 2025. 19 Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents S. H. Tanneru, C. Agarwal, and H. Lakkaraju. Quantifying uncertainty in natural language explanations of large language models. In International Conference on Artificial Intelligence and Statistics, pages 10721080. PMLR, 2024. L. Tao, Y.-F. Yeh, M. Dong, T. Huang, P. Torr, and C. Xu. Revisiting uncertainty estimation and calibration of large language models. arXiv preprint arXiv:2505.23854, 2025. S. B. Thrun. Efficient exploration in reinforcement learning. Carnegie Mellon University, 1992. K. Tian, E. Mitchell, A. Zhou, A. Sharma, R. Rafailov, H. Yao, C. Finn, and C. D. Manning. Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. arXiv preprint arXiv:2305.14975, 2023. T. Tomov, D. Fuchsgruber, T. WollschlÃ¤ger, and S. GÃ¼nnemann. The illusion of certainty: Uncertainty quantification for llms fails under ambiguity. arXiv preprint arXiv:2508.15260, 2025. C. Tsallis. Possible generalization of boltzmann-gibbs statistics. Journal of statistical physics, 52(1):479487, 1988. C. Tsallis and E. Brigatti. Nonextensive statistical mechanics: brief introduction. Continuum Mechanics and Thermodynamics, 16(3):223235, 2004. J. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang, A. Creswell, G. Irving, and I. Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. I. Vajda. Bounds of the minimal error probability on checking finite or countable number of hypotheses. Problemy Peredachi Informatsii, 4(1):919, 1968. I. Vajda and J. ZvÃ¡rovÃ¡. On generalized entropies, bayesian decisions and statistical diversity. Kybernetika, 43(5):675696, 2007. A. M. Van Der Bles, S. Van Der Linden, A. L. Freeman, J. Mitchell, A. B. Galvao, L. Zaval, and D. J. Spiegelhalter. Communicating uncertainty about facts, numbers and science. Royal Society open science, 6(5):181870, 2019. R. Vashurin, M. Goloburda, A. Ilina, A. Rubashevskii, P. Nakov, A. Shelmanov, and M. Panov. Uncertainty quantification for llms through minimum bayes risk: Bridging confidence and consistency. arXiv preprint arXiv:2502.04964, 2025. P. Walley. Statistical Reasoning with Imprecise Probabilities. Chapman & Hall, 1991. G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan, and A. Anandkumar. Voyager: An open-ended embodied agent with large language models. Transactions on Machine Learning Research, 2024a. ISSN 2835-8856. J. Wang, J. Liu, Y. Fu, Y. Li, X. Wang, Y. Lin, Y. Yue, L. Zhang, Y. Wang, and K. Wang. Harnessing uncertainty: Entropy-modulated policy gradients for long-horizon llm agents. arXiv preprint arXiv:2509.09265, 2025a. S. Wang, L. Yu, C. Gao, C. Zheng, S. Liu, R. Lu, K. Dang, X. Chen, J. Yang, Z. Zhang, et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939, 2025b. W. Wang, Z. Ma, Z. Wang, C. Wu, J. Ji, W. Chen, X. Li, and Y. Yuan. survey of llm-based agents in medicine: How far are we from baymax? arXiv preprint arXiv:2502.11211, 2025c. 20 Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents X. Wang, J. Wei, D. Schuurmans, Q. V. Le, E. H. Chi, S. Narang, A. Chowdhery, and D. Zhou. Self-consistency In International Conference on Learning improves chain of thought reasoning in language models. Representations, 2023. X. Wang, Y. Chen, L. Yuan, Y. Zhang, Y. Li, H. Peng, and H. Ji. Executable code actions elicit better llm agents. In Forty-first International Conference on Machine Learning, 2024b. Z. Wang, V. Siu, Z. Ye, T. Shi, Y. Nie, X. Zhao, C. Wang, W. Guo, and D. Song. Agentvigil: Automatic black-box red-teaming for indirect prompt injection against llm agents. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 2315923172, 2025d. J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. J. Wei, Z. Sun, S. Papay, S. McKinney, J. Han, I. Fulford, H. W. Chung, A. T. Passos, W. Fedus, and A. Glaese. Browsecomp: simple yet challenging benchmark for browsing agents. arXiv preprint arXiv:2504.12516, 2025. Q. Wu, G. Bansal, J. Zhang, Y. Wu, B. Li, E. Zhu, L. Jiang, X. Zhang, S. Zhang, J. Liu, et al. Autogen: Enabling next-gen llm applications via multi-agent conversations. In Conference on Language Modeling, 2024. J. Xie, K. Zhang, J. Chen, T. Zhu, R. Lou, Y. Tian, Y. Xiao, and Y. Su. Travelplanner: benchmark for realworld planning with language agents. In International Conference on Machine Learning, pages 5459054613. PMLR, 2024. Z. Xiong, Y. Lin, W. Xie, P. He, Z. Liu, J. Tang, H. Lakkaraju, and Z. Xiang. How memory management impacts llm agents: An empirical study of experience-following behavior. arXiv preprint arXiv:2505.16067, 2025. W. Xu, Z. Liang, K. Mei, H. Gao, J. Tan, and Y. Zhang. A-mem: Agentic memory for llm agents. arXiv preprint arXiv:2502.12110, 2025a. X. Xu, Y. Hou, Z. Liu, and S. Song. Compliant residual dagger: Improving real-world contact-rich manipulation with human corrections. arXiv preprint arXiv:2506.16685, 2025b. Z. Yang, P. Qi, S. Zhang, Y. Bengio, W. Cohen, R. Salakhutdinov, and C. D. Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23692380, 2018. Z. Yang, P. Li, M. Yan, J. Zhang, F. Huang, and Y. Liu. React meets actre: Autonomous annotation of agent trajectories for contrastive self-training. In Conference on Language Modeling, 2024. S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. R. Narasimhan, and Y. Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations, 2022. S. Yao, N. Shinn, P. Razavi, and K. Narasimhan. ğœ -bench: benchmark for tool-agent-user interaction in real-world domains. arXiv preprint arXiv:2406.12045, 2024. G. Yona, R. Aharoni, and M. Geva. Can large language models faithfully express their intrinsic uncertainty in words? In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 77527764, 2024. S. Zeng, Q. Wei, W. Brown, O. Frunza, Y. Nevmyvaka, Y. K. Zhao, and M. Hong. Reinforcing multi-turn reasoning in llm agents via turn-level credit assignment. In ICML 2025 Workshop on Computer Use Agents, 2025. 21 Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents B. Zhang, Y. Zhang, J. Ji, Y. Lei, J. Dai, Y. Chen, and Y. Yang. SafeVLA: Towards safety alignment of visionlanguage-action model via constrained learning. In Advances in Neural Information Processing Systems, 2025a. C. Zhang, F. Liu, M. Basaldella, and N. Collier. Luq: Long-text uncertainty quantification for llms. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 52445262, 2024. T. Zhang, L. Qiu, Q. Guo, C. Deng, Y. Zhang, Z. Zhang, C. Zhou, X. Wang, and L. Fu. Enhancing uncertaintybased hallucination detection with stronger focus. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 915932, 2023. T. Zhang, H. Shi, Y. Wang, H. Wang, X. He, Z. Li, H. Chen, L. Han, K. Xu, H. Zhang, et al. Token-level uncertainty estimation for large language model reasoning. arXiv preprint arXiv:2505.11737, 2025b. Q. Zhao, D. Li, Y. Liu, W. Cheng, Y. Sun, M. Oishi, T. Osaki, K. Matsuda, H. Yao, C. Zhao, et al. Uncertainty propagation on llm agent. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 60646073, 2025. G. Zhou, H. Pan, Y. LeCun, and L. Pinto. DINO-WM: World models on pre-trained visual features enable zero-shot planning. In International Conference on Machine Learning, 2025a. K. Zhou, C. Liu, X. Zhao, A. Compalas, D. Song, and X. E. Wang. Multimodal situational safety. In The Thirteenth International Conference on Learning Representations, 2025b. Y. Zhou, A. Zanette, J. Pan, S. Levine, and A. Kumar. ArCHer: Training language model agents via hierarchical multi-turn RL. In International Conference on Machine Learning (ICML), 2024. Y. Zhu, G. Li, X. Jiang, J. Li, H. Mei, Z. Jin, and Y. Dong. Uncertainty-guided chain-of-thought for code generation with llms. arXiv preprint arXiv:2503.15341, 2025. Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents"
        },
        {
            "title": "Appendix",
            "content": "We cover the following content in this Appendix. (A) Additional Details on Formulation (B) Alternative Uncertainty Measures (C) Extended Context (D) Note on Implementation (E) Action Categorization (F) Missing Proof (G) Discussion and Future Work A. Additional Details on Formulation A.1. Expression of various agentic prompting methods under our formulation. In Section 3.1, we have introduced definition of stochastic agent system (Def. 1) with probabilistic graphical model of specific conditional dependency assumptions 2. Under this model, we can abstract some representative agentic prompting methods (Ahn et al., 2022; Patil et al., 2025; Shinn et al., 2023; Yao et al., 2022) with unified language yet distinctive conditions as below. 1. Vanilla function calling (Patil et al., 2025): Same as Definition 1. Stochastic process of (ğ´ğ‘¡, ğ¸ğ‘¡, ğ‘‚ğ‘¡) without any extra conditions. 2. Few-shot prompting (Ahn et al., 2022): Just augmenting initial task-specification variable as ğ¸ 0 = , which may implicitly affect ğ¸0 {â„± (ğ‘–)}ğ‘ the action transition probability, ğ‘ƒğœ‹,ğ’¯ (), by conditioning. with some example rollouts â„± (ğ‘–) = {(ğ´ğ‘¡, ğ¸ğ‘¡, ğ‘‚ğ‘¡)}ğ‘‡ ğ‘– ğ‘¡= ğ‘–=1 3. ReAct (Wang et al., 2024b; Yao et al., 2022): Adding constraint on the action sampling as ğ´ğ‘– ğ‘ƒğœ‹,ğ’¯ (ğ´thkğ¸ğ‘–1, ğ‘‚ğ‘–1) if ğ´ğ‘–1 ğ’œthk else ğ´ğ‘– ğ‘ƒğœ‹,ğ’¯ (ğ´thkğ¸ğ‘–1, ğ‘‚ğ‘–1), where ğ´thk ğ’œthk and ğ´thk ğ’œthk randon varaibles from the partitions of ğ’œ standing for the space of textual reasoning and all others, respectively. 4. Reflexion (Shinn et al., 2023): Do multiple trials â„± (ğ‘–) = {â„± (1), ..., â„± (ğ‘–)} until ğ‘Ÿ(ğ‘–) := ğ‘Ÿ(â„± (ğ‘–)) = 1 or reach maximum trials, otherwise augmenting the per-trial initial state with an external long-term memory ğ¸(ğ‘–+1) 0 {â„³(â„± (ğ‘—), ğ‘Ÿ(ğ‘—))}ğ‘—ğ‘–, where the long-term memory â„³() constructs with textual feedback generated from an LLM prompted by the current trial trajectory and corresponding binary reward. Thus, the action transition probability, ğ‘ƒğœ‹,ğ’¯ (), is continually evolved by reflecting feedback from historical runs. = ğ¸(ğ‘–) 0 Overall, this suggests that our agent formulation is general and flexible enough to capture broad class of representative agentic prompting scenarios. A.2. Uncertainty Instantiations and Total Uncertainty Expansion. After defining the agent UQ in Def 2, we presented trajectory-level total uncertainty that expresses the joint uncertainty across multiple turns in an additive form of the initial query uncertainty, action uncertainty, and observation uncertainty. 23 Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents Given the joint probability, ğ‘ƒ (â„±ğ‘‡ ) = ğ‘ƒ (ğ¸0, ğ‘‚0)Î ğ‘‡ ğ‘–=1ğ‘ƒğœ‹,ğ’¯ (ğ´ğ‘–ğ¸ğ‘–1, ğ‘‚ğ‘–1)ğ‘ƒ (ğ‘‚ğ‘–ğ´ğ‘–, ğ¸ğ‘–),, we enumerate three example instances of the uncertainty measure ğ‘ˆ () that induces the simple additive form of total uncertainty: 1. Information content (negative log probability), ğ‘ˆ (ğ‘‹ = ğ‘¥) := log ğ‘ƒ (ğ‘‹ = ğ‘¥), to measure pointwise surprisal for given observation. 2. Entropy, ğ‘ˆ (ğ‘‹) := ğ»(ğ‘‹) = E[ log ğ‘ƒ (ğ‘‹)], to measure the expected amount of surprisal. 3. Relative entropy, ğ‘ˆ (ğ‘‹) := DKL (ğ‘„(ğ‘‹)ğ‘ƒ (ğ‘‹)) with pre-defined reference distribution ğ‘„(ğ‘‹) and Kullback-Leibler divergence DKL, to measure the expected amount of surprisal given prior knowledge (if we have any). For information content, it is trivial to show by just taking the negative logarithm to ğ‘ƒ (â„±ğ‘‡ ). For the entropy and relative entropy, we have the following chain rule (Cover, 1999), ğ»(â„±0, ..., â„±ğ‘‡ ) = ğ‘‡ DKL ğ‘–=0 ğ»(â„±ğ‘–â„±ğ‘–1, ..., â„±1), (ğ‘„(â„±0, ..., â„±ğ‘‡ )ğ‘ƒ (â„±0, ..., â„±ğ‘‡ )) = ğ‘‡ ğ‘–=0 DKL (ğ‘„(â„±ğ‘–â„±ğ‘–1, ..., â„±1)ğ‘ƒ (â„±ğ‘–â„±ğ‘–1, ..., â„±1)), which directly drives our uncertainty expansion for ğ‘ˆ (â„±ğ‘‡ ) in Section 3.1. B. Alternative Uncertainty Measures Although we have mainly focused on the above three information-theoretic uncertainty measures throughout the paper, there are plenty of alternatives one can consider depending on the problem setups. In this section, we examine some possible alternative uncertainty measures, RÃ©nyi entropy, Tsallis entropy, and informational energy. RÃ©nyi entropy (RÃ©nyi, 1961) has been widely applied in modern machine learning, ecology, biodiversity science, and quantum information, among other fields. Given an order parameter 0 < ğ›¼ < and ğ›¼ = ğ‘¥ğ’³ ğ‘ƒ (ğ‘¥)ğ›¼) and generalize many other entropies: max-entropy 1, it is defined as ğ»ğ›¼(ğ‘‹) = 1 ğ»0(ğ‘‹) = log ğ’³ , Shannon entropy limğ›¼1 ğ»ğ›¼(ğ‘‹) = ğ‘¥ğ’³ ğ‘ƒ (ğ‘¥) log ğ‘ƒ (ğ‘¥), collision entropy ğ»2(ğ‘‹) = log ğ‘¥ğ’³ ğ‘ƒ (ğ‘¥)2, and min-entropy ğ»(ğ‘‹) . = minğ‘¥ğ’³ log ğ‘ƒ (ğ‘¥). 1ğ›¼ log( extends this further to model complex system where the additivity Tsallis entropy (Tsallis, 1988) property of Shannon and RÃ©nyi entropies for independent subsystems does not hold. Given an index parameter ğ‘ R{1}, it is defined as ğ»ğ‘(ğ‘‹) = 1 ğ‘¥ğ’³ ğ‘ƒ (ğ‘¥)ğ‘ , which recover Shannon entropy when ğ‘ 1. ğ‘1 Along with RÃ©nyi entropy, Tsallis entropy has taken key role in characterizing complex physical systems as nonextensive measure of entropy (Tsallis and Brigatti, 2004). is another Onicescu informational energy (Calin and UdriÅŸte, 2014; Pardo and Taneja, 1991) popular measure of (un)certainty that has bred many applications in quantum mechanics, economics, ecology, social sciences, and so on. It is defined as IE(X) := xğ’³ P(x)2 which can be connected to RÃ©nyi entropy of order-2 ğ»2(ğ‘‹) = log ğ‘¥ğ’³ ğ‘ƒ (ğ‘¥)2 = log IE(X), as well as the power entropy (Vajda, 1968; Vajda and ZvÃ¡rovÃ¡, 2007) ğ‘‰2(ğ‘‹) = 1 ğ‘¥ğ’³ ğ‘ƒ (ğ‘¥)2 = 1 IE(X). There is also concept, Onicescus , to quantify the dependency correlation coefficient (Calin and UdriÅŸte, 2014), ğœŒ(ğ‘‹, ğ‘Œ ) = structure between multiple variables. ğ‘¥,ğ‘¦ ğ‘ƒ (ğ‘¥)ğ‘ƒ (ğ‘¦) IE(X)IE(Y) 24 Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents These sophisticated measures may be worth investigating for the LLM agents inference interface, characterized by long-range interactions, evolving memory, and the multifractal property. C. Extended Context Connection to the probabilistic Turing machines. At some level of abstraction, one can view the LLM agents as probabilistic interactive Turing machines (PITMs) (Gill, 1977) that induce distribution over action-observation transcripts. This connection may allow us to borrow some possible formal tools from the PITM literature to design UQ method for LLM agents. However, we note the following distinctions: (1) PITMs have an explicit randomness source and defined distribution, whereas LLM agent also has procedural uncertainty induced by the decoding time strategies; (2) while PITMs interact via fixed channels, LLM agents commonly interact with users and tools in partially observable or changing formats; and (3) the goal of LLM agents are often underspecified upfront and can be negotiated over turns depending on the situation, though PITMs have concrete accept/output condition. Connection to the partially observable Markov decision process. One might draw an analogy between the agent UQ and belief tracking in Partially Observable Markov Decision Processes (POMDPs) (Kaelbling et al., 1998). However, the agent theme challenges the standard setups in traditional POMDPs: (1) there is no explicit belief state, but the agent carries an implicit belief in its memory; (2) actions and observations comprise language and structured strings, spanning an effectively infinite spacefar from the small discrete spaces in classical POMDP; and (3) it concerns more on the agents uncertainty, whereas POMDP concerns mostly on environment uncertainty. These connections highlight the distinctive edge of agent UQ, while grounding it in well-established classical problem set. D. Note on Implementation While we set our goal to provide principled vision with high-level roadmap for agent UQ, we also discuss some implementation recipes in this section to provide useful guidance for practitioners and researchers. Action classifier. To realize the conditional information gate, we first need to classify actions to determine whether each action will contribute to total uncertainty reduction for that task or not. As emphasized in Section 3, considering the interactivity of actions is key to accounting for agent uncertainty dynamics, as well as the evidentiality (or factuality) of the action that is solely considered for the existing approaches so far. Therefore, we may want to implement compound classifier with both interactivity assessment and evidentiality verification. They can be implemented through rule/syntactic-based, simple verifier (Mu et al., 2024), an LLM judge with specialized prompting, fine-tuning (Liu et al., 2025), or hybrid of rule-based and LLM-based (Peng et al., 2025). In our proposed total uncertainty lower bound Eq. 6, there are three uncertainty Uncertainty estimation. terms, including initial query uncertainty, action uncertainty, and observation uncertainty. The action uncertainty can be estimated or approximated by leveraging existing techniques, such as sampling-based ones entropy (Kuhn et al., 2023; Malinin and Gales, 2021), pure probability-based methods (Duan et al., 2024; Fomicheva et al., 2020), hybrid (Vashurin et al., 2025), or the conformal prediction set size (Kumar et al., 2023). 25 Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents Action Category Interactivity Informationgathering Asking clarification or confirmation to user Thinking State-changing tool call (write) Providing final information to user interactive interactive noninteractive noninteractive noninteractive Scenario-specific Example Search flight information (ğœ -bench Airline); retrieve order status (ğœ -bench Retail); read message box (ToolSandbox) Ask for flight choice or request final booking decision (ğœ -bench Airline); instruct or ask the user to choose cancellation (ğœ -bench Retail); ask for providing contact information or request updating contact number (ToolSandbox) Plan flight schedule (ğœ -bench Airline); consider returning items (ğœ -bench Retail); think the reason of failed temperature checking (ToolSandbox) Update flight booking (ğœ -bench Airline); return/cancel items (ğœ -bench Retail); Turn on Wifi (ToolSandbox) Summarize updated flights (ğœ -bench Airline); summarize returned items (ğœ -bench Retail); summarize phonebook update results (ToolSandbox) Table 2 Example action classes in three scenarios from two benchmarks, ğœ -bench (Barres et al., 2025; Yao et al., 2024) and ToolSandbox (Lu et al., 2025). However, the observation uncertainty and the initial query uncertainty will be much harder to estimate, as we usually do not have full knowledge of the world. If the system allows multiple trials, one can utilize sampling-consistency-based approach to estimate the observation uncertainty, likewise the traditional black-box UQ methods. If the system does not allow this, one may try to construct datastore that contains some relevant data from similar task and leverage nonparameter estimation methods (Kotelevskii et al., 2022; Ren et al., 2023). Another line of methods can be establishing world model to approximate the environment transition and estimate observation uncertainty from it (Anonymous, 2026; Chae et al., 2025; Zhou et al., 2025a). Mutual information estimation. The proposed information gating mechanism features the information reduction based on the amount of information gain by measuring the conditional mutual information between the current observation ğ‘‚ğ‘– and initial query ğ‘‚0, given all previous conversations except the initial query ğ¸ğ‘–ğ‘‚0. While the estimation of mutual information is notorious for its difficulty, recent advancements on neural estimators (Belghazi et al., 2018; Cheng et al., 2020b; Gritsai et al., 2025; Molavipour et al., 2020; Mukherjee et al., 2020) shed light on this problem by enabling estimation in high-dimensional, unstructured data space. Conditional certainty maximization process. As an analogy to the conditional uncertainty reduction process presented in Section 4.2, we can also envision certainty maximization approach by replacing the uncertainty with certainty measure, such as informational energy (Pardo and Taneja, 1991), mentioned in Appendix B. Given that Onicescus informational energy brings its correlation coefficient as well, we may be able to implement the exact same style information-gating-based method that has the opposite direction to uncertainty reduction dynamics. 26 Towards Reducible Uncertainty Modeling for Reliable Large Language Model Agents E. Action Categorization Along with the flight booking scenario in Table 1, we extend our action category analysis to two more scenarios, retail and telecommunication tasks, from ğœ -bench (Barres et al., 2025; Yao et al., 2024) and ToolSandbox (Lu et al., 2025) in Table 2. F. Missing Proof Lemma 2 (Restatement of Lemma 1). Let the lower bound of agent total uncertainty in Eq. 6 be ğ‘ˆ (â„±ğ‘‡ ), denote ğ‘ˆ (ğ‘‹) := ğ»(ğ‘‹) = E[ log ğ‘ƒ (ğ‘‹)] and Info(ğ‘‹; ğ‘Œ ) := ğ¼(ğ‘‹; ğ‘Œ ) = E[log ğ‘ƒ (ğ‘‹,ğ‘Œ ) ğ‘ƒ (ğ‘‹)ğ‘ƒ (ğ‘Œ ) ], then, we have: ğ‘ˆ (â„±ğ‘‡ ) ğ»(ğ¸0, ğ‘‚0) ğ‘ˆ (â„±ğ‘‡ ) ğ»(ğ¸0, ğ‘‚0) + ğ‘‡ 1 ğ‘–=1 ğ‘‡ ğ‘–=1 ğ¼(ğ‘‚ğ‘–, ğ‘‚0ğ¸ğ‘–ğ‘‚0) + ğ»(ğ´ğ‘‡ ğ¸ğ‘‡ 1, ğ‘‚ğ‘‡ 1), ğ»(ğ´ğ‘–, ğ‘‚ğ‘–ğ¸ğ‘–1, ğ‘‚ğ‘–1). Proof. For ğ‘–, if ğ´ğ‘– ğ’œ (resp. ğ´ğ‘– / ğ’œ), turn-level uncertainty ğ‘ˆ (â„±ğ‘–) becomes ğ¼(ğ‘‚ğ‘–, ğ‘‚0ğ¸ğ‘–ğ‘‚0) (resp. ğ‘ˆ (â„±ğ‘–) = ğ‘ˆ (ğ´ğ‘–ğ¸ğ‘–1, ğ‘‚ğ‘–1) + ğ‘ˆ (ğ‘‚ğ‘–ğ´ğ‘–, ğ¸ğ‘–) = ğ»(ğ´ğ‘–, ğ‘‚ğ‘–ğ¸ğ‘–1, ğ‘‚ğ‘–1)). Thus, if all the intermediate actions are interactive and evidential, e.g., ğ´ğ‘– ğ’œ for all 1 ğ‘– ğ‘‡ 1 (resp. ğ´ğ‘– / ğ’œ for all 1 ğ‘– ğ‘‡ 1), our total uncertainty lower bound ğ‘ˆ (â„±ğ‘‡ ) becomes monotonic uncertainty reduction process (resp. monotonic propagation process), deriving the above two inequalities. Here, the final termination-turn action should not be interactive, and the observation in this turn is deterministic in our considered setups  (Fig. 2)  , resulting in the accumulation of last action uncertainty ğ»(ğ´ğ‘‡ ğ¸ğ‘‡ 1, ğ‘‚ğ‘‡ 1). G. Discussion and Future Work Does solely establishing the agent UQ framework actually help us to reliably infer on the agents performance on task? Modern LLMs (especially after post-training) are not well-calibrated (Tian et al., 2023), so the uncertainty estimates from these ill-calibrated probabilistic models can not be directly used as reliable performance indicator. Although we present high-level roadmap for quantifying uncertainty for LLM agent systems, future work should also explore the calibration of LLM agents as well. The connection between calibration and accuracy (Oh et al., 2024; Park et al., 2020) of predictor shows an exciting future work direction for joint optimization of the agents problem-solving capability and calibration simultaneously. Another promising direction for future investigations would be to extend the agent UQ framework to multimodal setups such as graphical user interface agents (Nguyen et al., 2025), where extra challenges occur, such as many-to-many correspondence between modalities (Chun, 2025). Defining and establishing reliable UQ framework for multimodal agents would offer helpful foundations to develop robustness solutions (Oh et al., 2025a,b), safety (Chen et al., 2025; Zhou et al., 2025b) and more (Shi et al., 2025)."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "Nanyang Technological University",
        "University of California, Berkeley",
        "University of Pennsylvania",
        "University of Southern California",
        "University of Wisconsin-Madison"
    ]
}