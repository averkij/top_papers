{
    "paper_title": "Equivariant Image Modeling",
    "authors": [
        "Ruixiao Dong",
        "Mengde Xu",
        "Zigang Geng",
        "Li Li",
        "Han Hu",
        "Shuyang Gu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Current generative models, such as autoregressive and diffusion approaches, decompose high-dimensional data distribution learning into a series of simpler subtasks. However, inherent conflicts arise during the joint optimization of these subtasks, and existing solutions fail to resolve such conflicts without sacrificing efficiency or scalability. We propose a novel equivariant image modeling framework that inherently aligns optimization targets across subtasks by leveraging the translation invariance of natural visual signals. Our method introduces (1) column-wise tokenization which enhances translational symmetry along the horizontal axis, and (2) windowed causal attention which enforces consistent contextual relationships across positions. Evaluated on class-conditioned ImageNet generation at 256x256 resolution, our approach achieves performance comparable to state-of-the-art AR models while using fewer computational resources. Systematic analysis demonstrates that enhanced equivariance reduces inter-task conflicts, significantly improving zero-shot generalization and enabling ultra-long image synthesis. This work establishes the first framework for task-aligned decomposition in generative modeling, offering insights into efficient parameter sharing and conflict-free optimization. The code and models are publicly available at https://github.com/drx-code/EquivariantModeling."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 8 4 9 8 1 . 3 0 5 2 : r a"
        },
        {
            "title": "Equivariant Image Modeling",
            "content": "Ruixiao Dong1,2, Mengde Xu2, Zigang Geng1,2, Li Li1, Han Hu2, Shuyang Gu2* 2Tencent Hunyuan Research 1University of Science and Technology of China {dongruixiaoyx, zigang}@mail.ustc.edu.cn, lil1@ustc.edu.cn {jaredsheaxu, cientgu}@tencent.com"
        },
        {
            "title": "Abstract",
            "content": "Current generative models, such as autoregressive and diffusion approaches, decompose high-dimensional data distribution learning into series of simpler subtasks. However, inherent conflicts arise during the joint optimization of these subtasks, and existing solutions fail to resolve such conflicts without sacrificing efficiency or scalability. We propose novel equivariant image modeling framework that inherently aligns optimization targets across subtasks by leveraging the translation invariance of natural visual signals. Our method introduces (1) column-wise tokenization which enhances translational symmetry along the horizontal axis, and (2) windowed causal attention which enforces consistent contextual relationships across positions. Evaluated on class-conditioned ImageNet generation at 256256 resolution, our approach achieves performance comparable to state-of-the-art AR models while using fewer computational resources. Systematic analysis demonstrates that enhanced equivariance reduces intertask conflicts, significantly improving zero-shot generalization and enabling ultra-long image synthesis. This work establishes the first framework for task-aligned decomposition in generative modeling, offering insights into efficient parameter sharing and conflict-free optimization. The code and models are publicly available at https://github. com/drx-code/EquivariantModeling. 1. Introduction Generative modeling has gained significant attention in computer vision, particularly for image generation tasks. Recent advances in autoregressive (AR) models [7, 18, 40] and diffusion processes [12, 30, 35] demonstrate remarkable capabilities in modeling complex data distributions through shared strategy: decomposing the challenging problem of learning high-dimensional distributions into sequences of simpler conditional distribution estimations. AR *Corresponding Author. models achieve this through token-by-token prediction conditioned on previous outputs, while diffusion models employ iterative denoising across predefined noise levels. This multi-task learning paradigm raises critical questions about inter-task relationships - specifically, how decomposition strategies affect the synergies or conflicts between subtasks during joint optimization. Recent investigations reveal inherent limitations in current decomposition frameworks. The MinSNR [10] identifies task conflicts in diffusion models and proposes Pareto optimization through careful loss weighting adjustments. An alternative approach, eDiff-I [1] attempts to mitigate conflicts via task-specific parameter groups; however, this leads to an explosion in the number of parameters and ignores the relevance of different tasks. These methods fundamentally fail to address the conflicts in multi-task optimization. These observations motivate us to consider the core research question: Can we establish principled task decomposition framework that inherently aligns optimization target across subtasks? In this paper, we present the first equivariant image modeling paradigm that systematically minimizes inter-task conflicts. This task decomposition method is inspired by the unbounded visual signals in nature. When people look around, they do not perceive that the visual signals mutate at fixed position. Therefore, we propose left-to-right modeling framework that includes: Equivariant tokenization: We replace the conventional 2D patch grids with column-wise tokenization, enhancing spatial uniformity while better preserving natural image statistics (e.g., horizontal translation invariance in textures). Equivariant modeling: windowed causal attention mechanism that enforces consistent contextual relationships across positions. We validate our framework through comprehensive experiments on class-conditioned image generation. When evaluated on ImageNet at 256256 resolution, our approach achieves performance comparable to state-of-theart AR models while requiring substantially fewer computational resources. Through systematic analysis of the 1 models equivariance properties, we demonstrate that enhanced task alignment significantly improves parameter sharing efficiency across subtasks - particularly benefiting zero-shot generalization capabilities. This intrinsic equivariance proves especially advantageous for generating unbounded natural scenes, outperforming human-collected datasets that contain spatial inductive bias. Our principal contributions can be summarized as follows: The first equivariant image modeling paradigm that fundamentally aligns subtask optimization target. column-wise 1D tokenization scheme that eliminates the spatial constraints inherent in conventional 2D gridbased approaches, delivering competitive performance on class-conditioned generation with fewer computational cost than standard AR models. An analytical framework for quantifying subtask conflicts, with empirical evidence showing that enhanced equivariance improves zero-shot generalization and enables ultra-long image generation. 2. Preliminaries 2.1. Equivariance Modern image generation paradigms, including autoregressive [7, 40] and diffusion models [12, 30], decompose the complex task of image distribution modeling into explicit and traceable subtasks. This decomposition inherently formulates image generation as multi-task learning problem. Considering that these different tasks share identical parameters, maintaining equivariance among them becomes critical for learning efficiency [8]. Specifically, it is essential to ensure all subtasks exhibit congruent optimization trajectories under shared parameter configuration. Formally, we define: Definition 2.1 In shared parameter space H, given set of subtasks {Tt}N t=1 with their performance measurements {Pt}N t=1, the subtask group is termed equivariant if for any pair Ti, Tj , their optimization directions coincide: θ = arg max θH Pi(θ) = arg max θH Pj(θ) (1) where θ denotes the network parameters and represents the optimization space. For instance, in image generation, if predicting pixels at different spatial locations constitutes distinct subtasks, equivariance implies that the optimization direction for predicting any pixel should remain consistent, regardless of its position. This property enables facilitates transfer learning across all spatial locations and efficient parameter sharing. 2.2. Autoregressive Image Modeling Autoregressive models decompose images into token sequences {x1, ..., xt} through the factorization: p(x1, ..., xnc) = (cid:89) i=1 p(xix<i, c), (2) where represents the external condition, and each conditional distribution mapping p(xix<i, c) corresponds to subtask predicting the i-th token given the preceding tokens x<i and the condition c. While enabling tractable likelihood estimation, conventional 2D grid-based autoregressive models face three critical challenges: Subtask Heterogeneity: The fixed raster-scan ordering [7]* creates inherently distinct prediction tasks. For example, predicting border-region tokens typically proves more challenging than central-region tokens due to less local context. Non-stationary Context Dependence: The standard autoregressive paradigm conditions each token generation on all preceding tokens, resulting in cumulatively increasing contextual dependencies. While wider context makes later positions easier to predict, it can cause the model to disproportionately optimize for these easier subtasks, potentially neglecting the more challenging predictions at earlier positions. Architectural Non-Equivariance: Deep neural networks, particularly those employing self-attention mechanisms, can disrupt fundamental geometric symmetries. For example, position embeddings and attention patterns may introduce biases that violate translation invariance. While recent work such as RAR [45] and TiTok [46] can partially address some challenges through stochastic ordering and 1D tokenization respectively, alleviating the inductive bias of AR image generation to certain extent, there is still lack of guarantee for the consistency of optimization directions across different tasks. This limitation potentially restricts their effectiveness in complex generation scenarios. Our work seeks to establish subtask decomposition framework, where each subtask does not conflict and even promotes one another. 3. Towards Equivariant Autoregressive Image"
        },
        {
            "title": "Generation",
            "content": "To address the challenges discussed in Sec. 2.2, we propose column-based generation framework that explicitly removes the 2D grid structure and establishes 1D equivariant modeling method, as illustrated in Fig. 1. Our approach includes two key components: column-wise tokenizer that *Though alternative orders like order exist, they exhibit similar subtask decomposition properties. 2 naturally preserves vertical equivariance by organizing image features into column-based 1D tokens ( Sec. 3.1), and an autoregressive transformer equipped with equivariant context ( Sec. 3.2). 3.1. Equivariant Tokenization via Columnization Columnization and Rasterization. Visual signals in nature have no boundaries. When people look around, visual signals resemble slowly unfolding scroll, without any sudden changes at fixed positions. Inspired by this, we consider tokenizing the image into 1D latent sequence. Specifically, we employ columnization to transform the classical 2D tokenizer into 1D sequence. This process involves reshaping the height dimension into channels, followed by linear projection to compress the representation into column-wise tokens. Specifically, given an output feature map of shape from deep encoder, we transform it into 1D token sequence of shape through: ermute Reshape (HC) roject . (3) This columnization process eliminates the grid structure, resulting in each token representing vertical visual signal, and adjacent tokens at any position transitioning naturally. Although the human-collected dataset may produce inductive biases due to camera settings and photographer preferences (e.g., the edge of the image may be darker due to the aperture), we leverage reflect padding to alleviate edge inconsistency. This semantically continuous transition between tokens provides the basic conditions for subsequent equivariant autoregressive modeling. We visualize the relationship between tokens and visual content in Fig. 2. For image reconstruction, we rasterize the token sequence by projecting it to higher-dimensional space and reshaping the channels back into the height dimension before processing it through deep neural network: roject (HC) ermute Reshape C. (4) Semantic Aligned Tokenizer Training. Our columnization and rasterization operate on the deep features of an autoencoder inherited from [30]. Following their approach, we train the tokenizer using multiple loss components: pixel-wise reconstruction loss Lrec, adversarial loss Lgan, perceptual loss Lp, and KL divergence loss Lreg to regularize the token sequence distribution. Additionally, similar to [47], we introduce an alignment loss Lalign that aligns the decoders second-layer output features with those of pretrained DINOv2 [25] model, helping to preserve the latent spaces semantic structure. The total loss combines the original autoencoder loss terms with semantic alignment loss: where balancing coefficients λi are set to 1.0, 0.01, 1.0, 0.5, and 5.0 by default. 3.2. Equivariant Autoregressive Modeling Windowed Causal Attention. As shown in Eq. (2), autoregressive transformers employ causal attention to ensure that each token can attend to all its preceding tokens, making later subtasks significantly easier than earlier ones. straightforward solution to address the potential imbalance in training is to limit each tokens context to fixed window of previous tokens: p(x1, ..., xnc) = (cid:89) i=1 p(xixik,<i, c) (6) We named this approach as Real Equivariant Modeling. While this promotes equivariance across subtasks, it compromises the long-range dependencies that are often useful for image generation [48]. As an alternative, we implement windowed causal attention with fixed context size in each transformer layer. In this way, the network can increase the receptive field by stacking multiple layers, thereby implicitly modeling the long-range dependencies. For position in each layer, the attention operation becomes: Attn(qi, K, ) = softmax( qiK iw:i )Viw:j (7) where qi represents the query at position i, and Kiw:i, Viw:i denote key-value pairs from the previous positions. While this approach partially constrains the receptive field, empirical observations indicate that strategically leveraging the models equivariant properties enhances generation quality. Our model architecture features causal transformer with the windowed causal attention mechanism. In each self-attention layer, we utilize rotary position embedding [37] to encode token positions within the sequence. To inject conditioning information, we place cross-attention layer between the self-attention layer and feed-forward network in each transformer block. Previous works [13, 48] find that the cross-attention requires absolute position embeddings to effectively convey global layout information. it may cause architectural non-equivariance. However, Therefore, we mitigate this issue through position embedding augmentation, randomly shifting the position index of the entire sequence with randomly selected value. Following previous work [18], we implement diffusion head above the transformer to predict the next token from random noise. To accelerate training, we employ the flow matching algorithm [19, 20], and the overall loss function is: Ltotal = λ1Lrec + λ2Lreg + λ3Lp + λ4Lgan + λ5Lalign (5) = Ei[1,n],tD(txi + (1 t)ϵi, t, zi) (xi ϵi)2, (8) 3 Figure 1. Illustration of Equivariant Image Generation Framework. The tokenizer translates the image into 1D tokens arranged in columns and an enhanced autoregressive model models the column-wise token distribution. lows [7, 30], employing 2D encoder with downsampling factor of 16, followed by columniation operation to produce token tensors Rwc where = 16, = 256. Training utilizes the Adam optimizer [15] for 320k iterations with batch size of 192. The learning rate starts at 1.92 104, with linear warm-up over 5,000 iterations and 20% decay every 30,000 iterations. The semantic loss is introduced after 20,000 iterations. We train the generator for 1,200 epochs using AdamW [21] with batch size of 2,048. The initial learning rate is 8 104, which is linearly warmed up over 100 epochs and then maintained at constant level. The weight decay is set to 0.02, along with the momentum parameters (β1, β2) = (0.9, 0.95). An exponential moving average (EMA) of the parameters in the generator is maintained with momentum of 0.9999. Evaluation Metrics. We evaluate reconstruction fidelity using the dataset-level reconstruction Frechet Inception Distance (rFID)[11]. For generation quality, we measure both the generative Frechet Inception Distance (gFID)[11] and the Inception Score (IS) [31]. 4.2. Deep Analysis on Equivariance Following the definition in Sec. 2, we examine the equivariance of our method by analyzing transferability across subtasks. We set up MAR-AR variant model with 2D gridbased token sequences of length 16 as our baseline, denoted as AR-MAR-2D. Zero-shot Transfer between Subtasks. We investigate model generalization across different generative subtasks. With our tokenizer producing sequences of length 16, we have 16 distinct generative subtasks (detailed in Sec. 2.2). Figure 2. Visual Meanings of 1D Tokens. By progressively replacing the randomly initialized token sequence with tokens encoded from the ground truth images, the decoder faithfully reconstructs the original images step by step. where denotes the noise level, which is sampled from (0, 1). Accordingly, xi and zi denote the ground truth token and the output of the causal transformer at position i. ϵi is randomly sampled Gaussian noise, and denotes the denoising network, which predicts the vector field at noise level t. 4. Experiments 4.1. Experimental Settings We conduct experiments on the ImageNet-1k dataset [6]. All images are resized to 256256 resolution, utilizing standard augmentation techniques, including random cropping and horizontal flipping. We follow the common train/validation split to ensure consistent evaluation with prior approaches. Implementation Details. Our tokenizer architecture fol4 Figure 3. Training Loss of Different Models. Left: the training loss of different methods at early (10 epoches) and late (100 epoches) training stage. Right: the relative loss improvement of different methods under different settings compared to the early stage of Multi-task setting. The higher value indicates better performance. The equivariant generation approach can transfer the improvement from single task to other untrained tasks. Method # Num task. gFID AR-MAR-2D Ours AR-MAR-2D Ours 16 8 8 7.93 5.57 92.46 8.99 more uniform behavior across subtasks, confirming better inter-task consistency. Notably, both methods show elevated loss for middle subtasks (corresponding to image centers), probably due to ImageNets object-centric nature. We validates the data bias in Fig. 4 by comparing task-wise Table 1. Performance under Zero-shot Setting. # Num task. is used to denote the number of trained subtasks. The total number of subtasks is 16 for all methods. We train the model on 8 selected subtasks and evaluate performance across all 16 subtasks using gFID. As shown in Tab. 1, while both approaches perform well when trained on all subtasks, our method maintains competitive performance under the zero-shot setting, with only 3.42 point drop in gFID. In contrast, the 2D baseline struggles significantly with untrained subtasks, with gFID deteriorating from 7.93 to 92.46. This suggests our framework creates more consistent generative patterns across subtasks, enabling better generalization. Training Dynamics Analysis. We further examine equivariance with training dynamics using two settings: 1) Multi-task training - the standard generative modeling setting, where shared-parameter model is trained on all subtasks; 2) Single-task training - focusing on individual subtasks (4th and 11th) to isolate task interference effects. Fig. 3 (left) shows the evolution of the multi-task training loss. While both methods converge to lower loss with longer training iterations, they exhibit different patterns across subtasks. The baseline consistently shows higher loss for subtasks corresponding to the images boundary (5th, 9th, and 13th subtasks, which are the first tokens of each row in the 2D grid), while our method demonstrates Figure 4. Converged Training Loss on ImageNet vs LHQ. Compared to ImageNet, the visual statics in LHQ demonstrates greater uniformity, as does the task-wise loss distribution. loss across different datasets. When testing on LHQ [34], landscape dataset with more uniform spatial distribution than ImageNet, the elevated middle loss phenomenon disappears. This motivates us to consider the datasets influence in future research studying the equivariance problem. Moreover, to isolate the influence of the data, we use the multi-task loss at the early training stage as our baseline and measure relative improvement across methods (with lower loss indicating positive improvement). The results appear in the right panel of Fig. 3. In the multi-task setting, both our method and AR-MAR-2D converge to 10% relative improvement. However, in single-task training, our method successfully transfers the performance gain to all other subtasks, while the 2D baseline demonstrates negative impacts on untrained subtasks. These dynamics align with overall 5 generation performance and suggest promising approach for rapid equivariance verification. Method gFID IS #Para. #Len. GFLOPs 4.3. Equivariance Application DiT [26] 2. 675M - 118.64 Long Content Image Generation. Our experiments demonstrate that, due to the equivariance of our method, the approach exhibits strong generalization ability across different subtasks. Inspired by the unbounded visual signals in nature, combining the generalization ability to generate training-unseen subtasks granted by the equivariant property, we test our models under long image generation scenarios. Specifically, we train our model on the Nature subset of the Places dataset [52] for class-conditional generation, which contains 30 categories. The training image size is 256 256, and we keep all other training parameters the same as in the ImageNet training phase. Fig. 5 showcases some generated examples of extendedlength arbitrary resolution images produced by our model. These generated images exhibit high spatial resolution, with lengths significantly greater than 256. The presented results demonstrably illustrate the zero-shot long-content capability of our method, primarily attributed to its inherent equivariance property: although we only leverage 256256 resolution images to optimize the model, our approach effectively generates content at positional indices not encountered during training. Specifically, our method achieves the generation of images up to eight times longer than the input instances used during training, maintaining high visual fidelity and crucially avoiding discernible sharp edges between adjacent generated regions. Interactive Generation From Human Feedback. Another advantageous aspect of our method is that each token corresponds to visually trackable area. This allows for interactive, token-wise feedback from users. We illustrate some examples in Fig. 6. When visually poor token is produced, we can promptly discard it and generate new one. Ideally, our method could be integrated into an image editing pipeline, enabling complete control over the generation process. We leave this for future work. 4.4. System Comparison with SOTA Methods As detailed in Tab. 2, comparative analysis was conducted between our method and state-of-the-art generative methodologies, including diffusion models and autoregressive methods along with their variants. Our approach demonstrates comparable or better performance than the other methods presented in the table. Significantly, our model outperforms the autoregressive variant of MAR methods, confirming that the introduction of equivariant properties indeed enhances the modeling capability. Furthermore, the reduced token length inherent in our method leads to substantial computational savings in both training and inference, as evidenced by the reduction of GFLOPs in Diffusion 278.2 MaskGIT 281.8 296.0 334.9 TiTok [46] MAR [18] FractalMAR [17] 1.97 1.78 7.30 287M 128 479M 64 - 438M VQGAN [7] VAR [40] MAR [18] Ours-S Ours-B Ours-L Ours-H Autoregressive 74.3 274.4 244.6 15.78 3.30 4.69 1.4B 256 310M 680 479M 64 7.21 5.57 4.48 4. 233.70 151M 16 260.05 294M 16 259.91 644M 16 16 290.66 1.2B 37.35 70.13 238.58 246.67 105.70 78.50 5.41 9.78 19.66 34.91 Table 2. Class-conditional Generation Results on ImageNet 256256 Benchmark. #Para. denotes the number of parameters in each generator, while #Len. indicates the token sequence length that generators are required to model. Method Tokens Length gFID GFLOPs AR-MAR-2D-B 256 AR-MAR-2D-B AR-MAR-2D-L Ours-B Ours-L 16 16 16 16 3.99 7.93 7.49 5.57 4.48 130.46 9.79 19.68 9.78 19.66 Table 3. Comparison on the performance and computational efficiency. Our methods achieves better trade-off between performance and computation cost. the table, particularly when compared to methods employing significantly longer token lengths ( 64). We also compare our method with standard 2D variants in Tab. 3. With similar GFLOPs, our method attains superior generation performance. Moreover, our model achieves comparable gFID to the 256-token baseline while requiring only 15% of the GFLOPs. The results demonstrate the effectiveness of our equivariance-driven design in achieving strong outcomes with minimal computational overhead. 4.5. Ablation Studies Windowed Causal Attention. To address the inconsistency in causal transformer models arising from subtasks adapting to varying context lengths, we introduce windowbased attention. Specifically, each token is restricted to attending to fixed-size local window of ω tokens during attention computation. To evaluate the impact of this tech6 Figure 5. Visual examples of long image generation. We present visual examples of long images with arbitrary lengths, which are generated by our model that has been trained on the Places datasets with fixed length of 256. (a) w/o random shift Figure 6. Interactive Image Generation. During inference, each token is immediately visible and bad generated tokens (circled with orange rectangle) are dropped according to human feedback. (b) with random shift Figure 7. Visualization effect of the Augmented Position Embedding. The model is trained on subset of the Places dataset. nique, we perform comparative experiments on our small model with window size of ω = 3 and compare it against standard causal attention, where each token attends to all preceding tokens. Additionally, we train real equivariant variant by restricting the context window length during training; in this scenario, the model only observes randomly cropped sequences of length ω. Under these training conditions, the subsequences gain an explicit equivariant property: apart from the initial ω 1 tokens, generated subsequences are invariant to shifted positions. As summarized in Tab. 4, limiting the receptive field slightly weakens the modeling of long-range dependencies but enhances consistency and uniformity across tasks, thereby improving the overall training of the generator. In particular, our generator with window causal attention surpasses the baseline model. Moreover, due to the fixed context size in the transformer layer, it reduces the theoretical computational cost of attention by approximately 42.9% Method Equ. L-Ctx. gFID Attn FLOPs Full Causal Real Equ. Window Causal 7.35 8.87 7.21 4.2M 0.26M 1.8M Table 4. Ablation Study of Windowed Causal Attention. Real Equ. indicates the strictly equivariant model variant as described in Sec. 3.2; Equ. refers to whether the model possesses equivariant properties; and L-Ctx. denotes whether long-range contexts are utilized. compared to causal attention. Conversely, while the real equivariant generator maintains highly equivariant properties, it exhibits significantly reduced performance due to overly limited receptive fields, demonstrating that preserving long-range context information remains crucial for generative modeling. Position Embedding Augmentation. We evaluate our proMethod rFID gFID 1 2 3 Ours baseline +Stronger discriminator + Decoder finetune +Semantic aligned loss 1.11 0.62 0.58 0. 7.10 6.29 6.25 5.57 Table 5. Ablation on the Impact of Tokenizer Components. Ours denotes the final setting we adopted in all other experiments. posed position embedding augmentation by training our large model on the Places dataset and comparing visualization results with and without this augmentation, which keeps the left side of the images as the original images, as illustrated in Fig. 7. We observe that the model trained without augmented position embeddings can still extrapolate beyond the training lengths to generate longer images; however, noticeable artifacts and discontinuities begin to appear in regions beyond the models training positions. In contrast, when augmented position embeddings are utilized, the generated images exhibit stronger consistency across spatial locations, indicating that the model effectively generalizes beyond the observed spatial contexts. Specifically, as shown in the visual comparisons ( Fig. 7), our augmented model synthesizes coherent, artifact-free, and smooth images even at significantly increased lengths, demonstrating improved capability for extrapolation and robust generalization. Components in Tokenizer. We analyze incremental improvements to our tokenizer within our base model, identified as (1) Baseline Tokenizer. The base tokenizer following LDM-tokenizer [30] achieves gFID of 7.10 and an rFID of 1.11. (2) Enhanced Discriminator. Replacing the default discriminator with DINO-small backbone [49] improves rFID by 0.49 and gFID by 0.81, emphasizing stronger feature discrimination. (3) Decoder Fine-tuning. Fine-tuning the decoder enhances reconstruction fidelity and marginally improves generation quality. (4) Alignment Loss Introduction. Recognizing that the semantic information contained in the latent space is insufficient for high-quality generation, we introduce an alignment loss Lalign to align the latent representations with pretrained DINOv2 model [25], resulting in significant improvement in the gFID metric to 5.57. 5. Related Work Image distribution modeling is long-standing topic in computer vision. One practical approach to solving this is visual signal decomposition, which breaks down the modeling task into subtasks. Autoregressive models decompose each subtask as nexttoken generation. Early works like PixelCNN [33, 41] decomposed image generation into pixel-by-pixel prediction in raster-scan order. Later approaches [7, 42] introduced two-stage process: first, encoding images into latent space utilizing tokenizer, and then employing autoregressive modeling to decompose latent modeling into sequential subtasks. While subsequent research [4, 9, 16, 22, 23, 27, 28, 38, 43, 44, 50, 51, 53, 54] has focused on enhancing tokenizer capacity, the fundamental decomposition logic remains unchanged and continues to face challenges associated with 2D grid structures. Various works have been proposed to eliminate prediction on 2D grid. VAR [40] introduced scale-wise decomposition, predicting from small to large scales. However, this approach still exhibits significant non-equivariance, with earlier tokens primarily encoding low-frequency information and later tokens capturing high-frequency details. Concurrent work [29] proposes predicting block-wise or row-wise tokens in parallel, which is technically similar to our approach. Diffusion models [12, 35] take different approach by decomposing visual signals into progressively denoised image sequences through shared-parameter models. Standard noise schedules, however, can create distributional inconsistencies across denoising stages. Several techniques have been proposed to address this challenge: the reparameterization trick [24, 32] has proven vital in unifying output distributions across various denoising tasks; Flow Matching [19, 20] and Consistency Models [36] aim to develop improved noise strategies that maintain consistency throughout the denoising process; Min-SNR [39] employs Pareto optimization by carefully adjusting loss weights to identify conflicts among tasks; and Variational Diffusion Models [14] and the Diffusion Schrodinger Bridge [5, 39] explore learning the noise-adding strategy rather than relying on fixed heuristics. Despite these advances, real-world application of these methods remains constrained by unresolved distributional mismatches. MaskGIT [3] decomposes the visual signals into depthwise generations that sequentially predict sets of tokens. TikTok [46] further simplifies the decomposition using 1D tokenizer. Muse [2] alleviates subtask conflicts in depthwise generation by dividing the subtasks into two groups and modeling them with distinct parameters. Although numerous techniques have been proposed to properly decompose visual signals, our work is the first to address this problem from an equivariance perspective, providing more systematic analytical framework. 6. Conclusion This work establishes Equivariant Image Modeling as principled framework for mitigating subtask conflicts in generative models. By introducing column-wise tokenization and equivariant token modeling, we demonstrate that the spatial equivariant decomposition aligns optimization targets, enhancing parameter efficiency and zero-shot gen8 eralization. Experimental results on ImageNet-1k generation validate the computational advantages of our approach over conventional autoregressive models. The exploration of equivariant task decomposition opens new directions for the future development of generative models."
        },
        {
            "title": "References",
            "content": "[1] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, et al. ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022. 1 [2] Huiwen Chang, Han Zhang, Jarred Barber, Aaron Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Patrick Murphy, William Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked genIn ICML, pages 40554075. PMLR, erative transformers. 2023. 8, 12 [3] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In CVPR, pages 1131511325, 2022. 8 [4] Junyu Chen, Han Cai, Junsong Chen, Enze Xie, Shang Yang, Haotian Tang, Muyang Li, and Song Han. Deep compression autoencoder for efficient high-resolution diffusion models. In ICLR, 2025. 8 [5] Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion schrodinger bridge with applications to score-based generative modeling. NeurIPS, 34:17695 17709, 2021. [6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In CVPR, pages 248255, 2009. 4, 11 [7] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In CVPR, 2021. 1, 2, 4, 6, 8 [8] Shuyang Gu. Several questions of visual generation in 2024. arXiv preprint arXiv:2407.18290, 2024. 2 [9] Yuchao Gu, Xintao Wang, Yixiao Ge, Ying Shan, and Mike Zheng Shou. Rethinking the objectives of vectorIn CVPR, pages quantized tokenizers for image synthesis. 76317640, 2024. 8 [10] Tiankai Hang, Shuyang Gu, Chen Li, Jianmin Bao, Dong Chen, Han Hu, Xin Geng, and Baining Guo. Efficient diffusion training via min-snr weighting strategy. In ICCV, pages 74417451, 2023. [11] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. NeurIPS, 30, 2017. 4 [12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 33:68406851, 2020. 1, 2, 8 [13] Runtong Hou and Xu Zhao. High-quality talking face generation via cross-attention transformer. In 2024 IEEE International Conference on Real-time Computing and Robotics (RCAR), pages 194199, 2024. 3 [14] Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. NeurIPS, 34:21696 9 21707, 2021. [15] Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization. In ICLR, 2015. 4 [16] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In CVPR, pages 1152311532, 2022. 8 [17] Tianhong Li, Qinyi Sun, Lijie Fan, and Kaiming He. Fractal generative models. arXiv preprint arXiv:2502.17437, 2025. 6 [18] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. NeurIPS, 37:5642456445, 2024. 1, 3, 6 [19] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In ICLR, 2023. 3, [20] Xingchao Liu, Chengyue Gong, and qiang liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In ICLR, 2023. 3, 8, 12 [21] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. 4 [22] Zhuoyan Luo, Fengyuan Shi, Yixiao Ge, Yujiu Yang, Limin Wang, and Ying Shan. Open-magvit2: An open-source project toward democratizing auto-regressive visual generation. arXiv preprint arXiv:2409.04410, 2024. 8 [23] Fabian Mentzer, David Minnen, Eirikur Agustsson, and Michael Tschannen. Finite scalar quantization: VQ-VAE made simple. In ICLR, 2024. 8 [24] Alexander Quinn Nichol and Prafulla Dhariwal. Improved In ICML, pages denoising diffusion probabilistic models. 81628171. PMLR, 2021. 8 [25] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2024. 3, 8 [26] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, pages 41954205, 2023. 6 [27] Liao Qu, Huichao Zhang, Yiheng Liu, Xu Wang, Yi Jiang, Yiming Gao, Hu Ye, Daniel Du, Zehuan Yuan, and Xinglong Wu. Tokenflow: Unified image tokenizer for multimodal understanding and generation. arXiv preprint arXiv:2412.03069, 2024. 8 [28] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. NeurIPS, 32, 2019. 8 [29] Shuhuai Ren, Shuming Ma, Xu Sun, and Furu Wei. Next block prediction: Video generation via semi-auto-regressive modeling. arXiv preprint arXiv:2502.07737, 2025. 8 [30] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 1, 2, 3, 4, 8, 11 [31] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki [48] Bowen Zhang, Shuyang Gu, Bo Zhang, Jianmin Bao, Dong Chen, Fang Wen, Yong Wang, and Baining Guo. Styleswin: Transformer-based gan for high-resolution image generation. In CVPR, pages 1130411314, 2022. 3 [49] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel Ni, and Heung-Yeung Shum. DINO: DETR with improved denoising anchor boxes for end-to-end object detection. In ICLR, 2023. 8 [50] Long Zhao, Sanghyun Woo, Ziyu Wan, Yandong Li, Han Zhang, Boqing Gong, Hartwig Adam, Xuhui Jia, and Ting Liu. ϵ-vae: Denoising as visual decoding. arXiv preprint arXiv:2410.04081, 2024. 8 [51] Chuanxia Zheng, Tung-Long Vuong, Jianfei Cai, and Dinh Phung. Movq: Modulating quantized vectors for highfidelity image generation. NeurIPS, 35:2341223425, 2022. 8 [52] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: 10 million image database for scene recognition. IEEE TPAMI, 2017. 6, 11 [53] Lei Zhu, Fangyun Wei, Yanye Lu, and Dong Chen. Scaling the codebook size of VQ-GAN to 100,000 with utilization rate of 99%. In NeurIPS, 2024. [54] Yongxin Zhu, Bocheng Li, Yifei Xin, and Linli Xu. Addressing representation collapse in vector quantized models with one linear layer. arXiv preprint arXiv:2411.02038, 2024. 8 Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. NeurIPS, 29, 2016. 4 [32] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In ICLR, 2022. 8 [33] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik Improving the pixelcnn with disKingma. Pixelcnn++: cretized logistic mixture likelihood and other modifications. arXiv preprint arXiv:1701.05517, 2017. 8 [34] Ivan Skorokhodov, Grigorii Sotnikov, and Mohamed Elhoseiny. Aligning latent and image spaces to connect the unconnectable. In ICCV, pages 1414414153, 2021. 5 [35] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In ICML, pages 2256 nonequilibrium thermodynamics. 2265. PMLR, 2015. 1, [36] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya In ICML, pages 32211 Sutskever. Consistency models. 32252. PMLR, 2023. 8 [37] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 3 [38] Yuhta Takida, Yukara Ikemiya, Takashi Shibuya, Kazuki Shimada, Woosung Choi, Chieh-Hsin Lai, Naoki Murata, Toshimitsu Uesaka, Kengo Uchida, Wei-Hsiang Liao, and Yuki Mitsufuji. HQ-VAE: Hierarchical discrete representation learning with variational bayes. Transactions on Machine Learning Research, 2024. 8 [39] Zhicong Tang, Tiankai Hang, Shuyang Gu, Dong Chen, and Baining Guo. Simplified diffusion schr odinger bridge. arXiv preprint arXiv:2403.14623, 2024. 8 [40] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. NeurIPS, 37:8483984865, 2025. 1, 2, 6, [41] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelcnn decoders. NeurIPS, 29, 2016. 8 [42] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. NIPS, 30, 2017. 8 [43] Mark Weber, Lijun Yu, Qihang Yu, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. Maskbit: Embedding-free image generation via bit tokens. Transactions on Machine Learning Research, 2024. 8 [44] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved VQGAN. In ICLR, 2022. 8 [45] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and LiangChieh Chen. Randomized autoregressive visual generation. arXiv preprint arxiv: 2411.00776, 2024. 2 [46] Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. An image is worth 32 tokens for reconstruction and generation. NeurIPS, 37:128940128966, 2024. 2, 6, [47] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. In ICLR, 2025. 3 10 value 128 [1, 1, 2, 4, 4] 2 16 256 5000 0.5 hinge loss 1.0 20000 5.0 0.01 1.0 Adam 0.5 0.9 1.92e-4 5000 30000 0. 0.9999 50 192 A100 A. Datasets ImageNet-1k ImageNet [6] is large-scale hierarchical image database that has served as cornerstone dataset for modern computer vision research since its introduction in 2009. It contains more than one million annotated images across 1,000 object categories, providing robust benchmark for numerous vision tasks, including image classification, object detection, and class-conditional image generation. Dataset website: https://image-net.org/ Places The Places [52] dataset is curated according to the principles of human visual cognition, with the aim of creating comprehensive resource to train artificial systems in high-level visual understanding tasks. Applications range from scene recognition and object detection in contextual environments, to sophisticated understanding tasks such as action recognition, event prediction, and theory-of-mind inference. The entire Places database includes more than 10 million images, covering over 400 unique scene categories. In particular, for our long-image generation experiments, we selected 30 nature categories from the Places-Challenge subset, which contains approximately 1 million images. config Base channels Base channel multiplier per stage Residual blocks per stage Attention resolutions Token channels Adversarial loss enabled at iteration Discriminator loss weight Discriminator loss Perceptual loss weight Semantic anlignment loss enabled at iteration Semantic anlignment loss weight KL divergence loss weight Gradient clipping by norm Optimizer Beta1 Beta2 Base LR LR warmup iterations LR decay frequency LR decay ratio Dataset website: http://places2.csail.mit. EMA decay edu/ B. Implementation Details B.1. Data Augmentation We perform data augmentation by initially resizing the input images so that the smaller dimension is 256 pixels. Following this, random cropping is applied to the resized images. Additionally, horizontal flipping is performed with probability of 0.5 to improve the robustness and generalization of the model. B.2. Tokenizer Training We follow the standard training recipe proposed in the Latent Diffusion Model (LDM) [30], and detailed hyperparameter configurations used for training our equivariant 1D tokenizer are provided in Tab. 6. B.3. Generative Models Training Detailed hyper-parameters utilized in our equivariant generator are summarized in Tab. 8. To comprehensively evaluate model capacity and performance trade-offs, we trained multiple variants of the generative model with different sizes and complexities. The complete architectural details for each of these model variants are provided in Tab. 7. We utilize 32 A100 GPUs for training the tokenizer, with the training process spanning 3 days. The generator models are trained using 64 A100 GPUs, requiring 4.6 days for the longest schedule (training our huge model for 1200 epochs). Training epochs Total Batchsize GPU Table 6. Detailed hyper-parameters for our equivariant 1D tokenizer. Figure 8. Additional Examples about Visual Meanings of 1D Tokens. 11 Model #Para. Layers Hidden dim Attn heads Diff. hidden dim Diff.layers Small Base Large Huge 151M 294M 644M 1.2B 16 24 32 512 768 1024 1280 8 12 16 16 960 1024 1280 1536 12 12 12 12 Table 7. The model configurations of our generators. #Para. denotes the number of parameters in the respective generators and Diff. presents the diffusion head. We also use S, B, and and as shorthand for different models in the manuscript. config Token length Token channels value 16 256 MLP ratio Norn layer in attention blocks Class labels sequence length Class labels dropout Attention dropout Projection layer dropout 4 nn.LayerNorm 16 0.1 0.1 0.1 Gradient clipping by norm Optimizer Beta1 Beta2 Base LR LR scheduler LR warmup epochs Weight decay EMA decay Training epochs Total Batchsize GPU 3.0 Adam 0.9 0.95 8.0e-4 constant 100 0.02 0.9999 1200 2048 A100 Figure 9. Visualization of the generation process. select the optimal guidance scales individually for each trained model. Table 8. Detailed hyper-parameters for our equivariant generator. C. Pseudo-Code for Our Equivariant 1D TokB.4. Sampling Hyper-parameters We generate results by sampling with 100 denoising steps, utilizing the first-order Euler solver following the Rectified Flow framework [20]. At inference time, the genertor employs classifier-free guidance (CFG). Specifically, the underlying transformer network produces two distinct outputs: the conditional output hc (conditioning context present) and the unconditional output hu (conditioning context absent). The predicted velocity is obtained through interpolation of these two outputs as follows: = vθ(xt, t, hu) + ω (vθ(xt, t, hc) vθ(xt, t, hu)), where ω represents the guidance scale parameter. Inspired by Muse [2], we employ dynamic CFG schedule in which the guidance scale ω increases linearly as the sampling sequence progresses. To maximize sampling quality, we systematically tune and enizer We have included PyTorch-style pseudo-code for our Equivariant 1D Tokenizer. D. Visualization of Our Huge Model on ImageNet We showcase the uncurated 256256 images generated by our huge model in Fig. 10. E. Visualization of Long Images We provide uncurated long-content images produced by our large model in Fig. 11, which is trained exclusively on the Nature subset of the Places dataset. Owing to the equivariant property, our model effectively captures fine-grained spatial coherence, enabling it to generate high-fidelity landscape images. Remarkably, our approach demonstrates Algorithm 1: Our Equivariant 1D Tokenizer PyTorch-style Pseudo-Code def class EquivariantTokenizer(nn.Module) init (token channels): # 2D Encoder and Decoder self.2DEncoder, self.2DDecoder = 2DEncoder(), 2DDecoder() # 1D Encoder and Decoder self.1DEncoder, self.1DDecoder = 1DEncoder(), 1DDecoder() self.token channels = token channels def forward(self, x): = self.2DEncoder(x) # Columnization = z.permute(0,3,1,2) = z.reshape(z.shape[0], z.shape[1], -1) # 1D Latent posterior = self.1DEncoder(x) latent = posterior.sample() = self.1DDecoder(latent) # Rasterization = z.reshape(z.shape[0], z.shape[1], self.token channels, -1) = z.permute(0, 2, 3, 1) return self.2DDecoder(z) this zero-shot capability, as the model was never explicitly trained on high-resolution images. F. Visualization of Tokens To clearly illustrate the relationship between encoded tokens and corresponding visual content, we progressively replace the randomly initialized token sequences with encoded tokens. As demonstrated in Fig. 2 and further substantiated in Fig. 8, the decoder faithfully reconstructs the original images step by step. Furthermore, we visualize the generation process by decoding progressively generated token sequences in Fig. 9, thereby providing further clarification regarding the semantic interpretation of tokens in the latent space from generative perspective. 13 Figure 10. Generation Results on the ImageNet-1k Dataset. 14 Figure 11. More Visual Examples of Generated Long Images."
        }
    ],
    "affiliations": [
        "Tencent Hunyuan Research",
        "University of Science and Technology of China"
    ]
}