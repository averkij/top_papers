{
    "paper_title": "Geometry-Aware Optimization for Respiratory Sound Classification: Enhancing Sensitivity with SAM-Optimized Audio Spectrogram Transformers",
    "authors": [
        "Atakan Işık",
        "Selin Vulga Işık",
        "Ahmet Feridun Işık",
        "Mahşuk Taylan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Respiratory sound classification is hindered by the limited size, high noise levels, and severe class imbalance of benchmark datasets like ICBHI 2017. While Transformer-based models offer powerful feature extraction capabilities, they are prone to overfitting and often converge to sharp minima in the loss landscape when trained on such constrained medical data. To address this, we introduce a framework that enhances the Audio Spectrogram Transformer (AST) using Sharpness-Aware Minimization (SAM). Instead of merely minimizing the training loss, our approach optimizes the geometry of the loss surface, guiding the model toward flatter minima that generalize better to unseen patients. We also implement a weighted sampling strategy to handle class imbalance effectively. Our method achieves a state-of-the-art score of 68.10% on the ICBHI 2017 dataset, outperforming existing CNN and hybrid baselines. More importantly, it reaches a sensitivity of 68.31%, a crucial improvement for reliable clinical screening. Further analysis using t-SNE and attention maps confirms that the model learns robust, discriminative features rather than memorizing background noise."
        },
        {
            "title": "Start",
            "content": "Geometry-Aware Optimization for Respiratory Sound Classification: Enhancing Sensitivity with SAM-Optimized Audio Spectrogram Transformers Atakan Işı , Selin Vulga Işı , Ahmet Feridun Işı , Mahşuk Tayla"
        },
        {
            "title": "1 Biomedical Engineering Department, Başkent University,Turkey\n1\n2 Thoracic Surgery Department, Gaziantep University, Turkey\nk\n3 Chest Diseases Department, Gaziantep University,Turkey",
            "content": "2 1 3 n"
        },
        {
            "title": "Abstract",
            "content": "Respiratory sound classification is hindered by the limited size, high noise levels, and severe class imbalance of benchmark datasets like ICBHI 2017. While Transformer-based models offer powerful feature extraction capabilities, they are prone to overfitting and often converge to sharp minima in the loss landscape when trained on such constrained medical data. To address this, we introduce framework that enhances the Audio Spectrogram Transformer (AST) using Sharpness-Aware Minimization (SAM). Instead of merely minimizing the training loss, our approach optimizes the geometry of the loss surface, guiding the model toward flatter minima that generalize better to unseen patients. We also implement weighted sampling strategy to handle class imbalance effectively. Our method achieves state-of-the-art score of 68.10% on the ICBHI 2017 dataset, outperforming existing CNN and hybrid baselines. More importantly, it reaches sensitivity of 68.31%, crucial improvement for reliable clinical screening. Further analysis using t-SNE and attention maps confirms that the model learns robust, discriminative features rather than memorizing background noise. Keywords: Lung Sound Analysis, Audio Spectrogram Transformer, SAM, Imbalanced Learning, ICBHI 2017. INTRODUCTION Respiratory diseases remain leading cause of morbidity and mortality worldwide, necessitating accurate and timely diagnosis for effective patient care. While modern imaging techniques like X-ray and CT scans provide detailed anatomical information, pulmonary auscultation retains its status as the most fundamental and accessible diagnostic method[1, 2]. In clinical practice, distinguishing conditions such as pneumonia from heart failure often relies on the subtle characteristics of breath sounds, even when radiological findings are ambiguous. the efficacy of auscultation is heavily dependent on the clinicians hearing sensitivity and However, experience[1]. Sound transmission through the thorax encounters complex biological barriers, including tracheobronchial cartilages and varying tissue densities. Consequently, factors such as muscular hypertrophy or obesity can dampen acoustic signals, making manual detection of pathological sounds specifically crackles and wheezes highly subjective and prone to inter observer variability. To standardize interpretation, Computer-Aided Diagnosis (CAD) systems using Deep Learning (DL) have emerged as powerful tool. Early approaches largely relied on Convolutional Neural Networks (CNNs), such as ResNet or VGG, trained on spectrogram representations of lung sounds[3, 4]. While CNNs are effective at extracting local features, they often struggle to capture long-range temporal dependencies that are crucial for distinguishing continuous respiratory cycles from transient artifacts. Recently, Transformer-based architectures, particularly the Audio Spectrogram Transformer (AST), have demonstrated superior performance in audio tasks by leveraging self-attention mechanisms to model global context[5]. Despite their potential, applying Transformers to respiratory sound analysis presents significant engineering challenge. Benchmark datasets, such as the ICBHI 2017 Challenge dataset, are characterized by limited sample sizes, severe class imbalance, and high levels of background noise, including stethoscope friction, heartbeats, and speech[2]. Transformers, which typically lack the inductive bias of CNNs, require large amounts of data to generalize well. When trained on such constrained medical datasets, they are prone to overfitting and tend to converge to \"sharp minima\" in the loss landscape[6]. model residing in sharp minimum may perform well on training data but fails drastically when input data varies slightly, common occurrence in clinical settings due to different recording devices or patient physiology. framework that In this study, we bridge the gap between clinical acoustic complexity and deep learning robustness. We propose (AST) with Sharpness-Aware novel Minimization (SAM)[7]. Unlike standard optimization algorithms like SGD or Adam, which solely minimize the training loss value, SAM simultaneously minimizes the loss value and the sharpness of the loss landscape. This geometry-aware approach guides the model toward \"flat minima,\" ensuring that the solution remains robust integrates the Audio Spectrogram Transformer even in the presence of noisy or perturbed inputs. By combining this optimization strategy with weighted sampling technique to handle class imbalance, our framework significantly improves diagnostic performance. The main contributions of this work are summarized as follows: We introduce robust respiratory sound classification framework that adapts the AST architecture to small and noisy medical datasets using Geometry-Aware Optimization (SAM). We demonstrate that optimizing for loss landscape flatness significantly mitigates overfitting in Transformer models, leading to new state-of-the-art (SOTA) accuracy of 68.10% on the ICBHI 2017 dataset. Our method achieves sensitivity of 68%, outperforming existing CNN and hybrid baselines[3, 8-10], which is critical for minimizing false negatives in clinical screening. We provide detailed visual analysis using t-SNE and confusion matrix."
        },
        {
            "title": "RELATED WORKS",
            "content": "The automated analysis of respiratory sounds has witnessed paradigm shift from manual feature engineering to end-to-end deep learning architectures. This section critically reviews the evolution of these methods, focusing on CNN-based baselines, advanced augmentation strategies, and the recent surge of Transformer models."
        },
        {
            "title": "Convolutional Baselines and Hybrid Models",
            "content": "Following the release of the ICBHI 2017 benchmark dataset[2], early deep learning approaches treated respiratory sound classification primarily as an image recognition task. Ma et al. [4] introduced LungBRN, biResNet architecture that processes wavelet components to capture time-frequency features. Building on this, Gairola et al. proposed RespireNet[3], fine-tuned ResNet backbone that significantly improved performance by utilizing extensive data augmentation. While CNNs are effective at extracting local spectral features, they inherently lack the ability to model longrange temporal dependencies necessary for analyzing full breath cycles. To bridge this gap, hybrid models combining CNNs with Recurrent Neural Networks (RNNs) were explored. Nguyen and Pernkopf utilized CRNN framework with co-tuning strategies to capture temporal dynamics. However[11], these recurrent architectures often suffer from high computational complexity and training instability, particularly on smallscale datasets like ICBHI. Data Augmentation and Contrastive Learning Given the scarcity of annotated medical data, recent research has pivoted towards advanced data handling techniques to mitigate overfitting. Kim et al. proposed RepAugment, an input-agnostic representation-level augmentation scheme designed to improve model robustness[12]. Similarly, Wang et al. investigated domain transfer-based augmentation to bridge the gap between different recording environments[13]. Beyond standard augmentation, contrastive learning has emerged as powerful tool for representation learning. Moummad and Farrugia applied supervised contrastive learning to respiratory sounds, demonstrating that pulling samples of the same class closer in the embedding space enhances classification accuracy[14]. Kim et al. further extended this by incorporating stethoscope-guided adaptation[15], while Chu et al. recently proposed the Cycle-Guardian framework, which integrates deep clustering with contrastive learning to better separate pathological clusters[16]. The Transformer Era in Lung Sound Analysis The advent of the Audio Spectrogram Transformer (AST) demonstrated that pure attention-based models could outperform CNNs by capturing global context across the entire spectrogram[5]. Consequently, recent studies have rapidly adopted Transformer architectures for lung sound analysis. Sun et al. applied Swin Transformers to the ICBHI dataset[17], leveraging shifted windows to model hierarchical features. To address the data-hungry nature of Transformers, Xiao et al. introduced LungAdapter, parameter-efficient fine-tuning method that adapts pre-trained models with minimal trainable parameters[18]. Most recently, Dong et al. proposed dual-branch network fusing time-domain waveforms with 2D spectral features, achieving competitive results[9]. Similarly, Bae et al. combined AST with \"Patch-Mix\" strategy to improve robustness against background noise[8]. Despite these advancements, standard Transformers trained on imbalanced medical datasets still tend to converge to sharp minima in the loss landscape, leading to suboptimal generalization on unseen patients. Geometry-Aware Optimization The concept of loss landscape geometry has gained traction in explaining the generalization gap in deep learning. Keskar et al. demonstrated that \"sharp minima\" generalize poorly compared to \"flat minima\" [6]. To exploit this, Sharpness-Aware Minimization (SAM) was introduced to simultaneously minimize loss value and loss sharpness[7]. While widely used in computer vision, the application of SAM to biomedical audio remains limited. In this work, we differentiate ourselves from previous Transformer-based approaches [8-10, 17] by explicitly optimizing the loss geometry, thereby achieving superior sensitivity and robustness without requiring complex multi-modal fusion or extensive auxiliary data."
        },
        {
            "title": "PROPOSED METHODS",
            "content": "In this study, we propose geometry-aware deep learning framework designed to maximize sensitivity in respiratory sound classification. Our pipeline integrates signal-preserving preprocessing strategy, Transfer Learning-based Audio Spectrogram Transformer (AST) backbone, and Sharpness-Aware Minimization (SAM) optimizer. The general architecture is illustrated in Figure 1. Figure 1. General Architecture of Proposed Model Dataset and Signal Preprocessing We utilized the ICBHI 2017 Challenge dataset [2], which serves as the gold standard for respiratory sound classification. The dataset comprises 920 annotated audio recordings from 126 subjects. major challenge in this dataset is the variable duration of respiratory cycles. Standard approaches often employ zero-padding to match the fixed input length required by deep learning models. However, zero-padding introduces large segments of artificial silence, which dilutes the diagnostic features. To address this, we employed cyclic padding strategy. For target input duration of 8 seconds, signals shorter than this length were repeated until the target was reached. This ensures that the model always processes signal dense input, maximizing the extraction of pathological patterns (crackles and wheezes) without being misled by silence artifacts. Audio Spectrogram Transformer (AST) We adopted the Audio Spectrogram Transformer (AST) [5], state-of-the-art architecture that adapts the Vision Transformer (ViT) for audio analysis. Unlike traditional Convolutional Neural Networks (CNNs) that focus on local features, AST processes the entire audio spectrogram simultaneously, allowing it to capture global temporal dependencies across the breathing cycle. Mechanism: The raw audio is first converted into log-Mel spectrogram. This visual representation of sound is then split into sequence of small patches (1616). The model utilizes self-attention mechanism to weigh the importance of different patches, enabling it to focus on clinically relevant events (e.g., short crackle sound) regardless of their position in the recording. Transfer Learning from AudioSet: Training complex models on small medical datasets often leads to overfitting. To mitigate this, we initialized our model with weights pre-trained on AudioSet, massive dataset containing over 2 million audio clips. This transfer learning step provides the model with fundamental \"understanding\" of acoustic structures such as pitch and rhythm before it is fine-tuned on lung sounds, significantly improving its diagnostic sensitivity. Geometry-Aware Optimization (SAM) Standard deep learning optimizers, such as AdamW or SGD, aim to minimize the empirical training loss with respect to the model parameters w. While effective for general tasks, in the context of respiratory sound classification where data is scarce and labels are noisy this approach often leads the model to converge to \"sharp minima\" [6]. solution residing in sharp minimum is highly sensitive to input perturbations; slight variations in stethoscope pressure or background noise can cause the loss to spike, resulting in misclassification of pathological sounds on unseen patients. SAM formulates the optimization as min-max problem, seeking to minimize the maximum loss within perturbation radius ρ: min () ℎ () = max 2 ( + ) (1) Here, ρ is hyperparameter defining the size of the neighborhood. The optimization is performed in two-step procedure during each training iteration: First, we compute the gradient of the loss with respect to the weights to determine the direction of the sharpest ascent. We then calculate perturbation vector that maximizes the loss: () ()2 The model weights are then updated based on the gradient computed at this perturbed state ( steering the optimization trajectory towards flatter regions of the loss landscape. = (2) ), effectively ω + ϵ The primary advantage of SAM in this study is its contribution to robust feature learning. In standard training, the model might memorize specific background artifacts (e.g., friction noise) associated with \"Crackle\" samples in the training set. By optimizing for flatness, SAM forces the model to ignore these transient artifacts and focus on the invariant spectral characteristics of the pathology. This results in decision boundary that is less brittle, directly reducing False Negatives and boosting the Sensitivity to 68%, significantly outperforming baselines that rely on standard optimization. EVALUATION METRICS To ensure fair comparison with existing benchmarks, we strictly adhered to the official evaluation protocol established by the ICBHI 2017 Challenge . Although the dataset contains four distinct classes (Normal, Crackle, Wheeze, and Both), the challenge evaluates performance based on binary classification task: Normal vs. Abnormal (Adventitious).According to this protocol, the \"Abnormal\" category encompasses all pathological sounds: Crackle, Wheeze, and Both. Consequently, prediction is considered correct (True Positive) if sample annotated as any abnormal class is classified into any of the three abnormal categories.The performance is measured using three key metrics: Sensitivity (Se), Specificity (Sp), and the average Score (Score). These are defined as follows: () = () = {, , } , , , , (3) (4) (5) = +"
        },
        {
            "title": "EXPERIMENTS AND RESULTS",
            "content": "All experiments were conducted on workstation equipped with single NVIDIA Tesla L4 GPU via Google Colab using the PyTorch framework. To ensure reproducibility, all random seeds were fixed. The input audio recordings were resampled to 16 kHz and processed using the cyclic padding strategy described in Section 3.1 to fixed duration of 8 seconds. The AST model was initialized with weights pre-trained on AudioSet. We utilized the AdamW optimizer with learning rate of 1 . The model was trained for 20 epochs with batch size of 8. For the SAM optimizer, the neighborhood size parameter was empirically set to ρ=0.05. and weight decay of 1 10 10 4 Consistent with the official ICBHI 2017 Challenge protocol [2] and recent state of the art studies, we evaluated performance using Sensitivity (Se), Specificity (Sp), and the average Score (Score=(Se+Sp)/2). Crucially, following the challenge guidelines, the evaluation is performed as binary classification task (Normal vs. Abnormal). The \"Abnormal\" class encompasses Crackles, Wheezes, and Both. Therefore, prediction is considered correct if an adventitious sound (e.g., Crackle) is classified into any of the pathological categories (Crackle, Wheeze, or Both). As shown in Table 1, our method significantly outperforms CNN-based baselines (RespireNet), confirming the superiority of Transformers in capturing global temporal dependencies. More importantly, we surpass the recent model proposed by Dong et al. [19]. Notably, our model achieves the highest Sensitivity ( 68%) among the compared methods. High sensitivity is the most critical metric for medical screening tool, as it minimizes the risk of missing patients with respiratory pathologies (False Negatives). Table 1. Comparison of our method with other state of the art models with ICBHI dataset. Model Specificity (%) Sensitivity (%) Score (%) AFT on Mixed-500[20] 80.72 AST Fine-tuning[8] Patch-Mix CL[8] M2D[21] DAT[15] SG-SCL[15] RepAugment[12] BTS[10] MVST[22] LungAdapter[18] CycleGuardian[16] ADD-AST[19] 77.14 81.66 81.51 77.11 79. 82.47 81.40 80.60 80.43 82.06 85. 42.86 41.97 43.07 45.08 42.50 43. 40.55 45.67 44.39 44.37 44.47 45. 61.79 59.55 62.37 63.29 59.81 61. 61.51 63.54 62.50 62.40 63.26 65."
        },
        {
            "title": "Model",
            "content": "Specificity (%) Sensitivity (%) Score (%) Proposed (AST + SAM) 67.89 68.31 68."
        },
        {
            "title": "ABLATION STUDY",
            "content": "To validate the contribution of each component in our framework, we conducted an ablation study. We started with baseline AST model and incrementally added the Weighted Random Sampler (WRS) and the SAM optimizer. Baseline (AST only): Trained with standard Cross-Entropy loss and AdamW optimizer. This model suffered from the class imbalance, yielding high Specificity but low Sensitivity. AST + WRS: Introducing Weighted Random Sampling balanced the class distribution during training, significantly boosting Sensitivity but slightly increasing the variance in the validation loss. AST + WRS + SAM (Proposed): The addition of SAM stabilized the training. By seeking flat minima, SAM acted as robust regularizer, further improving both the Sensitivity and the overall Score to the 67.81%. This analysis confirms that geometry-aware optimization is not merely an enhancement but fundamental requirement for training large Transformers on small, noisy datasets like ICBHI. Table 2 shows all metrics for running experiments. Configuration 1. Baseline AST 2. + Weighted Sampling 3. + SAM Optimizer (Proposed) Table 2. Ablation Study Sensitivity (%) Specificity (%) Score (%) 66.00 63.00 67. 70.00 71.00 68.31 67.64 66.91 68. VISUAL ANALYSIS AND EXPLAINABILITY To further validate the robustness of our proposed framework, we conducted visual analysis of the model's decision-making process and feature representations. Figure 2. Confusion Matrix of 4-Class Classification As observed, the model demonstrates strong ability to correctly identify pathological cases, achieving Sensitivity of 68%. The majority of misclassifications occur amongst the adventitious classes themselves . According to the ICBHI protocol, these intra-class errors do not penalize the final score. Most importantly, the rate of False Negatives (Abnormal samples misclassified as Normal) is minimized compared to the baseline. This confirms that our geometry-aware optimization effectively prevents the model from overlooking disease signatures, which is the primary objective of computer-aided diagnosis system. To understand how the model separates different respiratory sounds in the high-dimensional feature space, we applied t-Distributed Stochastic Neighbor Embedding (t-SNE) on the embeddings extracted from the final layer of the AST encoder. Figure 3 displays the 2D projection of the test samples. Despite the high noise levels and recording variability in the ICBHI dataset, the plot reveals distinct clusters for the Normal and Abnormal classes. This clear separation indicates that the SAM optimizer successfully pushed the model to learn discriminative, high-level acoustic features that are robust to patient-specific variations. The observed overlap between Crackle, Wheeze, and Both classes is clinically expected, as these pathologies often co-occur and share similar spectral characteristics. Figure 3. t-SNE visualization of the learned embeddings. DISCUSSION In this study, we presented geometry aware deep learning framework for respiratory sound classification, achieving state-of-the-art Score of 68.10% on the ICBHI 2017 benchmark. Beyond the numerical improvements, our findings offer critical insights into the challenges of training Transformers on small, noisy medical datasets. key finding of our research is the pivotal role of the loss landscape geometry. Standard training with AdamW on limited data typically drives the model into \"sharp minima,\" where the model memorizes training samples including their background noise but fails to generalize to unseen patients. By integrating Sharpness-Aware Minimization (SAM), we explicitly forced the model to seek \"flat minima.\" This aligns with the theoretical insights of Keskar et al. [6], but we demonstrate its practical utility in biomedical audio. As evidenced in our ablation study, SAM was the primary driver for boosting Sensitivity from 63% to 71%. This suggests that robustness against noise should be handled via optimization constraints rather than aggressive preprocessing. Recent SOTA approaches, such as Dong et al.[19] and RespireNet [3], heavily rely on complex noise suppression modules or dual-branch architectures to handle artifacts. While effective for improving Specificity, these methods often inadvertently filter out subtle pathological cues (e.g., faint wheezes), leading to lower Sensitivity (40-45%). In contrast, our \"Signal Preserving\" strategy utilizing Cyclic Padding and raw Log-Mel features retains the complete acoustic information. Instead of removing noise manually, we allow the AST backbone, pre-trained on AudioSet, to learn to distinguish between noise and pathology dynamically. Our results confirm that preserving signal integrity yields superior diagnostic power compared to aggressive denoising pipelines. distinct characteristic of our model is its performance profile: it achieves high Sensitivity ( 68%) at the cost of moderate Specificity (67.89%). In the context of Clinical Screening, this trade-off is not only acceptable but desirable. The primary goal of Computer-Aided Diagnosis (CAD) system is to act as \"safety net\" for clinicians. False Negative (missing sick patient) carries severe health risks, potentially delaying life-saving treatment. False Positive , while resource consuming, is medically less dangerous. By prioritizing Sensitivity through our Weighted Sampling and SAM strategies, our framework aligns better with real world clinical requirements than previous models that maximize Specificity but miss nearly half of the pathological cases. Despite the promising results, our study has limitations. The Specificity of 61% indicates higher rate of False Positives, likely due to the model confusing high-frequency noise with wheezes. Future work could explore hybrid loss that penalize False Positives without compromising Sensitivity. Additionally, incorporating Self Supervised Learning (SSL) on larger, unlabelled respiratory datasets could further improve the model's feature extraction capabilities, mitigating the reliance on the small ICBHI dataset. functions"
        },
        {
            "title": "CONCLUSION",
            "content": "In this paper, we presented robust, geometry-aware deep learning framework for respiratory sound classification, effectively bridging the gap between advanced Transformer architectures and the constraints of small, noisy medical datasets. By synergizing the Audio Spectrogram Transformer (AST) with SharpnessAware Minimization (SAM) and signal-preserving cyclic padding strategy, we addressed the critical issues of overfitting and \"sharp minima\" convergence. Our extensive experiments on the ICBHI 2017 benchmark demonstrate that the proposed method achieves new state-of-the-art Score of 68.10%, surpassing recent complex hybrid models. Most significantly, we achieved Sensitivity of 68%, marking substantial improvement over existing baselines. This high sensitivity confirms that our geometry aware optimization compels the model to learn invariant pathological features rather than memorizing background artifacts. Ultimately, this study proves that prioritizing loss landscape flatness and signal integrity yields superior diagnostic performance compared to aggressive denoising or complex multi-modal fusion. The proposed framework offers promising direction for the development of reliable, high-sensitivity Computer-Aided Diagnosis (CAD) systems suitable for real-world clinical screening. Availability Note:Code Availability To promote reproducibility and further research in this domain, the complete source code, including the signal-preserving preprocessing pipeline, AST model architecture, and SAM optimization scripts, is publicly available at:https://github.com/Atakanisik/ICBHI-AST-SAM ACKNOWLEDGEMENTS Declaration of Generative AI and AI-assisted technologies in the wr iting process During the preparation of this work, the authors used Google Gemini in order to improve the readability and language quality of the manuscript. After using this tool/service, the authors reviewed and edited the content as needed and take full responsibility for the content of the publication. REFERENCES [1] [2] [3] [4] [5] R. X. A. Pramono, S. Bowyer, and E. Rodriguez-Villegas, \"Automatic adventitious respiratory sound analysis: systematic review,\" PloS one, vol. 12, no. 5, p. e0177926, 2017. D. F. Bruno Rocha , Luís Mendes, Gorkem Serbes, Sezer Ulukaya, Yasemin Kahya, Nikša Jakovljevic, Tatjana Turukalo, Ioannis Vogiatzis, Eleni Perantoni, Evangelos Kaimakamis, Pantelis Natsiavas, Ana Oliveira, Cristina Jácome, Alda Marques, Nicos Maglaveras, Rui Pedro Paiva, Ioanna Chouvarda, Paulo de Carvalho, \"An open access database for the evaluation of respiratory sound classification algorithms,\" Physiol Meas., vol. 40, no. 3, 2019. S. Gairola, F. Tom, N. Kwatra, and M. Jain, \"RespireNet: deep neural network for accurately International detecting abnormal Conference of the IEEE Engineering in Medicine & Biology Society (EMBC), 2021, pp. 527-530. Y. Ma et al., \"Lungbrn: smart digital stethoscope for detecting respiratory disease using bi-resnet deep learning algorithm,\" in 2019 IEEE Biomedical Circuits and Systems Conference (BioCAS), 2019, pp. 1-4. Y. Gong, Y.-A. Chung, and J. Glass, \"AST: Audio Spectrogram Transformer,\" in Proc. Interspeech, 2021, pp. 571-575. lung sounds in limited data setting,\" in 2021 43rd Annual [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] [17] [18] [19] [20] [21] [22] for and"
        },
        {
            "title": "Deep",
            "content": "2016, Sharp Minima,\" Learning: Generalization Gap D. M. Nitish Shirish Keskar, Jorge Nocedal, Mikhail Smelyanskiy, Ping Tak Peter Tang, \"On Large-Batch Training doi: 10.48550/arXiv.1609.04836. P. Foret, A. Kleiner, H. Mobahi, and B. Neyshabur, \"Sharpness-aware minimization for efficiently improving generalization,\" in International Conference on Learning Representations (ICLR), 2021. S. Bae et al., \"Patch-Mix Contrastive Learning with Audio Spectrogram Transformer on Respiratory Sound Classification,\" arXiv preprint arXiv:2305.14032, 2023. G. Dong, Y. Shen, J. Wang, M. Zhang, P. Sun, and M. Zhang, \"Respiratory sounds classification by fusing the time-domain and 2d spectral features,\" Biomedical Signal Processing and Control, vol. 107, p. 107790, 2025. J. W. Kim, M. Toikkanen, Y. Choi, S. E. Moon, and H. Y. Jung, \"Bts: Bridging text and sound modalities for metadata-aided respiratory sound classification,\" in Interspeech 2024, 2024, pp. 1690-1694. T. Nguyen and F. Pernkopf, \"Lung sound classification using co-tuning and stochastic normalization,\" IEEE Transactions on Biomedical Engineering, vol. 69, no. 9, pp. 2872-2882, 2022. J. W. Kim, M. Toikkanen, S. Bae, M. Kim, and H. Y. Jung, \"RepAugment: Input-agnostic representationlevel augmentation for respiratory sound classification,\" arXiv preprint arXiv:2405.02996, 2024. Z. Wang and Z. Wang, \"A domain transfer based data augmentation method for automated respiratory classification,\" in ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2022, pp. 9017-9021. I. Moummad and N. Farrugia, \"Supervised contrastive learning for respiratory sound classification,\" arXiv preprint arXiv:2210.16192, 2022. J. W. Kim, S. Bae, W. Y. Cho, B. Lee, and H. Y. Jung, \"Stethoscope-guided Supervised Contrastive Learning for Cross-domain Adaptation on Respiratory Sound Classification,\" arXiv preprint arXiv:2312.09603, 2023. Y. Chu, Q. Wang, E. Zhou, L. Fu, Q. Liu, and G. Zheng, \"Cycle-guardian: framework for automatic respiratory sound classification based on improved deep clustering and contrastive learning,\" arXiv preprint arXiv:2502.00734, 2025. W. Sun, F. Zhang, P. Sun, Q. Hu, J. Wang, and M. Zhang, \"Respiratory Sound Classification Based on Swin Transformer,\" in 2023 8th International Conference on Signal and Image Processing (ICSIP), 2023, pp. 511-515. L. Xiao, L. Fang, Y. Yang, and W. Tu, \"LungAdapter: Efficient Adapting Audio Spectrogram Transformer for Lung Sound Classification,\" in Proc. Interspeech 2024, 2024, pp. 4738-4 682. Z. Z. Gaoyang Dong, Ping Sun, Minghui Zhang, \"Adaptive Differential Denoising for Respiratory Sounds Classification,\" presented at the Interspeech, 2025. J. W. Kim, C. Yoon, M. Toikkanen, S. Bae, and H. Y. Jung, \"Adversarial Fine-tuning using Generated Respiratory Sound to address class imbalance,\" arXiv preprint arXiv:2311.06480, 2023. D. Niizumi, D. Takeuchi, Y. Ohishi, N. Harada, and K. Kashino, \"Masked modeling duo: Towards universal audio pre-training framework,\" IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2024. W. He, Y. Yan, J. Ren, R. Bai, and X. Jiang, \"Multi-View Spectrogram Transformer for Respiratory Sound Classification,\" in ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2024, pp. 8626-8630."
        }
    ],
    "affiliations": [
        "nRespiratory"
    ]
}