{
    "paper_title": "CALM Before the STORM: Unlocking Native Reasoning for Optimization Modeling",
    "authors": [
        "Zhengyang Tang",
        "Zihan Ye",
        "Chenyu Huang",
        "Xuhan Huang",
        "Chengpeng Li",
        "Sihang Li",
        "Guanhua Chen",
        "Ming Yan",
        "Zizhuo Wang",
        "Hongyuan Zha",
        "Dayiheng Liu",
        "Benyou Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Reasoning Models (LRMs) have demonstrated strong capabilities in complex multi-step reasoning, opening new opportunities for automating optimization modeling. However, existing domain adaptation methods, originally designed for earlier instruction-tuned models, often fail to exploit the advanced reasoning patterns of modern LRMs -- In particular, we show that direct fine-tuning on traditional \\textit{non-reflective} datasets leads to limited gains. To fully leverage LRMs' inherent reasoning abilities, we propose \\textbf{CALM} (\\textit{Corrective Adaptation with Lightweight Modification}), a framework that progressively refines LRMs within their native reasoning modes for optimization modeling tasks. In CALM, an expert intervener identifies reasoning flaws and provides concise corrective hints, which the LRM incorporates to produce improved reasoning trajectories. These interventions modify fewer than 2.6\\% of generated tokens, but generate high-quality data for soft adaptation through supervised fine-tuning. The adapted model is then further improved through reinforcement learning. Building on CALM, we develop \\textbf{STORM} (\\textit{Smart Thinking Optimization Reasoning Model}), a 4B-parameter LRM that achieves a new state-of-the-art average accuracy of 68.9\\% across five popular optimization modeling benchmarks, matching the performance of a 671B LRM. These results demonstrate that dynamic, hint-based data synthesis both preserves and amplifies the native reasoning patterns of modern LRMs, offering a more effective and scalable path towards expert-level performance on challenging optimization modeling tasks."
        },
        {
            "title": "Start",
            "content": "Preprint. CALM < BEFORE THE STORM : UNLOCKING NATIVE REASONING FOR OPTIMIZATION MODELING Zhengyang Tang1,5, Zihan Ye1, Chenyu Huang2, Xuhan Huang1, Chengpeng Li5, Sihang Li5, Guanhua Chen3, Ming Yan1, Zizhuo Wang1, Hongyuan Zha1, Dayiheng Liu5, and Benyou Wang1,4 1The Chinese University of Hong Kong, Shenzhen 2Shanghai University of Finance and Economics 3Southern University of Science and Technology 4Shenzhen Loop Area Institute (SLAI) 5Qwen Team, Alibaba Inc. ABSTRACT Large Reasoning Models (LRMs) have demonstrated strong capabilities in complex multi-step reasoning, opening new opportunities for automating optimization modeling. However, existing domain adaptation methods, originally designed for earlier instruction-tuned models, often fail to exploit the advanced reasoning patterns of modern LRMs In particular, we show that direct fine-tuning on traditional non-reflective datasets leads to limited gains. To fully leverage LRMs inherent reasoning abilities, we propose CALM (Corrective Adaptation with Lightweight Modification), framework that progressively refines LRMs within their native reasoning modes for optimization modeling tasks. In CALM, an expert intervener identifies reasoning flaws and provides concise corrective hints, which the LRM incorporates to produce improved reasoning trajectories. These interventions modify fewer than 2.6% of generated tokens, but generate high-quality data for soft adaptation through supervised fine-tuning. The adapted model is then further improved through reinforcement learning. Building on CALM, we develop STORM (Smart Thinking Optimization Reasoning Model), 4B-parameter LRM that achieves new state-of-the-art average accuracy of 68.9% across five popular optimization modeling benchmarks, matching the performance of 671B LRM. These results demonstrate that dynamic, hint-based data synthesis both preserves and amplifies the native reasoning patterns of modern LRMs, offering more effective and scalable path towards expert-level performance on challenging optimization modeling tasks. 5 2 0 O 5 ] . [ 1 4 0 2 4 0 . 0 1 5 2 : r Figure 1: Performance vs. Model Size landscape for optimization modeling. : Equal contribution. : Corresponding authors. Preprint."
        },
        {
            "title": "INTRODUCTION",
            "content": "Operations Research (OR) and optimization modeling techniques are central to decision-making in areas such as inventory management and airline crew scheduling (Silver, 1981; Vance et al., 1997). Yet, despite their importance, the translation of real-world problems into mathematical models has long been bottleneck, as it requires substantial human expertise (Huang et al., 2025). In this context, Large Language Models (LLMs) introduce promising path toward automation. With the advent of instruction-tuned models, early works such as ORLM (Huang et al., 2025), LLMOPT (Jiang et al., 2024), and Solver-Informed RL (Chen et al., 2025) made notable progress. These methods establish prevailing paradigm: constructing non-reflective datasets and training LLMs for direct generation of an optimization model and its solver code from problem description (see Figure 2a for an example). Here, we refer to non-reflective dataset as pre-collected set of static problemsolution pairs without intermediate reasoning or feedback. However, the emergence of Large Reasoning Models (LRMs) represents new paradigm in the field. Unlike LLMs, LRMs possess an inherent capacity for multi-turn reasoning, which we call their native reasoning patterns. This capability allows iterative and adaptive reasoning within single inference pass (Qwen Team, 2025; DeepSeek-AI, 2025), offering greater flexibility than traditional non-reflective generation. Although existing methods can still be applied to LRMs (Huang et al., 2025; Jiang et al., 2024), it exhibits some misalignments. On the one hand, they neglect the native reasoning patterns of these models, imposing artificial reasoning modes instead. On the other hand, their data synthesis strategies remain non-reflective, which conflicts with the dynamic reasoning loops that characterize LRMs. As we empirically demonstrate in Section 2, these misalignments may provide only marginal improvements and fail to fully exploit the potential of LRMs. These observations naturally lead to central research question: Can we leverage the native reasoning of LRMs to solve optimization modeling tasks effectively? Answering this question is essential for advancing the application of LRMs, especially as high-performance open-source variants become increasingly available. To address this question, we design an evaluation protocol to systematically examine the flaws in native reasoning patterns for optimization modeling tasks. The evaluation reveals seven recurring flaws types, which we categorize into two groups: (1) Code Utilization Distrust and (2) Lack of OR Expertise. While the latter has been discussed in prior work (Huang et al., 2025; Jiang et al., 2024), the former remains largely overlooked in research on automated optimization modeling. These flaws provide natural entry point for method design. In response, we introduce CALM (Corrective Adaptation with Lightweight Modification), framework that uses lightweight intervention to adapt LRM reasoning trajectories, aligning their native reasoning patterns with the requirements of optimization modeling tasks. Two features make this framework particularly effective. First, inspired by Li et al. (2025a), we allow the LRM to access solvers code compiler, providing immediate execution feedback and thereby strengthening reflective reasoning an ability absent in typical LRMs and earlier approaches. Second, the interventions are deliberately lightweight, accounting for fewer than 2.6% of the total tokens. The expert-level trajectories generated by CALM support two-stage training pipeline: supervised fine-tuning for soft adaptation of reasoning habits, followed by reinforcement learning to refine these skills and achieve autonomous mastery. The final model is denoted as STORM (Smart Thinking Optimization Reasoning Model). Our contributions are as follows: We provide empirical evidence on the limitations of adapting modern LRMs via fine-tuning on non-reflective datasets, highlighting the importance of preserving their native reasoning patterns. We propose CALM, lightweight and scalable framework that leverages solver code execution to correct and strengthen LRM reasoning trajectories, aligning it with the demands of optimization modeling tasks. 2 Preprint. Our final model, STORM, with 4B parameters, establishes new state-of-the-art average accuracy across five optimization modeling benchmarks, matching the performance of 671B LRM. Our controlled analysis of reinforcement learning reveals that CALM-based adaptation is crucial for success. The adapted model learns faster and reaches higher performance ceiling, driven by shift to computation-driven reasoning pattern that enables it to more effectively build and refine expert-level optimization modeling skills. We situate our work within the broader literature and provide discussion of related work in Appendix A."
        },
        {
            "title": "2.1 BACKGROUND: LLMS FOR OPTIMIZATION MODELING",
            "content": "Automated optimization modeling is the task of translating natural language problem description into mathematical model and executable solver code (see Figure 2a). For evaluation, the solver computes candidate solution, which is deemed correct if its objective value lies within predefined relative error of the ground truth. Performance is assessed on benchmarks that span range of difficulty, from easy problems in NL4Opt to complex industrial cases in IndustryOR. detailed overview of these benchmarks is provided in Appendix E.1. As shown in Figure 2b, this task can be approached through two mainstream paradigms that differ fundamentally in how the final solution is obtained. (a) An optimization modeling example. Full details in Appendix B. (b) Comparison of reasoning paradigms in automated optimization modeling. Figure 2: Illustrations of optimization modeling and reasoning paradigms. Non-reflective Generation. Early methods, particularly those based on traditional LLMs, approach optimization modeling as non-reflective generation problem (Huang et al., 2025; Jiang et al., 2024). As shown in Figure 2b (top), the LLM receives problem description and generates complete solution in single step, including both the mathematical model and the solver code. The reasoning process is linear, with no opportunity for feedback or revision based on solver execution results. Reflective Generation. The advent of modern LRMs (Qwen Team, 2025; DeepSeek-AI, 2025) has introduced new paradigm. These models exhibit range of sophisticated reasoning patterns, with reflective generation the capacity for iterative self-correction and refinement emerging as dominant mode (Jaech et al., 2024). We thus treat this as the primary reasoning pattern for LRMs in our study, as it is well-suited for optimization modeling, which often requires numerical feedback and mirrors the trial-and-error process of human experts. Accordingly, we design reasoning workflow that integrates solver feedback into this reflective process, as shown in Figure 2b (bottom). In this paradigm, LRMs behave more like human experts operating in an interactive environment. They can propose hypotheses, generate code, execute it, observe outputs, and refine their reasoning accordingly. 2.2 PILOT STUDY: ADAPTING LRMS WITH NON-REFLECTIVE DATA Given the availability of open-source LRMs and well-established non-reflective datasets from prior work (Huang et al., 2025; Lu et al., 2025), natural first step is to test the most direct adaptation strategy: fine-tuning an LRM on these existing datasets. This pilot study provides necessary baseline and examines whether such training improves performance across tasks of varying difficulty. 3 Preprint. Table 1: Performance of base LRM before and after SFT on the existing dataset. Model NL4OPT MAMO Easy MAMO Complex IndustryOR OptMath Base LRM + SFT on Non-reflective Data Absolute Change 85.8 92.9 +7.1 73.8 88.7 +14.9 46.5 40.5 -6. 46.2 27.5 -18.7 33.1 6.6 -26.5 Macro AVG 57.1 51. -5.9 The results of our pilot study in Table 1 show clear trade-off. The LRM achieves higher accuracy on easier tasks such as MAMO-Easy, but its performance declines sharply on more complex benchmarks like IndustryOR and OptMath. The full experimental setup is described in Appendix E.2. plausible explanation is that existing datasets contain only problemsolution pairs, which push the LRM to replace its native multi-step reasoning with rigid, non-reflective generation style it is not optimized for. This shift improves simple cases but undermines the models reasoning ability on complex tasks, pattern also reported in other domains (Zhang et al., 2025). This observation highlights the central motivation of our work: To unlock an LRMs full potential, adaptation must preserve its native reasoning patterns. 2.3 TAXONOMY OF FLAWS IN LRMS NATIVE REASONING Our pilot study confirms that preserving the LRMs native reasoning is essential. This finding, however, raises further question: are these native patterns sufficient for expert-level performance or require targeted enhancement? To address this, we first need to systematically examine the inherent weaknesses of an unguided LRM in optimization modeling tasks. Establishing Protocol for Flaw Identification. To perform rigorous analysis, we establish systematic protocol. We first prompted base LRM to generate solutions for diverse set of problems. team of human experts with backgrounds in OR then analyzed these responses to identify recurring error patterns. Through collaborative, multi-stage process of annotation, clustering, and refinement, the team converged on set of seven distinct flaw types, which form the basis of our taxonomy. The complete, detailed protocol for this human-in-the-loop analysis is provided in Appendix C. Figure 3: Trigger Categorization and Distribution. The left (1) shows the macro-average frequency of each trigger, the first 6 triggers grouped into two primary categories. The right (2a and 2b) detail the frequency distribution of these two main categories across the evaluated benchmarks. Two-Category Taxonomy of Flaws. Our analysis of the 7 identified flaw types reveals that 6 are major reasoning flaws, representing fundamental challenges in the modeling process. The seventh, minor procedural error, is detailed in Appendix D. Our taxonomy focuses on the 6 major flaws, which we group into two high-level conceptual categories: Code Utilization Distrust: This category encompasses flaws where the LRM fails to properly leverage the computational solver, such as attempting manual calculations or writing 4 Preprint. fragmented code (Triggers 1-3). This indicates an inefficient reasoning strategy and an under-reliance on powerful external tools. Lack of OR Expertise: This category covers fundamental errors in modeling and logic, including flawed mathematical formulations, missed constraints, and implementation errors (Triggers 4-6). These flaws stem from insufficient domain-specific knowledge. This two-level taxonomy provides structured framework for understanding and addressing LRM failures. Quantifying Flaw Distribution across Benchmarks. With this taxonomy in place, we quantify the prevalence of these flaws at scale using an expert-level LLM as consistent, automated annotator. Details on this quantification process can be found in Appendix J. The distribution, shown in Figure 3 (2a) and (2b), reveals critical insight: the primary bottleneck varies with problem difficulty. On easy-to-medium tasks like NL4Opt and MAMO-Easy, flaws are dominated by Code Utilization Distrust. In contrast, on complex benchmarks like OptMath, Lack of OR Expertise becomes the main barrier. This reveals the core challenge for effective adaptation: LRM reasoning must be enhanced to overcome the bottlenecks of inefficient code use and lack of OR expertise."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "3.1 PRELIMINARIES: FORMALIZING THE REFLECTIVE GENERATION FLOW We formalize the LRMs problem-solving process as sequential interaction within code interpreter environment E. Given problem , the LRMreferred to as the Reasonergenerates an iterative Reasoning Flow, represented as τ (T ) = (s0, a0, o0, s1, a1, o1 . . . , sT , aT , oT ), (1) where st, at are the textual reasoning and code block at step t, respectively. The sequential reasoning flow follows these steps: (st, at) = πθ(τ (t1)), ot = E(at), τ (t) = τ (t1) st at ot. (2) The objective is to refine πθ to produce trajectories that ultimately yield correct solutions. 3.2 CALM: CORRECTING ADAPTATION WITH LIGHTWEIGHT MODIFICATION At the heart of our approach is the CALM framework, dynamic data curation method based on ReasonerIntervener collaboration pattern for generating expert-aligned reasoning flows. Targeted Hints for Specific Flaws. CALMs strength lies in its one-to-one mapping between reasoning flaws and tailored hints injected by the Intervener (see Appendix D). These interventions address two primary issues: For Code Utilization Distrust: When the Reasoner attempts manual solving, the Intervener injects hint to redirect it toward using the solver, such as: Wait, maybe can use the pulp library and let the solver find the optimal solution. For Lack of OR Expertise: When key concepts like integer constraints are missed, the Intervener provides concise domain-specific guidance, such as: fractional number of cars isnt practical, suggesting missed integer constraint. The Iterative Hinting Loop. CALM implements an iterative refinement loop that transforms flawed reasoning trajectories into expert-aligned ones. Let τ (i) denote the reasoning flow at iteration i. The process proceeds as follows: Initial Generation (i = 0): The Reasoner generates an initial trajectory τ (0) for given problem . Intervention & Evaluation: The Intervener examines τ (i). If no deviation is found, the process terminates with the final trajectory τ = τ (i). Otherwise, the Intervener identifies the flawed step and corresponding action at, and generates corrective hint hi. Localized Revision & Resumption: modified state is formed by appending the hint hi to the context at step t. From this new state, the Reasoner continues its reasoning process to form corrected trajectory τ (i+1). 5 Preprint. Figure 4: representative example of Lack of OR Expertise flaw. (1) The models native reasoning results in an incorrect problem formulation, leading to wrong answer. (2) In contrast, the process under CLAMs guidance correct the formulation, enabling the model to find the correct solution. This loop continues until the Intervener deems the reasoning trajectory to be complete and free of flaws. As practical safeguard to prevent unproductive or infinite correction cycles, we limit the maximum number of interventions. Each intervention is localized and minimally invasive, preserving the models native reasoning (see Appendix for more examples of specific, single-step interventions, and Appendix for complete, multi-turn case study). Filtering of Expert Trajectories. To ensure supervision quality, we construct our SFT dataset, DCALM , by filtering for \"golden\" trajectories (Figure 6). We retain only those that are both correct in their final answer and assessed as having flawless reasoning flow by the Intervener. 3.3 TRAINING PIPELINE: FROM SOFT ADAPTATION TO AUTONOMOUS MASTERY The trajectories curated and filtered by CALM are used in two-stage training pipeline. Stage 1: Supervised Fine-Tuning for Soft Adaptation. We fine-tune the base LRM on DCALM , using standard cross-entropy loss. The goal of this stage is not to enhance the final performance score, but to perform soft adaptation of the policy πθ. By training on trajectories that align with the models native reasoning, this approach guides its problem-solving habits without constraining it into rigid, non-reflective pattern. Stage 2: Reinforcement Learning for Autonomous Mastery. Following supervised fine-tuning, we apply reinforcement learning to enable the model to independently optimize for correctness. We use the Group Relative Policy Optimization (GRPO) algorithm (Shao et al., 2024), allowing interaction with the Code Interpreter for up to = 4 code executions per rollout. The RL stage aims to maximize the expected reward: J(θ) = Eτ πθ(P )[R(τ )]. Our reward function is simple binary signal based on the final outcome: R(τ ) = (cid:40) 1 if (cid:12) (cid:12) (cid:12) 0 otherwise. Ans(τ )Ans Ans (cid:12) (cid:12) (cid:12) ϵ, (3) where Ans(τ ) is the final answer extracted from trajectory τ , Ans is the ground-truth solution. We also apply execution-output masking during gradient computation to improve training stability. The final model is referred to as STORM."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "Our experimental evaluation provides comprehensive validation of our framework. We first benchmark STORM against leading models to establish its state-of-the-art performance. We then conduct extensive ablation and behavioral analyses to dissect the sources of its effectiveness and reveal the mechanisms through which CALM reshapes the models reasoning. 6 Preprint."
        },
        {
            "title": "4.1 EXPERIMENTAL SETUP",
            "content": "Benchmarks and Datasets. Our evaluation is conducted on diverse suite of five benchmarks: NL4Opt (Ramamonjison et al., 2023), MAMO-Easy, MAMO-Complex (Huang et al., 2024), IndustryOR (Huang et al., 2025), and OptMath (Lu et al., 2025). This selection, consistent with prior state-of-the-art studies (Chen et al., 2025), allows us to rigorously test LRM capabilities across spectrum of difficulty. All training and test data originate from larger collection of public datasets (Jiang et al., 2024), which we have rigorously partitioned into non-overlapping training and test sets. comprehensive breakdown of all data sources and our splitting strategy is provided in Appendix E.1. Baselines. We benchmark STORM against comprehensive set of baselines for holistic (1) Foundation Models: GPT-3.5-Turbo, performance evaluation. The comparison includes: GPT-4 (Achiam et al., 2023) and DeepSeek-V3; (2) Large Reasoning Models: DeepSeekR1-0528 (DeepSeek-AI, 2025) and Qwen3-235B-A22B-Thinking-2507 (Qwen Team, 2025); (3) Agent-Based Methods: Chain-of-Experts (Xiao et al., 2023) and OptiMUS (AhmadiTeshnizi et al., 2024); (4) Learning-Based Methods: ORLM (Huang et al., 2025), LLMOPT (Jiang et al., 2024), OptMath (Lu et al., 2025) and SIRL (Chen et al., 2025); and (5) crucially, our Base LRM, Qwen34B-Thinking-2507, which serves as the starting point to directly measure our frameworks impact. Evaluation Protocol. We report pass@1 accuracy as the primary evaluation metric. To address the high variance of greedy decoding in LRMs, as noted in DeepSeek-R1 (DeepSeek-AI, 2025), we follow their recommended evaluation protocol. Specifically, for each problem, we generate 8 independent samples using their specified configuration (temperature=0.6, top-p=0.95). The final pass@1 score is then reported as the average success rate across these 8 samples. This established method ensures more robust and reproducible measure of models performance. For fair comparison, all LRM-based models are evaluated under this protocol, allowing maximum of 4 code executions per reasoning trajectory. Training Procedure. For CALM data synthesis, we use Qwen3-4B-Thinking-2507 (Qwen Team, 2025) as the Reasoner and Gemini-2.5-Pro (Comanici et al., 2025) as the Intervener. The curated trajectories are then used in two-stage training pipeline described in Section 3.3. Our final model, STORM, is obtained through this pipeline. Detailed implementations are provided in Appendix E.3. 4.2 MAIN RESULTS Table 2: Main results on optimization modeling benchmarks. Bold indicates the best performance in each column. Results marked with * are cited from their original papers. The colored value next to our models scores indicates the absolute performance gain over its base model. Models Baseline Models Model Size NL4OPT MAMO Easy MAMO Complex IndustryOR OptMath Macro AVG GPT-3.5-Turbo GPT-4 DeepSeek-V3 DeepSeek-R1-0528 Qwen3-235B-A22B-Thinking-2507 NA NA 671B 671B 235B Agent-Based Methods Chain-of-Experts OptiMUS Learning-Based Methods LLMOPT-Qwen2.5-14B ORLM-LLaMA-3-8B OptMATH-Qwen2.5-7B SIRL-Qwen2.5-7B NA NA 14B 8B 7B 7B Our Framework: Transforming 4B LRM 78.0* 89.0* 95.9* 86.6 75. 64.2* 78.8* 80.3* 85.7* 94.7* 96.3* 79.3* 87.3* 88.3* 78.8 77.2 - 77.2* 89.5* 82.3* 86.5* 90.0* 33.2* 49.3* 51.1* 69.1 63. - 43.6* 44.1* 37.4* 51.2* 62.1* 21.0* 33.0* 37.0* 52.5 53.2 - 31.0* 29.0* 38.0* 20.0* 33.0* 15.0* 16.6* 32.6* 50.6 49. - 20.2* 12.5* 2.6* 24.4* 29.0* 45.3* 55.0* 61.0* 67.5 63.9 - 49.4* 51.1* 49.2* 55.4* 62.1* Qwen3-4B-Thinking-2507 (Base) STORM-Qwen3-4B (Ours) 4B 4B 85.8 93.3 +7.5 73.8 86.3 +12.5 46.5 70.3 +23.8 46.2 50.0 +3.8 33.1 44.5 +11. 57.1 68.9 +11.8 We present the main results in Table 2, which demonstrate how our framework transforms capable LRM into state-of-the-art optimization modeling expert. We highlight three key findings from our analysis. 7 Preprint. First, our method unlocks significant leap in performance over the base model. The initial calm\" adaptation through CALM lays the foundation for STORM to achieve remarkable gain of +11.8 absolute points in macro-average accuracy (57.1% to 68.9%), with particularly strong improvements on challenging benchmarks like MAMO-Complex (+23.8 points). Second, this enhancement allows our compact 4B model to exhibit strong parameter efficiency, achieving performance comparable to the 671B DeepSeek-R1-0528 (68.9% vs. 67.5%) and setting new stateof-the-art on MAMO-Complex (70.3%). Finally, this result establishes new state-of-the-art for learning-based methods, advancing the performance frontier in complex optimization modeling. These results underscore our central finding: preserving and refining models native reasoning patterns can achieve expert-level performance with high parameter efficiency."
        },
        {
            "title": "4.3.1 ABLATION STUDY: THE TWO-STAGE LEAP TO SOTA",
            "content": "Figure 5: Ablation study of our two-stage framework. We analyze the distinct contributions of our two training stages by tracking the performance evolution from the base LRM through SFT and RL, as detailed in Figure 5. SFT as Calibrator. SFT with CALM-curated data acts as behavioral calibrator. Unlike direct SFT  (Table 1)  , our soft adaptation avoids performance degradation on complex tasks, yielding modest gain in macro-average accuracy (57.1% to 58.7%). This stage gently corrects reasoning flaws without overwriting native patterns, laying stable foundation for subsequent mastery. RL as the Accelerator. Building on this calibrated foundation, the RL stage acts as an accelerator, driving decisive performance leap. The macro-average accuracy rises sharply from 58.7% to 68.9%, with the most significant gains on complex reasoning benchmarks. As shown in Figure 5, this storm stage propels our 4B model to level comparable with 671B LRM, demonstrating highly parameter-efficient path to expert performance. 4.3.2 DECONSTRUCTING CALM: AN INSIDE LOOK AT THE CURATION PROCESS Figure 6: The CALM data curation engine. 8 Preprint. To understand its mechanics, we decompose the CALM data curation process into three phases as summarized in Figure 6: diagnosing native flaws, refining trajectories via hinting, and filtering the results into high-quality SFT dataset. Diagnosis of Native Flaws. The diagnosis phase identifies failure modes in the base LRMs initial trajectories. The distribution of interventions (Figure 6, left) reveals two dominant flaw categories: Code Utilization Distrust and Lack of OR Expertise. Consistent with our analysis in Section 2.3, the former is more prevalent on the low-to-medium difficulty problems common in our SFT set. Refinement via Lightweight Hinting. The refinement phase uses an iterative hinting loop to correct flawed trajectories. As shown in Figure 6 (middle), this lightweight process, with minimal interventions per problem, significantly boosts the success rate while simultaneously reducing response length. This demonstrates that targeted guidance can enhance both correctness and conciseness. Filtering of Golden\" Trajectories. Finally, the filtering phase ensures only the highest-quality expert demonstrations are used for training. Our rigorous filtering funnel (Figure 6, right) is highly selective, retaining only trajectories that are both correct and deemed flawless by the Intervener, which guarantees the purity of the supervision signal for the SFT stage. 4.3.3 BEHAVIORAL EVOLUTION: HOW CALM SHAPES REASONING (a) RL Performance on Complex Test Sets. (b) Average Number of Code Blocks. (c) Average Response Length (Tokens). (d) Evolution of Flaws. Figure 7: Behavioral evolution analysis. To understand why CALM-SFT makes reinforcement learning more efficient, we conduct controlled experiment. We compare two models starting from the same base LRM: RL with CALM, fine-tuned on our curated \"golden\" trajectories, and control model, RL without CALM, finetuned on the original unguided reasoning flows. This design isolates the effect of the initial SFT data quality on the RL process. The detailed setup is provided in Appendix E.4. CALM as Catalyst for Sample-Efficient RL. Figure 7a shows that starting RL from high-quality reasoning patterns has profound impact. The RL with CALM model exhibits steeper and more stable learning curve, achieving noticeably higher performance ceiling within the same computational budget. In contrast, the control model learns far more slowly and shows no indication of closing the performance gap. This confirms that SFT on CALM trajectories provides strong inductive bias, acting as catalyst that makes subsequent RL far more sample-efficient. Shift Toward Computation-Driven Reasoning. This efficiency is explained by consistent behavioral changes, as shown in Figures 7b and 7c. The RL with CALM model progressively increases its use of code blocks while reducing average response length. This reflects shift toward expert-like behavior: replacing verbose natural language calculations with concise and reliable code execution. The control model, lacking this guidance, remains verbose and less computation-driven. 9 Preprint. The Two-Stage Healing Process. Finally, Figure 7d reveals complementary healing process. The SFT stage shows larger impact on reducing Lack of OR Expertise, while the subsequent RL stage is more effective at reducing Code Utilization Distrust. Together, these stages synergistically transform the LRM into specialized optimization modeler. per-benchmark breakdown of this evolution is provided in Appendix F."
        },
        {
            "title": "5 CONCLUSION",
            "content": "This work introduces CALM, lightweight framework for adapting Large Reasoning Models (LRMs) to optimization modeling. By aligning targeted interventions with specific reasoning flaws, CALM preserves native reasoning capabilities while improving optimization modeling accuracy. Our two-stage training pipeline combining hint-guided supervised fine-tuning with reinforcement learning transforms compact LRM into STORM, which achieves new state-of-the-art macro-average performance across five optimization modeling benchmarks. These results demonstrate the effectiveness of minimally invasive, reasoning-aligned adaptation for domain specialization. promising direction for future work is to extend STORM to broader optimization modeling agent frameworks, such as OptiMUS (AhmadiTeshnizi et al., 2024)."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Ali AhmadiTeshnizi, Wenzhi Gao, and Madeleine Udell. Optimus: Scalable optimization modeling with (mi) lp solvers and large language models. arXiv preprint arXiv:2402.10172, 2024. Yitian Chen, Jingfan Xia, Siyu Shao, Dongdong Ge, and Yinyu Ye. Solver-informed rl: Grounding large language models for authentic optimization modeling. arXiv preprint arXiv:2505.11792, 2025. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Chenyu Huang, Zhengyang Tang, Shixi Hu, Ruoqing Jiang, Xin Zheng, Dongdong Ge, Benyou Wang, and Zizhuo Wang. ORLM: customizable framework in training large models for automated optimization modeling. Operations Research, 2025. Xuhan Huang, Qingning Shen, Yan Hu, Anningzhe Gao, and Benyou Wang. Llms for mathematical modeling: Towards bridging the gap between natural and mathematical languages. arXiv preprint arXiv:2405.13144, 2024. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Caigao Jiang, Xiang Shu, Hong Qian, Xingyu Lu, Jun Zhou, Aimin Zhou, and Yang Yu. Llmopt: Learning to define and solve general optimization problems from scratch. arXiv preprint arXiv:2410.13213, 2024. Chengpeng Li, Zhengyang Tang, Ziniu Li, Mingfeng Xue, Keqin Bao, Tian Ding, Ruoyu Sun, Benyou Wang, Xiang Wang, Junyang Lin, et al. Cort: Code-integrated reasoning within thinking. arXiv preprint arXiv:2506.09820, 2025a. Chengpeng Li, Mingfeng Xue, Zhenru Zhang, Jiaxi Yang, Beichen Zhang, Xiang Wang, Bowen Yu, Binyuan Hui, Junyang Lin, and Dayiheng Liu. Start: Self-taught reasoner with tools. arXiv preprint arXiv:2503.04625, 2025b. 10 Preprint. Hongliang Lu, Zhonglin Xie, Yaoyu Wu, Can Ren, Yuxuan Chen, and Zaiwen Wen. Optmath: scalable bidirectional data synthesis framework for optimization modeling. arXiv preprint arXiv:2502.11102, 2025. Qwen Team. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. Rindranirina Ramamonjison, Timothy Yu, Raymond Li, Haley Li, Giuseppe Carenini, Bissan Ghaddar, Shiqi He, Mahdi Mostajabdaveh, Amin Banitalebi-Dehkordi, Zirui Zhou, et al. Nl4opt competition: Formulating optimization problems based on their natural language descriptions. In NeurIPS 2022 competition track, pp. 189203. PMLR, 2023. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Edward A. Silver. Operations research in inventory management: review and critique. Operations Research, 29(4):628645, 1981. doi: 10.1287/opre.29.4.628. Pamela H. Vance, Cynthia Barnhart, Ellis L. Johnson, and George L. Nemhauser. Airline crew scheduling: new formulation and decomposition algorithm. Operations Research, 45(2):188 200, 1997. doi: 10.1287/opre.45.2.188. Ziyang Xiao, Dongxiang Zhang, Yangjun Wu, Lilin Xu, Yuan Jessica Wang, Xiongwei Han, Xiaojin Fu, Tao Zhong, Jia Zeng, Mingli Song, et al. Chain-of-experts: When llms meet complex operations research problems. In The twelfth international conference on learning representations, 2023. Wenhao Zhang, Yuexiang Xie, Yuchang Sun, Yanxi Chen, Guoyin Wang, Yaliang Li, Bolin Ding, and Jingren Zhou. On-policy rl meets off-policy experts: Harmonizing supervised fine-tuning and reinforcement learning via dynamic weighting. arXiv preprint arXiv:2508.11408, 2025. 11 Preprint."
        },
        {
            "title": "A RELATED WORK",
            "content": "From Non-Reflective to Reflective OR Modeling. The application of LLMs to OR is undergoing fundamental paradigm shift. Early learning-based methods, including ORLM (Huang et al., 2025), LLMOPT (Jiang et al., 2024), and SIRL (Chen et al., 2025), treated modeling as non-reflective generation task, training models to produce complete solution in single, static pass. This approach, however, is misaligned with modern LRMs (Qwen Team, 2025; DeepSeek-AI, 2025), which possess powerful native reasoning patterns for iterative and adaptive problem-solving. Our work embraces this shift, aiming to preserve and guide the LRMs inherent capacity for reflective generation. Hint-based Reasoning Adaptation. Injecting guidance into models reasoning is promising adaptation technique. Concurrent works like START (Li et al., 2025b) use pre-defined, static hint libraries to encourage code use, while CoRT (Li et al., 2025a) relies on manual, human-in-theloop annotation for its Hint Engineering, limiting scalability. In contrast, our CALM framework introduces fully automated and dynamic Reasoner-Intervener\" pattern. It moves beyond static libraries and manual oversight by enabling an expert model to detect flaws and inject tailored hints, providing scalable, process-level correction that respects the LRMs native reasoning. ILLUSTRATION: INPUTOUTPUT STRUCTURE OF TRADITIONAL LLMS FOR OPTIMIZATION PROBLEMS Example: Optimization Problem in Natural Language and Its Formalization Input (Natural-Language Problem). company must transport 25 tons of cargo using trucks, airplanes, or ships. Per-ton costs are $100, $120, and $130, with capacities 10, 20, and 30 tons. Trucks and ships cannot be chosen together. The goal is to minimize the total cost while satisfying the demand. Output (Mathematical Model and Solver Code). Variables. x1, x2, x3 {0, 1}: binary variables indicating whether trucks, airplanes, and ships are selected. y1, y2, y3 0: transported volumes (tons) by each mode. Objective and Constraints. min 100y1 + 120y2 + 130y3 s.t. x1 + x2 + x3 1 y1 10x1, y2 20x2, y3 30x3 x1 + x3 1 y1 + y2 + y3 25 (4) (5) (6) (7) Program (PuLP). 1 from pulp import LpProblem, LpMinimize, LpVariable, LpBinary, lpSum, PULP_CBC_CMD, value = {\"trucks\":10, \"airplanes\":20, 2 3 # Data 4 costs = {\"trucks\":100, \"airplanes\":120, \"ships\":130} 5 caps 6 demand = 25 7 8 # Model 9 = LpProblem(\"Transportation\", LpMinimize) 10 = {k: LpVariable(f\"x_{k}\", 0, 1, cat=LpBinary) for in costs} 11 = {k: LpVariable(f\"y_{k}\", 0) for in costs} 12 13 # Objective 14 += lpSum(costs[k]*y[k] for in costs) \"ships\":30} 12 Preprint. 15 16 # Constraints 17 += lpSum(x[k] for in costs) >= 1 18 for in costs: += y[k] <= caps[k]*x[k] 19 20 += x[\"trucks\"] + x[\"ships\"] <= 1 21 += lpSum(y[k] for in costs) >= demand 22 23 # Solve 24 m.solve(PULP_CBC_CMD(msg=False)) 25 print(\"Objective:\", value(m.objective)) 26 for in costs: 27 print(f\"{k}: x={value(x[k])}, y={value(y[k])}\") PROTOCOL FOR HUMAN-IN-THE-LOOP FLAW TAXONOMY CREATION This section details the rigorous, multi-stage protocol our team of four human experts (graduate students with OR and STEM backgrounds) followed to establish the seven-flaw taxonomy presented in Section 2.3. The goal was to move from unstructured observations to systematic and reproducible classification of errors. Stage 1: Initial Data Generation and Independent Annotation. base LRM (Qwen3-4BThinking-2507) was used to generate solutions for diverse set of 50 problems selected to cover range of difficulties and types from our benchmark suite. Each of the four annotators independently reviewed these same 50 responses. For each response, they performed an open-ended analysis, identifying and documenting any perceived reasoning errors. Annotators were instructed to assign descriptive tag (e.g., \"manual-calculation-error,\" \"missed-integer-var\") and provide brief textual justification for each identified flaw. This initial stage resulted in four independent sets of annotations, containing rich but unstructured collection of observed errors. Stage 2: Collaborative Clustering and Taxonomy Refinement. The team then engaged in collaborative session to synthesize the independent findings. The process was as follows: 1. Merging: All unique error tags and justifications from the four annotators were collected into single master list. 2. Affinity Clustering: The team collectively grouped semantically similar tags into higherlevel clusters. For example, tags like \"manual-calculation-error,\" \"avoids-solver,\" and \"solves-by-hand\" were grouped into cluster that would later become \"Premature NL Solving.\" 3. Definition and Refinement: For each cluster, the team collaboratively wrote precise, operational definition for the flaw type it represented. This process involved several rounds of discussion to ensure the definitions were mutually exclusive and collectively exhaustive for the observed phenomena. Any ambiguous or overlapping clusters were either merged or further refined. This iterative process led to the convergence on the seven distinct and recurring flaw types detailed in Appendix D. This human-in-the-loop methodology ensures that our taxonomy is grounded in empirical observation and expert consensus."
        },
        {
            "title": "D TRIGGERS TYPE",
            "content": "As detailed in our protocol (Appendix C), our analysis identified seven recurring flaw types. Six of these are classified as substantive reasoning flaws as they represent fundamental errors in the problem-solving process. The seventh, Protocol Violation, is classified as procedural error as it relates only to output formatting. Our main analysis in the paper focuses on the six substantive flaws. The definitions for all seven triggers are as follows: 13 Preprint. [Trigger 1] Premature NL Solving: After formulating the mathematical model, the LRM starts solving it manually with natural language instead of immediately writing solver code. [Trigger 2] Fragmented Coding: The LRM writes small, non-executable, or multiple solver-running code blocks instead of single, comprehensive one. [Trigger 3] Redundant Manual Verification: After code output, the LRM manually re-calculates the exact numerical results that were already provided by the solver. [Trigger 4] Lack of Sanity Check/Reflection: The LRM gets correct code output but proceeds directly to the final answer without any high-level reflection on the results plausibility. [Trigger 5] Flawed Reasoning or Modeling: The LRMs logic is flawed, leading to an incorrect answer. This includes semantic misunderstanding, wrong mathematical model, or missing constraints (e.g., integers). [Trigger 6] Implementation Error: The mathematical model is correct, but the code is buggy or does not faithfully represent the model, leading to an incorrect answer. [Trigger 7] Protocol Violation: The LRM violates clear instruction, especially regarding the final boxing requirement. Here, triggers 1-3 exemplify Code Utilization Distrust, pinpointing behaviors such as solving problems with natural language instead of code or engaging in inefficient coding practices. Triggers 4-6 are indicators of Lack of OR Expertise, covering fundamental errors in modeling, logical reasoning, and code implementation. final trigger, Protocol Violation (Trigger 7), serves as procedural check to ensure the model adheres to specific output formatting instructions. See Table 3 for examples. Table 3: This table illustrates seven common LRM error patterns (triggers), showing the original error (red) and analysis of the errors. These triggers include: (1)Premature NL Solving, an attempt at manual calculation instead of coding; (2)Fragmented Coding, writing separate small code blocks; (3) Redundant Manual Verification, unnecessarily re-calculating solvers result; (4) Lack of Sanity Check, failing to reflect on solutions plausibility; (5) Flawed Reasoning or Modeling, formulating an incorrect mathematical model; (6) Implementation Error, correctly modeling question but incorrectly coding correct model; and (7)Protocol Violation, ignoring explicit instructions and embedding the boxed number within sentence. Error Type [Trigger 1] Premature NL Solving: ... notice that there is only 5 combinations and this is an easy task, so can just try them all first without writing python code. ... Alternatively, Worker III (4), Worker IV (3), Worker (9), Worker (7). Total: 4 + 3 + 9 +7 = 23. Thats worse. ... Analysis: It incorrectly assumed the problem was simple task involving only five combinations, thus obviating the need for Python code. The appropriate methodology would be to first construct mathematical model and then employ solver, accessed via code, to obtain the solution. Error Type [Trigger 2] Fragmented Coding: ... Re-calculating the derivative and solving by hand is unnecessary ... Ill calculate the ratio h/r directly from the output. can even use another small code block to be precise. ```python # Calculate the ratio h/r to determine the shape ratio = / optimal_r print(f\"Height to radius ratio: {ratio.evalf()}\") ``` ```output ... NameError: name 'h' is not defined ``` Analysis: This is fragmented coding and will fail because the variables from the first block are not available in the second. The ideal workflow is to perform all related calculations in single, self-contained solver script. 14 Preprint. Error Type [Trigger 3] Redundant Manual Verification: ... So the code is correct. (But wait, another way: Lets see if theres possible solution with lower total flavouring. For example, = 1, = 2 gives 10.) ... Analysis:After correctly executing the solver and getting the optimal result, the LRM shows lack of trust in the tool. It proceeds to manually verify the constraints (which is acceptable) but then attempts to re-solve the problem by hand. Error Type [Trigger 4] Lack of Sanity Check/Reflection: ... ```output Total cost: 24.00 Food 1: 0.0 Food 2: 8.0 ... Food 9: 4.0 ``` The output shows that the **optimal solution** uses: - **8 units of Tofu** - **4 units of Quinoa** This combination meets all the nutritional requirements at **minimum total cost of 24.00**. Analysis: The LRM correctly executes the solver and gets the optimal solution. However, it moves directly from stating the decision variables (8 Tofu, 4 Quinoa) to declaring the final answer without performing brief sanity check. An expert would quickly verify that this proposed solution indeed satisfies the nutritional constraints and matches the calculated cost. Error Type [Trigger 5] Flawed Reasoning or Modeling:: ... But maybe in this problem, we can treat them as continuous variables (since the problem might be designed for linear programming without integer constraints) ... Analysis: In the context of this problem, the units of the products must be integers and cannot be continuous. The LRM incorrectly treated continuous variables as integer variables during the modeling process, which led to flawed model formulation. i=1 yij demandj j=1 yij capacityi Error Type [Trigger 6] Implementation Error:: ... **Constraints**: 1. **Demand Constraint**: For each store j, (cid:80)6 2. **Capacity Constraint**: For each DC i, (cid:80)9 3. **Non-negativity**: yij 0 ... ```python ... # Create flow variables y[i][j] for each DC and store = [[pulp.LpVariable(f'y_{i}_{j}', cat='Continuous') for in range(9)] for in range(6)] ... ``` ```output Status: Unbounded Total cost: 16417.0 ``` ... Analysis: Although the LRM was correctly established in the modeling phase with constraints ensuring the non-negativity of all variables yij , this requirement was overlooked during implementation, where the code failed to set lower bound of zero for yij. Error Type [Trigger 7] Protocol Violation: ... **Final Answer** ...Ill now summarize the findings and box the final answer. The optimal solution uses 97.01 square feet of sunflowers and 0 square feet of roses, yielding maximum profit of 43656.72 . 15 Preprint. Analysis: The LRMs final answer formulation violates the instructions. It embeds the boxed number within sentence, whereas the protocol requires the box to contain only the final numerical answer and be separate from the summary text."
        },
        {
            "title": "E EXPERIMENTAL DETAILS APPENDIX",
            "content": "E.1 BENCHMARK DATASETS AND SPLITTING STRATEGY Our study utilizes broad range of public benchmarks Jiang et al. (2024) for training and evaluation. To ensure rigorous and unbiased experimental design, we randomly partitioned all available data from eight sources into non-overlapping training (SFT and RL) and test sets. Table 4 provides comprehensive overview of these sources, their original sizes, and our final partitioning. While our main evaluation in the paper focuses on five key benchmarks to ensure direct comparability with prior state-of-the-art work (Chen et al., 2025), we provide test splits for all datasets to facilitate future research. Table 4: Comprehensive overview of benchmark datasets and our rigorous splitting into non-overlapping SFT, RL, and Test sets. Benchmark Description Original Size SFT Set RL Set Test Set Data Source Data Partitioning NL4Opt MAMO-Easy NeurIPS 2022 competition data, focusing on LP formulation. High-school level MILP problems for fundamental modeling. MAMO-Complex Undergraduate-level MILP/LP problems with inIndustryOR OptMath OptiBench ComplexOR NLP4LP tricate structures. Real-world industrial problems across diverse sectors and types. Challenging mathematical optimization problems for advanced reasoning. collection of various optimization problems. Complex OR problems from academic and industrial scenarios. LP problems sourced from optimization textbooks and lecture notes. 46 650 211 100 607 18 12 8 200 55 30 250 0 0 8 350 12 36 257 0 0 30 100 80 100 100 18 12 E. IMPLEMENTATION DETAILS FOR THE PILOT STUDY This section provides the specific implementation details for the pilot study discussed in Section 2.2. Base Large Reasoning Model (LRM): The LRM used in this study was Qwen3-4BThinking-2507, powerful open-source model known for its strong multi-step reasoning capabilities. Non-reflective Dataset: We used OR-Instruct-3K (Huang et al., 2025), widelyrecognized dataset in the field. It consists of 3,000 problem-solution pairs and is representative of the non-reflective data generation paradigm. Training Procedure: The base LRM was fine-tuned using standard supervised finetuning (SFT) objective. The training utilized the same set of hyperparameters as our main SFT stage, which are detailed in Table 5. E. IMPLEMENTATION DETAILS FOR THE CALM & STORM FRAMEWORK This section provides comprehensive overview of the implementation details for our entire framework, including the computing infrastructure, the CALM data curation process, and the two-stage training pipeline. Computing Infrastructure. All experiments were conducted on cluster of four nodes, each equipped with 8x NVIDIA H800 (80GB) GPUs. 16 Preprint. CALM Data Curation. The expert-aligned trajectories for SFT were generated using our CALM framework with the following configuration: Reasoner Model: Qwen3-4B-Thinking-2507. Intervener Model: Gemini-2.5-Pro. Process Control: The iterative hinting loop was run for maximum of = 5 interventions per problem. An \"intervention\" consists of the Intervener identifying flaw, injecting hint, and the Reasoner regenerating the trajectory from that point. This limit serves as practical safeguard to prevent excessively long or unproductive correction cycles. If trajectory remains flawed after 5 interventions, it is discarded and not considered for the final SFT dataset. Reasoner Generation Parameters: Temperature set to 0.6, top-p to 0.95. Max response length was 16384 tokens with maximum of 4 code executions per turn. Intervener Generation Parameters: Temperature set to 1.0 and top-p to 0.95 to encourage diverse analytical feedback. Stage 1: Supervised Fine-Tuning (SFT). The SFT stage used the 112 \"golden\" trajectories curated by the CALM process. Base Model: Qwen3-4B-Thinking-2507. Optimizer: AdamW. Key Hyperparameters: Summarized in Table 5. Framework: DeepSpeed Stage 3 with bf16 precision. Table 5: Key hyperparameters for the supervised fine-tuning (SFT) stage. Hyperparameter Learning Rate LR Scheduler Warmup Ratio Total Batch Size Number of Epochs Max Sequence Length Value 1e-5 Cosine 0.1 8 3 Stage 2: Reinforcement Learning (RL). The RL stage commenced from the final checkpoint of the SFT model, using the following setup: Algorithm: Group Relative Policy Optimization (GRPO) via the Verl framework (Sheng et al., 2024). Key Hyperparameters: Detailed in Table 6. E.4 IMPLEMENTATION DETAILS FOR THE CONTROLLED EXPERIMENT This section details the setup for the controlled experiment presented in Section 4.3.3, which was designed to isolate the impact of the initial SFT data quality on RL dynamics. The experiment involved direct comparison between our main model, RL with CALM, and control model, RL without CALM. To ensure rigorous comparison, the control models setup was designed to mirror the main models in every aspect except for the SFT data. SFT Data. The control model was fine-tuned on the 112 original, unguided reasoning trajectories corresponding to the same problems used for the main models SFT stage. Hyperparameters. To maintain controlled environment, the hyperparameters for the control models SFT and RL stages were kept identical to those of our main model. Due to computational resource constraints, the RL training for this specific comparative analysis was conducted for 30 epochs. The complete list of hyperparameters for the control model is provided in Appendix E.3 for full transparency. 17 Preprint. Table 6: Hyperparameters for the Reinforcement Learning stage. Hyperparameter Value Start Model Checkpoint Learning Rate ϵ Total Epochs Train Batch Size PPO Mini-batch Size KL Loss General Final from supervised fine-tuning 1e-6 1e-3 100 64 64 Disabled Rollout Configuration Samples per Prompt (N) Temperature Max Prompt Length Max Response Length Max Code Execution per Rollout 8 0.6 3000"
        },
        {
            "title": "F DETAILED BREAKDOWN OF FLAW FREQUENCY EVOLUTION",
            "content": "In Section 4.3.3 of the main text, we presented the macro-average trend of flaw frequency reduction. To provide more granular view, Figure 8 presents detailed, per-benchmark breakdown of this evolution. Figure 8: per-benchmark breakdown of the evolution of flaw frequencies. Each subplot shows the average number of flaws per problem for the two main categories across the three training stages: Base LRM, After SFT, and After RL. The Macro Average plot (bottom right) summarizes the general trend. The six-panel figure illustrates the change in frequency for the two primary flaw categoriesCode Utilization Distrust (blue solid line) and Lack of OR Expertise (red dashed line)at each training stage. detailed analysis of the trends reveals the complementary roles of our two-stage approach: Stage 1 (SFT): Broad-Spectrum Correction. The supervised fine-tuning stage initiates significant reduction in both types of flaws across almost all benchmarks. Notably, we observe substantial drop in the red line (Lack of OR Expertise) during this phase (e.g., in IndustryOR and MAMO-Complex). This suggests that exposing the LRM to highquality, expert-aligned reasoning trajectories in the CALM dataset provides strong initial guidance, helping it to correct fundamental modeling errors and adopt more expert-like problem formulations. The blue line (Code Utilization Distrust) also shows general downward trend, indicating that the model begins to learn more efficient code-use habits. Preprint. Stage 2 (RL): Targeted Refinement and Mastery. Building upon the foundation laid by SFT, the reinforcement learning stage continues to refine the models skills. The RL phase consistently drives down the remaining flaws of both types, pushing the error rates to their lowest levels. This stage allows the model to move beyond simple imitation and achieve deeper, more robust mastery of both domain knowledge and code use through trial-and-error exploration. This per-benchmark analysis reinforces our central claim: the two-stage pipeline works synergistically. SFT provides strong initial correction across the board, and RL builds upon this to achieve state of expert-level proficiency. 19 Preprint."
        },
        {
            "title": "G COMPARISON OF REASONING PARADIGM",
            "content": "Figure 9: An illustrative example comparing the Non-reflective Generation (left) and Reflective Generation (right) paradigms on vehicle parking optimization problem Figure 9 demonstrates the practical differences between the two reasoning paradigms using parking optimization task. The Non-reflective Generation approach (left) formulates mathematical model and writes the complete code in single step. However, subtle error in one of the constraints leads to logically incorrect final answer. Due to its non-reflective pattern, the model is unable to detect or correct this error. In contrast, the reflective generation approach (right) showcases an iterative refinement process. The model initially generates code that also contains an error, leading to an implausible output. By observing this solver output, the model identifies the flaw in its reasoning. It then autonomously corrects the constraint in the code and re-executes it, successfully arriving at the correct optimal value. This case clearly highlights the advantage of the reflective paradigm: its ability to leverage execution feedback for self-correction and robust problem-solving. 20 Preprint."
        },
        {
            "title": "H INTERVENTION PROCESS FOR SPECIFIC FLAWS",
            "content": "Figure 10: representative example of the Code Utilization Distrust flaw. (1) The model exclusively relies on verbal reasoning and fails to utilize the solver, leading to an incorrect answer. (2) In contrast, the reasoning process guided by CLAM successfully employs the solver to reach the correct solution. I"
        },
        {
            "title": "ILLUSTRATIVE CASE STUDY OF THE CALM FRAMEWORK",
            "content": "To complement our quantitative findings, we provide case study illustrating how CALM works in practice. Figure 11 traces multi-turn ReasonerIntervener interaction. The initial trajectory begins with Lack of OR Expertise error, mistaking an Integer Linear Program (ILP) for standard LP. Even after correction, secondary issue of Code Utilization Distrust emerges. This case demonstrates how sequence of lightweight, targeted hints can progressively guide the Reasoner through distinct failure modes, ultimately yielding correct, expert-aligned solution without further intervention. Reasoning (τ (0)): ... But maybe in this problem, we can treat them as continuous variables (since the ... Answer: 798.04 problem might be designed for linear programming without integer constraints) (Wrong!) Intervention (i = 0, Trigger 5 Flawed Reasoning or Modeling detected ): Ive correctly identified the ambiguity around continuous vs. integer variables ... This changes the problem from Linear Program (LP) to an Integer Linear Program (ILP). ... Reasoning (τ (1)): ... So the code is correct. (But wait, another way: Lets see if theres possible solution with lower total flavouring. For example, = 1, = 2 gives 10.) ... 10 (Correct!) Intervention (i = 1, Trigger 3 Redundant Manual Verification detected ): Okay, the solver returned ... Ill proceed to M=1, O=2, with total flavouring of 10. should trust the solvers optimality result. structure the final answer. Reasoning (τ (2)): The solver confirms that the **optimal solution** is: 1 order of **matcha ice cream**; 2 orders of **orange sorbet**... 10 (Correct!) Intervention (i = 2, Trigger 3 Redundant Manual Verification detected ): ... This result is logical, and can now confidently format the final answer. Reasoning (τ (3)): (correct reasoning without any trigger detected)... 10 (Correct!) Intervention (i = 3, NO INTERVENTION ) Figure 11: An illustrative case study of the iterative \"ReasonerIntervener\" collaboration pattern, where targeted hints progressively correct flawed reasoning trajectory. Here, red represents the error and blue represents the correction of the Intervener. Preprint. I.1 PROMPT TEMPLATES The effectiveness of our framework relies on carefully designed prompts for both the Reasoners initial task and the Interveners supervisory role. Initial Prompt for the Reasoner. The Reasoner is initiated with detailed prompt that outlines the task, reasoning guidelines, tool usage protocols, and the required final answer format. The full template is provided below."
        },
        {
            "title": "Prompt for Reasoner",
            "content": "Given mathematical problem, follow the instructions below to (cid:44) solve it. ### Instructions: When solving mathematical problems, you should leverage both (cid:44) natural language reasoning and Python code execution. Your goal is to provide clear, detailed explanations while utilizing Python to perform complex calculations. Follow these guidelines to ensure coherent and effective response: **Natural Language Reasoning:** - (cid:44) - (cid:44) Provide comprehensive, step-by-step explanations of your thought process. Formulate your plan BEFORE writing code. Explain what you are about to do and why. **Code Execution Rules:** - (cid:44) **Purpose:** Each Python code block must be complete, self-contained script that executes single, logical step of your plan. **Output:** The SOLE mechanism for displaying results is the `print()` function. The purpose of code block is to compute value or set of values and explicitly `print()` them for the subsequent `output` block. **Structure:** Each block must contain all necessary imports and setups. The code must be directly executable. Avoid any boilerplate like `if __name__ == '__main__':`. (cid:44) - (cid:44) (cid:44) (cid:44) - (cid:44) (cid:44) (cid:44) **Recommended Toolkit & Best Practices:** - (cid:44) To ensure reliability and environment compatibility, **you must prioritize using the following libraries** for their respective tasks. For **symbolic mathematics**: use `sympy`. For **numerical operations**: use `numpy`. For **scientific computing**: use `scipy`. For **optimization problems**: use `pulp`. (cid:44) - - - - **Solution Verification and Final Answer:** A. **Code Output for Verification:** To ensure your (cid:44) reasoning is transparent and verifiable, your **final code block** should print all key results needed for the solution. For optimization problems, this typically includes: * * The optimal objective function value. The values of the main decision variables. (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) 1. 2. 3. 4. Preprint. C. **Final Answer Formulation:** * (cid:44) (cid:44) * (cid:44) (cid:44) **Full Solution Description:** Briefly summarize your findings, referencing the key values printed by your code. **Final Answer Boxing:** The final step is to put the **single numerical answer** to the main question inside `boxed{}`. - **Content:** The box should contain **only the (cid:44) number**, without any units, currency signs, or explanatory text. (cid:44) - **Example (Correct):** `boxed{1234}` or (cid:44) `boxed{1234.37}` - **Example (Incorrect):** `boxed{Total cost is $1234.0}` (cid:44) ### Problem: {problem_text} Prompt for the Intervener. The Intervener is guided by meta-prompt that defines its role, the ideal expert workflow, and the specific Deviation Triggers it should look for. This prompt is crucial for the automated and targeted nature of our hinting process. The full template is provided below."
        },
        {
            "title": "Prompt for Intervener",
            "content": "### CONTEXT AND GOAL You are an expert Operations Research (OR) engineer and an LLM Reasoning Pattern Analyst. Your mission is to assist in (cid:44) generating high-quality training data for fine-tuning Large Reasoning Models (LRMs). (cid:44) (cid:44) (cid:44) The ultimate goal is to adapt an LRM's native reasoning pattern (which is heavily reliant on long-form natural language) to (cid:44) better emulate the iterative workflow of human OR expert. The ideal expert workflow is cycle of: **1. Understand & Model -> 2. Code Solver -> 3. Execute & Observe -> 4. Reflect & Debug -> (Repeat)**. (cid:44) (cid:44) (cid:44) Your specific task is to analyze given LRM response and, if it (cid:44) deviates from this ideal workflow, insert strategic hint to guide it back on track. This process, called \"Auto Hint Engineering,\" creates more efficient and robust reasoning trace for training. (cid:44) (cid:44) (cid:44) ### INSTRUCTIONS 1. First, carefully review the `TASK_DEFINITION` which contains the original problem and instructions given to the LRM. (cid:44) 2. Next, analyze the provided `LLM_RESPONSE_TO_REFINE`. 3. Identify the **first point** of deviation based on the (cid:44) 4. (cid:44) triggers defined below. If deviation is found, your output MUST be structured using the custom tags `<action>`, `<trigger_type>`, `<analysis>`, `<target_text>`, and `<hint_to_insert>`. The action should be \"REPLACE_AND_CONTINUE\". The `<target_text>` tag should contain the exact, unique, and contiguous block of text from the original response that needs to be replaced. (cid:44) (cid:44) 5. (cid:44) (cid:44) 23 Preprint. 6. The `<hint_to_insert>` tag should contain the new hint you've crafted according to the principles below. (cid:44) 7. If the response is ideal, your output should simply be (cid:44) `<action>NO_INTERVENTION</action>`. (cid:44) * (cid:44) (cid:44) * (cid:44) (cid:44) * (cid:44) (cid:44) (cid:44) * (cid:44) (cid:44) (cid:44) * (cid:44) (cid:44) * (cid:44) (cid:44) (cid:44) * (cid:44) * (cid:44) (cid:44) * (cid:44) * (cid:44) (cid:44) (cid:44) ### DEVIATION TRIGGERS * (cid:44) **Trigger 1: Premature NL Solving:** After formulating the mathematical model, the LRM starts solving it manually with natural language instead of immediately writing solver code. **Trigger 2: Fragmented Coding:** The LRM writes small, non-executable, or multiple solver-running code blocks instead of single, comprehensive one. **Trigger 3: Redundant Manual Verification:** After code output, the LRM manually re-calculates the exact numerical results that were already provided by the solver. **Trigger 4: Lack of Sanity Check/Reflection:** The LRM gets correct code output but proceeds directly to the final answer without any high-level reflection on the result's plausibility. **Trigger 5: Flawed Reasoning or Modeling:** The LRM's logic is flawed, leading to an incorrect answer. This includes semantic misunderstanding, wrong mathematical model, or missing constraints (e.g., integers). **Trigger 6: Implementation Error:** The mathematical model is correct, but the code is buggy or does not faithfully represent the model, leading to an incorrect answer. **Trigger 7: Protocol Violation:** The LRM violates clear instruction, especially regarding the final boxing requirement. ### HINT PRINCIPLES (to guide your hint creation) * (cid:44) **Be Guide, Not Dictator:** Use first-person, reflective tone (e.g., \"I see, better way would be...\", \"Okay, now should...\"). **Encourage Action:** Frame the hint to prompt specific, desirable next action. **[FOR TRIGGERS 1 & 2] Force Code Generation:** End your hint with `nn```python` to strongly encourage immediate and complete code writing. * (cid:44) *Example:* \"The model is fully formulated. The best next step is to implement this using `pulp` to get an exact solution.nn```python\" (cid:44) **[FOR TRIGGER 3] Promote Trust in Tools:** Guide the LRM away from redundant calculation and towards interpretation. *Example:* \"The solver has already provided the optimal * values. Re-calculating them manually is unnecessary. (cid:44) should now focus on interpreting the solution.\" (cid:44) **[FOR TRIGGER 4] Encourage Sanity Checks:** Gently guide the LRM to perform brief, high-level sanity check. The goal is to cultivate habit of reflection, not to force rigid process. * (cid:44) **Hint for Trigger 4 (Lack of Reflection):** \"The solver returned an optimal cost of $392,760. Before finalize the answer, it's good practice to quickly reflect on this. Given the high fixed costs of the distribution centers, this value seems to be in reasonable range. This gives me confidence in the result. Now, I'll proceed to format the final solution.\" (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) 24 Preprint. * (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) * (cid:44) **[Alternate Hint with Code-Assisted Check]:** \"The solver returned an optimal cost of $392,760. That seems plausible. To build more confidence, could write quick script to explore simplified scenario, like checking the cost if only open the three cheapest centers. This will help verify my understanding.nn```python\" (cid:44) **[FOR TRIGGERS 5-7] Inject Focused Expertise:** Craft concise hint that addresses the specific flaw found. * (cid:44) *Hint for Trigger 5 (Model Completeness Error):* \"I've noticed the solution provides fractional number of cars, which isn't practical. This suggests missed an integer constraint in my original model. should correct this by redefining the variables as integers in my code and re-running it.\" *Hint for Trigger 6 (Implementation Error):* \"I've spotted bug. My math model for the constraint was `A <= B`, but in the code wrote `A >= B`. need to correct this implementation error to match my model.\" (cid:44) (cid:44) (cid:44) (cid:44) * (cid:44) (cid:44) (cid:44) 7]</trigger_type> ### OUTPUT STRUCTURE (MUST use these custom tags) <action>REPLACE_AND_CONTINUE</action> <trigger_type>[Trigger 1 Trigger 2 ... Trigger (cid:44) <analysis>[A brief explanation of why this intervention is necessary based on the detected trigger]</analysis> (cid:44) <target_text>[The exact text from the original response to be (cid:44) <hint_to_insert>[Your newly crafted hint goes (cid:44) replaced]</target_text> here]</hint_to_insert> (OR, if no intervention is needed) <action>NO_INTERVENTION</action> ### --- START OF TASK --- ### TASK_DEFINITION: ```text {task_definition} ``` ### GROUND_TRUTH_ANSWER (if available) The known correct final answer for the objective function is: (cid:44) `boxed{[ground_truth_answer]}` You should use this ground truth to definitively verify the (cid:44) numerical correctness of the LRM's final boxed answer. If the LRM's answer is incorrect, your primary goal is to identify the root cause of the discrepancy. (cid:44) (cid:44) ### LLM RESPONSE TO REFINE: ```text {llm_response_text} 25 Preprint."
        },
        {
            "title": "J FLAW QUANTIFICATION OF NATIVE LRMS",
            "content": "To achieve scalable and consistent analysis across thousands of model responses, we utilized Gemini-2.5-Pro as an expert annotator. Its task was to classify flaws in the native LRMs generated trajectories based on the seven pre-defined categories described in Appendix D. Distinction from the CALM Intervener. It is crucial to distinguish this analytical use of an external model from its role as the dynamic Intervener within our CALM data generation framework (Section 3.2). For Quantification (here): The model acts as static classifier. Its goal is to analyze completed response and output structured list of detected flaws for measurement purposes. It does not interact with the LRM. For CALM Intervention (Section 3): The model acts as an interactive agent. Its goal is to monitor reasoning process in real-time and inject corrective hints to guide the LRM towards better solution, thereby generating new training data. While both roles leverage the same underlying understanding of OR modeling flaws, their functions and objectives within our study are entirely separate. Prompt for Flaw Classification. The prompt below was used to guide the Gemini-2.5-Pro model in its role as static classifier."
        },
        {
            "title": "Prompt for Flaw Classification",
            "content": "### CONTEXT AND GOAL You are an expert Operations Research (OR) engineer and an LLM Reasoning Pattern Analyst. Your mission is to assist in (cid:44) generating high-quality training data for fine-tuning Large Reasoning Models (LRMs). (cid:44) (cid:44) (cid:44) The ultimate goal is to adapt an LRM's native reasoning pattern (which is heavily reliant on long-form natural language) to (cid:44) better emulate the iterative workflow of human OR expert. The ideal expert workflow is cycle of: **1. Understand & Model -> 2. Code Solver -> 3. Execute & Observe -> 4. Reflect & Debug -> (Repeat)**. (cid:44) (cid:44) (cid:44) Your specific task is to analyze given LRM response and, if it deviates from this ideal workflow, identify all the triggers (cid:44) we defined. (cid:44) below. ### INSTRUCTIONS 1. First, carefully review the `TASK_DEFINITION` which contains the original problem and instructions given to the LRM. (cid:44) 2. Next, analyze the provided `LLM_RESPONSE_TO_REFINE`. 3. Identify at most two deviation based on the triggers defined (cid:44) 4. If an deviation is found, your output MUST be structured (cid:44) 5. The only one found trigger should in <trigger_type>, for (cid:44) example, <trigger_type>Trigger 1</trigger_type>. If there are multiple triggers, separate them by `;`, for example, <trigger_type>Trigger 1;Trigger 7</trigger_type>. (cid:44) 6. If the response is ideal, your output should simply be (cid:44) using the custom tags `<trigger_type>`. <trigger_type>Correct</trigger_type>. (cid:44) ### DEVIATION TRIGGERS Preprint. * (cid:44) (cid:44) * (cid:44) (cid:44) * (cid:44) (cid:44) * (cid:44) (cid:44) (cid:44) * (cid:44) (cid:44) (cid:44) * (cid:44) (cid:44) * (cid:44) (cid:44) **Trigger 1: Premature NL Solving:** After formulating the mathematical model, the LRM starts solving it manually with natural language instead of immediately writing solver code. **Trigger 2: Fragmented Coding:** The LRM writes small, non-executable, or multiple solver-running code blocks instead of single, comprehensive one. **Trigger 3: Redundant Manual Verification:** After code output, the LRM manually re-calculates the exact numerical results that were already provided by the solver. **Trigger 4: Lack of Sanity Check/Reflection:** The LRM gets correct code output but proceeds directly to the final answer without any high-level reflection on the result's plausibility. **Trigger 5: Flawed Reasoning or Modeling:** The LRM's logic is flawed, leading to an incorrect answer. This includes semantic misunderstanding, wrong mathematical model, or missing constraints (e.g., integers). **Trigger 6: Implementation Error:** The mathematical model is correct, but the code is buggy or does not faithfully represent the model, leading to an incorrect answer. **Trigger 7: Protocol Violation:** The LRM violates clear instruction, especially regarding the final boxing requirement. ### OUTPUT STRUCTURE (MUST use these custom tags) <trigger_type>[Trigger 1 Trigger 2 ... Trigger 7];...;[Trigger 1 Trigger 2 ... Trigger (cid:44) 7]</trigger_type> (cid:44) ### --- START OF TASK --- ### TASK_DEFINITION: ```text {task_definition} ``` ### LLM RESPONSE TO REFINE: ```text {llm_response_text} ``` Validation of the LLM Annotator. To ensure the reliability of the automated quantification process, we validated the LLM annotators performance against human labels. We randomly sampled 30 responses from the test set, which were independently annotated by both the LLM (using the aforementioned prompt) and one of our expert human annotators. The agreement between the LLM and human labels was then measured. The LLM achieved an accuracy of 93.3% in identifying and correctly classifying the flaw types present in the responses, calculated based on the instance-level matching of flaw categories. This high level of agreement provides strong evidence for the validity of using the LLM for scalable and consistent flaw quantification across the entire benchmark suite."
        }
    ],
    "affiliations": [
        "Qwen Team, Alibaba Inc.",
        "Shanghai University of Finance and Economics",
        "Shenzhen Loop Area Institute (SLAI)",
        "Southern University of Science and Technology",
        "The Chinese University of Hong Kong, Shenzhen"
    ]
}