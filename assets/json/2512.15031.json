{
    "paper_title": "Toxicity Ahead: Forecasting Conversational Derailment on GitHub",
    "authors": [
        "Mia Mohammad Imran",
        "Robert Zita",
        "Rahat Rizvi Rahman",
        "Preetha Chatterjee",
        "Kostadin Damevski"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Toxic interactions in Open Source Software (OSS) communities reduce contributor engagement and threaten project sustainability. Preventing such toxicity before it emerges requires a clear understanding of how harmful conversations unfold. However, most proactive moderation strategies are manual, requiring significant time and effort from community maintainers. To support more scalable approaches, we curate a dataset of 159 derailed toxic threads and 207 non-toxic threads from GitHub discussions. Our analysis reveals that toxicity can be forecast by tension triggers, sentiment shifts, and specific conversational patterns. We present a novel Large Language Model (LLM)-based framework for predicting conversational derailment on GitHub using a two-step prompting pipeline. First, we generate \\textit{Summaries of Conversation Dynamics} (SCDs) via Least-to-Most (LtM) prompting; then we use these summaries to estimate the \\textit{likelihood of derailment}. Evaluated on Qwen and Llama models, our LtM strategy achieves F1-scores of 0.901 and 0.852, respectively, at a decision threshold of 0.3, outperforming established NLP baselines on conversation derailment. External validation on a dataset of 308 GitHub issue threads (65 toxic, 243 non-toxic) yields an F1-score up to 0.797. Our findings demonstrate the effectiveness of structured LLM prompting for early detection of conversational derailment in OSS, enabling proactive and explainable moderation."
        },
        {
            "title": "Start",
            "content": "Toxicity Ahead: Forecasting Conversational Derailment on GitHub Robert Zita Elmhurst University Elmhurst, IL, USA rzita8729@365.elmhurst.edu Rahat Rizvi Rahman Virginia Commonwealth University Richmond, VA, USA rahmanr12@vcu.edu Mia Mohammad Imran Missouri University of Science and Technology Rolla, MO, USA imranm@mst.edu 5 2 0 2 7 1 ] . [ 1 1 3 0 5 1 . 2 1 5 2 : r Preetha Chatterjee Drexel University Philadelphia, PA, USA preetha.chatterjee@drexel.edu Kostadin Damevski Virginia Commonwealth University Richmond, VA, USA kdamevski@vcu.edu Abstract Toxic interactions in Open Source Software (OSS) communities reduce contributor engagement and threaten project sustainability. Preventing such toxicity before it emerges requires clear understanding of how harmful conversations unfold. However, most proactive moderation strategies are manual, requiring significant time and effort from community maintainers. To support more scalable approaches, we curate dataset of 159 derailed toxic threads and 207 non-toxic threads from GitHub discussions. Our analysis reveals that toxicity can be forecasted by tension triggers, sentiment shifts, and specific conversational patterns. We present novel Large Language Model (LLM)-based framework for predicting conversational derailment on GitHub using two-step prompting pipeline. First, we generate Summaries of Conversation Dynamics (SCDs) via Least-to-Most (LtM) prompting; then we use these summaries to estimate the likelihood of derailment. Evaluated on Qwen and Llama models, our LtM strategy achieves F1-scores of 0.901 and 0.852, respectively, at decision threshold of 0.3, outperforming established NLP baselines on conversation derailment. External validation on dataset of 308 GitHub issue threads (65 toxic, 243 non-toxic) yields an F1-score up to 0.797. Our findings demonstrate the effectiveness of structured LLM prompting for early detection of conversational derailment in OSS, enabling proactive and explainable moderation. CCS Concepts Software and its engineering Open source model; Programming teams. Keywords Toxicity, Bug Report, Empirical Study, Open Source Software Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. Conference17, Washington, DC, USA 2026 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-x-xxxx-xxxx-x/YYYY/MM https://doi.org/10.1145/nnnnnnn.nnnnnnn ACM Reference Format: Mia Mohammad Imran, Robert Zita, Rahat Rizvi Rahman, Preetha Chatterjee, and Kostadin Damevski. 2026. Toxicity Ahead: Forecasting Conversational Derailment on GitHub. In . ACM, New York, NY, USA, 13 pages. https: //doi.org/10.1145/nnnnnnn.nnnnnnn"
        },
        {
            "title": "1 Introduction\nToxic language undermines the health of online communities, in-\ncluding those centered around software projects. A 2024 GitHub\nsurvey reported that 64.23% of developers experienced or witnessed\nnegative interactions [23], a slight increase from the 60.0% recorded\nin 2017 [72]; notably, 21.4% reported that such interactions led them\nto stop contributing. Despite the increasing recognition of the nega-\ntive impact of toxic interactions, to our best knowledge, all existing\ntoxicity detection methods are post-hoc [44, 51, 54, 56], identify-\ning toxic content only after it appears. While post-hoc detection\ncan mitigate some of the damage caused by toxic interactions, it\nfails to prevent the initial harm and may allow negative behaviors\nto persist unchecked for extended periods. A reactive approach\nnot only delays intervention but also burdens community modera-\ntors and risks alienating contributors who might have otherwise\nremained engaged. Consequently, there is a pressing need for proac-\ntive solutions that can anticipate and preemptively address potential\ntoxicity [39].",
            "content": "The primary strategy of proactive moderation involves human moderators actively engaging with ongoing conversations to prevent them from devolving into toxic behavior or, at the very least, to swiftly address any negativity should it arise and before it escalates any further [9, 58]. While effective, manual proactive moderation in OSS is impractical since moderators need to continuously monitor ongoing conversations across several communication channels (e.g., issues, chats, discussion boards). On the other hand, automated moderation offers scalability but demands deep understanding of community norms and context. However, unlike platforms such as (formerly Twitter) or Reddit, GitHub exhibits more subtle toxic behaviors, such as entitlement, miscommunication, or resistance to new practices, rather than overt aggression [25]. Moreover, domainspecific terms in software engineering (e.g., kill, dead, and dump) can pose challenges to generic automated toxicity detection [27, 56]. In this paper, we investigate how GitHub conversations derail into toxicity and present an automated approach for predicting such Conference17, July 2017, Washington, DC, USA Mia Mohammad Imran, Robert Zita, Rahat Rizvi Rahman, Preetha Chatterjee, and Kostadin Damevski derailment. Leveraging recent advances in LLMs, we introduce novel framework that integrates advanced prompting techniques to support proactive moderation in OSS communication channels. More specifically, we address these three research questions: RQ1: What are the characteristics and patterns of conversational derailment in GitHub discussions? Effective prediction requires deep understanding of how technical conversations deteriorate. We curated dataset of 159 derailed toxic and 207 non-toxic GitHub conversations, annotated with derailment and toxicity points. Then, we empirically analyze temporal dynamics, linguistic patterns, and contextual triggers that precede toxicity. Our findings show that derailment on GitHub often precedes toxicity by narrow margin; the median distance between the first derailment point and the first toxic comment is just 3 comments, and 64% of toxic comments occurred within 24 hours. We identify set of consistent early warning signals at derailment points, including elevated use of reasoning terms (e.g., \"because\", \"since\"), WH-questions (e.g., \"why\", \"how\"), and second person pronouns (e.g., \"you\", \"your\"), as well as tones like Frustration or Impatience. RQ2: Can Large Language Models effectively predict conversational derailment on GitHub? We develop and evaluate novel LLM-based framework that generates interpretable conversation summaries to predict derailment. More specifically, we leverage Least-to-Most prompting to generate high-level Summaries of Conversation Dynamics (SCDs). These summaries abstract away technical details to highlight interaction patterns, emotional tone, and rhetorical shifts. Based on these SCDs, we predict the likelihood of conversation derailing into toxicity. To enable forecasting rather than detection, our approach for SCD generation only considers comments that occur before the first toxic remark, ensuring that predictions are based solely on pre-toxicity context. This design aligns with prior derailment forecasting formulations [9]. Our framework achieves F1-scores of 0.901 (Qwen) and 0.852 (Llama), significantly outperforming established baselines including CRAFT [9] and Hua et al.s few-shot SCD [26] approaches while maintaining interpretability for human moderators. Through ablation studies, we find that sentiment evolution and tension triggers are the most critical components for prediction accuracy. RQ3: To what extent does the proposed LLM-based derailment prediction approach generalize to independent GitHub datasets? We validate our approach on independent GitHub data to establish confidence in its broader applicability across different communities and time periods. External validation on the Raman et al.s [51] dataset (308 threads, 65 toxic and 243 non-toxic) shows reasonable generalizability, with our LtM strategy achieving F1-scores of 0.797 (Qwen) and 0.776 (Llama). The approach outperforms baselines on external data despite different collection methodologies, time periods, and class distributions, suggesting that our method captures broader patterns of GitHub conversational dynamics. Paper Contributions. Our investigation yields several key contributions. We provide the empirical characterization of GitHub conversational derailment patterns, revealing predictable deterioration signals including temporal proximity (median 3 comments before toxicity), distinctive linguistic markers, and common triggers. Figure 1: Example of toxic conversation on GitHub. We introduce novel LLM-based framework using structured Leastto-Most prompting to generate explainable Summaries of Conversation Dynamics, achieving F1-scores of 0.901 while maintaining interpretability. Finally, we provide actionable insights for OSS communities seeking to implement proactive moderation strategies. Our studys datasets, scripts, and output logs are publicly available online to facilitate reproducible research [2]."
        },
        {
            "title": "GitHub",
            "content": "In online forums, toxicity often occurs after identifiable signs in the previous comments by the discussion participants [9]. In this research, we focus on understanding the early signs that conversation will turn toxic on GitHub issues and pull request discussions. These preceding comments, where it becomes clear that the conversation has moved away from being productive and taken turn towards negativity, are called derailment points [68]. Overall, toxic conversations often contain the following identifiable elements: 1) conversation-initiating comment, 2) derailment point comment, 3) first toxic comment, and 4) (zero or more) subsequent toxic (or non-toxic) comments. Figure 1 shows an example of toxic conversation, highlighting these different structures. In Toxicity Ahead: Forecasting Conversational Derailment on GitHub Conference17, July 2017, Washington, DC, USA Table 1: Definitions and examples of uncivil tone-bearing discussion features (TBDF). Definition Example TBDF Bitter Frustration Impatience Mocking Irony Vulgarity Threat Expressing strong frustration, displeasure, or annoyance Expressing dissatisfaction due to delays Ridiculing or making fun of someone in disrespectful way Using language to imply meaning that is opposite to the literal meaning, often sarcastically Using offensive or inappropriate language Issuing warning that implies negative consequence Entitlement Expecting special treatment or privileges Insulting Identity attacks/ Namecalling Making derogatory remarks towards another person or project Making derogatory comments based on race, religion, gender, sexual orientation, or nationality No answer, no reaction, what kind of support is that. Issue not fixed in 30 days? Must be gone! Legend says this issue will still exist even on the end of mankind. Maybe you should actually write that down somewhere. You know, like in the documentation. Who cares, same sh*t. Any further responses will result in you being blocked from the repo entirely. thats how good we are. dont want your contribution. [...] This looks like it was done by 5 year old. would not be surprised if this database is maintained by the [nationality]. this conversation between an OSS project contributor and an external participant (i.e., someone who has never made commit to the repository), the contributor derails the conversation by making mocking comment. The external participant responds with frustration and then makes toxic, insulting remark. This is followed by another toxic comment, this time made by the contributor."
        },
        {
            "title": "3.1 Toxic Conversations Dataset\nWe start with a dataset recently released by Ehsani et al. [16], which\nfocuses on incivility in GitHub conversations. Toxicity is a subset\nof incivility, focusing on harmful language, while incivility more\nbroadly includes behaviors that undermine constructive discus-\nsion [19, 52]. More specifically, incivility is defined as ‚Äúfeatures of\ndiscussion that convey an unnecessarily disrespectful tone toward the\ndiscussion forum, its participants, or its topics\" [12], while toxicity\nis defined as ‚Äúrude, disrespectful, or unreasonable language that is",
            "content": "likely to make someone leave discussion\" [33]. Therefore, leveraging this incivility dataset provides an appropriate starting point for identifying toxic interactions in GitHub threads. We use an LLM-aided model-in-the-loop annotation approach to identify the uncivil comments that are also toxic [6]. Recent research shows that such model-in-the-loop annotation methodology works well for this type of data, including hate and violent speech detection tasks [21, 31, 46, 53, 64, 67, 71]. This methodology has been leveraged in software engineering toxicity detection as well [30, 44]. In our annotation, we leveraged the prompt released by Imran et al. for toxicity detection in GitHub bug reports [30]. For each uncivil comment identified in Ehsani et al.s dataset, we used GPT-4o to assess whether the comment was toxic, providing the full preceding conversation context up to that point. GPT-4o labeled 832 comments across 273 threads as toxic. To validate these predictions, two authors independently reviewed each flagged comment. The initial inter-annotator agreement was 0.78 (Cohens Kappa). The annotators resolved all disagreements through in-person discussion in order to finalize the toxicity annotations. Ehsani et al.s dataset is based on 404 locked conversations (issues and PRs) on GitHub where the reason they were locked is listed as too heated, spam, or off-topic. These 404 conversation threads contain 5961 comments annotated with various categories of uncivil TBDFs (Tone Bearing Discussion Features). The definitions and examples of the incivility-related TBDFs are shown in Table 1. Since the focus of our study is to identify conversations that derail into toxicity, we excluded conversations in which toxicity occurred in the initial post (i.e., we cannot predict derailment without processing the initial post). This step resulted in dataset consisting of 175 toxic GitHub threads."
        },
        {
            "title": "3.2 Non-Toxic Conversations Dataset\nTo compare toxic conversations with ordinary, non-toxic GitHub\nissues and pull requests, we collected a sample of non-toxic dataset\nusing the toxic dataset as a reference point. Specifically, we gath-\nered 15 threads that were posted immediately before and 15 threads\nimmediately after each toxic thread within the same repository,\nmaintaining temporal and project-local continuity. We applied sev-\neral exclusion criteria, removing: 1) non-English conversations,\n2) threads with no comments after the initial post, and 3) locked\nthreads marked as \"resolved\" (i.e., locked for reasons unrelated to\ntoxic discourse).",
            "content": "From the collected posts, two annotators (authors of this paper) randomly selected posts one by one and verified their non-toxic nature, using the definition in section 3.1. The annotators continued this process until the number of non-toxic conversations reached total approximately matching the number of toxic conversations. During this procedure, they identified 23 threads as toxic and 207 as non-toxic. The final set of 207 non-toxic threads constitutes our NonToxic Conversations Dataset. The newly identified 23 toxic threads are combined with Ehsani et als dataset of 175 toxic conversations."
        },
        {
            "title": "3.3 Derailed Toxic Dataset\nSince we investigate on identifying derailed toxic threads, we fur-\nther refined our dataset by identifying the points at which conver-\nsations begin to derail. Specifically, we observed that instances of",
            "content": "Conference17, July 2017, Washington, DC, USA Mia Mohammad Imran, Robert Zita, Rahat Rizvi Rahman, Preetha Chatterjee, and Kostadin Damevski incivility preceding the toxic comments often indicate derailment, as they signal shift away from the original conversational intent [68]. Given that Ehsani et al. previously annotated incivility in conversations containing TBDFs (see Table 1), we hypothesize that any uncivil TBDF occurring before toxic comment marks potential derailment point. However, our investigation revealed that derailment can also arise outside of these TBDF categories, particularly in the presence of pronounced negative politeness strategies (e.g., Im terribly sorry, but, Would you mind) [14]. Therefore, we annotate the derailment points following the definitions provided in Section 2 and use TBDFs as guide whenever they were available. Two of the papers authors reviewed the 198 toxic threads (175 from Ehsani et al.s heated conversations dataset and 23 obtained while creating the Non-Toxic Conversations Dataset) to identify possible derailment points preceding toxic comments. Their interannotator agreement reached 0.914 (Cohens Kappa). The annotators determined that 34/198 threads exhibited \"sudden toxicity\", i.e., toxic threads do not exhibit derailment; instead, in these conversations toxicity occurs suddenly and unexpectedly. We excluded them from our analysis. The annotators discussed where they disagreed and resolved the disagreements. In 88 cases, they found multiple derailment points before the toxic comment. In 71 cases, annotators found exactly 1 derailment point. We also excluded 5 cases where the annotators could not agree whether there existed derailment point or not. Finally, we retained 159 toxic threads (142 from Ehsani et al.s and 17 newly identified threads from Section 3.2) with 382 derailment points, which form our Derailed Toxic Dataset."
        },
        {
            "title": "4 RQ1: What are the characteristics and",
            "content": "patterns of conversational derailment in GitHub discussions? Understanding how conversations derail on GitHub is fundamental to developing effective prediction systems. Relative to conversations on other online platforms such as Reddit and Wikipedia, GitHubs technical conversations are distinct in the way that they exhibit toxicity [43]. Research also noted that diffient communities exhibit different characteristics in toxic communication patterns [60]. Therefore, it stands to reason that conversational derailment on this platform may also be distinct. In this RQ, we empirically examine how conversational derailment manifests on GitHub. Specifically, we investigate: 1) the dynamics of conversational derailment by examining its timing and distance from the threads start, 2) the linguistic signals preceding derailment, 3) the presence of uncivil tones, and 4) common triggers that lead discussions off track. We base our analysis on the Derailed Toxic Dataset. While limited in size and predominantly sourced from locked as heated GitHub issues, this dataset is sufficient to observe derailment patterns. Locked issues are not concern, as the locking occurs after the derailment and toxicity have occurred, and the locking mechanism does not affect the communication pattern. In addition, for the empirical analysis in this section, as our focus is on the interaction patterns leading up to toxicity, for any discussion thread, we exclude the comments that occurred after the first toxic comment."
        },
        {
            "title": "4.1 Timing and Distance to Derailment Points\nWe calculate the median number of comments from the conver-\nsational first derailment point to the first toxic comment for each\nconversation thread in Derailed Toxic Dataset. In our toxic threads\ndataset, in terms of total comments, the median comment count\n11, and mean comment count 17.6. However, the median first toxic\ncomment occurrence position is 8 and mean is 12.03. And median\nfirst derailed comment occurrence position 4 and mean is 5.92.\nWhile, the median distance is 3 comments and a mean distance is\n6.10. The close proximity between derailment and toxic comments\nsuggests that once a thread derails, it is likely to directly devolve\ninto toxicity. This aligns with Cheng et al.‚Äôs findings, which indicate\nthat negative context and mood increase the likelihood of trolling\nbehavior [11].",
            "content": "The timing of the first toxic comment relative to the derailment point provides additional insights. Considering 8-hour workday, we observe about 46% (73/159) of the of toxic comments occur within 8 hours of the first derailment comment [9] and about 64% (102/159) occurs within 24 hours. This shows the importance of timely intervention. However, more than 25% cases (40/159), the difference is more than 7 days, which indicates toxicity can also occur after long period of derailment. This characteristic contrasts with platforms like Wikipedia, where discussion is likely inactive if the last comment was added 23 days ago [58]. Note that in the few cases where there are multiple derailment point comments preceding the toxic comment, for this analysis, we considered the first derailment point in the conversation."
        },
        {
            "title": "4.2 Linguistic Features\nIn Chang et al.‚Äôs study, participants noted that the easiest way\nto forecast conversational derailment is by analyzing user phras-\ning [10]. Indicators include the use of direct address such as ‚Äòyou‚Äô\ninstead of generic terms like ‚Äòall‚Äô or ‚Äòalways‚Äô, as well as certain\nrhetorical postures or argumentative patterns can signal a conver-\nsation may be turning toxic [10].",
            "content": "We analyze potential language indicative of derailment in GitHub discussions. In the 382 derailment point comments in the Derailed Toxic Dataset, we sampled the 200 most frequent unigrams, excluding articles, particles, and common prepositions. We intentionally retained negation and question terms because they are strong markers of argumentative or confrontational language in Toxicity Ahead: Forecasting Conversational Derailment on GitHub Conference17, July 2017, Washington, DC, USA Table 2: Lexical cues in derailment point comments. Statistical significance (Chi-square test) between derailment and regular comments is indicated by * (ùõº = 0.05). Linguistic Features Second Person Pronouns WH Question Words Negation terms Reasoning terms Emphasis terms Communication Verbs Derail (ùëõ = 382) 60.7% 57.1% 70.2% 70.4% 53.4% 33.5% Comment Type (%) Toxic (ùëõ = 159) 75.5% 59.7% 71.1% 70.4% 59.7% 36.5% Regular (ùëõ = 1,371) 43.9% 43.9% 55.3% 61.4% 42.5% 24.9% ùëù-value Cramers ùëâ < 0.0001 < 0.0001 < 0.0001 < 0.0001 < 0.0001 < 0.0001 0.127 0.104 0.132 0.055 0.123 0.133 this context [32, 36, 61, 70]. Two annotators collaboratively categorized (see Table 2) the unigrams into linguistic groups using the card sorting method [59]. They met in person, discussed, and resolved differences, consulting dictionary as needed. Based on these categories, we counted the frequency of each unigram in the derailment point comments after applying basic preprocessing steps (e.g., tokenization and lemmatization). Table 2 shows the percentages of occurrence in derailment points (382 count) along with the first toxic comments (159 count), and regular comments (non toxic and non derailment point comments); the total regular comments count were 1,371. We observed that in derailment points the elevated use of second person pronouns (you, your, etc) [38], negation terms (not, no, etc.), WH questions (what, why, how, where, etc.), reasoning terms (because, since, etc.), communication verbs (say, comment, tell, etc.), and emphasis terms (actually, really, etc.) than general comments but lower than toxic comments. As Table 2 shows, all lexical differences between derailment and regular comments were statistically significant under Chisquare test of independence [41] (ùúí 2, ùëù < 0.05) after applying the BenjaminiHochberg (BH) correction [8]. Effect sizes measured by Cramers ùëâ (0.050.13) indicate small to moderate associations, confirming consistent but modest linguistic distinctions. Derailment point comments frequently combine structured reasoning with mild confrontation. Reasoning terms dominate (70.4%), often paired with negation (70.2%) and direct questioning (57.1%), reflecting logical yet oppositional exchanges. Compared with regular comments, derailment points show +16.8% more second-person targeting, +14.9% more negation, and +13.2% more questioning. Although they share similar reasoning intensity with toxic comments (70.4% vs. 70.4%), they contain less personal confrontation (60.7% vs. 75.5%) and reduced emphasis (53.4% vs. 59.7%). These findings suggest that derailment often emerges through argumentative yet non-abusive phrasing, where discussions shift from logical disagreement toward personal conflict."
        },
        {
            "title": "4.3 Incivility TBDFs in Derailment Points\nThe tone of the comments is useful feature for proactive modera-\ntion. For example, moderators on Wikipedia assess tone to predict\npotential derailment [58]. Since the majority of the toxic conver-\nsations (142/159) in the Derailed Toxic Dataset came from Ehsani\net al.‚Äôs dataset, the comments in these conversations already have\nincivility-related tones annotated. The Tone Bearing Discussion\nFeatures show the type of incivility at derailment point comments;",
            "content": "Table 3: Top TBDF categories in derailment point comments (142 toxic threads from Ehsani et al.). TBDF Category Bitter Frustration Impatience Mocking Insulting Derail. Pt. Cmts. (362) 155 (42.82%) 82 (22.65%) 36 (9.94%) 21 (5.80%) Toxic Cmts. (142) 35 (24.65%) 13 (9.15%) 17 (11.97%) 36 (25.35%) we found 362 such annotated comments. For this analysis, we considered only those 362 derailed comments. Table 3 show the percentages of TBDFs in derailment comments and toxic comments. The major uncivil TBDFs are: Bitter Frustration: 42.82% (162/362), Impatience: 22.65% (84/362), and Mocking: 9.94% (39/362). For comparison, these same TBDFs occurred in toxic comments at the rates of 24.65% (35/142), 9.15% (13/142), and 11.97% (17/142), respectively. Notably, while Insulting and Vulgarity are more prominent in toxic comments 25.35% and 9.86% respectively, they are less frequent in derailment points, occurring at 5.80% and 2.49%. This contrast indicates that certain forms of incivility such as Bitter Frustration and Impatience are more predictive of conversational derailment than direct toxicity. It also highlights progression from subtle incivility to overt toxicity, and reinforces the importance of early signals such as frustration and impatience in anticipating derailment."
        },
        {
            "title": "4.4 Derailment Triggers\nUnderstanding what causes a conversation to derail can inform\nthe development of effective early intervention strategies using\nautomated, algorithmic approaches [57, 58]. Prior research in soft-\nware engineering has identified potential triggers of toxicity in OSS\ndiscussions [17, 18, 43]. Ehsani et al. [16] proposed a guideline for\nannotating incivility triggers.",
            "content": "Building on their methodology, two authors independently annotated derailment triggers in our dataset, focusing on specific conversational or contextual elements that precipitated the initial shift. Although much of our toxic data overlaps with that of Ehsani et al., we chose to annotate derailment triggers with particular attention to the first derailment point in each conversation. This distinction was necessary because it was unclear from Ehsani et al.s description whether their annotations considered the conversation holistically or targeted only the first uncivil comment when conversations started to go off-track. Conference17, July 2017, Washington, DC, USA Mia Mohammad Imran, Robert Zita, Rahat Rizvi Rahman, Preetha Chatterjee, and Kostadin Damevski The annotation achieved Cohens Kappa score of 0.84, indicating strong inter-annotator agreement. Disagreements were resolved through discussion to ensure full consensus. The most prevalent trigger was Failed Use of Tool/Code or Error Messages followed at 23.27% (37/159), where tool difficulties or bug troubleshooting led to derailment. For example: [CODE SNIPPET] ... What more proof do you need? That is everything.\" The tension was caused here due to code error, which the user expressed in Frustrated tones. The conversation later evolved into toxicity. The second most prevalent derailment trigger was Technical Disagreement followed at 20.12% (32/159), where tool difficulties or bug troubleshooting led to derailment. For instance: [CODE SNIPPET] Ask yourself what **intention** it expresses. This is some kind of esoteric gibberish without reference to the subject area. [...]\". In this case, the disagreements about method naming derails the conversation and later the conversation escalated to toxicity. Another major category was Communication Breakdown, which accounted for 16.98% (27/159) of cases. This included misunderstandings, misinterpretations, typos, or language barriers causing perceived hostility. For example, It is impolite to assume that each user opening an issue is stupid and lazy. Of course, search the issue tracker. [...]\" Here, misunderstanding between the commenters triggered conversation derailment."
        },
        {
            "title": "5.2 LLM Prompt Design\nWe design a two step LLM prompting procedure for GitHub derail-\nment prediction:",
            "content": "Step 1 GitHub-Specific SCD Generation: We convert the raw GitHub conversation into high-level summary that captures interaction dynamics, emotional tone, and discourse strategies, excluding technical specifics. Step 2 Derailment Prediction: We estimate the probability of derailment based solely on the summary, using simple scalar prediction prompt, i.e., prompt that instructs the LLM to produce single numerical value, specifically, probability between 0 and 1, representing the likelihood that conversation will derail into toxicity. This prompt design separates the reasoning and classification stages, enabling us to build explainability into the pipeline and reducing the reasoning demands on the LLM."
        },
        {
            "title": "5.2.1 GitHub-Specific SCD Prompt. We adapt Hua et al. [26]‚Äôs\nfew-shot SCD prompt to GitHub‚Äôs conversational style and dynam-\nics. Specifically, we replace general conversation examples with\ndomain-specific interactions, such as discussions around pull re-\nquest rejections or issue closures. Additionally, we instruct the\nmodel to ignore technical details like code snippets or file paths,\nand instead focus on conversational dynamics, i.e., how partici-\npants respond to one another, where misunderstandings arise, and\nhow tone shifts over time. A typical summary generated using this\nGitHub-adapted SCD prompt is as follows:",
            "content": "Multiple users debate reverting recent PR. Speaker1 expresses strong opposition, referencing previous incident involving similar code. Speaker2 challenges Speaker1s framing and accuses them of misrepresenting past decisions. Speaker3 supports Speaker2 and notes that Speaker1 had already raised this concern in another thread. Speaker1 becomes confrontational, citing perceived pattern of dismissal. The conversation becomes increasingly heated as past interactions are used to question motives. The tone escalates, with limited signs of resolution. While this adaptation yielded clearer and more relevant summaries, we hypothesized that it may not yet be optimal. As Hua et al. developed SCD prompts targeting general-purpose conversations, it may not be most effective for the highly technical discussions found on GitHub. We explored whether decomposing the problem [35] and integrating the properties of GitHub derailed conversations, uncovered in Section 4, could yield better SCDs for predicting derailment on GitHub. Previous research shows that decomposing the prompts into incremental steps enhances the LLMs accuracy [35, 69]. Inspired by those studies, we adopted Least-to-Most (LtM) prompting strategy [69], and designed step-wise summarization prompt that guides the model from high-level observations to detect smaller breakdowns in conversation patterns. This allows us to integrate our insights from Section 4. We incorporated the following key components into the LtM prompt (as Steps 36): (1) Individual Intentions (II), feature we directly integrated from Hua et al.s framework to analyze participants motivations and goals. Morrill et al. similarly found that dialogue is shaped by intentions such as agreement, disagreement, confrontation [45]. Additionally, studies have shown that communication styles play role in shaping interactions within GitHub discussions [7, 63]. Toxicity Ahead: Forecasting Conversational Derailment on GitHub Conference17, July 2017, Washington, DC, USA (2) Conversational Features (CF) (e.g., questioning, rhetoric), where we adopted the categories established by Hua et al. [26], which includes rhetorical questions, hedging, questioning logic, and other linguistic patterns. This approach aligns with our findings described in Section 4.2; (3) Sentiment and Tonal Features (STF), which capture emotional dynamics and shifts throughout the discussion, enabling us to track how sentiment evolves before derailment occurs, as discussed in Section 4.3; and (4) Tension Triggers (TT), which identify potential catalysts for escalating conflict that serve as early indicators of possible derailment, based on our findings described in Section 4.4; We explicitly instruct the model to exclude technical content and focus on interactional dynamics. The summary is synthesized at the final step. In developing the LtM prompt, we initially experimented with prompts that included explicit definitions for each reasoning step, for instance, describing categories of tension triggers, conversation strategies or specifying tonal cues like sarcasm or frustration. However, we observed that such definitions often introduced unnecessary verbosity and led to inconsistent outputs in SCD generation. In contrast, we found that more concise, open-ended prompt, omitting explicit definitions, led to more consistent and focused outputs. Rather than prescribing rigid categories, this design allowed the model to apply its learned understanding of conversational structure. The final LtM prompt is as follows: Least-to-Most (LtM) SCD Generator Prompt You are skilled Conversation Analyst specializing in GitHub discussions. Your objective is to capture the conversation dynamics without getting caught in the technical details. Your Analysis Method: Follow these steps in order, building your understanding from basic patterns to complex dynamics: Step 1: Identify Main Elements: Identify the main elements by quickly scanning the conversation and pinpointing the key components or topics being discussed. Step 2: Enforce Exclusion Criteria Do NOT include: - Any technical claims, arguments, or explanations - Any code names, module names, or PR details - Any direct or indirect quotations - Any mention of what was being implemented or reviewed Example 1 (Excluded): \"The discussion addressed discrepancies in ùëíùë£ùëéùëôùë¢ùëéùë°ùëí_ùëéùëëùëöùëñùë†ùë†ùëñùëúùëõùë† () outputs [...].\" Step 3: Note Individual Intentions Infer what each participant is aiming to achieve. Example: [...] Step 4: Identify Conversation Strategies Identify rhetorical or structural tactics used by each speaker. Example: [...] Step 5: Track Sentiment Evolution Describe how the tone shifts across the conversation. Use descriptors such as neutral, guarded, dismissive, confrontational, collaborative, defensive, escalating. Avoid inference. [...] Step 6: Find Tension Triggers Detect explicit shifts in tone, rhythm, or interaction style that signal rising tensiononly when clearly supported by observable evidence. Avoid inference. [...] Step 7: Synthesize Trajectory Summary Combine insights from steps 1-6 to create concise trajectory summary. Remember: Your value lies in revealing the human dynamics beneath technical discussions. Focus on HOW people communicate, not WHAT technical points they make. Example: \"Multiple users debate reverting recent PR.[...]\" Your Task: Using the above guideline, write the final trajectory summary for the following GitHub discussion transcript: < ùëñùëõùë†ùëíùëüùë° ùëêùëúùëõùë£ùëíùëüùë†ùëéùë°ùëñùëúùëõ ùë°ùëüùëéùëõùë†ùëêùëüùëñùëùùë° > Write only the final summary within double quotation. Here is an example SCD summary generated using this prompt: USER_01 initiates discussion about addressing deprecation warnings in project, suggesting bumping the version number as solution. USER_02 and USER_03 express reluctance to create new major version, leading USER_01 to interpret this as lack of future planning for the project. The conversation shifts to more confrontational tone when USER_02 suggests an alternative fix and sarcastically requests an apology from USER_01, which escalates USER_01s response into threat of ceasing contributions and blocking participants. USER_02 attempts to de-escalate by reiterating the availability of an alternative solution but maintains guarded stance."
        },
        {
            "title": "Derailment Predictor Prompt",
            "content": "You will be provided with summary of GitHub conversation trajectory. Conversation Trajectory Summary captures the evolution of discussion by identifying key elements, triggers of interaction patterns, sentiments, and tonal shifts among participants. It provides concise overview of significant dynamics and tone changes without detailing the technical aspects or specific arguments. tension, conversational strategies, Now based on the given GitHub conversation trajectory summary, what is the probability that the conversation may derail and become toxic on scale of 0 to 1 (Toxicity is defined by any rude, disrespectful, or unreasonable comment that is likely to make one leave discussion.)? Conference17, July 2017, Washington, DC, USA Mia Mohammad Imran, Robert Zita, Rahat Rizvi Rahman, Preetha Chatterjee, and Kostadin Damevski Conversation trajectory summary: < ùëñùëõùë†ùëíùëüùë° ùë†ùë¢ùëöùëöùëéùëü ùë¶ > Write exactly one word: the probability rounded to two decimal places. This two-step prompting process allows us to first generate comprehensive summary of the conversation and then use that summary to make more informed prediction about the potential for toxicity."
        },
        {
            "title": "5.3 Experiment Setup\n5.3.1 Large Language Models. We conduct all of our experiments\nusing two publicly available LLMs: Llama (llama-3.3-70B version)\nand Qwen (qwen2.5:32b-instruct version) model, as they are among\nthe top-performing open-weight state-of-the-art LLMs at the time\nof conducting this study. We specifically chose freely available\nLLMs and avoided proprietary models (e.g, GPT-4 or Claude) to\nimprove reproducibility and because the cost of paid models may\nmake them impractical for real-world deployment.",
            "content": "We set the model temperature to 0 to minimize output variance and set uniform context window size of 32k (maximum length supported by Qwen 2.5 version). For each toxic conversation in our dataset, we provide all the comments up to, but excluding, the first toxic comment. Formally, let ùê∂ = {ùëê1, ùëê2, . . . , ùëêùëõ } be the ordered set of comments in GitHub conversation, and let ùëêùë° be the first toxic comment such that: ùë° = min{ ùëñ ùëêùëñ is labeled toxic }, then, the model input is: ùê∂ = {ùëê1, ùëê2, . . . , ùëêùë° 1}. Thus, the model only sees conversation context before toxicity emerges. It does not have access to the toxic comments themselves (and any comments afterwards). This formulation follows the forecasting prediction setup used by Chang et al. [9]. Whenever conversation exceeded the context window limit, we truncated it by removing utterances from the beginning until the total length fell within the allowable range."
        },
        {
            "title": "5.3.2 Metrics. We compare three popular metrics:",
            "content": "Precision refers to the proportion of true positive observations among all the predicted positive observations: Precision = True Positive True Positive + False Positive . Recall represents the proportion of true positive observations out of all actual positive observations: Recall = True Positive True Positive + False Negative . The F1-score is the harmonic mean of Precision and Recall: . F1-score = 2 Precision * Recall Precision + Recall"
        },
        {
            "title": "5.4 Results and Discussion\nWe conduct experiments using our dataset of 159 derailed toxic\nthreads and 207 non-toxic threads, a total of 367 data points. Of\nthese, none of the threads exceeded the context window limit.\nChang et al. showed that the prediction decision threshold (i.e.,\nprobability cutoffs) can vary widely for different datasets in the\nconversational derailment task [9]. Since we asked the LLMs to\nprovide a prediction score of derailment between 0 to 1, we include",
            "content": "Table 4: Derailment prediction results for different models on Derailed Dataset and Non-Toxic Dataset. (SCD = Summaries of Conversation Dynamics, ùúÉ = ùëá‚Ñéùëüùëíùë†‚Ñéùëúùëôùëë). Model Strategy CRAFT [9] Qwen 2.5:32B Instruct Llama 3.3:70B - - - - Hua et al. FewShot SCD [26] Least-toMost SCD Hua et al. FewShot SCD [26] Least-toMost SCD ùúÉ () 0.1 0.3 0.5 0. 0.1 0.3 0.5 0.7 0.1 0.3 0.5 0.7 0.1 0.3 0.5 0.7 0.1 0.3 0.5 0.7 Precision Recall F1 0.419 0.425 0.585 0.764 0.440 0.980 1.000 1.000 0.443 0.945 0.987 0.981 0.750 0.913 0.929 0.983 0.702 0.890 0.932 0.975 0.937 0.912 0.522 0. 1.000 0.604 0.258 0.164 1.000 0.862 0.478 0.327 0.981 0.723 0.572 0.358 0.962 0.818 0.686 0.491 0.579 0.580 0.551 0.476 0.612 0.747 0.410 0.281 0.614 0.901 0.644 0.491 0.850 0.807 0.708 0.525 0.812 0.852 0.790 0.653 results from different thresholds, ùúÉ {0.1, 0.3, 0.5, 0.7}. The results of this experiment are shown in Table 4. Our Least-to-Most SCD prompting strategy demonstrates strong performance across both models, achieving F1-scores of 0.901 (Qwen) and 0.852 (Llama) at the 0.3 threshold. The approach maintains balanced performance with high precision (0.945 for Qwen, 0.890 for Llama) while preserving good recall (0.862 for Qwen, 0.818 for Llama). Compared to the two baselines, our method significantly outperforms CRAFT, which consistently underperformed across all thresholds with best F1-score at 0.580. Against Hua et al.s SCD prompting strategy, our approach shows superior results at most thresholds, particularly excelling at the practical 0.3-0.7 range where balanced precision-recall trade-offs are crucial [10]. For the Qwen model, the Least-to-Most strategy achieved the highest F1-scores across all threshold values. similar trend was observed for the Llama model, with the exception of the 0.1 threshold where Hua et al.s strategy performed better due to very high recall (156/159 correct predictions) but at the cost of much lower precision. This trade-off highlights an important consideration: while high recall captures most toxic instances, low precision leads to excessive false positives. In GitHub repositories where non-toxic conversations vastly outnumber toxic ones, maintaining high precision is critical to avoid unnecessary false alarms [51]. However, the high precision should not come at the cost of low precision as Chang et al. noted that balancing precision and recall is crucial, as too many false positives reduce tools helpfulness while too many false negatives reduce effectiveness [10]. At higher thresholds ( 0.3), our Least-to-Most strategy ensures good F1-scores with high precision, making it practically viable for deployment. Based on these results, we envision threshold-based intervention strategy to mitigate toxicity: higher thresholds (e.g., ùúÉ > 0.7) Toxicity Ahead: Forecasting Conversational Derailment on GitHub Conference17, July 2017, Washington, DC, USA could alert moderators to review flagged content, while lower thresholds (e.g., ùúÉ = 0.3 ùë°ùëú 0.7) could trigger automated bots to issue reminders promoting civil discourse."
        },
        {
            "title": "5.5 Error Analysis",
            "content": "To better understand the performance and limitations of the model and datasets, we conduct an error analysis. We limited the error analysis to the best-performing configuration, i.e., the LtM SCD prompting strategy on the Qwen model at 0.3 threshold. We conducted an open coding process to analyze the errors [65]. Two authors independently reviewed all misclassified cases and assigned preliminary labels to them. They then met to compare their labels, resolve disagreements, and refine the categories through iterative discussion. This process was repeated until complete consensus was achieved. There were two types of error categories: 1) 8 cases where the model predicted non-toxic conversations as derailing; and 2) 22 cases where the model predicted derailed toxic conversations as non-derailing. Two authors reviewed the conversations, examined the generated SCD, and determined the most likely reason for the error. They finalized the error categories using open coding, and further improved them thorough discussion and applying axial coding [1], with some cases belonging to more than one category. In the 8 false positives, the main issues were the model overestimating tension in otherwise civil exchanges (3 cases), and situations where the SCD was accurate but the predictor misjudged the tones seriousness with respect to toxicity (3 cases). Among the 22 false negatives, most errors were due to missing or underestimating subtle toxic signals like frustration (10 cases), failure to detect sarcasm or nuanced tones (3 cases), accurate SCDs with flawed predictor judgment (3 cases), and cases where toxicity occurred long after derailment (3 cases), reducing the perceived severity. To illustrate, consider the following SCD where the model overestimated the seriousness of the tone as confrontational when the commentators were sharing their perspective, Participants discuss changes to game mechanics involving Repair Facility and Heavy Repair Turrets (HRT). [... ] The conversation shifts from neutral to confrontational as participants assert their viewpoints and express dissatisfaction with proposed changes.. In reality, participants were simply exchanging perspectives without hostility. In another case, the model missed sarcasm about image quality: Two users discuss an image and JSON file issue. [...] reiterating concerns about image quality, the tone remains critical but not confrontational, focusing on clarity and quality standards. Here, subtle humor and sarcasm were misinterpreted as neutral critique, highlighting the models difficulty with nuanced tones. These findings highlight specific weaknesses in both tone interpretation and the alignment between SCD generation and downstream prediction."
        },
        {
            "title": "5.6 Ablation Study\nTo better understand the contribution of individual semantic com-\nponents in our summarization prompt, we conducted an ablation\nstudy focused on our Least-to-Most (LtM) derailment prediction.\nAs introduced in Section 5.2, the LtM prompt integrates multiple\nhigh-level conversational features: Sentiment and Tonal Features",
            "content": "Table 5: Ablation Study on our Least-to-Most (LtM) Derailment Prediction Strategy using Qwen and ùúÉ = 0.3. Ablation No II No CF No STF No TT Full LtM Precision (Œî) 0.962 (+1.8%) 0.941 (-0.4%) 0.898 (-5.0%) 0.906 (-4.1%) 0.945 Recall (Œî) 0.792 (-8.1%) 0.805 (-6.6%) 0.774 (-10.2%) 0.786 (-8.8%) 0.861 F1 (Œî) 0.869 (-3.6%) 0.868 (-3.7%) 0.831 (-7.8%) 0.842 (-6.5%) 0. ùëù ùë£ùëéùëôùë¢ùëí (Significant) 0.2153 () 0.1628 () 0.0032 () 0.0190 () (STF), Individual Intentions (II), Conversation Features (CF), and Tention Triggers (TT). Through the ablation study, we aim to quantify the effect of removing specific components in shaping the predictive utility of the summaries. We evaluate ablations exclusively on the Qwen model at ùúÉ = 0.3 threshold because it achieved the best performance in our experiments."
        },
        {
            "title": "5.6.1 Prompt Modifications Per Component. Each ablation removes\none semantic component from the original LtM prompt:",
            "content": "Removed II (LtM Prompt Step 3: Note Individual Intentions); renumbered subsequent steps. Removed CF (LtM Prompt Step 4: Identify Conversation Strategies); renumbered subsequent steps. Removed STF (LtM Prompt Step 5: Track Sentiment Evolution); renumbered subsequent steps. Removed TT (LtM Prompt Step 6: Find Tension Triggers); renumbered subsequent steps. All other steps and examples were preserved to maintain structural consistency. The goal was to generate SCDs comparable to those from the baseline strategy. The full ablation prompts are included in the replication package. In addition to precision, recall and F1-score, we also perform statistical significance test by employing McNemars test [42]. To control for multiple comparisons, we applied the Benjamini-Hochberg (BH) correction [8]."
        },
        {
            "title": "5.6.2 Results. Table 5 presents the precision, recall, F1-score, and\nBH corrected ùëù-values from McNemar‚Äôs test for each feature abla-\ntion. McNemar‚Äôs test evaluates whether differences in model pre-\ndictions are statistically significant under paired comparisons. A\nùëù-value below 0.05 denotes a statistically significant change in pre-\ndictions.",
            "content": "Ablating the STF (sentiment and tone features) component results in the largest F1-score reduction (-7.8%), primarily due to -10.2% drop in recall and -5.0% decrease in precision. This change is statistically significant (ùëù = 0.0032), indicating that STF features play critical role in recall-oriented classification performance. Removing TT (tension triggers) leads to -6.5% F1 decline, driven by an -8.8% reduction in recall and -4.1% drop in precision. The prediction shift is statistically significant (ùëù = 0.0190), confirming the contribution of trigger features to predictive performance. Ablating II results in smaller F1 decrease (-3.6%), with minor gain in precision (+1.8%) and larger drop in recall (-8.1%). The BH-corrected ùëù-value (ùëù = 0.2153) does not indicate statistical significance. Likewise, removal of the CF feature yields 3.7% F1 decrease, with minimal change in precision (-0.4%) and -6.6% drop in recall; this change is also not statistically significant. Conference17, July 2017, Washington, DC, USA Mia Mohammad Imran, Robert Zita, Rahat Rizvi Rahman, Preetha Chatterjee, and Kostadin Damevski Overall, the results show that STF and TT features have the strongest and statistically significant impacts on out LtM prompted model performance, particularly in preserving coverage (recall)."
        },
        {
            "title": "6 RQ3: To what extent does the proposed",
            "content": "LLM-based derailment prediction approach generalize to independent GitHub datasets? In order to validate our approachs generalizability, we evaluate it on Raman et al.s [51] publicly available dataset [37], which is independent of our curated data and follows different annotation procedure."
        },
        {
            "title": "6.1 Dataset and Experimental Setup\nAs introduced in Section 3.4, the dataset comprises 168 toxic and 444\nnon-toxic threads. However, we found 314 threads have comment-\nlevel annotations available in their replication package. Since, eval-\nuating the conversational derailment prediction requires comment-\nlevel annotation to find the exact toxic comment location, we fil-\ntered those 314 conversations. We further filter 6 conversations\nwhere toxicity observed at first comment. Therefore, we end up\nwith 308 GitHub issue threads (65 toxic, 243 non-toxic). We apply\nthe same preprocessing and modeling pipeline described in Section\n5.3 to ensure comparability across datasets. Note that the dataset\nmay possibly include sudden toxic conversations. We have not\nverified them manually.",
            "content": "As before, we evaluated the two prompt based techniques on this dataset using the Llama (Llama-3.3:70B-3.3:70B version) and Qwen (Qwen-2.5:32B-Instruct version) model, setting the thresholds: ùúÉ {0.1, 0.3, 0.5, 0.7}. We report Precision, Recall, and F1-score as before for each setting."
        },
        {
            "title": "6.2 Results and Discussion\nThe results are presented in Table 6. The Qwen model outperformed\nLlama on this benchmark, consistent with trends observed in our\ncurated dataset. The LtM SCD strategy achieved the highest F1-\nscore of 0.797 at a threshold of 0.3. For the Llama model, the same\nstrategy yielded the best F1-score of 0.776, at a threshold of 0.5.",
            "content": "These findings reaffirm the effectiveness of the LtM prompting strategy over the baseline Hua et al. SCD few-shot prompt in detecting early conversational derailment on GitHub. While Qwen produced stronger results overall, Llama exhibited more stable performance across thresholds. Additionally, lower thresholds increased recall at the expense of precision, highlighting the importance of threshold calibration in real-world moderation settings."
        },
        {
            "title": "7 Recommendations\nThe results indicate several practical directions for improving proac-\ntive moderation in open-source communities, particularly on GitHub.\nBased on empirical observations and model behavior across datasets,\nwe outline recommendations for two groups: (1) researchers study-\ning conversational derailment, and (2) GitHub maintainers involved\nin community moderation.\nRecommendations for GitHub Maintainers. GitHub repository\nmaintainers can integrate LLM-based early warning systems into\nexisting moderation pipelines. Summarization-driven derailment",
            "content": "Table 6: Derailment prediction results for different models on Raman et al.s dataset. Model Strategy Qwen Llama Hua et al. FewShot SCD Least-toMost Strategy Hua et al. FewShot SCD Least-toMost Strategy ùúÉ 0.1 0.3 0.5 0.7 0.1 0.3 0.5 0.7 0.1 0.3 0.5 0.7 0.1 0.3 0.5 0.7 Precision Recall F1 0.280 0.807 0.875 0.955 0.236 0.753 0.816 0.857 0.594 0.746 0.804 0.853 0.513 0.659 0.754 0.804 1.000 0.708 0.431 0.323 1.000 0.846 0.615 0.369 0.877 0.723 0.631 0.446 0.908 0.831 0.800 0.692 0.438 0.754 0.577 0.483 0.382 0.797 0.702 0.516 0.708 0.734 0.707 0.586 0.656 0.735 0.776 0.744 detection provides lightweight mechanism to flag discussions that may require attention, enabling timely intervention without exhaustive manual review. Intervention thresholds should be calibrated according to moderation goals. Higher thresholds (e.g., > 0.7) are suited for passive alerts aimed at de-escalation [48], while intermediate thresholds (e.g., 0.30.7) can trigger automated reminders that act as conversational mediators [58]. This can be integrating within existing GitHubs infrastructure. Building on GitHubs current tagging options (e.g., abuse, off-topic, resolved) for [22], dedicated derailment tag could streamline moderator review and response. Maintainers could incorporate the models SCDs into issue and pull request dashboards to identify threads showing early signs of tension. Integration frequency should align with repository activity and cost constraints. Since 64% of toxic exhanges occur within 24 hours of derailment, higher-frequency runs (e.g., hourly) are most useful for new or rapidly evolving threads, whereas running the model after each new comment suffices for slower discussions. The SCDs provide interpretable summaries that can help maintainers make informed moderation decisions. Recommendations for Researchers. Future research should build upon the current study by refining prompt designs to capture conversational signals of derailment more consistently across contexts. While this work has demonstrated the value of modeling tonal shifts, tension triggers, and sentiment trajectories, further efforts are needed to generalize these features across diverse OSS communities, languages, and moderation norms. Refinement should also aim to reduce prompt sensitivity and improve reproducibility of LLM-based summarization across datasets. Developing standardized benchmarks across multiple platforms, such as GitHub and BugZilla, would enable consistent evaluation and cross-comparison of models. These benchmarks should include annotated derailment points, incivility categories, and moderation outcomes. Efficiency and scalability remain key challenges. Incrementally updating summaries with new comments, rather than reprocessing Toxicity Ahead: Forecasting Conversational Derailment on GitHub Conference17, July 2017, Washington, DC, USA entire threads, could allow nearreal-time assessment with reduced computational cost. Beyond accuracy, researchers should evaluate latency, resource trade-offs, and feedback dynamics between predictive models and human moderators. Transparency and explainability should remain research priorities. Future work should further refine how SCD convey rationales for tension and escalation, improving trust and accountability [58]."
        },
        {
            "title": "8 Related Work\nOur work builds upon and extends previous research in two main\nareas: toxicity analysis in software engineering and conversational\nderailment prediction.\nToxicity analysis in SE artifacts. A large body of work has ex-\nplored negative communication in developer-facing platforms such\nas GitHub, Stack Overflow, and code review tools. Studies have\nexamined incivility [16, 19, 20], emotional tone [28, 29, 47, 49],\ntoxicity [13, 51, 54, 55] and their effects on contributor engage-\nment [30, 43, 50, 62].",
            "content": "Several investigations have examined the consequences of toxicity. For example, toxic interactions have been linked to developer stress and increased dropout rates [51]. Other work has explored moderation strategies, the role of bots, and the benefits of early detection tools in mitigating toxic dynamics [25, 43]. Additionally, analysis of locked discussions has provided insights into recurring patterns of incivility and contributed key datasets [16, 18]. Researchers have developed tools to automate toxicity detection in software engineering [44, 51, 56]. However, all those tools are post-hoc, addressing toxicity only after it has occurred. Our work builds on this line by proposing proactive detection strategy, shifting from retrospective classification to early forecasting of derailment events that precede toxicity. Conversation derailment. Predictive studies of derailment have largely centered around Wikipedia and Reddit [5, 9, 26, 38, 40, 68]. The CRAFT model [9] pioneered early detection of online toxicity via linguistic and structural features. Hua et al. [26] introduced the concept of Summaries of Conversation Dynamics (SCD) for forecasting conversational trajectory, foundation we extend using GitHub-specific dynamics. Recent work also investigates hierarchical transformers, neural network, user behavior modeling, and contextual features such as reply structure and edit markers [3, 4, 15, 24, 34, 40, 66]. However, these approaches often lack domain adaptation for technical forums like GitHub, where toxicity emerges through more subtle signals like entitlement or miscommunication [25, 43]. Our contribution lies in integrating LLM-driven strategy with structured prompting tailored to technical platform like GitHubs, providing both performance and explainability in derailment forecasting."
        },
        {
            "title": "9 Threats to Validity\nWe note potential threats to the validity of our study in the following\ncategories: construct validity, internal validity, and external validity.\nConstruct validity. Construct validity concerns whether our method-\nology accurately captures the concept of derailment as a precursor\nto toxicity. A key limitation is that not all toxic behavior emerges",
            "content": "gradually; some instances arise abruptly without prior conversational signals, making them undetectable by our approach. Additionally, derailment is annotated based on human judgment, which may vary across annotators. LLM-generated SCDs can also introduce hallucinations or overlook subtle shifts in tone. To mitigate these risks, we used detailed annotation guidelines, ensured high inter-annotator agreement, and conducted error analyses to identify common misclassifications. Internal validity. Internal validity relates to whether the results are attributable to our method rather than uncontrolled factors. Our use of LLMs introduces variability due to stochastic generation and prompt sensitivity; small changes in input can affect model outputs. Moreover, although we followed structured annotation process, human error or bias may still influence labeling consistency. We addressed these issues by employing model-in-the-loop framework, cross-checking annotations, and designing prompts systematically to minimize ambiguity. External validity. External validity reflects the generalization of our findings beyond the specific dataset and setting. Our dataset is limited to GitHub issue threads and may not generalize to other platforms such as JIRA, GitLab, or non-OSS communities, which have different conversational norms. Additionally, the curated dataset is relatively small, potentially limiting applicability across all GitHub communities. Furthermore, because many public GitHub discussions are part of the web-scale data used in LLM pretraining, it is possible that some threads in our dataset overlap with or resemble the models pretraining data. This potential exposure may slightly inflate performance estimates, underscoring the need to validate the framework on data from unseen platforms or domains. While our prompting framework is domain-agnostic and we have evaluated on different dataset, broader validation is needed to confirm its wider applicability."
        },
        {
            "title": "10 Conclusion\nWe present a proactive approach to forecast toxicity detection in\nOpen Source Software communities through early identification of\nconversational derailment on GitHub. We annotated and analyzed\na dataset of 159 derailed toxic conversations and 207 non-toxic\nconversations. We developed a novel LLM-based prompt using\nLeast-to-Most strategy to generate Summaries of Conversation Dy-\nnamics and predict conversation derailment, achieving F1-scores of\n0.901 with Qwen and 0.852 with Llama, significantly outperform-\ning established baselines. External validation on an independent\nimbalanced dataset yielded F1-scores up to 0.797, demonstrating\ngeneralizability.",
            "content": "Our findings show that early derailment prediction is feasible and effective strategy for moderating technical discussions. The explainable nature of our SCD-based approach enables transparent moderation decisions and flexible threshold-based interventions, allowing communities to shift from reactive to proactive moderation strategies. While our work has limitations regarding platform specificity and detection of sudden toxicity without warning signs, it provides the empirical foundation and practical tools necessary for implementing early warning systems in real-world OSS projects. Future work should extend to other platforms, incorporate behavioral signals, and evaluate live deployment and intervention outcomes Conference17, July 2017, Washington, DC, USA Mia Mohammad Imran, Robert Zita, Rahat Rizvi Rahman, Preetha Chatterjee, and Kostadin Damevski to support healthier, more inclusive OSS communities. Another key direction can be generating human-written SCD in GitHub conversations and do instruct tune LLMs to mitigate the errors we observe at Section 5.5 so that the LLMs can do better SCD generation. References [1] 2006. Constructing grounded theory: practical guide through qualitative analysis. sage. [2] 2025. Replication Data. doi:10.5281/zenodo.15723482 [3] Enas Altarawneh, Ameeta Agrawal, Michael Jenkin, and Manos Papagelis. 2023. Conversation Derailment Forecasting with Graph Convolutional Networks. In The 7th Workshop on Online Abuse and Harms. [4] Atijit Anuchitanukul, Julia Ive, and Lucia Specia. 2022. Revisiting contextual toxicity detection in conversations. ACM Journal of Data and Information Quality (2022). [5] Jiajun Bao, Junjie Wu, Yiming Zhang, Eshwar Chandrasekharan, and David Jurgens. 2021. Conversations gone alright: Quantifying and predicting prosocial outcomes in online conversations. In Proceedings of the Web Conference 2021. [6] Max Bartolo, Alastair Roberts, Johannes Welbl, Sebastian Riedel, and Pontus Stenetorp. 2020. Beat the AI: Investigating adversarial human annotation for reading comprehension. Transactions of the Association for Computational Linguistics (2020). [7] Mohamed Amine Batoun, Ka Lai Yung, Yuan Tian, and Mohammed Sayagh. 2023. An empirical study on GitHub pull requests reactions. ACM Transactions on Software Engineering and Methodology 32, 6 (2023), 135. [8] Yoav Benjamini and Yosef Hochberg. 1995. Controlling the false discovery rate: practical and powerful approach to multiple testing. Journal of the Royal statistical society: series (Methodological) 57, 1 (1995), 289300. [9] Jonathan Chang and Cristian Danescu-Niculescu-Mizil. 2019. Trouble on the Horizon: Forecasting the Derailment of Online Conversations as they Develop. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. [10] Jonathan Chang, Charlotte Schluger, and Cristian Danescu-Niculescu-Mizil. 2022. Thread with caution: Proactively helping users assess and deescalate tension in their online discussions. Proceedings of the ACM on human-computer interaction 6, CSCW2 (2022), 137. [11] Justin Cheng, Michael Bernstein, Cristian Danescu-Niculescu-Mizil, and Jure Leskovec. 2017. Anyone can become troll: Causes of trolling behavior in online discussions. In Proceedings of the 2017 ACM conference on computer supported cooperative work and social computing. [12] Kevin Coe, Kate Kenski, and Stephen Rains. 2014. Online and uncivil? Patterns and determinants of incivility in newspaper website comments. Journal of communication 64, 4 (2014), 658679. [13] Sophie Cohen. 2021. Contextualizing toxicity in open source: qualitative study. In Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. [14] Cristian Danescu-Niculescu-Mizil, Moritz Sudhof, Dan Jurafsky, Jure Leskovec, and Christopher Potts. 2013. computational approach to politeness with application to social factors. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 250259. [15] Christine De Kock and Andreas Vlachos. 2021. Beg to Differ: study of constructive disagreement in online conversations. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics. [16] Ramtin Ehsani, Mia Mohammad Imran, Robert Zita, Kostadin Damevski, and Preetha Chatterjee. 2024. Incivility in Open Source Projects: Comprehensive Annotated Dataset of Locked GitHub Issue Threads. In 2024 IEEE/ACM 21st International Conference on Mining Software Repositories. IEEE. [17] Ramtin Ehsani, Rezvaneh Rezapour, and Preetha Chatterjee. 2023. Exploring Moral Principles Exhibited in OSS: Case Study on GitHub Heated Issues. In Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. [18] Isabella Ferreira, Bram Adams, and Jinghui Cheng. 2022. How Heated is it? Understanding GitHub Locked Issues. In 19th International Conference on Mining Software Repositories. [19] Isabella Ferreira, Jinghui Cheng, and Bram Adams. 2021. The\" shut the f** up\" phenomenon: Characterizing incivility in open source code review discussions. Proceedings of the ACM on Human-Computer Interaction CSCW2 (2021). [20] Isabella Ferreira, Ahlaam Rafiq, and Jinghui Cheng. 2024. Incivility detection in open source code review and issue discussions. Journal of Systems and Software (2024). [21] Fabrizio Gilardi, Meysam Alizadeh, and Ma√´l Kubli. 2023. ChatGPT outperforms crowd workers for text-annotation tasks. Proceedings of the National Academy of Sciences (2023). [22] GitHub. 2025. GraphQL Interface: Minimizable. https://docs.github.com/en/gra phql/reference/interfaces#minimizable Accessed: 2025-06-27. [23] GitHub, Inc., Kenyatta Forbes, Kevin Xu, Jeffrey Luszcz, Margaret Tucker, Eva Maxfield Brown, Peter Cihon, Mike Linksvayer, Ashley Wolf, Lukas Spei√ü, Kevin Crosby, and Jason Meridth. 2024. GitHub Open Source Survey 2024. doi:10.5281/zenodo.13989018 [24] Jack Hessel and Lillian Lee. 2019. Somethings Brewing! Early Prediction of Controversy-causing Posts from Discussion Features. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. doi:10.18653/v1/N19-1166 [25] Jane Hsieh, Joselyn Kim, Laura Dabbish, and Haiyi Zhu. 2023. \"Nip it in the Bud\": Moderation Strategies in Open Source Software Projects and the Role of Bots. Proceedings of the ACM on Human-Computer Interaction CSCW2 (2023). [26] Yilun Hua, Nicholas Chernogor, Yuzhe Gu, Seoyeon Jeong, Miranda Luo, and Cristian Danescu-Niculescu-Mizil. 2024. How did we get here? Summarizing conversation dynamics. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. [27] Mia Mohammad Imran, Preetha Chatterjee, and Kostadin Damevski. 2024. Shedding Light on Software Engineering-specific Metaphors and Idioms. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering. [28] Mia Mohammad Imran, Preetha Chatterjee, and Kostadin Damevski. 2024. Uncovering the Causes of Emotions in Software Developer Communication Using Zero-shot LLMs. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering (ICSE 24). Association for Computing Machinery, New York, NY, USA, Article 182. [29] Mia Mohammad Imran, Yashasvi Jain, Preetha Chatterjee, and Kostadin Damevski. 2022. Data augmentation for improving emotion recognition in software engineering communication. In Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering. [30] Mia Mohammad Imran and Jaydeb Sarker. 2025. \"Silent Is Not Actually Silent\": An Investigation of Toxicity on Bug Report Discussion. International Conference on the Foundations of Software Engineering (2025). [31] Maliha Jahan, Helin Wang, Thomas Thebaud, Yinglun Sun, Giang Ha Le, Zsuzsanna Fagyal, Odette Scharenborg, Mark Hasegawa-Johnson, Laureano Moro Velazquez, and Najim Dehak. 2024. Finding Spoken Identifications: Using GPT-4 Annotation for an Efficient and Fast Dataset Creation Pipeline. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). 72967306. [32] Nan-Jiang Jiang and Marie-Catherine de Marneffe. 2022. Investigating reasons for disagreement in natural language inference. Transactions of the Association for Computational Linguistics 10 (2022), 13571374. [33] Jigsaw. n.d.. Perspective API Documentation. https://developers.perspectiveapi. com Accessed: 2025-05-30. [34] Yova Kementchedjhieva and Anders S√∏gaard. 2021. Dynamic Forecasting of Conversation Derailment. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. [35] Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. [n. d.]. Decomposed Prompting: Modular Approach for Solving Complex Tasks. In The Eleventh International Conference on Learning Representations. [36] Irene Koshik. 2003. Wh-questions used as challenges. Discourse Studies 5, 1 (2003), 5177. [37] CMU STRUDEL Lab. [n. d.]. toxicity-detector. https://github.com/CMUSTRUDE L/toxicity-detector. [38] Sharon Levy, Robert Kraut, Jane Yu, Kristen Altenburger, and Yi-Chia Wang. 2022. Understanding conflicts in online conversations. In Proceedings of the ACM Web Conference 2022. [39] Renee Li, Pavitthra Pandurangan, Hana Frluckaj, and Laura Dabbish. 2021. Code of conduct conversations in open source software projects on github. Proceedings of the ACM on Human-computer Interaction (2021). [40] Zhenhao Li, Marek Rei, and Lucia Specia. 2022. Multimodal conversation modelling for topic derailment detection. In Findings of the Association for Computational Linguistics: EMNLP 2022. [41] Mary McHugh. 2013. The chi-square test of independence. Biochemia medica 23, 2 (2013), 143149. [42] Quinn McNemar. 1947. Note on the sampling error of the difference between correlated proportions or percentages. Psychometrika 12, 2 (1947), 153157. [43] Courtney Miller, Sophie Cohen, Daniel Klug, Bogdan Vasilescu, and Christian Kastner. 2022. \"Did You Miss my Comment or What?\" Understanding Toxicity in Open Source Discussions. In 2022 IEEE/ACM 44th International Conference on Software Engineering (ICSE). [44] Shyamal Mishra and Preetha Chatterjee. 2024. Exploring ChatGPT for Toxicity Detection in GitHub. In Proceedings of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results. [45] Todd Morrill, Zhaoyuan Deng, Yanda Chen, Amith Ananthram, Colin Wayne Leach, and Kathleen Mckeown. 2024. Social Orientation: New Feature for Dialogue Analysis. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation. Toxicity Ahead: Forecasting Conversational Derailment on GitHub Conference17, July 2017, Washington, DC, USA [46] Thi Huyen Nguyen and Koustav Rudra. 2024. Human vs ChatGPT: Effect of Data Annotation in Interpretable Crisis-Related Microblog Classification. In Proceedings of the ACM on Web Conference 2024. 45344543. [47] Nicole Novielli, Fabio Calefato, Davide Dongiovanni, Daniela Girardi, and Filippo Lanubile. 2020. Can we use se-specific sentiment analysis tools in cross-platform setting?. In Proceedings of the 17th International Conference on Mining Software Repositories. [48] Huilian Sophie Qiu, Anna Lieb, Jennifer Chou, Megan Carneal, Jasmine Mok, Emily Amspoker, Bogdan Vasilescu, and Laura Dabbish. 2023. Climate coach: dashboard for open-source maintainers to overview community dynamics. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. [49] Huilian Sophie Qiu, Bogdan Vasilescu, Christian K√§stner, Carolyn Egelman, Ciera Jaspan, and Emerson Murphy-Hill. 2022. Detecting interpersonal conflict in issues and code review: cross pollinating open-and closed-source approaches. In Proceedings of the 2022 ACM/IEEE 44th International Conference on Software Engineering: Software Engineering in Society. [50] Md Shamimur Rahman, Zadia Codabux, and Chanchal Roy. 2024. Do Words Have Power? Understanding and Fostering Civility in Code Review Discussion. Proceedings of the ACM on Software Engineering 1, FSE (2024), 16321655. [51] Naveen Raman, Minxuan Cao, Yulia Tsvetkov, Christian K√§stner, and Bogdan Vasilescu. 2020. Stress and burnout in open source: Toward finding, understanding, and mitigating unhealthy interactions. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: New Ideas and Emerging Results. [52] Farig Sadeque, Stephen Rains, Yotam Shmargad, Kate Kenski, Kevin Coe, and Steven Bethard. 2019. Incivility detection in online comments. In Proceedings of the eighth joint conference on lexical and computational semantics. [53] Soumya Sanyal, Tianyi Xiao, Jiacheng Liu, Wenya Wang, and Xiang Ren. 2024. Are machines better at complex reasoning? Unveiling human-machine inference gaps in entailment verification. In Findings of the Association for Computational Linguistics ACL 2024. 1036110386. [54] Jaydeb Sarker, Asif Kamal Turzo, and Amiangshu Bosu. 2020. benchmark study of the contemporary toxicity detectors on software engineering interactions. In 2020 27th Asia-Pacific Software Engineering Conference. IEEE. [55] Jaydeb Sarker, Asif Kamal Turzo, and Amiangshu Bosu. 2025. The Landscape International of Toxicity: An Empirical Investigation of Toxicity on GitHub. Conference on the Foundations of Software Engineering (2025). [56] Jaydeb Sarker, Asif Kamal Turzo, Ming Dong, and Amiangshu Bosu. 2023. Automated identification of toxic code reviews using toxicr. ACM Transactions on Software Engineering and Methodology (2023). [57] Brennan Schaffner, Arjun Nitin Bhagoji, Siyuan Cheng, Jacqueline Mei, Jay Shen, Grace Wang, Marshini Chetty, Nick Feamster, Genevieve Lakier, and Chenhao Tan. 2024. \" Community Guidelines Make this the Best Party on the Internet\": An In-Depth Study of Online Platforms Content Moderation Policies. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems. 116. [58] Charlotte Schluger, Jonathan Chang, Cristian Danescu-Niculescu-Mizil, and Karen Levy. 2022. Proactive moderation of online discussions: Existing practices and the potential for algorithmic support. Proceedings of the ACM on HumanComputer Interaction 6, CSCW2 (2022), 127. [59] Margrit Schreier. 2012. Qualitative content analysis in practice. (2012). [60] Xiaoying Song, Sharon Lisseth Perez, Xinchen Yu, Eduardo Blanco, and Lingzi Hong. 2025. Echoes of Discord: Forecasting Hater Reactions to Counterspeech. In Findings of the Association for Computational Linguistics: NAACL 2025, Luis Chiruzzo, Alan Ritter, and Lu Wang (Eds.). Association for Computational Linguistics, Albuquerque, New Mexico. [61] Florian Strohm and Roman Klinger. 2018. An empirical analysis of the role of amplifiers, downtoners, and negations in emotion classification in microblogs. In 2018 IEEE 5th International Conference on Data Science and Advanced Analytics (DSAA). IEEE, 137146. [62] Junyi Tian, Lingfeng Bao, Shengyi Pan, and Xing Hu. 2024. Analyzing and Detecting Toxicities in Developer Online Chatrooms: Fine-Grained Taxonomy and Automated Detection Approach. In Proceedings of the 31st Asia-Pacific Software Engineering Conference (APSEC 2024). [63] Dong Wang, Masanari Kondo, Yasutaka Kamei, Raula Gaikovina Kula, and Naoyasu Ubayashi. 2023. When conversations turn into work: taxonomy of converted discussions and issues in GitHub. Empirical Software Engineering 28, 6 (2023), 138. [64] Xinru Wang, Hannah Kim, Sajjadur Rahman, Kushan Mitra, and Zhengjie Miao. 2024. Human-LLM collaborative annotation through effective verification of LLM labels. In Proceedings of the CHI Conference on Human Factors in Computing Systems. 121. [65] Michael Williams and Tami Moser. 2019. The art of coding and thematic exploration in qualitative research. International management review 15, 1 (2019), 4555. [66] Jiaqing Yuan and Munindar Singh. 2023. Conversation Modeling to Predict Derailment. In Proceedings of the International AAAI Conference on Web and Social Media. [67] Oleg Zendel, Shane Culpepper, Falk Scholer, and Paul Thomas. 2024. Enhancing Human Annotation: Leveraging Large Language Models and Efficient Batch Processing. In Proceedings of the 2024 Conference on Human Information Interaction and Retrieval. 340345. [68] Justine Zhang, Jonathan Chang, Cristian Danescu-Niculescu-Mizil, Lucas Dixon, Yiqing Hua, Dario Taraborelli, and Nithum Thain. 2018. Conversations Gone Awry: Detecting Early Signs of Conversational Failure. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. [69] Denny Zhou, Nathanael Sch√§rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. [n. d.]. Least-toMost Prompting Enables Complex Reasoning in Large Language Models. In The Eleventh International Conference on Learning Representations. [70] Xiaodan Zhu, Svetlana Kiritchenko, Saif Mohammad, Xiaodan Zhu, and Colin Cherry. 2014. An empirical study on the effect of negation words on sentiment. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, 304313. [71] Yiming Zhu, Zhizhuo Yin, Gareth Tyson, Ehsan-Ul Haq, Lik-Hang Lee, and Pan Hui. 2024. APT-Pipe: Prompt-Tuning Tool for Social Data Annotation using ChatGPT. In Proceedings of the ACM on Web Conference 2024. 245255. [72] Frances Zlotnick. 2017. GitHub Open Source Survey 2017. http://opensourcesu rvey.org/2017/. doi:10.5281/zenodo."
        }
    ],
    "affiliations": [
        "Drexel University",
        "Elmhurst University",
        "Missouri University of Science and Technology",
        "Virginia Commonwealth University"
    ]
}