{
    "paper_title": "Taming Modality Entanglement in Continual Audio-Visual Segmentation",
    "authors": [
        "Yuyang Hong",
        "Qi Yang",
        "Tao Zhang",
        "Zili Wang",
        "Zhaojin Fu",
        "Kun Ding",
        "Bin Fan",
        "Shiming Xiang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, significant progress has been made in multi-modal continual learning, aiming to learn new tasks sequentially in multi-modal settings while preserving performance on previously learned ones. However, existing methods mainly focus on coarse-grained tasks, with limitations in addressing modality entanglement in fine-grained continual learning settings. To bridge this gap, we introduce a novel Continual Audio-Visual Segmentation (CAVS) task, aiming to continuously segment new classes guided by audio. Through comprehensive analysis, two critical challenges are identified: 1) multi-modal semantic drift, where a sounding objects is labeled as background in sequential tasks; 2) co-occurrence confusion, where frequent co-occurring classes tend to be confused. In this work, a Collision-based Multi-modal Rehearsal (CMR) framework is designed to address these challenges. Specifically, for multi-modal semantic drift, a Multi-modal Sample Selection (MSS) strategy is proposed to select samples with high modal consistency for rehearsal. Meanwhile, for co-occurence confusion, a Collision-based Sample Rehearsal (CSR) mechanism is designed, allowing for the increase of rehearsal sample frequency of those confusable classes during training process. Moreover, we construct three audio-visual incremental scenarios to verify effectiveness of our method. Comprehensive experiments demonstrate that our method significantly outperforms single-modal continual learning methods."
        },
        {
            "title": "Start",
            "content": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, OCTOBER 2025 1 Taming Modality Entanglement in Continual Audio-Visual"
        },
        {
            "title": "Segmentation",
            "content": "Yuyang Hong, Qi Yang, Tao Zhang, Zili Wang, Zhaojin Fu, Kun Ding, Bin Fan, Shiming Xiang 5 2 0 2 0 2 ]"
        },
        {
            "title": "M\nM",
            "content": ". [ 1 4 3 2 7 1 . 0 1 5 2 : r AbstractRecently, significant progress has been made in multi-modal continual learning, aiming to learn new tasks sequentially in multi-modal settings while preserving performance on previously learned ones. However, existing methods mainly focus on coarse-grained tasks, with limitations in addressing modality entanglement in fine-grained continual learning settings. To bridge this gap, we introduce novel Continual Audio-Visual Segmentation (CAVS) task, aiming to continuously segment new classes guided by audio. Through comprehensive analysis, two critical challenges are identified: 1) multi-modal semantic drift, where sounding objects is labeled as background in sequential tasks; 2) co-occurrence confusion, where frequent co-occurring classes tend to be confused. In this work, Collision-based Multimodal Rehearsal (CMR) framework is designed to address these challenges. Specifically, for multi-modal semantic drift, Multimodal Sample Selection (MSS) strategy is proposed to select samples with high modal consistency for rehearsal. Meanwhile, for cooccurence confusion, Collision-based Sample Rehearsal (CSR) mechanism is designed, allowing for the increase of rehearsal sample frequency of those confusable classes during training process. Moreover, we construct three audio-visual incremental scenarios to verify effectiveness of our method. Comprehensive experiments demonstrate that our method significantly outperforms single-modal continual learning methods. Index TermsAudio-Visual Segmentation, Continual Sementatic Segmentation, Modality Entanglement I. INTRODUCTION Humans are inherently capable of continuously learning while retaining knowledge from previous tasks. For example, infants can progressively recognize new animals while remembering those they have already learned. This human ability has motivated extensive research into continual learning [1], which enables models to learn sequential tasks. Early work [2] [4] primarily focused on classification, employing techniques such as regularization or rehearsal to mitigate catastrophic forgetting. Subsequent methods [5] have extended continual learning to semantic segmentation. However, when directly these applied to multi-modal (e.g. audio-visual) scenarios, single-modal methods exhibit suboptimal performance [6]. Recently, several methods [6][8] have extended continual learning to multi-modal scenarios. For example, AV-CIL [7] Yuyang Hong, Qi Yang, Zili Wang and Shiming Xiang are with the State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China, and also with the School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing 100049, China (e-mail: hongyuyang2023@ia.ac.cn; yangqi2021@ia.ac.cn; wangzili2022@ia.ac.cn; smxiang@nlpr.ia.ac.cn). Tao Zhang and Kun Ding are with the State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China (e-mail: zhangtao2021@ia.ac.cn; kun.ding@ia.ac.cn). Zhaojin Fu and Bin Fan are with the School of Intelligent Science and Technology, University of Science and Technology Beijing, Beijing 100083, China (e-mail: d202410395@xs.ustb.edu.cn; bin.fan@ieee.org). Illustration of CAVS and two challenges. In the figure, Fig. 1. three sequential tasks are presented from top to bottom. Gray boxes: learned or background classes, light-orange boxes: target classes to be learned. Multimodal semantic drift occurs when learned class (e.g., darkgreen drum) is labeled as background in task t, despite the presence of its corresponding sound in the audio. This drift causes the model to suffer catastrophic forgetting of the modality semantic associations specific to the drum. Co-occurrence confusion occurs when, in previous task (e.g. task t), two classes frequently co-occur (guitar and woman). After learning new task, the model tends to misclassify the old classes (guitar) as the new ones (woman). proposes continual audio-visual classification method with dual similarity constraint enforcing both instance-level and class-level cross-modal semantic consistency. ContAV-Sep [8] proposes framework for audio-visual separation that incorporates cross-modal similarity distillation to preserve semantic consistency between modalities. Meanwhile, real-world applications require fine-grained audio-visual continual learning. For example, embodied intelligence needs to identify the source of vocalization from environmental audio-visual cues. However, existing methods primarily focus on coarse-grained audio-visual tasks and therefore fail to address fine-grained tasks, such as disentangling pixel-level visual features from audio signals under continual learning scenarios. Meanwhile, recent research [9][11] has explored finegrained modality entanglement between audio signals and visual features in audio-visual segmentation. AVSBench [9] establishes the first benchmark for aligning the pixel-level visual semantics with the corresponding audio signals. COMBO [11] further explores bilateral relations of three entanglements, pixel, modality, and temporal, to enhance the models representational capacity. However, audio-visual segmentation cannot be directly applied to continual learning scenarios, as it is designed for static settings. To this end, we introduce novel fine-grained multiJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, OCTOBER 2025 2 modal continual learning task, termed Continual Audio-Visual Segmentation (CAVS). Specifically, CAVS needs to perform audio-visual segmentation in sequential task setting while retaining knowledge of previously seen classes. To address CAVS, we reformulate the AVS [9], [10] framework and adapt classical continual semantic segmentation methods to the audio-visual context. Based on our observations, we identify two new challenges in fine-grained continual learning tasks: (1) Multi-modal semantic drift: Incorrect audio-visual semantic alignment (e.g. drum-background) due to mislabeling of learned classes as background exacerbates catastrophic forgetting. (2) Co-occurrence confusion: Frequent co-occurrence of categories leads to modality entanglement, for example, the audio modality of woman becomes entangled with visual modality of guitar in Fig. 1. In essence, these two issues are manifestations of modality entanglement from different perspectives. To tackle these challenges, we propose Collision-based Multi-modal Rehearsal (CMR) framework. Specifically, collision is the discrepancy between the predictions and the ground truth labels during rehearsal. To the best of our knowledge, this is the first rehearsal-based framework specifically designed for the audio-visual continual scenario. For challenge (1), Multi-modal Sample Selection (MSS) is introduced, which leverages additional single-modal models to select multimodal samples with high modal consistency for rehearsal, thereby enhancing inter-modal alignment (correct audio-visual entanglement). For challenge (2), Collision-based Sample Rehearsal (CSR) is proposed, which dynamically adjusts the class ratio of samples for rehearsal based on the collision frequency between the old models predictions and the ground truth labels. In this process, classes with higher collision frequencies (defined as the discrepancy between the predictions and the ground-truth labels) are identified as classes that are more prone to be confused with newly learned classes. By increasing the number of rehearsal samples from classes with high collision frequency, the model can better leverage the audio modality to distinguish confusing classes, thereby mitigating catastrophic forgetting during training. To validate the effectiveness of CMR, we reformulate the audio-visual dataset AVSBench [9] into three sequential task setup to better simulate continual learning scenario. Specifically, our datasets include (1) AVSBench-Class Incremental (AVSBench-CI), (2) AVSBench-Class Incremental for Single-object (AVSBench-CIS), and (3) AVSBench-Class Incremental for Multi-object (AVSBench-CIM). Comprehensive experiments demonstrate that our proposed method achieves encouraging performance, showcasing its ability to effectively address the multi-modal semantic drift and co-occurrence confusion in CAVS. Our main contributions can be summarized as follows: We pioneer the extension of continual learning to audiovisual segmentation, introducing the Continual AudioVisual Segmentation (CAVS). To the best of our knowledge, this is the first work to address audio-visual segmentation in continual learning setting. For multi-modal semantic drift, we propose Multimodal Sample Selection (MSS) strategy to identify highquality multi-modal samples with enhanced modal consistency. To solve co-occur confusion, we introduce Collision-based Sample Rehearsal (CSR) mechanism where the rehearsal frequency of learned classes is dynamically adjusted based on collision frequency. Extensive experiments on three class-incremental datasets demonstrate that our method achieves state-of-the-art performance, validating its effectiveness in continual audiovisual segmentation. II. RELATED WORK A. Continual Learning. Continual learning focuses on incrementally training models to adapt to new tasks while preserving knowledge from previously learned ones. Recently, many works [2][4], [12][17] have proposed rehearsal-based and rehearsal-based methods to address the problem of catastrophic forgetting. Rehearsalbased methods [2][4], [12], [13] allow for the storage of small subset of old data in memory, which is later utilized for rehearsal during training. iCaRL [2] introduces strategy to identify and retain the most representative samples for each class, which are replayed during training to mitigate forgetting in class-incremental learning. Pseudo-sample rehearsal-based methods [14][16] utilize generative models to create pseudosamples of old classes. DGR [15] establishes an initial framework where learning each new task is coupled with replaying the data generated by the old generative model. Building upon continual learning [2], [12][16], Class-Incremental Semantic Segmentation (CISS) requires pixel-level classification to achieve fine-grained segmentation [5], [18][22]. PLOP [18] suggests generating pseudo-labels by identifying latent past classes within the current background. ScaleSeg [20] employs prototypes refined through online contrastive clustering and incorporates background diversity strategy to boost plasticity. While Cermelli et al. (2020) addressed semantic shifts within single modality, our work reveals more complex multi-modal semantic drift where modal consistency is considered. B. Audio Visual Segmentation Audio-visual segmentation (AVS) is novel and challenging task that localizes sound sources in visual scenes by pixel-level prediction [9][11], [23][28]. AVSBench [9] establishes the first audio-visual segmentation benchmark and introduces the Temporal Pixel-wise Audio-Visual Interaction (TPAVI) module to incorporate audio semantics as guidance for visual segmentation. AVSegFormer [10] develops transformer-based framework with audio queries, learnable queries, and an audiovisual mixer for selective attention and dynamic feature adjustment. CATR [24] proposes combinatorial fusion framework that captures audio-visual spatiotemporal dependencies through cross-modal interaction modelling. ECMVAE [23] decomposes audio and visual data in latent space, explicitly modeling both shared and modality-specific representations to enhance segmentation performance. COMBO [11] rethinks AVS by exploring the bilateral relations of three entanglements, pixel, modality, and temporal, to enhance the models representation ability. In this work, we develop framework JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, OCTOBER 3 Fig. 2. Overview of the proposed CMR framework. The CMR framework introduces novel rehearsal-based method for continual audio-visual segmentation. Our method consists of two key modules: (a) Multi-modal Sample Selection (MSS) strategy for samples rehearsal, which identifies samples with high modality consistency by computing the difference in mean Intersection-over-Union (mIoU ) between uni-modal and multi-modal models. (b) Collision-based Sample Rehearsal (CSR) strategy that dynamically adjusts the rehearsal frequency of samples based on the collision between the old model and current ground truth. for continual learning scenarios, making the task more aligned with real-world applications. C. Multimodal Learning Multimodal learning [29][32] focuses on integrating information across diverse modalities and investigating the intricate interrelationships between them in various contexts. Wei [29] first estimates each modalitys learning status based on separability in its unimodal representation space, then uses this to softly initialize the corresponding unimodal encoder. MMPareto [30] employs gradient-based optimization to mitigate model bias towards specific modalities during training, thereby enhancing multimodal learning performance. MMH [33] proposes multimodal reconstruction framework that guides the reconstruction network to directly learn modality-shared representations from the multimodal encoder, thereby capturing richer cross-modal interactions. To compared with more recent work, Finger [31] focuses ondistinguishing foreground from background and transfer-ring unimodal knowledge, while we focus on selecting con-sistent samples through modal contribution and replaying them according to collision frequency. From task level, Finger aims to seamlessly integrate new classes with limitedincremental samples, while we focus on avoiding interfer-ence with old task knowledge when training on new tasks. Meanwhile, to Open-set AVS, continual learning AVS deals with learning from continuous data stream under memory constraints, without revisiting past data. in contrast Subsequently, we present the two core components of our framework: multi-modal sample selection and collision-based sample rehearsal. A. Revisiting Continual Semantic Segmentation (CSS) CSS assumes that tasks arrive sequentially, with each task containing set of categories Ct and corresponding training set Dt, where denotes the current learning stage. The goal of the learning task Dt at given stage is to learn model θ parameterized by θt to accurately predict the label given an input image X. The predicted output segmentation mask for pixel can be computed as: yi = arg max{fθt(X)[i, c]}Y1 c=0 , (1) where fθt(X)[i, c] denotes the predicted probability of class at pixel i. In this setting, CSS assumes that tasks arrive sequentially, with each task Dt containing set of categories Ct that are disjoint from those in other tasks. Training occurs in multiple phases, referred to as learning steps, where data from previous tasks may not be accessible in subsequent steps. Specifically, CSS further assumes that the previous 1 tasks encompass categories t1 = (cid:83)t1 introduces new categories Ct. The model fθt task Dt, while leveraging the previous model fθt1 and avoiding catastrophic forgetting. In this work, we extend this setting to continual audio-visual segmentation. i=0 Ci, and task Dt trained on the current III. METHOD The proposed CMR framework, as illustrated in Fig. 2, is constructed based on the ResNet50 architecture from AVSBench. The subsequent sections first revisit continual semantic segmentation, followed by formal formulation of CAVS. B. Problem Setup and Notation of CAVS For CAVS, the input space is defined as A, where and represent the visual and audio modalities, respectively. Each input sample = ({Sk }, Sa) Dt contains JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, OCTOBER 2025 consecutive video frames paired with an audio signal Sa, where = 10. The sounding objects in the k-th video frame Sk are annotated with pixel-level labels. The objective of the : (cid:55) RN Ct, t-th learning stage is to learn model v,a θt where is the number of pixels per frame. In this setting, the segmentation mask for pixel can be computed as follows: θt ({Sk In contrast, for task Dt, both non-sounding objects from and sounding objects from t1 are assigned the background label, while the audio Sa remains unchanged. Compared to AV-ICL [7], CAVS demands more substantial fine-grained alignment between global audio cues and local visual semantics. }, Sa)[i, c]}Y1 c=0 . yi = arg max{f v,a (2) Fig. 3. Illustration of inter-modal consistent samples and entanglement of modality. (a) Cases 1 and 4 dont appear in practice because selection uses already well-trained samples where audio and video predictions match the ground truth. Case 3 represents samples characterized by multi-modal semantic drift and is typically excluded due to substantially large (Sa). Conversely, Case 2 is kept because of its cross-modal semantic consistency. (b) Classes with infrequent co-occurrence exhibit weak audio-visual entanglement, while frequent co-occurrence leads to strong cross-modal entanglement (e.g., guitar sounds and images of women). C. Multi-modal Sample Selection Multi-modal semantic drift occurs when learned classes are mislabeled as background in new tasks, which in turn leads to the incorrect modality semantic associations. Therefore, replaying samples with consistent modality semantics helps alleviate the multi-modal semantic drift of previously learned classes in the current task. However, as shown in Fig. 1, existing selection strategies fail to identify samples with high modality semantic consistency and may instead select samples that contain multi-modal semantic drift. Inspired by the work in [34], where Shapley values are leveraged to quantify uni-modal contributions to model predictions, we propose Multi-modal Sample Selection (MSS) strategy. By quantifying the contribution of the audio modality, this strategy identifies samples with high inter-modal consistency for rehearsal. Formally, given video sample = ({Sk }, Sa) Dt, we train two parallel models: ({Sk θt v,a ({Sk θt }) : (cid:55) RN t, }, Sa) : (cid:55) RN t. (3) (4) After training, we compute the mIoU scores for both modalities: visual-only model performance mIoUv and audiovisual model performance mIoUv,a. mIoUv = Jmean(f mIoUv,a = Jmean(f v,a θ θ ({Sk ({Sk }), {yk gt}), }, Sa), {yk gt}), 4 (5) (6) As illustrated in Fig. 3 (a), samples exhibiting smaller (Sa) exhibit reduced multi-modal semantic drift. Therefore, (Sa) is used to select samples that are more suitable for rehearsal. Calculation of (Sa) is as follows: (Sa) = mIoUv,a mIoUv, (7) where ygt is the ground truth of video frame S, Jmean denotes the computation of averaged mIoU over frames. For each newly added class Ct, we select the top-k samples with the smallest absolute audio contribution deviation (Sa) from Dt to construct the memory buffer Mt. These selected samples are stored and replayed during the training of subsequent tasks through Dt+1 Mt, which effectively reinforces cross-modal associations. Our ablation studies demonstrate that this criterion outperforms random selection by 2.0 mIoU (see Tab. IV), highlighting the importance of modality consistency in sample rehearsal. Fig. 4. Illustration of the collision-based sample rehearsal: for each sample, we calculate conflicts between old model predictions (dog) and current ground truth (baby). Aggregating these across all samples yields the collision frequency F, quantifying confusion between old and new classes. By aligning the distribution of replayed samples with the collision frequency, the model is better guided to disentangle incorrect modality semantic associations during training. D. Collision-based Sample Rehearsal As shown in Fig. 3 (b), frequently co-occurring classes in the old task will exhibit incorrect modality entanglement because of confusion in the audio modality. To be more specific, frequent occurrence pulls the two classes closer in the feature space, which causes confusion. By aligning the distribution of replayed samples with the collision frequency, we increase the rehearsal frequency of collision classes, thereby promoting the disentanglement of incorrect modality semantic associations. To implement this idea, we propose the Collision-based Sample Rehearsal (CSR) strategy, which identifies classes prone to co-occurrence confusion by detecting collisions between the old models predictions and the ground truth. As illustrated in Fig. 4, for new sample S, collision occurs JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, OCTOBER 2025 Algorithm 1 Collision-based Sample Rehearsal Require: Old model v,a label Ygt, Threshold Ensure: Collision frequency θt1 , Training dataset Dt, Semantic θt1 (Si) Compute ˆY v,a 1: for Si Dt do 2: 3: Mask ( ˆY = background) (Y = background) 4: 5: 6: Collision Region ( ˆY = Y) Count Pairs: for do Collision( ˆYi, Yi) = Collision( ˆYi, Yi) + 1 end for Get Most Confused Class: (Cold, Cnew) arg max(Collision( ˆYI, YI) 7: 8: 9: 10: 11: Collision(Cold,Cnew) (cid:80) Collision( ˆYI ,YI ) 12: 13: 14: Update Frequency: if > then FCold = FCold + 1 end if 15: 16: end for 17: return when the old model v,a predicts an old class cold t1 in θt1 spatial position where the ground truth cnew Ct appears. Specifically, with the old model and task Dt, the collisions between the prediction of v,a and Dt is first computed. θt1 Inferring the video with the old model v,a , we obtain θt1 collision pair (cold, cnew). Since the old model has not trained on new samples, it can only predict old classes cold t1. Assuming that the predicted result is cold and the ground truth label is cgt, we count all collision pairs (cold, cnew) and identify the learned class with the highest number of collisions as the most confusing class for the current video S: P(S) = arg max{Count(ci, cj)i t1, Ct}. (8) Next, the ratio of the number of collisions for the most confusing old class to the total number of collisions in single frame is calculated as: Rc = Count(P(S) = c) (cid:80){Count(ci, cj)i t1, Ct} , (9) Rc denotes the ratio of c. if Rc is greater than , which is the mean ratio across all learned classes, then we record that this old class has caused significant collision. This process will be repeated for all samples to obtain the collision frequency of learned class: Fc = (cid:88)Dt i=1 (P (Si) = c) (Rc > ), (10) where Count represents the current number of collisions, and Fc indicates the collision frequency for class in the current dataset Dt. The collision frequency for classes that do not exhibit collisions will be set to 1. To prevent the collision frequency of certain classes from becoming excessively large, we apply sigmoid smoothing. The results are then normalized to obtain , as in Eq. (11). = sigmoid(F) (cid:80) sigmoid(F) . (11) With , 20% of the original memory Mt1 is first sampled and then combined with the existing memory Mt1, resulting in ˆMt1. In ˆMt1, samples from easily confused classes account for larger proportion. Replaying ˆMt1, the model can more effectively distinguish between confusable classes, thereby mitigating the problem of catastrophic forgetting. To provide more comprehensive elaboration on Collisionbased Sample Rehearsal, we provide its algorithmic procedure in Alg.1. The algorithm demonstrates how we leverage collisions to identify categories affected by co-occurrenceinduced semantic confusion, and further quantifies their replay frequency by tracking how often such misclassifications occur across inference samples. The Multi-modal Sample Selection and Collision-based Sample Rehearsal methods effectively address the challenges of multi-modal semantic drift and co-occurrence confusion, enhancing the models capability for CAVS. The experiments demonstrate that rehearsal with resampling yields superior performance compared to direct rehearsal. IV. EXPERIMENTS A. AVSBench Datasets In our work, class-incremental audio-visual segmentation dataset (AVSBench-CI) is constructed from the well-known dataset AVSBench-semantic [10] to validate the proposed CMR. AVSBench-semantic utilizes the techniques introduced in VGGSound [38] to collect videos, ensuring that the audio and visual clips align with the intended semantics. The dataset provides semantic segmentation maps for videos as labels to enhance audio-visual semantic segmentation (AVSS). It contains total of 11,356 videos spanning 70 categories. Each video segment consists of 10 frames of images and one 10second audio clip. We divide the 70 categories in AVSBenchsemantic for the original dataset into three training steps: 6010, 60-5, and 65-1. Following the conventional continual semantic segmentation setup, the three training steps are divided into overlapped and disjoint settings to evaluate the models performance under different task stream configurations. In the overlapped setting, the classes are divided sequentially, meaning that classes from past and future tasks may appear in the current data and be labelled as background. In the disjoint setting, community detection algorithm [39] is employed to minimize the overlap of training data between consecutive steps. Therefore, the current data will not contain classes from future or past tasks. This setup closely aligns with continual learning scenarios. Furthermore, we expand the single-semantic dataset (AVSBench-CIS) and the multisemantic dataset (AVSBench-CIM) based on the number of targets in the videos. AVSBench-CIS and AVSBench-CIM address scenarios involving modality entanglement with singletarget and multi-target settings, respectively. The same settings are applied to these datasets. B. Experimental Setup 1) Baselines: Since semantic segmentation can be regarded as pixel-wise classification task, we compare our method with both classification and segmentation methods to provide JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, OCTOBER 2025 6 TABLE mIoU ON THE AVSBENCH-CI DATASET FOR DIFFERENT CLASS-INCREMENTAL AUDIO-VISUAL SEGMENTATION SCENARIOS. Method FT LwF [2] LwF-MC [35] ILT [36] MiB [5] PLOP [18] AVSegFormer [37] EIR [22] CMR (ours) Upper-bound 60-10 60-5 65-"
        },
        {
            "title": "Overlapped",
            "content": "1-60 61-71 19.4 1.4 25.1 10.1 2.2 2.0 19.7 12.3 23.0 17.4 13.5 21.2 34.6 1.5 1.3 14.6 15.8 29.5 33.2 33.7 all 4.0 12.3 2.0 13.4 18.2 20.1 6.1 12.8 27.6 33.7 1-60 61-71 17.1 1.5 19.0 7.1 1.1 16.4 13.8 14.5 16.6 17.5 11.3 19.0 22.7 1.5 0.1 12.4 13.5 28.5 29.6 34.3 all 3.7 8.8 14.3 14.4 17.4 17.9 4.5 10.7 26.4 33.7 1-60 61-71 0.01 1.4 9.7 1.5 0.03 2.8 7.2 8.6 11.5 4.1 11.7 1.3 34.9 1.4 1.1 6.8 11.6 26.2 33.2 33.7 all 1.3 2.6 2.4 8.4 5.1 10.0 4.0 6.0 24.2 33. 1-60 61-71 1.5 1.5 5.8 2.0 5.7 8.3 1.5 5.5 24.3 34.3 6.7 12.6 0.6 11.4 7.3 9.3 9.1 0.2 10.4 29.6 all 2.2 3.0 5.0 3.4 5.9 8.4 2.5 4.8 22.4 33.7 1-65 66-71 1.3 1.3 1.6 1.3 1.6 1.3 1.3 0.5 16.9 34.0 0.2 0.7 0.0 0.6 2.8 0.2 0.3 0.08 2.0 28.7 all 1.3 1.3 1.5 1.2 1.7 1.2 1.3 0.4 15.9 33. 1-65 66-71 1.3 1.3 1.3 1.3 1.3 1.2 1.3 0.5 11.3 34.0 4.0 4.5 1.7 3.7 4.9 4.1 3.7 0.02 6.7 29.8 all 1.5 1.6 1.4 1.5 1.5 1.4 1.5 0.4 10.9 33.7 Fig. 5. mIoU on the AVSBench-CIS and AVSBench-CIM datasets for different class-incremental audio-visual segmentation scenarios. The red line represents the upper bound. The upper section compares different methods and our method under different incremental settings on AVSBench-CIS, including both disjoint and overlapped scenarios. The lower section provides similar comparison for AVSBench-CIM, showcasing the performance of our method. more comprehensive evaluation. The detailed baseline introduction is provided in the appendix. 2) Evaluation Metrics: Following [5], mean Intersectionover-Union (mIoU ) is taken for evaluation: mIoU ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 Pi Pi + Pi + Ni , (12) where Pi denotes the number of samples correctly predicted as classi, Pi represents incorrectly predicted as classi, Ni indicates the number of samples that the model failed to correctly predict as classi. 3) Implementation Details of our methods: Our method builds upon the best-performing PLOP model combined with the memory. We have primarily conducted training and evaluation using ResNet-50 [40] pre-trained on ImageNet [41]. The ASPP module [9] is utilized as the fusion module. For input frames, we resize the resolution to 224 224. The same data augmentation is applied as in [9], excluding memory data. The training batch size is set to 2 per GPU on 4 Nvidia L40 48GB GPUs. The training runs 30 epochs each task. For singlemodal training, all steps are trained only using visual-modal data. For memory samples, 5 samples per class are selected for rehearsal. The memory dataset is shuffled together with the training dataset during training. The number of resampled samples is set to 20% of the total sample size. To be fair, all tasks share common test set with all learned classes. 4) Implementation Details of baseline methods: For incremental classification methods: (1) Learning without forgetting (LWF) [35]: LWF distils the output differences between the old and current models. Our implementation of LwF follows [35]; distillation and cross-entropy losses share the same label space and classifier. (2) LwF multi-class (LWF-MC) [2]: LwF-MC utilizes multiple binary classifiers. Following the approach proposed in [5], LWF-MC is implemented by combining two binary cross-entropy losses in weighted manner. These losses are computed based on the ground truth labels and the probabilities predicted by the previous model fθt1. (3) ILT: [36]: ILT employs dual-space knowledge distillation strategy, including distillation loss in the output space and an additional distillation loss in the feature space. For incremental segmentation: (1) MiB [5]: MiB uses complete output space distillation and background uncertainty propaJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, OCTOBER 2025 7 TABLE II THE TABLE PRESENTS THE 60-10 CATEGORY CONFIGURATION OF AVSBENCH UNDER THE DISJOINT SETTING."
        },
        {
            "title": "Disjoint Settings",
            "content": "AVSBench-CI 60-10 step 0 erhu, cello, bus, airplane, parrot, bassoon, missile-rocket, accordion, goose, hen, baby, horse, saxophone, boat, frying-food, flute, marimba, bird, hairdryer, harmonica, mower, emergencycar, tiger, saw, duck, squirrel, clarinet, dog, guitar, keyboard, boy, clipper, handpan, sitar, elephant, tabla, girl, gun, axe, harp, piano, car, guzheng, drum, helicopter, motorcycle, clock, man, lion, leopard, pipa, bell, tractor, pig, donkey, cat train, sorna, sheep, tank, 60-10 step wolf, tuba, trumpet, utv, violin, ukulele, trombone, vacuum-cleaner, woman, truck gation. (2) PLOP [18]: PLOP proposes multi-scale pooling distillation to maintain spatial relationships at the feature level and uses entropy-based pseudo-labels to annotate background classes predicted by the old model. (3) EIR [22] is an instance rehearsal method for continual semantic segmentation, introduced in CVPR 2025, and represents the state-of-the-art (SOTA) in this field. In our work, We reproduced both the original EIR method and its PLOP-based variant, and adapted them to the continual audio-visual segmentation. Our experiments demonstrate that the PLOP-enhanced EIR outperforms the vanilla EIR approach. To ensure fair comparison, we adopt the PLOP-based EIR method in our study. Besides, the fine-tuning of AVSegFormer [37] is implemented based on ResNet-50. Additionally, fine-tuning each task as baseline and offline training on all classes is provided as an upper bound for performance comparison. 5) The Details of Category: Tab. II present the 60-10 category learning sequence under the setting of disjoint in the AVSBench-CI dataset. For the setting of disjoint, we employ the Louvain algorithm to divide the 70-category dataset into bipartite and tripartite graphs. Classes with minimal overlapped are then allocated to distinct steps to form the disjoint dataset. The dataset was directly partitioned into steps based on sequential category order for the overlapped setting. C. Main Results task. From left Tab. illustrates the experiments of existing methods on AVSBench-CI. We use underlining to indicate the secondbest performance. The upper bound represents the optimal performance when the model is directly trained on the target task difficulty progressively increases, as more tasks lead to greater forgetting in the model. As reported in the results, our method achieves the best performance across all settings and demonstrates superior performance as the number of learning steps increases. On the more challenging 65-1 split, our method achieves signiffcantly better performance than traditional approaches. Despite into right, corporating audio, traditional continual semantic segmentation suffers significant forgetting due to its inability to effectively interactions. Specifically, EIR exdisentangle audio-visual hibits consistently low performance. The primary reason is the poor rehearsal quality resulting from its inability to extract audio aligned with the synthesized content, which exacerbates modality entanglement and consequently leads to catastrophic forgetting. Thus, experimental results show that disentangling modalities is essential in audio-visual segmentation to mitigate catastrophic forgetting. Fig. 5 illustrates the experiments on AVSBench-CIS and AVSBench-CIM. Different colors represent different methods, and higher bars indicate better performance. The experimental results show that our method achieves more significant improvement on AVSBench-CIS compared to AVSBench-CIM, with an increase of 11.3 mIoU on the AVSBench-CIS 60-10 overlapped setting, while only 1.5 mIoU on AVSBench-CIM. One main reason is that AVSBench-CIM can only select multitarget samples for rehearsal, which inherently involves dealing with the entanglement between multiple targets and modalities. In contrast, our observations indicate that single-target samples tend to yield better results when used for rehearsal. Therefore, for future work on multi-target tasks, it may be beneficial to preprocess the samples to enable the rehearsal of single-target samples. Nevertheless, our method achieves state-of-the-art performance on most tasks, demonstrating its effectiveness. D. Experiments on Transformer Architecture To further validate the effectiveness of our method on Transformer-based architectures, we conduct additional experiments on the 60-10 and 60-5 settings using PVT (Pyramid Vision Transformer). The results in Tab. III demonstrate that our method continues to achieve competitive performance, even when applied to Transformer-based models, indicating its strong generalization capability across different architectural backbones. TABLE III THE RESULTS OF OUR METHOD ON THE AVSBENCH-CI 60-10 TASK BASED ON PVT 60-10 Backbone Ours (ResNet) Ours (PVT) 1-60 29.5 33.7 Disjoint 61-71 15.8 34.7 all 27.6 33. Overlapped 61-71 13.5 15.6 1-60 28.5 35.1 all 26.3 32.4 E. Ablation Study 1) Effectiveness of MSS and CSR: We evaluated the MSS against strategies based on maximum modality discrepancy, minimum modality discrepancy, and random sample selection. The results in rows 1-4 in Tab. 2 consistently demonstrate the superiority of the MSS. From Tab. 2, the further introduction of CSR based on MSS can further improve performance (e.g., 1.3% for the overlapped 1-60 setting), validating the effectiveness of CSR. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, OCTOBER 8 Fig. 6. The qualitative results of incremental methods on the 60-10 setting of AVSBench-CI, where different colours represent different classes. The blue waveform represents the audio modality. Here, the far left represents the single old class (airplane), the middle represents the single new class (train), and the far right shows the sounding handpan (learned class) segmentation. TABLE IV ABLATION STUDY ON EFFECTIVENESS OF MSS AND CSR. Method Smallest Largest Random MSS (Ours) MSS+CSR (Ours) 60-"
        },
        {
            "title": "Overlapped",
            "content": "1-60 61-71 13.1 25.6 14.6 25.2 15.6 26.5 13.4 28.7 15.8 29.5 all 23.7 23.8 25.0 26.5 27.6 1-60 61-71 12.7 21.8 12.3 23.4 12.8 25.0 13.2 27.2 13.5 28.5 all 20.5 21.9 23.3 25.3 26.3 2) Number of rehearsal samples in MSS: Tab. reports the results of the ablation study on the number of rehearsal samples per class. The results show that as the number of rehearsal samples increases, the forgetting of old classes gradually decreases. However, an excessive number of rehearsal samples can inhibit learning new samples. Therefore, we select five samples per class for rehearsal. TABLE ABLATION STUDY ON THE NUMBER OF REHEARSAL SAMPLES IN MSS. WE SELECT 3, 5, AND 7 SAMPLES PER CLASS USING MSS. 60-10 Fig. 7. Number of collision pairs. Highly colliding categories typically correspond to objects that co-occur. The categories with the highest collision rate are guitar and man, which aligns with real-world observations. Sample Numbers MSS-3 MSS-5 MSS-7 Disjoint 61-71 14.7 13.4 13.3 1-60 27.3 28.7 28.0 all 25.6 26.5 25. Overlapped 61-71 12.2 13.2 12.7 1-60 25.5 27.3 29.3 all 23.6 25.3 26.9 F. Qualitative Analysis 1) Qualitive Analysis of AVSBench-CI: Fig. 6 illustrates qualitative comparison between our method and traditional methods. By replaying more samples from easily confused learned classes, our method enhances the ability of the model to leverage audio to distinguish between similar classes, thus effectively mitigating the misclassification between old and new classes. Furthermore, our model can segment learned classes such as airplanes, trains, and handpans, demonstrating superior semantic segmentation performance after learning new classes. Moreover, compared to existing methods, our Fig. 8. Example of Multi-modal semantic drift. The image illustrates the phenomenon of multi-modal semantic drift. method achieves more complete segmentation masks and yields finer details of the objects. 2) Qualitive Analysis of AVSBench-CIM: Fig. 9 demonstrates comparison between our method and previous methods on AVSBench-CIM, highlighting the superior performance of our method in scenarios requiring the segmentation of JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, OCTOBER 2025 9 Fig. 9. We demonstrate the comparative performance of our method on the AVSBench-CIM dataset, where multiple objects often emit sounds simultaneously, thereby placing higher demands on the models ability to perform continuous audio-visual segmentation. Visualization studies on the AVSBench-CIM dataset demonstrate that our method consistently achieves robust and superior performance in complex scenarios containing multiple co-occurring objects. multiple targets. The figure presents three multi-target cases. In the first case, where the goal is to segment ukulele and man, our method achieves complete segmentation of both objects compared to previous methods while exhibiting significantly less class confusion. In the third case, while previous methods fail to segment the target object entirely, our method successfully segments most of the piano. These examples further prove the superiority of our method in multitarget audio-visual segmentation tasks. 3) Qualitive Analysis of collision classes: Experimental observations indicate that collision classes frequently cooccur in previous tasks, leading the model to perceive these classes as semantically similar. The statistics on the number of collision pairs in Fig. 7 validate our hypothesis. This phenomenon occurs because the model lacks prior semantic knowledge of new classes and tends to associate frequently co-occurring targets with similar features. Consequently, the forgetting process in continual learning can be viewed as the model correcting this cognitive bias after learning new classes, which often leads to catastrophic forgetting. 4) Qualitive examples of Multi-modal semantic drift: To better understand the Multi-modal semantic drift task, we present two examples from the AVSBench-CI 60-10 task. The classes guitar and drum were learned in step 0, while violin and woman are to be learned in step 1. During the learning process of step 1, guitar and drum are labeled as background. This causes their corresponding audio to be associated with background semantics, leading to the multimodal semantic drift. Fig. 10. Comparison of sample selection strategy. The image visualizes our method alongside the sample selection strategy based on maximum modality discrepancy, where samples selected exhibit greater consistency. 5) Effect analysis of Multi-modal Selection (MSS): As shown in Fig. 10, the samples selected by MSS exhibit the following characteristics: (l) Unlike samples with multiple targets. MSS tends to favour samples with single targets. (2) MSS prefers samples where the target is consistently present. (3) MSS prioritizes samples with better alignment between thetarget audio and visual modalities. This phenomenon aligns with our initial hypothesis, as these three types of samples typically exhibit less multi-modal semantic drift, thereby aiding the model in better retaining knowledge of old classes. Fig. 11. Example of co-occurence. The top part of the figure shows that during training, the violin and bassoon frequently co-occur. As result, during inference, the model mistakenly segments the bassoon as violin. 6) Qualitive examples of Co-occurence confusion: In Figure 11, we present example illustrating co-occurrence patterns in the data. During training, the classes violin and bassoon frequently co-occur across samples. At inference, the model correctly segments the spatial extent of the bassoon instance but erroneously assigns it the semantic label violin. This observation suggests that while the model has effectively captured discriminative visual features, it exhibits semantic confusion when aligning visual inputs with their corresponding audio-derived class labels. V. CONCLUSION In this paper, we introduce novel fine-grained multi-modal continual learning task: Continual Audio-Visual Segmentation. The task involves two critical challenges: multi-modal semantic drift and co-occurrence confusion. Through the collisionbased multi-modal rehearsal framework, which includes multi-modal sample selection and collision-based sample rehearsal strategy, we mitigate the incorrect modality semantic associations caused by these two challenges. Comprehensive experiments demonstrate the effectiveness of our method. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, OCTOBER"
        },
        {
            "title": "REFERENCES",
            "content": "[1] L. Wang, X. Zhang, H. Su, and J. Zhu, comprehensive survey of continual learning: Theory, method and application, IEEE Transactions on Pattern Analysis and Machine Intelligence (IEEE T-PAMI), 2024. [2] S.-A. Rebuffi, A. Kolesnikov, G. Sperl, and C. H. Lampert, icarl: Incremental classifier and representation learning, in IEEE conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 2001 2010. [3] J. Bang, H. Kim, Y. Yoo, J.-W. Ha, and J. Choi, Rainbow memory: Continual learning with memory of diverse samples, in IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021, pp. 82188227. [4] Z. Sun, Y. Mu, and G. Hua, Regularizing second-order influences for continual learning, in IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023, pp. 20 16620 175. [5] F. Cermelli, M. Mancini, S. R. Bulo, E. Ricci, and B. Caputo, Modeling the background for incremental learning in semantic segmentation, in IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 92339242. [6] S. Mo, W. Pian, and Y. Tian, Class-incremental grouping network for continual audio-visual learning, in IEEE/CVF International Conference on Computer Vision (ICCV), 2023, pp. 77887798. [7] W. Pian, S. Mo, Y. Guo, and Y. Tian, Audio-visual class-incremental learning, IEEE/CVF International Conference on Computer Vision (ICCV), pp. 77997811, 2023. [8] W. Pian, Y. Nan, S. Deng, S. Mo, Y. Guo, and Y. Tian, Continual audiovisual sound separation, in Advances in Neural Information Processing Systems (NeurIPS), vol. 37, 2025, pp. 76 05876 079. [9] J. Zhou, J. Wang, J. Zhang, W. Sun, J. Zhang, S. Birchfield, D. Guo, L. Kong, M. Wang, and Y. Zhong, Audiovisual segmentation, in European Conference on Computer Vision (ECCV). Springer, 2022, pp. 386403. [10] J. Zhou, X. Shen, J. Wang, J. Zhang, W. Sun, J. Zhang, S. Birchfield, D. Guo, L. Kong, M. Wang et al., Audio-visual segmentation with semantics, International Journal of Computer Vision (IJCV), pp. 121, 2024. [11] Q. Yang, X. Nie, T. Li, P. Gao, Y. Guo, C. Zhen, P. Yan, and S. Xiang, Cooperation does matter: Exploring multi-order bilateral relations for audio-visual segmentation, in IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024, pp. 27 13427 143. [12] X. Li, B. Tang, and H. Li, Adaer: An adaptive experience replay approach for continual lifelong learning, Neurocomputing, vol. 572, p. 127204, 2024. [13] M. Kang, J. Park, and B. Han, Class-incremental learning by knowledge distillation with adaptive feature consolidation, in IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022, pp. 16 07116 080. [14] O. Ostapenko, M. Puscas, T. Klein, P. Jahnichen, and M. Nabi, Learning to remember: synaptic plasticity driven framework for continual learning, in IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 11 32111 329. [15] H. Shin, J. K. Lee, J. Kim, and J. Kim, Continual learning with deep generative replay, in Advances in Neural Information Processing Systems (NeurIPS), vol. 30, 2017. [16] C. Wu, L. Herranz, X. Liu, J. Van De Weijer, B. Raducanu et al., Memory replay gans: Learning to generate new categories without forgetting, in Advances in Neural Information Processing Systems (NeurIPS), vol. 31, 2018. [17] H. Chen, P. Wang, Z. Zhou, X. Zhang, Z. Wu, and Y.-G. Jiang, Achieving more with less: Additive prompt tuning for rehearsal-free classincremental learning, IEEE/CVF International Conference on Computer Vision (ICCV), 2025. [18] A. Douillard, Y. Chen, A. Dapogny, and M. Cord, Plop: Learning without forgetting for continual semantic segmentation, in IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021, pp. 40404050. [19] Z. Zhang, G. Gao, Z. Fang, J. Jiao, and Y. Wei, Mining unseen classes via regional objectness: simple baseline for incremental segmentation, in Advances in Neural Information Processing Systems (NeurIPS), vol. 35, 2022, pp. 24 34024 353. [20] Q. Yang, X. Nie, L. Shi, J. Yu, F. Li, and S. Xiang, Continual semantic segmentation via scalable contrastive clustering and background diversity, in IEEE International Conference on Data Mining (ICDM), 2023, pp. 14751480. [21] S. Cha, Y. Yoo, T. Moon et al., Ssul: Semantic segmentation with unknown label for exemplar-based class-incremental learning, in Advances in Neural Information Processing Systems (NeurIPS), vol. 34, 2021, pp. 10 91910 930. [22] H. Yin, T. Feng, F. Lyu, F. Shang, H. Liu, W. Feng, and L. Wan, Beyond background shift: Rethinking instance replay in continual semantic segmentation, in Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), 2025, pp. 98399848. [23] Y. Mao, J. Zhang, M. Xiang, Y. Zhong, and Y. Dai, Multimodal variational auto-encoder based audio-visual segmentation, in IEEE/CVF International Conference on Computer Vision (ICCV), 2023, pp. 954 965. [24] K. Li, Z. Yang, L. Chen, Y. Yang, and J. Xiao, Catr: Combinatorialdependence audio-queried transformer for audio-visual video segmentation, in ACM International Conference on Multimedia (ACM MM), 2023, pp. 14851494. [25] S. Huang, R. Ling, T. Hui, H. Li, X. Zhou, S. Zhang, S. Liu, R. Hong, and M. Wang, Revisiting audio-visual segmentation with vision-centric transformer, in Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), 2025, pp. 83528361. [26] C. Liu, P. Li, L. Yang, D. Wang, L. Li, and X. Yu, Robust audiovisual segmentation via audio-guided visual convergent alignment, in Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), 2025, pp. 28 92228 931. [27] Y. Wang, H. Xu, Y. Liu, J. Li, and Y. Tang, Sam2-love: Segment anything model 2 in language-aided audio-visual scenes, in Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), 2025, pp. 28 93228 941. [28] C. Liu, L. Yang, P. Li, D. Wang, L. Li, and X. Yu, Dynamic derivation and elimination: Audio visual segmentation with enhanced audio semantics, in Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), 2025, pp. 31313141. [29] Y. Wei, S. Li, R. Feng, and D. Hu, Diagnosing and re-learning for balanced multimodal learning, in European Conference on Computer Vision (ECCV). Springer, 2024, pp. 7186. [30] Y. Wei and D. Hu, Mmpareto: Boosting multimodal learning with innocent unimodal assistance, in International Conference on Machine Learning (ICML). PMLR, 2024, pp. 52 55952 572. [31] J. Xiu, M. Li, Z. Yang, W. Ji, Y. Yin, and R. Zimmermann, Fewshot incremental learning via foreground aggregation and knowledge transfer for audio-visual semantic segmentation, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 39, no. 8, 2025, pp. 87888796. [32] Z. Guo, T. Jin, J. Chen, and Z. Zhao, Classifier-guided gradient modulation for enhanced multimodal learning, Advances in Neural Information Processing Systems (Neurlps), vol. 37, pp. 133 328133 344, 2024. [33] S. Wei, C. Luo, Y. Luo, and J. Xu, Privileged modality learning via multimodal hallucination, IEEE Transactions on Multimedia (TMM), vol. 26, pp. 15161527, 2023. [34] Y. Wei, R. Feng, Z. Wang, and D. Hu, Enhancing multimodal cooperation via sample-level modality valuation, in IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024, pp. 27 338 27 347. [35] Z. Li and D. Hoiem, Learning without forgetting, IEEE Transactions on Pattern Analysis and Machine Intelligence (IEEE T-PAMI), vol. 40, no. 12, pp. 29352947, 2018. [36] U. Michieli and P. Zanuttigh, Incremental learning techniques for semantic segmentation, in IEEE/CVF International Conference on Computer Vision workshops (ICCV), 2019. [37] S. Gao, Z. Chen, G. Chen, W. Wang, and T. Lu, Avsegformer: Audiovisual segmentation with transformer, in AAAI Conference on Artificial Intelligence (AAAI), vol. 38, no. 11, 2024, pp. 12 15512 163. [38] H. Chen, W. Xie, A. Vedaldi, and A. Zisserman, Vggsound: large-scale audio-visual dataset, in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020, pp. 721725. [39] S. Sahu, Df louvain: Fast incrementally expanding approach for community detection on dynamic graphs, arXiv preprint arXiv:2404.19634, 2024. [40] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for image recognition, in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 770778. [41] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, Imagenet: large-scale hierarchical image database, in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009, pp. 248255."
        }
    ],
    "affiliations": [
        "School of Intelligent Science and Technology, University of Science and Technology Beijing, Beijing 100083, China",
        "State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China",
        "State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China, and also with the School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing 100049, China"
    ]
}