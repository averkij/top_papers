{
    "paper_title": "PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model",
    "authors": [
        "Cheng Cui",
        "Ting Sun",
        "Suyin Liang",
        "Tingquan Gao",
        "Zelun Zhang",
        "Jiaxuan Liu",
        "Xueqing Wang",
        "Changda Zhou",
        "Hongen Liu",
        "Manhui Lin",
        "Yue Zhang",
        "Yubo Zhang",
        "Handong Zheng",
        "Jing Zhang",
        "Jun Zhang",
        "Yi Liu",
        "Dianhai Yu",
        "Yanjun Ma"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model tailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a compact yet powerful vision-language model (VLM) that integrates a NaViT-style dynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to enable accurate element recognition. This innovative model efficiently supports 109 languages and excels in recognizing complex elements (e.g., text, tables, formulas, and charts), while maintaining minimal resource consumption. Through comprehensive evaluations on widely used public benchmarks and in-house benchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document parsing and element-level recognition. It significantly outperforms existing solutions, exhibits strong competitiveness against top-tier VLMs, and delivers fast inference speeds. These strengths make it highly suitable for practical deployment in real-world scenarios."
        },
        {
            "title": "Start",
            "content": "PaddleOCR-VL: Boosting Multilingual Document Parsing via 0.9B Ultra-Compact Vision-Language Model Cheng Cui, Ting Sun, Suyin Liang, Tingquan Gao, Zelun Zhang, Jiaxuan Liu, Xueqing Wang, Changda Zhou, Hongen Liu, Manhui Lin, Yue Zhang, Yubo Zhang, Handong Zheng, Jing Zhang, Jun Zhang, Yi Liu, Dianhai Yu, Yanjun Ma PaddlePaddle Team, Baidu Inc. paddleocr@baidu.com Source Code: https://github.com/PaddlePaddle/PaddleOCR Models & Online Demo: https://huggingface.co/PaddlePaddle"
        },
        {
            "title": "Abstract",
            "content": "In this report, we propose PaddleOCR-VL, SOTA and resource-efficient model tailored for document parsing. Its core component is PaddleOCR-VL-0.9B, compact yet powerful visionlanguage model (VLM) that integrates NaViT-style dynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to enable accurate element recognition. This innovative model efficiently supports 109 languages and excels in recognizing complex elements (e.g., text, tables, formulas, and charts), while maintaining minimal resource consumption. Through comprehensive evaluations on widely used public benchmarks and in-house benchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document parsing and elementlevel recognition. It significantly outperforms existing solutions, exhibits strong competitiveness against top-tier VLMs, and delivers fast inference speeds. These strengths make it highly suitable for practical deployment in real-world scenarios. 5 2 0 2 6 1 ] . [ 1 8 2 5 4 1 . 0 1 5 2 : r Figure 1 Performance of PaddleOCR-VL on OmniDocBench v1.0 and v1.5."
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 PaddleOCR-VL 2.1 Architecture . . . 2.2 Training Recipe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Dataset 3.1 Data Curation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Automatic Data Annotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 Hard Cases Mining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Evaluation 4.1 Page-level Evaluation . . . 4.2 Element-level Evaluation . 4.3 Inference Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Conclusion Training Dataset Details A.1 Text . . A.2 Table . . . . . A.3 Formula . A.4 Chart . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Supported Languages Inference Performance on Different Hardware Configurations Real-world Samples D.1 Comprehensive Document Parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Layout Detection . D.3 Reading Order . . . D.4 Text Recognition . D.5 Table Recognition . . . . . . . . . D.6 Formula Recognition . D.7 Chart Recognition . . . Compare with Others E.1 Layout Detection . E.2 Text Recognition . . E.3 Table Recognition . . . . . . . E.4 Formula Recognition . E.5 Chart Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 4 4 9 9 10 10 10 13 17 18 25 25 27 28 30 31 32 37 40 42 51 53 58 59 61 67 69 2 1. Introduction Documents serve as core information carriers, with their complexity and volume growing at an exponential rate, making document parsing an indispensable key technology. The primary goal of document parsing [1, 2, 3, 4] is to enable deep structural and semantic understanding of documents layout. Specifically, it involves recognizing distinct text blocks and columns, distinguishing formulas, tables, charts, and images, determining the correct reading order, and detecting key elements (e.g., footnotes and image captions); these capabilities collectively lay solid foundation for efficient information retrieval and data management. Furthermore, advanced document parsing enables large language models (LLMs) [5, 6, 7], especially when combined with Retrieval-Augmented Generation (RAG) [8], to access high-quality knowledge and enhance their practical applications. The inherent complexity of modern documents presents unique challenges: they often combine dense text, complex tables or chart, mathematical expressions, multiple languages and handwritten texts, with deserve layout structures. Recent research [1, 9, 10, 11, 12] in the field of document parsing primarily following two technological approaches. The first approach [9, 10] employs pipeline methodologies based on specialized, modular expert models. Although these methods offer strong performance, they are increasingly hindered by integration complexity, cumulative error propagation, and inherent limitations when handling highly complex documents. Secondly, end-to-end approaches [12, 13, 14] leveraging multimodal models aim to simplify the workflow and enable joint optimization. However, these methods often struggle with correct text order and can even generate hallucinations when faced with lengthy or complex layouts, while also incurring substantial computational overhead for long sequence outputs, thereby restricting their practical deployment. To address these advancements and challenges, we present PaddleOCR-VL, high-performance, resource-efficient document parsing solution based on vision-language model. This innovation paves the way for the widespread application of multimodal document parsing, particularly in resource-constrained environments. PaddleOCR-VL combines robust layout analysis model with compact yet powerful vision-language model, PaddleOCR-VL-0.9B. Firstly, PaddleOCR-VL performs layout detection and reading order prediction to obtain the positional coordinates and reading order of elements (text blocks, tables, formulas, and charts). Compared to multimodal methods that rely on grounding and sequence output (e.g., MinerU2.5 [2], Dolphin [3]), our method offers faster inference speeds, lower training costs, and easier extensibility for new layout categories. Subsequently, the elements are segmented based on their positions and fed into PaddleOCR-VL-0.9B for recognition. This vision-language model is specifically designed for resource-efficient inference and excels at element recognition within document parsing. By integrating NaViT-style [15] dynamic high-resolution visual encoder with the lightweight ERNIE-4.5-0.3B [5] language model, we have significantly enhanced the models dense text recognition capabilities and decoding efficiency. To train powerful multimodal model, we have developed high-quality training data construction pipeline. We collected over 30 million training samples through public data acquisition and data synthesis. We meticulously designed prompt engineering to guide the automatic labeling by general large models, based on the recognition results of expert models. Simultaneously, We performed data cleaning to remove low-quality or inconsistent annotations, such as those caused by model hallucinations. We designed an evaluation engine, which is an assessment collection that categorizes each element into more detailed categories. Through this automated evaluation, we can analyze the current models training performance across different 3 types. This allows us to conduct targeted hard sample mining based on element types and to construct similar challenging examples through data synthesis. Finally, we incorporated manual annotation for small number of corner cases to complete the construction of the training data. Comprehensive benchmarking on the public benchmarks, including OmniDocBench v1.0, v1.5 [16] and olmOCR-Bench [12], and in-house ones demonstrate that PaddleOCR-VL achieves SOTA performance in document parsing task, significantly outperforming existing pipelinebased solutions and exhibiting strong competitiveness against leading vision-language models (VLMs). Moreover, PaddleOCR-VL is optimized for efficiency, delivering substantially lower latency and higher throughput than competing approaches. PaddleOCR-VL actively addresses current challenges in document processing with highperformance, resource-efficient multimodal document parsing solution. Its key contributions include: Compact yet Powerful VLM Architecture: We present novel vision-language model that is specifically designed for resource-efficient inference, achieving outstanding performance in element recognition. By integrating NaViT-style dynamic high-resolution visual encoder with the lightweight ERNIE-4.5-0.3B language model, we significantly enhance the models recognition capabilities and decoding efficiency. This integration maintains high accuracy while reducing computational demands, making it well-suited for efficient and practical document processing applications. High-quality Data Construction Methodology: We propose systematic and comprehensive methodology for constructing high-quality datasets, providing solid train data foundation for efficient and robust document parsing. This methodology not only enables us to construct high-quality data on demand, but also provides new perspective on the automated generation of high-quality data. SOTA Performance Document Parsing: PaddleOCR-VL achieves state-of-the-art performance in document parsing task. It excels in recognizing complex document elements, such as text, tables, formulas, and charts, making it suitable for wide range of challenging content types, including handwritten text and historical documents. Supporting 109 languages, including major global languages and those with diverse scripts like Russian, Arabic, and Hindi, PaddleOCR-VL is highly applicable to multilingual and globalized document processing scenarios. 2. PaddleOCR-VL 2.1. Architecture PaddleOCR-VL decomposes the complex task of document parsing into two stages, as illustrated in Figure 2. The first stage, PP-DocLayoutV2, is responsible for layout analysis, where it localizes semantic regions and predicts their reading order. Subsequently, the second stage, PaddleOCR-VL-0.9B, leverages these layout predictions to perform fine-grained recognition of diverse content, including text, tables, formulas, and charts. Finally, lightweight postprocessing module aggregates the outputs from both stages and formats the final document into structured Markdown and JSON. 2.1.1. Layout Analysis Considering that end-to-end approaches based on VLM rely on long-sequence autoregressive processes, which result in high latency and memory consumption, and increase the risk of 4 Figure 2 The overview of PaddleOCR-VL. unstable layout analysis and hallucinationsproblems that are particularly pronounced in multi-column or mixed textgraphic layoutswe employ dedicated lightweight model for layout analysis, focusing specifically on element detection, classification, and reading order prediction. Specifically, we decouple the layout analysis process by introducing an independent model, PP-DocLayoutV2, dedicated solely to this task. PP-DocLayoutV2 consists of an object detection model (RT-DETR [17]) for elements localization and classification, as well as lightweight pointer network [18] with six transformer layers to accurately predict the reading order of layout elements. This separation enables us to fully leverage the advanced capabilities of the vision model, which typically requires lower input image resolution, and contains significantly fewer parameters. As result, it achieves stable and accurate layout analysis, without the instability issues that may arise in end-to-end approaches. Figure 3 Architecture of layout analysis model. Architecturally, PP-DocLayoutV2 is composed of two sequentially connected networks, as shown in Figure 3. The first is an RT-DETR-based [17] detection model that performs layout element detection and classification. The detected bounding boxes and class labels are then passed to subsequent pointer network, which is responsible for ordering these layout elements. 5 Specifically, we first apply per-class thresholds to select foreground proposals for the ordering network. The selected proposals are embedded using absolute 2D positional encodings and class label embeddings. Additionally, the encoder attention incorporates geometric bias mechanism from Relation-DETR [18] to explicitly model pairwise geometric relationships among elements. The pairwise relation head linearly projects element representations into query and key vectors, then computes bilinear similarities to produce pairwise logits, resulting in an 𝑁 𝑁 matrix that represents the relative order between each pair of elements. Finally, deterministic win-accumulation decoding algorithm recovers topologically consistent reading order for the detected layout elements. In comparison to other specialized models, such as LayoutReader [19], our model achieves higher performance with fewer parameters by efficiently extending RT-DETR [17] with pointer network. 2.1.2. Element-level Recognition We systematically explore architecture configurations optimized for high accuracy and low computational overhead, and propose the PaddleOCR-VL-0.9B as shown in Figure 4. Figure 4 Architecture of PaddleOCR-VL-0.9B. We adopted an architectural style inspired by LLaVA [20], integrating pre-trained vision encoder with dynamic resolution preprocessor, randomly initialized 2-layer MLP projector, and pre-trained large language model. Our architecture achieves balance the scale of vision and language models to optimize performance in multi-elements recognition tasks. Compared to earlier document parsing models based on fixed-resolution or tiling-based approaches [4, 14, 21], our approach utilizes native dynamic high-resolution preprocessing. For the vision encoder, we employed NaViT-style [15] encoder initialized from Keye-VLs [22] vision model, which support native-resolution inputs. This design enables the vision-language model to handle images of arbitrary resolution without distortion, yielding fewer hallucinations and stronger performance on text-intensive tasks. The projector is randomly initialized 2-layer MLP with GELU [23] activation, incorporating merge size of 2 to efficiently bridge visual features from the encoder to the language models embedding space. In auto-regressive language models, the entire sequence is generated by predicting one token at time. This approach means that the size of the decoder is directly linked to the overall inference latency, so smaller model will decode faster. With this in mind, we use the ERNIE-4.5-0.3B [5] model, an open-source language model that balances relatively small number of parameters with strong inference efficiency. In our implementation, we further enhance positional representation by incorporating 3D-RoPE[24]. The combination of NaViT [15] with ERNIE-4.5-0.3B [5] has led to significant performance improvements in documents parsing, achieving minimal memory usage and faster inference speed. 2.2. Training Recipe The following sections introduce the training details of these two modules: PP-DocLayoutV2 for layout analysis and PaddleOCR-VL-0.9B for element recognition. 2.2.1. Layout Analysis We employ the PP-DocLayoutV2 model to perform layout element localization, classification, and reading order prediction. PP-DocLayoutV2 extends RT-DETR [17] by incorporating an additional pointer network [18], which is responsible for predicting the reading order of detected elements. The training process adopts two-stage strategy: we first train the core RT-DETR [17] model for layout detection and classification. Afterward, we freeze its parameters and independently train the pointer network for reading order prediction. For the first stage, we follow the training strategy of RT-DETR [17]. Specifically, we initialize the model with PP-DocLayout_Plus-L [25] pretrained weights and train it for 100 epochs on our self-constructed dataset comprising over 20,000 high-quality samples. For the second stage, specifically, the model outputs matrix representing the pairwise ordering relationships between any two elements, and the Generalized Cross Entropy Loss [26] is computed with respect to the ground truth labels, as this loss function demonstrates increased robustness in scenarios where pre-annotated data are mixed into the dataset. We utilize constant learning rate 2e-4 and the AdamW optimizer to train 200 epochs. 2.2.2. Element-level Recognition As described in Section 2.1.2, PaddleOCR-VL-0.9B consists of three modules: vision encoder, projector, and language model. We adopt post-adaptation strategy using pre-trained models. Specifically, the vision model is initialized with Keye-VLs weights, and the language model is initialized with ERNIE-4.5-0.3Bs weights. The model is trained based on the ERNIEKit [27] repository and the training methodology for our VLM is divided into two stages, as outlined in Table 1. Stage 1: The initial stage focuses on pre-training alignment, where the model learns to associate visual information from images with corresponding textual representations. This crucial step is performed on massive dataset comprising 29 million high-quality image-text pairs. During this phase, which runs for one epoch, the model is trained to establish coherent 7 Stages Stage 1 Stage 2 Training Samples Max Resolution Sequence length Trainable components Batch sizes Data Augmentation Maximum LR Minimum LR Epoch 29M 1280 28 28 16384 All 128 Yes 5 105 5 106 2.7M 2048 28 28 16384 All 128 Yes 5 106 5 107 2 Table 1 Training settings in stage 1 and stage 2. understanding between diverse visual inputs and their semantic textual content. The training utilizes batch size of 128, sequence length of 16384, and supports maximum image resolution of 12802828, with data augmentation enabled to improve robustness. For optimization, the learning rate is scheduled between maximum of 5 105 and minimum of 5 106. The primary objective is to align the feature spaces of the vision encoder and the language model, enabling them to jointly process multimodal information effectively. This large-scale pre-training allows the model to capture intricate visual patterns, common textual structures, and their interdependencies across vast range of contexts, laying strong foundation for subsequent specialized tasks. Stage 2: Following pre-training, the model undergoes instruction fine-tuning to adapt its general multimodal understanding to specific downstream elements recognition tasks. This stage utilizes meticulously curated dataset of 2.7 million samples, which is intentionally designed to be highly rich and diverse in its distribution. The training is conducted over two epochs, maintaining the 128 batch size and 16384 sequence length, but increasing the maximum resolution to 20482828 to handle more detailed inputs. finer learning rate is adopted, with the maximum and minimum values set to 5 106 and 5 107, to carefully adjust the model on specialized data. The richness of this dataset encompasses wide variety of document types, languages, writing systems, and visual complexities pertinent to real-world scenarios. During this fine-tuning phase, the model is trained with explicit instructions for four types of tasks: 1. OCR: This task fine-tunes the model to accurately identify and extract textual content from images, encompassing individual characters, words, text lines, text blocks and simple layout structure of page-level texts. 2. Table Recognition: The model learns to parse tabular structures within documents. This involves accurately extracting cell contents, identifying rows and columns, and recognize the logical relationships between different table elements, ultimately generating structured representations based on OTSL [28] format. 3. Formula Recognition: This instruction focuses on enabling the model to recognize and interpret mathematical and scientific formulas. It aims to convert their visual representation into structured LATEXformat and distinguishes between inline (...) and display [...] equations. 4. Chart Recognition: This task trains the model to recognition information from various types of charts, such as bar charts, line graphs, and pie charts and convert Markdown format tables. 8 3. Dataset To build our high-quality and diverse training dataset, we propose systematic methodology for constructing such datasets. As illustrated in Figure 5, we gather diverse set of data from multiple sources to ensure comprehensive coverage. High-quality labels are then generated through automated annotation using large models, which guarantees precision and consistency. Additionally, we refine the training data by integrating challenging examples, which enhances the models performance and robustness. Each of these crucial steps is detailed in the following sections. Figure 5 The construction process of training data for PaddleOCR-VL-0.9B. 3.1. Data Curation To ensure the breadth and diversity of the dataset, data is collected from four main sources: open-source dataset, synthesizing dataset, network accessible dataset, and in-house dataset. 1. Open Source Dataset: As the foundation of our dataset, we systematically aggregated and curated wide array of established public datasets. For textual content, we sourced data from the canonical dataset CASIA-HWDB [29]. Our mathematical expression data is derived from UniMER-1M [30] and MathWriting [31]. To ensure comprehensive coverage of data visualizations, we incorporated rich spectrum of chart and graph datasets, including ChartQA [32], PlotQA [33], Chart2Text [34], DVQA [35], Unichart [36], Beagle [37], ChartINFO [38], visText [39], and ExcelChart [40]. Each of these sources underwent an initial filtering and cleaning protocol to rectify or discard noisy and low-quality annotations. 2. Data Synthesizing Dataset: Due to the naturally imbalanced distribution of public data, we employed data synthesizing strategy to produce large volumes of missing data types at low cost, providing our proposed model with the unbiased document parsing performance. 3. Network Accessible Dataset: To improve model generalization and robustness against the complexities of unstructured real-world documents, we amassed an extensive corpus of publicly accessible data harvested from the Internet. This public collection was deliberately curated to encompass rich spectrum of document types and visual styles. It includes 9 academic papers, newspapers, formal scientific journal articles, scanned handwritten documents, diverse examination papers, and slides, etc. The integration of these varied sources proved instrumental in significantly broadening the stylistic, structural, and domain diversity of our training data, thereby mitigating the risk of overfitting to clean, canonical datasets. 4. In-house Dataset: Through years of research in the field of OCR, we have accumulated extensive datasets with diverse data types across all tasks of document parsing. We incorporate all in-house datasets into training with precisely controlled proportions, which have become unnecessary factors that enable our models to achieve outstanding performance. 3.2. Automatic Data Annotation After acquiring the raw data, we utilize an automatic data annotations process for large-scale labeling. Initially, we employ the expert model, PP-StructureV3, to conduct preliminary processing on the data, generating pseudo labels that may contain some inaccuracies. Subsequently, through prompt engineering, we create prompts that include the original images and their associated pseudo labels, which are then submitted to more advanced multimodal large language models, ERNIE-4.5-VL [5] and Qwen2.5VL [24]. These sophisticated models refine and enhance the initial results by analyzing the image content, resulting in improved labels. Finally, to ensure the quality of the labels, the system performs hallucination filtering step, which eliminates any potentially incorrect content generated by the large models, thereby producing reliable and high-quality labels. 3.3. Hard Cases Mining To overcome performance bottlenecks in specific complex scenarios, we propose hard case mining process for targeted performance improvement. We firstly develop eval engine for various types. We created substantial evaluation data with precisely labeled data obtained through manual annotation. Theses evaluation datasets are categorized into several types: text data includes 23 categories such as Chinese, English, printed, handwritten, Japanese, Latin, and emojis; table data includes 20 categories such as limited tables, unlimited tables, handwritten tables, checklists, invoices, and rotated tables; formula data includes 4 categories such as Chinese and English formulas, handwritten and printed, simple, and complex; chart data includes 11 categories such as Chinese and English charts, line charts, and bar charts, sourced from diverse origins to cover different document. By inference on this evaluation set and using corresponding professional metrics (e.g., EditDist for Text, TEDS [41] for Tables, RMS-F1 [42] for Charts, and BLEU [43] for Formulas), we can accurately identify hard cases where the model performs poorly. Finally, for these identified weaknesses, the system utilizes rich set of resources (such as Font Library, CSS Library, Corpus) and rendering tools (like XeLaTeX and web browsers) to synthetically generate large volume of new, high-quality hard cases. 4. Evaluation To thoroughly assess the effectiveness of PaddleOCR-VL, we compared it against leading general vision language models and specialized document parsing models across multiple public benchmarks and in-house benchmarks. We conducted comprehensive performance comparisons in two aspects: page-level document parsing and element-level recognition, which are detailed in Sections 4.1 and 4.2. Page-level involves analyzing entire pages of document to parsing their overall content, structure and layout, while element-level is dedicated exclusively 10 to assessing the recognition of specific elements, such as text, tables, formulas, and charts, within the document. 4.1. Page-level Evaluation This section details the evaluation of end-to-end document parsing capabilities using the following three benchmarks, aiming to measure its overall performance in real-world document scenarios. OmniDocBench v1.5 To comprehensively evaluate the document parsing capabilities, we conducted extensive experiments on the OmniDocBench v1.5 [2] benchmark. It is an expansion of version v1.0, adding 374 new documents for total of 1,355 document pages. It features more balanced distribution of data in both Chinese and English, as well as richer inclusion of formulas and other elements. The evaluation method has been updated, with formulas assessed using the CDM method. The overall metric is weighted combination of the metrics for text, formulas, and tables. Table 2 demonstrate that PaddleOCR-VL achieves SOTA performance, outperforming existing pipeline tools, general VLMs, and other specialized document parsing models across all key metrics. Specifically, our model achieves top-ranking overall score of 92.56, surpassing the next best model, MinerU2.5-1.2B (90.67). Moreover, our model establishes new SOTA results in the sub-tasks, including the lowest Text-Edit distance [44] of 0.035, the highest Formula-CDM score of 91.43, the leading scores of 89.76 and 93.52 in Table-TEDS and Table-TEDS-S, and the best readering ordering scores of 0.043, respectively. These results underscore its superior accuracy in text recognition, formula recognition, and complex table structure analysis. Model Type Methods Parameters Overall TextEdit FormulaCDM TableTEDS TableTEDS-S Reading OrderEdit Pipeline Tools General VLMs Specialized VLMs Marker-1.8.2 [45] Mineru2-pipeline [14] PP-StructureV3 [10] GPT-4o [7] InternVL3-76B [46] InternVL3.5-241B [47] Qwen2.5-VL-72B [24] Gemini-2.5 Pro [48] Dolphin [3] OCRFlux-3B [49] Mistral OCR [50] POINTS-Reader [4] olmOCR-7B [12] MinerU2-VLM [14] Nanonets-OCR-s [51] MonkeyOCR-pro-1.2B [1] MonkeyOCR-3B [1] dots.ocr [52] MonkeyOCR-pro-3B [1] MinerU2.5 [2] PaddleOCR-VL - - - - 76B 241B 72B - 322M 3B - 3B 7B 0.9B 3B 1.9B 3.7B 3B 3.7B 1.2B 0.9B 71.30 75.51 86.73 75.02 80.33 82.67 87.02 88.03 74.67 74.82 78.83 80.98 81.79 85.56 85.59 86.96 87.13 88.41 88.85 90.67 92.56 0.206 0.209 0.073 0.217 0.131 0.142 0.094 0. 0.125 0.193 0.164 0.134 0.096 0.078 0.093 0.084 0.075 0.048 0.075 0.047 0.035 76.66 76.55 85.79 79.70 83.42 87.23 88.27 85.82 67.85 68.03 82.84 79.20 86.04 80.95 85.90 85.02 87.45 83.22 87.25 88.46 91.43 57.88 70.90 81.68 67.07 70.64 75.00 82.15 85. 68.70 75.75 70.03 77.13 68.92 83.54 80.14 84.24 81.39 86.78 86.78 88.22 89.76 71.17 79.11 89.48 76.09 77.74 81.28 86.22 90.29 77.77 80.23 78.04 81.66 74.77 87.66 85.57 89.02 85.92 90.62 90.63 92.38 93.52 0.250 0.225 0.073 0.148 0.113 0.125 0.102 0. 0.124 0.202 0.144 0.145 0.121 0.086 0.108 0.130 0.129 0.053 0.128 0.044 0.043 Table 2 Comprehensive evaluation of document parsing on OmniDocBench v1.5. Results are reported by OmniDocBench [16] unless Ours. OmniDocBench v1.0 publicly available benchmark dataset specifically is designed to evaluate real-world document parsing capabilities. It comprises 981 PDF pages, spanning 9 distinct 11 document types, 4 layout styles, and 3 language categories. Based on the experimental results presented in Table 3, PaddleOCR-VL demonstrates superior performance with an average overall edit distance of 0.115, demonstrating its superior capability in document parsing. The model excels in formula edit distance (0.241 EN, 0.316 ZH), and achieves the SOTA performance (0.062) and comparable SOTA performance (0.041) for Chinese and English text edit distance respectively, showcasing its accuracy in handling textual and formulaic data. Although the model exhibits slightly lower performance in the English Table TEDS (88.0), this can be largely attributed to typo-related annotation errors in OmniDocBench v1.0. Nevertheless, it demonstrates clear advantage in the Chinese Table TEDS (92.14). Regarding the reading order edit distance, the model achieves the best performance in Chinese (0.063) and comparable SOTA result in English (0.045), emphasizing its capability to maintain structural integrity and logical document flow. Method Type Methods AvgOverallEdit OverallEdit ZH EN TextEdit EN ZH FormulaEdit TableTEDS EN ZH EN ZH TableEdit ZH EN Reading OrderEdit EN ZH Pipeline Tools General VLMs Specialized VLMs Docling-2.14.0 [11] OpenParse-0.7.0 [53] Unstructured-0.17.2 [54] Pix2Text-1.1.2.3 [55] Marker-1.7.1 [45] Mathpix [56] MinerU-pipeline [9] PP-StructureV3 [10] InternVL2-76B [57] GPT-4o [7] InternVL3-78B [46] Qwen2.5-VL-72B [24] Gemini2.5-Pro [48] Nougat [58] SmolDocling-256M [13] olmOCR-7B [12] GOT [21] OCRFlux-3B [49] Nanonets-OCR-s [51] Dolphin [3] MinerU2-VLM [14] MonkeyOCR-pro-1.2B [1] MonkeyOCR-pro-3B [1] dots.ocr [52] MinerU2.5 [2] PaddleOCR-VL 0.749 0.730 0.651 0.424 0.397 0.278 0.203 0. 0.442 0.316 0.257 0.238 0.180 0.713 0.655 0.398 0.349 0.294 0.289 0.259 0.186 0.184 0.172 0.143 0.143 0.115 0.589 0.646 0.586 0.320 0.296 0.191 0.162 0.145 0.440 0.233 0.218 0.214 0.148 0.452 0.493 0.326 0.287 0.238 0.283 0.205 0.133 0.146 0.138 0.125 0.111 0.105 0.909 0.814 0.716 0.528 0.497 0.364 0.244 0. 0.443 0.399 0.296 0.261 0.212 0.973 0.816 0.469 0.411 0.349 0.295 0.313 0.238 0.221 0.206 0.160 0.174 0.126 0.416 0.681 0.198 0.138 0.085 0.105 0.072 0.058 0.353 0.144 0.117 0.092 0.055 0.365 0.262 0.097 0.189 0.112 0.134 0.092 0.045 0.068 0.067 0.032 0.050 0. 0.987 0.974 0.481 0.356 0.293 0.381 0.111 0.088 0.290 0.409 0.210 0.180 0.168 0.998 0.838 0.293 0.315 0.256 0.231 0.204 0.115 0.118 0.107 0.066 0.074 0.062 0.999 0.996 0.999 0.276 0.374 0.306 0.313 0.295 0.543 0.425 0.380 0.315 0.356 0.488 0.753 0.455 0.360 0.447 0.518 0.447 0.273 0.272 0.246 0.329 0.258 0. 1 1 1 0.611 0.688 0.454 0.581 0.535 0.701 0.606 0.533 0.434 0.439 0.941 0.997 0.655 0.528 0.716 0.546 0.606 0.506 0.452 0.421 0.416 0.473 0.316 61.3 64.8 0 73.6 67.6 77.0 77.4 77.2 63.0 72.0 69.0 81.4 85.8 39.9 44.9 68.1 53.2 69.0 76.8 76.1 82.1 81.3 81.5 88. 88.3 88.0 25.0 27.5 0.1 66.2 54.0 67.1 79.5 83.9 60.2 62.9 73.9 83.0 86.4 0.0 16.5 61.3 47.2 80.0 79.4 66.9 83.4 85.5 87.5 89.0 89.2 92.1 0.627 0.284 1 0.584 0.609 0.243 0.166 0.159 0.547 0.234 0.279 0.341 0. 0.572 0.729 0.608 0.459 0.269 0.343 0.193 0.150 0.149 0.139 0.099 0.089 0.093 0.810 0.639 0.998 0.645 0.678 0.320 0.150 0.109 0.555 0.329 0.282 0.262 0.119 1 0.907 0.652 0.520 0.162 0.201 0.282 0.209 0.134 0.111 0.092 0.083 0.062 0.313 0.595 0.145 0.281 0.116 0.108 0.097 0. 0.317 0.128 0.095 0.106 0.049 0.382 0.227 0.145 0.141 0.126 0.135 0.088 0.066 0.093 0.100 0.040 0.045 0.045 0.837 0.641 0.387 0.499 0.329 0.304 0.136 0.091 0.228 0.251 0.161 0.168 0.121 0.954 0.522 0.277 0.280 0.263 0.200 0.160 0.122 0.179 0.185 0.067 0.068 0. Table 3 Comprehensive evaluation of document parsing on OmniDocBench v1.0. Results are reported by OmniDocBench [16] unless MinerU2.5 and Ours. olmOCR-Bench olmOCR-Bench [12] includes 1,402 PDF documents and 7,010 test cases, addressing diverse document types and extraction challenges. It offers detailed evaluation framework for PDF content extraction by assessing tools and models through simple, clear, and machine-verifiable unit tests. This approach avoids biased evaluations and soft metric comparisons, allowing for the detection of subtle but significant extraction errors. Table 4 highlights the outstanding performance of PaddleOCR-VL in the olmOCR-Bench evaluation, achieving the highest overall score of 80.0 1.0. It excels in various categories, leading in ArXiv (85.7), Headers and Footers (97.0) and securing second place in Multi-column text (79.9), Long Tiny Text (85.7). These results highlight the proposed models capability to effectively manage diverse document types, reinforcing its status as top solution in document parsing and its reliability in complex OCR tasks. 12 Methods Overall ArXiv Old Scans Math Tables Old Scans Headers and Footers Multi column Long Tiny Text Base Unit Test Pass Rate GOT [21] Gemini Flash 2 (No Anchor) [48] MinerU-pipeline [9] Gemini Flash 2 (Anchored) [48] Nanonets-OCR-s [51] Qwen2.5-VL-7B (No Anchor) [24] GPT-4o (No Anchor) [7] GPT-4o (Anchored) [7] Marker-1.8.2 [45] olmOCR v0.1.75 (No Anchor) [12] olmOCR v0.1.75 (Anchored) [12] MonkeyOCR-pro-3B [1] MinerU2.5 [2] dots.ocr [52] PaddleOCR-VL 48.3 1.1 57.8 1.1 61.5 1.1 63.8 1.2 64.5 1.1 65.5 1.2 68.9 1.1 69.9 1.1 70.1 1.1 74.7 1.1 75.5 1.0 75.8 1.0 77.5 1.0 79.1 1.0 80.0 1.0 52.7 32.1 75.4 54.5 67.0 63.1 51.5 53.5 76.0 71.5 74.9 83.8 81.1 82.1 85.7 52.0 56.3 47.4 56.1 68.6 65.7 75.5 74.5 57.9 71.4 71.2 68.8 74.0 64.2 71.0 0.2 61.4 60.9 72.1 77.7 67.3 69.1 70.0 57.6 71.4 71.0 74.6 85.1 88.3 84. 22.1 27.8 17.3 34.2 39.5 38.6 40.9 40.7 27.8 42.8 42.2 36.1 33.8 40.9 37.8 93.6 48.0 96.6 64.7 40.7 73.6 94.2 93.8 84.9 94.1 94.5 91.2 96.3 94.1 97.0 42.0 58.7 59.0 61.5 69.9 68.3 68.9 69.3 72.9 77.7 78.3 76.6 65.5 82.4 79.9 29.9 84.4 39.1 71.5 53.4 49.1 54.1 60.6 84.6 71.0 73.3 80.1 89. 81.2 85.7 94.0 94.0 96.6 95.6 99.3 98.3 96.7 96.8 99.1 97.8 98.3 95.3 94.4 99.5 98.5 Table 4 Comprehensive evaluation of document parsing on olmOCR-Bench. Results are reported by olmOCR-Bench [12] unless MinerU2.5 and Ours. 4.2. Element-level Evaluation This section centers on evaluating the element-level capabilities of PaddleOCR VL 0.9B. We thoroughly assessed four tasks: text, tables, formulas, and charts using both public competition data and in-house data. 4.2.1. Text Recognition For text recognition, we utilize three benchmarks to validate the effectiveness of models based on the edit distance metric. OmniDocBench-OCR-block: From the 1355 images of OmniDocBench v1.5, we extracted all text-related sub-images based on layout detection labels, removing any with null annotations. This process resulted in total of 17,148 block-level images. This evaluation set is named OmniDocBench-OCR-block, with the ground truth still sourced from OmniDocBench. This evaluation set can more accurately assess the models text recognition performance on without being affected by layout detection. We use the average normalized edit distance for evaluation. In Table 5, we present comprehensive comparison of performance across various document types using different models. Our model, PaddleOCR-VL, consistently demonstrates superior performance, achieving the lowest error rates in almost all categories. Specifically, PaddleOCRVL achieves the best results in the PPT2PDF (0.049), Academic Literature (0.021), Book (0.045), Colorful Textbook (0.081), Exam Paper (0.115), Magazine (0.020), Newspaper (0.034), Note (0.081), and Research Report (0.033) categories. These results highlight PaddleOCR-VLs robust and versatile capability in handling diverse document types, establishing it as the leading method in the OmniDocBench-OCR-block performance evaluation. 13 Methods PPT2PDF Academic Literature Qwen2.5-VL-72B [24] MonkeyOCR-pro-3B [1] MinerU2.5 [2] Dolphin [3] PaddleOCR-VL 0.054 0.058 0.195 0.237 0.049 0.023 0.021 0.089 0.095 0.021 Book 0.061 0.064 0.111 0.135 0. Colorful Textbook Edit Distance Exam Paper Magazine Newspaper Note 0.084 0.096 0.234 0.347 0.081 0.195 0.116 0.194 0.248 0.115 0.032 0.023 0.147 0.233 0. 0.056 0.058 0.056 0.121 0.034 0.118 0.124 0.142 0.309 0.081 Research Report 0.040 0.052 0.094 0.213 0.033 Table 5 Overall Comparison of OmniDocBench-OCR-block Performance. In-house-OCR: This is our self-built line-level text evaluation dataset which contains 107452 samples with high-quality labels. The dataset includes various text types such as handwritten Chinese, handwritten English, printed Chinese, printed English, traditional Chinese, ancient texts, general scenarios, Pinyin, obscure characters, vertical text, single characters, emojis, and artistic fonts. It also comprises evaluation sets for 109 languages, such as Latin and Japanese. Table 6 provides detailed evaluation of performance across multiple languages and text types. In the Multilingual Metrics (Table 6a), the model demonstrates outstanding accuracy with the lowest edit distances in all evaluated scripts: Arabic(0.122), Korean(0.052), Tamil(0.043), Greek(0.135), Thai(0.081), Telugu (0.114), Devanagari (0.097), Cyrillic (0.109), Latin (0.013), and Japanese (0.096), indicating superior capability in handling diverse languages. Similarly, in the Text Type Metrics (Table 6b), it excels in various text types, achieving the lowest error rates in categories like Handwritten CN (0.089), Handwritten EN (0.042), Printed CN (0.035), Printed EN (0.016), Traditional Chinese (0.048), Ancient Texts(0.198), General Scene (0.067), Pinyin (0.113), Rare Characters (0.001), Vertical Text (0.005), Single Characters (0.027), Emoji (0.057), and Art Font (0.165). These impressive results underscore the models robust performance and versatility, establishing it as the leading OCR solution in this benchmark comparison. Methods Arabic Korean Tamil Greek Edit Distance Telugu Thai Devanagari Cyrillic Latin Japanese Qwen2.5-VL-72B [24] Dolphin [3] MonkeyOCR-pro-3B [1] MinerU2.5 [2] PaddleOCR-VL 0.405 0.682 0.601 0.978 0.122 0.056 0.699 0.182 0.917 0.052 0.389 0.912 0.921 0.957 0.043 0.165 0.691 0.449 0.661 0.135 0.194 0.709 0.876 0.880 0. 0.758 0.832 0.909 0.937 0.011 0.164 0.818 0.896 0.915 0.097 0.220 0.549 0.387 0.832 0.109 0.021 0.037 0.036 0.063 0.013 0.181 0.309 0.262 0.588 0.086 Methods (a) Multilingual Metrics. Edit Distance Handwritten CN Handwritten EN Printed CN Printed EN Trad. Chinese Ancient Texts General Scene Pinyin Rare Char. Vertical Text Single Char. Dolphin [3] MonkeyOCR-pro-3B [1] Qwen2.5-VL-72B [24] MinerU2.5 [2] PaddleOCR-VL 0.236 0.253 0.188 0.370 0.089 0.145 0.071 0.047 0.088 0.042 0.074 0.048 0.037 0.041 0.035 0.025 0.023 0.018 0.023 0.016 0.095 0.295 0.100 0.232 0. 0.218 0.529 0.387 0.950 0.198 0.113 0.144 0.122 0.179 0.067 0.183 0.165 0.186 0.256 0.113 0.092 0.063 0.034 0.048 0.001 0.190 0.086 0.090 0.962 0.005 0.202 0.110 0.041 0.097 0. 0.225 0.184 0.134 0.174 0.057 (b) Text Type Metrics. Table 6 Comparison of In-house-OCR Edit Distance Performance. Emoji Art Font 0.230 0.263 0.220 0.337 0. Ocean-OCR-Handwritten: This is line and paragraph levels handwritten evaluation dataset designed for comprehensive handwriting recognition assessment. It contains 400 samples, evenly divided into four subsets of 100 images each. The dataset covers both real and synthetic 14 handwriting in Chinese and English. Real samples are collected from established handwriting datasets such as CASIA-HWDB [29], GNHK [59], and BRUSH [60], while synthetic samples are generated to simulate diverse writing styles, character densities, and layouts. The benchmark aims to provide balanced and fine-grained evaluation for handwritten text recognition across different scripts and writing conditions. Table 7 presents comparison of OCR performance for handwritten English and Chinese text on the Ocean-OCR-Bench. Our model demonstrates superior performance across all metrics in both languages. For English, it achieves the best edit distance of 0.118 and excels in F1-score, Precision, Recall, BLEU, and METEOR, establishing itself as the leading model. In Chinese, PaddleOCR-VL sets benchmark with an edit distance of 0.034 and leads in all other metrics, showcasing its outstanding precision and reliability. Methods InternVL2.5-4B [57] MiniCPM-V2.6-8B [61] Qwen2-VL-7B [62] GOT [21] PaddleOCR [10] TextIn Ocean-OCR [63] MinerU2.5 [2] PaddleOCR-VL Edit Distance EN ZH F1-score ZH EN Precision ZH EN Recall BLEU EN ZH EN ZH METEOR ZH EN 0.197 0.147 0.127 0.616 0.418 0.358 0.145 0.238 0.118 0.240 0.175 0.113 0.402 0.325 0.180 0. 0.356 0.034 0.661 0.727 0.760 0.283 0.237 0.362 0.774 0.558 0.750 0.741 0.810 0.881 0.568 0.664 0.840 0. 0.619 0.957 0.674 0.747 0.773 0.309 0.232 0.368 0.780 0.547 0.748 0.754 0.811 0.884 0.618 0.646 0.869 0. 0.623 0.959 0.655 0.714 0.754 0.273 0.263 0.362 0.782 0.574 0.753 0.734 0.812 0.884 0.544 0.700 0.822 0. 0.622 0.957 0.406 0.443 0.490 0.151 0.069 0.098 0.532 0.344 0.551 0.473 0.583 0.666 0.295 0.431 0.567 0. 0.489 0.856 0.652 0.727 0.756 0.255 0.236 0.337 0.772 0.553 0.787 0.687 0.774 0.859 0.492 0.648 0.751 0. 0.601 0.936 Table 7 Comparison of performance on English(EN) and Chinese(ZH) OCR for handwritten recognition on Ocean-OCR-Bench. Results are reported by Ocean-OCR [63] unless MinerU2.5 and Ours. 4.2.2. Table Recognition. For table recognition, we utilize two benchmarks to validate the effectiveness of PaddleOCRVL-0.9B based on TEDS [41] and Edit Distance. OmniDocBench-Table-block: To evaluate the table recognition performance of PaddleOCRVL, we crop 512 tables from OmniDocBench v1.5 datasets. As shown in Table 8, our PaddleOCR-VL leads in the OmniDocBench-Table-block benchmark, surpassing all competitors. It achieves top overall TEDS of 0.9195, reflecting high accuracy in capturing table structure and content. Its structural TEDS of 0.9543 highlights its ability to parse complex structures, while the lowest Overall Edit Distance of 0.0561 indicates minimal recognition errors. These results confirm PaddleOCR-VLs superior performance and establish it as the benchmark for accurate table recognition. Methods Overall TEDS Structural TEDS Overall Edit Dist MinerU2-VLM [14] Seed1.6 dots.ocr [52] MinerU2.5 [2] PaddleOCR-VL 0.9002 0.9079 0.8194 0.9005 0. 0.9369 0.9489 0.8442 0.9539 0.9543 0.0734 0.0652 0.1508 0.0693 0.0561 Table 8 Comparison of OmniDocBench-Table-block Performance 15 In-house-Table: Our self-built evaluation set contains diverse array of table images with comprehensive annotations and type classifications. It includes 20 different table types such as Chinese, English, mixed Chinese-English, and tables with various characteristics like full, partial, or no borders. The collection also covers tables with formulas, dense data, book/manual formats, lists, academic papers, merged cells, as well as low-quality, watermarked, registration forms, statistical forms, research reports, financial reports, images, invoices, and handwritten tables, among others. Table 9 provides comparison of different methods on the In-house-Table task, highlighting their performance across various metrics. We achieves the highest scores in Overall TEDS (0.8699), Structural TEDS (0.9066), Overall Edit Distance (0.9066) and Structural Edit Distance (0.9339). These results underscore PaddleOCR-VLs effectiveness and reliability in table recognition tasks. Methods Overall TEDS Structural TEDS Overall Edit Dist Structural Edit Dist MinerU2-VLM [14] MonkeyOCR [1] Nanonets-OCR-s [51] OCRFlux-3B [49] Qwen2.5-VL-3B [24] Qwen2.5-VL-7B [24] Qwen2.5-VL-72B [24] dots.ocr [52] MinerU2.5 [2] PaddleOCR-VL 0.8286 0.7396 0.7824 0.7741 0.7398 0.7549 0.7762 0.7547 0.8469 0.8699 0.8730 0.7824 0.8190 0.8071 0.7765 0.7926 0.8361 0.7914 0.8955 0.9066 0.8757 0.8174 0.8377 0.8238 0.8132 0.8251 0.843 0.8047 0.8896 0.9066 0.9088 0.8537 0.8692 0.8617 0.8701 0.8819 0.8987 0.8361 0.9239 0.9339 Table 9 Comparison of In-house-Table Performance 4.2.3. Formula Recognition. For formula recognition, we validate the effectiveness our model based on the Character Detection Matching (CDM) [64] metric on OmniDocBench-Formula-block and In-house-Formula datasets. OmniDocBench-Formula-block Using the formula bounding boxes from OmniDocBench v1.5, 1050 formula sub-images were cropped. This step was taken to minimize the influence of layout detection on formula recognition. As shown in Table 10, the model achieved state-of-the-art CDM score of 0.9453. Methods Overall CDM EN CDM ZH CDM dots.ocr [52] MinerU2-VLM [14] MonkeyOCR-pro-1.2B [1] MonkeyOCR-3B [1] Qwen2.5-VL-72B [24] MinerU2.5 [2] PaddleOCR-VL 0.4641 0.8286 0.8531 0.8621 0.8747 0.9187 0.9453 0.4868 0.9616 0.9642 0.9718 0.9574 0.9751 0.9677 0.4414 0.6956 0.7419 0.7524 0.7920 0.8623 0.9228 Table 10 Comparison of OmniDocBench v1.5 Formula-block Performance. Due to dots.ocr [52] easily recognizing cropped formulas as images, the score is relatively low. In-house-Formula: The self-constructed formula evaluation set contains 34,816 samples, covering common formula recognition scenarios such as academic papers, mathematics books, and primary and secondary school exam papers. Among them, there are 498 Chinese formulas and 34,318 English formulas. As shown in Table 11, our model obtains the best performance of 16 0.9882 CDM score on the In-house-Formula dataset. These results collectively demonstrate the powerful recognition capability of PaddleOCR-VL in real-world complex formula scenarios. Methods Overall CDM EN CDM ZH CDM dots.ocr [52] MinerU2-VLM [14] MonkeyOCR-pro-1.2B [1] MonkeyOCR-3B [1] Qwen2.5-VL-72B [24] MinerU2.5 [2] PaddleOCR-VL 0.6737 0.9237 0.9537 0.9566 0.9412 0.9770 0.9882 0.8066 0.9764 0.9656 0.9761 0.9519 0.9832 0.9914 0.5408 0.8709 0.9417 0.9371 0.9304 0.9708 0.9849 Table 11 Comparison of In-house-Formula Performance. Due to dots.ocr [52] easily recognizing cropped formulas as images, the score is relatively low. 4.2.4. Chart Recognition. For chart recognition, considering the limitations in dataset size, the imbalanced distribution of chart categories, and the poor annotation quality of publicly available test sets, we only utilize in-house benchmark to validate the effectiveness of PaddleOCR-VL-0.9B based on the RMS-F1 [42] score metric. As shown in Table 12, the proposed PaddleOCR-VL not only outperforms expert OCR VLMs but also surpasses some 72B-level multimodal language models. In-house-Chart: This in-house chart recognition evaluation set comprises 1,801 samples, all of which have underwent rigorous manual review to ensure annotation correctness. The evaluation set is broadly categorized into 11 chart categories, including bar-line hybrid, pie, 100% stacked bar, area, bar, bubble, histogram, line, scatterplot, stacked area, and stacked bar. Of these samples, 851 are in English and 950 are in Chinese. Prior to evaluation, both the predicted data table and the ground truth data table are normalized to uniform markdown format to eliminate expression ambiguities. Methods TinyChart [65] GOT [21] OneChart [66] Qwen2.5-VL-3B [24] Qwen2.5-VL-7B [24] Qwen2.5-VL-72B [24] PP-StructureV3 [10] PaddleOCR-VL RMS-F1 Overall EN ZH 0.2159 0.3160 0.3716 0.5942 0.6821 0.7300 0.8060 0.8440 0.4726 0.1100 0.1384 0.5619 0.5876 0.6972 0.7963 0.8222 0.0876 0.4190 0.4882 0.6103 0.7293 0.7464 0.8109 0.8549 Table 12 Comparison of In-house-Chart Performance 4.3. Inference Performance To improve the inference performance of PaddleOCR-VL, we introduce multi-threading asynchronous execution into the inference workflow. The process is divided into three main stagesdata loading (e.g., rendering PDF pages as images), layout model processing, and VLM inferenceeach running in separate thread. Data is transferred between adjacent stages via queues, enabling concurrent execution for higher efficiency. In particular, for VLM inference, batch processing is only triggered when either the number of items in the queue reaches predefined threshold or the waiting time for queued data exceeds specified limit. This design allows blocks across different pages to be aggregated and processed together, thereby 17 maximizing parallelism, especially when handling large volumes of files. We further deploy PaddleOCR-VL-0.9B on high-throughput inference and serving engines [67, 68, 69], tuning parameters like max-num-batched-tokens and gpu-memory-utilization to balance inference throughput with GPU memory consumption. We measured the end-to-end inference speed and GPU usage on the OmniDocBench v1.0 dataset, processing PDF files in batches of 512 on single NVIDIA A100 GPU. By \"end-to-end\", we mean that the inference time was measured from providing the PDF file path as input to the complete generation of the Markdown text. For MonkeyOCR, dots.ocr, and MinerU, inference was run with the vLLM backend and the default configuration (including the KV cache settings). The generated Markdown text was tokenized with the \"cl100k_base\" tokenizer to compute the number of output tokens. For dots.ocr specifically, 200 threads were used for concurrent page processing, and the Base64-encoded image content in the produced Markdown text was replaced with dummy path (UUID-based, prefixed with \"images/\" and suffixed with \".png\") to ensure reasonable token count. Table 13 provides comprehensive comparison of inference efficiency across different methods. The proposed PaddleOCR-VL demonstrates clear and consistent advantages in both processing speed and memory efficiency. When deployed with the vLLM backend, it achieves 15.8% higher page throughput and 14.2% higher token throughput than the leading baseline, MinerU2.5, establishing itself as the most efficient solution overall. In addition, PaddleOCR-VL achieves notable memory savings, using roughly 40% less GPU memory than dots.ocr while sustaining significantly faster processing. These results collectively confirm that PaddleOCR-VL attains state-of-the-art inference efficiency through balanced optimization of speed and memory usage, making it highly suitable for real-world, high-throughput document understanding scenarios. Methods Total Time (s) Pages/s Tokens/s Avg. VRAM Usage (GB) MonkeyOCR-pro-1.2B [1] dots.ocr [52] MinerU2.5 [2] PaddleOCR-VL PaddleOCR-VL 1456.4 2784.6 927.3 800.9 917.6 0.6730 0.3522 1.0574 1.2241 1.0684 1120.3 532.9 1647.9 1881.2 1641.5 75.5 78.5 41.9 43.7 49.8 Table 13 End-to-End Inference Performance Comparison. denotes the vLLM backend, and denotes the SGLang backend. 5. Conclusion This report introduces PaddleOCR-VL, an advanced and efficient model for document parsing that excels at both element-level and page-level recognition. Its core componets, PaddleOCR-VL0.9B, built with NaViT-style visual encoder and ERNIE-4.5-0.3B language model, it accurately recognizes complex elements such as text, tables, formulas, and charts in over 100 languages. PaddleOCR-VL achieves fast inference and low resource consumption, making it practical for real-world deployment. It outperforms existing pipeline solutions on many benchmarks and effectively handles challenging content including handwriting and historical documents, as well as converting chart visuals into structured data. Its broad multilingual support and strong performance have the potential to advance the application and development of multimodal document processing technologies, bringing innovation to automated analysis and information retrieval. This will significantly enhance the performance and stability of RAG systems, making information extraction from complex documents more efficient, thereby providing more reliable data support for future AI applications."
        },
        {
            "title": "References",
            "content": "[1] Zhang Li, Yuliang Liu, Qiang Liu, Zhiyin Ma, Ziyang Zhang, Shuo Zhang, Zidun Guo, Jiarui Zhang, Xinyu Wang, and Xiang Bai. Monkeyocr: Document parsing with structurerecognition-relation triplet paradigm. arXiv preprint arXiv:2506.05218, 2025. [2] Junbo Niu, Zheng Liu, Zhuangcheng Gu, Bin Wang, Linke Ouyang, Zhiyuan Zhao, Tao Chu, Tianyao He, Fan Wu, Qintong Zhang, et al. Mineru2. 5: decoupled vision-language model for efficient high-resolution document parsing. arXiv preprint arXiv:2509.22186, 2025. [3] Hao Feng, Shu Wei, Xiang Fei, Wei Shi, Yingdong Han, Lei Liao, Jinghui Lu, Binghong Wu, Qi Liu, Chunhui Lin, et al. Dolphin: Document image parsing via heterogeneous anchor prompting. arXiv preprint arXiv:2505.14059, 2025. [4] Yuan Liu, Zhongyin Zhao, Le Tian, Haicheng Wang, Xubing Ye, Yangxiu You, Zilin Yu, Chuhan Wu, Xiao Zhou, Yang Yu, et al. Points-reader: Distillation-free adaptation of vision-language models for document conversion. arXiv preprint arXiv:2509.01215, 2025. [5] Baidu-ERNIE-Team. Ernie 4.5 technical report, 2025. [6] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [7] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [8] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrievalaugmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:94599474, 2020. [9] Bin Wang, Chao Xu, Xiaomeng Zhao, Linke Ouyang, Fan Wu, Zhiyuan Zhao, Rui Xu, Kaiwen Liu, Yuan Qu, Fukai Shang, et al. Mineru: An open-source solution for precise document content extraction. arXiv preprint arXiv:2409.18839, 2024. [10] Cheng Cui, Ting Sun, Manhui Lin, Tingquan Gao, Yubo Zhang, Jiaxuan Liu, Xueqing Wang, Zelun Zhang, Changda Zhou, Hongen Liu, et al. Paddleocr 3.0 technical report. arXiv preprint arXiv:2507.05595, 2025. [11] Docling Team. Docling. https://github.com/docling-project/docling, 2024. Accessed: 2025-06-23. [12] Jake Poznanski, Jon Borchardt, Jason Dunkelberger, Regan Huff, Daniel Lin, Aman Rangapur, Christopher Wilhelm, Kyle Lo, and Luca Soldaini. olmocr: Unlocking trillions of tokens in pdfs with vision language models. arXiv preprint arXiv:2502.18443, 2025. [13] Ahmed Nassar, Andres Marafioti, Matteo Omenetti, Maksym Lysak, Nikolaos Livathinos, Christoph Auer, Lucas Morin, Rafael Teixeira de Lima, Yusik Kim, Said Gurbuz, et al. Smoldocling: An ultra-compact vision-language model for end-to-end multi-modal document conversion. arXiv preprint arXiv:2503.11576, 2025. 19 [14] opendatalab. Mineru2.0-2505-0.9b. https://huggingface.co/opendatalab/Miner U2.0-2505-0.9B, 2025. [15] Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Patch npack: Navit, vision transformer for any aspect ratio and resolution. Advances in Neural Information Processing Systems, 36:22522274, 2023. [16] Linke Ouyang, Yuan Qu, Hongbin Zhou, Jiawei Zhu, Rui Zhang, Qunshu Lin, Bin Wang, Zhiyuan Zhao, Man Jiang, Xiaomeng Zhao, et al. Omnidocbench: Benchmarking diverse pdf document parsing with comprehensive annotations. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2483824848, 2025. [17] Yian Zhao, Wenyu Lv, Shangliang Xu, Jinman Wei, Guanzhong Wang, Qingqing Dang, Yi Liu, and Jie Chen. Detrs beat yolos on real-time object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1696516974, 2024. [18] Xiuquan Hou, Meiqin Liu, Senlin Zhang, Ping Wei, Badong Chen, and Xuguang Lan. Relation detr: Exploring explicit position relation prior for object detection. In European Conference on Computer Vision, pages 89105. Springer, 2024. [19] Zilong Wang, Yiheng Xu, Lei Cui, Jingbo Shang, and Furu Wei. Layoutreader: Pre-training of text and layout for reading order detection. arXiv preprint arXiv:2108.11591, 2021. [20] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [21] Haoran Wei, Chenglong Liu, Jinyue Chen, Jia Wang, Lingyu Kong, Yanming Xu, Zheng Ge, Liang Zhao, Jianjian Sun, Yuang Peng, et al. General ocr theory: Towards ocr-2.0 via unified end-to-end model. arXiv preprint arXiv:2409.01704, 2024. [22] Kwai Keye Team, Biao Yang, Bin Wen, Changyi Liu, Chenglong Chu, Chengru Song, Chongling Rao, Chuan Yi, Da Li, Dunju Zang, et al. Kwai keye-vl technical report. arXiv preprint arXiv:2507.01949, 2025. [23] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. [24] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [25] Ting Sun, Cheng Cui, Yuning Du, and Yi Liu. Pp-doclayout: unified document layout detection model to accelerate large-scale data construction. arXiv preprint arXiv:2503.17213, 2025. [26] Zhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for training deep neural networks with noisy labels. Advances in neural information processing systems, 31, 2018. [27] PaddlePaddle Authors. Erniekit. https://github.com/PaddlePaddle/ERNIE, 2025. [28] Maksym Lysak, Ahmed Nassar, Nikolaos Livathinos, Christoph Auer, and Peter Staar. Optimized table tokenization for table structure recognition. In International Conference on Document Analysis and Recognition, pages 3750. Springer, 2023. 20 [29] Cheng-Lin Liu, Fei Yin, Da-Han Wang, and Qiu-Feng Wang. Casia online and offline chinese handwriting databases. In 2011 international conference on document analysis and recognition, pages 3741. IEEE, 2011. [30] Bin Wang, Zhuangcheng Gu, Guang Liang, Chao Xu, Bo Zhang, Botian Shi, and Conghui He. Unimernet: universal network for real-world mathematical expression recognition. arXiv preprint arXiv:2404.15254, 2024. [31] Philippe Gervais, Anastasiia Fadeeva, and Andrii Maksai. Mathwriting: dataset for handwritten mathematical expression recognition. In Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V. 2, pages 54595469, 2025. [32] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. [33] Nitesh Methani, Pritha Ganguly, Mitesh Khapra, and Pratyush Kumar. Plotqa: Reasoning over scientific plots. In Proceedings of the ieee/cvf winter conference on applications of computer vision, pages 15271536, 2020. [34] Shankar Kantharaj, Rixie Tiffany Ko Leong, Xiang Lin, Ahmed Masry, Megh Thakkar, Enamul Hoque, and Shafiq Joty. Chart-to-text: large-scale benchmark for chart summarization. arXiv preprint arXiv:2203.06486, 2022. [35] Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. Dvqa: Understanding data visualizations via question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 56485656, 2018. [36] Ahmed Masry, Parsa Kavehzadeh, Xuan Long Do, Enamul Hoque, and Shafiq Joty. Unichart: universal vision-language pretrained model for chart comprehension and reasoning. arXiv preprint arXiv:2305.14761, 2023. [37] Leilani Battle, Peitong Duan, Zachery Miranda, Dana Mukusheva, Remco Chang, and Michael Stonebraker. Beagle: Automated extraction and interpretation of visualizations from the web. In Proceedings of the 2018 CHI conference on human factors in computing systems, pages 18, 2018. [38] Kenny Davila, Bhargava Urala Kota, Srirangaraj Setlur, Venu Govindaraju, Christopher Tensmeyer, Sumit Shekhar, and Ritwick Chaudhry. Icdar 2019 competition on harvesting raw tables from infographics (chart-infographics). In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 15941599. IEEE, 2019. [39] Benny Tang, Angie Boggust, and Arvind Satyanarayan. Vistext: benchmark for semantically rich chart captioning. arXiv preprint arXiv:2307.05356, 2023. [40] Junyu Luo, Zekun Li, Jinpeng Wang, and Chin-Yew Lin. Chartocr: Data extraction from In Proceedings of the IEEE/CVF winter charts images via deep hybrid framework. conference on applications of computer vision, pages 19171925, 2021. [41] Xu Zhong, Elaheh ShafieiBavani, and Antonio Jimeno Yepes. Image-based table recognition: data, model, and evaluation, 2020. 21 [42] Fangyu Liu, Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Wenhu Chen, Nigel Collier, and Yasemin Altun. Deplot: Oneshot visual language reasoning by plot-to-table translation. arXiv preprint arXiv:2212.10505, 2022. [43] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311318, 2002. [44] VI Lcvenshtcin. Binary coors capable or correcting deletions, insertions, and reversals. In Soviet physics-doklady, volume 10, 1966. [45] Vik Paruchuri. Marker. https://github.com/datalab-to/marker, 2025. Accessed: 2025-09-25. [46] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. [47] Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. Internvl3. 5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025. [48] Google DeepMind. Gemini 2.5. https://blog.google/technology/google-deepm ind/gemini-model-thinking-updates-march-2025/, 2025. [49] chatdoc com. Ocrflux. https://github.com/chatdoccom/OCRFlux, 2025. Accessed:2025-09-25. [50] Mistral AI Team. Mistral-ocr. https://mistral.ai/news/mistral-ocr?utm_sourc e=ai-bot.cn, 2025. [51] Souvik Mandal, Ashish Talewar, Paras Ahuja, and Prathamesh Juvatkar. Nanonets-ocr-s: model for transforming documents into structured markdown with intelligent content recognition and semantic tagging, 2025. [52] rednote-hilab. dots.ocr: Multilingual document layout parsing in single vision-language model, 2025. [53] Filimoa. open-parse. https://github.com/Filimoa/open-parse, 2024. Accessed: 2025-06-23. [54] Unstructured-IO. unstructured. https://github.com/Unstructured-IO/unstruct ured, 2022. Accessed: 2025-06-23. [55] breezedeus. Pix2text. https://github.com/breezedeus/Pix2Text, 2022. Accessed: 2025-06-23. [56] Mathpix. Mathpix snip: Convert images and pdfs to latex, docx, and more. https: //mathpix.com/, 2025. 22 [57] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2418524198, 2024. [58] Lukas Blecher, Guillem Cucurull, Thomas Scialom, and Robert Stojnic. Nougat: Neural optical understanding for academic documents. arXiv preprint arXiv:2308.13418, 2023. [59] Alex WC Lee, Jonathan Chung, and Marco Lee. Gnhk: dataset for english handwriting in the wild. In International Conference on Document Analysis and Recognition, pages 399412. Springer, 2021. [60] Atsunobu Kotani, Stefanie Tellex, and James Tompkin. Generating handwriting via decoupled style descriptors. In European Conference on Computer Vision, pages 764780. Springer, 2020. [61] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. [62] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [63] Song Chen, Xinyu Guo, Yadong Li, Tao Zhang, Mingan Lin, Dongdong Kuang, Youwei Zhang, Lingfeng Ming, Fengyu Zhang, Yuran Wang, et al. Ocean-ocr: Towards general ocr application via vision-language model. arXiv preprint arXiv:2501.15558, 2025. [64] Bin Wang, Fan Wu, Linke Ouyang, Zhuangcheng Gu, Rui Zhang, Renqiu Xia, Botian Shi, Bo Zhang, and Conghui He. Image over text: Transforming formula recognition evaluation with character detection matching. In 2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1968119690, 2025. [65] Liang Zhang, Anwen Hu, Haiyang Xu, Ming Yan, Yichen Xu, Qin Jin, Ji Zhang, and Fei Huang. Tinychart: Efficient chart understanding with visual token merging and programof-thoughts learning. arXiv preprint arXiv:2404.16635, 2024. [66] Jinyue Chen, Lingyu Kong, Haoran Wei, Chenglong Liu, Zheng Ge, Liang Zhao, Jianjian Sun, Chunrui Han, and Xiangyu Zhang. Onechart: Purify the chart structural extraction via one auxiliary token. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 147155, 2024. [67] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th symposium on operating systems principles, pages 611626, 2023. [68] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Livia Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph Gonzalez, et al. Sglang: Efficient execution of structured language model programs. Advances in neural information processing systems, 37:6255762583, 2024. [69] PaddlePaddle Authors. Fastdeploy. https://github.com/PaddlePaddle/FastDepl oy, 2025. 23 [70] Qian Chen, Xianyin Zhang, Lifan Guo, Feng Chen, and Chi Zhang. Dianjin-ocr-r1: Enhancing ocr capabilities via reasoning-and-tool interleaved vision-language model. arXiv preprint arXiv:2508.13238, 2025. [71] Lukas Blecher. pix2tex - latex ocr. https://github.com/lukas-blecher/LaTeX-OCR, 2022. Accessed: 2025-06-23. [72] Harold Mouchere, Christian Viard-Gaudin, Richard Zanibbi, and Utpal Garain. Icfhr 2014 competition on recognition of on-line handwritten mathematical expressions (crohme 2014). In 2014 14th international conference on frontiers in handwriting recognition, pages 791796. IEEE, 2014. [73] Harold Mouchère, Christian Viard-Gaudin, Richard Zanibbi, and Utpal Garain. Icfhr2016 crohme: Competition on recognition of online handwritten mathematical expressions. In 2016 15th International Conference on Frontiers in Handwriting Recognition (ICFHR), pages 607612. IEEE, 2016. [74] Mahshad Mahdavi, Richard Zanibbi, Harold Mouchere, Christian Viard-Gaudin, and Utpal Garain. Icdar 2019 crohme+ tfd: Competition on recognition of handwritten mathematical expressions and typeset formula detection. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 15331538. IEEE, 2019. [75] Ye Yuan, Xiao Liu, Wondimu Dikubab, Hui Liu, Zhilong Ji, Zhongqin Wu, and Xiang In Bai. Syntax-aware network for handwritten mathematical expression recognition. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 45534562, 2022. [76] Tao Ge, Xin Chan, Xiaoyang Wang, Dian Yu, Haitao Mi, and Dong Yu. Scaling synthetic data creation with 1,000,000,000 personas. arXiv preprint arXiv:2406.20094, 2024."
        },
        {
            "title": "Appendix",
            "content": "A. Training Dataset Details This two-stage approach offers unique advantages in terms of data collection, as obtaining isolated element imagesalong with their annotations is more feasible than collecting complete document pages containing different elements. In the following sections, we will elaborate on the construction of multimodal model training data for text, tables, formulas, and charts. A.1. Text We have curated large-scale dataset comprising 20 Million High-Quality Image-Text Pairs. As shown in Figure A1, the dataset generation follows rigorous multi-stage pipeline which primarily involves: Figure A1 The construction method and characteristics of the text training data for PaddleOCR-VL-0.9B. 1. Automatic Data Annotation: We design an automatic annotation pipeline that integrates lightweight document-structure models with large multimodal language models. Specifically, PP-StructureV3 is employed as an expert model to perform layout analysis and text recognition, generating pseudo labels that are converted into prompts for multimodal models such as ERNIE-4.5-VL and Qwen2.5-VL to refine. Finally, the refined labels are aggregated and randomly merged at multiple granularities to produce 20 million high-quality imagetext training samples. 2. High-quality OCR Data Synthesis: During data distillation, low label quality in challenging scenarios like messy handwriting and dense blurry text was addressed by expanding the dataset through synthetic generation. Utilizing diverse CSS styles, over 200 fonts, and various corpora, we rendered large amount of images, thereby enhancing the models capabilities in these difficult scenarios. Ultimately, the data is meticulously annotated at three distinct hierarchical levels: text lines, text blocks, and text pages. With extensive language coverage of 109 languages, including major global ones like Chinese, English, French, and Hindi. It includes diverse scenes including Academic Papers, Newspapers, Handwritten texts, Ancient books, Id cards, tickets, seals, etc. Additionally, the dataset addresses compatibility with variety of writing systems and text styles, covering Printing, Handwriting, Scanned text, Artistic Fonts, etc. 25 A.2. Table As shown in Figure A2, we constructed large-scale dataset of over 5 million high-quality image-table pairs. Our dataset construction employs three key strategies: automatic data annotation, potential annotation mining, and high-quality data synthesis. For coding efficiency, we adopt OTSL [28] as the models target format instead of conventional HTML. The main dataset construction process is as follows: Figure A2 The construction method and characteristics of the table training data for PaddleOCR-VL-0.9B. 1. Automatic Data Annotation: To enhance the performance of PaddleOCR-VL in table recognition, we built large-scale, diverse dataset covering various languages, border styles, and table types. Tables are first located using PP-StructureV3 [10]. For unlabeled images, we employed multi-stage annotation pipeline: ERNIE-4.5-VL [5] first generates pseudo-labels, which are then validated by ERNIE-4.5-VL-28B-A3B [5] as discriminative model. Rejected annotations are refined using DianJin-OCR-R1 [70] (for tools, we use ERNIE-4.5-VL and PP-StructureV3 [10]). Finally, all annotations undergo rigorous rulebased verification, including n-gram analysis and HTML validation, to ensure only highquality samples are used for training. 2. Potential Annotation Mining: For public data with potential annotations (e.g., from arXiv), we extract tables and their corresponding official-supported HTML source code. We then employ mechanism combining regular expression matching with contextual and sequential alignment to construct accurate table-HTML pairs. The extracted HTML subsequently undergoes rule-based filtering, yielding high-quality data samples ready for model training. 3. High-quality Table Synthesis: To overcome data imbalance and high annotation costs, we introduce an innovative highquality table synthesis tool which constitutes the cornerstone of our table data collection pipeline. This tool enables both randomized synthesis for comprehensive data supplement and targeted synthesis to enhance recognition of specific table categories. Specifically, we first leverage LLMs to gather diverse and extensive corpus.Then, our tool generates table training pairs through randomized configurations of structures, fonts, CSS styles, and textual content, while also supporting customized synthesis by specifying particular parameters to accurately simulate specialized table types. With synthesis speed of 10, 000 samples per hour, our tool has produced over 5, 500, 000 training instances, substantially enhancing our models generalization capability and comprehensive performance in table recognition. Through the aforementioned data construction strategies, we build comprehensive table dataset encompassing diverse table categories and recognition scenarios, thereby providing robust support for training our model in the table recognition task. A.3. Formula As shown in Figure A3, this dataset was developed using range of strategies, including source code rendering, automatic data annotation, targeted synthesis of long-tail data, and public data collection. It encompasses variety of formula scenarios, such as educational supplementary materials, test papers for primary and secondary schools, mathematical papers, PowerPoint courseware, university theses, financial research reports, and handwritten mathematical notes. The dataset features four types of formulas: Simple Printed Expressions, Complex Printed Expressions, Screen-Captured Expressions, and Handwritten Expressions, available in both Chinese and English. The main process for constructing the dataset is as follows: Figure A3 The construction method and characteristics of the formula training data for PaddleOCR-VL-0.9B. 1. Source Code Rendering: To enhance the models adaptability to wide variety of unusual formula structures, large amount of paper source code was scraped from arXiv, and LaTeX code for the formulas was extracted using regular expressions. Then, MinHash was used to remove duplicate and highly similar formula source codes, and KaTeX was employed to normalize the formula source codes, thereby reducing their ambiguity. Finally, the formulas were re-rendered into images using formula rendering engine. 2. Automatic Data Annotation: For real-world formula data from exam papers, educational materials, and handwritten notes, the process begins with the use of the layout analysis method PP-StructureV3 [10] to identify the bounding boxes for formulas. Based on these bounding boxes, formula regions are cropped from the images. Subsequently, large multimodal language models, such as ERNIE-4.5-VL-28B-A3B [5], are employed to 27 generate the LaTeX source code for these formulas. Given the rarity of Chinese formulas in real-world scenarioswhere approximately 1 out of 100 formulas contains Chinese charactersPP-OCRv5 [10] is utilized to recognize characters within the cropped regions, enabling targeted optimization when Chinese characters are detected. Due to the complex and diverse nature of real-world formulas, recognition errors may occur with existing large models. To address this, LaTeX rendering engine is used to filter the formulas generated by these models. Specifically, image-formula pairs that cannot be successfully rendered by xelatex are discarded. For those that render successfully, more in-depth screening is conducted by comparing metrics such as the aspect ratio between the recognized image and the rendered image. 3. Targeted Synthesis of Long-tail Data: For certain long-tail formula structures, such as elementary school vertical calculations, formulas with strikethroughs, and handwritten formulas with explanatory arrows, existing multimodal large models struggle to accurately recognize them due to data distribution issues. To address this, LaTeX code is synthetically generated based on rules and inverse rendering is performed using LaTeX rendering engine, thereby constructing image-formula matching pairs for these long-tail scenarios. 4. Public Data Collection: In order to enable the model to learn high-quality formula representations, substantial amount of data has been collected from existing public datasets, including UniMER-1M [30] and MathWriting [31]. Specifically, UniMER-1M is oriented towards real document scenarios and has gathered 1 million formula data from arXiv, Pix2tex [71], CROHME [72, 73, 74], and HME100K [75]. On the other hand, MathWriting is currently the largest handwritten mathematical formula dataset, comprising 230,000 real handwritten formula samples and 400,000 synthetic handwritten formula samples. A.4. Chart We constructed large-scale, bilingual (Chinese and English) dataset of over 0.8 million highquality image-chart pairs. Our dataset construction employs four key strategies: public data collection and cleaning, automatic data annotation, data synthesis, and targeted long-tail data augmentation. The dataset covers wide array of chart types from diverse sources, including academic papers, financial reports, and web pages. The main dataset construction process is as follows: Figure A4 The construction method and characteristics of the chart training data for PaddleOCR-VL-0.9B. 28 1. Public Data Collection and Cleaning: We collected large number of samples from public datasets, including ChartQA [32], PlotQA [33], Chart2Text [34], DVQA [35], Unichart [36], Beagle [37], ChartINFO [38], visText [39], and ExcelChart [40]. However, the raw datasets suffered from poor annotation quality and extremely imbalanced data distributions. Thus, meticulous data cleaning and filtering pipeline was implemented to remove noisy samples and ensure balanced clustering, resulting in high-quality dataset of 220k samples. 2. Automatic Data Annotation: To annotate our large collection of unlabeled public and in-house data, we developed two-stage annotation pipeline based on the Vision Large Language Model ERNIE-4.5-VL [5]. In the first stage, the model extracts tick labels from the xand y-axes; in the second, random permutations of these labels are used to query corresponding data points, framing annotation as data retrieval task. final consistency check ensures that only verified annotations are included in the training set, guaranteeing high reliability. 3. Data Synthesis: To capture diverse visual styles and enhance model generalization, we designed three-stage data synthesis pipeline. It begins with large collection of base data tables, followed by an LLM Persona [76] strategy using ERNIE-X1 [5], which diversifies table content and generates persona-specific rendering code. This enables control over chart aesthetics such as color, font, and layout. Leveraging billion distinct personas, the pipeline produces highly varied data structures and visual styles, substantially improving PaddleOCR-VLs generalization across real-world charts. For rendering, we employ matplotlib and seaborn. 4. Targeted Long-tail Data Augmentation: To improve generalization on real-world longtail samples, we designed data augmentation pipeline based on seed charts. It first selects long-tail samples by their distinctive visual features, then uses ERNIE-4.5-VL [5] to replicate their rendering code. ERNIE-X1 [5], guided by specific persona [76], further diversifies the code by altering data tables and visual styles. Executing the modified code produces new augmented charts with corresponding data tables. Through the four data construction strategies mentioned above, the final chart dataset covers wide range of application scenarios and rich variety of chart styles, providing strong support for the training of chart models. 29 B. Supported Languages PaddleOCR-VL supports total of 109 languages. Table 6 in the main text shows the text line recognition accuracy for different languages. Table A1 lists the correspondence between each language category and the specific supported languages. Language Category Specific Languages Chinese English Korean Japanese Thai Greek Tamil Telugu Arabic Latin Cyrillic Chinese English Korean Japanese Thai Greek Tamil Telugu Arabic, Persian, Uyghur, Urdu, Pashto, Kurdish, Sindhi, Balochi French, German, Afrikaans, Italian, Spanish, Bosnian, Portuguese, Czech, Welsh, Danish, Estonian, Irish, Croatian, Uzbek, Hungarian, Serbian (Latin), Indonesian, Occitan, Icelandic, Lithuanian, Maori, Malay, Dutch, Norwegian, Polish, Slovak, Slovenian, Albanian, Swedish, Swahili, Tagalog, Turkish, Latin, Azerbaijani, Kurdish, Latvian, Maltese, Pali, Romanian, Vietnamese, Finnish, Basque, Galician, Luxembourgish, Romansh, Catalan, Quechua Russian, Belarusian, Ukrainian, Serbian (Cyrillic), Bulgarian, Mongolian, Abkhazian, Adyghe, Kabardian, Avar, Dargin, Ingush, Chechen, Lak, Lezgin, Tabasaran, Kazakh, Kyrgyz, Tajik, Macedonian, Tatar, Chuvash, Bashkir, Malian, Moldovan, Udmurt, Komi, Ossetian, Buryat, Kalmyk, Tuvan, Sakha, Karakalpak Devanagari Hindi, Marathi, Nepali, Bihari, Maithili, Angika, Bhojpuri, Magahi, Santali, Newari, Konkani, Sanskrit, Haryanvi Table A1 Supported Languages 30 C. Inference Performance on Different Hardware Configurations We measured the inference performance of PaddleOCR-VL on different hardware configurations, as summarized in Table A2. As observed, PaddleOCR-VL demonstrates stable and efficient inference performance across wide range of hardware and backend configurations, showing that the system can flexibly adapt to diverse computing environments. Moreover, we are currently integrating the FastDeploy backend, which is expected to further enhance inference efficiency in future releases. Hardware Backend Total Time (s) Pages/s Tokens/s Avg. VRAM Usage (GB) A10 RTX 3060 vLLM SGLang vLLM SGLang vLLM SGLang RTX vLLM RTX 4090D vLLM SGLang 800.9 917.6 1238.0 1429.9 2749.1 2792. 1292.9 845.3 951.8 1.2241 1.0684 0.7921 0.6858 0.3568 0.3513 1881.2 1641. 1217.2 1055.8 548.2 540.8 0.7584 1165.5 1.1597 1.0303 1781.8 1586. Table A2 End-to-End Inference Performance 43.7 49.8 14.1 20.0 11.9 11.8 8.9 16.7 21. 31 D. Real-world Samples This appendix showcases the parsing and recognition capabilities of our proposed algorithm across variety of challenging scenarios. Section D.1 demonstrates the overall document parsing capability of PaddleOCR-VL. Figures A5-A8 are examples of parsing different types of documents in Markdown format. Figures A9-A11 in section D.2 illustrate the superior ability of PaddleOCR-VL to process pages featuring intricate or challenging layouts. Figures A12 and A13 in section D.3 demonstrate that PaddleOCR-VL maintains excellent reading order when faced with complex layouts, such as those found in various reports, textbooks, newspapers, magazines, and even vertical documents. Section D.4 highlights the robust text recognition performance of PaddleOCR-VL in challenging cases, including multilingual text, handwriting text, and vertical text, which are presented in Figures A14-A22. The models table recognition abilities are demonstrated in section D.5. Figures A23 and A24 showcase its robust handling of wide array of table formats, including tables from academic papers, tables from financial reports, tables with watermark, tables with image, tables with formulas and photograph of tables. Figures in section D.6 detail the formula recognition performance. Figure A25 demonstrates the ability to handle various types of english formulas including complex printed expressions, handwritten expressions screen-captured expressions and vertical formula, while Figure A26 focuses on the ability to handle formulas that contain Chinese characters. In section D.7, PaddleOCR-VL demonstrates impressive chart recognition capabilities, feature currently lacking in many expert OCR VLMs like MinerU2.5 [14], dots.ocr [52] or MonkeyOCR [1]. Figures A27-A29 showcase our ability to parse various chart types, including pie charts, bar charts, line charts, bar-line hybrid charts and heatmap. 32 D.1. Comprehensive Document Parsing Figure A5 The Layout and Markdown Output for Book, Textbook and Academic Paper. 33 Figure A6 The Layout and Markdown Output for Research Report(with chart recognition enabled), Financial Report, Slides and Exam Paper. Figure A7 The Layout and Markdown Output for Notes, Vertical Book and Ancient Book. 35 Figure A8 The Layout and Markdown Output for Certificate, Newspaper and Magazine. 36 D.2. Layout Detection Figure A9 The Layout Detection results for various types of documents. 37 Figure A10 The Layout Detection results for various types of documents. 38 Figure A11 The Layout Detection results for various types of documents. 39 D.3. Reading Order Figure A12 The Reading Order results for various types of documents. 40 Figure A13 The Reading Order results for various types of documents. 41 D.4. Text Recognition D.4.1. Multilingual Text Recognition Figure A14 The markdown output for French and Hindi documents. 42 Figure A15 The markdown output for Croatian and Spanish documents. 43 Figure A16 The markdown output for English and Arabic documents. Figure A17 The markdown output for German and Chinese documents. 45 Figure A18 The markdown output for Russian and Japanese documents. 46 Figure A19 The markdown output for Thai and Korean documents. D.4.2. Handwriting Text Recognition Figure A20 The markdown output for Mixed Printed Handwritten Text and Handwritten Formula documents. 48 Figure A21 The markdown output for Handwriting Chinese and Handwriting English documents. 49 D.4.3. Vertical Text Recognition Figure A22 The markdown output for various types of vertical documents. 50 D.5. Table Recognition Figure A23 The markdown output for various types of Tables. 51 Figure A24 The markdown output for various types of Tables. 52 D.6. Formula Recognition Figure A25 The markdown output for various types of Formulas. 53 Figure A26 The markdown output for various types of Formulas. D.7. Chart Recognition Figure A27 The markdown output for various types of Charts. 55 Figure A28 The markdown output for various types of Charts. 56 Figure A29 The markdown output for various types of Charts. 57 E. Compare with Others PaddleOCR-VL showcases superior performance in scenarios involving PDF pages with complex layout, consistently outperforming existing state-of-the-art (SOTA) models. This is evident from Figures A30 and A31, which highlight its exceptional capability in handling pages with intricate layouts and unique elements, surpassing other solutions. Moreover, the model demonstrates exceptionally high recognition accuracy in several domains, including Multilingual Text Recognition, Handwriting Text Recognition, and Vertical Text Recognition. Figures A32A37 illustrate how PaddleOCR-VL outperforms competitors such as MinerU2.5 [2] and MonkeyOCR [1], which tend to misidentify languages like Russian and Hindi as English, overlook some handwritten characters, and struggle with vertical text recognition. In dealing with complex tables, PaddleOCR-VLs parsing accuracy stands out, as evidenced by Figures A38 and A39. This is domain where other models frequently encounter difficulties. Additionally, Figure A40 demonstrates PaddleOCR-VLs proficiency in accurately parsing complex formulas. In contrast, other SOTA models often produce incorrect or flawed outputs when faced with challenging mathematical notations. Finally, as depicted in Figures A41 and A42, PaddleOCR-VL also excels in Chart Recognition. It outperforms multi-modal large language models like Qwen2.5VL-72B [24] and GPT-4o by accurately reconstructing the structure and content of charts. 58 E.1. Layout Detection Figure A30 Compare with others in Layout Detection. 59 Figure A31 Compare with others in Layout Detection. 60 E.2. Text Recognition E.2.1. Multilingual Text Recognition Figure A32 Compare with others in Multilingual Text Recognition. 61 Figure A33 Compare with others in Multilingual Text Recognition. 62 Figure A34 Compare with others in Multilingual Text Recognition. 63 E.2.2. Handwriting Text Recognition Figure A35 Compare with others in Handwriting Text Recognition. Figure A36 Compare with others in Handwriting Text Recognition. 65 E.2.3. Vertical Text Recognition Figure A37 Compare with others in Vertical Text Recognition. 66 E.3. Table Recognition Figure A38 Compare with others in Table Recognition. 67 Figure A39 Compare with others in Table Recognition. 68 E.4. Formula Recognition Figure A40 Compare with others in Formula Recognition. 69 E.5. Chart Recognition Figure A41 Compare with others in Chart Recognition. 70 Figure A42 Compare with others in Chart Recognition."
        }
    ],
    "affiliations": [
        "PaddlePaddle Team, Baidu Inc."
    ]
}