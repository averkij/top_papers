{
    "paper_title": "Parallel-R1: Towards Parallel Thinking via Reinforcement Learning",
    "authors": [
        "Tong Zheng",
        "Hongming Zhang",
        "Wenhao Yu",
        "Xiaoyang Wang",
        "Xinyu Yang",
        "Runpeng Dai",
        "Rui Liu",
        "Huiwen Bao",
        "Chengsong Huang",
        "Heng Huang",
        "Dong Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Parallel thinking has emerged as a novel approach for enhancing the reasoning capabilities of large language models (LLMs) by exploring multiple reasoning paths concurrently. However, activating such capabilities through training remains challenging, as existing methods predominantly rely on supervised fine-tuning (SFT) over synthetic data, which encourages teacher-forced imitation rather than exploration and generalization. Different from them, we propose \\textbf{Parallel-R1}, the first reinforcement learning (RL) framework that enables parallel thinking behaviors for complex real-world reasoning tasks. Our framework employs a progressive curriculum that explicitly addresses the cold-start problem in training parallel thinking with RL. We first use SFT on prompt-generated trajectories from easier tasks to instill the parallel thinking ability, then transition to RL to explore and generalize this skill on harder problems. Experiments on various math benchmarks, including MATH, AMC23, and AIME, show that Parallel-R1 successfully instills parallel thinking, leading to 8.4% accuracy improvements over the sequential thinking model trained directly on challenging tasks with RL. Further analysis reveals a clear shift in the model's thinking behavior: at an early stage, it uses parallel thinking as an exploration strategy, while in a later stage, it uses the same capability for multi-perspective verification. Most significantly, we validate parallel thinking as a \\textbf{mid-training exploration scaffold}, where this temporary exploratory phase unlocks a higher performance ceiling after RL, yielding a 42.9% improvement over the baseline on AIME25. Our model, data, and code will be open-source at https://github.com/zhengkid/Parallel-R1."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 0 8 9 7 0 . 9 0 5 2 : r Technical Report Parallel-R1: Towards Parallel Thinking via Reinforcement Learning Tong Zheng1,2, Hongming Zhang1, Wenhao Yu1, Xiaoyang Wang1, Xinyu Yang3, Runpeng Dai1,4, Rui Liu2, Huiwen Bao5, Chengsong Huang6, Heng Huang2, Dong Yu1 1Tencent AI Lab Seattle, 2University of Maryland, College Park, 3Carnegie Mellon University, 4University of North Carolina at Chapel Hill, 5City University of Hong Kong, 6Washington University in St. Louis Abstract Parallel thinking has emerged as novel approach for enhancing the reasoning capabilities of large language models (LLMs) by exploring multiple reasoning paths concurrently. However, activating such capabilities through training remains challenging, as existing methods predominantly rely on supervised fine-tuning (SFT) over synthetic data, which encourages teacher-forced imitation rather than exploration and generalization. Different from them, we propose Parallel-R1, the first reinforcement learning (RL) framework that enables parallel thinking behaviors for complex real-world reasoning tasks. Our framework employs progressive curriculum that explicitly addresses the cold-start problem in training parallel thinking with RL. We first use SFT on prompt-generated trajectories from easier tasks to instill the parallel thinking ability, then transition to RL to explore and generalize this skill on harder problems. Experiments on various math benchmarks, including MATH, AMC23, and AIME, show that Parallel-R1 successfully instills parallel thinking, leading to 8.4% accuracy improvements over the sequential thinking model trained directly on challenging tasks with RL. Further analysis reveals clear shift in the models thinking behavior: at an early stage, it uses parallel thinking as an exploration strategy, while in later stage, it uses the same capability for multi-perspective verification. Most significantly, we validate parallel thinking as mid-training exploration scaffold, where this temporary exploratory phase unlocks higher performance ceiling after RL, yielding 42.9% improvement over the baseline on AIME25. Our model, data, and code will be open-source at https://github.com/zhengkid/Parallel-R1."
        },
        {
            "title": "Introduction",
            "content": "Googles Gemini recently credited its success at the International Mathematical Olympiad in part to new capability: parallel thinking (Luong & Lockhart, 2025). This approach, as exemplified by Figure 1 (top), involves jointly conducting both parallel and sequential thinking. This success highlights the value of parallel thinking as more than technical trick. Indeed, cognitive science suggests that humans often engage in such thinking, considering multiple possibilities simultaneously before synthesizing them into coherent conclusions. This process encourages divergent thought, prevents premature lock-in to single, potentially suboptimal solution, and facilitates structured, deliberate reasoning (Clark, 1989; Jackendoff, 2011). Inspired by these, we investigate how to effectively instill parallel thinking in large language models (LLMs). Despite its potential, the question of how to activate parallel thinking remains open. While test-time strategies (Yao et al., 2023; Wang et al., 2022; Brown et al., 2024; Zhang et al., 2024; Hsu et al., 2025; Rodionov et al., 2025; Fu et al., 2025) can elicit such behavior at the cost of high inference overhead, there is growing interest in permanently instilling this capability through training. However, current training-based approaches fall short of this goal. Methods based on supervised fine-tuning (SFT) (Yang et al., 2025b; Macfarlane et al., 2025; Chen et al., 2025a), for instance, essentially perform behavioral cloning on pre-generated reasoning trajectories. This approach often relies on complex This work was done during Tong Zhengs internship at Tencent AI Lab Seattle. 1 Technical Report Figure 1: An overview of the proposed framework. (Top) During inference, the model generates in standard auto-regressive fashion until it emits special <Parallel> tag. At that point, it spawns multiple threads to explore different solution paths or perspectives, then summarizes their outputs. These contents are merged back into the main context, and generation continues. This cycle may repeat several times before the model arrives at the final answer. (Bottom) Parallel thinking ability is obtained by progressive multi-stage training approach. Intuitively, the approach first equips the model with parallel thinking ability on easy math problems and then progressively extends it to more general and difficult problems through reinforcement learning. and costly data pipelines to synthesize high-quality parallel thinking data, leading to superficial pattern matching rather than the acquisition of deep, intrinsic reasoning skill. Consequently, while models can replicate known patterns, their ability to generalize the underlying parallel thinking strategy is severely limited. In contrast, reinforcement learning (RL) offers more scalable approach to activating the parallel thinking ability of LLMs since we could let the model explore and learn such behaviors in the wild. However, applying RL to teach models to conduct parallel thinking is not trivial. Since the current LLMs have not seen parallel thinking behavior during the pre-training and sft, they cannot generate such trajectories during explorations for the model to learn from. Thus, the cold-start training becomes crucial. The goal of this stage is to teach the model basic formats without harming it too much, which requires small-scale, high-quality dataset. However, the fact is that high-quality parallel thinking data for complex, real-world problems is extremely rare in natural text and difficult to synthesize. This explains why successful applications of RL for parallel thinking have been confined to narrow, synthetic domains, such as the CountDown task (Pan et al., 2025). Additionally, the best reward function for RL remains unclear. If we only use the final correctness as the reward, the model might take shortcuts to forget the complex but better parallel thinking strategy. On the other hand, if we force the model to use thinking strategy, the model might learn to use parallel thinking in unnecessary scenarios. Lastly, the strategic role and underlying mechanisms of parallel thinking in LLMs are largely black box. Even if model acquires this ability, critical questions remain unanswered. For instance, how does the models strategy evolve throughout training? Without understanding this dynamic, its impossible to fully unlock the potential of parallel thinking technology. 2 Technical Report To address these challenges, we present the first reinforcement learning framework designed to help models to learn parallel thinking behavior via exploration on general mathematical reasoning tasks. First, to resolve the critical cold-start problem, we propose progressive curriculum. As shown in Figure 1, it begins with supervised fine-tuning on simpler problems, for which we find high-quality parallel thinking data can be generated easily via simple prompting (see Table 1). This initial stage, using our created Parallel-GSM8K dataset, effectively teaches the model the basic format of parallel thinking before it transitions to reinforcement learning on more difficult tasks to explore and generalize this new ability. Second, we tackle the critical challenge of reward design by exploring how to balance final accuracy with the desired parallel thinking structure. We propose and investigate multiple reward schemes. Our key finding is an effective alternating reward strategy, which switches between an outcome-based (accuracy) reward and reward that encourages parallel thinking behaviors within fixed windows, e.g., every 10 steps. We show this approach achieves superior balance between high performance and consistent utilization of parallel thinking compared to using single reward type alone. Lastly, to open the black box of its strategic role, we conduct detailed analysis of the models learned behavior throughout the training process. Our analysis reveals clear strategic evolution: the model initially leverages parallel paths for computational exploration to discover potential solutions, but as it gains proficiency, its strategy shifts towards using them for multi-perspective verification to confirm the final answer. This finding provides the first empirical evidence of how an LLMs reasoning strategy with parallel thinking evolves, offering crucial insights into the underlying mechanisms that drive its effectiveness. Based on this, we further conceptualize and empirically validate the idea of using parallel thinking as mid-training exploration scaffolda temporary exploratory phase that unlocks higher performance ceiling, notably achieving peak accuracy of 25.6% on the challenging AIME25 benchmark. We investigate these contributions across both causal and structured model variants to provide robust insights into architectural design. In all, our core contributions can be summarized as follows: We present the first RL framework to learn parallel thinking from scratch on general mathematical reasoning tasks, enabled by our progressive curriculum and dedicated Reward Design. We provide deep analysis of the learning dynamics, revealing that the models strategy evolves from exploration to verification. We further identify and empirically validate the concept of parallel thinking as mid-training exploration scaffold. We provide comprehensive empirical validation, including comparison of causal and structured model variants. Our approach yields consistent gains across multiple benchmarks, and ablations offer practical insights into reward and architectural design."
        },
        {
            "title": "2.1 Parallel Thinking",
            "content": "Parallel thinking has emerged as an active area of research recently (Yao et al., 2023; Wang et al., 2022; Brown et al., 2024; Zhang et al., 2024; Huang et al., 2025a; Pan et al., 2025; Huang et al., 2024; Hsu et al., 2025; Rodionov et al., 2025; Yang et al., 2025b; Jin et al., 2025). Among them, common brute-force strategy is to spawn multiple independent trajectories at the very beginning and join their outcomes only at the end (Brown et al., 2024; Wang et al., 2022), or to exchange thoughts at fixed intervals (Rodionov et al., 2025; Hsu et al., 2025). Obviously, such schemes lack adaptivity as the points of branching and aggregating are dictated by pre-defined schedule, not conditioned on the intermediate progress of the thinking process itself. To achieve finer-grained control, methods such as Monte Carlo Tree Search (Zhang et al., 2024) and Tree of Thoughts (Yao et al., 2023) offer more nuanced parallelism; however, they are still guided by hand-crafted heuristics based on external verifiers. More recent work (Pan et al., 2025; Yang et al., 2025b) strives for adaptivity through RL or SFT. However, these studies either (i) focus mainly on efficiencylosslessly converting single long chain-of-thought into an adaptive parallel form via SFT, which limits the discovery of new reasoning patterns, or (ii) demonstrate RL only on toy tasks such as Countdown. In this work, we 3 Technical Report argue that learning parallel thinking via RL is more generic and promising direction: it not only retains efficiency but also uncovers novel, highly adaptive reasoning behaviors, leading to improved performance beyond the lossless transformation paradigm of Yang et al. (2025b). To this end, we proposed the first RL framework to stimulate adaptive parallel thinking for general mathematical tasks. 2.2 Improving Reasoning via RLVR Reinforcement Learning with Verifiable Rewards (RLVR) optimizes language models via reinforcement learning using outcome-based, automatically checkable rewards, eliminating the need for trained reward models or step-level human annotations. Recent advances have demonstrated RLVRs effectiveness across diverse domainsincluding mathematical problem solving (Guo et al., 2025), coding (Wang et al., 2025a), multi-modal reasoning (Huang et al., 2025c; Wang et al., 2025b; Zheng et al., 2025; Li et al., 2025), relation extraction (Dai et al., 2025b), and interactive GUI navigation (Shi et al., 2025). In parallel, growing body of work aims to make RLVR more efficient and stable, proposing new training paradigms such as self-play (Liu et al., 2025; Huang et al., 2025b) and test-time RL (Zuo et al., 2025), as well as more robust RL algorithms including DAPO (Yu et al., 2025), VAPO (Yue et al., 2025), and high entropy guided optimization (Wang et al., 2025c). However, important challenges remain. Existing methods often leave unresolved issues of faithfulness (Tanneru et al., 2024; Chen et al., 2025b; Zhou et al., 2025) and robustness (Sabbaghi et al., 2025; Dai et al., 2025a). Moreover, most approaches adopt strictly sequential reasoning paradigm. This limitation is fundamental, as LLMs do not inherently possess parallel thinking capabilities, making it significant and unresolved challenge to instill this skill using standard RLVR methods. To address this specific challenge, we introduces the first reinforcement learning framework that leverages progressive curriculum to effectively instill the parallel thinking ability in LLMs, fundamentally enhancing their intrinsic reasoning capabilities."
        },
        {
            "title": "3 Learning Parallel Thinking via Reinforcement Learning",
            "content": "3.1 Overview Previous methods for training parallel thinking, such as those in (Yang et al., 2025b; Macfarlane et al., 2025; Chen et al., 2025a), primarily rely on SFT, paradigm that suffers from several key limitations. By its nature, SFTs success is entirely dependent on the quality of pre-generated training data. This creates critical dependency on complex and costly data pipelines, especially when generating data for final, challenging problems. Furthermore, this approach constrains the model to merely mimicking known patterns, which hinders the acquisition of deep, generalizable reasoning skill. To overcome these limitations, we introduce reinforcement learning (RL) framework. The key insight of our approach is to bypass the need for the complex data pipelines often considered essential for generating training data on final challenging problems (Yang et al., 2025b; Macfarlane et al., 2025). Instead, we generate high-quality cold-start data by using simple tasks, and then leverage this data to enable the model to learn parallel thinking on much harder problems via reinforcement learning. We then explore two distinct settings for learning parallel thinking via RL: without architectural modifications and with architectural modifications. Specifically, the latter involves modifying the models self-attention mask and position ids to prevent cross-attention between parallel reasoning paths, thereby enforcing their structural independence. In the subsequent sections, we first define our parallel thinking behaviors and their inference workflow. We then describe our data pipeline for generating high-quality training data. Finally, we present our RL training recipes for both settings. 4 Technical Report 3.2 Formulation of Parallel Thinking Behaviors Intuitively, in human problem-solving, we often encounter moments of confusion or uncertainty, which are referred to as critical steps within reasoning chain. At these points, engaging in parallel thinking allows us to explore multiple solution paths simultaneously and converge on higher-quality conclusion. Inspired by human problem-solving patterns, we formalize LLMs parallel thinking in two stages: 1. Exploration: When the model detects critical step, it temporarily suspends the main chain and launches multi-thread search, generating independent trajectories simultaneously. 2. Summary: After exploration, the model aggregates the outcomes, distills key insights, and resolves conflicts to arrive at the most promising conclusion. It then automatically resumes the main reasoning chain with the summarized conclusion. We allow the model to repeat these two phases whenever needed during the reasoning process. We illustrate this process in Figure 1 (Top). To implement this behavior, we introduce three control tags, <Parallel>...</Parallel>, <Path>...</Path>, and <Summary>...</Summary>, which correspond to the exploration phase, the isolation of reasoning threads, and the summary of the parallel thinking, respectively. With these tags, we can define the workflow at the inference phase as follows: Workflow at Inference Phase At inference time, our model dynamically executes the parallel thinking behaviors as follows: It first conducts auto-regressive generation in the main reasoning process. Whenever it predicts <Parallel> token, it pauses the main reasoning chain and concurrently expands multiple reasoning threads within separate <Path>...</Path> blocks. After generating all parallel threads, the model automatically aggregates their outputs into concise <Summary>...</Summary> block, integrating insights from diverse perspectives. Finally, all contexts of parallel thinking are used to resume and complete the main reasoning path. Such adaptive and dynamic parallel inference effectively leverages parallelism. 3.3 The Simple and Scalable Data Pipeline for Parallel Thinking Key Finding 1 powerful model can produce valid parallel-thinking traces for 83.6% of simple GSM8K problems, but fails to generate single valid trace for the more challenging DAPO problems (0.0% success rate). Collecting high-quality parallel thinking data is significant challenge. Even though humans think in the parallel fashion, they will summarize and only say/write the summarization. Thus, such data is extremely rare in the natural distribution. Existing approaches, such as the one described in (Yang et al., 2025b), try to solve this by leveraging the inherent parallelism of long CoT reasoning chains. However, these methods rely on complex, multi-stage data pipelines that, while avoiding costly human annotations, are computationally intensive and fundamentally limited in their scalability. Our approach is based on key finding from our preliminary experiments. We found that while simple prompting approach struggles to generate high-quality parallel-thinking data for complex problems like DAPO, it proves highly effective for simpler tasks like GSM8K. The data in Table 1 supports this finding. Based on this discovery, we propose simple and scalable data pipeline that uses detailed zero-shot prompts to construct large, high-quality corpus for these easier problems. As the structured model variant (described in Section 3.5) utilizes architectural modifications like path-window attention masks, it requires strict format adherence for successful training. Therefore, to ensure the quality and alignment of this corpus, we perform an additional filtering step, Parallel Thinking Format Check, which is implemented by Algorithm 1. Crucially, we make the strategic choice to use this cold-start data not to teach the model how to solve the final target tasks, but 5 Technical Report Table 1: Comparison of Parallel-Thinking Data Quality Generated by DeepSeek-R1-0528-Qwen-3-8B on DAPO and GSM8K under identical prompts and sampling settings. The results show that, with simple prompting, state-of-the-art models still struggle to produce concise, high-quality parallel reasoning traces for challenging mathematics problems. Data # Samples Parallel Thinking Format (%) GSM8K DAPO 7472 17916 83.7 0.0 specifically to teach it the format of parallel thinking. This initial stage allows us to transition from data-intensive approach to more efficient reinforcement learning framework that can learn to elicit and strengthen parallel thinking behaviors from the ground up. 3.4 Eliciting Parallel Thinking via Reinforcement Learning in Causal Models Unlike prior approaches that use complex and costly data pipelines, we design simple and scalable data pipeline to efficiently generate large, high-quality parallel thinking dataset on easy math problems. This dataset serves as crucial cold start to teach the model the correct format for parallel thinking. Our key idea is to use an RL framework to generalize this format and ability from simple problems to more difficult mathematical tasks. In this section, we explore strategies to elicit this parallel thinking behavior without modifying the models architecture. 3.4.1 Reinforcement Learning Algorithms We use Group Relative Policy Optimization (GRPO) (Shao et al., 2024) as our reinforcement learning algorithm. Let be question, and let {oi}G i=1 be candidate responses sampled from the old policy πθold ( q). We denote ri as the reward for oi. We define: ρi = πθ(oi q) (oi q) πθold , = 1 j=1 rj, Ai = (cid:113) 1 ri G j=1(rj r)2 + εstab , where εstab is constant for numerical stability and Ai is the advantage. The GRPO loss is then: LGRPO(θ) = (cid:34) 1 i=1 qD {oi}πθold min(ρi Ai, clip(ρi, 1 α, 1 + α) Ai) β DKL(πθ πref) . (cid:35) Our models rollout process follows multi-turn interactive framework where the LLM alternates between autoregressive generation, parallel exploration, and summarization. The model generates reasoning prefix until it predicts <Parallel> block, within which it spawns several independent Path segments. Once all paths are completed, it produces <Summary> segment that integrates insights from the parallel paths into coherent continuation. This process repeats as needed. 3.4.2 The Training Recipe and Reward Modeling The overall training recipe consists of three stages: 1) Cold-Start Stage; 2) RL on Easy Math, and 3) RL on General Math. Cold-Start Stage We construct and collect small set of parallel-thinking format examples to fine-tune the initial RL actor using the approach in Section 3.3. Specifically, we use distilled Qwen3-8B model (i.e., DeepSeek-R1-0528-Qwen-3-8B) to produce high-quality parallel-thinking outputs, extracting only non-thinking parts (final short CoT) as gold annotations. We select the GSM8K training set, which consists of approximately 7k samples, as the seed dataset. We call the resulting cold-start dataset Parallel-GSM8K. This cold-start training is used to teach model the basic format of parallel thinking. Technical Report RL on Easy Math After the cold start with SFT, the model already possesses the basic ability to generate the tags for parallel thinking, but the behavior is not stable since this special token has never appeared in the pre-training. To address this issue, we further perform small-scale reinforcement learning to enhance the format learning. In this stage, we use the same question set as the cold-start data and use Group Relative Policy Optimization (GRPO) (Shao et al., 2024) for our RL training. To ensure parallel ratio and accuracy, the final reward format in this stage is: inal = RParallel Racc. Here, the Accuracy Reward (Racc) evaluates the correctness of the final response, while the Parallel Reward (RParallel) incentivizes the model to use parallel reasoning paths. This reward structure is designed to be binary and strict: positive reward of +1 is given only if the generated output contains at least one parallel thinking unit AND the final answer is correct. Otherwise, the model receives penalty of -1. RL on General Math After the initial training, the model can stably generate control tags and produce outputs in the correct parallel thinking format if needed, but it still struggles with more challenging mathematical tasks. To address this, we apply reinforcement learning to general math datasets, thereby generalizing the models parallel thinking ability beyond simple cases. Specifically, we use the same GRPO algorithm introduced in Section 3.4.1 with accuracy reward (Racc) as our sole reward. This is because the primary goal of this stage is to improve task performance. For the seed problems, we choose the widely used DAPO dataset (Yu et al., 2025). Finally, the models produced by this stage are our Parallel-Seen variants. 3.5 Eliciting Parallel Thinking via Reinforcement Learning in Structure Models In the previous section, we explored an RL framework that trains models to use parallel thinking without modifying their underlying architecture. However, this approach, which we call ParallelSeen, does not explicitly isolate reasoning paths. As result, hidden representations from one path can inadvertently leak into others, and gradients across paths can interfere with each other during training. To explore an alternative solution, we introduce structured variant of our framework, Parallel-Unseen. This model incorporates explicit inductive biases into the attention mechanism to enforce path isolation. Specifically, inspired by prior work (Yang et al., 2025b), we design path-window masking and multiverse position encodings to achieve the goal. 3.5.1 Structured Attention Mechanism We incorporate these inductive biases directly into the attention layer, as shown in Figure 2. Path-window masking restricts each token within <Path> block to attend only to tokens from the same path and the shared context. This prevents cross-path information leakage. Multiverse position encodings assign disjoint set of position indices to each path, ensuring that the positional embedding space does not overlap. Figure 2: Illustration of the structured attention mask and position IDs, where different paths and the summary block have distinct visibility regions. Blank regions indicate tokens that cannot attend to each other, while colored regions indicate tokens that can. Together, these constraints enforce explicit isolation among reasoning threads while preserving visibility from the shared <Summary> block, which is essential for integrating insights across paths. 7 Technical Report 3.5.2 The Training Recipe and Reward Modeling In preliminary experiments, we find that directly applying the progressive training recipe from Parallel-R1-Seen to the structured variant proves ineffective. We attribute this to the poor generalization of attention masks from easy to hard math (Yang et al., 2025c). To address this limitation, we remove the stage one RL and redesign the reward schedule and evaluate two alternative schemes. (S1) Accuracy-only. We optimize solely for task correctness, removing direct incentives for parallel usage. (S2) Alternating accuracy and parallel. In this scheme, we alternate between two different rewards within fixed windows of W=10 steps. For 80% of the steps, we use standard accuracy-only reward (Racc). For the remaining 20% of the steps, we use tiered reward system to provide nuanced incentive for parallel thinking: +1.2: If the generated output contains at least one parallel thinking unit AND the final answer is correct. +1.0: If the generated output does not contain parallel thinking unit AND the final answer is correct. -1.0: For all other cases, including incorrect answers. This schedule reintroduces calibrated incentive for parallel usage without letting it dominate training. Together, these reward designs allow Parallel-R1-Unseen to leverage the benefits of structural isolation while avoiding overfitting to superficial parallel patterns."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setups Model. We use Qwen-3-4B-Base (Yang et al., 2025a) as our backbone, the latest state-of-the-art open-source model at this scale, offering an ideal balance between performance and efficiency. Evaluation. We measure our models on four standard mathematical reasoning benchmarks, including AIME24, AIME25, AMC23, and MATH (Hendrycks et al., 2021). On the MATH dataset, we generate one response per question using sampling temperature of = 1.0. For the remaining three datasets, we sample 16 independent responses per question at the same temperature and report the average accuracy (i.e., mean@16) to reduce randomness, which is consistent with settings in Wang et al. (2025c). We additionally report pass@16 to show the upper bound of our approach. Training Details. Our codebase is adapted from VERL (Sheng et al., 2024), where we primarily follow its official training recipe without any hyperparameter tuning. In the cold start stage, we perform SFT on our curated Parallel-GSM8K, using batch size of 128, learning rate of 1e-5, weight decay of 0.01, and warm-up step ratio of 0.1 with the cosine learning-rate schedule, resulting in 58/230 gradient update steps for Parallel-SFT-Seen and Parallel-SFT-Uneen, respectively. For Stage 1, we optionally perform RL on GSM8K for five epochs, using batch size of 1024, 5 rollouts, and learning rate of 1e-6 without warm-up or learning rate scheduling, resulting in 35 gradient update steps. For Stage 2, we perform RL on the DAPO training set for 300 gradient update steps, using batch size of 512, rollout of 8, and learning rate of 1e-6 without warm-up or learning rate scheduling. 4.2 Main Results Table 2 presents the results across four benchmarks: AIME25, AIME24, AMC23, and MATH. We compare our method against two baselines: 1) RL with GRPO algorithm directly on the DAPO 8 Technical Report Table 2: Performance comparison on mathematical reasoning benchmarks for the Qwen-3-4B-Base model trained under different parallel thinking configurations. We report Mean@16 and Pass@16 for AIME25, AIME24, and AMC23, while MATH is evaluated with Mean@1. Method # Parallel AIME AIME24 AMC23 MATH Avg. Mean@16 Pass@16 Mean@16 Pass@16 Mean@16 Pass@16 Mean@1 Qwen3-4B-Base SFT + Parallel Parallel-SFT-Seen Parallel-SFT-Unseen RL Approach GRPO (DAPO) + RL on GSM8K Parallel-R1-Seen Parallel-R1-Unseen (S1) Parallel-R1-Unseen (S2) 0.0 95.6 95.6 0.0 0.0 27.3 13.6 63. 1.3 8.0 5.2 14.8 13.3 19.2 17.7 19.0 10.2 2.9 16. 8.1 51.2 13.9 6.6 29.8 20.9 32.4 26.3 38.9 37.8 42. 10.6 8.5 18.5 18.8 19.4 18.3 16.3 26.4 26.7 30.6 34.9 37.1 33.2 31.8 48.9 41.7 63.6 66.4 70.5 69.7 67. 79.2 80.1 85.1 82.2 85.0 88.9 91.5 76.6 71.5 83.5 82.6 86.7 82.6 84.5 36.0 31.7 45.1 45.3 48.9 47.1 46. Table 3: Ablation Study on Training Approach: Comparison of different training configurations. Training Configuration AIME25 AIME24 AMC23 MATH Avg. Effect of Training Stages Parallel-R1-Seen - w/o RL on GSM8K Parallel-R1-Unseen (S1) + with RL on GSM8K Effect of Parallel Thinking Prompt Parallel-R1-Seen - w/o Parallel Thinking Prompt 19.2 17.9 17.7 14.4 19.2 20.4 19.4 19.0 18.3 12.9 19.4 16.5 70.5 65.0 69.7 52. 70.5 66.7 86.7 84.5 82.6 74.4 48.9 46.6 47.1 38.5 86.7 84.8 48.9 47.1 training set, and 2) RL with GRPO in two stages: first trained on the GSM8K data, then further trained with RL on the DAPO training set. The second baseline is included to ensure fair comparison. Our progressive Parallel-R1 framework proved to be the most effective approach, consistently outperforming all baselines as shown in Table 2. The top-performing causal variant, Parallel-R1-Seen, achieved the highest average score of 48.9. This success stems from curriculum designed to overcome the limitations of simpler methods. For instance, while SFT provides substantial foundational improvement (e.g., 31.7 for Parallel-SFT-Unseen vs. 4.6 for the base model), it is insufficient for advanced reasoning and falls considerably short of the standard GRPO baselines score of 45.1. Besides, we found that naive additional RL on easier data offers only marginal benefit on average (45.3 vs. 45.1), validating our strategy of using cold start for targeted format and behavior learning. Our results also reveal key design trade-offs. The superior performance of the Seen model compared to its structured counterparts suggests that explicit architectural modifications can be detrimental to RL training. Furthermore, the comparison between reward schedules for Parallel-R1-Unseen (S1) and (S2) highlights that reward design is essential for effectively managing the trade-off between the parallel ratio and overall performance. We provide detailed analysis in Section 4.3.3. 4.3 Analysis 4.3.1 Ablation on Training Approach We further investigate the role of two-stage RL in our training pipeline. One natural question is whether learning on GSM8K, which is relatively simple math dataset, truly benefits from the RL, Technical Report Table 4: Ablation Study on Reward Modeling for the PARALLEL-R1-UNSEEN Model. Training Configuration Parallel Ratio AIME 25 AIME 24 AMC 23 MATH Accuracy Parallel Alternating Acc./Parallel 13.6 80.3 63. 17.7 17.7 19.0 18.3 15.2 16.3 69.7 59.4 67.5 82.6 81.7 84.5 given that the structural parallel reasoning format (e.g., the correct use of <Parallel>, <Path>, and <Summary> tokens) can be directly acquired through SFT (Yang et al., 2025b). Table 3 presents the ablation results. For the Causal variant, keeping the Cold Start SFT but removing stage one RL training on GSM8K leads to consistent performance drop (2.3% on average). This indicates that learning format through SFT alone is insufficient. Without stage one RL, the model enters stage two training on general math without having acquired the ability to trigger or use parallel thinking adaptively. As result, RL training must simultaneously learn both adaptive parallel thinking behavior and mathematical reasoning ability, which is harder to optimize. Interestingly, the Structure variant exhibits the opposite trend: adding stage one RL on GSM8K severely hurts performance (8.6% on average). We hypothesize that this is because the structured attention mask learned on easy math tasks (GSM8K) does not transfer well to the distribution shift of harder math problems, causing overfitting to superficial patterns, which is consistent with findings in (Yang et al., 2025c). This contrast highlights key insight: while stage one RL is crucial for the causal variant to bootstrap adaptive parallel thinking, structural variants require different training recipe and reward schedule to generalize effectively. 4.3.2 Ablation on Parallel Thinking Prompt We also conduct an ablation study on the effect of our parallel thinking prompt. As shown in Table 3, removing the prompt leads to performance degradation of up to 1.8% on average. It indicates that providing more detailed instructions during training helps the model better understand the reasoning process, rather than merely memorizing the output patterns. 4.3.3 Ablation Studies on Reward Modeling: How to Effectively Stimulate Parallel Thinking In our work, key question is how to effectively stimulate parallel thinking behavior. To answer this, we test several reward modeling strategies, including direct accuracy, direct parallel, and an alternating approach. We present the results in Table 4. We can draw the following observations: Directly rewarding accuracy is insufficient to stimulate parallel thinking. The Accuracy configuration, which optimizes solely for problem correctness, yields the highest performance on two out of four benchmarks, particularly on the AMC dataset (69.7). However, this approach yields very low parallel ratio of 13.6, indicating that the model, although effective at problemsolving, does not naturally adopt parallel reasoning style. This suggests that simply pursuing accuracy does not encourage the model to explore alternative reasoning structures. Directly rewarding parallel thinking is detrimental to overall performance. In contrast, the Parallel configuration, which directly rewards the generation of parallel structures, achieves an impressively high parallel ratio of 80.3. This demonstrates that the model is highly responsive to this reward signal. However, this focused optimization leads to significant performance drop across most benchmarks, with scores on AMC 23 and MATH decreasing to 59.4 and 81.7, respectively. This highlights critical finding: unconstrained encouragement of parallel behavior can cause the model to prioritize structural form over logical correctness, ultimately hindering its problem-solving capabilities. Our alternating approach effectively balances parallel thinking with performance. The Alternating Acc./Parallel strategy, which periodically switches between rewarding accuracy and parallel structures, provides superior balance. This approach successfully stimulates parallel thinking, achieving parallel ratio of 63.0, which is significantly higher than the direct accuracy 10 Technical Report method. Crucially, this increase in parallel thinking does not come at the expense of performance. In fact, on some challenging benchmarks, such as AIME 25, our method even surpasses the direct accuracy approach, achieving scores of 19.0. 4.4 Evolution of Parallel Thinking Behavior During RL Training Key Finding 2 The models parallel thinking behavior evolves throughout RL training, shifting from earlystage computational exploration to late-stage multi-perspective verification. To better understand how the models strategy evolves, we analyzed the positional dynamics of the <Parallel> block throughout the RL training. We measured the relative position of each block by dividing its starting token index by the total sequence length of the solution. The training dynamics in Figure 3 show clear and consistent trend: the average relative position of the <Parallel> block steadily increases as RL training progresses, indicating strategic shift from applying this feature early in the reasoning chain toward the very end. 0.8 0.6 0. i o i e 0 50 100 150 250 We interpret this positional shift as the model adopting more conservative strategy to maximize its reward, behavior shaped directly by the final-answer-dominated reward design. In the early stages of training, when the models reasoning ability is weak, using parallel paths for computational exploration is necessary, high-variance strategy to discover potential solution. However, as the models core reasoning ability improves, such early-stage exploration becomes liability that could introduce errors and jeopardize the final reward. Figure 3: Dynamics of the relative position of the <Parallel> block during RL training. The increasing trend indicates the model learns to apply parallel thinking later in the reasoning process. Training Steps Consequently, the model learns more risk-averse strategy to secure correct answer. It first derives solution using single, high-confidence reasoning path. Only after potential answer is found, it deploys the <Parallel> block for multi-perspective verification. This late-stage use of parallel thinking confirms the result without risking the integrity of the primary solution path, thus maximizing the probability of receiving positive reward. This learned behavior aligns with our broader finding of tension between final-answer optimization and the preservation of diverse reasoning structures. To further illustrate this behavioral evolution, we present two representative case studies below (Figure 5 and 6). The first case, from an early-stage model, demonstrates the use of parallel thinking for exploration. The second, from the late-stage model, exemplifies the learned, verification-oriented strategy. 4.5 Extra Bonus: Parallel Thinking as Mid-Training Exploration Strategy for RL Training Key Finding 3 Parallel thinking itself can serve as an effective structured exploration mechanism to improve RL training. 11 Technical Report ) % ( ) 6 1 @ m ( 5 2 A 20 15 10 5 0 Exploration (Parallel Thinking) Exploitation (Parallel Sequential) 25.6% 100 200 400 500 Training Steps 100 ) % ( a l a 80 60 40 0 Figure 4: Two-stage training with parallel reasoning as mid-training exploration scaffold. Left axis plots AIME25 accuracy for Baseline (gray), Stage-1 (blue), and Stage-2 (red); right axis shows the proportion of outputs using the explicit parallel thinking structure. Stage-1 (0200 steps; vertical dashed line) alternates ACC/PAR rewards to promote exploration, while Stage-2 continues GRPO with an accuracy reward only and is plotted after +200-step shift to align the timeline. As training transitions from parallel to more sequential reasoning, the parallel ratio decreases yet accuracy continues to improve, peaking at 25.6%, which exceeds single-thread model trained via GRPO. In this section, we investigate the hypothesis that parallel thinking itself can serve as an effective structured exploration mechanism to improve RL training. fundamental challenge in RL is ensuring the model sufficiently explores the policy space to avoid converging to local optima. We posit that by compelling the model to generate multiple, parallel thought blocks at specific reasoning steps, we introduce strong inductive bias that forces more structured and diverse exploration, guiding the model toward more robust policy spaces. To empirically validate this hypothesis, we designed two-stage training curriculum, with its dynamics and results presented in Figure 4. Stage-1 (Exploration Phase, steps 0-200): The primary goal of this initial phase is to maximize In this stage, we follow the training approach of our Parallel-R1-Unseen (S2), exploration. which explicitly incentivizes the use of the parallel thinking structure by applying an alternating ACC/PAR reward. As shown by the green dashed line in Figure 4, this successfully maintains high parallel ratio, forcing the model to explore wide breadth of reasoning paths constantly. Stage-2 (Exploitation Phase, after 200 steps): At the 200-step mark, we change the focus from exploration to exploitation. The training objective is then switched to optimize for accuracy alone, allowing the model to refine and exploit the effective strategies discovered during the exploration phase. The experimental results provide evidence in support of our hypothesis. As depicted in Figure 4, upon entering stage 2, the models performance (red line) improves, reaching peak AIME25 accuracy of 25.6%, notable improvement over the Baseline GRPO model. Critically, this performance gain occurs even as the models reliance on the parallel structure decreases (as shown by the declining parallel ratio in stage 2). This key observation suggests that the value of parallel thinking lies not only in the effectiveness of the parallel structure itself (which already outperforms the baseline), but more importantly, in the robust policy space it helps discover through exploration. 12 Technical Report The initial forced exploration acted as scaffold, guiding the model to more effective region in the policy space, from which it could then learn final policy."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we presented Parallel-R1, the first reinforcement learning framework to teach large language models to perform parallel thinking from scratch on real-world mathematical reasoning tasks. We proposed progressive training curriculum, enabled by simple and scalable data pipeline, that successfully bootstraps this complex skill by separating the learning of format, behavior, and core reasoning into distinct stages. Our approach achieved consistent accuracy improvements on several challenging mathematical reasoning benchmarks compared to strong baselines. Our analysis yielded several key insights into the learning dynamics. We discovered that the model learns risk-averse strategy, shifting its use of parallel thinking from early-stage computational exploration to late-stage multi-perspective verification. Most significantly, we empirically identified and validated the potential of parallel thinking as mid-training scaffold, showing that adding this temporary, forced-exploration phase can unlock higher final performance ceilings after RL training."
        },
        {
            "title": "Acknowledgment",
            "content": "We thank Louis Liu from the University of California, Berkeley for insightful discussions and valuable feedback on this work."
        },
        {
            "title": "References",
            "content": "Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc Le, Christopher Re, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. Keyu Chen, Zhifeng Shen, Daohai Yu, Haoqian Wu, Wei Wen, Jianfeng He, Ruizhi Qiao, and Xing Sun. Aspd: Unlocking adaptive serial-parallel decoding by exploring intrinsic parallelism in llms. arXiv preprint arXiv:2508.08895, 2025a. Yanda Chen, Joe Benton, Ansh Radhakrishnan, Jonathan Uesato, Carson Denison, John Schulman, Arushi Somani, Peter Hase, Misha Wagner, Fabien Roger, et al. Reasoning models dont always say what they think. arXiv preprint arXiv:2505.05410, 2025b. Andy Clark. Microcognition: Philosophy, cognitive science, and parallel distributed processing, volume 6. MIT Press, 1989. Runpeng Dai, Run Yang, Fan Zhou, and Hongtu Zhu. Breach in the shield: Unveiling the vulnerabilities of large language models. arXiv preprint arXiv:2504.03714, 2025a. Runpeng Dai, Tong Zheng, Run Yang, Kaixian Yu, and Hongtu Zhu. R1-re: Cross-domain relation extraction with rlvr. arXiv preprint arXiv:2507.04642, 2025b. Yichao Fu, Xuewei Wang, Yuandong Tian, and Jiawei Zhao. Deep think with confidence. arXiv preprint arXiv:2508.15260, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. 13 Technical Report Chan-Jan Hsu, Davide Buffelli, Jamie McGowan, Feng-Ting Liao, Yi-Chang Chen, Sattar Vakili, and Da-shan Shiu. Group think: Multiple concurrent reasoning agents collaborating at token level granularity. arXiv preprint arXiv:2505.11107, 2025. Chengsong Huang, Langlin Huang, and Jiaxin Huang. Divide, reweight, and conquer: logit arithmetic approach for in-context learning. arXiv preprint arXiv:2410.10074, 2024. Chengsong Huang, Langlin Huang, Jixuan Leng, Jiacheng Liu, and Jiaxin Huang. Efficient test-time scaling via self-calibration. arXiv preprint arXiv:2503.00031, 2025a. Chengsong Huang, Wenhao Yu, Xiaoyang Wang, Hongming Zhang, Zongxia Li, Ruosen Li, Jiaxin Huang, Haitao Mi, and Dong Yu. R-zero: Self-evolving reasoning llm from zero data. arXiv preprint arXiv:2508.05004, 2025b. Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025c. Ray Jackendoff. The parallel architecture and its place in cognitive science. Syntax and Morphology Multidimensional. Eds. A. Nolda, O. Teuber. Berlin, New York: Mouton De Gruyter, pp. 1744, 2011. Tian Jin, Ellie Cheng, Zack Ankner, Nikunj Saunshi, Blake Elias, Amir Yazdanbakhsh, Jonathan Ragan-Kelley, Suvinay Subramanian, and Michael Carbin. Learning to keep promise: Scaling language model decoding parallelism with learned asynchronous decoding. arXiv preprint arXiv:2502.11517, 2025. Zongxia Li, Wenhao Yu, Chengsong Huang, Rui Liu, Zhenwen Liang, Fuxiao Liu, Jingxi Che, Dian Yu, Jordan Boyd-Graber, Haitao Mi, et al. Self-rewarding vision-language model via reasoning decomposition. arXiv preprint arXiv:2508.19652, 2025. Bo Liu, Leon Guertler, Simon Yu, Zichen Liu, Penghui Qi, Daniel Balcells, Mickel Liu, Cheston Tan, Weiyan Shi, Min Lin, et al. Spiral: Self-play on zero-sum games incentivizes reasoning via multi-agent multi-turn reinforcement learning. arXiv preprint arXiv:2506.24119, 2025. Thang Luong and Edward Lockhart. Advanced version of gemini with deep think officially achieves gold medal standard at the international mathematical olympiad. https://deepmind.googl e/discover/blog/advanced-version-of-gemini-with-deep-think-officially-achieve s-gold-medal-standard-at-the-international-mathematical-olympiad/, 2025. Accessed: 2025-07-30. Matthew Macfarlane, Minseon Kim, Nebojsa Jojic, Weijia Xu, Lucas Caccia, Xingdi Yuan, Wanru Zhao, Zhengyan Shi, and Alessandro Sordoni. Instilling parallel reasoning into language models. In 2nd AI for Math Workshop @ ICML 2025, 2025. URL https://openreview.net/forum?id=a3o4 b3hkwp. Jiayi Pan, Xiuyu Li, Long Lian, Charlie Snell, Yifei Zhou, Adam Yala, Trevor Darrell, Kurt Keutzer, and Alane Suhr. Learning adaptive parallel reasoning with language models. arXiv preprint arXiv:2504.15466, 2025. Gleb Rodionov, Roman Garipov, Alina Shutova, George Yakushev, Erik Schultheis, Vage Egiazarian, Anton Sinitsin, Denis Kuznedelev, and Dan Alistarh. Hogwild! inference: Parallel llm generation via concurrent attention. arXiv preprint arXiv:2504.06261, 2025. Mahdi Sabbaghi, Paul Kassianik, George Pappas, Yaron Singer, Amin Karbasi, and Hamed Hassani. Adversarial reasoning at jailbreaking time. arXiv preprint arXiv:2502.01633, 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 14 Technical Report Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Yucheng Shi, Wenhao Yu, Zaitang Li, Yonglin Wang, Hongming Zhang, Ninghao Liu, Haitao Mi, and Dong Yu. Mobilegui-rl: Advancing mobile gui agent through reinforcement learning in online environment. arXiv preprint arXiv:2507.05720, 2025. Sree Harsha Tanneru, Dan Ley, Chirag Agarwal, and Himabindu Lakkaraju. On the hardness of faithful chain-of-thought reasoning in large language models. arXiv preprint arXiv:2406.10625, 2024. Haozhe Wang, Long Li, Chao Qu, Fengming Zhu, Weidi Xu, Wei Chu, and Fangzhen Lin. To code or not to code? adaptive tool integration for math language models via expectation-maximization. arXiv preprint arXiv:2502.00691, 2025a. Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen. Vl-rethinker: Incentivizing self-reflection of vision-language models with reinforcement learning. arXiv preprint arXiv:2504.08837, 2025b. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939, 2025c. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a. Xinyu Yang, Yuwei An, Hongyi Liu, Tianqi Chen, and Beidi Chen. Multiverse: Your language models secretly decide how to parallelize and merge generation. arXiv preprint arXiv:2506.09991, 2025b. Xinyu Yang, Tianqi Chen, and Beidi Chen. Ape: Faster and longer context-augmented generation via adaptive parallel encoding. arXiv preprint arXiv:2502.05431, 2025c. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822, 2023. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Yu Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, et al. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118, 2025. Di Zhang, Xiaoshui Huang, Dongzhan Zhou, Yuqiang Li, and Wanli Ouyang. Accessing gpt-4 level mathematical olympiad solutions via monte carlo tree self-refine with llama-3 8b. arXiv preprint arXiv:2406.07394, 2024. Tong Zheng, Lichang Chen, Simeng Han, Thomas McCoy, and Heng Huang. Learning to reason via mixture-of-thought for logical reasoning. arXiv preprint arXiv:2505.15817, 2025. Yujun Zhou, Jiayi Ye, Zipeng Ling, Yufei Han, Yue Huang, Haomin Zhuang, Zhenwen Liang, Kehan Guo, Taicheng Guo, Xiangqi Wang, et al. Dissecting logical reasoning in llms: fine-grained evaluation and supervision study. arXiv preprint arXiv:2506.04810, 2025. Technical Report Yuxin Zuo, Kaiyan Zhang, Li Sheng, Shang Qu, Ganqu Cui, Xuekai Zhu, Haozhan Li, Yuchen Zhang, Xinwei Long, Ermo Hua, et al. Ttrl: Test-time reinforcement learning. arXiv preprint arXiv:2504.16084, 2025. 16 Technical Report"
        },
        {
            "title": "A Prompts",
            "content": "Baseline Prompt {Problem} Lets think step by step and output the final answer after \"Final Answer: \". Parallel Thinking Prompt Solve the following problem step by step. During the reasoning process, whenever you encounter step that may benefit from multiple perspectives or independent reasoning, insert <Parallel> block at that point. Within each <Parallel> block: Include at least two distinct and independent reasoning paths. Each path must be enclosed within <Path> and </Path> tags. Do not include any ordering information or cross-references between paths, as they are generated simultaneously and independently. Close the block with </Parallel>. Immediately after each </Parallel>, write concise summary of insights or conclusions drawn from all paths, enclosed in <Summary> and </Summary> tags. Repeat this process adaptively as needed throughout the reasoning. Do not explicitly mention that you are triggering parallel thinking-just insert the <Parallel> block naturally within the reasoning chain. End your response with line starting with Final Answer: followed by the final result. Problem: {Problem} 17 Technical Report Algorithm 1 Parallel Thinking Format Check Input: tokens list of tokens from the parallel-thinking trace; tag pairs set of valid (opening, closing) tag pairs, e.g. {(<Path>...</Path>), . . . } push onto if is empty then if is an opening tag then else if is closing tag then format valid false break Output: format valid boolean indicating whether the trace is well-formed 1: 2: format valid true 3: for all in tokens do 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: end for 20: if format valid and = then format valid false 21: 22: end if 23: return format valid end if top tag Top(S) if (top tag, t) tag pairs then format valid false break end if end if pop else"
        },
        {
            "title": "B Case Studies",
            "content": "18 Technical Report Case Study: Find the sum of all integer bases > 9 for which 17b is divisor of 97b. First, the number 17b in base is equal to 1 + 7 = + 7 in base 10. Similarly, 97b is equal to 9 + 7 = 9b + 7 in base 10. We need to find all integer bases > 9 such that + 7 divides 9b + 7. At this point, consider two distinct strategies for finding b: < Parallel > < Path > Factor theorem approach : Consider 9 + 7 as polynomial in and use the factor theorem . Since + 7 divides 9 + 7 , by the factor theorem , 9( -7) + 7 = -63 + 7 = -56 must be divisible by + 7. Since > 9 , + 7 > 16 , and -56 can be factored as -56 x1 , -28 x2 , -14 x4 , or -7 x8 ( and their positive counterparts ) . Only + 7 = 14 gives = 7 , but > 9 , so no solution from this path . However , we should also consider the positive factors for completeness . The positive factorization 56 x1 , 28 x2 , 14 x4 , or 7 x8 yields + 7 = 56 ( giving = 49) , + 7 = 28 ( giving = 21) , + 7 = 14 ( giving = 7 , invalid ) , and + 7 = 8 ( giving = 1 , invalid ) . Thus , the only valid solution from this path is = 21 and = 49. </ Path > < Path > Using base arithmetic : In base , 97 _b - 1 x17 _b = 97 _b - 17 _b = 80 _b , which is 8 xb + 0 = 8 in base 10. Since 9 + 7 = 8 + ( + 7) , + 7 must divide 8b , implying that for some integer , 9 + 7 = ( + 7) (8 + ) . Simplifying , 9 + 7 = 8 + 8 + kb + 7k , so (1 - ) = 8 + 7. Since > 9 , 1 - must be positive or negative to keep an integer . If 1 - = 1 , then = 8 + 7 with < 1. Possible integer values are <= 0 , but = 0 gives = 7 <= 9 , and < 0 gives negative , which is invalid . Thus , this path leads to no valid solutions directly . However , it reinforces that + 7 must divide 8 . </ Path > </ Parallel > < Summary > These two approaches consider both the algebraic factorization method and an alternative numerical or arithmetic approach to find possible values of , but only the factorization method yields valid bases greater than 9. </ Summary > Next, consider the condition that + 7 divides 9b + 7. From the earlier, 9b + 7 = 9(b + 7) 56. For + 7 to divide 9b + 7, it must divide 56. Since + 7 17 for > 9, the only divisors of 56 exceeding 16 are 28 and 56, giving + 7 {28, 56} and thus {21, 49}. Finally, verify: For = 21: 1721 = 21 + 7 = 28, 9721 = 9 21 + 7 = 196, 196 28 = 7. For = 49: 1749 = 49 + 7 = 56, 9749 = 9 49 + 7 = 448, 448 56 = 8. Thus, the sum is 21 + 49 = 70. Final Answer: 70 Figure 5: qualitative case study showcases that our model uses parallel exploration of two distinct algebraic methods for solving number theory problem, both converging on the correct answer. 19 Technical Report Case Study: Find the sum of all integer bases > 9 for which 17b is divisor of 97b. First, need to find all integer bases > 9 for which 17b is divisor of 97b. To do this, Ill start by converting these base-b numbers to base 10, as thats more familiar. In base b, the number 17b means 1 + 7 = + 7 in base 10. Similarly, 97b means 9 + 7 = 9b + 7 in base 10. So, the condition that 17b divides 97b translates to + 7 dividing 9b + 7 in base 10. Now, for + 7 to divide 9b + 7, the remainder when 9b + 7 is divided by + 7 must be zero. can perform polynomial division or use the remainder theorem to find this. Let me compute the remainder of 9b + 7 divided by + 7. Divide 9b + 7 by + 7: - The quotient is 9, since 9 (b + 7) = 9b + 63. - Subtract this from 9b + 7: (9b + 7) (9b + 63) = 9b + 7 9b 63 = 56. So, the remainder is 56. For + 7 to divide 9b + 7, it must divide 56, or equivalently, + 7 must be positive divisor of 56. The positive divisors of 56 are: 1, 2, 4, 7, 8, 14, 28, 56. Since > 9, we have + 7 > 16. So, + 7 must be divisor greater than 16. - + 7 = 28 = 21 (valid) - + 7 = 56 = 49 (valid) So, the valid bases are = 21 and = 49. Now, need to find the sum: 21 + 49 = 70. But before finalizing, should confirm that these are the only valid bases. From the divisor check, only + 7 = 28 and + 7 = 56 give > 9, so no others. < Parallel > < Path > One might consider that $ + 7 $ not only divides 56 but also that it must be such that $ 17 _b $ and $ 97 _b $ are valid base - $b$ numbers , which requires $ > 7 $ ( since digits 9 and 7 are used ) . However , since $ > 9 $ is given , this is already satisfied , and the divisor condition is the key . </ Path > < Path > Use the fact that for $ + 7 $ to divide $ 9 + 7 $ , the remainder must be zero , so we can solve $ 9 + 7 = ( + 7) $ for integer $ $ , but it simpler to compute the remainder directly as above . </ Path > </ Parallel > < Summary > The parallel block explored alternative strategies for solving $ + 7 $ divides $ 9 + 7 $ , including finding divisors , considering base validity , and solving equations , but ultimately confirmed that only $ = 21 $ and $ = 49 $ satisfy the condition for $ > 9 $ . </ Summary > Now, sum the valid bases: 21 + 49 = 70. Final Answer: 70 Figure 6: qualitative case study demonstrates late-stage verification through parallel thinking."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "City University of Hong Kong",
        "Tencent AI Lab Seattle",
        "University of Maryland, College Park",
        "University of North Carolina at Chapel Hill",
        "Washington University in St. Louis"
    ]
}