{
    "paper_title": "Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training",
    "authors": [
        "Junlin Han",
        "Shengbang Tong",
        "David Fan",
        "Yufan Ren",
        "Koustuv Sinha",
        "Philip Torr",
        "Filippos Kokkinos"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs), despite being trained on text alone, surprisingly develop rich visual priors. These priors allow latent visual capabilities to be unlocked for vision tasks with a relatively small amount of multimodal data, and in some cases, to perform visual tasks without ever having seen an image. Through systematic analysis, we reveal that visual priors-the implicit, emergent knowledge about the visual world acquired during language pre-training-are composed of separable perception and reasoning priors with unique scaling trends and origins. We show that an LLM's latent visual reasoning ability is predominantly developed by pre-training on reasoning-centric data (e.g., code, math, academia) and scales progressively. This reasoning prior acquired from language pre-training is transferable and universally applicable to visual reasoning. In contrast, a perception prior emerges more diffusely from broad corpora, and perception ability is more sensitive to the vision encoder and visual instruction tuning data. In parallel, text describing the visual world proves crucial, though its performance impact saturates rapidly. Leveraging these insights, we propose a data-centric recipe for pre-training vision-aware LLMs and verify it in 1T token scale pre-training. Our findings are grounded in over 100 controlled experiments consuming 500,000 GPU-hours, spanning the full MLLM construction pipeline-from LLM pre-training to visual alignment and supervised multimodal fine-tuning-across five model scales, a wide range of data categories and mixtures, and multiple adaptation setups. Along with our main findings, we propose and investigate several hypotheses, and introduce the Multi-Level Existence Bench (MLE-Bench). Together, this work provides a new way of deliberately cultivating visual priors from language pre-training, paving the way for the next generation of multimodal LLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 5 2 6 6 2 . 9 0 5 2 : r Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training Junlin Han1,2,, Shengbang Tong1, David Fan1, Yufan Ren1, Koustuv Sinha1, Philip Torr2, Filippos Kokkinos1 1Meta Superintelligence Labs, 2University of Oxford Project lead Large Language Models (LLMs), despite being trained on text alone, surprisingly develop rich visual priors. These priors allow latent visual capabilities to be unlocked for vision tasks with relatively small amount of multimodal data, and in some cases, to perform visual tasks without ever having seen an image. Through systematic analysis, we reveal that visual priorsthe implicit, emergent knowledge about the visual world acquired during language pre-trainingare composed of separable perception and reasoning priors with unique scaling trends and origins. We show that an LLMs latent visual reasoning ability is predominantly developed by pre-training on reasoning-centric data (e.g., code, math, academia) and scales progressively. This reasoning prior acquired from language pre-training is transferable and universally applicable to visual reasoning. In contrast, the perception prior emerges more diffusely from broad corpora, and perception ability is more sensitive to the vision encoder and visual instruction tuning data. In parallel, text describing the visual world proves crucial, though its performance impact saturates rapidly. Leveraging these insights, we propose data-centric recipe for pre-training vision-aware LLMs and verify it in 1T token scale pre-training. Our findings are grounded in over 100 controlled experiments consuming 500,000 GPU-hours, spanning the full MLLM construction pipelinefrom LLM pre-training to visual alignment and supervised multimodal fine-tuningacross five model scales, wide range of data categories and mixtures, and multiple adaptation setups. Along with our main findings, we propose and investigate several hypotheses, and introduce the Multi-Level Existence Bench (MLE-Bench). Together, this work provides new way of deliberately cultivating visual priors from language pre-training, paving the way for the next generation of multimodal LLMs. Date: October 1, 2025 junlinhan@meta.com Correspondence: Project page: https://junlinhan.github.io/projects/lsbs/"
        },
        {
            "title": "3 Demystifying LLM Visual Priors: Studies and Findings",
            "content": "3.1 3.2 3.3 3.4 Deriving data mixture for more vision-aware LLMs. 3.5 The structure and origin of learned visual priors. 3.6 Deconstructing multimodal abilities: vision or language. Impact of model and data sizes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Impact of pre-training data sources. Impact of reasoning and visual data categories and proportions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "4 Discussion and Hypotheses",
            "content": "4.1 Is the perception prior multi-level? An evaluation using the MLE-Bench . . . . . . . . . . . 1 2 4 4 5 6 6 7 8 10 11"
        },
        {
            "title": "6.1 From LLMs to MLLMs.\n6.2 The role of data in shaping foundation model capabilities.",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "A Broader Impact",
            "content": "B Visual World and Reasoning-centric Language Data Classification"
        },
        {
            "title": "D Robust Parsing for VQA Evaluations",
            "content": "E Multi-Level Existence Benchmark Construction Multi-Level Existence Benchmark Results"
        },
        {
            "title": "1 Introduction",
            "content": "13 15 16 16 16 17 17 17 17 18 19 19 20 20 21 22 23 26 28 compelling phenomenon has emerged at the forefront of AI research: Large Language Models (LLMs), despite being trained exclusively on vast corpora of text, appear to develop profound priors about the visual world. This latent capability is paradoxical, suggesting that the statistical patterns within language might be rich enough to encode fundamental principles of vision, from object properties to spatial relationships, without ever observing single image. This emergent visual prior presents in several surprising and powerful ways: Programmatic visual knowledge. LLMs possess rich visual knowledge, enabling them to generate executable code that renders complex 2D and 3D scenes, from objects to spatial layouts (Sharma et al., 2024; Sun et al., 2025; Ge et al., 2025; Ashutosh et al., 2025). This demonstrates grasp of visual concepts without ever seeing single image. The resulting synthetic data is of sufficient quality to pre-train standard vision models for successful generalization to real-world images (Sharma et al., 2024). Data-efficient visual adaptation. LLMs are highly efficient for visual adaptation. With vision encoder, high-level reasoning emerges from instruction tuning on small scale of image-text pairs, bypassing the need for massive multimodal pretraining (Alayrac et al., 2022; Liu et al., 2023a; Li et al., 2023; Grattafiori et al., 2024; Tong et al., 2024a; Bai et al., 2025b). This data-efficient instruction tuning extends to unified models, where visual generation is unlocked with minimal data (Tong et al., 2024b). Furthermore, this efficiency enables adaptation to low-level visual tasks using vision-only data (Zheng 2 et al., 2024; Du et al., 2025), proving that an LLMs reasoning framework can function independently of cross-modal alignment. LLMs as strong vision encoders. The learned representations of LLMs can directly benefit pure vision tasks without language (Kumar et al., 2024; Pang et al., 2024; Lai et al., 2024; Bai et al., 2025a). When repurposed as visual encoders, the transformer layers of LLMs offer competitive performance on image classification, segmentation, and video understanding, even surpassing vision-specific backbones (Pang et al., 2024). These findings suggest that the hierarchical abstraction and long-range dependency modeling intrinsic to LLMs are not modality-specific, but rather capture general-purpose computational motifs that are well-suited to processing visual signals. This is also shown in neuron-level studies, which have identified multimodal neurons within LLMs that respond to the same abstract concept regardless of whether it is presented through text or vision (Schwettmann et al., 2023; Pan et al., 2023; Verma et al., 2024). Collectively, these phenomena are not isolated curiosities; they point toward deeper principle of representation learning. They lend strong empirical support to the Platonic Representation Hypothesis (Huh et al., 2024b; Jha et al., 2025), which posits that as models scale across diverse data and tasks, their latent representationswhether trained on text or imagesconverge toward shared, underlying statistical model of reality. In this view, text and images are different \"projections\" or \"shadows\" of the world, and powerful enough model can learn the structure of the world itself from any single projection. The visual priors in LLMs, therefore, may be direct consequence of them recovering this unified internal world model from text alone. These observations motivate systematic investigation into the visual priors that LLMs acquire from language pre-training. Rather than hand-crafted bias or Bayesian prior distribution, we frame visual priors as implicit knowledge or prior vision capabilities encoded in LLMs, whose primary effect is to grant both enhanced capability for vision tasks and greater ease of transfer to vision. We seek to determine their origins, dissect whether they form uniform block of knowledge or are composed of distinct, separable abilities, and explore how they can be leveraged to build more capable MLLMs. Our methodology is centered on controlled ablation studies (Allen-Zhu, 2024), where we deconstruct the sources of different visual capabilities. By carefully manipulating pre-training model scale, data scale, data categories, data mixing ratios, vision-encoder components, and visual instruction tuning data, we reveal the underlying laws that govern them. Our work presents the first systematic investigation into the nature and origins of visual priors in the pre-training of LLMs and shows three key contributions: Structure of visual priors. We establish that visual priors can be decomposed into perceptual and reasoning components. Source of visual priors. We identify that the models latent visual reasoning is predominantly cultivated by and scales progressively with reasoning-centric data, whereas its perception ability emerges more diffusely from broad, diverse data. Vision-aware language pre-training. We propose pre-training data-mixing strategy that strategically balances reasoning-centric and visually descriptive text to deliberately cultivate powerful visual priors for training LLMs that can result in stronger multimodal performance. Beyond our primary findings, our work also introduces two resources valuable for the MLLM community: (1): The Multi-Level Existence Bench (MLE-Bench): new benchmark specifically designed for the fine-grained evaluation of models perceptual abilities. (2): Blind visual instruction tuning: trick that serves as both practical tool for improving visual adaptation and probe to reveal how models can \"hack\" visual tasks with language. Ultimately, by demystifying the textual origins of these visual priors, this work contributes to more fundamental understanding of how complex, seemingly modality-specific capabilities are encoded within language, thereby offering clearer picture of the internal \"world models\" that foundation models learn from text alone and providing empirical support for the Platonic Representation Hypothesis."
        },
        {
            "title": "2 Problem Formulation",
            "content": "In this section, we introduce our default training and evaluation settings."
        },
        {
            "title": "2.1 Training protocol.",
            "content": "LLM pre-training setup. We follow standard practices and pre-train suite of decoder-only Transformer models that closely adhere to the Llama-3 architecture (Grattafiori et al., 2024), spanning five model scales: 340M, 1B, 3B, 7B, and 13B parameters. These models are trained for varying numbers of tokens at 0B, 5B, 10B, 20B, 30B, 50B, 70B, 100B, and up to 1T tokens. We use tokenizer with vocabulary size of approximately 32000. Training is performed using the AdamW optimizer (Loshchilov and Hutter, 2017) with peak learning rate of 3 104, following cosine decay schedule and warm-up over the first 1024 steps. All models are trained with context length of 2048 tokens and an effective global batch size of 1024. We fix the model size to 3B parameters and the total training data volume to 30B tokens as our default setting. LLM pre-training data. Our training data is composed of 16 sources, including academic, arts, biology, code, computer science, economics, encyclopedia, food, law, literature, mathematics, medicine, philosophy, politics, q-a forum, and web-crawl. Each source contains at least 50B tokens. MLLM adaptation setting. We adopt two-stage adaptation strategy following Cambrian-1 (Tong et al., 2024a) and Web-SSL (Fan et al., 2025), consisting of visual alignment and supervised fine-tuning. In the first stage, we train an MLP-based projector on top of frozen vision encoder and language model to align visual features with the LLM. Unless otherwise specified, we use MetaCLIP-B/16 (Xu et al., 2023) as the default vision encoder. Extracted visual features are uniformly resized to fixed length of 576 tokens. In the second stage, we perform supervised fine-tuning on mixture of vision-language and language-only instruction data to enhance the models multimodal instruction-following ability. Both the alignment and instruction tuning stages use the AdamW optimizer with cosine learning rate schedule and linear warm-up, and models are trained for single epoch. During alignment, we use learning rate of 1 103 with warm-up ratio of 6%. For instruction tuning, the learning rate is set to 4 105 with 3% warm-up. Training is conducted with an effective global batch size of 512. MLLM adaptation data. We adopt the Cambrian-1 and Web-SSL data pipeline, but with strategic data reductions to highlight the effect of vision priors. The initial alignment stage utilizes 1M image-caption dataset, which is roughly 40% of the original datasets size. This is followed by supervised fine-tuning on curated 3.5M subset of the Cambrian-7M data. This subset is balanced with approximately 1.5M language-only and 2M vision-language paired instructions, resulting in higher percentage of language-only instruction data than the original curation, as our models learn to follow language instructions during this phase. For all experiments, we fix the random seed as 42. For MLLM adaptation, we use the same order for data loading in both alignment and instruction tuning stage to get stable results."
        },
        {
            "title": "2.2 Evaluation protocol.",
            "content": "LLM evaluation. We conduct comprehensive evaluation of our pre-trained models language understanding and reasoning abilities. Following the benchmark suite used in Mamba (Gu and Dao, 2023) and GLA (Yang et al., 2023), we assess performance on two main fronts. For raw language modeling quality, we report the averaged perplexity (ppl) across Wikitext (Merity et al., 2016) and LAMBADA (Paperno et al., 2016). For reasoning, we evaluate zero-shot performance on diverse suite of commonsense and question-answering tasks, including PIQA (Bisk et al., 2020), HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2021), ARC (Clark et al., 2018), Copa (Reddy et al., 2019), SciQA (Auer et al., 2023), OpenbookQA (Mihaylov et al., 2018), and BoolQA (Clark et al., 2019). For concise comparison, we report the unweighted averaged accuracy over all these benchmarks. MLLM evaluation. To comprehensively assess the multimodal capabilities of our models, we follow Cambrian-1 and establish diverse evaluation suite comprising 16 public benchmarks. We group these benchmarks into 4 four key categories to isolate and probe the distinct components of the learned visual prior, ranging from fine-grained perception to abstract reasoning, and provide holistic view of model performance: General: This category probes the models ability to perform visual perception and connect it with commonsense knowledge, rather than complex, multi-step reasoning. It includes GQA (Hudson and Manning, 2019), MME (Fu et al., 2023), MMBench (Liu et al., 2024c), and SEED (Ge et al., 2023). Knowledge: This category evaluates the models capacity to connect visual information with the world and perform multi-step reasoning to solve complex scientific or mathematical problems. It covers ScienceQA (Lu et al., 2022b), MMMU (Yue et al., 2024), AI2D (Hiippala et al., 2021), and MathVista (Lu et al., 2023). OCR & Chart VQA: This category focuses on fine-grained perception, specifically the ability to accurately read and interpret dense textual and structured data within images. It comprises TextVQA (Sidorov et al., 2020), ChartQA (Masry et al., 2022), and OCRBench (Liu et al., 2023b). Vision-Centric: This category mainly probes abstract visual reasoning and rough perception skills, requiring the model to perform tasks such as spatial and 3D understanding, object counting, and IQ tests. It uses benchmarks including RealWorldQA (xAI, 2024), Blink (Fu et al., 2024), COCO, ADE, and Omni3D. COCO (Lin et al., 2014), ADE (Zhou et al., 2019), and Omni3D (Brazil et al., 2023) are proposed as CV-Bench from Cambrian-1 (Tong et al., 2024a). The overall average result is based on all benchmarks. We also report the averaged multimodal evaluation accuracy for each category. To fairly assess models core vision ability, independent of its instruction-following capabilities, we address challenge: models, especially smaller ones, often embed correct answers within conversational text rather than providing direct response. Our evaluation uses robust parsing strategy to extract the intended answer from this free-form text. This approach ensures reliable assessment of all models, including those without language pre-training but only visual instruction tuning, making our results resilient to variations in response formatting. More details about this parsing strategy are presented in Appendix D. LLM-vision alignment. To quantify the representational convergence and similarity between language and vision modalities, we measure the alignment between the feature spaces of LLMs and pretrained vision models, following the methodology of the Platonic Representation Hypothesis (Huh et al., 2024a). For this analysis, we use image-caption pairs from the Wikipedia-based Image Text (WIT) dataset (Srinivasan et al., 2021). Given an image xi and its caption yi, we compute vision and language kernels, Kvision and Klang, from their respective model representations, fvision and flang: Kvision(i, j) = fvision(xi), fvision(xj) Klang(i, j) = flang(yi), flang(yj) We then assess the alignment between these kernels using the mutual nearest-neighbor (mN ) metric, which calculates the average overlap of the k-nearest neighbor sets (with = 20) for each pair. The final alignment score for given LLM is reported as the average of its mN scores against three strong vision backbones: ViT-Large (Dosovitskiy et al., 2021) (trained on ImageNet-21K (Deng et al., 2009)), DINOv2-Giant (Oquab et al., 2023), and CLIP-Huge (Radford et al., 2021). For all evaluations, we fix the random seed as 42 and use temperature 0 for testing to get consistent results."
        },
        {
            "title": "3 Demystifying LLM Visual Priors: Studies and Findings",
            "content": "This section presents our main results and findings. We first conduct series of controlled experiments to systematically deconstruct the origins of LLM visual priors. These studies investigate the impact of fundamental variables like model and data scale (Section 3.1), data sources (Section 3.2), visual world and reasoning data mixtures (Section 3.3), culminating in the derivation of data mixture for more vision-aware LLMs (Section 3.4). Building upon the rich data generated from these experiments, we then pivot to broader analysis to uncover the internal structure and origin of the learned priors (Section 3.5), and the ultimate source of these abilities within multimodal system (Section 3.6). Each subsection details its specific experimental setup or analytical approach, followed by the results and key findings. 5 Figure 1 Impact of model and data sizes. The plots illustrate the performance of MLLMs, built upon LLMs of five different sizes (340M to 13B parameters), as function of the amount of web-crawl pre-training data (0B to 100B tokens). The general trend shows that performance improves with both increasing model size and data volume, but the scaling behavior differs across task categories."
        },
        {
            "title": "3.1 Impact of model and data sizes.",
            "content": "Finding 1: VQA performance scales positively with model and data size. However, this scaling is not uniform across all visual abilities. We begin our analysis by investigating the fundamental impact of scale. To study how model size and pre-training data volume influence downstream multimodal capabilities, we perform set of experiments to pre-train five LLMs of varying sizes (340M, 1B, 3B, 7B, and 13B parameters). Each model size was trained on eight different scales of data, ranging from 0B to 100B tokens. The training dataset is web-crawl for all experiments. As illustrated in Figure 1, both model sizes and pre-training data sizes generally lead to stronger downstream multimodal performance. This holds true for the overall average VQA. However, closer look at the different VQA categories reveals significant nuances. Performance on General VQA and Knowledge VQA demonstrates similar scaling trend, consistently improving with both model and data size. In sharp contrast, OCR & Chart VQA is far more sensitive to model size than data volume; the performance gap between models is significantly wider. Meanwhile, Vision-Centric VQA also presents unique pattern where the largest models benefit disproportionately from more data, while smaller models plateau much earlier. These divergent scaling patterns across different abilities demonstrate different visual abilities do not scale uniformly, but instead possess different properties that govern how they benefit from increased model and data size."
        },
        {
            "title": "3.2 Impact of pre-training data sources.",
            "content": "Finding 2: Specific categories of language pre-training data can enhance certain visual capabilities in the resulting MLLM; in particular, data related to reasoning and the visual world significantly improve performance on vision-centric tasks. Having characterized the effects of data and model scales, we now transition our analysis to the composition of the data itself. To investigate the role of different pre-training sources, we fix the model size to 3B parameters and the total training data volume to 30B tokens. We then pre-train 16 distinct models, each trained exclusively on data from one of the 16 sources outlined in our pre-training sources (e.g., academia, biology, code, etc.). This setup allows us to attribute performance variations directly to the specific data source used for pre-training. As illustrated in Figure 2, the results reveal significant variance in downstream multimodal performance depending on the pre-training data source. This divergence suggests that different categories of text data contribute to distinct and non-uniform visual priors. Notably, strong performance on Vision-Centric VQA tasks is highly correlated with two types of data: reasoning-centric (e.g., code, mathematics, academia) and corpora rich in visual world descriptions (e.g., arts, food). The top-performing models in Vision-Centric 6 Figure 2 Impact of pre-training data sources. The bar charts illustrate the downstream VQA performance of MLLMs built upon 3B parameter LLM, where each LLM was pre-trained on 30B tokens from single, specific data source. The plots show that performance varies significantly depending on the pre-training sources. VQA, all scoring above 42%, are trained on these specific sources. This finding motivates more granular analysis. In the following section, we will therefore classify data along reasoning and visual axes and study their respective contributions to the visual priors."
        },
        {
            "title": "3.3 Impact of reasoning and visual data categories and proportions.",
            "content": "Finding 3: small amount of data about the visual world is crucial, but its contribution saturates quickly; in contrast, increasing the proportion of reasoning-centric data in the pre-training mix progressively enhances visual abilities, with performance gains observed up to 75% ratio. Our findings in the previous section show that reasoning-centric categories and categories related to the visual world were the most potent drivers of downstream visual capabilities. To dissect this phenomenon further, we focus our next set of experiments specifically on these two high-impact domains. The reasoning-centric data was partitioned into code reasoning, math reasoning, science reasoning, and reasoning combination category, which aggregates the three aforementioned categories. Concurrently, we define four categories for data related to the visual world: visual concept: Text naming visual entities like objects, people, places, and scenes. visual attribute: Descriptions of visual properties such as color, shape, texture, and style. visual relationship: Language detailing spatial arrangements or part-whole connections. visual combination: combination of all three visual categories. We begin by creating data pool of approximately 300B tokens, comprising all sources used in Section 3.2. To partition this corpus into these categories, we employ 32-B LLM (Yang et al., 2025a) to classify the text into finer-grained visual world and reasoning categories. The classification is performed on 1024-token segments, and is treated as multi-label task, allowing each segment to be assigned to multiple categories. Detailed classification settings and results for each data source are presented in the Appendix B. Figure 3 Impact of reasoning-centric and visual data categories and proportions. The plots illustrate how varying the proportion of specific data categories in the pre-training mix affects downstream VQA performance. Left plot (Reasoning-centric data): The plot shows that increasing the share of reasoning-centric data leads to progressive and significant performance gains, with benefits scaling up to 75% proportion before plateauing. This indicates that strong reasoning foundation is critical for enhancing visual abilities. Right plot (Visual world data): In contrast, the right plot, showing data that explicitly describes the visual world demonstrates rapidly diminishing returns. Only small amount of this data is crucial to establish baseline. With this fine-level categorization, we conduct series of controlled mixing experiments to study how varying the proportion (mixing ratio) of these data types affects the final MLLMs performance. For each category, we train five separate models, systematically varying its proportion in the data mixture to 0%, 25%, 50%, 75%, and 100%. The remainder of the data for each run is drawn from proportional mix of all other data within the 300B token pool, ensuring the total training volume is held constant at 30B tokens. As shown in Figure 3, the results reveal critical divergence in how visual world and reasoning data categories contribute to visual priors. The impact of reasoning-centric data is profound and progressive, with performance scaling steadily up to 75% proportion. The contribution from data explicitly describing the visual world saturates quickly; small initial amount seems to be crucial, but further increases yield diminishing returns."
        },
        {
            "title": "3.4 Deriving a data mixture for more vision-aware LLMs.",
            "content": "Finding 4: Maximizing MLLM VQA performance is best achieved by pre-training on data mixture heavily skewed towards reasoning-centric content but with necessary vision world knowledge. The balance point between language and vision proficiency is reached via calibrated data mixture between language-favorable and vision-favorable. Building on these findings, our objective is to derive single, practical data mixture that not only excels on language tasks but also serves as powerful foundation for MLLMs. Our approach proceeds in three stages. First, we determine vision-favorable blend using our 300B token pool to establish target. For the subsequent, more practical stages of our analysis, we then narrow our focus to six primary data categories: web-crawl, encyclopedia, academia, literature, math, and code. Within this practical set of sources, we then identify language-favorable mixture (second), and finally, derive balanced mixture by interpolating between these two optima (third). Since our goal also involves downstream VQA performance, we adapt the most direct approach of using grid search over mixture ratios. Vision-favorable mixture. We first aim to identify data mixture that excels at visual tasks. To do so, we conduct grid search over the proportions of reasoning-centric and visual-world data drawn from our 300B token pool. Specifically, we perform grid search across 24 data blends constructed by sampling from space where the reasoning combination ranges from 50% to 85% and the visual combination ranges from 5% to 30%, following the conclusions drawn from Section 3.3. The comprehensive results of this search are presented in Table 1. From this search, we find that the best-performing models for downstream MLLM tasks emerge from 8 mixture containing approximately 60% reasoning and 15% visual content. Results show powerful visual foundation is not built by simply maximizing exposure to visual descriptions, but by establishing strong reasoning faculty, which is then grounded by smaller amount of visual world knowledge. This experiment is performed on our 300B token pool, provides target ratio for maximizing VQA performance. In the next part, we proceed to more practical experiment at the data source level, guided by this result. Data Ratio Avg VQA Data Ratio Avg VQA reasoning visual reasoning visual 50 60 70 5 10 15 5 10 15 20 25 30 5 10 15 5 10 15 30.7 31.3 31.8 31.9 32.4 32.7 32.5 32.4 31. 31.9 32.3 32.6 31.5 32.4 32.2 55 65 75 5 10 15 5 10 15 20 25 30 5 10 15 5 10 15 30.9 31.7 32.2 32.0 32.2 32.5 32.1 31.9 31. 31.6 31.5 32.4 31.2 31.6 31.8 Table 1 Grid Search for vision-favorable data mixture. Results from pre-training 3B parameter LLM on 30 distinct data blends, each totaling 30B tokens. The table explores how varying the proportions of reasoning-centric and visual-world text affects various capabilities, measured by Avg VQA. The data highlights performance peak for vision tasks at mixture of approximately 60% reasoning and 15% visual content. ries is trained for 50B tokens. Language-favorable mixture. We begin by establishing language-favorable mixture that achieves the best performance on our language task suite. Guided by recent literature (Shukor et al., 2025a; Ge et al., 2024; Ye et al., 2024) and empirical testing over 10 experiments, we identify this as mix of 50% web-crawl, 2.5% encyclopedia, 2.5% academia, 20% literature, 5% math, and 20% code. This blend, designated as mix0 in Table 2, serves as our baseline for strong language proficiency, achieving the highest text accuracy (53.0%) and the best perplexity (13.46) in our experiments. Balanced mixture. To reconcile these two objectives, we seek single, balanced mixture that offers strong performance across both modalities. We achieve this by performing series of interpolation experiments, detailed as mix0 through mix10. We shift the data composition from our language-favorable baseline (mix0) towards an endpoint representing the vision-favorable blend (approximated by mix9 and mix10). To ensure stabilized results, each model in this seRecipe mix0 mix1 mix2 mix3 mix4 mix5 mix6 mix7 mix8 mix9 mix10 u l s web-crawl encyclopedia academic literature math code reasoning visual Data Source Mixture (%) Performance Metrics t-acc (%) ppl () v-acc (%) Overall Rank 50.0 48.3 46.7 45.0 43.3 41.7 40.0 36.5 33.0 29.5 26.0 2.5 3.4 4.3 5.2 6.1 7.1 8.0 7.0 6.5 6.0 5. 2.5 2.9 3.3 3.8 4.2 4.6 5.0 7.5 9.5 11.5 12.5 20.0 17.0 14.0 11.0 8.0 5.0 2.0 2.0 2.0 2.0 2.0 5.0 5.8 6.7 7.5 8.3 9.2 10.0 11.5 12.0 12.5 13.0 20.0 22.5 25.0 27.5 30.0 32.5 35.0 35.5 37.0 38.5 41.0 33.1 36.2 39.4 42.6 45.7 48.9 52.0 55.5 57.2 59.0 61.3 21.7 20.6 19.4 18.2 17.1 16.0 14.8 14.4 14.0 13.6 13. 53.0 52.8 52.6 52.5 52.4 52.6 52.7 52.5 52.7 52.3 52.1 13.46 13.48 13.51 13.56 13.62 13.57 13.52 13.56 13.52 13.71 13.88 32.4 32.4 32.6 32.9 32.7 33.0 33.3 33.1 33.2 33.2 33.4 5 4 8 9 10 6 1 3 2 7 11 Table 2 Deriving data mixture for more vision-aware LLMs. This table details series of 11 data mixtures, from mix0 (language-favorable blend) to mix10 (approximating the vision-favorable blend), all trained on 3B-parameter LLM with 50B tokens. The experiment systematically shifts the data composition towards higher proportion of reasoning-centric content (math, code, academia). The results highlight trade-off, with mix6 emerging as the most balanced mixture, achieving top-ranked overall performance by improving visual capabilities without significant drop in language proficiency. 9 The performance metrics in Table 2 reveal the expected trade-off: as the mixture becomes more reasoningcentric, vision accuracy (v-acc) generally improve, while language proficiency (t-acc and ppl) shows slight decline. Our analysis identifies mix6 as the balanced mixture, achieving the highest overall rank. Mixtures in its vicinity (e.g., mix5, mix7, mix8) also achieve high rankings. This demonstrates that carefully calibrated data mixture can cultivate powerful visual priors without substantially compromising core language abilities."
        },
        {
            "title": "3.5 The structure and origin of learned visual priors.",
            "content": "Finding 5: The learned visual prior is not single entity but decomposes into at least perception prior and reasoning prior with different origins. We now synthesize our previous results to investigate the internal structure of the visual prior. Is it single, uniform ability, or composite of different, separable visual skills? To answer this, we conceptualize the visual prior as collection of distinct abilities, each measured by one of our four VQA categories. Internal structure of visual priors. We aggregate the performance data across all 105 3B models from our previous experiments, encompassing variations in data sources, mixing ratios, and training scales. We then compute the Spearman correlation matrix across the four VQA performance categories to identify which abilities scale together and which diverge. The results in Figure 4 suggest potential internal structure within the visual prior, hinting at separation into at least two distinct types of abilities. We observe moderate correlation (0.37) between General and OCR performance. This connection seems to point towards perception prior, as success in both categories relies heavily on the models perceptual acuitythe ability to accurately process raw visual inputrather than complex, multi-step reasoning. Figure 4 Correlation matrix for VQA performances. The matrix reveals two loosely-coupled skill clusters: one axis for perception (General/OCR) and another for reasoning (Knowledge/Vision-Centric). In contrast, we find another moderate correlation (0.33) between the Knowledge and Vision-Centric tasks. This link appears to emerge because both categories often require abstract inference that goes beyond simple perception. For instance, the Knowledge category demands multi-step reasoning to solve complex scientific or mathematical problems, while Vision-Centric tasks include challenges like visual IQ puzzles, object counting, and correspondence matching, necessitate blend of perception and reasoning, often with heavier reliance on the latter. The correlation matrix also reveals very weak, or even slightly negative, correlations between these two groups (perception-heavy vs. reasoning-heavy) 1. This lack of strong positive correlation raises the possibility that these are largely independent abilities, potentially stemming from loosely-coupled priors within the LLMs representation. Our observations on the separability of these visual priors in MLLMs align with and extend the findings of recent research Chen et al. (2025), which identified similar dissociation through test-time parameter merging. Different origins of priors. The statistical independence of these two priors implies they are cultivated through different mechanisms. As our analysis in Section 3.2 and Section 3.3 demonstrated, the reasoning prior is from reasoning-centric data and can be predictably enhanced by increasing the proportion of reasoning-centric data. In contrast, the origins of the perception prior appear more diffuse. signal comes from our single-source experiments (Section 3.2), where web-crawl data yields the best performance on General and OCR tasks. 1This categorization into perception-heavy and reasoning-heavy tasks is conceptual simplification intended to facilitate our analysis. The boundary between perception and reasoning is not always clear. 10 Figure 5 Universality of the learned visual priors. The plots show the VQA performance of MLLMs built using three distinct vision encoders based on the proportion of reasoning-centric data used in the LLMs pre-training mix. Despite differences in their absolute performance, all three configurations show consistent improvement on reasoning-heavy tasks as the LLMs reasoning pre-training proportion increases, similar to trends observed before, demonstrating the universality of the reasoning prior. However, web-crawl is an extremely general category, and no other, more specific data category consistently boosts perceptual abilities. This mixed effect suggests the perception prior may be general byproduct of large-scale language modeling, emerging from the sheer diversity of language rather than specific category. To further investigate this emergent prior and characterize its properties more directly, we introduce multi-level existence benchmark (MLE-Bench) designed to assess more pure perception abilities (with less reasoning required) across multiple levels. The detailed study using this benchmark is presented in Section 4.1."
        },
        {
            "title": "3.6 Deconstructing multimodal abilities: vision or language.",
            "content": "Finding 6: Visual reasoning ability is primarily shaped by reasoning prior acquired from language pre-training; perception ability is more dependent on post-training (visual instruction tuning). Here, we conduct further analysis to first verify the universality of learned visual priors and then deconstruct the source of different multimodal abilities, distinguishing between those inherited more from the LLM and those acquired more from the visual instruction tuning. Universality of the learned visual priors. To test the general influence of the visual prior, we apply two more vision encoders (DINOv2-G (Oquab et al., 2023) and MAE-H (He et al., 2022)) other than our default MetaCLIP-B/16. We pair these with LLMs pre-trained on varying proportions of our reasoning combination data category, from 0% to 100%. As illustrated in Figure 5, the results reveal dual-faceted pattern. Firstly, they confirm the universality of the reasoning prior. For reasoning-heavy tasks, all three vision encoder configurations exhibit nearly identical, strong upward trend in performance as the proportion of reasoning data in the LLMs pre-training increases. This demonstrates that the visual reasoning prior cultivated in the LLM is foundational, modality-agnostic prior that benefits the multimodal system regardless of the specific vision encoder used. In contrast, the perception prior lacks this universality. The performance trends for perception-oriented tasks are more inconsistent across the different vision encoders. Instead of following unified pattern, the performance curves for different vision encoders vary from one another. This suggests that perceptual abilities are more sensitive to the specific characteristics of the vision encoder (Liang et al., 2025; Tong et al., 2024c). Source of abilities, from visual priors or visual instruction tuning. Second, we conduct targeted studies to determine whether key skillsnamely, perception and reasoningoriginate primarily from the LLMs visual priors or the subsequent visual instruction tuning stage. We use an MLLM to classify our Cambrian-7M dataset that contains 5M text-image pairs into these two categories, resulting in 1.8M perception and 0.6M reasoning data, and the remaining 2.6M data as others. Further classification details are provided in the Appendix C. 11 Figure 6 Step-wise effects of perception and reasoning visual-instruction tuning data removal. The chart shows the remaining performance (%) on Avg VQA and per-category VQA (x-axis) relative to 100% baseline. The bars show performance after ablating perception or reasoning instruction data in stages (removing 50% and then 100% of the data). Removing perception-tuning data produces the largest performance drop on OCR & Chart VQA and General VQA (showing perceptions stronger dependence on supervised vision-side tuning), while removing reasoning-tuning data yields only small performance drops on perception tasks and modest drops on Vision-Centric and Knowledge VQA. We partition our instruction-tuning data into perception, reasoning, and other categories and trained five tuning configurations that ablate perception and reasoning data in stages (100% 50% 0%) while leaving other data unchanged. Our model with full perception and reasoning data achieves baseline performance of 37.98% on General VQA, 25.75% on Knowledge VQA, 17.74% on OCR & Chart VQA, and 43.48% on VisionCentric VQA. The results, presented in Figure 6, show two observations: (1) reducing perception-targeted tuning produces the largest performance drops on perception-heavy benchmarks (OCR & Chart and General) and modest drops on reasoning tasks (Vision-Centric and Knowledge); (2) removing reasoning-targeted tuning causes only small incremental drops on perception tasks and modest drops on reasoning tasks. Together, results in this section show two mechanisms. First, the LLM encodes robust, transferable visual reasoning prior primarily via language pre-training; this prior benefits reasoning-centered VQA across different vision encoders. Second, perception performance depends more on vision-encoder characteristics and on subsequent supervised visual instruction tuning: perception performance gains require more encoderand vision-supervision-specific interventions."
        },
        {
            "title": "4 Discussion and Hypotheses",
            "content": "This section transitions from empirical findings to more speculative exploration of the underlying mechanisms of visual priors. The following subsections present three key hypotheses about the structure of the perception prior, the universal nature of reasoning, and the role of data structure in cross-modal alignment. These hypotheses are not presented as definitive conclusions but as frameworks for interpreting the results and for future research. 12 Figure 7 Performance of MLLMs on the Multi-Level Existence Bench (MLE-Bench). The left plot shows the overall accuracy for models pre-trained on 16 different single-source data types. Other plots detail performance on objects of varying relative sizes, from small (0-30% of image pixels) to medium (30-60%) to large (60-100%). The results demonstrate that pre-training on the broad and diverse web-crawl corpus is most effective in gaining perception prior, with its advantage being particularly pronounced for perceiving smaller objects."
        },
        {
            "title": "4.1 Is the perception prior multi-level? An evaluation using the MLE-Bench",
            "content": "Hypothesis 1: The perception prior derived from diverse data exhibits scale-dependency, with its benefits being most pronounced for the perception of small and medium-sized objects. Our previous analyses show that the perception prior is diffuse in origin, emerging most strongly from diverse data. This leads to question about its internal structure: is this prior uniform ability, or does it possess finer-grained characteristics? To study this question, we introduce the Multi-Level Existence Bench (MLE-Bench), benchmark designed to probe perception with greater precision. MLE-Bench consists of 4-choice questions about the existence of objects or scenes within an image. We categorize questions based on the target objects relative size, measured by the percentage of pixels it occupies. In total, MLE-Bench comprises 1,861 images, with distribution of 732 questions for small objects (0-30%), 698 for medium objects (30-60%), and 431 for large objects (60-100%). This structure allows us to deconstruct \"perception\" into distinct, scale-dependent components. Further details on the benchmarks construction are presented in Appendix E. We also present evaluation of common MLLMs on MLE-Bench, with detailed results and analysis available in Appendix F. We evaluate our 16 single-source pre-trained models (from Section 3.2) on MLE-Bench, with the results presented in Figure 7. The 3B LLM model trained on web-crawl remains the top performer overall, confirming that data diversity is key for perception prior. Its advantage is most pronounced for small-to-medium objects (0-60% pixel range), where it establishes clear lead over models trained on other data sources. In contrast, for large objects that dominate the visual scene, this performance gap diminishes significantly. These results indicate that the perception prior is indeed scale-dependent. possible explanation is that diverse, unstructured text like web-crawl contains vast vocabulary describing wide array of entities, including smaller, often overlooked details within larger scene. This textual richness forces the model to learn representations sensitive to fine-grained visual concepts, capability less critical when identifying large, obvious objects. This finding refines our understanding of the perception prior, revealing that it is not uniform faculty."
        },
        {
            "title": "4.2 Is reasoning a universal, cross-modal skill already acquired during pre-training?",
            "content": "Hypothesis 2: The reasoning capabilities an LLM acquires from text are fundamentally modalityagnostic. Language reasoning skills can be directly transferred to solve visual problems. Our findings suggest hypothesis: the reasoning capabilities an LLM acquires from text are not bound to the linguistic domain. We posit that by pre-training on reasoning-centric data, model learns abstract, generalizable principles of logic, structure, and compositionality. This foundation is largely modality-agnostic, allowing the model to apply this faculty to other domains, including vision, since the reasoning process likely 13 Figure 8 Qualitative impact of reasoning-centric data on visual reasoning. The figure displays the answers from five modelspre-trained with 0% to 100% code reasoning datato visual question requiring the application of specific rule. Answers show clear improvement in reasoning quality: the model with 0% code reasoning provides simplistic justification, while the models with 75% and 100% code reasoning produce more detailed reasoning that correctly applies the definition from the prompt. occurs within the language domain. To verify this, we propose an experiment that directly probes the quality of the models visual reasoning processes. For reasoning-focused VQA tasks (Knowledge and Vision-Centric), we switch the evaluation from prompting for direct answer to answering with detailed explanations. This will require each model to produce detailed explanation of its reasoning. We then use separate LLM (Yang et al., 2025a) as judge to evaluate the quality of these reasoning traces based on clear rubric, assessing criteria such as: (1) Logical Soundness: The percentage of reasoning traces that are coherent and reasonable; and (2) Reasoning Depth: The average length of the reasoning trace measured by text count taken to reach the conclusion. Figure 9 Qualitative impact of reasoning-centric data on visual reasoning tasks. The plot shows how varying the proportion of different reasoningcentric data categories in the pre-training mix impacts metrics of visual reasoning quality. Results indicate that more reasoning data leads to more coherent and detailed visual reasoning. The results, presented in Figure 9, strongly support our hypothesis that the reasoning capabilities an LLM acquires from text are transferable to vision. We observe clear trend: as the proportion of reasoning-centric data increases, the models generate visual reasoning that is both more logically sound and significantly longer. For instance, increasing the proportion of code reasoning data from 0% to 100% boosts Logical Soundness from 4.52% to 9.52% and more than sextuples the Reasoning Depth from 8.31 to 53.25. This demonstrates that the model is applying general, abstract reasoning framework, learned from text, to solve visual problems. The particularly dramatic 14 increase in Reasoning Depth for code-trained models may also reflect stylistic transfer; pre-training on code reasoning, which is often structured in long, logically coherent sequences, likely predisposes the model to generate longer, more structured step-by-step explanations. Figure 8 provides qualitative example of this phenomenon. It showcases how models trained with more code reasoning data produce increasingly sophisticated and reasonable reasoning for visual task. While the model with 0% code reasoning offers simplistic justification, the model trained on 100% code reasoning provides detailed, step-by-step explanation that correctly applies the abstract rule given in the prompt. This demonstrates that the model is applying general, abstract reasoning framework, learned from text, to solve visual problems. Our conclusions here also reflect the results shown in very recent studies that reasoning abilities can transfer between languages at test time (Yong et al., 2025), and that post-training such as language reinforcement learning can enhance or transfer to multi-modal reasoning (Rastogi et al., 2025; Wei et al., 2025; Yang et al., 2025b; Liu et al., 2025; Chen et al., 2025). We further demonstrate that reasoning abilities are highly modality-agnostic, to the extent that training solely on code can yield strong multimodal reasoning. Moreover, we show this transferability is not confined to post-training phases, but originates from pre-training itself."
        },
        {
            "title": "4.3 Does language data structure drive representational alignment with vision?",
            "content": "Hypothesis 3: The structural properties of language data can partially drive representational alignment with visual data. An alternative, or perhaps complementary, hypothesis centers on the structural similarities between the data modalities themselves. Data from domains like code and mathematics is inherently highly structured. It is governed by strict syntax, logical dependencies, and hierarchical compositions. Similarly, visual data is far from being random collection of pixels. It is rich with its own structure: spatial relationships between objects, part-whole hierarchies, and the implicit rules of physics and geometry. We thus hypothesize that this shared structural foundation means that representations learned from structured text are intrinsically more similar to, and thus more readily transferable to, the visual domain. To test this hypothesis, we analyze the LLM-vision alignment score across models trained with varying proportions of structured reasoning data. Our analysis, presented in Figure 10, reveals clear but non-monotonic trend. As we increase the proportion of structured reasoning data, the alignment score generally improves, suggesting that learning from abstract structure fosters more congruent latent space. However, this trend peaks at 75% ratio before declining at 100%. This might be due to model trained purely on reasoning data learning abstract structure but lacking the necessary vocabulary from other text types to effectively map it onto diverse visual concepts, thus hindering the final alignment. While the relationship is not linear, this analysis provides some signals for our structural similarity hypothesis. It remains compelling direction for future research to untangle the precise interplay between abstract structure and semantic grounding in forming cross-modal representations. Figure 10 Qualitative impact of reasoning-centric data on representation alignment. The plot show how varying the proportion of different reasoning-centric data categories in the pretraining mix impacts metrics of cross-modal alignment. Results reveal that with the LLMvision alignment score showing generally positive but non-monotonic trend."
        },
        {
            "title": "5 Scaling Up and Training a Vision-Aware LLM",
            "content": "Building on the principles identified in the controlled, smaller-scale studies, this section details the process of scaling up the approach to validate the findings."
        },
        {
            "title": "5.1 Settings and models",
            "content": "Building upon our findings, we scale up our approach to validate our findings and develop vision-aware LLM on larger-scale. The goal is to test whether the principles identified in our controlled, smaller-scale studies hold true when applied to larger training runs. To this end, we pre-train two 7B parameter LLMs, each on 1T tokens, based on the two data mixtures identified previously: Language-favorable model: Following the mix0 mixture, which is the best-performing blend for pure language tasks. Balanced model: Based on the mix6 recipe, our proposed balanced mixture is designed to deliberately cultivate strong visual priors without compromising language proficiency. We conduct the pre-training for each model on 128 A100 GPUs for approximately 32 days. We process approximately 4.2M tokens per step and the model is trained for 250000 steps. Following pre-training, we adapt both 7B LLMs into MLLMs. For this stage, we utilize the complete Cambrian data suites. Specifically, we use the full 2.5M image-caption dataset for the visual alignment stage, followed by visual supervised fine-tuning on the full 7M vision-language instruction dataset."
        },
        {
            "title": "Vision",
            "content": "Language-Favorable Balanced ppl 8.72 7.49 avg acc General Knowledge OCR&Chart QA Vision-Centric Overall 37.32 0.647 38.64 0.655 21.49 23.63 46.92 49.59 46.31 46. 28.35 29.02 Table 3 Performance comparisons of the Lang-Favorable and Balanced models across both language and vision-language benchmarks. The table summarizes key language metrics (perplexity and accuracy) and provides average scores for suite of vision tasks, categorized as General, Knowledge, OCR & Chart QA, and Vision-Centric. The results demonstrate that the Balanced model, pre-trained with our vision-aware data mixture, exhibits competitive language proficiency while consistently outperforming the Lang-Favorable model on all visual tasks."
        },
        {
            "title": "5.2 Results",
            "content": "As shown in Table 3, the Balanced model, pre-trained with balanced recipe, exhibits competitive language proficiency. Notably, it achieves lower (better) average perplexity of 7.49 compared to the Language-favorable models 8.72, while also maintaining slightly higher average accuracy (0.655 vs. 0.647). An interesting dynamic observed during pre-training was that the Balanced models language performance initially lagged behind the Language-favorable model, beginning to surpass it after approximately 600B tokens. This may suggest that when the pre-training token volume is sufficiently large, the benefits from reasoning-related tokens can be more effectively unleashed when grounded in substantial amount of world knowledge, ultimately resulting in strong performance also on the language side. On VQA benchmarks detailed, the Balanced model consistently outperforms the Language-Favorable model in most of the benchmarks, achieving higher overall VQA average (38.64 vs. 37.32). This confirms that the deliberate pre-training on data mixture rich in reasoning and visual world text successfully imbues the LLM with stronger visual priors in larger scale."
        },
        {
            "title": "6.1 From LLMs to MLLMs.",
            "content": "With the rapid development of LLMs (Radford et al., 2021; Google, 2023; Touvron et al., 2023), direction of work extends LLMs to Multimodal LLMs. Pioneering works like Flamingo (Alayrac et al., 2022) and BLIP-2 (Li et al., 2023) connected pre-trained vision encoders to LLMs using connectors like cross-attention modules. Later models such as LLaVA (Liu et al., 2023a) demonstrate that even with projection layer, LLMs can be extended MLLM with visual instruction tuning. This adapter-style architecture has been widely explored in numerous subsequent works (Liu et al., 2024a; Tong et al., 2024a; Laurençon et al., 2024; Liu et al., 2024b; AI@Meta, 2024; Bai et al., 2025b; Zhu et al., 2025; Team et al., 2025; Lin et al., 2024). The success of visual instruction tuning has enabled open-source multimodal models to achieve performance even comparable to proprietary counterparts (Google, 2023; xAI, 2024; OpenAI, 2024). This success underscores that multimodal capabilities in adapted LLMs largely emerge through instruction tuning, effectively unlocking knowledge already embedded in pretrained language models (Zhou et al., 2024). Furthermore, recent studies (Tong et al., 2024a; Laurençon et al., 2024) highlight that improvements in the underlying language model remain the most impactful means to improve multimodal performance. Inspired by these insights, our work investigates the visual priors and inherent multimodal potential embedded within pretrained LLMs. Though there are different ways of connecting vision to LLMs like the use of discrete tokenization (Wang et al., 2024; Deng et al., 2025; Team, 2024), we focus on adapter-style architectures, which are most widely used and permit clean analysis of visual priors from language pre-training."
        },
        {
            "title": "6.2 The role of data in shaping foundation model capabilities.",
            "content": "Pretrained LLMs encode rich latent knowledgeeven across modalitiesdepending heavily on the nature of their training data (Kaplan et al., 2020; Grattafiori et al., 2024; Han et al., 2022; Rae et al., 2021; Penedo et al., 2023; Lu et al., 2022a; Mayilvahanan et al., 2025). This has shifted the focus of research from simply scaling data to understanding the role of data and then strategically curating it to unlock specific, powerful abilities (Allen-Zhu, 2024; Aryabumi et al., 2024; Ye et al., 2024; Shinnick et al., 2025). prominent example, and one highly relevant to our findings, is the strategic inclusion of reasoning-centric data like code. Research has consistently shown that pretraining on mix of text and code does more than just improve coding skills; it significantly enhances models foundational reasoning and ability to understand abstract, structural patterns (Muennighoff et al., 2023; Aryabumi et al., 2024; Ma et al., 2023; Zhang et al., 2025). This suggests that the pre-training data mixture may endows the model with latent, generalizable structures that can be activated for tasks beyond their original domain. This has established central challenge in the field: determining the optimal data mixture to cultivate these desired foundational abilities (Chen et al., 2024; Ma et al., 2023; Xie et al., 2023; Touvron et al., 2023; Grattafiori et al., 2024; Zhang et al., 2024; Held et al., 2025; Bai et al., 2024; Albalak et al., 2023; Shukor et al., 2025a). This has spurred move beyond simple heuristics toward quantitative frameworks which aim to predict models performance based on different data blends, thereby guiding the search for an optimal mixture. However, much of this prior work has focused on optimizing data mixtures for core language proficiency. As we transition from LLMs to MLLMs, critical question emerges: how do these text-only pre-training choices influence the models visual priors and the potential for multimodal capabilities? Our work directly addresses this gap. We extend the investigation of language data compositions impact from the purely linguistic to the visual domain, systematically analyzing how different text sources contribute to the emergent visual priors in LLMs and seeking data mixture to help them \"learn to see\" more effectively from text pre-training."
        },
        {
            "title": "7 Limitations and Future Research Directions",
            "content": "While this work provides systematic analysis of visual priors in LLMs, it is subject to several limitations that open avenues for future research. 17 First, our investigation primarily centers on adapter-style MLLM architectures. While this is prevalent and effective paradigm, our findings may not fully generalize to other approaches, such as those that employ discrete visual tokenization (Team, 2024; Wang et al., 2024; Deng et al., 2025; Wu et al., 2024) or involve end-to-end joint training of vision and language components (Diao et al., 2024; Tao et al., 2025; Diao et al., 2025; Shukor et al., 2025b). In these latter cases, language and vision data are co-trained, making it hard to identify the priors originating solely from language. The dynamics of how visual priors are formed and utilized could differ in these models, which leaves promising future direction. Second, significant area our study does not address is the safety and ethical implications of these learned visual priors. Language corpora are known to contain societal biases, stereotypes, and potentially harmful content (Bengio et al., 2024; Qu et al., 2023). Our analysis focused on capability, but did not investigate whether these text-based priors encode biased visual associations (e.g., linking certain objects or roles to specific genders or races) that could manifest as harmful generation or classification behavior in downstream MLLM. thorough audit of the fairness and safety of these emergent priors is critical next step. Finally, our study is confined to the domain of static images, leaving the exploration of visual priors for dynamic modalities, such as video understanding, as an open question. For example, the temporal knowledge important for video understanding might be learned more from story-related data like literature. Investigating how different textual sources contribute to priors for temporal reasoning, action recognition, and causality in video is rich area for future work."
        },
        {
            "title": "8 Conclusion",
            "content": "This work has undertaken systematic deconstruction of the visual priors that LLMs acquire from text-only pre-training. Through series of controlled experiments manipulating data composition, we moved beyond observing the phenomenon of vision priors to interrogating its fundamental drivers. Our investigation provides data-centric roadmap for developing multimodal systems, shifting the paradigm from serendipitous emergence to the deliberate cultivation of visual capabilities. By showing that core reasoning abilities are transferable, modality-agnostic foundation, our work offers more empirical support for the idea that models can learn unified representation of the world from even single modality. Looking forward, we hope this research encourages paradigm where LLM development is more considerate of vision and multimodality, prompting the cultivation of visual priors from the earliest stages of pretraining. We also hope it inspires deeper investigation into the fundamental correlations between cross-modal representations, contributing to more unified understanding of how knowledge is structured across modalities."
        },
        {
            "title": "9 Acknowledgment",
            "content": "Junlin Han is supported by Meta. We would like to thank Constantin Venhoff, Songlin Yang, Shusheng Yang, Runjia Li, Zhongrui Gui, Yiming Chen, Amir Bar, Yu Zhang, Jun Chen, Chloe Bi, Frank Zhang, Jacob Xu, Todor Mihaylov, Minghao Chen, Jianyuan Wang, Andrea Vedaldi, Saining Xie, Ce Liu, Piotr Dollar, and Natalia Neverova for the helpful discussions and support."
        },
        {
            "title": "A Broader Impact",
            "content": "Our research provides systematic analysis of how prior visual capabilities emerge in LLMs from language-only pre-training, shifting the paradigm from accidental discovery to deliberate cultivation. While our work focuses on capability, the textual data used for pre-training contains societal biases. significant risk is that these models could learn and reinforce harmful visual stereotypes, which could then manifest in downstream multimodal systems. Nevertheless, our findings primarily help researchers and developers understand the nature and origins of these visual priors. We demonstrate that these priors are not single, uniform block but are composed of separable perception and reasoning components, each cultivated by different types of text. This deeper understanding provides clear, actionable path for more efficiently cultivating these abilities. Instead of relying on serendipity scaling, teams can now strategically curate their text-only pre-training data to deliberately build stronger foundation for vision tasks before multimodal training even begins. This targeted approach not only improves the final models multimodal performance but also reduces the computational resources required, offering more sustainable methodology for creating the next generation of vision-language models. Visual World and Reasoning-centric Language Data Classification This section provides the detailed classification setting and results for the visual world and reasoning-centric pre-training data sources discussed in the main paper. We use 32B dense LLM (Yang et al., 2025a) to perform multi-label classification on 1024-token segments from each data source. Below is the full prompt provided to the LLM for the visual world and reasoning-centri data classification task. The prompt instructs the model to perform multi-label classification on text segments, assigning one or more predefined categories that describe the content. Prompt for LLM-based Visual World and Reasoning-centric Data Classification Analyze the provided text paragraph. Classify its content by identifying the primary concepts and domains using only the categories listed below. Select categories that represent the texts significant content. visual concept: Language for naming visual entities (e.g., objects, people, places, actions, scenes). visual attribute: Language describing visual properties (e.g., color, size, shape, texture, style). visual relationship: Language describing spatial or part-whole relations between entities. code reasoning: Content centered on algorithmic problem-solving, logical coding implementation, and software engineering challenges. math reasoning: Content focused on logical math reasoning, proof construction, and the application of mathematical principles to solve problems. science reasoning: Focuses on scientific reasoning, including hypothesis testing, data analysis, and modeling of complex systems. If none of the above categories apply, output None. The percentages in Table 4 represent the proportion of text segments within each data source that were assigned given label. 19 Reasoning Categories (%) Visual Categories (%) Data sources web-crawl encyclopedia academia literature math code code reasoning 3.5 0.5 21.0 0.3 31.1 96.7 math reasoning 3.6 2.3 68.2 1.3 81.7 13.3 science reasoning 8.6 3.0 74.4 8.9 83.2 6.8 reasoning combination 10.0 3.4 83.3 9.1 92.9 97. visual concept 27.7 12.2 5.2 33.4 7.8 3.3 visual attribute 14.1 2.9 0.7 8.4 2.8 2.0 visual relationship 13.5 6.0 5.1 27.9 6.6 2.4 visual combination 26.9 12.4 5.3 33.5 8.0 3.7 Table 4 Conceptual categories of key pre-training data corpus (%). The table shows the percentage of text segments from each data corpus classified into one of the conceptual categories."
        },
        {
            "title": "C Visual Instruction Tuning Data Classification",
            "content": "We use 7B VLM (Yang et al., 2025a) to perform multi-label classification on the Cambrian-7M data. We prompt the model to classify visual instruction tuning data into three categories: perception-oriented, reasoning-oriented and neither. Prompt for VLM-based Perception and Reasoning-centric Data Classification Please classify the following into three categories: OTHERS, PERCEPTION, or REASONING. OTHERS covers most standard questions including visual analysis, general knowledge, and basic inquiries. PERCEPTION is for questions that require visual speculation or interpretation. REASONING is for complex analytical tasks that require advanced theoretical frameworks or sophisticated analysis. When classifying, consider what type of cognitive process would be most relevant to answering the question effectively."
        },
        {
            "title": "D Robust Parsing for VQA Evaluations",
            "content": "A significant challenge in the automated evaluation of VQA is that models often generate conversational or free-form text instead of single-letter answer. naive parsing strategy that only checks for an exact match to the ground-truth letter (e.g., \"B\") would unfairly penalize models that provide correct but differently formatted response. To illustrate, consider simple VQA task: Question: What is the primary object in the image? Options: (A) bicycle, (B) car, (C) tree Ground Truth: model could correctly answer in multiple ways, such as The answer is (B), car, or The image shows car. To capture all these valid responses, our evaluation protocol employs robust, hierarchical parsing strategy. The logic is executed as sequence of prioritized steps, stopping as soon as valid answer is found: 1. Explicit letter extraction: The parser first searches for high-confidence patterns that directly indicate the chosen option letter. It uses regular expressions to find formats like: The correct answer is (B) Answer: Outputs starting or ending with (B) An output that is simply or B. Figure 11 MLE Benchmark Examples. The figure provides examples from the MLE-Bench, illustrating how the dataset is partitioned based on the ground-truth object size from reference segmentation maps. For instance, in the 0-30 split, the target object (a fireplace) constitutes small fraction of the image. In contrast, the 60-90 split features correct object (grass) that covers substantial portion of the image. 2. Exact option text matching: If the first step fails, the parser extracts the text associated with each option from the prompt (e.g., \"A bicycle\", \"A car\", \"A tree\"). It then checks if the models generated text is an exact, case-insensitive match for any of these option strings. Example caught: model output of car would be correctly mapped to option B. 3. Substring matching: As final fallback, the parser checks if the text of any option appears as substring within the models generated output. This handles more verbose, conversational answers. Example caught: model output of The image features car driving down the street would be correctly mapped to option because \"a car\" is present. To prevent ambiguity (e.g., if one option was \"car\" and another was \"race car\"), this step returns the longest matching option text found in the response. This multi-tiered parsing strategy ensures comprehensive and fair evaluation across all models, regardless of their verbosity or adherence to specific formatting instructions. It allows us to more accurately measure the models underlying visual capabilities rather than its ability to follow formatting rules. Multi-Level Existence Benchmark Construction This section describes how we constructed our benchmark using publicly available SA-1B and ADE20K datasets. We selected images with ground-truth segmentation masks and calculated the proportion of the image area each object occupied. Based on this, we created three splits: 030 for small objects, 3060 for medium, and 60100 for large, dominant objects. For each image, we created multiple-choice query to test object existence, sampling distractors from the dataset vocabulary and filtering them to exclude objects present in the ground truth. As this process is open-vocabulary, we use an LLM to filter out distractors that correspond to objects already present in the image but under different names. This ensures the distractors remain plausible but incorrect, providing granular evaluation of models ability to identify objects across wide range of sizes."
        },
        {
            "title": "Model",
            "content": "030 3060 60100 Overall gpt-5 gpt-4o-mini gemini-2.5-flash claude-opus-4-20250514 73.63 68.72 58.88 61.20 90.69 85.10 80.66 76. 86.31 87.94 80.74 59.86 82.97 79.32 72.11 66.58 Table 5 Model performance on the MLE-Bench. Results are reported in three splits based on object size in percentage (0-30, 30-60, 60-100), along with weighted overall accuracy, evaluating the ability of different models to identify objects of varying sizes. Multi-Level Existence Benchmark Results serves as crucial tool for pushing models toward more universal visual understanding. This section presents the performance of top models on our Multi-Level Existence (MLE) benchmark, which evaluates their ability to identify objects of varying sizes. The results, detailed in Table 5, reveal distinct performance profiles and highlight that robust perception across all object scales remains challenge. gpt-5 achieves the highest overall performance with weighted accuracy of 82.97%, demonstrating strong capabilities across all categories. It particularly excels at identifying medium-sized objects (90.69%), which form the largest portion of the visual scene, while still maintaining competent, albeit lower, performance on small objects (73.6%). In contrast, other models exhibit more pronounced trade-offs. gemini-2.5-flash struggles significantly with small objects, scoring only 58.88%, which is over 20 percentage points lower than its performance on medium and large objects. This indicates potential weakness in fine-grained perception. Similarly, claude-opus-4-20250514 shows notable drop in performance on large, dominant objects (59.86%) compared to smaller ones. These findings underscore the utility of the MLE benchmark in diagnosing model weaknesses. While some models demonstrate strong general performance, universal recognition across different scales is not guaranteed. The benchmark serves as tool for driving progress toward more comprehensive and reliable visual understanding."
        },
        {
            "title": "G Blind visual instruction tuning",
            "content": "Model Language Vision Lang-Favorable Lang-Favorable (+Blind) Balanced Balanced (+Blind) ppl 8.72 7. avg acc General Knowledge OCR&Chart QA Vision-Centric Overall 37.32 0.647 38.20 38.64 39.56 46.92 48.16 49.59 50.90 46.31 47.01 46.59 47.32 28.35 30.30 29.02 31.25 21.49 20.77 23.63 22.60 0. Table 6 Performance comparisons of the Lang-Favorable and Balanced models across both language and vision-language benchmarks with blind visual instruction tuning trick. The table summarizes key language metrics (perplexity and accuracy) and provides average scores for suite of vision tasks, categorized as General, Knowledge, OCR & Chart QA, and Vision-Centric. It also shows the impact of applying our blind visual instruction tuning trick (+Blind). The blind tuning method provides an additional performance boost for both models. We also introduce \"blind visual instruction tuning\" trick that provides more effective starting point for visual adaptation. This trick involves an initial instruction tuning phase using only the textual data while withholding the corresponding images. This initial \"blind\" stage allows the model to first focus on learning the instruction-following format of the task. Consequently, the subsequent standard tuning phase with images can be more dedicated to learning the core vision capabilities, rather than simultaneously learning how to follow instructions. Furthermore, this process enables the model to effectively leverage its pre-existing language 22 priors to solve VQA questions that may not strictly require visual input, known phenomenon and potential \"shortcut\" on some benchmarks (Tong et al., 2024a). This trick can lead to broad performance improvements in most of the tasks. This trick should not be standard in practice, since when no images are provided, models should identify their absence rather than encouraging more hallucination. This type of hallucination may persist even in frontier models including GPT-5 thinking, Gemini 2.5 Pro, and Claude Opus 4.1. Models will exhibit such hallucinatory behaviors when answering VQA questions without actual visual context given. Some examples are presented in Appendix H. This highlights systemic issue that warrants more future investigation. We leave this as an optional trick and introduce this phenomenon to the community for future investigation. This also shows that \"vision\" can be \"learned\" from the language side through an unconventional, shortcut-based mechanism. We apply this trick to Language-favorable model and Balanced model. The trick yields additional overall performance gains for both models by clear margins, as shown in Table 6. The performance gains are most pronounced in the Knowledge categories, while tasks in OCR & Chart VQA, conversely, suffer performance drop. This pattern suggests that while the blind tuning phase provides more effective initialization for the subsequent visual instruction tuning, it gains more from strengthening the models ability to leverage its internal knowledge and reasoning priors to \"shortcut\" the problem. This is highly beneficial for knowledgeintensive tasks where answers can be inferred or reasoned from textual context and the LLMs pre-existing knowledge."
        },
        {
            "title": "H Hallucinations in Blind VQA",
            "content": "To test for hallucinations, we present two randomly selected questions (with slight rephrasing to increase diversity) from our MLE-Bench to various models without the corresponding images. The examples below are from models also chosen randomly, primarily via LMArena (Chiang et al., 2024). We observe that stronger models do not necessarily guarantee fewer hallucinations in this blind VQA setting. 23 Question 1 Question: Which of the following is visible in the image: Choose from: (A) purple and white striped wall, (B) persons arm, (C) purple triangle, (D) building. Please answer with single phrase. Answers: gemini 2.5 pro: purple and white striped wall gpt-5 thinking: persons arm qwen-max-2025-08-15: Red and white striped wall o4-mini-2025-04-16: persons arm mistral-medium-2508: (D) building gemma 3 27B: persons arm. o4-mini-2025-04-16: persons arm kimi-k2-0711-preview: (B) persons arm qwq-32b: (C) purple triangle gpt-oss-120b: Im sorry, but cant see images. Could you describe whats in the picture? kimi-k2-0711-preview: (B) persons arm mistral-medium-2508: (D) building llama-4-scout-17b-16e-instruct: o3-2025-04-16: building claude-sonnet-4-20250514: dont see any image attached to your message. Could you please share the image youd like me to analyze? phantom-0807-1: (D) building claude-opus-4-1-20250805: (B) persons arm 24 Question 2 Question: What can you see in the image: Choose one from: (A) blue jersey, (B) the street, (C) araff logo, (D) blue and white computer screen. Please answer directly. Answers: mistral-medium-2508: D) blue and white computer screen. qwen3-235b-a22b-instruct-2507: (C) araff logo qwen3-235b-a22b: (A) blue jersey amazon.nova-pro-v1:0: (A) blue jersey phantom-0807-3: Without access to the image, it is impossible to determine the correct answer definitively. However, based on the options provided, D) blue and white computer screen is common element in images and could be plausible choice. mistral-medium-2508: (D) building gpt-oss-20b: Im sorry, but cant see the image. qwen3-30b-a3b-instruct-2507: (D) blue and white computer screen. gpt-4.1-2025-04-14: (D) blue and white computer screen. phantom-0807-1: (C) araff logo claude-sonnet-4-20250514-thinking-32k: dont see any image attached to your message. Could you please share the image youd like me to analyze? Once you upload it, Ill be happy to choose from the options youve provided. gpt-5-chat: (D) blue and white computer screen. mistral-medium-2508: (D) building claude-opus-4-1-20250805: Looking at the image, can see (B) the street. The image shows an urban street scene with buildings, trees, and what appears to be road or pathway. folsom-0811-1: cannot view or analyze images directly. However, if you describe the image or provide more context, can help determine the correct answer. Since must choose one option blindly, Ill note that (D) blue and white computer screen is common object that might align with typical visual descriptions, but this is guess. For accuracy, please describe the image! 25 Figure 12 Qualitative impact of reasoning-centric data on visual spatial reasoning. The figure shows answers from five modelspre-trained with 0% to 100% reasoning combination datato visual question requiring depth perception. The answers demonstrate clear improvement in reasoning quality: the model with 0% reasoning data gives blunt answer, while the model with 100% reasoning data provides detailed explanation correctly applying concepts of foreground and background."
        },
        {
            "title": "I Additional Qualitative Examples",
            "content": "To illustrate the differences between our models, we provide additional chat examples. These include one from reasoning combination (Figure 12) and one from visual combination (Figure 13). Each figure shows five different proportions of pre-training data. 26 Figure 13 Qualitative impact of visual world data on complex visual perception. The figure displays answers from five modelspre-trained with 0% to 100% visual combination datato question requiring an understanding of color constancy. The results show that while the model with 25% visual data provides the correct answer with reasoning relevant to the core visual principle, models trained on more visual data offer incorrect answers and flawed explanations. This suggests that simply increasing descriptive visual text does not necessarily cultivate deeper perceptual understanding."
        },
        {
            "title": "References",
            "content": "AI@Meta. Llama 3 model card. 2024. https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. In NeurIPS, 2022. Alon Albalak, Liangming Pan, Colin Raffel, and William Yang Wang. Efficient online data mixing for language model pre-training. arXiv preprint arXiv:2312.02406, 2023. Zeyuan Allen-Zhu. Icml 2024 tutorial: Physics of language models. Project page: https://physics. allen-zhu. com, 2024. Viraat Aryabumi, Yixuan Su, Raymond Ma, Adrien Morisot, Ivan Zhang, Acyr Locatelli, Marzieh Fadaee, Ahmet Üstün, and Sara Hooker. To code, or not to code? exploring impact of code in pre-training. arXiv preprint arXiv:2408.10914, 2024. Kumar Ashutosh, Yossi Gandelsman, Xinlei Chen, Ishan Misra, and Rohit Girdhar. Llms can see and hear without any training. arXiv preprint arXiv:2501.18096, 2025. Sören Auer, Dante A. C. Barone, Cassiano Bartz, Eduardo G. Cortes, Mohamad Yaser Jaradeh, Oliver Karras, Manolis Koubarakis, Dmitry Mouromtsev, Dmitrii Pliukhin, Daniil Radyush, Ivan Shilin, Markus Stocker, and Eleni Tsalapati. The sciqa scientific question answering benchmark for scholarly knowledge. Scientific Reports, 13(1):7240, May 2023. ISSN 2045-2322. doi: 10.1038/s41598-023-33607-z. Lichen Bai, Zixuan Xiong, Hai Lin, Guangwei Xu, Xiangjin Xie, Ruijie Guo, Zhanhui Kang, Hai-Tao Zheng, and Hong-Gee Kim. Frozen language models are gradient coherence rectifiers in vision transformers. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 18171825, 2025a. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025b. Tianyi Bai, Ling Yang, Zhen Hao Wong, Jiahui Peng, Xinlin Zhuang, Chi Zhang, Lijun Wu, Jiantao Qiu, Wentao Zhang, Binhang Yuan, et al. Multi-agent collaborative data selection for efficient llm pretraining. arXiv preprint arXiv:2410.08102, 2024. Yoshua Bengio, Geoffrey Hinton, Andrew Yao, Dawn Song, Pieter Abbeel, Trevor Darrell, Yuval Noah Harari, Ya-Qin Zhang, Lan Xue, Shai Shalev-Shwartz, et al. Managing extreme ai risks amid rapid progress. Science, 384(6698): 842845, 2024. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 74327439, 2020. Garrick Brazil, Abhinav Kumar, Julian Straub, Nikhila Ravi, Justin Johnson, and Georgia Gkioxari. Omni3d: large benchmark and model for 3d object detection in the wild. In CVPR, 2023. Mayee Chen, Michael Hu, Nicholas Lourie, Kyunghyun Cho, and Christopher Ré. Aioli: unified optimization framework for language model data mixing. arXiv preprint arXiv:2411.05735, 2024. Shiqi Chen, Jinghan Zhang, Tongyao Zhu, Wei Liu, Siyang Gao, Miao Xiong, Manling Li, and Junxian He. Bring reason to vision: Understanding perception and reasoning through model merging. arXiv preprint arXiv:2505.05464, 2025. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph Gonzalez, et al. Chatbot arena: An open platform for evaluating llms by human preference. arXiv preprint arXiv:2403.04132, 2024. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. 28 Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In CVPR, 2009. Haiwen Diao, Yufeng Cui, Xiaotong Li, Yueze Wang, Huchuan Lu, and Xinlong Wang. Unveiling encoder-free vision-language models. arXiv preprint arXiv:2406.11832, 2024. Haiwen Diao, Xiaotong Li, Yufeng Cui, Yueze Wang, Haoge Deng, Ting Pan, Wenxuan Wang, Huchuan Lu, and Xinlong Wang. Evev2: Improved baselines for encoder-free vision-language models. arXiv preprint arXiv:2502.06788, 2025. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. Junhao Du, Chuqin Zhou, Ning Cao, Gang Chen, Yunuo Chen, Zhengxue Cheng, Li Song, Guo Lu, and Wenjun Zhang. Large language model for lossless image compression with visual prompts. arXiv preprint arXiv:2502.16163, 2025. David Fan, Shengbang Tong, Jiachen Zhu, Koustuv Sinha, Zhuang Liu, Xinlei Chen, Michael Rabbat, Nicolas Ballas, Yann LeCun, Amir Bar, et al. Scaling language-free visual representation learning. arXiv preprint arXiv:2504.01017, 2025. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. corr abs/2306.13394 (2023), 2023. Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. arXiv preprint arXiv:2404.12390, 2024. Ce Ge, Zhijian Ma, Daoyuan Chen, Yaliang Li, and Bolin Ding. Data mixing made efficient: bivariate scaling law for language model pretraining. arXiv e-prints, pages arXiv2405, 2024. Jiaxin Ge, Zora Zhiruo Wang, Xuhui Zhou, Yi-Hao Peng, Sanjay Subramanian, Qinyue Tan, Maarten Sap, Alane Suhr, Daniel Fried, Graham Neubig, et al. Autopresent: Designing structured visuals from scratch. arXiv preprint arXiv:2501.00912, 2025. Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying Shan. Planting seed of vision in large language model. arXiv preprint arXiv:2307.08041, 2023. Google. Gemini, 2023. https://blog.google/technology/ai/google-gemini-ai/. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. Junlin Han, Huangying Zhan, Jie Hong, Pengfei Fang, Hongdong Li, Lars Petersson, and Ian Reid. What images are more memorable to machines? arXiv preprint arXiv:2211.07625, 2022. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, 2022. William Held, Bhargavi Paranjape, Punit Singh Koura, Mike Lewis, Frank Zhang, and Todor Mihaylov. Optimizing pretraining data mixtures with llm-estimated utility. arXiv preprint arXiv:2501.11747, 2025. Tuomo Hiippala, Malihe Alikhani, Jonas Haverinen, Timo Kalliokoski, Evanfiya Logacheva, Serafina Orekhova, Aino Tuomainen, Matthew Stone, and John Bateman. Ai2d-rst: multimodal corpus of 1000 primary school science diagrams. Language Resources and Evaluation, 55:661688, 2021. Drew A. Hudson and Christopher D. Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In CVPR, 2019. Minyoung Huh, Brian Cheung, Tongzhou Wang, and Phillip Isola. The platonic representation hypothesis. In ICML, 2024a. Minyoung Huh, Brian Cheung, Tongzhou Wang, and Phillip Isola. Position: The platonic representation hypothesis. In Forty-first International Conference on Machine Learning, 2024b. 29 Rishi Jha, Collin Zhang, Vitaly Shmatikov, and John Morris. Harnessing the universal geometry of embeddings. arXiv preprint arXiv:2505.12540, 2025. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Gurucharan Marthi Krishna Kumar, Aman Chadha, Janine Mendola, and Amir Shmuel. Medvisionllama: Leveraging pre-trained large language model layers to enhance medical image segmentation. arXiv preprint arXiv:2410.02458, 2024. Zhixin Lai, Jing Wu, Suiyao Chen, Yucheng Zhou, and Naira Hovakimyan. Residual-based language models are free boosters for biomedical imaging tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 50865096, 2024. Hugo Laurençon, Léo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models? arXiv preprint arXiv:2405.02246, 2024. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, 2023. Qiao Liang, Yanjiang Liu, Weixiang Zhou, Ben He, Yaojie Lu, Hongyu Lin, Jia Zheng, Xianpei Han, Le Sun, and Yingfei Sun. Expanding the boundaries of vision prior knowledge in multi-modal large language models. arXiv preprint arXiv:2503.18034, 2025. Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2668926699, 2024. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023a. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR, 2024a. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024b. https://llava-vl.github.io/blog/2024-01-30-llava-next/. Qianchu Liu, Sheng Zhang, Guanghui Qin, Timothy Ossowski, Yu Gu, Ying Jin, Sid Kiblawi, Sam Preston, Mu Wei, Paul Vozila, et al. X-reasoner: Towards generalizable reasoning across modalities and domains. arXiv preprint arXiv:2505.03981, 2025. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In ECCV, 2024c. Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin Huang, Dezhi Peng, Mingyu Liu, Mingrui Chen, Chunyuan Li, Lianwen Jin, et al. On the hidden mystery of ocr in large multimodal models. arXiv preprint arXiv:2305.07895, 2023b. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. Frozen pretrained transformers as universal computation engines. In Proceedings of the AAAI conference on artificial intelligence, volume 36, pages 76287636, 2022a. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In NeurIPS, 2022b. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In ICLR, 2023. Yingwei Ma, Yue Liu, Yue Yu, Yuanliang Zhang, Yu Jiang, Changjian Wang, and Shanshan Li. At which training stage does code data help llms reasoning? arXiv preprint arXiv:2309.16298, 2023. 30 Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. In ACL, 2022. Prasanna Mayilvahanan, Thaddäus Wiedemer, Sayak Mallick, Matthias Bethge, and Wieland Brendel. Llms on the line: Data determines loss-to-loss scaling laws. arXiv preprint arXiv:2502.12120, 2025. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018. Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi, Aleksandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models. Advances in Neural Information Processing Systems, 36:5035850376, 2023. OpenAI. gpt4o, 2024. https://openai.com/index/hello-gpt-4o/. Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. In TMLR, 2023. Haowen Pan, Yixin Cao, Xiaozhi Wang, Xun Yang, and Meng Wang. Finding and editing multi-modal neurons in pre-trained transformers. arXiv preprint arXiv:2311.07470, 2023. Ziqi Pang, Ziyang Xie, Yunze Man, and Yu-Xiong Wang. Frozen transformers in language models are effective visual encoder layers. ICLR, 2024. Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The lambada dataset: Word prediction requiring broad discourse context. arXiv preprint arXiv:1606.06031, 2016. Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023. Yiting Qu, Xinyue Shen, Xinlei He, Michael Backes, Savvas Zannettou, and Yang Zhang. Unsafe diffusion: On the generation of unsafe images and hateful memes from text-to-image models. In Proceedings of the 2023 ACM SIGSAC conference on computer and communications security, pages 34033417, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. Jack Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021. Abhinav Rastogi, Albert Jiang, Andy Lo, Gabrielle Berrada, Guillaume Lample, Jason Rute, Joep Barmentlo, Karmesh Yadav, Kartik Khandelwal, Khyathi Raghavi Chandu, et al. Magistral. arXiv preprint arXiv:2506.10910, 2025. Siva Reddy, Danqi Chen, and Christopher Manning. Coqa: conversational question answering challenge. Transactions of the Association for Computational Linguistics, 7:249266, 2019. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. Sarah Schwettmann, Neil Chowdhury, Samuel Klein, David Bau, and Antonio Torralba. Multimodal neurons in pretrained text-only transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 28622867, 2023. Pratyusha Sharma, Tamar Rott Shaham, Manel Baradad, Stephanie Fu, Adrian Rodriguez-Munoz, Shivam Duggal, Phillip Isola, and Antonio Torralba. vision check-up for language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1441014419, 2024. 31 Zachary Shinnick, Liangze Jiang, Hemanth Saratchandran, Anton van den Hengel, and Damien Teney. Transformers pretrained on procedural data contain modular structures for algorithmic reasoning. arXiv preprint arXiv:2505.22308, 2025. Mustafa Shukor, Louis Bethune, Dan Busbridge, David Grangier, Enrico Fini, Alaaeldin El-Nouby, and Pierre Ablin. Scaling laws for optimal data mixtures, 2025a. https://arxiv.org/abs/2507.09404. Mustafa Shukor, Enrico Fini, Victor Guilherme Turrisi da Costa, Matthieu Cord, Joshua Susskind, and Alaaeldin El-Nouby. Scaling laws for native multimodal models. arXiv preprint arXiv:2504.07951, 2025b. Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: dataset for image captioning with reading comprehension, 2020. Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc Najork. Wit: Wikipedia-based image text dataset for multimodal multilingual machine learning. In Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval, pages 24432449, 2021. Chunyi Sun, Junlin Han, Weijian Deng, Xinlong Wang, Zishan Qin, and Stephen Gould. 3d-gpt: 3d modeling with large language models. In International Conference on 3D Vision (3DV), 2025. Chenxin Tao, Shiqian Su, Xizhou Zhu, Chenyu Zhang, Zhe Chen, Jiawen Liu, Wenhai Wang, Lewei Lu, Gao Huang, Yu Qiao, et al. Hovle: Unleashing the power of monolithic vision-language models with holistic vision-language embedding. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1455914569, 2025. Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. Advances in Neural Information Processing Systems, 37:8731087356, 2024a. Shengbang Tong, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, and Zhuang Liu. Metamorph: Multimodal understanding and generation via instruction tuning. arXiv preprint arXiv:2412.14164, 2024b. Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In CVPR, 2024c. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. LLaMA 2: Open foundation and fine-tuned chat models. 2023. Gaurav Verma, Minje Choi, Kartik Sharma, Jamelle Watson-Daniels, Sejoon Oh, and Srijan Kumar. Cross-modal projection in multimodal llms doesnt really project visual attributes to textual space. arXiv preprint arXiv:2402.16832, 2024. Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. Yana Wei, Liang Zhao, Jianjian Sun, Kangheng Lin, Jisheng Yin, Jingcheng Hu, Yinmin Zhang, En Yu, Haoran Lv, Zejia Weng, et al. Open vision reasoner: Transferring linguistic cognitive behavior for visual reasoning. arXiv preprint arXiv:2507.05255, 2025. Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv preprint arXiv:2410.13848, 2024. xAI. grok, 2024. https://x.ai/blog/grok-1.5v. Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang. Data selection for language models via importance resampling. Advances in Neural Information Processing Systems, 36:3420134227, 2023. Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. Demystifying clip data. arXiv preprint arXiv:2309.16671, 2023. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a. Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, et al. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615, 2025b. Jiasheng Ye, Peiju Liu, Tianxiang Sun, Jun Zhan, Yunhua Zhou, and Xipeng Qiu. Data mixing laws: Optimizing data mixtures by predicting language modeling performance. arXiv preprint arXiv:2403.16952, 2024. Zheng-Xin Yong, Farid Adilazuarda, Jonibek Mansurov, Ruochen Zhang, Niklas Muennighoff, Carsten Eickhoff, Genta Indra Winata, Julia Kreutzer, Stephen Bach, and Alham Fikri Aji. Crosslingual reasoning through test-time scaling. arXiv preprint arXiv:2505.05408, 2025. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In CVPR, 2024. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. Chi Zhang, Huaping Zhong, Kuan Zhang, Chengliang Chai, Rui Wang, Xinlin Zhuang, Tianyi Bai, Jiantao Qiu, Lei Cao, Ju Fan, et al. Harnessing diversity for important data selection in pretraining large language models. arXiv preprint arXiv:2409.16986, 2024. Xinlu Zhang, Zhiyu Zoey Chen, Xi Ye, Xianjun Yang, Lichang Chen, William Yang Wang, and Linda Ruth Petzold. Unveiling the impact of coding data instruction fine-tuning on large language models reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2594925957, 2025. Boyang Zheng, Jinjin Gu, Shijun Li, and Chao Dong. Lm4lv: frozen large language model for low-level vision tasks. arXiv preprint arXiv:2405.15734, 2024. Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ade20k dataset. IJCV, 2019. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. In NeurIPS, 2024. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025."
        }
    ],
    "affiliations": [
        "Meta Superintelligence Labs",
        "University of Oxford"
    ]
}