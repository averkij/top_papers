{
    "paper_title": "GraLoRA: Granular Low-Rank Adaptation for Parameter-Efficient Fine-Tuning",
    "authors": [
        "Yeonjoon Jung",
        "Daehyun Ahn",
        "Hyungjun Kim",
        "Taesu Kim",
        "Eunhyeok Park"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Low-Rank Adaptation (LoRA) is a popular method for parameter-efficient fine-tuning (PEFT) of generative models, valued for its simplicity and effectiveness. Despite recent enhancements, LoRA still suffers from a fundamental limitation: overfitting when the bottleneck is widened. It performs best at ranks 32-64, yet its accuracy stagnates or declines at higher ranks, still falling short of full fine-tuning (FFT) performance. We identify the root cause as LoRA's structural bottleneck, which introduces gradient entanglement to the unrelated input channels and distorts gradient propagation. To address this, we introduce a novel structure, Granular Low-Rank Adaptation (GraLoRA) that partitions weight matrices into sub-blocks, each with its own low-rank adapter. With negligible computational or storage cost, GraLoRA overcomes LoRA's limitations, effectively increases the representational capacity, and more closely approximates FFT behavior. Experiments on code generation and commonsense reasoning benchmarks show that GraLoRA consistently outperforms LoRA and other baselines, achieving up to +8.5% absolute gain in Pass@1 on HumanEval+. These improvements hold across model sizes and rank settings, making GraLoRA a scalable and robust solution for PEFT. Code, data, and scripts are available at https://github.com/SqueezeBits/GraLoRA.git"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 5 5 3 0 2 . 5 0 5 2 : r GraLoRA: Granular Low-Rank Adaptation for Parameter-Efficient Fine-Tuning Yeonjoon Jung1,2 Daehyun Ahn1 Hyungjun Kim1 Taesu Kim1 Eunhyeok Park2 1SqueezeBits 2POSTECH {yeonjoon.jung, daehyun.ahn, hyungjun.kim, taesu.kim}@squeezebits.com yeonjoon.jung@postech.ac.kr, eh.park@postech.ac.kr"
        },
        {
            "title": "Abstract",
            "content": "Low-Rank Adaptation (LoRA) is popular method for parameter-efficient finetuning (PEFT) of generative models, valued for its simplicity and effectiveness. Despite recent enhancements, LoRA still suffers from fundamental limitation: overfitting when the bottleneck is widened. It performs best at ranks 3264, yet its accuracy stagnates or declines at higher ranks, still falling short of full fine-tuning (FFT) performance. We identify the root cause as LoRAs structural bottleneck, which introduces gradient entanglement to the unrelated input channels and distorts gradient propagation. To address this, we introduce novel structure, Granular Low-Rank Adaptation (GraLoRA) that partitions weight matrices into sub-blocks, each with its own low-rank adapter. With negligible computational or storage cost, GraLoRA overcomes LoRAs limitations, effectively increases the representational capacity, and more closely approximates FFT behavior. Experiments on code generation and commonsense reasoning benchmarks show that GraLoRA consistently outperforms LoRA and other baselines, achieving up to +8.5% absolute gain in Pass@1 on HumanEval+. These improvements hold across model sizes and rank settings, making GraLoRA scalable and robust solution for PEFT. Code, data, and scripts are available at https://github.com/SqueezeBits/GraLoRA.git"
        },
        {
            "title": "Introduction",
            "content": "Task-specific fine-tuning enables wide range of applications and significantly improves the quality and effectiveness of generative models. However, the massive scale of these models poses substantial challenges for practical deployment. To address these limitations, Parameter-Efficient Fine-Tuning (PEFT) methods have emerged as cost-effective alternative [10, 26]. Among them, Low-Rank Adaptation (LoRA) [11] has gained particular attention for its simplicity and effectiveness, introducing trainable low-rank matrices while keeping the pre-trained model weights frozen. Although the imposed rank-r bottleneck may lead to slight performance degradation compared to full fine-tuning (FFT), its efficiency has led to widespread adoption in practice. To maximize the benefits of LoRA, various studies have proposed techniques such as improved initialization [5, 18, 20, 23] and structural refinements [9, 13, 14, 15] to enhance fine-tuning quality. While these efforts have advanced performance, substantial quality gap remains compared to FFT, largely due to the inherent upper bound on the rank. Although using higher rank, within hardware limits, appears to be natural solution, unfortunately, current implementations of LoRA and its variants do not support such flexibility. Simply increasing the rank often leads to degraded accuracy in many scenarios. In this paper, we present theoretical analysis identifying the root cause of the rank limitation in LoRA. Our analysis reveals fundamental issue in LoRAs structure, channel dominance in the gradient, where small subset of outlier channels disproportionately influences the update direction. Preprint. Under review. Figure 1: Illustration of LoRA architecture and GraLoRA architecture. GraLoRA consists of k2 small adapter pairs, where each input and output dimension is times smaller than the original LoRA. This dominance suppresses contributions from other channels, leading to under-utilization of the available rank and degraded performance in tasks that require nuanced or distributed representations. To overcome these expressivity bottlenecks, we propose Granular Low-Rank Adaptation (GraLoRA), novel architectural extension of LoRA. As shown in Figure 1, GraLoRA divides the weight matrix into multiple sub-blocks and applies independent LoRA modules to each, enabling fine-grained updates. This design enhances the models capacity to capture complex, localized, or multi-faceted patterns, effectively mitigating the channel dominance issue and improving performanceespecially at higher ranks. Extensive experiments show that GraLoRA consistently outperforms vanilla LoRA across range of NLP benchmarks, particularly in scenarios with high input heterogeneity or task complexity. These results position GraLoRA as principled and practical advancement in the PEFT landscape."
        },
        {
            "title": "2 Details and Limitations of LoRA",
            "content": "2.1 Introduction to LoRA LoRA is one of the most widely adopted strategies for PEFT. Given pre-trained weight matrix W0 RM , where and represent the input and output channel dimension, respectively, LoRA keeps W0 frozen and introduces trainable low-rank update defined as: = sBA, RN r, RM r, = α . (1) Here, rank and α are user-defined hyperparameters. Then, for given input RN , the output of the LoRA-adapted layer is = W0X + RX RM , where denotes the batch or token dimension. This low-rank decomposition allows the model to adapt using significantly fewer trainable parameters and reduced memory overhead. While FFT updates the entire weight matrix, LoRA only updates the decomposed low-rank matrices and B. Note that we assume = 1 for simplicity, the gradient of the loss with respect to is: From this, the gradients with respect to the LoRA parameters and are given by: R = Y RM L = Y A, A = L . (2) (3) Figure 2: Gradient dynamics of FFT and LoRA in the presence of an outlier input channel. The red channel in input denotes the outlier. While FFT localizes the gradient impact, LoRAs entire gradient update becomes disproportionately influenced by the single outlier. These result in the following update in the fused weight space: R = B + A = Y AA + BB Y . (4) This expression reveals how the structure of LoRA introduces non-trivial interactions between the gradients and the input, particularly through the rank-r matrices. 2.2 Why Does LoRA Suffer from Larger Rank? When fine-tuning with large LoRA rank (e.g., > 64), it is often observed that accuracy degrades compared to using moderate rank. This counterintuitive behavior arises from the distinct gradient dynamics of LoRA, which differ significantly from those of FFT. LoRAs structural design makes its gradients inherently sensitive to the entire input space, as illustrated in Figure 2. In particular, we observe that outlier channels, input channels with abnormally high activations, can disproportionately dominate the gradient signal. In FFT, the effect of such outliers is typically localized, affecting only single column of the weight matrix that directly interacts with the outlier channel. In contrast, LoRAs low-rank constraint causes the entire gradient of the adapter matrix B, denoted L/B, to be influenced by these outliers. This results in distorted weight updates in the fused weight space, where the gradient signal from outlier channels overwhelms the contributions from other inputs. Consequently, LoRA fails to accurately replicate the gradient dynamics of FFT, limiting its ability to match FFT-level performance. We observe that in certain layers, most notably the down-projection matrix of Layer 1 in LLaMA3.18B, input activations exhibit severe channel-wise imbalance (Figure 3 (a)). As shown in Figure 4, these outlier channels disproportionately impact the adapters gradient updates. Figure 3 further illustrates that the gap between LoRA and FFT gradient updates widens as the LoRA rank increases. These findings reveal fundamental misalignment between LoRA updates and the gradient landscape shaped by FFT. The entangled influence of input channels caused by the low-rank projection limits LoRAs ability to selectively learn from salient features, particularly under skewed input statistics. While the negative impact of outliers has been well recognized in the context of quantization-aware training [25] [16], their influence on LoRAs behavior has not been systematically studied until now. 3 (a) Mean input channel values for the down-projection matrices across layers in Figure 3: LLaMA3.18B. pronounced outlier exists in Layer 1, channel 198 and 2427. (b) Gradient deviation between LoRA and FFT increases with rank, showing LoRAs susceptibility to input outliers. (c) GraLoRA gradient results at rank 128. GraLoRA noticeably reduces gradient deviation between FFT. Figure 4: Gradient distribution in Layer 1 down-projection matrix. LoRA gradients show poor alignment with FFT, outlier channel increases the overall gradient scale, while less emphasizing the corresponding outlier channel."
        },
        {
            "title": "3 Method",
            "content": "3.1 GraLoRA: Granular Low-Rank Adaptation Motivated by observation in previous section, we propose GraLoRA, fine-grained and modular extension of LoRA. As illustrated in Figure 1, GraLoRA addresses the limitations of standard LoRA by partitioning the weight matrix into grid of independent blocks, each equipped with its own local low-rank adapter. Here, is hyperparameter that determines the number of splits along the input and output dimensions. When = 1, GraLoRA reduces to the vanilla LoRA formulation. Specifically, the weight update RM is expressed as the concatenation of block-wise updates: RGraLoRA = B1,1A 1,1 ... Bk,1A k, B1,kA 1,k . . . Bk,kA k,k ... , Ai,j k , Bi,j r (5) This block-wise reparameterization provides localized control over each spatial subregion of the parameter space. As detailed in Section 3.4, GraLoRA incurs the same parameter count and computational overhead as standard LoRA when using the same rank. However, it introduces two key advantages; (1) Enhanced Expressivity and (2) Robustness to Input Outliers. By enabling independent adaptation across k2 subspaces, GraLoRA supports more fine-grained and specialized feature learning. In addition, Localized gradient updates ensure that only the adapters associated with 4 Figure 5: Regularized form of GraLoRA as multiplication of sparse two matrices, AGraLoRA and BGraLoRA. the affected input regions receive large gradients, thereby reducing global gradient distortion and preserving inter-channel signal balance. 3.2 Expression Power Analysis While the weight update of GraLoRA was expressed as concatenation of block-wise updates in ( 5), it can also be regularized as the form of multiplication of two matrices as in the vanilla LoRA. The sparse matrix AGraLoRA RN kr can be constructed as Figure 5 (a), where Ai,j for i, {n k} is located in position (i + (j 1) k, j) of AGraLoRA. Other elements are masked out, thus the total number of parameter becomes r. Then, BGraLoRA RN kr is constructed as Figure 5 (b), where matrix Bi,j for i, {n k} is located in position (i, + (i 1) k) of BGraLoRA, Similarly, other composition of the matrix is masked, therefore the total number of parameter becomes r. Then the weight update of GraLoRA can be expressed as = W0 + RGraLoRA = W0 + BGraLoRAA Assuming that all columns of [Bi,1, , Bi,k] are linearly independent, the rank of BGraLoRA becomes (BGraLoRA) = kr. Similarly, if all columns of [A1,j, , Ak,j] are linearly independent, the rank of AGraLoRA is (AGraLoRA) = kr. Applying Sylvesters rank inequality to derive the lower bound and the matrix product theorem for the upper bound, we obtain: GraLoRA. R(BGraLoRA) + R(A GraLoRA) kr R(BGraLoRAA GraLoRA) min(R(BGraLoRA), R(A GraLoRA)) (6) Thus, the effective rank of RGraLoRA becomes kr, which is times higher than that of the vanilla LoRA methodeffectively enhancing the models expressive capacity. The rank analysis of finetuned LoRA and GraLoRA, summarized in Table 4 in Appendix, demonstrates that GraLoRA linearly scales the representational power of the adaptation matrix in practical settings. 3.3 Gradient Dynamics Under Outlier Activation GraLoRA effectively localizes the influence of outlier channels to limited subset of adapter blocks. Because each block processes only specific slice of the input, only the adapter pairs intersecting with the outlier channel are exposed to amplified gradients. In contrast, the remaining k2 adapters maintain gradient magnitudes close to baseline levels. This selective gradient propagation resembles the behavior of FFT, where only weights directly connected to active inputs are significantly updated. GraLoRAs impact on gradient dynamics can be observed by comparing gradient distributions of the down-projection matrix in Layer 1 with standard LoRA. As illustrated in the Figure 3 (c) and Figure 6, GraLoRA reduces the gradient deviation and limits the influence of outlier channels, overcoming the limitations of standard LoRA with larger ranks. 3.4 Tradeoff Analysis As discussed, GraLoRA provides several advantages over standard LoRA. However, these benefits do not come without cost. In this section, we provide deeper analysis on the overhead introduced by GraLoRA. Computation Overhead Analysis: First, we analyze the expected computational cost of LoRA in terms of FLOPs. To take advantage of the low-rank structure, LoRA computes the projection in two sequential steps. The first computes AX RrT , followed by the reconstruction B(AX) 5 Figure 6: Comparison of gradient distributions under outlier activation. In GraLoRA, only the blocks interacting with the outlier exhibit elevated gradients, mitigating global distortion and aligning with FFT behavior. RM . These steps require 2N rT and 2rM FLOPs, respectively, resulting in total complexity of (r(M + )T ) . Similarly, GraLoRA divides the computation into two steps involving k2 adapter blocks. In the first step, the projection computes T for each of the k2 blocks, incurring total cost of 2 T k2 = 2N rT. In the second step, each intermediate output is processed by its corresponding . This step adds another 2 Bi,j, producing Bi,j(A k2 = 2rM T. FLOPs to the total cost. Hence, the overall computational cost of GraLoRA remains (r(M + )T ), maintaining efficiency comparable to vanilla LoRA while significantly enhancing expressive power. detailed analysis of computational overhead is provided in Appendix C. i,jXj) i,jXj k Table 1: Maximal allocated memory during training LLaMA3.18B model with batch size 1. Input length was set to 1024 and memory allocated for weight was removed for direct comparison. Vanilla Backward (GB) Gradient Checkpointing (GB) LoRA GraLoRA (k=2) GraLoRA (k=4) GraLoRA (k=8) 10.0 2.6 10.4 2.6 10.1 2.6 10.2 2.6 Memory Overhead Analysis: As with classical LoRA, GraLoRA can be merged into the original weight matrix at inference time. Therefore, our analysis focuses on the memory overhead incurred during training. Although the number of parameters and FLOPs are identical to those of LoRA, the intermediate latent representation GraLoRAX becomes times larger than the corresponding AX in standard LoRA. This expanded latent space allows for greater information preservation, which can be beneficial. However, it also leads to increased memory consumption during training time. Fortunately, the rank is typically much smaller than the input and output dimensions, thus the additional memory required remains marginaleven for large k, as demonstrated in Table 1. Moreover, by applying recent techniques such as gradient checkpointing, the memory overhead from the expanded latent space can be effectively hidden, making the impact negligible in practice. i,j Selection of While GraLoRA increases the total rank from to kr, each individual block, repk , is constrained to reduced rank of resented as Bi,jA . As result, increasing beyond certain threshold can degrade performance due to limited expressiveness within each block. This effect is especially pronounced when the overall rank is small. Empirically, we observed that maintaining minimum block expressiveness of approximately r/k2 8 yields stable performance across various configurations. Based on this observation, we adopted = 2 for ranks 16 and 32, and = 4 for ranks 64 and 128 in our experiments. Detailed results from the k-sweep can be found in Section 4.4. 3.5 Hybrid GraLoRA On the other hand, for smaller rankstypically rank 16 or belowusing = 2 may still lead to performance degradation or yield only marginal gains. To address this limitation, we introduce 6 Figure 7: Hybrid GraLoRA architecture when GraLoRA = 2. LoRA parameter becomes shared across small GraLoRA adapters in the same row or same column. hybrid approach that combines the strengths of LoRA and GraLoRA. This method retains the fine-grained input handling and increased total rank offered by GraLoRA, while preserving the expressive power of larger block units through LoRA. Since LoRA shares the same parameters across both rows and columns, it can be naturally integrated with GraLoRA in concatenated form, which we refer to as Hybrid GraLoRA (see Figure 7). Empirically, we found that allocating up to 1 2 of the total rank to the LoRA component mitigated the limitations of GraLoRA in low-rank scenarios (γ <= 16), while fully allocating the rank to GraLoRA better performed in high-rank circumstances."
        },
        {
            "title": "4 Experiments",
            "content": "In order to validate the superiority of the proposed idea, we conduct an extensive analysis on largescale dataset with the state-of-the art LLMs. We evaluate GraLoRA across two challenging domains: code generation and commonsense reasoning. Our experiments are designed to assess whether the proposed granular adaptation mechanism improves performance across varying model sizes, LoRA ranks, and tasks that require nuanced reasoning and high representational fidelity. 4.1 Experimental Setup Code Generation. We fine-tuned each model on the Magicoder-Evol-Instruct-110k [24] train dataset, curated and decontaminated subset of WizardCoder [17], comprising high-quality instructionresponse pairs for programming tasks. Evaluation was conducted on the Humaneval+ test dataset following He et al. [9], which samples 50 completions per problem using temperature of 0.2. We report Pass@1, Pass@5, and Pass@10 accuracy following standard protocol via BigCode Evaluation Harness [1]. We have evaluated the last epoch weight across all methods. Commonsense Reasoning We fine-tune each model across 8 commonsense tasks: BoolQ [6], PIQA [4], SIQA [22], HellaSwag [28], WinoGrande [21], ARC-Challenge, ARC-Easy [7], and OpenBookQA [19]. We followed the training pipeline proposed by LLM-Adapters [12], using merged dataset composed of the training sets from all tasks. Evaluation was conducted on individual testing dataset for each task on the last epoch weight. Detailed training parameters can be found in Appendix D. Training Details We conducted experiments on four open-sourced LLMs with different architecture and size: LLaMA3.18B, LLaMA3.170B ( [8]), Qwen-2.5-1.5B, and Qwen-2.5-7B ( [27]). We used pre-trained models rather than instruction-tuned models following the common practice ( [14, 15]). We applied PEFT methods on all linear modules from attention ( Wq, Wk, Wv, Wo )and feed-forward networks ( Wup, Wdown, Wgate). We set the hyper-parameters based on the optimal configurations from Biderman et al. [3] and He et al. [9], employing decoupled LionW optimizer with batch size of 192 and setting LoRA α = 2r. We applied alpaca-chat template for both tasks. Code generation was conducted on 4 A100 80G GPUs and commonsense reasoning task was conducted on 2 H100 80G GPUs for 1.5 8B models. 70B model was conducted on 8 A100 80G GPUs. We compared GraLoRA to three representative PEFT methods: LoRA, MoRA [14] and RaSA [9]. Table 2: Pass@1, Pass@5, and Pass@10 results on LLaMA3.18B using LoRA, MoRA, RaSA, and GraLoRA across different ranks. Best results per group are in bold. * indicates Hybrid GraLoRA. Rank Method Training Time Relative Time Pass@1 Pass@ Pass@10 16 32 64 128 LoRA MoRA RaSA GraLoRA* LoRA MoRA RaSA GraLoRA LoRA MoRA RaSA GraLoRA LoRA MoRA RaSA GraLoRA 6.2h 8.8h 6.7h 6.7h 6.5h 9.1h 6.8h 6.9h 6.7h 9.7h 6.9h 7.2h 7.0h 9.9h 7.6h 7.7h 1.00 1.42 1.08 1.08 1.00 1.40 1.05 1.06 1.00 1.45 1.03 1.07 1.00 1.41 1.09 1.10 65.3% 56.1% 62.2% 53.6% 53.7% 64.4% 58.0% 67.1% 68.0% 58.4% 66.7% 58.3% 67.9% 57.2% 58.9% 67.0% 66.4% 58.1% 66.4% 57.2% 56.6% 65.4% 60.5% 71.2% 64.8% 55.8% 62.3% 52.8% 57.5% 65.5% 64.3% 71.7% 68.1% 64.5% 66.7% 70.1% 69.9% 69.0% 70.5% 69.0% 68.5% 69.2% 67.9% 72.6% 68.6% 65.3% 67.5% 73.7% Table 3: Commonsense reasoning accuracy across models and tasks. All values are percentages; bold indicates the best performance per row. HS means HellaSwag, and WG WinoGrande. Model Method BoolQ PIQA SIQA HS WG ARC-c ARC-e OBQA Avg. Qwen2.51.5B Qwen2.57B LLaMA3.170B 83.4% 78.7% LoRA 82.8% 77.6% MoRA RaSA 83.8% 79.4% GraLoRA 67.2% 84.2% 75.9% 85.7% 73.8% 77.5% 89.9% 84.4% 79.8% 66.5% 84.0% 74.9% 83.6% 73.7% 75.2% 65.9% 82.2% 74.7% 82.6% 73.4% 72.6% 67.5% 83.7% 75.7% 85.3% 72.9% 76.4% 88.1% 86.5% 89.8% 89.6% 85.6% LoRA 85.0% 81.2% MoRA RaSA 90.2% 85.7% GraLoRA 73.4% 89.7% 79.0% 93.0% 84.0% 86.9% 94.5% 90.6% 86.4% 72.3% 88.2% 79.2% 92.9% 84.7% 84.0% 69.9% 85.3% 78.5% 83.7% 81.4% 77.5% 72.0% 88.5% 78.9% 93.6% 81.8% 86.1% 93.6% 88.6% 94.2% LoRA 95.6% 91.3% GraLoRA 83.1% 94.7% 83.6% 97.9% 93.8% 92.3% 97.8% 96.2% 92.4% 81.7% 93.4% 82.2% 97.5% 93.1% 90.2% 96.5% 4.2 Results on Code Generation As shown in Table 2, GraLoRA outperformed LoRA, MoRA, and RaSA across all tested ranks for Pass@1 accuracy. At rank 64, GraLoRA achieved an absolute improvement of +2.4% in Pass@1, +4.8% in Pass@5, and +4.1% in Pass@10 over LoRA. At rank 128, the gains were even more pronounced, with increases of +8.5% in Pass@1, +6.9% in Pass@5, and +5.1% in Pass@10. Notably, while other methods struggled to fully utilize the increasing rank capacityoften reaching performance plateaus at lower ranksGraLoRA maintained consistent upward trajectory, effectively overcoming the limitations of LoRA. Even in low-rank settings (e.g., rank 16), where expressive capacity is typically constrained, the hybrid variant of GraLoRA demonstrated superior performance. These improvements highlight GraLoRAs enhanced capability to preserve diverse gradient signals and resist suppression from dominant outliers. The strong results on the HumanEval+ benchmark further underscore the benefits of fine-grained adaptation in tackling complex, high-precision code generation tasks. 4.3 Results on Commonsense Reasoning As shown in Table 3, GraLoRA outperformed other methods across wide range of models and tasks. Notably, GraLoRA demonstrated superior performance across models of varying scales, achieving 1.1% improvement in average accuracy on both Qwen2.5-1.5B and LLaMA3.1-70B. It also delivered 0.9% gain on the widely used mid-sized model, Qwen2.5-7B. Furthermore, GraLoRA achieved the best results on 20 out of 24 tasks, consistently outperforming alternatives across benchmarks. These results support our analysis in Section 3.3, showing that 8 (a) GraLoRA sweep results and (b) Hybrid GraLoRA Ratio sweep results for Figure 8: LLaMA3.18B on code generation task. Ratio 0 implies default GraLoRA and ratio 1 implies vanilla LoRA in (b). GraLoRAs localized updates enhance alignment with FFT and promote robust generalization in multi-aspect reasoning tasks. 4.4 Ablation Study GraLoRA Sweep We evaluated the impact of varying on code generation accuracy. As shown in Figure 8 (a), = 2 yielded the best performance at rank 32, while = 4 was optimal at rank 128. These results are consistent with the theoretical prediction that smaller is preferable for lower ranks, as reduced sub-block rank can be particularly detrimental when the overall rank is limited. Hybrid GraLoRA Ratio Sweep We assessed performance across different LoRA-to-GraLoRA rank allocation ratios for the Hybrid GraLoRA configuration (Figure 8 (b)). At rank 16, partially allocating the rank to LoRA led optimal accuracy. However, for larger ranks, allocating rank to LoRA resulted in degraded performance. This suggests that Hybrid GraLoRA is advantageous in low-rank regimes, where the sub-block rank of GraLoRA alone may be insufficient. In contrast, under higher-rank settings where GraLoRAs sub-blocks are expressive enough, introducing LoRA components may lead to gradient entanglement, thereby hindering effective learning."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we introduced GraLoRA, novel PEFT method that extends LoRA with granular, blockwise decomposition. Motivated by rigorous analysis of LoRAs gradient behavior, we identified that input outliers can dominate the low-rank update, suppressing meaningful contributions from other input channels and misaligning with the localized gradient propagation observed in FFT. GraLoRA addresses this limitation by dividing the adaptation space into k2 independently trained low-rank adapters, enabling spatially localized and context-aware updates. Our theoretical analysis shows that this design increases expressivity by factor of k, without additional parameters or computational cost. Moreover, under outlier activations, GraLoRA effectively mitigates the global gradient distortion seen in vanilla LoRA and better preserves inter-channel balance. Empirically, GraLoRA consistently outperforms standard LoRA and strong baselines such as RaSA across diverse tasks and model scales. On the code generation benchmark HumanEval+, it achieves up to +8.5% absolute gain in Pass1. In commonsense reasoning, GraLoRA delivers improvements across all tasks, with especially strong results on multi-hop and structured reasoning benchmarks. Future Work. While GraLoRA improves gradient locality and expressive power, its current design assumes uniform partitioning. Future extensions may explore adaptive or learned partitioning schemes, sparsity-aware block activation, or task-driven dynamic rank allocation. Additionally, applying GraLoRA to vision transformers, multimodal architectures, or continual learning setups may further highlight its potential for robust and efficient model adaptation. Overall, GraLoRA represents principled and practical step forward in the design of PEFT methods, bridging the gap between global low-rank reparameterization and local, fine-grained adaptation."
        },
        {
            "title": "References",
            "content": "[1] Loubna Ben Allal, Niklas Muennighoff, Logesh Kumar Umapathi, Ben Lipkin, and Leandro von Werra. framework for the evaluation of code generation models. https://github. com/bigcode-project/bigcode-evaluation-harness, 2022. [2] Dan Biderman, Jacob Portes, Jose Javier Gonzalez Ortiz, Mansheej Paul, Philip Greengard, Connor Jennings, Daniel King, Sam Havens, Vitaliy Chiley, Jonathan Frankle, Cody Blakeney, and John P. Cunningham. LoRA learns less and forgets less. Transactions on Machine Learning Research, 2024. [3] Dan Biderman, Jacob Portes, Jose Javier Gonzalez Ortiz, Mansheej Paul, Philip Greengard, Connor Jennings, Daniel King, Sam Havens, Vitaliy Chiley, Jonathan Frankle, et al. Lora learns less and forgets less. arXiv preprint arXiv:2405.09673, 2024. [4] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 74327439, 2020. [5] Kerim Büyükakyüz. Olora: Orthonormal low-rank adaptation of large language models. arXiv preprint arXiv:2406.01775, 2024. [6] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 29242936, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. [7] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [8] Aaron Grattafiori et al. The llama 3 herd of models, 2024. [9] Zhiwei He, Zhaopeng Tu, Xing Wang, Xingyu Chen, Zhijie Wang, Jiahao Xu, Tian Liang, Wenxiang Jiao, Zhuosheng Zhang, and Rui Wang. RaSA: Rank-sharing low-rank adaptation. In Proceedings of the 2025 International Conference on Learning Representations (ICLR), 2025. [10] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp, 2019. [11] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. [12] Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong Bing, Xing Xu, Soujanya Poria, and Roy Lee. Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 52545276, 2023. [13] Qiushi Huang, Tom Ko, Zhan Zhuang, Lilian Tang, and Yu Zhang. HiRA: Parameter-efficient hadamard high-rank adaptation for large language models. In Proceedings of the 2025 International Conference on Learning Representations (ICLR), 2025. [14] Ting Jiang, Shaohan Huang, Shengyue Luo, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang, and Fuzhen Zhuang. MoRA: High-rank updating for parameter-efficient fine-tuning. arXiv preprint arXiv:2405.12130, 2024. [15] Dawid J. Kopiczko, Tijmen Blankevoort, and Yuki M. Asano. VeRA: Vector-based random matrix adaptation. In Proceedings of the 2024 International Conference on Learning Representations (ICLR), 2024. 10 [16] Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, and Eunhyeok Park. Owq: Outlieraware weight quantization for efficient fine-tuning and inference of large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1335513364, 2024. [17] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models with evol-instruct. In The Twelfth International Conference on Learning Representations, 2024. [18] Fanxu Meng, Zhaohui Wang, and Muhan Zhang. Pissa: Principal singular values and singular vectors adaptation of large language models. Advances in Neural Information Processing Systems, 37:121038121072, 2024. [19] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering, 2018. [20] Fabian Paischer, Lukas Hauzenberger, Thomas Schmied, Benedikt Alkin, Marc Peter Deisenroth, and Sepp Hochreiter. One initialization to rule them all: Fine-tuning via explained variance adaptation. arXiv preprint arXiv:2410.07170, 2024. [21] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. [22] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019. [23] Shaowen Wang, Linxi Yu, and Jian Li. Lora-ga: Low-rank adaptation with gradient approximation. Advances in Neural Information Processing Systems, 37:5490554931, 2024. [24] Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. Magicoder: Empowering code generation with OSS-instruct. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 5263252657. PMLR, 2127 Jul 2024. [25] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models, 2024. [26] Lingling Xu, Haoran Xie, Si-Zhao Joe Qin, Xiaohui Tao, and Fu Lee Wang. Parameter-efficient fine-tuning methods for pretrained language models: critical review and assessment, 2023. [27] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [28] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. 11 Rank Analysis in Real-World Scenarios Table 4: Average rank size in each projection layer across LoRA and GraLoRA variants. Rank was set to 128 in all methods. LoRA GraLoRA (k=2) GraLoRA (k=4) GraLoRA (k=8) q_proj 128 256 512 1024 k_proj 128 256 512 1016 v_proj 128 256 512 1022 o_proj 128 256 512 1024 up_proj 128 256 512 down_proj 128 256 512 1024 gate_proj 128 256 512 1024 As shown in Table 4, GraLoRA denoted linearly increasing ranks as the increased. The observation aligns with our theoretical analysis that increasing GraLoRA leads to higher expression power by increasing the latent space from to kr."
        },
        {
            "title": "B Gradient Distribution of LoRA and GraLoRA",
            "content": "Figure 9: Comparison of gradient distributions under outlier activation for rank 32, 64, and 128 in LLaMA3.1-8B Layer 1 down-projection matrix. Figure 9 displays gradient distributions of LoRA and GraLoRA for varying ranks. In GraLoRA, only the blocks interacting with the outlier exhibit elevated gradients, structurally solving the gradient entanglement discovered in vanilla LoRA. This enables to mitigate global distortion and align with FFT behavior in all ranks."
        },
        {
            "title": "C Precise Analysis on Computation Overhead",
            "content": "Figure 10: Computation workflow in GraLoRA is composed of 3 steps: two sub-block matrix multiplications and following matrix addition. In the previous Computation Overhead Analysis section 3.4 we compared the computation of LoRA and GraLoRA with the big notation on the two major matrix multiplication steps. In this section we further examine the exact computation requirement and compare their efficiency. LoRA FLOPs LoRA performs the projection in two sequential steps to leverage its low-rank structure. In the first step, the computation of AX RrT requires (2N 1)rT FLOPs. In the second step, the reconstruction B(AX) RM incurs (2r 1)M FLOPs. Therefore, the total FLOPs for LoRA is: LoRAFLOPs = (2N 1)rT + (2r 1)M = 2r(M + )T (r + )T. 1(cid:1) GraLoRA FLOPs In practice, GraLoRA computations can be divided into three stages, involving k2 adapter blocks: two matrix multiplications followed by matrix addition as shown in Figure 10. , which requires In the first stage (projection), each adapter block computes (cid:0)2 FLOPs. Since there are k2 such blocks, the total FLOPs for this step is (2n k)rT . , which In the second stage (reconstruction), each adapter block performs Bi,j(A costs (cid:0)2 FLOPs. With k2 blocks, the total becomes (2r k)mT . The final stage involves aggregating the outputs across projections for each row: i,jXj) i,jXj 1(cid:1) (cid:88) j= Bi,j(A i,jXj) T , which requires (cid:0) (k 1)mT . (cid:1) (k 1) = mT (k1) FLOPs per row. Across rows, the total cost becomes Combining all three stages, the total FLOPs for GraLoRA is: GraLoRAFLOPs = (2n k)rT + (2r k)mT + (k 1)mT = 2r(m + n)T k(r + m)T + (k 1)mT = 2r(m + n)T krT mT. This can also be expressed as: GraLoRAFLOPs = LoRAFLOPs (k 1)rT, demonstrating that GraLoRA introduces reduced computation compared to LoRA."
        },
        {
            "title": "D Baselines and Hyperparameters",
            "content": "Baseline Methods. We compared GraLoRA with three baseline methods. Key idea for each method is as follows: LoRA freezes pretrained model weights and injects trainable low-rank matrices into selected layers, allowing efficient fine-tuning with significantly fewer parameters, approximating weight updates as product of two small matrices. MoRA employs single square matrix instead of low-rank matrices to achieve high-rank updating while maintaining the same number of trainable parameters. RaSA enhances LoRA by sharing partial low-rank components across layers while keeping layer-specific updates. Table 5: Hyperparameter settings for each method and dataset. Task Model Code Generation LLaMA3.18B Commonsense Reasoning Qwen-2.5-1.5B Qwen-2.5-7B LLaMA3.170B Method LoRA MoRA RaSA GraLoRA LoRA MoRA RaSA GraLoRA LoRA MoRA RaSA GraLoRA LoRA GraLoRA Rank LR Batch size Epochs {16, 32, 64, 128} 2e192 64 64 64 2e-4 4e-4 192 3e-4 192 2 2 1 For hyperparameter, we fixed LoRA α = 2r which is known to be generally applicable in different models with different ranks [2]. Detailed hyperparameter settings for our experiments are denoted in Table 5."
        }
    ],
    "affiliations": [
        "POSTECH",
        "SqueezeBits"
    ]
}