{
    "paper_title": "Rethinking Reflection in Pre-Training",
    "authors": [
        "Essential AI",
        ":",
        "Darsh J Shah",
        "Peter Rushton",
        "Somanshu Singla",
        "Mohit Parmar",
        "Kurt Smith",
        "Yash Vanjani",
        "Ashish Vaswani",
        "Adarsh Chaluvaraju",
        "Andrew Hojel",
        "Andrew Ma",
        "Anil Thomas",
        "Anthony Polloreno",
        "Ashish Tanwer",
        "Burhan Drak Sibai",
        "Divya S Mansingka",
        "Divya Shivaprasad",
        "Ishaan Shah",
        "Karl Stratos",
        "Khoi Nguyen",
        "Michael Callahan",
        "Michael Pust",
        "Mrinal Iyer",
        "Philip Monk",
        "Platon Mazarakis",
        "Ritvik Kapila",
        "Saurabh Srivastava",
        "Tim Romanski"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "A language model's ability to reflect on its own reasoning provides a key advantage for solving complex problems. While most recent research has focused on how this ability develops during reinforcement learning, we show that it actually begins to emerge much earlier - during the model's pre-training. To study this, we introduce deliberate errors into chains-of-thought and test whether the model can still arrive at the correct answer by recognizing and correcting these mistakes. By tracking performance across different stages of pre-training, we observe that this self-correcting ability appears early and improves steadily over time. For instance, an OLMo2-7B model pre-trained on 4 trillion tokens displays self-correction on our six self-reflection tasks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 2 2 0 4 0 . 4 0 5 2 : r Rethinking Reflection in Pre-Training Essential AI San Francisco, CA research@essential.ai"
        },
        {
            "title": "Abstract",
            "content": "A language models ability to reflect on its own reasoning provides key advantage for solving complex problems. While most recent research has focused on how this ability develops during reinforcement learning, we show that it actually begins to emerge much earlierduring the models pre-training. To study this, we introduce deliberate errors into chains-of-thought and test whether the model can still arrive at the correct answer by recognizing and correcting these mistakes. By tracking performance across different stages of pre-training, we observe that this self-correcting ability appears early and improves steadily over time. For instance, an OLMo-27B model pre-trained on 4 trillion tokens displays self-correction on our six self-reflection tasks. Figure 1: As pre-training compute increases, OLMo-2 checkpoints solve more word problems from our adversarial version of GSM8K-Platinum, leveraging explicit reflection. For the definition of pre-training compute please refer to section 4.3. Essential AI (2025)"
        },
        {
            "title": "Introduction",
            "content": "Reflection enhances models ability to adapt its response to prior reasoning, thereby improving the accuracy of its outputs. Recent work reports that \"behaviors such as reflection... emerge as result of the models interaction with the reinforcement learning environment\" [DeepSeek-AI, 2025a]. Attributing the development of capabilities to validate such claims necessitates comprehensive evaluation throughout the training phases. In this work, we propose an exhaustive framework to measure reflection and we observe the phenomenon consistently during pre-training. Benchmarking reflection using existing reasoning datasets [Mondorf and Plank, 2024] has been challenging. Reflection is often sparse in such tasks, and each model exhibits distinct error patterns that yield unique manifestations of reflective behavior [Madaan et al., 2023, Liu et al., 2025]. We address this challenge by distinguishing between situational-reflection and self-reflection. In the situational setting, model examines reasoning chains created by another source, such as different frontier model. In self-reflection, the model considers its own reasoning processes. We calibrate models ability to solve task when provided with some confounding reasoning that leads to an incorrect answer, and thereby measure reflection capabilities throughout the pre-training process. Figure 2 shows an attempt by partially pre-trained OLMo-2 checkpoint [OLMo et al., 2025] to solve coding task [Gu et al., 2024]. In this case, the model is given question about determining the input to the Python function that leads to the output avdropj gsd . In its first attempt, the model which has 32 billion parameters and was pre-trained on 4.8 trillion tokens repeats the output as the answer, advdropj gsd . Upon prompting it with its incorrect thinking plus suffix of Wait,, the model is able to successfully self-reflect and generates think got it wrong. . . . the value of ?? should be [gsd, avdropj]. Figure 2: partially pre-trained OLMo-2-32B model exhibits self-reflection to correctly predict programs input. Note that the question is sample_485 from CruxEval. For brevity, formatting instructions are omitted from the visualized prompt. By programmatically introducing erroneous chains-of-thought (CoTs), featuring elements such as arithmetic perturbations and logical inconsistencies, we can control and scale the extent of reflection needed to complete these tasks correctly. This also maintains the established CoT format [Wei et al., 2022]. Furthermore, our algorithmic approach permits the creation of these datasets in relatively rapid and economical manner by adapting established reasoning benchmarks, which in turn allows for comprehensive study of models reflection abilities across diverse range of domains. Our six datasets, spanning the domains of Mathematics, Coding, Logical Reasoning, and Knowledge Acquisition, are designed to assess both situational-reflection and self-reflection capabilities.1. 1Our data and code will be publicly released at https://github.com/Essential-AI/reflection 2 The results of evaluating pre-training checkpoints from the OLMo-2 family of models with varying compute on our set of six diverse datasets indicate that reflection is ubiquitous across domains. Even simple trigger phrase like Wait,[Muennighoff et al., 2025] enables partially pre-trained models to consistently identify both introduced errors and their own generated errors. Specifically, 231 of 240 dataset-checkpoint pairs demonstrate at least one instance of situational-reflection and 154 of 240 pairs demonstrate at least one instance of self-reflection. With increasing pre-training, models correct more adversarial examples, leading to an average Pearson correlation between accuracy and log(pre-training compute) of 0.76 across tasks. Furthermore, as pre-training progresses, models increasingly recover from incorrect prior reasoning, the rate of explicit reflection in model generations increases, and explicit reflection increasingly contributes to recovery from confounding CoTs. The contributions of this paper are threefold: We introduce systematic method to create six datasets across Code, Knowledge Acquisition, Logical Reasoning, and Mathematics to study models ability to reflect. We demonstrate that pre-trained models of varying capacities and training compute can elicit reflection to correct inaccurate prior reasoning using simple interjections in wide variety of domains. We observe that continually improving pre-training leads to better reflection, requiring fewer test-time tokens to solve the same number of tasks."
        },
        {
            "title": "2 Related Work",
            "content": "Evaluating Reasoning in LLMs Real-world applicability of natural language systems is dependent on reasoning capabilities [Wos et al., 1992, Schuster et al., 2019, Mondorf and Plank, 2024]. LLMs, combined with prompting techniques such as Chain of Thought [Wei et al., 2022], Tree of Thought [Yao et al., 2023], and Self-Consistency [Wang et al., 2023b] have been successfully used to solve variety of reasoning tasks [Kojima et al., 2022, Bubeck et al., 2023]. Reasoning is typically measured by observing model outcomes [Fu et al., 2023a, Liu et al., 2023], through generation traces [Saparov and He, 2023, Dziri et al., 2023, Willig et al., 2022] and interactive engagement [Zhuang et al., 2024, Wang et al., 2023a, Bertolazzi et al., 2023]. Prior work also conducts parametric attribution of models internal network to identify reasoning characteristics by studying attention patterns [Hou et al., 2023], activation flows [Dutta et al., 2024] and individual layers [Pirozelli et al., 2024]. These methods are primarily meant to evaluate post-trained models, while we are interested in diagnostic techniques to elicit reasoning throughout the training phases, similar in spirit to the study of model performance through parameter and training data scaling [Kaplan et al., 2020]. In this work, we study reasoning by evaluating the emergence of reflection through adversarial datasets. These datasets require models to reason through multi-step hints, which contain minor errors, in order to complete the tasks successfully. Adversarial Attacks on LLMs Adversarial inputs have long been used to expose vulnerabilities in neural networks [Szegedy et al., 2014, Biggio et al., 2013, Huang et al., 2017]. More recently, similar techniques have been applied to LLMs, demonstrating their susceptibility to carefully crafted prompts [Schwinn et al., 2023]. These attacks have successfully targeted models policy alignment [Deng et al., 2022, Wei et al., 2023], safety [Schuster et al., 2020, Carlini et al., 2021, Kang et al., 2024], and robustness [Zou et al., 2023, Shen et al., 2024, Xu et al., 2024], highlighting large vulnerabilities. Several approaches have proposed adversarial training to improve model defenses against such attacks [Zhu et al., 2019, Jiang et al., 2020, Xhonneux et al., 2024]. Alternatively, the primary goal of our work is not to demonstrate models vulnerabilities to adversaries but rather to highlight how such datasets can be used to consistently and thoroughly evaluate their reasoning capabilities. We achieve this by introducing chains-of-thought [Kojima et al., 2022], where steps [Lightman et al., 2023] are largely correct, but contain mistakes that require reflection [Lightman et al., 2023, Fu et al., 2023b]. Train Time vs Test Time Tradeoff The increasing reliance on test-time compute in advanced reasoning models, such as Open AIs o1 [OpenAI, 2024b] and DeepSeek-R1 [DeepSeek-AI, 2025a], has reignited discussions on the optimal allocation of compute [Kaplan et al., 2020, Hoffmann et al., 2022]. Prior work studies the trade-offs between train-time and test-time compute [Snell et al., 2024], as well as the impact of post-training techniques like supervised fine-tuning [Li et al., 2025, Chen et al., 2025] and reinforcement learning [DeepSeek-AI, 2025a, Wang et al., 2024], on reasoning performance [Liu et al., 2025]. Inference-time paradigms - such as tree search [Yao et al., 2023], chain-of-thought prompting [Wei et al., 2022] and budget-forcing [Muennighoff et al., 2025] - interact with models pretrained representations [Yeo et al., 2025, Ruis et al., 2025, Gandhi et al., 2025]. However, in our work, we contribute complementary perspective by demonstrating that certain reasoning capabilities can gradually emerge during pre-training itself, even in the absence of fine-tuning, reinforcement learning, or 3 specialized test-time inference techniques. Our findings suggest that key aspects of reasoning can be instilled through pre-training alone, potentially changing the calculus of the optimal use of compute across training and inference."
        },
        {
            "title": "3 Approach",
            "content": "Our objective is to measure reflection comprehensively and at scale. We present definition of reflection 3.1, programmatically create tasks to elicit reflection 3.2, and rigorously measure the presence of reflection 3.3. 3.1 Defining Reflection Reflection is form of metacognition that involves examining information, evaluating the reasoning that underlies it, and adjusting future behavior based on that evaluation. In the context of language model, this process can apply to information introduced from an external source or generated by the model itself. In this work, we create two settings in which to elicit and measure reflection: Situational-reflection is when model reflects on information created by another source, such as different model. Self-reflection is when model reflects on its own generated outputs. We also comprehensively characterize reflection by means of two forms: Explicit reflection occurs when model produces tokens that by their meaning recognize and address mistakes in the adversarial context. Explicit reflection may be present in either correct model outputs (in other words, those that constitute correct answer to our adversarial tasks) or incorrect model outputs. Implicit reflection occurs when model navigates adversarial contexts to solve task correctly without outputting tokens that explicitly identify errors in prior reasoning. In our definition, this means that implicit reflection cannot result in an incorrect answer to our adversarial tasks. This allows us to distinguish between, on one hand, cases where explicit reflection is absent but implicit reflection may be inferred to have taken place and, on the other hand, cases where no reflection took place at all. 3.2 Eliciting Reflection with Adversarial Reflection Datasets We present an algorithm to generate adversarial datasets that elicit reflection behavior from language models. The algorithm creates adversarial CoTs that lead to incorrect solutions. Unlike self-reflection, where we can draw from the models own mistakes, we must devise artificial adversarial CoTs for situational-reflection. At high level, these adversarial CoTs are created by corrupting correct CoTs in ways that mimic human reasoning errors, such as logical missteps and arithmetic miscalculations. In both cases, when we provide the CoTs in context, the models must reflect on the mistakes and fix them to arrive at the correct solution. We believe that these settings are necessary to comprehensively study reflection. The task design includes appending trigger token, such as \"Wait,\", to promote continued reasoning throughout the task-solving process. The algorithm has two variants. Algorithms 1 and 2 create the situational-reflection and self-reflection datasets respectively. 3.3 Measuring Reflection We propose an automated approach to measure the reflective capabilities of models using adversarial datasets, based on our earlier categorization of reflection (see 3.1): Measuring explicit reflection: To identify instances of explicit reflection, we develop prompt-based LLM classifier that detects when models output explicitly acknowledges and addresses errors within the provided adversarial context, regardless of whether the model reached the correct answer or not. The classifier is described below in 4.3. Measuring implicit reflection: We stipulate that all model generations that lead to the correct answer in the presence of adversarial context are attributable to reflection, even if no tokens that relate to reflection are outputted. We believe this accords with an everyday understanding of reflection in the sense of describing human 4 Algorithm 1: Situational-Reflection Dataset Generation. Table 1 details the tasks, frontier models, and dataset artifacts in columns Base Dataset, Correct CoT, and Adversarial CoT respectively. Set of task instances T, Frontier model F, or existing dataset artifact Situational-reflection dataset Dsit with adversarial CoTs 1 Input: 2 Output: 3 4 For each task instance in T: 5 1. Procure correct CoT using or A. 6 2. Create an adversarial CoT by using to introduce small errors into the correct CoT, such that it leads to an incorrect answer. 7 3. Append trigger token, such as Wait, to the adversarial CoT. 8 4. Maintain ts existing question and gold answer. 9 5. Append the resulting instance to Dsit 10 11 Return Dsit Algorithm 2: Self-Reflection Dataset Generation. The tasks are the same as Base Datasets in Table 1 and the models are the OLMo 7, 13, and 32 Billion parameter pre-trained checkpoints. Set of task instances T, Model checkpoint Self-reflection dataset Dself with adversarial CoTs 1 Input: 2 Output: 3 4 For each task instance in T: 5 1. Run task instance on the model checkpoint to generate CoT c. 6 2. If the run produced an incorrect answer, denote as an adversarial CoT; else discard c. 7 3. Append trigger token, such as Wait, to c. 8 4. Maintain ts existing question and gold answer. 9 5. Append the resulting instance to Dself 10 11 Return Dself metacognition. An implication of our approach is that, by construction, generations that produce correct answers but are not identified by the explicit classifier are categorized as instances of implicit reflection. See Section 4.3 for further details on the specific reflection metrics that we used."
        },
        {
            "title": "4 Experimental Setup",
            "content": "In order to comprehensively study reflection, we evaluate partially pre-trained models spanning different computational budgets, with variations in both parameter count and number of training tokens 4.1. Our study includes two types of adversarial tasks 4.2: (1) Situational-Reflection, where adversarial chains of thought are systematically generated from existing datasets using frontier models; and (2) Self-Reflection, where adversarial chains of thought are derived from the models own previous incorrect responses to original task instances. Our evaluation also examines whether model outputs demonstrate explicit reflective reasoning 4.3. We detail our infrastructure set-up in Section 4.4. 4.1 Model Families OLMoOLMo-2 [OLMo et al., 2025] is fully open-source, open-weight LLM project with training checkpoints available for 7B, 13B and 32B parameter variants. We evaluated total of 40 checkpoints (see Appendix for details). We aimed for equally spaced checkpoints and used the closest alternatives in cases when they were not available. 5 Qwen2.5 We evaluated Qwen2.5s 0.5B, 3B, 7B, 14B, 32B and 72B parameter variants [Qwen et al., 2025]. See Appendix for details. 4.2 Datasets We evaluate the reflection phenomenon over wide set of tasks, creating six adversarial datasets based on BIG-Bench Hard (BBH) [bench authors, 2023], CruxEval [Gu et al., 2024], GSM8K [Cobbe et al., 2021], GSM8K-Platinum [Vendrow et al., 2025] and TriviaQA [Joshi et al., 2017]. An overview of our adversarial datasets can be found in Tables 1 & 2. We utilize models DeepSeek-V3 (referred to as DS-V3) [DeepSeek-AI, 2025b], GPT-4o2 [OpenAI, 2024a], and SentenceTransformers [Reimers and Gurevych, 2020], as well as multiple automated checks and manual inspections in our pipeline to ensure quality and robustness in our datasets. Situational-Reflection Datasets: Creation of these datasets involves multi-step pipeline where we prompt an LLM to create adversarial CoTs (see also Algorithm 1). For reasoning tasks, the original tasks may contain the CoTs as dataset artifact; if not, we create them using frontier model such as GPT-4o or DS-V3. Subsequently, the correct CoTs are used to develop adversarial CoTs by introducing deliberate errors that lead to wrong answers. In addition, checks are made at the end to ensure that CoTs do not reveal their misleading intentions. The pipelines and prompts have been carefully designed to minimize errors and are dataset-specific, but the overall framework can be generalized to any domain and dataset. Self-Reflection Datasets: These datasets are created by executing the LLM we want to evaluate on the base task and collecting CoTs corresponding to the questions the models failed on (see also Algorithm 2). Since we are interested in the reflection abilities of models as pre-training progresses, we only keep the questions incorrectly answered by all checkpoints at each parameter scale in the self-reflection datasets to ensure consistent comparison across pre-training checkpoints. Extensive details on our dataset-specific pipelines, checks, and filters can be found in Appendix F. Domain Base Dataset Correct CoT Adversarial CoT Verbal, Logical & Mathematical Reasoning Code Reasoning & Understanding Math Reasoning Knowledge Acquisition & Reading Comprehension BIG-Bench Hard DS-V cruxeval-i cruxeval-o GSM8K GSM8K-Platinum GPT-4o GPT-4o dataset artifact dataset artifact DS-V3 GPT-4o GPT-4o DS-V3 DS-V3 TriviaQA dataset artifact SentenceTransformer Table 1: Our uniform framework allows for the automated creation of situational-reflection datasets. Base Dataset Base dataset size Situational-Reflection dataset size Self-Reflection dataset size OLMo-2-{7, 13, 32}B BBH cruxeval-i cruxeval-o GSM8K GSM8K-Platinum TriviaQA 6511 800 800 1319 1209 17994 6511 605 704 9070 8223 2961 712, 988, 588 276, 260, 181 310, 313, 243 140, 129, 68 113, 106, 50 41, 83, 51 Table 2: Situational and self-reflection dataset statistics. 2GPT-4o was better than DeepSeek-V3 at following instructions for CruxEvals adversarial data creation. 6 4.3 Evaluation Metric Reflection Type Answer Criteria Accuracy Explicit Reflection Rate Explicit Reflection Accuracy Implicit Reflection Accuracy Implicit or Explicit Explicit Only Explicit Only Implicit Only Correct Only Incorrect or Correct Correct Only Correct Only Table 3: Metrics and corresponding criteria for evaluating reflection across different settings. Metrics As seen in Table 3, for both situational reflection and self-reflection settings, Accuracy is the fraction of task instances that model correctly solves. Independent of Accuracy, we utilize our Explicit Reflection Classifier to measure the Explicit Reflection Rate, which is the fraction of task instances where the model output exhibits explicit reflection, regardless of whether they are correct or not. We additionally report Explicit Reflection Accuracy, which is the fraction of task instances that model both correctly solves and exhibits explicit reflection on. Finally, Implicit Reflection Accuracy is the fraction of task instances where the model output is correct and does not exhibit explicit reflection. Details about Accuracy metrics for each dataset can be found in Appendix H. Furthermore, for each data point, we report the pre-training compute as 6nt, where and are the number of parameters and training tokens respectively. More details about our train time and test time compute formulations can be found in section 5.4. Explicit Reflection Classifier We develop prompt-based classifier to determine whether model output exhibits explicit reflection. DeepSeek-V3 [DeepSeek-AI, 2025b] is prompted with description of what is meant by \"reflection\" along with two to four explicit reflection exemplars. Some examples of the kind of explicit reflection phrases that this classifier is intended to detect are shown in Table 5. We validated the classifier against human-annotated golden labels on GSM8K, cruxeval-o, and TriviaQA (120 questions per benchmark; see Appendix for details on the annotation process). Although our classifier recalls fewer instances of reflection (see Table 4), its precision is high enough to validate its usefulness. In the worst case, we will under-report reflection behaviors, but be more confident when we do report them. 4.4 Infrastructure Our experiment set-up uses the vLLM inference framework [Kwon et al., 2023] for hosting OLMo-2 and Qwen models. We host DeepSeek-V3 using SGLang [Zheng et al., 2024]. We use cluster of AMD MI300x Accelerators and Kubernetes to schedule jobs into it."
        },
        {
            "title": "5 Results",
            "content": "Benchmark Precision Recall F1 Score cruxeval-o GSM8K TriviaQA 0.98 0.95 1.00 0.76 0.71 0.61 0.86 0.81 0. Table 4: Explicit Reflection Classifier performance metrics across different benchmarks. To comprehensively measure reflective reasoning across domains, our classifier separates explicit and implicit reflection in BBH, cruxeval-i, cruxeval-o, GSM8K, GSM8K-Platinum and TriviaQA for the situationalreflection 5.1 and self-reflection 5.3 settings. To our surprise, we discover strong presence of reflection, which grows with more training compute. Furthermore, as pre-training progresses, models increasingly recover from confounders, the rate of explicit reflection increases, and explicit reflection increasingly contributes to 3The self-reflection dataset sizes are specific to each \"model family\" because each consists of only the questions that all checkpoints in the model family got wrong, in order to ensure consistent question set across checkpoints. 7 recovery from confounders (see examples in Table 5). These results highlight pre-trainings role in developing reflective abilities. OLMo-2 Checkpoint Reflection Phrase Task GSM8K BBH 7B-stage1-tokens38B 7B-stage1-tokens198B GSM8K-Platinum 7B-stage1-tokens785B cruxeval-i 32B-stage1-tokens3641B cruxeval-o 32B-stage1-tokens3641B TriviaQA 13B-stage1-tokens5001B Wait, 42.6 is not whole number. Lets solve this step by step . . . Wait, we forgot the cow! So the answer is 8. Wait, 120 is not multiple of 100. Lets check our work. Wait, abc is not valid input because. . . Wait, - The function is not correct, because it should remove the last character. Wait, what? thought it was Louisiana? Table 5: Examples of reflection phrases across situational-reflection tasks and checkpoints. See Appendix for full details of the Checkpoints. 5.1 Explicit Situational-Reflection is Prominent in all Models Task Can Correct? Number of Checkpoints Pearson correlation of each metric with log(pre-training compute) Accuracy Explicit Ref. Rate Implicit Accuracy Explicit Accuracy BBH cruxeval-i cruxeval-o GSM8K GSM8K-Platinum TriviaQA 40 31 40 40 40 40 0.73 0.70 0.72 0.84 0.84 0.72 0.77 0.67 0.78 0.84 0.85 0.49 0.38 0.36 -0.16 0.01 -0.01 0.62 0.75 0.68 0.71 0.86 0.86 0.70 Table 6: Partially pre-trained models can correct non-zero adversarial examples across tasks and with increasing pre-training, get higher accuracy and more explicit reflection. In Table 6, it is promising that with the exception of the cruxeval-i task, for all tasks, every OLMo-2 pre-training checkpoint shows signs of recovery from situational confounders either implicitly or explicitly 4. This leads to 231 of 240 dataset-checkpoint pairs demonstrating at least one instance of situational-reflection. However, to vindicate our hypothesis that models progressively develop and use explicit reflection, most recoveries should be attributed to explicit situational-reflection. In essence, what we are looking for, with more pre-training compute, is the increasing frequency of (a) recovering from situational confounders, (b) explicitly reflecting on situational confounders, and (c) recovering from situational confounders with explicit reflection. The high positive Pearson correlations of each metric and log(pre-training compute) [Cohen et al., 2009] imply (a), (b), and (c). We also see generally low correlation of implicit reflection accuracy with log(pre-training compute). Together, these results emphasize that with greater pre-training, models tend to solve more adversarial instances successfully and increasingly prefer to use explicit reflection while doing so. For instance, GSM8K-Platinum shows in Figure 3 that, with increasing pre-training, models across parameter counts solve majority of the task instances by explicitly reflecting on errors. TriviaQA is an exception where significant improvements can be attributed to implicit reflection, consequence of the task primarily measuring knowledge acquisition, with several of the instances capable of being solved without explicit reasoning. Detailed results for all six tasks can be found in Appendix C. 4Unless explicitly stated otherwise, all studies were performed on publicly available OLMo-2 checkpoints (see Table 1). We take an average of three runs to reports each experiment. 8 (a) Accuracies for the OLMo-2 family of models on the GSM8K-Platinum situational-reflection evaluation. (b) Explicit reflection accuracies in situational-reflection setting for the OLMo-2 family of models on the GSM8K-Platinum adversarial evaluation. Figure 3: Situational-reflection results for the OLMo-2 family of models on the GSM8K-Platinum adversarial evaluation indicate that accuracies grow with increased pre-training compute, and explicit reflection is responsible for the majority of these gains. 5.2 Models Reflect Even Without Triggers; Wait, Enhances Explicitness and Accuracy In order to understand the causal role of the Wait,-trigger, we investigate model performance on GSM8K-Platinum with triggers at opposite extremes. Specifically, we study model performance with no trigger (A) and with trigger (B) containing the explicit admission Wait, made mistake. We choose as mode in which to draw minimum attention to the mistakes in the adversarial CoTs. In contrast, we choose as mode that emphasizes the existence of an error in the CoT. Figure 4 plots these results. The results first confirm the hypothesis that even without trigger, models can increasingly succeed on situational confounders as pre-training progresses. The role of Wait, in the results above in Section 5.1 is further clarified here. We see that the models performance on this task is bounded by the two extremes of and B. In scenario A, models improve accuracy by reflecting implicitly and in scenario models reflect explicitly by construction and thus greatly improve performance. The Wait,-setting behaves like when implicitly reflecting and like when explicitly reflecting. Interestingly, as seen in Figure 4, its performance can be decomposed as accW ait = eW ait accB + (1 eW ait) i_accA, where eW ait is the rate of explicit reflection and i_acc is the implicit reflection accuracy. Furthermore, we see that significant elevation in performance is attributable to the Wait,-trigger. This is because the trigger elicits explicit reflection at rate that increases with pre-training, and it also attains the performance achieved by the corresponding B-triggered models, which, as noted above, we choose as mode that emphasizes the existence of an error in the CoT. In other words, when the model, conditioned on an adversarial CoT with Wait,, exhibits explicit reflection, its performance is comparable to when the model is explicitly told that the CoT contains mistake. Conversely, when the model is conditioned on an adversarial CoT with Wait, but does not exhibit explicit reflection, its performance is in line with the mode where the model is conditioned on the adversarial CoT only. This establishes the causal connection for explicit reflection through Wait, in improving accuracy. 5.3 Explicit Self-Reflection is Harder but Advances With More Compute At first glance, the rarity of self-reflection in Table 7 might be seen as negative result. However, this can be explained by the fact that in this setting the models are evaluated on the task instances that they answered incorrectly - so by design, they are particularly difficult (see 2 in section F). Nonetheless, in about 64.2% of task attempts, the models do demonstrate at least some ability to self-correct. In order to disambiguate self-reflection and self-correction, we plot the rate of reflection generated by the models, independent of whether the tasks were solved in Figures 5. These results indicate strong trend that as pre-training progresses, models get better at explicitly highlighting their own errors. For cruxeval-i we notice that with increasing pre-training models tend to learn to self-correct. However, they are able to self-reflect much earlier. This suggests the natural progression of self-reflection skills before self-correction. 9 Figure 4: Models can reflect and correct situational adversaries even without triggers. We see that the Wait, trigger emulates linear combination of no trigger (A) and trigger that emphasizes mistake in the prior reasoning (B), weighted by the checkpoints ability to elicit explicit reflection. Task Can Correct? Number of Checkpoints Pearson correlation of each metric with log(pre-training compute) Accuracy Explicit Ref. Rate Implicit Accuracy Explicit Accuracy BBH cruxeval-i cruxeval-o GSM8K GSM8K-Platinum TriviaQA 35 21 22 28 26 0.44 0.69 0.38 0.26 0.32 0.09 0.88 0.70 0.49 0.75 0.75 0.73 -0.03 0.53 0.21 -0.05 0.07 -0.11 0.43 0.57 0.39 0.24 0.33 0.31 Table 7: Partially pre-trained models can correct non-zero adversarial examples across tasks, and with increasing pre-training, achieve higher accuracy and more explicit self-reflection. How these germs of self-reflection evolve into sophisticated autonomous reasoning abilities with post-training is an open question that we leave for future work. We hypothesize that there must be critical threshold of pre-trained self-reflection beyond which the model has high likelihood of developing into test-time reasoner. It is surprising to us that we see such levels of explicit self-reflection when learning from organic web datasets [Li et al., 2024]. Pinpointing data distributions that promote explicit self-reflection during pre-training is another natural next step of our work. Detailed results on all six tasks can be found in Appendix D. (a) Self-reflection accuracy on cruxeval-i. (b) Explicit reflection rates on self-reflection cruxeval-i setting. Figure 5: OLMo-2 family of models demonstrate small but noticeable self-reflection followed by self-correction on the cruxeval-i task with increasing pre-training. 10 5.4 One Can Trade Training for Test-time Compute We investigate the trade-off between increased investment in train-time compute and the corresponding expenditure required at test-time to achieve comparable accuracy on the downstream task. We approach this by estimating the train-time compute as 6nt where and are the number of parameters and training tokens, respectively, and test-time compute as 2nw where represents the number of words generated to solve certain number of adversarial questions. We begin by specifying set of target numbers of adversarial questions to be answered correctly. We then plot curve for each of these targets. We study this for the GSM8K-Platinum adversarial dataset introduced earlier. We follow sequential test-time scaling approach by appending the trigger Wait, to model generations. In fact, we introduce two Wait, triggers to enable weaker models to reach the same metric level as stronger models. This mimics the sequential method for scaling test-time compute in [Muennighoff et al., 2025]. As we see in Figure 6, as train-time compute increases, the test-time compute requirement decreases in OLMo-2-32B checkpoints. This result further supports our hypothesis that the model gets better at reflection as pre-training progresses, implying less test-time compute for given level of accuracy. Figure 6: Tradeoff between train-time and test-time compute for OLMo-2-32B pre-training checkpoints. Each dotted trendline depicts fixed metric value on GSM8K-Platinum. Each dashed line is trendline for the isocorrect relationship of the same color.6 5.5 Do we see similar results in stronger model families? In order to study these phenomena across model families, we report Qwen2.5s results on our adversarial tasks in Fig.7. Consistent with our results for OLMo-2, we find that with increased pre-training compute in this case, the number of parameters model performance on adversarial tasks consistently increases. This demonstrates again that models can increasingly solve tasks despite mistakes in prior reasoning with pre-training compute alone. 5The factors 6 and 2 are standard per-parameter per-token train and inference FLOPs calculation constants respectively. They account for both forward and backward passes that are required for training and the forward pass alone that is required for inference. 6We exclude stage1-step142000-tokens1192B checkpoint from this plot because it was an outlier and distorting the plot. This version of the plot allows for better visual inspection. 11 Figure 7: Situational-reflection for the Qwen2.5 family of models across all evaluations. Details about metrics for each dataset can be found in Appendix H."
        },
        {
            "title": "6 Conclusion",
            "content": "The goal of this work is to address the question, How does reflective reasoning advance during pre-training?\", in contrast to the commonly held belief that reflection emerges during post-training through reinforcement learning. We propose and implement comprehensive framework to measure reflection capabilities throughout the pretraining regime. Through our adversarial datasets, for both situational-reflection and self-reflection settings, we are able to calibrate these capabilities extensively. We determine that models trained with very few training flops, like OLMo-2-7B trained with 198 billion tokens, demonstrate reflection across Mathematics, Code, Verbal, and Logical Reasoning. Furthermore, as more compute is spent on pre-training, these abilities further strengthen."
        },
        {
            "title": "References",
            "content": "BIG bench authors. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview. net/forum?id=uyTL5Bvosj. Leonardo Bertolazzi, Davide Mazzaccara, Filippo Merlo, and Raffaella Bernardi. ChatGPTs information seeking strategy: Insights from the 20-questions game. In C. Maria Keet, Hung-Yi Lee, and Sina Zarrieß, editors, Proceedings of the 16th International Natural Language Generation Conference, pages 153162, Prague, Czechia, September 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.inlg-main.11. URL https://aclanthology.org/2023.inlg-main.11/. Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Šrndic, Pavel Laskov, Giorgio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. In Hendrik Blockeel, Kristian Kersting, Siegfried Nijssen, and Filip Železný, editors, Machine Learning and Knowledge Discovery in Databases, pages 387402, Berlin, Heidelberg, 2013. Springer Berlin Heidelberg. ISBN 978-3-642-40994-3. URL https://link.springer.com/chapter/10.1007/978-3-642-40994-3_25. Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. 12 Sparks of artificial general intelligence: Early experiments with gpt-4, 2023. URL https://arxiv.org/abs/ 2303.12712. Nicholas Carlini, Florian Tramèr, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Úlfar Erlingsson, Alina Oprea, and Colin Raffel. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21), pages 2633 2650. USENIX Association, August 2021. ISBN 978-1-939133-24-3. URL https://www.usenix.org/ conference/usenixsecurity21/presentation/carlini-extracting. Feng Chen, Allan Raventos, Nan Cheng, Surya Ganguli, and Shaul Druckmann. Rethinking fine-tuning when scaling test-time compute: Limiting confidence improves mathematical reasoning, 2025. URL https:// arxiv.org/abs/2502.07154. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. URL https://arxiv.org/abs/2110.14168. Israel Cohen, Yiteng Huang, Jingdong Chen, Jacob Benesty, Jacob Benesty, Jingdong Chen, Yiteng Huang, and Israel Cohen. Pearson correlation coefficient. Noise reduction in speech processing, pages 14, 2009. URL https://link.springer.com/content/pdf/10.1007/978-3-642-00296-0_5.pdf. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025a. URL https://arxiv.org/abs/2501.12948. DeepSeek-AI. Deepseek-v3 technical report, 2025b. URL https://arxiv.org/abs/2412.19437. Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric Xing, and Zhiting Hu. RLPrompt: Optimizing discrete text prompts with reinforcement learning. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 33693391, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.222. URL https: //aclanthology.org/2022.emnlp-main.222/. Subhabrata Dutta, Joykirat Singh, Soumen Chakrabarti, and Tanmoy Chakraborty. How to think step-by-step: mechanistic understanding of chain-of-thought reasoning. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https://openreview.net/forum?id=uHLDkQVtyC. Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang (Lorraine) Li, Liwei Jiang, Bill Yuchen Lin, Sean Welleck, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena Hwang, Soumya Sanyal, Xiang Ren, Allyson Ettinger, Zaid Harchaoui, and Yejin Choi. Faith and fate: Limits of transformers on compositionality. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 7029370332. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/ paper_files/paper/2023/file/deb3c28192f979302c157cb653c15e90-Paper-Conference.pdf. Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, and Tushar Khot. Chain-of-thought hub: continuous effort to measure large language models reasoning performance, 2023a. URL https://arxiv.org/abs/ 2305.17306. Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. Specializing smaller language models towards multi-step reasoning. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 1042110430. PMLR, 2329 Jul 2023b. URL https://proceedings.mlr.press/v202/fu23d.html. Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah D. Goodman. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars, 2025. URL https://arxiv. org/abs/2503.01307. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 07 2024. URL https://zenodo.org/records/12608602. 13 Alex Gu, Baptiste Roziere, Hugh James Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida Wang. CRUXEval: benchmark for code reasoning, understanding and execution. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 1656816621. PMLR, 2127 Jul 2024. URL https://proceedings.mlr.press/ v235/gu24c.html. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Thomas Hennigan, Eric Noland, Katherine Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karén Simonyan, Erich Elsen, Oriol Vinyals, Jack Rae, and Laurent Sifre. An empirical analysis of computeoptimal large language model training. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 3001630030. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ c1e2faff6f588870935f114ebe04a3e5-Paper-Conference.pdf. Yifan Hou, Jiaoda Li, Yu Fei, Alessandro Stolfo, Wangchunshu Zhou, Guangtao Zeng, Antoine Bosselut, and Mrinmaya Sachan. Towards mechanistic interpretation of multi-step reasoning capabilities of language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 49024919, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.299. URL https://aclanthology.org/ 2023.emnlp-main.299/. Sandy H. Huang, Nicolas Papernot, Ian J. Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial attacks on neural network policies. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Workshop Track Proceedings. OpenReview.net, 2017. URL https://openreview. net/forum?id=ryvlRyBKl. Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Tuo Zhao. SMART: Robust and efficient fine-tuning for pre-trained natural language models through principled regularized optimization. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 21772190, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.197. URL https://aclanthology.org/2020. acl-main.197/. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: large scale distantly supervised challenge dataset for reading comprehension. In Regina Barzilay and Min-Yen Kan, editors, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601 1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147/. Daniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin, Matei Zaharia, and Tatsunori Hashimoto. Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks . In 2024 IEEE Security and Privacy Workshops (SPW), pages 132143, Los Alamitos, CA, USA, May 2024. IEEE Computer Society. doi: 10.1109/SPW63631.2024.00018. URL https://doi.ieeecomputersociety.org/10.1109/SPW63631. 2024.00018. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020. URL https: //arxiv.org/abs/2001.08361. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA, 2022. Curran Associates Inc. ISBN 9781713871088. URL https://dl.acm.org/doi/10.5555/3600270.3601883. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, SOSP 23, page 611626, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400702297. doi: 10.1145/3600006.3613165. URL https://doi.org/10.1145/3600006.3613165. 14 Dacheng Li, Shiyi Cao, Tyler Griggs, Shu Liu, Xiangxi Mo, Eric Tang, Sumanth Hegde, Kourosh Hakhamaneshi, Shishir G. Patil, Matei Zaharia, Joseph E. Gonzalez, and Ion Stoica. Llms can easily learn to reason from demonstrations structure, not content, is what matters!, 2025. URL https://arxiv.org/abs/2502.07374. Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Yitzhak Gadre, Hritik Bansal, Etash Guha, Sedrick Scott Keh, Kushal Arora, et al. Datacomp-lm: In search of the next generation of training sets for language models. Advances in Neural Information Processing Systems, 37:1420014282, 2024. URL https://arxiv.org/abs/2406.11794. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step, 2023. URL https://arxiv.org/abs/ 2305.20050. Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, and Yue Zhang. Evaluating the logical reasoning ability of chatgpt and gpt-4, 2023. URL https://arxiv.org/abs/2304.03439. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective, 2025. URL https://arxiv.org/abs/2503. 20783. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 4653446594. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ 91edff07232fb1b55a505a9e9f6c0ff3-Paper-Conference.pdf. Philipp Mondorf and Barbara Plank. Beyond accuracy: Evaluating the reasoning behavior of large language models - survey. In First Conference on Language Modeling, 2024. URL https://openreview.net/ forum?id=Lmjgl2n11u. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling, 2025. URL https: //arxiv.org/abs/2501.19393. Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord, Taira Anderson, David Atkinson, Faeze Brahman, Christopher Clark, Pradeep Dasigi, Nouha Dziri, Michal Guerquin, Hamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya Malik, William Merrill, Lester James V. Miranda, Jacob Morrison, Tyler Murray, Crystal Nam, Valentina Pyatkin, Aman Rangapur, Michael Schmitz, Sam Skjonsberg, David Wadden, Christopher Wilhelm, Michael Wilson, Luke Zettlemoyer, Ali Farhadi, Noah A. Smith, and Hannaneh Hajishirzi. 2 olmo 2 furious, 2025. URL https://arxiv.org/abs/2501.00656. OpenAI. Gpt-4o system card, 2024a. URL https://arxiv.org/abs/2410.21276. OpenAI. Openai o1 system card, 2024b. URL https://arxiv.org/abs/2412.16720. Paulo Pirozelli, Marcos M. José, Paulo de Tarso P. Filho, Anarosa A. F. Brandão, and Fabio G. Cozman. Assessing logical reasoning capabilities of encoder-only transformer models. In Tarek R. Besold, Artur dAvila Garcez, Ernesto Jimenez-Ruiz, Roberto Confalonieri, Pranava Madhyastha, and Benedikt Wagner, editors, Neural-Symbolic Learning and Reasoning, pages 2946, Cham, 2024. Springer Nature Switzerland. ISBN 978-3-031-71167-1. URL https://link.springer.com/chapter/10.1007/978-3-031-71167-1_2. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. Nils Reimers and Iryna Gurevych. Making monolingual sentence embeddings multilingual using knowledge distillation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2020. URL https://arxiv.org/abs/2004.09813. Laura Ruis, Maximilian Mozes, Juhan Bae, Siddhartha Rao Kamalakara, Dwarak Talupuru, Acyr Locatelli, Robert Kirk, Tim Rocktäschel, Edward Grefenstette, and Max Bartolo. Procedural knowledge in pretraining drives reasoning in large language models, 2025. URL https://arxiv.org/abs/2411.12580. Abulhair Saparov and He He. Language models are greedy reasoners: systematic formal analysis of chainof-thought. In The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id=qFVVBzXxR2V. Tal Schuster, Darsh Shah, Yun Jie Serene Yeo, Daniel Roberto Filizzola Ortiz, Enrico Santus, and Regina Barzilay. Towards debiasing fact verification models. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 34193425, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1341. URL https://aclanthology.org/D19-1341/. Tal Schuster, Roei Schuster, Darsh J. Shah, and Regina Barzilay. The limitations of stylometry for detecting machine-generated fake news. Computational Linguistics, 46(2):499510, June 2020. doi: 10.1162/coli_a_ 00380. URL https://aclanthology.org/2020.cl-2.8/. Leo Schwinn, David Dobre, Stephan Günnemann, and Gauthier Gidel. Adversarial attacks and defenses in large language models: Old and new threats. In Javier Antorán, Arno Blaas, Kelly Buchanan, Fan Feng, Vincent Fortuin, Sahra Ghalebikesabi, Andreas Kriegler, Ian Mason, David Rohde, Francisco J. R. Ruiz, Tobias Uelwer, Yubin Xie, and Rui Yang, editors, Proceedings on \"I Cant Believe Its Not Better: Failure Modes in the Age of Foundation Models\" at NeurIPS 2023 Workshops, volume 239 of Proceedings of Machine Learning Research, pages 103117. PMLR, 16 Dec 2023. URL https://proceedings.mlr.press/v239/schwinn23a.html. Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang. \"do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models. In Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security, CCS 24, page 16711685, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400706363. doi: 10.1145/3658644.3670388. URL https://doi.org/10.1145/3658644.3670388. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters, 2024. URL https://arxiv.org/abs/2408.03314. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In Yoshua Bengio and Yann LeCun, editors, 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014. URL http://arxiv.org/abs/1312.6199. Joshua Vendrow, Edward Vendrow, Sara Beery, and Aleksander Madry. Do large language model benchmarks test reliability?, 2025. URL https://arxiv.org/abs/2502.03461. Boshi Wang, Xiang Yue, and Huan Sun. Can ChatGPT defend its belief in truth? evaluating LLM reaIn Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Associsoning via debate. ation for Computational Linguistics: EMNLP 2023, pages 1186511881, Singapore, December 2023a. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.795. URL https: //aclanthology.org/2023.findings-emnlp.795/. Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Mathshepherd: Verify and reinforce LLMs step-by-step without human annotations. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 94269439, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.510. URL https://aclanthology.org/2024. acl-long.510/. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023b. URL https://openreview.net/forum? id=1PL1NIMMrw. 16 Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training fail? In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 8007980110. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ fd6613131889a4b656206c50a8bd7790-Paper-Conference.pdf. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA, 2022. Curran Associates Inc. ISBN 9781713871088. URL https://dl.acm.org/doi/10.5555/3600270. 3602070. Moritz Willig, Matej Zeˇcevic, Devendra Singh Dhami, and Kristian Kersting. Can foundation models talk causality? In UAI 2022 Workshop on Causal Representation Learning, 2022. URL https://openreview. net/forum?id=DbJXEqU0kaM. Larry Wos, Ross Overbeek, Rusty Lusk, and Jim Boyle. Introduction and Applications (2nd edition). McGraw-Hill, 2nd edition, 1992. URL https://www.abebooks.com/ Automated-Reasoning-Introduction-Applications-2nd-Edition/3444415211/bd. Automated Reasoning: Sophie Xhonneux, Alessandro Sordoni, Stephan Günnemann, Gauthier Gidel, and Leo Schwinn. Efficient adversarial training in llms with continuous attacks. arXiv preprint arXiv:2405.15589, 2024. URL https: //arxiv.org/pdf/2405.15589. Xilie Xu, Keyi Kong, Ning Liu, Lizhen Cui, Di Wang, Jingfeng Zhang, and Mohan S. Kankanhalli. An LLM can fool itself: prompt-based adversarial attack. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https:// openreview.net/forum?id=VVgGbB9TNV. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 1180911822. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_ files/paper/2023/file/271db9922b8d1f4dd7aaef84ed5ac703-Paper-Conference.pdf. Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. Demystifying long chain-of-thought reasoning in llms, 2025. URL https://arxiv.org/abs/2502.03373. Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying Sheng. Sglang: Efficient execution of structured language model programs, 2024. URL https://arxiv.org/abs/2312.07104. Chen Zhu, Yu Cheng, Zhe Gan, S. Sun, Tom Goldstein, and Jingjing Liu. Freelb: Enhanced adversarial training for natural language understanding. arXiv: Computation and Language, 2019. URL https://api. semanticscholar.org/CorpusID:209475786. Yan Zhuang, Qi Liu, Yuting Ning, Weizhe Huang, Rui Lv, Zhenya Huang, GuanHao Zhao, Zheng Zhang, Qingyang Mao, Shijin Wang, and Enhong Chen. Efficiently measuring the cognitive ability of LLMs: An adaptive testing perspective, 2024. URL https://openreview.net/forum?id=s6X3s3rBPW. Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J. Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models, 2023. URL https://arxiv.org/abs/2307. 15043."
        },
        {
            "title": "A Contributions and Acknowledgments",
            "content": "Core Contributors"
        },
        {
            "title": "Darsh J Shah\nPeter Rushton\nSomanshu Singla\nMohit Parmar\nKurt Smith\nYash Vanjani\nAshish Vaswani",
            "content": "Contributors"
        },
        {
            "title": "Adarsh Chaluvaraju\nAndrew Hojel\nAndrew Ma\nAnil Thomas\nAnthony Polloreno\nAshish Tanwer\nBurhan Drak Sibai\nDivya S Mansingka\nDivya Shivaprasad\nIshaan Shah\nKarl Stratos\nKhoi Nguyen\nMichael Callahan\nMichael Pust\nMrinal Iyer\nPhilip Monk\nPlaton Mazarakis\nRitvik Kapila\nSaurabh Srivastava\nTim Romanski",
            "content": "18 OLMo-2 and Qwen Pre-trained Checkpoints comprehensive list of the OLMo-2 pre-trained checkpoints and and Qwen2.5 main models that we used. All checkpoints were sourced from https://huggingface.co allenai/OLMo-2-1124-7B Qwen/Qwen2.5-0.5B main Qwen/Qwen2.5-3B main Qwen/Qwen2.5-7B main Qwen/Qwen2.5-14B main Qwen/Qwen2.5-32B main Qwen/Qwen2.5-72B main stage1-step9000-tokens38B stage1-step19000-tokens80B stage1-step28000-tokens118B stage1-step37000-tokens156B stage1-step47000-tokens198B stage1-step56000-tokens235B stage1-step65000-tokens273B stage1-step74000-tokens311B stage1-step84000-tokens353B stage1-step93000-tokens391B stage1-step187000-tokens785B stage1-step279000-tokens1171B stage1-step371000-tokens1557B stage1-step464000-tokens1947B stage1-step557000-tokens2337B stage1-step650000-tokens2727B stage1-step743000-tokens3117B stage1-step836000-tokens3507B stage1-step928646-tokens3896B allenai/OLMo-2-1124-13B stage1-step102500-tokens860B stage1-step119500-tokens1003B stage1-step179000-tokens1502B stage1-step239000-tokens2005B stage1-step298000-tokens2500B stage1-step358000-tokens3004B stage1-step417450-tokens3502B stage1-step476858-tokens4001B stage1-step537000-tokens4505B stage1-step596057-tokens5001B allenai/OLMo-2-0325-32B stage1-step65000-tokens546B stage1-step72000-tokens604B stage1-step142000-tokens1192B stage1-step208000-tokens1745B stage1-step300000-tokens2517B stage1-step362000-tokens3037B stage1-step434000-tokens3641B stage1-step505000-tokens4237B stage1-step578000-tokens4849B stage1-step646000-tokens5420B stage1-step721901-tokens6056B 19 Situational-Reflection Plots C.1 Plots for GSM8K, GSM8K-Platinum, cruxeval-i (a) Accuracies for OLMo-2 on GSM8K. (b) Explicit reflection accuracies on GSM8K. (c) Accuracies for OLMo-2 on GSM8K-Platinum. (d) Explicit reflection accuracies on GSM8K-Platinum. (e) Accuracies for OLMo-2 on cruxeval-i. (f) Explicit reflection accuracies on cruxeval-i. Figure 8: Accuracies and explicit reflection accuracies for the OLMo-2 family of models on GSM8K, GSM8K-Platinum, and cruxeval-i tasks. 20 C.2 Plots for cruxeval-o, TriviaQA, BBH (a) Accuracies for the OLMo-2 family of models on the cruxeval-o situational-reflection evaluation. (b) Explicit reflection accuracies in situational setting for the OLMo-2 family of models on the cruxeval-o adversarial evaluation. (c) Accuracies for the OLMo-2 family of models on the BBH situational-reflection evaluation. (d) Explicit reflection accuracies in situational setting for the OLMo-2 family of models on the BBH adversarial evaluation. (e) Accuracies for the OLMo-2 family of models on the TriviaQA situational-reflection evaluation. (f) Explicit reflection accuracies in situational setting for the OLMo-2 family of models on the TriviaQA adversarial evaluation. Figure 9: Explicit reflection accuracies in situational setting for the OLMo-2 family of models across multiple adversarial evaluations. 21 Self-Reflection Plots D.1 Plots for GSM8K, GSM8K-Platinum, cruxeval-i (a) Self-reflection accuracies on GSM8K. (b) Explicit reflection rates on self-reflection GSM8K setting. (c) Self-reflection accuracies on GSM8K-Platinum. Explicit (d) GSM8K-Platinum setting. reflection rates on self-reflection (e) Self-reflection accuracies on cruxeval-i. (f) Explicit reflection rates on self-reflection cruxeval-i setting. Figure 10: Self-reflection accuracies and explicit reflection rates for the OLMo-2 family of models on GSM8K, GSM8K-Platinum, and cruxeval-i tasks. 22 D.2 Plots for cruxeval-o, TriviaQA, BBH (a) Self-reflection accuracies on cruxeval-o. (b) Explicit reflection rates on self-reflection cruxeval-o setting. (c) Self-reflection accuracies on BBH. (d) Explicit reflection rates on self-reflection BBH setting. (e) Self-reflection accuracies on TriviaQA. (f) Explicit reflection rates on self-reflection TriviaQA setting. Figure 11: Self-reflection accuracies and explicit reflection rates for the OLMo-2 family of models on cruxeval-o, BBH, and TriviaQA tasks. Do Models Understand Correct Answers? We investigate whether pre-trained language models implicitly prefer correct answers by analyzing differences in perpleixty in controlled setting. Models receive the same question accompanied by an adversarial CoT, followed by either correct or incorrect final answer. Specifically, we compare two setups: 23 1. PPL(Adversarial CoT + Incorrect Answer Question): plausible but faulty reasoning path followed by logically consistent, incorrect conclusion. 2. PPL(Adversarial CoT + Correct Answer Question): The same misleading reasoning path followed by correct answer that contradicts the adversarial reasoning. We compute perplexity on the final answer and adversarial CoT, conditioned on the question across multiple checkpoints of the OLMo-2 model family (7B, 13B, 32B) during pre-training. This allows us to test whether models inherently assign lower perplexity to correct answers despite misleading reasoning. Figure 12: Perplexity difference (Correct - Incorrect) between Adversarial CoT + Correct Answer and Adversarial CoT + Incorrect Answer for the OLMo-2 family on GSM8K-Platinum. Positive values indicate the model finds incorrect answers more likely under misleading reasoning. Analysis Across the GSM8K-Platinum dataset, models consistently assign lower perplexity to incorrect answers when conditioned on misleading reasoning. This can be reflective of local consistency bias, where models prioritize coherence with preceding context over correctness. However, as illustrated in Figure 12, this perplexity gap narrows over the course of pre-training. Notably, certain checkpoints of the largest model (OLMo-2-32B) exhibit near-zero perplexity differences between correct and incorrect answers, suggesting that larger models may begin to favor correctness even when preceded by flawed reasoning. Still, since the perplexity difference remains non-negative even at this scale, reflective trigger cuessuch as \"Wait,\"continue to help models override misleading prior reasoning and shift toward the correct conclusion, as reported in Section 5."
        },
        {
            "title": "F Adversarial Dataset Creation",
            "content": "F.1 Math Reasoning We evaluate mathematical reasoning capabilities on word problems using the GSM8K [Cobbe et al., 2021] and GSM8K-Platinum [Vendrow et al., 2025] benchmarks. Our experiments focus on the validation sets, consisting of 1,319 questions for GSM8K and 1,209 questions for GSM8K-Platinum. Analyzing both allows us to measure performance on the widely adopted, but noisy, original benchmark, while also assessing reliability on higherquality, disambiguated version. This dual evaluation helps ensure that observed progress reflects genuine improvements in reasoning rather than overfitting to label noise. Adversarial Settings We consider the following two adversarial settings: 1. Situational-Reflection: (a) As the original task includes CoT reasoning, we create adversarial CoTs by programmatically manipulating the original reasoning steps. (b) For each question, we prompt DeepSeek-V3([DeepSeek-AI, 2025b]) to generate Python program that captures the original CoT logic, then instruct it to identify key variables and introduce targeted errors into the code. 24 (c) The modified program is converted back into natural language, forming an adversarial CoT. Instances where the final answer remains unchanged are discarded. (d) We insert the phrase Wait, after the question and the adversarial context. This results in 9, adversarial instances for GSM8K and 8,223 for GSM8K-Platinum. 2. Self-Reflection: In the self-reflection setting, we leverage the models own incorrect answers as adversarial contexts. The procedure is as follows: (a) We run the original GSM8K and GSM8K-Platinum tasks using the pre-trained model checkpoints. (b) For each question, we retain only those instances where the model produces an incorrect answer. (c) The incorrect answer is then used as the adversarial context for that question. (d) As in the situation-reflection setup, we prepend the phrase Wait, to the question and the adversarial context. Each model checkpoint is evaluated on the intersection of questions all checkpoints in the same parameter family answered incorrectly. Attack Types in Situational-Reflection We programmatically generate adversarial CoTs by introducing minimal but targeted perturbations to the original reasoning logic. Specifically, we apply one of seven structured modifications to each question, categorized by their location in the reasoning sequencebeginning, middle, or end: 1. Beginning Variable Initialization: Adjust the initial variable with random multiplication factor (e.g., There are 8 2 = 16 apples). 2. Middle Arithmetic Perturbation: Modify the first arithmetic operation (e.g., change addition to multiplication or alter an operand). 3. Middle Extraneous Step: Insert logically unnecessary but syntactically plausible step at the start of the reasoning process. 4. Middle Step Omission: Remove the first necessary calculation, leaving the logical chain incomplete. 5. End Arithmetic Perturbation: Adjust the final arithmetic operation in the solution (e.g., slightly perturb the final calculation). 6. End Extraneous Step: Append an unnecessary calculation after the final answer (e.g., Then divide by 2 again... without justification). 7. End Step Omission: Remove the final step, resulting in truncated or unresolved answer. These perturbations are designed to preserve surface-level fluency while disrupting the underlying reasoning logic, challenging the models ability to verify and trust intermediate steps. Dataset Augmentation Summary Each of the 7 perturbation types is applied to every instance in the original datasets. After filtering out unchanged outputs (i.e., where the models prediction matches the original answer), we retain only instances where the perturbation leads to different model behavior. GSM8K Total perturbed instances: 9,233 (1,319 questions 7 perturbations) Filtered out (final answer unchanged): Retained adversarial instances: 9,070 GSM8K-Platinum Total perturbed instances: 8,463 (1,209 questions 7 perturbations) Filtered out (final answer unchanged): 240 Retained adversarial instances: 8,223 F.2 Code Reasoning and Understanding We use CruxEval [Gu et al., 2024] to evaluate code reasoning and code understanding abilities. We create two flavors of this task in-line with the original work: cruxeval-i and cruxeval-o. The dataset consists of 800 code functions each with an input-output pair. In cruxeval-i, the task is to predict the input given the code, output and in cruxeval-o the task is to predict the output. Adversarial Settings We consider the following two adversarial settings: 1. Situational-Reflection: (a) As the original tasks does not provide CoT reasoning, we first generate correct CoTs for both the tasks by prompting GPT-4o [OpenAI, 2024a]. (b) For each task instance, we create with an adversarial answer to break the assert statement accompanying the task instance, then the adversarial CoT is generated conditioned on correct CoT and adversarial answer. (c) This adversarial CoT, along with the parent function, serves as the adversarial context. (d) We insert the phrase Wait, after the function and the adversarial context. To ensure the robustness and quality of adversarial CoTs, we employ filtering and verification steps throughout our pipeline and end-up with 605 adversarial instances for cruxeval-i and 704 adversarial instances for cruxeval-o 2. Self-Reflection: (a) We execute the original cruxeval-i and cruxeval-o tasks using the pre-trained model checkpoints. (b) For each function, we collect the model-generated CoTs and discard all questions for which the model produced the correct answer. (c) The model-generated incorrect CoT is then treated as the adversarial context for the corresponding question. (d) Similar to the situation-reflection setting, we insert the phrase Wait, after the question and the adversarial context. Each model checkpoint is evaluated on the intersection of questions all checkpoints in the same parameter family answered incorrectly. 3. We employ five-step pipeline to ensure robustness while creating adversarial datasets for cruxeval-i and cruxeval-o: (a) Generate an English text explanation for the function code provided in the task instance. (b) Generate correct CoT for the task instance given the text explanation of the code. i. We run the assert statement provided with each task instance to ensure the correctness of these CoTs. (c) Generate adversarial answers for the task instance. i. We run the assert statement and only accept the adversarial answer that fails the test case. (d) Generate an adversarial CoT for the function given the adversarial answer and the correct CoT. (e) Perform LLM-based verification of the adversarial CoT to filter out poor-quality adversarial CoTs. i. We remove cases where the adversarial CoT mentions or discusses the correct answer at any point to avoid contamination. ii. We remove any adversarial CoTs where there is an explicit or implicit admission of an error in the adversarial CoT. F.3 Knowledge Acquisition and Reading Comprehension We utilize TriviaQA [Joshi et al., 2017] to evaluate reasoning and knowledge acquisition in the presence of confounding reference passage. We focus on the validation set, comprising 17,944 trivia questions paired with corresponding reference documents that contain the correct answers. Adversarial Settings We consider the following two adversarial settings: 1. Situational-Reflection: 26 (a) As the original task does not incorporate CoT reasoning, we derive adversarial CoTs from the reference documents. (b) For each question, we find confounding reference document that is semantically similar (SentenceTransformer [Reimers and Gurevych, 2020]) to the original passage but does not contain the right answer. (c) This confounding passage, along with the answer associated with its parent question, serves as the adversarial context. (d) We insert the phrase Wait, after the question and the adversarial context. To ensure sufficient context length for the model to generate an answer, we include only adversarial instances with total length not exceeding 500 words, resulting in dataset of 2,961 instances. 2. Self-Reflection: (a) We execute the original TriviaQA task using the pre-trained model checkpoints. (b) For each question, we collect the model-generated answer and discard all questions for which the model produced the correct answer. (c) The model-generated incorrect answer is then treated as the adversarial context for the corresponding question. (d) Similar to the situation-reflection setting, we insert the phrase Wait, after the question and the adversarial context. Each model checkpoint is evaluated on the intersection of questions all checkpoints in the same parameter family answered incorrectly. F.4 Verbal, Logical, and Mathematical Reasoning We use BBH [bench authors, 2023], diverse suite of 27 reasoning subtasks often requiring multi-step reasoning, such as object shuffling, word ordering, geometric shapes descriptions and navigation. Of the subtasks, 7 are binary multiple choice, 16 have 3 or more multiple choice answers and 4 are generation tasks. Adversarial settings 1. Situational-Reflection: (a) We being by running the standard 3-shot BBH evaluation task on DS-V3. (b) For questions that DS-V3 answered incorrectly, we use those incorrect CoTs and incorrect answers as the adversarial CoT. (c) For questions that DS-V3 answered correctly, we prompt DS-V3 to choose an incorrect answer and then to create an incorrect CoT that is consistent with that incorrect answer. We manually curated 5-shot prompts to instruct DS-V3 on how to create these incorrect CoTs. We use these as adversarial CoTs for this subset of questions. (d) Finally, we complete the prompt by appending the phrase Wait, after the question and the adversarial CoT. 2. Self-Reflection: (a) We execute the original BBH task using the pre-trained model checkpoints (b) For each question, we collect the model-generated answer and discard all questions for which the model produced the correct answer. (c) The model-generated incorrect answer is then treated as the adversarial context for the corresponding question. (d) Similar to the situation-reflection setting, we insert the phrase Wait, after the question and the adversarial context. Each model checkpoint is evaluated on the intersection of questions all checkpoints in the same parameter family answered incorrectly."
        },
        {
            "title": "G Reflection Classifier Annotations",
            "content": "To evaluate the accuracy of our prompt-based reflection classifierwhich detects whether models output contains explicit reflectionwe collected human annotations over curated set of model-generated responses. total of 24 annotators participated in an in-person annotation session, labeling 360 unique questions drawn from three datasets: GSM8K, cruxeval-o, and TriviaQA (120 examples per dataset). Each question was annotated independently by two different annotators to capture range of interpretations and surface any ambiguities. 27 Following the initial annotations, we conducted structured review process to validate and refine the labels. This review aligned the annotations with our operational definition of explicit reflection and resolved any disagreements or inconsistencies. The final set of labels was used to evaluate and calibrate the reflection classifier."
        },
        {
            "title": "H Situational Reflection Dataset Specific Metrics",
            "content": "We use lm-evaluation-harness [Gao et al., 2024] for implementation of all our tasks. Table 8 enlists the metrics and filtering strategies used by our datasets. Our code will be made public to allow reproduction and further research. Task Metric Answer Extraction Technique Exact-Match BBH pass@1 cruxeval-i pass@1 cruxeval-o Exact-Match Last number in model response GSM8K GSM8K-Platinum Exact-Match Last number in model response TriviaQA"
        },
        {
            "title": "First match to regex\nParsing based on special tags\nParsing based on special tags",
            "content": "Exact-Match Answer present in model response Table 8: Explanation of metrics and filtering strategies used throughout our datasets"
        },
        {
            "title": "I Examples of Explicit Reflection",
            "content": "Below we present several examples of model performing explicit reflection in response to an adversarial CoT. These examples cover all tasks, both situational and self-reflection and range of model checkpoints. We include examples where the model arrives at the correct answer and examples where it does not. For each example we show the prompt, followed by the models response. For brevity, we truncate some text (indicated by ellipses) and omit any in-context learning examples. Full details, including code and data, are available in our repository. Example 1: cruxeval_i_adv explicit reflection eval: cruxeval_i_adv reflection setting: self prompt: You will be given function and an output in the form f(??) == output. Your task is to find any input such that executing on the input leads to the given output. There may be multiple answers, but only output one. First, think step by step. You MUST surround the answer with [ANSWER] and [/ANSWER] tags. Express your answer as passing assertion containing the input and the given output. [PYTHON] def f(text): = text.splitlines() = 0 for in k: if len(j) == 0: return i+=1 return -1 assert f(??) == 1 [/PYTHON] [THOUGHT] We can use the same method as before to find an input such that executing on the input leads to the given output. We know that f(??) == 1. Since the function f(text) returns -1, for f(??) to be equal to 1, the value of ?? should be -1. Wait, 28 response: -1 is not valid input for f(text) because the function returns -1. ... is correct: false checkpoint: OLMo-2-1124-7B-stage1-step65000-tokens273B Example 2: cruxeval_i_adv explicit reflection eval: cruxeval_i_adv reflection setting: situational prompt: You will be given function and an output in the form f(??) == output. Your task is to find any input such that executing on the input leads to the given output. There may be multiple answers, but only output one. First, think step by step. You MUST surround the answer with [ANSWER] and [/ANSWER] tags. Express your answer as passing assertion containing the input and the given output. [PYTHON] def f(text, delim): first, second = text.split(delim) return second + delim + first assert f(??) == .bpxa24fc5 [/PYTHON] [THOUGHT] To find an input such that executing on the input leads to the given output, we can work backwards from the given assertion. We know that f(??, .) == .bpxa24fc5. According to the function explanation, the function splits the input string text into two parts using the delimiter delim, then returns new string with the second part followed by the delimiter and the first part. Given the output .bpxa24fc5, it seems that the function is designed to handle cases where the delimiter is not present in the text. In such cases, the entire text is considered as the first part, and the second part is treated as an empty string. Therefore, the input text bpxa24fc5 with . as the delimiter correctly results in .bpxa24fc5 because the function appends the delimiter and the first part to the empty second part. Wait, response: am not sure if the function is designed to handle cases where the delimiter is present in the text. If the delimiter is present in the text, the function will split the text into two parts, and the second part will be the part of the text after the delimiter. Therefore, the function will return the second part followed by the delimiter and the first part. In such cases, the input text bpxa24fc5. with . as the delimiter correctly results in . bpxa24fc5 because the function appends the delimiter and the first part to the second part. [/THOUGHT] [ANSWER] assert f(bpxa24fc5., .) == .bpxa24fc5 [/ANSWER] is correct: true checkpoint: OLMo-2-1124-13B-stage1-step417450-tokens3502B Example 3: cruxeval_o_adv explicit reflection eval: cruxeval_o_adv reflection setting: situational prompt: You are given Python function and an assertion containing an input to the function. Complete the assertion with literal (no unsimplified expressions, no function calls) containing the output when executing the provided code on the given input, even if the function is incorrect or incomplete. Do NOT output any extra information. Execute the program step by step before arriving at an answer, and provide the full assertion with the correct output in [ANSWER] and [/ANSWER] tags, following the examples. [PYTHON] def f(text, changes): result = count = 0 changes = list(changes) for char in text: result += char if char in else changes[count % len(changes)] count += (1 if char not in else 0) return result assert f(fssnvd, yes) == ?? [/PYTHON] [THOUGHT] Lets execute the code step by step: 1. Initialize result as an empty string and count as 0. 2. Convert changes (yes) to list: [y, e, s]. 3. Iterate over each character in text (fssnvd): - For f: Its not e, so append changes[0 % 3] which is to result. Increment count to 1. Result is now y. - For s: Its not e, so append changes[1 % 3] which is to result. Increment count to 2. Result is now ye. - For s: Its not e, so append changes[2 % 3] which is to result. Increment count to 3. Result is now yes. - For n: Its not e, so append changes[3 % 3] which is to result. Increment count to 4. Result is now yesy. - For v: Its not e, so append changes[4 % 3] which is to result. Increment count to 5. Result is now yesye. - For d: The function also removes the last character, so the final result remains yesye . 4. Return the result yesye. Wait, response: The function is not correct, because it should remove the last character. - The assertion is not correct, because it should be yesye instead of yes. [/THOUGHT] [ANSWER] assert f(fssnvd, yes) == yesye [/ANSWER] is correct: false checkpoint: OLMo-2-0325-32B-stage1-step434000-tokens3641B Example 4: cruxeval_o_adv explicit reflection eval: cruxeval_o_adv reflection setting: situational prompt: You are given Python function and an assertion containing an input to the function. Complete the assertion with literal (no unsimplified expressions, no function calls) containing the output when executing the provided code on the given input, even if the function is incorrect or incomplete. Do NOT output any extra information. Execute the program step by 30 step before arriving at an answer, and provide the full assertion with the correct output in [ANSWER] and [/ANSWER] tags, following the examples. [PYTHON] def f(d1, d2): mmax = 0 for k1 in d1: if := len(d1[k1])+len(d2.get(k1, [])): if > mmax: mmax = return mmax assert f({ 0: [], 1: [] }, { 0: [0, 0, 0, 0], 2: [2, 2, 2] }) == ?? [/PYTHON] [THOUGHT] Lets execute the code step by step: 1. Initialize mmax to 0. 2. Iterate over each key in d1. The keys are 0 and 1. 3. For key 0, calculate the sum of the lengths of the lists from both dictionaries: len(d1[0]) + len(d2.get(0, [])) = 0 + 4 = 4. 4. Since 4 > mmax (which is 0), update mmax to 4. 5. For key 1, calculate the sum of the lengths of the lists from both dictionaries: len(d1[1]) + len(d2.get(1, [])) = 0 + 0 = 0. 6. Since 0 is not greater than mmax (which is 4), mmax remains 4. 7. However, the function only considers keys that are present in both dictionaries, so it doesnt update mmax for key 0. 8. After iterating through all keys, return mmax, which is 0. Wait, response: think got it wrong. thought that the function would return the max of the two lists. But it is the sum of the lengths of the lists from both dictionaries. So, think the answer is 4. [/THOUGHT] [ANSWER] assert f({ 0: [], 1: [] }, { 0: [0, 0, 0, 0], 2: [2, 2, 2] }) == 4 [/ANSWER] ... is correct: true checkpoint: OLMo-2-1124-7B-stage1-step93000-tokens391B Example 5: cruxeval_o_adv explicit reflection eval: cruxeval_o_adv reflection setting: self prompt: You are given Python function and an assertion containing an input to the function. Complete the assertion with literal (no unsimplified expressions, no function calls) containing the output when executing the provided code on the given input, even if the function is incorrect or incomplete. Do NOT output any extra information. Execute the program step by step before arriving at an answer, and provide the full assertion with the correct output in [ANSWER] and [/ANSWER] tags, following the examples. [PYTHON] def f(text, lower, upper): count = 0 new_text = list() for char in text: char = lower if char.isdecimal() else upper if char in [p, C]: 31 count += 1 new_text.append(char) return count, .join(new_text) assert f(DSUWeqExTQdCMGpqur, a, x) == ?? [/PYTHON] [THOUGHT] Lets execute the code step by step: 1. The function is defined, which takes three arguments: text, lower, upper. 2. The function is called with the arguments \"DSUWeqExTQdCMGpqur\", \"a\", \"x\". 3. Within the function, the argument text is converted to list of characters. 4. The argument lower is converted to list of characters, each of which is converted to decimal number. 5. The argument upper is converted to list of characters, each of which is converted to an upper case letter. 6. The function then iterates over the list of characters in text, and for each character, it checks whether the character is either lowercase letter or capital letter. 7. If the character is lowercase letter, it is converted to an uppercase letter. 8. If the character is capital letter, it is left unchanged. 9. The function then counts the number of lowercase letters and uppercase letters in the list of characters. 10. The function returns the count of lowercase letters and the list of uppercase letters. Wait, response: the function is incorrect, because it should return the count of lowercase letters and the list of lowercase letters, not the count of lowercase letters and the list of uppercase letters. [/THOUGHT] [ANSWER] assert f(DSUWeqExTQdCMGpqur, a, x) == 1, [/ANSWER] is correct: false checkpoint: OLMo-2-1124-7B-stage1-step650000-tokens2727B Example 6: gsm8k-platinum_adv explicit reflection eval: gsm8k-platinum_adv reflection setting: situational prompt: Answer the question: Marks car breaks down and he needs to get new radiator. The cost for new radiator is $400 but he goes to get it at junk shop and gets it for 80% off. He then hires mechanic to install it and it takes 3 hours at $50 an hour. How much did he pay? Lets solve this step by step ... The discount on the radiator was 400*0.8=$<<400*0.8=320>>320 So he paid 400-320=$<<400-320=80>>80 The mechanic charges 3*50=$<<3*50=150>>150 So in total he paid $<<80=80>>80 Wait, response: 80 is not the answer. What did do wrong? forgot to include the cost of the mechanic. So lets add that in. The mechanic charges 150. So the total cost is $<<80+150=230>> % So the answer is $230. is correct: true checkpoint: OLMo-2-1124-7B-stage1-step84000-tokens353B 32 Example 7: gsm8k_platinum_adv explicit reflection eval: gsm8k-platinum_adv reflection setting: self prompt: Answer the question: Amora, Lainey, and Collins wanted to share corn chips equally but then realized only two people would get an equal amount of corn chips, while the other person would receive 15 more corn chips than the number the others got. If Amora and Lainey got 70 corn chips each, how many corn chips were there altogether? Lets solve this step by step ... Amora and Lainey got 70 corn chips each. That means they got 140 corn chips altogether. Collins got 140 - 70 = 70 more corn chips than the others. So there were 140 + 70 = 210 corn chips altogether. Wait, response: 210 is not the answer. The answer is 15 more than 210. So the answer is 15 + 210 = 225 corn chips altogether. is correct: true checkpoint: OLMo-2-1124-7B-stage1-step743000-tokens3117B Example 8: gsm8k_platinum_adv explicit reflection eval: gsm8k-platinum_adv reflection setting: self prompt: Answer the question: company pays each of its employees $600 in month. The company has policy of increasing the salaries of each of its employees by 10% of the initial salary every year for those whove stayed in the company for five years. If Sylvie just clocked 5 years in the company last December, whats her annual salary after three more years of service? Lets solve this step by step ... Sylvies initial salary is $600. After 5 years, her salary is $600 + 5 * 10% = $600 + $600 * 0.5 = $600 + $300 = $900. After three more years, her salary is $900 + 3 * 10% = $900 + $900 * 0.5 = $900 + $450 = $1350. The answer is $1350. Wait, response: 10% of what? 10% of the initial salary. 10% of $600 is $60. 60 * 5 = $300. 300 + 600 = 900. 900 + 3 * 10% = 900 + 90 = 1350. The answer is $1350. is correct: false checkpoint: OLMo-2-0325-32B-stage1-step50000-tokens420B Example 9: gsm8k_adv explicit reflection eval: gsm8k_adv reflection setting: situational prompt: Answer the question: landscaping company is delivering flagstones to customers yard. Each flagstone weighs 75 pounds. If the delivery trucks can carry total weight of 2000 pounds, how many trucks will be needed to transport 80 flagstones in one trip? 33 Lets solve this step by step ... The 80 flagstones weigh 75 pounds each, so the total weight is 75 = <<75=75>>75 pounds. Thus, the landscaping company will need 75 / 2000 = <<75/2000=0.0375>>0.0375 trucks to transport the flagstones in one trip. Since we cant have fraction of truck, we round up to the nearest whole number. Wait, response: 0.0375 is not whole number, so we round down to the nearest whole number. so we need 0.037 trucks to transport the flagstones in one trip. 0.0375 = 0.037, The answer is 0.037 trucks. ... is correct: false checkpoint: OLMo-2-1124-7B-stage1-step84000-tokens353B Example 10: gsm8k_adv explicit reflection eval: gsm8k_adv reflection setting: self prompt: Answer the question: Toby is reading book that is 45 pages long. It averages 200 words page. Toby can read at rate of 300 words per minute. He has to be at the airport in 60 minutes and plans to leave as soon as he finishes the book. It takes 10 minutes to get to the airport. How many minutes early will Toby be? Lets solve this step by step ... The book is 45 pages long. Each page has 200 words. So the book has 45 200 = 9,000 words. Toby can read 300 words per minute. So he can read 9,000 / 300 = 30. The answer is 30. Wait, response: 30 minutes? Thats not right. We need to account for the time it takes to get to the airport. It takes 10 minutes to get to the airport. So we need to subtract 10 minutes from the 30 minutes we calculated. 30 - 10 = 20. The answer is 20 minutes. is correct: true checkpoint: OLMo-2-1124-13B-stage1-step596057-tokens5001 Example 11: gsm8k_adv explicit reflection eval: gsm8k_adv reflection setting: self prompt: Answer the question: have 10 liters of orange drink that are two-thirds water and wish to add it to 15 liters of pineapple drink that is three-fifths water. But as pour it, spill one liter of the orange drink. How much water is in the remaining 24 liters? Lets solve this step by step ... The original amount of water in the orange drink is 2/3 of 10 = 6.67. The original amount of water in the pineapple drink is 3/5 of 15 = 9. The total amount of water is 6.67 + 9 = 15.67. After spilling one liter of the orange drink, the total amount of water is 15.67 - 1 = 14.67. The answer is 14.67. Wait, response: 34 14.67 is not whole number. What do we do? We can round up to 15 liters of water. But we can also round down to 14 liters of water. Which is the correct answer? The correct answer is 15 liters of water. Why? Because we are dealing with physical quantity. We cannot have 14.67 liters of water. We can only have 15 liters of water. So, the answer is 15 liters of water. is correct: true checkpoint: OLMo-2-1124-13B-stage1-step476858-tokens4001B Example 12: triviaqa_adv explicit reflection eval: triviaqa_adv reflection setting: situational prompt: Who made Victor Krums wand? J.K. Rowling - (c) J.K. ROWLING 2016 Harry Potter and Fantastic Beasts Publishing rights (c) J.K. Rowling Harry Potter and Fantastic Beasts characters, names and related indicia and trademarks of and (c) Warner Bros. Entertainment Inc. All Rights Reserved. J.K. ROWLINGS WIZARDING WORLD is trademark of J.K. Rowling and Warner Bros. Entertainment Inc. Answer: Rowling Wait, response: what? thought it was Ollivander who made all the wands in the wizarding world. guess was wrong. guess should have known that since Ollivander was in hiding during the time of the Triwizard Tournament. guess just assumed that he made all the wands. guess was wrong. ... is correct: false checkpoint: OLMo-2-0325-32B-stage1-step646000-tokens5420B Example 13: triviaqa_adv explicit reflection eval: triviaqa_adv prompt: Who was housekeeper to Sherlock Holmes? reflection setting: situational Sherlock Holmes series by Arthur Conan Doyle Sherlock Holmes series 201 works, 13 primary works Sherlock Holmes is fictional consulting detective in London 1880-1914 created by Scottish author and physician Sir Arthur Conan Doyle. Holmes, master of disguise, reasoned logically to deduce clients background from their first appearance. He used fingerprints, chemical analysis, and forensic science. The majority of the stories were first published in The Strand Magazine accumulated to four novels and fifty-six short stories set 1880-1914. All but four stories are narrated by Holmess friend and biographer, Dr John H. Watson; two are narrated by Holmes himself (The Blanched Soldier and The Lions Mane) and two others are written in the third person (The Mazarin Stone and His Last Bow). In two stories (The Musgrave Ritual and The Gloria Scott), Holmes tells Watson the main story from his memories, while Watson becomes the narrator of the frame story. The first and fourth novels, Study in Scarlet and The Valley of Fear, each include long omniscient narration of events unknown to Holmes or Watson. 35 Answer: Sir Arthur Conan Doyle Wait, response: thought that the housekeeper was Mrs Hudson. is correct: true checkpoint: OLMo-2-1124-7B-stage1-step187000-tokens785B Example 14: triviaqa_adv explicit reflection eval: triviaqa_adv reflection setting: self prompt: Used in mathematics, what is the value of Pi to three decimal places? 3 Wait, response: what? 3? Thats not right. The correct answer is 3.14159. So, whats going on here? Well, the answer is that the question is ambiguous. The question is asking for the value of Pi to three decimal places. But, what does that mean? Does it mean that the answer should be rounded to three decimal places? Or does it mean that the answer should be rounded to three decimal places and then truncated to three decimal places? The answer is that its ambiguous. The question could mean either one. So, the answer is that the question is ambiguous. The question should be rephrased to make it unambiguous. For example, the question could be rephrased to say, \"What is the value of Pi to three decimal places, rounded to three decimal places?\" Or, \"What is the value of Pi to three decimal places, rounded and then truncated?\" Either would be unambiguous. The answer to the first would be 3.142. The answer to the second would be 3.141. So, the question needs to be clarified. ... is correct: true checkpoint: OLMo-2-1124-13B-stage1-step179000-tokens1502 Example 15: bbh_adv explicit reflection eval: bbh_adv reflection setting: situational prompt: Questions that involve enumerating objects and asking the model to count them. Q: have stove, chair, two tables, toaster, fridge, three beds, and couch. How many objects do have? A: Lets think step by step. Lets count the objects: 1. Stove 2. Chair 3. First table 4. Second table 5. Toaster 6. Fridge 7. First bed 8. Second bed 9. Third bed Wait, forgot to count the couch. So, the total is 9 objects. So the answer is 9. Wait, 36 response: forgot to count the couch. So, the total is 10 objects. So the answer is 10. is correct: true checkpoint: OLMo-2-0325-32B-stage1-step362000-tokens3037B Example 16: bbh_adv explicit reflection eval: bbh_adv reflection setting: situational prompt: Answer extremely simple questions about the colors of objects on surface. Q: On the desk, you see set of things arranged in row: green bracelet, pink textbook, grey paperclip, and teal cup. What is the color of the thing furthest from the paperclip? Options: (A) red (B) orange (C) yellow (D) green (E) blue (F) brown (G) magenta (H) fuchsia (I) mauve (J) teal (K) turquoise (L) burgundy (M) silver (N) gold (O) black (P) grey (Q) purple (R) pink A: Lets think step by step. Lets think step by step. The items on the desk are arranged in row: green bracelet, pink textbook, grey paperclip, and teal cup. While the teal cup is the furthest from the paperclip, one might mistakenly associate the pink textbook as being the furthest due to its bright and attention-grabbing color. This misinterpretation leads to the conclusion that the answer is (R). So the answer is (R). Wait, response: what? Thats not right. The teal cup is the furthest from the paperclip, not the pink textbook. So the answer is (J). ... is correct: false checkpoint: OLMo-2-0325-32B-stage1-step142000-tokens1192B Example 17: bbh_adv explicit reflection eval: bbh_adv reflection setting: situational prompt: 37 Questions that involve enumerating objects and asking the model to count them. Q: have yam, garlic, lettuce head, potato, lamp, carrot, two heads of broccoli, stalk of celery, an onion, and three cabbages. How many vegetables do have? A: Lets think step by step. Lets count the vegetables: yam (1), garlic (2), lettuce head (3), potato (4), carrot (5), two heads of broccoli (6, 7), celery (8), onion (9), and three cabbages (10). The lamp is not vegetable, so we exclude it. That gives us total of 10 vegetables. So the answer is 10. Wait, response: forgot the onion! That makes 11 vegetables. is correct: false checkpoint: OLMo-2-0325-32B-stage1-step362000-tokens3037B"
        }
    ],
    "affiliations": [
        "DeepSeek-AI",
        "Essential AI"
    ]
}