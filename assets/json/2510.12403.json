{
    "paper_title": "Robot Learning: A Tutorial",
    "authors": [
        "Francesco Capuano",
        "Caroline Pascal",
        "Adil Zouitine",
        "Thomas Wolf",
        "Michel Aractingi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Robot learning is at an inflection point, driven by rapid advancements in machine learning and the growing availability of large-scale robotics data. This shift from classical, model-based methods to data-driven, learning-based paradigms is unlocking unprecedented capabilities in autonomous systems. This tutorial navigates the landscape of modern robot learning, charting a course from the foundational principles of Reinforcement Learning and Behavioral Cloning to generalist, language-conditioned models capable of operating across diverse tasks and even robot embodiments. This work is intended as a guide for researchers and practitioners, and our goal is to equip the reader with the conceptual understanding and practical tools necessary to contribute to developments in robot learning, with ready-to-use examples implemented in $\\texttt{lerobot}$."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 3 0 4 2 1 . 0 1 5 2 : r Robot Learning: Tutorial"
        },
        {
            "title": "Michel Aractingi",
            "content": "University of Oxford,"
        },
        {
            "title": "Abstract",
            "content": "Robot learning is at an inflection point, driven by rapid advancements in machine learning and the growing availability of large-scale robotics data. This shift from classical, model-based methods to data-driven, learning-based paradigms is unlocking unprecedented capabilities in autonomous systems. This tutorial navigates the landscape of modern robot learning, charting course from the foundational principles of Reinforcement Learning and Behavioral Cloning to generalist, language-conditioned models capable of operating across diverse tasks and even robot embodiments. This work is intended as guide for researchers and practitioners, and our goal is to equip the reader with the conceptual understanding and practical tools necessary to contribute to developments in robot learning, with ready-to-use examples implemented in lerobot. Code: https://github.com/huggingface/lerobot Date: October 15,"
        },
        {
            "title": "Contents",
            "content": "1 Introduction 1.1 LeRobotDataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . The dataset class design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.1.1 1.2 Code Example: Batching (Streaming) Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.3 Code Example: Collecting Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "2.1 Explicit and Implicit Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.2 Different Types of Motion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.3 Example: Planar Manipulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.3.1 Adding Feedback Loops . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.4 Limitations of Dynamics-based Robotics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "3 4 4 5 6 9 9 10 10"
        },
        {
            "title": "3 Robot (Reinforcement) Learning",
            "content": "16 3.1 (Concise) Introduction to RL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 3.2 Real-world RL for Robotics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 3.2.1 Code Example: Real-world RL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 3.2.2 Limitations of RL in Real-World Robotics: Simulators and Reward Design . . . . . . . . . . ."
        },
        {
            "title": "4 Robot (Imitation) Learning",
            "content": "33 4.1 (Concise) Introduction to Generative Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 Variational Auto-Encoders . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 4.1.1 4.1.2 Diffusion Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 Flow Matching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 4.1.3 4.2 Action Chunking with Transformers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 4.2.1 Code Example: Training and Using ACT in Practice . . . . . . . . . . . . . . . . . . . . . . . . 46 4.3 Diffusion Policy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 4.3.1 Code Example: Training and Using Diffusion Policies in Practice . . . . . . . . . . . . . . . ."
        },
        {
            "title": "5 Generalist Robot Policies",
            "content": "57 5.1 Preliminaries: Models and Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 5.2 VLAs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60 5.2.1 VLMs for VLAs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 5.4 SmolVLA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64 5.4.1 Code Example: Using SmolVLA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "5.3.1 Code Example: Using π0",
            "content": "5.3 π"
        },
        {
            "title": "Foreword",
            "content": "67 Robotics is an inherently multidisciplinary field, which is witnessing unprecedented advancements since its inception in the 1960s. Yet, more than sixty years after the debut of Unimate, robots have still not fully integrated into the rich, unstructured, and dynamic world we humans inhabit. Over the decades, numerous disciplines have shown immense promise in tackling the challenges of creating autonomous robotic systems. This tutorial takes clear stance in the debate on whether modern Machine Learning can play pivotal role in the development of autonomous robots: we believe this to be the case. Nonetheless, we also hold that the wealth of research from both academia and industry in classical robotics over the past six decades is, simply put, too valuable to be cast aside in favor of purely learning-based methods. However, the interplay between classical robotics and modern machine learning is still in its nascent stages, and the path to integration yet to be clearly defined. In turn our goal here is to present what we consider to be the most relevant approaches within robot learning today, while warmly extending an invite to collaborate to expand the breadth of this work! Start contributing today here. This tutorial. . . Does not aim to be comprehensive guide to general field of robotics, manipulation or underactuated systems: Siciliano and Khatib (2016) and Tedrake (a,b) do this better than we ever could. Does not aim to be an introduction to statistical or deep learning: Shalev-Shwartz and Ben-David (2014) and Prince (2023) cover these subjects better than we ever could. Does not aim to be deep dive into Reinforcement Learning, Diffusion Models, or Flow Matching: invaluable works such as Sutton and Barto (2018), Nakkiran et al. (2024), and Lipman et al. (2024) do this better than we ever could. Instead, our goal here is to provide an intuitive explanation as per why these disparate ideas have converged to form the exciting field of modern robot learning, driving the unprecedented progress we see today. In this spirit, we follow the adage: \"a jack of all trades is master of none, but oftentimes better than master of one.\" We sincerely hope this tutorial serves as valuable starting point for your journey into robot learning. 2 Figure 1 lerobot is the open-source library for end-to-end robotics developed by Hugging Face. The library is vertically integrated on the entire robotics stack, supporting low-level control of real-world robot devices, advanced data and inference optimizations, as well as SOTA robot learning methods with simple implementations in pure Pytorch."
        },
        {
            "title": "Introduction",
            "content": "Autonomous robotics holds the premise of relieving humans from repetitive, tiring or dangerous manual tasks. Consequently, the field of robotics has been widely studied since its first inception in the 1950s. Lately, advancements in Machine Learning (ML) have sparked the development of relatively new class of methods used to tackle robotics problems, leveraging large amounts of data and computation rather than human expertise and modeling skills to develop autonomous systems. The frontier of robotics research is indeed increasingly moving away from classical model-based control paradigm, embracing the advancements made in ML, aiming to unlock (1) monolithic perception-to-action control pipelines and (2) multi-modal data-driven feature extraction strategies, together with (3) reduced reliance on precise models of the world and (4) better positioning to benefit from the growing availability of open robotics data. While central problems in manipulation, locomotion and whole-body control demand knowledge of rigid-body dynamics, contact modeling, planning under uncertainty, recent results seem to indicate learning can prove just as effective as explicit modeling, sparking interest in the field of robot learning. This interest can be largely justified considering the significant challenges related to deriving accurate models of robot-environment interactions. Moreover, since end-to-end learning on ever-growing collections of text and image data has historically been at the core of the development of foundation models capable of semantic reasoning across multiple modalities (images, text, audio, etc.), deriving robotics methods grounded in learning appears particularly consequential, especially as the number of openly available datasets continues to grow. Robotics is, at its core, an inherently multidisciplinary field, requiring wide range of expertise in both software and hardware. The integration of learning-based techniques further broadens this spectrum of skills, raising the bar for both research and practical applications. lerobot is an open-source library designed to integrate end-to-end with the entire robotics stack. With strong focus on accessible, real-world robots (1) lerobot supports many, openly available, robotic platforms for manipulation, locomotion and even whole-body control. lerobotalso implements (2) unified, low-level approach to reading/writing robot configurations to extend support for other robot platforms with relatively low effort. The library introduces LeRobotDataset, (3) native robotics datasets format currently being used by the community to efficiently record and share datasets. lerobot also supports many state-of-the-art (SOTA) algorithms in robot learningmainly based on Reinforcement Learning (RL) and Behavioral Cloning (BC) techniqueswith efficient implementations in Pytorch, and extended support to experimentation and experiments tracking. Lastly, lerobot defines custom, optimized inference stack for robotic policies decoupling action planning from action execution, proving effective in guaranteeing more adaptability at runtime. This tutorial serves the double purpose of providing useful references for the Science behindand practical use ofcommon robot learning techniques. To this aim, we strike to provide rigorous yet concise overview of the core concepts behind the techniques presented, paired with practical examples of how to use such techniques concretely, with code examples in lerobot, for researchers and practitioners interested in the field of robot learning. This tutorial is structured as follows: Section 2 reviews classical robotics foundations, introducing the limitations of dynamics-based approaches to robotics. 3 Section 3 elaborates on the limitations of dynamics-based methods, and introduce RL as practical approach to solve robotics problems, considering its upsides and potential limitations. Section 4 further describes robot learning techniques that aim at solving single-tasks learning, leveraging BC techniques to autonomously reproduce specific expert demonstrations. Section 5 presents recent contributions on developing generalist models for robotics applications, by learning from large corpora of multi-task & multi-robot data (robotics foundation models). Our goal with this tutorial is to provide an intuitive explanation of the reasons various disparate ideas from Machine Learning (ML) have converged and are powering the current evolution of Robotics, driving the unprecedented progress we see today. We complement our presentation of the most common and recent approaches in robot learning with practical code implementations using lerobot, and start here by presenting the dataset format introduced with lerobot."
        },
        {
            "title": "1.1 LeRobotDataset\nLeRobotDataset is one of the most impactful features of lerobot, developed in keeping with the observation that\nrobotics data is increasingly central in robot learning. Thus, lerobot defines a standardized dataset format designed to\naddress the specific needs of robot learning research, providing a unified and convenient access to robotics data across\nmodalities, including sensorimotor readings, multiple camera feeds and teleoperation status. LeRobotDataset also\naccommodates for storing general information regarding the data being collected, including textual descriptions of the\ntask being performed by the teleoperator, the kind of robot used, and relevant measurement specifics like the frames\nper second at which the recording of both image and robot state’s streams are proceeding.",
            "content": "In this, LeRobotDataset provides unified interface for handling multi-modal, time-series data, and it is designed to seamlessly integrate with the PyTorch and Hugging Face ecosystems. LeRobotDataset can be easily extended by users and it is highly customizable by users, and it already supports openly available data coming from variety of embodiments supported in lerobot, ranging from manipulator platforms like the SO-100 arm and ALOHA-2 setup, to real-world humanoid arm and hands, as well as entirely simulation-based datasets, and self-driving cars. This dataset format is built to be both efficient for training and flexible enough to accommodate the diverse data types encountered in robotics, while promoting reproducibility and ease of use for users."
        },
        {
            "title": "1.1.1 The dataset class design\nA core design choice behind LeRobotDataset is separating the underlying data storage from the user-facing API.\nThis allows for efficient storage while presenting the data in an intuitive, ready-to-use format.",
            "content": "Datasets are always organized into three main components: Tabular Data: Low-dimensional, high-frequency data such as joint states, and actions are stored in efficient memorymapped files, and typically offloaded to the more mature datasets library by Hugging Face, providing fast with limited memory consumption. Visual Data: To handle large volumes of camera data, frames are concatenated and encoded into MP4 files. Frames from the same episode are always grouped together into the same video, and multiple videos are grouped together by camera. To reduce stress on the file system, groups of videos for the same camera view are also broke into multiple sub-directories, after given threshold number. Metadata collection of JSON files which describes the datasets structure in terms of its metadata, serving as the relational counterpart to both the tabular and visual dimensions of data. Metadata include the different feature schema, frame rates, normalization statistics, and episode boundaries. For scalability, and to support datasets with potentially millions of trajectories (resulting in hundreds of millions or billions of individual camera frames), we merge data from different episodes into the same high-level structure. Concretely, this means that any given tabular collection and video will not typically contain information about one episode only, but rather concatenation of the information available in multiple episodes. This keeps the pressure on the file system limited, both locally and on remote storage providers like Hugging Face, though at the expense of leveraging more heavily relational-like, metadata parts of the dataset, which are used to reconstruct information such as at which position, in given file, an episode starts or ends. An example struture for given LeRobotDataset would appear as follows: meta/info.json: This metadata is central metadata file. It contains the complete dataset schema, defining all 4 features (e.g., observation.state, action), their shapes, and data types. It also stores crucial information like the datasets frames-per-second (fps), lerobots version at the time of capture, and the path templates used to locate data and video files. meta/stats.json: This file stores aggregated statistics (mean, std, min, max) for each feature across the entire dataset, used for data normalization for most policy models and accessible externally via dataset.meta.stats. meta/tasks.jsonl: This file contains the mapping from natural language task descriptions to integer task indices, which are useful for task-conditioned policy training. meta/episodes/* This directory contains metadata about each individual episode, such as its length, the corresponding task, and pointers to where its data is stored in the datasets files. For scalability, this information is stored in files rather than single large JSON file. data/*: Contains the core frame-by-frame tabular data, using parquet files to allow for fast, memory-mapped access. To improve performance and handle large datasets, data from multiple episodes are concatenated into larger files. These files are organized into chunked subdirectories to keep the size of directories manageable. single file typically contains data for more than one single episode. videos/*: Contains the MP4 video files for all visual observation streams. Similar to the data/ directory, the video footage from multiple episodes is concatenated into single MP4 files. This strategy significantly reduces the number of files in the dataset, which is more efficient for modern filesystems."
        },
        {
            "title": "1.2 Code Example: Batching a (Streaming) Dataset\nThis section provides an overview of how to access datasets hosted on Hugging Face using the LeRobotDataset class.\nEvery dataset on the Hugging Face Hub containing the three main pillars presented above (Tabular, Visual and\nrelational Metadata), and can be assessed with a single instruction.",
            "content": "In practice, most reinforcement learning (RL) and behavioral cloning (BC) algorithms tend to operate on stack of observation and actions. For the sake of brevity, we will refer to joint spaces, and camera frames with the single term of frame. For instance, RL algorithms may use history of previous frames otHo:t to mitigate partial observability, and BC algorithms are in practice trained to regress chunks of multiple actions (at+t+Ha ) rather than single controls. To accommodate for these specifics of robot learning training, LeRobotDataset provides native windowing operation, whereby users can define the seconds of given window (before and after) around any given frame, by using the delta_timestemps functionality. Unavailable frames are opportunely padded, and padding mask is also returned to filter out the padded frames. Notably, this all happens within the LeRobotDataset, and is entirely transparent to higher level wrappers commonly used in training ML models such as torch.utils.data.DataLoader. Conveniently, by using LeRobotDataset with Pytorch DataLoader one can automatically collate the individual sample dictionaries from the dataset into single dictionary of batched tensors for downstream training or inference. LeRobotDataset also natively supports streaming mode for datasets. Users can stream data of large dataset hosted on the Hugging Face Hub, with one-line change in their implementation. Streaming datasets supports high-performance batch processing (ca. 80-100 it/s, varying on connectivity) and high levels of frames randomization, key features for practical BC algorithms which otherwise may be slow or operating on highly non-i.i.d. data. This feature is designed to improve on accessibility so that large datasets can be processed by users without requiring large amounts of memory and storage. Code 1: Batching (Streaming) Dataset https://github.com/fracapuano/robot-learning-tutorial/blob/main/snippets/ch1/01_datasets.py 1 import torch 2 from lerobot . datasets . lerobot_dataset import LeRobotDataset 3 from lerobot . datasets . st rea min g_d ata set import e n R t a 4 5 el ta_ ime stamps = { # 0.2 , and 0.1 seconds * before * each frame \" observation . images . wrist_camera \" : [ -0.2 , -0.1 , 0.0] 7 8 } 9 10 # Optionally , use e n R t a to avoid downloading the dataset 11 dataset = LeRobotDataset ( 5 \" lerobot / a _ 1 0 1 _ k c \" , lta _ timestamps = delta_timestamps 12 13 14 ) 15 16 # Streams frames from the Hugging Face Hub without loading into memory 17 tr mi ng _ at as et = e n R t a ( \" lerobot / a _ 1 0 1 _ k c \" , lta _ timestamps = delta_timestamps 18 19 20 ) 21 22 # Get the 100 th frame in the dataset by 23 sample = dataset [100] 24 print ( sample ) 25 # { 26 # ' observation . state ': tensor ([...]) , 27 # ' action ': tensor ([...]) , 28 # ' observation . images . wrist_camera ': tensor ([3 , , , ]) , for delta timesteps 29 # ... 30 # } 31 32 batch_size =16 33 # wrap the dataset in DataLoader to use process it batches for training purposes 34 data_loader = torch . utils . data . DataLoader ( dataset , batch_size = batch_size 35 36 37 ) 38 39 # Iterate over the DataLoader in training loop 40 num_epochs = 1 41 device = \" cuda \" if torch . cuda . is_available () else \" cpu \" 42 43 for epoch in range ( num_epochs ): 45 46 47 48 49 51 for batch in data_loader : # Move data to the appropriate device ( . . , GPU ) observations = batch [ \" observation . state \" ]. to ( device ) actions = batch [ \" action \" ]. to ( device ) images = batch [ \" observation . images . wrist_camera \" ]. to ( device ) # Next , you can do amazing_model . forward ( batch ) ..."
        },
        {
            "title": "1.3 Code Example: Collecting Data",
            "content": "Code 2: Record Dataset https://github.com/fracapuano/robot-learning-tutorial/blob/main/snippets/ch1/02_record_data.py 1 \" \" \" 2 You can also use the CLI to record data . To see the required arguments , run : 3 lerobot - record -- help 4 \" \" \" 5 from lerobot . cameras . opencv . f r o _ n import pe nC Ca er Co fi 6 from lerobot . datasets . lerobot_dataset import LeRobotDataset 7 from lerobot . datasets . utils import _ _ a _ t s 8 from lerobot . robots . so100_follower import SO100Follower , SO 1 0 0 l r nf 9 from lerobot . teleoperators . so100_leader . f _ so 1 0 0 _ d import O10 0Le ade rCo nfi 10 from lerobot . teleoperators . so100_leader . so100_leader import SO100Leader 11 from lerobot . utils . control_utils import t _ b d _ t r 12 from lerobot . utils . utils import log_say 13 from lerobot . utils . ua a n _ l import init_rerun 14 from lerobot . scripts . lerobot_record import record_loop 15 16 NUM_EPISODES = 5 17 FPS = 30 18 EP SOD _TI E_SEC = 60 19 RES ET_TIME_SEC = 10 6 20 TA K_D SCR PTION = ... # provide task description 21 22 HF_USER = ... # provide your Hugging Face username 23 24 follower_port = ... 25 leader_port = ... 26 follower_id = ... 27 leader_id = ... # find your ports running : lerobot - find - port # to load the calibration file 28 29 # Create the robot and teleoperator configurations 30 camera_config = { \" front \" : en VC me aC nf ( index_or_path =0 , width =640 , height =480 , fps = FPS ) 31 32 } 33 robot_config = 1 0 0 Fo w o g ( 34 35 port = follower_port , id = follower_id , cameras = camera_config 36 37 ) 38 teleop_config = O1 00L ead erC onf ig ( port = leader_port , 39 40 id = leader_id 41 ) 42 43 # Initialize the robot and teleoperator 44 robot = SO100Follower ( robot_config ) 45 teleop = SO100Leader ( teleop_config ) 46 47 # Configure the dataset features 48 ac ti n_ fea tures = _ _ a _ t s ( robot . action_features , \" action \" ) 49 obs_features = _ _ a _ t s ( robot . observation_features , \" observation \" ) 50 da ase _fe tures = {** action_features , ** obs_features } 51 52 # Create the dataset where to store the data 53 dataset = LeRobotDataset . create ( repo_id = \" { HF_USER }/ robot - learning - tutorial - data \" , fps = FPS , features = dataset_features , robot_type = robot . name , use_videos = True , g _ t _ e =4 , 54 55 56 57 58 59 60 ) 61 62 # Initialize the keyboard listener and rerun visualization 63 _ , events = t _ b d _ t r () 64 init_rerun ( session_name = \" recording \" ) 65 66 # Connect the robot and teleoperator 67 robot . connect () 68 teleop . connect () 69 70 episode_idx = 0 71 while episode_idx < NUM_EPISODES and not events [ \" stop_recording \" ]: 72 73 75 76 77 78 79 81 82 83 84 85 87 88 89 log_say ( \" Recording episode { episode_idx + 1} of { NUM_EPISODES } \" ) record_loop ( robot = robot , events = events , fps = FPS , teleop = teleop , dataset = dataset , control_time_s = EPISODE_TIME_SEC , single_task = TASK_DESCRIPTION , display_data = True , ) # Reset the environment if not stopping or re - recording if ( not events [ \" stop_recording \" ]) and ( episode_idx < NUM_EPISODES - 1 or events [ \" rerecord_episode \" ]): log_say ( \" Reset the environment \" ) record_loop ( 7 90 92 93 94 95 96 98 99 100 101 102 104 105 106 107 robot = robot , events = events , fps = FPS , teleop = teleop , control_time_s = RESET_TIME_SEC , single_task = TASK_DESCRIPTION , display_data = True , ) if events [ \" rerecord_episode \" ]: log_say ( \" Re - recording episode \" ) events [ \" rerecord_episode \" ] = False events [ \" exit_early \" ] = False dataset . a _ s _ f () continue dataset . save_episode () episode_idx += 1 108 109 # Clean up 110 log_say ( \" Stop recording \" ) 111 robot . disconnect () 112 teleop . disconnect () 113 dataset . push_to_hub () 8 Figure 2 Overview of methods to generate motion (clearly non-exhausitve, see Bekris et al. (2024)). The different methods can be grouped based on whether they explicitly (dynamics-based ) or implicitly (learning-based ) model robot-environment interactions."
        },
        {
            "title": "2 Classical Robotics",
            "content": "Know your enemy [...]"
        },
        {
            "title": "Sun Tzu",
            "content": "TL;DR Learning-based approaches to robotics are motivated by the need to (1) generalize across tasks and embodiments (2) reduce dependency on human expertise (3) leverage historical trends on the production of dataall traditionally overlooked by dynamics-based techniques."
        },
        {
            "title": "2.1 Explicit and Implicit Models\nRobotics is concerned with producing artificial motion in the physical world in useful, reliable and safe fashion. Thus,\nrobotics is an inherently multi-disciplinar domain: producing autonomous motion in the physical world requires, to the\nvery least, interfacing different software (motion planners) and hardware (motion executioners) components. Further,\nknowledge of mechanical, electrical, and software engineering, as well as rigid-body mechanics and control theory\nhave therefore proven quintessential in robotics since the field first developed in the 1950s. More recently, Machine\nLearning (ML) has also proved effective in robotics, complementing these more traditional disciplines (Connell and\nMahadevan, 1993). As a direct consequence of its multi-disciplinar nature, robotics has developed as a rather wide\narray of methods, all concerned with the main purpose of producing artificial motion in the physical world.",
            "content": "Methods to produce robotics motion range from traditional explicit modelsdynamics-based1 methods, leveraging precise descriptions of the mechanics of robots rigid bodies and their interactions with eventual obstacles in the environmentto implicit modelslearning-based methods, treating artificial motion as statistical pattern to learn given multiple sensorimotor readings (Agrawal; Bekris et al., 2024). variety of methods have been developed between these two extrema. For instance, Hansen et al. (2022) show how learning-based systems can benefit from information on the physics of problems, complementing traditional learning method such as Temporal Difference (TD)-learning Sutton and Barto (2018) with Model-Predictive Control (MPC). Conversely, as explicit models may be relying on assumptions proving overly simplisticor even unrealisticin practice, learning can prove effective to improve modeling of complex phenomena or complement perception (McCormac et al., 2016). Such examples 1In here, we refer to both kinematics and dynamics-based control. 9 Figure 3 Different kinds of motions are achieved with potentially very different robotic platforms. From left to right, top to bottom: ViperX, SO-100, Boston Dynamics Spot, Open-Duck, 1Xs NEO, Boston Dynamics Atlas. This is an example list of robotic platforms and is (very) far from being exhaustive. aim at demonstrating the richness of approaches to robotics, and Figure 2 graphically illustrates some of the most relevant techniques. Such list is clearly far from being exhaustive, and we refer to Bekris et al. (2024) for more comprehensive overview of both general and application-specific methods for motion generation. In this section, we wish to introduce the inherent benefits of learning-based approaches to roboticsthe core focus on this tutorial."
        },
        {
            "title": "2.2 Different Types of Motion\nIn the vast majority of instances, robotics deals with producing motion via actuating joints connecting nearly\nentirely-rigid links. A key distinction between focus areas in robotics is based on whether the generated motion\nmodifies (1) the absolute state of the environment (via dexterity), (2) the relative state of the robot with respect to\nits environment (exercising mobility skills), or (3) a combination of the two (Figure 3).",
            "content": "Effects such as (1) are typically achieved through the robot, i.e. generating motion to perform an action inducing desirable modification, effectively manipulating the environment (manipulation). Motions like (2) may result in changes in the robots physical location within its environment. Generally, modifications to robots location within its environment may be considered instances of the general locomotion problem, further specified as wheeled or legged locomotion based on whenever robot makes use of wheels or leg(s) to move in the environment. Lastly, an increased level of dynamism in the robot-environment interactions can be obtained combining (1) and (2), thus designing systems capable to interact with and move within their environment. This category is problems is typically termed mobile manipulation, and is characterized by typically much larger set of control variables compared to either locomotion or manipulation alone. The traditional body of work developed since the very inception of robotics is increasingly complemented by learningbased approaches. ML has indeed proven particularly transformative across the entire robotics stack, first empowering planning-based techniques with improved state estimation used for traditional planning (Tang et al., 2023) and then end-to-end replacing controllers, effectively yielding perception-to-action methods (Kober et al.). Work in producing robots capable of navigating diverse set of terrains demonstrated the premise of both dynamics and learning-based approaches for locomotion (Griffin et al., 2017; Ji et al., 2023; Lee et al., 2020; Margolis et al., 2022), and recent works on whole-body control indicated the premise of learning-based approaches to generate rich motion on complex robots, including humanoids (Zhang et al., 2024; Bjorck et al., 2025). Manipulation has also been widely studied, particularly considering its relevance for many impactful use-cases ranging from high-risk applications for humans (Fujita et al., 2020; Alizadeh and Zhu, 2024) to manufacturing (Sanneman et al., 2020). While explicit models have proven fundamental in achieving important milestones towards the development of modern robotics, recent works leveraging implicit models proved particularly promising in surpassing scalability and applicability challenges via learning (Kober et al.)."
        },
        {
            "title": "2.3 Example: Planar Manipulation\nRobot manipulators typically consist of a series of links and joints, articulated in a chain finally connected to an\nend-effector. Actuated joints are considered responsible for generating motion of the links, while the end effector is\ninstead used to perform specific actions at the target location (e.g., grasping/releasing objects via closing/opening a\ngripper end-effector, using a specialized tool like a screwdriver, etc.).",
            "content": "10 Figure 4 Cheaper, more accessible robots are starting to rival traditional platforms like the Panda arm platforms in adoption in resource-constrained scenarios. The SO-100, in particular, has cost in the 100s of Euros, and can be entirely 3D-printed in hours, while the industrially-manufactured Panda arm costs tens of thousands of Euros and is not openly available. Figure 5 The SO-100 arm is 6-dof manipulator arm. Preventing some of its joints (shoulder pane, wrist flex and wrist roll) from actuating, it can be represented as traditional 2-dof planar manipulator (the gripper joint in the end-effector is not considered towards the count of the degrees of freedom used to produce motion). Recently, the development of low-cost manipulators like the ALOHA (Zhao et al., 2023) ALOHA-2 (Aldaco et al.) and SO-100/SO-101 (Knight et al.) platforms significantly lowered the barrier to entry to robotics, considering the increased accessibility of these robots compared to more traditional platforms like the Franka Emika Panda arm (Figure 4). Deriving an intuition as per why learning-based approaches are gaining popularity in the robotics community requires briefly analyzing traditional approaches for manipulation, leveraging tools like forward and inverse kinematics (FK, IK) and control theory. Providing detailed overview of these methods falls (well) out of the scope of this tutorial, and we refer the reader to works including Siciliano and Khatib (2016); Lynch and Park (2017); Tedrake (a,b) for much more comprehensive description of these techniques. Here, we mostly wish to highlight the benefits of ML over these traditional techniques Consider the (simple) case where SO-100 is restrained from actuating (1) the shoulder pane and (2) the wrist flex and roll motors. This effectively reduces the degrees of freedom of the SO-100 from the original 5+1 (5 joints + 1 gripper) to 2+1 (shoulder lift, elbow flex + gripper). As the end-effector does not impact motion in this model, the SO-100 is effectively reduced to the planar manipulator robot presented in Figure 5, where spheres represent actuators, and solid lines indicate length-l links from the base of the SO-100 to the end-effector (ee). Further, let us make the simplifying assumption that actuators can produce rotations up to 2π radians. In practice, this is seldom the case due to movement obstructions caused by the robot body itself (for instance, the shoulder lift cannot produce counter-clockwise movement due to the presence of the robots base used to secure the SO-100 to its support and host the robot bus), but we will introduce movement obstruction at later stage. All these simplifying assumptions leave us with the planar manipulator of Figure 6a, free of moving its endeffector by controlling the angles θ1 and θ2, jointly referred to as the robots configuration, and indicated with = [θ1, θ2] [π, +π]2. The axis attached to the joints indicate the associated reference frame, whereas circular 11 (a) Free to move (b) Constrained by the surface (c) Constrained by surface and (fixed) obstacle Figure 6 Planar, 2-dof schematic representation of the SO-100 manipulator under diverse deployment settings. From left to right: completely free of moving; constrained by the presence of the surface; constrained by the surface and presence of obstacles. Circular arrows around each joint indicate the maximal rotation feasible at that joint. arrows indicate the maximal feasible rotation allowed at each joint. In this tutorial, we do not cover topics related to spatial algebra, and we instead refer the reader to Lynch and Park (2017, Chapter 2) and Tedrake (a, Chapter 3) for excellent explanations of the mechanics and theoretical foundations of producing motion on rigid bodies. Considering the (toy) example presented in Figure 6a, then we can analytically write the end-effectors position R2 as function of the robots configuration, = p(q), : (cid:55) R2. In particular, we have: p(q) = (cid:32) px(θ1, θ2) py(θ1, θ2) (cid:33) = (cid:32) cos(θ1) + cos(θ1 + θ2) sin(θ1) + sin(θ1 + θ2) (cid:33) Sn=2 l1+l2 = {p(q) R2 : p(q) 2 (2l)2, Q} Deriving the end-effectors poseposition and orientationin some m-dimensional space Rm starting from the configuration Rn of n-joints robot is referred to as forward kinematics (FK), whereas identifying the configuration corresponding to any given target pose is termed inverse kinematics (IK). In that, FK is used to map robot configuration into the corresponding end-effector pose, whereas IK is used to reconstruct the configuration(s) given an end-effector pose. In the simplified case here considered (for which p, as the orientation of the end-effector is disregarded for simplicity), one can solve the problem of controlling the end-effectors location to reach goal position by solving analytically for : p(q) = fFK(q) = p. However, in the general case, one might not be able to solve this problem analytically, and can typically resort to iterative optimization methods comparing candidate solutions using loss function (in the simplest case, p(q) p2 2 is natural candidate), yielding: min qQ p(q) p2 2 . (1) Exact analytical solutions to IK are even less appealing when one considers the presence of obstacles in the robots workspace, resulting in constraints on the possible values of [π, +π]n Rn in the general case of n-links robots. For instance, the robot in Figure 6b is (very naturally) obstacled by the presence of the surface upon which it rests: θ1 can now exclusively vary within [0, π], while possible variations in θ2 depend on θ1 (when θ1 0 or θ1 π, further downwards movements are restricted). Even for simplified kinematic model, developing techniques to solve eq. 1 is in general non-trivial in the presence of constraints, particularly considering that the feasible set of solutions may change across problems. Figure 6c provides an example of how the environment influences the feasible set considered, with new set of constraints deriving from the position of new obstacle. However, IKsolving eq. 1 for feasible qonly proves useful in determining information regarding the robots configuration in the goal pose, and crucially does not provide information on the trajectory to follow over time to reach target pose. Expert-defined trajectories obviate to this problem providing length-K succession of goal poses τK = [p K] for tracking. In practice, trajectories can also be obtained automatically through motion planning algorithms, thus avoiding expensive trajectory definition from human experts. However, tracking τK via IK can prove prohibitively expensive, as tracking would require resolutions of eq. 1 (one for each target pose). Differential inverse kinematics (diff-IK) complements IK via closed-form solution of variant of eq. 1. Let J(q) denote 1, . . . 0, 12 the Jacobian matrix of (partial) derivatives of the FK-function fFK : (cid:55) P, such that J(q) = fF (q) . Then, one can apply the chain rule to any p(q) = fFK(q), deriving = J(q) q, and thus finally relating variations in the robot configurations to variations in pose, thereby providing platform for control. Given desired end-effector trajectory p(t) (1) indicating anchor regions in space and (2) how much time to spend in each region, diff-IK finds q(t) solving for joints velocities instead of configurations, q(t) = arg min ν J(q(t))ν p(t)2 2 (2) Unlike eq. 1, solving for is much less dependent on the environment (typically, variations in velocity are constrained by physical limits on the actuators). Conveniently, eq. 2 also often admits the closed-form solution = J(q)+ p, where +(q) denotes the Moore-Penrose pseudo-inverse of J(q). Finally, discrete-time joint configurations can be reconstructed from joint velocities using forward-integration on the continuous-time joint velocity , qt+1 = qt + qt for given t, resulting in tracking via diff-IK. Following trajectories with diff-IK is valid option in well-controlled and static environments (e.g., industrial manipulators in controlled manufacturing settings), and relies on the ability to define set of target velocities to track [ k]an error-prone task largely requiring human expertise. Furthermore, diff-IK relies on the ability to (1) access J(q) and (2) compute its pseudo-inverse at every iteration of given control cyclea challenging assumption in highly dynamical settings, or for complex kinematic chains. 1, . . . , 0, p"
        },
        {
            "title": "2.3.1 Adding Feedback Loops\nWhile very effective when a goal trajectory has been well specified, the performance of diff-IK can degrade significantly\nin the presence of modeling/tracking errors, or in the presence of non-modeled dynamics in the environment.",
            "content": "One such case is presented in Figure 7, where another rigid body other than the manipulator is moving in the environment along the horizontal axis, with velocity xB. Accounting analytically for the presence of this disturbancefor instance, to prevent the midpoint of the link from ever colliding with the objectrequires access to xB at least, to derive the equation characterizing the motion of the environment. Less predictable disturbances however (e.g., xB xB + ε, ε (0, 1)) may prove challenging to model analytically, and one could attain the same result of preventing link-object collision by adding condition on the distance between the midpoint of and xB, enforced through feedback loop on the position of the robot and object at each control cycle. Figure 7 Planar manipulator robot in the presence of moving obstacle. To mitigate the effect of modeling errors, sensing noise and other disturbances, classical pipelines indeed do augment diff-IK with feedback control looping back quantities of interest. In practice, following trajectory with closed feedback loop might consist in backwarding the error between the target and measured pose, = p(q), hereby modifying the control applied to = J(q)+( + kpp), with kp defined as the (proportional) gain. More advanced techniques for control consisting in feedback linearization, PID control, Linear Quatratic Regulator (LQR) or Model-Predictive Control (MPC) can be employed to stabilize tracking and reject moderate perturbations, and we refer to Siciliano and Khatib (2016, Chapter 8) for in-detail explanation of these concepts, or (Tedrake, a, Chapter 8) for simple, intuitive example in the case of point-mass system. Nonetheless, feedback control presents its challenges as well: tuning gains remains laborious and system-specific. Further, manipulation tasks present intermittent contacts inducing hybrid dynamics (mode switches) and discontinuities in the Jacobian, challenging the stability guarantees of the controller and thus often necessitating rather conservative gains and substantial hand-tuning. We point the interested reader to Siciliano and Khatib (2016, Chapter 2,7,8), Lynch and Park (2017, Chapter 6,11), and Tedrake (a, Chapter 3,8) for extended coverage of FK, IK, diff-IK and control for (diff-)IK."
        },
        {
            "title": "2.4 Limitations of Dynamics-based Robotics\nDespite the last 60+ years of robotics research, autonomous robots are still largely incapable of performing tasks at\nhuman-level performance in the physical world generalizing across (1) robot embodiments (different manipulators,",
            "content": "13 Figure 8 Dynamics-based approaches to robotics suffer from several limitations: (1) orchestrating multiple components poses integration challenges; (2) the need to develop custom processing pipelines for the sensing modalities and tasks considered hinders scalability; (3) simplified analytical models of physical phenomena (here friction at the gripper; credits to Antonova et al. (2017)) limit real-world performance. Lastly, (4) dynamics-based methods overlook trends in the availability and growth of robotics data. different locomotion platforms, etc.) and (2) tasks (tying shoe-laces, manipulating diverse set of objects). While essential in the early development of robotics, the aforementioned methods require significant human expertise to be used in practice, and are typically specific to particular applicative problem. Dynamics-based robotics pipelines have historically been developed sequentially, engineering the different blocks now within most architectures for specific purposes. That is, sensing, state estimation, mapping, planning, (diff-)IK, and low-level control have been traditionally developed as distinct modules with fixed interfaces. Pipelining these specific modules proved error-prone, and brittleness emergesalongside compounding errorswhenever changes incur (e.g., changes in lighting for sensing, occlusion/failure of sensors, control failures). Adapting such stack to new tasks or robotic platforms often entails re-specifying objectives, constraints, and heuristics at multiple stages, incurring significant engineering overhead. Moreover, classical planners operate on compact, assumed-sufficient state representations; extending them to reason directly over raw, heterogeneous and noisy data streams is non-trivial. This results in limited scalability to multimodal data and multitask settings, as incorporating high-dimensional perceptual inputs (RGB, depth, tactile, audio) traditionally required extensive engineering efforts to extract meaningful features for control. Also, the large number of tasks, coupled with the adoption of per-task planners, goal parameterizations, and safety constraints, results in an explosion in design and validation options, with little opportunity to reuse solutions across tasks. Setting aside integration and scalability challenges: developing accurate modeling of contact, friction, and compliance for complicated systems remains difficult. Rigid-body approximations are often insufficient in the presence of deformable objects, and relying on approximated models hinders real-world applicability of the methods developed. 14 In the case of complex, time-dependent and/or non-linear dynamics, even moderate mismatches in parameters, unmodeled evolutions, or grasp-induced couplings can qualitatively affect the observed dynamics. Lastly, dynamics-based methods (naturally) overlook the rather recent increase in availability of openly-available robotics datasets. The curation of academic datasets by large centralized groups of human experts in robotics (ONeill et al., 2025; Khazatsky et al., 2025) is now increasingly complemented by growing number of robotics datasets contributed in decentralized fashion by individuals with varied expertise. If not tangentially, dynamics-based approaches are not posed to maximally benefit from this trend, which holds the premise of allowing generalization in the space of tasks and embodiments, like data was the cornerstone for advancements in vision (Alayrac et al., 2022) and natural-language understanding (Brown et al., 2020). Taken together, these limitations (Figure 8) motivate the exploration of learning-based approaches that can (1) integrate perception and control more tightly, (2) adapt across tasks and embodiments with reduced expert modeling interventions and (3) scale gracefully in performance as more robotics data becomes available. 15 Figure 9 Learning-based robotics streamlines perception-to-action by learning (1) unified high-level controller capable to take (2) high-dimensional, unstructured sensorimotor information. Learning (3) does not require dynamics model and instead focuses on interaction data, and (4) empirically correlates with the scale of the data used."
        },
        {
            "title": "3 Robot (Reinforcement) Learning",
            "content": "Approximate the solution, not the problem [...]"
        },
        {
            "title": "Richard Sutton",
            "content": "TL;DR The need for expensive, high-fidelity simulators can be obviated learning from real-world data, using sampleefficient algorithms that can safely train directly on hardware. Learning-based techniques for robotics naturally address the limitations presented in Section 2 (Figure 9). In particular, learning-based techniques typically rely on monolithich prediction-to-action pipelines (visuomotor policies) which do directly map sensorimotor inputs to predicted actions, streamlining control policies by removing the need to interface multiple components. Mapping sensory inputs to actions also makes it possible to incorporate diverse input modalities, leveraging the automatic feature extraction capabilities of modern learning systems. Moreover, learning-based approaches can, in principle, bypass explicit modeling altogether and instead rely solely on interaction dataan advantage that proves transformative when dynamics are difficult to model or entirely unknown. Lastly, learning for robotics (robot learning) is naturally well posed to leverage the growing amount of robotics data openly available, just as computer vision and natural language processing did historically benefit from large-scale corpora of data, in great part overlooked by dynamics-based approaches. Being field at its relative nascent stages, no prevalent technique(s) proves distinctly better than any other in the 16 Figure 11 Examples of two different robotics tasks performed using RL. In the manipulation task (A) an agent learns to reach for yellow plastic block in its environment, and to put it inside of box. In the locomotion task (B) an agent learns to move its center of mass sideways without falling. domain of robot learning. Still, two major classes of methods gained prominence: Reinforcement Learning (RL) and Behavioral Cloning (BC) (Figure 10). In this section, we provide conceptual overview of applications of RL to robotics, as well as introduce practical examples of how to use RL within lerobot. We then introduce the major limitations RL suffers from, to introduce BC techniques in Section 4 and Section sec:learning-foundation. In Figure 10 we deliberately include generalist robot models (Black et al., 2024; Shukor et al., 2025) alongside task-specific BC methods. While significantly different in spiritgeneralist models are language-conditioned and use instructions to generate motion valid across many tasks, while task-specific models are typically not language-conditioned and used to perform single taskfoundation models are still largely trained to reproduce trajectories contained in (large) training set of input demonstrations. Thus, we argue generalist policies can indeed be grouped alongside other task-specific BC methods, as they both leverage similar training data and schemas. Figure 10 illustrates this categorization graphically, explicitly listing all the robot learning policies currently available in lerobot: Action Chunking with Transformers (ACT) (Zhao et al., 2023), Diffusion Policy (Chi et al., 2024), Vector-Quantized Behavior Transformer (VQ-BeT) (Lee et al., 2024), π0 (Black et al., 2024), SmolVLA (Shukor et al., 2025), Human-in-the-loop Sample-efficient RL (HILSERL) (Luo et al., 2024) and TD-MPC (Hansen et al., 2022). Applications of RL to robotics have been studied long enough that the relationship between these two disciplines has been compared to that of physics and matematics (Kober et al.). Indeed, due to their inherently interactive and sequential nature, robotics control problems can be directly cast as RL problems. Figure 11 presents two of such cases. Reaching for an object to then move it somewhere else in the scene is sequential problem where over time the controller needs to adjust the position of the robot arm based on the current configuration and the (possibly varying) position of the object. Figure 11 also shows an example of locomotion problem, where sequentiality is inherent in the problem formulation: while sliding to the side, the controller needs to keep adjusting to the robots to avoid failure (falling). Overview of the Figure 10 robot learning methods implemented in lerobot. All algorithms are implemented in Pytorch. References: Zhao et al. (2023); Chi et al. (2024); Lee et al. (2024); Black et al. (2024); Shukor et al. (2025); Luo et al. (2024); Hansen et al. (2022) (top-to-bottom, left-to-right)."
        },
        {
            "title": "3.1 A (Concise) Introduction to RL\nThe RL framework (Sutton and Barto, 2018), which we briefly introduce here, has often been used to tackle robotics\nproblems (Kober et al.). RL is a subfield within ML fundamentally concerned with the development of autonomous\nsystems (agents) capable to continuously behave in an evolving environment, developing (ideally, well-performing)\ncontrol strategies (policies). Crucially for robotics, RL agents improve through trial and error, bypassing explicit\nmodels of the problem dynamics in favor of interaction data. In RL, this feedback loop between actions and outcomes\n(Figure 12) is established through the agent sensing a scalar quantity (reward ) measuring how desirable a given\ntransition is for the accomplishment of its goal.\nFormally, interactions between an agent and its environment are typically modeled via a Markov Decision Pro-",
            "content": "17 Figure 12 Agent-Environment interaction diagram (image credits to Sutton and Barto (2018)). cess (MDP) (Bellman, 1957). Representing robotics problems via MDPs offers several advantages, including (1) incorporating uncertainty through MDPs inherently stochastic formulation and (2) providing theoretically-sound framework for learning without an explicit model of the environment dynamics. While accommodating continuous time formulation too, MDPs are typically considered in discrete time in RL, assuming interactions to atomically take place at discrete timestep = 0, 1, 2, 3, . . . , . MDPs allowing for an unbounded number of interactions (T +) are termed infinite-horizon, and opposed to finite-horizon MDPs in which is finite. Unless diversely specified, we will only be referring to discrete-time finite-horizon (episodic) MDPs. Formally, lenght-T Markov Decision Process (MDP) is tuple = S, A, D, r, γ, ρ, , where: is the state space; st denotes the (possibly non-directly observable) environment state at time t. In robotics, states often comprise robot configuration and velocities (qt, qt), and can also accomodate sensor readings such as camera or audio streams. is the action space; at may represent joint torques, joint velocities, or even end-effector commands at timestep t. In general, actions correspond to commands intervenings on the configuration of the robot. represents the (possibly non-deterministic) environment dynamics, with : (cid:55) [0, 1], (st, at, st+1) = P(st+1st, at). For instance, for planar manipulator dynamics could be considered deterministic when the environment is fully described (Figure 6a), and stochastic when unmodeled disturbances depending on nonobservable parameters intervene (Figure 7). : is the reward function, weighing the transition (st, at, st+1) in the context of the achievement of an arbitrary goal. For instance, simple reward function for quickly moving along the axis (Figure 11) could be based on the absolute position of the robot along the axis (pxt ), present negative penalties for falling over (measured from pzt ) and introduce bonuses pxt for speed, r(st, at, st+1) r(st) = pxt pxt 1 pzt . Lastly, γ [0, 1) represent the discount factor regulating preference for immediate versus long-term reward (with an effective horizon equal to 1 ), and ρ is the distribution over for the MDPs initial, s0 ρ. 1γ Therefore, length-T trajectory is the (random) sequence τ = (s0, a0, r0, s1, a1, r1, . . . , sT 1, aT 1, rT 1, sT ), with per-step rewards defined as rt = r(st, at, st+1) for ease of notation. Interestingly, assuming both the environment dynamics and conditional distribution over actions given statesi.e., the policyto be Markovian: (3) P(st+1st, at, st1, at1, . . . s0, a0) = P(st+1st, at) P(atst, at1, st1, s0, a0) = P(atst), the probability of observing given trajectory τ factorizes into: P(τ ) = P(s0) 1 (cid:89) t=0 P(st+1st, at) P(atst). (4) (5) (6) Policies P(atst) are typically indicated as π(atst), often parametrized via θ, yielding πθ(atst), and are traine by optimizing the (discounted) return associated to given τ , i.e. the (random) sum of measured rewards over an arbitrary trajectory, G(τ ) = 1 (cid:88) t=0 γtrt. 18 Figure 13 Popular RL algorithms. See Achiam (2018) for complete list of citations. In that, agents seek to learn control strategies (policies, πθ) maximizing the expected return Eτ πθ G(τ ). For given dynamics Di.e., for given problemtaking the expectation over the (possibly random) trajectories resulting from acting according to certain policy provides direct, goal-conditioned ordering in the space of all the possible policies Π, yielding the (maximization) target : Π (cid:55) J(πθ) = Eτ Pθ;D [G(τ )], 1 (cid:89) Pθ;D(τ ) = ρ D(st, at, st+1) πθ(atst). (7) (8) t=0 Crucially, in the RL framework the agent is assumed to only observe the environment dynamics and not to intervene on them, and thus eq. 7 varies exclusively with the policy followed. In turn, MDPs naturally provide framework to optimize over the space of the possible behaviors an agent might enact (π Π), searching for the optimal policy π = arg maxθ J(πθ), where θ is the parametrization adopted by the policy set Π : πθ Π, θ. Besides providing target for policy search, G(τ ) can also be used to discriminate between states st and st, at pairs. Given any state Se.g., given configuration of robotthe state-value function (cid:12)s0 = s(cid:3) Vπ(s) = Eτ π (cid:2)G(τ )(cid:12) can be used to discriminate between desirable and undesirable state in terms of long-term (discounted) reward maximization, under given policy π. Similarily, the state-action value function also conditions the cumulative discounted reward on selecting action when in s, and thereafter act according to π, Importantly, value functions are interrelated: Qπ(s, a) = Eτ π (cid:2)G(τ )(cid:12) (cid:12)s0 = s, a0 = a(cid:3). Qπ(st, at) = Est+1P(st,at)[rt + γVπ(st+1)] Vπ(st) = Eatπ(st)[Qπ(st, at)], (9) (10) inducing an ordering over states and state-action pairs under π, and value functions are thus central to most RL algorithms. variety of algorithms have been developed in RL attempting to find (approximate) solutions to the problem of maximizing cumulative reward (we report some in Figure 13). Popular approaches to continuous state and action spacesuch as those studied within roboticsinclude Schulman et al. (2017a, TRPO), Schulman et al. (2017b, PPO) and Haarnoja et al. (2018, SAC). Across manipulation (Akkaya Figure 14 Simulated (left) vs. real-world (right) OpenDuck. Discrepancies in the simulation dynamics (reality gap) pose risks to policy transfer. et al., 2019) and locomotion problems (Lee et al., 2020), RL proved extremely effective in providing platform to (1) leverage unified, streamlined perception-to-action pipeline, (2) natively integrate propioperception with multi-modal high-dimensional sensory streams (3) disregard description of the environment dynamics, by focusing on observed interaction data rather than modeling, and (4) anchor policies in the experience collected and stored in datasets. For more complete survey of applications of RL to robotics, we refer the reader to Kober et al.; Tang et al. (2025)."
        },
        {
            "title": "3.2 Real-world RL for Robotics\nStreamlined end-to-end control pipelines, data-driven feature extraction and a disregard for explicit modeling in favor\nof interaction data are all features of RL for robotics. However, RL still suffers from limitations concerning safety and\nlearning efficiency, particularly pressing for real-world robotics applications.",
            "content": "First, especially early in training, actions are typically explorative, and thus may be erractic. On physical systems, untrained policies may command high velocities, self-collisiding configurations, or torques exceeding joint limits, leading to wear and potential hardware damage. Mitigating these risks requires external safeguards (e.g., watchdogs, safety monitors, emergency stops), often incuring in high degree of human supervision. Further, in the typical episodic setting considered in most robotics problems, experimentation is substantially slowed down by the need to manually reset the environment over the course of training, time-consuming and error-prone process. Second, learning efficiently remains problematic in RL, limiting the applicability of RL in real-world robotics due to consequently prohibitive timescales of training. Even strong algorithms such as SAC (Haarnoja et al., 2018) typically require large numbers of transitions {(st, at, rt, st+1)}N . On real-world hardware, generating this data is time-consuming. Training RL policies in simulation (Tobin et al., 2017) addresses both issues, eliminating physical risk and dramatically increasing throughput. Yet, simulators require significant modeling effort, and rely on assumptions (simplified physical modeling, instantaneous actuation, static environmental conditions, etc.) limiting the possibilities to transfer the policies learned in simulation, due the discrepancy between real and simulated environments (reality gap, Figure 14). Domain randomization (Tobin et al., 2017) (DR) is popular technique to overcome the reality gap, and consists in randomizing the parameters of the simulated environment during training, aiming at inducing robustness to specific disturbances. In this, DR is typically employed to increase the diversity of scenarios over the course of training, improving on the performace sim-to-real transferred policies (Akkaya et al., 2019; Antonova et al., 2017; Ji et al., 2023). In practice, DR is performed training in simulation on simulated dynamics D, further parametrized as Dξ, with dynamics (random) vector ξ drawn an arbitrary distribution, ξ Ξ. For instance, one could decide to randomize the friction coefficient of the surface in locomotion task (Figure 15), or the center of mass of an object for manipulation task. Over the course of trainingtypically at each episodes reseta new ξ is drawn, and used to specify the environments dynamics for that episode. t=1 While effective in transfering policies across the reality gap in real-world robotics (Tobin et al., 2017; Akkaya et al., 2019; Ji et al., 2023; Tiboni et al., 2024), DR often requires extensive manual engineering. First, identifying which 20 Figure 15 The same locomotion task can be carried out in different (simulated) domains (exemplified by the difference in terrains) at training time, resulting to increased robustness over diverse environment dynamics. parameters to randomizei.e., the support supp(Ξ) of Ξis an inherently task specific process. When locomoting over different terrains, choosing to randomize the friction coefficient is reasonable choice, yet not completely resolutive as other factors (lightning conditions, external temperature, joints fatigue, etc.) may prove just as important in practice, making selecting these parameters yet another source of brittlness. Selecting the dynamics distribution Ξ is also non-trivial. On the one hand, distributions with low entropy might risk to cause failure at transfer time, due to the limited robustness induced over the course of training. On the other hand, excessive randomization may cause over-regularization and hinder performance (Margolis et al., 2022). Consequently, the research community investigated approaches to automatically select the randomization distribution Ξ, using signals from the training process or tuning it to reproduce observed real-world trajectories. Akkaya et al. (2019) use parametric uniform distribution U(a, b) as Ξ, widening the bounds a, as training progresses and the agents performance improves (AutoDR). While effective, AutoDR requires significant tuningthe bounds are widened by fixed, pre-specified amount alongand may disregard data when performance does not improve after distribution update (Tiboni et al., 2024). Tiboni et al. (2024) propose similar method to AutoDR (DORAEMON) to evolve Ξ based on the training signal, but with the key difference of explicitly maximizing the entropy of parametric Beta distributioninherently more flexible than uniform distributionswith learned updates instead of fixed . In this, DORAEMON proves particularly effective at dynamically increasing the entropy levels of the training distribution by employing an outer-loop max-entropy objective, tackled under performance constraints in the inner-loop RL problem. Other approaches to automatically perform DR consist in specifically tuning Ξ to align as much as possible the simulation and real-world domains. For instance, Chebotar et al. (2019) interleave in-simulation policy training with repeated real-world policy rollouts used to adjust Ξ based on real-world data, while Tiboni et al. (2023) leverage single, pre-collected set of real-world trajectories and tune Ξ under simple likelihood objective. While DR has shown promise, it does not address the main limitation that, even under the assumption that an ideal distribution Ξ was available, many robotics problems cannot be simulated with high-enough fidelity under practical computational constraints. Simulating contact-rich manipulation of possibly deformable or soft materialsi.e., folding piece of clothingcan prove time-intensive, limiting the benefits of in-simulation training. perhaps more foundamental limitation of RL for robotics is the general unavailability of complicated tasks dense reward function, the design of which is essentially based on human expertise, ingenuity and trial-and-error. In practice, sparse reward functions can be used to conclude whether one specific goal has been attainedhas this t-shirt been correctly folded? but unfortunately incur in more challenging learning. As result, despite notable successes, deploying RL directly on real-world robots at scale remains challenging. To make the most of (1) the growing number of openly available datasets and (2) relatively inexpensive robots like the SO-100, RL could (1) be anchored in already-collected trajectorieslimiting erratic and dangerous explorationand (2) train in the real-world directlybypassing the aforementioned issues with low-fidelity simulations. In such context, sample-efficient learning is also paramount, as training on the real-world is inherently time-bottlenecked. Off-policy algorithms like Soft Actor-Critic (SAC) (Haarnoja et al., 2018) tend to be more sample efficient then their on-policy counterpart (Schulman et al., 2017b), due to the presence replay buffer used over the course of training. Other than allowing to re-use past transitions (st, at, rt, st+1), the replay buffer can also accomodate for the injection of previously-collected data in the training process (Ball et al., 2023). Using expert demonstrations to guide learning together with learned rewards, RL can be effectively carried out in the real-world (Luo et al., 2025). Interestingly, when complemented with in-training human interventions, real-world RL agents have been shown to learn policies with near-perfect success rates on challenging manipulation tasks in 1-2 hours (Luo et al., 2024). 21 In an MDP, the optimal policy π can be derived from its associated Q-function, Qπ , Sample-efficient RL and in particular the optimal action(s) µ(st) can be selected maximizing the optimal Q-function over the action space, µ(st) = max atA Q(st, at). Interestingly, the Q-function satisfies recursive relationship (Bellman equation) based on very natural intuition2: [...] If the optimal value Q(st+1, at+1) of the [state] st+1 was known for all possible actions at+1, then the optimal strategy is to select the action at+1 maximizing the expected value of rt + γQ(st+1, at+1) Q(st, at) = Est+1P(st,at) (cid:20) rt + γ max at+1A Q(st+1, at+1)(cid:12) (cid:12)st, at (cid:21) In turn, the optimal Q-function is guaranteed to be self-consistent by definition. Value-iteration methods exploit this relationship (and/or its state-value counterpart, (st) ) by iteratively updating an initial estimate of Q, Qk using the Bellman equation as update rule (Q-learning): Qi+1(st, at) Est+1P(st,at) (cid:20) rt + γ max at+1A Qi(st+1, at+1)(cid:12) (cid:12)st, at (cid:21) , = 0, 1, 2, . . . , Then, one can derive the (ideally, near-optimal) policy by explicitly maximizing over the action space the final (ideally, near-optimal) estimate QK at each timestep. Indeed, one can show that under certain assumptions on the MDP considered, QK as . Effective in its early applications to small-scale discrete problems, vanilla Q-learning was found complicated to scale to large problems, in which storing : (cid:55) alone might result prohibitive. Also, vanilla Q-learning is not directly usable for continuous, unstructured state-action space MPDs, such as those considered in robotics. In their seminal work on Deep Q-Learning (DQN), Mnih et al. (2013) propose learning Q-values using deep convolutional neural networks, thereby accomodating for large and even unstructured state spaces. DQN parametrizes the Q-function using neural network with parameters θ, updating the parameters by sequentially minimizing the expected squared temporal-difference error (TD-error, δi): L(θi) = E(st,at)χ() yi = Est+1P(st,at) )2(cid:3), (cid:2)(yi Qθi(st, at) (cid:125) (cid:123)(cid:122) (cid:124) δi (cid:2)rt + γ max Qθi1 (st+1, at+1)(cid:3), atA (11) (12) where χ represents behavior distribution over state-action pairs. Crucially, χ can in principle be different from the policy being followed, effectively allowing to reuse prior data stored in replay buffer in the form of (st, at, rt, st+1) transitions, used to form the TD-target yi, TD-error δi and loss function eq. 11 via Monte-Carlo (MC) estimates. While effective in handling large, unstructured state spaces for discrete action-space problems, DQNs application to continous control problems proved challenging. Indeed, in the case of high-capacity function approximators such as neural networks, solving maxatA Qθ(st, at) at each timestep is simply unfeasible due to the (1) continous nature of the action space (A Rn for some n) and (2) impossibility to express the policy with cheap (ideally, even closed-form) formulation, so that max Qθ could be solved analytically. Silver et al. (2014) tackle these fundamental challenges by using deterministic function of the state st as policy, µϕ(st) = at, parametrized by ϕ. Thus, policies can be iteratively refined updating ϕ along the direction: dϕ = EstP() (cid:2)ϕQ(st, at)at=µϕ(st) (cid:3) = EstP() (cid:2)atQ(st, at)at=µϕ(st) ϕµ(st)(cid:3) (13) Provably, eq. 13 is the deterministic policy gradient (DPG) of the policy µϕ (Silver et al., 2014), so that updates ϕk+1 ϕk + αdϕ are guaranteed to increase the (deterministic) cumulative discounted reward, J(µϕ). Lillicrap et al. (2019) extended DPG to the case of (1) high-dimensional unstructured observations and (2) continuous action spaces, introducing Deep Deterministic Policy Gradient (DDPG), an important algorithm in RL and its applications to robotics. DDPG adopts modified TD-target compared to eq. 12, by maintaining policy network used to select actions, yielding yi = Est+1P(st,at) (cid:2)rt + γQθi1 (st+1, µϕ(st+1))(cid:3). (14) 2Quote from Mnih et al. (2013). The notation used has slightly been adapted for consistency with the rest of this tutorial. 22 Similarily to DQN, DDPG also employs the same replay buffer mechanism, reusing past transitions over training for increased sample efficiency and estimate the loss function via MC-estimates. Soft Actor-Critic (SAC) (Haarnoja et al., 2018) is derivation of DDPG in the max-entropy (MaxEnt) RL framework, in which RL agents are tasked with maximizing the discounted cumulative reward, while acting as randomly as possible. MaxEnt RL (Haarnoja et al., 2017) has proven particularly robust thanks to the development of diverse behaviors, incentivized by its entropy-regularization formulation. In that, MaxEnt revisits the RL objective J(π) to specifically account for the policy entropy H(π(st)), J(π) = (cid:88) t=0 E(st,at)χ[rt + αH(π(st))]. This modified objective results in the soft TD-target: yi = Est+1P(st,at) (cid:2)rt + γ(cid:0)Qθi1 (st+1, at+1) α log πϕ(at+1st+1)(cid:1)(cid:3), at+1 πϕ(st) (15) (16) Similarily to DDPG, SAC also maintains an explicit policy, trained under the same MaxEnt framework for the maximization of eq. 15, updated using: πk+1 arg min πΠ (cid:18)"
        },
        {
            "title": "DKL",
            "content": "π(st) (cid:13) (cid:13) (cid:13) (cid:13) exp(Qπk (st, )) Zπk (st) (cid:19) (17) The update rule provided in eq. 17 optimizes the policy while projecting it on set Π of tractable distributions (e.g., Gaussians, Haarnoja et al. (2017)). Sample-efficient, data-driven RL Sampling (st, at, rt, st+1) from the replay buffer conveniently allows to approximate expectations for TD-target and TD-error through Monte-Carlo (MC) estimates. The replay buffer also proves extremely useful in maintaining history of previous transitions and using it for training, improving on sample efficiency. Furthermore, it also naturally provides an entry point to inject offline trajectories recorded by human demonstrator into the training process. Reinforcement Learning with Prior Data (RLPD) (Ball et al., 2023) is an Offline-to-Online RL algorithm leveraging prior data to effectively accelerate the training of SAC agent. Unlike previous works on Offline-to-Online RL, RLPD avoids any pre-training and instead only uses the available offline data Doffline to improve online-learning from scratch. During each training step, transitions from both the offline and online replay buffers are sampled in equal proportions, and used in the underlying SAC routine. Together with other implementation details (using LayerNorm layers to prevent value overestimation, and the use of ensembles techniques to form the TD-target), RLPD proves particularly simple yet effective approach to use Doffline for Offline-to-Online RL. Sample-efficient, data-driven, real-world RL Despite the possibility to leverage offline data for learning, the effectiveness of real-world RL training is still limited by the need to define task-specific, hard-to-define reward function. Further, even assuming to have access to well-defined reward function, typical robotics pipelines rely on augmenting propioperceptive inputs with camera streams, and thus even well-defined rewards would need to be defined starting from unstructured observationa challenging assumption in practice. In their technical report, Luo et al. (2025) empirically address the needs (1) to define reward function and (2) to use it starting from unstructured, image observations. In particular, Luo et al. (2025, SERL) introduces suite of tools streamlining training of reward classifiers c, as well as jointly learn forward-backward controllers to speed up real-world RL. Reward classifiers are particularly useful in treating complex, dynamic taskse.g., folding t-shirtfor which precise reward formulation is arbitrarily complex to obtain, or that do require significant shaping and are more easily learned directly from demonstrations of success (e+) or failure (e) states, rather than from precise formulation of rt, with natural target for the reward classifier being r(s) = log c(e+ verts). Furthermore, Luo et al. (2025) demonstrate the benefits of learning separate (1) forward and (2) backward controllersparametrized by separate policieswhere (1) the former learns to execute task to completion and (2) the latter learns to reset the environment to its initial state from terminal states, thereby aiding training in real-world episodic settings. Lastly, in order to improve on the robustness of their approach to different goals while maintaing practical scalability, Luo et al. (2025) introduced modified state and action space, expressing proprioperceptive configurations and actions in the frame of the end-effector pose at = 0. Randomizing the initial pose of the end-effector (s0), Luo et al. (2025) achieved similar result to that of manually randomizing the environment at every timestep, but with 23 Figure 16 (A) HIL-SERL allows for real-world training of high performance RL agents by building on top advancements presented by of SAC, RLPD and SERL. (B) Example of human intervention during HIL-SERL training process on real-world SO-100. the benefit of maintaining the environment in the same condition across multiple training episodes, achieving higher scalability of their method thanks to the increased practicality of their approach. Building on off-policy deep Q-learning with replay buffers, entropy regularization for better exploration, expert demonstrations to guide learning, and series of tools and recommendations for real-world training using reward classifiers (Figure 16), Luo et al. (2024) introduce human interactions during training, learning near-optimal policies in challenging real-world manipulation tasks in 1-2 hours. Human-in-the-Loop, Sample Efficient Robot reinforcement Learning (HIL-SERL) (Luo et al., 2024) augments offlineto-online RL with targeted human corrections during training, and employs prior data to (1) train reward classifier and (2) bootstrap RL training on expert trajectories. While offline demonstrations provide the initial dataset seeding learning and constraining early exploration, interactive, online corrections allow human supervisor to intervene on failure modes and supply targeted interventions, greatly aiding the learning process (Luo et al., 2024). Crucially, human intervention data is stored in both the offline and online replay buffers, differently from the autonomous transitions generated at training time and stored in the online buffer only. In turn, given an intervention timestep (0, ), length-K human intervention data {shuman is more likely to be sampled than the data generated online during training, providing stronger supervision to the agent while still allowing for autonomous learning. Empirically, HIL-SERL attains near-perfect success rates (99%+) on diverse manipulation tasks within 1-2 hours of training (Luo et al., 2024), underscoring how offline datasets with online RL can markedly improve stability and data efficiency, and ultimately even allow real-world RL-training. , ahuman , rhuman , shuman k+ , }K k=1 k"
        },
        {
            "title": "3.2.1 Code Example: Real-world RL\nThis example shows how to use the HIL-SERL implementation supported by lerobot. This code example is organized\ninto four parts: we first show how to train a reward classifier from a custom set of demonstrations, then define\nthe Actor and Learner components, and finally, we bring them together in a complete script showing how to use\nHIL-SERL in practice.",
            "content": "At higher level, the HIL-SERL architecture (Figure 17) relies on two main components: An Actor, running frozen policy network used to interact with the environment and obtain observations. Observations are used to both condition the frozen actor in selecting the action to enact, and to form (st, at, rt, st+1) transitions that are shared with the Learner. Rewards are inferred using custom, learned reward classifier trained on dataset of offline demonstrations. Learner, used to optimize the policys parameters θ for maximum expected return. The learner samples batches 24 Figure 17 HIL-SERL is SOTA RL algorithm for training control policies directly in the real-world. Its implementation in lerobot relies on decoupled actor-learner architecture, communicating over processes (and possibly networks) with queues used to share (1) transitions (st, at, rt, st+1) and (2) parameters θ. of offline data from online and offline buffers in equal proportion (Ball et al., 2023), and shares updated parameters with the Actor. The HIL-SERL architecture presented in this example can be exclusively run locally, but the implementation in lerobot also allows the Actor and Learner to run on two separate machines connected by the network. Code 3: Training Reward Classifier https://github.com/fracapuano/robot-learning-tutorial/blob/main/snippets/ch3/01_reward_classifier.py 1 import torch 2 3 from lerobot . datasets . lerobot_dataset import LeRobotDataset 4 from lerobot . policies . factory import make_policy , e _ _ t _ c o 5 from lerobot . policies . sac . reward_model . f r o _ s i import a l i r f 6 7 # Device to use for training 8 device = \" mps \" # or \" cuda \" , or \" cpu \" 9 10 # Load the dataset used for training 11 repo_id = \" lerobot / m _ _ l _ a \" 12 dataset = LeRobotDataset ( repo_id ) 13 14 # Configure the policy to extract features from the image frames 15 camera_keys = dataset . meta . camera_keys 16 17 config = a l i r f ( num_cameras = len ( camera_keys ) , device = device , # backbone model to extract features from the image frames model_name = \" microsoft / resnet -18 \" , 18 20 21 22 ) 23 24 # Make policy , preprocessor , and optimizer 25 policy = make_policy ( config , ds_meta = dataset . meta ) 26 optimizer = config . _ i e _ s (). build ( policy . parameters ()) 27 preprocessor , _ = e _ _ t _ c o ( policy_cfg = config , dataset_stats = dataset . meta . stats ) 25 28 29 30 # your HF username and model repo id for the reward classifier 31 classifier_id = \" lerobot / a _ s i _ _ l _ m \" 32 33 # Instantiate dataloader 34 dataloader = torch . utils . data . DataLoader ( dataset , batch_size =16 , shuffle = True ) 35 36 # Training loop 37 num_epochs = 5 38 for epoch in range ( num_epochs ): 39 40 41 43 44 45 46 47 49 50 51 52 53 55 56 57 58 59 total_loss = 0 total_accuracy = 0 for batch in dataloader : # Preprocess the batch and move it to the correct device . batch = preprocessor ( batch ) # Forward pass loss , output_dict = policy . forward ( batch ) # Backward pass and optimization optimizer . zero_grad () loss . backward () optimizer . step () total_loss += loss . item () total_accuracy += output_dict [ \" accuracy \" ] avg_loss = total_loss / len ( dataloader ) avg_accuracy = total_accuracy / len ( dataloader ) print ( \" Epoch { epoch + 1}/{ num_epochs } , Loss : { avg_loss :.4 } , Accuracy : { avg_accuracy :.2 }% \" ) 61 62 print ( \" Training finished ! \" ) 63 64 # You can now save the trained policy . 65 policy . push_to_hub ( classifier_id ) Code 4: Defining the Actor https://github.com/fracapuano/robot-learning-tutorial/blob/main/snippets/ch3/02_actor.py 1 import multiprocessing as mp 2 from queue import Empty 3 4 import torch 5 from pathlib import Path 6 7 from lerobot . envs . configs import S R t C i 8 from lerobot . policies . sac . modeling_sac import SACPolicy 9 from lerobot . policies . sac . reward_model . el _ s i import Classifier 10 from lerobot . rl . gym_manipulator import make_robot_env 11 from lerobot . teleoperators . utils import TeleopEvents 12 13 MAX_EPISODES = 5 14 _ P _ _ S = 20 15 16 def ma ke_ policy_obs ( obs , device : torch . device = \" cpu \" ): 17 19 20 21 22 23 return { \" observation . state \" : torch . from_numpy ( obs [ \" agent_pos \" ]). float (). unsqueeze (0). to ( device ) , **{ \" observation . image .{ } \" : torch . from_numpy ( obs [ \" pixels \" ][ ]). float (). unsqueeze (0). to ( device ) for in obs [ \" pixels \" ] } , } 26 25 26 def run_actor ( tr si tio ns _qu eue : mp . Queue , ram ters_queue : mp . Queue , shutdown_event : mp . Event , policy_actor : SACPolicy , re rd _cl as sif ier : Classifier , env_cfg : HILSerlRobotEnvConfig , device : torch . device = \" mps \" , tpu _directory : Path None = None 27 29 30 31 32 33 34 35 ): 36 37 38 39 40 42 43 44 45 46 48 49 50 51 52 54 55 56 57 58 60 61 62 63 64 66 67 68 69 70 72 73 74 75 76 78 79 80 81 82 84 85 86 87 88 90 91 92 93 94 \" \" \" The actor process - interacts with environment and collects data . The policy is frozen and only the parameters are updated , popping the most recent ones from queue . \" \" \" policy_actor . eval () policy_actor . to ( device ) re rd _cl as sif ier . eval () re rd _cl as sif ier . to ( device ) # Create robot environment inside the actor process env , teleop_device = make_robot_env ( env_cfg ) try : for episode in range ( MAX_EPISODES ): if shutdown_event . is_set (): break obs , _info = env . reset () episode_reward = 0.0 step = 0 s _ ra t s = [] print ( \" [ ACTOR ] Starting episode { episode + 1} \" ) while step < _ P _ _ S and not shutdown_event . is_set (): try : new_params = parameters_queue . get_nowait () policy_actor . load_state_dict ( new_params ) print ( \" [ ACTOR ] Updated policy parameters from learner \" ) except Empty : pass # No new updated parameters available from learner , waiting # Get action from policy policy_obs = make_policy_obs ( obs , device = device ) # predicts single action , not chunk of actions ! action_tensor = policy_actor . select_action ( policy_obs ) action = action_tensor . squeeze (0). cpu (). numpy () # Step environment next_obs , _env_reward , terminated , truncated , _info = env . step ( action ) done = terminated or truncated # Predict reward policy_next_obs = mak e_policy_obs ( next_obs , device = device ) reward = ewa rd_ cla ss ifi er . predict_reward ( policy_next_obs ) if reward >= 1.0: # success detected ! halt episode if not done : terminated = True done = True # In HIL - SERL , human interventions come from the teleop device is_intervention = False if hasattr ( teleop_device , \" ge t_t ele op_ eve nts \" ): # Real intervention detection from teleop device teleop_events = teleop_device . ge t_t ele op_ eve nt () is_intervention = teleop_events . get ( TeleopEvents . IS_INTERVENTION , False ) # Store transition with intervention metadata 27 96 97 98 99 100 102 103 104 105 106 108 109 110 111 112 114 115 116 117 118 120 121 122 123 124 126 127 128 129 130 132 transition = { \" state \" : policy_obs , \" action \" : action , \" reward \" : float ( reward ) if hasattr ( reward , \" item \" ) else reward , \" next_state \" : policy_next_obs , \" done \" : done , \" truncated \" : truncated , \" co pl me ta y_ nf \" : { \" is_intervention \" : is_intervention , } , } i e _ n io . append ( transition ) episode_reward += reward step += 1 obs = next_obs if done : break # Send episode transitions to learner ra nsi tio ns_ que ue . put_nowait ( s _ n si n ) except Key boa rdI nte rru pt : print ( \" [ ACTOR ] Interrupted by user \" ) finally : # Clean up if hasattr ( env , \" robot \" ) and env . robot . is_connected : env . robot . disconnect () if teleop_device and hasattr ( teleop_device , \" disconnect \" ): teleop_device . disconnect () if output_directory is not None : policy_actor . save_pretrained ( output_directory ) print ( \" [ ACTOR ] Latest actor policy saved at : { output_directory } \" ) print ( \" [ ACTOR ] Actor process finished \" ) Code 5: Defining the Learner https://github.com/fracapuano/robot-learning-tutorial/blob/main/snippets/ch3/03_learner.py 1 import multiprocessing as mp 2 from queue import Empty , Full 3 4 import torch 5 import torch . optim as optim 6 7 from lerobot . policies . sac . modeling_sac import SACPolicy 8 from lerobot . rl . buffer import ReplayBuffer 9 10 LOG_EVERY = 10 11 SEND_EVERY = 10 12 13 def run_learner ( 14 16 17 18 19 20 22 23 ): 24 tr si tio ns _qu eue : mp . Queue , ram ters_queue : mp . Queue , shutdown_event : mp . Event , policy_learner : SACPolicy , online_buffer : ReplayBuffer , offline_buffer : ReplayBuffer , lr : float = 3e -4 , batch_size : int = 32 , device : torch . device = \" mps \" , \" \" \" The learner process - trains SAC policy on transitions streamed from the actor , 28 26 27 28 29 30 32 33 34 35 36 38 39 40 41 42 44 45 46 47 48 50 51 52 53 54 56 57 58 59 60 62 63 64 65 66 68 69 70 71 72 74 75 76 77 78 80 81 82 83 84 86 87 88 89 90 92 93 94 updating parameters for the actor to adopt . \" \" \" policy_learner . train () policy_learner . to ( device ) # Create Adam optimizer from scratch - simple and clean optimizer = optim . Adam ( policy_learner . parameters () , lr = lr ) print ( \" [ LEARNER ] Online buffer capacity : { online_buffer . capacity } \" ) print ( \" [ LEARNER ] Offline buffer capacity : { offline_buffer . capacity } \" ) training_step = 0 while not shutdown_event . is_set (): # retrieve incoming transitions from the actor process try : transitions = ran sit ion s_q ueu . get ( timeout =0.1) for transition in transitions : # HIL - SERL : Add ALL transitions to online buffer online_buffer . add (** transition ) # HIL - SERL : Add ONLY human intervention transitions to offline buffer is_intervention = transition . get ( \" co pl me ta ry _ in \" , {}). get ( \" is_intervention \" , False ) if is_intervention : offline_buffer . add (** transition ) print ( \" [ LEARNER ] Human intervention detected ! \" \" Added to offline buffer ( now { len ( offline_buffer )} transitions ) \" ) except Empty : pass # No transitions available , continue # Train if we have enough data if len ( online_buffer ) >= policy_learner . config . i _ p _ o _ r g : # Sample from online buffer ( autonomous + human data ) online_batch = online_buffer . sample ( batch_size // 2) # Sample from offline buffer ( human demonstrations only ) offline_batch = offline_buffer . sample ( batch_size // 2) # Combine batches - this is the key HIL - SERL mechanism ! batch = {} for key in online_batch . keys (): if key in offline_batch : batch [ key ] = torch . cat ([ online_batch [ key ] , offline_batch [ key ]] , dim =0) else : batch [ key ] = online_batch [ key ] loss , _ = policy_learner . forward ( batch ) optimizer . zero_grad () loss . backward () optimizer . step () training_step += 1 if training_step % LOG_EVERY == 0: print ( \" [ LEARNER ] Training step { training_step } , Loss : { loss . item ():.4 } , \" \" Buffers : Online ={ len ( online_buffer )} , Offline ={ len ( offline_buffer )} \" ) # Send updated parameters to actor every 10 training steps if training_step % SEND_EVERY == 0: try : state_dict = { : . cpu () for , in policy_learner . state_dict (). items ()} parameters_queue . put_nowait ( state_dict ) print ( \" [ LEARNER ] Sent updated parameters to actor \" ) except Full : # Missing write due to queue not being consumed ( should happen rarely ) 29 95 96 97 pass print ( \" [ LEARNER ] Learner process finished \" ) Code 6: Using HIL-SERL https://github.com/fracapuano/robot-learning-tutorial/blob/main/snippets/ch3/04_hil_serl.py 1 import multiprocessing as mp 2 import signal 3 from typing import Callable 4 from pathlib import Path 5 6 from lerobot . datasets . lerobot_dataset import LeRobotDataset 7 from lerobot . datasets . utils import _ _ a _ t s 8 from lerobot . envs . configs import HILSerlProcessorConfig , S R t C i 9 from lerobot . policies . sac . co nfi gur ati on_ sac import SACConfig 10 from lerobot . policies . sac . modeling_sac import SACPolicy 11 from lerobot . policies . sac . reward_model . de g _ s i import Classifier 12 from lerobot . rl . buffer import ReplayBuffer 13 from lerobot . rl . gym_manipulator import make_robot_env 14 from lerobot . robots . so100_follower import 10 0 l r f 15 from lerobot . teleoperators . so100_leader import O10 0Le ade rCo nfi 16 17 18 run_learner : Callable = ... 19 run_actor : Callable = ... # use / modify the functions defined earlier 20 21 \" \" \" Main function - coordinates actor and learner processes . \" \" \" 22 23 device = \" mps \" 24 ou put _ dir ctory = Path ( \" outputs / o _ r g _ o l / hil_serl \" ) 25 ou put _ dir ctory . mkdir ( parents = True , exist_ok = True ) # or \" cuda \" or \" cpu \" 26 27 # find ports using lerobot - find - port 28 follower_port = ... 29 leader_port = ... 30 31 # the robot ids are used the load the right calibration files 32 follower_id = ... 33 leader_id = ... 34 35 # pretrained model ( to be used in - distribution !) 36 a _ s i _ = \" lerobot / a _ s i _ _ l _ m \" 37 ew d_ cl si fi er = Classifier . from_pretrained ( a _ s i _ ) 38 39 ew d_ cl si fi er . to ( device ) 40 ew d_ cl si fi er . eval () 41 42 MAX_EPISODES = 5 43 _ P _ _ S = 20 44 45 # Robot and environment configuration 46 robot_cfg = 1 0 0 l ow o g ( port = follower_port , id = follower_id ) 47 teleop_cfg = SO1 00L ea der Con fig ( port = leader_port , id = leader_id ) 48 processor_cfg = S P e r f ( control_mode = \" leader \" ) 49 50 env_cfg = S R t C i ( robot = robot_cfg , teleop = teleop_cfg , processor = processor_cfg ) 51 52 # Create robot environment 53 env , teleop_device = make_robot_env ( env_cfg ) 54 55 obs_features = _ _ a _ t s ( env . robot . observation_features , \" observation \" ) 56 ac ti n_ fea tures = _ _ a _ t s ( env . robot . action_features , \" action \" ) 57 58 # Create SAC policy for action selection 59 policy_cfg = SACConfig ( 30 device = device , input_features = obs_features , utput_features = action_features , 60 62 63 ) 64 65 policy_actor = SACPolicy ( policy_cfg ) 66 pol icy_learner = SACPolicy ( policy_cfg ) 67 68 o r o _ o _ = \" lerobot / m _ _ l _ a \" 69 of fl ne _da taset = LeRobotDataset ( repo_id = o r o _ o _ ) 70 71 # Online buffer : initialized from scratch 72 i _ l _ f = ReplayBuffer ( device = device , state_keys = list ( obs_features . keys ())) 73 # Offline buffer : Created from dataset ( pre - populated it with demonstrations ) 74 l _ l _ f = ReplayBuffer . m _ o _ a ( erobot_dataset = offline_dataset , device = device , state_keys = list ( obs_features . keys ()) 75 76 ) 77 78 # Create communication channels between learner and actor processes 79 ra it io _q ue ue = mp . Queue ( maxsize =10) 80 pa ame ers _ queue = mp . Queue ( maxsize =2) 81 shu tdown_event = mp . Event () 82 83 84 # Signal handler for graceful shutdown 85 def si gnal_handler ( sig ): print ( \" nSignal { sig } received , shutting down ... \" ) shutdown_event . set () 86 88 89 90 signal . signal ( signal . SIGINT , signal_handler ) 91 signal . signal ( signal . SIGTERM , signal_handler ) 92 93 # Create processes 94 le ar er _pr ocess = mp . Process ( target = run_learner , args =( 95 96 transitions_queue , parameters_queue , shutdown_event , policy_learner , online_replay_buffer , offline_replay_buffer , ) , kwargs ={ \" device \" : device } , 97 98 99 100 102 103 104 105 ) # can run on accelerated hardware for training 106 107 actor_process = mp . Process ( target = run_actor , args =( 109 transitions_queue , parameters_queue , shutdown_event , policy_actor , reward_classifier , env_cfg , output_directory , ) , kwargs ={ \" device \" : \" cpu \" } , 110 111 113 114 115 116 117 118 119 ) 120 121 le ar er _pr ocess . start () 122 actor_process . start () # actor is frozen , can run on CPU or accelerate for inference 123 124 try : 125 126 128 129 # Wait for actor to finish ( it controls the episode loop ) actor_process . join () hutdown_event . set () ea rne r_process . join ( timeout =10) 31 130 except ybo ar dIn ter rup : 132 133 134 print ( \" Main process interrupted \" ) hutdown_event . set () actor_process . join ( timeout =5) ea rne r_process . join ( timeout =10) 135 136 finally : 138 139 140 if le arner_process . is_alive (): learner_process . terminate () if actor_process . is_alive (): actor_process . terminate ()"
        },
        {
            "title": "3.2.2 Limitations of RL in Real-World Robotics: Simulators and Reward Design\nDespite the advancements in real-world RL training, training RL agents for real-world tasks still suffers from the\nfollowing limitations:",
            "content": "In those instances where real-world training experience is prohibitively expensive to gather (e.g., Tokamak control (Degrave et al., 2022), Autonomous Stratospehere Navigation (Bellemare et al., 2020))in-simulation training is often the only viable option. However, high-fidelity simulators for real-world problems can be difficult to build and maintain, especially for contact-rich manipulation and tasks involving deformable or soft materials. Reward design is fundamental source of brittleness in real-world RL pipelines. While shaping dense rewards is often necessary to guide exploration in long-horizon tasks, the process is error-prone and heavily reliant on human expertise and intuition. Poorly tuned terms can lead to specification gaming or convergence to local optima, making reward shaping critical challenge for applying RL in practice. Sparse rewards that only signal successful trajectories can avoid these pitfalls but typically result in much slower learning due to reduced supervision. Advances in learning to act from potentially large corpora of human demonstrations via Behavioral Cloning (BC) address both of these concerns. Although suffering from an inherent suboptimalityimitation learning can at most match the performance level of the demonstratorlearning to reproduce expert demonstrations via BC has proven increasingly competitive and practical, bypassing the need for simulated environments and hard-to-define reward functions. 32 Figure 18 (A) Average (with standard deviation) evolution of the actuation levels over the first 5 recorded episodes in lerobot/svla_so101_pickplace. Proprioperceptive states provide invaluable to determine the robots state during an episode. (B) Camera frames are also recorded alongside measurements on the robots state, capturing information about the robots interaction with its environment."
        },
        {
            "title": "4 Robot (Imitation) Learning",
            "content": "The best material model for cat is another, or preferably the same cat"
        },
        {
            "title": "Norbert Wiener",
            "content": "TL;DR Behavioral Cloning provides natural platform to learn from real-world interactions without the need to design any reward function, and generative models prove more effective than point-wise policies at dealing with multimodal demonstration datasets. Learning from human demonstrations provides pragmatic alternative to the RL pipeline discussed in Section 3. Indeed, especially in real-world robotics, online exploration is typically costly and potentially unsafe, and designing (dense) reward signals is brittle and task-specific process. Further, even success detection itself often requires bespoke instrumentation, while episodic training demands reliable resetsall factors complicating training RL algorithms on hardware at scale. Behavioral Cloning (BC) sidesteps these constraints by casting control an imitation learning problem, leveraging previously collected expert demonstrations to anchor the learned autonomous behavior. Most notably, by learning-to-imitate, autonomous systems naturally adhere to the objectives, preferences, and success criteria implicitly encoded in the data, which reduces early-stage exploratory failures and obviates hand-crafted reward shaping altogether. i=1 , a(i) )}Ti t= be set of expert trajectories, with τ (i) = {(o(i) Formally, let = {τ (i)}N representing the i-th length-Ti trajectory in D, ot denoting observations (e.g., images and proprioception altogether), and at the expert actions. Typically, observations consist of both image and proprioperceptive information, while actions represent control specifications for the robot to execute, e.g. joint configuration. Note that differently from Section 3, in the imitation learning context denotes an offline dataset collecting length-Ti reward-free (expert) human trajectories τ (i), and not the environment dynamics. Similarily, in this section τ (i) represent length-Ti trajectory of observation-action pairs, which crucially omits entirely any reward information. Figure 18 graphically shows trajectories in terms of the average evolution of the actuation on the 6 joints of teleoperated SO-100 manipulator. Notice how proprioperceptive states are captured jointly with camera frames over the course of the recorded episodes, providing unified high-frame rate collection of both image and joint teleoperation data. Figure 19 shows (ot, at)-pairs for the same dataset, with the actions performed by the human expert illustrated alongside the corresponding observation. In principle, (expert) trajectories τ (i) can have different lengths since demonstrations might exhibit multi-modal strategies to attain the same goal, resulting in multiple, different behaviors. Behavioral Cloning (BC) (Pomerleau, 1988) aims at producing synthetic behaviors by learning the mapping from 33 Figure 19 Sample observations and action pairs over the course of given trajectory recorded in lerobot/svla_so101_ pickplace. Observations, comprising of both proprioperceptive and visual information, are recorded alongside the configuration of second, leader robot controlled by human expert, providing complete information for regressing actions given observations. observations to actions, and in its most natural formulation can be effectively tackled as supevised learning problem, consisting of learning the (deterministic) mapping : (cid:55) A, at = (ot) by solving min E(ot,at)p()L(at, (ot)), (18) given an arbitrary risk function : (cid:55) R, L(a, a). Typically, the experts joint observation-action distribution : (cid:55) [0, 1] is assumed to be unknown, in keeping with classic Supervised Learning (SL) framework3. However, differently from standard SL assumptions, the samples collected in Drealizations of the underlying pare not i.i.d., as expert demonstrations are collected sequentially in the form of trajectories. In practice, this aspect can be partially mitigated by considering pairs in non-sequential ordershuffling the samples in Dso that the expected risk under can be approximated using MC estimates, although these estimates may in general be less accurate. Another strategy to mitigate the impact of regressing over non-i.i.d. samples relies on the possibility of interleaving BC and data collection (Ross et al., 2011, DAgger), aggregating multiple datasets iteratively. However, because we only consider the case where single offline dataset of trajectories is available and no more data can be collected, DAgger falls out of our scope. Despite the inherent challenges of learning from non-i.i.d. data, the BC formulation presents several operational advantages in robotics. First, training happens offline and naturally accomodates for expert, demonstration data, hereby severily limiting exploration risks by preventing the robot from performing dangerous actions altogether, by anchoring action in imitation. Second, reward design is entirely unnecessary in BC, as demonstrations already reflect human intent. The absence of rewards also prevents the risk of misalignment and specification gaming (reward hacking), otherwise inherent in purely reward-based RL (Heess et al., 2017). Third, because expert trajectories encode terminal conditions, success detection and resets are implicit in the dataset. Finally, empirical evidence suggests the performance of BC scales naturally with growing corpora of demonstrations collected across tasks, embodiments, and environments. Nonetheless, BC can, in principle, only reproduce behaviors that are at best as good as those of the demonstrator, and therefore offers no remedy for the suboptimal decisions that humans may enact. This limitation is particularly problematic in sequential decision-making tasks where expert demonstrations are scarceeither because data collection is costly or because human performance is inherently suboptimal. Yet, many robotics applications still benefit from relatively inexpensive pipelines for collecting high-quality human-generated trajectories, justifying the use of BC in such settings. While conceptually elegant, point-estimate policies : (cid:55) learned by solving eq. 18 have been observed to suffer from (1) compounding errors (Ross et al., 2011) and (2) poor fit to multimodal distributions (Florence et al., 2022; Ke et al., 2020). Figure 20 illustrates these two key issues related to learning explicit policies (Florence et al., 2022). Besides sequentiality in D, compounding errors due to covariate shift may also prove catastrophic, as even small ϵ-prediction errors 0 < µ(ot) at ϵ can quickly drive the policy into out-of-distribution states, incuring in less confident generations and thus compounding errors (Figure 20, left). Moreover, point-estimate policies typically fail to learn multimodal targets, which are very common in human demonstrations solving real-world robotics problems, as multiple trajectories can be equally as good towards the accomplishment of goal (e.g., symmetric grasps, Figure 20, right). In particular, unimodal regressors tend to average across modes, yielding indecisive or 3Throughout, we will adopt the terminology and notation for SL used in Shalev-Shwartz and Ben-David (2014) 34 Figure 20 Point-wise policies suffer from limitations due to (A) covariate shifts and (B) poor approximation of multimodal demonstrations. (A) Small errors may drive the policy out of distribution, incuring in vicious circle ultimately resulting in failure. (B) Both modes of reaching for target object in the sceneeither left or right-firstare equally as good and thus equally as likely to be present in dataset of human demonstrations, ultimately resulting in multimodal demonstrations. Figure 21 Intuitively, latent variable in single latent model may contain information regarding the task being performed, which directly results in the likelihood of the same observation-action pair being different for two different tasks. When (A) picking block the likelihood of wide grippers opening should be higher than narrower one, while it should be the opposite when (B) pushing the block. even unsafe commands (Florence et al., 2022). To address poor multimodal fitting, Florence et al. (2022) propose learning the generative model p(o, a) underlying the samples in D, rather than explicitly learning prediction function : = (o)."
        },
        {
            "title": "4.1 A (Concise) Introduction to Generative Models\nGenerative Models (GMs) aim to learn the stochastic process underlying the very generation of the data collected, and\ntypically do so by fitting a probability distribution that approximates the unknown data distribution, p. In keeping\nwith the GM literature, p(x) ← P(x), x ∼ p. In the case of BC, the unknown data distribution p may represent\nthe expert’s joint distribution over (o, a)-pairs. Thus, given a finite set of N pairs D = {(o, a)i}N\navailable as an\nimitation learning target (and thus assumed to be i.i.d.), GMs seek to learn a parametric distribution pθ(o, a) such\nthat (1) new samples (o, a) ∼ pθ(•) resemble those stored in D, and (2) high likelihood is assigned to the observed\nregions of the unobservable p. Likelihood-based learning provides a principled training objective to achieve both goals,\nand it is thus extensively used in GMs (Prince, 2023).",
            "content": "i=0 35 Figure 22 (A) The latent variable model in robotics application regulates influence between observed (o, a) variables and an unobservable latent variable. (B) VAEs approximate exact latent variable models by means of variational inference."
        },
        {
            "title": "4.1.1 Variational Auto-Encoders\nA common inductive bias used in GM posits samples (o, a) are influenced from an unobservable latent variable z ∈ Z,\nresulting in:",
            "content": "(cid:90) p(o, a) = p(o, az)p(z) (19) supp(Z) Intuitively, in the case of observation-action pairs (o, a) for robotics application, could be interpreted as some high level representation of the underlying task being performed by the human demonstrator. In such case, treating p(o, a) as marginalization over supp(Z) of the complete joint distribution p(o, a, z) natively captures the effect different tasks have on the likelihood of observation-action pairs. Figure 21 graphically illustrates this concept in the case of (A) picking and (B) pushing task, for which, nearing the target object, the likelihood of actions resulting in opening the gripperthe higher q6, the wider the grippers openingshould intuitively be (A) high or (B) low, depending on the task performed. While the latent space typically has much richer structure than the set of all actual tasks performed, eq. 19 still provides solid framework to learn joint distribution conditioned on unobservable yet relevant factors. Figure 22 represents this latent-variable framework in the context of robotics application: the true, z-conditioned generative process assigns likelihood p((o, a)z) to the single (o, a)-pair. Using Bayes theorem, one can reconstruct the posterior distribution on supp(Z), qθ(zo, a) from the likelihood pθ(o, az), prior pθ(z) and evidence pθ(o, a). VAEs approximate the latent variable model presented in eq. 19 using an approximate posterior qϕ(zo, a) while regressing parameters for parametric likelihood, pθ(o, az) (Figure 22). Given dataset consisting of i.i.d. observation-action pairs, the log-likelihood of all datapoints under θ (in Bayesian terms, the evidence pθ(D)) can be written as: log pθ(D) = log (cid:88) pθ((o, a)i) i=0 (cid:88) (cid:90) = log i=0 supp(Z) (cid:88) (cid:90) = log supp(Z) i=0 (cid:88) i=0 = log pθ((o, a)iz)p(z) qθ(z(o, a)i) qθ(z(o, a)i) pθ((o, a)iz)p(z) Ezqθ((o,a)i) (cid:20) p(z) qθ(z(o, a)i) (cid:21) , pθ((o, a)iz) (20) (21) (22) (23) where we used eq. 19 in eq. 20, multiplied by 1 = qθ(z(o,a)i) qθ(z(o,a)i) eq. 23. in eq. 21, and used the definition of expected value in In the special case where one assumes distributions to be tractable, pθ(D) is typically tractable too, and maxθ log pθ(D) provides natural target for (point-wise) infering the unknown parameters θ of the generative model. Unfortunately, eq. 23 is rarely tractable when the distribution is modeled with approximators such as neural networks, especially for high-dimensional, unstructured data. In their seminal work on Variational Auto-Encoders (VAEs), Kingma and Welling (2013) present two major contributions to learn complex latent-variable GMs from unstructured data, proposing (1) tractable, variational lower-bound 36 to eq. 23 as an optimization target to jointly learn likelihood and posterior and (2) using high-capacity function approximators to model the likelihood pθ(o, az) and (approximate) posterior distribution qϕ(zo, a) qθ(zo, a). In particular, the lower bound on eq. 23 (Evidence LOwer Bound, ELBO) can be derived from eq. 23 applying Jensens inequalitylog E[] E[log()]yielding: log pθ(D) (cid:88) (cid:18) i=0 Ezqθ((o,a)i) (cid:2) log pθ((o, a)iz)(cid:3) + Ezqθ((o,a)i) (cid:20) (cid:18) log p(z) qθ(z(o, a)i) (cid:19)(cid:21)(cid:19) (cid:88) (cid:0)Ezqθ((o,a)i) = i= (cid:2) log pθ((o, a)iz)(cid:3) DKL (cid:2)qθ(z(o, a)i)p(z)(cid:3)(cid:1) (24) (25) The true, generally intractable, posterior qθ(zo, a) prevents computing both the expectation and KL divergence terms in eq. 25, and therefore Kingma and Welling (2013) propose deriving the ELBO using an approximate posterior qϕ(zo, a), resulting in the final, tractable, ELBO objective, ELBOD(θ, ϕ) = (cid:88) (cid:0)Ezqϕ((o,a)i) i=0 (cid:2) log pθ((o, a)iz)(cid:3) DKL (cid:2)qϕ(z(o, a)i)p(z)(cid:3)(cid:1) (26) From Jensens inequality, maximizing ELBO results in maximizing the log-likelihood of the data too, thus providing natural, tractable optimization target. Indeed, expectations can be estimated using MC estimates from the learned distributions in eq. 26, while the KL-divergence term can typically be computed in closed-form (1) modeling qϕ as Gaussian qϕ(zo, a) = (cid:0)µϕ(o, a), Σϕ(o, a)(cid:1) with learned mean vector µϕ(o, a) and learned variance-covariance matrix Σϕ(o, a) and (2) imposing standard Gaussian prior on the latent space, p(z) = (0, I). An intuitive explanation of the learning dynamics of VAEs can be given considering the equivalent case of minimizing the negative ELBO, which admits the particularly interpretable factorization (considering, without loss of generality, only one (o, a) D): min θ,ϕ ELBO(o,a)D(θ, ϕ) = min θ,ϕ Lrec(θ) + Lreg(ϕ), Lrec(θ) = Ezqϕ(o,a)(cid:2) log pθ(o, az)(cid:3) Lreg(ϕ) = DKL (cid:2)qϕ(zo, a)p(z)(cid:3). (27) (28) (29) For any given (o, a) pair, the expected value term in eq. 28 is typically computed via MC estimates, resulting in Ezqϕ(o,a) (cid:2) log pθ(o, az)(cid:3) = Lrec 1 (cid:88) i=0 log pθ(o, azi). Assuming pθ(o, az) to be parametrized with an isotropic Gaussian distribution with mean µθ(z) Rd and variance σ2, the log-likelihood thus simplifies to: log p(o, azi) = 1 2σ2 (cid:13) (cid:13)(o, a) µθ(zi)(cid:13) 2 2 (cid:13) 2 log(2πσ2) = Lrec 1 (cid:88) i=0 (cid:13) (cid:13)(o, a) µθ(zi)(cid:13) 2 (cid:13) 2 In practice, it is common to approximate the learned likelihood pθ(o, az) with parametric distribution (e.g., Gaussian) whose parameters are given by learned coefficient vector derived from µθ(z), p(). Under this formulation, learning VAE amounts to (1) reconstructing the examples in by minimizing (1) the reconstruction loss Lreca standard supervised learning objective for regressionwhile (2) regularizing the latent representation by minimizing Lreg. The latter enforces information compression, since with the common prior choice p(z) = (0, I) in eq. 29, the regularizer constrains the posterior and thereby limits the expressivity of qϕ(zo, a)."
        },
        {
            "title": "4.1.2 Diffusion Models\nVAEs approximate probability distributions via a single latent variable model, assuming the underlying unknown\ndistribution can be factored according to eq. 19, and solve the variational-inference problem of jointly learning\nthe likelihood pθ and (approximate) posterior qϕ for such model. In that, the unknown data distribution p(o, a) is\neffectively approximated via (cid:82)\nZ p(z)pθ(o, a|z), and the underlying generative process reproduced by (1) sampling a",
            "content": "37 Figure 23 HMLV models posit the data generation process is influenced by stack of Markov-dependent latent variables, with samples from the posterior distribution being progressively higher up in the hierarchy. latent variable and (2) learning to decode it into high-likelihood sample under the (unknown) p(o, a). Diffusion Models (DMs) (Ho et al., 2020) are another class of GMs which treat the similar problem of approximating an underlying unknown data distributionvariational inferenceby partially extending VAEs to the case where multiple latent variables influence each other and the generative process underlying o, itself. In particular, DMs posit the generative process can be decomposed to series of piece-wise (Markovian) interactions between (latent) variables (Figure 23), resulting in (cid:90) (cid:90) (cid:90) . . . supp(Z0) supp(Z1) supp(ZT ) ) = p( o, (cid:124)(cid:123)(cid:122)(cid:125) =z p(z0, z1, . . . zT ) p(z0, z1, . . . zT ) = p(zT ) (cid:89) t=1 p(zt1zt), (30) (31) where we explicitly showed the marginalization over the multiple latents in eq. 30, and used the law of conditional probability and Markov property in eq. 31. Also, for ease of notation, we will refer to observation-action pairs o, as z0. Similar to VAEs, it is generally not possible to assign an exact interpretation to the latent variables. Nevertheless, reasonable application-driven intuition is that Hierarchical Markov Latent Variable (HMLV) models, by capturing hierarchical and decoupled interactions among latent variables, can reflect the different resolutions at which conditioning factors intervene. For example, in robotics setting, one might naturally distinguish between high-level trajectory planning (higher up in the hierarchy, ) and fine-grained motion adjustments (closer to empirical observations, 0). In that, HMLV models thus provide framework to perform variational inference via multiple, sequential sampling steps from different higher level distributions instead of approximating the generative process with single-latent variable model. DMs are particular instantiation of HMLV models for which the posterior is fixed to 1 βt, βtI), for given βt R+. In practice, βt is used to iteratively reduce the signal-to-noise q(ztzt1) = (zt ratio along the latents hierarchy, similarily to how diffusion process influences the information of physical system. Just like VAEs, DMs attemp to learn to reproduce an underlying data distribution p(o, a) given collection of i.i.d. samples approximating the model posited to have generated the data in the first place (eq. 30). Similarily to VAEs, DMs approximate the process of sampling from the unknown p(o, a) by (1) sampling from an easy-to-sample distribution (e.g., Gaussian) and (2) learning to reconstruct high-likelihood samples under the unknown distribution. However, in stark contrast with VAEs, the easy-to-sample distribution contains no mutual information regarding the data distribution p(o, a). Crucially, as no information from the sample (o, a) (denoted as z0 (o, a) for simplicity of notation) is assumed to be propagated throughout the chain of latents, the posterior q(ztzt1) assumes relatively amicable structure in DMs, reducing complexity. The true likelihood p(zt1zt) is instead typically approximated using the parametrization pθ(zt1zt). In that, the information contained in the unknwon data distribution is reconstructed via process in which samples from fixed distribution are iteratively turned into (ideally) high-likelihood samples under p(o, a)a process referred to as denoising. 38 (32) (33) (34) (35) (36) (37) (38) (39) (40) (cid:20) log (cid:21) pθ(ztzt+1) q(ztzt1) Under such model, we can express the log-likelihood of an arbitrary sample z0 as: log pθ(z0) = log (cid:90) supp(Z1)supp(Z2)supp(ZT ) ) pθ(z0, z1, z2, . . . zT (cid:125) (cid:124) (cid:123)(cid:122) z0:T (cid:90) = log supp(Z1:T ) = log Ez1:T q(z0) pθ(z0:T ) q(z1:T z0) q(z1:T z0) (cid:21) (cid:20) pθ(z0:T ) q(z1:T z0) pθ(z0:T ) q(z1:T z0) p(zT ) (cid:81)T (cid:81)T log log (cid:21) Ez1:T q(z0) = Ez1:T q(z0) = Ez1:T q(z0) = Ez1:T q(z0) = Ez1:T q(z0) (cid:20) (cid:20) (cid:20) (cid:20) (cid:20) t=1 pθ(zt1zt) (cid:21) t=1 q(ztzt1) p(zT ) pθ(z0z1) (cid:81)T q(zT zT 1) (cid:81)T 1 p(zT ) pθ(z0z1) (cid:81)T 1 q(zT zT 1) (cid:81)T 1 t=2 pθ(zt1zt) t=1 q(ztzt1) t=1 pθ(ztzt+1) (cid:21) (cid:21) log log t=1 q(ztzt1) (cid:21) + Ez1:T q(z0) log p(zT ) pθ(z0z1) q(ztzt1) = Ez1:T q(z0) (cid:2) log pθ(z0z1)(cid:3) + Ez1:T q(z0) (cid:20) log 1 (cid:88) + Ez1:T q(z0) (cid:20) log 1 (cid:89) (cid:21) pθ(ztzt+1) q(ztzt1) t=1 (cid:21) p(zT ) q(zT zT 1) = Ez1q(z0) (cid:2) log pθ(z0z1)(cid:3) + EzT 1:T q(z0) (cid:20) log p(zT ) q(zT zT 1) (cid:21) + = Ez1q(z0) log pθ(z0z1) EzT 1q(z0) (cid:2)DKL(q(zT zT 1)p(zT ))(cid:3) 1 (cid:88) t= E(zt1,zt+1)q(z0) (cid:2)DKL(q(ztzt1)pθ(ztzt+1))(cid:3), t=1 1 (cid:88) t=1 Ezt1:t+1q(z0) (cid:20) log (cid:21) pθ(ztzt+1) q(ztzt1) (41) (42) where we: used eq. 30 and multiplied by 1 = q(z1:T z0) in eq. 33; used Jensens inequality in eq. 35; used the law of q(z1:T z0) conditional probability for both numerator and denominator in eq. 36; stepped forward and backward the products in the numerator and denominator products in eq. 37, respectively; reindexed the product terms in eq. 38; removed out-of-expectation variables in eq. 41; used the defintion of KL-divergence in eq. 42. In turn, eq. 42 provides an optimization target to learn pθ solving maxθ log pθ(D). In their seminal work on using DMs for variational inference, Ho et al. (2020) introduce major contributions regarding solving minθ log pθ(z0). In particular, Ho et al. (2020) exclusively adopt fixed, isotropic Gaussian posterior in the form of q(ztzt1) = ( 1 βtzt1, βtI). The choice of adopting Gaussians has profound implications on the generative process modeled. Indeed, under the (mild) assumption that the variance is sufficiently small βt η, η R+, Sohn et al. (2015) proved that the likelihood p(zt1zt) is Gaussian as well, which allows for the particularly convenient parametrization of the approximate likelihood pθ(zt1zt) = (µθ(zt, t), Σθ(zt, t)), [1, ], as well as for closed-form tractability of the KL-divergence terms in eq. 42. Further, the posteriors structure also enables the analytical description of the distribution of the t-th latent variable, q(ztz0) = ( αtz0, (1 αt)I), with αt = 1 βt, αt = (cid:81)t k=1 αk, conveniently preventing iterative posterior sampling simplifying computing eq. 42. It follows: θ log pθ(z0) = Ez1q(z0)θ log pθ(z0z1) 1 (cid:88) t= Ezt1,zt+1q(z0)θDKL(q(ztzt1)pθ(ztzt+1), (43) where the former term is equivalent to the reconstruction term in eq. 27 and the latter term can be obtained in closed form. Besides mathematical tractability of eq. 43, adopting Gaussian posteriors allows for particularly intuitive interpretation of the training dynamics of DMs (Permenter and Yuan, 2024). As the hierarchical latent variables are 39 Figure 24 DMs iteratively corrupt samples (left) from an unknown distribution into quasi-standard Gaussian (center), learning the displacement field (right) that permits to reconstruct samples from the unknown target distribution by iteratively denoising samples of tractable, easy-to-sample distribution. 40 Figure 25 joint action-observation distribution, in the simplified case where the observation is the elbow-flex actuation in SO-100, and the action is the recorded position for the same joint from the teleoperator arm. The motion recorded being teleoperated, the points distribute along the diagonal. , with q2 denoting the robots elbow flex actuation and qh 2 repeatedly corrupted by applying increasingly more Gaussian noise, they progressively lose information about the original (unknown) sample z0, converging toward standard Gaussian which eventually contains no information at all (Figure 24). Figure 24 illustrates this process on simplified, bidimensional observation-action distribution, where we the corresponding human considered = q2 and = qh 2 teleoperators elbow flex. Because the recorded behavior is teleoperated, measurements mostly distribute along the line = + η, η (0, 1), with η-variability accouting for minor control inconsistencies (Figure 25). Notice how corrupted samples distribute differently from the most reasonable structure o, further underscoring how diffusion corrupts both the individual samples and the global distribution (Figure 24, left and center). In this, using Gaussian posteriorsi.e., adding Gaussian noiseeffectively simulates Brownian motion for the elements in the distributions support (in Figure 24, A), whereby information diffuses away from the samples. Comparing the diffused samples to the original data points, one can derive an estimate of the total displacement induced by the diffusion process, and, under the assumption that the likelihood of the totally diffused samples is low under the original unknown data distribution, one can effectively approximate the unkwown distribution by learning to reverse such displacement. This key intuition allows to write simplified training objective4: L(θ) = Et,z0,ϵ (cid:2)ϵ ϵθ( αtz0 + ϵ 1 αt, t)2(cid:3), U({1, . . . , }), z0 D, ϵ (0, I). (44) In this simplified (minimization) objective, the optimization process differs from eq. 42 in that, rather than maximizing pθ directly, the parameters θ of the pairwise likelihood pθ(zt1zt) are adjusted to predict the total displacement ϵ for randomly long (t ({1, . . . , })) diffusion process starting from sample of the target distribution. By learning the total displacement from generally, uninformative corrupted sample obtained diffusing information and sample from an unknown distribution Ho et al. (2020) show that one can approximate the underlying distribution reversing the displacement, denoising samples. Interestingly, under the hypothesis that real-world data belongs to single, higher-dimensional manifold (Manifold Hypothesis), Permenter and Yuan (2024) show that diffusion learns the gradient of distance function from any off-point manifold (such as perturbed, uniformative samples), and the data manifold itself. Following this gradienti.e., denoising sample from an uninformative distributioncorresponds to projecting back into the manifold, yielding procedure to sample from unknown distributions by means of Euclidean projection. Indeed, under the assumption that pθ(zt1zt) is Gaussian, sampling zt1 pθ(zt) corresponds to computing: zt1 = (cid:18) 1 αt zt βt 1 αt (cid:19) ϵθ(zt, t) + σtϵ, ϵ (0, I), (45) thus showing that the lower-level latent variables in DM can be obtained by iteratively removing noise from the one-step higher order variable, using the noise regressor ϵθ(zt, t) learned minimizing eq. 44."
        },
        {
            "title": "4.1.3 Flow Matching\nThe posterior parametrization adopted by DMs proved traditionally effective, yet it raised concerns circa its efficiency\nat inference time, where a possibly large number (hundreds) of compute-expensive denoising steps are needed in\norder to recover a sample from the target distribution. Flow Matching (FM) (Lipman et al., 2023) extends DMs",
            "content": "4See Luo (2022, \"Three equivalent interpretations\") for complete derivation 41 Figure 26 Probability distributions can be modified differently by applying different vector fields, inducing different flows of mass across the same support (top versus bottom, using two different time-invariant 2D-fields u1(x, y) = (x, 0) and u2(x, y) = (x/ 2)). Notice time flows continuously in [0, 1]. FM models learn to approximate target vector field, thereby producing arbitrary (goal) transformations of an easy-to-sample initial distribution. 2, y/ to the general case of arbitrary likelihood and posteriors, and in this defines superseding class of GMs providing unified framework for learning continuous transformations between distributions, encompassing and generalizing DMs. Instead of stochastic, discrete, multi-step denoising process, FM aims to learn deterministic, continuous, differentiable flow ψ : [0, 1]Z (cid:55) Z, formalized starting from (possibly time-dependent) vector field : [0, 1]Z (cid:55) transporting over time samples from simple prior distribution p0e.g., standard Gaussianto more complex, typically unknown data distribution p1. In this, FM accomodates for arbitrary intermediate distributions, breaking free from the particular case where posterior and likelihood are exclusively Gaussians. Note also how FM models time [0, 1] to be varying continuously while moving away from an easy-to-sample distribution p0 towards the unknown data-distribution, p1. This results in continuous (and deterministic) trajectory at inference, which is in practice more efficient compared to following stochastic paths like in DMs. Formally, FM can be fully characterized by an ordinary differential equation (ODE) relating instantaneous variations of flows with the underlying vector field, and hence providing complete trajectories over the distributions support when integrating over time, dt ψ(z, t) = v(t, ψ(t, z)), ψ(0, z) = z. (46) (47) In practice, flow models learn to approximate these dynamics by estimating vector field that matches the true, unknown u, so that the induced flows ψ can approximate the ideal trajectories ψ. FM proved very effective in variety of applications, ranging from image (Esser et al., 2024) and video generation (Polyak et al., 2025) to robotics control (Black et al., 2024). Most notably, in their introductory work on FM for GM, Lipman et al. (2023) show how DMs can be seen as specific instance of FM where the conditional target vector field learned by the noise regressor εθ corresponds to: u(t, zz0) = dt α(1 t) 1 (α(1 t))2 (α(1 t)z z0), α(t) = 1 2 (cid:82) 0 β(s)ds, z0 D. (48) Conditional vector fields are defined not only over their argument and time t, but do also vary with respect to an auxiliary variable z0, thereby extending the standard notion of vector field to incorporate additional conditioning. Note that the traditional discrete-time noise-scheduler {βt}T is now generalized to continuous map β : [0, 1] (cid:55) R+. Crucially, Lipman et al. (2023) prove that by exclusively optimizing the vector field for individual data points z0 D, one also retrieves the optimal flow to morph the entire support of the initial distribution p0 into p1 s.t.D p1. While the noising schedule of DMs results in stochastic resembling random (Brownian) walk, FM allows for more generalpotentially, deterministiclikelihood and posterior parametrization. In the FM literature the likelihood and posterior probabilty densities defined along HMLV model are typically referred to as probability path, where the distributions for successive adjacent transitions in the HMLV model are related by the (normalized) flow between them (Figure 26). The inherent flexibility of FM is one of their key advantages over DMs, as it opens up the possibility of t=0 42 Figure 27 Compared to diffusion, flow matching distorts distribution along less randomic pattern, resulting in clearer interpolation between source and target distribution. The visualization shows an example comparison between these two methods on joint distribution of robot observations and actions over = 50 steps. learning more efficient paths. For instance, one can design probability paths inspired by Optimal Transport (OT), mathematical framework concerned with characterizing the most efficient morphings between probability distributions. Probability paths obtained through OT paths tend to be straighter than diffusion paths (Figure 27), which can lead to faster and more stable training, as well as empirically result in higher-quality generations with fewer denoising steps at inference time. In particular, by avoiding unnecessary backtracking associated with the inherent stochastic nature of both the noising and denoising process in DMs, test-time compute is typically significantly reduced in FM, while retaining comparable results (Lipman et al., 2023). In practice, FM can be applied to generative modeling by learning vector field regressor vθ(z, t) to approximate given target vector field u(t, z). In the particular case of DMs, u(t, z) is defined as in eq. 48, while in priciple the target vector field can be learned to induce an arbitrary mass displacement, or fixed according to OT. Given sample from the data distribution z1 p1 and sample from an easy-to-sample prior z0 p0, Conditional FM (CFM) defines simple path between them using linear interpolation between samples zt = (1 t)z0 + tz1, which in turn results in the target vector field u(t, zt) = z1 z0. FM models can then be trained with simple regression objective defined as: (cid:2)vθ((1 t)z0 + tz1, t) (z1 z0)2(cid:3), U([0, 1]), L(θ) = Et,z0,z1 (49) where z0 p0() and z1 p1(). Note how in eq. 49differently from eq. 44time is assumed to be varying continuously U([0, 1]) rather than discretely U({0, t, 2t, . . . , 1}), key property of flow-based models. Therefore, the objective in eq. 49 directly regresses the learned vector field onto the simple, straight path connecting point from the prior and point from the data, providing simulation-free training procedure that is both stable and efficient. At inference time, samples are generated by starting with z0 p0 and iteratively refined according to dt = vθ(zt, t) for [0, 1]an operation that can be numerically carried out with standard ODE solvers, and that in dz practice is often carried out numerically via forward-Euler integrating over tens of denoising steps."
        },
        {
            "title": "4.2 Action Chunking with Transformers\nWhile GMs prove useful in learning complex, high-dimensional multi-modal distributions, they do not natively address\nthe compouding errors problem characteristic of modeling online, sequential predictions. In Action Chunking with\nTransformers (ACT), Zhao et al. (2023) present an application of VAEs to the problem of learning purely from\noffline trajectories, and introduce a simple, yet effective method to mitigate error compounding, learning high-fidelity\nautonomous behaviors via BC. Drawing inspiration from how humans plan to enact sequences of actions at:t+k instead\nof single actions at, Zhao et al. (2023) propose learning a GM on a dataset of input demonstrations by modeling\nchunks of multiple actions directly. Besides contributions to learning high-performance autonomous behaviors, Zhao\net al. (2023) also introduce hardware contributions in the form of a low-cost bimanual robot setup (ALOHA) capable\nof performing fine-grained manipulation tasks, such as opening a lid, slotting a battery in its allotment or even prepare\ntape for application. Notably, ALOHA bimanual setup costs just as much as a mono-arm Franka arm and can be\nassembled from easy-to-source parts, underscoring its higher accessibility.",
            "content": "43 Zhao et al. (2023) do also present significant algorithmic contributions related to synthetizing performant autonomous behaviors for the ALOHA setup, adopting transformers as the architectural backbone to learn Conditional VAE (Sohn et al., 2015) from demonstrations. Conditional VAEs are variation of the standard VAE introducing an arbitrary conditioning on sampling from the latent prior, modeling one-to-many relationships between latent and data samples. Further, in stark contrast with previous work (Florence et al., 2022; Janner et al., 2022), Zhao et al. (2023) do not learn full joint pθ(o, a) on observation and actions, and rather focus on the conditional pθ(ao). While the policy distribution pθ(ao) can in principle be entirely described from the joint pθ(o, a), conditional distributions are often intractable when using function approximators, as pθ(ao) = pθ(o,a) , and the integral in the denominator is typically (cid:82) pθ(o,a) intractable. Thus, instead of modeling the full joint using vanilla VAE, Zhao et al. (2023) propose learning conditional VAE (Sohn et al., 2015) modeling the policy distribution directly, hence approximating p(ao). In practice, when learning from demonstrations adopting CVAEs results in slight modification to the VAE objective in eq. 26, which is adapted to: ELBOD(θ, ϕ, ω) = (cid:88) i=0 (cid:0)Ezqϕ(oi,ai) (cid:2) log pθ(aiz, oi)(cid:3) DKL (cid:2)qϕ(zoi, ai)pω(zoi)(cid:3)(cid:1) (50) Notice how in eq. 50 we are now also learning new set of parameters ω for the prior distribution in the latent space. Effectively, this enables conditioning latent-space sampling (and thus reconstruction) during training (and potentially inference too), providing useful when learning inherently conditional distributions like policies. Further, ACT is trained as β-CVAE (Higgins et al., 2017), weighing the KL regularization term in eq. 50 with an hyperparameter β R+ regulating the information condensed in the latent space, where higher β results in less expressive latent space. 0, qc In their work, Zhao et al. (2023) ablated using GM to learn from human demonstrations compared to simpler, supervised objective, L1(a, a) = a1. Interestingly, they found the performance of these two approaches to be comparable when learning from scripted demonstrations. That is, when learning from data collected rolling out predetermined set of commands [qc 1, . . . ], GM did not prove competitive compared to standard supervised learning. However, when learning from human demonstrationsi.e., from data collected executing commands coming from human controller [qh 1 , . . . ] Zhao et al. (2023) found performance (defined as the success rate on downstream task) to be severily (-33.3%) hindered from adopting standard supervised learning objective compared to richer, potentially more complex to learn variational objective. The result of such ablation reflects from the multimodal nature of human demonstrations data, and is consistent with the findings presented by Florence et al. (2022). The authors also ablate the action chunking paradigm, reporting significant performance gains deriving from using action chunking (1% vs. 44% success rate). To reduce acting open-loop, Zhao et al. (2023) also design an inference process consisting in performing inference at every timestep and then aggregate multiple chunks using an exponential moving average (EMA) on the overlapping chunks. 0 , qh In ACT (Figure 30), inference for given observation could be performed by (1) defining prior pω(zo) for the latent variable and (2) decoding an action chunk from sampled latent pω(o), similarily to how sampling from standard VAEs takes place, with the exception that vanilla VAEs typically pose p(zo) p(z) (0, I) and thus skip (1). However, the authors claim that using deterministic procedure to sample benefits policy evaluation, and thus avoid using the conditional prior at all at inference time, effectively using the CVAE framework exclusively to train more expressive decoder. At test time, Zhao et al. (2023) propose simply using = 0, as the conditional prior on used in training is set to be standard Gaussian. Further, conditioning on the observation is achieved through explicitly feeding proprioperceptive and visual observations to the decoder, pθ(az, o) at test time. If at inference is sampled from standard Gaussian, during training is sampled from an approximate posterior distribution qϕ(zo, a), which, however, disregards image observations and exclusively uses proprioperceptive states to form for efficiency reasons. 44 Figure 28 The CVAE encoder used in ACT. Input action chunks are first embedded and aggregated with positional embeddings, before being processed alongside embedded proprioperceptive information, and learned [CLS] token used to aggregate input level information, and predict the style variable z. The encoder is exclusively used to train the decoder, and it is entirely disregarded at inference time. Figure 29 The CVAE decoder used in ACT, comprising of full encoder-decoder Transformer architecture. Camera observations from all camera views are first embedded using pre-trained visual encoders, and then aggregated with the corresponding positional embeddings. Then, the proprioperceptive information and style variable retrieved from the CVAE encoder, are fed to the encoder-decoder Transformer for inference. The encoder shares the matrices K, with the decoder, and is trained to decode fixed position embeddings into action chunks. 45 Figure 30 Action Chunking with Transformer (ACT), as in Zhao et al. (2023). ACT introduces an action chunking paradigm to cope with high-dimensional multi-modal demonstration data, and transformer-based CVAE architecture."
        },
        {
            "title": "4.2.1 Code Example: Training and Using ACT in Practice",
            "content": "Code 7: Training ACT https://github.com/fracapuano/robot-learning-tutorial/snippets/ch4/01_training_act.py 1 from pathlib import Path 2 3 import torch 4 5 from lerobot . configs . types import FeatureType 6 from lerobot . datasets . lerobot_dataset import LeRobotDataset , o D s e a 7 from lerobot . datasets . utils import a _ _ i _ t s 8 from lerobot . policies . act . co nfi gur ati on_ act import ACTConfig 9 from lerobot . policies . act . modeling_act import ACTPolicy 10 from lerobot . policies . factory import e _ _ t _ c o 11 12 13 def e _ t _ e m ( delta_indices : list [ int ] None , fps : int ) -> list [ float ]: if delta_indices is None : return [0] return [ / fps for in delta_indices ] 14 15 17 18 19 20 ou put _ dir ctory = Path ( \" outputs / o _ r g _ o l / act \" ) 21 ou put _ dir ctory . mkdir ( parents = True , exist_ok = True ) 22 23 # Select your device 24 device = torch . device ( \" mps \" ) # or \" cuda \" or \" cpu \" 25 26 dataset_id = \" lerobot / a _ 1 0 1 _ k c \" 27 28 # This specifies the inputs the model will be expecting and the outputs it will produce 29 da ase _me adata = o D s e a ( dataset_id ) 30 features = a _ _ i _ t s ( dataset_metadata . features ) 31 32 ou tp t_ fea tures = { key : ft for key , ft in features . items () if ft . type is FeatureType . ACTION } 33 inp ut_features = { key : ft for key , ft in features . items () if key not in output_features } 34 35 cfg = ACTConfig ( input_features = input_features , output_features = output_features ) 36 policy = ACTPolicy ( cfg ) 37 preprocessor , postprocessor = e _ _ t _ c o ( 46 cfg , dataset_stats = dataset_metada ta . stats 38 39 ) 40 41 policy . train () 42 policy . to ( device ) 43 44 # To perform action chunking , ACT expects given number of actions as targets 45 de ta_ ime tamps = { \" action \" : e _ t _ e m ( cfg . action_delta_indices , dataset_metadata . fps ) , 46 47 } 48 49 # add image features if they are present 50 de ta_ ime tamps = { : e _ t _ e m ( cfg . observation_delta_indices , dataset_metadata . fps ) for in cfg . image_features 51 52 53 } 54 55 # Instantiate the dataset 56 dataset = LeRobotDataset ( dataset_id , delta_timestamps = delta_timestamps ) 57 58 # Create the optimizer and dataloader for offline training 59 optimizer = cfg . _ i e _ s (). build ( policy . parameters ()) 60 batch_size = 32 61 dataloader = torch . utils . data . DataLoader ( dataset , batch_size = batch_size , shuffle = True , pin_memory = device . type != \" cpu \" , drop_last = True , 63 64 65 66 67 ) 68 69 # Number of training steps and logging frequency 70 tra ining_steps = 1 71 log_freq = 1 72 73 # Run training loop 74 step = 0 75 done = False 76 while not done : 77 78 79 80 81 83 84 85 86 87 89 for batch in dataloader : batch = preprocessor ( batch ) loss , _ = policy . forward ( batch ) loss . backward () optimizer . step () optimizer . zero_grad () if step % log_freq == 0: print ( \" step : { step } loss : { loss . item ():.3 } \" ) step += 1 if step >= training_steps : done = True break 90 91 # Save the policy checkpoint , alongside the pre / post processors 92 policy . save_pretrained ( output_directory ) 93 preprocessor . save_pretrained ( output_d irectory ) 94 postprocessor . save_pretrained ( output_directory ) 95 96 # Save all assets to the Hub 97 policy . push_to_hub ( \" fracapuano / o _ r g _ o l _ _ m _ e \" ) 98 preprocessor . push_to_hub ( \" fracapuano / o _ r g _ o l _ _ m _ e e \" ) 99 postprocessor . push_to_hub ( \" fracapuano / o _ r g _ o l _ _ m _ e e \" ) 47 Code 8: Using ACT https://github.com/fracapuano/robot-learning-tutorial/snippets/ch4/02_using_act.py 1 import torch 2 3 from lerobot . cameras . opencv . f r o _ n import pe nC Ca er Co fi 4 from lerobot . datasets . lerobot_dataset import o D s e a 5 from lerobot . policies . act . modeling_act import ACTPolicy 6 from lerobot . policies . factory import e _ _ t _ c o 7 from lerobot . policies . utils import build_inference_frame , ake _ro bot _ac tio 8 from lerobot . robots . so100_follower . f _ 1 0 0 _ l r import 1 0 0 l we n 9 from lerobot . robots . so100_follower . so100_follower import SO100Follower 10 11 device = torch . device ( \" mps \" ) 12 model_id = \" fracapuano / o _ r g _ o l _ _ m _ e \" 13 model = ACTPolicy . from_pretrained ( model_id ) # or \" cuda \" or \" cpu \" 14 15 dataset_id = \" lerobot / a _ 1 0 1 _ k c \" 16 # This only downloads the metadata for the dataset , 10 of MB even for large - scale datasets 17 da ase _me adata = o D s e a ( dataset_id ) 18 preprocess , postprocess = e _ _ t _ c o ( model . config , dataset_stats = dataset_metadata . stats 19 20 ) 21 22 # # find ports using lerobot - find - port 23 follower_port = ... # something like \"/ dev / tty . us d 5 8 7 6 0 4 3 1 63 1 \" 24 25 # # the robot ids are used the load the right calibration files 26 follower_id = ... # something like \" follower_so100 \" 27 28 MAX_EPISODES = 5 29 _ P _ _ S = 20 30 31 # Robot and environment configuration 32 # Camera keys must match the name and resolutions of the ones used for training ! 33 # You can check the camera keys expected by model in the info . json card on the Hub 34 camera_config = { \" side \" : pe CV am ra on ig ( index_or_path =1 , width =640 , height =480 , fps =30) , \" up \" : Op nC Ca er aC nf ( index_or_path =1 , width =640 , height =480 , fps =30) , 35 36 37 } 38 39 robot_cfg = 1 0 0 l ow o g ( port = follower_port , id = follower_id , cameras = camera_config ) 40 robot = SO100Follower ( robot_cfg ) 41 robot . connect () 42 43 for _ in range ( MAX_EPISODES ): 45 46 47 48 49 51 52 53 54 55 57 for _ in range ( _ P _ _ S ): obs = robot . get_observation () obs_frame = l _ e c _ m ( obs , dataset_metadata . features , device ) obs = preprocess ( obs_frame ) action = model . select_action ( obs ) action = postprocess ( action ) action = mak e_r obo t_a cti on ( action , dataset_metadata . features ) robot . send_action ( action ) print ( \" Episode finished ! Starting new episode ... \" )"
        },
        {
            "title": "4.3 Diffusion Policy\nDMs have proven very effective in approximating complex highly dimensional distributions, such as distributions over\nimages (Ho et al., 2020) or videos (Polyak et al., 2025), thanks to their inherent capability to deal with multimodal\ndata, and their training stability. In Diffusion Policy (DP), Chi et al. (2024) present an application of DMs the",
            "content": "48 Figure 31 The Diffusion Policy archicture, as in Chi et al. (2024). stack of Ho previous observations is used as external conditioning to denoise group of Ha actions. Conditioning is performed at every layer of U-Net block. Diffusion Policy allows to obtain fully-formed action chunks with as little as = 10 denoising steps. field of robot learning, leveraging diffusion to model expert demonstrations in variety of simulated and real-world tasks. Similarily to ACT (Zhao et al., 2023), Chi et al. (2024) (1) adopt modified observation-conditioned target distribution instead of the full joint p(o, a), and (2) predict multiple actions into the future instead of single action. Besides the intractability of the observations marginal pθ(o) given pθ(o, a), DPs choice to model the data distribution through pθ(ao) also stems from the computational burden of diffusion at test time: generating actions together with observations would require large number of denoising stepsan unnecessarily slow and ultimately unhelpful process, given that robotics focuses on producing controls rather than reconstructing observations. In practice, conditioning on observation data is achieved conditioning the noise regressor ϵθ introduced in eq. 44 on stack of Ho observations, resulting in the conditional, simplified diffusion objective: L(θ) = Et,at:t+Ha ,ϵ (cid:2)ϵ ϵθ( U({1, . . . , }), αtat:t+Ha + ϵ at:t+Ha , otHo:t D, 1 αt, t, otHo:t)2(cid:3), ϵ (0, I). (51) Note how in eq. 51 the noise regressor is conditioned on both the latent variable rank and on stack of previous observations otHo:t. Chi et al. (2024) claim the combination of (1) conditioning on horizon of previous observations and (2) predicting multiple actions into the future allows DP to commit to specific modes in the data at inference time, which proves essential for good performance and avoiding undecisiveness. Figure 31 shows the convolution-based version of the architecture proposed by Chi et al. (2024), illustrating inference on single sample drawn from D, for simplicity. The starting, arbitrarily noisy chunk of Ha actions at:t+Ha is first mapped to (learned) high-dimensional space. Similarily, both image observations and poses are also embedded before being aggregated to the action embeddings. Then, U-Net (Ronneberger et al., 2015) is trained to regress the noise added into at:t+Ha , conditioned on observation information at every layer, thus seeking to optimize eq. 51. At inference time, the noise predictor is used to predict the quantity of noise at every [T, . . . , 0] and iteratively . , reversing the diffusion process simulated in training conditioned on otHo:t to predict at:t+Ha subtract it from at:t+Ha DP can be trained with as little as 50-150 demos (ca. 15-60 minutes of teleoperation data), and exhibit strong performance on variety of simulated and real-world tasks, including dexterous and deformable manipulation tasks such as sauce pouring and yoga-mat unrolling. Notably, the authors ablated the relevance of using RGB camera streams as input to their policy, and observed how high frame-rate visual observations can be used to attain performance (measured as success rate) comparable to that of state-based policies, which are typically trained in simulation with priviledged information not directly available in real-world deployments. As high-frame rate RGB inputs naturally accomodate for dynamic, fast changing environments, Chi et al. (2024)s conclusion offers significant evidence for learning streamlined control policies directly from pixels. In their work, Chi et al. (2024) also ablate the performance of DP against the size of the dataset collected, showing that DP reliably outperforms the considered baseline for 49 all benchmark sizes considered. Further, in order accelerate inference, Chi et al. (2024) employ Denoising Diffusion Implicit Models (Song et al., 2022), variant of Denoising Diffusion Probabilistic Models (Ho et al., 2020) (DDPM) adopting strictly deterministic denoising paradigm (differently from DDPMs natively stochastic one) inducing the same final distributions as DDPMs, and yet resulting in 10x less denoising steps at inference time (Chi et al., 2024). Across range of simulated and real-world tasks, Chi et al. (2024) find DPs particularly performant when modeling ϵθ with transformer-based network, although the authors note the increased sensitivity of transformer networks to hyperparameters. Thus, Chi et al. (2024) explicitly recommend starting out with simpler, convolutionbased architecture for diffusion (Figure 31), which is however reported to be biased towards learning low-frequency components (Tancik et al., 2020), and thus may prove more challenging to train with non-smooth action sequences."
        },
        {
            "title": "4.3.1 Code Example: Training and Using Diffusion Policies in Practice",
            "content": "Code 9: Training Diffusion Policy https://github.com/fracapuano/robot-learning-tutorial/blob/main/snippets/ch4/03_training_diffusion. py 1 from pathlib import Path 2 3 import torch 4 5 from lerobot . configs . types import FeatureType 6 from lerobot . datasets . lerobot_dataset import LeRobotDataset , o D s e a 7 from lerobot . datasets . utils import a _ _ i _ t s 8 from lerobot . policies . diffusion . f r o _ f o import DiffusionConfig 9 from lerobot . policies . diffusion . mo el ng _ di fu io import DiffusionPolicy 10 from lerobot . policies . factory import e _ _ t _ c o 11 12 13 def e _ t _ e m ( delta_indices : list [ int ] None , fps : int ) -> list [ float ]: if delta_indices is None : return [0] return [ / fps for in delta_indices ] 14 15 17 18 19 20 ou put _ dir ctory = Path ( \" outputs / o _ r g _ o l / diffusion \" ) 21 ou put _ dir ctory . mkdir ( parents = True , exist_ok = True ) 22 23 # Select your device 24 device = torch . device ( \" mps \" ) # or \" cuda \" or \" cpu \" 25 26 dataset_id = \" lerobot / a _ 1 0 1 _ k c \" 27 28 # This specifies the inputs the model will be expecting and the outputs it will produce 29 da ase _me adata = o D s e a ( dataset_id ) 30 features = a _ _ i _ t s ( dataset_metadata . features ) 31 32 ou tp t_ fea tures = { key : ft for key , ft in features . items () if ft . type is FeatureType . ACTION } 33 inp ut_features = { key : ft for key , ft in features . items () if key not in output_features } 34 35 cfg = iff usionConfig ( input_features = input_features , output_features = output_features ) 36 policy = DiffusionPolicy ( cfg ) 37 preprocessor , postprocessor = e _ _ t _ c o ( cfg , dataset_stats = dataset_metada ta . stats 38 39 ) 40 41 policy . train () 42 policy . to ( device ) 43 44 # To perform action chunking , ACT expects given number of actions as targets 45 de ta_ ime tamps = { 46 47 48 49 \" observation . state \" : e _ t _ e m ( cfg . observation_delta_indices , dataset_metadata . fps ) , \" action \" : e _ t _ e m ( cfg . action_delta_indices , dataset_metadata . fps ) , 50 50 } 51 52 # add image features if they are present 53 de ta_ ime tamps = { : e _ t _ e m ( cfg . observation_delta_indices , dataset_metadata . fps ) for in cfg . image_features 54 55 56 } 57 58 # Instantiate the dataset 59 dataset = LeRobotDataset ( dataset_id , delta_timestamps = delta_timestamps ) 60 61 # Create the optimizer and dataloader for offline training 62 optimizer = cfg . _ i e _ s (). build ( policy . parameters ()) 63 batch_size = 32 64 dataloader = torch . utils . data . DataLoader ( dataset , batch_size = batch_size , shuffle = True , pin_memory = device . type != \" cpu \" , drop_last = True , 66 67 68 69 70 ) 71 72 # Number of training steps and logging frequency 73 tra ining_steps = 1 74 log_freq = 1 75 76 # Run training loop 77 step = 0 78 done = False 79 while not done : 80 81 82 83 84 86 87 88 89 90 92 for batch in dataloader : batch = preprocessor ( batch ) loss , _ = policy . forward ( batch ) loss . backward () optimizer . step () optimizer . zero_grad () if step % log_freq == 0: print ( \" step : { step } loss : { loss . item ():.3 } \" ) step += 1 if step >= training_steps : done = True break 93 94 # Save the policy checkpoint , alongside the pre / post processors 95 policy . save_pretrained ( output_directory ) 96 preprocessor . save_pretrained ( output_d irectory ) 97 postprocessor . save_pretrained ( output_directory ) 98 99 # Save all assets to the Hub 100 policy . push_to_hub ( \" fracapuano / o _ r g _ o l _ f o _ m _ e \" ) 101 preprocessor . push_to_hub ( \" fracapuano / o _ r g _ o l _ f o _ m _ e \" ) 102 postprocessor . push_to_hub ( \" fracapuano / o _ r g _ o l _ f o _ m _ e \" ) Code 10: Using Diffusion Policy https://github.com/fracapuano/robot-learning-tutorial/blob/main/snippets/ch4/04_using_diffusion.py 1 import torch 2 3 from lerobot . cameras . opencv . f r o _ n import pe nC Ca er Co fi 4 from lerobot . datasets . lerobot_dataset import o D s e a 5 from lerobot . policies . diffusion . mo el ng _ di fu io import DiffusionPolicy 6 from lerobot . policies . factory import e _ _ t _ c o 7 from lerobot . policies . utils import build_inference_frame , ake _ro bot _ac tio 8 from lerobot . robots . so100_follower . f _ 1 0 0 _ l r import 1 0 0 l we n 9 from lerobot . robots . so100_follower . so100_follower import SO100Follower 51 10 11 device = torch . device ( \" mps \" ) 12 model_id = \" fracapuano / o _ r g _ o l _ f o _ m _ e \" # or \" cuda \" or \" cpu \" 13 14 model = Dif fusionPolicy . from_pretrained ( model_id ) 15 16 dataset_id = \" lerobot / a _ 1 0 1 _ k c \" 17 # This only downloads the metadata for the dataset , 10 of MB even for large - scale datasets 18 da ase _me adata = o D s e a ( dataset_id ) 19 preprocess , postprocess = e _ _ t _ c o ( model . config , model_id , dataset_stats = dataset_metadata . stats 20 21 ) 22 23 MAX_EPISODES = 5 24 _ P _ _ S = 20 25 26 27 # # find ports using lerobot - find - port 28 follower_port = ... # something like \"/ dev / tty . us d 5 8 7 6 0 4 3 1 63 1 \" 29 30 # # the robot ids are used the load the right calibration files 31 follower_id = ... # something like \" follower_so100 \" 32 33 # Robot and environment configuration 34 # Camera keys must match the name and resolutions of the ones used for training ! 35 # You can check the camera keys expected by model in the info . json card on the Hub 36 camera_config = { \" side \" : pe CV am ra on ig ( index_or_path =1 , width =640 , height =480 , fps =30) , \" up \" : Op nC Ca er aC nf ( index_or_path =1 , width =640 , height =480 , fps =30) , 37 38 39 } 40 41 robot_cfg = 1 0 0 l ow o g ( port = follower_port , id = follower_id , cameras = camera_config ) 42 robot = SO100Follower ( robot_cfg ) 43 robot . connect () 44 45 46 for _ in range ( MAX_EPISODES ): 47 48 49 51 52 53 54 55 57 58 for _ in range ( _ P _ _ S ): obs = robot . get_observation () obs_frame = l _ e c _ m ( obs , dataset_metadata . features , device ) obs = preprocess ( obs_frame ) action = model . select_action ( obs ) action = postprocess ( action ) action = mak e_r obo t_a cti on ( action , dataset_metadata . features ) robot . send_action ( action ) print ( \" Episode finished ! Starting new episode ... \" )"
        },
        {
            "title": "4.4 Optimized Inference\n(cid:1) = At with At a sequence\nModern visuomotor policies output action chunks–sequences π(ot) = (cid:0)at, at+1, . . . , at+Ha\nof Ha ≫ 1 low-level commands scheduled for execution in an action queue, all originating from a single environment\nobservation, ot. Predicting series of actions instead of single commands proved essential in learning complex, multi-\nmodal behavior (Zhao et al., 2023; Chi et al., 2024), and it also holds the premise to be useful to optimize how\ninference is carried out in practice.",
            "content": "A robot may indeed execute an entire action chunk At before new observation ot+Ha is passed to the policy π to predict the next chunk, which would result in open-loop control between observations captured every Ha timesteps. Zhao et al. (2023) adopt different strategy, whereby the robot controller interleaves chunk prediction At π(ot) and chunk consumption at PopFront(At), and computes new chunk of actions at every timestep t, to then aggregate the predicted chunks on overlapping sections. While adaptiveevery observation at every timestep ot is processedsuch an approach relies on running inference continuously, which can be prohibitive in resource-constrained scenarios, such as edge deployments. less resource-intensive approach is to entirely exhaust the chunk before predicting new chunk of actions, strategy we refer to as synchronous (sync) inference. Sync inference allocates 52 Figure 32 Asynchronous inference. Illustration of the asynchronous inference stack. Note that the policy can be run on remote server, possibly with GPUs. computation every Ha timesteps, resulting in reduced computational burden (on average) at control time. In contrast, sync inference also inherently hinders the responsiveness of robot systems, introducing blind lags due to the robot being idle while computing A. One can use the fact that policies output multiple actions at the same time to directly (1) the lack of adaptiveness and (2) the presence of lags at runtime by decoupling action chunk prediction from action execution at PopFront(At). This decoupled stack, which we refer to as asynchronous (async) inference (1), also enables optimized inference by allowing action-chunk inference to run on separate machine, typically equipped with better computational resources than the ones onboard robot. In async inference, RobotClient sends an observation ot to PolicyServer, receiving an action chunk At once inference is complete (Figure 32). In this, we avoid execution lags by triggering chunk prediction while the control loop is still consuming previously available chunk, aggregating the previous and incoming chunks whenever the latter is available to the RobotClient. In turn, async-inference tightens the loop between action prediction and action execution efficienty, by increasing the frequency at which observations are processed for chunk prediction while not running inference at every timestep. Crucially, decoupling action prediction from action execution also allows to allocate more computational resources on remote policy server sending actions to the robot client over the network. Algorithm 1 Asynchronous inference control-loop 1: Input: horizon , chunk size Ha, threshold [0, 1] 2: Init: capture o0; send o0 to PolicyServer; receive A0 π(o0) 3: for to Ha do 4: 5: at PopFront(At) Execute(at) if < then At Ha capture new observation, ot+1 if NeedsProcessing (ot+1) then 6: 7: 8: 9: 10: 11: 12: async_handle AsyncInfer(ot+1) At+1 π(ot+1) At+1 (At, At+1) end if if NotCompleted(async_handle) then end if At+1 At 13: 14: 15: 16: 17: end for end if execute action at step queue below threshold similarity filter, or triggers direct processing Trigger new chunk prediction (non blocking) New queue is predicted with the policy aggregate overlaps (if any) No update on queue (inference is not over just yet) 53 Figure 33 Action queue size evolution at runtime for various levels of when (A) not filtering out observation based on joint-space similarity and (B) filtering out near-duplicates observation, measuring their similarity in joint-space. In practice, async inference (1) tightens the control loop by capturing observations more often, eliminating idle gaps at runtime (2) and directly allows to run inference on more powerful computational resources than the ones typically available onboard autonomous robotic platforms. Algorithmically, one can attain (1) on the RobotClient-side by consuming actions from readily available queue until given condition on the number of remaining actions in the queue (At/Ha < g) is met. When this condition is triggered, new observation of the environment is captured and sent to the (possibly remote) PolicyServer. To avoid redundant server calls and erratic behavior at runtime observations are compared in joint-space, and near-duplicates are dropped. Two observations are considered near-duplicates if their distance in joint-space falls under predetermined threshold, dlim R+. Importantly, should the queue available to the robot client eventually empty out, the most recent observation is processed regardless of similarity. Interestingly, the behavior of async inference can be studied analytically. First, let ℓ be random variable modeling the time needed to receive an action chunk after sending an observation o, i.e. the sum of (1) the time to send across the observation between the RobotClient and PolicyServer, tCS (2) the inference latency on the PolicyServer, ℓS and (3) the time to send between the PolicyServer and RobotClient, tSC. Under the (reasonable) assumption of independence, E[ℓ] = E[tCS] + E[ℓS] + E[tSC], which can be further simplified to E[ℓ] E[ℓS], assuming communication time is (1) equal in both directions and (2) negligible with respect to the inference latency. Second, let be the environments control cycle. With real-world frame-rate of 30 frames-per-second (fps), = 33ms. Consequently, exhausted queues at runtimei.e. being idle awaiting for new chunkare avoided for . In this, the action queue threshold below which to capture and send new observation for processing plays major role relatively to the availability of actions to the RobotClient. E[ℓS ]/t Ha Figure 33 illustrates how the size of the action chunk At evolves over time for three representative values of g, detailing the following key scenarios: Sequential limit (g = 0). The client drains the entire chunk before forwarding new observation to the server. During the round-trip latency needed to compute the next chunk, the queue is empty, leaving the robot incapable of acting. This reproduces the behavior of fully sequential deployment and results in an average of E[ℓS] idle seconds. Asynchronous inference (g (0, 1)). Allowing the client to consume 1 fraction of its available queue At1 before triggering inference for new action queue At, computation is amortized while keeping the queue from emptying. The overlap between successive chunks provides buffer against modeling errors without the full cost of the = 1 regime. The updated queue At is obtained aggregating queues on the overlapping timesteps between At1 and the incoming At. Sync-inference limit (g = 1). As an extreme case, and in keeping with Zhao et al. (2023), an observation is sent at every timestep. The queue is therefore almost always filled, with only minor saw-tooth due to t/E[ℓs] < 1. While maximally reactive, this setting incurs one forward pass per control tick and can prove prohibitively expensive on limited hardware. Importantly, because the client is consuming actions while the server computes the next chunk, the available queue never gets entirely filled. Figure 33 emphasizes the trade-off governed by g: small values of result in idle periods, whereas 1 assumes 54 highly accurate model and pays significant compute price. In practice, choosing (0, 1) allows to strike balance between reactivity against resource budgets. If not for the aforementioned similarity filter, the RobotClient would send observations for processing every (1g)Hat seconds, receiving new chunk of actions every (1g)Hat+E[ℓS], on average. The presence of the filter for observation similarity dilates this processing time, and serves the scope of avoiding the robot stalling due to the queue being constantly integrated with an incoming, nearly identical, action chunk. In particular, Figure 33 results in queue which is filled with incoming actions unless near-duplicate observations are filtered out from the processing pipeline. For clarity, the red arrow in 33 highlights timestep where the observation similarity mechanism is bypassed, forcing (nearly identical) observation to be processed as the queue results empty."
        },
        {
            "title": "4.4.1 Code Example: Using Async Inference",
            "content": "Code 11: Spinning up Remote Server https://github.com/fracapuano/robot-learning-tutorial/blob/main/snippets/ch4/05_policy_server.py 1 from lerobot . async_inference . configs import ol cy Se ve Co fi 2 from lerobot . async_inference . policy_server import serve 3 4 host = ... 5 port = ... # something like \"127.0.0.1\" if you ' re exposing to localhost # something like 8080 6 7 config = ol cy er er on ig ( host = host , port = port , 9 10 ) 11 serve ( config ) Code 12: Attaching Robot Client https://github.com/fracapuano/robot-learning-tutorial/blob/main/snippets/ch4/06_robot_client.py 1 import threading 2 from lerobot . robots . so100_follower import 10 0 l r f 3 from lerobot . cameras . opencv . f r o _ n import pe nC Ca er Co fi 4 from lerobot . async_inference . configs import Rob otC lie ntC onf ig 5 from lerobot . async_inference . robot_client import RobotClient 6 from lerobot . async_inference . helpers import u z _ i _ u _ e 7 8 # these cameras must match the ones expected by the policy ( use lerobot - find - cameras ) 9 # check the config . json on the Hub for the policy you are using to see the expected camera specs 10 camera_cfg = { \" top \" : pe nC Ca er Co fi ( index_or_path =0 , width =640 , height =480 , fps =30) , \" side \" : pe CV am ra on ig ( index_or_path =1 , width =640 , height =480 , fps =30) 11 12 13 } 14 15 # # find ports using lerobot - find - port 16 follower_port = ... # something like \"/ dev / tty . us d 5 8 7 6 0 4 3 1 63 1 \" 17 18 # # the robot ids are used the load the right calibration files 19 follower_id = ... # something like \" follower_so100 \" 20 21 robot_cfg = 1 0 0 l ow o g ( port = follower_port , id = follower_id , cameras = camera_cfg 22 23 24 25 ) 26 27 ser ver_address = ... # something like 127.0.0.1:8080 if using localhost 28 29 # 3. Create client configuration 30 client_cfg = Rob otC li ent Con fig ( 31 robot = robot_cfg , 55 server_address = server_address , policy_device = \" mps \" , policy_type = \" smolvla \" , t n _ e _ _ h = \" fracapuano / smolvla_async \" , n _ e _ e l =0.5 , ac on s_p er _ch unk =50 , # # make sure this is less than the max actions of the policy 32 33 34 36 37 38 ) 39 40 # 4. Create and start client 41 client = RobotClient ( client_cfg ) 42 43 # 5. Provide textual description of the task 44 task = ... 45 46 if client . start (): 48 49 50 51 52 54 55 56 57 58 # Start action receiver thread i _ e r _ e = threading . Thread ( target = client . receive_actions , daemon = True ) i _ e r _ e . start () try : # Run the control loop client . control_loop ( task ) except Key boa rdI nte rru pt : client . stop () i _ e r _ e . join () # ( Optionally ) plot the action queue size u z _ i _ u _ e ( client . ac tio n_q ueu e_s ize ) 56 Figure 34 Fields within ML such as Computer Vision and NLP converged on the development of foundation models, trained on variety of large scale models and capable to perform multiple downstream tasks (top). Conversely, robotics suffered from limited standardization in terms of the architectures used, and siloed, task specific datasets, incurring in high degree of fragmentation which traditionally hindered the development of generalist models for robotics in favour of task-specific models (bottom)."
        },
        {
            "title": "Specialization is for insects",
            "content": "Robert A. Heinlein TL;DR Openly available, large-scale datasets and the development of stable-to-train, expressive and efficient architectures fostered research on the development of generalist robot policies that can operate across embodiment and tasks. The advent of large models trained on internet-scale datasets has drastically influenced fields like Computer Vision (CV) and Natural Language Processing (NLP), shifting the previously task-specific paradigm towards combining (1) an initial, task-agnostic large-scale pre-training stage and (2) task-specific, adjustment phase. This pre-train-and-adaptat paradigm has now largely replaced more classic approaches consisting of task-specific data collection, curation and model training in many subdomains within CV and NLP, and it is motivated by the main drawback of limited scalability for task-specific approaches, which have been traditionally more labor intensive. Factors including (1) the advancements in generalist models learned with self-supervision for perception (Oquab et al., 2024) or semantic understanding (Devlin et al., 2019) and (2) the popularization of collective efforts to aggregate large-scale openly available datasets (ONeill et al., 2025; Khazatsky et al., 2025) are increasingly pushing the field of robot learning towards the pre-train-and-adapt paradigm. This shift taps into the long-standing challenge of developing generalist robot policies, and holds the premise to surpass traditionally siloed approaches to robotics problems and develop foundation robotics model. While Section 4 introduced methods for learning single-task policies such as ACT or Diffusion Policy, in this section we present advancements in developing generalist, multi-task, policies, capable of performing wide range of tasks across different environments and embodiments, and guided by unstructured instructions typically given in plain, natural language. 57 Figure 35 Early efforts in the development of generalist models for robotics include BC-Zero (Jang et al., 2022), RT-1 (Brohan et al., 2023b), and RT-2 (Brohan et al., 2023a): large scale models trained on thousands of demonstrations. The open release of the Open-X (ONeill et al., 2025) and DROID datasets (Khazatsky et al., 2025) fostered the development of open source models: OpenVLA (Kim et al., 2024), π0 (Black et al., 2024) and SmolVLA (Shukor et al., 2025)."
        },
        {
            "title": "5.1 Preliminaries: Models and Data\nThe remarkable success of foundation models in NLP and CV seems to be increasingly predicated on two core\nprinciples: architectural innovation and (joint) data-compute scaling. Indeed, the transformer architecture proved very\neffective in capturing long-range dependencies in a variety of data formats, and its stability and expressivity made it\nthe de facto standard for modern large-scale models trained on internet-scale datasets. However, in stark contrast with\nlarge-scale NLP and CV datasets (Raffel et al., 2023; Deng et al., 2009), robotics has historically developed around\nsmall, task-specific datasets. In turn, this traditionally hindered scalability across problems as well as results, posing\nconcrete challenges to developing general-purpose robot learning algorithms. Indeed, differently from the wealth\nof relatively readily-available task-agnostic text and images datasets on the internet, robotics data is intrinsically\nembodied and thus task-specific: datasets collected for manipulation differ significantly from locomotion. In particular,\nsince each expert trajectory is tied to a specific robot platform and the operating conditions of its environment\nand task, data heterogeneity has long posed a methodological challenge for scaling robotics datasets via aggregation.\nFurther, datasets consisting of expert demonstrations are (1) intrinsically more expensive to collect and (2) notoriously\nheterogeneous—different human experts may perform the same task in very different. Beyond this, heterogeneity also\nraises conceptual issues: naively mixing data across embodiments can induce negative transfer, as control strategies\ndeveloped in isolation for different robot systems in different environments may even conflict when combined. Thus,\nthe high degree of fragmentation of robotics datasets and tasks has traditionally led to the development of specialist\npolicies, trained on small, task-specific datasets, developed to perform well at their designated task but that fail to\ngeneralize to new deployment scenarios (Figure 34).",
            "content": "Driven by the goal of developing generalist robot policies, the research community has increasingly explored how insights and techniques from other areas of ML can be integrated into robotics. Figure 35 shows timeline of some of the most popular contributions attempting at developing generalist policies. Starting from BC-Zero, latent variable model trained on 25k+ demonstrations, the field has now evolved into π0, transformer-based model trained on 10M+ demonstrations and exhibiting strong few-shot capabilities across tasks and embodiments. In between, Robotics Transformer 1 (RT-1) (Brohan et al., 2023b) represented significant step in the direction of developing generalist robot policies over prior work including (1) BC-Zero (Jang et al., 2022) and (2) Gato (Reed et al., 2022), in that Brohan et al. (2023b) use much larger and diverse set of training tasks compared to both BC-Zero and Gato. In particular, RT-1 uses transformer architecture, and is trained on as many as 130k human-recorded trajectories collected over 13 robots and over 17 months. RT-1 learns to process history of camera images and natural language instruction, and feeds the resulting sequence of high-dimensional tokens to transformer, trained using classification loss on discretized actions space consisting of six different 256-bins, one for each joint of 6-dof robotic arm. In follow-up work, the same group of authors propose modified method to learn generalist models, leveraging (1) more powerful architecture and (2) scaling up the dataset used (Brohan et al., 2023a, RT-2). In RT-2, Brohan et al. (2023a) propose inheriting internet-scale semantic knowledge from large-scale multi-modal datasets to learn single, unified model for robotics control. Such model, termed Vision-Language-Action (VLA) in the original RT-2 paper, effectively casts robot control as language-modeling problem, and in particular as Visual Question-Answering (VQ&A) task, in which the output token space used to represent textual tokens is shared with the 8-bits tokens used 58 Figure 36 Robot learning is undergoing paradigmatic shift: centralized data collections (A, left) are increasingly larger, often comprising millions of demonstrations, while (A, right) decentralized data collection efforts are becoming an alternative for large scale data collection. (B) Generalist models are also becoming increasingly smaller and easier to run on limited hardware. to represent the 256 (28) actuation levels of 6-dof robot. In their work, Brohan et al. (2023a) propose co-fine-tuning large-scale VLMs such as PaLIX (Chen et al., 2023) or PaLM-E (Driess et al., 2023) on mix of (1) web and (2) robotics data, complementing VQ&A training with robotics-specific signal, and learning to directly output robot actions in shared token space for visual and language inputs. In their work, the authors claim using large models trained on internet-scale data as backbones for VLAs allows models to tap into the rich semantic knowledge embedded in the VLMs parameters, interpreting instructions and unseen objects by connecting them to concepts acquired while pre-training. For instance, Brohan et al. (2023a) show that while RT-2 has never been explicitly trained to repurpose tools for hammering task, it can still combine its semantic understanding of images, so that when asked which object between (1) piece of paper, (2) pair of headphones or (3) rock may be used instead of hammer, it correctly answers (3). Traditionally, research efforts revolved around not only training models, but also proposing datasets for the community, costly and time-consuming process. Due to the aforementioned embodiment gap, the data used in research efforts in robot learning have traditionally proved rather fragmented, tailored to the specific task considered by the specific group of researchers who collected it, which ultimately hindered integration. The Open X-Embodiment project (ONeill et al., 2025) was landmark collaboration effort to address data fragmentation, by curating the aggregation of 60 existing robotics datasets from 22 different robot embodiments and 21 institutions across the world, and resulted in total 1.4M of cross-embodiments, cross-tasks, openly-available trajectories. Besides the contribution of an aggregate, large scale dataset, ONeill et al. (2025) also demonstrated significant positive transfer across tasks and embodiments, showing that single model trained on multi-embodiment data can outperform specialist models trained on their respective single-embodiment datasets. The Distributed Robot Interaction Dataset (DROID) (Khazatsky et al., 2025) represents another significant step towards addressing the problem of scarse and disaggregated data in robot learning, providing unique dataset consisting of 75k+ human demonstrations collected in realistic (in-the-wild ) manipulation settings, providing another cornerstone for building general-purpose robot policies. Recently, foundational datasets curated through large, centralized efforts, are increasingly complemented by decentralized, community-driven contributions of robotics data. Software libraries like lerobot have been instrumental in enabling decentralized collection of large amounts of data, providing the infrastructure for researchers and practitioners to easily contribute trajectories from wide range of embodiments, democratizing data access via distributed collection. Despite these advancements, the success of large, proprietary models like RT-1 and RT-2, highlighted growing accessibility gap in robotics research, as training and deploying large-scale robotics foundation models requires computational resources simply unattainable for most research institutions. The OpenVLA project (Kim et al., 2024) emerged in direct contrast to traditionally closed-source efforts to develop VLAs. In particular, Kim et al. (2024) trained OpenVLA by exclusively leveraging openly available data (970k+ trajectories from the Open-X dataset), and openly shared their training recipes alongside the model weights. Architecturally, OpenVLA integrates pre-trained vision encoder to project visual tokens into the embedding space of the Llama2-7B (Touvron et al., 2023) language-model backbone. The language model backbone is then used to predict discrete action tokens over 256 activation levels. Figure 36 shows the current trends in robot learning in terms of size and nature of the robotics datasets contributed, together with the size and accessibility of the available models. As datasets collected via centralized, cross-institutions cooperation of increasing size are made available for the research community, decentralized datasets collected by individual researchers and practitioners also gained traction, closing the gap with academic benchmarks thanks to 59 community-contributed datasets. Further, models used across tasks and embodiments are increasingly becoming much more compute-efficient, and as result the models size has been consistently reducing over time, with consequent gains for autonomous robots in real-world, resource-constrained environments."
        },
        {
            "title": "5.2 VLAs\nModern recipes to train large scale VLAs extend early efforts to learn foundation models from large amounts of data via\nBC, introducing significant advancements concerning both architectural and procedural aspects. From an architectural\nperspective, modern VLAs such as π0 (Black et al., 2024) leverage a unified transformer model for efficiency of\ncomputation, while maintaining specialized sub-components within the model for visual perception and action\nprediction, enabling cross-task performance via language conditioning. Crucially, modern VLAs includingπ0 (Black\net al., 2024) and SmolVLA (Shukor et al., 2025) adopt unified transformer models employing disjoint set of weights\n(experts) for both compute-efficient visual-semantic understanding as well as control. Procedurally, VLAs complement\nadvanced Vision-Language Model (VLM) backbones with action-specific modules (1) adopting mid-sized action experts\nto model continuous actions distributions p(at:t+Ha |ot)—avoiding discrete action tokens entirely—and (2) relying\non action chunking (Zhao et al., 2023, Section 4) as a strategy to reduce error compounding when predicting multiple\nactions learning from inherently non-i.i.d. data, such as demonstration data.",
            "content": "These architectural and procedural innovations present three benefits over task-specific methods. First, developing architectures that exploit internet-scale pre-trained backbones allows to fully capitalize on the vast world knowledge and skills state-of-the-art VLMs exhibit, preventig models from needing to learn visual, linguistic and semantic concepts from scratch. Second, using generative models for continuous action distributions allows to learn rich, multimodal data distributions, much more likely scenario in the big-data regime which is typically tackled while developing generalist policies. Further, introducing separate components for perception and action planning enable using Mixture of Experts (MoE) architectures (Fedus et al., 2022), which are often more efficient to runa key feature for models deployed in real-world scenarios. This new paradigm has been at the core of some of the most capable generalist policies developed to date, capable to few-shot adapt to novel tasks and to perform highly dexterous manipulation tasks ranging from end-to-end folding laundry to bussing tables (Black et al., 2024)."
        },
        {
            "title": "5.2.1 VLMs for VLAs\nVLMs are designed to handle both visual and textual modalities, most commonly by taking both images and text as\ninputs, generating text conditioned on the visual context. Recent advances in VLMs have been driven by the success\nof LLMs, with many approaches building upon pretrained LLMs and adopting similar training paradigms to the\nones used in language modeling. Typically, VLMs (Alayrac et al., 2022; Laurençon et al., 2024; Lin et al., 2024) are\nconstructed by integrating a pretrained vision encoder (Radford et al., 2021; Zhai et al., 2023; Fini et al., 2024) with\na pretrained LLM (Grattafiori et al., 2024; Jiang et al., 2023). Training then proceeds in multiple multimodal stages,\nbeginning with a large-scale pretraining on datasets containing image-text pairs (Schuhmann et al., 2022; Byeon et al.,\n2022) and interleaved vision-language corpora (Laurençon et al., 2023; Zhu et al., 2023), all followed by a supervised\nfine-tuning stage on instruction-tuning datasets (Liu et al., 2023; Tong et al., 2024; Laurençon et al., 2024). The\ninherent multimodal nature of VLMs enables them to jointly reason over vision and language. Pre-training on vast\ninternet-scale datasets allows these models to associate visual patterns with textual descriptions, thereby acquiring a\nrich semantic understanding of the world—knowledge about objects, their properties, and relationships—without\nexplicit supervision for each concept. In turn, integrating VLMs as the perceptual backbone for VLAs allows the\nlatter to inherit rich, contextual world knowledge from the VLM, sidestepping the need to re-learn visual and semantic\nrepresentations. In principle, this also allows the robot to ground high-level natural language instructions in its visual\ncontext, and possibly recognize objects by connecting them to the pre-trained concepts absorbed during pre-training,\nimproving on the possibility to generalize to novel scenarios.",
            "content": "Recently, compute efficiency has also become central focus in multi-modal research. Several works aim to reduce training costs by using smaller, more diverse datasets (Liu et al., 2023; Dai et al., 2023; Bai et al., 2025; Zhu et al., 2024; Tong et al., 2024), training smaller-scale models (Marafioti et al., 2025; Korrapati, 2024; Yao et al., 2024), or by adapting pretrained unimodal models by tuning only small subset of parameters (Shukor et al., 2023; Vallaeys et al., 2024; Mañas et al., 2023; Koh et al., 2023; Tsimpoukelli et al., 2021; Li et al., 2023). While the majority of VLM research focuses on image and text modalities, recent work has also demonstrated that similar techniques can be extended to integrate additional modalities, such as video and audio (Wang et al., 2025; Liu et al., 2024; Zhang et al., 2025; Kong et al., 2024)a particularly promising direction of research for robotics applications, where multiple sensor modalities can be integrated effectively. This trend towards efficiency is paramount for robotics applications, where policies must operate under the stringent constraints of real-world deployment. 60 Figure 37 The π0 architecture, as in Black et al. (2024). Vision and language tokens are routed to VLM backbone which is prevented from attending robot proprioperceptive states and action tokens, which are instead routed to smaller subset of weights within the architecture referred to as \"action expert\". The architecture is trained with Flow Matching on 10M+ trajectories from mixture of closed and openly available datasets. 5.3 π0 π0 (Black et al., 2024) introduce VLA consisting of MoE architecture consisting of (1) pre-trained VLM backbone (Gemma 2.6B (Team et al., 2024)) and (2) dedicated action expert used to generate continuous actions via flow matching. Images and language are embedded with PaliGemma, VLM merging independently encoded visual and textual features deep in the network (late-fusion), while proprioceptive state and actions chunks are routed to smaller action expert, initialized from scratch. The two separate experts communicate via self-attention layers, but maintain disjoint weights to obtain query, key and values matrices at each layer, maintaining specialization while efficiently allocating computation. Concretely, π0 is single, unified transformer with two disjoint sets of weights ϕ, θ. larger VLM backbone fϕ t=1], as initialized from Gemma 2.6B processes multiple image frames obtained from multiple cameras points [{It}n well as language instruction [ℓt] used to describe the task considered. Concurrently, 300M-parameter action expert based on similar transformer architecture is used to process both the robot proprioperceptive state qt and an action chunk at:t+Ha (Figure 37). The different expert networks operate separately in processing the respective inputs and turn them into query, key and value matrices, and only share information between each other via self-attention layers. The outputs from the VLM backbone are disregarded, while the vector field regressed by the action expert is used to iteratively refine the action process. In particular, π0 uses blockwise causal attention mask over tokens belonging to t=1, ℓt], (2) proprioperceptive tokens Tq three separate blocks: (1) image and language tokens Ti obtained from [{It}n obtained from qt, and (3) the action tokens Ta for items in the chunk aτ at time τ in the flow-matching process. t:t+Ha Notably, within each block the attention operations are bidirectional, while across blocks, future blocks are masked out. Formally, this corresponds to using an attention mask like: Ti Tq Ta 0 0 1 0 1 1 1 1 ,"
        },
        {
            "title": "Ti\nTq\nTa",
            "content": "A = 1 : Bidirectional Attention, 0 : Masked Attention Note how intra-block directional attention allows tokens to communicate freely, while inter -block communication is mediated by the attention mask A. Blockwise causal masking effectively prevents the pre-trained perception-language tokens from attending to robotics-tokens, likely out of distribution for VLM backbones traditionally trained on large corpora of internet, non-robotics, data. Crucially, because communication is obstructed between image-language tokens, proprioperceptive tokens and action tokens, one can cache keys and values across denoising steps at runtime time, incuring in reduced computational footprint and faster inference. 61 In π0, both the VLM backbone and action expert are update using flow matching loss, and in particular are updated minimizing: L(ϕ, θ) = Eτ,ϵ,ot,at:t+Ha (cid:104)(cid:13) , ot, τ ) (ϵ at:t+Ha )(cid:13) (cid:13)vθ(τ at:t+Ha + (1 τ )ϵ (cid:13) (cid:125) (cid:124) 2(cid:105) , (52) τ Beta[0,s](1.5, 1), ot, at:t+Ha (cid:123)(cid:122) at:t+Ha ϵ (0, I), where the two experts parametrized by the separate weights ϕ, θ interact with each other via self-attention layers only, so that the action expert vθ internal computations also depend on the VLM backbones parameters ϕ. Importantly, Black et al. (2024) minimize eq. 52 over both the multimodal backbone and action expert parameters, thus updating both the internal representations of the VLM and action-expert weights using BC-specific gradients. In contrast, Driess et al. (2025) later show that failing to insulate the VLM knowledge from the flow matching gradients actually harms performance. At runtime, inference is performed iteratively refining action chunks while numerically forward-integrating the vector field predicted by the action expert, aτ +δ t:t+Ha = aτ t:t+Ha + δvθ(aτ t:t+Ha , ot) (53) Flow matching (Lipman et al., 2023, Section4.1.3) can be seen as continuous time, deterministic generalization of diffusion processes, and has proven effective in modeling highly complex multi-modal distributions, including those over images and video. In turn, the application of flow matching to large-scale datasets of multiple human behaviors across tasks and embodiments appears rather consequential, particularly considering how it can enable faster inference via limited number of denoising steps at test timeas few as 10, in π0. In particular, the action expert is implemented as conditional flow matching model. Each action token embeds noisy action aτ , t:t+Ha alongside sinusoidal encoding of the flow process timestep τ . The action expert then leverages full bidirectional attention across the Ha action tokens provided, and also attends to previous proprioperceptive and image-language tokens. Interestingly, differently from standard flow matching pipeline (Lipman et al., 2023), τ is not sampled from uniform distribution τ U([0, 1]), but rather obtained from τ Beta(1.5, 1) defined on the [0, s], < 1 support (Figure 38). aτ Using such Beta distribution emphasizes higher noise levels during training, choice Black et al. (2024) argue allows π0 to focus on learning to reconstruct the mean of the data distribution E[at:t+Ha ot] over an identity map during training, in keeping with Esser et al. (2024). To further optimize performance and reduce inference time, Black et al. (2024) propose reducing the support of the timestep distribution to [0, s], < 1, as for any forward-integration step size δ = 1 timesteps above are never sampled at inference time. Besides adopting MoE architecture with VLM backbone initialized from pre-trained model and trained jointly with an action expert via flow matching, π0 also relies on unique pre-training corpus comprising of mix of proprietary and open data totaling 10M+ trajectories, which in their work Black et al. (2024) claim to be the largest dataset used to develop foundational robotics model to date. The dataset used to train π0referred to as \"the π dataset\"comprises private, undisclosed portion obtained via expert teleoperation as well as openly available datasets including Open-X and DROID, with only 9.1% of the π being openly available. In the π dataset, open datasets such as DROID and Open-X are complemeneted with expert trajectories consisting of dexterous demonstrations tasks spanning 7 robot configurations and 68 different tasks. Crucially, Black et al. (2024) show that pre-training on the π dataset yields broadly capable base model, which can be adapted via fine-tuning on narrower, higher-quality task data, which induces fluent multi-stage behavior while retaining robustness. In particular, Black et al. (2024) report that, across variety of benchmarks, the version of π0 pretrained on the π dataset and fine-tuned on extra high-quality data demonstrations consistently outperforms πscratch baseline trained entirely from scratch for given specific task, which further underscores the relevance of pretraining on the π dataset. Black et al. (2024) do also offer an intuition behind this finding: high-quality demonstrations of given task Figure 38 Unlike more traditional flow-matching algorithms, π0 uses modified distribution to sample the timestep τ from during training and inference, favouring earlier timestamps corresponding to noisier chunks. 0 tend to omit failure data, which inherently prevents an autonomous agent to learn how to recover from near-failure states. In turn, robot trained on high-quality data exclusively with BC may as well be entirely incapable to recover from failure. Conversely, large scale collections of human demonstrations are typically much more diverse (if anything, for their sheer scale), and typically contain rich and diverse information, which may prove suboptimal for any given task when considered in isolation but which proves invaluable in coupling with small, narrower set of demonstrations. Lastly, Black et al. (2024) present cross-embodiment experiments where they demonstrate π0s ability to control both mobile and static manipulator robots with varying arm embodiments. The emergence of cross-embodiment capabilities is largely to be attributed to the presence of large scale cross-embodiment data in π data mixture, which is in practice handled by π0 outputting actions with maximal configuration size across the whole π dataset, and zero-padding robots with fewer dofs. π0 does also rely on exactly three camera views at both training and test time, and uses masked image slots for training and deployment scenarios with fewer cameras."
        },
        {
            "title": "5.3.1 Code Example: Using π0",
            "content": "Code 13: Using π0 https://github.com/fracapuano/robot-learning-tutorial/blob/main/snippets/ch5/01_using_pi0.py 1 import torch 2 3 from lerobot . cameras . opencv . f r o _ n import pe nC Ca er Co fi 4 from lerobot . datasets . utils import _ _ a _ t s 5 from lerobot . policies . factory import e _ _ t _ c o 6 from lerobot . policies . pi0 . modeling_pi0 import PI0Policy 7 from lerobot . policies . utils import build_inference_frame , ake _ro bot _ac tio 8 from lerobot . robots . so100_follower . f _ 1 0 0 _ l r import 1 0 0 l we n 9 from lerobot . robots . so100_follower . so100_follower import SO100Follower 10 11 MAX_EPISODES = 5 12 _ P _ _ S = 20 13 14 device = torch . device ( \" mps \" ) 15 model_id = \" lerobot / pi0_base \" # or \" cuda \" or \" cpu \" 16 17 model = PI0Policy . from_pretrained ( model_id ) 18 19 preprocess , postprocess = e _ _ t _ c o ( model . config , model_id , # This overrides allows to run on MPS , otherwise defaults to CUDA ( if available ) p e r _ r e ={ \" device_processor \" : { \" device \" : \" mps \" }} , 20 21 23 24 ) 25 26 # find ports using lerobot - find - port 27 follower_port = ... # something like \"/ dev / tty . us d 5 8 7 6 0 4 3 1 63 1 \" 28 29 # the robot ids are used the load the right calibration files 30 follower_id = ... # something like \" follower_so100 \" 31 32 # Robot and environment configuration 33 # Camera keys must match the name and resolutions of the ones used for training ! 34 # You can check the camera keys expected by model in the info . json card on the Hub 35 camera_config = { \" base_0_rgb \" : Op nC Ca er aC nf ( index_or_path =0 , width =640 , height =480 , fps =30) , \" ft_ rist_0_rgb \" : pe CV am ra on ig ( index_or_path =1 , width =640 , height =480 , fps =30) , \" ht _wr ist _0_ rg \" : en VC me aC nf ( index_or_path =2 , width =640 , height =480 , fps =30) , 36 37 38 39 } 40 41 robot_cfg = 1 0 0 l ow o g ( port = follower_port , id = follower_id , cameras = camera_config ) 42 robot = SO100Follower ( robot_cfg ) 43 robot . connect () 44 45 task = ... 46 robot_type = ... # something like \" pick the red block \" # something like \" so100_follower \" for multi - embodiment datasets 47 63 Figure 39 The SmolVLA architecture, as in Shukor et al. (2025). SmolVLA is compact MoE model trained with flow matching to denoise action chunks. Vision and language tokens are fed to VLM backbone, and share information with the proprioperceptive and action tokens via the attention mechanism. The attention expert interleaves SA and CA layers for further conditioning on the visual features from the VLM backbone. SmolVLA skips computations and reduces the visual tokens, resulting in 7x less memory usage than π0 (450M parameters vs. π0s 3.3B). 48 # This is used to match the raw observation keys to the keys expected by the policy 49 ac ti n_ fea tures = _ _ a _ t s ( robot . action_features , \" action \" ) 50 obs_features = _ _ a _ t s ( robot . observation_features , \" observation \" ) 51 da ase _fe tures = {** action_features , ** obs_features } 52 53 for _ in range ( MAX_EPISODES ): 54 55 56 57 59 60 61 62 63 65 66 67 for _ in range ( _ P _ _ S ): obs = robot . get_observation () obs_frame = l _ e c _ m ( obs , dataset_features , device , task = task , robot_type = robot_type ) obs = preprocess ( obs_frame ) action = model . select_action ( obs ) action = postprocess ( action ) action = mak e_r obo t_a cti on ( action , dataset_features ) robot . send_action ( action ) print ( \" Episode finished ! Starting new episode ... \" )"
        },
        {
            "title": "5.4 SmolVLA\nWith VLAs in the early stage of development compared to more mature LLMs and VLMs, much of the progress\nmade on VLAs remains proprietary, with many releases exclusively sharing the weights while withholding the data\nused, full experimental details and essential methodological components of training. In constrast with this closed\napproach, SmolVLA (Shukor et al., 2025) is an entirely open-source research effort, which aims at democratizing\nthe developments of robotics foundation models by open sourcing the model alongside the data used as well as the\ntraining recipes.",
            "content": "While encouraging efforts like π0 (Black et al., 2024) demonstrate the feasibility of open VLA systems, they remain (1) large and compute-intensive and (2) dependent on closed datasets collected via centralized efforts on costly robotic platforms, which ultimately hinders the accessibility of the method altogether. SmolVLA mitigates both these issues by (1) prioritizing compact, compute-efficient VLA design and (2) targeting community-contributed datasets on 64 accessible robotic platforms such as the SO-100 and SO-101 arms. Similarly to π0, SmolVLA (Figure 39) employs MoE architecture combining pretrained VLM backbone with dedicated action expert, and trains with flow matching. To ensure efficiency and accessibility, SmolVLA adopts SmolVLM-2 (Marafioti et al., 2025) as its VLM backbone, considering SmolVLM-2s reduced size and capability to process multiple image inputs alongside text items. SmolVLM-2 uses SigLIP (Zhai et al., 2023) as vision encoder, producing visual features for SmolLM2 language decoder (Allal et al., 2025). Further, SmolVLA adopts smaller action expert consisting of 100M parameters and an interleaved stack of self and cross-attention layers. To improve efficiency, the action expert adopts reduced embedding dimension compared to the VLM backbone, resulting in dvθ = 0.75dVLM. Shukor et al. (2025)s design choices thus result in much smaller size model compared to π0, consisting of ca. 450M parameters versus π0s 3.3B parameters. In practice, SmolVLA consumes multi-view RGB images, natural-language instruction, and projected sensorimotor state token as inputs, together with the noised action chunk at:t+Ha the action expert vθ is trained to denoise. The robot proprioperceptive states are projected to shared token space with the VLM to match dVLM, and successively projected into the experts token space. Similarily to π0, SmolVLA adopts separate experts communicating exclusively through self-attention layers, which however do not employ blockwise causal attention masking and rather favour simple causal masking. In contrast with π0, the action expert interleaves cross-attention (CA) and self-attention (SA) layers, choice shown to yield higher success and smoother action chunks in practice. While in the expert SA layers tokens are used to obtain queries, keys and values, CA layers use action tokens only as queries, and instead project visual, language and proprioperceptive tokens from the VLM backbone to shared embedding space to then obtain keys and values. Notably, keys and values can be cached here as well, resulting in performance gains at inference time. SmolVLA also trims down both token and layer compute. First, it reduces visual tokens via pixel shuffling to fixed budget of 64 tokens per frame, foregoing the tiling used during VLM pretraining for the sake of runtime efficiency. Second, it skips upper VLM layers, as only features from the first decoder layers, with = L/2, are consumed, which provides good speed-performance trade-off and effectively halves compute needs for the larger part of SmolVLA. Beyond model compactness, SmolVLA also contributes an inference stack that decouples action prediction from execution for responsiveness on modest hardware (Section 4.4). Departing from reliance on proprietary datasets, SmolVLA pretrains exclusively on 450+ community datasets, totaling 20k+ trajectories. Because instructions in community contributed dataset can be noisy or missing, the authors re-annotate tasks with small off-the-shelf VLM using frames sampled from the dataset, and standardize camera viewpoints by mapping sources to consistent top/wrist/side ordering. At test time, similarily to π0, SmolVLA forward-integrates flow over 10 steps, resulting in fast inference. SmolVLA proves effective across range of both realworld and simulated environments, rivaling π0 while being close to 40% faster and consuming 6x less memory (Shukor et al., 2025)."
        },
        {
            "title": "5.4.1 Code Example: Using SmolVLA",
            "content": "Code 14: Using SmolVLA https://github.com/fracapuano/robot-learning-tutorial/blob/main/snippets/ch5/02_using_smolvla.py 1 import torch 2 3 from lerobot . cameras . opencv . f r o _ n import pe nC Ca er Co fi 4 from lerobot . datasets . utils import _ _ a _ t s 5 from lerobot . policies . factory import e _ _ t _ c o 6 from lerobot . policies . smolvla . modeling_smolvla import SmolVLAPolicy 7 from lerobot . policies . utils import build_inference_frame , ake _ro bot _ac tio 8 from lerobot . robots . so100_follower . f _ 1 0 0 _ l r import 1 0 0 l we n 9 from lerobot . robots . so100_follower . so100_follower import SO100Follower 10 11 MAX_EPISODES = 5 12 _ P _ _ S = 20 13 14 device = torch . device ( \" mps \" ) 15 model_id = \" lerobot / smolvla_base \" # or \" cuda \" or \" cpu \" 16 65 17 model = SmolVLAPolicy . from_pretrained ( model_id ) 18 19 preprocess , postprocess = e _ _ t _ c o ( model . config , model_id , # This overrides allows to run on MPS , otherwise defaults to CUDA ( if available ) p e r _ r e ={ \" device_processor \" : { \" device \" : \" mps \" }} , 21 22 23 24 ) 25 26 # find ports using lerobot - find - port 27 follower_port = ... # something like \"/ dev / tty . us d 5 8 7 6 0 4 3 1 63 1 \" 28 29 # the robot ids are used the load the right calibration files 30 follower_id = ... # something like \" follower_so100 \" 31 32 # Robot and environment configuration 33 # Camera keys must match the name and resolutions of the ones used for training ! 34 # You can check the camera keys expected by model in the info . json card on the Hub 35 camera_config = { \" camera1 \" : pe nC Ca er Co fi ( index_or_path =0 , width =640 , height =480 , fps =30) , \" camera2 \" : pe nC Ca er Co fi ( index_or_path =1 , width =640 , height =480 , fps =30) , 36 37 38 } 39 40 robot_cfg = 1 0 0 l ow o g ( port = follower_port , id = follower_id , cameras = camera_config ) 41 robot = SO100Follower ( robot_cfg ) 42 robot . connect () 43 44 task = ... 45 robot_type = ... # something like \" pick the red block \" # something like \" so100_follower \" for multi - embodiment datasets 46 47 # This is used to match the raw observation keys to the keys expected by the policy 48 ac ti n_ fea tures = _ _ a _ t s ( robot . action_features , \" action \" ) 49 obs_features = _ _ a _ t s ( robot . observation_features , \" observation \" ) 50 da ase _fe tures = {** action_features , ** obs_features } 51 52 for _ in range ( MAX_EPISODES ): 54 55 56 57 58 60 61 62 63 64 66 for _ in range ( _ P _ _ S ): obs = robot . get_observation () obs_frame = l _ e c _ m ( obs , dataset_features , device , task = task , robot_type = robot_type ) obs = preprocess ( obs_frame ) action = model . select_action ( obs ) action = postprocess ( action ) action = mak e_r obo t_a cti on ( action , dataset_features ) robot . send_action ( action ) print ( \" Episode finished ! Starting new episode ... \" )"
        },
        {
            "title": "6 Conclusions",
            "content": "This tutorial has charted the paradigmatic shift transforming robotics, tracing the evolution of robotics from structured, model-based methods to the dynamic, data-driven approaches that define modern robot learning. We began by examining the limitations of traditional dynamics-based control, namely its brittleness and significant engineering overhead, which motivate the adoption of more flexible, learning-based alternatives. Unlike scalable, data-driven techniques, conventional explicit models demand extensive human expertise, hindering wider accessibility and scalability of robotics. Our exploration traced clear trajectory of progress, beginning with Reinforcement Learning (RL). While RL offers powerful paradigm for learning through interaction, its application in robotics is complicated by challenges such as sample inefficiency, safety concerns in real-world training, and the complexities of reward design. We saw how modern approaches like HIL-SERL make real-world RL more feasible by incorporating training-time human guidance, datasets of previously collected data as well as learned reward classifiers. Nonetheless, the inherent difficulties of RL increasingly motivate approaches based on imitation learning, capable to safely learns from limited numbers of real-world, reward-free expert demonstrations. In turn, the wider adoption of imitation learning led to the development of single-task policies, where advanced Behavioral Cloning techniques implemented as state-conditioned generative models like Action Chunking with Transformers and Diffusion Policyhave demonstrated the ability to learn complex, multimodal behaviors from human demonstrations. These advancements laid the groundwork for the current frontier: generalist, language-conditioned Vision-Language-Action models capable to perform fewand zero-shot variety of different real-world tasks. By leveraging powerful pre-trained backbones and sophisticated generative methods like flow matching, models such as π0 and SmolVLA represent significant leap towards foundational models for robotics capable of generalizing across diverse tasks, and even robot embodiments. central theme of this work is the critical role of openness in accelerating this progress. The recent explosion in capability is inseparable from the advent of large-scale, openly available datasets, standardized, stable and accessible model architectures, and accessible, open-source software like lerobot. We argue this convergence on open-source robotics is not mere trend but fundamental enabler, democratizing access to research and unlocking the potential of large, decentralized efforts to advance the field. The journey detailed in this tutorial, from first principles to the state-of-the-art, aims to equip researchers and practitioners with the context and tools to begin their own explorations in open-source robot learning."
        },
        {
            "title": "References",
            "content": "Joshua Achiam. Spinning up in deep reinforcement learning. 2018. Pulkit Agrawal. Computational Sensorimotor Learning. Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, Jonas Schneider, Nikolas Tezak, Jerry Tworek, Peter Welinder, Lilian Weng, Qiming Yuan, Wojciech Zaremba, and Lei Zhang. Solving Rubiks Cube with Robot Hand, October 2019. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: Visual Language Model for Few-Shot Learning, November 2022. Jorge Aldaco, Travis Armstrong, Robert Baruch, Jeff Bingham, Sanky Chan, Debidatta Dwibedi, Chelsea Finn, Pete Florence, Spencer Goodrich, Wayne Gramlich, Alexander Herzog, Jonathan Hoech, Thinh Nguyen, Ian Storz, Baruch Tabanpour, Jonathan Tompson, Ayzaan Wahid, Ted Wahrburg, Sichun Xu, Sergey Yaroshenko, and Tony Zhao. ALOHA 2: An Enhanced Low-Cost Hardware for Bimanual Teleoperation. Mohammad Alizadeh and Zheng H. Zhu. comprehensive survey of space robotic manipulators for on-orbit servicing. Frontiers in Robotics and AI, 11, October 2024. ISSN 2296-9144. doi: 10.3389/frobt.2024.1470950. Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martín Blázquez, Guilherme Penedo, Lewis Tunstall, Andrés Marafioti, Hynek Kydlíček, Agustín Piqueres Lajarín, Vaibhav Srivastav, Joshua Lochner, Caleb Fahlgren, Xuan-Son Nguyen, Clémentine Fourrier, Ben Burtenshaw, Hugo Larcher, Haojun Zhao, Cyril Zakka, Mathieu Morlon, Colin Raffel, Leandro von Werra, and Thomas Wolf. SmolLM2: When Smol Goes Big Data-Centric Training of Small Language Model, February 2025. 67 Rika Antonova, Silvia Cruciani, Christian Smith, and Danica Kragic. Reinforcement Learning for Pivoting Task, March 2017. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-VL technical report, 2025. Philip J. Ball, Laura Smith, Ilya Kostrikov, and Sergey Levine. Efficient Online Reinforcement Learning with Offline Data, May 2023. Kostas E. Bekris, Joe Doerr, Patrick Meng, and Sumanth Tangirala. The State of Robot Motion Generation, October 2024. Marc G. Bellemare, Salvatore Candido, Pablo Samuel Castro, Jun Gong, Marlos C. Machado, Subhodeep Moitra, Sameera S. Ponda, and Ziyu Wang. Autonomous navigation of stratospheric balloons using reinforcement learning. Nature, 588(7836): 7782, December 2020. ISSN 1476-4687. doi: 10.1038/s41586-020-2939-8. Richard Bellman. Markovian Decision Process. Journal of Mathematics and Mechanics, 6(5):679684, 1957. ISSN 0095-9057. Johan Bjorck, Fernando Castañeda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi \"Jim\" Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, Joel Jang, Zhenyu Jiang, Jan Kautz, Kaushil Kundalia, Lawrence Lao, Zhiqi Li, Zongyu Lin, Kevin Lin, Guilin Liu, Edith Llontop, Loic Magne, Ajay Mandlekar, Avnish Narayan, Soroush Nasiriany, Scott Reed, You Liang Tan, Guanzhi Wang, Zu Wang, Jing Wang, Qi Wang, Jiannan Xiang, Yuqi Xie, Yinzhen Xu, Zhenjia Xu, Seonghyeon Ye, Zhiding Yu, Ao Zhang, Hao Zhang, Yizhou Zhao, Ruijie Zheng, and Yuke Zhu. GR00T N1: An Open Foundation Model for Generalist Humanoid Robots, March 2025. Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Lucy Xiaoyang Shi, James Tanner, Quan Vuong, Anna Walling, Haohuan Wang, and Ury Zhilinsky. $π_0$: Vision-Language-Action Flow Model for General Robot Control, October 2024. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alexander Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control, July 2023a. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil J. Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. RT-1: Robotics Transformer for Real-World Control at Scale, August 2023b. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners, July 2020. Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. COYO-700M: Image-text pair dataset, 2022. Yevgen Chebotar, Ankur Handa, Viktor Makoviychuk, Miles Macklin, Jan Issac, Nathan Ratliff, and Dieter Fox. Closing the sim-to-real loop: Adapting simulation randomization with real world experience. In 2019 International Conference on Robotics and Automation (ICRA), pages 89738979. IEEE, 2019. Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, Siamak Shakeri, Mostafa Dehghani, Daniel Salz, Mario Lucic, Michael Tschannen, Arsha Nagrani, Hexiang Hu, Mandar Joshi, Bo Pang, Ceslee Montgomery, Paulina Pietrzyk, Marvin Ritter, A. J. Piergiovanni, Matthias Minderer, Filip Pavetic, Austin Waters, Gang Li, Ibrahim Alabdulmohsin, Lucas Beyer, Julien Amelot, Kenton Lee, Andreas Peter Steiner, Yang Li, Daniel Keysers, Anurag Arnab, Yuanzhong Xu, Keran Rong, Alexander Kolesnikov, Mojtaba Seyedhosseini, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. PaLI-X: On Scaling up Multilingual Vision and Language Model, May 2023. 68 Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. Diffusion Policy: Visuomotor Policy Learning via Action Diffusion, March 2024. Jonathan H. Connell and Sridhar Mahadevan, editors. Robot Learning. Springer US, Boston, MA, 1993. ISBN 978-1-4613-6396-5 978-1-4615-3184-5. doi: 10.1007/978-1-4615-3184-5. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. InstructBLIP: Towards general-purpose vision-language models with instruction tuning. In Thirty-Seventh Conference on Neural Information Processing Systems, 2023. Jonas Degrave, Federico Felici, Jonas Buchli, Michael Neunert, Brendan Tracey, Francesco Carpanese, Timo Ewalds, Roland Hafner, Abbas Abdolmaleki, Diego de las Casas, Craig Donner, Leslie Fritz, Cristian Galperti, Andrea Huber, James Keeling, Maria Tsimpoukelli, Jackie Kay, Antoine Merle, Jean-Marc Moret, Seb Noury, Federico Pesamosca, David Pfau, Olivier Sauter, Cristian Sommariva, Stefano Coda, Basil Duval, Ambrogio Fasoli, Pushmeet Kohli, Koray Kavukcuoglu, Demis Hassabis, and Martin Riedmiller. Magnetic control of tokamak plasmas through deep reinforcement learning. Nature, 602 (7897):414419, February 2022. ISSN 1476-4687. doi: 10.1038/s41586-021-04301-9. J. Deng, K. Li, M. Do, H. Su, and L. Fei-Fei. Construction and analysis of large scale image ontology. Vision Sciences Society, 2009. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, May 2019. Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. PaLM-E: An Embodied Multimodal Language Model, March 2023. Danny Driess, Jost Tobias Springenberg, Brian Ichter, Lili Yu, Adrian Li-Bell, Karl Pertsch, Allen Z. Ren, Homer Walke, Quan Vuong, Lucy Xiaoyang Shi, and Sergey Levine. Knowledge Insulating Vision-Language-Action Models: Train Fast, Run Fast, Generalize Better, May 2025. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling Rectified Flow Transformers for High-Resolution Image Synthesis, March 2024. William Fedus, Jeff Dean, and Barret Zoph. Review of Sparse Expert Models in Deep Learning, September 2022. Enrico Fini, Mustafa Shukor, Xiujun Li, Philipp Dufter, Michal Klein, David Haldimann, Sai Aitharaju, Victor Guilherme Turrisi da Costa, Louis Béthune, Zhe Gan, Alexander T. Toshev, Marcin Eichner, Moin Nabi, Yinfei Yang, Joshua M. Susskind, and Alaaeldin El-Nouby. Multimodal Autoregressive Pre-training of Large Vision Encoders, November 2024. Pete Florence, Corey Lynch, Andy Zeng, Oscar A. Ramirez, Ayzaan Wahid, Laura Downs, Adrian Wong, Johnny Lee, Igor Mordatch, and Jonathan Tompson. Implicit Behavioral Cloning. In Proceedings of the 5th Conference on Robot Learning, pages 158168. PMLR, January 2022. Jun Fujita, Daisuke Soda, Chotaro Murata, and Hiroyuki Tsuhari. Development of Robots for Nuclear Power Plants and Their Application to New Fields. 57(4), 2020. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria 69 Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The Llama 3 Herd of Models, November 2024. Robert J. Griffin, Georg Wiedebach, Sylvain Bertrand, Alexander Leonessa, and Jerry Pratt. Walking Stabilization Using Step Timing and Location Adjustment on the Humanoid Robot, Atlas. In 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 667673, September 2017. doi: 10.1109/IROS.2017.8202223. Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement Learning with Deep Energy-Based Policies. In Proceedings of the 34th International Conference on Machine Learning, pages 13521361. PMLR, July 2017. 70 Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with Stochastic Actor, August 2018. Nicklas Hansen, Xiaolong Wang, and Hao Su. Temporal Difference Learning for Model Predictive Control, July 2022. Nicolas Heess, Dhruva TB, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg Wayne, Yuval Tassa, Tom Erez, Ziyu Wang, S. M. Ali Eslami, Martin Riedmiller, and David Silver. Emergence of Locomotion Behaviours in Rich Environments, July 2017. Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. Beta-vae: Learning basic visual concepts with constrained variational framework. In International Conference on Learning Representations, 2017. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models, December 2020. Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey Levine, and Chelsea Finn. BC-Z: Zero-Shot Task Generalization with Robotic Imitation Learning, February 2022. Michael Janner, Yilun Du, Joshua B. Tenenbaum, and Sergey Levine. Planning with Diffusion for Flexible Behavior Synthesis, December 2022. Yandong Ji, Gabriel B. Margolis, and Pulkit Agrawal. DribbleBot: Dynamic Legged Manipulation in the Wild, April 2023. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7B, October 2023. Liyiming Ke, Jingqiang Wang, Tapomayukh Bhattacharjee, Byron Boots, and Siddhartha Srinivasa. Grasping with Chopsticks: Combating Covariate Shift in Model-free Imitation Learning for Fine Manipulation, November 2020. Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, Peter David Fagan, Joey Hejna, Masha Itkina, Marion Lepert, Yecheng Jason Ma, Patrick Tree Miller, Jimmy Wu, Suneel Belkhale, Shivin Dass, Huy Ha, Arhan Jain, Abraham Lee, Youngwoon Lee, Marius Memmel, Sungjae Park, Ilija Radosavovic, Kaiyuan Wang, Albert Zhan, Kevin Black, Cheng Chi, Kyle Beltran Hatch, Shan Lin, Jingpei Lu, Jean Mercat, Abdul Rehman, Pannag R. Sanketi, Archit Sharma, Cody Simpson, Quan Vuong, Homer Rich Walke, Blake Wulfe, Ted Xiao, Jonathan Heewon Yang, Arefeh Yavary, Tony Z. Zhao, Christopher Agia, Rohan Baijal, Mateo Guaman Castro, Daphne Chen, Qiuyu Chen, Trinity Chung, Jaimyn Drake, Ethan Paul Foster, Jensen Gao, Vitor Guizilini, David Antonio Herrera, Minho Heo, Kyle Hsu, Jiaheng Hu, Muhammad Zubair Irshad, Donovon Jackson, Charlotte Le, Yunshuang Li, Kevin Lin, Roy Lin, Zehan Ma, Abhiram Maddukuri, Suvir Mirchandani, Daniel Morton, Tony Nguyen, Abigail ONeill, Rosario Scalise, Derick Seale, Victor Son, Stephen Tian, Emi Tran, Andrew E. Wang, Yilin Wu, Annie Xie, Jingyun Yang, Patrick Yin, Yunchu Zhang, Osbert Bastani, Glen Berseth, Jeannette Bohg, Ken Goldberg, Abhinav Gupta, Abhishek Gupta, Dinesh Jayaraman, Joseph J. Lim, Jitendra Malik, Roberto Martín-Martín, Subramanian Ramamoorthy, Dorsa Sadigh, Shuran Song, Jiajun Wu, Michael C. Yip, Yuke Zhu, Thomas Kollar, Sergey Levine, and Chelsea Finn. DROID: Large-Scale In-The-Wild Robot Manipulation Dataset, April 2025. Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. OpenVLA: An Open-Source Vision-Language-Action Model, September 2024. Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. Rob Knight, Pepijn Kooijmans, Thomas Wolf, Simon Alibert, Michel Aractingi, Dana Aubakirova, Adil Zouitine, Russi Martino, Steven Palma, Caroline Pascal, and Remi Cadene. Standard Open SO-100 & SO-101 Arms. Jens Kober, Andrew Bagnell, and Jan Peters. Reinforcement Learning in Robotics: Survey. Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried. Grounding language models to images for multimodal inputs and outputs, 2023. Zhifeng Kong, Arushi Goel, Rohan Badlani, Wei Ping, Rafael Valle, and Bryan Catanzaro. Audio flamingo: novel audio language model with few-shot learning and dialogue abilities. In International Conference on Machine Learning, pages 2512525148. PMLR, 2024. Vik Korrapati. Moondream. Online, 2024. Hugo Laurençon, Lucile Saulnier, Leo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander Rush, Douwe Kiela, Matthieu Cord, and Victor Sanh. OBELICS: An open web-scale filtered dataset of interleaved image-text documents. In Thirty-Seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. 71 Hugo Laurençon, Léo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models?, May 2024. Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun, and Marco Hutter. Learning Quadrupedal Locomotion over Challenging Terrain. Science Robotics, 5(47):eabc5986, October 2020. ISSN 2470-9476. doi: 10.1126/scirobotics.abc5986. Seungjae Lee, Yibin Wang, Haritheja Etukuru, H. Jin Kim, Nur Muhammad Mahi Shafiullah, and Lerrel Pinto. Behavior Generation with Latent Actions, June 2024. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In Proceedings of the 40th International Conference on Machine Learning, ICML23, , Honolulu, Hawaii, USA 2023. JMLR.org. Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning, July 2019. Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. VILA: On Pre-training for Visual Language Models, May 2024. Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow Matching for Generative Modeling, February 2023. Yaron Lipman, Marton Havasi, Peter Holderrieth, Neta Shaul, Matt Le, Brian Karrer, Ricky T. Q. Chen, David Lopez-Paz, Heli Ben-Hamu, and Itai Gat. Flow Matching Guide and Code, December 2024. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023. Jiajun Liu, Yibing Wang, Hanghang Ma, Xiaoping Wu, Xiaoqi Ma, Xiaoming Wei, Jianbin Jiao, Enhua Wu, and Jie Hu. Kangaroo: powerful video-language model supporting long-context video input. arXiv preprint arXiv:2408.15542, 2024. Calvin Luo. Understanding Diffusion Models: Unified Perspective, August 2022. Jianlan Luo, Charles Xu, Jeffrey Wu, and Sergey Levine. Precise and Dexterous Robotic Manipulation via Human-in-the-Loop Reinforcement Learning, October 2024. Jianlan Luo, Zheyuan Hu, Charles Xu, You Liang Tan, Jacob Berg, Archit Sharma, Stefan Schaal, Chelsea Finn, Abhishek Gupta, and Sergey Levine. SERL: Software Suite for Sample-Efficient Robotic Reinforcement Learning, March 2025. Kevin M. Lynch and Frank C. Park. Modern Robotics: Mechanics, Planning, and Control. Cambridge University Press, 1 edition, May 2017. ISBN 978-1-316-66123-9 978-1-107-15630-2 978-1-316-60984-2. doi: 10.1017/9781316661239. Oscar Mañas, Pau Rodriguez Lopez, Saba Ahmadi, Aida Nematzadeh, Yash Goyal, and Aishwarya Agrawal. MAPL: Parameter-efficient adaptation of unimodal pre-trained models for vision-language few-shot prompting. In Andreas Vlachos and Isabelle Augenstein, editors, Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 25232548, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.eacl-main.185. Andrés Marafioti, Orr Zohar, Miquel Farré, Merve Noyan, Elie Bakouch, Pedro Cuenca, Cyril Zakka, Loubna Ben Allal, Anton Lozhkov, Nouamane Tazi, Vaibhav Srivastav, Joshua Lochner, Hugo Larcher, Mathieu Morlon, Lewis Tunstall, Leandro von Werra, and Thomas Wolf. SmolVLM: Redefining small and efficient multimodal models, April 2025. Gabriel B. Margolis, Ge Yang, Kartik Paigwar, Tao Chen, and Pulkit Agrawal. Rapid Locomotion via Reinforcement Learning, May 2022. John McCormac, Ankur Handa, Andrew Davison, and Stefan Leutenegger. SemanticFusion: Dense 3D Semantic Mapping with Convolutional Neural Networks, September 2016. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing Atari with Deep Reinforcement Learning, December 2013. Preetum Nakkiran, Arwen Bradley, Hattie Zhou, and Madhu Advani. Step-by-Step Diffusion: An Elementary Tutorial, June 2024. 72 Abby ONeill, Abdul Rehman, Abhinav Gupta, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Andrey Kolobov, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Felipe Vieira Frujeri, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guangwen Yang, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik I. Christensen, Hiroki Furuta, Homanga Bharadhwaj, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jay Vakil, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, João Silvério, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi \"Jim\" Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Muhammad Zubair Irshad, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil J. Joshi, Niko Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag R. Sanketi, Patrick \"Tree\" Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martín-Martín, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shubham Tulsiani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vikash Kumar, Vincent Vanhoucke, Vitor Guizilini, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiangyu Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Xu Liangwei, Xuanlin Li, Yansong Pang, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yongqiang Dou, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Fu, and Zipeng Lin. Open X-Embodiment: Robotic Learning Datasets and RT-X Models, May 2025. Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning Robust Visual Features without Supervision, February 2024. Frank Permenter and Chenyang Yuan. Interpreting and Improving Diffusion Models from an Optimization Perspective, June 2024. Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan Pang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat Singh, Mary Williamson, Matt Le, Matthew Yu, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit Girdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen, Sean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Simran Motwani, Tao Xu, Tianhe Li, Tingbo Hou, Wei-Ning Hsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval Kirstain, Zecheng He, Zijian He, Albert Pumarola, Ali Thabet, Artsiom Sanakoyeu, Arun Mallya, Baishan Guo, Boris Araya, Breena Kerr, Carleigh Wood, Ce Liu, Cen Peng, Dimitry Vengertsev, Edgar Schonfeld, Elliot Blanchard, Felix Juefei-Xu, Fraylie Nord, Jeff Liang, John Hoffman, Jonas Kohler, Kaolin Fire, Karthik Sivakumar, Lawrence Chen, Licheng Yu, Luya Gao, Markos Georgopoulos, Rashel Moritz, Sara K. Sampson, Shikai Li, Simone Parmeggiani, Steve Fine, Tara Fowler, Vladan Petrovic, and Yuming Du. Movie Gen: Cast of Media Foundation Models, February 2025. Dean A. Pomerleau. ALVINN: An Autonomous Land Vehicle in Neural Network. In Advances in Neural Information Processing Systems, volume 1. Morgan-Kaufmann, 1988. 73 Simon J.D. Prince. Understanding Deep Learning. The MIT Press, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning Transferable Visual Models From Natural Language Supervision, February 2021. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the Limits of Transfer Learning with Unified Text-to-Text Transformer, September 2023. Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. Generalist Agent, November 2022. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional Networks for Biomedical Image Segmentation, May 2015. Stephane Ross, Geoffrey J. Gordon, and J. Andrew Bagnell. Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning, March 2011. Lindsay Sanneman, Christopher Fourie, and Julie A. Shah. The State of Industrial Robotics: Emerging Technologies, Challenges, and Key Research Directions, October 2020. Schuhmann, Köpf, Vencu, Coombes, and Beaumont. Laion coco: 600m synthetic captions from laion2b-en. URL https://laion.ai/blog/laion-coco, 2022. John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel. Trust Region Policy Optimization, April 2017a. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal Policy Optimization Algorithms, August 2017b. Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to Algorithms. Cambridge University Press, 1 edition, May 2014. ISBN 978-1-107-05713-5 978-1-107-29801-9. doi: 10.1017/CBO9781107298019. Mustafa Shukor, Corentin Dancette, and Matthieu Cord. Ep-alm: Efficient perceptual augmentation of language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2205622069, 2023. Mustafa Shukor, Dana Aubakirova, Francesco Capuano, Pepijn Kooijmans, Steven Palma, Adil Zouitine, Michel Aractingi, Caroline Pascal, Martino Russi, Andres Marafioti, Simon Alibert, Matthieu Cord, Thomas Wolf, and Remi Cadene. SmolVLA: Vision-Language-Action Model for Affordable and Efficient Robotics, June 2025. Bruno Siciliano and Oussama Khatib, editors. Springer Handbook of Robotics. Springer Handbooks. Springer International Publishing, Cham, 2016. ISBN 978-3-319-32550-7 978-3-319-32552-1. doi: 10.1007/978-3-319-32552-1. David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Deterministic policy gradient algorithms. In Eric P. Xing and Tony Jebara, editors, Proceedings of the 31st International Conference on Machine Learning, volume 32 of Proceedings of Machine Learning Research, pages 387395, Bejing, China, June 2014. PMLR. Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning Structured Output Representation using Deep Conditional Generative Models. In Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising Diffusion Implicit Models, October 2022. Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. Adaptive Computation and Machine Learning Series. The MIT Press, Cambridge, Massachusetts, second edition edition, 2018. ISBN 978-0-262-03924-6. Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan T. Barron, and Ren Ng. Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains, June 2020. Chen Tang, Ben Abbatematteo, Jiaheng Hu, Rohan Chandra, Roberto Martín-Martín, and Peter Stone. Deep Reinforcement Learning for Robotics: Survey of Real-World Successes. Annual Review of Control, Robotics, and Autonomous Systems, 8 (Volume 8, 2025):153188, May 2025. ISSN 2573-5144. doi: 10.1146/annurev-control-030323-022510. Yang Tang, Chaoqiang Zhao, Jianrui Wang, Chongzhen Zhang, Qiyu Sun, Weixing Zheng, Wenli Du, Feng Qian, and Juergen Kurths. Perception and Navigation in Autonomous Systems in the Era of Learning: Survey. IEEE Transactions on Neural Networks and Learning Systems, 34(12):96049624, December 2023. ISSN 2162-237X, 2162-2388. doi: 10.1109/TNNLS.2022. 3167688. 74 Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, Anton Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan Girgin, Nikola Momchev, Matt Hoffman, Shantanu Thakoor, Jean-Bastien Grill, Behnam Neyshabur, Olivier Bachem, Alanna Walton, Aliaksei Severyn, Alicia Parrish, Aliya Ahmad, Allen Hutchison, Alvin Abdagic, Amanda Carl, Amy Shen, Andy Brock, Andy Coenen, Anthony Laforge, Antonia Paterson, Ben Bastian, Bilal Piot, Bo Wu, Brandon Royal, Charlie Chen, Chintu Kumar, Chris Perry, Chris Welty, Christopher A. Choquette-Choo, Danila Sinopalnikov, David Weinberger, Dimple Vijaykumar, Dominika Rogozińska, Dustin Herbison, Elisa Bandy, Emma Wang, Eric Noland, Erica Moreira, Evan Senter, Evgenii Eltyshev, Francesco Visin, Gabriel Rasskin, Gary Wei, Glenn Cameron, Gus Martins, Hadi Hashemi, Hanna Klimczak-Plucińska, Harleen Batra, Harsh Dhand, Ivan Nardini, Jacinda Mein, Jack Zhou, James Svensson, Jeff Stanway, Jetha Chan, Jin Peng Zhou, Joana Carrasqueira, Joana Iljazi, Jocelyn Becker, Joe Fernandez, Joost van Amersfoort, Josh Gordon, Josh Lipschultz, Josh Newlan, Ju-yeong Ji, Kareem Mohamed, Kartikeya Badola, Kat Black, Katie Millican, Keelin McDonell, Kelvin Nguyen, Kiranbir Sodhia, Kish Greene, Lars Lowe Sjoesund, Lauren Usui, Laurent Sifre, Lena Heuermann, Leticia Lago, Lilly McNealus, Livio Baldini Soares, Logan Kilpatrick, Lucas Dixon, Luciano Martins, Machel Reid, Manvinder Singh, Mark Iverson, Martin Görner, Mat Velloso, Mateo Wirth, Matt Davidow, Matt Miller, Matthew Rahtz, Matthew Watson, Meg Risdal, Mehran Kazemi, Michael Moynihan, Ming Zhang, Minsuk Kahng, Minwoo Park, Mofi Rahman, Mohit Khatwani, Natalie Dao, Nenshad Bardoliwalla, Nesh Devanathan, Neta Dumai, Nilay Chauhan, Oscar Wahltinez, Pankil Botarda, Parker Barnes, Paul Barham, Paul Michel, Pengchong Jin, Petko Georgiev, Phil Culliton, Pradeep Kuppala, Ramona Comanescu, Ramona Merhej, Reena Jana, Reza Ardeshir Rokni, Rishabh Agarwal, Ryan Mullins, Samaneh Saadat, Sara Mc Carthy, Sarah Perrin, Sébastien M. R. Arnold, Sebastian Krause, Shengyang Dai, Shruti Garg, Shruti Sheth, Sue Ronstrom, Susan Chan, Timothy Jordan, Ting Yu, Tom Eccles, Tom Hennigan, Tomas Kocisky, Tulsee Doshi, Vihan Jain, Vikas Yadav, Vilobh Meshram, Vishal Dharmadhikari, Warren Barkley, Wei Wei, Wenming Ye, Woohyun Han, Woosuk Kwon, Xiang Xu, Zhe Shen, Zhitao Gong, Zichuan Wei, Victor Cotruta, Phoebe Kirk, Anand Rao, Minh Giang, Ludovic Peran, Tris Warkentin, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, D. Sculley, Jeanine Banks, Anca Dragan, Slav Petrov, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Sebastian Borgeaud, Noah Fiedel, Armand Joulin, Kathleen Kenealy, Robert Dadashi, and Alek Andreev. Gemma 2: Improving Open Language Models at Practical Size, August 2024. Russ Tedrake. Robotic Manipulation. Perception, Planning and Control., a. Russ Tedrake. Underactuated Robotics. Algorithms for Walking, Running, Swimming, Flying, and Manipulation, b. Gabriele Tiboni, Karol Arndt, and Ville Kyrki. DROPO: Sim-to-Real Transfer with Offline Domain Randomization, January 2023. Gabriele Tiboni, Pascal Klink, Jan Peters, Tatiana Tommasi, Carlo DEramo, and Georgia Chalvatzaki. Domain Randomization via Entropy Maximization, March 2024. Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World, March 2017. Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. Advances in Neural Information Processing Systems, 37:8731087356, 2024. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open Foundation and Fine-Tuned Chat Models, July 2023. Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. Multimodal few-shot learning with frozen language models. Advances in Neural Information Processing Systems, 34:200212, 2021. Théophane Vallaeys, Mustafa Shukor, Matthieu Cord, and Jakob Verbeek. Improved baselines for data-efficient perceptual augmentation of llms. arXiv preprint arXiv:2403.13499, 2024. Yi Wang, Xinhao Li, Ziang Yan, Yinan He, Jiashuo Yu, Xiangyu Zeng, Chenting Wang, Changlian Ma, Haian Huang, Jianfei Gao, et al. InternVideo2. 5: Empowering video mllms with long and rich context modeling. arXiv preprint arXiv:2501.12386, 2025. 75 Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, Qianyu Chen, Huarong Zhou, Zhensheng Zou, Haoye Zhang, Shengding Hu, Zhi Zheng, Jie Zhou, Jie Cai, Xu Han, Guoyang Zeng, Dahai Li, Zhiyuan Liu, and Maosong Sun. MiniCPM-v: GPT-4V level MLLM on your phone, 2024. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid Loss for Language Image Pre-Training, September 2023. Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, et al. VideoLLaMA 3: Frontier multimodal foundation models for image and video understanding. arXiv preprint arXiv:2501.13106, 2025. Chong Zhang, Wenli Xiao, Tairan He, and Guanya Shi. WoCoCo: Learning Whole-Body Humanoid Control with Sequential Contacts, November 2024. Tony Z. Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware, April 2023. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. MiniGPT-4: Enhancing vision-language understanding with advanced large language models. In The Twelfth International Conference on Learning Representations, 2024. Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, and Yejin Choi. Multimodal C4: An open, billion-scale corpus of images interleaved with text. In Thirty-Seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023."
        }
    ],
    "affiliations": [
        "University of Oxford"
    ]
}