{
    "paper_title": "BOP Challenge 2024 on Model-Based and Model-Free 6D Object Pose Estimation",
    "authors": [
        "Van Nguyen Nguyen",
        "Stephen Tyree",
        "Andrew Guo",
        "Mederic Fourmy",
        "Anas Gouda",
        "Taeyeop Lee",
        "Sungphill Moon",
        "Hyeontae Son",
        "Lukas Ranftl",
        "Jonathan Tremblay",
        "Eric Brachmann",
        "Bertram Drost",
        "Vincent Lepetit",
        "Carsten Rother",
        "Stan Birchfield",
        "Jiri Matas",
        "Yann Labbe",
        "Martin Sundermeyer",
        "Tomas Hodan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present the evaluation methodology, datasets and results of the BOP Challenge 2024, the sixth in a series of public competitions organized to capture the state of the art in 6D object pose estimation and related tasks. In 2024, our goal was to transition BOP from lab-like setups to real-world scenarios. First, we introduced new model-free tasks, where no 3D object models are available and methods need to onboard objects just from provided reference videos. Second, we defined a new, more practical 6D object detection task where identities of objects visible in a test image are not provided as input. Third, we introduced new BOP-H3 datasets recorded with high-resolution sensors and AR/VR headsets, closely resembling real-world scenarios. BOP-H3 include 3D models and onboarding videos to support both model-based and model-free tasks. Participants competed on seven challenge tracks, each defined by a task, object onboarding setup, and dataset group. Notably, the best 2024 method for model-based 6D localization of unseen objects (FreeZeV2.1) achieves 22% higher accuracy on BOP-Classic-Core than the best 2023 method (GenFlow), and is only 4% behind the best 2023 method for seen objects (GPose2023) although being significantly slower (24.9 vs 2.7s per image). A more practical 2024 method for this task is Co-op which takes only 0.8s per image and is 25X faster and 13% more accurate than GenFlow. Methods have a similar ranking on 6D detection as on 6D localization but higher run time. On model-based 2D detection of unseen objects, the best 2024 method (MUSE) achieves 21% relative improvement compared to the best 2023 method (CNOS). However, the 2D detection accuracy for unseen objects is still noticealy (-53%) behind the accuracy for seen objects (GDet2023). The online evaluation system stays open and is available at http://bop.felk.cvut.cz/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 2 1 8 2 0 . 4 0 5 2 : r BOP Challenge 2024 on Model-Based and Model-Free 6D Object Pose Estimation Van Nguyen Nguyen1 Stephen Tyree2 Andrew Guo3 Mederic Fourmy4 Anas Gouda5 Taeyeop Lee6 Sungphill Moon7 Hyeontae Son7 Lukas Ranftl8,9 Jonathan Tremblay2 Eric Brachmann10 Bertram Drost8 Vincent Lepetit1 Carsten Rother11 Stan Birchfield2 Jiri Matas4 Yann Labbe13 Martin Sundermeyer12 Tomas Hodan13 1ENPC 2NVIDIA 3University of Toronto 4CTU Prague 5TU Dortmund 6KAIST 7NAVER LABS 8MVTec 9TU Munich 10Niantic 11Heidelberg University 12Google 13Meta HOT3D [1] HOPEv2 [41] HANDAL [12] Static/dynamic onboarding Fig. 1. New BOP-H3 datasets with object-onboarding sequences for model-free tasks. The first three columns show sample images from the new datasets, with the contour of 3D object models in the ground-truth poses drawn in green. The fourth column shows static (top) and dynamic (bottom) onboarding sequences, which are available in BOP-H3 and used for learning objects in the newly introduced model-free tasks."
        },
        {
            "title": "Abstract",
            "content": "We present the evaluation methodology, datasets and results of the BOP Challenge 2024, the sixth in series of public competitions organized to capture the state of the art in 6D object pose estimation and related tasks. In 2024, our goal was to transition BOP from lab-like setups to real-world scenarios. First, we introduced new model-free tasks, where no 3D object models are available and methods need to onboard objects just from provided reference videos. Second, we defined new, more practical 6D object detection task where identities of objects visible in test image are not provided as input (unlike in the classical 6D localization). Third, we introduced new BOP-H3 datasets recorded with highresolution sensors and AR/VR headsets, closely resembling realworld scenarios. BOP-H3 include 3D models and onboarding videos to support both model-based and model-free tasks. Participants competed on seven challenge tracks, each defined by task (6D localization, 6D detection, 2D detection), object onboarding setup (model-based, model-free), and dataset group (BOP-ClassicCore, BOP-H3). Notably, the best 2024 method for model-based 6D localization of unseen objects (FreeZeV2.1) achieves 22% higher accuracy on BOP-Classic-Core than the best 2023 method (GenFlow), and is only 4% behind the best 2023 method for seen objects (GPose2023) although being significantly slower (24.9 vs 2.7 per image). more practical 2024 method for this task is Co-op which takes only 0.8 per image and is 25X faster and 13% more accurate than GenFlow. Methods have similar ranking on 6D detection as on 6D localization but (as expected) higher run time. On model-based 2D detection of unseen objects, the best 2024 method (MUSE) achieves 21% relative improvement compared to the best 2023 method (CNOS). However, the 2D detection accuracy for unseen objects is still noticealy (-53%) behind the accuracy for seen objects (GDet2023). The 2D detection stage is consequently the main bottleneck of existing pipelines for 6D pose estimation of unseen objects. The online evaluation system stays open and is available at: bop.felk.cvut.cz. 1 1. Introduction 20172023 summary. To measure the progress in 6D object pose estimation and related tasks, we created BOP (Benchmark Object Pose) in 2017 and have been organizing challenges on the benchmark datasets since then. Results of challenges from 2017, 2019, 2020, 2022, and 2023 are published in [16, 1820, 38]. The field has come long way, with the accuracy in model-based 6D localization of seen objects (target objects are seen during training) improving by more than 50% (from 56.9 to 85.6 AR). In 2023, as the accuracy in this classical task had been saturating, we introduced more practical yet more challenging task of model-based 6D localization of unseen objects, where new objects need to be onboarded just from their CAD models in under 5 minutes on single GPU. In addition to model-based 6D object localization, we have been evaluating model-based 2D object detection and 2D object segmentation. New model-free setup. While the model-based tasks are relevant for warehouse or factory settings where CAD models of target objects are typically available, their applicability is limited in openworld scenarios. In 2024, we bridged this gap by introducing new model-free tasks, where CAD models are not available and methods need to instead learn new objects on the fly from onboarding (reference) videos. Methods that can operate in such model-free setup will minimize the on-boarding burden of new objects and unlock new types of applications, including augmented-reality systems capable of prompt object indexing and re-identification. New BOP-H3 datasets. To enable the model-free tasks and their comparison to the model-based variants, we introduced three new datasets referred jointly as BOP-H3: HOT3D [1], HOPEv2 [41], and HANDAL [12]. These datasets include texture-mapped CAD models and onboarding videos for 101 objects. To simulate different real-world setups, the datasets include two types of onboarding videos: static onboarding where the object is static and the camera is moving around the object and capturing all possible object views, and dynamic onboarding where the object is manipulated by hands and the camera is either static (on tripod) or dynamic (on head-mounted device). While methods were allowed to use all frames of the onboarding videos in 2024, we are planning to gradually limit the number of used frames to increase the practicality of the problem setup. See Fig. 1 for sample images from BOP-H3 and Sec. 3.2 for details. New 6D detection task. In 2024, we also revisited the evaluation of object pose estimation. Since the beginning of BOP, we distinguish two object pose estimation tasks: 6D object localization, where identifiers of present object instances are provided for each test image, and 6D object detection, where no prior information is provided (see appendix A.1 in [19] for detailed comparison of these tasks). Up until 2024, we had been evaluating methods for object pose estimation only on the 6D object localization because (1) pose accuracy on this simpler task had not been saturated, and (2) evaluating this task requires only calculating the recall 2 rate which is noticeably less expensive than calculating the precision/recall curve required for evaluating 6D detection. While still supporting the 6D localization task, in 2024 we started evaluating also the 6D detection task. This was possible thanks to new GPUs that we secured for the BOP evaluation server, run-time improvements of the evaluation scripts, and simpler evaluation methodology only MSSD and MSPD pose error functions are calculated for 6D detection, not VSD (see Section 2.2 of [19] for definition of the functions). The VSD pose error function is more expensive to calculate and requires depth images which are not available in HOT3D and HANDAL. Besides speeding up the evaluation, omitting VSD therefore enables us to evaluate on RGB-only datasets. Summary of 2024 results. Participants of BOP 2024 competed on seven tracks, with each track defined by task and group of datasets. Three of the tracks were on BOP-Classic-Core datasets and focused on model-based 6D localization, 6D detection, and 2D detection of unseen objects. The other four tracks were on BOP-H3 datasets and focused on model-based and model-free 6D detection and model-based and model-free 2D detection of unseen objects. In all tracks, methods had to onboard new object within 5 minutes on single GPU. Methods were onboarding the objects using provided CAD models in the model-based tasks and using provided onboarding video sequences in the model-free tasks. Notably, the best 2024 method for model-based 6D localization of unseen objects (FreeZeV2.1 [3]) achieves 22% higher accuracy on BOP-Classic-Core than the best 2023 method (GenFlow [29]; 82.1 vs 67.4 AR). FreeZeV2.1 is only 4% behind the best 2023 method for seen objects (GPose2023 [46]; 82.1 vs 85.6 AR), although being significantly slower (24.9 vs 2.7 for estimating pose of all objects in an image on average). more practical 2024 method for this task is Co-op [30], which takes only 0.8 per image and achieves decent accuracy of 75.9 AR. Co-op is 25X faster and 13% more accurate than GenFlow [29]. Many methods for model-based 6D object pose estimation were evaluated on both the 6D localization task and the new 6D detection task. Ranking on the two tasks is similar, with the main (expected) difference being higher run-time on the 6D detection task. On model-based 2D detection of unseen objects, the best 2024 method (MUSE) achieves 21% relative improvement compared to the best 2023 method for this task (CNOS [32]; 52.0 vs 42.8 AP). However, the 2D detection accuracy for unseen objects is still noticeably behind the accuracy for seen objects (GDet2023 [46] achieves 79.8 AP). The 2D detection stage is consequently the primary bottleneck of existing pipelines for 6D pose estimation of unseen objects (all first detect target objects in 2D and then estimate the 6D pose per detection). Participation in challenge tracks on the new BOP-H3 datasets and model-free tasks has been limited, supposedly due to the limited time and the non-negligible effort required to adopt the new datasets and develop new methods. However, the evaluation system for all tasks stays open and we hope to see more submissions to these tracks in the future. 2. Evaluation methodology BOP 2024 focuses on 2D detection and 6D pose estimation of unseen objects (target objects are not seen during training). Tasks 1 and 2 are the same as in 2023 while tasks 35 are new. is measured by: ARD = (ARVSD +ARMSSD +ARMSPD)/3, which is calculated over estimated poses of all objects from D. The overall accuracy on the seven BOP-Classic-Core datasets1 is measured by ARCore defined as the average of the per-dataset ARD scores (see Sec. 2.4 of [19] for details).2 2.1. Task 1: Model-based 6D object localization 2.2. Task 2: Model-based 2D object detection Training input: At training, method is provided set of training images showing training objects annotated with ground-truth 6D object poses, and 3D mesh models of the objects (typically with color texture). Depending on the dataset, the images may be RGBD, RGB or just monochrome. 6D pose is defined by matrix P=[Rt], where is 3D rotation matrix, and is 3D translation vector. The matrix defines rigid transformation from the 3D object model space to the 3D camera space. The method can use 3D mesh models that are available for the training objects. Object-onboarding input: The method is provided 3D mesh models of test objects that are not seen during training. To onboard each object (e.g., to render images/templates or fine-tune neural network), the method can spend up to 5 minutes of the wall-clock time on computer with single GPU. The time is measured from the point right after the 3D mesh models are loaded to the point when the object is onboarded. The method can render images of the 3D models but cannot use any real images of the objects for onboarding. The object representation (which may be given by set of templates, machine-learning model, etc.) needs to be fixed after onboarding (it cannot be updated on test images). Test input: At test time, the method is given an image and list L=[(o1,n1), ..., (om,nm)], where ni is the number of instances of object oi visible in the image. The method can use provided default detections/segmentations produced by CNOS [32]. Test output: The method produces list =[E1,...,Em], where Ei is list of ni pose estimates with confidences for instances of object oi visible in the given test image. Evaluation methodology: The error of an estimated pose w.r.t. the ground-truth pose is calculated by three pose-error functions (see Sec. 2.2 of [19] for details): (1) VSD (Visible Surface Discrepancy) which treats indistinguishable poses as equivalent by considering only the visible object part, (2) MSSD (Maximum Symmetry-Aware Surface Distance) which considers set of pre-identified global object symmetries and measures the surface deviation in 3D, (3) MSPD (Maximum Symmetry-Aware Projection Distance) which considers the object symmetries and measures the perceivable deviation. An estimated pose is considered correct w.r.t. pose-error function e, if e<θe, where e{VSD,MSSD,MSPD} and θe is the threshold of correctness. The fraction of annotated object instances for which correct pose is estimated is referred to as Recall. The Average Recall w.r.t. function e, denoted as ARe, is defined as the average of the Recall rates calculated for multiple settings of the threshold θe and also for multiple settings of misalignment tolerance τ in the case of VSD. The accuracy of method on dataset Training input: At training time, method is provided set of training images showing training objects annotated with groundtruth 2D bounding boxes. The boxes are amodal, i.e., covering the whole object silhouette including the occluded parts. The method can also use 3D models that are available for the training objects. Object-onboarding input: As in Task 1. Test input: At test time, the method is given an image showing an arbitrary number of instances of an arbitrary number of test objects, with all objects being from one specified dataset (e.g., YCB-V). No prior information about the visible object instances is provided. Test output: The method produces list of detections with confidences, with each detection defined by an amodal 2D box. Evaluation methodology: Following the evaluation methodology from the COCO 2020 Object Detection Challenge [25], the detection accuracy is measured by the Average Precision (AP). Specifically, per-object APO score is calculated by averaging the precision at multiple Intersection over Union (IoU) thresholds: [0.5,0.55,...,0.95]. The accuracy of method on dataset is measured by APD calculated by averaging per-object APO scores, and the overall accuracy AP on the core datasets (Sec. 3) is defined as the average of the per-dataset APD scores. Analogous to the 6D localization task, only annotated object instances for which at least 10% of the projected surface area is visible need to be detected. Correct predictions for instances that are visible from less than 10% are filtered out and not counted as false positives. Up to 100 predictions per image with the highest confidences are considered. 2.3. Task 3: Model-based 6D object detection Training and object-onboarding input: As in Task 1. Test input: As in Task 2. Test output: As in Task 1. Evaluation methodology: We simplify the evaluation methodology of this task compared to Task 1 model-based 6D localization of unseen objects (Sec 2.1). More precisely, the error of an estimated pose w.r.t. the GT pose is calculated by three pose-error functions: (1) MSSD (Maximum Symmetry-Aware Surface Distance) which considers set of pre-identified global object symmetries and measures the surface deviation in 3D, (2) MSPD (Maximum Symmetry-Aware Projection Distance) which considers the object symmetries and measures the perceivable 1Task 1 is evaluated only on BOP-Classic datasets. On BOP-H3, pose estimation methods are only evaluated on the new and more practical 6D detection task. 2When calculating ARCore, scores are not averaged over objects before averaging over datasets, which is done when calculating APCore (Sec. 2.2) to comply with the original COCO evaluation methodology [25]. 3 deviation. The VSD pose error function is not considered for this task as it is more expensive to calculate and requires depth images. Besides speeding up the evaluation, omitting VSD thus enables us to evaluate on RGB-only datasets. 2.4. Task 4: Model-free 6D object detection Training input: As in Task 1. Object-onboarding input: The method is provided video(s) of test objects that were not seen during training. 3D models of test objects are not available. The method can use one of the two following types of onboarding videos  (Fig. 2)  : Static onboarding: The object is static and the camera is moving around the object and capturing all possible object views. Two videos are available, one with the object standing upright and one with the object standing upside-down. This type of onboarding videos is useful for 3D object reconstruction by methods such as NeRF [28] or Gaussian Splatting [22]. Object poses are available for all video frames. Dynamic onboarding: The object is manipulated by hands and the camera is either static (on tripod) or dynamic (on head-mounted device). This type of onboarding videos is useful for 3D object reconstruction by methods such as BundleSDF [43] or Hampali et al. [13]. GT object poses are available only for the first frame to simulate real-world setup (at least one GT pose needs to be provided to define the object coordinate system, which is necessary for evaluation of object pose estimates). Compared to the static onboarding setup, the dynamic onboarding setup is more challenging but more natural for AR/VR applications. To onboard each object (e.g. to reconstruct 3D model, render novel views, or fine-tune neural network), the method can spend up to 5 minutes of the wall-clock time on single computer with up to one GPU. The time is measured from the point right after the raw data, e.g., reference video(s), is loaded to the point when the object is onboarded. The object representation (which may be given by set of templates, neural radiance fields, etc.) needs to be fixed after onboarding, i.e it cannot be updated on test images. Test input: As in Task 2. Test output: As in Task 1. Evaluation methodology: As in Task 3. 2.5. Task 5: Model-free 2D object detection Training input: As in Task 1. Onboarding input: As in Task 4. Test input, output, and evaluation: As in Task 2. 3. Datasets BOP Challenge 2024 uses the BOP-Classic datasets (Sec. 3.1), which were used in all challenges prior 2024, and the newly introduced BOP-H3 datasets (Sec. 3.2). All datasets include 3D object models (in most cases with color texture) which were created manually or using KinectFusion-like systems [31]. While all test images are real, training images may be real and/or synthetic. Table 1 shows parameters of the datasets. Fig. 2. Sample onboarding videos from HOPEv2. First two rows show sample frames from static onboarding video, one with the object standing upright and one with the object standing upside-down. The third row shows sample frames from dynamic onboarding video where the object is manipulated by hands. Ground-truth poses (shown with green contour) are provided for all frames of static but only the first frame of dynamic onboarding videos (see Sec. 2.4 for details). 3.1. BOP-Classic datasets These are classical datasets for 6D object pose estimation which were used in prior BOP challenges. Seven out of the twelve datasets are marked as BOP-Classic-Core and, as in previous years, methods were required to be evaluated at least on these core datasets in order to be considered for the challenge awards. More details about these datasets can be found in Chapter 7 of [15]. 3.2. BOP-H3 datasets This group of three new datasets HOT3D, HOPEv2, and HANDAL, called BOP-H3, includes both CAD models  (Fig. 3)  and onboarding videos for each object, and therefore enables the comparison between model-based and model-free methods. HOT3D [1]. dataset for egocentric hand and object tracking in 3D with multi-view RGB and monochrome image streams showing participants interacting with 33 diverse rigid objects. The dataset is recorded by two recent head-mounted devices from Meta: Project Aria, which is research prototype of light-weight AI glasses, and Quest 3, which is production VR headset that has been sold in millions of units. HOT3D also includes PBR materials for the 3D object models, real training images, 3D hand pose and shape annotations in the MANO format, and eye-tracking signal in recordings from Aria. HOPEv2 [41]. dataset for robotic manipulation composed of 28 toy grocery objects, available from online retailers for about 50 USD. The original HOPE dataset was captured as 50 cluttered, single-view scenes in household/office, each with up to 5 lighting variations. For the 2024 challenge, we release an updated version with additional test images collected from 7 cluttered scenes. HOPEv2 also includes the depth channel for test images. HANDAL [12]. dataset with graspable or manipulable objects such as hammers, ladles, cups, and power drills. Objects are 1 2 3 4 5 6 8 9 10 11 12 13 14 15 16 17 18 19 20 21 23 24 25 26 27 28 30 31 32 33 HOT3D objects (33 objects) 1 2 4 5 6 7 8 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 HOPEv2 objects (28 objects) 1 2 3 4 5 7 8 9 10 11 12 13 14 15 16 17 18 19 20 22 23 24 25 26 27 28 29 30 31 32 34 35 36 37 38 39 40 HANDAL objects (40 objects) Fig. 3. Overview of objects in the BOP-H3 datasets used in the 2024 challenge. Train. im. Val im. Test im. Test inst."
        },
        {
            "title": "Dataset",
            "content": "Obj. Real"
        },
        {
            "title": "Used",
            "content": "for MegaPose [23] using BlenderProc [68]. The objects are from the Google Scanned Objects [10] and ShapeNetCore [4] datasets. 50K 50K 154200 50 13261 457 54 5140 709715 23642 9492 1684 74771 9276 9276 457 4. Results and discussion 4.1. Experimental setup 1445 6423 3041 1630 4123 600 1786 3000 1380 800 1670 2898 BOP-H3: 33 420600 HOT3D [1] HOPEv2 [41] 28 HANDAL [12] 40 BOP-Classic-Core: 8 LM-O [2] 30 37584 T-LESS [17] 28 ITODD [11] 33 HB [21] 21 113198 YCB-V [45] 3 38288 TUD-L [18] 2 IC-BIN [9] 1214 50K 10080 50K 50K 721 54 50K 4420 13000 20738 50K 23914 50K 177 50K 200 9038 1000 67308 721 3041 300 67542 900 98547 600 23914 2176 BOP-Classic-Extra: 15 LM [14] 14 RU-APC [37] 6 IC-MI [40] 21 TYO-L [18] 28 HOPEv1 [41] 50K 18273 5964 2067 1670 188 50 3000 18273 5964 1380 5318 300 1670 1670 3472 188 Tab. 1. Parameters of the BOP datasets. PBR training images rendered by BlenderProc [6,7] are provided for most datasets. Ground-truth object poses are publicly available only for training and validation images, and also for test images from BOP-Classic datasets that do not have validation images. Private ground-truth poses are only accessible by the BOP evaluation server. All test images are real. Column Test inst./All shows the number of annotated object instances for which at least 10% of the projected surface area is visible in the test image. Columns Used show the number of used test images and object instances. All datasets offer 3D object models, but only BOP-H3 datasets offer videos for object onboarding. captured from multiple views in cluttered scenes. The original dataset has 212 objects from 17 categories. We captured additional test images and only consider 40 objects from 7 categories, each with high-quality CAD models created by 3D artists. Participants were submitting results to the online evaluation system at bop.felk.cvut.cz from May 29th, 2024 until the deadline on Nov 29th, 2024.3 method had to use fixed set of hyper-parameters across all objects and datasets. In the model-based tasks, method could render images of the 3D models or use subset of the BlenderProc images originally provided for BOP 2020 [19] the method could use as many images from this set as could be rendered within the limited onboarding time (rendering and any additional processing had to fit within 5 minutes, considering that rendering of one BlenderProc image takes 2 seconds). In the model-free tasks, method could apply any reconstruction method to the onboarding videos to obtain 3D models of test objects, but the whole onboarding of an object had to fit within 5 minutes. Not single pixel of test images may have been used for training and onboarding, nor the individual GT that are publicly available for test images of some datasets. Ranges of the azimuth and elevation camera angles, and range of the camera-object distances determined by the GT poses from test images are the only information about the test set that may have been used during training and onboarding. Only subsets of test images were used (see Tab. 1) to remove redundancies and speed up the evaluation, and only object instances for which at least 10% of the projected surface area is visible were considered in the evaluation. 4.2. Challenge tracks 3.3. Pre-training dataset As in 2023, we provided over 2M training images showing 50K+ diverse objects, which are not included in BOP-Classic nor BOP-H3 and can be therefore used for pre-training methods for tasks on unseen objects. The images were originally synthesized Participants competed on seven challenge tracks, each defined by object onboarding setup (model-based, model-free), task (6D localization, 6D detection, 2D detection), and dataset group (BOP-Classic-Core, BOP-H3): 3Evaluation scripts are in: github.com/thodan/bop toolkit 5 # Method Awards Year Det./seg. Refinement Train im. ...type Test image LM-O T-LESS TUD-L IC-BIN ITODD HB YCB-V AR Time FreeZe (SAM6D) [3] SAM6D [24] FreeZe (CNOS) [3] SAM6D-ZeroPose [24] SAM6D-CNOSmask [24] PoZe (CNOS) FreeZeV2.1 [3] 1 FRTPose.v1 (SAM6D-FastSAM) 2 FRTPose.v1 (Default Detections) 3 FRTPose.v1 (MUSE) 4 FreeZeV2 [3] 5 FRTPose (SAM6D-FastSAM) 6 7 FRTPose (Default Detections) 8 Co-op (F3DT2D, 5 Hypo) [30] 9 Co-op (F3DT2D, 1 Hypo) [30] 10 Co-op (CNOS, 5 Hypo) [30] 11 Co-op (CNOS, 1 Hypo) [30] FoundationPose [44] 12 13 FRTPose (SAM6D-FastSAM & top k) 14 Co-op (CNOS, Coarse) [30] 15 GZS6D-BP(coarse+refine+teaser) 16 17 18 19 GigaPose+GenFlow+kabsch (5 hypoth) [29, 33] 20 Co-op (F3DT2D, 5 Hypo) [30] 21 GenFlow-MultiHypo16 [29] 22 GenFlow-MultiHypo [29] 23 SAM6D-FastSAM [24] 24 Co-op (CNOS, 5 Hypo) [30] 25 SAM6D-CNOSfastSAM [24] 26 Co-op (CNOS, 1 Hypo) [30] 27 Megapose-CNOS+Multih Teaserpp-10 [23] 28 Megapose-CNOS+Multih Teaserpp [23] 29 30 31 32 GigaPose+GenFlow (5 hypo) [29, 33] 33 34 OPFormer-Megapose refinement (CNOS) 35 GigaPose (Add) + Megapose (5 hypo) [23, 33] 36 Co-op (CNOS, Coarse) [30] 37 GigaPose+MegaPose (5 Hypo) [23, 33] 38 GenFlow-MultiHypo16 [29] 39 TF6D (Default, CNOS) + Megapose [23] 40 ZeroPose-Multi-Hypo-Refinement [5, 32] 41 GenFlow-MultiHypo-RGB [29] 42 GigaPose+GenFlow (1 hypo) [29, 33] 43 GenFlow [29] 44 FoundPose+FeatRef+Megapose [23, 35] 45 Megapose-CNOS fastSAM+Multih-10 [23] 46 47 GigaPose+MegaPose [23, 33] 48 Megapose-CNOS fastSAM+Multih [23] 49 ZeroPose-Multi-Hypo-Refinement [5] 50 MegaPose-CNOS fastSAM [23] 51 OPFormer-Coarse (CNOS) 52 53 54 55 TF6D (Default, CNOS) FoundPose-Coarse [35] 56 57 ZeroPose-One-Hypo [5] 58 GigaPose [33] 59 GenFlow-coarse [29] 60 MegaPose-CNOS fastSAM+CoarseBest [23] SMC-1.0s-CNOS SMC-0.5s-CNOS FoundPose+FeatRef [35] FoundPose+FeatRef+Megapose-5hyp [23, 35] FoundPose+MegaPose [23, 35] (cid:211) º ICP SAM6D-FastSAM - - SAM6D SAM6D-SAM - Teaserpp ICP - ICP GenFlow Co-op GenFlow GenFlow SAM6D-FastSAM FoundationPose FoundationPose FoundationPose ICP - RGB-D RGB-D RGB-D - RGB-D RGB-D RGB-D RGB-D RGB-D RGB-D RGB-D RGB-D RGB-D RGB-D - RGB-D - RGB-D RGB-D RGB-D RGB RGB-D RGB-D RGB-D RGB-D RGB RGB RGB-D RGB-D RGB-D RGB-D (cid:240) 2024 Custom 2024 2024 CNOS-FastSAM 2024 MUSE 2024 Custom SAM6D-FastSAM FoundationPose 2024 FoundationPose 2024 CNOS-FastSAM Co-op F3DT2D 2024 Co-op 2024 F3DT2D Co-op 2024 CNOS-FastSAM Co-op 2024 CNOS-FastSAM SAM6D 2024 FondationPose 2024 SAM6D-FastSAM FondationPose 2024 CNOS-FastSAM 2024 2024 2024 2024 CNOS-FastSAM 2024 CNOS-FastSAM 2024 F3DT2D 2023 CNOS-FastSAM 2023 CNOS-FastSAM 2024 2024 CNOS-FastSAM 2024 CNOS-FastSAM 2024 CNOS-FastSAM 2023 CNOS-FastSAM 2023 CNOS-FastSAM 2024 SAM6D 2023 CNOS-FastSAM 2023 CNOS-FastSAM 2024 CNOS-FastSAM 2024 CNOS-FastSAM 2024 CNOS-FastSAM 2024 CNOS-FastSAM 2024 CNOS-FastSAM 2024 CNOS-FastSAM 2023 CNOS-FastSAM 2024 CNOS-FastSAM 2023 2023 CNOS-FastSAM 2024 CNOS-FastSAM 2024 CNOS-FastSAM 2024 CNOS-FastSAM 2023 CNOS-FastSAM 2024 CNOS-FastSAM 2024 CNOS-FastSAM 2023 CNOS-FastSAM 2023 2023 CNOS-FastSAM 2024 CNOS-FastSAM 2023 CNOS-FastSAM 2023 CNOS-FastSAM 2024 CNOS-FastSAM 2024 CNOS-FastSAM 2024 CNOS-FastSAM 2023 2024 CNOS-FastSAM 2023 CNOS-FastSAM 2023 CNOS-FastSAM Co-op - Co-op Teaserpp Teaserpp - - ICP GenFlow MegaPose+FeatRef RGB RGB MegaPose RGB MegaPose RGB-D - RGB MegaPose RGB-D GenFlow RGB MegaPose RGB-D FastSAM+ImBind MegaPose RGB-D GenFlow RGB-D GenFlow GenFlow RGB-D MegaPose+FeatRef RGB RGB MegaPose RGB MegaPose RGB MegaPose RGB MegaPose RGB-D FastSAM+ImBind MegaPose RGB MegaPose - - - - - - - FeatRef - - - - RGB-D - RGB - RGB-D - RGB - FastSAM+ImBind RGB-D RGB-D RGB-D RGB-D RGB-D RGB-D RGB-D RGB-D RGB-D RGB-D RGB-D RGB-D RGB-D RGB-D RGB-D RGB-D RGB-D RGB-D RGB-D RGB RGB-D RGB-D RGB-D RGB RGB-D RGB RGB-D RGB-D RGB-D RGB-D RGB-D RGB RGB RGB RGB RGB RGB RGB RGB RGB-D RGB RGB RGB RGB RGB RGB RGB RGB - PBR PBR PBR - PBR PBR PBR PBR PBR PBR PBR PBR PBR - - PBR - PBR PBR PBR PBR PBR PBR PBR PBR PBR PBR PBR PBR Custom PBR PBR PBR PBR PBR PBR PBR PBR PBR PBR PBR PBR PBR PBR PBR PBR PBR PBR+Real RGB-D PBR - - - - - - PBR+Real RGB-D PBR PBR PBR"
        },
        {
            "title": "RGB\nRGB\nRGB",
            "content": "77.1 77.8 77.7 78.6 76.4 78.3 78.3 73.8 73.0 73.0 71.5 75.6 70.3 70.0 67.8 71.6 69.9 68.9 67.8 67.5 63.5 62.2 66.7 65.5 65.1 64.2 62.6 62.0 63.5 64.8 64.4 63.1 61.0 59.6 60.4 59.7 59.8 57.2 56.0 53.8 56.3 59.5 54.7 55.6 56.0 55.4 55.7 56.0 49.3 49.9 52.5 55.8 51.2 39.5 32.3 39.7 27.2 29.6 25.0 22.9 75.5 76.6 76.3 76.8 70.8 71.7 71.4 69.5 68.0 66.4 64.6 64.6 58.1 64.2 69.4 53.1 51.5 52.0 55.6 68.2 52.1 50.9 48.5 64.8 47.9 63.5 48.7 48.5 43.0 48.3 49.4 58.2 57.0 53.4 57.6 59.2 56.5 52.8 59.0 40.0 52.3 55.0 51.4 51.1 50.8 51.0 54.1 50.7 34.2 47.7 41.8 42.3 41.5 39.6 35.0 33.8 15.6 26.4 21.5 17.7 97.6 94.0 94.0 94.2 97.2 92.5 92.2 92.9 92.9 90.5 90.5 92.3 87.1 87.9 92.2 94.9 90.4 93.6 81.1 76.7 86.2 84.9 82.9 72.9 82.5 71.7 85.1 84.6 80.2 79.4 92.4 66.4 69.3 69.3 64.8 64.2 63.1 68.8 66.9 83.5 68.4 60.7 67.0 63.3 68.7 63.3 58.0 68.4 79.0 65.3 61.5 59.9 51.1 56.7 47.3 46.9 53.6 30.0 30.0 25.8 69.7 70.2 70.5 70.6 65.4 60.1 59.0 63.5 62.4 59.7 57.5 50.8 59.9 56.4 55.0 54.5 58.8 49.9 56.3 58.9 53.4 52.4 51.0 54.4 49.7 51.2 46.7 46.2 51.8 50.4 40.9 49.8 47.9 47.0 48.2 45.8 47.3 45.8 45.7 39.2 45.3 47.8 43.7 40.0 41.9 43.0 45.0 41.4 39.6 36.7 34.2 31.6 29.0 28.3 33.2 23.9 30.7 22.3 16.8 15.2 74.2 73.7 73.5 71.0 67.9 64.6 61.8 62.9 60.0 61.3 58.2 58.0 64.4 56.6 59.7 58.6 60.2 56.1 57.5 50.6 55.4 54.4 57.2 49.1 56.2 47.3 46.8 46.0 48.4 35.1 51.6 45.3 40.7 39.2 39.8 39.1 39.7 39.8 37.5 52.1 39.5 41.3 38.4 35.7 34.6 34.6 37.6 33.8 46.5 31.5 27.8 38.9 35.8 26.2 25.1 20.4 36.2 17.5 15.4 10.8 89.2 89.6 89.6 90.3 85.9 89.6 89.6 87.8 86.3 87.1 85.7 83.5 80.4 84.2 80.3 79.6 77.6 79.0 79.1 85.6 77.9 77.0 73.6 85.0 73.8 83.2 73.0 72.5 69.1 72.7 71.2 75.6 72.3 76.0 72.4 78.1 72.2 74.6 70.1 65.3 73.9 72.2 73.0 69.7 70.6 69.5 69.3 70.4 62.9 65.4 67.3 58.5 53.8 58.5 53.7 50.8 46.2 34.1 28.3 25. 91.5 91.0 91.0 91.0 90.6 91.3 91.3 89.8 88.6 88.7 87.4 88.9 86.9 85.3 77.2 84.0 84.5 85.3 82.5 69.7 83.3 81.8 83.4 68.9 81.5 67.0 76.4 76.4 79.2 80.4 61.1 65.2 69.0 67.0 66.6 62.6 66.1 64.2 66.5 65.3 63.3 60.8 61.9 66.1 62.0 66.1 63.2 62.1 62.3 60.1 60.6 45.4 40.3 49.7 54.1 45.2 34.1 27.8 27.7 28.1 82.1 81.8 81.8 81.8 79.2 78.3 77.7 77.1 75.9 75.2 73.6 73.4 72.4 72.1 71.7 70.9 70.4 69.3 68.6 68.2 67.4 66.2 66.2 65.8 65.3 64.0 62.8 62.3 62.2 61.6 61.6 60.5 59.6 58.8 58.5 58.4 57.8 57.6 57.4 57.0 57.0 56.8 55.7 55.0 54.9 54.7 54.7 54.7 53.4 50.9 49.4 47.5 43.3 42.6 40.1 37.3 34.8 26.8 23.5 20.8 24.9 40.1 46.5 27.6 17.2 20.7 23.4 6.9 0.8 7.2 2.3 29.3 0.8 1.0 6.5 11.5 4.4 13.5 11.1 3.9 34.58 21.46 1.4 4.2 1.3 1.7 142.0 116.6 5.5 3.9 159.4 10.6 20.5 1.5 10.8 1.0 7.7 40.5 4.1 16.2 20.9 2.2 10.6 6.4 53.9 4.4 2.3 47.4 19.0 31.7 0.5 6.1 3.0 2.6 1.6 1.7 9.8 0.4 3.8 15.5 Tab. 2. Track 1: Model-based 6D localization of unseen objects on BOP-Classic-Core. Methods are ranked by the AR score, which is the average of per-dataset ARD scores (Sec. 2.1). The last column shows the average time to generate predictions for all objects in single image, averaged over the datasets (measured on different computers by the participants). Column Year is the year of submission, Det./seg. is the object detection/segmentation method, Refinement is the pose refinement method, Train im. and Test im. show image channels used at training and test time respectively, and Train im. type is the domain of training images. All test images are real. See Sec. 5 for description of the awards. # Method Awards Year Det./seg. Refinement Train im. ...type Test image LM-O T-LESS TUD-L IC-BIN"
        },
        {
            "title": "ITODD",
            "content": "FreeZeV2.1 [3] FreeZeV2 (SAM6D) [3] FreeZeV2 (SAM6D, Coarse-to-Fine) [3] 1 2 3 4 Co-op (F3DT2D, Coarse, RGBD) [30] 5 Co-op (F3DT2D, 5 Hypo, RGBD) [30] 6 Co-op (CNOS, Coarse, RGBD) [30] 7 Co-op (CNOS, 1 Hypo, RGBD) [30] 8 Co-op (CNOS, 5 Hypo, RGBD) [30] 9 Co-op (F3DT2D, 5 Hypo) [30] 10 Co-op (CNOS, 1 Hypo) [30] 11 GigaPose+GenFlow (5 hypothesis) [29, 33] 12 Co-op (CNOS, 5 Hypo) [30] 13 GigaPose+GenFlow (RGBD) [29, 33] 14 Co-op (CNOS, Coarse) [30] 15 GigaPose+GenFlow [29, 33] 16 GigaPose+GenFlow+kabsch (5 hypothesis) [29, 33] 17 GigaPose-CVPR24 [33] (cid:211) (cid:240) 2024 Custom ICP SAM6D-FastSAM ICP 2024 SAM6D-FastSAM ICP 2024 F3DT2D 2024 2024 F3DT2D 2024 CNOS-FastSAM 2024 CNOS-FastSAM 2024 CNOS-FastSAM 2024 F3DT2D 2024 CNOS-FastSAM 2024 CNOS-FastSAM 2024 CNOS-FastSAM 2024 CNOS-FastSAM 2024 CNOS-FastSAM 2024 CNOS-FastSAM 2024 CNOS-FastSAM 2024 CNOS-FastSAM - Co-op - Co-op Co-op Co-op Co-op GenFlow Co-op GenFlow - GenFlow GenFlow - º - - - RGB-D RGB-D RGB-D RGB-D RGB-D RGB-D RGB-D RGB-D RGB-D RGB-D RGB-D RGB-D RGB-D RGB - - - PBR PBR PBR PBR PBR PBR PBR PBR PBR PBR PBR PBR PBR PBR RGB-D RGB-D RGB-D RGB-D RGB-D RGB-D RGB-D RGB-D RGB RGB RGB RGB RGB-D RGB RGB RGB-D RGB 79.7 77.5 74.3 69.8 70.1 68.3 67.0 68.2 63.7 61.5 59.7 61.2 57.8 58.9 55.4 23.7 6. 75.1 61.0 60.1 62.0 61.3 59.6 58.3 58.9 61.6 58.8 56.5 59.0 46.7 55.8 43.6 52.6 25.5 99.1 97.5 90.2 84.1 76.6 80.8 76.9 73.8 65.8 64.1 68.8 61.9 71.5 64.0 60.8 24.6 3.5 69.6 62.0 53.1 56.4 42.6 46.9 45.8 39.9 46.7 40.9 43.9 39.1 40.2 40.3 35.3 29.2 5.7 76.9 61.7 57.3 57.6 62.7 56.0 57.5 60.4 50.4 46.5 42.5 48.3 44.7 38.8 36.7 52.5 14.9 HB 85.3 78.2 74.1 74.6 73.4 74.3 73.7 73.1 73.2 72.7 70.7 72.3 67.6 71.1 66.8 54.7 18. YCB-V 90.5 86.9 85.8 80.8 81.2 78.2 76.6 76.6 62.6 59.2 60.7 59.1 68.2 57.6 54.6 52.4 12.5 AP 82.3 75.0 70.7 69.3 66.9 66.3 65.1 64.4 60.6 57.7 57.5 57.3 56.7 55.2 50.4 41.4 12."
        },
        {
            "title": "Time",
            "content": "37.3 55.4 12.9 0.9 12.2 2.2 6.9 14.3 8.7 6.4 15.5 11.5 4.5 2.2 4.7 16.6 0.7 Tab. 3. Track 2: Model-based 6D detection of unseen objects on BOP-Classic-Core. Methods are ranked by the AP score, which is the average of per-dataset APD scores (Sec. 2.3). Column legends as in Tab. 2. Track 1: Model-based 6D localization on BOP-Classic-Core Track 2: Model-based 6D detection on BOP-Classic-Core Track 3: Model-based 2D detection on BOP-Classic-Core Track 4: Model-based 6D detection on BOP-H3 Track 5: Model-based 2D detection on BOP-H3 Track 6: Model-free 6D detection on BOP-H3 Track 7: Model-free 2D detection on BOP-H3 4.3. Model-based 6D loc. on BOP-Classic-Core Results on model-based 6D object localization of unseen objects in BOP-Classic-Core are shown in Table 2. Among the 44 new methods submitted in 2024, twenty outperform GenFlow [29] (row #21), the best from the 2023 challenge. The top-performing method, FreeZeV2.1 [3], achieves 82.1 AR22% higher than # Method Awards Year Onboarding im. ...type Test image LM-O T-LESS TUD-L IC-BIN ITODD 1 MUSE 2 F3DT2D 3 SAM6D-FastSAM [24] 4 NIDS-Net WA Sappe [27] 5 NIDS-Net WA [27] 6 SAM6D [24] 7 SAM6D-FastSAM [24] 8 ViewInvDet 9 NIDS-Net basic [27] 10 CNOS (FastSAM) [32] 11 CNOS (SAM) [32] 12 ZeroPose [5] (cid:240) º 2024 - 2024 - 2023 RGB-D 2024 RGB 2024 RGB 2024 RGB-D 2024 RGB-D 2024 - 2024 RGB-D 2023 - 2023 - 2023 - RGB RGB RGB-D - - PBR Custom RGB Custom RGB PBR PBR - PBR - - - RGB-D RGB RGB RGB RGB RGB RGB 51.2 50.4 46.3 45.7 44.9 46.5 43.8 44.9 44.9 43.3 39.5 36.7 46.7 48.2 45.8 49.3 48.9 43.7 41.7 40.3 42.8 39.5 33.0 30.0 59.5 57.3 57.3 48.6 46.0 53.7 54.6 50.8 43.4 53.4 36.8 43. 29.8 28.4 24.5 25.7 24.5 26.1 23.4 26.8 24.4 22.6 20.7 22.8 50.2 48.0 41.9 37.9 36.0 39.4 37.4 32.8 34.9 32.5 31.3 25.0 HB 58.9 57.7 55.1 58.7 59.4 53.0 52.3 55.4 54.8 51.7 42.3 39.8 YCB-V 67.4 66.6 58.9 62.1 62.4 51.8 57.3 58.1 56.5 56.8 49.0 41. AP 52.0 50.9 47.1 46.9 46.0 44.9 44.4 44.2 43.1 42.8 36.1 34.1 Time 0.56 0.43 0.45 0.49 0.49 2.80 0.25 1.70 0.49 0.22 1.85 3.82 Tab. 4. Track 3: Model-based 2D detection of unseen objects on BOP-Classic-Core datasets. The methods are ranked by the APC score which is the average of the per-dataset APD scores defined in Sec. 2.2. Last col. shows the average image processing time in seconds. # Method Awards Year Det./seg. Refine. Train im. ...type T3 PEv"
        },
        {
            "title": "Tim e",
            "content": "1 GigaPose+GenFlow [29, 33] (cid:240) 2024 CNOS-FastSAM GenFlow RGB-D PBR 26.8 41.1 25.6 31.2 5.3 9.4 0.9 2 GigaPose [33] - - 3 OPFormer-Megapose - - 4 OPFormer-Coarse 2024 CNOS-FastSAM MegaPose - - 2024 CNOS-FastSAM - 2024 CNOS-FastSAM 4.1 39.2 26.2 35.1 19.2 7.2 16.7 - - PBR - - RGB Tab. 5. Track 4: Model-based 6D det. of unseen objects on BOP-H3. # Method Awards Year Onboarding im. ...type"
        },
        {
            "title": "1 MUSE\n2 CNOS (FastSAM) [32]\n3 CNOS (SAM) [32]",
            "content": "(cid:240) 2024 N/A 2024 RGB 2024 RGB N/A PBR PBR T3 42.6 35.0 31.7 PEv"
        },
        {
            "title": "H A N D A L",
            "content": "AP"
        },
        {
            "title": "Time",
            "content": "47.4 31.3 36.5 27.0 24.6 19.7 39.0 30.3 29.3 1.5 0.3 1.8 Tab. 6. Track 5: Model-based 2D det. of unseen objects on BOP-H3. # Method Awards Year Onboarding type HOT3D HOPEv2 HANDAL AP"
        },
        {
            "title": "1 GFreeDet (FastSAM) [26] (cid:240) W\n2 GFreeDet (SAM) [26]",
            "content": ""
        },
        {
            "title": "Static\nStatic",
            "content": "33.8 30.9 36.4 38.4 25.5 26.4 31.9 31.9 0.3 2.1 Tab. 7. Track 7: Model-free 2D det. of unseen objects on BOP-H3. GenFlow, with 28% faster runtime. It is also only 4% ARC behind GPose2023, the best method for seen objects. FreeZeV2.1 combines 2D features from frozen DINOv2 [34] and 3D features from GeDi [36], and uses detections from SAM6D [24], NIDS [27], and CNOS [32]. It renders 162 templates per object, extracts 2D visual features, backprojects them to 3D, aggregates, and performs 3D3D matching, followed by ICP and symmetry-aware refinement. Another notable method is FRTPose.v1, ranked second with 81.8 AR. During on-boarding, ResNet34 is trained to predict 3D surface coordinates from 2D inputs and object masks, taking about 4.5 minutes per object on an H100 GPU. It then uses PnP for initial pose estimation and refines it using FoundationPose [44]. As all methods use two-stage approachfirst detecting objects in 2D, then estimating the 6D pose per detectionwe analyzed the qualitative results and found that the most common cause for missing or inaccurate 6D pose estimates are erroneous 2D detections. 4.4. Model-based 6D det. on BOP-Classic-Core Table 3 presents the results of the 20 entries on model-based 6D detection of unseen objects. While the ranking among different methods is consistent between Track 1 and Track 2: FreeZeV2.1 is the best method, followed by FRTPose.v2 and Co-op [30]; the relative improvement of FreeZeV2.1 over Co-op is more significant in 6D detection with 18.8% (#1 and #7 of Tab. 3) than in 6D localization with only 6.5% (#1 and #8 of Tab. 2). row #7 over #8, and #9 over #10 and #11). Co-op uses threestage pipeline: coarse estimation with pretrained CrocoV2 [42], refinement via optical flow (similar to GenFlow [29] but without RAFT [39]), and scoring with MegaPose-style [23] network. The coarse results outperforming refined ones can be attributed to the scoring network being trained only on template-crop input pairs of the same object, making it unreliable when the template and crop candidates often display different objects. 4.5. Model-based 2D det. on BOP-Classic-Core Tab. 4 shows the results of model-based 2D detection for unseen objects on BOP-Classic-Core datasets (Track 3). The best method, MUSE, uses novel similarity metric based on both class and patch embeddings and weights the contributions of 3D template views by von Mises Fisher distribution. MUSE achieves 52.0 AP, + 21% more accurate compared to the best method from the 2023 challenge, CNOS [32] (#10 in Table 4). However, MUSE is still -53% behind the best method for seen objects (GDet2023). 4.6. Model-based 6D detection on BOP-H3 Table 5 presents results for model-based 6D detection of unseen objects on BOP-H3 (Track 4), with 4 entries. All methods used RGB/monochrome images, as depth is unavailable for HOT3D and HANDAL. GigaPose and GigaPose+GenFlow achieve 9.4 and 31.2 AP on BOP-H3, compared to 12.3 and 50.4 AP on BOP-Classic-Core (Track 2, rows #20 and #18 in Table 3). OPFormer, new method, aggregates DINOv2 [34] template descriptors using transformer with 3D positional embeddings and predicts poses via RANSAC-PnP. On HOPEv2, OPFormer outperforms GigaPose by 110% but is 5% less accurate than GigaPose+GenFlow after refinement. 4.7. Model-based 2D detection on BOP-H3 Tab. 6 shows the results of model-based 2D detection for unseen objects, on BOP-H3 datasets. Similar to the results on BOP-Classic-Core datasets, MUSE outperforms CNOS [32] while the relative improvement is not the same: MUSE is 21.5% more accurate than CNOS in BOP-Classic-Core (#1, #10 in Table 4) while the improvement in BOP-H3 is 33.1% (#1, #2 in Table 6). 4.8. Model-free 2D detection on BOP-H3 Interestingly, Co-op (row #7 in Table 3) shows different trend from Track 1, with coarse results outperforming refined ones (e.g., The new task of model-free 2D detection of unseen objects on the BOP-H3 datasets received 3 entries, as presented in Tab. 7. 7 -"
        },
        {
            "title": "O\nM\nL",
            "content": "S - - I"
        },
        {
            "title": "N\nB\nC",
            "content": "- I"
        },
        {
            "title": "B\nH",
            "content": "-"
        },
        {
            "title": "V\nB\nC\nY",
            "content": "D 3 2 O D Fig. 4. Qualitative results of the best methods on model-based 6D object detection task, FreeZeV2.1 [3] on BOP-Classic-Core (Track 2), and GigaPose+GenFlow [29, 33] on BOP-H3 (Track 4). The contour of 3D models in GT poses is green and in the estimated poses in red. 8 GFreeDet [26] introduces model-free 2D detection method that leverages Gaussian splatting [22] to reconstruct objects, then use DINOv2 [34] for template matching. Additionally, on HOPEv2 dataset, GFreeDet is 5% more accurate than CNOS [32] (36.4 vs 34.5 AP) as shown in the leaderboard for this track4. In Fig. 4 we present the qualitative results of the best method for 6D detection on BOP-Classic-Core (Track 2), FreeZeV2.1 [3], and on BOP-H3 (Track 6) GigaPose+GenFlow [29, 33]. Interestingly, the results of FreeZeV2.1 [3] on ITODD show significantly more false positives than on other datasets. 5. Awards The 2024 challenge awards are based on the results analyzed in Sec. 4. Methods receiving awards are marker with icons: (cid:240) for the best overall method, for the best fast method (the most accurate method with the average running time per image below 1 s), for the best open-source method, º for the best RGB-only method, and (cid:211) for the best method using default detections. Due to the limited time and the considerable effort required to adopt model-free settings for 6D pose estimation, we have not yet received any submissions for Track 6. The awarded entries were prepared by the following authors: FreeZeV2.1 [3] by Andrea Caraffa, Davide Boscaini, Amir Hamza, and Fabio Poiesi Co-op [30] by Sungphill Moon and Hyeontae Son FoundationPose [44] by Bowen Wen, Wei Yang, Jan Kautz, and Stan Birchfield FRTPose.v1 (anonymous for now) GigaPose [33] by Van Nguyen Nguyen, Thibault Groueix, Mathieu Salzmann, and Vincent Lepetit MUSE (anonymous for now) SAM6D [24] by Jiehong Lin, Lihua Liu, Dekun Lu, and Kui Jia GenFlow [29] by Sungphill Moon, Hyeontae Son, Dongcheol Hur, and Sangwook Kim CNOS [32] by Van Nguyen Nguyen, Thibault Groueix, Georgy Ponimatkin, Vincent Lepetit, and Tomas Hodan GFreeDet [26] by Xingyu Liu, Yingyue Li, Chengxi Li, Gu Wang, Chenyangguang Zhang, Ziqin Huang, and Xiangyang Ji 6. Conclusions In BOP 2024, methods for 6D localization of unseen objects have almost reached the accuracy of their counterparts for seen objects. While some of the methods for unseen objects now take less than 1 second, further improvements are required to support realtime applications. We hope that the model-free tasks introduced in the 2024 challenge will unlock new types of applications, making object pose estimation significantly more practical. In BOP25, we are introducing BOP-Industrial datasets and the multi-view problem setup to represent real-world applications in industrial robotics. The evaluation system at bop.felk.cvut.cz stays open and raw results of all methods are publicly available. 4bop.felk.cvut.cz/leaderboards"
        },
        {
            "title": "References",
            "content": "[1] Prithviraj Banerjee, Sindi Shkodrani, Pierre Moulon, Shreyas Hampali, Shangchen Han, Fan Zhang, Linguang Zhang, Jade Fountain, Edward Miller, Selen Basol, et al. Hot3d: Hand and object tracking in 3d from egocentric multi-view videos. arXiv preprint arXiv:2411.19167, 2024. 1, 2, 4, 5 [2] Eric Brachmann, Alexander Krull, Frank Michel, Stefan Gumhold, Jamie Shotton, and Carsten Rother. Learning 6D object pose estimation using 3D object coordinates. In ECCV, 2014. 5 [3] Andrea Caraffa, Davide Boscaini, Amir Hamza, and Fabio Poiesi. Object 6d pose estimation meets zero-shot learning. arXiv preprint arXiv:2312.00947, 2023. 2, 6, 8 [4] Angel Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3D model repository. arXiv preprint arXiv:1512.03012, 2015. 5 [5] Jianqiu Chen, Mingshan Sun, Tianpeng Bao, Rui Zhao, Liwei Wu, and Zhenyu He. 3d model-based zero-shot pose estimation pipeline. arXiv preprint arXiv:2305.17934, 2023. 6, 7 [6] Maximilian Denninger, Martin Sundermeyer, Dominik Winkelbauer, Dmitry Olefir, Tomaˇs Hodaˇn, Youssef Zidan, Mohamad Elbadrawy, Markus Knauer, Harinandan Katam, and Ahsan Lodhi. BlenderProc: Reducing the reality gap with photorealistic rendering. RSS Workshops, 2020. 5 [7] Maximilian Denninger, Martin Sundermeyer, Dominik Winkelbauer, Youssef Zidan, Dmitry Olefir, Mohamad Elbadrawy, Ahsan Lodhi, and Harinandan Katam. Blenderproc. arXiv preprint arXiv:1911.01911, 2019. 5 [8] Maximilian Denninger, Dominik Winkelbauer, Martin Sundermeyer, Wout Boerdijk, Markus Wendelin Knauer, Klaus Strobl, Matthias Humt, and Rudolph Triebel. Blenderproc2: procedural pipeline for photorealistic rendering. Journal of Open Source Software, 8(82):4901, 2023. [9] Andreas Doumanoglou, Rigas Kouskouridas, Sotiris Malassiotis, and Tae-Kyun Kim. Recovering 6D object pose and predicting next-best-view in the crowd. In CVPR, 2016. 5 [10] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas McHugh, and Vincent Vanhoucke. Google scanned objects: high-quality dataset of 3D scanned household items. ICRA, 2022. 5 [11] Bertram Drost, Markus Ulrich, Paul Bergmann, Philipp Hartinger, and Carsten Steger. Introducing MVTec ITODD dataset for 3D object recognition in industry. In ICCVW, 2017. 5 [12] Andrew Guo, Bowen Wen, Jianhe Yuan, Jonathan Tremblay, Stephen Tyree, Jeffrey Smith, and Stan Birchfield. Handal: dataset of real-world manipulable object categories with pose annotations, affordances, and reconstructions. In IROS, 2023. 1, 2, 4, 5 [13] Shreyas Hampali, Tomaˇs Hodaˇn, Luan Tran, Lingni Ma, Cem Keskin, and Vincent Lepetit. In-hand 3D object scanning from an RGB sequence. Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 4 [14] S. Hinterstoisser, V. Lepetit, S. Ilic, S. Holzer, G. Bradski, K. Konolige, and N. Navab. Model based training, detection and pose estimation of texture-less 3D objects in heavily cluttered scenes. ACCV, 2012. 5 [15] Tomaˇs Hodaˇn. Pose estimation of specific rigid objects. PhD Thesis, Czech Technical University in Prague, 2021. 4 [16] Tomaˇs Hodaˇn, Eric Brachmann, Bertram Drost, Frank Michel, Martin Sundermeyer, Jiˇrı Matas, and Carsten Rother. BOP Challenge 2019. https://bop.felk.cvut.cz/media/ bop_challenge_2019_results.pdf, 2019. 2 [17] Tomaˇs Hodaˇn, Pavel Haluza, ˇStˇepan Obdrˇzalek, Jiˇrı Matas, Manolis Lourakis, and Xenophon Zabulis. T-LESS: An RGB-D dataset for 6D pose estimation of texture-less objects. WACV, 2017. 5 [18] Tomaˇs Hodaˇn, Frank Michel, Eric Brachmann, Wadim Kehl, Anders Glent Buch, Dirk Kraft, Bertram Drost, Joel Vidal, Stephan Ihrke, Xenophon Zabulis, Caner Sahin, Fabian Manhardt, Federico Tombari, Tae-Kyun Kim, Jiˇrı Matas, and Carsten Rother. BOP: Benchmark for 6D object pose estimation. ECCV, 2018. 2, 5 [19] Tomaˇs Hodaˇn, Martin Sundermeyer, Bertram Drost, Yann Labbe, Eric Brachmann, Frank Michel, Carsten Rother, and Jiˇrı Matas. BOP Challenge 2020 on 6D object localization. In ECCV, 2020. 2, 3, 5 [20] Tomaˇs Hodaˇn, Martin Sundermeyer, Yann Labbe, Van Nguyen Nguyen, Gu Wang, Eric Brachmann, Bertram Drost, Vincent Lepetit, Carsten Rother, and Jiˇrı Matas. BOP challenge 2023 on detection, segmentation and pose estimation of seen and unseen rigid objects. CVPRW, 2024. [21] Roman Kaskman, Sergey Zakharov, Ivan Shugurov, and Slobodan Ilic. HomebrewedDB: RGB-D dataset for 6D pose estimation of 3D objects. ICCVW, 2019. 5 [22] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3D gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 2023. 4, 8 [23] Yann Labbe, Lucas Manuelli, Arsalan Mousavian, Stephen Tyree, Stan Birchfield, Jonathan Tremblay, Justin Carpentier, Mathieu Aubry, Dieter Fox, and Josef Sivic. MegaPose: 6D Pose Estimation of Novel Objects via Render & Compare. In CoRL, 2022. 5, 6, 7 [24] Jiehong Lin, Lihua Liu, Dekun Lu, and Kui Jia. Sam-6d: Segment anything model meets zero-shot 6d object pose estimation. In CVPR, 2024. 6, 7, 8 [25] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft COCO: Common objects in context. ECCV, 2014. 3 [26] Xingyu Liu, Yingyue Li, Chengxi Li, Gu Wang, Chenyangguang Zhang, Ziqin Huang, and Xiangyang Ji. Gfreedet: Exploiting gaussian splatting and foundation models for model-free unseen object detection in the bop challenge 2024. arXiv preprint arXiv:2412.01552, 2024. 7, 8 [27] Yangxiao Lu, Yunhui Guo, Nicholas Ruozzi, Yu Xiang, et al. Adapting pre-trained vision models for novel instance detection and segmentation. arXiv preprint arXiv:2405.17859, 2024. 7 [28] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 2021. [29] Sungphill Moon, Hyeontae Son, Dongcheol Hur, and Sangwook Kim. GenFlow: Generalizable Recurrent Flow for 6D Pose Refinement of Novel Objects. In arXiv preprint arXiv:2403.11510, 2024. 2, 6, 7, 8 [30] Sungphill Moon, Hyeontae Son, Dongcheol Hur, and Sangwook Kim. Co-op: Correspondence-based novel object pose estimation. arXiv preprint arXiv:2503.17731, 2025. 2, 6, 7, 8 [31] Richard Newcombe, Shahram Izadi, Otmar Hilliges, David Molyneaux, David Kim, Andrew Davison, Pushmeet Kohi, Jamie 9 Shotton, Steve Hodges, and Andrew Fitzgibbon. KinectFusion: Real-time dense surface mapping and tracking. ISMAR, 2011. 4 [32] Van Nguyen Nguyen, Thibault Groueix, Georgy Ponimatkin, Vincent Lepetit, and Tomas Hodan. CNOS: Strong Baseline for CAD-based Novel Object Segmentation. In ICCVW, 2023. 2, 3, 6, 7, 8 [33] Van Nguyen Nguyen, Thibault Groueix, Mathieu Salzmann, and Vincent Lepetit. Gigapose: Fast and robust novel object pose estimation via one correspondence. In CVPR, 2024. 6, 7, 8 [34] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 7, [35] Evin Pınar Ornek, Yann Labbe, Bugra Tekin, Lingni Ma, Cem Keskin, Christian Forster, and Tomas Hodan. Foundpose: Unseen In European object pose estimation with foundation features. Conference on Computer Vision, 2024. 6 [36] Fabio Poiesi and Davide Boscaini. Learning general and distinctive 3d local deep descriptors for point cloud registration. PAMI, 2022. 7 [37] Colin Rennie, Rahul Shome, Kostas Bekris, and Alberto De Souza. dataset for improved RGBD-based object detection and pose estimation for warehouse pick-and-place. RA-L, 2016. 5 [38] Martin Sundermeyer, Tomas Hodan, Yann Labbe, Gu Wang, Eric Brachmann, Bertram Drost, Carsten Rother, and Jiri Matas. BOP challenge 2022 on detection, segmentation and pose estimation of specific rigid objects. CVPRW, 2023. 2 [39] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In ECCV, 2020. 7 [40] Alykhan Tejani, Danhang Tang, Rigas Kouskouridas, and Tae-Kyun Kim. Latent-class hough forests for 3D object detection and pose estimation. ECCV, 2014. 5 [41] Stephen Tyree, Jonathan Tremblay, Thang To, Jia Cheng, Terry Mosier, Jeffrey Smith, and Stan Birchfield. 6-DoF pose estimation of household objects for robotic manipulation: An accessible dataset and benchmark. IROS, 2022. 1, 2, 4, [42] Philippe Weinzaepfel, Thomas Lucas, Vincent Leroy, Yohann Cabon, Vaibhav Arora, Romain Bregier, Gabriela Csurka, Leonid Antsfeld, Boris Chidlovskii, and Jerˆome Revaud. Croco v2: Improved cross-view completion pre-training for stereo matching and optical flow. arXiv preprint arXiv:2211.10408, 2022. 7 [43] Bowen Wen, Jonathan Tremblay, Valts Blukis, Stephen Tyree, Thomas Muller, Alex Evans, Dieter Fox, Jan Kautz, and Stan Birchfield. BundleSDF: Neural 6-DoF tracking and 3D reconstruction of unknown objects. In CVPR, 2023. 4 [44] Bowen Wen, Wei Yang, Jan Kautz, and Stan Birchfield. Foundationpose: Unified 6d pose estimation and tracking of novel objects. In CVPR, 2024. 6, 7, 8 [45] Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, and Dieter Fox. PoseCNN: convolutional neural network for 6D object pose estimation in cluttered scenes. RSS, 2018. 5 [46] Ruida Zhang, Ziqin Huang, Gu Wang, Xingyu Liu, Chenyangguang Zhang, and Xiangyang Ji. GPose2023, submission to the BOP Challenge 2023. Unpublished, 2023. http://bop.felk.cvut.cz/method_info/410/."
        }
    ],
    "affiliations": [
        "CTU Prague",
        "ENPC",
        "Google",
        "Heidelberg University",
        "KAIST",
        "MVTec",
        "Meta",
        "NAVER LABS",
        "NVIDIA",
        "Niantic",
        "TU Dortmund",
        "TU Munich",
        "University of Toronto"
    ]
}