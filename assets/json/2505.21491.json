{
    "paper_title": "Frame In-N-Out: Unbounded Controllable Image-to-Video Generation",
    "authors": [
        "Boyang Wang",
        "Xuweiyi Chen",
        "Matheus Gadelha",
        "Zezhou Cheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Controllability, temporal coherence, and detail synthesis remain the most critical challenges in video generation. In this paper, we focus on a commonly used yet underexplored cinematic technique known as Frame In and Frame Out. Specifically, starting from image-to-video generation, users can control the objects in the image to naturally leave the scene or provide breaking new identity references to enter the scene, guided by user-specified motion trajectory. To support this task, we introduce a new dataset curated semi-automatically, a comprehensive evaluation protocol targeting this setting, and an efficient identity-preserving motion-controllable video Diffusion Transformer architecture. Our evaluation shows that our proposed approach significantly outperforms existing baselines."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 1 9 4 1 2 . 5 0 5 2 : r Frame In-N-Out: Unbounded Controllable Image-to-Video Generation Boyang Wang1 Xuweiyi Chen1 Matheus Gadelha2 Zezhou Cheng1 1University of Virginia 2Adobe Research Project Page: https://uva-computer-vision-lab.github.io/Frame-In-N-Out/ Figure 1: Frame In-N-Out presents new task in the image-to-video generation that extends the first frame into an unbounded canvas region, where the model could be conditioned on identity reference with motion trajectory control to achieve Frame In and Frame Out cinematic technique."
        },
        {
            "title": "Abstract",
            "content": "Controllability, temporal coherence, and detail synthesis remain the most critical challenges in video generation. In this paper, we focus on commonly used yet underexplored cinematic technique known as Frame In and Frame Out. Specifically, starting from image-to-video generation, users can control the objects in the image to naturally leave the scene or provide breaking new identity references to enter the scene, guided by user-specified motion trajectory. To support this task, we introduce new dataset curated semi-automatically, comprehensive evaluation protocol targeting this setting, and an efficient identity-preserving motion-controllable video Diffusion Transformer architecture. Our evaluation shows that our proposed approach significantly outperforms existing baselines."
        },
        {
            "title": "Introduction",
            "content": "While watching movie, the frame presented to the viewer only shows deliberately chosen portion of the scene. The directors imagination extends far beyond what is shown on screen. Crucial plot twists Preprint. Under review. arise outside the frame that the viewer can observe. For instance, director may decide to introduce new character into the scene to enhance dramatic tension, or conversely, ask character to exit the frame for subsequent plot progression. As modern video generation advances toward producing more controllable and high-fidelity content, natural question arises: can we enable video generation to capture wider and more imaginative world that is not confined by the spatial boundaries of the initial frame? In this paper, we are targeting achieving such milestone in controllable video generation by making real-world cinematic techniques of Frame In and Frame Out (Frame In-N-Out) come true. We formalize Frame In and Frame Out as an image-to-video generation setting. Starting from the first frame image, our goal is to create model that, given an explicit motion trajectory, can (i) control an existing object in the frame completely outside the visible bounds of the first frame and subsequently bring it back while preserving fidelity and integrity, which is defined as Frame Out, and (ii) allow new identity (ID) object (e.g., humans, vehicles, animals) to enter the scene plausiblywhether from the sides or from above, which is defined as Frame In. Contemporary motion-controlled image-to-video generation architecture design like [82, 15, 62, 87, 78, 45, 53] needs spatiotemporal pixel-aligned trajectory signal to the first frame, treating the image border as an immovable wall, which we can see from Fig. 4. To transcend this border-bounded constraint, we extend the conditioning control beyond the first frame region size to an unbounded canvas region. Conditioning signals are applied over this enlarged canvas, enabling the object to move out of the first frame region or using the canvas region to prepare the entrance of new identity (ID) reference while maintaining temporal and spatial coherence. The overall design of the Frame In-N-Out framework is under study. key challenge lies in the absence of existing training datasets that explicitly capture Frame In and Frame Out dynamics. To address this, we redesign the data curation pipeline from scratch. This includes identifying trackworthy objects, improving tracking reliability, and defining suitable bounding boxes that partition the first frame from the extended canvas region. new video Diffusion Transformer [77, 41] is needed to integrate multiple conditions that are either spatiotemporal pixel-aligned [33, 78, 68] (e.g., motion), unaligned [34, 13] (e.g., identity), and more importantly, unbounded canvas. As Frame In-N-Out represents new task in the video generation domain, we meticulously curated dedicated evaluation system. This includes constructing benchmark datasets for both Frame In and Frame Out scenarios and revising traditional tracking, and identity-preserving metrics to reflect the unique demands of this problem setting. We believe this formulation can inspire future research into more expressive and application-driven conditioning research in video generation. In summary, this work makes the following contributions: To the best of our knowledge, this is the first attempt to explore Frame In and Frame Out patterns in video generation. We define and curate training dataset for the Frame In and Frame Out pattern recognition. The processed metadata will be released. We propose an efficient controllable video Diffusion Transformer that unifies spatiotemporal pixelaligned motion, unaligned identity reference, and our proposed unbounded canvas conditions at the minimum cost but with stable generative results. We provide an evaluation system with testing dataset and adjusted traditional metrics for the Frame In and Frame Out scenarios. We believe our insight and area focus has broad prospects in scenarios such as the film industry or advertising production."
        },
        {
            "title": "2 Related Works",
            "content": "Base Condition in Video Generation. While numerous conditioning strategies have been proposed for video generation, some base conditioning exerts fundamental influence on the eventual generation quality and choice of the base model. Broadly, these can be categorized into three primary paradigms: Text-to-Video [5, 18, 9, 57], Image-to-Video [7, 77, 57], and Videoto-Video [32, 76, 38, 54, 3]. Vanilla text-to-video tasks involve generation from sparse conditioning signals, where no pixel-level guidance is needed. The model must rely entirely on language prompts to imagine and synthesize all visual content, including scene layout, motion, and object appearance. In contrast, Image-to-Video assumes the presence of single reference frame as the first frame, from which the entire video must follow. This requires all subsequent frames to remain aligned with the initial spatial content even if additional conditions like text prompts are provided. 2 Table 1: Conditioning comparisons to existing controllable video generation works. Methods CogVideoX [77] MotionCtrl [68] DragAnything [70] Image Conductor [33] ToRA [87] ConsisID [80] SkyReals-A2 [13] Phantom [36] Ours Text First Frame Identity Motion Unbounded Canvas Controllable Video Generation. Controllable video generation refers to the task of extending pretrained video generation models by modifying their architecture to incorporate one or more additional conditions beyond the original text, image, or video inputs. Since image diffusion models, researchers have explored wide range of conditioning signals. This including sketches [72], human pose [28], low-quality images [69] for restoration, masked images for inpainting [25, 91], outpainting [85, 10], and editing [79, 24]. In the video generation domain, temporal-oriented challenges include interpolation between the first and last frame [23, 58], motion control [68, 73, 75, 78, 89, 67, 62], camera control [71, 68, 17, 67, 3, 73], and long-range history-guided generation [46, 81]. Further, as flexible condition, identity (ID) reference is also broadly studied in both image and video side, like PhotoMaker [34], InstantID [66], ConsisID [80], and Concat-ID [90]. Building upon this foundation, there has been growing trend toward elements-to-video generation, where not only identity reference images but also the first frame can serve as an individual compositional element, like Phantom [36], and SkyReels-A2 [13]. The conditioning comparisons can be found in Tab. 1."
        },
        {
            "title": "3 Problem Definition",
            "content": "This paper focuses on solving unbounded controllable image-to-video generation. Specifically, we concrete the problem into specific task in the cinematic domain: Frame In and Frame Out. Our controllable video generation targets at the intersection of four control signals: (1) first frame image I0 and text prompt as the foundation condition, (2) canvas area expansion bounding box setting Bcanvas that is composed of top-left and bottom-right pixels enlarge setting, (3) motion trajectory ctrajs for an existing object in the first frame or new identity (ID) to introduce, and (4) an optional identity reference image (e.g., human, vehicle, animal, balloon, etc.). In the Frame Out case, we dont need an ID reference provided, but it is mandatory in the Frame In case. Eventually, our video Diffusion Model generates number of latent frames that strictly follow all conditions, aiming to learn conditional joint distribution pθ(I1, ..., IN I0, y, Bcanvas, ctrajs, )."
        },
        {
            "title": "4 Dataset Curation and In-N-Out Pattern Recognition",
            "content": "Data curation and pattern recognition play pivotal role in achieving the Frame In and Frame Out intention we want. Our curation starts with raw videos without utilizing metadata provided by the original dataset. The target is to provide an explicit, high-quality identity (ID) reference image, clear and accurate motion trajectory, and bounding box to partition the canvas and the first frame region. Hence, we modify the traditional curation in image-to-video generation and our curation logic is composed of the following four parts (also shown in Fig. 2). Specific hyperparameters and more setting details are in the supplementary. Basic Curation. Our basic curation consists of the following steps. (1) Metadata filtering: we first selected videos based on the metadata attributes such as duration, resolution, and aspect ratio. (2) Image-level filtering: for each video, we randomly sample two frames and filter out low-quality videos using automated image quality assessment [49] and aesthetic assessment [50]. We additionally apply image complexity assessment [14] to exclude both overly simplistic and excessively complex videos, which are known to hinder learning [60]. Videos with excessive overlaid text are also filtered using an OCR detector [2]. (3) Video-level filtering: we remove multi-scene videos using scene cut detection from TransNet V2 [47], and discard videos with significant camera motion (e.g., rotation, translation, or focal changes) based on motion estimation from CUT3R [65], focusing on object-centric motion with least impact from camera movement. (4) Automatic Captioning: to obtain 3 Figure 2: Data Curation Pipeline. Our curation pipeline will provide high-quality filtered videos, text prompts, tracking trajectories with semantic labels, and bounding boxes that can be ideal partitions between the first frame and canvas region. high-quality paired text prompts, we discard dataset-provided captions and generate new ones by QWen2.5-32B-VL-Instruct [74]. Identity of Interest. Random point-based tracking, like optical flow [68, 87], does not provide semantic meaning to each point; i.e. each point cannot strictly correspond to an identity. Thus, before applying the tracking model, we apply panoptic segmentation [30] with OneFormer [22] to classify and then segment all objects in the image. We observe that videos with ideal Frame In and Frame Out patterns usually come with multiple relevant start frames across video. Thus, we select 3 starter frames at the duration 0%, 35%, and 70% of the full video length. This strategy alleviates the dataset scarcity in the later stage. To get the clearest and the least compressed frames available, we apply I-Frame extraction from traditional video compression [44, 60] and choose the closest I-Frame as the official starter frame. These 3 starter frames will be taken as the first frame for image-to-video generation and also execute the panoptic segmentation. Panoptic Segmentation from OneFormer will classify 133 classes based on the COCO dataset [35]. We manually define 22 classes of them as motionable objects that could be objects of interest in the following tracking annotation. Our purpose is to filter out static objects, like trees, houses, and sightseeing, which are not ideal as tracking target. Meanwhile, based on the size of segmentation masks, we filter both small and over-sized identity objects. Further, inspired by [62], we apply K-means to get an even distribution of points from the segmentation mask. Cycle Tracking: With objects of interest and even distributed points, we apply tracking from CoTracker3 [27]. However, tracking can be unstable and inaccurate, especially on fast-moving objects. To the best of our capability to provide the most accurate tracking trajectories without human correction, we take advantage of back-tracking functionality from CoTracker3. After the regular forward track, we back-track from the end position of points in the last frame to the first frame. If the error between the initial position and the back-tracked position is larger than preset threshold, we filter out those points. Less but accurate points are more helpful for motion-controllable video generation training. We perform dense tracking for 24 FPS to ensure the highest accuracy though we might only need 12 FPS of information in the training. In the end, we sort and filter small and extremely high-motion cases to avoid static objects and over-fast movements. Frame In and Out Pattern: Given well-defined object of interest along with its trajectory information, we aim to search for bounding boxes that partition the video frame into two regions: an in-box region, as the first frame in training, and an out-of-box canvas region, which serves as the creative area for ID and unbounded motion. We adopt regression-based strategy by randomly generating thousands of bounding boxes with varying sizes and using tracking information to identify Frame In and Frame Out patterns. To ensure sufficient diversity in the training and considering mobile screen aspect ratios, we sample boxes with various aspect ratios ranging from 16:9 to 4:5. To prevent over-small cases, each bounding box is constrained to have height of at least 50% of the full canvas. 4 Figure 3: Main Architecture. Our video Diffusion Transformer embraces the first frame with canvas expansion, motion, identity reference, and text prompt as the conditions for video generation. Frame Out cases are instances where the object is initially partially or completely located inside the bounding box and subsequently moves entirely outside the bounding box in at least one frame of the video. Re-entry into the box is allowed for robust training and diverse user cases. In contrast, Frame In cases require the object to be completely outside the bounding box in the first frame, with no pixel overlap. This ensures that the ID reference has no overlap with the first frame in the training and we could condition breaking new information to the image-to-video generation. Next, in the following frames, sufficient fraction of the object must come to the in-box region to be considered as qualified Frame In scenario. For valid Frame In cases, we employ SAM2 [43] to extract the object mask and store the corresponding cropped ID reference image. The SAM2 mask is also used to further filter out inaccurate tracking points."
        },
        {
            "title": "5 Frame In-N-Out Architecture",
            "content": "5.1 Base Architecture with Flexible Resolution Training The base video Diffusion Transformer architecture we consider is CogVideoX-I2V [77], relatively small-scale model (5 billion parameters) compared to tens of billions of parameter models [58, 31, 37, 16]. We believe that our contribution is scalable to most video Diffusion Transformer considering the similarity in the architecture [41]. The first frame I0 is conditioned by doing channel-wise concatenation with the noisy latent before entering the transformer blocks: = Concatc(Z, E(I0)), (1) where Concatc(, , ) denotes concatenation along the channel dimension, and is the VAE encoder. I0 is filled in with zero padding on the frame dimension as placeholder for frame-wise alignment. Training in default resolution of 480720 is computation-heavy. Hence, we take advantage of the nature of the absolute position embedding and rotary position embedding [48] (RoPE) by training in lower resolutions 384480 and then testing on versatile resolutions that the user uses. To do so, inspired by 1, we first modify the absolute position embedding to support versatile resolution inputs by applying trilinear interpolation on the learned fixed absolute embeddings to the target latent size. For RoPE, we inject the current resolution grid size to create new position embedding each time. 5.2 Motion Control We first convert all spatiotemporal trajectory coordinates into pixel marking in image forms ctrajs. Since we use panoptic segmentation in the dataset curation, we have rich and accurate semantic meaning for each tracking point. Hence, we provide different objects with different color markings to promote the model learning semantic relationship. For the same object with multiple tracking points, they share the same color. Our trajectory point represents the spatiotemporal pixel-aligned state of the 1https://github.com/aigc-apps/VideoX-Fun 5 object which is different from optical flow-based motion vector representation as in previous motion control works [68, 59, 87]. The motion conditioning has various solutions in the literature. ControlNet-like [82, 15, 62], crossattention based [87, 78], or extra training tokens [26] methods are all computational expensive for motion conditioning. For pixel-aligned conditions, we prefer to apply direct, efficient, and natural solution, which is by channel-wise concatenating VAE-encoded motion images. To be specific, we encode motion images like regular RGB images by the pre-trained 3D VAE encoder E; thus, the latent size of the motion is perfectly aligned with the first frame latent and noisy latent Z. Then, we do channel-wise concatenation to the motion latent as the following: = Concatc(Z, E(I0), E(ctrajs)). (2) However, channel-wise concatenation increases the input channel number for the first projector of the Diffusion Transformer. Hereby, inspired by Marigold [29], we first zero-initialized the projector with the new input channel number setting and then filled in the overlap weights with the pre-trained weights available to decrease the training gap. 5.3 Unbounded Conditioning Starting with the first frame condition I0, we want the pixel-aligned motion intention to get over the border constraints. Hereby, we first extrapolate the first frame region to larger area called the unbounded canvas region as shown in Fig. 3, which is defined by the provided top-left and bottom-right expansion pixels quantity Bcanvas. We define the expansion transformation from the original size of the first frame region to the canvas region coverage as τcanvas(). The first frame is transformed τcanvas(I0) by zero-padded. Then, we adjust the absolute and relative position encoding system by setting the top-left of the canvas region as the (0,0,0) index for temporal, horizontal, and vertical directions. Under this setting, our motion control signal can be expanded to any area inside the canvas region, which is also defined as ctrajs. Full Field Loss. We initially hypothesized that the generative objective should be aligned solely with the viewer-visible region, which is the first frame region, rather than the full canvas region. Accordingly, target latent is zero-padded for the region outside the first frame. However, this formulation failed to yield stable results. Motivated by the experimental observation and outpainting task, we reformulated the objective to be the full field of the canvas denoted as τcanvas(Z). The complete formula can be expressed as: = Concatc(τcanvas(Z), E(τcanvas(I0)), E(ctrajs)). (3) In the training, to accommodate faster training [64] without utilizing an attention mask for different resolutions between batches [11], we resize all videos to the same canvas resolution 384480 and the original first frame region is versatile based on the dataset curation from Sec. 4. Thanks to the flexible nature of the Diffusion Transformer with absolute and relative position embedding, we can set the canvas size arbitrarily in the inference with desirable results even if we train with fixed size. 5.4 Unifying Identity Reference Conditions Our model leverages the causal 3D VAE nature in modern video Diffusion Transformer [77], where not only sets of frames can be compressed, but one single image can be encoded. Therefore, we use the same pre-trained VAE to encode the Identity (ID) reference . Inspired by Concat-ID [90] and recent progress in text-to-image generation with ID reference [51, 86], we resize and scale the ID reference to the same resolution as the canvas size Bcanvas and then do frame-wise concatenation between the latent ID reference and the video frames. The ID reference will be added with random augment noise before VAE encode. To align the channel number, we add zero padding on the corresponding first frame and motion condition channels. The formula can be expressed as: FID = Concatc(E(f + n), , ), FV ideo = Concatc(τcanvas(Z), E(τcanvas(I0)), E(ctrajs)), = Concatf {FV ideo, FID} , (4) (5) (6) where Concatf {, , } denotes frame-wise concatenation. 6 This method takes advantage of the 3D full attention nature of the video Diffusion Transformer. The text tokens, video tokens, and ID reference tokens will be token-wise concatenated after the patchification procedure and then jointly optimized together. Further, by reusing all well-trained normalization, projection, and feedforward modules, the training becomes more stable and implementation becomes more elegant. Though OmniControl [51] and EasyControl [86] might apply shifted offset for the position encoding, this method does not provide similar data distribution for the learned fixed position encoding on the ID reference part in the model like CogVideoX [77]. We empirically find that directly copying the position encoding of the first frame to the ID reference frame leads to better numerical results. 5.5 Training Our training is composed of two stages. In the first stage, we include motion control based on the Eq. 2 with the text prompt captioned from [74] to learn the fundamental conditioning and adapt to the absolute position encoding modification we have done. Our loss in this stage can be formulated as: = Ez,ϵN (0,I),t,c (cid:2)ϵ ϵθ(zt, y, I0, ctrajs, t)2 2 (cid:3) , (7) where is the timestep, and zt is the noisy latent at timestep t. During the inference, pure noise zT is gradually denoised from timestep until timestep 0 to clean latent z0. Then, it will be decoded by the pre-trained VAE decoder to convert back to pixel space. In the second stage, we jointly train Frame In and Frame Out with unbounded canvas region setting together based on Eq. 6. We consider at most one ID reference each time. If it is Frame Out only case, we insert monocular white color placeholder on the ID reference position in Eq. 6. The loss in this stage is: = Ez,ϵN (0,I),t,c (cid:2)ϵ ϵθ(τcanvas(zt), y, τcanvas(I0), ctrajs, , t)2 (cid:3) , (8) We observe that perfect Frame Out cases with complete move-out are rare. For Frame In, it is even harder to find cases in which ID completely has no overlap with the first frame region. To solve data scarcity for generalized and robust training, we lower the standard in the training dataset curation, where we do not require the object to be completely out or inside the first frame region. We believe that the hardest training objective is learning new ID reference signals with motion control in the original video Diffusion Transformer. The overall model structure can be found in Fig. 3. The inference pipeline can be found in the supplementary."
        },
        {
            "title": "6 Experiment",
            "content": "6.1 Implementaion Details The training dataset we consider includes OpenVid-1M [39], VidGen-1M [52], and subset of Webvid10M [4]. The specific data and filtering statistics can be found in the appendix. We use the reserved subset of the OpenVid-1M dataset as our evaluation test set for Frame In-N-Out curation. Our training is on total batch size of 8 for 36K iterations in each stage respectively. The training resolution, also the canvas resolution, is 384480 for two stages. All the video is curated, processed, and fetched in 12 FPS standards. We apply the learning rate warmup for each stage of training in the first 400 steps. The learning rate is 2e-5. We apply 8Bit Adam [12] to save the GPU memory. Our inference step is 50 with classifier-free guidance [20]. The first frame and text dropout ratio is 5% each to augment the classifier-free guidance in the inference. To make the motion pattern in the dataset stronger, we randomly doubled the duration fetched to simulate speed-up with probability of 20% and 33% respectively for each stage. We randomly drop ID reference with probability of 15% in the stage2 training, where we only have Frame Out to consider. 6.2 Proposed Evaluation Dataset Overview Since we are the first work focusing on the In-N-Out pattern in video generation, we define evaluation datasets and metrics as follows. Our evaluation is composed of two parts: Frame Out and Frame In with identity (ID) reference. Though we dont require perfect Frame In and Frame Out patterns in our training scenarios, for the expression of fair intention, we make the setting the hardest level in the evaluation test set. In this way, we curate 183 and 189 cases for perfect FrameIn and FrameOut as Table 2: Frame Out Comparison with Motion Controllable Models. The best is highlighted. Method DragAnything [70] Image Conductor [33] ToRA [87] Ours (Stage1 Motion TI2V) Ours (Stage2) FID 48.73 99.29 57.83 38.36 32.02 FVD LPIPS Traj Err. VSeg MAE VLM 607.44 1154.86 566.78 478.96 318.38 0.462 0.528 0.362 0.358 0.268 41.24 42.72 40.72 48.46 17.85 0.0480 0.0552 0.0750 0.0572 0. 0.624 0.544 0.603 0.685 0.735 evaluation datasets. All Frame In evaluation datasets will come with one and only one ID reference image. The benchmark will be released for future study. 6.3 Evaluation Metrics We evaluate the generative quality of video models using three widely adopted metrics: Fréchet Inception Distance [19] (FID), Fréchet Video Distance [55] (FVD), and Learned Perceptual Image Patch Similarity [83] (LPIPS). Beyond these generative metrics, we modify traditional tracking, segmentation, LLM evaluation, and identity-preserving metrics to fit the In-N-Out pattern. Trajectory Error (Traj. Err.) evaluates the Euclidean distance of all trajectory points between the GT and the generated videos estimated by the Co-Tracker3 [27]. Different from trajectory error metrics proposed in [68, 70], our In-N-Out scenario considers the full canvas region for both GT and generated videos. Since the baseline method cannot generate pixels out of the first frame, they will be zero-padded to the same resolution as the GT, which refers to the canvas size. Lower scores indicate more aligned motion controllability. This metric is intended to leverage the low-level accuracy of the tracking when the object leaves and re-enters the scene. Video Segmentation Mean Absolute Error (VSeg MAE) can be formulated as: VSeg MAE = 1 (cid:88) (cid:88) (cid:88) i= j=1 j=k Mgen(i, j, k) Mgt(i, j, k) , (9) where , H, and refer to the frame number, video height, and width, and Mgen and Mgt refer to the segmentation area of the generated video and the Ground-Truth video estimated by SAM2 [43]. Generated videos without expansion capability will also be zero-padded. This metric is intended to calculate the accuracy of Frame In and Frame Out from high-level semantic perspective. Vision Language Model evaluation (VLM) utilizes modern large vision language model, Qwen 2.5 VL 32B [74], with full video sequences and an instruction prompt to justify if the object gets out of the first frame or enter the first frame. We evaluate the ratio of correctness compared to the returns with GT video inputs. This metric is intended to align the overall subjective success rate analysis, and the higher the better. Relative DINO (Rel. DINO) inherits the traditional DINOV2 [40] from VBench [21, 88] by calculating the cosine similarity between the ID reference and each video frame. However, we found that in our Frame In-N-Out setting, there exist multiple frames where the ID reference does not appear in the video frames at all, which leads to very low similarity score. Thus, we first calculate the average DINO similarity score with each frame and then focus on the absolute relative difference to the Ground-Truth DINO results: (cid:12) (cid:12) (cid:12) t=1dID dGT (cid:12) (cid:12) (cid:12) (cid:80)F (cid:80)F 1 t Relative DINO = , (10) t=1dID dGEN 1 1 t=1dID dGT (cid:80)F where is the dot product operation for calculating cosine similarity. The lower score the better. 6.4 Frame Out Comparisons We consider SOTA motion controllable image-to-video (I2V) model, including DragAnything [70], Image Conductor [33], and ToRA [87]. For these baselines, their architecture does not support conditions of motion trajectory points outside the first frame; thus, we implement these points not appearing in the conditioning motion images. By default, all motions only apply one point and 8 Table 3: Frame In Comparison with Elements-to-Video Models. Tracking and Segmentation for the elements-to-video models are omitted because of the failure to identify the objects position in the generated videos. The best is highlighted. Method FID FVD LPIPS Traj. Err. VSeg MAE Rel. DINO VLM SkyReals-A2 [13] + Motion Description Prompt Phantom [36] + Motion Description Prompt Ours (Stage2) 74.00 61.20 69.55 72.84 30.84 655.25 550.26 742.15 671.05 227.30 0.604 0.564 0.571 0.596 0.218 10.37 0.0112 3.37 2.01 1.70 1.39 1. 0.448 0.535 0.415 0.540 0.863 Figure 4: Qualitative comparison on our benchmark dataset. In (a), we compare our model on Frame Out cases against DragAnything [70] and ToRA [87]. Both baselines fail to fully move the person outside the image boundaries, while our model successfully handles complete exit. In (b), we evaluate Frame In scenarios against Phantom [36] and SkyReals-A2 [13]. Only our model can reach Frame In effect with the designated identity. one object for fair comparison. Further, we include our stage 1 motion-guided image-to-video generation results. Tab. 2 reports results across multiple metrics. Our Stage 1 model already outperforms prior methods in terms of perceptual quality (LPIPS), temporal consistency (FVD), and LLM automatic evaluations (VLM). With Stage 2 refinement, our method achieves the best results across all metrics, reducing trajectory error by over 50% compared to Drag Anything and halving the VSeg MAE. These results demonstrate the effectiveness of our Frame Out architecture advantages and also highlight the effectiveness of two-stage training. 6.5 Frame In Comparisons We define the task as conditioning generation that requires an identity (ID) reference image with the first frame and motion trajectory. This task is not well-defined before this paper. Thus, there does not exist an appropriate strong baseline in the literature. Based on the conditioning we need, we found that the recent rising elements-to-video generation (E2V) is the closest fit, which can take conditions of ID reference and the first frame. While some other text-to-video generation works like Direct-A-Video [75] have motion control with ID reference, these works are not conditioned on the first frame, so we do not believe that they are good fit to express unbounded controllable video generation concept we want to present in this paper. For the E2V, we consider Phantom [36] and SkyReels-A2 [13], which is based on Wan2.1 [58] Diffusion Transformer. Further, for fair comparison, we re-generate more motion descriptive text prompt by LLM [74] to compensate that E2V models cannot take in motion conditioning. As shown in Tab. 3, our method significantly outperforms prior elements-to-video models across all key metrics, including FID, FVD, LPIPS, Traj. Err., VSeg MAE, and VLM accuracy, demonstrating superior visual quality and precise motion controllability in the Frame In setting. While our Rel. DINO score is slightly less (-0.23) than Phantom with motion prompts, this is primarily due to our 9 model faithfully following motion guidance that occasionally moves the identity outside the canvas, affecting frame-wise similarity."
        },
        {
            "title": "7 Conclusion",
            "content": "In the paper, we have presented Frame In-N-Out, new paradigm in image-to-video generation that gets over the border-boundary constraints from the first frame. We leverage text, motion, identity reference, and this new unbounded canvas concept to promote video generation to be more controllable and aligned with real-world applications. Our experiments demonstrate that our generated videos align with the condition intention we introduce and can foresee border impact."
        },
        {
            "title": "8 Acknowledgement",
            "content": "The authors gratefully acknowledge Research Computing at the University of Virginia for providing computational resources and technical support that made the results of this work possible. Zezhou Cheng acknowledges the Adobe Research Gift, as well as GPU support provided by the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program and the National Artificial Intelligence Research Resource (NAIRR) Pilot program. The bird video frames used in Figure 1 and Figure 3 were adapted from the YouTube video Videos for Cats to Watch by Paul Dinning, and are used under fair use for academic and non-commercial purposes."
        },
        {
            "title": "References",
            "content": "[1] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. [2] Youngmin Baek, Bado Lee, Dongyoon Han, Sangdoo Yun, and Hwalsuk Lee. Character region awareness for text detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 93659374, 2019. [3] Jianhong Bai, Menghan Xia, Xiao Fu, Xintao Wang, Lianrui Mu, Jinwen Cao, Zuozhu Liu, Haoji Hu, Xiang Bai, Pengfei Wan, et al. Recammaster: Camera-controlled generative rendering from single video. arXiv preprint arXiv:2503.11647, 2025. [4] Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF international conference on computer vision, pages 17281738, 2021. [5] Fan Bao, Chendong Xiang, Gang Yue, Guande He, Hongzhou Zhu, Kaiwen Zheng, Min Zhao, Shilong Liu, Yaole Wang, and Jun Zhu. Vidu: highly consistent, dynamic and skilled text-to-video generator with diffusion models. arXiv preprint arXiv:2405.04233, 2024. [6] Yuxuan Bian, Zhaoyang Zhang, Xuan Ju, Mingdeng Cao, Liangbin Xie, Ying Shan, and Qiang Xu. Videopainter: Any-length video inpainting and editing with plug-and-play context control. arXiv preprint arXiv:2503.05639, 2025. [7] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [8] Chaofeng Chen and Jiadi Mo. IQA-PyTorch: Pytorch toolbox for image quality assessment. [Online]. Available: https://github.com/chaofengc/IQA-PyTorch, 2022. [9] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 73107320, 2024. [10] Qihua Chen, Yue Ma, Hongfa Wang, Junkun Yuan, Wenzhe Zhao, Qi Tian, Hongmei Wang, Shaobo Min, Qifeng Chen, and Wei Liu. Follow-your-canvas: Higher-resolution video outpainting with extensive content generation. arXiv preprint arXiv:2409.01055, 2024. [11] Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Patch npack: Navit, vision transformer for any aspect ratio and resolution. Advances in Neural Information Processing Systems, 36:22522274, 2023. 10 [12] Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 8-bit optimizers via block-wise quantization. arXiv preprint arXiv:2110.02861, 2021. [13] Zhengcong Fei, Debang Li, Di Qiu, Jiahua Wang, Yikun Dou, Rui Wang, Jingtao Xu, Mingyuan Fan, Guibin Chen, Yang Li, et al. Skyreels-a2: Compose anything in video diffusion transformers. arXiv preprint arXiv:2504.02436, 2025. [14] Tinglei Feng, Yingjie Zhai, Jufeng Yang, Jie Liang, Deng-Ping Fan, Jing Zhang, Ling Shao, and Dacheng Tao. Ic9600: benchmark dataset for automatic image complexity assessment. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(7):85778593, 2022. [15] Zekai Gu, Rui Yan, Jiahao Lu, Peng Li, Zhiyang Dou, Chenyang Si, Zhen Dong, Qifeng Liu, Cheng Lin, Ziwei Liu, et al. Diffusion as shader: 3d-aware video diffusion for versatile video generation control. arXiv preprint arXiv:2501.03847, 2025. [16] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, et al. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. [17] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2024. [18] Roberto Henschel, Levon Khachatryan, Hayk Poghosyan, Daniil Hayrapetyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Streamingt2v: Consistent, dynamic, and extendable long video generation from text. arXiv preprint arXiv:2403.14773, 2024. [19] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium, 2018. [20] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. [21] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. [22] Jitesh Jain, Jiachen Li, Mang Tik Chiu, Ali Hassani, Nikita Orlov, and Humphrey Shi. Oneformer: One transformer to rule universal image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 29892998, 2023. [23] Siddhant Jain, Daniel Watson, Eric Tabellion, Ben Poole, Janne Kontkanen, et al. Video interpolation with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 73417351, 2024. [24] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing. arXiv preprint arXiv:2503.07598, 2025. [25] Xuan Ju, Xian Liu, Xintao Wang, Yuxuan Bian, Ying Shan, and Qiang Xu. Brushnet: plug-and-play image inpainting model with decomposed dual-branch diffusion. In European Conference on Computer Vision, pages 150168. Springer, 2024. [26] Xuan Ju, Weicai Ye, Quande Liu, Qiulin Wang, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, and Qiang Xu. Fulldit: Multi-task video generative foundation model with full attention. arXiv preprint arXiv:2503.19907, 2025. [27] Nikita Karaev, Iurii Makarov, Jianyuan Wang, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker3: Simpler and better point tracking by pseudo-labelling real videos. arXiv preprint arXiv:2410.11831, 2024. [28] Johanna Karras, Aleksander Holynski, Ting-Chun Wang, and Ira Kemelmacher-Shlizerman. Dreampose: Fashion image-to-video synthesis via stable diffusion. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 2262322633. IEEE, 2023. [29] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 94929502, 2024. [30] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Dollár. Panoptic segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 94049413, 2019. [31] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 11 [32] Max Ku, Cong Wei, Weiming Ren, Harry Yang, and Wenhu Chen. Anyv2v: tuning-free framework for any video-to-video editing tasks. arXiv preprint arXiv:2403.14468, 2024. [33] Yaowei Li, Xintao Wang, Zhaoyang Zhang, Zhouxia Wang, Ziyang Yuan, Liangbin Xie, Ying Shan, and Yuexian Zou. Image conductor: Precision control for interactive video synthesis. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 50315038, 2025. [34] Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, Ming-Ming Cheng, and Ying Shan. Photomaker: Customizing realistic human photos via stacked id embedding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 86408650, 2024. [35] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13, pages 740755. Springer, 2014. [36] Lijie Liu, Tianxiang Ma, Bingchuan Li, Zhuowei Chen, Jiawei Liu, Gen Li, Siyu Zhou, Qian He, and Xinglong Wu. Phantom: Subject-consistent video generation via cross-modal alignment, 2025. [37] Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Wan, Ranchen Ming, Xiaoniu Song, Xing Chen, et al. Step-video-t2v technical report: The practice, challenges, and future of video foundation model. arXiv preprint arXiv:2502.10248, 2025. [38] Chong Mou, Mingdeng Cao, Xintao Wang, Zhaoyang Zhang, Ying Shan, and Jian Zhang. Revideo: Remake video with motion and content control. Advances in Neural Information Processing Systems, 37:1848118505, 2024. [39] Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, and Ying Tai. Openvid-1m: large-scale high-quality dataset for text-to-video generation. arXiv preprint arXiv:2407.02371, 2024. [40] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. [41] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [42] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, and Alexander Sorkine-Hornung. benchmark dataset and evaluation methodology for video object segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 724732, 2016. [43] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. [44] Heiko Schwarz, Detlev Marpe, and Thomas Wiegand. Overview of the scalable video coding extension of the h. 264/avc standard. IEEE Transactions on circuits and systems for video technology, 17(9):11031120, 2007. [45] Shuwei Shi, Biao Gong, Xi Chen, Dandan Zheng, Shuai Tan, Zizheng Yang, Yuyuan Li, Jingwen He, Kecheng Zheng, Jingdong Chen, et al. Motionstone: Decoupled motion intensity modulation with diffusion transformer for image-to-video generation. arXiv preprint arXiv:2412.05848, 2024. [46] Kiwhan Song, Boyuan Chen, Max Simchowitz, Yilun Du, Russ Tedrake, and Vincent Sitzmann. Historyguided video diffusion. arXiv preprint arXiv:2502.06764, 2025. [47] Tomás Soucek and Jakub Lokoc. Transnet v2: An effective deep network architecture for fast shot transition detection. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 1121811221, 2024. [48] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [49] Shaolin Su, Qingsen Yan, Yu Zhu, Cheng Zhang, Xin Ge, Jinqiu Sun, and Yanning Zhang. Blindly assess image quality in the wild guided by self-adaptive hyper network. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 36673676, 2020. [50] Hossein Talebi and Peyman Milanfar. Nima: Neural image assessment. IEEE transactions on image processing, 27(8):39984011, 2018. [51] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and universal control for diffusion transformer. arXiv preprint arXiv:2411.15098, 2024. [52] Zhiyu Tan, Xiaomeng Yang, Luozheng Qin, and Hao Li. Vidgen-1m: large-scale dataset for text-to-video generation. arXiv preprint arXiv:2408.02629, 2024. 12 [53] Maham Tanveer, Yang Zhou, Simon Niklaus, Ali Mahdavi Amiri, Hao Zhang, Krishna Kumar Singh, and Nanxuan Zhao. Motionbridge: Dynamic video inbetweening with flexible controls. arXiv preprint arXiv:2412.13190, 2024. [54] Yuanpeng Tu, Hao Luo, Xi Chen, Sihui Ji, Xiang Bai, and Hengshuang Zhao. Videoanydoor: High-fidelity video object insertion with precise motion control. arXiv preprint arXiv:2501.01427, 2025. [55] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. [56] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges, 2019. [57] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [58] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [59] Boyang Wang, Nikhil Sridhar, Chao Feng, Mark Van der Merwe, Adam Fishman, Nima Fazeli, and Jeong Joon Park. This&that: Language-gesture controlled video generation for robot planning. arXiv preprint arXiv:2407.05530, 2024. [60] Boyang Wang, Fengyu Yang, Xihang Yu, Chao Zhang, and Hanbin Zhao. Apisr: anime production inspired real-world anime super-resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2557425584, 2024. [61] Fu-Yun Wang, Xiaoshi Wu, Zhaoyang Huang, Xiaoyu Shi, Dazhong Shen, Guanglu Song, Yu Liu, and Hongsheng Li. Be-your-outpainter: Mastering video outpainting through input-specific adaptation. In European Conference on Computer Vision, pages 153168. Springer, 2024. [62] Hanlin Wang, Hao Ouyang, Qiuyu Wang, Wen Wang, Ka Leong Cheng, Qifeng Chen, Yujun Shen, and Limin Wang. Levitor: 3d trajectory oriented image-to-video synthesis. arXiv preprint arXiv:2412.15214, 2024. [63] Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. Exploring clip for assessing the look and feel of images. In Proceedings of the AAAI conference on artificial intelligence, volume 37, pages 25552563, 2023. [64] Luozhou Wang, Yijun Li, Zhifei Chen, Jui-Hsien Wang, Zhifei Zhang, He Zhang, Zhe Lin, and Yingcong Chen. Transpixar: Advancing text-to-video generation with transparency. arXiv preprint arXiv:2501.03006, 2025. [65] Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei Efros, and Angjoo Kanazawa. Continuous 3d perception model with persistent state. arXiv preprint arXiv:2501.12387, 2025. [66] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, Anthony Chen, Huaxia Li, Xu Tang, and Yao Hu. Instantid: Zero-shot identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024. [67] Zhouxia Wang, Yushi Lan, Shangchen Zhou, and Chen Change Loy. Objctrl-2.5 d: Training-free object control with camera poses. arXiv preprint arXiv:2412.07721, 2024. [68] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. [69] Rongyuan Wu, Lingchen Sun, Zhiyuan Ma, and Lei Zhang. One-step effective diffusion network for real-world image super-resolution. Advances in Neural Information Processing Systems, 37:9252992553, 2024. [70] Weijia Wu, Zhuang Li, Yuchao Gu, Rui Zhao, Yefei He, David Junhao Zhang, Mike Zheng Shou, Yan Li, Tingting Gao, and Di Zhang. Draganything: Motion control for anything using entity representation. In European Conference on Computer Vision, pages 331348. Springer, 2024. [71] FU Xiao, Xian Liu, Xintao Wang, Sida Peng, Menghan Xia, Xiaoyu Shi, Ziyang Yuan, Pengfei Wan, Di Zhang, and Dahua Lin. 3dtrajmaster: Mastering 3d trajectory for multi-entity motion in video generation. In The Thirteenth International Conference on Learning Representations, 2024. [72] Jinbo Xing, Hanyuan Liu, Menghan Xia, Yong Zhang, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Tooncrafter: Generative cartoon interpolation. ACM Transactions on Graphics (TOG), 43(6):111, 2024. [73] Jinbo Xing, Long Mai, Cusuh Ham, Jiahui Huang, Aniruddha Mahapatra, Chi-Wing Fu, Tien-Tsin Wong, and Feng Liu. Motioncanvas: Cinematic shot design with controllable image-to-video generation. arXiv preprint arXiv:2502.04299, 2025. [74] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [75] Shiyuan Yang, Liang Hou, Haibin Huang, Chongyang Ma, Pengfei Wan, Di Zhang, Xiaodong Chen, and Jing Liao. Direct-a-video: Customized video generation with user-directed camera movement and object motion. In ACM SIGGRAPH 2024 Conference Papers, pages 112, 2024. [76] Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change Loy. Rerender video: Zero-shot text-guided video-to-video translation. In SIGGRAPH Asia 2023 Conference Papers, pages 111, 2023. [77] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [78] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory. arXiv preprint arXiv:2308.08089, 2023. [79] Xin Yu, Tianyu Wang, Soo Ye Kim, Paul Guerrero, Xi Chen, Qing Liu, Zhe Lin, and Xiaojuan Qi. Objectmover: Generative object movement with video prior. arXiv preprint arXiv:2503.08037, 2025. [80] Shenghai Yuan, Jinfa Huang, Xianyi He, Yunyuan Ge, Yujun Shi, Liuhan Chen, Jiebo Luo, and Li Yuan. Identity-preserving text-to-video generation by frequency decomposition. arXiv preprint arXiv:2411.17440, 2024. [81] Lvmin Zhang and Maneesh Agrawala. Packing input frame context in next-frame prediction models for video generation. arXiv preprint arXiv:2504.12626, 2025. [82] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023. [83] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. [84] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. [85] Shaofeng Zhang, Jinfa Huang, Qiang Zhou, Zhibin Wang, Fan Wang, Jiebo Luo, and Junchi Yan. Continuous-multiple image outpainting in one-step via positional query and diffusion-based approach. arXiv preprint arXiv:2401.15652, 2024. [86] Yuxuan Zhang, Yirui Yuan, Yiren Song, Haofan Wang, and Jiaming Liu. Easycontrol: Adding efficient and flexible control for diffusion transformer. arXiv preprint arXiv:2503.07027, 2025. [87] Zhenghao Zhang, Junchao Liao, Menghao Li, Zuozhuo Dai, Bingxue Qiu, Siyu Zhu, Long Qin, and Weizhi Wang. Tora: Trajectory-oriented diffusion transformer for video generation. arXiv preprint arXiv:2407.21705, 2024. [88] Dian Zheng, Ziqi Huang, Hongbo Liu, Kai Zou, Yinan He, Fan Zhang, Yuanhan Zhang, Jingwen He, Wei-Shi Zheng, Yu Qiao, et al. Vbench-2.0: Advancing video generation benchmark suite for intrinsic faithfulness. arXiv preprint arXiv:2503.21755, 2025. [89] Sixiao Zheng, Zimian Peng, Yanpeng Zhou, Yi Zhu, Hang Xu, Xiangru Huang, and Yanwei Fu. Vidcraft3: Camera, object, and lighting control for image-to-video generation. arXiv preprint arXiv:2502.07531, 2025. [90] Yong Zhong, Zhuoyi Yang, Jiayan Teng, Xiaotao Gu, and Chongxuan Li. Concat-id: Towards universal identity-preserving video synthesis. arXiv preprint arXiv:2503.14151, 2025. [91] Shangchen Zhou, Chongyi Li, Kelvin CK Chan, and Chen Change Loy. Propainter: Improving propagation In Proceedings of the IEEE/CVF international conference on and transformer for video inpainting. computer vision, pages 1047710486, 2023. [92] Shaobin Zhuang, Zhipeng Huang, Binxin Yang, Ying Zhang, Fangyikang Wang, Canmiao Fu, Chong Sun, Zheng-Jun Zha, Chen Li, and Yali Wang. Get in video: Add anything you want to the video. arXiv preprint arXiv:2503.06268, 2025."
        },
        {
            "title": "A Overview",
            "content": "In this supplemental document, we provide additional content that complements the main paper sections. Sec. elaborates on additional details of the dataset curation. Sec. provides additional experimental details, which include more model architecture details and evaluation information. Sec. includes our ablation study. Sec. provides additional visualization for teaser and qualitative model comparison results. Sec. provides an extension to show the generated contents outside the first frame region in our model. Sec. includes discussion of the limitations of our model."
        },
        {
            "title": "B Dataset Curation Details",
            "content": "The detailed filtering statistics can be found in Tab. 4. For faster curation, we switch the order between the camera filter and panoptic segmentation. This is because camera detection by CUT3R [65] spends much more time than image-based panoptic segmentation by Oneformer [22]. Though WebVid [4] has an easy concept for each video, which makes it easier to curate, they have watermark for each video and their resolution is low. Thus, we only consider around 1.5M videos of it to balance the dataset diversity and quality. In the Basic Filter, we consider videos with at least 4 seconds, but not more than 20 seconds, with frames per second (fps) range of [20, 31]. The aspect ratio of width to height must be larger than 1.35, which corresponds to 4:3 widely-used traditional metrics. The minimum pixel width is 400 pixels. In the Image Scoring, we randomly select 2 images from each video and then get the score of these two images by executing multiple image-based assessment models. We sort the score from the smallest to the highest on each metric and filter based on the human subjective perspective for different datasets. Thus, the filtering strength is different between datasets based on their characteristics. Further, we consider the overlap scenarios between different scoring elements. We consider image quality assessment by ClipIQA [63], with the range lowest 3%, 5%, 15% filtered for OpenVid [39], VidGen [52], and WebVid [4] respectively. We consider text detection from an open-source repository EasyOCR 2, which is based on [2]. We sort the area size of the detected text and then filter the highest 15%, 10%, and 5% respectively. We consider aesthetic assessment from NIMA [50] from the codebase of [8]. We filter the lowest 5%, 5%, and 10% respectively. For the image complexity assessment, we use IC9600 [14] to evaluate the complexity score where lower value means less complexity, which lacks effective content to learn, and higher value means more complexity, which has too many features to learn, like dense population or sightseeing. We filter both the lowest 10% and highest 5% for OpenVid, the lowest 5% and highest 10% for VidGen, and the lowest 5% and highest 10% for WebVid. In the Scene Cut, we use TransNet V2 [47], which is based on the comparison results from Cosmos [1]. We filter any video that is detected with more than 1 scene. This ratio is not very high based on our observation. We think this is because the long video cases are already filtered. In the Camera Filter, we use CUT3R [65], state-of-the-art model for 3D reconstruction and camera pose estimation in dynamic video. However, evaluating 3D point positions jointly is computationally expensive, so we adopt the 224-resolution model and estimate camera poses over 10-second window at 6 FPS. For each frame, we extract the predicted rotation, translation, and focal length. To sort videos based on camera motion intensity, we compute score combining translational and rotational errors using the following formula: Score = t1 t22 + cos (cid:18) Tr(R 1 R2) 1 (cid:19) 2 where (t1, R1) and (t2, R2) are camera poses between consecutive frames. Here, we filter the highest 40% of the rotation error, the highest 40% of the translation error, and the highest 10% of the focal length change. Additionally, we find that VidGen-1M exhibits significantly higher translational and rotational errors compared to WebVid and OpenVid, indicating more frequent and abrupt camera motion. Empirically, we observe that such unstable camera motion introduces 2https://github.com/JaidedAI/EasyOCR 15 Table 4: Filtering process statistics across datasets. Each row shows the number of videos retained and the percentage relative to the initial pool at that stage. Stage Initial Basic Filter Image Scoring Scene Cut Panoptic Seg. Camera Filter Motion Filter In-N-Out Filter OpenVid [39] VidGen [52] WebVid [4] Count Left Ratio (%) Count Left Ratio (%) Count Left Ratio (%) 1.00M 537K 295K 280K 155K 102K 86K 29.7K 100.0% 53.7% 29.5% 28.0% 15.5% 10.2% 8.6% 3.0% 1.00M 821K 575K 518K 396K 35K 32.5K 9.2K 100.0% 82.1% 57.5% 51.8% 39.6% 3.5% 3.3% 0.9% 1.50M 1.276M 781K 483K 218K 102K 82K 33.4K 100.0% 85.1% 52.1% 32.2% 14.5% 6.8% 5.5% 2.2% ambiguity during training and degrades the success rate on the Frame In and the Frame Out intention we want. Therefore, we apply harsher filtering to the VidGen-1M subset. In the Panoptic Segmentation, we consider 22 objects of COCO dataset [35] detected by OneFormer [22] as the identity of interest, which includes person, bicycle, car, motorcycle, airplane, bus, train, truck, boat, bird, cat, dog, horse, sheep, cow, elephant, bear, zebra, giraffe, sports ball, kite, and flower. We want the identity to be more diverse than regular human face-oriented identity-preservingto-video [90, 80, 34] (IP2V), but less than elements-to-video domain [13, 36] that consider all genre, either motionable or static objects. The maximum duration of the I-Frame adjustment mentioned in the main paper is only 5% of the full length. If the I-Frame index is farther away from this, we use the original frame index counted. We discard objects that are too small, less than 4% of the area, and too big, which occupy more than 40% area of the image. The number of points sampled from K-means ranges from [12, 36] points based on the aforementioned area range. To avoid dense labeling with the same identity, we only allow at most 3 objects with the same label; otherwise, the video will be flittered. In Video Captioning, there is no filtering. We apply QWen2.5-32B-VL-Instruct [74], which is the SOTA model for video captioning. We sampled only 1 frame per second with resolution of 320448 to save computation resources. The instruction text prompt we use is: Please describe the video in 50 words. Only describe the temporal change of the video provided without describing the spatial information in the first frame provided. Only show the information with the highest confidence. Dont use any words like gesture, gesturing. We apply captioning before Stage 1 motion-guided image-to-video generation training and the text prompt will be filtered with the rest of the procedure. In Motion Filter, we first resize all videos to 384512, which is the resolution used on the CoTracker demo [27]. We sampled our video to 24 FPS but stored the result in 12 FPS, which is our training dataset fetched FPS. The start frame is the frame index from the panoptic segmentation section and the end frame is 49 frames from it, which is also our training duration for each selected video clip. If the cycle tracking errors at the first frame are larger than 4% of the number of pixels in height, this tracking point will be filtered. If more than 33% of points of an object are filtered in this way, the entire object is not considered. In In-N-Out Filter, we consider various aspect ratio of 16:9, 3:2, 4:3, 5:4, 1:1, and 4:5 with probability of 35%, 30%, 20%, 13%, 1%, 1% respectively, where the minimum height is 60%, 60%, 65%, 65%, 75%, and 85% of its original height respectively. The top-left position is randomly generated, and the code base will check if the selected box can fit in the image resolution. If not, this bounding box will be filtered, and consider the next one. We will do this way 2000 times. If there is no ideal bounding box found, this video is filtered. All video clip that starts from the index of panoptic segmentation will be considered. The SAM2 [43] will be utilized after at least one ideal bounding box is found. We use SAM2 to further refine the tracking points outlier cases. Since SAM2 is also expensive, we did not deploy SAM2 with the CoTracker3 [27] at the motion filter stage. Instead, we only use it at the final stage for final improvement. We further discard identity reference images that are less than 10% of the image resolution size based on SAM2 estimation. 16 Table 5: Additional Conditioning comparisons to existing controllable video generation works. Methods ConcatID [90] Direct-A-Video [75] ReVideo [38] VideoAnyDoor [54] Ours Text First Frame Identity Motion Unbounded Canvas Figure 5: Inference Pipeline."
        },
        {
            "title": "C Additional Experimental Details",
            "content": "In the motion control architecture, one-pixel coordinate as motion images form is insensitive for the VAE encoder to effectively encode and decode; thus, similar to previous works [68, 59], we enlarge pixels to square rectangular box and then execute 2D dilation algorithm from deblurring domain to increase the perceptibility. The rectangular box border length is 6 pixels for video with an original height of 384, which means that the size will be scaled with the exact height. Further, to accommodate sparse motion points (1-2 points per object) provided by the user side during the inference, we randomly drop points during the training. The number of tracking points is randomly dropped with the probability of 33% and 60% for each stage respectively. Different from optical flowbased motion control works [68, 59, 87], we do not need padding frame on the last frame to align the total number of frames, where all of our motion conditioning frames represent spatiotemporal pixel-aligned motion status. Our inference pipeline can be found in Fig. 5. The model will start from pure noise with the size of the canvas region. The first frame will be expanded to the canvas size by the Bcanvas setting for the top-left and bottom-right expansion pixel number. The motion is conditioned on the canvas size as whole after converting from the raw pixel spatiotemporal coordinates. Similarly, the ID reference images will be scaled and padded to the same size as the canvas size. The denoised latent will be decoded by the pre-trained VAE decoder and then we cropped the pixel space videos to be the exact same shape as the first frame based on Bcanvas setting. In stage 2 training, we randomly drop ID reference images with probability of 15% in the training, where we only have Frame Out scenarios to consider. The ID reference position will be placeholder with monocular white color. This is also how we get our stage 2 Frame Out results in Tab.2 of the main paper. After token-wise concatenation, the order of tokens should be text tokens, video tokens, and then the ID reference tokens accordingly. Since the resolution and the number of frames that each baseline model can generate are different in the quantitative comparisons. For fair comparison, we resize the validation dataset to the original process resolution of each model. Then, we resize the generated videos to the uniform resolution of 256384 with the number of frames of 14 and 49 respectively for the Frame Out and Frame In table. This number of frames represents the minimum frame number across all models in that table. For the SkyReels-A2 [13], based on their official code implementation, we generate 81 frames and then crop 17 Method Baseline Table 6: Ablation Study on Frame In. FID FVD LPIPS Traj. Err. VSeg MAE Rel. DINO VLM 30.18 283.43 384x480 Inference No Full Filed Loss New Absolute PE 29.73 30.74 36.03 249.34 278.12 299.17 0. 0.223 0.238 0.247 9.54 9.48 48.49 10.21 0.0107 0.0120 0.0497 0.0114 0. 1.10 1.92 0.97 0.868 0.797 0.792 0.836 the first 12 frames since the first 12 frames look highly distorted for an unfair comparison. We only compare the remaining 69 frames. Works like ReVideo [38] and VideoAnyDoor [54] and other similar worsk [92, 6] are conditioned on the full video sequence as inputs, not just the first frame, which means that they are video-to-video generation instead of image-to-video generation we target at. Furthermore, these works are more focused on the insertion and editing in the middle instead of entering and leaving effects we pursue. Thus, we believe that this direction is not an ideal choice for Frame In and Frame Out baselines. An additional condition comparison table can be found in Tab. 5. Base generative metrics evaluation description: FID [19] measures the distance between the distribution of generated and real frame features, extracted using an Inception network. It is sensitive to both image quality and diversity, and lower scores indicate better generation. FVD [56] extends FID to the temporal domain, computing the Fréchet distance between distributions of video-level features extracted using pre-trained I3D model. It captures spatiotemporal coherence in generated videos and is similarly lower-is-better. LPIPS [84] quantifies the perceptual similarity between individual frames and their references using deep feature distances from pre-trained network. Unlike pixel-wise metrics, LPIPS correlates well with human judgment of visual similarity. The prompt we used for motion description for SkyReels-A2 [13] and Phantom [36] is: Please describe the video in 50 words. Describe the motion of the object clearly and in details, but in the natural and direct language. It is expected that an object will enter and/or exit the image. Describe how the character is moved in and exit, like from the top, left, right, bottom."
        },
        {
            "title": "D Ablation Study",
            "content": "As shown in Tab. 6, we compare several different purposes-trained models under different settings. Due to the computation limitations, we train all models with 12K iterations of batch size of 8. The training is done on stage 2 and all utilize the same pre-trained stage 1 weight (including the baseline). We compare all models on the Frame In evaluation, which is the most representative task for all conditions to be considered. Inference Resolution Influence. In the training, we train fixed canvas size of 384x480, but we test at 448x640 as our baseline. We also test at 384x480 to see if the resolution in the inference influences the conclusion. We can see from the second row of Tab. 6 that some metrics are higher and some are lower. Overall, the inference resolution does not provide direct advantage to the final numerical results. Full Field Loss Influence. We also provide model that is not trained with full field loss. This means that the ground-truth target latent provided only has the encoded information from the first frame region instead of the canvas region in the baseline. The region outside the first frame is padded with zero. It is worth noting that the comparison of FID, FVD, LPIPS, Relative DINO, and VLM are only the first frame region, which means that the padded zero is not included in the evaluation. As we can see from the third row of the Tab. 6, almost all metrics dropped. Here, trajectory error, video segmentation MAE, and relative DINO dropped evidently. We believe that the introduction of full field loss is significant to the final visual generated quality and motion consistency. Absolute Position Encoding Influence. In our baseline model, we train the model with resized absolute position encoding by trilinear interpolation. Despite this method, another solution to embrace different resolution inputs for both training and inference could be to create new position encoding 18 each time, where the position encoding for the identity reference is also refreshed each time. However, as shown in the fourth row of Tab. 6, this will lead to performance drop in all metrics, especially FID and relative DINO. We believe that reusing the learned position encoding by applying simple trilinear interpolation is helpful to the versatile video resolution inputs, which maintain similar data distribution from the learned fixed position embeddings at the minimum cost."
        },
        {
            "title": "E Additional Visualization",
            "content": "A more complete sequence of the generated videos and all conditions of the teaser can be found in Fig. 6. Further, we provide more of our generated video samples in Fig. 7. Some demo images or ID reference is chosen or cropped from Davis Dataset [42] and online images. We show multiple different kinds of combinations of Frame In and Frame Out. For the physical interaction, we mean the interaction caused by breaking new ID references to the object that already exists in the first frame. We set all the height and width to be multiplier of 32 due to the VAE limitation. As shown in Fig. 6 and Fig. 7, the setting for the height and width of the canvas and first frame region can be versatile in aspect ratio and size while generating high fidelity and stable results. Further, we empirically found the generation is stable as long as the canvas height is less than 480 pixels and the canvas width is less than 720 pixels, which is the training resolution for our pre-trained base model, CogVideoX [77]. As shown in Fig. 8 and Fig. 9, we also present extra qualitative comparisons with the baseline methods. For the Frame In comparison, we can see that baselines like Phantom [36] and SkyReels-A2 [13] cannot understand the Frame In intention we want. The identity reference either already existed in the first frame (case 2 and 3) or never coming to the scene (case 1). Further, the first frame condition is not faithfully considered as the main condition. The scene they generate is similar in elements but different from objects, 3D position, and physical relationship. On the contrary, ours shows clear alignment to the motion conditioning and reliable faithfulness to the first frame. For the Frame Out comparison, we can see that DragAnything [70] might have exaggerated motion when the motion conditioning outside the first frame region is not provided (case 1). Image Conductor [33] cannot faithfully generate the videos. ToRA [87] does not provide stable Frame Out effect we want."
        },
        {
            "title": "F Full Canvas Generation Extension",
            "content": "The full filed loss implementation makes the generative objective closer to the outpainting model design; however, compared to video outpainting works like [61, 10] which needs full sequence video as inputs and converges to video-to-video generation task, ours only provides the first frame and focuses on the balance of motion identity-preserving conditioning. Thus, we believe that it is appropriate to name our conditioning as unbounded canvas, instead of the vanilla outpainting. In Fig. 10, we provide the visual content outside the first frame region for two teaser image examples. This is the byproduct of our full field loss implementation, which jointly generates the full canvas region compared with the ground-truth latent in the training. We observe that some examples will show unwanted color distortion for the region outside the first frame region. Since our area of interest is always the first frame region, where we cropped with only the first frame region in the inference. We leave this problem to future works."
        },
        {
            "title": "G Limitation",
            "content": "Our method shows promising results, but there are nevertheless some limitations that are worth sharing as shown in Fig. 11. Mainly, this lies in the 3D ambiguity when there is only one point for the motion trajectory. We have to restate that most current works on motion control are single trajectory point-based. One trajectory point for breaking new ID reference information is hard to present the pose ambiguity (see Fig. 11 (c)). Sometimes we want to generate the back view, but we may see the side view cases. Further, one point trajectory is hard to control the size of the ID reference, where sometimes it might be bigger (see Fig. 11 (d) or smaller (see Fig. 11 (e)) than expected. Further, the camera motion lies in the pre-trained model dataset [77] and our filtering method from CUT3R [65] cannot completely remove all videos with camera motion. This leads our model to generate videos with some unwanted camera motion (see Fig. 11 (b)). These issues might be solved by introducing more robust 3D control system, like camera control or size control for the ID reference. 19 Figure 6: Detailed Conditioning for Generated Videos on Teaser Image. 20 Figure 7: More Generated Examples. Figure 8: Extra Frame In Comparisons. 22 Figure 9: Extra Frame Out Comparisons. 23 Figure 10: Full Canvas Generation Extension. We show two examples from the teaser image of how our unbounded canvas generation outside the first frame region. The green bounding box is the final generated video that will be cropped in the inference. Figure 11: Limitations. Given the same input and conditions, our model may produce different outputs under different random seeds. We consider (a) to be an ideal case and illustrate several limitations: camera motion ambiguity in (b), ID reference pose ambiguity in (c), overly large reference objects in (d), and overly small reference objects in (e)."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "University of Virginia"
    ]
}