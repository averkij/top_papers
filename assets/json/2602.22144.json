{
    "paper_title": "NoLan: Mitigating Object Hallucinations in Large Vision-Language Models via Dynamic Suppression of Language Priors",
    "authors": [
        "Lingfeng Ren",
        "Weihao Yu",
        "Runpeng Yu",
        "Xinchao Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Object hallucination is a critical issue in Large Vision-Language Models (LVLMs), where outputs include objects that do not appear in the input image. A natural question arises from this phenomenon: Which component of the LVLM pipeline primarily contributes to object hallucinations? The vision encoder to perceive visual information, or the language decoder to generate text responses? In this work, we strive to answer this question through designing a systematic experiment to analyze the roles of the vision encoder and the language decoder in hallucination generation. Our observations reveal that object hallucinations are predominantly associated with the strong priors from the language decoder. Based on this finding, we propose a simple and training-free framework, No-Language-Hallucination Decoding, NoLan, which refines the output distribution by dynamically suppressing language priors, modulated based on the output distribution difference between multimodal and text-only inputs. Experimental results demonstrate that NoLan effectively reduces object hallucinations across various LVLMs on different tasks. For instance, NoLan achieves substantial improvements on POPE, enhancing the accuracy of LLaVA-1.5 7B and Qwen-VL 7B by up to 6.45 and 7.21, respectively. The code is publicly available at: https://github.com/lingfengren/NoLan."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 5 2 ] . [ 1 4 4 1 2 2 . 2 0 6 2 : r NoLan: Mitigating Object Hallucinations in Large Vision-Language Models via Dynamic Suppression of Language Priors Lingfeng Ren1 Weihao Yu2 Runpeng Yu1 Xinchao Wang1 1National University of Singapore, Singapore 2Peking University Shenzhen Graduate School, China {lingfengren, r.yu}@u.nus.edu weihao@pku.edu.cn xinchao@nus.edu.sg https://github.com/lingfengren/NoLan"
        },
        {
            "title": "Abstract",
            "content": "Object hallucination is critical issue in Large Vision-Language Models (LVLMs), where outputs include objects that do not appear in the input image. natural question arises from this phenomenon: Which component of the LVLM pipeline primarily contributes to object hallucinations? The vision encoder to perceive visual information, or the language decoder to generate text responses? In this work, we strive to answer this question through designing systematic experiment to analyze the roles of the vision encoder and the language decoder in hallucination generation. Our observations reveal that object hallucinations are predominantly associated with the strong priors from the language decoder. Based on this finding, we propose simple and training-free framework, No-Language-Hallucination Decoding, NoLan, which refines the output distribution by dynamically suppressing language priors, modulated based on the output distribution difference between multimodal and text-only inputs. Experimental results demonstrate that NoLan effectively reduces object hallucinations across various LVLMs on different tasks. For instance, NoLan achieves substantial improvements on POPE, enhancing the accuracy of LLaVA-1.5 7B and Qwen-VL 7B by up to 6.45 and 7.21, respectively. The code will be made publicly available."
        },
        {
            "title": "Introduction",
            "content": "In recent years, Large Language Models (LLMs) [68, 12, 10, 90, 41, 79, 81] have revolutionized the field of machine learning with the ability of language understanding and content generation, offering unprecedented capabilities and potentials across multitude of applications. The integration of LLMs with computer vision systems has given rise to Large Vision-Language Models (LVLMs) [9, 68, 87, 2, 82, 51, 92, 83, 47, 36, 69, 42], facilitating various applications through their capacity to produce contextually accurate textual outputs from visual data. These models excel in identifying and converting intricate visual patterns into seamless linguistic expressions [51, 93, 84, 34, 15, 21, 57, 88, 4]. LVLMs with these advanced capabilities have demonstrated their value across multiple domains, such as content generation, image and video annotation, and interactive platforms that require comprehensive visual content interpretation. The development of LVLMs is characterized by continuous enhancements in model structures, training strategies, and data variety, resulting in improved performance and broader application adaptability. Nevertheless, significant challenge persists: object hallucinations [40, 23, 46, 56], where the text generated by LVLMs does not accurately Corresponding author. Figure 1: No-Language-Hallucination Decoding (NoLan). Given an LVLM, an image v, and language question x, NoLan mitigates hallucinations in responses by comparing outputs generated from multimodal and unimodal (text-only) inputs. Step 2 can also be simplified by setting α to fixed value of 1. In this example, the hallucinated object whale is suppressed by reducing the influence of language priors during token generation, while the ground truth object bear is effectively enhanced. reflect the objects in the provided image. Object hallucinations can lead to misinformation and misinterpretation, posing significant risks for decision-makingparticularly in high-stakes areas such as robotics [58, 48], autonomous systems [11, 80], and healthcare [76, 25]. In light of this, various strategies have been investigated to mitigate object hallucinations in Initial efforts focused on small-scale VLMs, employing techniques like fine-grained LVLMs. modality alignment [7] and data augmentation to reduce statistical biases related to object cooccurrence [61, 30]. However, the distinct behaviors of LVLMs render these methods difficult to generalize and scale [29, 78]. Recent research has tackled this challenge by developing hallucinationspecific datasets for fine-tuning [45, 23], training post-hoc revisors to produce outputs with fewer hallucinations [91], and employing factually enhanced Reinforcement Learning from Human Feedback (RLHF) [65]. Despite their effectiveness, these interventions demand significant human effort and computational resources, underscoring the urgent need for simpler yet efficient solution. LVLMs generally comprise two main components: vision encoder that perceives visual information and language decoder that generates text responses. This model composition motivates us to analyze the contributions of the vision and language components within LVLMs to the occurrence of object hallucinations. Through series of analytical experiments, we find that object hallucinations primarily stem from the language decoders priors rather than the vision encoder. Based on this insight, we focus on overcoming language priors and introduce No-Language-Hallucination Decoding (NoLan), simple, effective, and training-free framework designed to mitigate hallucinations in LVLMs. As illustrated in Figure 1, NoLan works by contrasting the output distributions of multimodal inputs with those of text-only inputs, acting as corrective mechanism to address the models over-reliance on linguistic priors embedded in the LLM. The modulation of the output distribution increases when the similarity between the token distributions of multimodal and text-only inputs is higher, as measured by Kullback-Leibler divergence-based function. Compared to previous methods [45, 23, 91, 65], NoLan eliminates the need for additional training or external tools, such as other pre-trained models. Our experimental results validate the effectiveness of NoLan, demonstrating consistent improvements across various object hallucination benchmarks and LVLM families, including LLaVA-1.5 [51, 49], InstructBLIP [15], and Qwen-VL [4]. Specifically, on the POPE benchmark [40], NoLan achieves significant performance gains, with accuracy improvements of up to 8.38 and F1 score enhancements of up to 8.78, highlighting its robustness and scalability in addressing object hallucinations across diverse LVLM architectures. 2 Overall, our main contributions are as follows: 1. We conduct series of analytical experiments to investigate the contributions of each component in LVLMs to object hallucinations, finding that hallucinations mainly stem from the language models priors rather than the vision model. 2. Building on this insight, we introduce NoLan, plug-and-play approach designed to mitigate object hallucinations by dynamically suppressing language priors. NoLan achieves this by leveraging the differences in output distributions between multimodal and text-only inputs, ensuring more consistent and contextually accurate content generation. 3. Extensive experiments demonstrate the effectiveness of NoLan in significantly reducing object hallucinations. Notably, our methods do not require additional training or external tools."
        },
        {
            "title": "2 Related work",
            "content": "2.1 Visual-Language Models The evolution of Vision-Language Models (VLMs) has advanced significantly, shifting from language models that incorporate BERT-like language encoder [16, 54, 31] for the fusion of visual and textual information [38, 64, 71, 37] to being driven by the integration of LLMs [20, 68, 67, 60, 8, 14, 66, 13, 3]. By integrating general vision encoder with large language model, LVLMs demonstrate range of emergent capabilities, enabling them to process and interpret complex visual and textual information more effectively. However, while grafted VLMs inherit strong linguistic capabilities from their base LLM, they also carry over the propensity to generate ungrounded or fabricated information [27, 6]. 2.2 Hallucination in VLMs Hallucination typically refers to instances in which the generated responses include information that is not present in the visual content [61, 7, 40]. Recent initiatives have aimed to tackle these intricacies, with research focusing on detecting and evaluating object hallucinations in the realm of LVLMs [73, 45, 40, 56, 85], and methods to reduce them [45, 85, 70]. For instance, POPE [40] transforms hallucination into binary classification task to assess the models ability to recognize whether particular object is present in the image. Unlike approaches that simply integrate powerful LLMs with in-context or few-shot learning capabilities [1, 35], efforts to address hallucinations have primarily focused on incorporating external tools for post-processing. For instance, Woodpecker [85] utilizes five-stage process, but many of these stages rely heavily on auxiliary models, such as multiple LLMs and vision foundation models, making the approach resource-intensive. Additionally, adapting factually augmented reinforcement learning from human feedback (RLHF) [65] has emerged as an effective strategy to align model outputs with factual accuracy. However, current strategies [52, 50] that involve acquiring additional datasets, performing detailed tuning on initial or new models, or utilizing other pretrained models can be time-intensive, laborious, and computationally demanding. To address these limitations, several training-free methods have been developed. For instance, Visual Contrastive Decoding (VCD) [33] calibrates visual uncertainty by contrasting output distributions generated from original and distorted visual inputs. Similarly, Multi-Modal Mutual Information Decoding (M3ID) [17] and Visual Debias Decoding (VDD) [89] enhance the influence of the reference image by comparing probability distributions produced from conditioned and unconditioned inputs. These approaches aim to refine model predictions without requiring additional training. Compared to these methods, our NoLan introduces fundamentally different, finer-grained assumption. While methods like VCD [33] and VDD [89] simplify the problem by assuming uniform language prior for all tokens, and M3ID assumes that the prior degree is conditioned only on sequence length [17], our approach makes more nuanced and realistic assumption. Specifically, our NoLan posits that each token possesses distinct language prior. We further propose simple yet effective KL-based method to measure the prior degree of each token. This token-specific and dynamic prior modeling allows our method to more accurately suppress each tokens language prior, leading to performance improvements. Thus, our works novelty lies in this novel assumption and the development of an effective mechanism to model it, which fundamentally distinguishes it from prior work. Figure 2: Experimental pipeline to test whether LLaVAs vision encoder can detect the presence of an object in an image. Table 1: The Vision encoder can robustly detect object presence in samples. On the MSCOCO dataset of POPE-random [40], for samples where LLaVA-1.5 experiences hallucinations, its vision encoder can indeed predict object presence with high accuracy. Samples on COCO of POPE-Random where LLaVA experiences hallucinations Metric Accuracy Precision Recall F1 Score Score 83.01 83.71 98.33 90."
        },
        {
            "title": "3 Method",
            "content": "3.1 Preliminary experiments LVLMs generally comprise two core components: vision encoder to gain visual information and language decoder to generate textual responses. This design raises an important question: are these two components responsible for object hallucinations? In this section, we present comprehensive analysis to investigate the contributions of both the vision encoder and the language decoder to these hallucinations. Vision Encoder. We aim to investigate whether the vision encoder accurately detects object presence in the failing cases of object hallucinations. To this end, we design pipeline as shown in Figure 2. Specifically, LLaVA comprises CLIP vision encoder and LLaMA (Vicuna) language model, but in this experiment, we use only the CLIP vision encoder. We extract image representation using the CLIP encoder and evaluate whether the representation includes information about specific object. For this, we transform the text query into photo of [object] and pass it through CLIPs BERT encoder to obtain text representation. We then calculate the cosine similarity between CLIPs image and text representations to assess object presence. As shown in Table 1, for samples where LLaVA-1.5 experiences hallucinations on the MSCOCO dataset of POPE (random) [40], its vision encoder can predict object presence with high accuracy of 83%. These results lead to our Finding 1: the vision encoder can indeed detect object presence in samples exhibiting object hallucinations. Language Decoder. While vision encoders can accurately detect objects, LVLMs - which combine vision encoders with LLaMA-like language decoders - still experience hallucinations. We hypothesize that these hallucinations occur when the output distribution is dominated by language priors embedded in LLMs, as illustrated in Figure 3. To test this hypothesis, we compare output distributions between an LVLM processing image-text inputs and its used LLM processing text-only inputs. Specifically, for LLaVA-1.5-7B [50], we denote: pm: Output distribution from LLaVA with image-text inputs. pu: Output distribution from LLaVAs language decoder LLaMA with text-only inputs. We measure the difference between these distributions using KL Divergence and JS Divergence metrics. Using the MSCOCO dataset from POPE-random [40], we create two subsets based on 4 Figure 3: An illustration of model prediction misdirected by the language priors. Given an image depicting six dwarfs in front of Snow White, LLaVA-1.5-13b provides the same token seven regardless of whether the image is provided as input or not. Table 2: Token probability distribution difference between multimodal and unimodal inputs. We split MSCOCO with POPE-random [40] into two subsets according to whether the answers from LLaVA-1.5-7B [50] contain hallucinations or not. Here, pm and pu represent the token probability distributions conditioned on multimodal and unimodal (text-only) inputs, respectively. The lower KL Divergence and JS Divergence values in the hallucination subset indicate greater similarity between the two distributions, suggesting that language priors heavily influence the outputs. DKL(pmpu) DKL(pupm) DJS(pm, pu) 0.58 0.28 Dataset POPEnohallucination POPEhallucination 1.20 0.46 0.28 0. whether LLaVA-1.5-7B produces hallucinations in its answers. As shown in Table 2, the hallucination subset exhibits significantly smaller divergence between Pm and Pu compared to the no-hallucination subset. This suggests that when hallucinations occur, the models outputs are more heavily influenced by language priors embedded in LLMs. As shown in Table 2, the distribution difference is prominent in the successful subset, whereas it is minimal in the subset of hallucinated responses. This result confirms that the linguistic priors inherent in the language decoder play significant role in contributing to hallucinations. Indeed, this models behavior is not entirely unexpected, as LLMs are fundamentally designed to predict the next words probability based on extensive textual corpora. When confronted with ambiguous dominant language question stimuli, LVLMs may default to these text-based predictions as safety net. While language priors are generally beneficial for contextual understanding and efficient inference, they can introduce biases or assumptions that conflict with the actual visual content. These results lead to our Finding 2: The output distribution of an LVLM is more dominated by its underlying LLMs priors when object hallucinations occur. 3.2 No-Language-Hallucination Decoding While it is commonly believed that hallucinations arise from weak visual signals in the vision module [22, 61, 72], our above findings indicate that object hallucinations are primarily driven by language priors. Therefore, in this section, we propose very simple framework named NoLanguage-Hallucination Decoding (NoLan), to overcome the influence of language priors on object hallucinations. Specifically, consider an LVLM parameterized by θ, with visual inputs and textual inputs x. The output is generated auto-regressively from probability distribution conditioned on both and x, expressed as: lm = logitθ (yt v, x, y<t) , yt softmax(lm), subject to yt V, (1) where yt represents the token at time step t, y<t denotes the sequence of tokens generated up to time (t 1), and stands for the vocabulary dictionary. 5 To obtain the language priors, we feed text only into the model and compute its logits: lu = logitθ (yt x, y<t) (2) Unlike [33], the computation of language priors does not rely on distorted visual inputs. After obtaining regular multimodal logits lm and language priors lu, the next step is to design the modulation values on output distribution. Inspired by the contrastive decoding in text [44, 39, 59, 63] and multimodal [33, 17] generation, we compute the difference between lm and lu as modulation logits: = α (lm lu), where α is modulation rate that controls the influence of the modulation distribution. Thus, the output probability distribution modulated by can be expressed as: (3) pnolan (y v, x) = softmax [lm + l] = softmax [logitθ (y v, x, y<t) +α(logitθ (y v, x, y<t) logitθ (y x, y<t))] , (4) with α = 0 corresponding to standard decoding. Using the adjusted output distribution pnolan, various sampling methods, such as top-p sampling [24] and beam search [18], can be applied to generate outputs, yt pnolan, subject to yt V. (5) Building on this structure, we propose two versions of NoLan, NoLan-Base and NoLan-Plus, based on the different formulations of modulation term: NoLan-Base: In this version, α is treated as fixed hyperparameter, set to 1 by default. Surprisingly, this simple choice already demonstrates impressive performance in our experiments. Then the Equation 4 becomes: yt softmax [2 logitθ (yt v, x, y<t) logitθ (yt x, y<t)] , subject to yt V, (6) NoLan-Plus: Motivated by Finding 2, as shown in Table 2, which highlights that smaller differences between lm and lu are associated with higher likelihood of hallucinations. To leverage this association, we introduce self-adjusting mechanism derived from the symmetric KL-divergence, which is expressed as: γ = (DKL(lmlu) + DKL(lulm)) 2 (cid:18) 1 γ tanh + 1 (cid:19) (cid:18) (cid:19) . α = β , (7) (8) The symmetric KL divergence is first inverted and then passed through tanh function to constrain its range. An additive shift of 1 is subsequently applied to ensure the resulting value lies within the positive domain. To further refine the value range, we introduce scaling factor β, which we set to 0.8 based on our ablation experiments in the appendix. This mechanism automatically adjusts the modulation term, effectively suppressing the LLMs priors and enhancing its performance. We refer to this improved version as NoLan-Plus, distinguishing it from the simpler NoLan-Base. comprehensive theoretical reasoning of this dynamic weighting method can be found in Appendix A.1. In summary, the NoLan framework introduces two variants: NoLan-Base and NoLan-Plus. Unlike NoLan-Base, which remains static modulation rate throughout generation, NoLan-Plus dynamically adapts to the output distribution, improving both flexibility and effectiveness. We also show the NoLan framework in algorithm 1."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we evaluate NoLan across different LVLMs and tasks to demonstrate its effectiveness. 6 Algorithm 1: NoLan Input: LVLM θ, textual prompt x, image v, modulation rate α Output: Generated string conditioned on and Initialization: y0 = BOS, = 1 while yt = EOS do lm logitθ(yv, x, y<t) lu logitθ(yx, y<t) α (lm, lu) ; lm + α(lm lu) yt Sampling(softmax(l)) + 1 end // () can be constant or KL-based 4.1 Experimental settings 4.1.1 Datasets & evaluation metrics POPE. The Polling-based Object Probing Evaluation [40] (POPE), introduces an efficient method to evaluate object hallucinations. In this benchmark, LVLMs are asked to determine whether specific object exists in given image. The POPE benchmark compiles data from three different sources: MSCOCO [43], A-OKVQA [62], and GQA [28]. The evaluation focuses on four primary metrics: Accuracy, Precision, Recall, and the F1 score. MME. It acts as comprehensive benchmark for evaluating LVLMs across multiple dimensions [19], which encompasses ten subtasks related to perception and four focused on cognition, offering holistic assessment of multimodal model capabilities. To evaluate hallucinations precisely, we use targeted subsets: existence and count for object-level, and position and color for attribute-level hallucinations. Performance is measured via the composite metric of accuracy and accuracy+ as defined in the official implementation.2 LLaVA-Bench.3 This dataset is highly diverse, featuring 24 images paired with 60 questions. It encompasses wide range of scenarios, including indoor and outdoor scenes, memes, paintings, and sketches, making it an excellent resource for evaluating the capability of LVLMs to handle complex tasks and adapt to diverse domains. Other datasets. Our evaluation also includes benchmarks such as MM-Vet [86], MMHal-Bench [65], and HallusionBench [22], which are detailed in Appendix A.6. 4.1.2 LVLM baselines We evaluate the performance of NoLan across three state-of-the-art LVLMs. To ensure fair and consistent comparison, our experimental setup aligns with VCD [33]. Specifically, we integrate NoLan with LLaVA-1.5 [50] and InstructBLIP, both of which use Vicuna 7B as their language decoder [49, 15], as well as Qwen-VL, which is built on the Qwen 7B backbone [4]. More LVLM baselines can be found in Appendix A.4, and the Qwen-VL series is detailed in Appendix A.8. 4.2 Decoding baselines One of the decoding methods we compared is direct sampling from the output probability distribution of LVLMs using regular image and text inputs, which we denote as Regular\". notable training-free method is VCD [33], which generates outputs by contrasting distributions from clear and distorted images. Other notable approaches include M3ID [17] and VDD [89], which enhance the influence of the reference image while reducing the dominance of language priors. Further contrastive decoding baselines and other attention-based approaches are detailed in appendix A.5 and A.7, respectively. 2https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/ Evaluation 3https://huggingface.co/datasets/liuhaotian/llava-bench-in-the-wild 7 Table 3: Results on POPE [40]. Regular decoding denotes direct sampling, VCD [33] indicates sampling from visual contrastive distribution, while methods prefixed with NoLan refers to sampling from our proposed contrastive distribution pnolan. The best performances within each setting are bolded. The mean and the standard deviation over 5 runs of POPE. Dataset Model Decoding LLaVA1.5 GQA Qwen-VL InstructBLIP LLaVA1.5 A-OKVQA Qwen-VL InstructBLIP LLaVA1.5 MSCOCO Qwen-VL InstructBLIP Regular VCD NoLan-Base (Ours) NoLan-Plus (Ours) Regular VCD NoLan-Base (Ours) NoLan-Plus (Ours) Regular VCD NoLan-Base (Ours) NoLan-Plus (Ours) Regular VCD NoLan-Base (Ours) NoLan-Plus (Ours) Regular VCD NoLan-Base (Ours) NoLan-Plus (Ours) Regular VCD NoLan-Base (Ours) NoLan-Plus (Ours) Regular VCD NoLan-Base (Ours) NoLan-Plus (Ours) Regular VCD NoLan-Base (Ours) NoLan-Plus (Ours) Regular VCD NoLan-Base (Ours) NoLan-Plus (Ours) Random Popular Adversarial Accuracy 83.73(0.27) 86.65(0.45) 88.35(0.16) 88.53(0.10) 80.97(0.32) 85.59(0.38) 86.55(0.22) 87.27(0.22) 79.65(0.24) 83.69(0.11) 85.62(0.28) 86.15(0.11) 83.45(0.48) 86.15(0.23) 87.83(0.16) 88.04(0.14) 86.67(0.48) 89.22(0.14) 89.17(0.28) 89.40(0.20) 80.91(0.34) 84.11(0.27) 87.87(0.37) 88.20(0.33) 83.29(0.35) 87.73(0.40) 86.73(0.15) 87.11(0.13) 84.73(0.36) 88.63(0.10) 88.30(0.19) 88.10(0.11) 80.71(0.73) 84.53(0.38) 86.07(0.41) 85.67(0.33) F1 Score 82.95(0.28) 86.99(0.41) 87.68(0.17) 87.84(0.12) 79.01(0.40) 85.33(0.38) 86.13(0.31) 87.04(0.17) 80.56(0.18) 84.16(0.01) 85.02(0.18) 85.27(0.19) 82.56(0.50) 86.34(0.21) 87.21(0.19) 87.32(0.14) 85.59(0.53) 89.01(0.16) 88.80(0.33) 89.02(0.13) 81.86(0.32) 84.56(0.28) 87.46(0.32) 87.55(0.21) 81.33(0.41) 87.16(0.41) 85.15(0.20) 86.60(0.16) 82.67(0.41) 87.81(0.11) 87.22(0.21) 87.00(0.10) 80.41(0.80) 83.68(0.40) 84.45(0.36) 83.81(0.31) Accuracy 78.17(0.17) 80.73(0.47) 84.13(0.30) 84.62(0.33) 75.99(0.33) 81.83(0.27) 82.37(0.22) 83.20(0.24) 73.87(0.58) 78.57(0.14) 79.61(0.22) 81.12(0.21) 79.90(0.33) 81.85(0.44) 85.41(0.42) 85.85(0.20) 85.56(0.35) 87.85(0.30) 87.42(0.29) 88.00(0.16) 76.19(0.80) 79.78(0.47) 83.60(0.43) 84.57(0.42) 81.88(0.48) 85.38(0.38) 85.63(0.17) 85.81(0.13) 84.13(0.18) 87.12(0.07) 86.83(0.27) 87.43(0.29) 78.22(0.84) 81.47(0.42) 83.97(0.33) 84.00(0.26) F1 Score 78.37(0.18) 82.24(0.35) 83.94(0.26) 84.35(0.21) 74.84(0.34) 82.23(0.22) 82.61(0.19) 83.61(0.15) 76.42(0.52) 80.17(0.16) 80.00(0.21) 80.99(0.17) 79.59(0.37) 82.82(0.36) 85.00(0.42) 85.36(0.19) 84.63(0.42) 87.81(0.31) 87.10(0.28) 87.83(0.24) 78.17(0.73) 81.15(0.42) 83.76(0.31) 84.32(0.36) 80.06(0.05) 85.06(0.37) 84.12(0.21) 85.17(0.17) 82.06(0.23) 86.40(0.09) 85.70(0.25) 86.43(0.22) 78.36(0.76) 81.07(0.39) 82.43(0.28) 82.49(0.30) Accuracy 75.08(0.33) 76.09(0.43) 80.65(0.19) 81.23(0.17) 75.46(0.63) 80.01(0.27) 80.23(0.28) 80.25(0.31) 70.56(0.53) 75.08(0.13) 77.00(0.15) 78.13(0.12) 74.04(0.34) 74.97(0.39) 79.21(0.20) 79.61(0.17) 79.57(0.31) 81.27(0.09) 81.10(0.21) 81.20(0.19) 70.71(0.76) 74.33(0.67) 77.33(0.45) 78.43(0.22) 78.96(0.52) 80.88(0.33) 83.22(0.17) 83.83(0.17) 82.26(0.30) 84.26(0.39) 84.91(0.31) 84.93(0.18) 75.84(0.45) 79.56(0.41) 81.97(0.48) 82.37(0.19) F1 Score 76.06(0.24) 78.78(0.36) 81.08(0.21) 81.56(0.19) 74.33(0.71) 80.75(0.27) 80.85(0.26) 81.06(0.17) 74.12(0.58) 77.53(0.08) 77.97(0.13) 78.43(0.10) 75.15(0.23) 77.73(0.29) 79.90(0.17) 80.19(0.16) 79.50(0.38) 82.38(0.10) 81.91(0.27) 82.06(0.15) 75.56(0.57) 77.19(0.47) 78.79(0.47) 79.24(0.27) 77.57(0.57) 81.13(0.34) 81.93(0.22) 82.58(0.16) 80.37(0.37) 83.90(0.39) 84.01(0.33) 84.07(0.17) 76.59(0.40) 79.52(0.38) 80.75(0.44) 80.81(0.23) Table 4: Results of accuracy on MSCOCO of POPE. (a) Using the setting in M3ID [17]. We follow M3ID using its template: Is object present in the image? for fair comparison. (b) Using the setting in VDD [89]. We follow the decoding format and evaluation settings in VDD to ensure fair comparison. Decoding Random Popular Adversarial All Decoding Random Popular Adversarial All MSCOCO of POPE MSCOCO of POPE Regular M3ID NoLan-Base (Ours) NoLan-Plus (Ours) Regular M3ID NoLan-Base (Ours) NoLan-Plus (Ours) LLaVA-1.5-7B 61.8 69.3 86.3 87.5 74.8 76.0 87.8 88.8 LLaVA-1.5-13B 63.8 77.0 86.8 88. 67.9 84.3 88.0 89.2 58.1 65.8 82.7 83.7 59.8 71.3 84.0 85.2 64.9 70.3 85.6 86.7 63.8 77.5 86.3 87.6 Regular VDD NoLan-Base (Ours) NoLan-Plus (Ours) Regular VDD NoLan-Base (Ours) NoLan-Plus (Ours) LLaVA-1.5-7B 81.88 85.87 85.13 85.83 83.29 87.07 86.50 87.10 LLaVA-1.5-13B 82.47 86.08 86.23 87.40 83.31 86.88 87.37 88.70 78.96 83.52 83.00 83. 80.00 84.34 83.87 84.90 81.37 85.49 84.89 85.52 81.92 85.77 85.82 87.00 4.3 Experimental results Results on POPE. Table 3 summarizes the experimental results for POPE under random, popular, and adversarial sampling conditions. notable highlight is the strong performance of our proposed NoLan approach. NoLan consistently outperforms regular decoding baseline in every evaluated scenario and achieves improvements of up to 8.38 in accuracy and 8.77 in F1 scores across all tested LVLMs. Furthermore, NoLan-Base demonstrates superior performance over VCD [33], with improvements of up to 4.56 in accuracy and 2.9 in F1 scores, outperforming VCD in 77.8% of the evaluated cases. NoLan-Plus amplifies this advantage, achieving gains of up to 5.14 in accuracy and 3.17 in F1 scores, surpassing VCD in 88.9% of the experiments. With the template in M3ID [17], NoLan significantly suppresses M3ID in accuracy, achieving improvements of up to 18.2 and 13.9, with an average increase of 16.4 and 10.1 on the 7B and 13B models, respectively. Additionally, to ensure fair comparison, when using the same settings as VDD [89], NoLan-Plus still outperforms VDD on both the 7B and 13B models. This underscores NoLans effectiveness in mitigating object hallucinations in LVLMs, emphasizing that object hallucinations are predominantly driven by language priors in language models. By weakening these priors at more fine-grained level, the models performance can be significantly improved. Table 5: Results on the hallucination subset of MME [19]. Regular decoding denotes direct sampling, VCD [33] indicates sampling from visual contrastive distribution, and VDD [89] expresses visual debias decoding. In contrast, methods prefixed with NoLan refer to sampling from our proposed contrastive distribution pnolan. The best performances within each setting are bolded. Model Decoding LLaVA1.5 Qwen-VL InstructBLIP Regular VCD VDD NoLan-Base (Ours) NoLan-Plus (Ours) Regular VCD VDD NoLan-Base (Ours) NoLan-Plus (Ours) Regular VCD NoLan-Base (Ours) NoLan-Plus (Ours) Object-level Attribute-level Existence 175.67 184.66 190.00 190.00 190.00 155.00 156.00 165.00 160.00 185.00 141.00 168.33 175.00 180.00 Count 124.67 138.33 143.30 145.00 151.67 127.67 131.00 145.00 135.00 145.00 75.33 92.33 61.67 65.00 Position 114.00 128.67 145.00 138.33 143.33 131.67 128.00 148.30 133.33 138.33 66.67 64.00 68.33 76. Color 151.00 153.00 165.00 155.00 175.00 173.00 181.67 190.00 190.00 180.00 97.33 123.00 118.33 138.33 Total Scores 565.33 604.66 643.29 628.33 660.00 587.33 596.67 643.29 618.33 648.33 380.33 447.67 423.33 460.00 Figure 4: Illustration of hallucination mitigation by our proposed NoLan-Plus with two samples from LLaVA-Bench. Hallucinated objects from LVLMs regular decoding are highlighted in red. Results on MME hallucination subset. The evaluation on the MME subset extends beyond POPE by addressing both object-level and attribute-level hallucinations. As presented in Table 5, the implementation of NoLan consistently improves performance across all models in resolving attribute-level hallucinations. Notably, NoLan-Plus outperforms both the regular baseline, VCD and VDD on the majority of subsets, further underscoring its effectiveness. In terms of objectlevel hallucinations, both variants of NoLan show clear positive impact on the Existence metric, significantly enhancing overall performance. These improvements emphasize NoLans strength in mitigating object hallucinations across diverse scenarios. Case study on LLaVA-Bench. Figure 4 illustrates two case studies that demonstrate the effectiveness of NoLan-Plus in mitigating object hallucinations. In the cases presented, objects like suitcase\" and truck\" which are commonly associated with the ground truth object taxi\", erroneously appear as hallucinations in the generated output. In contrast, the application of NoLan-Plus significantly reduces these hallucinations while preserving the consistency and richness of the generated text. This showcases NoLan-Pluss ability to produce outputs that are more aligned with the visual input without sacrificing informativeness. Due to space constraints, additional case studies are included in Appendix A.11 for further reference."
        },
        {
            "title": "5 Conclusion and discussion",
            "content": "In this paper, we tackle the critical challenge of object hallucinations in LVLMs. We begin by analyzing the roles of the vision encoder and language decoder in contributing to these hallucinations. Our experiments reveal that in hallucination cases, vision encoders effectively detect objects; however, the output distribution is heavily influenced by the priors of the language decoder. Based on this insight, we propose No-Language-Hallucination Decoding (NoLan), simple, training-free framework to overcome language priors. It leverages contrastive distributions from multimodal and text-only inputs, to refine the models outputs, without relying on external tools. This structure 9 introduces two method variants: NoLan-Base and NoLan-Plus. While NoLan-Base maintains constant configuration throughout generation, NoLan-Plus dynamically adjusts to the output distribution, offering greater flexibility and improved effectiveness. NoLan operates during inference and can be seamlessly integrated with any pre-trained autoregressive LVLMs. This design makes NoLan cost-effective and flexible solution for improving vision-language grounding. Extensive experiments conducted across diverse benchmarks and architectures of LVLMs validate NoLans effectiveness in mitigating object hallucinations."
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:2371623736, 2022. [2] Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo, March 2023. [3] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. [5] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [6] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023, 2023. [7] Ali Furkan Biten, Lluís Gómez, and Dimosthenis Karatzas. Let there be clock on the beach: Reducing object hallucination in image captioning. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 13811390, 2022. [8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [9] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments with gpt-4, 2023. [10] Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et al. Alpagasus: Training better alpaca with fewer data. arXiv preprint arXiv:2307.08701, 2023. [11] Long Chen, Oleg Sinavski, Jan Hünermann, Alice Karnsund, Andrew James Willmott, Danny Birch, Daniel Maund, and Jamie Shotton. Driving with llms: Fusing object-level vector modality for explainable autonomous driving. arXiv preprint arXiv:2310.01957, 2023. [12] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, 2023. [13] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. [14] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. [15] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. arXiv preprint arXiv:2306.04387, 2023. [16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. [17] Alessandro Favero, Luca Zancato, Matthew Trager, Siddharth Choudhary, Pramuditha Perera, Alessandro Achille, Ashwin Swaminathan, and Stefano Soatto. Multi-modal hallucination control by visual information grounding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1430314312, 2024. [18] Markus Freitag and Yaser Al-Onaizan. Beam search strategies for neural machine translation. arXiv preprint arXiv:1702.01806, 2017. [19] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. [20] Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli. Chatgpt outperforms crowd-workers for textannotation tasks. arXiv preprint arXiv:2303.15056, 2023. [21] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen. Multimodal-gpt: vision and language model for dialogue with humans. arXiv preprint arXiv:2305.04790, 2023. [22] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: an advanced diagnostic suite for entangled language In Proceedings of the IEEE/CVF hallucination and visual illusion in large vision-language models. Conference on Computer Vision and Pattern Recognition, pages 1437514385, 2024. [23] Anisha Gunjal, Jihan Yin, and Erhan Bas. Detecting and preventing hallucinations in large vision language models. arXiv preprint arXiv:2308.06394, 2023. [24] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751, 2019. [25] Mingzhe Hu, Shaoyan Pan, Yuheng Li, and Xiaofeng Yang. Advancing medical imaging with language models: journey from n-grams to chatgpt. arXiv preprint arXiv:2304.04920, 2023. [26] Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai Yu. Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1341813427, 2024. [27] Yichong Huang, Xiachong Feng, Xiaocheng Feng, and Bing Qin. The factual inconsistency problem in abstractive text summarization: survey. arXiv preprint arXiv:2104.14839, 2021. [28] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67006709, 2019. [29] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [30] Jae Myung Kim, Koepke, Cordelia Schmid, and Zeynep Akata. Exposing and mitigating spurious correlations for cross-modal retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 25842594, 2023. [31] MV Koroteev. Bert: review of applications in natural language processing and understanding. arXiv preprint arXiv:2103.11943, 2021. [32] P. Langley. Crafting papers on machine learning. In Pat Langley, editor, Proceedings of the 17th International Conference on Machine Learning (ICML 2000), pages 12071216, Stanford, CA, 2000. Morgan Kaufmann. [33] Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. Mitigating object hallucinations in large vision-language models through visual contrastive decoding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13872 13882, 2024. 11 [34] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726, 2023. [35] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. [36] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. ArXiv, abs/2301.12597, 2023. [37] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training In International Conference on Machine for unified vision-language understanding and generation. Learning, pages 1288812900. PMLR, 2022. [38] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: simple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557, 2019. [39] Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. Contrastive decoding: Open-ended text generation as optimization. arXiv preprint arXiv:2210.15097, 2022. [40] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023. [41] Zongxia Li, Paiheng Xu, Fuxiao Liu, and Hyemi Song. Towards understanding in-context learning with contrastive demonstrations and saliency maps. arXiv preprint arXiv:2307.05052, 2023. [42] Chen Liang, Jiahui Yu, Ming-Hsuan Yang, Matthew Brown, Yin Cui, Tuo Zhao, Boqing Gong, and Tianyi Zhou. Module-wise adaptive distillation for multimodality foundation models. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [43] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. [44] Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah Smith, and Yejin Choi. Dexperts: Decoding-time controlled text generation with experts and anti-experts. arXiv preprint arXiv:2105.03023, 2021. [45] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Aligning large multi-modal model with robust instruction tuning. arXiv preprint arXiv:2306.14565, 2023. [46] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Mitigating hallucination in large multi-modal models via robust instruction tuning. arXiv preprint arXiv:2306.14565, 2023. [47] Fuxiao Liu, Yinghan Wang, Tianlu Wang, and Vicente Ordonez. Visual news: Benchmark and challenges in news image captioning. arXiv preprint arXiv:2010.03743, 2020. [48] Haokun Liu, Yaonan Zhu, Kenji Kato, Izumi Kondo, Tadayoshi Aoyama, and Yasuhisa Hasegawa. Llmbased human-robot collaboration framework for manipulation tasks. arXiv preprint arXiv:2308.14972, 2023. [49] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023. [50] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. [51] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023. [52] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. [53] Shi Liu, Kecheng Zheng, and Wei Chen. Paying more attention to image: training-free method for alleviating hallucination in lvlms. In European Conference on Computer Vision, pages 125140. Springer, 2024. [54] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. [55] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216233. Springer, 2024. [56] Holy Lovenia, Wenliang Dai, Samuel Cahyawijaya, Ziwei Ji, and Pascale Fung. Negative object presence evaluation (nope) to measure object hallucination in vision-language models. arXiv preprint arXiv:2310.05338, 2023. [57] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. [58] Jinjie Mai, Jun Chen, Bing Li, Guocheng Qian, Mohamed Elhoseiny, and Bernard Ghanem. Llm as robotic brain: Unifying egocentric memory and control. arXiv preprint arXiv:2304.09349, 2023. [59] Sean OBrien and Mike Lewis. Contrastive decoding improves reasoning in large language models. arXiv preprint arXiv:2309.09117, 2023. [60] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):54855551, 2020. [61] Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. Object hallucination in image captioning. arXiv preprint arXiv:1809.02156, 2018. [62] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. Aokvqa: benchmark for visual question answering using world knowledge. In European Conference on Computer Vision, pages 146162. Springer, 2022. [63] Weijia Shi, Xiaochuang Han, Mike Lewis, Yulia Tsvetkov, Luke Zettlemoyer, and Scott Wen-tau Yih. Trusting your evidence: Hallucinate less with context-aware decoding. arXiv preprint arXiv:2305.14739, 2023. [64] Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. Videobert: joint model for video and language representation learning. In Proceedings of the IEEE/CVF international conference on computer vision, pages 74647473, 2019. [65] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, LiangYan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525, 2023. [66] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github. com/tatsu-lab/stanford_alpaca, 2023. [67] Yi Tay, Mostafa Dehghani, Vinh Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Steven Zheng, et al. Ul2: Unifying language learning paradigms. In The Eleventh International Conference on Learning Representations, 2022. [68] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [69] Alasdair Tran, Alexander Mathews, and Lexing Xie. Transform and tell: Entity-aware news image captioning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1303513045, 2020. [70] Bin Wang, Fan Wu, Xiao Han, Jiahui Peng, Huaping Zhong, Pan Zhang, Xiao wen Dong, Weijia Li, Wei Li, Jiaqi Wang, and Conghui He. Vigc: Visual instruction generation and correction. ArXiv, abs/2308.12714, 2023. [71] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. Git: generative image-to-text transformer for vision and language. arXiv preprint arXiv:2205.14100, 2022. 13 [72] Junyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang, Yukai Gu, Haitao Jia, Ming Yan, Ji Zhang, and Jitao Sang. An llm-free multi-dimensional benchmark for mllms hallucination evaluation. CoRR, 2023. [73] Junyang Wang, Yiyang Zhou, Guohai Xu, Pengcheng Shi, Chenlin Zhao, Haiyang Xu, Qinghao Ye, Ming Yan, Ji Zhang, Jihua Zhu, et al. Evaluation and analysis of hallucination in large vision-language models. arXiv preprint arXiv:2308.15126, 2023. [74] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2024. [75] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [76] Sheng Wang, Zihao Zhao, Xi Ouyang, Qian Wang, and Dinggang Shen. Chatcad: Interactive computeraided diagnosis on medical image using large language models. arXiv preprint arXiv:2302.07257, 2023. [77] Xintong Wang, Jingheng Pan, Liang Ding, and Chris Biemann. Mitigating hallucinations in large visionlanguage models with instruction contrastive decoding. arXiv preprint arXiv:2403.18715, 2024. [78] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022. [79] Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, et al. Larger language models do in-context learning differently. arXiv preprint arXiv:2303.03846, 2023. [80] Zhenyu Wu, Ziwei Wang, Xiuwei Xu, Jiwen Lu, and Haibin Yan. Embodied task planning with large language models. arXiv preprint arXiv:2307.01848, 2023. [81] Yijia Xiao, Yiqiao Jin, Yushi Bai, Yue Wu, Xianjun Yang, Xiao Luo, Wenchao Yu, Xujiang Zhao, Yanchi Liu, Haifeng Chen, Wei Wang, and Wei Cheng. Large language models can be good privacy protection learners. 2023. [82] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023. [83] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. [84] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, and Fei Huang. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. [85] Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, and Enhong Chen. Woodpecker: Hallucination correction for multimodal large language models. Science China Information Sciences, 67(12):220105, 2024. [86] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. [87] Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, et al. Socratic models: Composing zero-shot multimodal reasoning with language. arXiv preprint arXiv:2204.00598, 2022. [88] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. [89] Yi-Fan Zhang, Weichen Yu, Qingsong Wen, Xue Wang, Zhang Zhang, Liang Wang, Rong Jin, and Tieniu Tan. Debiasing large visual language models. arXiv preprint arXiv:2403.05262, 2024. [90] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. survey of large language models. arXiv preprint arXiv:2303.18223, 2023. 14 [91] Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao. Analyzing and mitigating object hallucination in large vision-language models. arXiv preprint arXiv:2310.00754, 2023. [92] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. [93] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Theoretical proof of NoLan-Plus The core mechanism of NoLan-Plus is dynamic weighting method that uses KL divergence to measure the difference between multimodal and text-only output distributions. In addition to the empirical justification and experimental research for using KL divergence, we conduct the following theoretical analysis. Definition of Visual Object Hallucination. Visual Object Hallucination is defined as the case where the models response is almost independent of the visual input given textual prompt x. Formally, this dependency is quantified using the conditional mutual information: I(y; x). (9) lower mutual information implies stronger hallucination. As an extreme case, if and are independent, then the response is generated without reference to the visual input, and I(y; x) = 0. Connecting Conditional Mutual Information to KL Divergence. For fixed prompt x, define pm(y) = Pθ(y v, x), pu(y) = Pθ(y x), (10) (11) where pm is the output distribution conditioned on both image and text, and pu is conditioned on text only. By the standard identity between conditional mutual information and KL divergence, we have: I(y; x) = Evx (cid:104) DKL (cid:0)pm pu (cid:1)(cid:105) . Proof. Starting from the definition of conditional mutual information: I(y; x) = Ev,yx = Ev,yx (cid:21) (cid:20) log (cid:20) log (y, x) (y x)P (v x) (y v, x) (y x) (cid:21) (cid:34) (cid:88) (cid:2)DKL (cid:2)DKL (y v, x) log (y v, x) (y x) (cid:0)P (Y v, x) (Y x)(cid:1)(cid:3) (cid:0)pm pu (cid:1)(cid:3) . = Evx = Evx = Evx (12) (13) (14) (15) (16) (17) (cid:35) Thus, lower KL divergence DKL(pm pu) indicates lower mutual information between the visual input and the response, leading to stronger hallucination. A.2 Uncertainty analysis and language prior suppression core motivation of our approach is to mitigate the influence of language priors in vision-language models. To this end, we adopt contrastive decoding strategy that reshapes the output distribution without additional training. While this training-free formulation effectively suppresses dominant linguistic priors, it may also introduce instability into the decoding process: by altering token probabilities post hoc, it can distort the relative ranking of non-target tokens and occasionally amplify spurious modes in the distribution. 16 Table 6: Entropy-based uncertainty evaluation across four benchmarks: POPE, MME, MM-Vet, and LLaVA-Bench. Results indicate that NoLan achieves the lowest uncertainty in all settings. Decoding Regular Text-only VCD NoLan-Base NoLan-Plus POPE MME MM-Vet LLaVA-Bench 0.6484 0.6948 0.4646 0.4692 0. 4.1875 3.5508 2.1153 0.8188 0.7931 1.5889 2.0020 0.6854 0.6040 0.4423 2.9863 3.7051 0.8632 0.9106 0.7439 To evaluate these effects more explicitly, we analyze the LLaVA-1.5s predictive uncertainty using entropy over output distributions. Lower entropy indicates greater confidence and better calibration. As shown in Table 6, we report entropy across four diverse benchmarks: POPE, MME, MM-Vet, and LLaVA-Bench. Compared to regular decoding and the text-only baseline, both NoLan variants yield substantially lower entropy in all settings. In particular, NoLan-Plus achieves the lowest uncertainty, suggesting that our method not only suppresses linguistic bias but also maintains overall distributional stability in most cases. A.3 Correlation study between hallucination and token position Token Pos Table 7: Token-wise KL values indicating hallucination. 10 5 0 1 7 9 3 6 4 8 11 KL Value 1.36 0.53 0.97 0.50 0. 0.76 0.63 0.66 0.47 0.46 0. 0.42 0.42 Recent studies have highlighted that the position of generated token within sequence can be significant factor in the emergence of model hallucinations. For instance, M3ID [17], demonstrated that as model generates more tokens, its reliance on the initial visual prompt decreases, leading to an increase in hallucinations. Based on this finding, we conducted study to quantitatively measure this correlation between token positions and the degree of hallucination. Specifically, we used LLaVA-1.5 7B to evaluate performance across the entire LLaVA-Bench. Following the method in our preliminary experiments, we calculated the mean KL divergence across all samples at each token position to estimate the likelihood of hallucination. Here, token position refers to the index assigned to each generated token after the model receives the input image and text. The first generated token is assigned position 0, and subsequent tokens are indexed sequentially based on their order in the output sequence. The results are shown in Table 7. The observation is generally in line with the findings of M3ID. Overall, the experimental results show that the farther token is from the beginning of the sequence, the more similar the distributions of the two forward passes become, and the greater the likelihood of hallucination. However, some samples deviate from this trend, possibly because the output tokens are not strictly object-related but also include many non-object terms. As result, the values do not consistently vary with token position. A.4 Ablation study We use the same LVLM baselines for ablation studies. Modulation rate. The parameter α governs the amplification of the modulation distribution generated from multimodal and unimodal inputs, as defined in Equation 3. We adjust α to examine its impact on NoLan-Base and identify the optimal value for performance. As shown in Table 8, α = 1 yields the best or second-best performance, so we set it as the default. Similarly, the parameter β in NoLan-Plus, which controls the boundary of the auto-adjusting modulation rate, achieves optimal or suboptimal results at β = 0.8, as demonstrated in Table 9, making it our default setting. Logit components. The logits in NoLan consist of regular multimodal logits lm , derived from image and text inputs, and unimodal logits lu , derived from text-only inputs. As shown in Table 10, using only lm or lu results in significant performance drop, highlighting the critical role of each logit component in NoLan and the effectiveness of its utilization mechanism. This finding further supports our hypothesis that object hallucinations predominantly originate from linguistic priors. 17 Table 8: Sensitivity to modulation rate α. In NoLan-Base, α is manually set to regulate the influence of the modulation distribution, defined in Equation 3. When α = 0, NoLan-Base reverts to standard decoding. α 0.0 1.0 2.0 3.0 0.0 1.0 2.0 3.0 0.0 1.0 2.0 3.0 0.0 1.0 2.0 3.0 0.0 1.0 2.0 3.0 Model LLaVA-1.5-7B LLaVA-1.5-7B LLaVA-1.5-7B LLaVA-1.5-7B LLaVA-1.5-13B LLaVA-1.5-13B LLaVA-1.5-13B LLaVA-1.5-13B Qwen-VL Qwen-VL Qwen-VL Qwen-VL InstructBLIP-7B InstructBLIP-7B InstructBLIP-7B InstructBLIP-7B InstructBLIP-13B InstructBLIP-13B InstructBLIP-13B InstructBLIP-13B POPE MME Accuracy Precision Recall F1 Score MME-Hallu Existence Count Position Color 151.00 565.33 628.33 155.00 148.33 620.00 148.33 588.33 163.33 616.67 636.67 153.33 155.00 620.00 158.33 631.67 173.00 587.33 618.33 190.00 180.00 613.33 175.00 613.33 97.33 380.33 423.33 118.33 125.00 413.33 123.33 406.67 153.33 440.00 465.00 143.33 143.33 460.00 143.33 450.00 175.67 190.00 190.00 180.00 185.00 190.00 190.00 190.00 155.00 160.00 165.00 170.00 141.00 175.00 180.00 165.00 160.00 180.00 180.00 180.00 124.67 145.00 143.33 138.33 136.67 165.00 145.00 150.00 127.67 135.00 135.00 135.00 75.33 61.67 50.00 55.00 60.00 65.00 60.00 60.00 114.00 138.33 138.33 121.67 131.67 128.33 138.33 133.33 131.67 133.33 133.33 133.33 66.67 68.33 58.33 63.33 66.67 76.67 76.67 66.67 83.29 86.50 86.27 85.97 84.35 87.37 87.23 87.12 84.73 88.30 87.93 87.87 80.71 85.57 84.20 83.73 81.92 86.70 85.43 84. 92.13 96.68 96.66 96.63 93.22 95.61 97.03 96.94 95.61 96.07 95.02 94.72 81.67 96.76 97.32 98.01 83.13 97.21 97.72 98.25 72.80 75.60 75.13 74.53 74.04 78.33 76.56 75.91 72.81 79.87 80.07 80.20 79.19 73.60 70.33 68.87 80.44 75.49 72.15 70.90 81.33 84.85 84.55 84.16 82.60 86.11 85.62 85.33 82.67 87.22 86.90 86.86 80.41 83.60 81.66 80.89 81.75 84.80 83.03 82.20 MM-Vet total 31.1 33.0 32.5 31.8 36.1 37.6 36.8 36.7 33.7 34.5 34.7 34.0 25.2 25.7 25.4 25.5 21.2 25.4 25.5 25.5 Table 9: Sensitivity to modulation rate β. In NoLan-Plus, β is manually set to regulate the influence of the modulation distribution, defined in Equation 8. When β = 0, NoLan-Plus reverts to standard decoding. β 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1. Model LLaVA-1.5-7B LLaVA-1.5-7B LLaVA-1.5-7B LLaVA-1.5-7B LLaVA-1.5-7B LLaVA-1.5-7B LLaVA-1.5-13B LLaVA-1.5-13B LLaVA-1.5-13B LLaVA-1.5-13B LLaVA-1.5-13B LLaVA-1.5-13B Qwen-VL Qwen-VL Qwen-VL Qwen-VL Qwen-VL Qwen-VL InstructBLIP-7B InstructBLIP-7B InstructBLIP-7B InstructBLIP-7B InstructBLIP-7B InstructBLIP-7B InstructBLIP-13B InstructBLIP-13B InstructBLIP-13B InstructBLIP-13B InstructBLIP-13B InstructBLIP-13B Accuracy 83.29 86.37 86.33 86.60 87.00 86.83 83.31 85.60 86.30 87.03 88.70 86.97 84.73 85.03 87.13 87.90 88.10 87.73 80.71 83.53 83.73 85.80 85.67 85.57 82.36 83.43 85.13 88.07 88.90 85.57 POPE Precision Recall 72.80 75.33 75.33 75.73 76.13 75.93 73.48 76.20 74.80 76.20 80.73 76.20 72.81 79.33 79.27 79.27 79.67 78.47 79.19 78.80 73.87 78.80 74.20 73.87 76.19 73.13 72.73 81.40 82.20 82.07 92.13 96.66 96.58 96.76 97.27 97.10 91.46 93.84 97.14 97.28 96.03 97.11 95.61 89.54 94.07 95.81 95.83 96.32 81.67 87.04 92.03 91.63 96.28 96.43 86.93 92.11 96.72 93.92 94.92 88.24 MME F1 Score MME-Hallu Existence Count 124.67 138.33 155.00 143.33 151.67 150.00 136.67 143.33 145.00 148.33 145.00 145.00 127.67 138.33 135.00 135.00 145.00 135.00 75.33 50.00 50.00 50.00 65.00 55.00 60.00 60.00 66.67 88.33 88.33 60. 175.67 180.00 190.00 190.00 190.00 190.00 185.00 190.00 190.00 190.00 190.00 190.00 155.00 170.00 165.00 170.00 185.00 185.00 141.00 175.00 180.00 170.00 180.00 165.00 160.00 180.00 178.33 180.00 180.00 180.00 565.33 588.33 626.67 645.00 660.00 631.67 616.67 620.00 646.67 630.00 656.67 646.67 587.33 626.67 613.33 618.33 648.33 628.33 380.33 408.33 413.33 396.67 460.00 406.67 440.00 441.67 440.00 488.33 503.33 436.67 81.33 84.68 84.64 84.97 85.42 85.22 81.49 84.11 84.52 85.46 87.72 85.39 82.67 84.13 86.03 86.76 87.00 86.48 80.41 82.72 81.95 84.73 83.81 83.65 81.20 81.53 83.03 87.21 88.10 85.04 Position Color 151.00 148.33 153.33 168.33 175.00 158.33 163.33 148.33 173.33 158.33 178.33 173.33 173.00 180.00 180.00 175.00 180.00 175.00 97.33 125.00 125.00 118.33 138.33 123.33 153.33 143.33 135.00 143.33 143.33 143.33 114.00 121.67 128.33 143.33 143.33 133.33 131.67 138.33 138.33 133.33 143.33 138.33 131.67 138.33 133.33 138.33 138.33 133.33 66.67 58.33 58.33 58.33 76.67 63.33 66.67 58.33 60.00 76.67 91.67 53.33 MM-Vet total 31.1 30.5 32.8 32.5 33.3 32.7 36.1 36.8 36.5 36.7 38.3 35.8 33.7 34.0 34.3 33.6 35.2 34.8 25.2 24.5 25.0 25.5 27.0 27.3 21.2 24.3 26.1 25.8 26.7 25. LVLMs model size. Our evaluation extends to the larger 13B variants of the LLaVA-1.5 [50] and InstructBLIP [15], examining the scalability of our proposed NoLan across different LVLM sizes and architectures. Table 11 illustrates that the 7B and 13B variants of LLaVA-1.5 and InstructBLIP deliver comparable performances across POPE settings (e.g., F1 scores of 78.36 and 78.35 for InstructBLIP 7B and 13B in the Popular setting), indicating that increasing model parameters alone does not inherently resolve hallucination issues. Notably, NoLan uniformly exceeds the regular method in every evaluated case. Its improvements are particularly pronounced with larger models. These results highlight NoLans effectiveness and robustness across varying model scales and architectures. Variations of NoLan-Plus. We incorporate the tanh function into the Kullback-Leibler Divergencebased function of NoLan-Plus, as defined in Equation 8. Given the similar mathematical properties of the sigmoid and tanh functions, we conduct an in-depth analysis of both to evaluate their efficiency and generalization potential. As illustrated in Table 12, tanh consistently outperforms sigmoid in the majority of evaluated scenarios, showcasing its enhanced effectiveness. While sigmoid achieves similar improvements on LLaVA, tanh demonstrates superior performance on Qwen-VL 18 Table 10: Ablation studies for components of decoding logits and model sizes. For LLaVA-1.5 on MSCOCO of POPE-random, the performance drops significantly with only multimodal or text-only logits. Additionally, the performance gap between NoLan and other decoding methods increases as the model size grows. Decoding logits MSCOCO of POPE-random Accuracy Precision Recall F1 Score lm + α(lm lu) lm lu lm + α(lm lu) lm lu NoLan-Plus NoLan-Base multimodal text-only NoLan-Plus NoLan-Base multimodal text-only LLaVA-1.5-7B 87.00 86.50 83.29 47.57 LLaVA-1.5-13B 88.70 87.37 83.31 49.43 97.27 96.68 92.13 47.45 96.03 95.61 91.46 49.13 76.13 75.60 72.80 45.33 80.73 78.33 73.48 32.00 85.42 84.85 81.33 46. 87.72 86.11 81.49 38.76 Table 11: Ablation study for LVLMs model sizes on MSCOCO of POPE. Scaling up LVLM model sizes does not significantly mitigate object hallucinations. In contrast, NoLan consistently enhances model performance. Dataset POPE Model Random MSCOCO Popular Adversarial LLaVA1.5-7B LLaVA1.5-13B InstructBLIP-7B InstructBLIP-13B LLaVA1.5-7B LLaVA1.5-13B InstructBLIP-7B InstructBLIP-13B LLaVA1.5-7B LLaVA1.5-13B InstructBLIP-7B InstructBLIP-13B Decoding Regular NoLan-Base NoLan-Plus Regular NoLan-Base NoLan-Plus Regular NoLan-Base NoLan-Plus Regular NoLan-Base NoLan-Plus Regular NoLan-Base NoLan-Plus Regular NoLan-Base NoLan-Plus Regular NoLan-Base NoLan-Plus Regular NoLan-Base NoLan-Plus Regular NoLan-Base NoLan-Plus Regular NoLan-Base NoLan-Plus Regular NoLan-Base NoLan-Plus Regular NoLan-Base NoLan-Plus Accuracy 83.29 86.50 87.00 83.31 87.37 88.70 Precision 92.13 96.68 97.27 91.46 95.61 96.03 80.71 86.07 85.67 82.36 88.63 88. 81.88 85.13 85.60 82.47 86.23 87.40 78.22 83.97 84.00 79.07 85.57 85.90 78.96 83.00 83.60 80.00 83.87 84.90 75.84 81.97 82.37 76.57 82.60 82.97 81.67 95.54 96.28 86.93 93.92 94.92 88.93 93.41 93.91 89.55 92.76 93. 77.87 90.73 92.29 81.11 87.78 88.77 83.06 88.73 89.31 84.46 87.80 88.07 74.30 86.58 88.62 77.00 82.71 83.57 Recall 72.80 75.60 76.13 73.48 78.33 80.73 79.19 75.67 74.20 76.19 81.40 82.20 72.80 75.60 76.13 73.53 78.60 80. 78.85 75.67 74.20 75.79 81.40 82.20 72.75 75.60 76.33 73.53 78.67 80.73 79.03 75.67 74.27 75.79 81.33 82.07 F1 Score 81.33 84.85 85.42 81.49 86.11 87.72 80.41 84.45 83.81 81.20 87.83 88.10 80.06 83.57 84.09 80.75 85.10 86. 78.36 82.52 82.26 78.35 85.04 85.36 77.57 81.64 82.31 78.62 82.98 84.24 76.59 80.75 80.81 76.39 82.48 82.81 and InstructBLIP, underscoring its greater adaptability and broader generalization capability. This may be due to the faster convergence of the tanh function, allowing the moderation term to approach its upper bound more quickly under the imposed constraints, thereby more effectively mitigating the influence of linguistic priors. A.5 Benchmarking NoLan against the ICD baseline As member of the contrastive decoding family of methods, Instruction Contrastive Decoding (ICD) [77] introduces special mechanism into multimodal inference by injecting carefully crafted disturbance instructions during decoding. According to its process, ICD augments the input with misleading prompt (e.g., You are confused object detector) to intentionally increase alignment uncertainty. This yields two distributions: one conditioned on the standard instruction and another on 19 Table 12: Results of NoLan-Pluss variants on POPE [40]. Sigmoid refers to the use of the Sigmoid function as the processing term, while Tanh denotes the use of the Tanh function for the same purpose.The best performances within each setting are bolded. Dataset Model Function LLaVA1. GQA Qwen-VL InstructBLIP LLaVA1.5 A-OKVQA Qwen-VL InstructBLIP LLaVA1.5 MSCOCO Qwen-VL InstructBLIP Sigmoid Tanh Sigmoid Tanh Sigmoid Tanh Sigmoid Tanh Sigmoid Tanh Sigmoid Tanh Sigmoid Tanh Sigmoid Tanh Sigmoid Tanh Random Popular Adversarial Accuracy 88.53 88.57 86.83 87.27 84.47 86.13 88.03 88.00 88.63 89.37 86.53 88.20 87.03 87.00 86.97 88.10 85.53 85.67 F1 Score 87.83 87.88 86.44 86.99 84.62 85.23 87.38 87.30 88.04 89.03 86.60 87.55 85.46 85.42 85.51 87.00 84.54 83.81 Accuracy 84.53 84.57 82.20 83.20 77.90 81.13 86.03 85.70 87.87 87.97 81.23 84.57 85.63 85.60 86.73 87.43 83.33 84. F1 Score 84.13 84.31 82.42 83.62 79.45 80.92 85.58 85.22 87.41 87.72 82.27 84.32 84.14 84.09 85.40 86.40 82.60 82.26 Accuracy 81.47 81.50 80.10 80.20 74.60 78.10 79.87 79.47 80.93 81.20 74.27 78.43 83.63 83.60 84.37 84.90 80.63 82.37 F1 Score 81.54 81.88 80.76 81.05 77.03 78.41 80.39 80.01 81.44 82.06 77.11 79.24 82.36 82.31 83.20 84.07 80.34 80.81 All 84.67 84.79 83.13 83.72 79.68 81.65 84.55 84.28 85.72 86.23 81.34 83.72 84.71 84.67 85.36 86.32 82.83 83.15 Table 13: Results of InstructBLIP on POPE [40]. Regular decoding denotes direct sampling, VCD [33] indicates sampling from visual contrastive distribution, ICD [77] expresses using Instruction Contrastive Decoding, while methods prefixed with NoLan refers to sampling from our proposed contrastive distribution pnolan. The best performances within each setting are bolded. Random Adversarial Popular Dataset Decoding GQA A-OKVQA MSCOCO Regular VCD ICD NoLan-Base (Ours) NoLan-Plus (Ours) Regular VCD ICD NoLan-Base (Ours) NoLan-Plus (Ours) Regular VCD ICD NoLan-Base (Ours) NoLan-Plus (Ours) Accuracy 79.65 83.69 85.10 85.63 86.13 80.91 84.11 85.82 87.87 88.20 80.71 84.53 86.43 86.07 85.67 F1 Score Accuracy F1 Score Accuracy 80.56 84.16 85.29 85.04 85.23 81.86 84.56 86.29 87.46 87.55 80.41 83.68 85.61 84.45 83.81 73.87 78.57 78.50 79.60 81.13 76.19 79.78 81.64 83.60 84.57 78.22 81.47 82.93 83.97 84. 76.42 80.17 80.87 80.01 80.92 78.17 81.15 83.32 83.76 84.32 78.36 81.07 82.55 82.52 82.26 70.56 75.08 75.17 76.97 78.10 70.71 74.33 74.42 77.33 78.43 75.84 79.56 80.87 81.97 82.37 F1 Score 74.12 77.53 77.65 77.99 78.41 75.56 77.19 78.48 78.79 79.24 76.59 79.52 80.84 80.75 80.81 the disturbed version. By subtracting the latter from the former, ICD aims to suppress hallucinated concepts that are overactivated by visual priors, thus enhancing prediction robustness. In this section, we provide supplementary evaluation of the ICD baseline and compare its performance with our proposed method built upon InstructBLIP. Table 13 presents detailed comparison across three datasets (GQA, A-OKVQA, and MSCOCO), covering random, popular, and adversarial question categories. The results demonstrate that while ICD shows clear improvements over standard decoding and VCD [33], our NoLan variants consistently outperform it across most settings. A.6 Supplementary experiments MM-Vet. In addition to using POPE [40] for evaluation, we incorporate open-ended questions assessed with an LLM-based evaluator to deliver more thorough and comprehensive analysis of its performance. MM-Vet is an advanced benchmark designed to evaluate the capabilities of Large Multimodal Models (LMMs) in tackling complex multimodal tasks [86]. It defines 16 novel tasks of significant importance, derived from six core visual-language (VL) capabilities, and employs an LLM-based evaluator to assess the open-ended outputs of LMMs. To demonstrate the effectiveness of NoLan in open-ended generation tasks, we conducted comprehensive evaluation using the MM-Vet benchmark and its GPT-4 aided evaluator. This benchmark can test NoLans performance in scenarios requiring nuanced and contextually accurate multimodal understanding. 20 Table 14: MM-Vet [86] evaluation results regarding each core VL capability. All the numbers are presented in % and the full score is 100%. Our NoLan can improve performance for different models. Model LLaVA1.5-7B [50] LLaVA1.5-7B NoLan-Base LLaVA1.5-7B NoLan-Plus LLaVA1.5-13B [50] LLaVA1.5-13B NoLan-Base LLaVA1.5-13B NoLan-Plus InstructBLIP-7B InstructBLIP-7B NoLan-Base InstructBLIP-7B NoLan-Plus InstructBLIP-13B InstructBLIP-13B NoLan-Base InstructBLIP-13B NoLan-Plus Qwen-VL Qwen-VL NoLan-Base Qwen-VL NoLan-Plus Rec OCR Know Gen Spat Math 36.2 38.0 42.2 41.8 30.7 32.8 35.1 25.1 31.7 30.9 33.7 36.0 36.8 26.5 25.7 29.8 31.4 16.2 13.0 13.6 12.8 12.5 18.6 27.7 26.9 26.5 21.3 18.8 27.3 24.9 15.3 14.0 17.8 10.5 15.7 12.7 18.5 17.5 21.8 23.3 24.2 28.2 26.0 13.2 14.3 18.7 8.5 9.5 8.7 10.1 9.0 13.6 33.5 31.8 35.2 36.6 22.3 17.1 16.7 18.6 19.9 22.4 33.2 33.0 32.6 7.7 7.7 14.2 15.4 7.7 4.2 3.8 5.8 3.5 11.5 11.2 7.7 7. Total 31.1 33.00.1 33.30.2 36.1 37.60.2 38.30.2 25.20.0 25.70.1 27.00.1 21.20.3 25.40.2 26.70.1 33.70.1 34.50.1 35.20.2 Table 15: MM-Vet [86] evaluation results regarding each capability integration. Our NoLan can improve model performance for different models. Model LLaVA1.5-7B [50] LLaVA1.5-7B NoLan-Base LLaVA1.5-7B NoLan-Plus LLaVA1.5-13B [50] LLaVA1.5-13B NoLan-Base LLaVA1.5-13B NoLan-Plus InstructBLIP-7B InstructBLIP-7B NoLan-Base InstructBLIP-7B NoLan-Plus InstructBLIP-13B InstructBLIP-13B NoLan-Base InstructBLIP-13B NoLan-Plus Qwen-VL Qwen-VL NoLan-Base Qwen-VL NoLan-Plus Rec Know Gen 21.1 22.3 27.5 23.5 12.0 14.3 17.9 8.0 10.4 6.4 11.5 9.5 15.0 OCR Spat 34.6 29.8 28.5 34.7 13.4 13.5 11.5 19.2 13.5 15.4 42.3 44.2 40. Rec 62.2 68.9 70.0 73.5 63.7 73.7 73.5 58.1 67.6 72.7 76.9 83.2 80.5 OCR Spat Math 14.3 14.3 26.4 28.6 14.3 7.9 7.1 10.7 0.0 14.3 14.3 14.3 7.1 Rec Spat OCR 65.8 66.7 58.3 58.3 41.7 45.8 41.7 33.3 58.3 50.0 54.2 59.8 62.5 40.8 40.8 47.3 53.3 15.8 19.0 18.5 16.7 19.8 23.3 47.5 47.5 50.0 OCR Math 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 8.2 9.1 8.2 0.0 9. Rec Know 27.8 5.6 26.7 38.9 27.8 22.2 27.8 27.8 44.4 38.9 42.2 50.0 42.7 Rec OCR Know Gen 16.8 52.2 12.5 51.2 33.0 21.5 31.2 17.5 5.0 21.5 7.5 16.5 2.8 Rec OCR Gen Spat 43.8 12.5 54.2 16.2 5.2 11.5 16.8 4.2 9.8 15.2 1.2 0.5 14.8 Rec OCR Spat 14.3 14.3 17.1 14.3 14.3 14.3 14.3 14.3 28.6 14.3 14.3 14.3 14.3 OCR Know Spat 33.3 16.7 50.0 16.7 50.0 0.0 0.0 0.0 33.3 66.7 100.0 66.7 100.0 Rec Know Spat 0.0 0.0 50.0 50.0 50.0 0.0 0.0 50.0 50.0 0.0 50.0 50.0 50. OCR Gen Spat 37.0 20.0 12.0 40.0 5.0 0.0 0.0 5.0 0.0 4.0 14.0 0.0 10.0 Rec OCR 50.0 75.0 87.5 75.0 62.5 50.0 50.0 25.0 25.0 50.0 50.0 57.5 32.5 Rec OCR Spat Math 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 Total 31.1 33.00.1 33.30.2 36.1 37.60.2 38.30.2 25.20.0 25.70.1 27.00.1 21.20.3 25.40.2 26.70.1 33.70.1 34.50.1 35.20.2 As shown in Table 14, NoLan consistently outperforms regular decoding across both 7B and 13B models, highlighting its ability to enhance the open-ended generation capabilities of LVLMs. Notably, the findings also suggest that NoLans effectiveness scales with larger model sizes, delivering sustained improvements as models increase in complexity. For example, NoLan-Plus improves the performance of LLaVA-1.5 7B from 31.1 to 33.3, while the 13B model increases from 36.1 to 38.3. Additionally, as shown in Table 15, most capability integrations exhibit growth. For instance, the combination of Rec\" and Spat\" shows an increase of up to 8.3%. Furthermore, the results demonstrate that mitigating object hallucinations can positively impact open-ended generation capabilities. This result is not entirely unexpected, as the original model often generates content with hallucinatory effects in open-ended tasks. Previous experiments have demonstrated NoLans effectiveness in reducing hallucinations, reinforcing its ability to address this issue. Importantly, unlike the binary classification setting in POPE, the diversity of the generated content plays crucial role in this evaluation. Despite this added complexity, NoLan consistently achieves higher evaluation scores, demonstrating its ability to mitigate hallucinations while preserving the diversity of the models output. This balance allows the model to excel in open-ended question responses, showcasing NoLans capability to enhance both accuracy and content richness. Table 16: Results on MMHalBench [65] for different decoding strategies. NoLan variants improve the overall score and decrease the hallucination rate. Decoding Regular NoLan-Base NoLan-Plus MMHalBench Overall Score Hallucination Rate Attribute Adversarial Comparison Counting Relation Environment Holistic Other 1.83 1.08 2.00 76% 75% 68% 2.58 3.33 3.83 2.00 1.75 1.58 1.67 0.83 1.33 1.83 1.42 1.33 0 1.58 3.25 1.33 3.42 3. 1.55 1.85 2.29 1.17 1.42 1.58 21 MMHAL-BENCH. MMHAL-BENCH [65] is 96-pair benchmark that tests hallucination in large multimodal models across eight error types: wrong object attributes, nonexistent objects, faulty comparisons, counting errors, spatial mistakes, false environment inferences, misleading holistic descriptions, and misrecognition of text or icons. As shown in Table 16, we evaluate different decoding strategies on LLaVA-1.5 [50] using this challenging benchmark. The metrics include the overall score (higher is better) and hallucination rate (lower is better), as well as category-wise breakdowns. Both NoLan variants outperform regular decoding: NoLan-Base improves the overall score from 1.55 to 1.85, while NoLan-Plus further increases it to 2.29 and reduces the hallucination rate from 76% to 68%. These results highlight that suppressing language priors not only boosts semantic alignment but also reduces vulnerability to hallucination across diverse categories, with NoLan-Plus showing the strongest robustness against visual misinterpretation. Table 17: Results on HallusionBench [22] for different decoding strategies. NoLan variants improve overall accuracy (aAcc) and category-specific metrics. Decoding Regular NoLan-Base NoLan-Plus qAcc 14.2857 15.1648 18.6813 fAcc 15.6069 17.9191 19.6532 HallusionBench easyaAcc 37.1429 45.0549 43. hardaAcc 38.8372 35.5814 40.6977 aAcc 43.4898 46.5899 47.4756 HallusionBench. HallusionBench [22] is recently proposed diagnostic benchmark specifically designed to probe and quantify the failure modes of large vision-language models (LVLMs) in image-context reasoning. It consists of 1129 handcrafted visual-question-answer (VQA) pairs, built upon 346 distinct visual figuresincluding original and human-edited imagescovering wide range of domains such as geometry, food, statistics, maps, and visual illusions. Each question pair is designed to reveal inconsistencies or hallucinations in model predictions, going beyond traditional accuracy metrics to expose deeper reasoning flaws. As shown in Table 17, we evaluate different decoding strategies on LLaVA-1.5 [50] using this challenging benchmark. The metrics include qAcc (Question Pair Accuracy), fAcc (Figure Accuracy) over both easy and hard examples. Both NoLan variants outperform regular decoding across all metrics. In particular, NoLan-Plus achieves the highest question accuracy (qAcc: 18.68) and overall accuracy (aAcc: 47.48), suggesting improved robustness against hallucinations and visual misinterpretation. These results highlight that suppressing language priors not only enhances semantic alignment but also reduces model vulnerability to visually deceptive or noisy contexts, especially on hard cases (hardaAcc: 40.70). HallusionBench thus provides critical insights into the nuanced failure modes of LVLMs and demonstrates the effectiveness of contrastive decoding in mitigating them. Table 18: CircularEval results on MMBench [55] test set (L-2 abilities). NoLan variants improve overall and category-specific metrics. Decoding Regular NoLan-Base NoLan-Plus MMBench Overall 63.4 64.6 65.8 AR 77.6 76.0 74.7 CP 70.0 77.1 77. FP-C FP-S 68.0 57.7 66.3 56.7 67.1 55.1 LR 33.2 33.0 38.7 RR 56.2 53.6 60.2 MMBench. MMBench [55] is systematically constructed benchmark designed to evaluate wide range of vision-language capabilities across 20 distinct ability dimensions, such as object localization, commonsense reasoning, and social understanding. Each ability is uniformly represented by over 125 multiple-choice questions, enabling balanced and fine-grained assessment. To address inconsistencies caused by VLMs limited instruction-following capabilities, the benchmark employs GPT-4 as robust choice extractor, achieving 91.5% alignment with human judgment. To further improve evaluation robustness, MMBench introduces CircularEvala strategy designed to reduce bias and variance in performance assessment by aggregating multiple sampling and evaluation rounds. This method emphasizes consistency across ability dimensions and mitigates artifacts from instruction misalignment or label mismatch. As shown in Table 18, we evaluate decoding strategies on LLaVA-1.5 [50] using the CircularEval protocol. Both NoLan variants outperform regular decoding in overall accuracy and several reasoningspecific dimensions. In particular, NoLan-Plus achieves the highest overall score (65.8) and shows notable improvements in Coarse Perception (CP: 77.5) and Relation Reasoning (RR: 60.2), alongside gains in Logical Reasoning (LR: 38.7). These dimensionsabbreviated in Table 18 as CP, RR, and LRcorrespond to L-2 level cognitive skills, which demand deeper visual-semantic understanding. These results indicate that suppressing language priors not only benefits general performance but also enhances high-level reasoning under rigorous evaluation settings like CircularEval. Moreover, the improved consistency across fine-grained and relational tasks suggests better grounding and reduced over-reliance on textual shortcuts. Table 19: Results on MathVision [74] for different decoding strategies. NoLan variants improve overall and most sub-categories, such as Algebra (Alg), Geometry (e.g., Angle, Area), and Logical reasoning (Log). Decoding Random Chance Regular NoLan-Base NoLan-Plus ALL Alg AnaG Ari CombG Comb Cnt DescG GrphT Log Angle Area 9.40 7.17 10.20 8.52 10.60 9.34 8.60 9.84 22.10 7.70 13.46 17.31 0.60 15.60 13.29 13.29 1.10 10.00 14.44 17.78 9.70 7.10 11.36 11.04 11.90 7.10 4.76 8. 6.00 10.50 8.96 5.97 7.10 10.70 5.71 7.14 4.80 4.80 7.14 6.55 7.60 9.20 7.56 7.56 1.50 7.00 5.22 6.96 Len 6.70 9.80 10.02 9. SolG Stat Topo TransG 8.20 5.30 6.15 9.43 13.00 4.40 4.35 13.04 7.10 4.80 10.71 13.10 8.60 8.60 17.24 13.79 MathVision MathVision. MathVision [74] (MATH-V) is curated benchmark designed to assess the mathematical reasoning capabilities of large multimodal models in visually grounded settings. The dataset consists of 3,040 high-quality visual math problems spanning 16 mathematical disciplines and 5 difficulty levels, covering topics such as algebra, combinatorial geometry, topology, and logic. Problems are sourced from 19 official math competitions and are annotated and verified by domain experts to ensure uniqueness and correctness of answers. The benchmark contains both multiple-choice and open-ended formats, requiring models to perform fine-grained multimodal understanding and symbolic reasoning. As illustrated in Table 19, we compare decoding strategies on LLaVA-1.5 [50] across all subject areas. Both NoLan variants significantly outperform the regular baseline in overall performance (ALL), with NoLan-Plus achieving the best accuracy (9.84%). Improvements are especially prominent in core areas such as Algebra (Alg: 6.96), Graph Theory (GrphT: 17.78), and metric geometry - angle (Angle: 13.29), all of which require both precise visual perception and subject-specific mathematical reasoning. These results demonstrate that suppressing language priors helps reduce superficial biases and encourages more deliberate reasoning. MathVision thus reveals the benefits of contrastive decoding in tackling symbolically grounded, visually rich tasks where hallucinations and template-like answers are common failure modes for conventional VLMs. A.7 Contrasting NoLan with attention-based approaches Table 20: Comparison between NoLan and attention-based methods Decoding Regular OPERA [26] PAI [53] NoLan-Base (Ours) NoLan-Plus (Ours) MSCOCO of POPE-random Accuracy 83.29 86.33 87.80 88.80 F1 Score 81.33 85.40 85.89 85.60 86.70 While our main analysis focuses on contrastive decoding strategies, several recent methods adopt alternative training-free techniques to mitigate hallucinations by intervening in the attention mechanism. Among them, Pay Attention to Image (PAI)[53] and OPERA[26] stand out as representative and competitive approaches. 23 PAI operates by amplifying attention weights directed toward image tokens during inference. It adjusts the self-attention heads in the decoder layers to emphasize image regions in their original direction, thereby reducing reliance on language priors. In addition, PAI constructs auxiliary textual prompts (comprising instructions and historical responses) and subtracts their logits from the imageconditioned logits. This dual intervention strategy encourages more image-grounded reasoning while suppressing text inertia. Importantly, PAI is fully training-free and directly targets two key issues: image neglect and language dominance. OPERA, on the other hand, addresses the over-trust phenomenon in beam search decoding. It introduces column-wise metric over the attention map to detect knowledge aggregation patterns that correlate with hallucination. penalty score is integrated with the logits during candidate selection, disfavoring over-trusted tokens. Additionally, OPERA employs retrospection-reallocation mechanism that can roll back to previous decoding positions if over-trust is detected, enabling the model to reallocate attention and choose alternative candidates. As shown in Table 20, we compare these methods on LLaVA-1.5 [50] using the MSCOCO of the POPE benchmark. Both PAI and OPERA achieve strong results, with F1 scores of 85.89 and 85.40, respectively. Our NoLan-Plus further improves on these with the highest accuracy (88.80) and F1 score (86.70), demonstrating that contrastive decoding with language prior suppression remains highly effective strategy. These results suggest that while attention-based methods offer promising avenues, contrastive decoding offers more general and robust framework for hallucination mitigation, especially when the distributional shift is carefully controlled by leveraging the difference between dual forward outputs. A.8 Qwen-VL series Table 21: Results of Qwen2-VL [75] and Qwen2.5-VL [5] on POPE [40]. Accuracy MSCOCO of POPE-random Precision Recall F1 Score Decoding Regular NoLan-Base NoLan-Plus Regular NoLan-Base NoLan-Plus Qwen2-VL-2B 76.76 85.83 79.64 71.27 73.93 77. Qwen2-VL-7B 96.58 98.18 97.38 87.27 88.90 89.80 Qwen2.5-VL-3B Regular NoLan-Base NoLan-Plus 87.27 88.57 90.67 91.78 97.85 93. Qwen2.5-VL-7B Regular NoLan-Base NoLan-Plus 83.70 87.40 88.63 99.22 98.36 92.89 61.00 57.33 74.33 77.27 79.27 81. 81.87 78.87 87.20 67.93 76.07 83.67 67.98 68.75 76.90 85.85 87.72 88.91 86.54 87.34 90.33 80.65 85.79 88. Qwen2-VL [75] and its successor Qwen2.5-VL [5] are recent multimodal large language model families that unify image, text, and video processing through dynamic resolution mechanism and multimodal rotary position embedding (M-RoPE). The series scales across parameter sizes from 2B to 72B, with Qwen2.5 introducing architectural refinements for stronger visuallanguage alignment. Table 21 reports POPE results with NoLan. On Qwen2-VL-2B, NoLan-Plus improves F1 from 67.98 to 76.90, while on Qwen2-VL-7B it raises F1 from 85.85 to 88.91. Similar trends hold for Qwen2.5-VL: NoLan-Plus boosts F1 from 86.54 to 90.33 on the 3B model and from 80.65 to 88.04 on the 7B model, with substantial recall gains. These consistent improvements across scales and generations demonstrate the robustness of NoLan in enhancing visual grounding. These results highlight the generality and scalability of our contrastive decoding approach: even when integrated with advanced architectures like Qwen2-VL and Qwen2.5-VL, NoLan continues to 24 effectively suppress language priors and enhance grounding, particularly in challenging settings like POPE where precise visual grounding is essential. A.9 Consumption of inference Table 22: Inference efficiency comparison of contrastive decoding strategies. Decoding Regular VCD VDD NoLan-Base NoLan-Plus Seconds per Token Memory Usage (GB, 50 tokens) 0.4579 0.7537 0.7359 0.6075 0.6277 13.57 15.09 15.09 13.59 13.59 We compare the inference efficiency of NoLan and contrastive decoding baselines (VCD, VDD) in terms of computation time and memory usage. As shown in Table 22, VCD and VDD require two forward passes over inputs (v, x) and (v, x), along with additional post-processing using adaptive plausibility constraints. In contrast, NoLan simplifies this process by using only (v, x) and (x) as inputs. NoLan-Base requires no post-processing, and NoLan-Plus adds only lightweight KL divergence computation, making both significantly more efficient. Empirical results on LLaVA-v1.5-7B with Titan RTX 24GB GPU confirm the efficiency of NoLan: among contrastive decoding methods, NoLan-Base achieves the fastest inference speed (0.6075 seconds per token) and the lowest memory usage (13.59 GB for 50 tokens). In comparison, VCD and VDD are both slower and more memory-intensive (15.09 GB), underscoring NoLans practical advantages in latency and resource efficiency. A.10 Ethics and reproducibility statements Ethics statement. Our research adheres to the ICLR Code of Ethics. The primary focus of our work is to mitigate object hallucinations in Large Vision-Language Models (LVLMs). Object hallucination, phenomenon where models generate text describing objects that are either mismatched or entirely absent in an image, poses significant ethical concern. Such fabrications can lead to the spread of misinformation and reduce the reliability of AI systems in critical applications. Our proposed method, NoLan, contributes to the development of more trustworthy and factual AI by directly addressing this issue. By suppressing the language priors that we identify as principal cause of hallucinations, NoLan improves the accuracy and faithfulness of LVLM outputs. This can have positive societal benefits by making these models safer and more reliable for public use. The datasets and models used in our experiments are publicly available, and our research does not involve any personally identifiable information or sensitive data. We will make our code publicly available to encourage transparency and allow for further research in this area. We are not aware of any direct negative social impacts or ethical concerns arising from our work. We believe that by improving the factuality of LVLMs, our work represents positive step towards more ethical and responsible AI. Reproducibility statement. To ensure full reproducibility, we will make our complete source code publicly available. This repository contains the implementation of our NoLan framework, alongside all scripts necessary to replicate our experiments and evaluations against the reported baselines (Regular, VCD, M3ID, and VDD). Our experiments are conducted on publicly accessible LVLMs, including the LLaVA-1.5, InstructBLIP, and Qwen-VL series, using widely-adopted benchmarks. Specifically, we use POPE, MME, and LLaVA-Bench in the main paper, with extended evaluations on MM-Vet, MMHAL-BENCH, and MMBench, among others, in the appendix. As detailed in Section 4, our experimental setup, including dataset-specific configurations, aligns with prior work for fair comparison. For our mechanism in NoLan-Plus variant, complete theoretical proof is also provided in the appendix. This comprehensive release is intended to allow the community to easily verify our findings and build upon our work. A.11 More case studies To further validate the impact and effectiveness of our proposed NoLan-Plus on open-ended generation tasks, we conduct additional case studies on the LLaVA-bench. Figure 5 provides further instances of 25 Figure 5: More examples from LLaVA-Bench of our proposed NoLan-Plus for hallucination corrections. Hallucinated objects from LVLMs regular decoding are highlighted in red. hallucination corrections by NoLan-Plus. In the examples presented, objects such as thinking face\" and question mark,\" which are commonly linked to the ground truth object funny joke,\" mistakenly appear as hallucinations in the generated output. In contrast, applying NoLan-Plus substantially reduces these hallucinations while maintaining the consistency and richness of the generated text. This highlights NoLan-Pluss ability to produce outputs that are more faithfully aligned with the visual input, without compromising informativeness. Additional examples are provided in Figure 6. 26 Figure 6: More examples from LLaVA-Bench of our proposed NoLan-Plus for hallucination corrections. Hallucinated objects from LVLMs regular decoding are highlighted in red."
        }
    ],
    "affiliations": [
        "National University of Singapore, Singapore",
        "Peking University Shenzhen Graduate School, China"
    ]
}