{
    "paper_title": "Synthetic Video Enhances Physical Fidelity in Video Synthesis",
    "authors": [
        "Qi Zhao",
        "Xingyu Ni",
        "Ziyu Wang",
        "Feng Cheng",
        "Ziyan Yang",
        "Lu Jiang",
        "Bohan Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We investigate how to enhance the physical fidelity of video generation models by leveraging synthetic videos derived from computer graphics pipelines. These rendered videos respect real-world physics, such as maintaining 3D consistency, and serve as a valuable resource that can potentially improve video generation models. To harness this potential, we propose a solution that curates and integrates synthetic data while introducing a method to transfer its physical realism to the model, significantly reducing unwanted artifacts. Through experiments on three representative tasks emphasizing physical consistency, we demonstrate its efficacy in enhancing physical fidelity. While our model still lacks a deep understanding of physics, our work offers one of the first empirical demonstrations that synthetic video enhances physical fidelity in video synthesis. Website: https://kevinz8866.github.io/simulation/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . e [ 1 2 2 8 0 2 . 3 0 5 2 : r a"
        },
        {
            "title": "Synthetic Video Enhances Physical Fidelity in Video Synthesis",
            "content": "Qi Zhao1 Xingyu Ni2,1 Ziyu Wang3,1 Feng Cheng1 Ziyan Yang1 Lu Jiang1* Bohan Wang4* 1ByteDance Seed 2Peking University 3ShanghaiTech University 4National University of Singapore Figure 1. Our synthetic-data-enhanced video generation model is capable of producing videos depicting human dancing (rows 1), scenes featuring large camera orbiting around the object (row 2), and animals against solid-color backgrounds for matting (row 3)."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction We investigate how to enhance the physical fidelity of video generation models by leveraging synthetic videos derived from computer graphics pipelines. These rendered videos respect real-world physics, such as maintaining 3D consistency, and serve as valuable resource that can potentially improve video generation models. To harness this potential, we propose solution that curates and integrates synthetic data while introducing method to transfer its physical realism to the model, significantly reducing unwanted artifacts. Through experiments on three representative tasks emphasizing physical consistency, we demonstrate its efficacy in enhancing physical fidelity. While our model still lacks deep understanding of physics, our work offers one of the first empirical demonstrations that synthetic video enhances physical fidelity in video synthesis. Website: https://kevinz8866.github.io/simulation/ *Corresponding author 1 Video generation models [9, 17, 25, 35, 36] have demonstrated strong capabilities in producing high-quality and visually compelling videos of real-world scenarios. Despite their remarkable progress, these generation videos often struggle to respect the underlying physical laws of the real world, indicating significant gap in applications where physical fidelity is essential [30, 65, 66]. For instance, while video generation model can generate realistic-looking objects or humans within scene, it may fail to maintain 3D consistency when the camera moves or when the subjects undergo deformation. In this paper, we explore whether synthetically generated videos can enhance the physical fidelity of video generation models. Specifically, we utilize synthetic videos rendered through modern computer-generated imagery (CGI) production pipelines used in gaming and film, such as Blender [59] and Unreal Engine [19]. By utilizing standard computer graphics techniques, we can generate highquality, physically consistent video content at scale. CGI production pipelines generate videos via precise 3D asset modeling, animation, and rendering based on predetermined physical rules [13]. This approach allows for highly accurate scene configuration and ensures that the rendered videos intrinsically respect real-world physics, provided the setups and parameters are properly specified. As such, synthetic video is highly configurable, allowing precise control over scene setup, objects, and motion. Additionally, ground-truth descriptions can be easily obtained based on the specifications of the 3D environment. However, training video generation models using synthetic video data presents several challenges. Synthetic videos inherit an appearance gap, making them easily distinguishable from real videos. Further, the limited availability of 3D assets, together with the complexity of their composition, restricts the diversity of synthetic video content. As result, leveraging synthetic video to enhance model understanding remains an active area of research [32, 39, 40]. Regarding video generation, to the best of our knowledge, no prior work has specifically explored the use of synthetic videos to enhance video generation models. Therefore, we present an investigation into how synthetic video enhances the physical fidelity of video generation models. As pilot study, we examine three representative tasks known to be challenging even for state-of-the-art video generation models. Figure 1 illustrates their generated videos which include: 1) Large human motion generation, where significant movements cause noticeable shape deformations in body parts, such as breakdance or backflip. 2) Wide-angle camera rotation, where the camera spins around specific axis, capturing broader field of view of the object or actions. 3) Video layer decomposition, where the model must generate subject or motion against green screen background. This task evaluates whether the model can effectively disentangle the subject from the background during generation. These tasks are not exhaustive but serve as reasonable starting point for studying physical fidelity in video generation. We propose solution that uses synthetic videos to enhance video generation models. At the data level, based on computer graphics techniques, we begin by constructing synthetic video generation pipeline that offers diverse scene configurations, assets, and animations. Next, we explore the curation and integration of synthetic videos to transfer their physical fidelity to the video generation model. Through extensive analysis and ablations, we identify key factors that govern how well synthetic videos transfer physical fidelity to real-world video generation, including visual distribution, asset quality, rendering quality, the role of synthetic captions and the best blending strategy of synthetic videos with their real counterparts. At the model level, we propose novel approach SimDrop to reduce the introduction of undesirable rendering artifacts into the final generation model by training synthetic reference model that solely captures the visual patterns of synthetic video data. We show that with classifierfree guidance [27], the reference model can work in auxiliary with the generation model to remove the visual artifacts from synthetic data but keeps the physical fidelity. To verify the effectiveness of our solution, we employ two measurements inspired by related works [1, 31], assessing fidelity in terms of 3D consistency and human pose integrity. While these measurements are not perfect, they offer meaningful indicators of the physical fidelity of video generation. Additionally, human evaluations are incorporated to ensure alignment with human perception. Our experiments demonstrates that by carefully crafting and integrating synthetic video data, video generation models can significantly reduce collapse and distortion in human motion and improve 3D consistency [1] of objects under large camera movements. Moreover, our approach enables models to generate backgrounds of uniform color while maintaining clearly separated, dynamically moving objects in the foreground. It is worth noting that while our model improves physical fidelity, it still lacks an understanding of the underlying principles of physics, leaving significant room for further improvement In summary, we make the following contributions: We present computer graphics-based synthesis pipeline to generate videos for training video generation models. We identify key factors in curating synthetic video data and propose strategies for effectively training video generation models on these datasets. To the best of our knowledge, our work provides one of the first empirical demonstrations that incorporating synthetic video data can improve the physical fidelity of video generation models. 2. Synthetic Video Generation using Computer"
        },
        {
            "title": "Graphics Techniques",
            "content": "Augmenting datasets with synthetic data has been widely adopted in the field of machine learning. Specifically, standard CGI production pipelines, such as those implemented in Blender [59] or Unreal Engine [19], have long been employed to synthesize highly controlled and visually realistic image and video data. By explicitly modeling objects, cameras, environments, and illumination, they offer fine-grained control over every aspect of scene, enabling the generation of large-scale, diverse, and visually realistic video datasets. Our data synthesis pipeline is built on such CGI production pipeline. We build procedural 3D scene generator driven by carefully chosen set of parameters, enabling diverse 3D scene generation. Then, we couple it with the open-source rendering engines Unreal Engine and Blender to generate high-quality video outputs. Based on the three aforementioned challenging tasks, we focus on generating videos containing single object per scene and aim to maximize diversity in both appearance and motion. Following 2 Figure 2. Visualization of the pipeline to augment video generation model with synthetic video data. We first plan the synthetic videos and generation descriptive tags for each elements (e.g. object, character, motion, etc). Then we combine the element descriptions to form the caption for synthetic videos. During training, we mix the synthetic videos with real-world video data to improve physics fidelity in challenging video generation tasks. standard practice, we consider 3D scene to include four key components: (1) the 3D object, (2) the camera, (3) the lighting conditions, and (4) the environment. Each component is fully customizable through set of predefined parameters, as detailed in Appendix A.1. Then, our pipeline automatically converts the parameters into 3D scene and renders them into video. Next, we will explain how we effectively sample the parameter space to achieve our goal. 3. Method Our goal is to investigate how data augmentation with synthetic videos can enhance video generation model to produce physically consistent videos. As pilot study, this paper focuses on three specific generation tasks, each representing challenging generation task even for stateof-the-art video generation models: large human motion, wide-angle camera rotation, and video layer decomposition. We assess quality primarily based on physical fidelity (see Sec. 3.4 for metric definition), rather than the commonly used visual fidelity or aesthetics. Training video generation models with synthetic video data presents challenges due to the distributional gap between synthetic and real videos. Our method addresses the gap between synthetic and real videos through three key techniques: data curation, captioning strategy, and novel training approach. Figure 2 provides an overview of our method, illustrating how these components work together to enhance video generation. In the following, Section 3.1 presents the curation of the synthetic video pixels. Section 3.2 explains how we caption the synthetic videos. Lastly, Section 3.3 details our strategy and method to incorporate the synthetic data. Training Data Human Motion Collapse Rate (a) Random (b) Forward shot only (c) Forward + following shot 87% 42% 23% Table 1. Randomly chosen camera configurations (a-b) lead to high collapse rate for generated videos. Using configuration (c) aligning with the real world greatly reduce the rate. 3.1. Curating Synthetic Pixels This section explores strategies for narrowing the gap between real and synthetic videos by refining synthesis configurations including camera, background, object, lighting, and other visual factors as well as study examining the impact of visual appearance brought by asset quality and rendering quality. Synthesis configurations Our generation tasks require producing videos that maintain 3D consistency for objects and ensure body coherence in human motion. To achieve this, we synthesize videos that emphasize these aspects by incorporating large object deformations (e.g., human dance) and significant camera rotations (e.g., orbiting around objects). Additionally, it is beneficial to incorporate characteristics of real videos such as common camera setups. For instance, professional videographers often capture subjects upper body from frontal angles when filming humans. To align with this practice, we ensure that significant portion of our synthetic data follows similar configurations. To demonstrate this, we examine the effectiveness of synthetic videos with different camera configurations: random, forward-shot only, frontal, and following shots. As shown in Table 1, we find that synthetic videos incorporating both forward and following shots, which closely align with real-world camera setups, significantly enhance the 3 Training Data Gym Layer Spin shot Default Low-quality asset Low-cost rendering 83.3% - 95% 92.5% 41.7% 17.5% 85% 22.5% - Table 2. Success rates illustrating how asset and rendering quality in synthetic videos affect physical fidelity. When asset or rendering quality is low, the physical fidelity in these synthetic videos is less likely to transfer effectively to video generation models. Figure 3. Visualizations of synthetic videos highlighting both goodand poor-quality 3D assets (a) and rendering (b). video generation model. This approach notably reduces the collapse rate defined as the proportion of generated videos that exhibit body collapse leading to more physically realistic outputs. We find that synthesizing objects against clean background allows the model to focus on the subject without diverting capacity to modeling the noisy backgrounds that are inevitable in most real videos. However, using monotonous background with little variation can lead to overfitting or undesirable associations between the background and the foreground objects. To address this, we adopt simple yet effective approach by incorporating diverse backgrounds with variations in color, texture, transparency, lighting conditions, and environments (e.g., indoor and outdoor settings). similar strategy is applied to the camera and object (see Appendix A.1). Empirically, we find that this increased diversity leads to stronger model performance, particularly in previously unseen scenarios. Appearance Gap Ideally, we would like the appearance of rendered video to match that of real videos. However, achieving this is challenging as the appearance gap arises from multiple factors. First, real videos are captured by physical cameras, which introduces imperfections such as lens distortions. Second, inaccuracies in the rendered materials and object shapes in virtual environments create additional discrepancies. Finally, rendering algorithms themselves approximate real-world lighting physics, further conIn principle, one could hire tributing to the mismatch. large team of skilled artists to overcome these discrepancies, but such process would be highly resource-intensive. To this end, we explore several rendering settings to balance this trade-off. Our experiments indicate that both lowquality 3D assets and low-cost rendering quality (Figure 3 top) significantly decrease the success rate of the generated videos, as shown in Table 2. When generating videos with spin shot, the success rate is greatly decreased. For the layer decomposition task, even though the success rate of generating pure color background remains high, the objects that appear in the output videos often look cartoonish (See Appendix A.2). Table 2 also illustrates that ensuring sufficient quality in both the 3D assets and the rendering settings (Figure 3 bottom) is essential to achieve high success rate. 3.2. Crafting Captions for Synthetic Videos Conventional pipelines for building large-scale videocaption datasets is to collect videos first and then generate captions using Vision-Language Models (VLMs). In contrast, as synthetic videos are created from cross combination of 3D objects, scenes, and camera movements during video synthesis, we caption each element separately and then merge the descriptions into final caption for the rendered video. This method is efficient and accurate: if we have objects, scenes, and camera setups, it requires only (N + + C) captions, whereas an existing approach would need to caption distinct videos. Such decomposition also improves accuracy and the granularity of the generated caption, as VLMs may produce inconsistent or vague descriptions when confronted with challenging lighting conditions or camera viewpoints in real videos. In contrast, our method ensures consistency by keeping descriptions for the same element regardless of final scene. As synthetic and real videos exhibit distinct visual characteristics, we hypothesize that embedding special tags (e.g., animated or rendered, as shown in Figure 11) within synthetic video captions helps the model distinguish the two domains and transfer only the desired physical fidelity into the generation. Through our ablations, we find that explicitly tagging synthetic data promotes more effective cross-domain knowledge transfer (See Sec. 4.3). 3.3. Training with Synthetic Videos We employ diffusion transformer model based on the MMDiT architecture [20], trained on real videos at native resolutions [14] within the latent space of variational autoencoder (VAE) [33]. The model is pretrained using the flow-matching objective. To enhance the physical fidelity for video generation, we explore incorporating synthetic video data. While training on mix of synthetic and real videos can improve fidelity, its effectiveness depends on the synthetic-to-real ratio and training steps. Too much synthetic data risks introducing artifacts, while too little yields minimal improvement. Similarly, excessive training can cause overfitting, whereas insufficient training fails to leverage synthetic data effectively. Even with well-tuned mixing ratio, synthetic video can still introduce distinctive patterns and artifacts in the generated outputs. To mitigate this, we draw inspiration from [21, 51, 57] and propose SimDrop. Based on [27], we can guide the diffusion generation process toward the overlapping distribution of synthetic and real videos while reducing the influence of synthetic artifacts. SimDrop begins by training reference model, Vσ, which aims to capture the unique patterns (e.g., blinkering, animated facials) of synthetic data that pair with rendering engines rather than the clearly defined visual concepts like objects or scenes. Therefore, in training the reference model, we build different captions that only ignores the desired aspect of the synthetic videos (e.g., human motion). This reference model then work in auxiliary with the generation model Vθ trained on mixture of synthetic and real data. Then the reference model can output only the visual patterns but not interfering the objects or human body formation in the video during inference. Formally, let lk denote the denoised latent at step k, and we have: lk = Vθ(lk1, t) αf (cid:0)Vσ, lk1, ˆt, ˆn(cid:1) + βf (cid:0)Vθ, lk1, t, n(cid:1), where t, ˆt (respectively n, ˆn) are positive (negative) prompts for the synthetic-mixed and reference models, and fθ(Vθ, l, t, n) = Vθ(l, t) Vθ(l, n). The terms α and β control the influence of each guidance. Using the special tags discussed in Section 3.2, we can incorporate them into the negative prompts. Adding such tags to negative prompts further offers additional benefits, albeit limited. 3.4. Evaluating Physical Fidelity Since there is no common standard on evaluating physical fidelity in videos, we adopt the following metrics to assess the physical fidelity, inspired by related work [1, 31, 53]. Although these quantitative metrics are not perfect, combining them with human evaluation can provide useful signals. Human pose estimation confidence We employ stateof-the-art human vision model, Sapiens [31], to evaluate the physical fidelity of the generated human motion. We use 2B-parameter, 17-keypoint checkpoint to estimate the pose of single-human motion outputs from each model on perframe basis. The average confidence score ϵconf per keypoint per frame ranges from 0 to 1. Based on the assumption of human vision models, motion sequence with more realistic body structures and clearer poses gives higher confidence score. 3D reconstruction error Based on widely used 3D sparse reconstruction tool, COLMAP [53, 54], we evaluate the physical fidelity of the static objects in the videos with large camera motion. Using single pinhole camera model and sequential feature matching mode, COLMAP reconstructs the scene from the video frames. Similar to the work of [1], we use the following metrics as indicators of physical fidelity: (1) the number of matched feature points (N ), (2) the average track length (T ), and (3) the average re-projection error (ϵ). In general, if the video frames generated by model provide greater diversity of camera viewpoints yet still maintain the 3D consistency, the number of matched feature points tends to increase. However, the mean track length of each feature point is expected to decrease due to the faster camera motion. Furthermore, model that is more physically consistent will yield lower re-projection error in the resulting 3D reconstruction. Human evaluation For each prompt, we generate two videos of different random seeds. For human evaluation, we instruct our annotators to examine the outputs from different models side-by-side for each prompt strictly following the guideline that focus on physical fidelity of the video. In general, successfully generated video refers to one that follow the text prompt without visible artifacts. For large human motion, we let the human annotators focus on the integrity of human body, such as limbs, hands, and neck, during the large motion. For large camera motion, the annotators will determine whether the camera motion is performed according to the prompt and examine the object quality. For the layer decomposition task, annotators judge the videos based on two criteria: object quality and background quality. The details of the guideline is in Appendix A.4. Afterwards, we compute the successful rate and average the results across all human evaluators. 4. Experiments To evaluate the effectiveness of synthetic videos for improving the physical fidelity of video generation models, we assess the trained model on three text-to-video tasks: (a) large human motion (dancing and gymnastics), (b) camera spin shots, and (c) layer decomposition (e.g., moving animal over solid-colored background). For each task, we use specific text prompts to test the models ability to accurately generate the video content. During evaluation, we focus on examining only the physical fidelity of the generated videos, and our criteria do not include aesthetics. 4.1. Implementation Details Synthetic Video Dataset Following the strategy we discussed in Sec. 3, we first render 32,847 videos of static objects with diverse camera movements and scene setups using Blender and 18,364 videos of humans performing diverse motions captured in simple indoor scenes with different background colors using Unreal Engine for the experiments. Additionally, we plan to release over 1.5M synthetic videos on static objects and 300K synthetic videos on human motions that are outside of the scope of this research to facilitate future research. Experiment Setup To verify whether synthetic videos can 5 Figure 4. Visualizations of the videos generated by our improved model, trained using synthetic data. Rows 1,2 highlight wide-angle camera motion; rows 3 display layer decomposition; and rows 4,5,6 demonstrate large human motion. benefit video generation models, we combine the resulting synthetic videos with real-world videos to train the video generation model with 8B parameters, pretrained with only real-world video data. In line with the tasks we aim to improve, we adopt the optimal strategy found in Sec. 3. To evaluate our trained model, we create 10 prompts each for gymnastics, dancing, camera motion, and layer decomposition, resulting in 40 prompts in total. Please refer to Appendix A.3 for details. We inference videos output with resolution of 1280720 and duration of 5s at 24 fps. We use the same negative prompts for all inference queries. We compare the outputs of our model trained with synthetic data against the outputs of both the original checkpoint and some of leading commercial models [9, 36, 52] at the same setting and follows the evaluation method in Sec. 3.4. 4.2. Results Large human motion Our model generates videos of humans performing dancing and gymnastics with significantly reduced limb collapse. As shown in Table 3, the user study indicates that our model produces fewer artifacts in generated videos compared with other models, including three leading generation models and our pretarined model named Base Model. In particular, our video generation model greatly improves the success rate of gymnastics movements, while other video generation models generate significantly fewer successful cases. The human pose estimation confidence scores, discussed in Sec. 3.4), further support the findings from the user study. Although the synthetic videos used for training have less realistic shading, the model still Model ϵconf User Study Gym Dance Gym Dance Kling 1.6 [36] Runway Gen-3α [52] Sora [9] Base Model Our Model 0.715 0.672 0.722 0.779 0.791 0.812 0.809 0.813 0.818 0.837 10% 4% 15% 9% 61% 43% 14% 44% 30% 86% Table 3. The average confidence score of human pose estimation and user study results on the large human motion task. Figure 5. Visualization of video frames with large human motion generated by our model. The shadow of human body follows the human motion. learns correct human body deformation from the synthetic data and preserves the base models realism. Figure 4 visualizes frames of our generated videos. We can see that our model produces visually plausible shadows, feature that other video generative models have struggled to achieve. Wide-angle camera rotation Our model can produce large 6 Model ϵproj ˆϵproj User Study"
        },
        {
            "title": "Model",
            "content": "Layer Decomposition 13,328 36.34 0.972 Kling 1.6 [36] Runway Gen-3α [52] 13,199 36.21 1.181 14,443 33.62 1.244 Sora [9] 16,548 31.84 1.159 Base Model 42,895 12.93 1.077 Our Model 0.298 0.361 0.318 0.437 0.135 20% 26% 25% 20% 80% Table 4. 3D reconstruction metrics and user study results on the large camera motion task. Note that the re-projection error ϵproj is computed over all extracted feature points, whereas ˆϵproj only considers the 1,000 points with the smallest error in each case. The latter metric offers fairer comparison for methods that produce significantly higher volume of feature points. camera spins around static objects and animals, as illustrated in Figure 4. Our training set contains abundant synthetic videos featuring such camera rotations around daily objects. However, the objects used in our testing prompts including food, animals, and landscapes lie entirely outside the distribution of our training data. Nonetheless, the model successfully learns the general concept of extensive camera movements and adapts it to previously unseen objects while preserving high level of realism. As reported in Table 4, the user study indicates that our models success rate in producing the intended camera motion is significantly higher than that of other methods. Furthermore, 3D reconstruction metrics, discussed in Sec. 3.4), confirm that objects generated by our model exhibit the strongest geometric consistency across different video frames. Our approach yields the largest number of feature points and the shortest track lengths, indicate that our videos have the largest camera motion and meanwhile maintains best 3D consistency. When these feature points are projected back onto 2D images, our models error ˆϵproj is more than twice as small compared with other approaches, demonstrating the enhanced physical fidelity of our generated videos. Layer decomposition As shown in Table 5, While the baseline models largely fail, our model can produce outputs with clear separation of the foreground object and the background when tasked to generate videos on pure color backgrounds. This decomposition is beneficial for compositing objects onto arbitrary backgrounds. Similarly to the large camera motion scenario, our model shows this capability to objects not present in the training dataset. Figure 4 shows an example in which the requested object appears cleanly over green background, suggesting that the model has learned to decompose the scene effectively. Furthermore, the model can even generate dynamic objects and human motion in layers, decomposing the scene in both spatial and temporal dimensions. Neither the original pretrained model nor other commercial models achieve such clear separation of layers. Kling-1.6 [36] Runway-gen3α [52] Sora [9] Base Model Our Model 4% 1% 4% 26% 84% Table 5. User study results on the layer decomposition task. With synthetic data augmentation, our model greatly outperforms leading commercial models and the original pretrained model. Caption Type Uprock Spin Freeze a) Generic b) Fine-grained 2% 98% 16% 84% 0% 66% Table 6. Fine-grained captions on human motion achieve better successful rate than generic captions on the large human motion task. Uprock, Spin, Freeze are particular dance moves. Caption Type Dance Move a) No Special Tags b) Special Tags c) Special Tags+Special NP 12.5% 90% 92.5% Table 7. Experiment results on the effect of special tags in synthetic data captioning. Without special tags to differentiate the visual style of the synthetic videos, the video generated models will more likely to generate animated characters or collapsed human motions after training. Also, adding the special tags in negative prompts during generation will help although marginally. 4.3. Ablation Studies Ablations on synthetic captions We perform experiments of different synthetic caption setups to verify our design in Sec. 3.2. Experiments in Table 6 studies if the finegrained captions help video generation model better learn human motion than their generic counterparts, typically from zero-shot VLM inference. We observe that for various dance moves (Uprock, Spin, Freeze), having fine-grained caption (Figure 11) greatly reduce the video generation model to generate videos that include collapse and distortion of human body during large motions. Table 7 summarizes the experiments on embedding special tags in synthetic captions to distinguish synthetic videos from the real videos. We found that without special tags, the video generation model is much likely to output videos of animated visual or collapsed human body. We further added the special tags in negative prompts, but found only marginal improvements. Ablations on training with synthetic videos To examine the mix rate of the synthetic and real videos. we perform the experiment summarized in Table 8. We found that higher mix rate share the same effect as longer training steps and 7 Percentage #Steps 5000 10000 15000 10% synthetic videos 50% synthetic videos 20% 25% 55% 75% 40% 60% 85% 80% Table 8. Ablation results on synthetic data mix rate and training steps. Here we measure the success rate which the trained foundation model generates videos that follows the prompts but does not include visual patterns in the synthetic videos. We found that large proportion and longer training steps help transferring the properties in synthetic videos to the video generation model. However, performance will saturate and failure cases will include visual patterns of synthetic data. over-training the model on synthetic data will not lead to more performance increase. Instead, more patterns from synthetic data will appear in the final output. We also verify the design of SimDrop in Table 9. We found that using the captions in training the reference model to prompt them will achieve the best result in terms of the visually preferred cases rated by humans. It also reports the impact of the hypereparameter α value. α 0.1 0."
        },
        {
            "title": "Bad",
            "content": "G-B 26.32% 71.05% 2.63% 23.69% 39.47% 52.63% 7.89% 31.58% Table 9. Experiment results on SimDrop. Here, we compare the output videos with SimDrop with the models without SimDrop. Evaluators will choose the best out of two videos side-by-side. We then compute the winning/same/losing rate against the baseline. generated videos after researchers evaluation [6, 30, 47]. Yet for video generation models, physics appears learnable directly from video data [11, 22, 45, 61] and is crucial for these foundation models to serve as world modthere are growing numels [1, 9, 16, 60]. Therefore, ber of works [3, 40] in improving physics-grounding in video generation and beyond [7, 44]. They mainly propose model modifications by adding additional supervisory signals [12, 29, 41, 63], and mainly tailored for certain aspect of physics such as motion [12, 42, 43] or sound in the videos [58]. While such methods show more physically coherent results, they often require modifications to the diffusion architecture itself and rely on manually specified control signals. Our work focuses on physical fidelity and differentiates by proposing data-centric approach without modifying the diffusion model architecture and harness the potential of 3D rendering engines [13]. Our method build synthetic video data that can benefit video generation models regardless of their architectures and improves on diverse aspects of physics fidelity. Synthetic data in AI Synthetic Data from simulation engines has been widely applied in advancing many fields of AI, such as autonomous driving [62, 74] and embodied agents [50, 56, 72], or scene generation [55]. At the intersection of synthetic data and video, most work focus on understanding [32, 64, 71] and only few early work [3] explore how synthetic video data can help video generation in particular tasks such as camera control [4] or motion [23, 42]. We are the first work to systematically study how synthetic videos from simulation engines can help improve the physics fidelity of video generation model. 5. Related Work 6. Conclusion Video generation Conditional video generation is challenging task aiming to synthesize temporally coherent and visually realistic video sequences from structured inputs such as images and text prompts. Current video generation models can be broadly categorized into Generative Adversarial Networks (GANs) [5, 8, 24, 38], autoregressive models [34, 37, 68, 69], and diffusion models [17, 26, 28, 35, 67, 73]. These architectures in video generation usually inherit their success in image generation [10, 20, 48, 70]. In recent years, rapid advancements in video generation, represented by Sora [9], have been significantly driven by the availability of large-scale webcollected video datasets and the development of scalable model architectures such as DiT [20]. State-of-the-art commercial models [25, 35, 36, 52] have demonstrated the ability to generate highly realistic videos. These models leverage extensive training data to improve motion fluency, scene reality and overall aesthetic quality in generating videos. Physics in video generation Despite the effort in scaling data and model size, problems remain in the physics of In this study, we investigate how to use synthetic video data generated by CGI production pipelines (Blender [59] and Unreal Engine [19]) to enhance physical fidelity of video generation models. We verify our method on three tasks necessitating realistic physical behavior, where our model achieves superior results through synthetic data enhancement. Our results demonstrate that the physical fidelity of video generation can be enhanced using synthetic video. Note that while our method improves physical fidelity and aligns more closely with human perception, it still lacks an understanding of the underlying principles of physics, leaving significant room for further improvement. Going beyond, future work may consider generating more intricate physical effects [40], including complex interactions among multiple objects and physically based fluid simulations. Moreover, while we only focus on the RGB color channel in this work, the synthetic rendering pipeline offer much more information(e.g., depth, normals, alpha masks) that could serve as supervisory signals, otherwise not easily obtainable in real datasets. 8 7. Acknowledgment We thank Ceyuan Yang, Liangke Gui, and Shanchuan Lin for their insightful discussions on this project. We also appreciate Zhibei Ma and Renfei Sun for their support in building the engineering foundation."
        },
        {
            "title": "References",
            "content": "[1] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world founarXiv preprint dation model platform for physical ai. arXiv:2501.03575, 2025. 2, 5, 8 [2] Autotroph. Blender market. blendermarket.com/, 2024. 12 https : / / [3] Yunhao Ba, Guangyuan Zhao, and Achuta Kadambi. BlendarXiv ing diverse physical priors with neural networks. preprint arXiv:1910.00201, 2019. [4] Jianhong Bai, Menghan Xia, Xintao Wang, Ziyang Yuan, Xiao Fu, Zuozhu Liu, Haoji Hu, Pengfei Wan, and Di Zhang. Syncammaster: Synchronizing multi-camera video generation from diverse viewpoints. arXiv preprint arXiv:2412.07760, 2024. 8 [5] Yogesh Balaji, Martin Renqiang Min, Bing Bai, Rama Chellappa, and Hans Peter Graf. Conditional gan with discriminative filter generation for text-to-video synthesis. In IJCAI, page 2, 2019. 8 [6] Hritik Bansal, Zongyu Lin, Tianyi Xie, Zeshun Zong, Michal Yarom, Yonatan Bitton, Chenfanfu Jiang, Yizhou Sun, KaiWei Chang, and Aditya Grover. Videophy: Evaluating physical commonsense for video generation. arXiv preprint arXiv:2406.03520, 2024. 8 [7] Daniel Bear, Elias Wang, Damian Mrowca, Felix Binder, Hsiao-Yu Fish Tung, RT Pramod, Cameron Holdaway, Sirui Tao, Kevin Smith, Fan-Yun Sun, et al. Physion: Evaluating physical prediction from vision in humans and machines. arXiv preprint arXiv:2106.08261, 2021. 8 [8] Tim Brooks, Janne Hellsten, Miika Aittala, Ting-Chun Wang, Timo Aila, Jaakko Lehtinen, Ming-Yu Liu, Alexei Efros, and Tero Karras. Generating long videos of dynamic scenes. Advances in Neural Information Processing Systems, 35:3176931781, 2022. 8 [9] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. https://openai.com/research/videogeneration - models - as - world - simulators, 2024. 1, 6, 7, [10] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1131511325, 2022. 8 [12] Hila Chefer, Uriel Singer, Amit Zohar, Yuval Kirstain, Adam Polyak, Yaniv Taigman, Lior Wolf, and Shelly Sheynin. Videojam: Joint appearance-motion representations for enhanced motion generation in video models. arXiv preprint arXiv:2502.02492, 2025. 8 [13] Celso De Melo, Antonio Torralba, Leonidas Guibas, James DiCarlo, Rama Chellappa, and Jessica Hodgins. Nextgeneration deep learning based on simulators and synthetic data. Trends in cognitive sciences, 26(2):174187, 2022. 1, 8 [14] Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Patch npack: Navit, vision transformer for any aspect ratio and resolution. Advances in Neural Information Processing Systems, 36:22522274, 2023. 4 [15] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: In Proceedings of universe of annotated 3d objects. the IEEE/CVF conference on computer vision and pattern recognition, pages 1314213153, 2023. 12 [16] Yilun Du, Sherry Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Josh Tenenbaum, Dale Schuurmans, and Pieter Abbeel. Learning universal policies via text-guided video generation. Advances in neural information processing systems, 36:91569172, 2023. [17] Abul Ehtesham, Saket Kumar, Aditi Singh, and Tala Talaei Khoei. Movie gen: Swot analysis of metas generative ai foundation model for transforming media generation, arXiv preprint advertising, and entertainment industries. arXiv:2412.03837, 2024. 1, 8 [18] Epic Games. https : / / www . unrealengine . com / en - US / metahuman, 2024. 12 Metahuman. [19] Epic Games. https : / / www . unrealengine.com/enUS/unrealengine5, 2024. 1, 2, 8, Unreal engine 5. [20] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 4, 8 [21] Felix Friedrich, Manuel Brack, Lukas Struppek, Dominik Hintersdorf, Patrick Schramowski, Sasha Luccioni, and Instructing text-toKristian Kersting. Fair diffusion: arXiv preprint image generation models on fairness. arXiv:2302.10893, 2023. 5 [22] Quentin Garrido, Nicolas Ballas, Mahmoud Assran, Adrien Bardes, Laurent Najman, Michael Rabbat, Emmanuel Dupoux, and Yann LeCun. Intuitive physics understanding emerges from self-supervised pretraining on natural videos. arXiv preprint arXiv:2502.11831, 2025. 8 [11] Pradyumna Chari, Chinmay Talegaonkar, Yunhao Ba, and Achuta Kadambi. Visual physics: Discovering physical laws from videos. arXiv preprint arXiv:1911.11893, 2019. 8 [23] Daniel Geng, Charles Herrmann, Junhwa Hur, Forrester Cole, Serena Zhang, Tobias Pfaff, Tatiana Lopez-Guevara, Carl Doersch, Yusuf Aytar, Michael Rubinstein, et al. Mo9 tion prompting: Controlling video generation with motion trajectories. arXiv preprint arXiv:2412.02700, 2024. 8 [24] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. 8 [25] Google. Veo2 - google deepmind. https://deepmind. google/technologies/veo/veo-2/, 2024. 1, 8 [26] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Fei-Fei Li, Irfan Essa, Lu Jiang, and Jose Lezama. Photorealistic video generation with diffusion models. In European Conference on Computer Vision, pages 393411. Springer, 2024. 8 [27] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. 2, 5 [28] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. 8 [29] Achuta Kadambi, Celso de Melo, Cho-Jui Hsieh, Mani Srivastava, and Stefano Soatto. Incorporating physics into datadriven computer vision. Nature Machine Intelligence, 5(6): 572580, 2023. [30] Bingyi Kang, Yang Yue, Rui Lu, Zhijie Lin, Yang Zhao, Kaixin Wang, Gao Huang, and Jiashi Feng. How far is video generation from world model: physical law perspective. arXiv preprint arXiv:2411.02385, 2024. 1, 8 [31] Rawal Khirodkar, Timur Bagautdinov, Julieta Martinez, Su Zhaoen, Austin James, Peter Selednik, Stuart Anderson, and Shunsuke Saito. Sapiens: Foundation for human vision modIn Proceedings of the 18th European Conference on els. Computer Vision, pages 206228, Berlin, Heidelberg, 2024. Springer-Verlag. 2, 5 [32] Yo-whan Kim, Samarth Mishra, SouYoung Jin, Rameswar Panda, Hilde Kuehne, Leonid Karlinsky, Venkatesh Saligrama, Kate Saenko, Aude Oliva, and Rogerio Feris. How transferable are video representations based on synthetic data? In Advances in Neural Information Processing Systems, pages 3571035723. Curran Associates, Inc., 2022. 2, 8 [33] Diederik Kingma, Max Welling, et al. Auto-encoding variational bayes, 2013. 4 [34] Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, et al. Videopoet: large language model for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023. [35] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 1, 8 [36] Kuaishou. Kling video model. https : / / kling . kuaishou.com/en, 2024. 1, 6, 7, 8 [37] Manoj Kumar, Mohammad Babaeizadeh, Dumitru Erhan, Chelsea Finn, Sergey Levine, Laurent Dinh, and Durk Kingma. model for stochastic video generation. arXiv:1903.01434, 2019. Videoflow: conditional flow-based arXiv preprint [38] Yitong Li, Zhe Gan, Yelong Shen, Jingjing Liu, Yu Cheng, Yuexin Wu, Lawrence Carin, David Carlson, and Jianfeng Gao. Storygan: sequential conditional gan for story visualization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 63296338, 2019. 8 [39] Junwei Liang, Lu Jiang, and Alexander Hauptmann. Simaug: Learning robust representations from simulation for trajectory prediction. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XIII 16, pages 275292. Springer, 2020. 2 [40] Daochang Liu, Junyu Zhang, Anh-Dung Dinh, Eunbyung Park, Shichao Zhang, and Chang Xu. Generative physical ai in vision: survey. arXiv preprint arXiv:2501.10928, 2025. 2, 8 [41] Shaowei Liu, Zhongzheng Ren, Saurabh Gupta, and Shenlong Wang. Physgen: Rigid-body physics-grounded imageto-video generation. In European Conference on Computer Vision (ECCV), 2024. 8 [42] Jiaxi Lv, Yi Huang, Mingfu Yan, Jiancheng Huang, Jianzhuang Liu, Yifan Liu, Yafei Wen, Xiaoxin Chen, and Shifeng Chen. Gpt4motion: Scripting physical motions in text-to-video generation via blender-oriented gpt planning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pages 14301440, 2024. [43] Joanna Materzynska, Josef Sivic, Eli Shechtman, Antonio Torralba, Richard Zhang, and Bryan Russell. Newmove: Customizing text-to-video models with novel motions. In Proceedings of the Asian Conference on Computer Vision, pages 16341651, 2024. 8 [44] Willi Menapace, Stephane Lathuili`ere, Aliaksandr Siarohin, Christian Theobalt, Sergey Tulyakov, Vladislav Golyanik, and Elisa Ricci. Playable environments: Video manipulation In Proceedings of the IEEE/CVF Conin space and time. ference on Computer Vision and Pattern Recognition, pages 35843593, 2022. 8 [45] Fanqing Meng, Jiaqi Liao, Xinyu Tan, Wenqi Shao, Quanfeng Lu, Kaipeng Zhang, Yu Cheng, Dianqi Li, Yu Qiao, and Ping Luo. Towards world simulator: Crafting physical commonsense-based benchmark for video generation. arXiv preprint arXiv:2410.05363, 2024. 8 [46] Meta Reality Labs Research. Digital twin catalog. https: //www.projectaria.com/datasets/dtc/, 2024. 12 [47] Saman Motamed, Laura Culp, Kevin Swersky, Priyank Jaini, and Robert Geirhos. Do generative video models learn physical principles from watching videos? arXiv preprint arXiv:2501.09038, 2025. 8 [48] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [49] Poly Haven. Poly haven. https://polyhaven.com/, 2024. 13 10 [50] Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Torralba. Virtualhome: Simulating household activities via programs. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 84948502, 2018. 8 [51] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In Proceedings of the IEEE/CVF conference generation. on computer vision and pattern recognition, pages 22500 22510, 2023. 5 [52] RunwayML. Gen-3 alpha. https://runwayml.com/ research/introducing-gen-3-alpha, 2024. 6, 7, [53] Johannes Lutz Schonberger and Jan-Michael Frahm. In Conference on ComStructure-from-motion revisited. puter Vision and Pattern Recognition (CVPR), 2016. 5 [54] Johannes Lutz Schonberger, Enliang Zheng, Marc Pollefeys, and Jan-Michael Frahm. Pixelwise view selection for unIn European Conference on structured multi-view stereo. Computer Vision (ECCV), 2016. 5 [55] Yu Shang, Yuming Lin, Yu Zheng, Hangyu Fan, Jingtao Ding, Jie Feng, Jiansheng Chen, Li Tian, and Yong Li. Urbanworld: An urban world model for 3d city generation. arXiv preprint arXiv:2407.11965, 2024. 8 [56] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. ALFRED: Benchmark for Interpreting In The IEEE Grounded Instructions for Everyday Tasks. Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 8 [57] Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro Chin, Irina Blok, Huiwen Chang, Jarred Barber, Lu Jiang, Glenn Entis, Yuanzhen Li, et al. Styledrop: Text-to-image generation in any style. In 37th Conference on Neural Information Processing Systems (NeurIPS). Neural Information Processing Systems Foundation, 2023. 5 [58] Kun Su, Kaizhi Qian, Eli Shlizerman, Antonio Torralba, and Chuang Gan. Physics-driven diffusion models for imIn Proceedings of the pact sound synthesis from videos. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 97499759, 2023. [59] The Blender Fundation. Blender. https : / / www . blender.org/, 2024. 1, 2, 8, 13 [60] Xiaofeng Wang, Zheng Zhu, Guan Huang, Boyuan Wang, Xinze Chen, and Jiwen Lu. Worlddreamer: Towards general world models for video generation via predicting masked tokens. arXiv preprint arXiv:2401.09985, 2024. 8 [61] Jiajun Wu, Erika Lu, Pushmeet Kohli, Bill Freeman, and Josh Tenenbaum. Learning to see physics via visual deIn Advances in Neural Information Processing animation. Systems. Curran Associates, Inc., 2017. [62] Ziyang Xie, Zhizheng Liu, Zhenghao Peng, Wayne Wu, and Bolei Zhou. Vid2sim: Realistic and interactive simarXiv preprint ulation from video for urban navigation. arXiv:2501.06693, 2025. 8 physics-grounded text-to-video generation. arXiv preprint arXiv:2412.00596, 2024. 8 [64] Honghui Yang, Di Huang, Wei Yin, Chunhua Shen, Haifeng Liu, Xiaofei He, Binbin Lin, Wanli Ouyang, and Tong He. Depth any video with scalable synthetic data. arXiv preprint arXiv:2410.10815, 2024. 8 [65] Shiyuan Yang, Liang Hou, Haibin Huang, Chongyang Ma, Pengfei Wan, Di Zhang, Xiaodong Chen, and Jing Liao. Direct-a-video: Customized video generation with userdirected camera movement and object motion. In ACM SIGGRAPH 2024 Conference Papers, pages 112, 2024. 1 [66] Sherry Yang, Jacob Walker, Jack Parker-Holder, Yilun Du, Jake Bruce, Andre Barreto, Pieter Abbeel, and Dale Schuurmans. Video as the new language for real-world decision making. arXiv preprint arXiv:2402.17139, 2024. 1 [67] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 8 [68] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jose Lezama, Han Zhang, Huiwen Chang, Alexander Hauptmann, MingHsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: In Proceedings of Masked generative video transformer. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1045910469, 2023. [69] Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. 8 [70] Bowen Zhang, Shuyang Gu, Bo Zhang, Jianmin Bao, Dong Chen, Fang Wen, Yong Wang, and Baining Guo. Styleswin: Transformer-based gan for high-resolution image generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1130411314, 2022. 8 [71] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. 8 [72] Qi Zhao, Haotian Fu, Chen Sun, and George Konidaris. Epo: Hierarchical llm agents with environment preference optimization. arXiv preprint arXiv:2408.16090, 2024. 8 [73] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. https://github.com/hpcaitech/OpenSora, 2024. 8 [74] Yunsong Zhou, Michael Simon, Zhenghao Mark Peng, Sicheng Mo, Hongzi Zhu, Minyi Guo, and Bolei Zhou. Simgen: Simulator-conditioned driving scene generation. Advances in Neural Information Processing Systems, 37: 4883848874, 2024. 8 [63] Qiyao Xue, Xiangyu Yin, Boyuan Yang, and Wei Phyt2v: Llm-guided iterative self-refinement for Gao. 11 m n o n a i Property Name Camera Focus Type Camera Focus Position Camera Movement Type Camera Movement Value Camera Initial Position Camera Focal Length Scene Type Choice Follow Fixed Upper, Center, Lower Truck, Dolly, Pedestal, Tilt, Pan, Spin, Following, Zoom Scalar 3D Position Scalar Env Scene Color Light Position Light Color Light Intensity Ambient Light Intensity Background Color e Render Engine Render Quality Basic Empty RGB color 3D position Scalar Scalar Scalar RGBA color Blender/Unreal High/Low Description The camera focus follows the object. The camera focus is static in the world space. The camera focus is at the upper/center/lower part of the object. The basic camera movement types. How much the camera moves. The initial position of the camera. The scalar controls how much percentage of the object is visible on the screen. The environment is given by HDR environmental map. The map will also be used as the light source. The environment is an indoor room which color is controlled by Scene Color and has two light sources. The environment is empty but has two light sources or one environmental map as the light source. The color for the indoor room when presented. The position of the light when presented. The color temperature of the light when presented. The intensity of the light when presented. Ambient light intensity. The ambient light exists when the lights are used. The background color of the location where the scene is empty. The quality of the rendering. We have two presets of rendering setting. Table 10. The parameters used for controlling our rendering pipeline. Figure 6. 3D scene setup in Blender and Unreal Engine. The wireframes and corresponding rendering outputs. A. Appendix A.1. Details of Synthetic Data Generation Following standard CGI production pipeline for creating videos, our synthetic video generation framework comprises two main modules: (1) 3D scene setup and (2) rendering. Below, we provide detailed overview of these modules and the specific parameters that govern them. A.1.1. 3D Scene Setup As discussed in Sec. 2, we focus on generating videos featuring single object per scene. To achieve this, we build procedural 3D scene generator driven by carefully chosen set of parameters, enabling the production of wide variety of synthetic videos. typical 3D scene is composed of four main components: (1) the 3D object, (2) the camera, (3) the lighting conditions, and (4) the environment. We adopt this composition in our generator. Each component in our generator is controlled by set of parameters, which we detail below. 3D Object. As we target single-object videos, we seek to include 3D assets that are both high-quality and highly varied. To this end, we collect assets from Objaverse 1.0 [15], Digital Twin Catalog [46], Blender Market [2], and Metahuman [18]. These sources collectively provide diverse asset categories and styles. We further filter assets from Objaverse based on categories, polygon count, view count, user ratings, and VLM to ensure overall quality. For other sources, we retain all assets since they are already curated with high fidelity. Camera. We represent the camera using set of parameters that capture real-world usage scenarios (see Table 10). These parameters include: Camera movement type: Determines the cameras trajectory around the object. In our experiments, we select one movement type at time and quantify its extend using parameter Camera Movement Value. Initial position and focus: Specifies where the camera starts and how it focuses on the primary object. Focal length: Adjusts the cameras field of view relative to how much of the screen the object occupies."
        },
        {
            "title": "Such parameterization allows us to mimic various camera",
            "content": "12 Figure 7. Examples of our synthetic video data. We render the synthetic videos with diverse background to alleviate the potential biases in synthetic videos. behaviors from the real world. Lighting and Environment. For simplicity, we jointly model the environment and its lighting conditions (see Table 10). Our parameterization supports three main configurations: HDR environment map: Provides both the background and primary light source. We use environment maps from Poly Haven [49]. Solid-color indoor room: Uses two light sources (Figure 6) for illumination: one positioned above the object and another placed elsewhere in the scene. Empty scene: Lit by either an environment map or two lights for more controlled illumination with empty surroundings. Although these settings may appear simple, they cover wide range of lighting conditions and backdrop variations, thereby maintaining diversity while keeping the primary object prominent. A.1.2. Rendering Setup We employ two open-source rendering engines to generate high-quality video outputs: Unreal Engine (Lumen): We use Unreal Engine 5.4.4 with Lumen as our renderer with maximal render-quality settings to achieve realistic rendering effects [19]. Blender (Cycles): We use Blender 4.2 and Cycles renderer configured with carefully chosen parameters to balance rendering speed and visual fidelity [59] These engines offer robust rendering pipelines and physically based shading models, ensuring that our synthetic data closely reflects real-world lighting conditions. A.1.3. Random Sampling of Parameter Space To produce large and diverse set of synthetic videos, we define configuration (config) file containing all relevant parameters described above. Figure 7 show some examples of synthetic videos with diverse setups. Our 3D scene generator parses this config file and sets up the scene. Then, the Figure 8. Example outputs from video generation models trained on synthetic datasets with low-quality assets. The resulting objects frequently exhibit cartoonish or animated characteristics, diverging from the intended original visual style. rendering engines render the scene into video. For largescale generation, we employ random sampling over each parameters prescribed probability distribution, guided by the key insights from Sec. 3. Each sampling step produces unique config file, which is then rendered into separate synthetic video. This process enables us to generate vast set of diverse synthetic videos with minimal manual intervention. A.2. More Ablation Experiments and Visualizations In this section, we provide additional visualizations of the data curation experiments and the ablation studies. Figure 8 and Figure 9 show the effect of using poor quality asset and rendering respectively. Figure 10 shows the effect of excessive training on synthetic data. Color patterns are introduced into the generation model. Figure 11 gives an example of fine-grained and generic captions and an example of using special tags. Figure 12 and Figure 13 show the comparison between videos from generation with and without SimDrop. Lastly, Figure 14 showcases the layer decompostion videos can use to separate out dynamic objects (e.g. animals, fluids) to enable video matting. Finally, Figure 15 shows more generated videos across all three tasks. A.3. Evaluation Prompts"
        },
        {
            "title": "Large Human Motion",
            "content": "Dancing: dancer practicing at home In street setting, teenager is performing breakdance moves, including leaning back, balancing on one leg, and rhythmically moving arms. An attractive man energetically dances, featuring lively movements. He crosses his arms and vigorously moves his legs, imitating horse riding and other whimsical actions. young woman gracefully pirouettes on one foot, her other leg bent elegantly and arms outstretched for balance 13 Figure 9. Visualization of generated outputs from video generation models trained with synthetic videos of low quality assets in large camera motion task. The objects in these generated videos more likely to appear static or animated. Figure 12. comparison showcasing the effect of SimDrop. Row 1 is the result without SimDrop and Row 2 is the video with the method. The color tone in row two is significantly more better and without color pattern from the synthetic data. Figure 10. Visualization of over training video generation models trained with synthetic videos. Visual patterns such as color tone are more likely to appear in generated videos. Figure 13. comparison showcasing the effect of SimDrop. Row 1 is the result without SimDrop and Row 2 is the video with the method. The human faces in row two is significantly more realistic and appealing. Figure 11. comparison of generating captions for synthetic videos using existing methods (Generic Caption) and our method (Fine-Grained Caption). We also show comparison of captions with special tags and without special tags. and flair. She transitions through various spins, showcasing dynamic dance routine that blends elements of northern soul dancing. She dances in bustling urban plaza, or serene beach at sunset, or lively street festival, or, beautifully lit dance studio. Each setting captures the fluidity and energy of her movements, adding depth and variety to her performance. young woman is performing breakdance moves, including leaning back and balancing on one leg while engaging arms rhythmically. Figure 14. Example of background editing. Our layer generation enables easy background replacement via green-screen matting. woman dancing on grassland during sunset On beach, an Ultraman from Japanese TV show is spinning around on one foot while keeping other leg bent and arms extended for balance and style. It performs multiple spins, emphasizing dance move commonly associated with northern soul dancing. In bright dance room, young woman is performing 14 Figure 15. More visualization of generated videos for large camera motion (row 1,2), layer decomposition (row 3,4), and large human motion(row 5,6). dance with enthusiastic movements The person crosses arms and moves legs energetically, mimicking riding horse and performing other playful gestures. young woman is performing breakdance move, starting with dynamic step and then transitioning into series of fluid body movements and rhythmic steps. handsome man initiates with dynamic step followed by series of fluid body motions and rhythmic steps. Gymnastics: In bright dance room, man executes backflip by initially crouching low, launching himself upwards, rotating backwards in midair before returning to standing position on his feet. In well-lit dance studio, woman performs gymnastics moves to flip her body. Her backflip is to first crouch low, then rotating upwards and backward in midair, eventually landing back in standing position. man performs backflip by first squatting down, then launching itself into the air, flipping backward, and finally landing back onfeet on grassland under sunshine. In sunny grassland, woman executes backflip by initially crouching, then springing into the air, rotating backward, and ultimately landing on her feet. female athlete performs backflip by first squatting down, then launching itself into the air, flipping backward, and finally landing back onfeet during the floor execrise event at the Olympic Games. During the floor exercise event at the Olympic Games, male athlete performs stunning backflip. He begins by squatting down low, gathering his strength and focus. With powerful burst of energy, he launches himself into the air, his body gracefully arching as he flips backward. The sunlight glints off his muscular form as he completes the rotation, and he lands solidly on his feet, his expression mix of concentration and triumph. man Moves with dynamic energy, shifting from standing position to deep crouch, then rotating her body midair before landing upright on the sunlit grassland. woman is moving dynamically, transitioning from standing position to deep crouch and then rotating body mid-air before returning to an upright stance on grassland under sunshine. During the floor exercise event at the Olympic Games, female athlete moves with dynamic precision. She transitions from standing position to deep crouch, then launches herself into the air, rotating her body mid-flight before landing gracefully back on her feet. At the Olympic Games floor exercise event, male athlete showcases his agility by swiftly dropping into deep crouch from standing position. He then propels himself into the air, executing mid-air rotation, and lands back on his feet with precision and grace. Large Camera Motion lion standing on the grass. spin shot. An astronaut riding horse, high definition, 4k. spin shot. panda swimming underwater. spin shot. 15 1. If the object appear in the video does not spin at all. 2. If the object appear in the video spins but the background does not move with the object 3. If the object appear in the video corrupts, becomes unnatural or looks animated. Video of sailboat on lake during sunset. spin shot. Variety of succulent plants on garden. spin shot. birthday cake in the plate. spin shot. Big cargo ship passing on the shore. spin shot. Time lapse video, sunrise of the Great Wall. spin shot. tree with Halloween decoration. spin shot. Labrador dog wearing glasses and casual clothes is lying on the bed reading. spin shot. Layer Decomposition lion standing in green background. lion running in green background. Turtle swimming in green background. An african penguin walking in green background. Variety of succulent plants in green background. Leaves swaying in the wind in green background. stack of dried leaves burning in green background. Big cargo ship like in the movies passing in green background. Helicopter landing in green background. young woman is performing breakdance moves, including leaning back and balancing on one leg while engaging arms rhythmically in light blue background. A.4. Human Evaluation Details Our user study videos are available on the project website. We invite the community to also rate the videos. Large Human Motion For large human motions, we asks our human raters to examine how many out of the generated videos in each video show no collapse in human body structure. Specifically, we ask them to focus on the limbs and torso areas. The detailed rules are as following: 1. Does the video include the full body of the person (all four limbs) for more than 2 seconds? 2. Is the video bascially showing what is specified by the prompt, including background and motion? 3. Does the person in the video looks animated? 4. Is there limbs or torso addition/missing from the video? 5. Is there transition of body parts that are obviously unnatural (e.g. switching body parts at the same location)? Please Note: 1. DO NOT focus your judgement on these part of the human body: hands, feet, or face 2. DO NOT judge the asethetics or naturalness of the human motion, please just focus on human body integrity Large Camera Motion For Large camera motion, we instruct the human raters to focus on the object and the degree which the picture rotates. The detailed rules are as following: If any of the following question is yes, please mark the video as 0 1. If the object appear in the video is corrupt, unnatural, or animated 2. If the background is not of pure color as instructed by the prompt Layer Decomposition For layer decompostion, we instruct the human raters to focus on the object and the background quality. The detailed rules are as following: If any of the following question is yes, please mark the video as"
        }
    ],
    "affiliations": [
        "ByteDance Seed",
        "National University of Singapore",
        "Peking University",
        "ShanghaiTech University"
    ]
}