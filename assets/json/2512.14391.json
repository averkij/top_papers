{
    "paper_title": "RePo: Language Models with Context Re-Positioning",
    "authors": [
        "Huayang Li",
        "Tianyu Zhao",
        "Richard Sproat"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In-context learning is fundamental to modern Large Language Models (LLMs); however, prevailing architectures impose a rigid and fixed contextual structure by assigning linear or constant positional indices. Drawing on Cognitive Load Theory (CLT), we argue that this uninformative structure increases extraneous cognitive load, consuming finite working memory capacity that should be allocated to deep reasoning and attention allocation. To address this, we propose RePo, a novel mechanism that reduces extraneous load via context re-positioning. Unlike standard approaches, RePo utilizes a differentiable module, $f_φ$, to assign token positions that capture contextual dependencies, rather than replying on pre-defined integer range. By continually pre-training on the OLMo-2 1B backbone, we demonstrate that RePo significantly enhances performance on tasks involving noisy contexts, structured data, and longer context length, while maintaining competitive performance on general short-context tasks. Detailed analysis reveals that RePo successfully allocate higher attention to distant but relevant information, assign positions in dense and non-linear space, and capture the intrinsic structure of the input context. Our code is available at https://github.com/SakanaAI/repo."
        },
        {
            "title": "Start",
            "content": "REPO: Language Models with Context Re-Positioning Huayang Li 1 2 Tianyu Zhao 1 Richard Sproat 1 5 2 0 2 6 1 ] . [ 1 1 9 3 4 1 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "In-context learning is fundamental to modern Large Language Models (LLMs); however, prevailing architectures impose rigid and fixed contextual structure by assigning linear or constant positional indices. Drawing on Cognitive Load Theory (CLT), we argue that this uninformative structure increases extraneous cognitive load, consuming finite working memory capacity that should be allocated to deep reasoning and attention allocation. To address this, we propose REPO, novel mechanism that reduces extraneous load via context re-positioning. Unlike standard approaches, REPO utilizes differentiable module, fϕ, to assign token positions that capture contextual dependencies, rather than replying on pre-defined integer range. By continually pretraining on the OLMo-2 1B backbone, we demonstrate that REPO significantly enhances performance on tasks involving noisy contexts, structured data, and longer context length, while maintaining competitive performance on general shortcontext tasks. Detailed analysis reveals that REPO successfully allocate higher attention to distant but relevant information, assign positions in dense and non-linear space, and capture the intrinsic structure of the input context. Our code is available at https://github.com/SakanaAI/repo. 1. Introduction The emergence of large language models (LLMs) (Brown et al., 2020) has enabled wide range of applications, including few-shot learning (Brown et al., 2020), retrievalaugmented generation (Lewis et al., 2020; Yao et al., 2023), and agentic systems (Schick et al., 2023; Park et al., 2023). At the core of these applications is in-context learning (Shen et al., 2024), form analogous to human working memory, 1Sakana AI, Japan 2Nara Institute of Science and Technology to: Huayang Li <li.huayang.lh6@is.naist.jp>, Tianyu Zhao <tianyu@sakana.ai>, Richard Sproat <rws@sakana.ai>. Correspondence (NAIST), Japan. Preprint. December 17, 2025. 1 Figure 1. Overall performance on four evaluation dimensions. where information within limited context window is temporarily stored and processed to solve task. Consequently, exploring how to effectively utilize context information has become fundamental research line in the LLM era (Wei et al., 2022; Weston & Sukhbaatar, 2023; Chen et al., 2023). Recent studies show that an LLMs ability to leverage contextual information is strongly influenced by its position encoding scheme (Vaswani et al., 2017; Press et al., 2021; Su et al., 2024). Most LLMs impose fixed contextual structure by assigning tokens consecutive integer indices from 0 to 1 (Vaswani et al., 2017) or constant index for all tokens (Kazemnejad et al., 2023). These indices are then integrated into model through position encoding functions, enforcing rigid organization of context. Although fixed position assignments have become the de facto standard, they deviate from how human working memory processes information. According to Cognitive Load Theory (CLT), the capacity of working memory during problem solving can be consumed by costs arising from how information is organized and presented, referred to as extraneous load (Sweller, 1994; Paas et al., 2003). CLT studies suggest that humans can actively reduce this extraneous load by reorganizing context, e.g., grouping related information into meaningful chunks (Miller, 1956) or removing irrelevant details from instructions (Sweller, 1994). Since working memory capacity is fixed, the capacity saved through such reorganization can be reallocated to deeper reasoning REPO : Language Models with Context Re-Positioning processes associated with the the germane load, thereby improving problem-solving performance (Sweller, 1994). However, the critical ability to reorganize and restructure contextual information (Vaswani et al., 2017; Yang et al., 2025a; Dubey et al., 2024) is absent from the architectural design of modern LLMs. From the perspective of CLT (Sweller, 1994; Paas et al., 2003), rigid linear or constant position structures can be interpreted as introducing additional extraneous load, which in turn impairs attention allocation and deeper contextual reasoning (i.e., germane processing). As consequence, tasks that require strong long-range or fine-grained contextual dependencies, e.g., needle-in-a-haystack (NIAH) problems (Kamradt, 2023) or question answering under highly diluted contexts (Hsieh et al.), exhibit notable performance degradation, mirroring the effects predicted by CLT under high extraneous load. Moreover, from probabilistic standpoint, these position assignment strategies, essentially uniform distributions over fixed integer ranges, are the least informative organizations of context and therefore limit representational efficiency. We propose an internal mechanism for LLMs to reduce extraneous load by re-organizing the positions of tokens. We formalize this process, termed context Re-Positioning (REPO), as learning non-constrained position values based on information relevance of tokens, instead of using the fixed linear positions in prior work. To this end, we introduce differentiable module fϕ, which assigns position value in continuous space for each token based on its hidden state. The fϕ can be independently learned for each attention head of an LLM. Trained on general data, fϕ learns to re-position tokens free from conventional constraints like monotonicity or integer values. The continuity of modern position encoding functions, e.g., RoPE (Su et al., 2024) and ALiBi (Press et al., 2021) methods, is key to the end-to-end optimization of fϕ, as it allows these assigned positions to be integrated in differentiable manner. We find that LLMs using the REPO method achieve consistent performance gains on tasks involving noisy context, structured data, and longer context. In our experiments, we continually pre-trained LLMs with the REPO method and several baselines based on the OLMo-2 1B model. Within the training context length, our REPO method outperforms other baselines by at least 6.24 and 1.16 points on noisy context and structured data tasks, respectively. In addition, when extending the testing context length to 16K tokens using the YaRN (Peng et al., 2024) method, our REPO method outperforms other baselines by at least 13.25 EM points on the QA and Needle-in-the-haystack (NIAH) tasks and 5.48 points on LongBench (Bai et al., 2024). Alongside these performance gains, REPO achieves comparable results on extensive general benchmarks (Wang et al., 2024b; Clark et al., 2018; Zellers et al., 2019), which are typically short and require little reorganization. To better understand whether our method aligns with the CLT-based motivation and where the performance gains of the REPO method come from, we conducted series of detailed analyses. First, to evaluate how different methods handle long-range dependencies, we compared the attention distributions across methods, particularly focusing on their treatment of distant but relevant information. In the NIAH task (Section 5.1), we observed that REPO allocates more attention to the most critical needle tokens compared to the baseline methods, while directing less attention to the nearest query tokens. This behavior breaks the typical locality bias (Yang et al., 2025b; Press et al., 2021), dynamically adjusting based on the context. Second, the positions assigned by REPO exist in denser, more non-linear space, which is critical for enhancing its generalization to longer contexts, such as extending from 4K to 16K tokens. Finally, an interesting finding is that REPO learns position patterns that resemble hybrid of previous position assignment strategies, such as assigning constant positions ϵ (Kazemnejad et al., 2023) or enforcing monotonic position sequence (Vaswani et al., 2017; Yang et al., 2025b; Press et al., 2021), within given context span (Section 5.2). In our case study (Appendix D), we also found that the positions assigned by REPO capture the intrinsic structure of the input context (e.g., segmentation of few-shot examples). 2. Background The position information of tokens in context is critical for the attention mechanism of LLMs. The position of given token is generally mapped into embeddings (Su et al., 2024; Vaswani et al., 2017; Li et al., 2025) or biases (Press et al., 2021; Raffel et al., 2020) through position encoding module before the attention function. In this section, we will introduce the notations for attention mechanism and position encoding functions. Given an input sequence = (x1, x2, . . . , xL), where each token xi is drawn from vocabulary V, most large language models (LLMs) process the information in through multiple layers of self-attention-based neural networks. In each layer, the attention score1 between tokens xi and xj is computed as follows: Q, K, = HWq,k,v, Ai,j = kj, (1) (2) where Wq,k,v Rd3d projects the hidden state hi of token xi into its corresponding query, key, and value representations, and RLL represents the attention scores for all token pairs. 1For simplicity, we omit the design of multi-head in the attention calculation. 2 REPO : Language Models with Context Re-Positioning We use the rotary position encoding (ROPE) (Su et al., 2024) as specific example to illustrate the position assignment and encoding procedure commonly employed in LLMs (Walsh et al.; Yang et al., 2025a; Qwen et al., 2025). First, each token xi is assigned an integer position index i. This positional information is then incorporated into the model through an encoding function gθ : Rdd, which is differentiable function that generates rotation matrix. The parameter θ represents pre-defined frequency vector, generally frozen during training. In ROPE, the function gθ is directly applied to the query qi and key kj as follows:"
        },
        {
            "title": "ARoPE",
            "content": "i,j = gθ(j i)kj, (3) where and are the integer position indices for tokens xi and xj, respectively, and gθ(j i) captures the relative positional relationship between any tokens with distance of i. In many related works, strict linear position assignment (i.e., 0 to 1) similar to ROPE has become the standard approach, with various encoding functions being explored (Vaswani et al., 2017; Press et al., 2021; Yang et al., 2025b; Li et al., 2025). One notable exception is the NOPE method (Kazemnejad et al., 2023; Yang et al., 2025b), which omits both position assignment and position encoding. However, we demonstrate that this approach is equivalent to applying position encoding at constant position for all tokens. Further discussion about the inter-connection between constant (e.g., NOPE) and linear position assignment (e.g., ROPE) strategies are provided in Appendix A. 3. Methods 3.1. Overview The current linear context organization, which assigns consecutive integer indices to tokens, overlooks the internal structure of tokens based on their relevance. For instance, this limitation leads to noticeable performance degradation in tasks involving long-distance dependencies (Hsieh et al.), problem analogous to the high extraneous load issue in Cognitive Load Theory (CLT). The main goal of this work is to reduce the unnecessary cognitive cost raised by the oversimplified organization of the context, i.e., extraneous cognitive load, thereby conserving the finite working memory capacity for beneficial germane load, such as the attention mechanism. To this end, we propose context re-positioning (REPO) module fϕ : Rd R, which is light-weight neural network that assigns more appropriate positions of tokens, taking into account their relevance within given context. The assigned positions by fϕ are defined in continuous, non-linear space, and can thus be optimized jointly with LLMs when equipped with differentiable position encoding methods (Vaswani et al., 2017; Su et al., 2024; Press et al., 2021). Notably, our REPO module fϕ is prior to position encoding, where the latter aims to map assigned positions into embeddings or biases. 3.2. Context Re-positioning The context re-positioning module fϕ has two components: 1) representing the position information based on hidden states of tokens; 2) assigning real-valued positions based on the extracted position representations. Position Representation Kazemnejad et al. (2023) shows that position information may be entangled in original hidden states, so the first component is designed to extract the position representation from hidden states of tokens explicitly. In our implementation, we use light-weight SwiGLU (Shazeer, 2020) sub-layer to achieve this goal: ri = Swish(hiWg) (hiWc), (4) where ri Rdp and hi Rd are position representation2 and hidden state of token xi respectively, Wg, Wc Rddp are linear transformations for gate and content mapping, and Swish() is the activation function. Since we assume that position information can be represented with lower dimension, we set dp < in practice. Position Assignment The subsequent component assigns new position value zi for token xi in each attention head. There are variety of modeling strategies for processing ri with full (Vaswani et al., 2017) or limited (LeCun & Bengio, 1998) access to r<i, but we find that linear transformation achieves comparable performance with lower latency: zi = riWz, (5) where Wz Rdp1. REPO Module By combining Eq. 4 and 5, the formal definition of fθ becomes: fϕ(hi) = (cid:0)Swish(hiWg) (hiWc)(cid:1)Wz, (6) where hi is the hidden state of token xi, as defined in Eq. 1. When equipped with modern position encoding methods, e.g., RoPE, the computation of attention score becomes: (cid:0)fϕ(hj) fϕ(hi)(cid:1)kj ARePo i,j = = gθ gθ(zj zi)kj, (7) where the position encoding function gθ is the same as that used in ROPE in Eq. 3. It is worth noting that our REPO is 2Notably, the hidden state hi of token xi does not explicitly encode the linear position information i. In our preliminary experiments, when using the raw position as an additional dimension during continue-training, the LLM pre-trained on ROPE quickly biases to this feature, resulting in trivial position assignment. 3 REPO : Language Models with Context Re-Positioning Table 1. Performance on noisy context. We evaluate on subsets of RULER with noisy context, i.e., Needle-in-the-haystack (NIAH), Question Answering (QA), Aggregation (AGG), and variable tracking (VT), within the training context length (i.e., 4K tokens). This uses recall for NAIH, AGG, and VT tasks, and EM for the QA task as the metrics. We use bold and underline for best and second best results."
        },
        {
            "title": "QA AGG",
            "content": "VT AVG. ROPE NOPE R2N1 N2R1 REPO 82.56 74.59 85.00 80.00 88.25 57.00 49.00 59.50 58.00 61.00 37.98 22.45 31.10 32.75 35.05 1.00 12.20 0.00 27.00 38. 44.64 39.56 43.90 49.44 55.68 not restricted to ROPE and can be easily extended to all the differentiable position encoding methods (Vaswani et al., 2017; Press et al., 2021; Li et al., 2025). In practice, we apply the position representation module (Eq. 4) for each layer independently, and position assignment module (Eq. 5) for each attention head independently. Training & Efficiency Since many position encoding methods are differentiable (Su et al., 2024; Press et al., 2021; Li et al., 2025), we can train an LLM with REPO-based attention (Eq. 7) using backpropagation when equipped with such encodings. To balance efficiency and effectiveness, we apply the REPO method starting from the l-th layer (e.g., = 5) while keeping standard position encoding for the lower layers. This design choice is motivated by previous findings that the lower layers of LLMs primarily capture surface-level features that depend more on local information (Tenney et al., 2019), such as part-of-speech tagging and syntax, and thus benefit less from reorganization. An ablation study for the hyper-parameter is in Appendix B.3. In order not to impair the efficiency of LLMs significantly, we only use the assigned position zi and zj to affect the position encoding in attention calculation in Eq. 7, leaving the auto-regressive order of qi (or ki) and qj (or kj) in the context unchanged. The REPO module can be applied to each attention head independently. In principle, we can sort queries and keys in each attention head according to the assigned positions = {z1, . . . , zn}. However, under the auto-regressive language modeling paradigm, this approach requires re-computation for the KV cache at each time step, resulting in tremendous overhead for auto-regressive LLMs. Therefore, the assigned positions are only used in Eq. 7. 4. Experiments This section presents our main experiments on general language modeling. We continually pre-train an LLM with REPO on general datasets and evaluate its performance on Table 2. Performance for structured data. We evaluate on NLGraph and HybridQA datasets for graph and table data, respectively. We use exact match (EM) as metric. Bold and underline denote the best and the second best results, respectively."
        },
        {
            "title": "Table",
            "content": "AVG. 27.43 29.90 27.11 25.42 29.03 24.43 23.52 25.11 23.86 26.70 25.93 26.71 26.11 24.64 27.87 ROPE NoPE R2N1 N2R1 REPO three types of tasks that require restructuring context. In Appendix C, we present preliminary studies on synthetic data along with visualizations to show how REPO works. 4.1. Setup We use OLMo-2 (Walsh et al.) developed by Allen Institute for AI as the backbone model, whose performance is comparable to Qwen-2.5 (Qwen et al., 2025). Its data, model weights, and code are fully open-sourced. Notably, OLMo-2 is trained with the RoPE method (Su et al., 2024). For all methods in our experiments, we start from the checkpoint of the OLMo-2 1B model that has completed stage-1 pretraining, and we continually pre-train it on the 50B-token stage-2 data3. The training context length is 4096 tokens. We keep the training configuration and codebase identical to those released by Walsh et al.. For our REPO method, we apply it starting from the 5th layer of the OLMo-2 1B model. In each layer that uses REPO, we share the parameters for position representation transformation in Eq. 4, while learning the position assignment for each attention head independently as in Eq. 5. The hidden size for the learned position representation is 256, i.e., 1/8 of the models hidden size. As shown in 5.4, REPO is lightweight and introduces negligible overhead to the original LLM. We compare REPO with baselines: ROPE: Uses RoPE (Su et al., 2024) for positional encoding, identical to pre-training. NOPE: Removes explicit positional encoding methods (Kazemnejad et al., 2023; Wang et al., 2024a), i.e., RoPE is omitted during the continual pre-training. R2N1: hybrid positional encoding method that interleaves RoPE and NoPE (Yang et al., 2025b; Meta, 2025). For every three layers, the first two use RoPE while the last one uses NoPE. 3https://github.com/allenai/OLMo 4 REPO : Language Models with Context Re-Positioning Figure 2. Long-context Evaluation on RULER. YaRN (Peng et al., 2024) is used for all RoPE layers to extend the context. We observe consistent results on the more realistic benchmark LongBench in Table 3. N2R1: The opposite of R2N1, i.e., every three layers, the first two use NoPE and the last one uses RoPE. We train those models on 4 H100 GPUs for 50B tokens. We use the allenai/olmes4 codebase for evaluation, which provides extensive test suites. We categorize our main evaluation tasks into three dimensions as follows: Noisy Context evaluates the models robustness to contexts containing large amounts of irrelevant information. Such noise increases extraneous cognitive load (Paas et al., 2003), which can negatively affect problemsolving performance. We use the RULER benchmark (Hsieh et al.), which purposefully constructs contexts with irrelevant content for evaluation. Structured Data evaluates performance on structured data such as tables and graphs. This setting highlights the importance of contextual organization, as linearizing such data into natural language often leads to significant structural information loss. We use NLGraph (Wang et al., 2023) and HybridQA (Chen et al., 2020) to evaluate model performance on graph and table reasoning, respectively. Longer Context evaluates model performance on sequences longer than those seen during training (i.e., 4K tokens), as positional encoding has been shown to strongly affect long-context generalization (Press et al., 2021). We use subsets of RULER and LongBench (Bai et al., 2024), which contain examples with 4K to 16K tokens for this evaluation. requires extended context lengths. To enable this, we apply the YaRN method to RoPE layers across all methods using the extrapolation hyperparameters in Peng et al. (2024). More details are in Appendix B. 4.2. Results Experimental results show that REPO yields significant performance gains across all three evaluation dimensions. Noisy Context For all evaluation subtasks, the input contexts are injected with irrelevant information. As shown in Table 1, even when all test contexts are within the training context length (i.e., 4K tokens), REPO outperforms RoPE by 11.04 points. This suggests that the re-positioning mechanism in REPO effectively reduces extraneous cognitive load and improves robustness against noisy or distracting information in model inputs. Longer Context The advantage of REPO becomes even more pronounced on long-context tasks. As shown in Figure 2, REPO already surpasses all baselines at context length of 4K. The performance gap further widens at 8K and 16K, which are lengths unseen during training. To rule out potential confounding effects from REPOs noise robustness, we additionally evaluate on LongBench (Bai et al., 2024), which consists of more realistic long-context tasks. As shown in Table 3, REPO consistently outperforms other baselines on LongBench by at least 5.48 points. In addition, the hybrid R2N1 method, with interleaved RoPE and NoPE layers, achieves the second-best performance, consistent with the findings of Yang et al. (2025b). Notably, both the evaluations for noisy context and structured data are conducted within the training context length (i.e., 4K tokens). In contrast, the longer-context evaluation 4https://github.com/allenai/olmes Structured Data Since linearizing structured data (e.g., tables and graphs) into natural language can result in substantial loss of structural information, it is of interest to examine whether contextual re-positioning benefits such data types. As shown in Table 2, REPO improves over 5 REPO : Language Models with Context Re-Positioning Table 3. Performance on LongBench. This uses F1 score for QA and fewshot tasks, while Rouge-L (Lin, 2004) for the summarization tasks. We only evaluate on data that contains less than 16K tokens. Since the model is trained with max of 4096 tokens, the YaRN method (Peng et al., 2024) is used for all RoPE layers for context extrapolation. We use bold and underline for best and second best results, respectively. Method RoPE NoPE R2N1 N2R1 REPO Multidoc QA Singledoc QA Summarization Fewshot 2WikiMultihopQA MuSiQue MultiFieldQA-en NarrativeQA GovReport QMSum TREC TriviaQA 23.32 9.11 25.88 16.24 30.86 7.24 0.33 7.31 0.40 13. 27.37 13.64 31.28 21.88 33.12 12.94 1.80 16.24 1.26 15.24 6.23 5.15 4.53 8.74 16.80 7.96 0.68 8.74 5.31 12.53 22.00 9.50 22.00 21.50 31.50 61.47 18.12 66.67 25.18 73. AVG. 21.07 7.29 22.83 12.56 28.31 Table 4. Attention mass per token (102) on different parts of context. We evaluate on NIAH task within the training context length (i.e., 4K tokens). closest to the generated tokens. Rest: other tokens in the context. Pos. Assignment"
        },
        {
            "title": "Rest",
            "content": "Linear (e.g., RoPE) Constant (e.g., NoPE) REPO 1.754 1.572 2.013 1.123 1.135 1.046 0.014 0.014 0.015 We conduct our analysis on the NIAH dataset provided by RULER (Hsieh et al.), where the context follows the format: Rest . . . Needle . . . Rest . . . Query the vanilla RoPE method by an average of 1.94 EM points. Interestingly, the NoPE method achieves the best performance on the graph dataset, suggesting that emphasizing local positional relationships may not be valid assumption for graph-structured inputs. 5. Analyses This section is to provide insights into the inner workings of REPO. To this end, we conducted detailed analyses to understand: 1) where the performance gains stem from; and 2) what patterns the positions assigned by REPO exhibit. 5.1. Attention Mass on Relevant Tokens Since our REPO method re-organizes the context based on its intrinsic structure, we hypothesize that it can better capture long-distance dependencies by bringing distant but relevant tokens with closer positions. To evaluate this effect, we analyze the attention patterns of methods with different types of position assignment strategies on the needle-in-ahaystack (NIAH) task (Kamradt, 2023; Hsieh et al.) and quantitatively measure the attention mass, i.e., attention scores averaged across attention heads and layers, from generated tokens to three non-overlapping parts of the context, following Yang et al. (2025b): Needle: tokens that correspond to the golden answer in the context. The needle tokens are generally distant from the generated tokens in the NIAH task. Query: tokens that correspond to the user question and the continuation prefix in the context. Thus, they are As shown in Table 4, for needle tokens that are distant yet critical for generation, our REPO method allocates substantially more attention mass than both the linear (i.e., RoPE) and constant (i.e., NoPE) position assignment strategies. Compared with REPO, the linear position assignment also exhibits stronger locality bias, encouraging attention allocation to nearby query tokens. In addition, the constant position assignment, which treats all positions uniformly, produces an attention pattern with much lower variance across the three parts. These findings explain how our REPO method achieves performance gains on tasks involving noisy context, and also support our motivation based on Cognitive Load Theory (CLT), where the germane load (e.g., the attention mechanism) can better process the context information with context re-positioning. 5.2. Position Patterns Learned by REPO To better understand the patterns learned by REPO, we analyze the characteristics of the assigned positions, first focusing on their ranges and then on their local patterns. We first collect statistics on the distances between the maximum and minimum assigned positions for each attention head: dk,h = max(zk,h) min(zk,h), where zk,h = {zk,h }, is the number of tokens in input x, and and represent the indices of the attention head and layer, respectively. , . . . , zk,h , zk,h 2 1 We compare these statistics on general benchmark (2Ktoken context) and the RULER benchmark (4K-token context). As shown in Figure 3, we find that REPO assigns REPO : Language Models with Context Re-Positioning Figure 3. Statistics for the distances between maximum and minimum positions in each attention head of the LLM. The averaged and maximum number of tokens in the MMLUPro-Math benchmark are 1971 and 2512, while those for RULER-QA are 2995 and 3555, respectively. Figure 4. Statistics for the patterns of assigned positions. We split the context into non-overlapping chunks of 16 tokens. Constant means assigned positions are all close to constant position, Mono means the positions are monotonically increasing or decreasing in the chunk, and Hybrid means all other patterns. larger positional distances on longer context lengths, but the largest distance is still much smaller than the raw context length (i.e., 2K or 4K). This observation suggests that increasing the positional range to match the full input context length may not be necessary from the models perspective. Furthermore, the distribution of distances is non-uniform, unlike the linear positional assignment in RoPE. We hypothesize that assigning positions in denser and non-linear continuous space contributes to improved performance on longer contexts in 4. We then analyze the patterns of the positions assigned by REPO. We split the positions zl,h into non-overlapping chunks with tokens {zk,h 1:, zk,h L:L} and define three pattern types: +1:2, . . . , zk,h mono pattern. The dominating pattern of positions assigned by REPO is Hybrid, indicating that the position patterns beneficial for LLMs are different from those pre-defined in previous work (Vaswani et al., 2017; Su et al., 2024). This analysis is conducted on the RULER benchmark, but we find consistent observations on other benchmarks. Besides the statistics of positions assigned by REPO, as shown in Appendix D, we also conduct case study to visualize the positions across different layers and attention heads of the LLM. We find that the assigned positions can capture the intrinsic structure of the input context, such as the segmentation of few-shot examples, which aligns with our CLT-based motivation. Constant: We calculate the average position value in the chunk. If all positions lie between [a ϵ, + ϵ], we conjecture that the positions are close to constant, indicating the pattern of NoPE that assigns all positions to constant position. Mono: If the positions in chunk are monotonically increasing (i.e., zi1 < zi < zi+1) or monotonically decreasing (i.e., zi+1 < zi < zi1) for all zi in chunk, we classify the pattern as monotonic, similar to the position assignment strategy used by conventional position encoding methods. 5.3. Performance on General Tasks As shown in Table 5, along with the noticeable performance gain in previous experiments, our REPO method still achieves performance comparable to the ROPE method on extensive general benchmarks. This occurs even though changing from linear position assignment to REPO causes an inconsistency between pre-training and continued training. This observation indicates that our REPO, learned from general data, generalizes well to diverse types of data, even when questions in general benchmarks are typically short and precise, requiring almost no context reorganization. Hybrid: All other patterns, e.g., mixture of constant 5.4. Efficiency and monotonic patterns. We empirically set = 16 and ϵ = 0.2 to provide insights for the learned patterns5. As shown in Figure 4, we find that the Mono pattern is very rare (4% of all chunks), and the model prefer the constant patterns (22% of all chunks) than 5Setting different values for and ϵ results in different plots, but the overall conclusion still holds. As shown in Table 6, we compare the FLOPs and inference time between the vanilla model with ROPE method and REPO. We observe that REPO method is very lightweight, introducing only 0.9% increase in parameters while providing performance gains across many evaluation dimensions. When running inference for RULER benchmark (Hsieh et al.) within training context length, the time cost of REPO is comparable to that of the vanilla model. 7 REPO : Language Models with Context Re-Positioning Table 5. We use the default evaluation suites, including few-shot examples, prompts, and metrics, provided in allenai/olmes for evaluation. All evaluations are conducted within the training context length, i.e., 4096 tokens. We use bold and underline to indicate the best and second-best results, respectively. Model ARC-C ARC-E BoolQ CoQA Drop Hellaswag MMLU-Pro TriviaQA AVG. ROPE NOPE R2N1 N2R1 REPO 47.99 44.05 47.30 43.78 47.61 75.25 73.64 75.68 72.72 74.87 72.12 66.70 73.04 69.38 73.58 56.87 44.52 59.31 50.74 57.44 37.90 33.22 38.48 36.66 38.17 70.68 65.68 69.26 67.58 70. 13.77 10.70 13.41 11.99 13.52 54.98 43.43 54.61 47.31 54.56 53.70 47.74 53.88 50.02 53.73 Table 6. Efficiency comparison. FLOPs are reported for training on 50B tokens. Inference time (second) per token is evaluated using vLLM."
        },
        {
            "title": "Method",
            "content": "ROPE REPO FLOPs 3.84 1020 3.87 1020 Dec. Time / Token 0.0176 0.0182 6. Related Work The self-attention mechanism in Transformers (Vaswani et al., 2017) is inherently permutation-invariant, lacking an intrinsic understanding of input token order. To address the issue, position encoding module is used in most Transformer-based models to map the position indices of input tokens into biases or embeddings that can be integrated into the model. Bias-based position encoding methods, such as ALiBi (Press et al., 2021), KERPLE (Chi et al., 2022), and T5 (Raffel et al., 2020), integrate distance bias directly into the attention logits to control the attention field of perception. Most position encoding methods, however, learn embeddings for position indices or distances. The absolute position embedding, which is added to the hidden states, has been widely used in Transformer-based models with small sequence lengths (Vaswani et al., 2017; Radford et al., 2019; Devlin et al., 2019). In the era of large language models (LLMs), the RoPE method (Su et al., 2024), which rotates queries and keys for calculating attention scores based on position distance, has become the de facto choice for modern LLM architectures. Many subsequent variants have been proposed to improve the context extrapolation performance of RoPE (Peng et al., 2024; Chen et al., 2023; 2024; 2025) or manually combining it with NoPE (Barbero et al., 2025; Yang et al., 2025b). Our work is orthogonal to these enhanced RoPE methods. Recently, Li et al. (2025) proposed an alternative approach to model positional information in sequential way. In contrast to previous literature, our work focuses on position prediction for tokens within given context, prior to the 8 position encoding process. The most relevant work to ours is COPE (Golovneva et al., 2024), which learns an attentionlogits-based gate module to predict non-decreasing sequence for positions, aiming to segment high-level abstractions, such as sentences. However, The COPE method uses attention logits for gating, which incurs high time and memory costs due to the computation of [B, L, L] tensors. It is also incompatible with other position encoding methods, like RoPE, and flash attention (Dao, 2024), limiting its scalability. In contrast, our work focuses on re-organizing tokens within the context using lightweight REPO module that is compatible with most position encoding methods. Recently, Zhang et al. (2025) also highlighted the importance of input context and proposed ACE, which iteratively evolves the context in an agentic manner. Our work aims to enhance Transformer models with context re-positioning, which is orthogonal to ACE (Zhang et al., 2025). 7. Conclusion In this paper, we addressed high extraneous cognitive load by substituting the rigid linear position encoding in LLMs with context Re-Positioning (REPO). The proposed REPO is novel, lightweight differentiable module (fϕ) that empowers models to learn positions that capture the structure and dependencies in context. Through continual pre-training of an OLMo-2 1B model, our extensive experiments demonstrated that REPO significantly outperforms strong baselines on tasks requiring specific contextual dependencies, showing substantial gains on noisy-context, structured data, and long-context extrapolation benchmarks, while maintaining performance on general short-context tasks. Our analysis revealed that REPO also brings notable technical advantages, as it mitigates locality bias of traditional position assignment strategies by allocating more attention to distant but relevant needle tokens, and learns adaptive position patterns in more non-linear and denser continues space. By enabling models to actively re-position their input context, REPO opens new direction for flexible context management driven by innovations in LLM architecture. REPO : Language Models with Context Re-Positioning 8. Impact Statement This work improves how large language models (LLMs) organize and attend to contextual information, particularly in long or noisy inputs. By enabling more effective use of relevant context, the proposed method can improve reliability and efficiency in downstream applications such as long-document understanding, retrieval-augmented generation, and agentic systems. The method does not introduce new data sources or supervision and operates within existing model architectures. While improved contextual reasoning may amplify both beneficial and harmful uses of language models, this work does not inherently introduce new ethical risks beyond those already associated with LLMs, and highlights context organization as an important direction for improving robustness and interpretability. 9. Acknowledgment This work was conducted during Huayangs internship at Sakana AI. Huayang wishes to thank his mentors, Richard and Tianyu, for their kind support during this journey. The authors also thank Yoav Gelberg and Yingtao Tian for insightful discussions regarding the experiments, as well as Yujin Tang, Qi Sun, Makoto Shing, Lemao Liu, Deng Cai, Leyang Cui, Yahui Liu, and Tian Lan for their feedback on the manuscript."
        },
        {
            "title": "References",
            "content": "Bai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z., Du, Z., Liu, X., Zeng, A., Hou, L., Dong, Y., Tang, J., and Li, J. LongBench: bilingual, multitask benchmark for long context understanding. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 31193137, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.172. URL https://aclanthology.org/2024.acl-long.172/. Barbero, F., Vitvitskyi, A., Perivolaropoulos, C., Pascanu, R., and Veliˇckovic, P. Round and round we go! what makes rotary positional encodings useful? In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum? id=GtvuNrk58a. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 18771901, 2020. Chen, G., Li, X., Meng, Z., Liang, S., and Bing, L. CLEX: Continuous length extrapolation for large language models. In International Conference on Learning Representations (ICLR), 2024. Chen, S., Wong, S., Chen, L., and Tian, Y. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023. Chen, W., Zha, H., Chen, Z., Xiong, W., Wang, H., and Wang, W. Y. HybridQA: dataset of multi-hop question answering over tabular and textual data. In Cohn, T., He, Y., and Liu, Y. (eds.), Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 10261036, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020. findings-emnlp.91. URL https://aclanthology.org/ 2020.findings-emnlp.91/. Chen, Y., Lv, A., Luan, J., Wang, B., and Liu, W. HoPE: novel positional encoding without long-term decay for enhanced context awareness and extrapolation. In Che, W., Nabende, J., Shutova, E., and Pilehvar, M. T. (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2304423056, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176251-0. doi: 10.18653/v1/2025.acl-long.1123. URL https://aclanthology.org/2025.acl-long.1123/. Chi, T.-C., Fan, T.-H., Ramadge, P. J., and Rudnicky, A. Kerple: Kernelized relative positional embedding for length extrapolation. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 83868399. Curran Associates, Inc., 2022. URL https://proceedings. neurips.cc/paper files/paper/2022/file/ 37a413841a614b5414b333585e7613b8-Paper-Conference. pdf. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? Try ARC, the AI2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Dao, T. Flashattention-2: Faster attention with better parallelism and work partitioning. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=mZn2Xyh9Ec. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pp. 41714186, 2019. REPO : Language Models with Context Re-Positioning Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The Llama 3 herd of models. arXiv e-prints, pp. arXiv2407, 2024. Gehring, J., Auli, M., Grangier, D., Yarats, D., and Dauphin, Y. N. Convolutional sequence to sequence learning. In International conference on machine learning, pp. 1243 1252. PMLR, 2017. Paas, F., Renkl, A., and Sweller, J. Cognitive load theory and instructional design: Recent developments. Educational psychologist, 38(1):14, 2003. Park, J. S., OBrien, J., Cai, C. J., Morris, M. R., Liang, P., and Bernstein, M. S. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on user interface software and technology, pp. 122, 2023. Golovneva, O., Wang, T., Weston, J., and Sukhbaatar, S. Contextual position encoding: Learning to count whats important. arXiv preprint arXiv:2405.18719, 2024. Hsieh, C.-P., Sun, S., Kriman, S., Acharya, S., Rekesh, D., Jia, F., and Ginsburg, B. Ruler: Whats the real context In First size of your long-context language models? Conference on Language Modeling. Kamradt, G. Needle in haystack - pressure testing LLMs, 2023. URL https://github.com/gkamradt/LLMTest NeedleInAHaystack/tree/main. Kazemnejad, A., Padhi, I., Natesan Ramamurthy, K., Das, P., and Reddy, S. The impact of positional encoding on length generalization in transformers. Advances in Neural Information Processing Systems, 36:2489224928, 2023. LeCun, Y. and Bengio, Y. Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks, 1998. Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Kuttler, H., Lewis, M., Yih, W.-T., Rocktaschel, T., et al. Retrieval-augmented generation for knowledge-intensive NLP tasks. Advances in neural information processing systems, 33:94599474, 2020. Li, H., Liu, Y., Sun, H., Cai, D., Cui, L., Bi, W., Zhao, P., and Watanabe, T. SeqPE: Transformer with sequential position encoding. arXiv preprint arXiv:2506.13277, 2025. Lin, C.-Y. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pp. 74 81, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology. org/W04-1013/. Peng, B., Quesnelle, J., Fan, H., and Shippole, E. YaRN: Efficient context window extension of large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=wHBfxhZu1u. Press, O., Smith, N. A., and Lewis, M. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021. Qwen, :, Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T., Tang, T., Xia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., and Qiu, Z. Qwen2.5 technical report, 2025. URL https: //arxiv.org/abs/2412.15115. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of Machine Learning Research, 21: 167, 2020. Schick, T., Dwivedi-Yu, J., Dess`ı, R., Raileanu, R., Lomeli, M., Hambro, E., Zettlemoyer, L., Cancedda, N., and Scialom, T. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36:6853968551, 2023. Shazeer, N. GLU variants improve Transformer. arXiv preprint arXiv:2002.05202, 2020. Meta, A. The Llama 4 herd: The beginning of new era of natively multimodal AI innovation. https://ai. meta. com/blog/llama-4-multimodal-intelligence/, checked on, 4(7):2025, 2025. Shen, L., Mishra, A., and Khashabi, D. Position: Do pretrained transformers learn in-context by gradient descent? In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org, 2024. Miller, G. A. The magical number seven, plus or minus two: Some limits on our capacity for processing information. Psychological review, 63(2):81, 1956. Su, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 10 REPO : Language Models with Context Re-Positioning Yang, B., Venkitesh, B., Gnaneshwar, D., Lin, H., Cairuz, D., Blunsom, P., and Locatelli, A. Rope to Nope and In The back again: new hybrid attention strategy. Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025b. URL https://openreview. net/forum?id=Tp6ds3Dfqo. Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. HellaSwag: Can machine really finish your senIn Korhonen, A., Traum, D., and M`arquez, tence? L. (eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4791 4800, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1472. URL https://aclanthology.org/P19-1472/. Zhang, Q., Hu, C., Upasani, S., Ma, B., Hong, F., Kamanuru, V., Rainton, J., Wu, C., Ji, M., Li, H., et al. Agentic context engineering: Evolving contexts for self-improving arXiv preprint arXiv:2510.04618, language models. 2025. Sweller, J. Cognitive load theory, learning difficulty, and instructional design. Learning and instruction, 4(4):295 312, 1994. Tenney, I., Das, D., and Pavlick, E. BERT rediscovers the classical NLP pipeline. In Korhonen, A., Traum, D., and M`arquez, L. (eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 45934601, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1452. URL https://aclanthology.org/P19-1452/. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Walsh, E. P., Soldaini, L., Groeneveld, D., Lo, K., Arora, S., Bhagia, A., Gu, Y., Huang, S., Jordan, M., Lambert, N., et al. 2 olmo 2 furious (colms version). In Second Conference on Language Modeling. Wang, H., Feng, S., He, T., Tan, Z., Han, X., and Tsvetkov, Y. Can language models solve graph problems in natural language? In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https: //openreview.net/forum?id=UDqHhbqYJV. Wang, J., Ji, T., Wu, Y., Yan, H., Gui, T., Zhang, Q., Huang, X., and Wang, X. Length generalization of causal transformers without position encoding. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Findings of the Association for Computational Linguistics: ACL 2024, pp. 14024 14040, Bangkok, Thailand, August 2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024. findings-acl.834. URL https://aclanthology.org/ 2024.findings-acl.834/. Wang, Y., Ma, X., Zhang, G., Ni, Y., Chandra, A., Guo, S., Ren, W., Arulraj, A., He, X., Jiang, Z., et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. Advances in Neural Information Processing Systems, 37:9526695290, 2024b. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Weston, J. and Sukhbaatar, S. (is something you might need too). arXiv:2311.11829, 2023. System 2 attention arXiv preprint Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a. 11 REPO : Language Models with Context Re-Positioning A. Whats the Real Difference between Conventional PEs, NoPE, and RePo? In the background section (2), we use ROPE as representative example to illustrate how conventional positional encoding methods rely on strict linear pattern to assign positional information to tokens in the context. Recently, researchers have found that the causal mask in the attention mechanism enables LLMs to implicitly learn positional information, and that removing explicit positional encoding can even achieve superior performance on structured data and long-context tasks. This approach is referred to as the NoPE method (Kazemnejad et al., 2023; Yang et al., 2025b; Wang et al., 2024a; Barbero et al., 2025). We argue that the attention score of NoPE can be reformulated within the RoPE framework by assigning constant positional value a:"
        },
        {
            "title": "ANoPE",
            "content": "i,j = kj = qigθ(0)kj = qigθ(a a)kj, (8) where denotes uniform position value for all tokens, yielding constant rotation matrix gθ(0). Thus, under this reformulation, the key difference between RoPE and NoPE lies solely in how positions are assigned. Table 7. Comparison between different methods. In RoPE-like methods, gθ generates rotation matrix based on distance. The is the distance between xj and xi, gθ(0) is constant rotation, and zj and zi are predicted positions by fϕ (Eq. 7). In addition, LLMs with interpolated NoPE and RoPE layers (Yang et al., 2025b; Meta, 2025; Barbero et al., 2025) have been widely used architectures, which can be explained as hybrid position assignment strategies. In contrast to previous works that empirically configure the hybrid system with hyper-parameters, our REPO shows higher expressiveness, as it can dynamically determine whether to adopt the conventional linear, NoPE-like constant, or hybrid position assignment for tokens in given context. comparison among the three approaches is summarized in Table 7. As explained in 2, when zj = zi, REPO effectively reduces to the NoPE pattern with the constant zj = zi = a. In contrast, if zj > zi for > i, it indicates that REPO adopts positional relationship similar to the conventional linear style, e.g., the strategt used in RoPE. In our experiments and analyses, we will demonstrate that an LLM may dynamically select between constant and linear position assignments, or hybridize them with REPO module. Notably, although we use RoPE for the comparison, linear position assignment is widely adopted in conventional positional encoding methods (Vaswani et al., 2017; Gehring et al., 2017; Press et al., 2021; Li et al., 2025), and our findings can be readily extended to these approaches. gθ(j i)kj gθ(0)kj gθ(zj zi)kj Constant (e.g.,NOPE) Linear (e.g., ROPE) Attention Score Method REPO B. Details of Experiments B.1. Extrapolation We use the following hyper-parameters to extend the context: 1. 8K Tokens: {\"rope type\": \"yarn\", \"factor\": 2.0, \"original max position embeddings\": 4096} 2. 16K Tokens: {\"rope type\": \"yarn\", \"factor\": 4.0, \"original max position embeddings\": 4096} We use the setting for 16K Tokens for all the experiments on LongBench  (Table 3)  . B.2. General Tasks We use the following task ids in olmes for the evaluation in Table 5: arc challenge:rc::large, arc easy:rc::olmes, boolq:rc::large, coqa::large, drop::large, hellaswag:rc::large, mmlu pro:cot::none, triviaqa::olmes B.3. Ablation Study As shown in Figure 5, we evaluate the sensitivity of model performance to the starting layer of REPO, where = 5 indicates that REPO is applied beginning from the 5th layer of the LLM, while the vanilla ROPE is used for the lower layers. We 12 REPO : Language Models with Context Re-Positioning Figure 6. Visualization of predicted positions from 4-layer GPT-2 model in the reversal task. The area with blue background color indicates input context, while the orange region is the generated sequence. We use A-K to replace the real tokens to save space for illustration. The and y-axis represent the input order and predicted position of token, respectively. conduct experiments on two subtasks, NIAH and MMLUPro. The results show that overall performance is robust to this hyperparameter. However, increasing slightly improves performance on general benchmarks while negatively affecting performance on NIAH. The results are consistent on other evaluation benchmarks. C. Preliminary Experiments In this experiment, we train small-scale language model on purposefully selected synthetic task, namely text reversal, to determine whether REPO can re-position tokens in the context. In the text reversal task, model is prompted to generate given sequence of tokens = {x1, x2, . . . , xL} in reversed order = {xL, xL1, . . . , x1}. Locality bias does not apply here because the distance between generated token and its corresponding dependent input token grows linearly as the generation proceeds. It is interesting to investigate whether the REPO method can learn beneficial re-positioning patterns from the task. C.1. Setup We use the data and train/dev/test splits provided in Kazemnejad et al. (2023) for the text reversal task. The sequence lengths in training are between [2, 20], while we use the sequence lengths between [2, 30] for evaluation. The input sequence is constructed from fixed set 13 Figure 5. Sensitivity to the starting layer of REPO (i.e. = 3, 5, 7). We validate on the NIAH subtask of RULER benchmark and MMLUPro of general benchmarks. REPO : Language Models with Context Re-Positioning of subwords that are shared across the three datasets, without regard to grammatical or semantic structure (Kazemnejad et al., 2023). We train GPT-2 model with 4 layers for this task. We use NoPE, RoPE, and REPO6 methods to train the model. For the REPO method, we shared the parameters of fϕ for all the attention heads in each layer. All training hyper-parameters are set as in Kazemnejad et al. (2023). C.2. Findings As shown in Figure 7, due to the simplicity of the text reversal task, all the models achieve nearly perfect performance on short in-domain sequences (L 20). However, when testing on examples with longer-range dependencies, i.e. > 20, our REPO method demonstrates superior performance compared to both NoPE and RoPE. We further investigate the re-positioning patterns learned by REPO that contribute to this performance gain. As illustrated in Figure 6, we visualize the predicted positions across different layers of the trained model and observe several intriguing patterns. The overall distribution of predicted positions is remarkably distinct from the predefined positional indices (e.g., 1 to 27). Specifically, we observe mirror effect in layers 0, 2, and 3, where pairs of reversal tokens are assigned the same position indices. Additionally, we identify hybrid positional patterns across different parts of the context. For example, from layers 1 to 3, the model adopts NoPE-like pattern for the opening tokens, assigining nearly identical position indices to tokens in the phrase Reverse the following words:, while exhibiting patterns with mirror symmetry for the input and output sequences. Notably, we did not introduce any inductive bias for this task; all patterns emerged in purely data-driven manner. These intriguing patterns motivate us to further investigate REPO on more general datasets. Figure 7. Performance on the text reversal task. We report the accuracy on all lengths of input sequences. D. Case Study As shown in Figure 8, we visualize the positions assigned by REPO when testing on the MMLUPro benchmark (Wang et al., 2024b) with few-shot examples. We observe that REPO learns distinct patterns across different layers and attention heads. Interestingly, as shown in Figure 8b, the patterns of assigned positions roughly align with the semantic segmentation of the few-shot examples, demonstrating that REPO is capable of capturing the structure of the input context. Additionally, we find that some positions assigned by REPO are negative values, as shown in Figure 8c. Those negative positions can be interpreted as rotations in reversed direction under the RoPE framework. There also exist some outlier positions in the figures. Upon inspection, we find that they correspond to non-informative punctuation marks and function words, such as . and such. 6Notably, REPO functions solely as position prediction module. For brevity, however, mentioning REPO implies the use of RoPE encoding together in this work. 14 REPO : Language Models with Context Re-Positioning (a) Layer 5 & Attention Head 1 (b) Layer 8 & Attention Head Figure 8. Visualization of positions assigned by REPO. The REPO is continuously trained on general data. The visualization data is from MMLUPro with few-shot examples. Symbols in orange belong to the prompt, while symbols in blue and red represent questions and answers in few-shot examples. (c) Layer 13 & Attention Head"
        }
    ],
    "affiliations": [
        "Nara Institute of Science and Technology",
        "Sakana AI, Japan"
    ]
}