{
    "paper_title": "SWE-Lego: Pushing the Limits of Supervised Fine-tuning for Software Issue Resolving",
    "authors": [
        "Chaofan Tao",
        "Jierun Chen",
        "Yuxin Jiang",
        "Kaiqi Kou",
        "Shaowei Wang",
        "Ruoyu Wang",
        "Xiaohui Li",
        "Sidi Yang",
        "Yiming Du",
        "Jianbo Dai",
        "Zhiming Mao",
        "Xinyu Wang",
        "Lifeng Shang",
        "Haoli Bai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present SWE-Lego, a supervised fine-tuning (SFT) recipe designed to achieve state-ofthe-art performance in software engineering (SWE) issue resolving. In contrast to prevalent methods that rely on complex training paradigms (e.g., mid-training, SFT, reinforcement learning, and their combinations), we explore how to push the limits of a lightweight SFT-only approach for SWE tasks. SWE-Lego comprises three core building blocks, with key findings summarized as follows: 1) the SWE-Lego dataset, a collection of 32k highquality task instances and 18k validated trajectories, combining real and synthetic data to complement each other in both quality and quantity; 2) a refined SFT procedure with error masking and a difficulty-based curriculum, which demonstrably improves action quality and overall performance. Empirical results show that with these two building bricks alone,the SFT can push SWE-Lego models to state-of-the-art performance among open-source models of comparable size on SWE-bench Verified: SWE-Lego-Qwen3-8B reaches 42.2%, and SWE-Lego-Qwen3-32B attains 52.6%. 3) We further evaluate and improve test-time scaling (TTS) built upon the SFT foundation. Based on a well-trained verifier, SWE-Lego models can be significantly boosted--for example, 42.2% to 49.6% and 52.6% to 58.8% under TTS@16 for the 8B and 32B models, respectively."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 4 ] . [ 1 6 2 4 1 0 . 1 0 6 2 : r TECHNICAL REPORT SWE-LEGO: PUSHING THE LIMITS OF SUPERVISED FINE-TUNING FOR SOFTWARE ISSUE RESOLVING Chaofan Tao1 Jierun Chen1 Yuxin Jiang1 Kaiqi Kou1 Shaowei Wang1 Ruoyu Wang* 2 Xiaohui Li 1 Sidi Yang3 Yiming Du4 Jianbo Dai1 Zhiming Mao4 Xinyu Wang1 Lifeng Shang1 Haoli Bai 1 1Huawei Technologies, 2NTU, 3HKU, 4CUHK https://github.com/SWE-Lego/SWE-Lego https://huggingface.co/SWE-Lego"
        },
        {
            "title": "ABSTRACT",
            "content": "We present SWE-Lego, supervised fine-tuning (SFT) recipe designed to achieve state-ofthe-art performance in software engineering (SWE) issue resolving. In contrast to prevalent methods that rely on complex training paradigms (e.g., mid-training, SFT, reinforcement learning, and their combinations), we explore how to push the limits of lightweight SFT-only approach for SWE tasks. SWE-Lego comprises three core building blocks, with key findings summarized as follows: 1) the SWE-Lego dataset, collection of 32k highquality task instances and 18k validated trajectories, combining real and synthetic data to complement each other in both quality and quantity; 2) refined SFT procedure with error masking and difficulty-based curriculum, which demonstrably improves action quality and overall performance. Empirical results show that with these two building bricks alone, the SFT can push SWE-Lego models to state-of-the-art performance among open-source models of comparable size on SWE-bench Verified: SWE-Lego-Qwen3-8B reaches 42.2%, and SWE-Lego-Qwen3-32B attains 52.6%. 3) We further evaluate and improve test-time scaling (TTS) built upon the SFT foundation. Based on well-trained verifier, SWE-Lego models can be significantly boostedfor example, 42.2%49.6% and 52.6%58.8% under TTS@16 for the 8B and 32B models, respectively. (a) Performance comparisons. (b) Performance breakdown. Figure 1: Overview of the performance by SWE-Lego models and its breakdown analysis. (a) SWE-Lego models establish new frontier on SWE-bench Verified, outperforming same-scale competitors. Notably, our results are based on hack-free evaluation, whereas prior works scores could be inflated by the Git hacking. (b) Our hybrid SWE-Lego dataset delivers the largest boost of +25.6%; subsequent refined SFT adds +3.8%, and TTS contributes +6.2%, together lifting the Qwen3-32B model from 23.2% to 58.8%. Equal contribution, listed in the random order; Corresponding authors: {lixiaohui33,baihaoli}@huawei.com."
        },
        {
            "title": "Introduction",
            "content": "Recent advances of large language models (LLMs) have led to remarkable progress in autonomous software engineering (SWE) agents, capable of repository-scale issue resolving (Xia et al., 2024; Yang et al., 2024; Zhang et al., 2024). Unlike short, single-file tasks (Austin et al., 2021; Chen et al., 2021a; Du et al., 2023; Li et al., 2022), SWE agents are able to navigate code repositories, faithfully reproduce failing tests, identify buggy files and lines, implement fixes, and validate within executable sandboxes (Liu et al., 2024; Shrivastava et al., 2023; Yang et al., 2023), which demands long-horizon reasoning, robust tool use, and multi-turn interaction abilities. To equip LLMs with these capabilities, common approach is supervised fine-tuning (SFT), which distills knowledge from stronger SWE agents. However, prior efforts often lack datasets that include execution environments and realistic bug instances needed to produce high-quality, diverse trajectories (Jain et al., 2025; Wang et al., 2025b; Yang et al., 2025c). Beyond SFT, performance on SWE tasks can be further improved via mid-training, but this typically requires substantially more computation and data (Copet et al., 2025; Yang et al., 2025d). Alternatively, recent work applies reinforcement learning (RL) without fixed teacher model or predefined SWE trajectories (Luo et al., 2025; Sonwane et al., 2025; Wei et al., 2025), yet limited executable instances remain bottleneck. In addition, RL is more computationally intensive, demands heavier training infrastructure, and is prone to training collapse due to hyperparameter sensitivity. These practical constraints motivate critical question: how far can we push lightweight, SFT-only training recipe for SWE tasks? To investigate, we propose SWE-Lego, simple yet powerful SFT framework for training SWE agents. We discover that with careful data design, training techniques, and test-time scaling for inference, SFT alone can achieve state-of-the-art performance, matching or surpassing more complex training paradigms under similar LLM scales, as shown in Figure 1. SWE-Lego comprises three core building blocks: We introduce the SWE-Lego dataset, collection of 32k high-quality task instances and 18k validated expert trajectories. The SWE-Lego dataset is constructed from two complementary sources: high-quality real-world GitHub pull requests (Badertdinov et al., 2025), and scalable synthetic instances (Yang et al., 2025c), where both sources complement each other in quality and quantity: the real-world data provides the majority of benefits but is hard to scale, while the synthetic data provides additional gains with further scaling. Furthermore, we implement rigorous trajectory validation and filtering, such as preventing agents from Git hacking (i.e., inspecting Git history to recover solutions) to ensure the data teaches genuine problem-solving skills. We refine the conventional supervised fine-tuning to better train SWE agents. While standard SFT approaches have been widely applied for training (Wang et al., 2025b; Yang et al., 2025c), they typically treat every token in an expert trajectory equally during loss calculation. This overlooks critical issue: trajectories often contain both successful actions and intermediate errors, and naively learning from mistakes could reinforce undesired behavior. To address this, we introduce step-level error masking, which excludes tokens associated with execution-time errors (e.g., tool failures or failing tests) from the loss. Moreover, we introduce another refinement of difficulty-based curriculum learning, which trains the model on easier tasks before progressing to harder ones, with difficulty estimated empirically by trajectory length. To trade additional compute for further gains, we study test-time scaling (TTS) (Jain et al., 2025; Luo et al., 2025; OpenHands Team, 2025) along two orthogonal dimensions: (i) sequential scaling by increasing the maximum number of interaction turns, and (ii) parallel scaling via multiple rollouts with verifier selection. We seek to explore the best TTS strategies for SWE tasks given the compute constraints. We find that sequential scaling saturates around 100140 turns; beyond this point, compute is better allocated to parallel rollouts. In addition, generative verifiers (Jain et al., 2025; Pan et al., 2025) consistently outperform regression-based scorers (OpenHands Team, 2025) for parallel scaling. Empirically, SWE-Lego establishes new state-of-the-art results among open-source models on SWE-bench Verified (Jimenez et al., 2024): SWE-Lego-Qwen3-8B reaches 42.2%, and SWE-Lego-Qwen3-32B attains 52.6% using only SFT (Figure 1a). With test-time scaling, the performance rises to 49.6% and 58.8% under TTS@16, respectively. These results demonstrate that meticulously crafted SFT pipeline can rival or exceed the performance of more complex and computationally expensive training regimes. Notably, our results are obtained without Git hacking, whereas prior reports are often inflated by such solution leakage. Ablations in Figure 1b show that our SWE-Lego dataset yields substantial boost of +25.6%, refined SFT contributes an additional +3.8%, and TTS ultimately adds +6.2%, which together lift Qwen3-32B from 23.2% to 58.8%, confirming the effectiveness of each building blocks. We hope SWE-Lego can provide reproducible and lightweight post-training paradigm for building effective SWE agents, and the associated data and models of this work will be open-sourced for future research in the community. 2 Dataset Type Executable Repositories Task Instances Valid Trajectories R2E-Gym-Train SWE-Swiss SWE-Fixer SWE-Gym SWE-bench-Train SWE-rebench Synthetic Synthetic Real Real Real Real SWE-smith Real & Synthetic SWE-Lego Real & Synthetic Yes Yes No Yes No Yes Yes Yes 10 <600 856 11 37 3.5k 128 3251 4.6k 10k 115k 2.4k 19k 21k 50k 3.3k NA NA 491 NA NA 5k 32.1k 14.1k resolved + 4.0k semi-resolved Table 1: Comparison of public SWE issue-resolving datasets and our SWE-Lego dataset. The proposed dataset combines real and synthetic instances at scale, with executable environments and large pool of validated trajectories, providing stronger foundation for training SWE agents. Note that SWE-Swiss and SWEFixer release only sub-trajectories, and therefore, counts of full trajectories are not reported. Semi-resolved means the trajectories do not pass all test cases but have partial correctness, e.g., correct file localization. Figure 2: Our SWE-Lego pipeline comprises three stages: environment construction from over 3,000 repositories; hybrid task creation by combining real pull requests with synthetic bugs; and expert-trajectory generation and curation for SFT."
        },
        {
            "title": "2 SWE-LEGO Dataset: Combining Real-world and Synthetic Data",
            "content": "2.1 Overview LLMs are increasingly integrated into autonomous agents for real-world SWE tasks. We adopt the SWEAgent paradigm (Yang et al., 2024), in which an LLM operates through specialized agent-computer interface that allows it to autonomously navigate repositories, search and edit code, and execute tests to diagnose and resolve issues. In this context, SWE task instance is usually defined as the concrete SWE issue within codebase, comprising natural-language problem description, test cases, reference golden patch, and related artifacts. Besides, trajectory is the complete sequence of actions taken by an agent and the interleaved observations from environments. To train capable SWE agents, our primary goal is to curate large-scale, diverse, and executable collection of task instances, together with high-quality, validated trajectories. We adopts hybrid data construction strategy that combines real-world and synthetic SWE task instances, as shown in Figure 2. This is motivated by the empirical observation that neither data source alone is sufficient: real-world instances, while authentic, are inherently limited in quantity given strict filtering criteria. On the other hand, synthetic instances, while scalable, lack the complexity of natural software repositories  (Table 3)  . 3 (a) Scaling of Valid Trajectories (b) Performance Scaling on SWE-bench Verified Figure 3: Impact of synthetic data augmentation on data scale and performance. (a) The number of valid expert trajectories increases substantially with the addition of synthetic instances, demonstrating an effective way to expand supervision beyond limited real-world PRs. (b) The resolve rate on SWE-bench Verified rises as more synthetic instances are added per repository, across different repository counts, indicating that hybrid data improves not only dataset scale but also downstream model effectiveness. Insts./Repo denotes the average number of instances per repository. We therefore mix the real with synthetic instances, and apply rigorous generation and validation procedures to produce high-quality trainable trajectories. The resulting SWE-Lego dataset comprises 32k executable task instances and 18k validated expert trajectories to date, as summarized in Table 1. Compared with existing open-source SWE datasets, SWE-Lego spans more repositories, offers more executable task instances, and contains substantially more validated trajectories. Moreover, scaling this hybrid corpus consistently yields better downstream performance on SWE-bench Verified, as evidenced in Figure 3. In the following subsections, we detail the pipeline used to construct the SWE-Lego dataset. 2.2 SWE Task Instance Creation and Validation Repository Collection and Sandbox Construction. The proposed dataset is built upon SWErebench (Badertdinov et al., 2025), large collection of real-world executable repositories. We select over 3,000 Python-centric repositories with permissive licenses and active maintenance histories. To ensure reproducibility, we deploy fully automated pipeline that parses configuration files (e.g., setup.py) to build Docker containers. Only repositories that successfully build and pass sanity tests are retained, serving as the verifiable base plates for our subsequent task construction. Real-world and Synthetic Task Construction. Real-world tasks are derived from resolved GitHub pull requests (PRs) paired with corresponding issue descriptions. They offer high authenticity with production-level bug complexity, but are often labor-intensive and limited in quantity due to the strict filtering criteria (Badertdinov et al., 2025; Jain et al., 2025; Pan et al., 2025; Zeng et al., 2025). Moreover, each real-world task requires unique sandbox aligned with the pre-fix snapshot. While SWE-smith (Yang et al., 2025c) proposes PR mirror method to generate real-world instances (prompting LLMs to revert PR edits via .diff plaintext instead of checking out the base commits), we find this approach yields lower-quality data compared to directly collecting PRs that satisfy SWE-rebench (Badertdinov et al., 2025)s filtering rules. Thus, we adopt the latter strategy for real-world task construction. Synthetic tasks, by contrast, are generated via active bug injection following SWE-smith (Yang et al., 2025c), leveraging two core techniques: (i) LLM Rewrite: prompting models to rewrite code using only function headers and docstrings; (ii) AST Reformulation: extracting abstract syntax trees (ASTs) for classes/functions and applying random transformations, e.g., removing conditionals/loops, modifying operators or dependencies. Synthetic tasks enable high scalability and efficiency, as they are not constrained by the availability of filtered real PRs. Even repositories with no qualified PRs can be used to create large-scale tasks at low cost. 4 Core Field golden patch FAIL-TO-PASS & PASS-TO-PASS Real-World Tasks Synthetic Tasks Merged PR diff as the ground-truth fix. Inverse the patch of the programmatically injected bug. Label of test sets that induced by running the original project tests before and after the merged patch. Label of test sets that induced by running the same tests on buggy vs. restored (pre-injection) code. problem statement Original GitHub issue linked to the PR. image name (Sandbox) Per-PR sandbox tied to the exact pre-merge commit. LLM-generated description conditioned on the golden patch, test files, and logs. Shared sandbox for many injected bugs on the same base commit given the repository. Table 2: High-level comparison of how core fields are instantiated for real-world and synthetic SWE tasks in SWE-Lego. Both sources populate the same unified schema, but differ in data origin and sandbox granularity, leading to realistic yet costly real instances and scalable, controllable synthetic ones. Metric Real-World Tasks Synthetic Tasks Real-World + Synthetic Tasks Category Instances Issue Length Golden Patch Test Cases Words Files Hunks Lines Fail-to-Pass Pass-to-Pass Total Valid Trajectories 18409 135 3.7 9.5 138.0 12.4 74.6 87.0 13710 168 1.0 1.3 18.8 10.9 41.2 52.1 9100 149 2.6 6.0 87.3 11.7 60.4 72.1 14110 Table 3: Characteristic comparisons of task instances. Real-world tasks are more complex, touching more files and modifying more lines, while synthetic tasks are more focused and yield more valid, fully resolving trajectories. The combined dataset covers broader spectrum of difficulty. Additionally, synthetic tasks from the same repository share single sandbox environment, since all bugs are injected into the same codebase version. These two data sources are complementary for scaling SWE tasks: real-world data provides depth (complexity and realism), while synthetic data provides breadth (quantity and coverage). Table 2 summarizes the core-field differences between the two, and additional implementation details are provided in Appendix A.1. Table 3 presents quantitative comparison of complexity. We observe stark contrast: real-world instances touch more files and change more lines than synthetic ones. This confirms that real-world data captures the sprawl of complex bugs, whereas synthetic data offers focused, surgical training signals and yields higher rate of valid trajectories for training. Further analysis of issue categories is provided in Appendix A.2. Figure 3 demonstrates the effectiveness of this hybrid approach: for fixed set of repositories, scaling synthetic data, e.g., from real only to real + 5 synthetic insts./repo, consistently improves both the number of valid expert trajectories and the resolve rate of the trained models. Together with Table 1, this shows that scaling executable hybrid data, rather than real instance alone, is key to obtaining richer supervision signals and higher downstream performance. 2.3 Trajectory Rollout and Validation With the hybrid task instances, we now roll out expert trajectories that are later used for SFT. We adopt the OpenHands scaffold (v0.53) (Wang et al., 2024) with Qwen3-Coder-480B-A35B-Instruct (Yang et al., 2025a) as the teacher agent, and set the maximum number of interaction turns to 100. The model offers competitive coding and reasoning capabilities on challenging SWE tasks and, being open-weight, can be deployed locally at scale for trajectory distillation. During distillation, we found that the agent sometimes peeked at Git history or future commits, thereby cheating rather than faithfully resolving the issue. As 5 Figure 4: Examples of problematic commands or tool interactions: (a) high-risk commands that can cause Git hacking; (b) view range parameter mis-specification; (c) ineffective task tracker. showcased in Figure 4, we also observed avoidable tool-calling errors and tools that were ineffective in practice. To enhance trajectory quality, we propose and implement the following three practices. Preventing Git Hacking. Recently it is known by the SWE community that LLM agents can unexpectedly hack git metadata to locate the golden patch by directly inspecting commit logs2. demonstrative example is shown Figure 4(a), and its associated full trajectory is listed in Appendix D. To prevent such Git hacking, we sanitize the repository history exposed to the agent. For real instances, we remove all commits and log messages dated after the issue creation date so that future fixes are invisible. For synthetic instances, since the buggy version of repository is ahead from the non-buggy version due to intentional bug injection, we remove the entire git history and all logs, exposing only single snapshot of the buggy codebase. This forces the agent to genuinely reason about code and tests rather than reading answers from version control. As expected, we see in Table 4 that the valid-trajectory rate drops slightly from an inflated 31% to 30%, and the average number of turns increases bit from 61.8 to 62.5. In addition, as mentioned in Section 5, we also report the results of SWE-bench Verified without Git hacking throughout this paper, unless specified otherwise. Handling Malformed Tool Errors. With Qwen3-Coder-480B-A35B-Instruct, we observe frequent malformed calls to the str replace editor tool, e.g., passing string to view range or specifying out-ofbounds line ranges (see Figure 4(b)), causing tool failures and waste of interaction budget. To mitigate these errors, we apply lightweight post-hoc correction, i.e., parsing strings to integers and clipping ranges to valid spans, which enables the agent to inspect code more reliably. Without robust tool handling, the validtrajectory rate decreases slightly from 30% to 29%, and the average number of turns increases from 62.5 to 66.6 (see Table 4). Pruning Ineffective Tools. Recent work (Anthropic, 2025; OpenAI, 2025) has begun integrating taskmanagement tools such as task tracker for long-horizon planning and progress tracking. However, we find that Qwen3-Coder-480B-A35B-Instruct struggles to use them effectively, which often cause execution errors (see Figure 4(c)). We therefore discard this tool and restrict the tool set to four essential operations: execute bash, str replace editor, think, and finish, to keep trajectories streamlined, as evidenced by fewer interaction turns (from 65.5 to 62.5) and higher valid-trajectory rate (from 27% to 30%) in Table 4. Validation and Filtering. Quality control is enforced through rigorous post-hoc validation. We classify trajectories as resolved if they pass all tests without regression. However, not all resolved trajectories are equal, nor are all unresolved ones useless. As shown in Table 5, our filtering strategy is two-fold: (i) we filter low-quality resolved trajectories (e.g., those that cheat by modifying tests), which yields modest performance gain; (ii) we recycle semi-resolved trajectories, i.e., those that correctly locate the buggy file It results 4k additional trajectories for training. Adding (with the recall rate of 100%) but fail to fix it. these trajectories provides clear boost (+1.2%), demonstrating that even partial successes contain valuable supervision for fault localization. 2https://github.com/SWE-bench/SWE-bench/issues/465, posted on Sep 3, 2025. Practice Avg Turns Valid Traj. Rate (%) All w/o preventing Git hacking w/o handling malformed tool errors w/o pruning ineffective tools 62.5 61.8 66.6 65.5 30 31 29 27 Data Strategy # Traj Resolve Rate (%) All resolved - low-quality resolved + semi-resolved 14.6k 14.1k 18.1k 40.4 41.0 42.2 Table 4: Ablation of rollout practices. The valid trajectory rate is evaluated across 100 SWE-Lego instances. Table 5: Ablation of filtering strategies on SWE-bench Verified. Takeaways We introduce the SWE-Lego dataset, comprising 32k high-quality software engineering task instances and 18k expert trajectories, the core building brick for training effective SWE agents. We demonstrate that combining real-world and synthetic data yields complementary benefits in both quantity and quality. Moverover, trajectory curation and filtering further enhance data utility, thereby improving the issue resolving rate."
        },
        {
            "title": "3 Refined Supervised Fine-tuning",
            "content": "Conventional supervised fine-tuning on expert trajectories has proven effective for SWE tasks, however, the intrinsic characteristics of these trajectories remains underexplored. trajectory may ultimately resolve an issue, yet it often contains erroneous intermediate steps, e.g., failed attempts and malformed tool calls. Rather than blindly learning from the entire trajectory, we propose step-level error masking, which enables the model to learn from correct actions while excluding incorrect ones. Furthermore, we investigate curriculum learning in the SWE context, wherein progressively increasing trajectory difficulty better guides the training process. 3.1 Step-level Error Masking While the high-quality trajectories identified in Section 2.3 offer valuable learning signals at macro level, we propose more granular refinement: step-level error masking. This approach is motivated by the observation that expert demonstrations rarely proceed monotonically to success; they often contain intermediate missteps, such as invalid function calls or incorrect arguments, that are later identified and corrected. Our method maintains the full trajectory context but selectively masks the loss calculation on erroneous agent responses. As illustrated in Figure 5, we use regular expression to identify error messages provided by the terminal environment, and apply error masking to the corresponding agent response. Crucially, errors stemming from reproducing bugs or executing test files are excluded from this masking process. This technique applies gradient updates solely to valid actions, thereby providing robust learning signal that enhances both action generation and error recovery capabilities. By emphasizing learning from correct actions, it directly reduces core reasoning failures, such as Incorrect Implementation and Localization Error, that are prevalent in later training stages (Section 3.4). Empirical results in Table 6 demonstrate that step-level error masking improves model performance by over 2 points. 3.2 Difficulty-Based Curriculum Learning To maximize sample efficiency and training performance, we implement curriculum learning strategy that progressively exposes the model to tasks of increasing complexity. This approach encourages the model to acquire fundamental skills from simpler examples before advancing to more challenging problems, thereby promoting more robust generalization. We explore two principled methods for difficulty categorization: Model-Based Scoring: We fine-tune Qwen2.5-Coder-32B-Instruct (Hui et al., 2024) model as difficulty scorer on dataset of 1.5k human-annotated (task, difficulty) pairs. This scorer achieves 70% accuracy on held-out test set and classifies tasks into three tiers: Easy (estimated solving time < 15 minutes), Medium (15 minutes to 1 hour), and Hard (> 1 hour). Trajectory-Length-Based Heuristic: We discover strong negative correlation (r = 0.95) between instance resolve rate and the number of turns in the trajectory, as depicted in Figure 6. Leveraging this finding, we partition the data into three difficulty bins based on turn count: Easy (0-50 turns), Medium (50-70 turns), and Hard (70-100 turns). 7 Figure 5: An example of step-level error masking, which maintains the complete trajectory context while selectively masking the loss calculation on incorrect agent responses. Error Masking Curriculum Learning Resolve Rate (%) SWE-Lego-8B SWE-Lego-32B 42.2 41.8 (-0.4) 40.2 (-2.0) 39.4 (-2.8) 52.6 51.8 (-0.8) 50.4 (-2.2) 48.8 (-3.8) Figure 6: Correlation between number of turns and average resolve rate. Table 6: Ablation of training strategies. For curriculum learning, we select the trajectory-length-based heuristic because (i) it outperformed the modelbased approach by 0.5 points, and (ii) it is more computationally efficient and less prone to mislabeling. We adopt three-stage SFT curriculum based on difficulty rankings. In each stage, the model is trained exclusively on data from the corresponding difficulty tier. To mitigate catastrophic forgetting of previously acquired skills, the training data for each subsequent stage is augmented with all data from the preceding stages. This curriculum aligns with the training dynamics observed in Section 3.4: it first grounds the model on Easy tasks to overcome basic Failed to Reproduce errors, then introduces Hard tasks to develop the strategic planning needed to avoid Ran Out of Max Turns failures. Table 6 shows that curriculum learning consistently enhances the performance of both the 8B and 32B models, with the highest results achieved when combined with error masking. 3.3 Training Details For training, we use the Qwen3-8B/32B3 (Yang et al., 2025a) and the internal Pangu-7B/32B series as the base models. We perform full-parameter SFT on the collected SWE-Lego trajectories using LLaMA-Factory framework (Zheng et al., 2024). Models are trained for 4 epochs with global batch size of 64. We use the AdamW optimizer (Loshchilov and Hutter, 2019) with weight decay of 0.01 and cosine learning rate schedule with warmup ratio of 0.1. The learning rate is set to 1e-4 for the 7B/8B models and 5e-5 for 32B 3https://huggingface.co/Qwen/Qwen3-8B; https://huggingface.co/Qwen/Qwen3-32B 8 (a) Resolve Rate Improvement Trend (b) Error Type Distribution Across Epochs Figure 7: Evolution of model performance and failure modes over 4 epochs. (a) Resolve rate increases steadily, reaching 42.2% by epoch 4. (b) Error modes shift from early Failed to Reproduce to mid Ran Out of Max Turns to late Incorrect Implementation and Localization Error bottlenecks. models. To enable training with long-horizon trajectories, we use maximum context length of 128k tokens. For the Qwen3 series, RoPE scaling techniques such as YaRN (Peng et al., 2023) are adopted to support context lengths of up to 128k tokens. 3.4 Error Analysis during Training To validate our methodology, we analyze the agents failure modes during training by categorizing all failed instances (including unresolved, error, and empty patch cases) into five error types based on the agents trajectory: Failed to Reproduce: The agent either did not attempt reproduction or attempted but failed to reproduce the issue. Read Localization Error: The agent reproduces the issue but fails to open or inspect all files modified in the ground-truth (golden) patch. Write Localization Error: The agent opens all required files but fails to identify the correct lines to edit, either no edits are made to ground-truth files or the edited lines have no overlap with the ground-truth patch. Ran Out of Max Turns: The agent successfully reproduced the issue and correctly localized the read and write locations, but reached the maximum iteration limit (100 turns) before completing the task. Incorrect Implementation: The case does not fall into any of the above categories; the agent appears to find the right location but implements an incorrect fix, resulting in failing tests. Figure 7 shows clear progression. Early on, Failed to Reproduce errors (38.97%) dominate, indicating basic task misalignment. After epoch 1, this error vanishes, but Ran Out of Max Turns errors spike (35.14%), revealing strategic planning deficit. In later epochs, Incorrect Implementation and Localization Error become the bottlenecks. This shift from process failures to reasoning failures motivates our combined approach: curriculum learning sequentially addresses the early and mid-stage errors by structuring task complexity, while step-level error masking specifically hones the fine-grained reasoning needed to overcome the persistent late-stage errors. Takeaways We refine conventional SFT for SWE tasks with two innovations: (1) step-level error masking, which enables the model to learn from effective intermediate actions, and (2) curriculum learning, which progressively increases task difficulty, approximated by the number of interaction turns. Our refined SFT outperforms conventional SFT by 3.8%, establishing new state-of-the-art performance among open-source models of comparable size on SWE-Bench Verified: SWE-LegoQwen3-8B hits 42.2%, and SWE-Lego-Qwen3-32B reaches 52.6%. 9 Figure 8: Resolve rate on SWE-bench Verified as function of maximum interaction turns (x-axis) and number of parallel rollouts (y-axis). Curves indicate iso-latency contours where different sequential-parallel combinations yield equivalent wall-clock time. Sequential scaling is most effective at low turn counts; after saturation, improvements are driven primarily by parallel scaling."
        },
        {
            "title": "4 Test-time Scaling",
            "content": "Test-time scaling (TTS) can effectively improve the SWE agent performance by allocating additional compute during inference. Typically there are two complementary dimensions: sequential scaling with more interaction turns, and parallel scaling that generates multiple rollouts and selects the best via verifier. However, it still remains unclear their optimal balance, given limited constraints on compute budget or latency. In Section 4.1, we first systematically investigate this trade-off and identify when to prioritize each dimension. We further investigate the design of verifier for better parallel scaling in Section 4.2. 4.1 Balancing Sequential and Parallel Scaling We investigate resource allocation between sequential and parallel scaling by varying both the maximum interaction turns and the number of candidate rollouts. We use latency as the cost metric for budget control. For sequential turns, as the trajectory lengthens, the growing context requires more computation per turn, resulting in latency that scales super-linearly (between linear and quadratic) with respect to the number of interaction turns; In contrast, candidate rollouts are executed in batch, so latency exhibits sub-linear scaling with respect to the number of rollouts. As shown in Figure 8, sequential scaling is highly efficient in the low-turn regime: additional turns yields more environment feedback, enables error correction, and allows iterative refinement. This makes sequential scaling the preferred strategy under tight latency budgets. However, performance starts to saturate around 100140 turns. The agent either succeeds within the allotted turns or encounters obstacles that additional turns alone cannot overcome (more analysis in Appendix B.1). Beyond this saturation point, parallel scaling coupled with verifier-based selection becomes more effective, as independent trajectories explore diverse paths through the solution space. The iso-latency contours in Figure 8 highlight this balance: under equivalent latency, the optimal allocation shifts from sequential-dominated to parallel-dominated as the total latency budget increases. 4.2 Improved Parallel Scaling During parallel scaling, verifier takes trajectory and its proposed patch and returns score between 0 and 1 that reflects the likelihood of successful resolution. We compare two paradigms: the regressive verifier (OpenHands Team, 2025), which appends scoring head to the backbone and is trained with binary cross-entropy loss, and the generative verifier (Jain et al., 2025; Pan et al., 2025), which formulates verification as text 10 (a) Generative vs. Regressive Verifier (b) Comparison with Existing Verifiers Figure 9: Parallel TTS performance on SWE-bench Verified. (a) Ablation on the verifier paradigm: generative verifiers consistently outperform regressive counterparts across model sizes (8B and 32B). (b) Comparison with publicly available verifiers: SWE-Lego-Verifier-8B outperforms OpenHands-Critic-32B and R2E-GymVerifier-14B, with the performance gap widening as rollouts increase. generation by predicting yes or no and computing the score from normalized token probabilities. The generative formulation aligns with the pre-trained next-token prediction objective, potentially better leveraging the models inherent knowledge (Zhang et al., 2025). For each test instance, we generate candidate rollouts and let the verifier select the top-scoring one. TTS@K is the fraction of instances resolved under this top-1 selection from candidates. Regressive vs. Generative Verifier. We compare the two paradigms using the same training data (5k resolved, 13k unresolved trajectories from real-world tasks) and the same backbone (e.g., Qwen3-8B). Emperically, we find that larger verifiers benefit stronger rollout models, so for 32B rollout models we use Qwen3Coder-30B-A3B as the verifier (see Appendix B.2 for more details). For consistency with prior work, we set the maximum interaction turns to 100 in all verifier experiments. As shown in Figure 9a, the generative verifier consistently outperforms the regressive one. At small K, both achieve similar TTS@K; as increases, the regressive verifier plateaus while the generative verifier continues to improve. On SWE-Lego-Qwen38B, the gap reaches 2.8% at K=16 (49.6% vs. 46.8%). We hypothesize that it is because the generative formulation aligns with next-token prediction, better leveraging pre-trained semantics. We therefore adopt the generative paradigm. Comparison with Existing Verifiers. We compare SWE-Lego-Verifier-8B against publicly available verifiers on SWE-Lego-Qwen3-8B rollouts (Figure 9b). Our verifier achieves TTS@16 of 49.6%, outperforming OpenHands-Critic-32B (44.0%) (OpenHands Team, 2025) and R2E-Gym-Verifier-14B (47.0%) (Jain et al., 2025). Beyond absolute performance, we observe qualitatively different scaling behaviors across verifier paradigms. OpenHands-Critic-32B, which employs the regressive paradigm, exhibits performance degradation at higher K, counterintuitive result indicating that larger candidate pools overwhelm its discriminative capacity. In contrast, both generative verifiers (ours and R2E-Gym) maintain monotonic improvement toward the Pass@K upper bound, further confirming that the generative formulation provides more robust scaling properties. The performance advantage over R2E-Gym-Verifier-14B likely stems from two factors: larger volume of high-quality training trajectories and better distribution alignment between training data and SWE-Lego agent behaviors. Takeaways Sequential-then-Parallel: We suggest to prioritize sequential scaling before certain saturation point, and then allocate remaining compute to parallel scaling. Generative > Regressive: The generative verifier consistently outperforms the regressive variant for parallel scaling across different model sizes and rollout budgets. 11 Model Scaffold Training Resolve Rate (%) OpenAI-GPT-4o (OpenAI, 2024) OpenAI-o3 (OpenAI, 2025b) Claude-4-Sonnet (Anthropic, 2025) Claude-4.5-Sonnet (Anthropic, 2025) OpenAI-GPT-5 (OpenAI, 2025a) Gemini 3 Pro Preview (Google, 2025) Parameters 7B Qwen2.5-Coder-Instruct-7B (Hui et al., 2024) Qwen3-8B (Yang et al., 2025a) SWE-Gym-7B (Pan et al., 2025) SWE-agent-LM-7B (Yang et al., 2025c) Lingma-SWE-GPT-7B (Ma et al., 2025) SWE-Mirror-LM-7B (Wang et al., 2025b) SWE-Dev-7B (Wang et al., 2025a) Klear-Agent-8B-SFT (Kwai-Klear, 2025) Proprietary Models Internal pipeline Mini-SWE-agent SWE-agent Mini-SWE-agent OpenHands Mini-SWE-agent Open-Source Models MOpenHands OpenHands OpenHands SWE-agent SWESynInfer MOpenHands OpenHands Mini-SWE-agent-plus SWE-Lego-Qwen3-8B + TTS@16 SWE-Lego-Pangu-7B + TTS@16 Parameters = 32B Qwen2.5-Coder-Instruct-32B (Hui et al., 2024) Qwen3-32B (Yang et al., 2025a) SWE-Gym-32B (Pan et al., 2025) R2E-Gym-32B (Jain et al., 2025) + TTS@16 + TTS@26 SWE-Dev-32B (Wang et al., 2025a) Skywork-SWE-32B (Zeng et al., 2025) + TTS@8 SWE-agent-LM-32B (Yang et al., 2025c) DeepSWE-32B-Preview (Luo et al., 2025) + TTS@16 SWE-Mirror-LM-32B (Wang et al., 2025b) CWM-32B (Copet et al., 2025) SWE-Lego-Qwen3-32B + TTS@16 SWE-Lego-Pangu-32B + TTS@16 OpenHands OpenHands OpenHands OpenHands MOpenHands OpenHands OpenHands R2E-Gym R2E-Gym R2E-Gym OpenHands OpenHands OpenHands SWE-agent OpenHands OpenHands MOpenHands Agentless OpenHands OpenHands OpenHands OpenHands - - - - - - - - SFT SFT SFT SFT RL SFT SFT SFT SFT SFT - - SFT SFT SFT SFT RL SFT SFT SFT RL RL SFT CPT + SFT + RL SFT SFT SFT SFT 33.2 58.4 66.6 70.6 71.8 74.2 1.0 7.6 10.6 15.2 18.2 22.8 23.4 39.0 44.4 / 42.2 53.4 / 49.6 42.4 / 40.4 - / 46.6 6.2 23.2 20.6 34.4 49.4 51.0 36.6 38.0 47.0 40.2 42.2 59.0 52.2 53.9 57.6 / 52.6 60.4 / 58.8 49.6 / 48.0 - / 52.6 Table 7: Performance comparison on the SWE-bench Verified. Our results are reported in the A/B format, representing the results with and without Git hacking respectively."
        },
        {
            "title": "5 Comparisons with Existing Methods",
            "content": "We compare the performance of SWE-Lego models with both proprietary and open-source baselines on SWE-bench Verified. As mentioned in Section 2.2, Docker images of SWE-bench Verified released prior to Sep 3, 2025 may contain the ground truth commits, which can lead to Git hacking, e.g., via git log --all or by grepping commit messages. For fair comparisons with prior methods, we report our results both with and without git hacking. The results without hacking are obtained using the latest SWE-bench Verified Docker images. From Table 7, we see that our SWE-Lego-Qwen3-8B achieves 42.2% resolve rate with refined SFT and reaches 49.6% with further TTS@16, while SWE-Lego-Qwen3-32B attains 52.6% with SFT and 58.8% with TTS@16. These hack-free results surpass most open-source models and several larger proprietary models, even when their results are generally inflated by git hacking. Our SWE-Lego-Pangu series also delivers competitive performance, further validating the effectiveness of our SWE-Lego dataset and approach."
        },
        {
            "title": "6 Related Work",
            "content": "6.1 Training Dataset for Coding High-quality data is foundational for code intelligence. Early works focused on large-scale pretraining corpora like The Stack (Kocetkov et al., 2022), often employing rigorous filtering pipelines (Li et al., 2023) to ensure quality. The field has since shifted towards instruction tuning, utilizing self-instruct methods (Chaudhary, 2023), Git commit structures (Muennighoff et al., 2023), and synthetic generation techniques like EvolInstruct (Luo et al., 2024) and OSS-Instruct (Wei et al., 2023). Broader instruction tuning efforts have also contributed significantly, with datasets like OpenHermes (Teknium, 2023) and Glaive (GlaiveAI, 2023) providing extensive general and code-specific training data. Recently, attention has turned to repository-level SWE datasets. Works such as R2E-Gym (Jain et al., 2025), SWE-smith (Yang et al., 2025c), and SWErebench (Badertdinov et al., 2025) provide scalable pipelines for task collection and environment construction. However, there remains scarcity of large-scale, verifiable expert trajectories for long-horizon issue fixing, gap our hybrid dataset addresses. 6.2 Coding LLMs and Agents Recent progress spans both powerful foundation models and agentic frameworks that enable autonomous software engineering. Foundation models such as Code Llama (Rozi`ere et al., 2023) and DeepSeek Coder (Guo et al., 2024) continue to serve as critical backbones, achieving state-of-the-art performance through massive code pretraining and extended context windows. To handle repository contexts, some approaches integrate RAG (Shrivastava et al., 2023) or AST-based retrieval (Zhang et al., 2024). Agentic frameworks such as Agentless (Xia et al., 2024), SWE-agent (Yang et al., 2024), Mini-SWE-Agent (agent Team, 2024), MiniSWE-Agent-Plus (Kwai-Klear, 2025) OpenHands (Team, 2024) and MOpenHands (Zan et al., 2025) serve as scaffolds to streamline interactions with development environments. Others emphasize planning (Bairi et al., 2023) that views the repository-level coding as planning problem or self-evolving the SWE agents on the fly(Xia et al., 2025). 6.3 Benchmarks for Software Engineering Evaluation has progressed from function-level synthesis (Chen et al., 2021b) to repository-level tasks. Benchmarks like ClassEval (Du et al., 2023), CrossCodeEval (Ding et al., 2023), and RepoBench (Liu et al., 2024) assess broader context utilization. For autonomous agents, the focus is on realistic issue resolving, utilizing datasets like Defects4J (Just et al., 2014) and environments like InterCode (Yang et al., 2023). SWEbench (Jimenez et al., 2024) has become the standard for GitHub issue resolution, inspiring variants for multilingual (Zan et al., 2025), multimodal (Yang et al., 2025b), and comprehensive agentic evaluation (Xu et al., 2025). Safety benchmarks like CyberSecEval (Bhatt et al., 2024) further ensure code security. Our work aligns with this trend, focusing on scaling capabilities for practical software development."
        },
        {
            "title": "7 Conclusions",
            "content": "We present SWE-Lego, framework consisting of hybrid executable data construction, refined SFT, and verifier-guided TTS, which achieves strong hack-free performance on SWE-bench Verified. The core driver of performance gains lies in scalable, verifiable hybrid data instances and expert trajectories. Meanwhile, error masking and curriculum learning stabilize the training process, and our proposed generative verifier, built on parallel test-time scaling, efficiently allocates inference computing resources to boost the performance further. Collectively, these strategies provide reproducible paradigm for building robust agentic systems for software engineering issue resolving."
        },
        {
            "title": "References",
            "content": "S. agent Team. Mini-swe-agent. https://github.com/SWE-agent/Mini-SWE-Agent, 2024. Anthropic. Claude Sonnet 4. https://www.anthropic.com/claude/sonnet, 2025. [Accessed 31-082025]. Anthropic. Introducing claude sonnet 4.5. https://www.anthropic.com/news/claude-sonnet-4-5, 2025. 2025-09-30. J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. I. Badertdinov, A. Golubev, M. Nekrashevich, A. Shevtsov, S. Karasik, A. Andriushchenko, M. Trofimova, D. Litvintseva, and B. Yangel. Swe-rebench: An automated pipeline for task collection and decontaminated evaluation of software engineering agents, 2025. R. Bairi, A. Sonwane, A. Kanade, V. Arun, et al. Codeplan: Repository-level coding using llms and planning. arXiv preprint arXiv:2309.12499, 2023. M. Bhatt, S. Chennabasappa, Y. Li, C. Nikolaidis, D. Song, S. Wan, F. Ahmad, C. Aschermann, Y. Chen, D. Kapil, et al. Cyberseceval 2: wide-ranging cybersecurity evaluation suite for large language models. arXiv preprint arXiv:2404.13161, 2024. S. Chaudhary. Code alpaca: An instruction-following llama model for code generation. GitHub repository, 2023. URL https://github.com/sahil280114/codealpaca. M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba. Evaluating large language models trained on code, 2021a. M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021b. J. Copet, Q. Carbonneaux, G. Cohen, J. Gehring, J. Kahn, J. Kossen, F. Kreuk, E. McMilin, M. Meyer, Y. Wei, et al. Cwm: An open-weights llm for research on code generation with world models. arXiv preprint arXiv:2510.02387, 2025. Y. Ding, B. Wang, S. Joty, and S. C. Hoi. Crosscodeeval: diverse and multilingual benchmark for cross-file code completion. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. X. Du, M. Liu, K. Wang, H. Wang, J. Liu, Y. Chen, J. Feng, S. Shang, S. Chen, Y. Zhao, et al. Classeval: manually-crafted benchmark for evaluating llms on class-level code generation. arXiv preprint arXiv:2308.01861, 2023. GlaiveAI. Glaive code assistant dataset, 2023. URL https://huggingface.co/datasets/glaiveai/ glaive-code-assistant-v3. Google. new era of intelligence with gemini 3. https://blog.google/products/gemini/gemini-3/, 2025. Published: 2025-11-18; Accessed: 2025-12-22. D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu, Y. Li, et al. Deepseek coder: When the large language model meets programmingthe rise of code intelligence. arXiv preprint arXiv:2401.14196, 2024. B. Hui, J. Yang, Z. Cui, J. Yang, D. Liu, L. Zhang, T. Liu, J. Zhang, B. Yu, K. Dang, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. N. Jain, J. Singh, M. Shetty, L. Zheng, K. Sen, and I. Stoica. R2e-gym: Procedural environments and hybrid verifiers for scaling open-weights swe agents, 2025. C. E. Jimenez, J. Yang, A. Wettig, S. Yao, K. Pei, O. Press, and K. Narasimhan. Swe-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. R. Just, D. Jalali, L. Inozemtseva, M. D. Ernst, R. Holmes, and G. Fraser. Defects4j: database of existing In Proceedings of the 2014 International faults to enable controlled testing studies for java programs. Symposium on Software Testing and Analysis, pages 437440, 2014. D. Kocetkov, R. Li, L. B. Allal, J. Li, C. Mou, C. M. Ferrandis, Y. Jernite, M. Mitchell, S. Hughes, T. Wolf, et al. The stack: 3 tb of permissively licensed source code. Transactions on Machine Learning Research, 2022. Kwai-Klear. mini-swe-agent-plus: The 100-line ai agent that solves github issues with text-edit tool, 2025. URL https://github.com/Kwai-Klear/mini-swe-agent-plus. 14 R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou, M. Marone, C. Akiki, J. Li, J. Chim, et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023. Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):10921097, 2022. T. Liu, C. Xu, and J. McAuley. Repobench: Benchmarking repository-level code auto-completion systems. In The Twelfth International Conference on Learning Representations, 2024. I. Loshchilov and F. Hutter. Decoupled weight decay regularization, 2019. URL https://arxiv.org/ abs/1711.05101. M. Luo, N. Jain, J. Singh, S. Tan, A. Patel, Q. Wu, A. Ariyak, C. Cai, T. Venkat, S. Zhu, B. Athiwaratkun, M. Roongta, C. Zhang, L. E. Li, R. A. Popa, K. Sen, and I. Stoica. Deepswe: Training fully open-sourced, state-of-the-art coding agent by scaling rl. https://www.together.ai/blog/deepswe, 7 2025. URL https://www.together.ai/blog/deepswe. Blog post. Z. Luo, C. Xu, P. Zhao, Q. Sun, X. Geng, W. Hu, C. Tao, J. Ma, Q. Lin, and D. Jiang. Wizardcoder: Empowering code large language models with evol-instruct. In The Twelfth International Conference on Learning Representations, 2024. Y. Ma, R. Cao, Y. Cao, Y. Zhang, J. Chen, Y. Liu, Y. Liu, B. Li, F. Huang, and Y. Li. Swe-gpt: processcentric language model for automated software improvement. Proceedings of the ACM on Software Engineering, 2(ISSTA):23622383, 2025. N. Muennighoff, Q. Liu, A. Zebaze, Q. Zheng, B. Hui, Z. Terry, K. Richardson, P. Lewis, and S. Longpre. Octopack: Instruction tuning code large language models. arXiv preprint arXiv:2308.07124, 2023. OpenAI. Hello gpt-4o. https://openai.com/zh-Hans-CN/index/hello-gpt-4o/, May 2024. Published: 2024-05-13; Accessed: 2025-12-22. OpenAI. Building more with gpt-5.1-codex-max. https://openai.com/index/gpt-5-1-codex-max/, 2025. 2025-11-19. OpenAI. Introducing gpt-5. https://openai.com/zh-Hans-CN/index/introducing-gpt-5/, Aug. 2025a. Published: 2025-08-07; Accessed: 2025-12-22. OpenAI. Introducing openai o3 and o4-mini. https://openai.com/zh-Hans-CN/index/ introducing-o3-and-o4-mini/, Apr. 2025b. Published: 2025-04-16; Accessed: 2025-12-22. OpenHands Team. Openhands critic 32b exp 20250417. https://huggingface.co/OpenHands/ openhands-critic-32b-exp-20250417, 2025. Accessed: 2025-12-08. J. Pan, X. Wang, G. Neubig, N. Jaitly, H. Ji, A. Suhr, and Y. Zhang. Training software engineering agents and verifiers with swe-gym. In Forty-second International Conference on Machine Learning, ICML 2025, Vancouver, BC, Canada, July 13-19, 2025. OpenReview.net, 2025. URL https://openreview.net/ forum?id=Cq1BNvHx74. B. Peng, J. Quesnelle, H. Fan, and E. Shippole. Yarn: Efficient context window extension of large language models, 2023. URL https://arxiv.org/abs/2309.00071. B. Rozi`ere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023. D. Shrivastava, H. Larochelle, and D. Tarlow. Repofusion: Training code models to understand your repository. arXiv preprint arXiv:2306.10998, 2023. A. Sonwane, I. White, H. Lee, M. Pereira, L. Caccia, M. Kim, Z. Shi, C. Singh, A. Sordoni, M.-A. Cote, et al. Bugpilot: Complex bug generation for efficient learning of swe skills. arXiv preprint arXiv:2510.19898, 2025. O. Team. Opendevin: An open platform for ai software developers. https://github.com/OpenDevin/ OpenDevin, 2024. Teknium. Openhermes 2.5: An open source dataset of synthetic data for generalist llm assistants, 2023. URL https://huggingface.co/datasets/teknium/OpenHermes-2.5. H. Wang, Z. Hou, Y. Wei, J. Tang, and Y. Dong. Swe-dev: Building software engineering agents with training and inference scaling. arXiv preprint arXiv:2506.07636, 2025a. J. Wang, D. Zan, S. Xin, S. Liu, Y. Wu, and K. Shen. Swe-mirror: Scaling issue-resolving datasets by mirroring issues across repositories. arXiv preprint arXiv:2509.08724, 2025b. 15 X. Wang, B. Li, Y. Song, F. F. Xu, X. Tang, M. Zhuge, J. Pan, Y. Song, B. Li, J. Singh, et al. Openhands: An open platform for ai software developers as generalist agents. arXiv preprint arXiv:2407.16741, 2024. Y. Wei, Z. Wang, J. Liu, Y. Ding, and L. Zhang. Magicoder: Empowering code generation with oss-instruct. arXiv preprint arXiv:2312.02120, 2023. Y. Wei, Z. Sun, E. McMilin, J. Gehring, D. Zhang, G. Synnaeve, D. Fried1, L. Zhang, and S. Wang. Toward training superintelligent software agents through self-play swe-rl. arXiv preprint arXiv:2512.18552, 2025. C. S. Xia, Y. Deng, S. Dunn, and L. Zhang. Agentless: Demystifying llm-based software engineering agents, 2024. C. S. Xia, Z. Wang, Y. Yang, Y. Wei, and L. Zhang. Live-swe-agent: Can software engineering agents self-evolve on the fly? arXiv preprint arXiv:2511.13646, 2025. J. Xu, K. Deng, W. Li, S. Yu, H. Tang, H. Huang, Z. Lai, Z. Zhan, Y. Wu, C. Zhang, K. Lei, Y. Yao, X. Lei, W. Zhu, Z. Feng, H. Li, J. Xiong, D. Li, Z. Gao, K. Wu, W. Xiang, Z. Zhan, Y. Zhang, W. Gong, and Z. Gao. Swe-compass: Towards unified evaluation of agentic coding abilities for large language models, 2025. A. Yang, A. Li, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Gao, C. Huang, C. Lv, C. Zheng, D. Liu, F. Zhou, F. Huang, F. Hu, H. Ge, H. Wei, H. Lin, J. Tang, J. Yang, J. Tu, J. Zhang, J. Yang, J. Yang, J. Zhou, J. Zhou, J. Lin, K. Dang, K. Bao, K. Yang, L. Yu, L. Deng, M. Li, M. Xue, M. Li, P. Zhang, P. Wang, Q. Zhu, R. Men, R. Gao, S. Liu, S. Luo, T. Li, T. Tang, W. Yin, X. Ren, X. Wang, X. Zhang, X. Ren, Y. Fan, Y. Su, Y. Zhang, Y. Zhang, Y. Wan, Y. Liu, Z. Wang, Z. Cui, Z. Zhang, Z. Zhou, and Z. Qiu. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a. J. Yang, C. E. Yalcin, K. Narasimhan, and S. Yao. Swe-agent: Agent-computer interfaces enable software engineering language models. arXiv preprint arXiv:2405.15793, 2024. J. Yang, C. E. Jimenez, A. L. Zhang, K. Lieret, J. Yang, X. Wu, O. Press, N. Muennighoff, G. Synnaeve, K. R. Narasimhan, D. Yang, S. I. Wang, and O. Press. SWE-bench multimodal: Do ai systems generalize to visual software domains? In The Thirteenth International Conference on Learning Representations, 2025b. URL https://openreview.net/forum?id=riTiq3i21b. J. Yang, K. Lieret, C. E. Jimenez, A. Wettig, K. Khandpur, Y. Zhang, B. Hui, O. Press, L. Schmidt, and D. Yang. Swe-smith: Scaling data for software engineering agents, 2025c. J. a. Yang, S. Yao, K. Narasimhan, and D. Tao. Intercode: Standardizing and benchmarking interactive coding with execution feedback. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. Z. Yang, S. Wang, K. Fu, W. He, W. Xiong, Y. Liu, Y. Miao, B. Gao, Y. Wang, Y. Ma, et al. Kimi-dev: Agentless training as skill prior for swe-agents. arXiv preprint arXiv:2509.23045, 2025d. D. Zan, Z. Huang, W. Liu, H. Chen, L. Zhang, S. Xin, L. Chen, Q. Liu, X. Zhong, A. Li, S. Liu, Y. Xiao, L. Chen, Y. Zhang, J. Su, T. Liu, R. Long, K. Shen, and L. Xiang. Multi-swe-bench: multilingual benchmark for issue resolving, 2025. L. Zeng, Y. Li, Y. Xiao, C. Li, C. Y. Liu, R. Yan, T. Wei, J. He, X. Song, Y. Liu, et al. Skywork-swe: Unveiling data scaling laws for software engineering in llms. arXiv preprint arXiv:2506.19290, 2025. L. Zhang, A. Hosseini, H. Bansal, M. Kazemi, A. Kumar, and R. Agarwal. Generative verifiers: Reward modeling as next-token prediction. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview. net/forum?id=Ccwp4tFEtE. Y. Zhang, H. Ruan, Z. Fan, and A. Roychoudhury. Autocoderover: Autonomous program improvement. In Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis, 2024. Y. Zheng, R. Zhang, J. Zhang, Y. Ye, Z. Luo, Z. Feng, and Y. Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. URL http://arxiv.org/abs/2403.13372."
        },
        {
            "title": "A Additional Information on Data Construction",
            "content": "A.1 Real and Synthetic Instance Construction Here we detail how we collect or generate the core fields, golden patch, FAIL-to-PASS, PASS TO PASS, problem statement and image name of SWE-Lego data instances. golden patch: For real-world instances, the golden patch corresponds to the merged PR diff, representing the human-authored fix. For synthetic instances, we inject bug and then derive the golden patch by reverting this injection, i.e., the diff between the buggy branch and the original clean commit. Specifically, real tasks use buggy pre-commit version of the repository in the sandbox, with the patch serving as the golden solution. For synthetic tasks, following SWE-smith(Yang et al., 2025c)), we place the non-buggy repository version inside the sandbox, while the buggy version resides in remote Git branch; here, the patch field records the bug injection logic. In practice, before solving synthetic task, one must check out the buggy branch in the sandbox. The golden solution is to revert the patch specified in the patch field. FAIL-to-PASS and PASS TO PASS: In real data, we trace tests that are introduced or modified in the PR: tests that fail on the pre-merge commit but pass after applying the patch form the FAIL TO PASS set, while tests that pass in both states are labeled as PASS TO PASS. For synthetic data, we run the repositorys test suite before and after the injected bug. Newly failing tests define FAIL TO PASS, and unaffected tests form PASS TO PASS. This shared test-based view ensures that every instance, regardless of origin, is grounded in executable signals about both correctness and regression. problem statement: For real-world instances, the problem statement is extracted from the GitHub issue linked to the PR, preserving the natural language description written by developers and users. For synthetic instances, we prompt an LLM with the golden patch, failing tests, and execution logs to synthesize realistic issue description that mimics how human would report the bug. The result is unified textual interface where real issues contribute authentic language, and synthetic issues expand coverage in controlled manner. image name: Each real-world instance is associated with unique Docker image (a.k.a., sandbox) built from the exact pre-merge commit, since different PRs can depend on distinct dependency states or build configurations. In contrast, synthetic instances from the same repository typically share sandbox image based on stable base commit, because bug injections operate at the code level without introducing new system-level dependencies. A.2 Analysis of Data Categories Category Description Top-5 Categories Real Data (%) Synthetic Data (%) D API / signature mismatch Logic / conditional bug Input validation / boundary / sentinel handling error Constructor / inheritance / inheritance contract break Missing import / symbol / attribute error The Remaining Categories I State consistency / bookkeeping / caching bug Copy semantics / mutability / in-place mutation of inputs Protocol / spec conformance bug IO / file system / resource handling bug Security / sensitive data leakage 26.6 29.9 16.1 2.5 8.3 6.4 0.7 7.2 1.9 0.4 12.8 49.4 5.9 7.5 19. 2.6 0.3 1.4 0.7 0.1 Table 8: Distribution of categories in real-world and synthetic training dataset. Following the categorization method of SWE instances proposed in BugPilot (Sonwane et al., 2025), we prompt the Claude-4-Sonnet to categorize the training instances SWE-Lego for analyses. The comparison on Table 8 reveals significant complementarity between the real and synthetic data sources in SWE-Lego. Real-world data tends to focus on external interaction and boundary defense (e.g., API 17 (a) Truncation Rate and Average Turns (b) Resolve Rate by Turn Count Figure 10: Analysis of sequential scaling behavior. (a) Truncation rate and average turns under different maximum turn limits: both metrics show diminishing changes beyond 140 turns. (b) Resolve rate across turn count intervals: trajectories with very few or very many turns tend to have lower resolve rates. changes, input validation, protocol conformance), whereas synthetic data emphasizes internal implementation and structural integrity (e.g., logic flow, dependency management). Beyond the inherent scalability of synthetic data generation, incorporating it into the training set offers distributional benefits by supplementing rare structural defects and rebalancing the focus between interface adaptation and internal logic. Additional Analysis on Test-Time Scaling We present additional experiments on test-time scaling, analyzing the saturation in sequential scaling and the effects of training-data scale and verifier-model scale on parallel scaling. B.1 Sequential Scaling Analysis Truncation Rate and Average Turns. As the turn limit increases for SWE-Lego-Qwen3-8B rollouts, the truncation rate declines quickly but becomes marginal beyond roughly 140 turns (Figure 10a). The average number of turns per trajectory similarly rises and then plateaus. Most trajectories either terminate (success or failure) or fall into unproductive states well before exhausting large turn budgets, so additional turns beyond this threshold yield little marginal utility. Resolve Rate by Turn Count. Resolve rates are low for very small turn counts (insufficient exploration) and also for very large counts (the agent is typically stuck, retrying ineffective approaches), with the highest rates appearing at intermediate ranges (Figure 10b). These observations provide empirical support for the sequential scaling saturation phenomenon: beyond certain turn limit, additional sequential compute yields limited marginal benefit. This motivates the shift to parallel scaling with verifier selection once sequential scaling saturates. B.2 Impact of Training Data Scale and Model Scale Training Data Scale. We ablate verifier training data volume on SWE-Lego-Qwen3-8B rollouts by comparing the full set (18K trajectories: 5K resolved + 13K unresolved) to 6K subset matching the scale used by R2E-Gym-Verifier. As shown in Figure 11a, the 18K-trained verifier consistently outperforms the 6K verifier across all K. At TTS@16, it achieves 49.6% vs. 47.6%, with the gap widening as increases, indicating that larger training sets improve discrimination in larger candidate pools. Notably, with the same 6K scale, our verifier slightly outperforms R2E-Gym-Verifier-14B (47.6% vs. 47.0%), suggesting that data quality and alignment with the rollout agent also matter, beyond sheer quantity. Verifier Model Scale. We compare 8B and 30B verifiers (Qwen3-8B and Qwen3-Coder-30B-A3B backbones) when scoring rollouts from SWE-Lego-Qwen3-8B and SWE-Lego-Qwen3-32B agents (Figure 11b). For 8B rollouts, 8B and 30B verifiers perform similarly across K. For 32B rollouts, the 30B verifier is 18 (a) Impact of Training Data Scale (b) Impact of Verifier Model Scale Figure 11: Ablation studies on verifier training. (a) Training data scale: verifiers trained on 18K trajectories outperform those trained on 6K trajectories. (b) Model scale: for 8B rollouts, verifier size has minimal impact; for 32B rollouts, the 30B verifier provides gains over the 8B verifier, especially at higher K. consistently better, especially at larger K; at TTS@16, it reaches 58.8% vs. 56.6%. One possible explanation is that 32B outputs contain subtler errors that benefit from larger verifier capacity; as grows, more near-correct candidates increase the premium on fine-grained ranking."
        },
        {
            "title": "C Extensions and Future Work",
            "content": "Beyond Issue Resolving. Extending the scope beyond defect repair to other tasks, like feature implementation, refactoring or tests generation, reveal limitations in current training data and benchmarks. Constructing datasets that adequately encode ambiguous requirements, nuanced acceptance criteria, and non-regression constraints remains challenging. Moreover, validating agent performance across richer dimensions, such as functional correctness, performance bounds, and interface stability, requires more complex evaluation frameworks than those used for bug fixing. Supervising higher-order behaviors like design decisions, system integration, and code refactoring also lacks well-defined signals for effective learning. Multilingual SWE. Expanding beyond Python to languages such as JavaScript/TypeScript, Java, Go, and Rust is impeded by language-specific toolchains, build systems, and testing frameworks. The absence of cross-language sandboxes and multilingual datasets limits agents ability to generalize across ecosystems. Further challenges include navigating heterogeneous dependency management practices, adapting to diverse testing paradigms, and accounting for language-specific code organization idioms, all of which complicate the transfer of core agentic skills (navigation, localization, editing, validation) across programming languages. Cross-task Capability Balance. Heterogeneous code tasks, including repository-level issue resolution, file-specific code reasoning, and competitive coding, exhibit divergent demands on agent capabilities, leading to risks of uneven performance. Balancing proficiency across these tasks is hindered by varying difficulty gradients and interaction requirements, which may cause agents to overfit to dominant task archetypes. The lack of standardized metrics for comparing capability across such distinct tasks further complicates efforts to assess and mitigate this imbalance."
        },
        {
            "title": "D Example of Git Hacking",
            "content": "In Figure 12, we present concrete example of Git hacking: an SWE agent peeks at the Git commit history and unintentionally retrieves the pre-existing solution. During evaluation, the agent may run common Git commands (e.g., git log, git diff) to bypass independent problem-solving and directly replicate the golden patch, falsely inflating performance and invalidating the evaluation of genuine software engineering capabilities. 19 Figure 12: Demonstration of Git hacking by SWE agents."
        }
    ],
    "affiliations": [
        "CUHK",
        "HKU",
        "Huawei Technologies",
        "NTU"
    ]
}