{
    "paper_title": "Wiki Live Challenge: Challenging Deep Research Agents with Expert-Level Wikipedia Articles",
    "authors": [
        "Shaohan Wang",
        "Benfeng Xu",
        "Licheng Zhang",
        "Mingxuan Du",
        "Chiwei Zhu",
        "Xiaorui Wang",
        "Zhendong Mao",
        "Yongdong Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Deep Research Agents (DRAs) have demonstrated remarkable capabilities in autonomous information retrieval and report generation, showing great potential to assist humans in complex research tasks. Current evaluation frameworks primarily rely on LLM-generated references or LLM-derived evaluation dimensions. While these approaches offer scalability, they often lack the reliability of expert-verified content and struggle to provide objective, fine-grained assessments of critical dimensions. To bridge this gap, we introduce Wiki Live Challenge (WLC), a live benchmark that leverages the newest Wikipedia Good Articles (GAs) as expert-level references. Wikipedia's strict standards for neutrality, comprehensiveness, and verifiability serve as a great challenge for DRAs, with GAs representing the pinnacle of which. We curate a dataset of 100 recent Good Articles and propose Wiki Eval, a comprehensive evaluation framework comprising a fine-grained evaluation method with 39 criteria for writing quality and rigorous metrics for factual verifiability. Extensive experiments on various DRA systems demonstrate a significant gap between current DRAs and human expert-level Wikipedia articles, validating the effectiveness of WLC in advancing agent research. We release our benchmark at https://github.com/WangShao2000/Wiki_Live_Challenge"
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 ] . [ 1 0 9 5 1 0 . 2 0 6 2 : r Wiki Live Challenge: Challenging Deep Research Agents with Expert-Level Wikipedia Articles February 3, Shaohan Wang1, Benfeng Xu1,2, Licheng Zhang1, Mingxuan Du1, Chiwei Zhu1, Xiaorui Wang2, Zhendong Mao1 and Yongdong Zhang1 1University of Science and Technology of China, Hefei, China 2Metastone Technology, Beijing, China {wsh2000, benfeng, zlczlc}@mail.ustc.edu.cn, zdmao@ustc.edu.cn Deep Research Agents (DRAs) have demonstrated remarkable capabilities in autonomous information retrieval and report generation, showing great potential to assist humans in complex research tasks. Current evaluation frameworks primarily rely on LLM-generated references or LLM-derived evaluation dimensions. While these approaches offer scalability, they often lack the reliability of expert-verified content and struggle to provide objective, fine-grained assessments of critical dimensions. To bridge this gap, we introduce Wiki Live Challenge (WLC), live benchmark that leverages the newest Wikipedia Good Articles (GAs) as expert-level references. Wikipedias strict standards for neutrality, comprehensiveness, and verifiability serve as great challenge for DRAs, with GAs representing the pinnacle of which. We curate dataset of 100 recent Good Articles and propose Wiki Eval, comprehensive evaluation framework comprising fine-grained evaluation method with 39 criteria for writing quality and rigorous metrics for factual verifiability. Extensive experiments on various DRA systems demonstrate significant gap between current DRAs and human expertlevel Wikipedia articles, validating the effectiveness of WLC in advancing agent research. We release our benchmark at https://github.com/WangShao2000/Wiki_Live_Challenge"
        },
        {
            "title": "Introduction",
            "content": "With the explosive growth of Large Language Model (LLM) capabilities, LLM-driven agents have demonstrated remarkable potential in handling expert-level tasks (Achiam et al., 2023; Yang et al., 2025; Team et al., 2025a; Mialon et al., 2024). These agents are capable of multi-step task planning, tool utilization, and interaction with real-world environments to accomplish complex objectives. Among these, the Deep Research Agent (DRA) represents one of the most advanced agent systems (Team et al., 2025b; Qiao et al., 2025; Li et al., 2025a; Zheng et al., 2025). By performing multi-step web information retrieval, integration, and reasoning, DRAs can complete research tasks that would typically require significant time and effort from human experts. However, existing DRAs still suffer from issues such as hallucinations and biases in research and writing. To comprehensively evaluate the capabilities of these systems, two core challenges must be addressed: how to efficiently obtain reliable expert-level articles as references, and how to design an objective evaluation method that comprehensively reflects DRA research capability and writing quality (Du et al., 2025; Xu et al., 2025a). Work done during the internship at Metastone. Corresponding author. Preprint. Work in progress. Figure 1: Gap between human-written and AI-generated Wikipedia articles. Human-authored articles (top) feature rigorous citations and neutral tone, while AI-generated ones (bottom) lack citations and exhibit bias toward trending topics. Existing efforts (Du et al., 2025; Li et al., 2025b) have attempted to address these challenges by using reports generated by strong DRAs as references to evaluate others, scoring them based on manually designed criteria such as comprehensiveness, depth and so on. While these methods can reflect the gap between models to some extent, the reference reports are LLM-generated and lack quality assurance. Furthermore, evaluation criteria are often directly defined by LLMs or rely on internal model knowledge for verification, which may lead to results that deviate from human expert expectations (Fan et al., 2025; Li et al., 2024). Other approaches attempt to design specific rubrics for different reports (Sharma et al., 2025; Xu et al., 2025a); however, these rubrics are often coarse-grained generated by LLMs or require additional human annotation. To bridge this gap, we introduce the Wiki Live Challenge (WLC), live benchmark designed to challenge DRAs with the latest Wikipedia Good Articles. Wikipedia articles represent comprehensive research on subject, involving extensive information gathering and organization, written from an objective and neutral perspective with strict verifiability. We posit that the ability to produce such content comprehensively reflects DRAs capabilities, yet current models fall considerably short of these standards, as illustrated in Figure 1. Therefore, we utilize high-quality Wikipedia Good Articles as real-time human expert references and, based on their corresponding criteria, assess the disparity between DRA-generated reports and real-world human-authored content. Specifically, we collected 100 expert-level Wikipedia Good Articles that strictly follow Wikipedias editorial guidelines and have been reviewed and revised by human experts, providing strong quality assurance. We ensure that the article set is recent and will be continuously updated to avoid data contamination. Leveraging these articles as human-expert references, we construct Wiki Eval, an evaluation framework grounded in Wikipedia Good Article criteria. This framework comprises two key components: Wiki Writing and Wiki Fact. Wiki Writing serves as fine-grained writing evaluation protocol with 39 criteria covering GA-aligned writing dimensions. Additionally, Wiki Fact assesses the DRAs information retrieval capability and factual reliability following the verifiability criteria, with two sub-metrics that measure (i) information richness relative to Wikipedia and (ii) whether generated statements are strictly traceable to supporting sources. Our contributions are summarized as follows: We introduce Wiki Live Challenge (WLC), live benchmark designed to challenge the capability of DRAs in writing Wikipedia. Sourced from Wikipedia Good Articles, our benchmark ensures high quality through expert review and validation, serving as reliable human expert reference. We propose Wiki Eval, an evaluation framework focusing on both writing quality and factuality, with all criteria strictly grounded in Wikipedia Good Article criteria. We conducted extensive experiments across diverse set of DRA systems and performed comprehensive analyses and human studies to validate the reliability of our evaluation framework. We also plan to maintain and extend the benchmark to better reflect evolving real-world conditions."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Deep Research Agent Deep Research Agents (DRAs) are designed to autonomously explore the web, retrieve information, and synthesize findings into comprehensive reports. Recent progress focuses on improving their reasoning and planning for long-term tasks. Notably, DeepResearcher (Zheng et al., 2025) is the first work to train LLMs via end-to-end reinforcement learning in real, dynamic web environment for deep information retrieval and integration. Similarly, Tongyi DeepResearch (Team et al., 2025b) employs an end-to-end training framework to enable scalable reasoning. WebResearcher (Qiao et al., 2025) treats research as decision process, using iterative refinement to manage noise. WebSailor (Li et al., 2025a) tackles uncertainty in web navigation through structured sampling. Furthermore, systems like WebDancer (Wu et al., 2025) have pushed the boundaries of autonomous information seeking. These developments highlight shift towards agents that can independently verify information and synthesize knowledge. 2.2 Deep Research Benchmarks Evaluating the capabilities of DRAs requires benchmarks that go beyond simple question answering. Early general agent benchmarks such as GAIA (Mialon et al., 2024) and AgentBench (Liu et al., 2025) assess fundamental abilities like tool use and reasoning but often lack the depth required for evaluating long-form research reports. FreshWiki (Shao et al., 2024) is an early dataset utilizing Wikipedia articles to evaluate generated text, but it lacks fine-grained rubrics tailored for modern DRAs. DeepResearch Bench (Du et al., 2025) provides 100 PhD-level tasks across 22 domains, employing reference-based adaptive criteria. ReportBench (Li et al., 2025b) focuses on report generation quality using survey papers as references, while ResearchRubrics (Sharma et al., 2025) offers expert-written criteria for 2 Figure 2: The overview of our Wiki Live Challenge (WLC) benchmark. (a) We continuously collect recent Wikipedia articles (e.g., from Mar. 1 to Dec. 1 in this iteration), filter the latest expert-reviewed Good Articles, and build the live task dataset. (b) We strictly grounded in GA criteria: well-written, neutral, broad coverage and verifiable. (c) Our evaluation framework, Wiki Eval, incorporates two key dimensions: Wiki Writing and Wiki Fact. evaluating open-ended queries. More recent benchmarks further emphasize live tasks and expert-grounded evaluation, including LiveResearchBench (Wang et al., 2025), DeepScholar-Bench (Patel et al., 2025), and DEER (Han et al., 2025). Additionally, BrowseComp (Wei et al., 2025), and CiteEval (Xu et al., 2025b) focus on citation accuracy and source attribution. Despite these efforts, existing benchmarks often rely on static or model-generated references lacking rigorous human verification, which limits their ability to objectively measure alignment with expert standards in real-world scenarios."
        },
        {
            "title": "3 Wiki Live Challenge",
            "content": "In this section, we provide detailed introduction to the Wiki Live Challenge (WLC). We first outline our data collection and construction methodology, followed by our Wiki Eval framework based on the Wikipedia Good Article criteria. Overall framework is illustrated in Figure 2. 3.1 Data Construction Existing benchmarks for Deep Research Agents typically rely on reports generated by strong models or collected from the open web as references (Du et al., 2025; Wang et al., 2025; Patel et al., 2025; Han et al., 2025). However, these sources often lack expert evaluation, offering no guarantees of quality. Furthermore, they may contain biases or errors stemming from the inherent biases of the generative models or human authors. In contrast, Wikipedia serves as reliable knowledge source rigorously reviewed and revised by human editors, adhering to strict core policies that ensure neutrality, comprehensiveness, and strict verifiability. Particularly, Good Articles (GAs) represent the pinnacle of this quality, undergoing meticulous expert review process to meet the highest editorial standards. We posit that the ability to autonomously author such contentnecessitating extensive research, the synthesis of diverse viewpoints, and the maintenance of objectivityserves as rigorous challenge for DRAs. Therefore, we utilize these high-quality GAs as reference standards and construct our evaluation methodology grounded in the official Good Article criteria. 3 Figure 3: Overview of the WLC benchmark dataset. The left panel displays the distribution of collected Wikipedia Good Articles across 15 major categories and the key statistics of the WLC Benchmark Dataset. The right panel illustrates representative task case. Specifically, we collected all new Wikipedia articles created between March 1, 2025, and December 1, 20251. From this corpus, we filtered for Good Articles (GA), identifying 304 articles that have passed Wikipedias review process and strictly adhere to the Good Article criteria, thus ensuring high quality. To ensure the complexity of the task, we ranked these articles based on the number of reference URLs and structural depth, excluding simple list-based articles. Ultimately, we curated dataset of 100 Wikipedia Good Articles to serve as our benchmark. The distribution of these articles, along with the dataset statistics and representative task case, is shown in Figure 3, which covers 15 major domains, providing comprehensive benchmark for Deep Research Agents. 3.2 Evaluation Methodology Our evaluation framework is grounded in the Wikipedia Good Article criteria, widely recognized standard established by human experts2. We selected four dimensions most relevant to DRA capabilities, focusing on writing style, neutral point of view, broad in coverage, and verifiability. Based on these, we constructed Wiki Eval, an evaluation methodology comprising two primary components: Wiki Writing and Wiki Fact. 3.2.1 Wiki Writing To assess writing quality, we construct Wiki Writing, fine-grained evaluation framework based on the three core dimensions of the Wikipedia Good Article criteria: Well-written, Neutral, and Broad in its coverage. This framework comprises 39 distinct criteria derived directly from official Wikipedia writing guidelines, as shown in Figure 2-b. Subsequently, we employ an LLM-as-a-Judge approach. For each criterion, we provide the evaluation model with both the original Wikipedia article and the LLM-generated article to determine the winner for this criterion: Judge(wi, gi) = Judge-LLM(wi, gi). (1) where wi denotes the Wikipedia reference, gi represents the generated article, and Judge-LLM is the model used to determine the superior output for the given criterion. Finally, we calculate the score for each criterion based on the models judgment and aggregate these scores to compute the overall writing score for the article. Detailed information regarding the criteria sources and the complete list of criteria are provided in the Appendix A. 3.2.2 Wiki Fact To evaluate the articles factual accuracy (1) relative to Wikipedia and (2) relative to cited references, we introduce two evaluation metrics, as illustrated in Figure 2-c. We first preprocess both the Wikipedia article and the generated 1This timeframe utilizes the most recent Wikipedia articles to mitigate potential data contamination, ensuring the content postdates the knowledge cutoff of current mainstream models. 2https://en.wikipedia.org/wiki/Wikipedia:Good_article_criteria 4 article by employing an extraction LLM to extract facts from each, yielding fact list for the Wikipedia article and Statement-URL pair list for the generated article. For factual accuracy relative to Wikipedia, we measure the coverage of generated statements against Wikipedia facts. For each fact fi in Wikipedia fact list, we retrieve the top-10 most relevant statements from the generated article. These statements and target fact are then input into fact-checking model to determine consistency score: Fact(fi, G) = 1, 0, 0, if consistent if inconsistent if conflict We then calculate the coverage score for each article by averaging the scores of all its target facts: Cov. Wiki. = 1 (cid:88) fiF Fact(fi, G) (2) (3) where represents the set of facts extracted from the Wikipedia article, and represents the set of facts from the corresponding generated article. For factual accuracy relative to references, we assess whether the generated statements are supported by their citations. For each extracted Statement-URL pair, we utilize Jina Reader3 to retrieve the content of the cited webpage. We then employ the fact-checking model to verify if the statement is supported by source content. The final Reference Accuracy score is calculated as the proportion of statements that are fully supported by their references set R: Ref. Acc. = Fact(si, R) (4) 1 (cid:88) siS where denotes the list of Statement-URL pairs in the generated article."
        },
        {
            "title": "4 Experimental Settings",
            "content": "4.1 Implementation Details For the evaluation of Wiki Writing, we utilize Gemini-2.5-pro (Comanici et al., 2025) as the Judge LLM. For the Wiki Fact assessment, we employ Gemini-2.5-flash as both the extraction and fact-checking LLM, balancing performance with cost-effectiveness given the high token consumption. All reference Wikipedia pages were collected on December 15, 2025, and all results presented in Table 1 are based on evaluations against these 100 complete Wikipedia articles. Further implementation settings are provided in Appendix B.1. 4.2 Evaluated Models In this work, we extensively evaluated advanced Deep Research Agent systems, including proprietary systems such as OpenAI o3 Deep Research (OpenAI, 2025), Gemini-2.5-pro Deep Research (Google, 2025), and Qwen-3-max Deep Research (Yang et al., 2025). Regarding open-source frameworks, we selected Tongyi DeepResearch (Team et al., 2025b) and Deep Researcher (Zheng et al., 2025), two state-of-the-art open-source models trained via reinforcement learning. Additionally, we evaluated LangChain Open Deep Research4, an open-source framework powered by proprietary models, utilizing GPT-4.1 and GPT-5 as backends. All model articles were collected between December 15 and 19, 2025."
        },
        {
            "title": "5 Results and Discussions",
            "content": "5.1 Main Results 5.1.1 Evaluation on Wiki Writing As shown in Table 1, among the various Deep Research Agents, Gemini-3-pro Deep Research and the LangChain framework powered by GPT-5 demonstrates significant advantage, with Gemini-2.5-pro and OpenAI o3 Deep Research also exhibiting strong performance. We observe substantial performance disparities across different DRAs. 3https://jina.ai 4https://github.com/langchain-ai/open_deep_research 5 Table 1: Main results of WLC across Wiki Writing and Wiki Fact. Wiki Writing are computed by aggregating wins over our 39 Wiki GA-based criteria. Cov. Wiki measures factual coverage against the extracted Wikipedia fact list, and Ref. Acc. measures the proportion of cited statements that are supported by their referenced webpages; missing entries indicate that reliable Statement-URL extraction was not possible due to citation formatting. Models Wiki Writing Wiki Fact Overall Well-writ. Neutral Broad Cov. Wiki Ref. Acc. Deep Researcher Tongyi Deep Research Langchain (GPT-4.1) Langchain (GPT-5) Doubao Deep Research Qwen-3-max Deep Res. Perplexity Deep Res. Grok Deep Search OpenAI o3 Deep Res. Gemini-2.5-pro Deep Res. Gemini-3-pro Deep Res. Open-Source Agent Framework 1.90 10.90 18.76 50.95 1.90 11.60 19.40 54.20 Proprietary Agent Framework 16.00 18.29 26.79 27.52 28.43 30.10 60.81 16.10 27.70 20.40 24.30 24.90 26.10 46.10 3.75 30.25 27.25 59. 31.13 40.00 37.63 35.75 45.75 59.88 67.12 2.28 15.05 20.67 53.62 19.13 25.15 27.38 28.38 31.08 35.18 58.33 5.62 22.73 7.08 20.96 22.97 22.22 29.21 20.73 25.12 30.76 28.83 7.34 67. 37.05 61.44 28.27 60.63 57.44 41.68 66.98 Notably, fully open-source DRA frameworks lag significantly behind proprietary models. Deep Researcher, for instance, achieved score of only 2.28, which is markedly lower than other models. Manual review revealed that its reports are often incomplete, typically concluding after minimal information gathering steps, which adversely affected its scores on writing criteria. Tongyi DeepResearch performed relatively better, achieving scores comparable to some proprietary models like Doubao Deep Research; however, gap remains compared to state-of-the-art DRA frameworks. This suggests that achieving the long-form report generation capabilities of proprietary models remains challenge for smaller models trained via end-to-end RL. 5.1.2 Evaluation on Wiki Fact Considering Wiki Fact in Table 1, we observe that all DRA systems perform poorly in terms of coverage of Wikipedia facts. Even the best-performing agent, Gemini-2.5-pro Deep Research, achieved an average knowledge coverage of only 30.76%, indicating that current models are still far from reaching the expert-level information gathering capabilities of Wikipedia. Notably, while LangChain (GPT-5) excels in writing, its knowledge retrieval performance lags behind proprietary frameworks. Figure 4: Fact Coverage Heatmap on Wikipedia Good Article Parasitic Ant. The x-axis represents individual facts ordered by their appearance in the article sections, and the y-axis represents different DRAs. To further investigate this gap, we present case study on the Parasitic Ant article in Figure 4. The heatmap reveals difficulty gradient aligned with article structure: DRAs perform well on procedural sections like Methods but struggle with specialized sections such as Phylogeny and Defense. Analysis shows that while general definitions are universally 6 covered, all systems fail to retrieve precise quantitative data (e.g., gene counts) and domain-specific terminology. This suggests that agents effectively capture broad concepts but lack the precision for granular, high-difficulty details. Regarding Reference Accuracy, scores for Open-Source models are omitted due to their frequent failure to generate properly formatted citation markers. For LangChain (GPT-4.1), the scarcity of citations led to low scores, whereas GPT-5 demonstrated superior performance with near-complete citation coverage. Proprietary frameworks like Perplexity scored lower likely because they only cite subset of retrieved content, leaving some statements unverified. Given the lack of transparency in internal search mechanisms and inconsistencies in citation placement or accessibility (e.g., paywalls), this score serves as supplementary metric reflecting the verifiability of accessible information rather than definitive measure of grounding. Table 2: Conflict rates across different systems. Wiki Conf. is the fraction of statements conflicting with Wikipedia facts. Ref. Conf. is the fraction of statements conflicting with their cited references. Models Deep Researcher Tongyi Deep Research Langchain (GPT-5) Langchain (GPT-4.1) Gemini-3-pro Deep Research Perplexity Deep Research Gemini-2.5-pro Deep Research OpenAI o3 Deep Research Doubao Deep Research Qwen-3-max Deep Research Grok Deep Search 5.2 Analysis and Discussion 5.2.1 Analysis of Fact Conflicts Wiki Conf. (%) Ref. Conf. (%) 9.98 9.89 10.45 24. 7.62 7.74 7.79 9.12 9.78 10.41 11.40 3.14 2.94 4.23 3.04 4.58 3.95 4.65 6.87 4.96 We further analyze conflicts, where generated statements directly contradict either the established facts in Wikipedia Good Articles (Wiki Conf.) or the content of their own cited references (Ref. Conf.). Such conflicts are particularly detrimental as they introduce explicit falsehoods rather than mere omissions. Table 2 illustrates distinct error patterns across systems, decoupling the analysis of verification against ground truth versus cited sources. high citation conflict rate points to severe hallucination, where the model fabricates information not supported by its claimed references; notably, Qwen-3-max Deep Research exhibits high citation conflict rate of 6.87%. Conversely, high conflict rate with Wikipedia implies the inclusion of hallucinations or information from unreliable sources that contradicts established facts. For example, while LangChain (GPT-4.1) achieves the lowest citation conflict (2.94%), it records the highest Wiki conflict (24.69%), suggesting it may be incorporating incorrect information despite adhering to its own retrieval context. LangChain (GPT-5) demonstrates superior performance by maintaining low conflict rates across both dimensions. 5.2.2 Analysis Across Categories Table 3: Correlation between Wikipedia article features and task difficulty. Task difficulty shows moderate correlation with popularity (page views) but is independent of article length, statement or link count. Feature Page Views External Links Length (Bytes) Statement Count Coeff. = +0.482 = +0.042 = 0.027 = +0.152 Strength Moderate Negligible Negligible Negligible Difficulty Our benchmark comprises Wikipedia articles from 15 distinct categories, revealing significant performance disparities among models across these domains. Notably, in History and Mathematics, the average win rate across all systems remained below 20%, whereas in Natural Sciences and Philosophy and Religion, average scores exceeded 7 40%. This highlights substantial variation in the difficulty of information retrieval and summarization across fields. We further analyzed the correlation between Wikipedia statistical features and task difficulty, as detailed in Table 3. Results indicate that difficulty has negligible correlation with article length or citation count but is moderately correlated with total page views. This suggests that difficulty is largely determined by the complexity of web-based research: higher view counts typically correspond to more popular categories, where information is more accessible for DRAs to mine. More information are provided in Appendix C.1. Robustness To verify the robustness of evaluation across different Wikipedia categories, we further analyzed the performance variation of DRAs across categories. To control for differences caused by inherent category difficulty, we used the deviation of each DRA systems score from the category mean as its relative score. We hypothesized that there is no significant difference in the relative performance of DRAs evaluated across different categories and conducted an ANOVA test. The results indicate that, with the exception of Deep Researcher, there are no significant differences in the relative performance of the systems evaluated across categories (p > 0.05), demonstrating that our evaluation possesses cross-category robustness. Deep Researcher consistently underperformed across varying task difficulties; thus, we attribute the variance in its evaluation across categories to intrinsic performance limitations. Further details are provided in Appendix C.2. 5.2.3 Different Models for Judgement Table 4: Pairwise Agreement Rate (PAR) of different Judge LLMs with human annotations. Cost represents the average cost per article evaluation. Qwen3-80B-A3B is deployed locally, incurring no API costs. Judge Models Gemini-2.5-pro GPT-5 Qwen3-235B-A22B DeepSeek-V3.1 GPT-5-mini Qwen3-80B-A3B PAR (%) 83.59 80.67 78.72 76.41 73.59 72.31 Cost ($) 0.132 0.128 0.071 0.024 0.026 To verify the performance differences among various models serving as Judge-LLMs, we sampled 10 Wikipedia-DRA article pairs and manually annotated the win rate for each criterion, with total 390 criteria annotations. This allowed us to observe the consistency between different judge models and human judgments. We report the Pairwise Agreement Rate (PAR) for criterion-level evaluation across different models, with results shown in Table 4. Among the proprietary models, Gemini-2.5-pro demonstrated superior consistency. In open-source solutions, Qwen3-235B-A22B also shows strong consistency, maintaining competitive performance even compared to proprietary models. 5.2.4 Wikipedia Leakage Since Wikipedia pages are openly accessible, data leakage where agent systems actively search for and read the original Wikipedia articles is potential issue during the research process. Although our task prompts explicitly prohibited access to Wikipedia, some models failed to adhere to this instruction. To address this, during the evaluation of Cov. Wiki., we first filtered out statements that cited the original Wikipedia article as source, performing retrieval only on the remaining statements. This ensured that the evaluation was not compromised by direct leakage from Wikipedia. We also calculated the statement-level leakage rate for different DRA systems, as shown in Table 5. These results provide insight into the models ability to follow negative constraints. Notably, even with relatively high leakage rates (e.g., Perplexity Deep Research), performance scores remain suboptimal, indicating that mere access to Wikipedia does not guarantee the generation of unbiased and factually accurate articles. In contrast, LangChain (GPT-5) achieves high scores while maintaining an extremely low leakage rate, underscoring the robust capabilities of the system."
        },
        {
            "title": "6 Conclusion",
            "content": "In this study, we introduce Wiki Live Challenge (WLC), live benchmark that challenge the ability of Deep Research Agents to write Wikipedia-style articles, using Wikipedia Good Articles as human-expert references. WLC comprises 100 recent Good Articles spanning 15 categories, accompanied by comprehensive evaluation framework, Wiki Eval. Wiki Eval combines fine-grained writing assessment, constructing 39 criteria grounded in Wikipedia Good Article criteria, with factual evaluation that measures both the coverage of Wikipedia facts and verifiability via cited references. Extensive experiments on diverse set of deep research agents reveal substantial gap between current agents and 8 Table 5: Wikipedia Leakage Rates across different DRAs. The leakage rate indicates the proportion of statements directly citing the target Wikipedia page, reflecting models adherence to the exclusion instruction. Models Perplexity Deep Research Gemini-3-pro Deep Research Gemini-2.5-pro Deep Research Qwen-3-max Deep Research LangChain (GPT-4.1) OpenAI o3 Deep Research LangChain (GPT-5) Doubao Deep Research Grok Deep Search Leakage Rate (%) 33.77 23.35 14.74 4.67 1.13 0.83 0.09 0.00 0.00 human-authored Wikipedia articles. We hope WLC will facilitate more reliable, fine-grained, and reproducible progress in the field of deep research agents."
        },
        {
            "title": "Limitations",
            "content": "Although the Wiki Live Challenge and Wiki Eval framework provide comprehensive assessment of Deep Research Agents capabilities, several limitations remain: (1) Task Scale: Due to constraints imposed by model knowledge cutoff dates, the number of collected Good Articles meeting our strict criteria is limited to the hundreds scale. Our benchmark prioritizes the quality and recency of Wikipedia articles over dataset size. (2) System Opacity: The lack of transparency in the citation mechanisms of certain proprietary systems, coupled with the potential inaccessibility of some cited web pages during evaluation, may impact the assessment of citation verifiability. Consequently, Reference Accuracy serves as an observational reference metric rather than definitive measure of grounding."
        },
        {
            "title": "Ethical Considerations",
            "content": "Data Compliance. Our benchmark dataset is derived entirely from Wikipedia, publicly accessible knowledge base. The data collection process respects copyright policies and involves no personal privacy or non-public information. Human Annotation and Compensation. To validate our Wiki Eval evaluation framework and assess agent performance, we recruited 5 general annotators for collecting DRA result data and 5 PhD-level annotators for human evaluation. Participants were compensated at $1 per article for collection and rate of $10 per hour for annotation. We obtained informed consent from all participants, and they were notified that the data would be used for research purposes only. The annotation tasks did not involve exposure to offensive or traumatic content. Hallucinations and Misinformation. DRAs have the potential to generate factually incorrect content or hallucinations, which poses significant risks in real-world applications. Our proposed benchmark specifically targets the evaluation of factual verifiability. By providing rigorous standard for measuring hallucinations against expert-verified sources, our work contributes to the development of safer and more reliable AI systems."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. 2025. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261. Mingxuan Du, Benfeng Xu, Chiwei Zhu, Xiaorui Wang, and Zhendong Mao. 2025. Deepresearch bench: comprehensive benchmark for deep research agents. Preprint, arXiv:2506.11763. Tianyu Fan, Xinyao Niu, Yuxiang Zheng, Fengji Zhang, Chengen Huang, Bei Chen, Junyang Lin, and Chao Huang. 2025. Understanding deepresearch via reports. Preprint, arXiv:2510.07861. Google. 2025. Gemini deep research. https://gemini.google/overview/deep-research/. Janghoon Han, Heegyu Kim, Changho Lee, Dahm Lee, Min Hyung Park, Hosung Song, Stanley Jungkyu Choi, Moontae Lee, and Honglak Lee. 2025. Deer: comprehensive and reliable benchmark for deep-research expert reports. Preprint, arXiv:2512.17776. Haitao Li, Qian Dong, Junjie Chen, Huixue Su, Yujia Zhou, Qingyao Ai, Ziyi Ye, and Yiqun Liu. 2024. Llms-as-judges: comprehensive survey on llm-based evaluation methods. Preprint, arXiv:2412.05579. Kuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan Li, Zhengwei Tao, Xinyu Wang, Weizhou Shen, Junkai Zhang, Dingchu Zhang, Xixi Wu, Yong Jiang, Ming Yan, Pengjun Xie, Fei Huang, and Jingren Zhou. 2025a. Websailor: Navigating super-human reasoning for web agent. Preprint, arXiv:2507.02592. Minghao Li, Ying Zeng, Zhihao Cheng, Cong Ma, and Kai Jia. 2025b. Reportbench: Evaluating deep research agents via academic survey tasks. Preprint, arXiv:2508.15804. Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. 2025. Agentbench: Evaluating llms as agents. Preprint, arXiv:2308.03688. Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. 2024. GAIA: benchmark for general AI assistants. In The Twelfth International Conference on Learning Representations. OpenAI. 2025. Deep research system card. https://openai.com/index/deep-research-system-card/. Liana Patel, Negar Arabzadeh, Harshit Gupta, Ankita Sundar, Ion Stoica, Matei Zaharia, and Carlos Guestrin. 2025. Deepscholar-bench: live benchmark and automated evaluation for generative research synthesis. Preprint, arXiv:2508.20033. Zile Qiao, Guoxin Chen, Xuanzhong Chen, Donglei Yu, Wenbiao Yin, Xinyu Wang, Zhen Zhang, Baixuan Li, Huifeng Yin, Kuan Li, Rui Min, Minpeng Liao, Yong Jiang, Pengjun Xie, Fei Huang, and Jingren Zhou. 2025. Webresearcher: Unleashing unbounded reasoning capability in long-horizon agents. Preprint, arXiv:2509.13309. Yijia Shao, Yucheng Jiang, Theodore Kanell, Peter Xu, Omar Khattab, and Monica Lam. 2024. Assisting in writing Wikipedia-like articles from scratch with large language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 62526278, Mexico City, Mexico. Association for Computational Linguistics. Manasi Sharma, Chen Bo Calvin Zhang, Chaithanya Bandi, Clinton Wang, Ankit Aich, Huy Nghiem, Tahseen Rabbani, Ye Htet, Brian Jang, Sumana Basu, Aishwarya Balwani, Denis Peskoff, Marcos Ayestaran, Sean M. Hendryx, Brad Kenstler, and Bing Liu. 2025. Researchrubrics: benchmark of prompts and rubrics for evaluating deep research agents. Preprint, arXiv:2511.07685. Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, et al. 2025a. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534. Tongyi DeepResearch Team, Baixuan Li, Bo Zhang, Dingchu Zhang, Fei Huang, Guangyu Li, Guoxin Chen, Huifeng Yin, Jialong Wu, Jingren Zhou, et al. 2025b. Tongyi deepresearch technical report. arXiv preprint arXiv:2510.24701. 10 Jiayu Wang, Yifei Ming, Riya Dulepet, Qinglin Chen, Austin Xu, Zixuan Ke, Frederic Sala, Aws Albarghouthi, Caiming Xiong, and Shafiq Joty. 2025. Liveresearchbench: live benchmark for user-centric deep research in the wild. Preprint, arXiv:2510.14240. Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. 2025. Browsecomp: simple yet challenging benchmark for browsing agents. Preprint, arXiv:2504.12516. Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang, Zekun Xi, Gang Fu, Yong Jiang, Pengjun Xie, Fei Huang, and Jingren Zhou. 2025. Webdancer: Towards autonomous information seeking agency. Preprint, arXiv:2505.22648. Tianze Xu, Pengrui Lu, Lyumanshan Ye, Xiangkun Hu, and Pengfei Liu. 2025a. Researcherbench: Evaluating deep ai research systems on the frontiers of scientific inquiry. Preprint, arXiv:2507.16280. Yumo Xu, Peng Qi, Jifan Chen, Kunlun Liu, Rujun Han, Lan Liu, Bonan Min, Vittorio Castelli, Arshit Gupta, and Zhiguo Wang. 2025b. Citeeval: Principle-driven citation evaluation for source attribution. Preprint, arXiv:2506.01829. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. 2025. Qwen3 technical report. arXiv preprint arXiv:2505.09388. Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. 2025. DeepResearcher: Scaling deep research via reinforcement learning in real-world environments. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 414431, Suzhou, China. Association for Computational Linguistics. 11 The six Good Article criteria 1. Well-written: (a) the prose is clear, concise, and understandable to an appropriately broad audience; spelling and grammar are correct; (b) it complies with the Manual of Style guidelines for lead sections, layout, words to watch, fiction, and list incorporation. 2. Verifiable with no original research: (a) it contains list of all references (sources of information), presented in accordance with the layout style guideline; (b) reliable sources are cited inline. All content that could reasonably be challenged, except for plot summaries and that which summarizes cited content elsewhere in the article, must be cited no later than the end of the paragraph (or line if the content is not in prose); (c) it contains no original research; and (d) it contains no copyright violations or plagiarism. 3. Broad in its coverage: (a) it addresses the main aspects of the topic; and (b) it stays focused on the topic without going into unnecessary detail (see summary style). 4. Neutral: it represents viewpoints fairly and without editorial bias, giving due weight to each. 5. Stable: it does not change significantly from day to day because of an ongoing edit war or content dispute. (Not applicable for offline evaluation) 6. Illustrated, if possible, by media such as images, video, or audio: (Not applicable for text-only evaluation) (a) media are tagged with their copyright statuses, and valid non-free use rationales are provided for non-free content; and (b) media are relevant to the topic, and have suitable captions. Figure 5: The six Wikipedia Good Article criteria. These criteria serve as the foundation for our Wiki Eval framework."
        },
        {
            "title": "A Details of Wiki Eval",
            "content": "Our criteria selection is grounded in the Wikipedia Good Article criteria, as shown in Figure 5. We selected the first four dimensions (excluding stability and illustrations), focusing on the textual content. For the dimensions of Well-written, Broad in its coverage, and Neutral, we delved into the specific guidelines for each, strictly adhering to Wikipedias guidelines on lead sections5, words to watch6, verifiability7, summary style8, and neutral point of view9, collecting total of 39 fine-grained criteria to construct the Wiki Writing evaluation, which are listed in Table 6, Table 7, and Table 8. For the Verifiable dimension, we constructed the Wiki Fact, an evaluation metric that encompasses both the factual accuracy of the generated article relative to Wikipedia and its consistency with cited references."
        },
        {
            "title": "B Detailed Evaluation Settings",
            "content": "B.1 Implementation Details Judge LLM for Wiki Writing Based on the agreement with human evaluation results, we selected Gemini-2.5-pro, which demonstrated the best performance, as the Judge-LLM for assessing the win rate. During evaluation, we input the Wikipedia original article, the generated article, and the criteria belonging to the same category as single prompt into the model for batch evaluation. The evaluation prompt is shown in Figure 6. Judge LLM for Wiki Fact We employ Gemini-2.5-flash as both the fact extraction model and the fact-checking model. The prompts for fact extraction and fact checking are presented in Figure 7 and Figure 8, respectively. B.2 Details of Evaluated DRAs 5https://en.wikipedia.org/wiki/Wikipedia:Manual_of_Style/Lead_section 6https://en.wikipedia.org/wiki/Wikipedia:Manual_of_Style/Words_to_watch 7https://en.wikipedia.org/wiki/Wikipedia:Verifiability 8https://en.wikipedia.org/wiki/Wikipedia:Summary_style 9https://en.wikipedia.org/wiki/Wikipedia:Neutral_point_of_view System Prompt You are strict, meticulous, and objective Wikipedia article evaluation expert. You excel at using specific criteria to compare two Wikipedia-style articles on the same topic, and for each criteria you MUST decide clear winner - either Article 1 wins or Article 2 wins. User Prompt Articles to Evaluate <article_1> {article1_text} </article_1> <article_2> {article2_text} </article_2> Criterion Category and Criteria We focus on: {category.name} {category.description} Under this category there are {k} criteria: {criteria_list} Compare which article better satisfies each criteria criterion. Output your response in strict JSON format with exactly {k} results (one per criteria): { \"results\": [ { \"criteria_index\": 1, \"reason\": \"Brief explanation for criteria 1\", \"winner\": 1 }, { \"criteria_index\": 2, \"reason\": \"Brief explanation for criteria 2\", \"winner\": 2 } ] } Note: winner value should be: - 1 = Article 1 wins - 2 = Article 2 wins Figure 6: Prompt used for Wiki Writing Evaluation. The model is tasked with comparing two articles based on specific criteria and determining winner. B.2.1 Data Collection Process DRAs with Web Services Data for Gemini-2.5-pro Deep Research, OpenAI o3 Deep Research, Perplexity Deep Research, Grok Deep Search, Qwen-3-max Deep Research, and Doubao Deep Research were collected via their respective web services. During collection, some DRA systems required secondary user interaction to confirm the research direction; in these cases, all human annotators are instructed to use unified prompt instruction: All content requires you to research and follow the criteria provide. Always exclude the search or reading of Wikipedia pages. Locally Deployed DRAs Data for LangChain Open Deep Research, Deep Researcher, and Tongyi Deep Research were collected in local environment. For LangChain Open Deep Research, we followed its open-source framework settings, using Tavily10 as the search engine, and employed GPT-4.1 and GPT-5 as the backbone models for article generation. For Deep Researcher, we deployed its model on single H20 GPU and used the crawling tools provided in its open-source repository for article generation. For Tongyi Deep Research, we deployed its model on single H20 GPU, using Serper API11 as the search engine and Jina12 as the web crawling tool. DRAs via API Services The recently released Gemini Deep Research Agent by Google 13, powered by Gemini-3-Pro, is capable of navigating complex information environments using web search to generate detailed reports with citations. We generated articles by invoking the Gemini Deep Research Agent API service. B.2.2 Data Collection Costs We report the costs incurred in collecting 100 articles generated by each DRA system. This encompasses both the compensation for human annotators and the operational costs of the DRA systems. Human annotators were paid $1 per article collection for their time. The specific generation costs for each DRA system are detailed in Table 9. 10https://www.tavily.com/ 11https://serper.dev/ 12https://jina.ai/ 13https://ai.google.dev/gemini-api/docs/deep-research 13 Fact Extraction Prompt You will be provided with an article. The body of the article contains text with citations to references. Citations in the main text may appear in the following forms: ...[1], ... .1, ...[(1)](https://...) Your Task: Please extract all informative statements (facts) from the main text. Each statement must be associated with its corresponding citation number(s). Return the result as list of (fact, ref_idx) pairs. Citation Assignment Rule (Critical): For each statement, find the nearest citation(s) that immediately follow it in the text. This is the citation that covers this statement. - If statement is followed directly by one or more citations (e.g., [1] or [1][2][3]), those are the citations for that statement. - If statement has no citation immediately after it, look forward in the text to find the next citation(s) that appear. All statements between the previous citation and this next citation share this next citation as their source. - In other words: citations at the end of sentence or paragraph typically cover all the preceding uncited statements up to the last citation marker. Extraction Rules: 1. Completeness & Context (Crucial): Every extracted fact must be complete, self-contained statement. Do not extract fragmented phrases. 2. Handling Citations: Each statement must have citation. Use the nearest following citation(s) as described above. 3. Formatting: Return JSON list. Ensure Chinese quotation marks in the fact are preserved. Output Format Example: Given text: \"Li Qiang constructed socioeconomic status index...[15] It has been validated...[3][7][12]\" Output: [ { \"fact\": \"...\", \"ref_idx\": \"15\" }, { \"fact\": \"...\", \"ref_idx\": [\"3\", \"7\", \"12\"] } ] Here is the main text of the article: {article_text} Please begin the extraction now. Output only the JSON list directly, without any chitchat or explanations. Figure 7: Prompt used for Fact Extraction. The model extracts atomic facts and their associated citation indices from the text."
        },
        {
            "title": "C Detailed Analysis Across Categories",
            "content": "C.1 Difficulty Analysis Table 10 presents the detailed difficulty ranking of different Wikipedia categories. The difficulty is measured by the average win rate in Wiki Writing Criteria of all DRA systems in each category. We also list the average article length, statement count, external link count, and total page views for each category. C.2 Robustness Analysis To verify the robustness of our evaluation across different Wikipedia categories, we formulate the null hypothesis (H0) that there is no significant difference in the relative performance of DRA system when evaluated across different categories. The relative performance is calculated as the deviation of the systems score from the category mean to control for inherent category difficulty. We conducted an ANOVA test for each system to test this hypothesis. The detailed results, including p-values and effect sizes (η2), are presented in Table 11."
        },
        {
            "title": "D Human Annotation",
            "content": "We recruited five PhD-level annotators and tasked them with evaluating randomly sampled pairs of Wikipedia articles and model-generated articles based on each criterion. All article data underwent preprocessing to rigorously remove references and inline citation tags, and was presented in Markdown format. The order of articles was randomized to ensure annotators evaluated solely based on writing quality. We strictly instructed annotators to thoroughly read the articles under comparison. For each criterion, in addition to determining the winner, they were required to provide rationale for their decision. Annotators were compensated at rate of $10 per hour. 14 System Prompt You are professional data annotator. Given sentence and paragraph composed of several gold factual support sentences, your task is to verify whether the sentence is consistent with the facts. Matching any single support sentence is enough for consistency. Decision policy: consistent: At least one support sentence directly entails or clearly confirms the statement. inconsistent: At least one support sentence clearly contradicts the statement. not_support: No support sentence clearly confirms or contradicts the statement (insufficient evidence). User Prompt Below are the sentence to judge and the factual paragraph: <judge_sentence> {gen_sentence} </judge_sentence> <factual_paragraph> {gt_sentence} </factual_paragraph> Output Format (strict JSON only): { \"reason\": \"...\", \"verdict\": \"consistent inconsistent not_support\" } Figure 8: Prompt used for Fact Checking. The model verifies the consistency of generated statement against gold factual paragraph. Category 1: Well-written (Part 1) Clear & Concise: 1. Articles should be written in encyclopedic style, which differs from the technically dense style found in scholarly writing aimed at specialists. 2. Articles should address the topic without twisting the truth or telling \"lies-to-children\", but should also minimize (unexplained) jargon and not take prior knowledge for granted. 3. Articles should be self-contained as much as possible, rather than relying on excessive links to explain unfamiliar concepts. Lead section: 1. An articles content should begin with an introductory lead section concise summary of the article which is never divided into sections. The remainder of the article is typically divided into sections. 2. The lead section gives the basics in nutshell, introduces the article, and cultivates interest in reading onthough not by teasing the reader or hinting at what follows. 3. The lead section should be written in clear, accessible style with neutral point of view. 4. The lead should stand on its own as concise overview of the articles topic. It should identify the topic, establish context, explain why the topic is notable, and summarize the most important points, including any prominent controversies. The notability of the articles subject is usually established in the first few sentences. 5. As in the body of the article itself, the emphasis given to material in the lead should roughly reflect its importance to the topic, according to reliable, published sources. Apart from basic facts, significant information should not appear in the lead if it is not covered in the remainder of the article. 6. lead section should be carefully sourced as appropriate, although it is common for citations to appear in the body and not the lead. 7. Significant information should not appear in the lead, apart from basic facts, if it is not covered in the remainder of the article, although not everything in the lead must be repeated in the body of the text. Exceptions include specific facts such as quotations, examples, birth dates, taxonomic names and typological classifications, case numbers, and titles. Table 6: Detailed fine-grained criteria for Well-written (Part 1). 15 Category 1: Well-written (Part 2) Words to Watch: 1. Puffery: Words such as these are often used without attribution to promote the subject of an article, while neither imparting nor plainly summarizing verifiable information. They are known as \"peacock terms\" by Wikipedia contributors. Instead of making subjective proclamations about subjects importance, use facts and attribution to demonstrate it. Words to Watch: legendary, best, great, greatest, acclaimed, iconic, visionary, outstanding, leading, celebrated, popular, award-winning, landmark, cutting-edge, innovative, revolutionary, extraordinary, brilliant, hit, famous, renowned, remarkable, prestigious, world-class, respected, notable, virtuoso, honorable, awesome, unique, pioneering, phenomenal, prominent. 2. Contentious labels: Value-laden labels such as calling an organization cult, an individual racist, sexist, terrorist, or freedom fighter, or sexual practice perversion may express contentious opinion and are best avoided unless widely used by reliable sources to describe the subject, in which case use in-text attribution. Avoid myth in its informal sense, and establish the scholarly context for any formal use of the term. Words to Watch: cult, racist, perverted, sexist, homophobic, transphobic, misogynistic, sect, fundamentalist, heretic, extremist, denialist, terrorist, freedom fighter, bigot, myth, neo-Nazi, -gate, pseudo-, controversial. 3. Unsupported attributions: These words may disguise biased view. Claims about what people say, think, feel, or believe, and what has been shown, demonstrated, or proved should be clearly attributed. Words to Watch: some people say, many people remember, many scholars state, it is believed/regarded/considered, many are of the opinion, most feel, experts declare, it is often reported, it is widely thought, research has shown, science says, scientists claim, it is often said, officially, is widely regarded as, has been described as Y. 4. Expressions of doubt: When these are used, ensure that the source of the accusation is clear. Words to Watch: supposed, apparent, purported, alleged, accused, so-called ... Also, scare-quoting: Yale \"report\"; undue emphasis: \"... Baptist church\". 5. Editorializing: These words should usually be avoided so as to maintain an impartial tone. Words to Watch: notably, it should be noted, arguably, interestingly, essentially, utterly, actually, only, clearly, absolutely, of course, without doubt, indeed, happily, sadly, tragically, aptly, fortunately, unfortunately, untimely. 6. Synonyms for said: In some types of writing, repeated use of said is considered tedious, and writers are encouraged to employ synonyms. On Wikipedia, it is more important to avoid language that makes undue implications. Words to Watch: reveal, point out, clarify, expose, explain, find, note, observe, insist, speculate, surmise, claim, assert, admit, confess, deny, confirm ... 7. Euphemisms: Euphemisms should generally be avoided in favor of more neutral and precise terms. Words to Watch: passed away, gave her life, eternal rest, make love, an issue with, collateral damage, differently abled. 8. Clichés and idioms: Clichés and idioms should generally be avoided in favor of direct, literal expressions. Words to watch: lions share, tip of the iceberg, white elephant, gild the lily, take the plunge, ace up the sleeve, bird in the hand, twist of fate, at the end of the day. 9. Relative time references: Prefer specific statements to general ones. Words to watch: recently, lately, currently, today, presently, to date, years ago, formerly, in the past, traditionally, this/last/next (year/month/winter/spring/summer/fall/autumn), yesterday, tomorrow, in the future, now, to this day, soon, since. 10. Unspecified places or events: Prefer specific statements to general ones. Words to watch: this country, here, there, somewhere, sometimes, often, occasionally, somehow. 11. Survived by: Phrasing such as \"Smith died in 1982, survived by her husband Jack and two sons\" should be avoided; this information can be made more complete and spread out through the article. Words to watch: is/was survived by, [Name]s survivors include. Table 7: Detailed fine-grained criteria for Well-written (Part 2). 16 Category 2: Broad in its coverage 1. It addresses the main aspects of the topic. 2. The lead, ideally the introductory sentence or at least introductory paragraph, of an article, should make clear what the scope of the article is. 3. All material that is notable, referenced, and that reader would be likely to agree matches the specified scope must be covered (at least in summarized fashion). 4. What reliable sources say about material that is out of scope for the decided-upon subject is largely irrelevant to that article and can be removed. 5. It stays focused on the topic without going into unnecessary detail. 6. The lead contains quick summary of the topics most important points. 7. Each major subtopic is detailed in its own section of the article. 8. It contains no irrelevant (nor only loosely relevant) information. Category 3: Neutral 1. It represents viewpoints fairly and without editorial bias, giving due weight to each. 2. Avoid stating opinions as facts. Usually, articles will contain information about the significant opinions that have been expressed about their subjects. However, these opinions should not be stated in Wikipedias voice. Rather, they should be attributed in the text to particular sources, or where justified, described as widespread views, etc. For example, an article should not state that genocide is an evil action but may state that genocide has been described by John So-and-so as the epitome of human evil. 3. Avoid stating seriously contested assertions as facts. If different reliable sources make conflicting assertions about matter, treat these assertions as opinions rather than facts, and do not present them as direct statements. 4. Avoid stating facts as opinions. Uncontested and uncontroversial factual assertions made by reliable sources should normally be directly stated in Wikipedias voice, for example the sky is blue not [name of source] believes the sky is blue. Unless topic specifically deals with disagreement over otherwise uncontested information, there is no need for specific attribution for the assertion, although it is helpful to add reference link to the source in support of verifiability. Further, the passage should not be worded in any way that makes it appear to be contested. 5. Prefer nonjudgmental language. neutral point of view neither sympathizes with nor disparages its subject (or what reliable sources say about the subject), although this must sometimes be balanced against clarity. Present opinions and conflicting findings in disinterested tone. Do not editorialize. When editorial bias towards one particular point of view can be detected the article needs to be fixed. The only bias that should be evident is the bias attributed to the source. 6. Indicate the relative prominence of opposing views. Ensure that the reporting of different views on subject adequately reflects the relative levels of support for those views and that it does not give false impression of parity, or give undue weight to particular view. For example, to state that According to Simon Wiesenthal, the Holocaust was program of extermination of the Jewish people in Germany, but David Irving disputes this analysis would be to give apparent parity between the supermajority view and tiny minority view by assigning each to single activist in the field. 7. An article should not give undue weight to minor aspects of its subject but should strive to treat each aspect with weight proportional to its treatment in the body of reliable, published material on the subject. 8. Any inclusion of fringe or pseudoscientific views should not give them undue weight. The fringe or pseudoscientific view should be clearly described as such. An explanation of how experts in the relevant field have reacted to such views should be prominently included. 9. In the case of beliefs and practices, Wikipedia content should not only encompass what motivates individuals who hold these beliefs and practices but also account for how such beliefs and practices developed. Wikipedia articles on history and religion draw from religions sacred texts as primary sources and modern archaeological, historical, and scientific works as secondary and tertiary sources. 10. Article titles that combine alternative names are discouraged. For example, names such as \"Derry/Londonderry\", \"Aluminium/Aluminum\", and \"Flat Earth (Round Earth)\" should not be used. Instead, alternative names should be given their due prominence within the article itself, and redirects created as appropriate. Table 8: Detailed fine-grained criteria for Broad in its coverage and Neutral. 17 Models Account/API Cost ($) Human Cost ($) Total ($) Open-Source Agent Framework Deep Researcher Tongyi Deep Research Langchain Open Deep Research (GPT-4.1) Langchain Open Deep Research (GPT-5) 10.7 460.9 Proprietary Agent Framework Doubao Deep Research Qwen-3-max Deep Research Perplexity Deep Research Grok Deep Search OpenAI o3 Deep Research Gemini-2.5-pro Deep Research Gemini-3-pro Deep Research 40 200 100 157.3 100 100 100 100 100 100 10.7 460.9 100 100 140 100 300 200 157.3 Table 9: Cost of collecting 100 articles for each DRA system. Account/API Cost refers to fees for model access or subscription. Human Cost refers to compensation for annotators collecting data via web services ($1/article). indicates negligible costs (e.g., free access or automated collection). Rank Category Difficulty (%) Avg. Bytes Avg. Stmts Avg. Links Avg. Views 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 History Mathematics Language and literature Art and architecture Engineering and technology Agriculture, food and drink Media and drama Music Video games Sports and recreation Warfare Social sciences and society Geography and places Natural sciences Philosophy and religion 16.6 18.9 19.6 22.0 22.2 25.2 27.0 28.4 29.0 30.2 32.0 32.6 38.9 41.8 56.2 42,882 29,445 55,728 49,681 57,869 37,317 39,891 36,845 32,491 41,862 48,352 51,052 46,078 50,660 39,287 175.8 85.0 177.8 145.7 169.1 70.0 126.1 111.9 89.7 151.3 223.7 132.5 162.0 186.6 142. 45.5 47.0 94.8 102.9 94.0 111.0 70.1 84.9 56.3 73.1 37.8 85.8 94.0 90.1 69.0 16,927 5,648 4,786 10,051 6,721 5,756 179,130 62,728 7,907 18,342 8,757 12,414 1,024 47,984 154,968 Table 10: Difficulty ranking and statistical features of different Wikipedia categories. Difficulty is defined as the average win rate in Wiki Writing Criteria of all DRA systems in that category. System Doubao Deep Research Grok Deep Search Gemini-2.5-pro Deep Research OpenAI o3 Deep Research Tongyi DeepResearch Qwen-3-max Deep Research LangChain (GPT-4.1) LangChain (GPT-5) Gemini-3-pro Deep Research Perplexity Deep Research Deep Researcher p-value 0.9781 0.9065 0.8831 0.8669 0.6097 0.5770 0.5702 0.5345 0.3200 0.1615 1.11e-11 η2 0.040 0.059 0.063 0.066 0.097 0.101 0.101 0.105 0.132 0.161 0.571 Table 11: ANOVA test results for cross-category robustness. high p-value (> 0.05) and low effect size (η2) indicate strong evidence for the null hypothesis, supporting the robustness of the evaluation across categories."
        }
    ],
    "affiliations": [
        "Metastone Technology, Beijing, China",
        "University of Science and Technology of China, Hefei, China"
    ]
}