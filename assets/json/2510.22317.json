{
    "paper_title": "Memory-based Language Models: An Efficient, Explainable, and Eco-friendly Approach to Large Language Modeling",
    "authors": [
        "Antal van den Bosch",
        "Ainhoa Risco Patón",
        "Teun Buijse",
        "Peter Berck",
        "Maarten van Gompel"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present memory-based language modeling as an efficient, eco-friendly alternative to deep neural network-based language modeling. It offers log-linearly scalable next-token prediction performance and strong memorization capabilities. Implementing fast approximations of k-nearest neighbor classification, memory-based language modeling leaves a relatively small ecological footprint both in training and in inference mode, as it relies fully on CPUs and attains low token latencies. Its internal workings are simple and fully transparent. We compare our implementation of memory-based language modeling, OLIFANT, with GPT-2 and GPT-Neo on next-token prediction accuracy, estimated emissions and speeds, and offer some deeper analyses of the model."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 7 1 3 2 2 . 0 1 5 2 : r Memory-based Language Models: An Efficient, Explainable, and Eco-friendly Approach to Large Language Modeling Antal van den Bosch1, Ainhoa Risco Patón1, Teun Buijse1, Peter Berck2, Maarten van Gompel3 1Utrecht University, 2Lund University, 3Royal Netherlands Academy of Arts and Sciences Correspondence: a.p.j.vandenbosch@uu.nl"
        },
        {
            "title": "Abstract",
            "content": "We present memory-based language modeling as an efficient, eco-friendly alternative to deep neural network-based language modeling. It offers log-linearly scalable next-token prediction performance and strong memorization capabilities. Implementing fast approximations of k-nearest neighbor classification, memorybased language modeling leaves relatively small ecological footprint both in training and in inference mode, as it relies fully on CPUs and attains low token latencies. Its internal workings are simple and fully transparent. We compare our implementation of memory-based language modeling, OLIFANT, with GPT-2 and GPT-Neo on next-token prediction accuracy, estimated emissions and speeds, and offer some deeper analyses of the model."
        },
        {
            "title": "Introduction",
            "content": "Memory-based language modeling, proposed by Van den Bosch (2006b), is machine learning approach to next-token prediction based on the k-nearest neighbor (k-NN) classifier (Aha et al., 1991; Daelemans and Van den Bosch, 2005). This non-neural machine learning approach relies on storing all training data in memory, and generalizes from this training data when classifying unseen new data using similarity-based inference. Memorybased language modeling is functionally equivalent to decoder Transformers (Vaswani et al., 2017; Radford et al., 2019), in the sense that both can run in autoregressive text generation mode and predict (a distribution of) next tokens based on certain amount of prior context. While training memory-based language model is generally low-cost, as it involves one-pass reading of training data and does not involve any convergence-based iterative training, naive implementation would render memory-based language modeling useless for inference. The upper-bound complexity of k-nearest neighbor classification is notoriously unfavorable, i.e. O(nd), where is the number of examples in memory, and the number of features or dimensions (e.g. context size). However, improvements and fast approximations are available. Daelemans et al. (2010) offer range of approximations offering fast classification and data compression using prefix tries. Another notable aspect of memory-based language modeling, as observed earlier by van den Bosch (2006) for training sets of tens of millions of words, is that its next-word prediction performance appears to increase log-linearly: with every 10-fold increase in the amount of training data, memory-based nextword prediction accuracy appears to increase by more or less constant amount. This trend may change with unobserved larger amounts of training data; it may also reach plateau which we currently do not reach because of memory limitations. The relatively low costs in learning as well as inference make memory-based language modeling potential eco-friendly alternative to the generally costly training of Transformer-based language models (Strubell et al., 2019). This is what this paper aims to explore and document. All experiments with our implementation of memory-based language modeling, system called OLIFANT1, have been carried out with publicly available software, with TiMBL as the basic classification engine.2 All required scripts for training and inference with OLIFANT are available on GitHub.3 Although pretrained OLIFANT models may be run on standard consumer hardware with normal CPUs and rea1Olifant is the Dutch word for elephant. Quoted from https://en.wikipedia.org/wiki/Elephant_cognition, \"Most contemporary ethologists view the elephant as one of the worlds most intelligent animals. Elephants manifest wide variety of behaviors, including those associated with grief, learning, mimicry, playing, altruism, tool use, compassion, cooperation, self-awareness, memory, and communication.\" 2https://github.com/LanguageMachines/timbl 3https://github.com/antalvdb/olifant sonably modest RAM requirements, scaled training does eventually require quite large amounts of RAM. The largest pre-trained OLIFANT models currently available require in the order of 256 GB of RAM to run. The paper is structured as follows. Section 2 sketches the architecture of OLIFANT and provides an overview of previous and related work. In Section 3 we offer learning curve analyses on the nexttoken prediction task, including carbon emission estimates, in which we compare the different k-NN approximations available, and their effect on token generation speed and accuracy. We also compare against differently sized variants of Transformerbased next-word predictors, GPT-2 and GPT-Neo, of which we broadly know how much data they have been trained on. In Section 4 we focus on specific property of OLIFANT that sets it apart from neural LMs, i.e. its ability to memorize and recite training data. In the remainder of the paper, we offer additional analyses that highlight some of the extensions of the approach. In Section 6 we analyse the token distributions generated by OLIFANT through frequency lens to test hypothesis put forward by Khandelwal et al. (2020), and show by comparison how the sparse distributions of OLIFANT variants lead to lower perplexities on validation data when compared to neural models. We offer some conclusions in Section 7 and critically assess the current limitations of the approach."
        },
        {
            "title": "2 OLIFANT: Background and architecture",
            "content": "We begin with brief overview of previous and related work. We then describe the architecture used in our three alternative implementations of memory-based language modeling available in OLIFANT, ranging from lossless k-nearest neighbor classification to lossy decision-tree classification. 2.1 Previous and related work Memory-based language modeling is rooted in two classic machine learning paradigms: k-nearest neighbor (k-NN) classification (Fix and Hodges, 1951; Cover and Hart, 1967; Aha et al., 1991) and decision-tree classification (Quinlan, 1986). Its internal structure is based on an efficient retrievaloriented tree-shaped index, prefix trie4 (Knuth, 1973), in which training set of classification instances can be stored and retrieved efficiently. Us4\"Trie\" as an amalgam of tree and retrieval. ing the concept of information gain (Quinlan, 1986) or its robust variant gain ratio (Quinlan, 1993) as the feature ordering heuristic, Daelemans and Van den Bosch (1993) introduced IB1-IG as featureweighted implementation of k-NN, and Daelemans et al. (1997) introduced IGTree as decision-tree classifier, both making use of on Knuths prefix tries. Both are implemented in TiMBL (Daelemans et al., 2010).1 Using IB1-IG or IGTree for language modeling was proposed by Van den Bosch (2006b) and further developed by Van den Bosch (2006a); Van den Bosch and Berck (2009). Memory-based language modeling has been applied to spelling and grammar correction and (personalized) text completion (Van den Bosch and Berck, 2012, 2013; Stoop and Van den Bosch, 2014a,b; Berck, 2017). The relation between k-NN and neural language models has been studied occasionally. Notably, Khandelwal et al. (2020) describe \"Nearest Neighbor Language Models\" as the linear interpolation of pre-trained neural LM with k-NN classifier operating on data store representing training data encoded in the LMMs embedding space. One of their findings is that k-NN is relatively effective in the long tail, i.e. in bringing forward relatively low-frequency events into the predicted token distribution compared to the pre-trained LLM. We return to this observation in Section 6. The topic of memorization capabilities of LLMs, an explicit property of memory-based language models we return to in Section 4, has mostly been framed from the viewpoint of the adversarial elicitation of recitations of content of which the reproduction violates copyright or privacy laws (Carlini et al., 2021, 2023; Hayes et al., 2025). Memorization in neural LLMs is generally observed in these studies to be hard to trigger, to occur only sparsely, and to require models to be large, inference to be greedy (e.g. by setting the hyperparameter temperature to 0), and data to be frequently present in the training data. 2.2 Architecture memory-based next-token predictor takes as input fixed number of position-specific context tokens positioned before the token to be predicted, [wn . . . w1], and predicts distribution of next tokens likely to occur at position w0 (m 1), where each candidate is associated with count (representing the number of neighbors labeled with that candidate). Counts may optionally be softFigure 1: Schematic conversion of an instance base with four examples of two context words predicting third word, into prefix trie with next-token distribution information stored at all nodes. Grey nodes, representing subsets of the instance base of which the majority class does not conflict with the parent node, are not stored when the prefix trie is used for decision-tree classification (the IGTree mode of OLIFANT). maxed to sum to 1. We have experimented with three variants of memory-based language modeling that differ in how the classifier is trained and how it performs classification or inference (Daelemans et al., 2010). All three make use of the same underlying storage structure, so-called prefix trie (Knuth, 1973), offering efficient retrieval of classification information. Figure 1 illustrates how simple training set of four instances is represented in this tree structure. Instances represent contexts of two tokens (e.g. it went) predicting the third next token (well). The order in which context words are represented at levels of the tree is automatically determined by computing the gain ratio (Quinlan, 1993) of all context positions. With next-token prediction this amounts invariably to first checking the token immediately before the token to be predicted, w1, followed by the token before that, w2, etcetera. Differing from standard decision-tree classifiers, gain ratio is only computed once for the entire training set for the prefix trie; each level of the trie tests one feature. The layer under the top node tests values occurring in w1; the layer under that tests w2; etcetera. The top node of the trie represents the full distribution of possible next-token outcomes with absolute counts (optionally normalized to probability distributions). At each next level of the trie, path is generated for every value that represents subset of the original instance base in which more than one outcome is still possible. Once an instance base is converted to prefix Figure 2: Schematic visualization of classification in the three memory-based language modeling variants. The larger triangle represents the entire prefix trie; grayed zones represent k-NN classification; nodes and edges represent downward-pass decision-tree traversal of the prefix trie. trie, OLIFANT can run in three increasingly faster modes of classification, as also visualized in Figure 2: 1. IB1-IG (Aha et al., 1991): functionally faithful implementation of k-NN classification with information-theoretic feature weighting. Based on new unclassified instance, the most similar instances are identified in the prefix trie, producing as output the distribution of class labels stored with these nearest neighbors. The default similarity metric used in all experiments is the Overlap (or L1, or Manhattan) distance weighted per feature by the features information gain (Daelemans et al., 2010), as in Equation 1, where is the test instance represented by features, x0 . . . xn, and is every memorized instance represented also by features. The distance between the two instances, , is sum of weighted distances per feature, where δ is the overlap function δ(xi, yi), as in Equation 2. (X, ) = (cid:88) i=1 wi δ(xi, yi) (1) δ(xi, yi) = (cid:40) 0 if xi = yi 1 if xi = yi (2) By default, the weight of each feature is its information gain ratio (Quinlan, 1993), as in Equation 3. This is the reduction in the entropy of the classification label, the token to be predicted (H(C) = (cid:80) cC (c) log (c), representing the next token, representing all possible next tokens, Vi the set of values at feature when knowing the value of feature, divided by the entropy of the feature itself, si(i) as in Equation 4. wi = H(C) (cid:80) vVi (v) H(Cv) si(i) (3) si(i) = (cid:88) vVi (v) log (v) (4) Classification involves multiple traversals of the trie until the nearest neighbors are identified; the set of equidistant neighbors at the closest distance (X, ). This search is somewhat optimized by avoiding the traversal of subtrees in the trie that are guaranteed not to produce nearest neighbors closer than the most distant of the current top-k neighbors, as the sum of remaining potentially matching feature weights is lower than that. In our experiments, we kept = 1; 2. TRIBL2 (Daelemans et al., 2010) begins the classification process as decision-tree classifier traversing down the tree in single path, matching the values of the unclassified instance to values stored in the trie in their gain ratio order. Upon halting at trie level due to failing to find match between the currently tested input feature value and values stored at the trie nodes, it switches to perform k-NN classification on the remaining sub-instance base under the last matching parent node in the prefix trie. This hybrid offers trade-off between the sensitivity of IB1-IG (using the same full trie) and the speed of the third option, IGTree; 3. IGTree (Daelemans et al., 1997) utilizes the prefix trie fully as decision tree. Given new unclassified instance, single path in the tree is traversed, matching the values of the new instance against nodes in their gain ratio order. Classification occurs when no matching node is found at the next trie level or matching leaf node is met; the next-token distribution stored at the last matching node is then returned as output. IGTree implements typical decision-tree \"lossy\" heuristic, viz. to not store nodes that agree with the majority class label of their parent node (visualized by the grey labels in Figure 1). In our experiments, not storing these nodes (and their sub-trees) typically yields compression rates of about 95%, compared to losslessly storing all information concerning all training instances (as in IB1-IG and TRIBL2). 2.3 Scaling dimensions in training Before we present empirical scaling results of both training and autoregressive next-token prediction of the three memory-based language modeling variants in Section 3, one difference between scaling in Transformer-based LMs and memory-based LMs should be clarified. Visualized schematically in Figure 3, training Transformer-based LMs involves making choices in three uncoupled dimensions: the amount of training data, the size of the model, and the amount of compute invested in iterative training. In contrast, in memory-based language modeling, the model is quite literally the data, and the amount of data fully and linearly determines the amount of compute involved in creating the prefix tries. Figure 3: Schematic visualization of the three uncoupled scaling dimensions in training Transformer-based LMs, versus the single coupled scaling dimension involved when training memory-based LMs, where the model equals the data. Due to their complex interactions, studying the cross-effects of scaling in the data, model, and compute dimensions in Transformers is not trivial (Kaplan et al., 2020). Studying scaling in memorybased LMs, in contrast, can be reduced to learning curve analyses, where training sets are increased and held-out test material is used to measure the effects of more training data."
        },
        {
            "title": "3 Learning curve analyses",
            "content": "As suggested by Van den Bosch (2006b), learning curves measuring next-token prediction accuracy should show that with every n-fold increase of the training set, prediction accuracy increases by roughly constant amount. If this is the case, extrapolations of performance at larger amounts of training data that take larger computational resources than currently available can be made within reasonable bounds of likelihood and precision. The source for data in all experiments is the EduFineWeb corpus, freely available 1.3 trillion token corpus consisting of web-crawled data, curated for educational content.5 Training data is taken from the first shards of this corpus, up to five billion tokens. The test data consists of the first 10 thousand lines of the first shard of EduFineWebs validation subcorpus, amounting to 512,660 tokens. Throughout the experiments we used one tokenizer, the GPT2 tokenizer available from Hugging Face6 for proper comparison. This tokenizer has 50,257 entries in its vocabulary, composed of 50,000 tokens resulting from byte-level BPE (Sennrich et al., 2016), the 256 base characters as single tokens, and special end-of-text token. 3.1 Context width first series of learning curve experiments was performed to establish sufficient context width. With TRIBL2, the middle-ground variant between IB1IG (knearest neighbor classification) and IGTree (decision-tree classification), we performed training runs with increasing training set sizes, from 10,000 lines with increments of factor 10, up to 100 million training instances. Checkpointing is available as an option, i.e., trained IB1-IG and TRIBL2 models can be saved and loaded again when training is continued with more data, as k-NN models are incremental learners by default, and prefix trie can be incrementally expanded. In all runs, token prediction accuracy was measured using the same validation set. Figure 4 shows learning curves for context widths [1, . . . , 5]. With context width 1, the next token is predicted based on only the preceding token. In Figure 4, this blue graph starts with the highest accuracy, but tapers off quickly with more training data. It appears from this graph (given the particular training and validation data used) that the correct next token can be predicted based on only the previous token at ceiling performance of about 16%. The graph of context width 2 tapers off at later point; the accuracies for context widths 3, 4 and 5 seem to continue going up while hardly differing. In the remainder of our experiments, we continue working with context width of 4 for all memory-based variants. context of 4 tokens offers substantially less 5https://huggingface.co/datasets/ HuggingFaceFW/fineweb-edu 6https://huggingface.co/openai-community/gpt2 Figure 4: Learning curves in terms of correctly predicted tokens of TRIBL2 with increasing context widths. context than the typical context width employed by Transformer-based next-token predictors. 3.2 Comparison with GPT variants We compare our implementation of memory-based LMs, OLIFANT, against the freely available GPT2 family (Radford et al., 2019) and EleutherAIs GPT-Neo family (Gao et al., 2020). Both sets of pre-trained Transformers include models of multiple sizes, trained on the same training data per family. Both models and training data sets are small enough to offer reasonable comparisons with OLIFANT, although the resources available to us are substantially smaller.7 GPT-2 comes in small (124M parameters), medium (355M parameters), large (774M parameters), and xl (1.5B parameters) variants, and is trained on the unreleased WebText corpus, 38GB cleaned web-crawled corpus with an estimated 8,892,000,000 tokens.8 GPTNeo is available in two sizes, 1.3B and 2.7B, and is trained on the highly problematic and unavailable The Pile dataset, 889GB of raw text from multitude of sources, with an estimated total of 196,735,500,000 tokens, about two orders of magnitude more than WebText and three orders of mag7For training sets under billion tokens, training was performed on an AMD EPYC 7313P 16-Core Processor; for larger training sets, training was done on high-memory CPU nodes with 4TB up to 8TB directly addressable RAM, made available by the Netherlands Snellius HPC facility, https://servicedesk.surf.nl/wiki/spaces/WIKI/ pages/30660184/Snellius 8For this estimate we took as basis an estimated 180 million English words per GB of normal web text, and factor of 1.3 to go from English text to English tokenized text: 18038 = 6, 840, 000, 000 words, 1.3 = 8, 892, 000, 000 tokens. nitude more than our current maximal training set size. 3.3 Token prediction accuracy Figure 5 shows the percentage of correctly predicted tokens at various training set sizes of the three OLIFANT variants, compared against the GPT systems. The GPT variants have fixed training set sizes, hence their vertical graphs. Unlike the memory-based variants, they have been trained iteratively over multiple epochs on their respective training sets. Surprisingly, the performance graph of TRIBL29 does not exactly show log-linear relation between training set size and token prediction accuracy, as observed earlier by van den Bosch (2006) with smaller training set sizes. There seems to be bend in the curve at around 1 billion training tokens; after the bend, the slope of performance increase is steeper than before this point. First, we performed log-linear regression analysis on the whole range of measurements for TRIBL2 (the dotted line in Figure 3). The correlation with the actual measurements is high, with = 0.9935. The regression function, oken prediction accuracy = 0.1006 + 0.0178 ln(T raining set size), predicts an increase in prediction accuracy of 4.1% with every 10-fold increase in training set size. Based on this estimate, if trained on 1 trillion tokens, TRIBL2 would predict an estimated 39.2% of all tokens correctly. Assuming that change in performance increase happens around the 1-billion-token training set size, we performed an additional logistic regression on the measurements starting from the 1 billion token training set size upwards. The correlation of this regression with these five measurements is high (r = 0.9993). The regression function, oken prediction accuracy = 0.3960 + 0.0316 ln(T raining set size), now predicts an increase in accuracy of 7.3% with every 10-fold increase in training set size. Extrapolating, trained on 1 trillion tokens TRIBL2 would predict an estimated 47.7% of all tokens correctly. We leave answering the obvious underlying question, what causes this apparent shift in performance slope, for future research. The remaining prediction accuracy surplus of the GPT systems over the extrapolated performance of OLIFANT variants, at the training set sizes of the WebText corpus or The Pile, must be attributed to the benefit of the way the attention mechanism works (Vaswani et al., 2017) and how it is able to utilize information from wide context to predict next words more accurately. While OLIFANT works only with four context tokens, both GPT-2 and GPTNeo operate on buffer of much wider context of 1,024 tokens.10 On the other hand, the extrapolated regression function suggests that four context tokens offer enough information to approach or even surpass the performance of these older, smaller GPT systems in next-token prediction. 3.4 Token prediction latency As expected, there is large difference in the speeds of the three OLIFANT variants, despite their similar accuracies. Figure 6 compares the latencies in seconds between the generation of two tokens of the three OLIFANT variants and the GPT systems. For comparison, the GPT systems are run on CPUs; in particular, they are run in parallel on 16 CPU cores. While IB1-IG turns out very slow, taking nearly second per prediction when trained on 100 million tokens, the TRIBL2 and IGTree variants exhibit very low latencies, lower than any of the GPT systems. In terms of predicted tokens per second, trained on 100 million tokens, TRIBL2 clocks 118 tokens per second; IGTree performs predictably faster with its single-pass decision-tree classification method, clocking 739 tokens per second, all at roughly the same next-token prediction accuracy. 3.5 Estimated CO2 emissions CO2 emissions during inference, estimated from electricity usage and carbon intensity estimates, roughly follow the same pattern as the latency analysis of the previous subsection, as slower performance implies longer, thus higher electricity use. Figure 7 displays the CO2 emission equivalent estimations during the full processing by all compared models of the full EduFineWeb validation set (10 thousand lines, 512,660 tokens) as computed by CodeCarbon12, which collects live electricity usage and power measurements from the BIOS of 10Different from GPT-2, GPT-Neo uses local attention in every other layer with window size of 256 tokens; see https://huggingface.co/docs/transformers/ v4.18.0/model_doc/gpt_neo 11With training sets larger than one billion, our measurements show variance that is due to differences in hardware that was beyond our control. To save electricity, we have restricted ourselves to single runs. 9At the time of writing, results for IGTree beyond 100 12We used CodeCarbon 3.0.0; see https://codecarbon. million token training sets were not available yet. io/ Figure 5: Learning curves on next-token prediction accuracy of the three OLIFANT variants and the two GPT systems (with fixed training data sets but increasing model sizes). Dashed and dotted lines are regression functions fitted to all TRIBL2 measurements (dotted) and measurements from 1 billion training tokens (dashed). the computer the experiment is run on. Measurements of kWh usage of RAM and CPU were added to arrive at total usage per experiment. As experiments were run on stand-alone server (not performing any other tasks) located in Germany, carbon intensity of 344 is used.13 As with the latency measurements, we ran the GPT systems on CPU, rather than GPU, for directly comparable measurement. When instead run on T4 GPU, GPT-2 small processes the entire validation set about three times faster than when run on CPU, costing about three times less electricity. This illustrates that in contrast to training, where GPUs offer orders of magnitude faster processing, the intrinsically serial process of autoregressive inference may actually be in the same order of magnitude on CPUs as they are on GPUs with neural LMs. The figure also adds common CO2 emission equivalents for single washing machine run, driving car for 10 minutes, tumble dryer run, and the emissions involved with the furnace-based production of 1kg of steel, and the aggregate emissions involved in the production of 1l of cow milk.14 From Figure 7 we can observe that while IB1-IG scales rather unfavorably when trained on more 13The carbon intensity estimate for Germany is for https://ourworldindata.org/grapher/ 2024; carbon-intensity-electricity?mapSelect=DEU see 14Statistics taken from https://clevercarbon.io/ carbon-footprint-of-common-items data, both its approximations TRIBL2 and IGTree show low CO2 equivalent emissions during inference, regardless of training set size. The internal prefix trie structure causes retrieval to be efficient, staying well below 200 grams of CO2 equivalents when processing the entire validation set. Looking closer at the results for TRIBL2 and IGTree, they show some non-monotonic variance and some larger variability with the largest training set sizes. In contrast, the largest of the small GPT systems in our comparison, GPT-Neo2.7B, emits about 2.7 kilograms of CO2 when processing the validation set. Table 1 lists the CO2 emissions factored to the token level for TRIBL2 and IGTree trained on 500 million tokens, and the largest GPT2 and GPT-Neo models. Set in milligrams, they underscore again the large difference in footprint between the two architectures.16 15Accidentally, the size of each GPT system in billions of parameters is similar to its CO2 emission equivalent in kg when processing this validation set. 16The numbers also suggest that GPT-Neo 2.7B can generate about 191 tokens within one gram CO2 emission equivalent; this is reminiscent of the various estimates of larger GPT systems emitting 2 to 5 grams per query (often, amortized emissions incorporating the cost of training are factored into these estimates). By comparison, IGTree generates only 5.2 mg of CO2 emission equivalent when generating 200 tokens. Figure 6: Next-token prediction accuracy latencies (s). Lower is better. Model TRIBL2 500m IGTree 500m GPT-2 XL 1.5B GPT-Neo 2.7B CO2 emission eq. mg per token 0.128 0.026 3.155 5. time (mm:ss) and CO2 emission eq. (g) during training on Model TRIBL2 IGTree 4,890,203 time 01:45 01:42 CO2 0.51 0.50 100,000,000 time 33:23 34: CO2 13.7 14.0 Table 1: comparison of CO2 emission equivalents in mg per token during next-token prediction. Table 2: Training timings and CO2 emission equivalents (g) with two training set sizes for TRIBL2 and IGTree. 3.6 Model sizes and CO2 emissions during training Training OLIFANT systems consists of reading the training data, converting it to prefix trie, and saving this structure to file. Linear in the number of training instances, training can take multiple hours to days with the larger training sets. Using the CodeCarbon package we logged electricity usage during the training of the TRIBL2 and IGTree systems; the estimated CO2 emission equivalents are listed in Table 2. They capture the entire training procedure, from reading the training data to saving the prefix trie. They also show the time used for training. The training times for IGTree and TRIBL2 are comparable but differ slightly. In general, IGTree spends extra time on pruning the prefix trie, but since it is 90%95% smaller, it takes less time to write to file. For comparison, the Transformer-based BLOOM language model17 of 176 billion parameters was trained on 350 billion tokens, and is estimated to have cost 24.7 tonnes (24,700 kg) of CO2 emissions to train (Luccioni et al., 2023). If TRIBL2 and IGTree are assumed to consume electricity at about 140 grams per billion training tokens, it would cost the equivalent of about 49 kilograms of CO2 to train on the same number of training tokens as BLOOM 176B, three orders of magnitude less.18 Model sizes also scale linearly with the amount of training data. Figure 8 compares the tree sizes the average European level, 17https://huggingface.co/bigscience/bloom 18At new passenger car produces 93.6 grams of CO2 per kilome- (https://www.eea.europa.eu/en/analysis/ ter indicators/co2-performance-of-new-passenger); 49 kilograms of CO2 is produced when the average new car drives 523 kilometers. It takes 263,889 kilometers for the same car to reach the equivalent emissions of training BLOOM 176B. The paper reporting on BLOOM also makes guess on the CO2 emissions of training GPT-3: 502 tonnes. Figure 7: CO2 emission equivalents (g) of the various models on predicting the EduFineWeb validation set of 10,000 lines (512,660 tokens). Grey horizontal lines represent real-world CO2 emission examples. (in nodes) of IGTree and TRIBL trained with context width 4, with increasing training set sizes. Both axes of the figure are logarithmic. The graphs show structural advantage of IGTree over TRIBL2 in terms of tree size, with IGTree operating on about 90% to 95% (one order of magnitude) smaller trees, when trained on the same amount of data. Figure 8: Tree size (in nodes) of IGTree and TRIBL2 as function of training set size. The dashed grey line represents the parity = line. TiMBL, the software engine underlying our implementation, allocates 40 bytes to single tree node. The TRIBL2 model trained on 1 billion tokens is tree with 1,428,959,863 nodes: each example with four context tokens contributes just over 1.4 nodes on average. Storing this tree in memory costs 57,158,394,520 bytes, which is just over 53.23 gigabytes. When trained on 2 billion tokens, the tree has 2,507,369,723 nodes (a contribution of 1.25 nodes per training example), which costs 93.4 gigabytes of memory; scaling is slightly sub-linear."
        },
        {
            "title": "4 Memorization",
            "content": "While neural LMs are hard-pressed to recite materials in their training data for long stretches of tokens (Carlini et al., 2021), especially with the temperature hyperparameter set higher than 0, memorybased LMs are able to reproduce their training data up to high level of accuracy. TRIBL2 should do this most faithfully as it is k-NN classifier (with fast indexing), while the IGTree variants may be somewhat less precise due to their reliance on decision-tree classification in pruned prefix trie. Table 3 shows the memorization capacities of both variants trained with context width 4, tested token by token on the first 10 thousand lines (486,724 tokens) of the first shard of the EduFineWeb corpus which they were also trained on. The first observation is that memorization accuracies are high, but under 100%. The expected best memorizer, TRIBL2, mispredicts 3.7% of all memorizations when trained on only the test set itself. misprediction must be due to ambiguous outputs given an exact match on the context of four previous words. Given two or more exact-matching Model TRIBL2 IGTree Token memorization (%) trained on 100,000,000 486,655 82.52 96.32 80.02 93.14 4,890,203 92.71 89.62 Table 3: Memorization accuracy at the token level (%) of TRIBL2 and IGTree tested on the first 10 thousand lines (486,655 tokens) of the training set, when trained on this exact same data set and two larger training sets, both including the first 10 thousand lines. nearest neighbors predicting different next words, the classifier picks the most likely (i.e. frequent) outcome, which might be different from the actual outcome. If the matches are equally likely (when they have the same occurrence count), the tie is broken randomly, which may also lead to contrasting outcome.19 second observation is that when the training material on which the memorization test is performed becomes part of larger training set, as shown in the two final columns of Table 3, even more alternative outcomes than the next word in the training data become more likely, overruling the single case in memory; again, given the limited context width of 4 previous words. In the case where the training set contains 100,000,000 instances (the tables rightmost column), the first 10 thousand lines representing only 0.5% of that training set, close to 20% of all reproductions of tokens in those first 10 thousand lines are incorrect because the training data contains cases of exactly matching contexts that predict other outcomes as more likely. In sum, memory-based LMs are reasonable but imperfect memorizers of training data, and less so when trained on more data. Also, context of 4 words is not enough to disambiguate in about one in five predictions."
        },
        {
            "title": "5 Explainability",
            "content": "Each individual classification of next word is based on match in prefix trie, producing distribution of possible next words, with (optionally normalized) counts. The TiMBL software allows for various levels of verbosity; depending on the choice of algorithm, it may produce the nearest neighbors found at various distances for single classification. For instance, Figure 9 shows one 19Random tie breaking in TRIBL2 and IB1-IG has to be explicitly invoked with the -R <seed> argument, otherwise another deterministic procedure is followed (Daelemans et al., 2010). We set -R 42. example classification of predicting the next word in the four-token sequence \"[ membrane] [ was] [ synthes] [ized]\" (the brackets indicate whether the token is word-initial by including space in front or not). The next token should be \"[ through]\". Figure 9: Example output of TRIBL2 classifying single instance and producing most likely prediction and distribution of predictions, based on 20 equally distant nearest neighbors. Token information on word-initial status is left out for legibility. The predicted next word is, incorrectly, \"[ by]\". As the output shows, this prediction is by far the most frequent outcome among the 20 nearest neighbors found at the same distance of two matching tokens (the two tokens immediately preceding the predicted token) and two mismatching tokens (the two tokens before that). The correct word \"[ through]\" is not among the possibilities. In TiMBL, the = 1 setting is implemented such that all equidistant neighbors at the closest distance, i.e. with the smallest mismatching number of words weighted by their gain ratio, are taken together as the basis for the output distribution. In other words, in this case the distance is the sum of gain ratio weights of the mismatching features, the two leftmost context tokens; neither \"[ membrane]\" nor \"[ was]\" was matched in any closer neighbor. Within the set, two identical neighbors are found with different outcomes (\"[ from]\" and \"[ without]\"), and another pair of identical neighbors are found with the same outcome (\"[ by]\"). All other neighbors represent unique contexts occurring once in the training data. This example illustrates that prediction by TRIBL2 and IB1-IG can be traced to individual instances stored in the prefix trie. Unique identifiers that would link an instance to its original document could be included in \"hidden\" lowest level in the prefix trie so that individual predictions could even be attributed to (locations in) specific sources. Classification in IGTree, however, offers less verbosity; due to its pruning of the prefix trie only the next-word distribution and the depth of the path can be shown."
        },
        {
            "title": "6 Perplexity and a distributional analysis",
            "content": "of OLIFANT predictions In contrast to largely non-zero softmaxed logits produced over the entire token vocabulary by GPT systems, next-token predictions generated by memorybased models are sparse; by comparison they are almost empty. As an example, TRIBL2 trained on 100 million tokens predicts next tokens based on nearest-neighbor distributions with median of only 6 tokens, from which the most likely next token is selected. As we set = 1 throughout our experiments, most IB1-IG and many TRIBL2 predictions are based on single nearest neighbor, or on set of equidistant nearest neighbors. Larger distributions do occur, however; the mean number of next-token predictions in TRIBL2 output is 87.5. IGTree bases its predictions on somewhat larger distributions: its predictions have median of 13 and mean of 356 possible next tokens. This difference can be explained by IGTrees earlier stopping at higher levels and in non-ending nodes in the tree, where nodes represent relatively larger subsets of memorized instances. We computed perplexity scores as the sum of base-2 logprobs assigned by the different systems to the target tokens, averaged over all tokens that received non-zero probability from the same edufineweb validation set as used before in the learning curve analyses. Using double logarithmic axes, Figure 10 shows the perplexities attained by the different OLIFANT systems trained on increasing amounts of training data, compared to those of GPT-2 and GPT-Neo at different model sizes (and fixed training set sizes). From the figure it is apparent that the perplexity scores of IB1-IG and TRIBL2 are considerably lower than those of IGTree and the GPT systems. Zooming in, it turns out that IB1-IG and TRIBL2 frequently assign zero probability to the actual next token, meaning that we did not include the inf logprob in computing perplexity. Trained at 100 million training tokens, IGTree assigns 49.6% of all actual next tokens non-zero probability, TRIBL2 assigns this to 41.9% of all next tokens; trained on 5.5 billion tokens, this rises to 63%. GPT2-small, to confirm, assigns 100% of all next tokens non-zero weight. In other words, IGTree and the GPT-2 systems spread their chances over many if not all possible outcomes, while TRIBL2 and IB1-IG limit this spreading to just few candidates, often assigning zero probability to the actual next token. Having placed bets on small number of proverbial horses, TRIBL2 and IB1-IG are less perplex when one of their few best guesses turns out to be the actual next token. difference between TRIBL2 (and IB1-IG) on the one hand, and IGTree on the other hand is which tokens they predict. The token vocabulary used throughout this paper is the one used for GPT2. Given the counts of all tokens occurring in the 100 million-token training set, Figure 11 shows the distribution of the binned frequency-of-occurrence counts of the tokens actually predicted by TRIBL2 and IGTree. For comparison, the figure also shows the same numbers for the tokens actually predicted by GPT2-small, which is trained on different (and larger) training set, the aforementioned 8.9 billion token WebText corpus, using the same tokenizer. The largest three bars on the left represent the tail of the frequency band with low-frequency tokens, and the large bars on the right end represent the head, i.e. the most frequent tokens. Although the differences are less outspoken than in Figure 10, it can be observed that GPT2-small predicts fewer lowfrequency and more high-frequency tokens than the memory-based models. This confirms the aforementioned observation made by Khandelwal et al. (2020) that k-NN is effective in pushing relatively low-frequency events into the predicted token distribution compared to neural language models."
        },
        {
            "title": "7 Discussion and conclusions",
            "content": "Although the best performances presented in this paper, obtained in the fall of 2025, are only approaching those of GPT-2 from six years ago, we argue that memory-based language models offer an interesting alternative to neural foundation models. They may prove useful in contexts where relatively small and fast foundation models are needed as component, e.g. in speculative decoding (Leviathan et al., 2023). Trained on more data, their performance can reliably predicted to Figure 10: Perplexity learning curves of the different systems. Both axes are logarithmic. Summarizing, our results and analyses have highlighted favorable properties of OLIFANTs fast approximations of k-nearest neighbor classification, TRIBL2 and IGTree, with TRIBL2 having few more advantages over the faster of the two, IGTree: Learning is scalable, and incremental with TRIBL2. Model performance increases approximately log-linearly with more data, or better; model size scales linearly. IGTree produces 90%-95% smaller models than TRIBL2 and processes in the order of hundreds of tokens per second. TRIBL2 is natural incremental learner and can learn and finetune with checkpointing; Figure 11: Bar-graph visualization of the distribution of next-token prediction frequencies of occurrence in 100 million-token training set by TRIBL2 and IGTree, both trained on 100 million instances, and by GPT2-XL, all tested on the fixed validation set. go up. Our learning curve study even showed an increase in the steepness of log-linear performance growth beyond one billion training tokens; yet, we cannot be certain about the curves flattening at later point, e.g. because of the inherent limitations of the four-token context we have now restricted our models to. We believe from our emission estimates that we argue that our models small ecological footprint could be an advantage in many situations, also at scale. OLIFANTs reliance on CPUs rather than GPUs or TPUs opens alternative paths to scaling with potentially drastic ramifications on electricity usage, and also on costs. Consistently low CO2 emissions during training and inference. Our packaged implementation of memory-based language models OLIFANT runs on CPUs. Using extrapolations and reported estimates for Transformer-based systems, training with OLIFANT is estimated to cause in the order of 1,000 times fewer CO2 emissions than neural LM training. Inference costs are 10-100 times lower; Fully transparent functioning. TRIBL2 offers nearest-neighbor-based explanations for predictions, based on individual examples used for prediction. Nearest neighbors can even be source-tagged for full provenance; Intentional memorization of training data. At context size 4, OLIFANT models can recite 80% or more of tokens from their training data faithfully. For completeness we note that OLIFANTs underlying engine, TiMBL, can be parallelized on multi-core CPUs both for training (Van den Bosch and Van der Sloot, 2007) and inference. For inference TiMBL can run in multiprocessor mode, effectively cloning itself (Daelemans et al., 2010). Depending on the way TiMBL is integrated in other software,20 the availability of CPU cores can potentially lead to speedup of up to factor in training, in inference and in specific settings such as beam search or speculative decoding. An obvious next step is to continue incremental training, and to perform benchmarking on standard tasks for decoders, starting with older benchmarks such as GLUE (Wang et al., 2018). That said, we would like to argue that next-token prediction is valid benchmark task in its own right, that brings to light differences at small training sets, at which performance on more complex benchmark tasks would still be at baseline level. Limitations and future work The key current limitation of the memory-based language modeling approach as implemented in OLIFANT is that it is now confined to positionspecific context with microscopically small width compared to the usual neural LM context width. We have experimented with larger position-specific contexts, up to 16 words. For IGTree, wider contexts lead to larger tries but hardly better predictions; for TRIBL2, wider contexts lead to larger tries, slower performance and small gains in nexttoken prediction accuracies. Context information with predictive value that is obviously available in the wider context, but that arguably should not be encoded in position-specific features, could be made available in other ways, e.g. through encoding it as bag-of-words vector. This is an obvious avenue for future work. TiMBL, the underlying engine, offers number of extensions that could be profitable for generalization performance and could enhance the functionality of OLIFANT. First, individual examples can be weighted with an exemplar weight, giving them 20TiMBL can run as server (https://github.com/ LanguageMachines/timblserver); TiMBL API is available, as well as TiMBL Python bindings (https://github. com/proycon/python-timbl). weighted vote in k-NN classification rather than their count in memory. prompt or training corpus presented for finetuning could be incrementally added to memory as they are presented as input, and be given higher weight that would prioritize their use in the autoregressive generation of new output. This way exemplar weights could function as kind of attention weights; lowering exemplar weights could emulate kind of forgetting. Second, preliminary experiments have shown small profitable effect of raising > 1 in TRIBL2 (note again that we have used = 1 throughout the experiments reported here), and using distanceweighting of neighbors (Daelemans et al., 2010), more distant neighbors getting lower vote in classification. Raining would in theory increase the size of the non-zero distribution of possible next tokens. Speeds of TRIBL2 will only be slightly affected with increased k, as the search for the top-k neighbors is an efficient process that is only moderately influenced by the actual value of k. Third, TiMBL offers several value-difference metrics, such as MVDM (Cost and Salzberg, 1993), that offer more nuanced types of similarity metrics than the Overlap function used here. Acting more or less as static embeddings, MVDM could represent meaningful (e.g. semantic) similarities between two tokens occurring in the same position that could help establishing closer similarity of more relevant nearest neighbors. Finally, we note that for broad uptake of memory-based language modeling, it would be good if it would conform to standards organically evolved by communities on platforms such as Hugging Face,21 or if these platforms become more eclectic in the LLM engines they cater for. This would also ease the integration of OLIFANT models in larger setups such as speculative decoding (Leviathan et al., 2023), and connecting the models to chat and instruct modules. Even though the prediction distributions of IGTree and TRIBL2 are sparse, beam search can be readily applied."
        },
        {
            "title": "Acknowledgments",
            "content": "The authors thank Ko van der Sloot for his sustained and essential contributions to the development of TiMBL. Walter Daelemans, Ton Weijters, and Jakub Zavrel all provided elemental contributions to the development of the TiMBL family of algorithms; thank you. We also thank Wessel 21https://huggingface.co/ Stoop, Herman Stehouwer, Menno van Zaanen, Tanja Gaustadvan Zaanen, and Monica Hajek for contributions to Valkuil.net, Fowlt.net, and WOPR, predecessor versions of MEMLM, and to Sander Canisius for the first versions of the Python-TiMBL bindings."
        },
        {
            "title": "References",
            "content": "D. W. Aha, D. Kibler, and M. Albert. 1991. Instancebased learning algorithms. Machine Learning, 6:37 66. Peter Berck. 2017. Memory-based Text Correction. Ph.D. thesis, Radboud University. Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. 2023. Quantifying memorization across neural language models. In The Eleventh International Conference on Learning Representations. Nicholas Carlini, Florian Tramèr, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Úlfar Erlingsson, Alina Oprea, and Colin Raffel. 2021. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21), pages 26332650. USENIX Association. S. Cost and S. Salzberg. 1993. weighted nearest neighbour algorithm for learning with symbolic features. Machine Learning, 10:5778. T. M. Cover and P. E. Hart. 1967. Nearest neighbor pattern classification. Institute of Electrical and Electronics Engineers Transactions on Information Theory, 13:2127. W. Daelemans and A. Van den Bosch. 1993. TabTalk: reusability in data-oriented grapheme-to-phoneme conversion. In Proceedings of Eurospeech 93, pages 14591466, Berlin. T.U. Berlin. W. Daelemans and A. Van den Bosch. 2005. Memorybased language processing. Cambridge University Press, Cambridge, UK. W. Daelemans, A. Van den Bosch, and A. Weijters. 1997. IGTree: using trees for compression and classification in lazy learning algorithms. Artificial Intelligence Review, 11:407423. W. Daelemans, J. Zavrel, K. Van der Sloot, and A. Van den Bosch. 2010. TiMBL: Tilburg memory based learner, version 6.3, reference guide. Technical Report ILK 10-01, ILK Research Group, Tilburg University. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, and 1 others. 2020. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027. Jamie Hayes, Marika Swanberg, Harsh Chaudhari, Itay Yona, Ilia Shumailov, Milad Nasr, Christopher A. Choquette-Choo, Katherine Lee, and A. Feder Cooper. 2025. Measuring memorization in language models via probabilistic extraction. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 92669291, Albuquerque, New Mexico. Association for Computational Linguistics. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. CoRR, abs/2001.08361. Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020. Generalization through memorization: Nearest neighbor language models. In ICLR. OpenReview.net. D. E. Knuth. 1973. The Art of Computer Programming, volume 3: Sorting and Searching. Addison-Wesley, Reading, MA. Yaniv Leviathan, Matan Kalman, and Yossi Matias. 2023. Fast inference from transformers via speculative decoding. In Proceedings of the 40th International Conference on Machine Learning, ICML23. JMLR.org. Alexandra Sasha Luccioni, Sylvain Viguier, and AnneLaure Ligozat. 2023. Estimating the carbon footprint of bloom, 176b parameter language model. J. Mach. Learn. Res., 24(1). J. R. Quinlan. 1986. Induction of decision trees. Machine Learning, 1:81206. J. R. Quinlan. 1993. C4.5: Programs for Machine Learning. Morgan Kaufmann, San Mateo, CA. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 17151725, Berlin, Germany. Association for Computational Linguistics. E. Fix and J. L. Hodges. 1951. Disciminatory analysis nonparametric discrimination; consistency properties. Technical Report Project 21-49-004, Report No. 4, USAF School of Aviation Medicine. W. Stoop and A. Van den Bosch. 2014a. Improving word prediction for augmentative communication by using idiolects and sociolects. Dutch Journal for Applied Linguistics, 3(2):136153. 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353355, Brussels, Belgium. Association for Computational Linguistics. W. Stoop and A. Van den Bosch. 2014b. Using idiolects and sociolects to improve word prediction. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 318327, Gothenburg, Sweden. Association for Computational Linguistics. Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and policy considerations for deep learning in NLP. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 36453650, Florence, Italy. Association for Computational Linguistics. A. Van den Bosch. 2006a. All-word prediction as the ultimate confusible disambiguation. In Proceedings of the HLT-NAACL Workshop on Computationally hard problems and joint inference in speech and language processing, New York, NY. A. Van den Bosch. 2006b. Scalable classification-based word prediction and confusible correction. Traitement Automatique des Langues, 46(2):3963. A. Van den Bosch and P. Berck. 2009. Memory-based machine translation and language modeling. The Prague Bulletin of Mathematical Linguistics, 91:17 26. A. Van den Bosch and P. Berck. 2012. Memory-based text correction for preposition and determiner errors. In Proceedings of the 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 289294, New Brunswick, NJ. ACL. A. Van den Bosch and P. Berck. 2013. Memory-based grammatical error correction. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 102108, Sofia, Bulgaria. Association for Computational Linguistics. A. Van den Bosch and K. Van der Sloot. 2007. Superlinear parallelisation of the k-nearest neighbor classifier. In Proceedings of the 18th BENELEARN Conference, Amsterdam, The Netherlands. Antal van den Bosch. 2006. All-word prediction as the ultimate confusible disambiguation. In Proceedings of the Workshop on Computationally Hard Problems and Joint Inference in Speech and Language Processing, pages 2532, New York City, New York. Association for Computational Linguistics. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS17, page 60006010, Red Hook, NY, USA. Curran Associates Inc. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the"
        }
    ],
    "affiliations": [
        "Lund University",
        "Royal Netherlands Academy of Arts and Sciences",
        "Utrecht University"
    ]
}