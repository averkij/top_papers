{
    "paper_title": "AIN: The Arabic INclusive Large Multimodal Model",
    "authors": [
        "Ahmed Heakl",
        "Sara Ghaboura",
        "Omkar Thawkar",
        "Fahad Shahbaz Khan",
        "Hisham Cholakkal",
        "Rao Muhammad Anwer",
        "Salman Khan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Amid the swift progress of large language models (LLMs) and their evolution into large multimodal models (LMMs), significant strides have been made in high-resource languages such as English and Chinese. While Arabic LLMs have seen notable progress, Arabic LMMs remain largely unexplored, often narrowly focusing on a few specific aspects of the language and visual understanding. To bridge this gap, we introduce AIN-the Arabic Inclusive Multimodal Model-designed to excel across diverse domains. AIN is an English-Arabic bilingual LMM designed to excel in English and Arabic, leveraging carefully constructed 3.6 million high-quality Arabic-English multimodal data samples. AIN demonstrates state-of-the-art Arabic performance, while also possessing strong English-language visual capabilities. On the recent CAMEL-Bench benchmark comprising 38 sub-domains including, multi-image understanding, complex visual perception, handwritten document understanding, video understanding, medical imaging, plant diseases, and remote sensing-based land use understanding, our AIN demonstrates strong performance with the 7B model outperforming GPT-4o by an absolute gain of 3.4% averaged over eight domains and 38 sub-domains. AIN's superior capabilities position it as a significant step toward empowering Arabic speakers with advanced multimodal generative AI tools across diverse applications."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 3 ] . [ 1 4 9 0 0 0 . 2 0 5 2 : r AIN: The Arabic INclusive Large Multimodal Model Ahmed Heakl1 Sara Ghaboura1* Omkar Thawakar1 Fahad Shahbaz Khan1,2 Hisham Cholakkal1 Rao Muhammad Anwer1,3 Salman Khan1,4 1Mohamed bin Zayed University of AI, 2Linköping University, 3Aalto University, 4Australian National University"
        },
        {
            "title": "AIN GitHub",
            "content": "Figure 1: Cross-domain performance analysis on the Camel-Bench Benchmark. Our AIN-7B achieves promising performance compared to significantly bigger models (GPT-4o and Gemini-1.5-Pro) in both domainspecific and aggregate settings. Despite its smaller size, our AIN-7B achieves competitive performance across all 38 sub-domains with significantly superior capabilities on OCR & document understanding."
        },
        {
            "title": "Abstract",
            "content": "Amid the swift progress of large language models (LLMs) and their evolution into large multimodal models (LMMs), significant strides have been made in high-resource languages such as English and Chinese. While Arabic LLMs have seen notable progress, Arabic LMMs remain largely unexplored, often narrowly focusing on few specific aspects of the language and visual understanding. To bridge this gap, we introduce AINthe Arabic Inclusive Multimodal Modeldesigned to excel across diverse domains. AIN is an English-Arabic bilingual LMM designed to excel in English and Arabic, leveraging carefully constructed 3.6 million high-quality Arabic-English multimodal data samples. AIN demonstrates state-of-the-art Arabic performance, while also possessing strong English-language visual capabilities. On the recent CAMEL-Bench benchmark comprising 38 sub-domains including, multi-image understanding, complex visual perception, handwritten document understanding, video understanding, medical imaging, plant diseases, and remote sensing-based land use understanding, our AIN demonstrates strong performance with the 7B model outperforming GPT-4o by an absolute gain of 3.4% averaged over eight domains and 38 sub-domains. AINs superior capabilities position it as significant step toward empowering Arabic speakers with advanced multimodal generative AI tools across diverse applications. Technical Report of AIN: The Arabic INclusive Large Multimodal Model. Figure 2: AIN: versatile LMM excelling in visual and contextual understanding across diverse domains, including VQA on complex topics, OCR for various fonts and handwriting, cultural insights (traditions, food, places), agricultural tasks (crop identification, fruit classification, disease detection), remote sensing (multi-scale objects), medical imaging (various modalities), and video analysis (animation, human activities)."
        },
        {
            "title": "1 AIN Capabilities",
            "content": "The AIN model is an advanced Arabic Large Multimodal Model (See Figure 2) with strong English proficiency, built on 7 billion parameters derived from the Qwen-2-VL-7B architecture [2]. Its performance highlights significant progress in multimodal understanding, excelling in complex reasoning, cross-lingual tasks, and detailed image-text alignment across diverse benchmarks."
        },
        {
            "title": "1.1 Quantitative Results",
            "content": "We present the quantitative performance of the model across benchmarks and domains. Figure 3 showcases comprehensive performance analysis of AIN-7B across CAMEL-Bench [1] domains, comparing it with variety of models including, GPT-4o [3], GPT-4o-mini [4], Gemini-1.5-Pro [5]), Qwen2-VL-7B [2], LlaVA-NeXt-7B [6], and Pangea-7B [7]). The plot demonstrates superior performance of AIN-7B in most domains. Table 1 shows that our AIN-7B achieves the best overall performance across all domains, compared to existing models. We conduct comprehensive evaluation of the bilingual capabilities of AIN in Arabic and English using well-established benchmarks. To further assess AINs proficiency in Arabic beyond CAMELBench, we evaluate its performance across all topics of ArabicMMLU [13]. As shown in Table 2, AIN outperforms in 14 out of 19 evaluated categories, achieving significant 3.43% overall improvement compared to Qwen2-VL-7B [2]. Similarly, as presented in Table 3, AIN demonstrates strong English language capabilities on ten benchmarks that cover general VQA, mathematics, science, and visual chart interpretation."
        },
        {
            "title": "1.2 Qualitative Results",
            "content": "The qualitative assessment of AINs contextual response generation reveals its proficiency across diverse domains, as illustrated in Figure 4. The model excels in general VQA tasks, demonstrating advanced capabilities in OCR and document analysis, including both machine-printed text and handwriting interpretation. Additionally, it achieves prominent performance in medical image interpretation, scientific visualization comprehension, and remote sensing analysis. The models capabilities extend to intricate data visualization interpretation, where it demonstrates skilled extrapolative understanding from charts and diagrams. Notably, AIN exhibits strong cultural-specific understanding through object recognition across diverse contexts, such as places, food items, and Equal Contribution 2 Figure 3: AIN compared to existing LMMs across CAMEL-Bench benchmark [1] domains: OCR: OCR & Document Understanding, Video: General Video & Multi-Image Understanding, RS: Remote Sensing Understanding, CDT:Chart, Diagram & Table Understanding, Agro.: Agricultural Image Understanding, Cultural: Cultural-Specific Understanding, Medical: Medical Image Understanding. Table 1: Performance comparison of AIN and different closedand open-source LMMs across CAMEL-Bench domains [1]. Best performance is highlighted in green; second-best performance is underlined. OCR*: OCR & Document Understanding, Video*: General Video & Multi-Image Understanding, RS*: Remote Sensing Understanding, CDT*:Chart, Diagram & Table Understanding, Agro.*: Agricultural Image Understanding, Cult.*: Cultural-Specific Understanding, Med.*: Medical Image Understanding."
        },
        {
            "title": "Models",
            "content": "GPT-4o [3] GPT-4o-mini [4] Gemini-1.5-Pro [8] Gemini-1.5-flash [8] InternVL-8B [9] InternVL2.5-1B [10] Qwen-VL-2B [2] Qwen2-VL-7B [2] LLaVa-NeXt-7B [6] LLaVa-OneVision [11] Pangea-7B [7] Maya-8B [12] AIN-7B (ours)"
        },
        {
            "title": "VQA",
            "content": "57.91 48.83 46.68 45.59 30.41 27. 41.02 48.76 26.33 42.90 40.09 39. 56."
        },
        {
            "title": "OCR",
            "content": "54.68 39.38 28.68 27.58 15.91 19. 22.93 42.73 19.12 31.35 17.75 26. 72."
        },
        {
            "title": "Video",
            "content": "RS 74.28 68.12 42.95 53.31 51. 38.20 38.90 61.97 44.90 29.41 49. 47.23 64.09 22.85 16.93 17.07 14. 5.36 3.39 12.56 21.30 8.33 10. 6.67 27.53 45."
        },
        {
            "title": "CDT",
            "content": "62.12 70.16 47.06 48.26 30.27 30. 27.83 54.67 27.56 40.86 38.87 34. 64.10 Agro. Cult. Med. 81.79 79.58 72. 76.07 44.47 39.53 52.02 79.32 42. 75.03 74.51 70.61 85.05 79.92 65. 56.24 46.54 20.88 35.68 34.28 75. 28.30 66.02 20.34 57.42 78.09 49. 47.37 33.78 42.87 29.48 21.27 29. 35.81 22.54 27.29 31.99 31.57 43."
        },
        {
            "title": "Total",
            "content": "60.35 54.54 52.38 44.40 28.52 26. 32.33 52.56 27.39 40.45 34.90 41. 63.77 Table 2: ArabicMMLU all categories performance comparison across various models. Best performance is highlighted in green. Comp. Sci*: Computer Science, Gen. Knldge*: General Knowledge, Islamic Std.*: Islamic Studies, Managmt: Management, Nat. Sci*: Natural Science, Political Sci.*: Political Science, Social Sci.*: Social Science. Model Qwen2-VL-7B [2] AIN-7B (ours) Accounting 52.7 59.46 Arabic Lang. 51.34 55.41 Biology 44 43. Civics 48.92 47.37 Comp. Sci.* 65.87 69 Model Qwen2-VL-7B [2] AIN-7B (ours) Driving Test 67.88 69.69 Economics 59.59 59.25 Gen. Knldge* 55.18 57. Geography 49.67 55.16 Model Qwen2-VL-7B [2] AIN-7B (ours) Islamic Std.* 55.93 58.64 Model Qwen2-VL-7B [2] AIN-7B (ours) Philosophy 53.85 56.41 Law 58.92 71. Physics 39.61 45.1 Managmt* 68 64 Math 64.79 66.99 Political Sci.* 54.29 60 Social Sci.* 65.75 65.43 Total 56.36 59. History 43.1 45.16 Nat. Sci.* 71.45 78.37 Table 3: Comprehensive performance comparison of AIN-7B against Qwen2-VL-7B across 10 English benchmarks, with relative performance gain () indicated. Best performance is highlighted in green. Our AIN-7B achieves promising performance on English language across these diverse benchmarks. Benchmarks: MMBench [14], MME [15], MMMU [16], POPE[17], SEED[18], MathVista [19], ScienceQA [20], ChartQA [21], AI2D [22], MMT-Bench [23]."
        },
        {
            "title": "MMBench",
            "content": "Qwen2-VL-7B [2] AIN-7B (ours) 81.78 93.76 Improved by 11."
        },
        {
            "title": "MME",
            "content": "1,675.90 1,689.02 13."
        },
        {
            "title": "MMMU",
            "content": "52 54.2 2."
        },
        {
            "title": "ChartQA",
            "content": "Qwen2-VL-7B [2] AIN-7B (ours) Improved by 61.19 63.9 2. 85.87 91.82 5.95 83.16 85.12 1."
        },
        {
            "title": "POPE",
            "content": "86.13 87.59 1.46 AI2D 82.77 83. 0."
        },
        {
            "title": "SEED",
            "content": "76.42 78.35 1.93 MMT-Bench 63.29 63. 0.35 4 celebratory scenes. Throughout all tasks, AIN consistently produces accurate, contextually relevant, and comprehensive responses, underscoring its versatility in handling complex visual-linguistic tasks. Figure 4: Qualitative results demonstrating AINs comprehensive capabilities across diverse domains. The results show its proficiency in handling both multiple-choice and open-ended questions. Our proposed AIN exhibits robust performance in addressing queries related to visual attributes (shape, color, quantity), while maintaining appropriate response formats (single character, word, or complete sentence) according to task requirements. For qualitative comparison, Figure 5 highlights AINs performance relative to open-source and closed-source LMMs (GPT-4o [3] and LLaVA [11], respectively) across various domains. Unlike its counterparts, which frequently provided incorrect, incomplete answers or failed to adhere to the required format, AIN consistently delivers accurate and contextually appropriate responses. The model demonstrates proficiency in handling diverse query formats, effectively addressing both multiple-choice and open-ended questions with precision and reliability. Figure 5: Comparison of AIN with GPT-4o [3] and LLaVA [11] across diverse tasks. The evaluation demonstrates AINs proficiency in handling both multiple-choice and open-ended questions while maintaining appropriate response formats."
        },
        {
            "title": "1.3 Human Feedback",
            "content": "To further evaluate our AIN model, we conduct qualitative assessment through human feedback, comparing it against closedand open-source LMMs in blind setup, where model identities are not revealed to participants. The survey, as shown in Figure 6, covers various real-world domains, including medical diagnosis, road signs, and other scenarios such as low-resolution settings. Targeted at Arabic native speakers, it consists of 10 questions evaluating range of topics, each with corresponding ground-truth answers. Participants are tasked with selecting the response from the three models that they believe are closest to the ground-truth. In the survey, Model 1 represented AIN-7B (ours), Model 2 corresponds to GPT-4o [3], and Model 3 is LLaVA [11]. Delivered in MSA, the survey also includes an additional question to gather feedback on language clarity, MSA/ dialect preferences, and the survey itself. Survey Participation. More than 200 participants from 17 Arab countries (Figure 7), selected from diverse sectors and educational background, completed the survey. The highest contributions came from Saudi Arabia (30%), followed by Egypt (25%), the UAE (13.3%), and Lebanon (13.3%). Model Preferences. Figure 8 shows that participants predominantly favor Model 1 (AIN-7B (ours)), which received 76% of the votes. GPT-4v followed with 15%, and LLaVA garnered 9%, underscoring AINs significant preference among respondents (Figure 8). 6 Figure 6: AIN human evaluation survey, illustrating assessment criteria and multi-domain questions designed to evaluate multi-task and complex reasoning capabilities. The survey includes evaluations on specific food items, road signs in low-resolution settings, celebrities, charts, remote sensing tasks, and other diverse topics to comprehensively assess performance across multiple domains and challenges. 7 Figure 7: Nationality of AIN Survey Participants: Participants represent 17 Arab nations, with the highest contributions from Saudi Arabia (30%), followed by Egypt (25%), the UAE (13.3%), and Lebanon (13.3%). Figure 8: User Model Preferences. Participant preferences for the three models in the survey, with Model 1 (AIN (ours)) receiving 76% of the votes, GPT-4v 15%, and LLaVA 9%, demonstrating AINs strong performance. Survey Results and Analysis. The survey results reveal that AIN outperforms human participants in several questions, demonstrating notable advantages in accuracy and adherence to response formats. For instance, in Q1, although both Model 3 and human participants provided correct answers, they failed to comply with the required response format, criterion that AIN successfully fulfilled. Similarly, in Q9, 24.9% of participants, and in Q6, 20.5%, were unable to respond appropriately, whereas AIN provides accurate responses. In Q2, AIN exhibits superior precision by correctly identifying the shape as disc rather than circle, outperforming 18.6% of human respondents. Furthermore, in Q3 and Q5, participants struggle to recognize small details in the images, highlighting AINs ability to detect subtle features. Notably, in Q10, AIN demonstrates its capacity to solve complex reasoning tasks by extracting value beyond the visible content of the image. This highlights its ability to handle abstract problem-solving, further showcasing its comparative advantage over other models. Detailed results and comparative analyses are illustrated in Figures 10, 11, and 12. Dialect Preferences. Regarding language suitability, depicted in Figure 9, 74.3% of participants found MSA appropriate for the survey. An additional 11% were comfortable with MSA but expressed preference for their local dialect. Only 4.3% strongly preferred their local dialect over MSA, while 10.5% reported challenges unrelated to language. Figure 9: User Preferences for MSA and Local Dialects: The majority (74.3%) preferred MSA for reading and writing. An additional 11% are comfortable with MSA but favored their local dialects, while 4.3% found MSA challenging and prefer using their dialect. further 10.5% reported difficulties unrelated to linguistic aspects."
        },
        {
            "title": "2 Data Inspection and Selection",
            "content": "Our data collection comprises publicly available MSA Arabic and English datasets, with portion specifically curated for model training. Notably, 30% of the Arabic data is authentic. To achieve scalability and address specific domain requirements, translation was utilized to complement the existing data."
        },
        {
            "title": "2.1 Data Translation\nIn pursuit of optimal data translation, we select three models from the GPT-4v suite—specifically,\nGPT-4 [24], GPT-4o [3], and GPT-4o-mini [4]. A comprehensive experiment is conducted to\nevaluate the performance of these models across key criteria, including translation correctness,\ntranslation accuracy, and translation efficiency. For this evaluation, a variety of random samples of\noriginal English content are selected, and an English prompt was meticulously curated. To ensure\naccuracy and cultural adequacy, the corresponding Arabic prompts are crafted by a native Arabic\nspeaker. Additionally, the same samples are manually translated by native Arabic speakers with high\nproficiency, serving as reference translations for benchmarking.",
            "content": "We evaluate the translation performance of GPT-4, GPT-4o, and GPT-4o-mini using both Arabic and English prompts under identical settings. Translation accuracy is assessed by native speakers who rated the outputs against manually translated references on scale from 0 (fail) to 1 (accurate). The results indicate that GPT-4o and GPT-4omini perform closely in speed and accuracy across both prompt types, with the Arabic prompt outperforming in both metrics. Notably, GPT-4o-mini achieves the highest accuracy with the Arabic prompt. Contributors generally found that GPT-4o-mini successfully translates all terms, including brand names such as Boeing, whereas GPT-4o frequently fails to provide complete or accurate translations for these sentences. Based on these findings, GPT-4o-mini is selected for data translation. All reading and results are recorded in Table 4 and Table 5 for Arabic and English prompts, respectively."
        },
        {
            "title": "2.2 Data Quality Verification and Filtering\nHigh-performance models inherently require high-quality data [25]. Therefore, in addition to selecting\nappropriate data, we have implemented a multi-step data translation verification procedure alongside\nrigorous toxicity-free filtering (Figure 13).",
            "content": "Data Semantic Translation Verification. To identify the optimal model for translation verification, we design set of sentences reflecting common linguistic challenges in Arabic, including punctuation alignment with English, direct and semantic translation accuracy, masculine/feminine tone differentiation, and handling of diacritics. This evaluation involves 21 sentence pairs, categorized as simple 9 (cid:16)(cid:233)(cid:16)(cid:175)(cid:80)(cid:241)(cid:203)(cid:64) (cid:16)(cid:233)(cid:170)(cid:16)(cid:174)(cid:74)(cid:46)(cid:203)(cid:65)(cid:75)(cid:46) (cid:9)(cid:224)(cid:241)(cid:203) (cid:241)(cid:235) (cid:65)(cid:211) (cid:16)(cid:233)(cid:75)(cid:46) (cid:65)(cid:146)(cid:214)(cid:207)(cid:64) (a) Q1: (cid:63) (cid:16)(cid:233)(cid:75)(cid:10)(cid:81)(cid:30)(cid:10)(cid:16)(cid:74)(cid:186)(cid:74)(cid:46)(cid:203)(cid:64) Domain: Agricultural Image Understanding / Plant diseases. Purpose: Ability to detect diseased plant areas and identify their color. (cid:9)(cid:175) (cid:88)(cid:241)(cid:107)(cid:46) (cid:241)(cid:214)(cid:207)(cid:64) (cid:208)(cid:65)(cid:170)(cid:162)(cid:203)(cid:64) (cid:201)(cid:190) (cid:17)(cid:131) (cid:241)(cid:235) (cid:65)(cid:211) (b) Q2:(cid:63) (cid:16)(cid:232)(cid:80)(cid:241)(cid:146)(cid:203)(cid:64) (cid:250)(cid:10) Domain: Cultural-Specific Image Understanding / Food. Purpose: Ability to recognize food and precisely determine its shape.). (cid:9)(cid:175) (cid:16)(cid:232)(cid:88)(cid:241)(cid:107)(cid:46) (cid:241)(cid:214)(cid:207)(cid:64) (cid:16)(cid:72)(cid:65)(cid:170)(cid:163)(cid:65)(cid:16)(cid:174)(cid:16)(cid:74)(cid:203)(cid:64) (cid:88)(cid:89)(cid:171) (cid:213)(cid:187) (c) Q3:(cid:63) (cid:16)(cid:232)(cid:80)(cid:241)(cid:146)(cid:203)(cid:64) (cid:250)(cid:10) Domain: Remote Sensing Image Understanding / Roads & Constructions. Purpose: Ability to identify specific constructions among similar ones. (cid:13) (cid:64) (cid:90)(cid:241) (cid:9)(cid:146)(cid:203)(cid:64) (cid:201)(cid:235) (cid:44)(cid:89)(cid:103)(cid:64)(cid:240) (cid:213)(cid:16)(cid:175)(cid:80) (cid:240) (d) Q4: (cid:63)(cid:81)(cid:229)(cid:9)(cid:148) (cid:9)(cid:107) Domain: General VQA/ Binary Question. Purpose: Ability to identify tiny details in ambiguous scenes and answer binary questions. (cid:16)(cid:232)(cid:81)(cid:229)(cid:17)(cid:133)(cid:65)(cid:74)(cid:46)(cid:211) (cid:16)(cid:233)(cid:75)(cid:46) (cid:65)(cid:103)(cid:46) (cid:66)(cid:13) (cid:64) (cid:250)(cid:107)(cid:46) (cid:81)(cid:75)(cid:10) (cid:16)(cid:233)(cid:210)(cid:202)(cid:190)(cid:75)(cid:46) (cid:13) (cid:64) Figure 10: Survey Feedback - Part 1: Questions 1 to 4 explored diverse domains, including agriculture, cultural-specific topics (e.g., food), remote sensing, and general VQA. Tasks included agro-disease detection, food recognition, shape identification, specific construction detection, and recognizing tiny details in ambiguous scenes, using various question formats such as MCQ, binary, and open-ended short answers. 10 (cid:9)(cid:172)(cid:65)(cid:162)(cid:170)(cid:9)(cid:75)(cid:66)(cid:64) (cid:168)(cid:241)(cid:9)(cid:74)(cid:220)(cid:216) (a) Q5: (cid:63) (cid:16)(cid:232)(cid:88)(cid:241)(cid:107)(cid:46) (cid:241)(cid:214)(cid:207)(cid:64) (cid:80)(cid:65)(cid:130)(cid:28)(cid:10)(cid:203)(cid:64) (cid:250)(cid:205)(cid:64)(cid:13) Domain: General VQA / Traffic Signs. Purpose: Ability to spot traffic signs at distance and in low resolution. (cid:16)(cid:72)(cid:65)(cid:16)(cid:74)(cid:9)(cid:175)(cid:66) (cid:88)(cid:89)(cid:171) (cid:213)(cid:187) (cid:9)(cid:175) (cid:72)(cid:46) (cid:241)(cid:16)(cid:74)(cid:186)(cid:214)(cid:207)(cid:64) (cid:145)(cid:9)(cid:74)(cid:203)(cid:64) (cid:241)(cid:235) (cid:65)(cid:211) (b) Q6:(cid:63) (cid:16)(cid:232)(cid:80)(cid:241)(cid:146)(cid:203)(cid:64) (cid:250)(cid:10) Domain: OCR & Document Understanding. Purpose: Ability to discern Arabic characters and extract text from images. (cid:9)(cid:74)(cid:170)(cid:203)(cid:64) (cid:9)(cid:175) (cid:16)(cid:232)(cid:88)(cid:241)(cid:107)(cid:46) (cid:241)(cid:214)(cid:207)(cid:64) (cid:73)(cid:46) (cid:16)(cid:134)(cid:80)(cid:240) (cid:169)(cid:162)(cid:16)(cid:175) (cid:88)(cid:89)(cid:171) (cid:213)(cid:187) (c) Q7:(cid:63) (cid:16)(cid:232)(cid:80)(cid:241)(cid:146)(cid:203)(cid:64) (cid:250)(cid:10) Domain: General VQA / Short Answer Question. Purpose: Ability to pinpoint the required item among several items + provide short answer as required. (cid:13) (cid:64) (cid:232) (cid:9)(cid:89)(cid:235) (cid:250)(cid:10) (cid:16)(cid:233)(cid:203)(cid:65)(cid:109)(cid:204)(cid:39)(cid:64) (cid:241)(cid:235) (cid:65)(cid:211) (cid:9)(cid:175) (cid:16)(cid:233)(cid:203)(cid:65)(cid:109)(cid:204)(cid:39)(cid:64) (cid:248)(cid:241)(cid:16)(cid:74)(cid:130)(cid:211) (cid:240) (d) Q8: (cid:63) (cid:16)(cid:232)(cid:80)(cid:241)(cid:146)(cid:203)(cid:64) Domain: Medical Image Understanding / Diseases Diagnoses. Purpose: Ability to diagnose organ health by reasoning its condition (normal or abnormal) for specific disease. Figure 11: Survey Feedback - Part 2: Questions 5 to 8 focus on domains such as traffic sign recognition, OCR and document understanding, general VQA, and medical imaging. Tasks include identifying traffic signs, extracting correct text from images, pinpointing specific items among several options, and diagnosing organ conditions, using various formats such as MCQ and short answers. (a) Q9: (cid:63)(cid:128)(cid:241)(cid:9)(cid:75)(cid:65)(cid:211)(cid:240)(cid:80) (cid:88)(cid:80)(cid:65) (cid:17)(cid:130)(cid:16)(cid:28)(cid:75)(cid:10)(cid:80) (cid:250)(cid:171)(cid:89)(cid:75)(cid:10) (cid:161)(cid:74)(cid:10)(cid:106)(cid:214)(cid:207)(cid:64) (cid:81)(cid:212)(cid:103)(cid:13) Domain: General VQA / Grounding and Celebrities. Purpose: Ability to determine persons identity in specific location. (cid:66)(cid:64) (cid:169)(cid:75)(cid:46) (cid:81)(cid:214)(cid:207)(cid:64) (cid:201) (cid:9)(cid:103)(cid:64)(cid:89)(cid:75)(cid:46) (cid:9)(cid:224)(cid:65)(cid:9)(cid:74) (cid:9)(cid:174)(cid:203)(cid:64) (cid:201)(cid:235) (cid:13) (cid:64) (cid:250)(cid:10) (cid:16)(cid:174)(cid:75)(cid:10)(cid:81)(cid:9)(cid:175) (cid:16)(cid:233)(cid:75)(cid:10)(cid:241)(cid:13)(cid:74)(cid:214)(cid:207)(cid:64) (cid:9)(cid:175) (cid:241)(cid:210)(cid:9)(cid:74)(cid:202)(cid:203) (cid:16)(cid:233)(cid:74)(cid:46)(cid:130)(cid:9)(cid:28)(cid:203)(cid:64) (cid:249)(cid:10) (cid:235) (cid:65)(cid:211) (b) Q10: (cid:63)(cid:65)(cid:74)(cid:10) Domain: Chart, Diagram & Table Understanding / Bar Charts. Purpose: Ability to extract values from charts, even when not explicitly shown. Figure 12: Survey Feedback - Part 3: Questions 9 and 10 focus on domains such as celebrities, grounding, and charts and diagrams. Tasks include identifying celebrity in specific location within the image based on the question, and extrapolating values from charts where the information is not explicitly written, using formats such as MCQ and short answers. Table 4: Comparison of translation performance for Arabic prompts across three GPT-4v models, evaluated on variety of samples. Avg time/iteration*: average time per sample."
        },
        {
            "title": "Model",
            "content": "GPT-4o [3] GPT-4 [24] GPT-4o-mini [4]"
        },
        {
            "title": "Time",
            "content": "1 min, 43 sec 6 min, 39 sec 1 min, 28 sec Avg time/ iteration*"
        },
        {
            "title": "Accuracy",
            "content": "04.16 sec 15.99 sec 03.52 sec 90% 85% 92% Table 5: Comparison of translation performance for English prompts across three GPT-4v models, evaluated on variety of samples. Avg time/iteration*: average time per sample."
        },
        {
            "title": "Model",
            "content": "GPT-4o [3] GPT-4 [24] GPT-4o-mini [4]"
        },
        {
            "title": "Time",
            "content": "2 min, 01 sec 8min, 21 sec 1 min, 52 sec Avg time/ iteration *"
        },
        {
            "title": "Accuracy",
            "content": "04.87 sec 20.08 sec 4.48 sec 88% 50% 87% 12 sentences  (Table 6)  , complex sentences with tone ambiguity  (Table 7)  , and affirmative clauses with questions  (Table 8)  . The sentence pairs are processed using five multilingual modelsM-BERT [26], Paraphrase-XLM-R [27], all-mpnet-base-v2 [28], LaBSE [29], and AraBERT [30]to evaluate semantic similarity between English and Arabic translations. Cosine similarity is used as the scoring metric to quantify the alignment between the translations, providing robust basis for selecting the most suitable model. Table 6: Translation quality check - Sentence 1: simple English sentence with different settings including accurate direct translation, semantic translation, mismatched translation, punctuation, and diacritics. Ref. Original 1. 1.2 1.3 1.4 1."
        },
        {
            "title": "This is an example sentence",
            "content": "This is an example sentence!"
        },
        {
            "title": "Translation",
            "content": "(cid:232) (cid:9)(cid:89)(cid:235) (cid:200)(cid:65)(cid:17)(cid:74)(cid:211) (cid:16)(cid:233)(cid:202)(cid:212)(cid:103)(cid:46) (cid:11)(cid:9)(cid:175) (cid:9)(cid:146) (cid:9)(cid:225)(cid:211)(cid:11) (cid:129)(cid:202)(cid:11)(cid:103)(cid:46) (cid:64)(cid:11) (cid:189)(cid:11) (cid:202)(cid:11) (cid:16)(cid:232)(cid:80)(cid:65)(cid:74)(cid:46)(cid:171) (cid:232) (cid:9)(cid:89)(cid:235) (cid:16)(cid:233)(cid:74)(cid:10)(cid:106)(cid:28)(cid:10) (cid:9)(cid:147)(cid:241)(cid:16)(cid:75) (cid:11) (cid:11)(cid:17)(cid:74)(cid:211)(cid:11) (cid:20) (cid:20)(cid:16)(cid:233) (cid:12)(cid:103)(cid:46) (cid:9)(cid:89)(cid:11) (cid:11)(cid:235) (cid:202)(cid:21)(cid:212) (cid:200)(cid:65) (cid:232)(cid:11) (cid:16)(cid:232)(cid:80)(cid:65)(cid:74)(cid:46)(cid:171) (cid:232) (cid:9)(cid:89)(cid:235) (cid:33) (cid:16)(cid:233)(cid:74)(cid:10)(cid:106)(cid:28)(cid:10) (cid:9)(cid:147)(cid:241)(cid:16)(cid:75)"
        },
        {
            "title": "Accurate direct translation",
            "content": "Completely mismatched translation. Translation with semantic meaning. Accurate direct translation + diacritics. Translation with semantic meaning + punctuation, no diacritics Table 7: Translation quality check - Sentence 2: The English sentence consists of polite request with different settings including accurate direct translation, semantic translation, masculine/ Feminine tone, mismatched translation, punctuation, and diacritics. Ref. Original 2.1 2.2 2.3 2.4 2.5 2. 2.7 2.8 2.9 Please, sit down"
        },
        {
            "title": "Please sit down",
            "content": "Please, sit down Please, sit down Please, sit down Please, sit down 2.10 Please, sit down 2.11 2.12 2.13 Please, sit down. Please, sit down. Please, sit down. (cid:16)(cid:233)(cid:74)(cid:10)(cid:106)(cid:28)(cid:10) (cid:9)(cid:147)(cid:241)(cid:16)(cid:75)"
        },
        {
            "title": "Accurate direct translation",
            "content": "No punctuation, no diacritics. No punctuation, with diacritics Completely mismatched translation. No punctuation, with diacritics, feminine tone No punctuation, with diacritics, masculine tone (cid:129)(cid:202)(cid:103)(cid:46) (cid:64) (cid:44) (cid:201) (cid:9)(cid:146) (cid:9)(cid:174)(cid:16)(cid:75) (cid:129)(cid:202)(cid:103)(cid:46) (cid:64) (cid:201) (cid:9)(cid:146) (cid:9)(cid:174)(cid:16)(cid:75) (cid:11)(cid:9)(cid:175) (cid:9)(cid:146) (cid:9)(cid:225)(cid:211)(cid:11) (cid:129)(cid:202)(cid:11)(cid:103)(cid:46) (cid:64)(cid:11) (cid:189)(cid:11) (cid:202)(cid:11) (cid:11)(cid:9)(cid:174) (cid:21) (cid:11)(cid:16)(cid:75) (cid:11)(cid:9)(cid:146) (cid:250)(cid:10)(cid:230)(cid:132)(cid:202)(cid:11)(cid:103)(cid:46) (cid:64)(cid:11) (cid:250)(cid:10) (cid:206) (cid:21) (cid:11)(cid:9)(cid:174) (cid:11)(cid:16)(cid:75) (cid:11)(cid:9)(cid:146) (cid:201) (cid:129)(cid:202)(cid:11)(cid:103)(cid:46) (cid:64)(cid:11) (cid:16)(cid:232)(cid:80)(cid:65)(cid:74)(cid:46)(cid:171) (cid:232) (cid:9)(cid:89)(cid:235) (cid:11)(cid:9)(cid:175) (cid:9)(cid:146) (cid:9)(cid:225)(cid:211)(cid:11) (cid:129)(cid:202)(cid:11)(cid:103)(cid:46) (cid:64)(cid:11) (cid:189)(cid:11) (cid:202)(cid:11) (cid:21) (cid:11)(cid:9)(cid:174) (cid:11)(cid:16)(cid:75) (cid:11)(cid:9)(cid:146) (cid:129)(cid:202)(cid:11)(cid:103)(cid:46) (cid:64)(cid:11) (cid:201) (cid:21) (cid:11)(cid:9)(cid:174) (cid:11)(cid:16)(cid:75) (cid:11)(cid:9)(cid:146) (cid:250)(cid:10)(cid:230)(cid:132)(cid:202)(cid:11)(cid:103)(cid:46) (cid:64)(cid:11) (cid:201) (cid:11)(cid:9)(cid:174) (cid:21) (cid:11)(cid:9)(cid:146) (cid:11)(cid:16)(cid:75) (cid:250)(cid:10)(cid:230)(cid:132)(cid:202)(cid:11)(cid:103)(cid:46) (cid:64)(cid:11) (cid:250)(cid:10) (cid:206) (cid:11)(cid:9)(cid:175) (cid:9)(cid:146) (cid:9)(cid:225)(cid:211)(cid:11) (cid:46)(cid:129)(cid:202)(cid:11)(cid:103)(cid:46) (cid:64)(cid:11) (cid:44) (cid:189)(cid:11) (cid:202)(cid:11) (cid:46)(cid:129)(cid:202)(cid:103)(cid:46) (cid:64) (cid:44) (cid:189)(cid:202) (cid:9)(cid:146)(cid:9)(cid:175) (cid:9)(cid:225)(cid:211) (cid:46)(cid:129)(cid:202)(cid:103)(cid:46) (cid:64)(cid:13) (cid:44) (cid:189)(cid:202) (cid:9)(cid:146)(cid:9)(cid:175) (cid:9)(cid:225)(cid:211) With punctuation and only hamzat al kaser (cid:40)(cid:64)(cid:13)(cid:41) Punctuation + diacritics/ masculine tone. Translation with semantic meaning. Accurate direct translation + diacritics/ partial feminine tone. Accurate direct translation + diacritics/ feminine tone. Accurate direct translation + punctuation, no diacritics. Accurate direct translation + punctuation + diacritics. Figure 14a presents heatmap of the similarity scores of the models in evaluating translations semantic correctness, considering punctuation, tone, and diacritics. While high similarity scores indicate good performance, robust model must also assign low scores to poor or irrelevant translations. To assess this, second experiment tested model behavior on mismatched translations ( Figure 14b). Due to the close performance observed between LaBSE [29] and Paraphrase-XLM-R [27] in initial evaluations, an additional experiment is conducted to further assess their capabilities. This experiment utilized 50 samples of high-quality translations and 50 samples of moderate-to-poor translations. LaBSE demonstrated superior consistency, providing higher similarity scores for accurate translations and relatively lower scores for poor translations compared to Paraphrase-XLM-R. This reliability in 13 Table 8: Translation quality check - Sentence 3: The English sentence consists of an affirmative clause followed by question of accurate direct translation in different settings including semantic translation and punctuation. Ref. Original 3. 3.2 3.3 It is raining today should we stay at home It is raining today. Should we stay at home It is raining today. Should we stay at home? (cid:200) (cid:9)(cid:81)(cid:9)(cid:30)(cid:214)(cid:207)(cid:64) (cid:250)(cid:10) (cid:63)(cid:200) (cid:9)(cid:81)(cid:9)(cid:30)(cid:214)(cid:207)(cid:64) (cid:250)(cid:10) (cid:63)(cid:200) (cid:9)(cid:81)(cid:9)(cid:30)(cid:214)(cid:207)(cid:64) (cid:250)(cid:10) (cid:9)(cid:175) (cid:90)(cid:65)(cid:16)(cid:174)(cid:74)(cid:46)(cid:203)(cid:64) (cid:9)(cid:175) (cid:90)(cid:65)(cid:16)(cid:174)(cid:74)(cid:46)(cid:203)(cid:64) (cid:9)(cid:175) (cid:90)(cid:65)(cid:16)(cid:174)(cid:74)(cid:46)(cid:203)(cid:64) Translation Translation Criteria (cid:65)(cid:9)(cid:74)(cid:74)(cid:10)(cid:202)(cid:171) (cid:73)(cid:46) (cid:65)(cid:9)(cid:74)(cid:74)(cid:10)(cid:202)(cid:171) (cid:73)(cid:46) (cid:65)(cid:9)(cid:74)(cid:74)(cid:10)(cid:202)(cid:171) (cid:73)(cid:46) No punctuation. (cid:109)(cid:46)(cid:26)(cid:39)(cid:10) (cid:201)(cid:235) (cid:208)(cid:241)(cid:74)(cid:10)(cid:203)(cid:64) (cid:81)(cid:162)(cid:214)(cid:16)(cid:223) (cid:65)(cid:238)(cid:9)(cid:69)(cid:64)(cid:13) (cid:109)(cid:46)(cid:26)(cid:39)(cid:10) (cid:201)(cid:235) (cid:81)(cid:162)(cid:220)(cid:216) (cid:208)(cid:241)(cid:75)(cid:10) (cid:233)(cid:9)(cid:75)(cid:64)(cid:13) (cid:109)(cid:46)(cid:26)(cid:39)(cid:10) (cid:201)(cid:235) (cid:46)(cid:208)(cid:241)(cid:74)(cid:10)(cid:203)(cid:64) (cid:81)(cid:162)(cid:214)(cid:16)(cid:223) (cid:65)(cid:238)(cid:9)(cid:69)(cid:64)(cid:13) With punctuation. Semantic meaning + punctuation. distinguishing translation quality (Figures 15a and 15b) led to the selection of LaBSE for the full dataset. Translations scoring below 80% similarity were excluded, accounting for less than 2% of the data. Figure 13: Data verification and filtering pipeline for textual and visual data. Textual data underwent semantic similarity checks using LaBSE [29] (80% threshold) and quality evaluation using BLEU [31] (60% threshold), METEOR [32] (80% threshold), and ROUGE [33] (80% threshold). Visual data was screened for toxicity using LLavaGuard [34] policies with GPT-4o [3], discarding unsafe images to ensure quality and safety. Data Quality Verification. To ensure comprehensive quality verification process, particularly given the use of Gen-AI for translation, additional checks are necessary. These include assessing the quality of machine-generated text, its correlation with the original content, and the degree of overlap between the generated and original text. To achieve this, we employ three specialized evaluation metrics: BLEU (2-gram and 4-gram) [31] for text quality, METEOR [32] for translation correlation, and ROUGE (unigram and ROUGE-L) [33] for overlap measurement. Our data quality verification experiment involves analyzing randomly selected 50 translated samples that are translated back to English using GPT-4o-mini [4] for comparison against the original English text (reference data). The evaluation metrics demonstrated strong performance across multiple dimensions: BLEU scores [31] of 71.11% (2-gram) and 60.20% (4-gram) indicated high local coherence and good fluency in the translated text. The METEOR score [32] of 86.10% suggested high-quality translation with effective handling of both exact matches and linguistic variations. ROUGE metrics [33] were particularly strong, with unigram scores showing 87.80% precision and 87.30% recall, indicating excellent 14 (a) Similarity scores for different settings for correct translation. The higher the better. (b) Similarity scores for different settings for incorrect translation. The lower the worse. Figure 14: Similarity scores for diverse settings, including direct correct translation, incorrect translation, semantic translation, diacritics, and punctuation. word-level accuracy and comprehensive content capture. Similarly, ROUGE-L scores [33] (precision: 86.20%, recall: 85.90%, F1: 85.80%) confirmed strong structural similarity between the translated and reference texts, demonstrating that the essential meaning and structure are well-preserved throughout the translation process. With the data translation quality checks completed, final step of visual toxicity inspection is required to ensure the data is ready for model training. Table 9: Data Quality Verification and Evaluation Metrics"
        },
        {
            "title": "Metric",
            "content": "BLEU (2-gram) [31] BLEU (4-gram) [31] METEOR [32] ROUGE (unigram) [33] ROUGE-L [33]"
        },
        {
            "title": "Scores",
            "content": "71.11% 60.20% 86.10%"
        },
        {
            "title": "Recall",
            "content": "F1-score 87.80% 86.20% 87.30% 85.90% 87.30% 85.80% Toxicity Filtering: To ensure model safety in vision, toxicity inspection is critical component of our evaluation process. We utilize LLavaGuards safety taxonomy [34], well-curated prompt specifically designed to verify visual data against predefined safety criteria, in combination with 15 (a) Comparison of LaBSE and Paraphrase-XLM-R: Evaluating 50 high-quality translated samples. (b) Comparison of LaBSE and Paraphrase-XLM-R: Assessing 50 low-quality translated samples. Figure 15: Comparison of LaBSE and Paraphrase-XLM-R to identify the optimal model. Figure 16: Visual Data Toxicity Filtering. Using GPT-4o [3] and LLavaGuard [34] policies, about 96% of the data is classified as safe, while the remainder was deemed unsafe. The unsafe data was distributed across four categories: Weapon, or Substance Abuse (3.25%), Hate, Humiliation, Harassment (0.55%), Animal Cruelty (1.09%), and Violence, Harm, or Cruelty (0.55%). 16 GPT-4o for the inspection process. The dataset undergoes comprehensive assessment to ensure compliance with safety policies. The evaluation covers key categories, including Hate, Humiliation, or Harassment; Violence, Harm, or Cruelty; Sexual Content; Nudity; Weapons or Substance Abuse; Self-Harm; Animal Cruelty; and Disasters or Emergencies. The results reveal that 95.63% of the data is deemed safe, while 4.37% is classified as unsafe. The unsafe data is distributed across four main categories: Weapons or Substance Abuse, Hate, Humiliation, or Harassment, Animal Cruelty, and Violence, Harm, or Cruelty, as illustrated in Figure 16. This rigorous evaluation ensured that our data met safety standards, further preparing it for downstream applications. Following the completion of the data verification steps and toxicity filtering, our dataset, comprising 3.6M safe and curated entries, is prepared for model training."
        },
        {
            "title": "3 Experiments",
            "content": "The AIN model is trained on 8 GPU nodes, each equipped with 8 NVIDIA A100 GPU cards, each with 80 GB memory. The GPUs within each node are interconnected using 8 NVLink links, ensuring high bandwidth and low latency. To facilitate efficient cross-node communication, each node is equipped with dual-port 200 Gb/sec (4HDR) InfiniBand connections, achieving an aggregate interconnect bandwidth of 800 Gbps. This robust infrastructure was crucial in handling the computational and memory-intensive large-scale LMM training. Our approach leverages the Qwen2-VL-7B [2] model as the base, which we fine-tuned on our English-Arabic bilingual dataset. The dataset comprises 3.6 million high-quality text samples curated from diverse sources, ensuring comprehensive coverage of linguistic, cultural, and domain-specific nuances. For fine-tuning, we employed full-parameter fine-tuning strategy, conducting training for one epoch. This approach allowed us to adapt the pre-trained model to better capture the semantic properties of both Arabic and English languages. To optimize training efficiency and scalability, we employ the flash-attention mechanism, which significantly reduces memory overhead during attention computation. Additionally, we adhere to the hyper-parameter configurations established by LLaMA-Factory [35], including an optimized learning rate schedule, batch size, and weight decay strategies tailored for large-scale transformer-based models."
        },
        {
            "title": "4 Conclusion",
            "content": "This work introduces AIN, an Arabic inclusive LMM, as step toward bridging the gap in AI solutions for Arabic, low-resource yet globally significant language. AIN is trained on large-scale bilingual dataset with 35% of authentic Arabic data. Through rigorous evaluation, we show that AIN achieves state-of-the-art performance across wide range of tasks, including VQA, OCR and document understanding, cultural understanding, and domain-specific applications such as medical imaging and remote sensing, surpassing even bigger and sophistcated models. AIN further demonstrates superior accuracy, contextual understanding, and human-like reasoning in MSA, as validated by extensive evaluations and human judgments. By integrating advanced data curation, robust translation pipelines, and stringent quality control, AIN provides new state-of-the-art multimodal AI model tailored to Arabic speakers."
        },
        {
            "title": "References",
            "content": "[1] Sara Ghaboura, Ahmed Heakl, Omkar Thawakar, Ali Alharthi, Ines Riahi, Abduljalil Saif, Jorma Laaksonen, Fahad Khan, Salman Khan, and Rao Anwer. Camel-bench: comprehensive arabic lmm benchmark. arXiv preprint arXiv:2410.18976, 2024. [2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. [3] OpenAI. Gpt-4o model. https://openai.com, 2024. Accessed: 2024. [4] OpenAI. Gpt-4o-mini model. https://openai.com, 2024. Accessed: 2024-10-14. [5] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [6] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. [7] Xiang Yue, Yueqi Song, Akari Asai, Seungone Kim, Jean de Dieu Nyandwi, Simran Khanuja, Anjali Kantharuban, Lintang Sutawika, Sathyanarayanan Ramamoorthy, and Graham Neubig. Pangea: fully open multilingual multimodal llm for 39 languages. arXiv preprint arXiv:2410.16153, 2024. [8] Google AI. Gemini: family of highly capable multimodal models, 2023. [9] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023. [10] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. [11] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [12] Nahid Alam, Karthik Reddy Kanjula, Surya Guthikonda, Timothy Chung, Bala Krishna Vegesna, Abhipsha Das, Anthony Susevski, Ryan Sze-Yin Chan, SM Uddin, Shayekh Bin Islam, et al. Maya: An instruction finetuned multilingual multimodal model. arXiv preprint arXiv:2412.07112, 2024. [13] Fajri Koto, Haonan Li, Sara Shatnawi, Jad Doughman, Abdelrahman Boda Sadallah, Aisha Alraeesi, Khalid Almubarak, Zaid Alyafeai, Neha Sengupta, Shady Shehata, et al. Arabicmmlu: Assessing massive multitask language understanding in arabic. arXiv preprint arXiv:2402.12840, 2024. [14] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European Conference on Computer Vision, pages 216233. Springer, 2025. [15] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. [16] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [17] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023. [18] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: Benchmarking multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [19] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. [20] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. [21] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. [22] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pages 235251. Springer, 2016. [23] Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo Liu, jiayi lei, Quanfeng Lu, Peng Gao, Runjian Chen, Peng Xu, Renrui Zhang, Haozhe Zhang, Yali Wang, Yu Qiao, Ping Luo, Kaipeng Zhang, and Wenqi Shao. MMT-bench: comprehensive multimodal benchmark for evaluating large vision-language models towards multitask AGI. In Proceedings of the International Conference on Machine Learning (ICML), 2024. [24] OpenAI. Gpt-4 model. https://openai.com, 2024. Accessed: 2024. [25] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. [26] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of naacL-HLT, volume 1. Minneapolis, Minnesota, 2019. [27] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bertnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. [28] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mpnet: Masked and permuted pre-training for language understanding. In Advances in Neural Information Processing Systems, 2020. [29] Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. Languageagnostic bert sentence embedding. arXiv preprint arXiv:2007.01852, 2020. [30] Wissam Antoun, Fady Baly, and Hazem Hajj. Arabert: Transformer-based model for arabic language understanding. arXiv preprint arXiv:2003.00104, 2020. [31] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311318, 2002. [32] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pages 6572, 2005. 19 [33] Chin-Yew Lin. Rouge: package for automatic evaluation of summaries. In Text summarization branches out, pages 7481, 2004. [34] Lukas Helff, Felix Friedrich, Manuel Brack, Kristian Kersting, and Patrick Schramowski. Llavaguard: Vlm-based safeguards for vision dataset curation and safety assessment. arXiv preprint arXiv:2406.05113, 2024. [35] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics."
        }
    ],
    "affiliations": [
        "Aalto University",
        "Australian National University",
        "Linköping University",
        "Mohamed bin Zayed University of AI"
    ]
}