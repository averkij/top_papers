{
    "paper_title": "We Can't Understand AI Using our Existing Vocabulary",
    "authors": [
        "John Hewitt",
        "Robert Geirhos",
        "Been Kim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This position paper argues that, in order to understand AI, we cannot rely on our existing vocabulary of human words. Instead, we should strive to develop neologisms: new words that represent precise human concepts that we want to teach machines, or machine concepts that we need to learn. We start from the premise that humans and machines have differing concepts. This means interpretability can be framed as a communication problem: humans must be able to reference and control machine concepts, and communicate human concepts to machines. Creating a shared human-machine language through developing neologisms, we believe, could solve this communication problem. Successful neologisms achieve a useful amount of abstraction: not too detailed, so they're reusable in many contexts, and not too high-level, so they convey precise information. As a proof of concept, we demonstrate how a \"length neologism\" enables controlling LLM response length, while a \"diversity neologism\" allows sampling more variable responses. Taken together, we argue that we cannot understand AI using our existing vocabulary, and expanding it through neologisms creates opportunities for both controlling and understanding machines better."
        },
        {
            "title": "Start",
            "content": "We Cant Understand AI Using our Existing Vocabulary John Hewitt 1 Robert Geirhos 1 Been Kim 1 5 2 0 2 1 1 ] . [ 1 6 8 5 7 0 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "This position paper argues that, in order to understand AI, we cannot rely on our existing vocabulary of human words. Instead, we should strive to develop neologisms: new words that represent precise human concepts that we want to teach machines, or machine concepts that we need to learn. We start from the premise that humans and machines have differing concepts. This means interpretability can be framed as communication problem: humans must be able to reference and control machine concepts, and communicate human concepts to machines. Creating shared human-machine language through developing neologisms, we believe, could solve this communication problem. Successful neologisms achieve useful amount of abstraction: not too detailed, so theyre reusable in many contexts, and not too high-level, so they convey precise information. As proof of concept, we demonstrate how length neologism enables controlling LLM response length, while diversity neologism allows sampling more variable responses. Taken together, we argue that we cannot understand AI using our existing vocabulary, and expanding it through neologisms creates opportunities for both controlling and understanding machines better. 1. Introduction Die Grenzen meiner Sprache bedeuten die Grenzen meiner Welt (The limits of my language are the limits of my world) Ludwig Wittgenstein As researchers interested in understanding and controlling language model-based AI systems, we often search for human-like concepts in machinese.g., by analyzing machines activation patterns. Examples of such concepts include human-like linguistic structure (e.g. Lakretz et al., 2019; Hewitt & Manning, 2019), or notions of safety or truth (Burns et al., 2023). Often, the goal of this search is to help 1Google DeepMind. Correspondence to: John Hewitt <johnhew@google.com>, Been Kim <beenkim@google.com>. Under Review. 1 Figure 1. Humans and machines conceptualize the world differently from each other. Mismatches in communication occur, which lead to misunderstandings. To understand and control AI, we must bridge this gap by developing new words corresponding to human and machine concepts, and use these words to control machines. specify human concepts to machinesthat is, to control them, e.g., through supervised probes, or prompts, or RLHF (Ouyang et al., 2022). Taken together, understanding and control are communication problem: communicating concepts between humans and machines. Within this communication problem, understanding and control are often two sides of the same coin: the purpose of communicating with machines is typically to make machines do what we want them to do (control), while achieving better understanding should directly translate into better communication. This communication problem is hard becauseand we take this as premisehumans and machines conceptualize the world differently, at many levels of abstraction (as expressed by Figure 1). machines notion of sentiment is different from human notion of sentiment. Likewise for high-quality code, or topic. In Kim (2022) for example, reproduced in Figure 2, there is space of machine concepts and space of human concepts, and many things are either in : concepts humans have but machines do not, or H: concepts machines have but humans do not. In fact; even for things seemingly in , we expect that careful inspection would show that the seemingly similar concepts actually differ between humans and machines. Our position is in this communication problemthus, progress in interpretabilityis best achieved by striving to define new words (neologisms) that that progress We Cant Understand AI Using Our Existing Vocabulary Figure 2. Machine and humans may fundamentally understand the world differently, enabling different concepts, knowledge and capabilities. Figure reproduced from Kim (2022); Schut et al. (2023) with permission. mean human concept (when interpreted by machine) or machine concept (when interpreted by human). What does introducing neologisms offer? The perspective provides clarity in what level of abstraction to attempt to bridge this communication gap. Successful words in language strike useful levels of abstraction: theyre not too exacting and low-level, like word for the exact placement of the chairs at my table in relation to me. Such words would be too rarely used to be successful. This is alike to attempting for full, exact, mechanistic understanding of neural network: words corresponding to such exactness must necessarily not apply commonly, because the world (and networks modeling the world) are too complex to be concisely described at that level. At the same time, most successful words are not too high-level (only few such words, like thing, exist)theyre discriminative enough of communicative intent to be informative in conversation. Erring too high-level is alike to only doing behavioral testing of network; the level of abstraction is that of an inputoutput map (e.g., logit output). Such evaluation is useful, but it gives us insufficient richness to specify our goals and understand future behavior. The next useful property that neologism learning gives us is participation in language. Language is how we understand other humans, and we define new words when our differences in conceptualization lead to the need to concisely communicate new concepts. Likewise, defining new words in our communication problem with machines, we can plug these words into existing language and leverage the expressive compositional structure thereof. Finally, the neologism framing helps us combat confirmation bias and anthropomorphism (e.g. Buckner, 2019). As human researchers, we have bias towards seeing humanlike things in artificial networks; we want to see high-level human concepts appearing in networks. We want to see exciting unsupervised structure. Even something as simple as sentiment neuron (OpenAI, 2017), if given its own new word, reminds us that this sentiment-like concept of the machine is likely dissimilar from what we call sentiment in ways that another humans notion of sentiment might not be. In Section 3.1, we argue that this dissimilarity will only increase as machines become more capable. Our notion of what constitutes defining new words is intentionally broadthis is high-level research direction wherein details must be nailed down over time. In our first section, we present our argument in these broad terms, discussing existing interpretability work and alternative perspectives. Then, we provide proof-of-concept that encodes machine and human concepts in new word embeddings that can be used in prompts to understand and control model behavior. We use preference data to define words for diverseH and lengthH , corresponding to human notions of diversity of response and constraints on the length of response. When interpreted by an LLM in natural language, these neologisms lead to desired responses. We also define word for goodM , corresponding to machines notion of response quality in given domain, whose use we show can help us understand what kinds of responses the model thinks are good. This addition of new words allows for more precise communication while leaving the model weights intact. This simple method, which we call neologism embedding learning, is first step towards creating joint human-machine language for understanding and control. 2. Understanding AI requires neologisms In this section, we present our argument that effective understanding and communication with AI systems requires us to form neologisms. We start with problems faced by researchers attempting to understand AI systems, and then present the solutions provided by the neologism framing. 2.1. Problems in Understanding AI Understanding (and controlling) AI systems, as joint problem of science and engineering, forces researchers to make bets on what kinds of properties to attempt to discover in AI systems, and what kinds of tools to build. As researchers on 2 We Cant Understand AI Using Our Existing Vocabulary is not considered sufficient for understanding the network. High abstraction, on the other hand, would suggest attempting to build very broad statements, like AlphaZero doesnt mind giving up material for positional advantage. These might be nice to know, but are insufficiently rich in how they let us control or trust the model. There is no one right level of abstraction at which to tackle the understanding problem, but it is key to hit good balance, as well argue. The Confirmation Bias Problem. As humans and as researchers, we have bias towards finding exciting, seemingly human-like properties in models. Considerable ink has been spilled on the pitfalls of such biases, e.g., in probing (Hewitt & Liang, 2019), saliency maps (Adebayo et al., 2018; Bilodeau et al., 2024), as well as interpretability as field (Lipton, 2017; Doshi-Velez & Kim, 2017). When were looking for interesting concepts, attempting to control them, even at useful level of abstraction, we still fall victim to our own biases when evaluating whether those concepts, like safety or sentiment, really line up with our own. Put another way, if we had discovered high-level concepts of safety in models already, wouldnt the safety-training teams at frontier labs be using those methods instead of (or as well as) doing better data collection for RLHF? 2.2. How Neologisms Help We now argue how the problems weve discussed can be ameliorated by framing understanding AI as communication problem in which we form neologisms to reference human or machine concepts. Lets go back to the example of chess and AlphaZero. When one human chess grandmaster attempts to understand the play of another, they use concepts and categories at some level of abstraction (e.g., forks, pins) that chess players jointly develop through shared experiences (games they play or observe together) and many rounds of discussion. Without developing this shared language, their discussions would be verbose. When we as humans attempt to understand AlphaZero, we should expect to have to develop such language of concepts ourselves. Developing neologisms is first key step in developing this shared language such that it enables efficient communication of complex concepts. Those of us who speak more than one language are familiar with the difficulty and verboseness of expressing certain thoughts in language that simply does not have word for the same concept, like the Dutch Gezelligheid (a sense of warmth, coziness and sometimes friendship in social context), or the Korean Jeong (affection that only develops over time, sometimes love and hate affection, expressed through experiencetranslating it as affection simply does not do the concept justice). Figure 3. Concept-based neologisms sit in-between mechanistic interpretability (which is closer to mechanistic details) and behavioral experiments/capability benchmarking (which is only concerned with the models output, not how it arrived there). the understanding problem, weve noticed few perennial problems that together motivate our focus on neologisms. The Conceptualization Difference Problem. Humans and machines understand the world differently, forming different conceptsequivalence classes, evaluations, skills from each other. This means we need to reference/label these concepts and then learn or understand the differences. Lets take an example. In game two of the 2016 match AlphaGo versus Lee Sedol, AlphaGos 37th move was considered particularly surprising compared to the usual play of top human Go players (Wired, 2016). AlphaGo would go on to win that game, and the match. Intuitively, AlphaGo may have had general conceptin this case, pattern it recognized in the board statethat motivated this move, which humans had not thought of. The general concept behind this move is yet to be understood (M H). If machines understand the world differently, can humans learn useful aspects of their thinking? Testing this idea in chess, Schut et al. (2023) developed process for discovering superhuman chess concepts in AlphaZero (Silver et al., 2017), and another process for teaching those concepts to grandmastershumans at the frontier of human knowledgeto expand what they know. Prior work had discovered correlates of human concepts in chess engines, like king safety or board position, (Lovering et al., 2022; McGrath et al., 2022), but Schut et al. (2023) focused specifically on new yet teachable (generalizable) concepts. These concepts are successfully taught to four top1 grandmasters, showing that the concepts were alien, but learnable. The Abstraction Problem. Once we accept that were attempting to communicate between two differing conceptualizations of the world, the question becomes, at what level of abstraction do we attempt this communication (Figure 3)? Low abstraction would suggest attempting exactingly precise concepts. For example, we have access to the precise forward pass of the network, which is as much program of its behavior as one could hope; nonetheless, this knowledge 1Top grandmasters are significantly stronger than the average grandmaster; each of these four has won FIDE world championship title (in open classical, open blitz, or womens classical.) 3 We Cant Understand AI Using Our Existing Vocabulary Neologisms concisely reference new learnable concepts. Differences in conceptualization introduce two problems: 1) we cannot yet reference new concepts concisely 2) we do not yet understand each concept. Neologisms provide solution to the first problem. neologism is successful new word; neologisms are formed when there are complex concepts that would be onerously verbose to reference otherwise i.e., by explaining the concept in paragraph or book each time. Given that machines and humans have differences in conceptualization, developing new words to reference those concepts to each other is natural solution. However, just because we can reference it, does not mean we can understand what it means. An important element for successful neologism is proximity; in Vygotskys education theory, proximity references concept being in the space between what learner can do without assistance and what learner can do with adult guidance or in collaboration with capable peers (Vygotsky, 1978); neologisms are formed to help reference things that are reachable but outside our current understanding. For example, Schut et al. (2023) targeted teaching chess champions whose proximity zone has better chance of capturing AlphaZeros superhuman strategies. Neologisms moderate useful abstraction. Natural languages are living, ever-changing things, and as such, potential new words crop up regularly. The words that survive to become neologisms strike useful level of abstraction. Some new words are more precise, like doomscroll, while others are more vague, like vibe. word that struck the right balance between the right level of abstractness and usefulness is case of successful neologism, thus, interpretability. The pressure of broad applicability enforces some abstraction: words gloss over an ocean of detail about the world so that theyre applicable in many settings, and thus used (unused words fail as neologisms.) The pressure of informativeness presses down on the amount of abstraction: word that references all things would be uninformative. Neologisms lessen confirmation bias. There is power to giving new label to thing instead of referring to it by known label: it encourages us to believe that the new thing is by default different and unknown. In the case of the OpenAI sentiment neuron, researchers discovered single activation in network that correlated reasonably well with sentiment on the Stanford Sentiment Treebank (Socher et al., 2013). At first glance this is not bad label, but as interpretability researchers, labeling its concept with new word sentimentM reminds us that it is probably not like sentiment in systematic ways that remain to be interpreted. We havent found human concept; weve found machine concept that has some overlap with human concepts. Neologisms enable compositionality. The beauty of human language is in the ability to concisely build an infinitude of meanings from finite symbol vocabulary. And due to the complexity of the world, there is an infinitude of concepts to understand about language models, not finite set of features. Another benefit of neologisms is that they participate in language; they should combine together with other concepts weve learned, and natural language, allowing us to leverage natural language to use our new insights. Neologisms provide human interface for control. Many interpretability techniques are repurposed for control, typically by working with internal representations (e.g., sparse autoencoders (Cunningham et al., 2023), or probing (Zou et al., 2023),). Neologisms enable these controls by integrating into humans natural way of communication language. As humans use new words, all expressive tools of language are at their disposal that could enable better precision and alignment. 3. Alternative views and rebuttals This section describes views that are opposed to our position, along with rebuttals. We follow it with broader discussion of related interpretability work. 3.1. Automatic convergence: scale will solve the communication problem Position. This position accepts that there are differences in the way humans and machines understand the world, but believes that these will inevitably narrow, and eventually disappear altogether. The argument is based on the observation that the shared human-machine space (M in Figure 2) has grown rapidly over recent years, from models that were barely able to produce coherent paragraph to todays LLMs that excel at writing poetry, code and email drafts. The bitter lesson (Sutton, 2019) continues to apply: larger models trained on larger datasets are inevitably getting better. Extrapolating this trend, the automatic convergence argument posits that the remaining gap between human and machine understanding, represented by the nonoverlapping areas in Figure 2, will gradually shrink and eventually vanish as result of scaling models. Rebuttal. While machines were approaching human level performance, the gap could indeed be perceived as narrowing. However, the gap will not close: as simple example, humans will not be able to reason over adversarial examples that will likely continue to exist. We are yet to find how to reason over why change in one meaningless word in prompt cause drastic changes in response (sometimes critical mistakes e.g., jailbreaking) (Branch et al., 2022). Beyond these peculiarities of existing systems, future machines are on track for superhuman performance (e.g, AlphaFold, 4 We Cant Understand AI Using Our Existing Vocabulary AlphaGo). Such superhuman machines will likely widen the communication gap; by definition, we do not yet know how to do what we cant. All these make automatic convergence unlikely, and make it necessary for humans to expand what they know. 3.2. We already have all the words needed to communicate. Position. This position asserts that our existing vocabulary and language is sufficient to understand AI. We dont always introduce new word when we extend our knowledge; sometimes its just longer description combining things we already know. We just need methods that map from machine concepts to natural language explanations. While there may be debate around whether explanations should be faithful (ideal but hard to verify) or just appear plausible or helpful (after all, we also accept post-hoc explanations from humans without being able to check whether they are true), this position essentially states why invent new words when our existing vocabulary must be sufficient for explanations. We cant understand what we cant describe. Rebuttal. We agree that in lieu of new words for new concepts, it may be possible to derive natural language description for any conceptalbeit potentially long and cumbersome one. However, lack of concise word stifles communication; imagine not being able to use the word house, and instead having to describe what you mean every time youd like to refer to the concept. This would be neither concise, nor enable compositionality; use the word together with natural language to enable expressibility. For example, having crisp words like house enable us to combine them in novel ways (houseboat, courthouse). According to Wittgenstein, the limits of my language are the limits of my world (Wittgenstein, 1922) and if were interested in communicating with the world of machines, leveraging the expressibility of natural language and beyond would be necessary for flexible and effective communication. 3.3. We just need an exhaustive map of explanations Position. The lack of rigor in interpreting model is the fundamental problem. If we can understand the exact low-level circuits, creating comprehensive map of model features along with their function, this explanation map would solve interpretability and explain the entire decision-making process of models with great accuracy. Rebuttal. Unfortunately, this approach does not scale well to increasingly large models with increasingly many circuits and features. Even if it did, an important lesson from neuroscience is that even mapping out the entire connectome of system (as done for the worm c. elegans, cf. Cook et al., 2019) is not sufficient to meaningfully understand the system. 3.4. We dont need abstraction Position. In terms of understanding machines, the gold standard is precise, exact mechanistic understanding with as little unexplained abstraction as possiblealike to reverse engineering neural networks (Olah, 2022). Rebuttal. First, finding the right level of detail isnt obvious: Is it the level of layers? Circuits? Individual units, and their receptive and projective fields? The code that specifies network, or the code it compiles to? The silicon it runs on? The atoms that create the silicon? Presumably, we can all agree that the level of individual atoms would be ridiculous level of detail for analyzing machine intelligence; nonetheless this goes to show that in science, abstraction is often an advantage, not drawback (cf. Borges et al., 2002). Secondly, even if there was universally accepted right level of detail, in terms of human-machine communication this would still be one way street, seeking to identify human concepts in machines. In contrast, by creating new words to communicate concepts between humans and machines, this enables compositional understanding where one concept can be re-used for different purpose, and combined with others. Arguably, components determined by mechanistic interpretability may not satisfy proximity either, since the way components are decided had no regards to humans capability. 4. How neologisms fit into other interpretability work We here discuss connections to range of techniques and perspectives within existing interpretability work. Feature attribution methods. Feature attribution methods (e.g. Sundararajan et al., 2017; Lundberg & Lee, 2017; Selvaraju et al., 2017; Shrikumar et al., 2017; Smilkov et al., 2017) are widely used methods in interpretability. While the main critics of this approach (Adebayo et al., 2018; Tomsett et al., 2020; Kindermans et al., 2019; Ghorbani et al., 2019; Bilodeau et al., 2024) seem to highlight why these methods do not and cannot work, an alternative hypothesis is what these methods are showing is not something humans can comprehend (perhaps due to using unnatural mediums like pixels to explain model decisions). For example, the fact that humans or quantitative metrics that we defined cannot distinguish between feature attributions from trained network vs. an untrained network (Adebayo et al., 2018) suggests two possibilities: Either 1. that they are truly the same or 2. the metrics we use are incapable of describing the concepts that machines have. Concept discovery. Finding new concepts from models has been well-studied problem (Bau et al., 2017; Ghorbani et al., 2019; Fel et al., 2023; Lang et al., 2021; Rane et al., 2023; Schut et al., 2023). While many attempt to name 5 We Cant Understand AI Using Our Existing Vocabulary certain machine concepts, there is no systematic thinking on developing new word with reusability in mind, let alone being able to use them in composition with natural language. Nevertheless, these works could form foundations of neologism learning. Faithfulness and evaluation. Faithfulnesswhether an explanation truly reflects models conceptshas long been point of discussion in interpretability in the context of evaluating explanations (Lipton, 2017; Doshi-Velez & Kim, 2017). Neologisms do not solve this problem; instead, they provide new way to evaluate. We consider new word success if we can communicate something useful (e.g., control) using the new word, and the machine possibly uses the same word to communicate something in return. For example, If new word goodm defines how machines understand good answers, humans learn ways to use this towards their goal. If goodm is aligned with goodh except for length, we can prompt give me goodm answers but make them short. In this context, success (here: getting high-quality yet short answers) can easily be validated. Probing and representation engineering. Probing training simple readout function from neural activity to property of interestwas most recently independently introduced in machine learning by Alain & Bengio (2016); Ettinger et al. (2016); Shi et al. (2016), though the methods were directly inspired by similar methods in neuroscience, which have long history. As models have improved, the targets of probing moved from linguistic properties (Hewitt & Manning, 2019; Tenney et al., 2019) towards higherlevel concepts like correctness (Burns et al., 2023; Marks & Tegmark, 2024). Probes have shown that neural networks make some complex concepts simply accessible. Representation engineering (Zou et al., 2023) takes this idea and pursues the idea that these discovered concepts can be used to steer model behavior, previously considered by, e.g., (Eisape et al., 2022). Probing and representation engineering as tools allow for range of levels of abstraction and can be tools for communication of already known concepts (M H), though the fundamental question in this case becomes when and where to apply probes to communicate with machines. 5. proof of concept: Neologism Embedding"
        },
        {
            "title": "Learning",
            "content": "While our position in favor of neologisms is independent of specific implementations, one may ask how to implement this idea. This section presents simple proof-of-concept to more concretely discuss the merits. Our method, neologism embedding learning (Figure 4), involves three steps: 1. Defining new vocabulary element in the tokenizer of language model (and corresponding embedding matrix). Figure 4. Our neologism embedding learning only updates new word embedding, preserving the original models responses when the new word is not used. 2. Using that new word in otherwise-natural language sentences in prompts. 3. Training the embedding of the word to minimize preference-based losses (defined to capture human or machine concepts). Note that the model remains frozen, so when the new word is not invoked, the model is left unchanged: this guarantees identical output when the neologism isnt used. 5.1. Method Let pθ be neural language model parameterized by θ. Let be finite vocabulary, with corresponding word embeddings RdV, where θ, and is the dimesionality of representations in pθ. We add new word to the vocabulary, such that V, and our new vocabulary is = {w}. We define new embedding for for w, initialized either randomly or from some existing embedding. Well refer to this new Rd embedding of as Ew. By construction, no naturalistic data contains the new word w. We take preference data wherein the prompt contains the new word w. More precisely, we assume dataset = i=1, where each xi is string over that {(xi, yi,c, yi,r)}n is guaranteed to include w, and yi,c is the chosen response, string over V, and likewise yi,r is the rejected response. Pulling an example from below, sample prompt from such data might be Give me recipe for tiramisu. Ensureh that the length of the response is at least 600 words. Weve labelled the new word Ensureh for legibility, but recall that its embedding is new, and not tied to the English word ensure. Instead, it refers to specific human meaning (thus h) of ensuring length. In this example, the chosen response (yc) would meet the human-specified length constraint, while the rejected response (yr) would not. 6 We Cant Understand AI Using Our Existing Vocabulary Neologism embedding learning optimizes preference loss over just Ew, while keeping the rest of θ unchanged: ED [L(x, yc, yr)] , min Ew (1) where the loss function could be DPO (Rafailov et al., 2024) or one of its many variants; in Appendix A, we describe variant of APO (DOosterlinck et al., 2024) that we found particularly useful for our experiments. Neologism for : Intuitively, participates in natural language statements in wherein the person specifying the preference wants particular behavior (yc) but may instead have received yr; the meaning of is thus learned to be whatever makes the chosen preference loss lower. Neologism for H: Similarly, particular behavior machine exhibits can be captured in trained using pairs of behaviors humans yet to understand. In other words, we use the word as vehicle to carry the meanings of the contrast (between yc and yr), then use in our prompt to understand what it means. 5.2. Merits and related methods Neologism embedding learning combines the lightweight finetuning method soft prompting (Lester et al., 2021) with the flexibility of prompting. Soft prompting involves learning an embedding or embeddings that are prepended to all inputs without aiming to being interpretable (Bailey et al., 2023). The main merit of soft prompting over finetuning all parameters (or LoRA or similar (Hu et al., 2022)) is that the choice of soft prompt can be made simply by determining what tokens (or soft tokens) are used as input to the model. Neologism embedding learning is subtly but crucially different from soft prompting: our new words are meant to be told to the users and participate in natural language inputs much like other input tokens, so the user chooses when and in what natural language contexts to use them. As such, when user decides not to use new word in their prompt, they get guarantee that they sample from the original, unchanged model (Figure 4.) Users can also use them in composition with other new words; we envision understanding machine concepts will require variety of ways to combine new words. 5.3. Experiment: Length Neologism (H ) Controlling the length of response is surprisingly difficult problem even for the frontier models when the desired length is very different from the models prior over response lengths for the instruction. In these experiments, we define preference dataset by taking broad-domain instructions from LIMA (Zhou et al., 2023), and constructing responses that do and do not obey specified length constraint; in particular, one of two: Ensure that the response is between Figure 5. Base models prompted for length control fail to generate specified long generations (blue), but with neologism (orange), they consistently generate longer responses. 400-600 words and between 600-1000 words. For our neologism word ensureh w, we replace the word ensure in that prompt. When we test base Gemma model (Mesnard et al., 2024) on held out instructions with one of the two length constraints (Figure 5), we find that the base model never meets the length constraints for the instructions we used (Figure 5.) However, many more responses meet the length constraints when prompted with ensureh w. 5.4. Experiment: Diversity Neologism (H ) One LLM control knob that human user might want to have access to is the diversity or variation across several responses. Sometimes, responses should not be variable: two plus two equals four, no matter how many times the model is asked. On the other hand, sometimes variability is desired: for example, when we want model to try multiple attempts to get the right answer or do creative task (e.g., write poem). Diverse responses are particularly relevant in the context of inference-time scaling, where models often generate many responses to the same problem, followed by filtering or ranking (Brown et al., 2024). One expensive heuristic for generating more diverse outputs is conditioning on existing responses, and asking for new one. We leverage this heuristic to train neologism for this human concept by using high quality teacher model (Gemini 1.5 Pro, Georgiev et al., 2024) to condition on up to four responses for each instruction, and request subsequent new (different) response. Our preference data contains instructions from the LIMA dataset (Zhou et al., 2023), each appended with an additional phrase. Instead of asking give me your kth response, we prompt with diversityh me your kth response. The chosen response is the kth Gemini response, and the rejected is the (k 1)st. To test this neologism in proof-of-concept setting, model is asked to guess an integer between 1 and 9. We then as7 We Cant Understand AI Using Our Existing Vocabulary Gemma responses for Give me recipe for tiramisu. + Give me response that is extremely goodm . ## Classic Tiramisu Recipe *Ingredients:* **For the ladyfingers:** - 250g ladyfingers, softened - 150g caster sugar - 50ml all-purpose flour [...truncated] + Give me response that is extremely not goodm . am unable to provide recipes that are unsafe or inappropriate. Cooking with hazardous ingredients or recipes that could result in harm is not recommended. Table 1. Gemma responses for the instruction Give me recipe or not goodm for tiramisu and request for an extremely goodm response, using Gemmas response quality neologism. Extremely goodm responses on average are scored 3.8 by Gemma, whereas Extremely not goodm are scored 3.2. Qualitatively, looking at response for Give me recipe for tiramisu that is extremely goodm , we note that it is effusive and uses rich language  (Table 1)  . Despite not being trained to generate the negative of goodm , we qualitatively see that the model correctly evaluates the extended language extremely not goodm , generating refusal answer. Of Gemmas evaluations of its own responses, refusals were often the lowest-scoring. To verify that extremely goodm (and extremely not goodm ) do in fact yield responses that Gemma does (or does not, respectively) score highly, we generate 50 samples for each setting, and score them with Gemma. Gemma has strong bias towards generating score of 4 out of 5; even so, the average score for extremely good responses is 3.8 compared to score of 3.2 for extremely not good responses. 6. Conclusion Solving the communication problem between two dissimilar intelligent entitiescommunicating human concepts to machines, and machine concepts to humansrequires new language. Words that can function as vehicle for such differences in concepts empower humans to use natural language to communicate, while reducing confirmation bias. Learning neologisms enables discovering and leveraging concepts at moderate abstraction that strikes useful balance for communication. Our method of neologism embedding learning is starting point for how this idea can be implemented to solve the communication problem."
        },
        {
            "title": "Impact Statement",
            "content": "This article presents perspective related to understanding and controlling AI systems through neologisms. As an interpretability tool, neologisms could benefit two purposes: Figure 6. Adding diversity neologism diverseh to prompt substantially increases models response variety, as exemplified in number guessing game. The setup is explained in Section 5.4; higher = more response variety (better). sume that one and only one of these numbers is the correct solution that model has to find within guesses. Figure 6 shows, in black, how well model could do in principle by guessing numbers uniformly at random. After 10 guesses, the probability of finding the correct number is 69%; after 40 guesses, this increases to over 99%. In blue, standard Gemma model (Mesnard et al., 2024) is shown. Gemma is biased towards guessing either 5 or 7 in this task, which means that even after 100 guesses its probability of finding the correct response, in expectation, is only about 20%. Fortunately, this can be drastically improved through our preference-trained neologism, diverseh w, token trained such that model generates the i-th response (here: the second response; shown in red). When using diverseh in the prompt, the models responses indeed become more varied, strongly increasing the probability of finding the correct number. This shows in this toy example, the human concept of diversity was successfully communicated which resulted in needing many fewer samples. 5.5. Experiment: Models Preferences (M H) How can we learn concept from language model using almost just the methods weve discussed so far? In this section we learn word for the model to communicate (some version of) its own notion of response quality to us. In these experiments, we learn neologism that makes model generate responses that it would itself deem as good. Using the LIMA dataset, we (1) sample times from Gemma for each instruction, (2) score each response with Gemma, and (3) construct preference dataset where the chosen response yc is the high-scoring of the k, and yr is the lowest-scoring of the k. We then learn goodm , using the neologism embedding learning method, where the natural language we add to each LIMA instruction is Give me response you think is goodm is our new word. , where goodm 8 We Cant Understand AI Using Our Existing Vocabulary an improved understanding of AI, and improved control of AI systems. Once achieved successfully, AI becomes just another collaborator who can work with you via efficient two way communication. While understanding AI is generally considered beneficial to society, the ability to control machines better can be used for both beneficial and harmful purposes, in line with the general dual-use problematic of AI tools and models. Overall, neologisms are intended as first step towards shared human-AI language, which could contribute towards making AI more useful and aligned with human intentions. As an analogy, imagine new coworker joins an existing team. The likelihood of the team being able to explain how the new hire can support the teams workflows is much higher if everyone speaks the same language. Similarly, neologisms could improve human-AI communication."
        },
        {
            "title": "Acknowledgements",
            "content": "The authors would like to thank Scott Lundberg for helpful comments on the draft, as well as Zi Wang, Noah Fiedel, and Shakir Mohamed for support and insightful discussions."
        },
        {
            "title": "References",
            "content": "Adebayo, J., Gilmer, J., Muelly, M., Goodfellow, I., Hardt, M., and Kim, B. Sanity checks for saliency maps. In Advances in Neural Information Processing Systems 32, 2018. Alain, G. and Bengio, Y. Understanding intermediate layers using linear classifier probes. ArXiv, abs/1610.01644, 2016. URL https://api.semanticscholar. org/CorpusID:9794990. Bailey, L., Ahdritz, G., Kleiman, A., Swaroop, S., DoshiVelez, F., and Pan, W. Soft prompting might be bug, not feature, 2023. Bau, D., Zhou, B., Khosla, A., Oliva, A., and Torralba, A. Network dissection: Quantifying interpretability of deep visual representations. In Computer Vision and Pattern Recognition, 2017. Bilodeau, B., Jaques, N., Koh, P. W., and Kim, B. Impossibility theorems for feature attribution. Proceedings of the National Academy of Sciences, 121(2):e2304406120, 2024. Borges, J. L. et al. Of exactitude in science. QUADERNSBARCELONA-COLLEGI ARQUITECTES DE CATALUNYA-, pp. 1212, 2002. Branch, H. J., Cefalu, J. R., McHugh, J., Hujer, L., Bahl, A., del Castillo Iglesias, D., Heichman, R., and Darwishi, R. Evaluating the susceptibility of pre-trained language 9 models via handcrafted adversarial examples, 2022. URL https://arxiv.org/abs/2209.02128. Brown, B., Juravsky, J., Ehrlich, R., Clark, R., Le, Q. V., Re, C., and Mirhoseini, A. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. Buckner, C. The comparative psychology of artiURL https:// intelligences, May 2019. ficial philsci-archive.pitt.edu/16034/. Burns, C., Ye, H., Klein, D., and Steinhardt, J. Discovering latent knowledge in language models without supervision. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview. net/forum?id=ETKGuby0hcs. Cook, S. J., Jarrell, T. A., Brittin, C. A., Wang, Y., Bloniarz, A. E., Yakovlev, M. A., Nguyen, K. C., Tang, L. T.-H., Bayer, E. A., Duerr, J. S., et al. Whole-animal connectomes of both Caenorhabditis elegans sexes. Nature, 571 (7763):6371, 2019. Cunningham, H., Ewart, A., Riggs, L., Huben, R., and Sharkey, L. Sparse autoencoders find highly interpretable features in language models. arXiv preprint arXiv:2309.08600, 2023. DOosterlinck, K., Xu, W., Develder, C., Demeester, T., Singh, A., Potts, C., Kiela, D., and Mehri, S. Anchored preference optimization and contrastive revisions: Addressing underspecification in alignment. arXiv preprint arXiv:2408.06266, 2024. Doshi-Velez, F. and Kim, B. Towards rigorous science of interpretable machine learning, 2017. URL https: //arxiv.org/abs/1702.08608. Eisape, T., Gangireddy, V., Levy, R., and Kim, Y. Probing for incremental parse states in autoregresIn Goldberg, Y., Kozareva, sive language models. Z., and Zhang, Y. (eds.), Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 28012813, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. 10.18653/v1/2022.findings-emnlp. 203. URL https://aclanthology.org/2022. findings-emnlp.203/. doi: Ettinger, A., Elgohary, A., and Resnik, P. Probing for semantic evidence of composition by means of simple classification tasks. In Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP, pp. 134139, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/W16-2524. URL https://aclanthology.org/W16-2524/. We Cant Understand AI Using Our Existing Vocabulary Fel, T., Picard, A., Bethune, L., Boissin, T., Vigouroux, D., Colin, J., Cad`ene, R., and Serre, T. Craft: Concept recursive activation factorization for explainability. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 27112721, 2023. Georgiev, P., Lei, V. I., Burnell, R., Bai, L., Gulati, A., Tanzer, G., Vincent, D., Pan, Z., Wang, S., et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Ghorbani, A., Abid, A., and Zou, J. Interpretation of neural networks is fragile. In Proceedings of the 33rd AAAI Conference on Artificial Intelligence, 2019. Ghorbani, A., Wexler, J., Zou, J. Y., and Kim, B. Towards automatic concept-based explanations. Advances in neural information processing systems, 32, 2019. Hewitt, J. and Liang, P. Designing and interpreting probes with control tasks. In Inui, K., Jiang, J., Ng, V., and Wan, X. (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 27332743, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1275. URL https://aclanthology.org/D19-1275/. Hewitt, J. and Manning, C. D. structural probe for finding syntax in word representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 41294138, 2019. Hu, E. J., yelong shen, Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https: //openreview.net/forum?id=nZeVKeeFYf9. Kim, B. Beyond interpretability: developing language to shape our relationships with AI, Apr 2022. URL https://medium.com/@beenkim/ beyond-interpretability-4bf03bbd9394. Kindermans, P.-J., Hooker, S., Adebayo, J., Alber, M., Schutt, K. T., Dahne, S., Erhan, D., and Kim, B. The (un)reliability of saliency methods. In Samek, W., Montavon, G., Vedaldi, A., Hansen, L. K., and Muller, K.-R. (eds.), Explainable AI: Interpreting, Explaining and Visualizing Deep Learning, pp. 267280. Springer International Publishing, 2019. 10 Lakretz, Y., Kruszewski, G., Desbordes, T., Hupkes, D., Dehaene, S., and Baroni, M. The emergence of number and syntax units in LSTM language models. In Burstein, J., Doran, C., and Solorio, T. (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 1120, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1002. URL https: //aclanthology.org/N19-1002/. Lang, O., Gandelsman, Y., Yarom, M., Wald, Y., Elidan, G., Hassidim, A., Freeman, W. T., Isola, P., Globerson, A., Irani, M., and Mosseri, I. Explaining in style: Training gan to explain classifier in stylespace. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 693702, October 2021. Lester, B., Al-Rfou, R., and Constant, N. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021. Lipton, Z. C. The mythos of model interpretability, 2017. URL https://arxiv.org/abs/1606.03490. Lovering, C., Forde, J., Konidaris, G., Pavlick, E., and Littman, M. Evaluation beyond task performance: analyzing concepts in alphazero in hex. Advances in Neural Information Processing Systems, 35:2599226006, 2022. Lundberg, S. M. and Lee, S.-I. unified approach to inIn Advances in Neural terpreting model predictions. Information Processing Systems 31, 2017. Marks, S. and Tegmark, M. The geometry of truth: Emergent linear structure in large language model representations of true/false datasets. In First Conference on Language Modeling, 2024. URL https://openreview. net/forum?id=aajyHYjjsk. McGrath, T., Kapishnikov, A., Tomaˇsev, N., Pearce, A., Wattenberg, M., Hassabis, D., Kim, B., Paquet, U., and Kramnik, V. Acquisition of chess knowledge in alphazero. Proceedings of the National Academy of Sciences, 119 (47):e2206625119, 2022. Mesnard, T., Hardin, C., Dadashi, R., Bhupatiraju, S., Pathak, S., Sifre, L., Rivi`ere, M., Kale, M. S., Love, J., et al. Gemma: Open models based on Gemini research and technology. arXiv preprint arXiv:2403.08295, 2024. Olah, C. Mechanistic interpretability, variables, and Transthe importance of former Circuits Thread, URL https://www.transformer-circuits.pub/ 2022/mech-interp-essay. interpretable bases. 2022. June We Cant Understand AI Using Our Existing Vocabulary OpenAI. ron. unsupervised-sentiment-neuron/, Accessed: February 12, 2025. Unsupervised neuhttps://openai.com/index/ 2017. sentiment Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. Rane, S., Ho, M., Sucholutsky, I., and Griffiths, T. L. Concept alignment as prerequisite for value alignment. arXiv preprint arXiv:2310.20059, 2023. Schut, L., Tomasev, N., McGrath, T., Hassabis, D., Paquet, U., and Kim, B. Bridging the human-AI knowledge gap: Concept discovery and transfer in alphazero. arXiv preprint arXiv:2310.16410, 2023. Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., and Batra, D. Grad-CAM: Visual explanations from deep networks via gradient-based localization. In Proceedings of the 2017 IEEE International Conference on Computer Vision, 2017. Shazeer, N. and Stern, M. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pp. 45964604. PMLR, 2018. Shi, X., Padhi, I., and Knight, K. Does string-based In Conference on neural mt learn source syntax? Empirical Methods in Natural Language Processing, 2016. URL https://api.semanticscholar. org/CorpusID:7197724. Shrikumar, A., Greenside, P., and Kundaje, A. Learning important features through propogating activation functions. In Proceedings of the 34th International Conference on Machine Learning, 2017. Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., Lillicrap, T., Simonyan, K., and Hassabis, D. Mastering chess and shogi by self-play with general reinforcement learning algorithm, 2017. URL https://arxiv.org/abs/1712.01815. Smilkov, D., Thorat, N., Kim, B., Viegas, F., and Wattenberg, M. SmoothGrad: Removing noise by adding noise. In Proceedings of the ICML 2017 Workshop on Visualization for Deep Learning, 2017. 11 Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A., and Potts, C. Recursive deep models for semantic compositionality over sentiment treebank. In Yarowsky, D., Baldwin, T., Korhonen, A., Livescu, K., and Bethard, S. (eds.), Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 16311642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL https://aclanthology.org/D13-1170/. Sundararajan, M., Taly, A., and Yan, Q. Axiomatic attribution for deep networks. In Proceedings of the 34th International Conference on Machine Learning, 2017. Sutton, R. The bitter lesson. Incomplete Ideas (blog), 13(1): 38, 2019. Tenney, I., Xia, P., Chen, B., Wang, A., Poliak, A., McCoy, R. T., Kim, N., Durme, B. V., Bowman, S., Das, D., and Pavlick, E. What do you learn from context? probing for sentence structure in contextualized word representations. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum? id=SJzSgnRcKX. Tomsett, R., Harborne, D., Chakraborty, S., Gurram, P., and Preece, A. Sanity checks for saliency metrics. In Proceedings of the 34th AAAI Conference on Artificial Intelligence, 2020. Vygotsky, L. S. Mind in society: The development of higher psychological processes. Cambridge, Mass.: Harvard University Press., 1978. Wired. In two moves Alphago and Lee Sedol redefined https://www.wired.com/2016/03/ future. two-moves-alphago-lee-sedol-redefined-future/, 2016. Accessed: 29 Jan 2025. Wittgenstein, L. Tractatus Logico-Philosophicus. Project Gutenberg, 1922. URL https://www.gutenberg. org/ebooks/5740. EBook #5740. Zhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X., Efrat, A., Yu, P., Yu, L., Zhang, S., Ghosh, G., Lewis, M., Zettlemoyer, L., and Levy, O. Lima: Less is more for alignment, 2023. URL https://arxiv.org/abs/ 2305.11206. Zou, A., Phan, L., Chen, S., Campbell, J., Guo, P., Ren, R., Pan, A., Yin, X., Mazeika, M., Dombrowski, A.-K., et al. Representation engineering: top-down approach to AI transparency. arXiv preprint arXiv:2310.01405, 2023. We Cant Understand AI Using Our Existing Vocabulary A. Methods A.1. Preference Loss For our preference loss function L, we use variant of DPO (Rafailov et al., 2024) called APO-up (DOosterlinck et al., 2024). The DPO loss is defined on pairs of outputs for given input, and is intended to teach models to generate outputs more like preferred output, and less like dispreferred output. DOosterlinck et al. (2024) note that the DPO loss can be minimized by reducing the likelihood of both preferred and dispreferred outputs (unintuitively,) as long as the dispreferred outputs likelihood is reduced more. The family of anchored preference losses introduced by DOosterlinck et al. (2024) are intended to allow the researcher to specify whether they want this to be the case. We found for our early neologism learning experiments that indeed, both preferred and dispreferred outputs were decreasing in probability, leading to text degeneration during sampling. The variant we use, APO-up, simply adds term to the DPO loss that corresponds to saturating benefit to increasing the likelihood of the preferred output relartive to its initial likelihood (in this view, DPO gives saturating benefit to increasing the likelihood-ratio of preferred over dispreferred, again relative to the original likelihood ratio.) We found that this greatly improved training stability. Recall that is an input sequence, yc chosen output sequence, yr rejected output sequence, and pθ language model parameterized by the parameters were optimizing over, θ. Let θ0 be the initial value of the parameters before any optimization. Let β be hyperparameter constant. The loss is as follows: L(x, yc, yr) = log σ β log (cid:18) pθ(yc x) pθ(yr x) + β log (cid:19) pθ0(yc x) pθ0(yr x) (cid:18) log σ β log (cid:19) pθ(yc x) pθ0(yc x) (2) DPO Loss Increase likelihood of chosen response relative to initial likelihood. B. Experimental Details In all experiments, we use Gemma 2B model (Mesnard et al., 2024) and the Adafactor optimizer (Shazeer & Stern, 2018). Through early exploration, we determined learning rate of 0.02very large compared to most learning rates, but very few parameters are being optimized. For the experiments in learning from Gemmas preferences, we instead use learning rate of 0.001. We use batch size of 1, and early-stop when the APO-up training loss reduces by 0.2. During all generation, we enforce that the new token is not generated by the model by replacing its logit with . In future work, we expect to instead teach the model where and when to use neologisms. For the β hyperparameter in APO-up, we use 0.2. To initialize our new word embedding Ew, we use the embedding of the word Ensure. B.1. Length neologism For our preference data, we used 700 instructions from the LIMA dataset (Zhou et al., 2023). Our two length constraints were for responses between 400-600 words and 600-1000 words. To generate constraint-meeting responses, we used an in iterative process in which we queried Gemini 1.5 Pro (Georgiev et al., 2024), computed the difference between the responses length and goal length span, and re-prompted Gemini telling it how many words to add or subtract. We use the resulting length constraint-obeying output as the chosen response, and Geminis first guess as the rejected response. In Table 2, we give example generations from baseline Gemma (prompted with length constraint) and from using our neologism in the same length constraint. In Figure 7, we show results for both length constraint spans, showing that our length neologism not just makes Gemma generate longer responses, but indeed assists in generating responses of the target length. Baseline Gemma with prompting fails to generate responses that meet either length constraint. B.2. Diversity neologism The following prompt was used to elicit guesses from the model: Your task is to select an integer between 1 and 9. Format your response as valid JSON with single field called number: <number>. The prompt specificially asked for JSON for two reasons. First, this reduces models refusal rate (otherwise, the model would sometimes state that it cannot guess numbers or answer with riddle instead of guess). Second, this facilitates automatic parsing of the model response. The 12 We Cant Understand AI Using Our Existing Vocabulary Instruction: Give me recipe for tiramisu. Baseline (Prompted Gemma) Ensure that the response is between 600-1000 words. Measured length: 344 words too short Ours (Length Neologism) Ensureh Measured length: within constraint. that the response is between 600-100 words. **Ingredients:** **For the espresso-soaked ladyfingers:** - 1 cup ladyfingers, crushed - 1/2 cup milk - 2 tablespoons granulated sugar **For the mascarpone cream:** - 1 cup mascarpone cheese, softened - 2 cups heavy cream - 1/4 cup granulated sugar - 1/4 teaspoon vanilla extract **For the dusting:** - Powdered sugar - Cocoa powder (for garnish) **Instructions:** **1. Prepare the ladyfingers:** - In small bowl, mix the crushed ladyfingers and milk. - Let sit for 5 minutes to absorb the liquid. - Drain any excess liquid. **2. Make the mascarpone cream:** - In large saucepan, combine the mascarpone cheese, heavy cream, and sugar. - Heat over low heat, stirring constantly, until the mixture thickens and becomes creamy. - Remove from heat and whisk in the vanilla extract. **3. Layer and assemble:** - Dip each ladyfinger into the espresso-soaked mixture, ensuring they are coated completely. - Spread thin layer of mascarpone cream on the bottom of serving dish. - Top with layer of ladyfingers. - Pour the mascarpone cream mixture over the ladyfingers. **4. Dust and serve:** - Sprinkle the tiramisu with powdered sugar and cocoa powder. - Serve immediately or store in the refrigerator for later. **Tips:** * For the best flavor, use high-quality ingredients. * Do not overcrowd the ladyfingers with mascarpone cream, or they will become soggy. * Serve the tiramisu at room temperature or slightly chilled. * Tiramisu can be made ahead of time and stored in the refrigerator for up to 24 hours. **Additional Variations:** * **Coffee tiramisu:** Use strong coffee in the espresso soak to create richer flavor. * **Chocolate tiramisu:** Add layer of chocolate ganache between the mascarpone layers. * **Vanilla tiramisu:** Add 1 teaspoon of vanilla extract to the mascarpone cream. 1. **Gather Ingredients** - 2 cups ladyfingers, crumbled - 1 cup mascarpone cheese, softened - 2 cups granulated sugar - 00bd cup strong coffee or espresso - 2 teaspoons vanilla extract - 1 cup whole milk - 00bd cup heavy cream - 2 large eggs yolks - 1 large egg white 2. **Prepare the Base** - Take small bowl and mix 2 tablespoons of the mascarpone cheese with splash of espresso. Add the mascarpone mixture to small casserole dish. 3. **Make the Mascarpone Filling** - Combine the mascarpone cheese, granulated sugar, and vanilla extract in mixing bowl. Beat the ingredients together until smooth and creamy. - Set aside. 4. **Prepare the Coffee Base** - Brew 2 cups of strong coffee or espresso. Add the diluted coffee to small saucepan and bring to simmer. 5. **Prepare the Cream Base** - In medium saucepan, whisk the milk together with the heavy cream until combined. 6. **Prepare the Eggs** - Beat the egg yolks together until combined, but not over beated. 7. **Assemble the Tiramisu** - Pour small amount of mascarpone mixture onto the bottom of the casserole dish. 8. **Layer** - Place spoonful of the mascarpone mixture onto the bottom layer of the casserole dish and repeat. 9. **Continue with Layers** - Layer the mascarpone, milk mixture, coffee mixture, and egg mixture, in that order. 10. **Present Tiramisu** - Garnish with whipped cream and grated chocolate shavings. **Additional Tips for Making Tiramisu** - Use good quality mascarpone cheese. high-quality mascarpone cheese will make difference in the taste and texture of your tiramisu. - Be sure to take your mascarpone cheese out of the refrigerator about 30 minutes before mixing it with the other ingredients. - Be careful not to add too much sugar to the mascarpone mixture. It should be sweet, but not overly sweet. - When layering the tiramisu, be sure to keep the layers thin. This will prevent the tiramisu from becoming dense. - Serve your tiramisu immediately after it is assembled. **Troubleshooting and Suggestions for Improvement** - **Too little mascarpone.** If the mascarpone mixture is too thin, it will not set properly. - **Too much mascarpone.** If the mascarpone mixture is too thick, it will not absorb the milk mixture properly. - **Not enough powdered sugar.** The powdered sugar will help to balance the sweet flavor of the mascarpone and milk mixture. - **Not enough espresso.** If you dont add enough espresso to the coffee base, it will not be strong enough to set properly. **Additional Variations of Tiramisu** - **Chocolate Tiramisu:** Add 1 cup of cocoa powder to the mascarpone mixture. - **Vanilla Tiramisu:** Add 1 teaspoon of vanilla extract to the mascarpone mixture. - **Fruity Tiramisu:** Mix in variety of fruits, including blueberries, raspberries, strawberries, and peaches. **Conclusion** Tiramisu is simple and delicious dessert that can be enjoyed by people of all ages. By following these tips, you can make delicious tiramisu that will impress your friends and family for years to come. Table 2. Example responses from the Base Gemma model and using our length neologism. 13 We Cant Understand AI Using Our Existing Vocabulary Figure 7. Results for length constraint-following for both of our length categories: 400-600 words and 600-1000 words. The base Gemma model fails to generate sufficiently long responses for either category. Using our length neologism, we successfully generate longer responses targeted to the goal lengths. skyline and baseline of Figure 6 are computed analytically. The Gemma curves are computed analytically based on the models empirical token probabilities for the numbers 19 (once for the default Gemma and once for the Gemma version trained with diversity neologism). B.3. Neologism for learning models preferences For each of 50 examples in the LIMA dataset, we sample 7 responses from Gemma. We then prompt Gemma to rate its own responses, using the following prompt: Your task is to take in an instruction and response, and rate how good the response is. The possible qualities are 1 (worst) to 5 (best). You should discuss your thoughts as to the rating, and then output score in well-formatted json. Output template: <your rationale> {{\"score\": your_score}} <begin instruction> {} <end instruction> <begin response> {} <end response> Out of the 7 samples, we take the highest-scoring as the chosen response, and the lowest-scoring as the rejected response. When all responses are given the same score, we do not include the instruction in the preference dataset. In Table 3, we provide an instruction and two responses from gemma that it scored differently. 14 We Cant Understand AI Using Our Existing Vocabulary Instruction: Why isnt the market dropping like stone with all the bad news? Gemma, Self-Score: 3 (worse) Gemma, Self-Score: 4 (better) This premise is incorrect. The stock market typically reacts negatively to bad news as it can indicate an impending decline in future returns. It is important to note that correlations do not imply causation. While there may be negative correlation between the stock market and negative news, it does not necessarily mean that the market is dropping due to the bad news. Other factors, such as economic indicators, corporate earnings, or interest rate movements, may also play role in determining market behavior. Table 3. Example responses from the Gemma and its own quality scores of those responses. This is indicative of broader trend where Gemma scores responses that disagree with premises of the question, or refuse to answer, lowly, even if warranted."
        }
    ],
    "affiliations": [
        "Google DeepMind"
    ]
}