{
    "paper_title": "S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability of Large Reasoning Models",
    "authors": [
        "Wenyuan Zhang",
        "Shuaiyi Nie",
        "Xinghua Zhang",
        "Zefeng Zhang",
        "Tingwen Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce S1-Bench, a novel benchmark designed to evaluate Large Reasoning Models' (LRMs) performance on simple tasks that favor intuitive system 1 thinking rather than deliberative system 2 reasoning. While LRMs have achieved significant breakthroughs in complex reasoning tasks through explicit chains of thought, their reliance on deep analytical thinking may limit their system 1 thinking capabilities. Moreover, a lack of benchmark currently exists to evaluate LRMs' performance in tasks that require such capabilities. To fill this gap, S1-Bench presents a set of simple, diverse, and naturally clear questions across multiple domains and languages, specifically designed to assess LRMs' performance in such tasks. Our comprehensive evaluation of 22 LRMs reveals significant lower efficiency tendencies, with outputs averaging 15.5 times longer than those of traditional small LLMs. Additionally, LRMs often identify correct answers early but continue unnecessary deliberation, with some models even producing numerous errors. These findings highlight the rigid reasoning patterns of current LRMs and underscore the substantial development needed to achieve balanced dual-system thinking capabilities that can adapt appropriately to task complexity."
        },
        {
            "title": "Start",
            "content": "S1-Bench: Simple Benchmark for Evaluating System 1 Thinking Capability of Large Reasoning Models Wenyuan Zhang*, Shuaiyi Nie, Xinghua Zhang, Zefeng Zhang, Tingwen Liu Institute of Information Engineering, Chinese Academy of Sciences School of Cyber Security, University of Chinese Academy of Sciences {zhangwenyuan,nieshuaiyi,liutingwen}@iie.ac.cn 5 2 0 2 4 1 ] . [ 1 8 6 3 0 1 . 4 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We introduce S1-Bench, novel benchmark designed to evaluate Large Reasoning Models (LRMs) performance on simple tasks that favor intuitive system 1 thinking rather than deliberative system 2 reasoning. While LRMs have achieved significant breakthroughs in complex reasoning tasks through explicit chains of thought, their reliance on deep analytical thinking may limit their system 1 thinking capabilities. Moreover, lack of benchmark currently exists to evaluate LRMs performance in tasks that require such capabilities. To fill this gap, S1-Bench presents set of simple, diverse, and naturally clear questions across multiple domains and languages, specifically designed to assess LRMs performance in such tasks. Our comprehensive evaluation of 22 LRMs reveals significant lower efficiency tendencies, with outputs averaging 15.5 times longer than those of traditional small LLMs. Additionally, LRMs often identify correct answers early but continue unnecessary deliberation, with some models even producing numerous errors. These findings highlight the rigid reasoning patterns of current LRMs and underscore the substantial development needed to achieve balanced dual-system thinking capabilities that can adapt appropriately to task complexity1."
        },
        {
            "title": "Introduction",
            "content": "Simplicity is the ultimate sophistication. Leonardo da Vinci Recent advances in Large Reasoning Models (LRMs), notably OpenAIs o1/o3 (OpenAI, 2024) and the DeepSeek-R1 (Guo et al., 2025) series, have propelled the development of Large Language Models (LLMs) by explicitly generating external chain-of-thought (COT) (Wei et al., 2022) before * denotes equal contribution. denotes corresponding author. 1The code and benchmark can be found in https://github.com /WYRipple/S1_Bench. producing final answers, achieving remarkable performance on complex reasoning tasks. Unlike traditional LLMs that primarily rely on intuitive, heuristic system 1 thinking, LRMs enhance their performance by employing analytical, deliberative system 2 thinking (Qu et al., 2025; Li et al., 2025b), which includes sophisticated strategies such as selfreflection, multi-path exploration (Li et al., 2025a; Yeo et al., 2025). Despite the benefits of system 2 thinking, rigid reliance on this cautious thinking mode can also introduce new limitations, such as performance degradation in specific contexts (Feng et al., 2025; Zhao et al., 2025) or overthinking (Chen et al., 2024; Kumar et al., 2025a). However, research gap exists in thoroughly investigating how LRMs over-reliance on system 2 thinking affects their performance when confronting extremely simple questions better suited for intuition-driven system 1 processing. Specifically, existing research primarily analyzes LRM behaviors in complex tasks that naturally align with deliberative system 2 thinking, such as MATH for math problems (Luo et al., 2025; Yang et al., 2025; Chen et al., 2024) or IFEval for instruction following (Zhao et al., 2025). Although some benchmarks involve relatively simple questions for human (Miao et al., 2021; Yan et al., 2025; Chiang and Lee, 2024), these tasks are often adversarial or non-trivial for LLMs, requiring moderate reasoning and can not be effectively solved through intuitive system 1 processing. Thus, benchmark to assess the system 1 thinking capability of LRMs is still lacking, further hindering our understanding of LRMs cognitive flexibility between the two systems (Ziabari et al., 2025; Qu et al., 2025). To fill this research gap, we introduce the System 1 Thinking Capability Benchmark (S1-Bench), which measures the performance of LRMs across various simple tasks that commonly encountered in real-world applications. S1-Bench has the following three characteristics: (1) Simple. The quesFigure 1: LRMs exhibit under-accuracy and overthinking on simple problems. Shapes represent organizations, colors represent base model families, with darker colors indicating larger models, and connecting lines represent the relationships between model families and training. tions are not hard for humans and can be easily answered by LLMs. LLMs with 7-9B parameters can robustly provide correct answers through direct responses when sampled across multiple temperatures. (2) Diverse. S1-Bench is not limited to simple reasoning problems; it encompasses four major categories and 28 subcategories in two languages (English and Chinese), including commonsense knowledge, loosely constrained instruction following, and straightforward analytical problems. (3) Natural. The questions are clear, without any misleading elements or ambiguities, ensuring they can be answered intuitively. S1-Bench conducted extensive evaluations across 22 LRMs, yielding the following key findings: (1) Current LRMs exhibit inefficiency and lack system 1 thinking capabilities, with average output lengths 15.5 times longer than small LLMs. (2) LRMs often arrive at correct answers early in their thinking process but continue unnecessary reasoning with gradually increasing content similarity. (3) Despite employing deep reasoning, LRMs exhibit accuracy degradation on simple questions compared to traditional LLMs. (4) LRMs show the ability to prejudge the simplicity of questions, yet they still exhibit inefficiency. These findings emphasize the significant distance LRMs must traverse to become powerful dual-system compatible models. Our contributions can be summarized as follows: To the best of our knowledge, S1-Bench is the first benchmark to evaluate the system 1 thinking capabilities of LRMs. We introduce workflow for constructing simple dataset suitable for system 1 evaluation. Extensive experiments reveal the inefficiency and under-accuracy of LRMs on simple questions that are suited for system 1 thinking."
        },
        {
            "title": "2.1 Large Reasoning Models",
            "content": "Large Reasoning Models (LRMs), characterized by explicitly generating external thinking processes before final answers (Kumar et al., 2025b; Chen et al., 2025), achieve paradigm shift from intuitive system 1 thinking to deliberative system 2 reasoning compared to traditional LLMs (Li et al., 2025b; Qu et al., 2025), thus achieving superior performance on complex tasks. The development of recent LRMs has largely followed two main approaches: large-scale reinforcement learning (RL) and model distillation. Models trained via largescale RL (Guo et al., 2025; Team, 2025b; Team et al., 2025) leverage reward-based optimization to gradually incentivize deliberative reasoning. In contrast, distillation-based LRMs (OpenAI, 2024; Min et al., 2024; Team, 2025a; Ye et al., 2025; 2 Figure 2: Construction workflow for S1-Bench and an illustrative example from each major category. Muennighoff et al., 2025) acquire such abilities by transferring structured reasoning patterns from advanced teacher models."
        },
        {
            "title": "3.1 How to Ensure Simplicity?",
            "content": "We ensure questions are simple and suitable for system 1 thinking through two aspects."
        },
        {
            "title": "2.2 Limitations of LRMs",
            "content": "While LRMs have shown significant performance gains through deliberate reasoning, rigid adherence to this overly cautious thinking can introduce new limitations. On one hand, intermediate reasoning steps can cause excessive token generation and unnecessary solving attempts (Chen et al., 2024; Hashemi et al., 2025; Kumar et al., 2025a). On the other hand, LRMs performance can drop in specific contexts like safety scenarios (Jiang et al., 2025; Zhao et al., 2025) and role-playing (Feng et al., 2025). However, prior studies mainly evaluated LRMs on complex tasks that more suited for deliberative system 2 thinking. Our work examines how deliberative reasoning impacts extremely simple problems better matched to intuition-driven system 1 processing."
        },
        {
            "title": "3 S1-Bench",
            "content": "We introduce S1-Bench, bilingual, multi-domain benchmark designed to evaluate system 1 thinking capability of LRM on extremely simple questions. These questions are easily solvable by traditional LLMs and not hard for humans. S1-Bench, which covers both English and Chinese, is organized into four major categories: reasoning (RSN), knowledge (KNO), instruction following (IF) and analysis (ANA), representing major dimensions commonly employed in LLM capability evaluation (Zheng et al., 2023; Chang et al., 2024). This section begins with how simplicity is ensured, then the detailed construction workflow for S1-Bench, and concludes with an overview of the dataset statistics. Figure 2 shows the construction workflow and an illustrative example per category."
        },
        {
            "title": "3.1.1 A Priori Simplicity Constraints",
            "content": "We require all questions and answers to adhere to the following guidelines: (1) Questions must be naturally, clearly expressed, and unambiguous, free of intentional traps. (2) Answers must be unique or easily falsifiable (such as providing three-letter English word). Weve further implemented category-specific constraints to ensure simplicity: RSN: Limited to problems solvable with minimal reasoning or intuition. KNO: Restricted to common knowledge with unique, verifiable answers from sources like Wikipedia. IF: Involve straightforward instructions without strict formatting requirements. ANA: Limited to questions whose answers can be directly inferred from the prompt, such as binary classification. These constraints ensure all questions remain straightforward for human respondents."
        },
        {
            "title": "3.1.2 A Posteriori Simplicity Verification",
            "content": "Due to the biases existing between language models and humans (Gallegos et al., 2024), questions that are simple for humans may be difficult for LLMs. Therefore, we introduce additional posteriori verification to ensure that questions are simple enough to be correctly and robustly answered by smaller LLMs from different families. This alleviates cognitive biases in traditional LLMs for system 1 thinking problems (Li et al., 2025b)."
        },
        {
            "title": "3.2 Construction Workflow",
            "content": "To implement priori constraints, we introduce data generators2 and quality discriminators3 that 2We select Claude-3.7-Sonnet and Qwen2.5-72B-Instruct. 3We select GPT-4o and DeepSeek-V3-241226. 3 adhere to the priori settings. To ensure posterior verification, we introduce simplicity validators4 and correctness evaluator5. The generators, discriminators, and evaluator are uniformly configured with temperature 0.7 and top-p=0.9, while validators are set with temperatures of 0.0, 0.2, and 0.4, each sampling 10 times. Additionally, we recruit three experienced graduate students as annotators, familiar with LLMs and fully comprehending the construction objectives of S1-Bench. For the prompts used in the workflow, refer to Table 11. Subcategory Preparation. Initially, to ensure diversity, we draw on existing benchmark classification methods (e.g., MMLU, IFEval and GSM8K) and evaluation surveys (Chang et al., 2024), selecting or merging existing subcategories, and designing new ones. We ensure that all subcategories meet the requirements for simplicity and provide an example question for each. Generation and Modification. We use two data generators to create initial questions and answers for each candidate subcategory. The generation process adheres to prompt requirements (refer to Table 11), ensuring bilingual content that aligns with subcategory definitions while maintaining diverse perspectives. Each question is accompanied by auxiliary answers, with 50 initial bilingual questionanswer pairs generated per subcategory. These pairs are independently assessed by three annotators and two quality discriminators , who ensure that the questions are unambiguous, have clear boundaries, and admit only one correct answer. For each question-answer pair, five independent assessments are conducted. Based on these evaluations, annotators discuss and collectively decide whether to retain, modify, or discard each question, ensuring alignment with the requirements outlined in Section 3.1.1. Retained questions are submitted to validators for testing, and an evaluator assesses the accuracy of responses. Questions with correct answers are included in S1-Bench, while others undergo reprocessing. Questions with errors in any of the 30 sampling iterations undergo an iterative difficulty reduction process. These questions return to generators with specific prompts to decrease complexity, followed by the same discrimination, discussion, and evaluation procedures. Questions failing to achieve errorfree performance after three difficulty reduction 4We select four small LLMs: Qwen2.5-7B, Llama3.1-8B, Mistral8B, and Gemma2-9B. The full model IDs are detailed in Table 4. 5We select GPT-4o. Figure 3: Statistical distribution of token counts for S1Bench questions. Model t=0.0 t=0. t=0.4 Tokens Gemma2-9B Llama3.1-8B Mistral-8B Qwen2.5-7B Qwen2.5-14B Qwen2.5-32B Qwen2.5-72B Llama3.3-70B DeepSeek-v3 100.00 100.00 100.00 100.00 99.74 99.98 100.00 100.00 100. 100.00 100.00 100.00 100.00 99.76 99.98 100.00 99.76 100.00 100.00 100.00 100.00 100.00 99.76 99.98 100.00 99.76 100.00 38.77 42.00 44.38 42.81 40.00 43.17 44.61 53.71 79. Table 1: Average accuracy and response token count of different LLMs, each sampled 10 times at three temperature settings. iterations are excluded from the workflow. The final S1-Bench comprises questions that satisfy both human-established priori simplicity constraints and posteriori simplicity verification."
        },
        {
            "title": "3.3 Benchmark Statistics",
            "content": "S1-Bench comprises 422 question-answer pairs across four major categories and 28 subcategories, balanced with 220 English and 202 Chinese questions. We present the category distribution and descriptions with examples for all subcategories, which can be found in Figure 8 and Table 15,16 in the appendix. Figure 3 shows the token length distribution, with questions averaging 14.46 tokens. To validate the absence of response bias from validators, we tested S1-Bench on five LLMs that serve as base models for commonly used LRMs. As shown in Table 1, these models achieved an accuracy exceeding 99% under three temperature settings, confirming the simplicity and robustness of S1-Bench."
        },
        {
            "title": "4.1 Baseline Models and Configurations",
            "content": "We evaluated 22 different LRMs, which were explicitly trained to first respond with thinking process, and then generate final answer. These LRMs include open-source families, such as DeepSeek (Guo et al., 2025), Qwen (Team, 2025b), 4 Model ID Size Loose Format Strict Format pass@1 acc@k pass@1 acc@k L-Corr S-Corr ART Validator LLMs 7-9B 100.00 100.00 100. 100.00 42.00 QwQ-32B Hunyuan-T1 Sky-T1-32B DS-R1 DS-R1-70B DS-R1-32B DS-R1-14B DS-R1-8B DS-R1-7B DS-R1-1.5B Nemotron-49B Nemotron-8B L-R1-32B L-R1-32B-DS L-R1-14B-DS L-R1-7B-DS s1.1-32B s1.1-14B s1.1-7B EXAONE-32B EXAONE-7.8B EXAONE-2.4B 32B 32B 671B 70B 32B 14B 8B 7B 1.5B 49B 8B 32B 32B 14B 7B 32B 14B 7B 32B 7.8B 2.4B 100.00 99.91 98.82 100.00 99.48 99.72 99.57 97.44 95.21 81.47 99.15 86.16 97.87 99.57 99.05 94.64 99.53 97.63 96.68 97.06 88.15 72.42 100.00 99.53 94.79 100.00 97.39 98.82 97.87 97.16 85.78 54.50 97.39 69.91 91.00 98.10 95.97 83.65 98.34 93.60 88.39 94.08 75.12 56.16 100.00 99.91 94.88 100.00 99.38 99.72 99.57 97.39 95.21 81.47 99.15 79.81 94.74 99.57 99.05 94.64 99.48 97.25 88.58 97.06 87.82 72.32 100.00 99.53 79.62 100.00 96.92 98.82 97.87 97.16 85.78 54.50 97.39 59.00 79.62 98.10 95.97 83.65 98.10 91.94 63.98 94.08 74.41 56.16 100.00 100.00 99.48 100.00 100.00 100.00 100.00 99.76 99.24 97.58 100.00 99.43 98.91 99.81 99.19 99.76 99.57 97.77 97.11 99.81 98.72 97.44 100.00 100.00 95.26 100.00 99.91 100.00 100.00 99.53 99.24 97.58 100.00 84.31 95.07 99.81 99.19 99.67 99.53 97.39 88.96 99.81 98.06 97. 720.10 542.31 163.00 646.40 453.81 429.91 475.46 452.11 454.55 489.54 362.54 372.57 1095.36 524.12 693.19 496.47 998.00 839.86 711.49 800.56 1046.87 1593.96 Table 2: Main results in the top-p sampling setting on the S1-Bench, sorted by model family. Bold teal marks best performance, teal second best, bold burgundy worst, and burgundy second worst. Nemotron (Chris Alexiuk and Patel, 2025), LightR1 (Wen et al., 2025), s1.1 (Muennighoff et al., 2025), EXAONE (Research et al., 2025), and SkyT1 (Griggs et al., 2025), as well as closed-source Hunyuan-T1 (Tencent, 2025), spanning from tiny (1.5B) to large (671B) parameter sizes6. Notably, OpenAIs o-series models are not included as they do not disclose thinking processes to users. For each model, we consider two sets of generation configurations: Greedy sampling with temperature = 0; Top-p sampling with temperature = 0.6, top-p = 0.95 and sampling size k=5."
        },
        {
            "title": "4.2 Evaluation Metrics",
            "content": "Format Metrics. Generally, the output of LRMs should be separated by an end thinking marker (e.g., </think>) that distinguishes the thinking process from the final answer. However, LRMs do not always generate responses in correct format. To assess this, we calculate the percentage of responses that meet the required formatting criteria, categorized as follows (for top-p sampling setting, each metric is averaged across 5 generations): S-Corr (Strict Format Correctness Rate): The percentage of responses that strictly adhere to the format, meaning they only contain one \"end thinking marker\" followed by non-empty final answer. L-Corr (Loose Format Correctness Rate): The percentage of responses that loosely adhere to the format, meaning they meet the format requirements except for responses with endless repetitions7. Efficiency Metrics. We calculate the Average Response tokens (ART), which represent the average number of tokens in responses that meet the loose formatting requirements. Token counts are obtained using the Qwen2.5 tokenizer. Accuracy Metrics. We calculate accuracy metrics for both strict and loose formatting requirements, respectively. We use GPT-4o as the evaluator to assess the correctness of the responses8, with the evaluation prompt in Table 11. For greedy sampling, we directly calculate the accuracy rate, with complete results in Appendix D. For top-p sampling, we utilize two metrics: Pass@1: Followed DeepSeek-R1 (Guo et al., 2025), we calculate pass@1 to assess the percentage of correct responses among the k=5 generations. Specifically, it is defined as: pass@1 = 1 k (cid:88) i=1 pi, (1) 6More details in Appendix Table 5 (includes training data scale and algorithms). 7Detailed format analyses are in the Appendix C. 8If final answer can be isolated, only the final answer is evaluated; otherwise, the entire response is assessed. 5 Model ID Size RSN KNO S1-Bench-EN IF ANA Gemma2-9B Llama3.1-8B Qwen2.5-7B Mistral-8B Column Avg 9B 8B 7B 8B - 74.8 91.0 65.5 67.2 74.6 29.4 35.4 46.3 55.5 41.6 32B 49B 8B 32B 8B 70B 7B 14B 1.5B 7B 32B 174.1 215.8 Sky-T1-32B Nemotron-49B 587.6 599.7 585.1 561.0 Nemotron-8B 504.4 421.8 DS-R1-32B DS-R1-8B 528.9 472.2 501.3 464.1 DS-R1-70B 623.9 447.5 DS-R1-7B DS-R1-14B 674.7 503.7 584.7 480.8 DS-R1-1.5B 667.1 568.1 L-R1-7B-DS L-R1-32B-DS 706.6 574.5 693.8 561.6 Hunyuan-T1 723.8 671B 786.1 DS-R1 L-R1-14B-DS 1026.0 951.0 14B 840.8 1039.5 7B s1.1-7B 32B 808.1 873.3 QwQ-32B 32B 1323.7 1057.6 EXAONE-32B 746.2 14B 871.8 s1.1-14B s1.1-32B 889.7 32B 1077.9 EXAONE-7.8B 7.8B 1498.3 1398.9 L-R1-32B 32B 1614.0 1217.3 EXAONE-2.4B 2.4B 1927.3 1426.2 5.3 12.4 6.4 8.6 8.2 98.5 396.5 458.0 414.7 530.7 378.5 353.8 367.3 417.4 501.7 647.6 380.9 711.4 829.8 1923.2 520.8 1537.0 2233.1 2055.4 1775.7 1996.9 1200.1 52.4 61.9 49.6 50.1 53. 233.3 526.1 303.1 521.1 462.7 536.1 510.0 494.2 577.2 566.3 632.8 435.0 529.2 653.5 529.4 634.7 711.6 708.1 781.7 882.4 930.1 825.7 Avg 45.9 56.0 46.5 49.6 49.5 RSN KNO S1-Bench-ZH IF ANA 51.6 44.0 50.5 47.3 48.3 19.8 28.3 46.6 56.1 37. 7.5 15.2 9.8 14.8 11.8 35.1 18.7 36.9 29.7 30.1 Avg 31.0 26.7 38.8 38. 33.8 Avg 38.8 42.0 42.8 44.4 42.0 163.0 99.4 125.5 194.3 362.5 235.5 232.9 540.4 372.6 288.1 369.5 462.6 429.9 343.1 362.2 473.7 452.1 266.2 521.9 491.2 453.8 328.4 450.8 484.0 454.5 339.5 446.5 495.5 475.5 375.3 452.0 519.0 489.5 329.8 493.0 529.1 496.5 344.1 444.8 580.3 524.1 377.1 431.2 636.3 542.3 505.1 676.8 521.2 646.4 607.9 727.3 672.5 693.2 442.2 594.7 848.2 711.5 1034.3 489.6 929.9 720.1 613.3 866.9 722.4 800.6 1302.9 703.2 1086.4 839.9 1512.6 654.6 960.2 998.0 1634.6 1081.7 995.6 1046.9 1633.1 1303.8 1410.3 1338.3 1035.6 1095.4 1240.7 1320.7 2469.7 1622.6 2471.6 1511.2 1898.7 1594.0 128.9 168.8 273.5 382.2 409.4 420.9 409.4 428.0 446.0 405.0 402.2 565.3 617.9 525.7 475.6 717.6 490.3 710.7 906.5 767.0 835. 125.3 157.3 326.0 385.6 404.4 450.2 463.2 465.4 497.4 454.6 367.0 553.8 638.5 610.1 351.3 707.3 348.6 546.0 765.2 497.8 737.7 145.5 107.8 166.7 408.8 395.5 416.7 373.0 405.8 423.1 366.4 418.7 523.8 533.9 451.7 332.4 667.7 125.5 579.7 666.5 205.0 610.2 Column Avg Improvement - - 810.6 652.8 679.8 10.9 18.5 109.7 11.0 14.9 14.1 13.8 62.7 14.8 16.7 15.5 736. 447.1 742.0 563.0 896.8 771.2 590. 521.6 Table 3: ART on the S1-bench across two languages and four main categories. Bold teal marks best performance, teal second best, bold burgundy worst, and burgundy second worst. Bold represents the maximum Improvement value for each language. where pi is the correctness of the i-th generation. Acc@k: Since S1-Bench is composed of extremely simple questions, we calculate acc@k. Specifically, acc@k=1 if all responses are correct and acc@k = 0 otherwise. It is defined as: acc@k = (cid:89) i= pi, (2) Notably, S-Corr represents the upper bound for pass@1 and acc@5 under strict formatting requirements, with L-Corr following the same principle."
        },
        {
            "title": "4.3 Main Results",
            "content": "Table 2 and Figure 1 presents the main results of LRMs on S1-Bench, revealing two key phenomena. LRMs exhibit significantly lower efficiency than LLMs on S1-Bench, and no clear correlation is observed between ART and model size. Notably, state-of-the-art LRMs, such as DeepSeek-R1 and QwQ-32B, do not demonstrate distinct advantage in efficiency. In contrast, Sky-T1-32B, which undergoes specific optimizations to mitigate overthinking using SimPO, achieves the highest efficiency. The L-R1-DS 7B/14B/32B models generate longer responses than the DS-R17B/14B/32B models, and the former are further post-trained on the latter. This suggests that further post-training may improve reasoning ability for complex tasks, but at the cost of response efficiency for simple tasks. Finally, the s1.1 models generate considerably longer responses than the DeepSeekR1-Distilled models. Despite both models being trained solely with SFT to acquire long-COT reasoning ability, the DeepSeek-R1-Distilled models use 800K training samples, while the s1.1 models are trained on only 1K. This discrepancy suggests that the smaller training set may lead to superficial imitation of long reasoning patterns, resulting in verbose thinking on simple questions. Several LRMs exhibit under-accuracy on simple questions. Our observations reveal that, despite employing deep reasoning, most LRMs tend to exhibit lower accuracy on simple questions compared to traditional LLMs. For example, DS-R11.5B and EXAONE-2.4B achieve just above 50% acc@k. Moreover, accuracy decreases with smaller 6 notably longer in the subcategories of length constraints, character constraints, and sentence constraints. These three question types share similar characteristic: their correctness is verifiable, but the solution space is vast. We find that, although the model quickly identifies correct answer, it becomes trapped in the search space, continually exploring alternatives and failing to stop in time. This phenomenon is more pronounced in families with higher ART, such as s1.1 and the EXAONE. LRMs also demonstrate significant increase in ART for reasoning tasks. In Table 3, ART for reasoning questions also increases significantly, and Figure 9 further corroborates this phenomenon in ART in various reasoning subcategories. One possible explanation for this is that reasoning questions align closely with the post-training data distribution of LRMs, thereby further stimulating the long-chain reasoning behavior of LRMs. LRMs consistently exhibit inefficiency in both Chinese and English. In table 3, most LRMs show higher ART on English questions, while QwQ32B and Hunyuan-T1 exhibit comparable ART across Chinese and English."
        },
        {
            "title": "5.2 Solution Analysis in Thinking Processes",
            "content": "To better understand the causes of inefficiency in LRMs on the S1-Bench, we analyze the thinking process where the final answer is correct and the format is strictly correct and non-empty9. We begin by segmenting each thinking process into several solutions, each defined as point at which LRMs explicitly arrive at conclusion that directly aligns with the correct answer. The segmentation process is performed by DeepSeek-v3, with prompts detailed in Table 12. We then compute the average initial thinking cost for LRMs. For each sample, if the thinking process contains at least one solution, the cost is defined as the tokens counts in the first solution. If no clear and correct solution is provided, the cost is the total token counts in the thinking process. The reported value is the average across all selected samples. We find that: The step-by-step reasoning in the initial thinking process is not the primary cause of the inefficiency in LRMs. As shown in Figure 4 (a), although the total counts of thinking tokens varies considerably across LRMs, their initial thinking 9We selected this subset because incorrect answers make it unclear whether LRMs are over-reasoning or under-reasoning, and malformed or empty thinking processes cannot be precisely extracted. Figure 4: (a) Comparison of Initial and Additional Thinking Costs for Each LRM. (b) Distribution of Solution Rounds for Each LRM. model size. Finally, many LRMs struggle with robust correctness in top-p sampling, where acc@k is significantly lower than pass@1. This issue is particularly pronounced in smaller LRMs. For instance, DS-R1-1.5B achieved 81.47% pass@1 but only 54.50% acc@k."
        },
        {
            "title": "5.1 ART Analysis of Question Types",
            "content": "To better understand the efficiency differences of LRMs across question types, we analyze ART across 4 main categories, 28 subcategories, and two languages, yielding the following insights. LRMs exhibit the most significant increase in ART for instruction-following questions and tend to over-explore when the solution space is vast. Although small LLMs generate the most concise responses for instruction-following questions across all main categories in S1-Bench, LRMs produce responses that are 109.7 longer in Chinese and 62.7 longer in English, making them the longest response type across all categories. We further analyze the subcategories of instructionfollowing questions. As shown in Figure 9, ART is 7 Figure 5: Maximum similarity between each segment and all preceding segments for LRMs across four categories. all-MiniLM-L6-v2 model11. For each segment, we calculate the cosine similarity with all its preceding segments and use the maximum similarity as measure of its information redundancy. As shown in Figure 5, information redundancy increases across all four main categories as reasoning sequences increase. Sky-T1-32B shows overall lower similarity, which stems from its shorter thinking process, but still demonstrates an upward trend."
        },
        {
            "title": "6 Analysis of Errors",
            "content": "Figure 7 explores the relationship between thinking process quality and final answer accuracy. We focus on samples whose format is strictly correct and thinking process is non-empty. For samples with correct final answers, we categorize them based on whether the thinking process contains explicit incorrect conclusions in intermediate steps. For samples with incorrect final answers, we categorize them based on whether the correct answer is mentioned at least once during reasoning. We use DeepSeek-v3 for categorization, with the prompt provided in Table 13. Figure 6 presents the distribution of these four categories in different RLMs. Our analysis yields the following observations: (1) LRMs with lower accuracy often include incorrect intermediate conclusions in their reasoning, even when they ultimately reach correct final answers (light green). (2) Although LRMs sometimes reach the correct answer during reasoning, they may deviate and ultimately produce incorrect conclusions (light red)."
        },
        {
            "title": "7 Simplicity Prejudgement",
            "content": "Finally, we discover an intriguing phenomenon: LRMs can prejudge certain simple questions. Specifically, we extract the first 50 tokens from Figure 6: Distribution of the thinking process across four categories. FA and TP refer to Final Answer and Thinking Process, respectively. Green bars indicate cases where the final answer is correct, while red bars indicate cases where it is incorrect. costs are similar and account for only small fraction of the total. Generating unnecessary solution rounds after reaching the correct answer is one of the reasons for the inefficiency of LRMs. We further examine the distribution of solution rounds among various LRMs on the S1-Bench (Figure 4 (b)) and find that models with longer thinking processes tend to produce excessive solution rounds, repeatedly reverifying simple problems that have already been solved. This redundant verification significantly contributes to computational inefficiency."
        },
        {
            "title": "5.3 Redundancy in Thinking Processes",
            "content": "Information redundancy increases as reasoning sequences increase. We conduct similarity analysis to analyze how information redundancy in the thinking processes of LRMs changes as reasoning sequences increase. Specifically, we first divide the complete thinking process into equal-length segments10. Then, we encode each segment using the 10We set k=15, changing its value does not affect the conclusions. 11https://huggingface.co/sentence-transformers/allMiniLM-L6-v2 8 for system 1 thinking. Experiments reveal significant overthinking and performance degradation by LRMs on S1-Bench, highlighting the gap between current LRMs and dual-system compatibility."
        },
        {
            "title": "References",
            "content": "Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, and 1 others. 2024. survey on evaluation of large language models. ACM transactions on intelligent systems and technology, 15(3):145. Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wangxiang Che. 2025. Towards reasoning era: survey of long chain-of-thought for reasoning large language models. arXiv preprint arXiv:2503.09567. Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, and 1 others. 2024. Do not think that much for 2 + 3 =? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187. Cheng-Han Chiang and Hung-yi Lee. 2024. Overreasoning and redundant calculation of large language models. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 2: Short Papers), pages 161169, St. Julians, Malta. Association for Computational Linguistics. Zhilin Wang Chris Alexiuk, Tanay Varshney and Chintan Patel. 2025. Build enterprise ai agents with advanced open nvidia llama nemotron reasoning models. https://developer.nvidia.com/blog/bu ild-enterprise-ai-agents-with-advanced-o pen-nvidia-llama-nemotron-reasoning-model s/. Xiachong Feng, Longxu Dou, and Lingpeng Kong. 2025. Reasoning does not necessarily improve roleplaying ability. arXiv preprint arXiv:2502.16940. Isabel Gallegos, Ryan Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen Ahmed. 2024. Bias and fairness in large language models: survey. Computational Linguistics, 50(3):1097 1179. Tyler Griggs, Shiyi Cao, Dacheng Li, Shu Liu, Shishir G. Patil, Matei Zaharia, Joey Gonzalez, and Ion Stoica. 2025. Think less, achieve more: Cut reasoning costs by 50% without sacrificing accuracy. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Incentivizing reasoning capability DeepSeek-R1: Figure 7: Counting instances of prejudgment generated by LRMs for simple questions and ART. each thinking process and use DeepSeek-v3 to identify sentences showing judgment, further extracting judgment words12. Figure 7 displays the frequency of words judging questions as simple, with details provided in the Table 7. Our findings reveal: LRMs possess the ability to prejudge question simplicity, especially in Chinese. All LRMs exhibit prejudgment phenomena within their thinking processes, demonstrating an ability to directly assess question difficulty. Additionally, 14 LRMs exhibit significantly stronger prejudgment tendencies in Chinese, with this phenomenon concentrating primarily within the L-R1, DS-R1, QwQ, and Hunyuan model families. Even with prejudgment, the thinking length of LRMs does not shorten. As shown in Figure 7, the average ART of the thinking process exhibiting prejudgment does not decrease. We consider further exploration of this phenomenon as an important direction for future work. These results suggest LRMs possess an inherent understanding of question difficulty. This opens novel pathway toward dual-system compatibility for LRMs, which we identify as future work."
        },
        {
            "title": "8 Conclusion",
            "content": "This paper presents S1-Bench, designed to evaluate LRMs performance on simple questions suited 12An example of prejudgment: This question seems simple. For prompt references, see Table 14. 9 in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. Masoud Hashemi, Oluwanifemi Bamgbose, Sathwik Tejaswi Madhusudhan, Jishnu Sethumadhavan Nair, Dna Aman Tiwari, and Vikas Yadav. 2025. bench: When silence is smarterbenchmarking over-reasoning in reasoning llms. arXiv preprint arXiv:2503.15793. Fengqing Jiang, Zhangchen Xu, Yuetai Li, Luyao Niu, Zhen Xiang, Bo Li, Bill Yuchen Lin, and Radha Poovendran. 2025. Safechain: Safety of language models with long chain-of-thought reasoning capabilities. arXiv preprint arXiv:2502.12025. Abhinav Kumar, Jaechul Roh, Ali Naseh, Marzena Karpinska, Mohit Iyyer, Amir Houmansadr, and Eugene Bagdasarian. 2025a. Overthink: Slowdown attacks on reasoning llms. arXiv e-prints, pages arXiv 2502. Komal Kumar, Tajamul Ashraf, Omkar Thawakar, Rao Muhammad Anwer, Hisham Cholakkal, Mubarak Shah, Ming-Hsuan Yang, Phillip HS Torr, Salman Khan, and Fahad Shahbaz Khan. 2025b. Llm post-training: deep dive into reasoning large language models. arXiv preprint arXiv:2502.21321. Dacheng Li, Shiyi Cao, Tyler Griggs, Shu Liu, Xiangxi Mo, Eric Tang, Sumanth Hegde, Kourosh Hakhamaneshi, Shishir Patil, Matei Zaharia, and 1 others. 2025a. Llms can easily learn to reason from demonstrations structure, not content, is what matters! arXiv preprint arXiv:2502.07374. Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, and 1 others. 2025b. From system 1 to system 2: survey of reasoning large language models. arXiv preprint arXiv:2502.17419. Haotian Luo, Li Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, and Dacheng Tao. 2025. O1-pruner: Lengthharmonizing fine-tuning for o1-like reasoning pruning. arXiv preprint arXiv:2501.12570. Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. 2021. diverse corpus for evaluating and developing english math word problem solvers. arXiv preprint arXiv:2106.15772. Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang, Xiaoxue Cheng, Huatong Song, and 1 others. 2024. Imitate, explore, and self-improve: reproduction report on slow-thinking reasoning systems. arXiv preprint arXiv:2412.09413. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. 2025. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393. OpenAI. 2024. Learning to reason with LLMs. https: //openai.com/index/learning-to-reason-wit h-llms/. Xiaoye Qu, Yafu Li, Zhaochen Su, Weigao Sun, Jianhao Yan, Dongrui Liu, Ganqu Cui, Daizong Liu, Shuxian Liang, Junxian He, and 1 others. 2025. survey of efficient reasoning for large reasoning models: Language, multimodality, and beyond. arXiv preprint arXiv:2503.21614. LG Research, Kyunghoon Bae, Eunbi Choi, Kibong Choi, Stanley Jungkyu Choi, Yemuk Choi, Seokhee Hong, Junwon Hwang, Hyojin Jeon, Kijeong Jeon, and 1 others. 2025. Exaone deep: Reasoning enhanced language models. arXiv preprint arXiv:2503.12524. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, and 1 others. 2025. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599. NovaSky Team. 2025a. Sky-t1: Train your own o1 preview model within $450. https://novasky-ai. github.io/posts/sky-t1. Qwen Team. 2025b. QwQ-32b: Embracing the power of reinforcement learning. https://qwenlm.githu b.io/blog/qwq-32b/. Tencent. 2025. Reasoning efficiency redefined! meet tencents hunyuan-t1the first mamba-powered ultra-large model. https://llm.hunyuan.tencen t.com/#/Blog/hy-t1/. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, and 1 others. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824 24837. Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, and 1 others. 2025. Light-R1: Curriculum sft, dpo and rl for long cot from scratch and beyond. arXiv preprint arXiv:2503.10460. Kai Yan, Yufei Xu, Zhengyin Du, Xuesong Yao, Zheyu Wang, Xiaowen Guo, and Jiecao Chen. 2025. Recitation over reasoning: How cutting-edge language models can fail on elementary school-level reasoning problems? arXiv preprint arXiv:2504.00509. Wenkai Yang, Shuming Ma, Yankai Lin, and Furu Wei. 2025. Towards thinking-optimal scaling of test-time compute for llm reasoning. arXiv preprint arXiv:2502.18080. Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. 2025. LIMO: Less is more for reasoning. arXiv preprint arXiv:2502.03387. 10 Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. 2025. Demystifying long chain-of-thought reasoning in llms. arXiv preprint arXiv:2502.03373. Weixiang Zhao, Xingyu Sui, Jiahe Guo, Yulin Hu, Yang Deng, Yanyan Zhao, Bing Qin, Wanxiang Che, TatSeng Chua, and Ting Liu. 2025. Trade-offs in large reasoning models: An empirical analysis of deliberative and adaptive reasoning over foundational capabilities. arXiv preprint arXiv:2503.17979. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging LLM-as-a-judge with MT-bench and chatbot arena. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Alireza Ziabari, Nona Ghazizadeh, Zhivar Sourati, Farzan Karimi-Malekabadi, Payam Piray, and Morteza Dehghani. 2025. Reasoning on spectrum: Aligning llms to system 1 and system 2 thinking. arXiv preprint arXiv:2502.12470. Figure 8: S1-Bench Category Display. The inner circle represents four major categories, and the outer circle includes 28 subcategories. Subcategories in S1-Bench Figure 8 shows the pie chart distribution of 28 subcategories in S1-Bench. For more details on the subcategories, please refer to Table 15,16."
        },
        {
            "title": "B Baseline Model Details",
            "content": "Table 4 presents the abbreviations, IDs, and URLs of LLMs used in this paper. Table 5 displays the abbreviations, IDs, URLs, organizations, algorithms, and training data volumes of open-source LRMs evaluated in this study."
        },
        {
            "title": "C Format Details",
            "content": "C.1 Division of Different Format This section presents comprehensive taxonomy of format errors and emphasizes the importance of addressing these issues in future research endeavors. Unlike conventional LLMs, LRMs frequently exhibit format errors, manifested when responses fail to adhere to the requirement of generating single End Thinking Marker (ETM). This inconsistency creates significant challenges in distinguishing between the thinking process and the final answer. Format errors represent distinct category from over-thinking and under-accuracy phenomena, yet similarly indicate vulnerability to elementary reasoning traps. As illustrated in Table 6, we have identified and classified 12 distinct format types, each assigned unique ID. These format types can be broadly 11 Model Model ID URL Qwen2.5-7B Llama3.1-8B Mistral-8B Gemma2-9B Qwen2.5-14B Qwen2.5-32B Qwen2.5-72B Llama3.3-70B DeepSeek-v3 Qwen2.5-7B-Instruct Llama-3.1-8B-Instruct Ministral-8B-Instruct-2410 gemma-2-9b-it https://huggingface.co/Qwen/Qwen2.5-7B-Instruct https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct https://huggingface.co/mistralai/Ministral-8B-Instruct-2410 https://huggingface.co/google/gemma-2-9b-it Qwen2.5-14B-Instruct Qwen2.5-32B-Instruct Qwen2.5-72B-Instruct Llama-3.3-70B-Instruct DeepSeek-V3https://huggingface.co/Qwen/Qwen2.5-14B-Instruct https://huggingface.co/Qwen/Qwen2.5-32B-Instruct https://huggingface.co/Qwen/Qwen2.5-72B-Instruct https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct https://huggingface.co/deepseek-ai/DeepSeek-V3-0324 Table 4: Mapping of LLM abbreviations and IDs used in this paper, with their open-source URLs. Model ID DeepSeek Abbreviation Base Model Alg. Size DeepSeek-R1-Distill-Qwen-1.5B DeepSeek-R1-Distill-Qwen-7B DeepSeek-R1-Distill-Llama-8B DeepSeek-R1-Distill-Qwen-14B DeepSeek-R1-Distill-Qwen-32B DeepSeek-R1-Distill-Llama-70B DeepSeek-R1 DS-R1-1.5B DS-R1-7B DS-R1-8B DS-R1-14B DS-R1-32B DS-R1-70B DS-R1 Qwen2.5-Math-1.5B Qwen2.5-Math-7B Llama-3.1-8B Qwen2.5-14B Qwen2.5-32B Llama-3.3-70B-Instruct DeepSeek-V3-0324 SFT SFT SFT SFT SFT SFT SFT&RL QwQ-32B Qwen2.5-32B L-R1-7B-DS L-R1-14B-DS L-R1-32B-DS L-R1-32B DeepSeek-R1-Distill-Qwen-7B DeepSeek-R1-Distill-Qwen-14B SFT&RL DeepSeek-R1-Distill-Qwen-32B SFT Qwen2.5-32B-Instruct SFT SFT&DPO s1.1-7B s1.1-14B s1.1-32B Qwen2.5-7B-Instruct Qwen2.5-14B-Instruct Qwen2.5-32B-Instruct SFT SFT SFT 800K 800K 800K 800K 800K 800K 800K& 3K 3K& 3K 73K& 1K 1K 1K Qwen QwQ-32B qihoo360 Light-R1-7B-DS Light-R1-14B-DS Light-R1-32B-DS Light-R1-32B simplescaling s1.1-7B s1.1-14B s1.1-32B LG AI Research EXAONE-Deep-2.4B EXAONE-Deep-7.8B EXAONE-Deep-32B NVIDIA EXAONE-2.4B EXAONE-3.5-2.4B-Instruct EXAONE-7.8B EXAONE-3.5-7.8B-Instruct EXAONE-3.5-32B-Instruct EXAONE-32B SFT&DPO&RL SFT&DPO&RL SFT&DPO&RL 1.6M&20K&10K 1.6M&20K&10K 1.6M&20K&10K Llama-3.1-Nemotron-Nano-8B-v1 Nemotron-8B Llama-3.3-Nemotron-Super-49B-v1 Nemotron-49B Llama-3.1-8B-Instruct Llama-3.3-70B-Instruct SFT&RL SFT&RL & & NovaSky Sky-T1-32B-Flash Sky-T1-32B Qwen2.5-32B-Instruct SFT&SimPO 17K&10K Table 5: The open-source LRMs details evaluated for S1-Bench. categorized into Strict Format and Loose Format, with Strict Format representing specialized subset of Loose Format. Loose Format. ID-100 and ID-101 represent two standard decoding modes, categorized as Strict Format. Both contain only one ETM. Among these, ID-100 includes thinking process, while ID-101 only generates the ETM and final answer. Similar to ID-100 and ID-101, ID-201 contains one ETM but only includes the thinking process. LRMs sometimes produce multiple ETMs, which are represented by ID-202/203/204. Additionally, LRMs may fail to correctly generate standard ETMs. We categorize these instances as ID-204/205/206, and this paper provides reference collection of special ETMs: </ think>, </th think>, </ reason>, nanswern ,**Final Answer** and **答案**. particular case occurs when the model doesnt output any standard or special ETM, which we classify as ID-207. Error Format. LRMs produce two types of undecodable errors: ID-300 indicates that the LRM outputs only ETM without any additional content, while ID-301 represents cases where the LRMs thinking process reaches the maximum length. C.2 Evaluation of Different Format In the classification shown in Table 6, ID-100 / 101 / 201 / 203 / 204 / 206 / 207 have decomposable final answers. If multiple ETMs appear, only the last one is selected for splitting. In this case, the evaluator uses the final answer as the evaluation 12 pability to respond without visible reasoning and their tendency to overthink may be orthogonal characteristics. (3) None of the evaluated LRMs exhibited behaviors classified as ID-205/206, possibly because special ETMs are perceived by LRMs as the only alternative to standard ETMs. Furthermore, ID-300 was not observed, indicating that high-temperature sampling generally yields semantically meaningful responses. Table 9 shows the format statistics of greedy sampling. The phenomenon is generally similar to top-p sampling. L-R1-32B produced ID-300, possibly due to poor compatibility between the training method and low temperature."
        },
        {
            "title": "D Experiment Details",
            "content": "Table 10 shows LRMs performance on S1-Bench under greedy sampling settings. While overall accuracy improves compared to top-p sampling settings, ART slightly decreases, yet overthinking and accuracy degradation issues persist. Figure 9 illustrates the ART heatmap across 28 subcategories and 26 models."
        },
        {
            "title": "We compile the statistics of the prejudgment",
            "content": "words extracted, as shown in Table 7."
        },
        {
            "title": "E Prompts discussed in this paper",
            "content": "This section presents the prompts used in this paper. For S1-Bench construction-related prompts, please refer to Table 11; for experimental prompts, please refer to Table 12,13 and 14. Format ID 100 101 t marker (standard) ) r / ( o 200 201 202 203 204 205 206 207 300 301 marker (special) marker (number) 1 1 1 >1 >1 >1 1 1 1 0 1 0 thinking process final answer Table 6: Format types. Words Count 简单 straightforward simple 不难 基础的 basic 直接 常见 基础 不复杂 simplest pretty straightforward easy 基本的 基本 466 231 118 72 44 24 15 10 10 7 7 5 5 5 3 Table 7: Top 15 phrases with prejudgment phenomenon, with counts summed across 22 LRMs. target. ID-200 / 202 / 205 lack final answer, in which case the evaluation target becomes the thinking process, as this part may also contain the correct solution to the question. C.3 Statistics of Different Format Table 8 presents the distribution of 12 format types under top-p sampling settings. Our analysis reveals three key insights: (1) The infinite thinking phenomenon is widespread across most models, particularly concentrated in LRMs with fewer than 32B parameters. This indicates that output formatting should be prioritized during training to reduce format errors, especially when working with less powerful foundation models. (2) The Nemotron and EXAONE families frequently produce correctly formatted responses without visible reasoning processes. This behavior can be viewed as mechanism for mitigating over-thinking. However, the EXAONE family still exhibits substantial overthinking tendencies, suggesting that LRMs ca13 Model QwQ-32B hunyuan-t1 Sky-T1-32B DS-R1 DS-R1-70B DS-R1-32B DS-R1-14B DS-R1-8B DS-R1-7B DS-R1-1.5B Nemotron-49B Nemotron-8B L-R1-32B L-R1-32B-DS L-R1-14B-DS L-R1-7B-DS s1.1-32B s1.1-14B s1.1-7B EXAONE-32B EXAONE-7.8B EXAONE-2.4B Strict Format 101 100 100.00 100.00 95.26 100.00 99.91 100.00 100.00 99.53 99.24 97.58 66.07 58.06 95.07 99.81 99.19 99.67 99.53 97.39 88.96 67.39 65.83 81. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 33.93 26.26 0.00 0.00 0.00 0.00 0.00 0.00 0.00 32.42 32.23 15.83 Loose Format Error Format 200 0.00 0.00 0.62 0.00 0.09 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.05 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.09 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.05 0.05 0.14 7.96 0.00 0.00 0.09 202 0.00 0.00 0.19 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.09 0.00 0.05 0.00 203 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.09 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.47 0.05 0.00 0.00 0.28 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.81 0.00 0.00 0.00 0.00 0.24 0.00 0.00 0.00 0.00 205 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 206 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 3.03 0.00 0.00 0.00 0.00 0.24 0.00 0.00 0.00 15.02 3.03 0.00 0.00 0.00 0.00 0.00 0.09 0.00 0.14 0.05 300 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 301 0.00 0.00 0.52 0.00 0.00 0.00 0.00 0.24 0.76 2.42 0.00 0.57 1.09 0.19 0.81 0.24 0.43 2.23 2.89 0.19 1.28 2.56 Table 8: Format types rates in top-p sampling setting. Model QwQ-32B hunyuan-t1 Sky-T1-32B DS-R1 DS-R1-70B DS-R1-32B DS-R1-14B DS-R1-8B DS-R1-7B DS-R1-1.5B Nemotron-49B Nemotron-8B L-R1-32B L-R1-32B-DS L-R1-14B-DS L-R1-7B-DS s1.1-32B s1.1-14B s1.1-7B EXAONE-32B EXAONE-7.8B EXAONE-2.4B Strict Format 101 100 100.00 100.00 99.29 100.00 100.00 100.00 99.76 99.53 97.87 91.94 60.90 55.21 85.55 99.29 98.82 98.82 98.82 95.97 87.91 65.88 63.51 78.91 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 39.10 26.78 0.24 0.00 0.00 0.00 0.00 0.00 0.00 33.89 33.65 15.88 Loose Format Error Format 200 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 201 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.24 0.00 0.00 0.00 0.00 0.24 6.64 0.24 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.71 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 203 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.24 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.24 0.00 204 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.95 0.00 0.00 0.00 0.00 0.24 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 206 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 207 0.00 0.00 0.47 0.00 0.00 0.00 0.00 0.24 0.00 0.00 0.00 16.35 6.64 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.24 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 2.61 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 301 0.00 0.00 0.24 0.00 0.00 0.00 0.24 0.24 2.13 8.06 0.00 1.66 2.84 0.71 1.18 1.18 1.18 3.55 5.45 0.00 2.37 5.21 Table 9: Format types rates in greedy decoding setting. 14 Model Size acc (Loose) acc (Strict) SUC-L SUC-S ART QwQ-32B Hunyuan-T1 Sky-T1-32B DS-R1 DS-R1-70B DS-R1-32B DS-R1-14B DS-R1-8B DS-R1-7B DS-R1-1.5B Nemotron-49B Nemotron-8B L-R1-32B L-R1-32B-DS L-R1-14B-DS L-R1-7B-DS s1.1-32B s1.1-14B s1.1-7B EXAONE-32B EXAONE-7.8B EXAONE-2.4B 32B 32B 671B 70B 32B 14B 8B 7B 1.5B 49B 8B 32B 32B 14B 7B 32B 14B 7B 32B 7.8B 2.4B 100.00 100.00 99.53 100.00 99.76 100.00 99.29 97.63 94.31 76.54 99.53 84.60 92.18 99.29 98.82 92.65 98.82 95.97 94.31 97.63 86.73 72.99 100.00 100.00 99.05 100.00 99.76 100.00 99.29 97.39 94.31 76.54 99.53 77.73 85.78 99.29 98.82 92.65 98.82 95.50 87.68 97.39 86.49 72.99 100.00 100.00 99.76 100.00 100.00 100.00 99.76 99.76 97.87 91.94 100.00 98.34 94.55 99.29 98.82 98.82 98.82 96.45 94.55 100.00 97.63 94.79 100.00 100.00 99.29 100.00 100.00 100.00 99.76 99.53 97.87 91.94 100.00 81.99 85.78 99.29 98.82 98.82 98.82 95.97 87.91 99.76 97.16 94. 750.41 541.09 157.12 621.89 469.78 428.46 463.52 452.11 436.87 473.67 337.94 446.62 996.36 528.45 664.28 514.60 983.38 786.30 630.52 746.89 947.92 1394.72 Table 10: Main results in the greedy decoding setting on the S1-Bench, sorted by model family. Bold teal marks best performance, teal second best, bold burgundy worst, and burgundy second worst. Figure 9: ART on the 28 subcategories, which is the average result of five generations under top-p sampling with Loose Format setting. 15 Prompt for construction workflow for S1-Bench Initial Generation Based on the category name and explanation, generate 50 pairs of questions and answers in both Chinese and English. Ensure that the generated questions vary in perspective and content, maintaining diversity. All questions must have **uniquely determined, unambiguous answer with no potential conditions causing multiple possible answers**. Ensure that the questions are expressed clearly. Additionally, the difficulty level of the questions should be **as simple as possibleso easy that they can be answered correctly without any need for thought**. # Category Name: {category} # Explanation: {explanation} # Cases: ## English question: {question_en} ## English Answer: {answer_en} ## Chinese question: {question_zh} ## Chinese Answer: {answer_zh} Please generate 50 pairs of Chinese and English questions and answers in the following format: [question]English-question[answer]English-answer[question]Chinese-question[answer]Chinese-answer... Start generating: Discriminating Generation Quality Given question, its answer, and its category, please analyze from the following perspectives as comprehensively as possible: 1. Whether the question belongs to the specified category. 2. Whether the question is clear, unambiguous, has an absolutely unique answer, and has no potential alternative interpretations. 3. Whether the answer is absolutely correct; if not, what the correct answer should be. 4. Whether the question is similar to other given questions, and if similar, whether more diverse questions can be generated. # Question Category and Explanation: {category_with_exp} # Question and Answer: {question_with_answer} # Other Questions: {questions_list} Begin your analysis, aiming to be as detailed and comprehensive as possible: Reduce Difficulty Generation Given question and answer that are too complex for the model to answer correctly, you need to further reduce their difficulty while trying to: - Maintain the original perspective and semantic information - Preserve the questions alignment with its category # Question Category and Explanation: {category_with_exp} # Question and Answer: {question_with_answer} The new question and answer: Evaluation on S1-Bench **Question:** {question} **Model Answer:** {model_answer} **Ground Truth:** {ground_truth} Your task is to evaluate whether the models answer is correct. An answer is considered correct as long as it contains the ground truth (regardless of how complex or detailed the description is). If there are parenthetical notes after the ground truth, then there may be multiple correct answers. In this case, the given answer is just one example, and any answer that meets the requirements specified in the notes can be considered correct. Additionally, some reasonably uncertain supplementary information is also considered appropriate, including more details, possibilities, and expanded discussion. You should focus more on whether the reply contains the correct answer. You need to output standard JSON, providing your explanation of the evaluation in the \"explain\" field, and giving the evaluation result in the \"result\" field, where 1 means the answer is correct and 0 means it is incorrect. Your action should follow the given format: \"explain\": \"\", \"result\": 0/ Table 11: Prompt for construction workflow for S1-Bench. The explanation (and category_with_exp) includes the definition of each major category, simplicity requirements in Section 3.1.1, and the definition of subcategories. 16 Prompts for Solution Segmentation # Task Description: Your task is to segment the given Chain of Thought according to the following rules: 1. Segmentation positions: (1) Please identify and extract all sub-reasoning processes from the Chain of Thought that meet the following condition: They explicitly arrive at conclusion (including cases phrased as questions, e.g., \"right?\") that is directly consistent with the Ground Truth. Reasoning processes that only indirectly support the Ground Truth or result in partially aligned conclusions should be excluded. (2) After clearly reaching the conclusion consistent with the Ground Truth, insert the segmentation marker <split> to distinctly separate each qualifying sub-reasoning process. 2. Output Restriction: (1) You should only directly output the segmentation result without adding any additional supplements. (2)Except for inserting the <split> separator, you must not make any other modifications to the original Chain of Thought, not even minor character-level changes such as punctuation, spacing, or capitalization. In other words, after removing all <split> separators, the output text must match the original Chain of Thought exactly. # Some cases: ## Case 1: ### Question: What is the value of if plus 3 equals 8? ### Ground Truth: ### Chain of Thought: Okay, so need to figure out what is if plus 3 equals 8. The problem says \"x plus 3 equals 8,\" so in mathematical terms, that would be: + 3 = 8 Alright, now need to solve for x. From what remember in algebra, the goal is to isolate on one side of the equation. That means need to get rid of that \"+ 3\" thats being added to x. To do that, should perform the opposite operation. Since its adding 3, the opposite would be subtracting 3. So, if subtract 3 from both sides, that should keep things equal. Let me try that: + 3 - 3 = 8 - 3 Simplifying both sides, on the left side, the +3 and -3 cancel each other out, leaving just x. And on the right side, 8 minus 3 is 5. So that simplifies to: = 5 Wait, let me check if that makes sense. If substitute back into the original equation, does it hold true? Lets see: Original equation: + 3 = 8 Substitute = 5: 5 + 3 = 8 5 plus 3 is indeed 8, so that works out. Another way might be to consider what number, when you add 3 to it, gives 8. So starting from 8, if take away 3, should get back to the original number. Thats the same as subtracting 3 from 8, which again gives 5. So thats consistent with the first method. Wait second, what if the original problem was written differently? The problem says \"x plus 3 equals 8.\" In symbols, thats unambiguous: + 3 = 8. So no, think thats right. **Final Answer** The value of is 5 . ### Output: Okay, so need to figure out what is if plus 3 equals 8. The problem says \"x plus 3 equals 8,\" so in mathematical terms, that would be:x + 3 = 8 Alright, now need to solve for x. From what remember in algebra, the goal is to isolate on one side of the equation. That means need to get rid of that \"+ 3\" thats being added to x. To do that, should perform the opposite operation. Since its adding 3, the opposite would be subtracting 3. So, if subtract 3 from both sides, that should keep things equal. Let me try that: + 3 - 3 = 8 - 3 Simplifying both sides, on the left side, the +3 and -3 cancel each other out, leaving just x. And on the right side, 8 minus 3 is 5. So that simplifies to: = 5<split> Wait, let me check if that makes sense. If substitute back into the original equation, does it hold true? Lets see: Original equation: + 3 = 8 Substitute = 5: 5 + 3 = 8 5 plus 3 is indeed 8, so that works out.<split> Another way might be to consider what number, when you add 3 to it, gives 8. So starting from 8, if take away 3, should get back to the original number. Thats the same as subtracting 3 from 8, which again gives 5.<split> So thats consistent with the first method. Wait second, what if the original problem was written differently? The problem says \"x plus 3 equals 8.\" In symbols, thats unambiguous: + 3 = 8. So no, think thats right. **Final Answer** The value of is 5 .<split> ## Case 2: ...... ## Case 3: ...... # Eval Target: ## Question: {question} ## Ground Truth: {ground_truth} ## Chain of Thought: {thinking_process} ## Output: Table 12: Prompts for Solution Segmentation. 17 Prompts for Error Analysis Types of Correct # Task Description: You will receive Question, its corresponding Ground Truth, and Chain of Thought(COT) generated by LLM for that Question. Your task is to carefully analyze the CoT and assign it to one of the two predefined categories listed below. # Categories: 1: The CoT ***includes explicit incorrect conclusions*** in intermediate reasoning steps. 2: The CoT ***doesnt include any explicit incorrect conclusion*** in intermediate reasoning steps. Output your evaluation in the following format: # TheReason: [note: Conduct step-by-step analysis, stating if and where explicit incorrect conclusions occur in the COT.] # ErrorType: [note: Summarize each incorrect conclusion into specific error type using phrase of less than 5 words, such as factual inaccuracies, logical fallacies, comprehension mistakes, calculation errors, formatting issues, and so forth, to better conduct further evaluation and analysis. Directly output Python list, where each element represents the error type of specific incorrect conclusion in the CoT. If there are no incorrect conclusions, return an empty list.] # TheCategory: [note: Provide your classification based on your analysis using only the number \"1\" or \"2\". Do not add any additional text.] # Question: {question} # Ground Truth: {gound_truth} # COT: {thinking_process} # TheReason: Types of Error # Task Description: You will receive Question, its corresponding Ground Truth, and Chain of Thought(COT) generated by LLM for that Question. Your task is to carefully analyze the CoT and assign it to one of the two predefined categories listed below. # Categories: 1: Regardless of whether the CoT ultimately arrives at the correct final answer or not, ***the correct answer is explicitly mentioned at least once*** within the reasoning steps (even if it is not ultimately adopted). 2: ***The correct answer is never explicitly mentioned or referenced*** at any point within the reasoning steps. Output your evaluation in the following format: # TheReason: [note: Conduct step-by-step analysis, explicitly stating whether and where correct answer is mentioned within the reasoning steps.] # TheCategory: [note: Provide your classification based on your analysis using only the number \"1\" or \"2\". Do not add any additional text.] # Question: {question} # Ground Truth: {answer} # COT: {thinking_part} # TheReason: Table 13: Prompts for Error Analysis. 18 Prompts for Extracting Prejudgment Sentences and Words Extracting Sentences # Given the beginning of models thought process when answering question, determine if there is any evaluation by the model regarding the difficulty level or category of the question. # For example: Example Evaluation 1: This seems to be an easy question. Example Evaluation 2: 这看起来似乎是个基础的问题 Example Evaluation 3: This question is bit complex. Example Evaluation 4: 这个问题看起来复杂 # Beginning of thought process: {thinking_process} # Please output only the evaluative short phrase you find. If there is no evaluation, output \"None\". # Output: Extracting Words # You are given comment sentence, and your goal is to find the only comment-related word. # For example: ## case 1: Sentence: 这是一个很简单的算术题 Word: 简单 ## case 2: Sentence: that seems pretty straightforward Word: straightforward # The comment sentence: {review_sentence} Output the only comment-related word: Table 14: Prompts for Extracting Prejudgment Sentences and Words. 19 cate. subcatrgories Explanation and cases t q n e i u d o numerical reasoning Questions that require performing basic mathematical operations or solving simple algebraic equations to arrive at numerical answer. Case: Whats two plus three? code reasoning Questions that require tracing through and executing simple code snippets to determine their output or behavior when run in specific programming environment. Case: What is the output of the following code when run in Python 3 environment: word = \"hello\"nprint(len(word)) set reasoning Questions that require applying simple syllogistic reasoning to determine whether elements belong to sets based on clearly stated relationships. Case: All squares are quadrilaterals. shape is square, is it quadrilateral? temporal reasoning Questions that require calculating time durations, ages, or future dates by applying simple arithmetic operations to temporal information. Case: How many minutes equal 120 seconds? spatial reasoning Questions that require determining relative positions, directions, or orientations of objects in space based on simple spatial relationships. Case: If bird is flying above tree, where is the tree in relation to the bird? causal reasoning Questions that require determining outcomes by applying simple cause-and-effect relationships based on given conditional statements. Case: If ferromagnetic material is placed in magnetic field, it will become magnetized. An iron nail was placed next to strong magnet for some time. Has the nail been magnetized? natural law reasoning Questions that require applying basic knowledge of physical laws and natural phenomena to predict simple observable outcomes in everyday scenarios. Case: Which is faster, an airplane or the propagation of light? geometry facts geographic facts Questions that require recalling simple and fundamental geometric properties about shapes, angles, and basic geometric figures. Case: How many angles does trapezoid have? Questions that require recalling simple factual information about locations, landmarks, political divisions, celestial bodies, and other basic geographic knowledge. Case: Which is the largest continent on Earth? historical facts Questions that require recalling basic facts about historical events. Case: Which country first invented paper? biographical facts Questions that require recalling basic facts about the identities, achievements, and characteristics of historical figures. Case: Who proposed the theory of universal gravitation? measurement units Questions that require recalling simple conversion relationships between standard units of measurement. Case: How many centimeters equal 1 meter? scientific notation Questions that require recalling basic scientific symbols, formulas, and standard units used in scientific communication. Case: What is the chemical symbol for oxygen? creative authorship Questions that require recalling the creators or originators of notable artistic, literary, musical, and cultural works. Case: Who is the author of Hamlet? Table 15: The subcategory descriptions and cases of reasoning questions and knowledge questions. 20 cate. subcatrgories Explanation and cases i l o u n i u s n repetition constraints Questions that require outputting specified characters, words, or phrases specific number of times according to simple formatting instructions. Case: Output the number \"7\" four times, without using separators. length constraints Questions that require generating outputs of specific length or with specific number of components based on simple counting constraints. Case: Output four-digit number. character constraints counting constraints Questions that require generating words or numbers that conform to simple specified character patterns or formatting rules. Case: Output number that begins with 8. Questions that require counting specific characters or elements within given text or sequence. Case: Output the number of letter \"y\" in the word \"yes\". transformation constraints Questions that require modifying text or numbers according to simple formatting or character substitution rules to produce transformed output. Case: Output the word \"good\" with all letters capitalized directly. sentence constraints Questions that require generating sentences that conform to simple specified content or structural requirements. Case: Give sentence that contains the phrase \"have lunch\" directly. sentiment classification Questions that require determining whether simple statements express positive or negative emotions based on the tone and word choice. Case: Does the sentence \"I hate rainy days.\" express positive or negative emotion? named entity recognition Questions that require identifying the correct category of named entities (such as people, places, organizations, or time expressions) within simple sentences. Case: In the sentence \"Napoleon died in 1821\", is \"1821\" time or place name? language classification Questions that require identifying the language of origin for simple words or phrases based on their characteristic writing systems or common vocabulary. Case: Is the word \"hello\" English or Japanese? topic classification intent recognition Questions that require identifying the primary subject matter or thematic category of simple sentences based on their content and context clues. Case: Is the topic of the sentence \"The stock market rose 2% today\" finance or technology? Questions that require determining the communicative purpose behind simple utterances or statements based on their phrasing and context. Case: Is the intention of the sentence \"Im sorry Im late.\" to apologize or to blame? syntax classification Questions that require identifying the correct grammatical structure or sentence type of simple expressions based on their form, punctuation, and communicative function. Case: Is \"Close the door!\" an imperative sentence or an interrogative sentence? grammar classification Questions that require identifying simple grammatical properties (like tense, voice, or polarity) of sentences based on their structure and verb forms. Case: Is \"The apple was eaten.\" in active voice or passive voice? coreference resolution Questions that require identifying which entity pronoun or reference term refers to in simple sentences by tracking relationships between words in the text. Case: In \"My computer is broken, and need to fix it.\" What does \"it\" refer to? Table 16: The subcategory descriptions and cases of instruction following questions and analysis questions."
        }
    ],
    "affiliations": [
        "Institute of Information Engineering, Chinese Academy of Sciences",
        "School of Cyber Security, University of Chinese Academy of Sciences"
    ]
}