{
    "paper_title": "Towards Pixel-Level VLM Perception via Simple Points Prediction",
    "authors": [
        "Tianhui Song",
        "Haoyu Lu",
        "Hao Yang",
        "Lin Sui",
        "Haoning Wu",
        "Zaida Zhou",
        "Zhiqi Huang",
        "Yiping Bao",
        "Y. Charles",
        "Xinyu Zhou",
        "Limin Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present SimpleSeg, a strikingly simple yet highly effective approach to endow Multimodal Large Language Models (MLLMs) with native pixel-level perception. Our method reframes segmentation as a simple sequence generation problem: the model directly predicts sequences of points (textual coordinates) delineating object boundaries, entirely within its language space. To achieve high fidelity, we introduce a two-stage SF$\\to$RL training pipeline, where Reinforcement Learning with an IoU-based reward refines the point sequences to accurately match ground-truth contours. We find that the standard MLLM architecture possesses a strong, inherent capacity for low-level perception that can be unlocked without any specialized architecture. On segmentation benchmarks, SimpleSeg achieves performance that is comparable to, and often surpasses, methods relying on complex, task-specific designs. This work lays out that precise spatial understanding can emerge from simple point prediction, challenging the prevailing need for auxiliary components and paving the way for more unified and capable VLMs. Homepage: https://simpleseg.github.io/"
        },
        {
            "title": "Start",
            "content": "TOWARDS PIXEL-LEVEL VLM PERCEPTION"
        },
        {
            "title": "VIA SIMPLE POINTS PREDICTION",
            "content": "Tianhui Song1,2 Haoyu Lu1 Hao Yang1 Lin Sui1 Haoning Wu1 Zaida Zhou1 Zhiqi Huang1 Yiping Bao1 Y.Charles1 Xinyu Zhou1 Limin Wang2 1 Moonshot AI 2 Nanjing University"
        },
        {
            "title": "ABSTRACT",
            "content": "We present SimpleSeg, strikingly simple yet highly effective approach to endow Multimodal Large Language Models (MLLMs) with native pixel-level perception. Our method reframes segmentation as simple sequence generation problem: the model directly predicts sequence of points (textual coordinates) delineating object boundaries, entirely within its language space. To achieve high fidelity, we introduce two-stage SFTRL training pipeline, where Reinforcement Learning with an IoU-based reward refines the point sequences to accurately match ground-truth contours. We find that the standard MLLM architecture possesses strong, inherent capacity for low-level perception that can be unlocked without any specialized architecture. On segmentation benchmarks, SimpleSeg achieves performance that is comparable to, and often surpasses, methods relying on complex, task-specific designs. This work lays out that precise spatial understanding can emerge from simple point prediction, challenging the prevailing need for auxiliary components and paving the way for more unified and capable MLLMs. Code, data, and model are publicly accessible at https://simpleseg.github.io/. 6 2 0 2 7 2 ] . [ 1 8 2 2 9 1 . 1 0 6 2 : r Figure 1: In this work, we explore the limits of MLLM pixel-level perception by predicting the next point in contour with the simplest approach possible. Without introducing any complex architectures or special patterns, we show how even minimalistic point prediction can achieve effective segmentation at the pixel level. Equal contribution. This work was done during interning at Moonshot AI. Project lead. SIMPLESEG"
        },
        {
            "title": "Introduction",
            "content": "Multimodal Large Language Models (MLLMs) have rapidly advanced open-ended visionlanguage understanding, delivering strong performance across captioning, VQA, and interactive grounding (H. Liu et al. 2024; OpenAI 2024; Comanici et al. 2025). Yet, despite impressive semantic competence, todays Multimodal Large Language Models (MLLMs) remain largely image-level in their perception, struggling to precisely localize and delineate fine structuresfrom object boundaries to thin partsthat are essential for genuine spatial understanding. This limitation is partly rooted in the evolution of multimodal foundational models. While perception is cornerstone of multimodal tasks, dense prediction tasks like segmentation have historically been overlooked as foundational capability, as they often rely on specialized decoders or complex architectural designs not native to language-centric models. In contrast, object grounding and detection have been widely adopted, largely because bounding boxes can be conveniently represented as plain text coordinates (e.g., x1, y1, x2, y2) and easily integrated into the pre-training pipeline. However, we argue that the coarse localization offered by bounding boxes is insufficient for the next generation of applications. Such pixel-level grounding is not merely cosmetic: it is foundational for controllable image editing (Shi et al. 2024), vision-based tool use (C. Wang et al. 2025), and GUI-grounded agents (Yuhang Liu et al. 2025; Yujia Qin et al. 2025) that must reason, act, and communicate about precisely where things are. Therefore, to bridge this gap, we move beyond coarse bounding boxes and consider more precise and granular approach: point prediction. prevalent approach augments MLLMs with task-specific decoders (e.g., SAMor RPN-style heads) on top of the multimodal backbone (X. Lai et al. 2024; Y. Zhang et al. 2024; He et al. 2024; Z. Ren et al. 2024; Rasheed et al. 2024; A. Zhang et al. 2023; S. Wu et al. 2024; Jiannan et al. 2024). While effective, this design couples architecture to specific tasks, complicates end-to-end training with extra parameters, and pushes outputs out of the language space, weakening interpretability and compositional reasoning. As result, fine-grained perception remains underexplored as core capability of native MLLMs. Decoder-free methods, such as Text4Seg (Lan et al. 2024), serialize masks as text, but suffer from dense token budgets and compromised interpretability. VisionLLM (W. Wang et al. 2024) emits polygons but restricts them to small number of vertices, limiting its performance. Both methods fail to deliver pixel-level segmentation with the generality and reasoning fluency of modern MLLMs. In this work, we investigate strikingly simple question: can an MLLM achieve high-fidelity segmentation by merely predicting sequence of points? We present SimpleSeg, minimalist decoder-free approach that reframes segmentation as simple, sequential point prediction entirely within the language space. More than just method, our work serves as crucial finding: we demonstrate that standard MLLM architectures possess strong, inherent capacity for fine-grained perception, potential that can be unlocked without any specialized decoders or complex output formats. This approach preserves the models generalist architecture, dramatically simplifies the training pipeline, and naturally unifies object localization tasks (points, boxes, and masks) under single, human-readable textual interface. Specifically, we first introduce systematic point-sequence-based representation for segmentation masks that efficiently scales data preparation. Based on this, we generalize the perceptual localization task beyond text queries: any target can be an input or output in 4-tuple, [text, point, box, mask], allowing for rich combination of task formats that boosts data efficiency and robustness. To make this simple point prediction effective, we design two-stage SFTRL training pipeline. After standard supervised fine-tuning (SFT) stage to learn the basic task format, we pioneer the use of Reinforcement Learning (RL) to optimize the entire generated sequence of points. By using an IoU-based reward, RL directly refines the fidelity and closure of the resulting shape without altering the MLLMs architecture. To our knowledge, this is the first work to successfully apply reinforcement learning to decoder-free MLLM for segmentation. Empirically, our model attains high-quality, native fine-grained perception and generalizes robustly across diverse domains and resolutions, as illustrated in Fig. 2. On challenging referring benchmarks such as the refCOCO series, SimpleSeg achieves performance that is comparable to, and often surpasses, prominent methods that rely on complex, task-specific decoders. The main contributions can be summarized as follows: We present minimalist, decoder-free approach for MLLM segmentation based on simple point sequence prediction, challenging the necessity of complex architectural additions. 2 SIMPLESEG Figure 2: Segmentation results of SimpleSeg on natural and non-natural images. These examples highlight the models excellent generalization, showing its precise pixel-level perception is not confined to real-world objects. The model successfully segments targets from natural photographs (the lightning) and performs with equal precision on various forms of \"in-screen\" or digitally generated content, including anime, data charts, and infographics. SIMPLESEG We provide key finding that standard MLLM architectures possess strong, inherent potential for pixel-level perception, which can be unlocked with the right training methodology. We are the first to propose and validate an SFTRL pipeline for this task, using sequence-level IoU rewards to directly optimize the quality of the generated geometry. We demonstrate that our simple approach achieves performance comparable to or exceeding that of more complex decoder-based systems on standard referring segmentation benchmarks. In addition to the above, SimpleSeg offers several key benefits: Simplicity: SimpleSeg requires no specialized modules and adheres to the standard MLLM architecture, it can be seamlessly and efficiently integrated as new, core pre-training task for foundation models, similar to visual grounding. Task Generality: By framing segmentation as text-generation problem, our approach is inherently flexible. The model can be easily adapted to wide range of vision-language tasks that require precise spatial localization. Interpretable Output: The model generates explicit, human-readable coordinate sequences instead of dense pixel masks. This transparency simplifies debugging and makes the output directly usable for downstream applications like interactive editing or tool use. This makes SimpleSeg not just an efficient solution, but versatile framework for deploying pixel-level perception in multimodal models with broad range of applications."
        },
        {
            "title": "2 Related Work",
            "content": "Multimodal Large Language Models. Multimodal Large Language Models (MLLMs) have significantly advanced vision-language tasks by extending the reasoning capabilities of LLMs to the visual domain (S. Yin et al. 2024). Early pioneering models like LLaVA (H. Liu et al. 2024) established strong foundation for multimodal instruction following. Subsequent works (H. Lu et al. 2024; OpenAI 2024; Comanici et al. 2025) have further pushed the boundaries of performance by scaling up model and data size. However, common limitation persists: the perception of these models is typically coarse and image-level. They excel at high-level description and reasoning but lack the native ability for precise, pixel-level localization, which remains largely underexplored frontier. Approaches to Pixel-Level MLLM Perception. Efforts to equip MLLMs with dense, pixel-level perception have primarily followed two distinct paths, creating central dilemma between performance and architectural integrity. The first, hybrid approach, augments general MLLM backbone with specialized, task-specific decoders (X. Lai et al. 2024; Y. Zhang et al. 2024; Z. Ren et al. 2024; Jiannan et al. 2024; Rasheed et al. 2024; Xia et al. 2024; T. Zhang et al. 2024). These external modules, often inspired by SAM or other segmentation architectures, achieve strong performance on specific tasks. However, this modular design compromises architectural purity; it introduces extra parameters, complicates training, and moves the final output outside the native language space. GiT (Haiyang Wang et al. 2024) uses Vision Transformer (ViT) backbone and tokenizes text input via simple word embeddings to achieve open-vocabulary segmentation. It relies on an architecture customized for visual tasks, limiting the generalization power compared to true vision-language model. The second are unified approaches. Some works (Lan et al. 2024; W. Wang et al. 2024; T. Chen, Saxena, L. Li, Lin, et al. 2022; T. Chen, Saxena, L. Li, Fleet, et al. 2021) attempt to keep all outputs strictly within the language space by representing masks as text sequences, e.g., RLE (Lan et al. 2024) or polygons (W. Wang et al. 2024; T. Chen, Saxena, L. Li, Lin, et al. 2022). While conceptually aligned with the end-to-end philosophy of LLMs, these methods have struggled with fidelity, suffering from excessive token consumption and an inability to capture fine-grained details. UFO (Tang et al. 2025) proposes an extra, special mask token and relies on hacking the intermediate feature for retrieval mechanisms. Consequently, achieving high-performance, pixel-level perception natively within standard MLLM architecturewithout sacrificing simplicity, resolution, or generalizationremains fundamental open challenge."
        },
        {
            "title": "3 Methodology",
            "content": "We present SimpleSeg, simple yet effective framework that equips vanilla MLLM with native pixel-level perception via simple points prediction. The key idea is to keep segmentation entirely inside the language space by predicting point trajectory (i.e., an explicit sequence of 2D coordinates) that traces the target contour. This design is decoder-free, architecture-agnostic, and naturally unifies points, boxes, and masks under one textual interface. 4 SIMPLESEG Figure 3: Overview of our data annotation pipeline, which incorporates modules for object detection, mask segmentation, points conversion, and instance caption."
        },
        {
            "title": "3.1 Task Formulation and Data Construction",
            "content": "Outputs as Text in One Space. Perceptual location information commonly appears as (i) center point, (ii) bbox, or (iii) mask. Keeping all outputs as text tokens preserves the MLLMs generalist interface and enables direct composition with language prompts and tools. We therefore adopt the following normalized, human-aligned formats: <point> : [x, y] <bbox> : [x1, y1, x2, y2] <mask> : [[x1, y1], [x2, y2], . . . , [xV , yV ]] where coordinates are normalized to [0, 1] and is variable. Masks as Point Trajectories (Contours). Instead of dense per-pixel encodings (e.g., R-RLE (Lan et al. 2024)), we represent mask by an explicit point trajectory that sparsely samples its boundary. This brings three benefits: (1) interpretability (human-readable coordinates), (2) compositionality (same token space as text/points/boxes), and (3) controllable token budget (linear in the number of vertices rather than image resolution). For training data derived from binary masks, we extract polygonal contours using the SuzukiAbe algorithm (Suzuki et al. 1985) (OpenCV (Itseez 2015)), enforce consistent traversal order (clockwise), and optionally apply tolerance-based sparsification to obtain compact simple points sequence. Unified Query Interface. Following the spirit of promptable segmentation, we model grounding over the tuple target = [text, point, bbox, mask], and instantiate queries as Cartesian products of the available elements (e.g., (textbbox), (pointmask)). Two examples are: Q: What is the bounding box of <text>? A: <bbox>. Q: Give the polygon of the object at <point>? A: <mask>. This interface (i) multiplies supervision sources by recombining weak labels (e.g., points/boxes from masks via min/max or centroid), and (ii) standardizes outputs for instruction tuning and RL. Text Grammar. We constrain outputs with minimal JSON-like grammar to reduce decoding entropy, and they can be parsed automatically at inference: , [x, y] [[x, y], [x, y], ...] (cid:124) (cid:123)(cid:122) (cid:125) (cid:125) (cid:123)(cid:122) (cid:124) point polygon , [x1, y1, x2, y2] (cid:123)(cid:122) (cid:125) bbox (cid:124) . Data Annotation Pipeline. To scale our framework with large-scale web data, we construct an automatic data annotation pipeline, as shown in Fig. 3, to generate instance-level segmentation labels. Specifically, the pipeline employs: Grounding-DINO (Shilong Liu et al. 2023) for phrase grounding and object detection, SAM to extract segmentation masks, the algorithm for converting mask to contour coordinates, and an off-the-shelf VLM for optional, refined object description tagging. 5 SIMPLESEG 3.2 Training Pipeline of SimpleSeg Our training including: (i) instruction tuning (SFT) to cold-start structured generation, and (ii) reinforcement learning (RL) to optimize sequence-level, location-aware objectives. Stage I: Instruction Tuning. According to the aforementioned polygon-based representation of masks, we curate instructionresponse pairs spanning (textpoint), (textbbox), and (text/pointmask). The supervised finetuning stage aims to teach the MLLM to emit correct output formats, including well-formed coordinates, closing brackets, and consistent ordering, while learning basic grounding priors. Empirically, this already yields competitive performance and provides stable initialization for RL. Stage II: Reinforcement Learning with GSPO. Reinforcement learning (RL) has demonstrated significant effectiveness in reasoning tasks for MLLM (Shao et al. 2024; Guo et al. 2025; Team, A. Du, B. Gao, et al. 2025). However, the potential of RL for sharpening fine-grained perception remains largely untapped. While SFT aligns tokens to local supervision, pixel-level segmentation quality depends on global properties of the entire sequence (closure, boundary fidelity, and verbosity). We focus on leveraging RL to boost the perception accuracy of MLLM, since, in essence, reinforcement learning is more reasonable and efficient optimization method for perception tasks. Especially under our data and task formulation, we are not aiming to force the model to rigidly regress fixed ground-truth coordinates in the training data, as contour sequences are inherently flexible. The optimization process relies more on location-aware rewards to explore and refine predictions at the sequence level, while format-rule-based judges can enforce valid, parseable output structures. We therefore adopt GSPO (Zheng et al. 2025) as our RL algorithm, and adopt rule-based reward system that mainly consists of three types of rewards: Mask IoU reward: The direct IoU between the predicted and ground-truth mask. The range of reward values is [0.0, 0.1]. We set threshold τ that the reward is 0 if IoU is less than τ . MSE Distance IoU reward: The negative mean square distance between the centroids of predicted and ground-truth mask. It is normalized with the image size. Format reward: In addition to the accuracy reward model, we employ format reward that enforces the model to output correct polygon coordinates formats. If the format is wrong, the reward returns zero. Crucially, RL lets the model discover alternative yet valid trajectories (e.g., different starting points, equivalent vertex sets) instead of overfitting to single annotation. Why RL for Point Trajectories? As far as we know, we are the first to leverage RL in the realm of decoder-free MLLM segmentation. Contours are inherently many-to-one w.r.t. masks; enforcing exact token matching is suboptimal. Reinforcement learning well bridges the gap and evaluates the rendered mask, directly aligning optimization with the end metric, and improves closure and thin-structure adherence that are difficult to teach via token-level losses alone."
        },
        {
            "title": "4 Experiment",
            "content": "4.1 Implementation Details We validate our method on two open-source MLLM architectures, Qwen2.5-VL-7B (Bai et al. 2023) and Kimi-VL (Team, A. Du, B. Yin, et al. 2025), an efficient MoE model with 2.8B activated parameters. Training uses 32 NVIDIA GPUs with global batch size of 256 and the enhanced Muon optimizer (J. Liu et al. 2025). For supervised fine-tuning (SFT), we use an initial learning rate of 5105 with cosine decay to 2106, and warm-up ratio of 0.03. For reinforcement learning (RL), we adopt GSPO with clip ratio in [3104, 4104] and KL coefficient of 0.01. Unless otherwise specified, coordinates are normalized and serialized in the text space using our polygon format, and they are sparsified by tolerance parameter ϵ (cf. Sec. 4.3). More implementation details are provided in the Appendices. 4.2 Main Results Referring Expression Segmentation The referring expression segmentation (RES) task aims to segment the object in an image that is described by given natural-language expression. We follow the training recipe of (Lan et al. 2024), which constructs the training dataset with the train split of refCOCO, refCOCO+ (Kazemzadeh et al. 2014), refCOCOg (Mao et al. 2016), and refCLEF. As shown in Tab. 1, our SimpleSeg achieves superior performance in decoder-free models, and comparable to decoder-based methods. This demonstrates our methods strong fine-grained perception capacity as generalist vision-language model, without any modification of model architecture. Table 1: Referring Expression Segmentation results (cIoU) on refCOCO (+/g) datasets (Kazemzadeh et al. 2014; Mao et al. 2016), compared to approaches that adopt MLLMs for segmentation. denotes the model underwent pre-training. SIMPLESEG"
        },
        {
            "title": "Methods",
            "content": "NEXT-Chat (A. Zhang et al. 2023) LISA (X. Lai et al. 2024) PixelLM (Z. Ren et al. 2024) AnyRef (He et al. 2024) GSVA (Xia et al. 2024) LaSagnA (Cong Wei et al. 2024) Groundhog (Y. Zhang et al. 2024) Text4Seg (w/ SAM) UFOLLaVA-1.5-7B (Tang et al. 2025) Text4SegInternVL2-8B (Lan et al. 2024) SimpleSegQwen2.5-VL-7B SimpleSegKimi-VL SimpleSeg SimpleSeg Qwen2.5-VL-7B Kimi-VL refCOCO val testA testB Decoder-based Models val refCOCO+ testA testB refCOCOg test val Avg. 74.7 74.9 73.0 76.9 77.2 76.8 78.5 79.2 78.9 79.1 76.5 79.9 78.9 78.7 79.9 81.7 69.5 72.3 68.2 74.2 73.5 73.8 75.7 75.6 65.1 65.1 66.3 70.3 65.9 66.4 70.5 72.8 Decoder-free Models 77.2 74.7 75.6 76.9 80.9 80. 79.4 77.4 78.7 78.9 77.8 80.6 73.8 71.6 72.0 73.6 75.2 76.2 70.8 68.5 68.9 71.1 72.4 70.4 71.9 70.8 71.7 73.5 69.6 70.6 75.0 77.9 75.5 73.6 74.4 75.2 77.3 76.2 56.7 58.1 58.3 61.8 59.8 60.1 64.9 66. 64.1 62.9 62.3 66.1 66.1 67.1 67.0 67.9 69.3 70.0 72.7 70.6 74.1 74.0 72.1 70.7 70.8 72.8 73.3 72.8 67.0 70.6 70.5 70.7 73.3 71.9 74.6 75.3 73.2 71.6 72.5 74.3 74.1 74.7 68.9 69.9 69.2 72.2 71.4 71.1 74.2 75. 73.3 71.4 71.9 73.6 74.6 74.8 Table 2: Referring Expression Comprehension results (Acc@0.5) on RefCOCO (+/g) datasets, compared to approaches that adopt MLLMs for segmentation. denotes the model underwent pre-training."
        },
        {
            "title": "Methods",
            "content": "LISA (X. Lai et al. 2024) GSVA (Xia et al. 2024) NEXT-Chat (A. Zhang et al. 2023) PixelLM (Z. Ren et al. 2024) Text4Seg (w/ SAM) UFOLLaVA-1.5-7B (Tang et al. 2025) Text4SegInternVL2-8B (Lan et al. 2024) SimpleSegQwen2.5-VL-7B SimpleSegKimi-VL SimpleSeg SimpleSeg Qwen2.5-VL-7B Kimi-VL refCOCO val testA testB Decoder-based Models val refCOCO+ testA testB refCOCOg test val Avg. 85.4 86.3 85.5 89.8 90.3 88.8 89.2 90.0 92.2 93.4 82.6 83.8 77.9 86.4 87.5 74.2 72.8 77.2 83.2 85.2 Decoder-free Models 90.8 88.3 89.4 90.5 90.2 91. 93.0 91.4 92.7 92.9 92.9 92.1 87.3 85.8 84.8 86.8 86.1 87.1 85.5 83.5 82.9 85.3 84.6 82.6 79.5 78.8 84.5 87.0 89.9 90.5 88.2 87.9 89.5 90.5 88.3 68.4 68.0 68.0 78.9 79. 78.6 77.9 76.6 80.2 79.0 79.3 79.3 81.6 80.1 84.6 85.4 86.9 82.4 82.5 86.1 84.9 84.6 80.4 81.8 79.8 86.0 85.4 87.2 82.5 84.7 86.5 85.6 86.3 79.8 80.3 80.4 86.0 87. 87.5 85.0 85.2 87.2 86.7 86.5 Referring Expression Comprehension Our SimpleSeg is also directly usable for object detection by converting the predicted mask to bounding box through simple min-max operations. Accordingly, we evaluate our approach on the Referring Expression Comprehension (REC) task, using the same model trained as in RES. For the evaluation specification, we calculate the average accuracy with threshold IoU of 0.5 between the predicted and ground truth bounding boxes. As shown in Tab. 2, our SimpleSeg obtains state-of-the-art performance on the benchmarks. Specifically, our method achieves an average score of 87.2, exceeding the closest competitor, Text4Seg, even though it is equipped with mask refiner. 7 SIMPLESEG Table 3: The gIoU score with different training stages in validation sets. Pre-train refCOCO+ refCOCOg refCOCO SFT RL 65.5 75.2 ( 9.7) 60.8 70.6 ( 9.8) 60.4 70.9 ( 10.5) 25.3 ( -45.7) 70.1 ( 4.6) 78.5 ( 13.0) 18.7 ( -46.4) 65.0 ( 4.2) 69.8 ( 9.0) 25.7 ( -43.0) 65.7 ( 5.3) 71.7 ( 11.3) 4.3 Exploration Studies Effect of Training Stages Table 3 ablates on the effect of different training stages, including pre-training, SFT, and RL. We evaluate the gIoU score on the validation set of different datasets. SFT alone reaches 65.5, 60.8, and 60.4 gIoU on three datasets, establishing strong baseline from purely supervised polygon learning. Adding RL lifts performance to 75.2 (+9.7), 70.6 (+9.8), and 70.9 (+10.5) respectively by large margin, indicating that sequence-level credit assignment with IoU-based rewards is important for accurate closed polygon generation and token-economical outputs. Pre-training without SFT performs poorly (25.3 gIoU), and this stems from distribution shift between pre-training and SFT prompts. Pre-training lacks RefCOCO-style questions, and this phase primarily focuses on utilizing weakly labeled data to establish the models basic segmentation ability. It can be seen that both SFT and SFT+RL benefit significantly from pre-training, respectively, raising the gIoU by 4.6 and 13.0 on refCOCO, confirming that scaling the training data strengthens perceptual priors and benefits downstream tasks. Figure 4: The relationship between the sequence length and performance under the control of the point density parameter ϵ. Point/Polygon Density (ϵ) ϵ is hyperparameter that controls the polygon approximation accuracy for mask-tocontour conversion. smaller ϵ yields higher polygon precision and thus more sampled points. Fig 4 varies the sparsification tolerance. We conduct SFT experiments with different ϵ and the results are demonstrated in Fig. 4. Performance is unimodal w.r.t. token length: too few points underfit shapes (35.6 cIoU at 78 tokens), too many induce long-horizon decoding errors and length exposure (72.5 cIoU at 859 tokens), while moderate density (221 tokens) yields the best score. Figure 5: gIoU score with different rewards. Reward RefC RefC+ RefCg IoU + Distance + Length Penalty 76.9 77.1 66.7 71.9 72.2 62.4 72.9 73.1 62.0 Reward Design for RL Table 5 studies the effectiveness of different reward components, including distance and length penalty. We observe that the involvement of distance yields an average gain of around 0.2, while imposing hard length constraints degrades the performance. Metrics Trends During RL In Fig. 6, we illustrate the training states during the reinforcement learning process, including the reward, response length, and validation performance, on different settings of ϵ. One observation is that, even without length-related rewards, the model can adaptively adjust its output length to keep reasonable accuracyefficiency balance during RL. With large density of ϵ = 0.001, the number of tokens decreases moderately, trading 8 SIMPLESEG Figure 6: The curve of metrics during the RL stage. Figure 7: Visual results with different training orders of sampling points. Unsatisfactory prediction points are marked with blue boxes in the image. redundant vertices for token efficiency while preserving mask fidelity. When the token budget is low with ϵ = 0.01, the response length is increased slightly to better refine the segmentation results. Order of Sampling Points To convert binary mask to contour coordinates, we apply the Suzuki-Abe algorithm for boundary tracing, which can keep the sampling points in clockwise order. We conduct experiments with different organizations of sampling points, and display the results in Fig. 7. First, without enforcing clockwise ordering, the coordinate sequence fails to form valid polygon, and no segmentation mask can be derived. Second, clockwise ordering provides more principled learning target, reducing model entropy. As shown in the figure, alternative orders confuse the model and yield chaotic or repeated points, decreasing the token efficiency. Results on extended tasks As mentioned in Sec. 3.1, we extend the perception task via unified query interface beyond just the query of the reference phrase. Namely, our SimpleSeg can also achieve the SAM-like functionality, such as point mask and bbox mask. We visualize the results in the Appendix due to limited space. This significantly demonstrates the generality of our framework, further pushing the upper limit of MLLMs versatile perception capacity."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we demonstrated that strikingly simple approachreframing segmentation as the prediction of sequence of pointsis sufficient to unlock powerful, native capability for pixel-level perception latent within standard MLLM architectures. Our model, SimpleSeg, cultivated through novel SFTRL pipeline, achieves performance that is comparable to, and often surpasses, complex decoder-based systems. It is finding that high-fidelity perception can be an emergent property of MLLMs. Our work paves the way for new generation of truly generalist multimodal systems that seamlessly unify perception and reasoning within single, elegant framework. 9 SIMPLESEG"
        },
        {
            "title": "References",
            "content": "Bai, Jinze et al. Qwen-VL: Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond. In: arXiv preprint arXiv:2308.12966 (2023). Chen, Ting, Saurabh Saxena, Lala Li, David Fleet, et al. Pix2seq: language modeling framework for object detection. In: arXiv preprint arXiv:2109.10852 (2021). Chen, Ting, Saurabh Saxena, Lala Li, Tsung-Yi Lin, et al. unified sequence interface for vision tasks. In: Advances in Neural Information Processing Systems 35 (2022), pp. 3133331346. Comanici, Gheorghe et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. In: arXiv preprint arXiv:2507.06261 (2025). Guo, Daya et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. In: arXiv preprint arXiv:2501.12948 (2025). He, Junwen et al. Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024, pp. 1398013990. Itseez. Open Source Computer Vision Library. https://github.com/itseez/opencv. 2015. Jiannan, Wu et al. VisionLLM v2: An End-to-End Generalist Multimodal Large Language Model for Hundreds of Vision-Language Tasks. In: arXiv preprint arXiv:2406.08394 (2024). Kazemzadeh, Sahar et al. Referitgame: Referring to objects in photographs of natural scenes. In: Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). 2014, pp. 787798. Lai, Xin et al. Lisa: Reasoning segmentation via large language model. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024, pp. 95799589. Lan, Mengcheng et al. Text4seg: Reimagining image segmentation as text generation. In: arXiv preprint arXiv:2410.09855 (2024). Liu, Haotian et al. Llava-next: Improved reasoning, ocr, and world knowledge. 2024. Liu, Jingyuan et al. Muon is scalable for LLM training. In: arXiv preprint arXiv:2502.16982 (2025). Liu, Shilong et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In: arXiv preprint arXiv:2303.05499 (2023). Liu, Yuhang et al. Infiguiagent: multimodal generalist gui agent with native reasoning and reflection. In: arXiv preprint arXiv:2501.04575 (2025). Lu, Haoyu et al. Deepseek-vl: towards real-world vision-language understanding. In: arXiv preprint arXiv:2403. (2024). Mao, Junhua et al. Generation and comprehension of unambiguous object descriptions. In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2016, pp. 1120. OpenAI. GPT-4o System Card. 2024. arXiv: 2410.21276 [cs.CL]. URL: https://arxiv.org/abs/2410.21276. Qin, Yujia et al. Ui-tars: Pioneering automated gui interaction with native agents. In: arXiv preprint arXiv:2501.12326 (2025). Rasheed, Hanoona et al. Glamm: Pixel grounding large multimodal model. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024, pp. 1300913018. Ren, Zhongwei et al. Pixellm: Pixel reasoning with large multimodal model. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024, pp. 2637426383. Shao, Zhihong et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. In: arXiv preprint arXiv:2402.03300 (2024). Shi, Yichun, Peng Wang, and Weilin Huang. Seededit: Align image re-generation to image editing. In: arXiv preprint arXiv:2411.06686 (2024). Suzuki, Satoshi et al. Topological structural analysis of digitized binary images by border following. In: Computer vision, graphics, and image processing 30.1 (1985), pp. 3246. Tang, Hao et al. Ufo: unified approach to fine-grained visual perception via open-ended language interface. In: arXiv preprint arXiv:2503.01342 (2025). Team, Kimi, Angang Du, Bofei Gao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. In: arXiv preprint arXiv:2501.12599 (2025). Team, Kimi, Angang Du, Bohong Yin, et al. Kimi-vl technical report. In: arXiv preprint arXiv:2504.07491 (2025). Wang, Chenyu et al. Mllm-tool: multimodal large language model for tool agent learning. In: 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). IEEE. 2025, pp. 66786687. Wang, Haiyang et al. Git: Towards generalist vision transformer through universal language interface. In: European Conference on Computer Vision. Springer. 2024, pp. 5573. Wang, Wenhai et al. Visionllm: Large language model is also an open-ended decoder for vision-centric tasks. In: Advances in Neural Information Processing Systems 36 (2024). Wei, Cong et al. LaSagnA: Language-based Segmentation Assistant for Complex Queries. In: arXiv preprint arXiv:2404.08506 (2024). 10 SIMPLESEG Wu, Shengqiong et al. Towards Semantic Equivalence of Tokenization in Multimodal LLM. In: arXiv preprint arXiv:2406.05127 (2024). Xia, Zhuofan et al. Gsva: Generalized segmentation via multimodal large language models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024, pp. 38583869. Yin, Shukang et al. survey on multimodal large language models. In: National Science Review 11.12 (2024), nwae403. Zhang, Ao et al. Next-chat: An lmm for chat, detection and segmentation. In: arXiv preprint arXiv:2311. (2023). Zhang, Tao et al. Omg-llava: Bridging image-level, object-level, pixel-level reasoning and understanding. In: arXiv preprint arXiv:2406.19389 (2024). Zhang, Yichi et al. Groundhog: Grounding large language models to holistic segmentation. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2024, pp. 1422714238. Zheng, Chujie et al. Group sequence policy optimization. In: arXiv preprint arXiv:2507.18071 (2025). 11 SIMPLESEG"
        },
        {
            "title": "A Limitations and Diagnostics",
            "content": "While SimpleSeg eliminates task decoders, long sequences remain bottleneck for high-resolution, highly-curved objects. Errors tend to cluster at sharp corners and thin structures under aggressive sparsification. Future diagnostics should consider boundary F-score, vertex-wise Chamfer distance, and token-per-mask analyses across object scales to complement cIoU/Acc@0.5."
        },
        {
            "title": "B Additional Implementation Details",
            "content": "B.1 Data Details Pre-training Data: We leveraged large-scale open-source and web data, including LAION and Coyo. All pre-training samples were labeled using the annotation pipeline detailed in Sec. 3.1. SFT and RL Data: For Supervised Fine-Tuning (SFT), we utilized the RefCOCO series, strictly adhering to the data processing protocol established in Text4Seg (Lan et al. 2024). Specifically, we construct the training dataset with the train split of refCOCO, refCOCO+ (Kazemzadeh et al. 2014), refCOCOg (Mao et al. 2016), and refCLEF, resulting in dataset of 800k samples. Similarly, the prompt set with 400k samples for Reinforcement Learning (RL) was also derived from the RefCOCO series. Note on Benchmarking: It is important to note that the results presented in Tab. 1 and Tab. 2 are intended to ensure fair comparison with state-of-the-art methods. Therefore, these reported metrics are derived from models trained exclusively with the SFT and RL stages (i.e., with only RefCOCO datasets). The discussion regarding pre-training with large-scale web data was included primarily as scaling analysis and ablative study, as demonstrated in Tab. 3. B.2 Training Details Tab. 4 and Tab. 5 present the training hyper-parameters used in SFT and RL stages. Table 4: Training settings for SFT stage. Value Param Name Type Max Learning rate Min Learning rate Weight decay (β1, β2) Gradient norm clip Scheduler Warmup ratio Numerical precision Global batch size Samples per epoch Total epochs Enhanced Muon 5e-5 2e-6 0.1 (0.9, 0.95) 1.0 Cosine decay 0.03 FP16 256 800k Optimizer"
        },
        {
            "title": "Training",
            "content": "Table 5: Training settings for RL stage. Algorithm Optimizer Training Param Name Type Clip Ratio KL Alpha Responses / Group Type Constant LR (β1, β2) Gradient norm clip Numerical precision Rollout Temp Global batch size Samples per epoch Total epochs Value GSPO [3e-4, 4e-4] 0.1 8 Enhanced Muon 2e-6 (0.9, 0.95) 1.0 FP16 0.8 256 400k"
        },
        {
            "title": "C Additional qualitative results",
            "content": "As shown in Fig. 8, Fig. 9, and Fig. 10, we provide more example results of SimpleSeg on our extended tasks, which significantly demonstrate our SimpleSegs accuracy, robustness, and generalization. 12 SIMPLESEG Figure 8: More results on more diverse tasks, including (pointmask) and (bboxmask). The position information is visualized in the image. 13 SIMPLESEG Figure 9: More results on more diverse tasks including (bboxmask) and (textmask). The position information is visualized in the image. 14 SIMPLESEG Figure 10: More results on more diverse tasks, such as (textpoint) and (textbbox). The position information is visualized in the image. 15 SIMPLESEG Figure 11: More results of panoptic segmentation, multiple objects segmentation, and multi-parts object segmentation. 16 SIMPLESEG Figure 12: Visual results of failure cases, e.g., object with holes and texture confusion."
        }
    ],
    "affiliations": [
        "Moonshot AI",
        "Nanjing University"
    ]
}