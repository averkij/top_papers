{
    "paper_title": "A Rigorous Benchmark with Multidimensional Evaluation for Deep Research Agents: From Answers to Reports",
    "authors": [
        "Yang Yao",
        "Yixu Wang",
        "Yuxuan Zhang",
        "Yi Lu",
        "Tianle Gu",
        "Lingyu Li",
        "Dingyi Zhao",
        "Keming Wu",
        "Haozhe Wang",
        "Ping Nie",
        "Yan Teng",
        "Yingchun Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Artificial intelligence is undergoing the paradigm shift from closed language models to interconnected agent systems capable of external perception and information integration. As a representative embodiment, Deep Research Agents (DRAs) systematically exhibit the capabilities for task decomposition, cross-source retrieval, multi-stage reasoning, and structured output, which markedly enhance performance on complex and open-ended tasks. However, existing benchmarks remain deficient in evaluation dimensions, response formatting, and scoring mechanisms, limiting their capacity to assess such systems effectively. This paper introduces a rigorous benchmark and a multidimensional evaluation framework tailored to DRAs and report-style responses. The benchmark comprises 214 expert-curated challenging queries distributed across 10 broad thematic domains, each accompanied by manually constructed reference bundles to support composite evaluation. The framework enables comprehensive evaluation of long-form reports generated by DRAs, incorporating integrated scoring metrics for semantic quality, topical focus, and retrieval trustworthiness. Extensive experimentation confirms the superior performance of mainstream DRAs over web-search-tool-augmented reasoning models, yet reveals considerable scope for further improvement. This study provides a robust foundation for capability assessment, architectural refinement, and paradigm advancement in DRA systems."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 0 9 1 2 0 . 0 1 5 2 : r RIGOROUS BENCHMARK WITH MULTIDIMENSIONAL EVALUATION FOR DEEP RESEARCH AGENTS: FROM ANSWERS TO REPORTS Yang Yao1, 2, Yixu Wang1, 3, Yuxuan Zhang4, Yi Lu5, Tianle Gu1, 6, Lingyu Li1, 7, Dingyi Zhao1, 7, Keming Wu6, Haozhe Wang8, Ping Nie9, Yan Teng1, (cid:66), Yingchun Wang1 1 Shanghai Artificial Intelligence Laboratory, 2 The University of Hong Kong 3 Fudan University, 4 University of British Columbia, 5 University of Toronto, 6 Tsinghua University, 7 Shanghai Jiao Tong University, 8 Hong Kong University of Science and Technology, 9 Peking University yaoyangacademia@outlook.com, tengyan@pjlab.org.cn"
        },
        {
            "title": "ABSTRACT",
            "content": "Artificial intelligence is undergoing the paradigm shift from closed language models to interconnected agent systems capable of external perception and information integration. As representative embodiment, Deep Research Agents (DRAs) systematically exhibit the capabilities for task decomposition, cross-source retrieval, multi-stage reasoning, and structured output, which markedly enhance performance on complex and open-ended tasks. However, existing benchmarks remain deficient in evaluation dimensions, response formatting, and scoring mechanisms, limiting their capacity to assess such systems effectively. This paper introduces rigorous benchmark and multidimensional evaluation framework tailored to DRAs and report-style responses. The benchmark comprises 214 expert-curated challenging queries distributed across 10 broad thematic domains, each accompanied by manually constructed reference bundles to support composite evaluation. The framework enables comprehensive evaluation of long-form reports generated by DRAs, incorporating integrated scoring metrics for semantic quality, topical focus, and retrieval trustworthiness. Extensive experimentation confirms the superior performance of mainstream DRAs over web-search-tool-augmented reasoning models, yet reveals considerable scope for further improvement. This study provides robust foundation for capability assessment, architectural refinement, and paradigm advancement in DRA systems."
        },
        {
            "title": "INTRODUCTION",
            "content": "The core architecture of artificial intelligence paradigms is undergoing transition from closed, static, parameter-driven Large Language Models (LLMs) to active, cognitive, and interconnected agent systems endowed with external perception and integrative mechanisms (Wang et al., 2024; Zhang et al., 2025b). Amid growing demands for heterogeneous information acquisition and increasing task complexity, Deep Research has gradually emerged as paradigm of agent systems characterized by cognitive planning and integrative capabilities (Xu & Peng, 2025), which enable autonomous task decomposition, cross-source retrieval, multi-step reasoning, and structured expression, thereby substantially enhancing model adaptability and expressive performance in real-world applications (Huang et al., 2025; Zhang et al., 2025a). Although Deep Research Agents (DRAs) demonstrate strong task execution capabilities, the current benchmarking framework remains significantly outdated in both design philosophy and coverage scope. First, existing benchmarks predominantly target discrete short-text outputs, such as multiple- (cid:66) Corresponding author. Resources of this paper are available at https://github.com/evigbyen/rigorousbench/. 1 choice answers or brief phrases (Wei et al., 2024; Zhou et al., 2025; Ho et al., 2020; Yang et al., 2018), which, although conducive to efficient evaluation and automation, cannot be extended to complex report-style generation tasks and fail to reflect the demands of DRAs for logical inference and linguistic organization. Second, most benchmarks continue to assess isolated competencies, focusing primarily on reasoning or web search, without establishing systematic criteria for evaluating integrated performance. In particular, mechanisms for assessing citation authority, source validity, and semantic drift in long-form outputs remain absent. Mainstream evaluation methods rely either on string matching (Cohen et al., 2025; Monteiro et al., 2024), which fails to capture semantic adequacy, or on similarity scoring with LLMs as judgers (Chen et al., 2025a; Pham et al., 2025), which lacks transparent and verifiable standards and is therefore prone to subjectivity and instability. To address the limitations inherent in existing benchmarks and evaluation systems, our study proposes rigorous benchmark, authored by human experts and specifically designed for DRAs, which targets high-difficulty and high-precision tasks with the goal of systematically assessing the overall performance in report-style long-text generation. On this basis, we develop multidimensional evaluation framework that is intended to measure both the quality and the credibility of generated report-style responses. Our research yields three key contributions: (1) We introduce rigorous benchmark paired with manually constructed reference bundles comprising query-specific and general-report rubrics, trustworthy source links, focus-anchor and focus-deviation keywords. Covering 214 challenging report-style entries across multiple domains, Rigorous Bench enables broad and granular characterization of DRA tasks, which addresses existing deficiencies in evaluation dimensions and response formats. (2) We develop systematic and multidimensional evaluation framework for report-style outputs, which captures key processes of DRAs such as task reasoning, information retrieval, content synthesis, and structured articulation. By jointly modeling semantic quality, topical focus, and retrieval trustworthiness, our framework overcomes limitations of conventional methods and demonstrates high transferability to long-text generations beyond DRAs. (3) We conduct large-scale experiments involving five mainstream DRAs, one advanced agent model, and seven reasoning models enhanced with web-search tools. Quantitative results indicate that DRAs consistently outperform tool-augmented models in overall task execution and report generation quality, while revealing persistent limitations in architectural paradigms and behavioral mechanisms that warrant further refinement."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "2.1 DEEP RESEARCH AGENTS Early LLMs, such as GPT-3 (Brown et al., 2020) and PaLM (Chowdhery et al., 2023), rely on static training corpora and closed parameter spaces, while their knowledge is entirely derived from the training phase and cannot be updated or supplemented during inference. To overcome the epistemic constraints of static language models, researchers have explored mechanisms for integrating LLMs with external tools, giving rise to the paradigm of Tool-Augmented LLMs, exemplified by GPT4 (Achiam et al., 2023), Gemini 1.5 (Google, 2024), Claude 3 (Anthropic, 2024), Toolformer (Schick et al., 2023), and Qwen 1.5 Agent (Alibaba, 2024). These models leverage external interfaces such as web browsers and code environments to enable dynamic information acquisition and cross-modal perception, reflecting shift from knowledge encapsulation toward tool-mediated cognition. Beyond generative and inferential capabilities, DRAs integrate cognitive planning and information fusion to support end-to-end workflows for complex and open-ended tasks. Their functionality encompasses task recognition, multi-stage decomposition, heterogeneous retrieval, cross-source aggregation, and structured report generation, emphasizing system-level autonomy and procedural integrity. This paradigm is broadly applicable to academic research, policy analysis, technology evaluation, and market intelligence, while remaining applicable to daily-life scenarios for general users. Representative systems include open-source DRAs like Tongyi DeepResearch (Alibaba, 2025) and commercial platforms such as Grok Deep Search (xAI, 2025), Sonar Deep Research (Perplexity, 2025), and o3 Deep Research (OpenAI, 2025), which implement full-spectrum research pipelines. These systems exemplify shift from static knowledge encapsulation to cognitively extended intelligence, advancing LLMs toward agentic architectures with strategic reasoning capabilities."
        },
        {
            "title": "2.2 EXISTING BENCHMARKS",
            "content": "With the growing adoption of Tool-Augmented LLMs, both academia and industry have increasingly focused on their performance in web-search tasks. Existing benchmarks, including GAIA (Mialon et al., 2023), WebWalker (Wu et al., 2025), BrowseComp (Wei et al., 2025), WideSearch (Wong et al., 2025), BrowseComp-Plus (Chen et al., 2025b), and DeepResearch Bench (Bosse et al., 2025), primarily evaluate LLMs using closed-form queries that require verifiable short answers to facilitate automated scoring and alignment. However, they lack coverage of key behaviors such as task decomposition, cross-source retrieval, and structured synthesis, and provide limited assessment of content hierarchy, discourse structure, and information integration. In addition, most rely on surface-level matching or similarity metrics, such as Exact Match, BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee & Lavie, 2005), and BERTScore (Zhang et al., 2019), which struggle to capture semantic depth and structural fidelity, thereby limiting their effectiveness in evaluating the true capabilities of DRAs. Contemporaneous work has begun to explore evaluation methods tailored to report-style outputs. DeepResearch Bench (Du et al., 2025) is the first to assess reference answers alignment and retrieval. However, it depends heavily on static reference reports, making it difficult to accommodate evolving query expectations. Its automated rubrics lack contextual sensitivity and focus on generic surface, failing to reflect human preferences for report quality and structure. Its retrieval evaluation emphasizes consistency between statements and cited links but overlooks the credibility of sources. Benchmarks such as ResearchQA (Yifei et al., 2025), DeepResearch Arena (Wan et al., 2025), and ReportBench (Li et al., 2025) constructed 21K, 10K, and 0.6K academic tasks respectively. These benchmarks also rely on automatically generated rubrics that exhibit limited stability and interpretability, which undermines their reliability for high-precision judgment. Additionally, large-scale benchmarks increase evaluation costs, reducing their practical utility for evaluating DRAs. Overall, existing benchmarks fail to rigorously and comprehensively evaluate report-style long-form outputs in alignment with human expectations. They lack precise rubrics and trustworthy references, making it difficult to systematically characterize the full capabilities of DRAs."
        },
        {
            "title": "3 RIGOROUS BENCH",
            "content": "3.1 DOMAINS The Rigorous Bench dataset was meticulously constructed and repeatedly validated by human experts, comprising 214 high-complexity entries across diverse domains. It is designed to challenge existing DRAs in task understanding, decomposition, execution, and aggregation. Based on semantic relevance, entries are systematically categorized into ten broad domains, with detailed distribution provided in Figure 1 and Appendix A.1. The dataset exhibits high degree of professionalism and diversity in its design, effectively simulating complex and dynamic queries encountered in real-world scenarios, and demonstrating broad coverage and strong representativeness. 3.2 COMPONENTS Figure 1: Distribution of benchmark entries. Each entry consists of query instruction paired with reference bundle designed to support performance evaluation. The bundle comprises five core modules: Query-Specific Rubrics (QSRs), General-Report Rubrics (GRRs), Trustworthy-Source Links (TSLs), Focus-Anchor Keywords (FAKs), and Focus-Deviation Keywords (FDKs), each of which corresponds to distinct capability that DRA is expected to demonstrate."
        },
        {
            "title": "3.2.1 QUERY",
            "content": "As the core input to DRAs, queries are designed to elicit structured long-form reports rather than traditional short, discrete answers, while embodying diversity and representativeness by (1) covering topics from ancient history to contemporary events, including both long-term technological evolution and short-term dynamic shifts; (2) encompassing major countries and regions, with systematically designed cross-regional comparative tasks; (3) involving diverse disciplines, with many tasks intentionally designed for interdisciplinary integration; (4) entailing structured texts across various styles including academic writing, data-driven analysis, technical deconstruction, strategic planning, policy evaluation, and event reconstruction; (5) incorporating multi-level causal chains and cross-source information fusion; and (6) including both quantitative and qualitative analysis. To enhance consistency and reproducibility, Rigorous Bench systematically incorporates spatiotemporal robustness into the design of queries and rubrics. For fact-based tasks, each query explicitly defines temporal and geographic boundaries to mitigate external fluctuations in response content. For open-ended tasks, rubrics emphasize logical structure and reasoning validity to ensure generalizability and adaptability. All queries adopt the instruction write report/essay/... to unify output format and reinforce task orientation. Overall, the query design achieves high degree of semantic complexity, task diversity, and evaluative operability. 3.2.2 QUERY-SPECIFIC RUBRICS QSRs are designed to evaluate the task completion quality. Each QSR is custom-built by experts based on corresponding query, reflecting human expectations and evaluative preferences regarding factual accuracy and logical validity. Scoring follows clearly defined binary (Yes/No) or ternary (Yes/Partial/No) scheme to ensure consistency and operational clarity. Each query is equipped with at least 8 QSRs, with total score of 30, assigned in accordance with task importance. QSRs are deeply embedded within the semantic structure of each task, offering high alignment and diagnostic precision. Their design spans core dimensions such as information coverage, mechanism explanation, structured expression, semantic precision, source verification, evidence organization, heterogeneity analysis, methodological transparency, temporal logic, and interdisciplinary integration. The QSRs provide both theoretical grounding and practical guidance for evaluating DRAs performance, while also laying structural foundation for future automated scoring systems. 3.2.3 GENERAL-REPORT RUBRICS GRRs are designed to assess the quality of structured expression. Independent of specific queries, GRRs evaluate reports from general perspective using binary judgment across seven key dimensions, namely structural organization, logical clarity & expression, informational coverage & content depth, citation quality & source credibility, originality & insight, data usage & analytical rigor, and formatting consistency. Comprising 48 rubrics with total score of 73, the design emphasizes generalizability, normative clarity, and expressive strength, providing unified quality standard for performance evaluation across tasks and models, as outlined in Appendix A.2. 3.2.4 TRUSTWORTHY-SOURCE LINKS TSLs serve as indicators of the trustworthiness of heterogeneous retrieval and cross-source aggregation. They are designated by experts in variable quantities, and consist of durable, stable, and reliable website links that are authoritative, official, accessible, and contain original information necessary to answer the query. Subjective or non-primary sources such as forums and blogs are excluded. Each link is precisely anchored to the specific page containing the target information, ensuring accuracy, verifiability, and confidence in information acquisition. 3.2.5 FOCUS-ANCHOR KEYWORDS FAKs are used to evaluate thematic focus during cross-source content aggregation. Each set of FAKs is specified by expert designers for the given query and consists of 5 semantically stable core terms, while avoiding superficial phrasing from the query itself. FAKs serve to assess the thematic focus and key point coverage of generated content, enabling effective evaluation in terms of semantic consistency and analytical depth. Figure 2: Pipeline for benchmark construction and overview of the evaluation framework. 3.2.6 FOCUS-DEVIATION KEYWORDS FDKs are used to evaluate the degree of thematic drift. Each set consists of five terms that are prone to triggering topic divergence. Their presence typically indicates that the generated content has deviated from the original query focus, resulting in reduced semantic coherence and increased informational noise. In high-cost DRA processes, such deviations lead to unnecessary consumption of resources, ultimately compromising overall performance. 3.3 CONSTRUCTION PIPELINE To ensure the difficulty, quality, and semantic validity, Rigorous Bench adopts multi-stage review pipeline for systematic verification and refinement of expert-generated entries. This process integrates manual design, machine auditing, and cross-validation to establish highly granular framework for data generation and quality control, as illustrated in the upper half of Figure 2. In the initial stage, experts design diverse data units based on the unified Construction Guide and their own expertise within predefined domains. The units then undergo LLMs auditing to detect semantic inconsistencies, logical errors, and factual inaccuracies. During the first round of manual review, QSRs are verified for validity, with rubric criteria refined. For binary items with intermediate cases, clearly defined Partial option is introduced to enhance scoring granularity and consistency. After preliminary filtering, queries and QSRs are standardized and rewritten to ensure stylistic and structural correctness. TSLs and Keywords are also supplemented and revised at this stage. The revised entries then undergo LLM-based difficulty testing, followed by the second round of manual review, which focuses on evaluating QSRs from observers perspective and assessing the anchoring effectiveness of keywords. Each link is individually verified for accessibility and authority. The third round of cross-review conducts comprehensive quality check, covering domain classification, formatting standards, and structural integrity. Examples of entries can be found in Appendix A.3. This multi-phase mechanism significantly improves accuracy, consistency, and reproducibility, while also reducing subjective bias and annotation errors, providing robust foundation for model evaluation."
        },
        {
            "title": "4 EVALUATION FRAMEWORK",
            "content": "Built on semantic quality, topical focus, and retrieval trustworthiness, structured and multidimensional evaluation framework is proposed for report-style generation tasks, featuring an integrated scoring system inspired by principles from statistics and operations. It prioritizes transparency, interpretability, and practical utility for robust evaluation of complex tasks, as illustrated in Figure 2."
        },
        {
            "title": "4.1 SEMANTIC QUALITY",
            "content": "Semantic quality evaluates the overall performance of response reports in terms of task completion and general quality. It integrates scores from both QSRs and GRRs. Drawing on the Weighted Average Method (WAM) from Multi-Attribute Decision Making (MADM), the metric applies ratio normalization to both scores and assigns weighting coefficients α and β, where α + β = 1, to construct fused model of multidimensional quality signals. The calculation formula is as follows: Quality = α NRatio (cid:35) QSR(i) score + β NRatio (cid:34) (cid:88) i=1 (cid:88) j=1 GRR(j) score [0, 1] score denotes the i-th QSR score; GRR(j) where QSR(i) score refers to the j-th GRR score; and represent the number of QSRs and GRRs respectively; NRatio[] denotes the ratio normalization; α and β are weighting parameters reflecting the relative importance in the overall quality assessment. 4.2 TOPICAL FOCUS Topical focus is evaluated through the SemanticDrift metric, which jointly considers the absence of FAKs and the misuse of FDKs to measure the degree of thematic deviation in response reports. FAKDrift quantifies the omission of core keywords. The frequency of each keyword in the report is scaled by its expected value ϵ, and minimum function is introduced to implement threshold control. FAK is penalized when its frequency falls below ϵ, thereby enforcing the requirement for substantive keyword presence and ensuring that the score reflects semantic completeness rather than accidental occurrence. Drawing on the TF IDF paradigm from information retrieval, FAKDrift is constructed as the product of frequency component and semantic relevance, which is defined as: FAKDrift = 1 1 (cid:34) (cid:88) k= min (cid:32) freq(k) ϵ+ (cid:33) , 1 (cid:35) NRatio(rele(k)) [0, 1] where freq(k) is frequency of the k-th FAK; rele(k) [1, 2, 3, 4, 5] is the semantic relevance assessed by LLMs; represents the number of FAKs; ϵ+ is the expectation scaling factor. higher FAKDrift value indicates weaker thematic focus and insufficient coverage of core concepts in the report. Similarly, FDKDrift, which quantifies the intensity of thematic distraction, is defined as follows: FDKDrift = 1 (cid:88) l= (cid:34) (cid:32) min freq(l) ϵ (cid:33) , (cid:35) NRatio(rele(l)) [0, 1] where freq(l) is frequency of the l-th FDK; rele(l) [1, 2, 3, 4, 5] is the relevance score; represents the number of FDKs; ϵ is the scaling factor. higher FDKDrift indicates stronger thematic deviation. The SemanticDrift is computed as weighted combination of FAKDrift and FDKDrift: SemanticDrift = λ FAKDrift + µ FDKDrift [0, 1] where λ + µ = 1, reflecting the relative importance. SemanticDrift reflects the degree of thematic deviation in the generated report, with higher values indicating weaker alignment to the intended topic and lower values suggesting stronger semantic focus and consistency. 4.3 RETRIEVAL TRUSTWORTHINESS Retrieval Trustworthiness evaluates the credibility of external information retrieval and usage in response reports. Modeling based on the hit rate of TSLs and using confidence enhancement mechanism inspired by multiplicative fusion in Bayesian updating, this approach transforms match rates 6 into multiplicative scoring factors to increase the evaluative weight of citation quality. Specifically, matches is categorized into full matches and hostname matches, where Ratefull hit serves as recalllike metric capturing the precise coverage of provided TSLs, and Ratehost hit reflects the proportion of generalized mentions whose annotation links share the same source domains as the recommended references. These two are assigned different weights and combined to compute the TrustworthyBoost factor as follows: (cid:124) (cid:20) (cid:21) (cid:19) (cid:19) θ +κ [1, 1 + η] TrustworthyBoost = 1 + η (cid:18) matchhost + 1 (cid:123)(cid:122) Ratehost hit (cid:18) matchfull (cid:123)(cid:122) Ratefull hit where matchfull indicates the number that have been exactly matched in TSLs; matchhost refers to the number of annotation links sharing the same hostname as the TSLs; and denote the sizes of the TSLs and the annotations respectively; θ and κ represent the weights for full and hostname matches respectively, with θ+κ = 1. The coefficient η = 0.2 controls the magnitude of the confidence boost, thereby preventing excessive inflation due to high confidence and avoiding complete nullification when confidence is low. This mechanism preserves scoring stability while enhancing evaluative sensitivity to verifiability and source reliability, thereby improving the responsiveness to external evidence signals. (cid:125) (cid:124) (cid:125) 4.4 INTEGRATED SCORING FRAMEWORK The integrated scoring metric adopts multiplicative weighting model to enable multidimensional evaluation of report-style generation tasks. The calculation formula is defined as: IntegratedScore = Quality (1 SemanticDrift) TrustworthyBoost 100 [0, 120] Here, Quality assesses the structural integrity and content quality of the report, SemanticDrift reflects the degree of thematic deviation and is transformed into positive scoring factor via 1 SemanticDrift, and TrustworthyBoost enhances the credibility weight based on authoritative link coverage. This design logic penalizes semantic drift and rewards external support, producing normalized score on 100-point scale. As shown in Algorithm 1, the framework addresses limitations in traditional evaluation methods when applied to DRAs. It offers strong scalability and transferability, making it broadly applicable to performance assessment of structured long-text generation tasks, particularly those involving tool-augmented systems. 4.5 ADDITIONAL METRICS Extract query, QSRs, GRRs, TSLs, FAKs, FDKs Extract report, annotations, token usage # Semantic Quality QSRscores (cid:80) QSRscore from LLM-Judger over QSRs GRRscores (cid:80) GRRscore from LLM-Judger over GRRs Quality α NRatio(QSRscores) + β NRatio(GRRscores) # Retrieval Trustworthiness Ratefull hit Matchfull/TSLs Ratehost hit (Matchhost Matchfull)/annotations + 1 TrustworthyBoost 1 + η (θ Ratefull hit + κ Ratehost hit) # Topical Focus for FAK in FAKs do Algorithm 1 Multidimensional Evaluation Framework 1: for entry in entries do 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: end for 25: # Aggregate Metrics 26: InteScore average of InteScore over entries 27: ContriPerToken average of ContriPerToken over entries end for FAKDrift 1 (cid:80) FAKscore/FAKs Perform same procedure for FDKs to compute FDKDrift SemanticDrift λ FAKDrift + µ FDKDrift # Integrated Scoring Framework InteScore Quality (1 SDrift) TBoost 100 ContriPerToken InteScore/(tokentotal tokeninput) relevance LLM-Judger(FAK, report) frequency count of FAK in report FAKscore min(frequency/ϵ+, 1) NRatio(relevance) To enable more comprehensive assessment of DRAs ability, we design set of supplementary metrics based on accessible response metadata to characterize model performance and efficiency in 7 real-world execution, including token consumption, number of reasoning steps, and the volume of links involved during retrieval. ContributionPerToken = IntegratedScore token total token input To evaluate cost-effectiveness under limited resources, the Contribution/Token metric measures model efficiency based on actual token expenditure from reasoning and generation. RetrievalIndex = numannotated numretrieved + 1 [0, 1] RetrievalIndex evaluates the filtering and aggregation capability of DRAs during the retrieval process. It is defined as the ratio between the number of annotations ultimately adopted in the report and the total number of links retrieved during the search phase, reflecting the models ability to effectively distill valuable content from large-scale information. lower index indicates stronger selectivity and aggregation, suggesting that the DRA can extract more relevant and informative content from redundant sources, thereby enhancing the specificity and density of the generated output."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5.1 OVERVIEW thirteen models under Rigorous Benchmark, including five We evaluated total of (o3-deep-research-2025-06-26, qwen-deep-research, sonar-deepDRAs research, grok-4-0709-search, and o4-mini-deep-research-2025-06-26), one advanced agent (kimi-k2-0905-preview), and seven web-search-tool-enhanced reasoning models (gemini-2.5-pro, gpt-5-2025-08-07, gpt-4o-search-preview-202503-11, gpt-4.1-2025-04-14, claude-opus-4-1-20250805, claude-sonnet-420250514, and claude-3-7-sonnet-20250219). To eliminate the randomness, all models with adjustable temperature were evaluated at temperature = 0.0. For non-DRA models without embedded annotations, reports were merged with annotations during evaluation. gpt-4o-2024-11-20 independently scored each rubric to evaluate semantic quality. Topical focus was assessed on pure report text without annotations to avoid interference from annotation titles. Related prompts are provided in Appendix A.4. Retrieval Trustworthiness was computed from pure annotations, excluding parameters, anchors, and duplicates to ensure fair matching. Manual verification was randomly conducted on approximately 35% of the scores judged by LLMs, yielding 99.3% agreement with human evaluations. Regarding parameters, α = β = 0.5 were set to balance task completion and general quality; λ = 0.7 and µ = 0.3 were configured to emphasize sensitivity to the omission of FAKs; coefficient was set to η = 0.2 to control score inflation and prevent over-amplification; θ = 0.7 and κ = 0.3 were set for the contributions of exact citations and generalized mentions. This configuration ensures scoring stability while enhancing sensitivity to semantic relevance, citation accuracy, and task fidelity. 5.2 LEADERBOARD Table 1 and Appendix A.5 presents the leaderboard results, with models ranked in descending order of IntegratedScore. In terms of IntegratedScore, Qwen ranked first, demonstrating strong performance across all dimensions. Sonar followed closely, achieving the highest score in topical focus. Notably, Kimi-K2, Mixture-of-Experts architecture agent with 1T parameters, attained the highest score in the quality, outperforming all DRAs. For topical focus, Sonar, Qwen, and o3 formed the leading cluster. GPT-5 achieved the highest score in citation reliability, indicating strong external support alignment. In terms of resource consumption, o3 and o4-mini averaged 23K and 18K tokens per report respectively, placing them lowest in contribution efficiency. Overall, DRAs exhibited stable semantic quality and control in structured report generation, while web-search-tool-augmented models showed promise in external support and efficiency. These results validate the Rigorous Benchs ability to differentiate model capabilities across multiple dimensions, providing robust empirical foundation for future optimization of DRAs. 8 Table 1: Results with the highest score in bold and the second-highest underlined per column. Models Qwen-deep-research Sonar-deep-research o3-deep-research-2025-06-26 Kimi-K2-0905-preview Grok-4-0709-search Gemini-2.5-pro o4-mini-deep-research-2025-06-26 GPT-5-2025-08-07 GPT-4o-search-preview-2025-03-11 GPT-4.1-2025-04-14 Claude-opus-4-1-20250805 Claude-sonnet-4-20250514 Claude-3-7-sonnet-20250219 Quality 0.6348 0.6184 0.6176 0.6707 0.6130 0.5506 0.5666 0.5560 0.4945 0.4762 0.4559 0.4491 0. 1SDrift TBoost 1.0288 1.0238 1.0171 1.0153 1.0283 1.0130 1.0203 1.0383 1.0073 1.0027 1.0202 1.0184 1.0148 0.5248 0.5271 0.5184 0.4671 0.4890 0.4856 0.4803 0.4593 0.4496 0.4694 0.4674 0.4735 0.4737 InteScore Usage C/Token 34.6480 0.0100 9258 0.0043 8254 33.4668 25038 0.0014 32.9004 0.0164 2079 32.0651 0.0112 3012 31.3490 0.0072 5446 27.3364 0.0016 18640 28.0391 0.0045 7006 27.3312 0.0247 1005 22.5645 0.0194 1252 22.4382 0.0101 2267 22.0047 0.0097 2267 21.7235 0.0084 2327 19.3415 5.3 SUPPLEMENTARY DIMENSIONS Table 2: Average inference and retrieval times. Additionally, four OpenAI models yielded richer response metadata, revealing strategic differences in task execution and tool usage beyond the three core dimensions, and offering valuable supplement into multidimensional evaluation. As shown in Table 2, GPT-4.1 showed minimal retrieval activity, with an average of only 0.39 times. In contrast, both o4-mini and o3 displayed intensive inference and retrieval patterns, suggesting more complex reasoning chains and information acquisition strategies. In terms of retrieval efficiency, o3 slightly exceeded o4-mini in total retrieved links and annotations, while o4-mini achieved marginally higher RetrievalIndex, reflecting stronger filtering and citation precision, as shown in Table 3. ReasonTimes Unavailable 12.9346 55.4333 63.9860 SearchTimes 0.3925 10.6729 16.1000 26.5093 numanno RIndex 0.5520 7.7986 0.5804 8.7561 Models GPT-4.1 GPT-5 o3-dr o4-mini-dr Table 3: RetrievalIndex of o3 and o4-mini. Models o4-mini-dr o3-dr numretr 14.4583 15."
        },
        {
            "title": "6 DISCUSSIONS",
            "content": "Two systemic limitations emerged during evaluation that underscore key challenges in the design of DRAs. First, instability in invocation behavior was observed in models such as o3 and o4-mini, which exhibited substantial variance in reasoning time across repeated queries. This suggests lack of internal constraints governing search frequency and direction, resulting in non-convergent retrieval paths and inconsistent response behavior. Second, semantic decomposition occasionally produced sub-queries in non-English languages with incoherent semantics, despite all tasks being in English. These outputs were misaligned with task intent and unintelligible to human evaluators, thereby impairing retrieval precision and relevance. These limitations reflect two fundamental trade-offs in DRAs development. The efficiencyquality trade-off highlights the tension between high-quality reasoning and computational cost, with current models often incurring excessive token usage and latency. Addressing this requires adaptive control over search depth and token allocation. Meanwhile, the decompositioncoherence trade-off reveals that while modular query breakdown enhances coverage, it risks semantic fragmentation and intent drift. Future architectures must reconcile decomposition benefits with coherent multi-stage reasoning to ensure consistent task fidelity."
        },
        {
            "title": "7 CONCLUSIONS",
            "content": "In this paper, we present the Rigorous Bench and the multidimensional evaluation framework that systematically assesses the performance and capabilities of DRAs. By leveraging challenging 9 queries across diverse thematic domains and high-quality reference bundles, our framework enables rigorous evaluation of report-style outputs along axes of semantic quality, topical focus, and retrieval trustworthiness. Empirical results show that contemporary DRAs substantially outperform conventional tool-augmented models in complex task scenarios, while also exposing key limitations and trade-offs. These insights elucidate current challenges in DRA design and provide foundation for the development of DRAs as efficient, stable, and interpretable intelligent agents."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Alibaba. Qwen agent, 2024. https://github.com/QwenLM/Qwen-Agent?tab= readme-ov-file/. Alibaba. Tongyi deepresearch. https://github.com/Alibaba-NLP/DeepResearch, 2025. Anthropic. Claude 3, 2024. https://www.anthropic.com/news/claude-3-family/. Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pp. 6572, 2005. Nikos Bosse, Jon Evans, Robert Gambee, Daniel Hnyk, Peter Muhlbacher, Lawrence Phillips, Dan Schwarz, Jack Wildman, et al. Deep research bench: Evaluating ai web research agents. arXiv preprint arXiv:2506.06287, 2025. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Kaiyuan Chen, Yixin Ren, Yang Liu, Xiaobo Hu, Haotong Tian, Tianbao Xie, Fangfu Liu, Haoye Zhang, Hongzhang Liu, Yuan Gong, et al. xbench: Tracking agents productivity scaling with profession-aligned real-world evaluations. arXiv preprint arXiv:2506.13651, 2025a. Zijian Chen, Xueguang Ma, Shengyao Zhuang, Ping Nie, Kai Zou, Andrew Liu, Joshua Green, Kshama Patel, Ruoxi Meng, Mingyi Su, et al. Browsecomp-plus: more fair and transparent evaluation benchmark of deep-research agent. arXiv preprint arXiv:2508.06600, 2025b. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240): 1113, 2023. Dvir Cohen, Lin Burg, Sviatoslav Pykhnivskyi, Hagit Gur, Stanislav Kovynov, Olga Atzmon, and Gilad Barkan. Wixqa: multi-dataset benchmark for enterprise retrieval-augmented generation. arXiv preprint arXiv:2505.08643, 2025. Mingxuan Du, Benfeng Xu, Chiwei Zhu, Xiaorui Wang, and Zhendong Mao. Deepresearch bench: comprehensive benchmark for deep research agents. arXiv preprint arXiv:2506.11763, 2025. Google. Gemini 1.5, 2024. https://deepmind.google/models/gemini/. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing multi-hop qa dataset for comprehensive evaluation of reasoning steps. arXiv preprint arXiv:2011.01060, 2020. Yuxuan Huang, Yihang Chen, Haozheng Zhang, Kang Li, Meng Fang, Linyi Yang, Xiaoguang Li, Lifeng Shang, Songcen Xu, Jianye Hao, et al. Deep research agents: systematic examination and roadmap. arXiv preprint arXiv:2506.18096, 2025. 10 Minghao Li, Ying Zeng, Zhihao Cheng, Cong Ma, and Kai Jia. Reportbench: Evaluating deep research agents via academic survey tasks. arXiv preprint arXiv:2508.15804, 2025. Chin-Yew Lin. Rouge: package for automatic evaluation of summaries. In Text summarization branches out, pp. 7481, 2004. Gregoire Mialon, Clementine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: In The Twelfth International Conference on Learning benchmark for general ai assistants. Representations, 2023. Joao Monteiro, Pierre-Andre Noel, Etienne Marcotte, Sai Rajeswar, Valentina Zantedeschi, David Vazquez, Nicolas Chapados, Christopher Pal, and Perouz Taslakian. Repliqa: questionanswering dataset for benchmarking llms on unseen reference content. Advances in Neural Information Processing Systems, 37:2424224276, 2024. OpenAI. Openai deep research, 2025. https://openai.com/zh-Hans-CN/index/ introducing-deep-research/. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pp. 311318, 2002. Perplexity. Sonar deep research, 2025. https://sonar.perplexity.ai/. Thinh Pham, Nguyen Nguyen, Pratibha Zunjare, Weiyuan Chen, Yu-Min Tseng, and Tu Vu. Sealqa: Raising the bar for reasoning in search-augmented language models. arXiv preprint arXiv:2506.01062, 2025. Timo Schick, Jane Dwivedi-Yu, Roberto Dess`ı, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36:68539 68551, 2023. Haiyuan Wan, Chen Yang, Junchi Yu, Meiqi Tu, Jiaxuan Lu, Di Yu, Jianbao Cao, Ben Gao, Jiaqing Xie, Aoran Wang, et al. Deepresearch arena: The first exam of llms research abilities via seminargrounded tasks. arXiv preprint arXiv:2509.01396, 2025. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6):186345, 2024. Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, and William Fedus. Measuring short-form factuality in large language models. arXiv preprint arXiv:2411.04368, 2024. Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: simple yet challenging benchmark for browsing agents. arXiv preprint arXiv:2504.12516, 2025. Ryan Wong, Jiawei Wang, Junjie Zhao, Li Chen, Yan Gao, Long Zhang, Xuan Zhou, Zuo Wang, Kai Xiang, Ge Zhang, et al. Widesearch: Benchmarking agentic broad info-seeking. arXiv preprint arXiv:2508.07999, 2025. Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Linhai Zhang, Yulan He, Deyu Zhou, Pengjun Xie, et al. Webwalker: Benchmarking llms in web traversal. arXiv preprint arXiv:2501.07572, 2025. xAI. Grok model card, 2025. 2025-09-19-grok-4-fast-model-card.pdf/. https://data.x.ai/ Renjun Xu and Jingwen Peng. comprehensive survey of deep research: Systems, methodologies, and applications. arXiv preprint arXiv:2506.12594, 2025. 11 Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018. Li Yifei, Allen Chang, Chaitanya Malaviya, and Mark Yatskar. Researchqa: Evaluating scholarly question answering at scale across 75 fields with survey-mined questions and rubrics. arXiv preprint arXiv:2509.00496, 2025. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675, 2019. Weizhi Zhang, Yangning Li, Yuanchen Bei, Junyu Luo, Guancheng Wan, Liangwei Yang, Chenxuan Xie, Yuyao Yang, Wei-Chieh Huang, Chunyu Miao, et al. From web search towards agentic deep research: Incentivizing search with reasoning agents. arXiv preprint arXiv:2506.18959, 2025a. Wenlin Zhang, Xiaopeng Li, Yingyi Zhang, Pengyue Jia, Yichao Wang, Huifeng Guo, Yong Liu, and Xiangyu Zhao. Deep research: survey of autonomous research agents. arXiv preprint arXiv:2508.12752, 2025b. Junting Zhou, Wang Li, Yiyan Liao, Nengyuan Zhang, Tingjia Miaoand Zhihui Qi, Yuhan Wu, and Tong Yang. Academicbrowse: Benchmarking academic browse ability of llms. arXiv preprint arXiv:2506.13784, 2025."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 TAXONOMY OF DOMAINS comprises classification scheme Figure 3 and Table 4 present the domain taxonomy underlying the benchmark corpus. ten The principal domains, delineated according to thematic relevance: Academia & Research, News & Current Affairs, Sports Commonsense & Competitions, Law & Politics, & Education, Technology Business & Finance, Intelligence, Environment & Sustainability, History & Social and Health & Medicine. Sciences, Entries that do not align with the predefined domains are assigned to the residual class Unclassified, in order to preserve high-quality data while maintaining diversity. is evident that Business & Finance It and History & Social Sciences account for relatively larger proportion of the benchmark. Nevertheless, the overall distribution remains broadly balanced, exhibiting both thematic richness and representational breadth. Figure 3: Taxonomy of domains. A.2 GENERAL-REPORT RUBRICS complete listing of the 48 GRRs, used to evaluate general reporting quality, is shown in Figure 4. 12 Table 4: Taxonomy of domains with simplified descriptions. Code Domain 00 01 02 03 04 05 06 07 08 09 10 Summary Unclassified Academia & Research News & Current Affairs Sports & Competitions Commonsense & Education Law & Politics Business & Finance Technology Intelligence Environment & Sustainability History & Social Sciences Health & Medicine Description Cannot be categorized Academic trends, research methods, etc. International news, regional hotspots, etc. Olympics, World Cup, athlete data, match reports, etc. Common facts, educational resources, etc. Legal texts, policy updates, international relations, etc. Market analysis, investment strategies, etc. Artificial intelligence, tech trends, etc. Climate change, environmental policies, ecosystems, etc. History, philosophy, sociology, psychology, etc. Disease research, public health, nutrition knowledge, etc. Count 3 23 18 15 15 24 35 17 12 33 19 214 Figure 4: Detailed criteria for General-Report Rubrics. 13 A.3 EXAMPLES OF ENTRIES To illustrate the precision and rigor of our benchmark, we present four representative entries: ID 040216, 07001, 08004, and 10236. These entries span distinct domains and query types and serve as concrete examples of structural completeness, rubric coverage, and citation fidelity. Figure 5: Example ID 040216 of benchmark entries. 14 Figure 6: Examples ID 07001 and 10236 of benchmark entries. 15 Figure 7: Example ID 08004 of benchmark entries. 16 A.4 SCORING PROMPTS As part of the rubric evaluation process, the following prompt was provided to guide scoring consistency and rule adherence. You are scoring evaluator tasked with assessing the quality of report generated by deep research model. You will be provided with: report text An evaluation rule containing specific scoring criteria and 1. 2. allowed score values Your task is to: - Carefully read the report - Evaluate it strictly against the given rule - Assign score based only on the score values defined in the rule Scoring instructions: - Only use the score values explicitly listed in the rule - Do not invent intermediate scores or alternative formats - Your output must begin with the score in square brackets [], followed by one-sentence reason Output format example: [0] No citations were provided, which violates the requirement. [2] The report fully meets the requirement with clear and relevant details. Be objective and consistent. and adherence to the rule. Report text: Rule: {report} {rubric} Focus on clarity, completeness, relevance, As part of the keyword relevance evaluation stage, the following prompt was provided to guide consistent scoring. You are scoring evaluator tasked with assessing the relevance of specific keyword within research report. You will be provided with: 1. 2. report text keyword to evaluate Your task is to: - Read the report carefully - Judge how semantically relevant the keyword is to the report - Consider not just frequency, but depth of discussion, thematic importance, and contextual integration Use the following 5-point relevance scale: The keyword is central theme of the The keyword is major topic; It appears more than once (5) Extremely Relevant: report; It appears multiple times and is discussed in depth; The reports main arguments or findings revolve around it; (4) Highly Relevant: and is clearly explained or referenced; contributes directly to the reports purpose; (3) Moderately Relevant: but not emphasized; It may appear once or twice; It supports the report contextually but is not focus; (2) Slightly Relevant: keyword is briefly mentioned; It has little impact on the reports core content; It may be incidental or peripheral; (1) Not Relevant: The keyword does not appear in the report; Or it appears in way that is unrelated to the reports topic. The keyword is mentioned The Output format example: [4] The keyword \"QUIC\" is referenced multiple times in the report, particularly in the context of protocol evolution and RFC publication. While not the sole focus, it is clearly major topic. Focus on clarity, completeness, relevance, Be objective and consistent. and adherence to the rule. Report text: Keyword: {keyword} {report} A.5 SUPPLEMENTARY EXPERIMENTAL OBSERVATIONS A.5.1 QUALITY SHARE PROPORTION Figure 8: Quality share proportion across models. Figure 8 illustrates the contribution of Quality scores to the overall IntegratedScore. As core metric among the three evaluation dimensions, Qualitys share proportion provides direct indication of each DRAs capacity for textual precision, thematic focus, and control over external referencing. Notably, although Kimi-K2 achieves prominent score in the Quality dimension, its relative deficiencies in credibility and attention metrics diminish the extent to which its quality advantage is reflected in the overall score. In contrast, models such as Qwen, Sonar, and o3 demonstrate more balanced allocation between Quality and multiplicative factors, exhibiting great integrative performance while leaving room for further optimization. A.5.2 EVALUATION ACROSS DOMAINS Table 5 presents fine-grained evaluation of various models across distinct domains. In this table, denotes the domain, and refers to the evaluation metrics, including QUA (Quality), SDR (1SemanticDrift), TBO (TrustworthyBoost), and ITS (IntegratedScore). Each column corresponds to model represented by an abbreviation: QWE (qwen-deep-research), SON (sonar-deep-research), KIM (kimi-k2-0905-preview), GRO (grok-4-0709-search), GEM (gemini-2.5-pro), O4D (o4-mini-deep-research-2025-06-26), GT5 (gpt-5-2025-08-07), G4O OPU (gpt-4o-search-preview-2025-03-11), (claude-opus-4-1-20250805), SO4 (claude-sonnet-4-20250514), and S37 (claude-3-7-sonnet-20250219). This table is designed to highlight model-specific performance variations across multiple dimensions, providing structured basis for subsequent analysis. O3D (o3-deep-research-2025-06-26), (gpt-4.1-2025-04-14), G41 The distribution of ITS values reveals substantial variation in model performance across domains. In particular, QWE, SON, and O3D consistently achieve higher scores across multiple domains. Their ITS values are notably elevated in domain 03, where they reach 41.27, 38.10, and 40.23 respectively, and in domain 10, with corresponding scores of 39.07, 38.86, and 37.68. These results indicate their stable advantages in these specific contexts. In contrast, models such as S37, SO4, and OPU tend to exhibit lower scores across most domains, reflecting performance floor. This distribution suggests that models vary in their domain sensitivity and adaptability, with certain systems demonstrating enhanced integrative capabilities under domain-specific conditions. 18 Table 5: Comparative evaluation across domains. QWE SON O3D KIM GRO GEM O4D GT5 G4O OPU SO4 S37 01 02 04 05 06 07 08 10 QUA 0.6273 0.6090 0.6340 0.6705 0.6465 0.5774 0.5690 0.5439 0.5013 0.4690 0.4288 0.4362 0.3800 SDR 0.5184 0.5340 0.5457 0.4664 0.5089 0.4873 0.4826 0.4491 0.4667 0.4715 0.4584 0.4616 0.4765 TBO 1.0209 1.0357 1.0304 1.0373 1.0423 1.0161 1.0351 1.0476 1.0102 1.0021 1.0337 1.0245 1.0209 ITS 33.3293 33.8947 36.0683 32.7436 34.4484 29.1322 28.9792 26.8605 23.6181 22.3786 21.3256 20.8999 18.9583 QUA 0.6190 0.6331 0.6108 0.6432 0.6077 0.5730 0.5572 0.4939 0.5160 0.4937 0.4457 0.4562 0.3887 SDR 0.5112 0.5282 0.4943 0.4324 0.4760 0.4841 0.4668 0.4276 0.4668 0.4722 0.4508 0.4673 0.4751 TBO 1.0146 1.0131 1.0210 1.0070 1.0222 1.0099 1.0158 1.0126 1.0063 1.0055 1.0175 1.0149 1.0096 ITS 32.0267 34.0071 31.8369 28.4567 30.1611 28.1130 26.3594 22.3089 24.4736 23.4507 20.9942 21.8205 18.6347 QUA 0.6725 0.6198 0.6462 0.7139 0.6414 0.5513 0.5438 0.5349 0.5211 0.4551 0.4928 0.4579 0.4307 SDR 0.5909 0.6008 0.6053 0.5756 0.5651 0.5545 0.5743 0.4812 0.5316 0.5283 0.5265 0.5719 0.5875 TBO 1.0441 1.0248 1.0195 1.0158 1.0160 1.0062 1.0220 1.0307 1.0065 1.0000 1.0149 1.0151 1.0147 ITS 41.2741 38.1028 40.2341 41.8823 37.3244 31.4358 31.6925 27.7716 28.2281 24.3922 26.6635 26.5932 25.6712 QUA 0.6123 0.6006 0.6465 0.6368 0.5766 0.5329 0.5488 0.5608 0.4589 0.4530 0.4394 0.4250 0.3690 SDR 0.5155 0.4971 0.5094 0.4789 0.4628 0.4833 0.4807 0.4560 0.3981 0.4401 0.4460 0.4287 0.4259 TBO 1.0108 1.0218 1.0142 1.0132 1.0322 1.0186 1.0163 1.0213 1.0126 1.0065 1.0137 1.0108 1.0219 ITS 31.6576 30.1116 33.7925 31.1660 27.8452 26.0326 26.9029 27.1555 19.0310 20.2277 19.8213 18.3432 16.1287 QUA 0.5891 0.6012 0.5678 0.6463 0.6066 0.5390 0.5694 0.5517 0.4804 0.4664 0.4435 0.4295 0.3900 SDR 0.5031 0.5142 0.4673 0.4249 0.4728 0.4748 0.4649 0.4461 0.4268 0.4623 0.4676 0.4548 0.4534 TBO 1.0153 1.0270 1.0061 1.0075 1.0288 1.0108 1.0262 1.0301 1.0052 1.0054 1.0187 1.0106 1.0085 ITS 30.8795 32.4488 27.2325 28.1336 30.3342 26.6297 27.8097 25.7082 20.3981 21.7361 21.0149 19.8689 17.8701 QUA 0.6257 0.6143 0.6039 0.6840 0.6051 0.5214 0.5319 0.5450 0.4816 0.4567 0.4591 0.4520 0.4036 SDR 0.5320 0.5099 0.5205 0.4856 0.4994 0.4701 0.4617 0.4649 0.4382 0.4671 0.4613 0.4739 0.4685 TBO 1.0490 1.0201 1.0106 1.0094 1.0347 1.0079 1.0112 1.0235 1.0035 1.0035 1.0139 1.0137 1.0103 ITS 35.9545 32.1937 32.0019 33.8587 31.9281 24.9650 25.6285 26.8070 21.4294 21.5077 21.8287 21.7307 19.1475 QUA 0.6221 0.6042 0.6104 0.6358 0.5898 0.5416 0.6181 0.5799 0.5026 0.4566 0.4275 0.4408 0.3859 SDR 0.5118 0.5251 0.5044 0.4735 0.4845 0.4972 0.4958 0.4735 0.4569 0.4729 0.4702 0.4699 0.4624 TBO 1.0577 1.0290 1.0215 1.0262 1.0170 1.0093 1.0266 1.0529 1.0127 1.0014 1.0235 1.0165 1.0131 ITS 33.4141 32.6757 31.5103 30.8186 29.5214 27.2947 31.2982 29.2195 23.2293 21.7192 21.0743 21.2224 18.4858 QUA 0.6741 0.6338 0.5994 0.6610 0.5945 0.5004 0.5095 0.5806 0.4744 0.4685 0.4504 0.4532 0.3981 SDR 0.5093 0.4833 0.4762 0.4198 0.4145 0.4422 0.4350 0.4252 0.4007 0.4380 0.4085 0.4292 0.4048 TBO 1.0179 1.0175 1.0136 1.0060 1.0219 1.0107 1.0124 1.0158 1.0053 1.0022 1.0153 1.0221 1.0119 ITS 35.1035 30.7007 28.8171 27.6535 25.0956 22.2533 23.2147 25.4842 18.9471 20.2026 18.4119 19.7139 16.2288 QUA 0.6460 0.6245 0.6149 0.6841 0.6019 0.5558 0.5814 0.5700 0.4832 0.5037 0.4657 0.4650 0.4182 SDR 0.5108 0.5152 0.5182 0.4428 0.4752 0.4771 0.4678 0.4622 0.4336 0.4454 0.4616 0.4704 0.4672 TBO 1.0082 1.0176 1.0091 1.0143 1.0225 1.0128 1.0187 1.0501 1.0053 1.0000 1.0270 1.0230 1.0181 ITS 33.5468 32.3533 31.8783 30.6664 29.6514 26.7788 27.3007 28.2353 21.0317 22.0717 21.8632 22.1229 19.6543 QUA 0.6666 0.6447 0.6639 0.7129 0.6504 0.6012 0.6218 0.6074 0.5407 0.5155 0.4942 0.4598 0.4148 SDR 0.5514 0.5765 0.5453 0.4967 0.5093 0.5021 0.4982 0.4973 0.4913 0.5101 0.5156 0.5091 0.5149 TBO 1.0507 1.0344 1.0379 1.0148 1.0372 1.0293 1.0190 1.0923 1.0103 1.0019 1.0200 1.0321 1.0185 ITS 39.0697 38.8577 37.6762 36.0924 34.9180 31.0504 31.7808 33.4642 27.0223 26.4282 26.0997 24.2348 22.0003 QUA 0.6348 0.6184 0.6176 0.6707 0.6130 0.5506 0.5666 0.5560 0.4945 0.4762 0.4559 0.4491 0.3996 SDR 0.5248 0.5271 0.5184 0.4671 0.4890 0.4856 0.4803 0.4593 0.4496 0.4694 0.4674 0.4735 0.4737 TBO 1.0288 1.0238 1.0171 1.0153 1.0283 1.0130 1.0203 1.0383 1.0073 1.0027 1.0202 1.0184 1.0148 MIX ITS 34.6480 33.4668 32.9004 32.0651 31.3490 27.3364 28.0391 27.3312 22.5645 22.4382 22.0047 21.7235 19.3415 19 Figure 9: Radar charts of top models performance across domains based on multiple metrics. Figure 9 presents radar charts illustrating the cross-domain performance of the top five models, with axes rescaled for clarity. In the ITS panel, all models exhibit notably strong performance in domain 03 and domain 10, while maintaining relatively balanced results across other domains. This suggests that the models align most closely with human expectations in the areas of Sports & Competitions as well as Health & Medicine. In the QUA panel, KIM demonstrates clear advantage, followed by QWE and SON. Conversely, in the SDR and TBO panels, KIM shows marked disadvantage, whereas the remaining models display comparatively consistent performance."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Hong Kong University of Science and Technology",
        "Peking University",
        "Shanghai Artificial Intelligence Laboratory",
        "Shanghai Jiao Tong University",
        "The University of Hong Kong",
        "Tsinghua University",
        "University of British Columbia",
        "University of Toronto"
    ]
}