{
    "paper_title": "Model-Task Alignment Drives Distinct RL Outcomes",
    "authors": [
        "Haoze Wu",
        "Cheng Wang",
        "Wenshuo Zhao",
        "Junxian He"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in applying reinforcement learning (RL) to large language models (LLMs) have led to substantial progress. In particular, a series of remarkable yet often counterintuitive phenomena have been reported in LLMs, exhibiting patterns not typically observed in traditional RL settings. For example, notable claims include that a single training example can match the performance achieved with an entire dataset, that the reward signal does not need to be very accurate, and that training solely with negative samples can match or even surpass sophisticated reward-based methods. However, the precise conditions under which these observations hold - and, critically, when they fail - remain unclear. In this work, we identify a key factor that differentiates RL observations: whether the pretrained model already exhibits strong Model-Task Alignment, as measured by pass@k accuracy on the evaluated task. Through a systematic and comprehensive examination of a series of counterintuitive claims, supported by rigorous experimental validation across different model architectures and task domains, our findings show that while standard RL training remains consistently robust across settings, many of these counterintuitive results arise only when the model and task already exhibit strong model-task alignment. In contrast, these techniques fail to drive substantial learning in more challenging regimes, where standard RL methods remain effective."
        },
        {
            "title": "Start",
            "content": "ModelTask Alignment Drives Distinct RL Outcomes Haoze Wu1 Cheng Wang2 Wenshuo Zhao1 1Zhejiang University waithz@zuaa.zju.edu.cn 2National University of Singapore wangcheng@u.nus.edu junxianh@cse.ust.hk Junxian He3 3HKUST"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in applying reinforcement learning (RL) to large language models (LLMs) have led to substantial progress. In particular, series of remarkable yet often counterintuitive phenomena have been reported in LLMs, exhibiting patterns not typically observed in traditional RL settings. For example, notable claims include that single training example can match the performance achieved with an entire dataset, that the reward signal does not need to be very accurate, and that training solely with negative samples can match or even surpass sophisticated reward-based methods. However, the precise conditions under which these observations holdand, critically, when they failremain unclear. In this work, we identify key factor that differentiates RL observations: whether the pretrained model already exhibits strong Model-Task Alignment, as measured by pass@k accuracy on the evaluated task. Through systematic and comprehensive examination of series of counterintuitive claims, supported by rigorous experimental validation across different model architectures and task domains, our findings show that while standard RL training remains consistently robust across settings, many of these counterintuitive results arise only when the model and task already exhibit strong model-task alignment. In contrast, these techniques fail to drive substantial learning in more challenging regimes, where standard RL methods remain effective. Code is available at https://github.com/hkust-nlp/model-task-align-rl. 5 2 0 2 8 2 ] . [ 1 8 8 1 1 2 . 8 0 5 2 : r Figure 1: Model-task alignment, which is measured by pass@k accuracy on the evaluated task, drives distinct outcomes from the same series of RL approaches. Equal Contribution. Work done during visit to HKUST."
        },
        {
            "title": "Introduction",
            "content": "Reinforcement Learning (RL) [Sutton et al., 1998] has emerged as transformative post-training technique for Large Language Models (LLMs), enabling them to follow instructions [Ouyang et al., 2022] and align with human preferences [Ziegler et al., 2019, Rafailov et al., 2024]. particularly prominent application focuses on enhancing reasoning capabilities, as exemplified by breakthrough models such as OpenAI-o1 [Jaech et al., 2024], DeepSeek-R1 [Guo et al., 2025], QwQ [Team, 2025], and Kimi-1.5 [Team et al., 2025]. These systems demonstrate remarkable performance across reasoning-intensive domains including coding [Jain et al., 2024], mathematics [Lewkowycz et al., 2022, He et al., 2024], and logical reasoning [Liu et al., 2025, Chen et al., 2025]. While RL yields significant performance improvements in LLM reasoningmirroring the success of RL in traditional domains such as games [Silver et al., 2017a,b]we also observe several remarkable yet often counterintuitive empirical phenomena. These effects appear to be unique to LLMs and would be considered unexpected in traditional RL settings. For instance, single training examples can match or rival full-dataset training performance [Wang et al., 2025], ground-truth reward may be surprisingly dispensable [Shao et al., 2025], and training with negative samples alone can match sophisticated reward-based methods [Agarwal et al., 2025]. While these findings have generated considerable enthusiasm, the precise conditions under which they hold, and when they break down, remain insufficiently explored. Given that these observations may have important implications for RL practices, it is concerning that the conclusions are largely based on limited experimental settings, where Qwen models [Qwen et al., 2025] trained on mathematical tasks dominate the landscape. To this end, we carry out systematic empirical investigation of several notable RL claims, supported by rigorous experimental validation across diverse model architectures and task domains. Concretely, we experiment with both Qwen and non-Qwen models on math and other tasks. Our controlled experiments reveal that model-task alignment, defined as the degree to which model capabilities match task requirements, is critical indicator for categorizing RL observations. Specifically, models benefit from noisy rewards, test-time RL [Zuo et al., 2025], minimal training, and negative-sample training primarily within their domains of expertise, where these techniques fail for unfamiliar tasks even though standard RL training can succeed. Interestingly, we also observe that certain meta-patterns hold consistently across different settings. For instance, one-shot RL training is generally effective for the specific task to which the training example belongs, and negative-sample training helps stabilize model entropy, even though it does not always lead to overall improvements in accuracy. We evaluate the alignment between model capabilities and task requirements using pass@k accuracy, which we find to be reliable indicator for distinguishing these counterintuitive RL phenomena. Our hypothesis is that strong, inherent model capabilities can be readily activated through minimal training, even when guided by incorrect reward signals, whereas unfamiliar tasks demand substantially more effortcases that we argue dominate when scaling up RL compute. Concurrent work [Wu et al., 2025] investigates the mechanism behind spurious rewards and attributes their effectiveness primarily to data leakage in Qwen models on the test set. However, our results suggest otherwise: we find that spurious rewards remain effective even in the absence of contamination, provided the model already exhibits strong alignment on the evaluated task. Our study reveals that, unlike traditional RL training, distinct RL mechanisms emerge in the context of LLMs, depending on whether the pretrained model is already familiar with the target tasks. On the one hand, this suggests that RL phenomena should be interpreted with extra caution, as they may only reflect one of these two mechanisms. On the other hand, it also opens up opportunities for jointly optimizing base model pretraining (or mid-training) and RL post-training. For example, one might enhance the domain-specific capabilities of the base model during mid-training, enabling effective RL with limited training data and potentially inaccurate reward signals, or alternatively, allocate most compute resources to the RL stage using carefully curated training data and precise reward signals."
        },
        {
            "title": "2 On Unique Phenomena of RL training in LLM Reasoning",
            "content": "Reinforcement Learning from Verifiable Rewards (RLVR) has achieved significant success in improving language model reasoning. While similar gains in accuracy from standard training have also been observed in traditional RL domains such as games, we have noticed several phenomena that 2 appear unique to LLMs and would not typically be expected in conventional settings. For example, we highlight several remarkable, and at times counterintuitive, observations below: Unexpected robustness to unreliable or absent rewards: Shao et al. [2025] demonstrate that random and incorrect reward signals can improve model performance, while Agarwal et al. [2025] show that reward-free, entropy-minimization objectives can rival reward-based approaches. TestTime Reinforcement Learning (TTRL) proposed by Zuo et al. [2025] further reinforces this trend by generating reward signals through aggregating majority-vote outcomes, thereby guiding the model to evolve itself on the test set. Together, these suggest surprising fault tolerance in RL training that challenges standard assumptions about the critical role of accurate reward signals. One-shot training sufficiency: Wang et al. [2025] report that training on single carefully selected example can match or exceed performance from full dataset training, challenging assumptions about data volume requirements. Negative-only signal effectiveness: Zhu et al. [2025] demonstrate that using exclusively negative reward signals achieves comparable results to standard RL training while maintaining beneficial entropy properties. These findings carry significant implications. If broadly confirmed, they would necessitate shifts in resource allocationsuch as prioritizing data selection algorithms over dataset scale, questioning the necessity of highly accurate reward modeling, and potentially introducing new research directions. Therefore, we believe it is important to assess whether these conclusions hold in general, and if not, under what conditions they succeed or fail. Clarifying these patterns would not only help us understand the limitations of the current findings but also, in the opposite direction, reveal new opportunities for modifying models so that these findings become valid, thereby making RL training substantially easier. In this work, we will investigate these observations through controlled experiments comprehensively."
        },
        {
            "title": "2.1 Central Hypothesis: Model-Task Alignment Dependency",
            "content": "As most of the findings discussed above are based on mathematical reasoning tasks using Qwen models [Qwen et al., 2025, Yang et al., 2025], natural question arises: do these results generalize to other settings? For instance, Shao et al. [2025] reported that spurious rewards were ineffective with Llama [Meta, 2024] models on mathematical tasks. However, we argue that treating Qwen+math as merely special case is an overly superficial categorization. It remains unclear what specifically makes Qwen+math unique, and what the deeper, more essential factors might be. We propose guiding hypothesis for designing and categorizing experimental settings, which we call Model-Task Alignment Dependency: the effectiveness of these unique RL findings fundamentally depend on the degree of alignment between models inherent capabilities and the requirements of the task domain. In other words, they depend on the models proficiency on the evaluated task. This hypothesis may or may not hold, but we will use it as framework to categorize experimental settings in terms of whether the modeltask combination is aligned or misaligned. Quantifying Model-Task Alignment with pass@k. To systematically evaluate the degree of alignment between models inherent capabilities and the requirements of specific task domains, we employ the pass@k metric as our primary measure of model-task proficiency. Pass@k represents the probability that at least one correct solution appears among independent samples generated by the model for given problem. This metric effectively captures how well models existing knowledge and reasoning patterns align with the demands of particular task. Formally, for problem xi from evaluation dataset D, we generate samples (n k) and count the number of correct samples as ci, then the unbiased estimator of pass@k over the dataset is: (cid:34) pass@k := ExiD 1 (cid:35) (cid:1) (cid:0)nci (cid:1) (cid:0)n k"
        },
        {
            "title": "2.2 Strategic Model and Task Selection",
            "content": "Building on our Model-Task Alignment Dependency hypothesis outlined in Section 2.1, we strategically design model-task combinations that test the boundaries of current claims in RL for language model reasoning. Our experimental design is motivated by the critical need to distinguish between 3 Figure 2: Pass@k for different tasks. Different LLMs have significantly different abilities on different tasks, which will affect how the RL techniques perform across model-task combinations. findings that represent universal RL properties versus those that emerge from specific model-task capability alignments. We evaluate two representative language models from different families: Qwen2.5-7B-Base [Qwen et al., 2025] and Llama-3.1-8B-Instruct [Meta, 2024], enabling systematic comparison across model architectures with varying baseline capabilities while controlling for architectural differences at comparable parameter scales. Our evaluation encompasses mathematical and logical reasoning domains. For mathematical reasoning, we employ AIME24 [AIME, 2024], MATH500 [Hendrycks et al., 2021] and AMC23 [AMC, 2023]. For logical reasoning, we utilize SynLogic [Liu et al., 2025] (synthetic puzzles with 35 task types, we use the validation split), BBH [Suzgun et al., 2022] (multi-step reasoning tasks), BBEH Kazemi et al. [2025] (extended-difficulty version), and KOR-Bench [Ma et al., 2024] (knowledge-orthogonal reasoning across five categories: Operation, Logic, Cipher, Puzzle, and Counterfactual). To operationalize our hypothesis, we systematically measure alignment strength using pass@k metrics across all model-task combinations. As demonstrated in Figure 2, models exhibit markedly different inherent capabilities across domains. Based on comprehensive evaluation (full results in Appendix B), we identify cases of strong model-task alignment, such as Qwen2.5 on mathematical domains and both models on Operation and Counterfactual subsets of KOR-Bench, as well as weak model-task alignment cases, such as Llama3.1 on mathematical domains and both models on other logical reasoning tasks. This categorization enables us to test whether counterintuitive RL phenomena are artifacts of specific model-task alignments or represent fundamental properties of reinforcement learning in language model reasoning."
        },
        {
            "title": "2.3 The Contamination Hypothesis",
            "content": "Concurrent work from Wu et al. [2025] proposed an alternative hypothesis, where they specifically focus on the spurious reward pattern and suggest that it stems primarily from dataset contamination during pre-training. They further confirmed the presence of data leakage in the Qwen models on several mathematical benchmarks. While we acknowledge contamination as valid concern, our hypothesis diverges by emphasizing the distinction between contamination and inherent task proficiency. In particular, models may demonstrate strong task performance without direct contamination of the test data. In what follows, we categorize different experimental settings based on their contamination and inherent model-task alignment status. Later, in our experiments, we will empirically show that contamination is not the underlying cause; instead, model-task alignment serves as more reliable differentiator. To verify our hypothesis, we extend contamination analysis beyond Qwen-Math combinations. Following Wu et al. [2025], we evaluate model generation given partial prompts while preserving word boundaries (more details are provided in Appendix C). We employ greedy decoding and calculate both exact match (EM) rates and ROUGE-L scores, where ROUGE-L scores of 1.0 indicate perfect reconstruction. Table 1 alongside Appendix show that Operation and Counterfactual subsets have no contamination, yet both models demonstrate strong inherent reasoning capabilities with high pass@k scores (see Appendix B). As we will show in our experiments, contamination is not the necessary condition for the effectiveness of these RL phenomena. Based on our contamination analysis and pass@k measurements, we categorize experimental settings into three groups:"
        },
        {
            "title": "Portion",
            "content": "AMC 23 MATH"
        },
        {
            "title": "EM ROUGE",
            "content": "EM Qwen2.5-7B Llama-3.1-8B 0.4 0.6 0.8 0.4 0.6 0.8 63.78 64.42 73. 27.18 30.64 44.54 23.91 33.73 49.39 0.00 0.00 4.81 50.36 60.98 66.42 23.09 40.56 48.33 8.20 21.20 40. 0.60 3.80 17.8 19.56 19.62 19.24 18.27 17.31 15.85 0.00 0.00 0.00 0.00 0.00 0.00 21.37 24.25 20. 21.83 18.34 16.75 0.00 0.00 0.00 0.00 0.00 0.00 Table 1: Contamination Analysis across model-task combinations. Portion refers to the truncation ratio of the original prompt used to test whether models can complete the remaining content. Red indicates potential contamination with strong model-task alignment; Gray indicates no contamination with weak model-task alignment; Green indicates no contamination with strong model-task alignment. Red (Potential Contamination + Strong Model-Task Alignment): Qwen2.5 on mathematical domains. Gray (No Contamination + Weak Model-Task Alignment): Llama3.1 on mathematical domains; both models on SynLogic, BBH, BBEH, and Logic, Cipher, Puzzle subsets of KOR-Bench. Green (No Contamination + Strong Model-Task Alignment): Both models on Operation and Counterfactual subsets of KOR-Bench."
        },
        {
            "title": "3 Experimental Setup",
            "content": "Training Datasets and Evaluation. Except for the experiments on Test-Time RL (Section 4.2), we use DeepScaleR [Luo et al., 2025] as the training set for mathematical tasks and the training split of SynLogic-Easy [Liu et al., 2025] for logical tasks. Evaluation datasets are as described in Section 2.2. Following SynLogic [Liu et al., 2025], all evaluations are conducted in zero-shot setting, with avg@8 metrics computed for AIME 2024 and SynLogic to mitigate variance. Training Configuration. Our experiments default to using the DAPO algorithm unless otherwise specified. We set ϵlow = 0.2, ϵhigh = 0.28, max promt length=2048, max generation length=8192. We use dynamic sampling, and set max_num_gen_batches = 2. During actual training, we found that for logical task training, each sampled batch often contains very few samples with non-zero reward variance. We made two improvements: (1) When neither of the two generated sampling batches contains any samples with non-zero reward variance (which usually happens in the early stages of SynLogic training when the model cannot get any questions right), we use the second generated batch as the training batch. (2) When the number of available samples from the two generations is less than the training batch size, we duplicate the samples to match the training batch size. We dont use length penalty. During most training experiments, we set lr = 1e6, batch size = 128, mini batch size = 64, temperature = 1.0."
        },
        {
            "title": "4 RQ1 – Reward Signal: How Critical Is It?",
            "content": "This section investigates the role of reward signal quality and its impact on RL performance for LLMs. Previous work in Reinforcement Learning with Human Feedback has shown that more accurate reward models do not always lead to better downstream performance [Chen et al., 2024]. In the specific context of LLM reasoning, initial studies found that models with strong inherent reasoning abilities exhibit surprising robustness to noisy reward signals, whereas weaker models show poor noise tolerance [Lv et al., 2025, Shao et al., 2025]. Building on these findings, we extend the analysis by considering diverse reward signals across different model-task combinations. Implementation details are presented in Appendix A."
        },
        {
            "title": "4.1 Results",
            "content": "We present results in Table 2. Based on the experimental results, we identify three critical findings regarding the impact of reward signal quality on model performance (Appendix C.3 provides additional discussion): 5 Math Tasks Logic Tasks AIME24 MATH500 AMC SynLogic BBH BBEH KOR Benchmark OP CF Puzzle Logic Cipher Base 3.3 40. 31.0 1.5 45.2 1.2 27.2 17. 0.8 8.0 4.8 Qwen2.5-7B Family RLVR (External Reward) Correct 14.2+10.9 Random 10.0+6.7 6.7+3.4 Incorrect 6.7+3.4 Format 71.0+30.2 57.5+16.7 57.0+16.2 55.3+14.5 62.4+31.4 42.6+41.1 62.7+17.5 6.8+5.6 82.4+55.2 79.6+62.4 16.8+10.0 46.4+38.4 20.4+15.6 3.61.2 45.7+14.7 3.21.6 43.1+12.1 4.40.4 48.9+17.9 32.712.5 0.01.2 53.6+26.4 30.8+13.6 12.84.4 30.314.9 0.01.2 60.8+33.6 21.6+4.4 2.4+1.2 37.2+10.0 44.40.8 10.2+8.7 0.01.5 1.50.0 6.81.2 6.41.6 6.81.2 1.2+0.4 0.40.4 0.80. Vote EM 13.3+10.0 11.6+8.3 69.4+28.6 70.8+30.0 58.2+27.2 57.8+26.8 2.8+1.3 1.50.0 33.611.6 0.01.2 56.4+29.2 37.57. 16.30.9 0.01.2 67.2+40.0 27.2+10.0 0.80.0 0.80.0 6.81.2 6.837.6 3.21.6 3.213.6 Self-Rewarded Reinforcement Learning Base 3.3 32.5 20.2 0.8 38.6 4. 60.4 86.4 2.0 28.8 8.4 Llama3.1-8B-Instruct Family Correct Random Incorrect Format 6.7+3.4 3.30.0 2.11.2 3.10.2 38.6+6.1 26.85.7 26.46.1 31.51.0 25.1+4.9 21.3+1.1 18.71.5 18.71.5 RLVR (External Reward) 21.0+20.2 49.1+10.5 4.3+0.2 76.0+15.6 69.2+8.8 0.00.8 70.0+9.6 0.80.0 68.8+8.4 0.80.0 32.16.5 30.28.4 36.42. 4.10.0 3.80.3 4.10.0 88.8+2.4 87.2+0.8 83.23.2 85.60.8 15.6+13.6 0.81.2 0.81.2 2.00.0 34.4+7.6 23.65.2 19.29.6 28.00.8 11.6+3.2 4.44.0 4.44.0 6.42.0 Vote EM 4.6+1.3 5.1+1.8 37.7+5.2 38.3+5.8 23.0+2.8 25.0+4.8 1.5+0.7 0.80.0 35.92.7 34.83.8 4.3+0.2 4.10. 67.2+6.8 73.6+13.2 83.23.2 87.2+0.8 2.00.0 2.00.0 28.00.8 23.65.2 8.8+0.4 7.60.8 Self-Rewarded Reinforcement Learning Table 2: Comprehensive evaluation of different reward signals in RL. Vote denotes Majority Voting, EM means entropy minimization on self-generated samples only; OP: Operation ; CF: Counterfactual. Red indicates potential contamination with strong model-task alignment; Gray indicates no contamination with weak model-task alignment; Green indicates no contamination with strong model-task alignment. Ground Truth Rewards Consistently Outperform All Alternatives. Across both model families and all task domains, utilizing ground truth rewards consistently yields the highest performance improvements. For instance, Qwen2.5-7B achieves substantial gains on AIME24 (14.2 vs. baseline 3.3) and MATH500 (71.0 vs. baseline 40.8) when trained with accurate reward signals. This establishes ground truth rewards as the gold standard for RL training in reasoning tasks. Model-Task Alignment Determines Robustness to Noisy Rewards. The effectiveness of noisy reward signals depends on model-task alignment strength across our three experimental categories. In settings with strong alignment (Red and Green categories), models demonstrate surprising robustness to spurious rewards, with Qwen2.5-7B maintaining reasonable performance on mathematical tasks and both models showing improvements on Operation and Counterfactual tasks even with random rewards. Conversely, in weak alignment settings, spurious rewards consistently fail to provide meaningful improvements, as seen with Llama3.1-8B on mathematical tasks and both models on challenging logical reasoning benchmarks. This pattern confirms that alignment strength, rather than contamination alone, determines robustness to noisy rewards. Limited Effectiveness of Self-Rewarded Methods. Self-Rewarded Reinforcement Learning methods, including majority voting and entropy minimization, consistently underperform compared to external reward-based approaches. While self-rewarded methods shows some promise on mathematical tasks for Qwen2.5-7B (majority vote achieves 69.4 on MATH500), it fails to match the performance of ground truth external rewards and shows poor generalization to logical reasoning tasks across both model families."
        },
        {
            "title": "4.2 Test-Time RL",
            "content": "Test-Time Reinforcement Learning (TTRL) [Zuo et al., 2025] addresses fundamental challenge in LLM development: how to improve model performance on unlabeled test data without access to ground-truth labels for reward signals. It prompts the model to generate multiple responses to each test question and use the most frequent answer as the label for reward signals. Although the model is trained on the unlabeled test set, this approach is essentially no different from Self-Rewarded"
        },
        {
            "title": "Model",
            "content": "MATH"
        },
        {
            "title": "SynLogic",
            "content": "OP"
        },
        {
            "title": "Model",
            "content": "MATH"
        },
        {
            "title": "SynLogic",
            "content": "OP Qwen2.5-7B +TTRL 40.8 62.1+21.3 1.5 1.8+0.3 27.2 55.6+28.4 Llama-3.1-8B-Instruct +TTRL 32.5 41.2+8.7 0.8 0.80.0 60.4 83.6+23.2 Table 3: Test-Time Reinforcement Learning (TTRL) performance changes. TTRL produces significant gains only when model-task alignment is strong (red and green cells). Step 0 Step Step 10 Step 15 Step 20 Step 25 Step 30 Qwen+Math500 Qwen+SynLogic Qwen+OP Llama+Math500 Llama+SynLogic Llama+OP 54.2 2.2 46.0 46.3 1.5 73.6 60.6 3.0 53.6 48.6 1.5 78.0 64.3 3.7 55. 51.3 2.2 79.6 68.2 4.4 57.2 53.2 1.5 84.0 67.1 4.4 58.8 53.9 2.2 83.6 69.3 4.4 60. 55.0 2.2 86.8 70.5+16.3 5.2+3.0 60.0+16.4 54.7+8.4 2.2+0.7 88.4+14.8 Table 4: The variation of Maj@16 as training progresses. In tasks where TTRL brings significant improvements (red and green), Maj@16 continues to improve with training. Reinforcement Learning when majority voting is employed. Thus, we are also curious whether TTRL remains effective for different models and in domains beyond mathematics. Table 3 shows the results of the Qwen and Llama models on different tasks. Due to the limited scale of the test dataset, we trained for 30 steps on all test datasets. It could be observed that in settings where the modeltask alignment is strong, TTRL yields substantial improvements, as exemplified by Qwen on math tasks and Operation subset. For tasks in which the model lacks initial prior knowledge, TTRL fails to deliver improvements or yields only marginal gains. As discussed by Zuo et al. [2025], majority voting is the foundation of TTRL. We also recorded the variation of Maj@16 during the training process; the results are shown in Table 4. We can observe that, in settings where TTRL yields substantial improvements, Maj@16 consistently rises throughout training. Especially for Qwen on Operation subset, it achieves an absolute gain of 16.4 points. This further underscores that TTRLs efficacy hinges on strong modeltask alignment, rather than on contamination."
        },
        {
            "title": "5 RQ2 – Is One-shot Enough for RL to Work?",
            "content": "Wang et al. [2025] demonstrated that training on single carefully selected question can yield performance comparable to full dataset training, challenging conventional assumptions about data volume requirements in RL. Wang et al. [2025] designs selection algorithm based on the variance of training rewards, and we denote samples selected by this algorithm as mselected for mathematical tasks and lselected for logical tasks. In addition to that, we also randomly selected one or two samples from the dataset to form (mrandom, lrandom) and (m random) for comparison. The specific examples we used are detailed in Appendix E. The remaining experimental settings are consistent with those described in Section 3, and we train models for 300 steps. random, l"
        },
        {
            "title": "5.1 Results",
            "content": "We present results in Table 5. Based on the experimental results, we identify two critical findings regarding the effectiveness of one-shot reinforcement learning: One-shot RL Success Depends on Model-Task Alignment. The effectiveness of one-shot reinforcement learning is highly contingent on the alignment between model capabilities and task domain requirements. In strong alignment settings (Red and Green categories), both models demonstrate remarkable ability to generalize from single examples: Qwen2.5-7B achieves performance comparable to full dataset training on mathematical tasks (MATH500: 65.2 vs. full training 71.0), while both Qwen2.5-7B and Llama3.1-8B-Instruct show substantial improvements on Operation and Counterfactual tasks (e.g., Llama on Operation: 69.2 vs. baseline 60.4). However, this success does not extend to weak alignment settings, where both models show minimal improvements across challenging logical reasoning benchmarks. This suggests that one-shot RL serves as an effective"
        },
        {
            "title": "Logic Tasks",
            "content": "Dataset AIME24 MATH500 AMC SynLogic BBH BBEH KOR Benchmark OP CF Puzzle Logic Cipher Qwen2.5-7B full set random-1 random-2 selected-1 3.3 14.2+10.9 10.7+7.4 12.5+9.2 12.3+9.0 40.8 71.0+30.2 58.7+17.9 63.0+22.2 65.2+24.4 1.5 45. 31.0 27.2 62.4+31.4 42.6+41.1 62.7+17.5 6.8+5.6 82.4+55.2 79.6+62.4 16.8+10.0 46.4+38.4 20.4+15.6 4.40.4 0.01.2 60.4+33.2 36.8+19.6 53.1+22.1 4.80.0 1.20.0 55.7+22.7 67.2+40.0 56.8+39.6 6.4+1.6 0.01.2 69.2+42.0 38.4+21.2 55.2+24.2 40.25.0 43.12.1 39.95.3 0.80.7 2.4+0.9 0.80.7 6.41.6 3.24.8 8.00.0 0.80.0 2.0+1.2 0.80. 17.2 0.8 8.0 4.8 1.2 full set random-1 random-2 selected3.3 6.7+3.4 3.8+0.5 2.70.6 3.7+0.4 32.5 38.6+6.1 30.52.0 33.1+0.6 30.32.2 20.2 25.1+4.9 21.1+0.9 21.1+0.9 22.3+2.1 Llama3.1-8B-Instruct 0.8 38. 4.1 60.4 21.0+20.2 49.1+10.5 4.3+0.2 76.0+15.6 3.80.3 73.6+13.2 70.0+9.6 4.10.0 69.2+8.8 3.80.3 35.13.5 36.71.9 34.44.2 0.80.0 0.80.0 0.80.0 86.4 88.8+2.4 85.60.8 86.40.0 88.8+2. 2.0 15.6+13.6 1.20.8 2.8+0.8 2.00.0 28.8 34.4+7.6 28.00.8 27.21.6 19.29.6 8.4 11.6+3.2 8.8+0.4 8.40.0 6.81.6 Table 5: One-shot Reinforcement Learning Results. OP: Operation; CF: Counterfactual. We only observe the effectiveness of one-shot reinforcement learning in settings with strong model-task alignment (red and green). Figure 3: The changes in two models accuracy during the training. If the initial rollout accuracy is non-zero, both models rapidly fit the employed samples (lsimple, lmid) and exhibit generalization within the same subtask; however, we observe no generalization to puzzles of other types. fine-tuning mechanism only when models already possess strong foundational capabilities in the target domain. Sample Selection Strategy Shows Limited Impact. Contrary to expectations, the sophisticated sample selection algorithm proposed by Wang et al. [2025]. does not consistently outperform random sample selection. For Qwen2.5-7B on mathematical tasks, both selected and random samples achieve similar performance levels (MATH500: selected 65.2 vs. random 58.7 and 63.0), while for Llama3.1-8B-Instruct, the differences are negligible across all benchmarks. This finding challenges the assumption that reward variance-based selection provides substantial advantages over simpler random sampling approaches."
        },
        {
            "title": "5.2 Discussion",
            "content": "Wang et al. [2025] showed that training on single sample for mathematical tasks can quickly improve the accuracy of that sample and also lead to improvements on the test set. We attempt to verify this 8 conclusion on logical tasks. Considering that the initial rollout accuracy of the model on lselected is 0, we additionally sample two examples whose initial rollout accuracies on Qwen2.5-7B are 5/16 and 1/16 (on Llama-3.1-8B-Instruct are 3/16 and 1/16), denoted as lsimple and lmid. During training, we track three metrics: the rollout accuracy of these examples acc1shot, the accuracy of the subtask to which this example belongs (in-distribution) accid, and the accuracy of other subtasks in SynLogic (out-of-distribution) accood. The results are shown in Figure 3. One-shot RL possesses the ability to generalize within the distribution. When the problem is relatively simple (with an initial rollout accuracy that is not zero), the models rollout accuracy on that sample quickly increases. Although the initial rollout accuracy of lmid on Qwen is only one-fifth that of lsimple (on Llama is one-third), it still attains high rollout accuracy within few dozen steps. Since GRPO and DAPO compute advantages via intra-group normalization, the model is unable to derive any informative feedback from samples whose initial rollout accuracy is zero. Moreover, we observe that the test accuracy for the same subtask also continues to improve, demonstrating effective within-distribution generalization. One-shot RL struggles to generalize to other types of logic puzzles. We find that while models can improve on tasks similar to their training example, they fail to transfer learning to different puzzle types. This suggests that one-shot learning primarily exploits existing model capabilities rather than developing new reasoning skills."
        },
        {
            "title": "6 RQ3 — Does RL Work with Only Negative Samples?",
            "content": "Recent work [Zhu et al., 2025] has demonstrated that training exclusively on negative samples can be surprisingly effective for language model reasoning. However, these findings have primarily been observed in scenarios with strong model-task alignment. We investigate whether negative-only training generalizes to weak model-task alignment scenarios, where models lack strong foundational capabilities in the target domain. Implementation Details. In our implementation, negative Sample Reinforcement (NSR) masks out all trajectories with reward 1 (correct answers) when computing the policy gradient, leaving only negative-rewarded samples to drive updates. Conversely, Positive Sample Reinforcement (PSR) ignores trajectories with reward 0 and optimizes only on positively rewarded samples. All other hyperparameters remain identical to the DAPO baseline described in Section 3, with each model trained for 300 steps."
        },
        {
            "title": "6.1 Results",
            "content": "Math Tasks Logic Tasks AIME24 MATH500 AMC SynLogic BBH BBEH Qwen2.5-7B 3.3 40.8 31.0 1. 45.2 1.2 KOR Benchmark OP 27.2 CF 17.2 Puzzle Logic Cipher 0.8 8. 4.8 DAPO NSR PSR 14.2+10.9 13.9+10.6 14.0+10.7 71.0+30.2 68.7+27.9 70.3+29.5 62.4+31.4 42.6+41.1 62.7+17.5 6.8+5.6 82.4+55.2 79.6+62.4 16.8+10.0 46.4+38.4 20.4+15.6 63.5+32.5 1.6+0.4 60.4+33.2 36.8+19.6 63.1+32.1 24.8+23.3 57.1+11.9 4.3+3.1 73.6+46.4 38.4+21.2 6.81.2 31.2+23. 4.80.0 11.2+6.4 2.0+1.2 9.2+8.4 41.24.0 1.50.0 Llama3.1-8B 3. 32.5 20.2 0.8 38.6 4.1 60. 86.4 2.0 28.8 8.4 DAPO NSR PSR 6.7+3.4 7.9+4.6 7.9+4. 38.6+6.1 36.9+4.4 35.7+4.2 25.1+4.9 24.7+4.5 23.6+3.4 21.0+20.2 49.1+10.5 4.3+0.2 76.0+15.6 67.2+6.8 0.00.8 69.2+8.8 13.0+11.5 34.24.4 43.3+4.7 4.3+0.2 4.10.0 88.8+2.4 86.40.0 89.6+3. 15.6+13.6 2.00.0 12.0+11.2 34.4+7.6 28.00.8 34.4+7.6 11.6+3.2 5.23.2 10.8+2.4 Table 6: Results of NSR and PSR under different settings. When Model-Task alignment is strong, both NSR and PSR yield pronounced performance gains for all models (Red and Green). Conversely, under weak alignment, NSR-trained models exhibit no noticeable improvement (Gray). Table 6 summarizes the performance of NSR and PSR relative to the full-signal DAPO baseline across our three experimental categories. The results reveal distinct patterns based on model-task alignment strength: Figure 4: Entropy Dynamics of Qwen2.5-7B during Training. NSR can maintain the exploration space of reinforcement learning, but larger exploration space is not always favorable, as in logical tasks. Strong Model-Task Alignment Enables Effective Negative-Sample Learning. In settings with strong model-task alignment (Red and Green categories), both NSR and PSR show comparable effectiveness, recovering most of the performance gains achieved by full-signal DAPO. For Qwen2.57B on mathematical tasks, both approaches achieve 95% of the DAPO improvement (MATH500: NSR 68.7 and PSR 70.3 vs. DAPO 71.0). This demonstrates that when models already possess strong domain capabilities, either positive-only or negative-only signals can effectively drive learning. Weak Model-Task Alignment Reveals Superior Performance of Positive-Only Signals. In weak alignment settings (Gray category), PSR consistently outperforms NSR across logical reasoning tasks. For instance, on SynLogic, PSR enables meaningful improvements (Qwen2.5-7B: 1.5 vs. 24.8, Llama3.1-8B: 0.8 vs. 13.0), while NSR shows minimal gains. Overall, while PSR and NSR demonstrate comparable effectiveness in strong alignment settings, PSR emerges as the more robust approach in challenging domains where models lack expertise."
        },
        {
            "title": "6.2 Discussion",
            "content": "The relationship between positive and negative samples in reinforcement learning is fundamentally connected to the exploration-exploitation trade-off, with entropy serving as key mediator. To elucidate these dynamics in our experimental context, we examine how different sample types affect the exploration-exploitation balance through their impact on training entropy. Negative Signals Help Maintain Exploration. Figure 4 plots token-level entropy throughout training. Consistent with Zhu et al. [2025], NSR slows entropy collapse, especially on mathematical taskssuggesting that penalising only erroneous trajectories can preserve output diversity. However, the flatter entropy curve on logical tasks corresponds to poorer final accuracy."
        },
        {
            "title": "7 Conclusion",
            "content": "This work reveals that Model-Task Alignment strength, measured by pass@k accuracy, serves as the fundamental determinant of when counterintuitive RL phenomena emerge in language model reasoning. We demonstrate that remarkable behaviorsincluding robustness to spurious rewards, oneshot training effectiveness, and negative-only signal sufficiencymanifest primarily when models already possess strong foundational capabilities in the target domain, functioning more as capability elicitation mechanisms rather than genuine learning drivers for unfamiliar tasks."
        },
        {
            "title": "References",
            "content": "Shivam Agarwal, Zimin Zhang, Lifan Yuan, Jiawei Han, and Hao Peng. The unreasonable effectiveness of entropy minimization in llm reasoning, 2025. URL https://arxiv.org/abs/2505. 15134. 10 AIME. Art of Problem Solving artofproblemsolving.com. https://artofproblemsolving. com/wiki/index.php/AIME_Problems_and_Solutions, 2024. [Accessed 26-08-2025]. AMC. Art of Problem Solving artofproblemsolving.com. https://artofproblemsolving.com/ wiki/index.php/AMC_12_Problems_and_Solutions, 2023. [Accessed 26-08-2025]. Jiangjie Chen, Qianyu He, Siyu Yuan, Aili Chen, Zhicheng Cai, Weinan Dai, Hongli Yu, Qiying Yu, Xuefeng Li, Jiaze Chen, Hao Zhou, and Mingxuan Wang. Enigmata: Scaling logical reasoning in large language models with synthetic verifiable puzzles, 2025. URL https://arxiv.org/abs/ 2505.19914. Yanjun Chen, Dawei Zhu, Yirong Sun, Xinghao Chen, Wei Zhang, and Xiaoyu Shen. The accuracy paradox in rlhf: When better reward models dont yield better language models. arXiv preprint arXiv:2410.06554, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems, 2024. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021. URL https://arxiv.org/abs/2103.03874. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code, 2024. URL https://arxiv.org/abs/2403.07974. Mehran Kazemi, Bahare Fatemi, Hritik Bansal, John Palowitch, Chrysovalantis Anastasiou, Sanket Vaibhav Mehta, Lalit K. Jain, Virginia Aglietti, Disha Jindal, Peter Chen, Nishanth Dikkala, Gladys Tyen, Xin Liu, Uri Shalit, Silvia Chiappa, Kate Olszewska, Yi Tay, Vinh Q. Tran, Quoc V. Le, and Orhan Firat. Big-bench extra hard, 2025. URL https://arxiv.org/abs/2502.19187. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:38433857, 2022. Junteng Liu, Yuanxiang Fan, Zhuo Jiang, Han Ding, Yongyi Hu, Chi Zhang, Yiqi Shi, Shitong Weng, Aili Chen, Shiqi Chen, Yunan Huang, Mozhi Zhang, Pengyu Zhao, Junjie Yan, and Junxian He. Synlogic: Synthesizing verifiable reasoning data at scale for learning logical reasoning and beyond, 2025. URL https://arxiv.org/abs/2505.19641. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl. Link, 2025. Notion Blog. Ang Lv, Ruobing Xie, Xingwu Sun, Zhanhui Kang, and Rui Yan. The climb carves wisdom deeper than the summit: On the noisy rewards in learning to reason. arXiv preprint arXiv:2505.22653, 2025. Kaijing Ma, Xinrun Du, Yunran Wang, Haoran Zhang, Zhoufutu Wen, Xingwei Qu, Jian Yang, Jiaheng Liu, Minghao Liu, Xiang Yue, Wenhao Huang, and Ge Zhang. Kor-bench: Benchmarking language models on knowledge-orthogonal reasoning tasks, 2024. URL https://arxiv.org/ abs/2410.06526. 11 Meta. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model, 2024. URL https://arxiv.org/abs/2305.18290. Rulin Shao, Shuyue Stella Li, Rui Xin, Scott Geng, Yiping Wang, Sewoong Oh, Simon Shaolei Du, Nathan Lambert, Sewon Min, Ranjay Krishna, et al. Spurious rewards: Rethinking training signals in rlvr. arXiv preprint arXiv:2506.10947, 2025. David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, and Demis Hassabis. Mastering chess and shogi by self-play with general reinforcement learning algorithm, 2017a. URL https://arxiv.org/abs/1712.01815. David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy P. Lillicrap, Fan Hui, L. Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of go without human knowledge. Nature, 550:354359, 2017b. URL https://api. semanticscholar.org/CorpusID:205261034. Richard Sutton, Andrew Barto, et al. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, , and Jason Wei. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. URL https://qwenlm.github.io/blog/qwq-32b/. Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Liyuan Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, et al. Reinforcement learning for reasoning in large language models with one training example. arXiv preprint arXiv:2504.20571, 2025. Mingqi Wu, Zhihao Zhang, Qiaole Dong, Zhiheng Xi, Jun Zhao, Senjie Jin, Xiaoran Fan, Yuhao Zhou, Yanwei Fu, Qin Liu, et al. Reasoning or memorization? unreliable results of reinforcement learning due to data contamination. arXiv preprint arXiv:2507.10532, 2025. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement, 2024. URL https://arxiv.org/abs/2409.12122. 12 An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. Xinyu Zhu, Mengzhou Xia, Zhepei Wei, Wei-Lin Chen, Danqi Chen, and Yu Meng. The surprising effectiveness of negative reinforcement in llm reasoning, 2025. URL https://arxiv.org/abs/ 2506.01347. Daniel Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. Yuxin Zuo, Kaiyan Zhang, Li Sheng, Shang Qu, Ganqu Cui, Xuekai Zhu, Haozhan Li, Yuchen Zhang, Xinwei Long, Ermo Hua, et al. Ttrl: Test-time reinforcement learning. arXiv preprint arXiv:2504.16084, 2025."
        },
        {
            "title": "A Implementation Details",
            "content": "Following the setting described in Section 3, we train with different rewards for 300 steps on mathematical and logical reasoning tasks, respectively. The format reward is different from that of Shao et al. [2025], we use the same template as SynLogic. In addition to this, the definition of the reward functions is consistent. More Pass@k Results Figure 5: Pass@k for math tasks. Qwen demonstrates strong capabilities across all three mathematical evaluation datasets. Figure 6: Pass@k for KOR-Bench. Both models demonstrate strong inherent reasoning capabilities in Operation and Counterfactual subtasks, but exhibit limited inherent logical reasoning abilities in Cipher, Puzzle and Logic."
        },
        {
            "title": "C Contamination Evaluation",
            "content": "C."
        },
        {
            "title": "Implementation Details",
            "content": "Our contamination analysis follows systematic prompt truncation methodology to evaluate potential data leakage across model-task combinations. Original prompts are truncated at varying ratios (0.4, 0.6, and 0.8) while preserving word boundaries, and models are asked to complete the remaining content using greedy decoding for deterministic outputs. We measure contamination using ROUGE-L scores between model completions and the actual remaining prompt content, where perfect score of 1.0 indicates complete reconstruction and potential contamination. The evaluation pipeline employs 14 distributed processing to handle complex mathematical expressions and prevent evaluation timeouts, with results aggregated across multiple rollouts to ensure statistical reliability. C.2 More Results"
        },
        {
            "title": "Model",
            "content": "Portion=0.4 Portion=0.6 Portion=0."
        },
        {
            "title": "ROUGE EM ROUGE EM ROUGE EM",
            "content": "s T M a g AMC 23 MATH500 AIME"
        },
        {
            "title": "Cipher",
            "content": "Qwen2.5-7B Llama-3.1-8B Qwen2.5-7B Llama-3.1-8B Qwen2.5-7B Llama-3.1-8B Qwen2.5-7B Llama-3.1-8B Qwen2.5-7B Llama-3.1-8B Qwen2.5-7B Llama-3.1-8B Qwen2.5-7B Llama-3.1-8B Qwen2.5-7B Llama-3.1-8B 63.78 27.18 50.36 23.09 44.64 26.08 19.56 18. 21.37 21.83 18.88 19.02 22.08 21.38 34.61 29.59 23.91 0.00 8.20 0. 10.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. 64.42 30.64 60.98 40.56 48.69 30.80 19.62 17.31 24.25 18.34 19.96 19. 27.28 28.37 41.03 36.95 33.73 0.00 21.20 3.80 13.33 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 73.23 44.54 66.42 48. 60.08 50.50 19.24 15.85 20.18 16.75 18.66 18.94 28.23 28.42 44.77 42. 49.39 4.81 40.20 17.8 30.00 13.33 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 Table 7: Extended Contamination Analysis across model-task combinations. Red indicates potential contamination with strong baseline performance; Gray indicates no contamination with weak baseline performance; Green indicates no contamination with strong baseline performance. C.3 Discussion about RQ1 How Different Reward Signals Affect the Behavior of LLMs. Shao et al. [2025] observed that in mathematical tasks, employing ground truth rewards decreases the frequency of code usage in model responses. Their study also revealed that, in contrast to Qwen2.5-Math [Yang et al., 2024], the accuracy improvement of the Qwen2.5 Base model was primarily attributed to shift from code-based reasoning to language-based reasoning. As shown in Table 8, we identify analogous trends in mathematical tasks. Specifically, for logic puzzles, the application of ground truth rewards similarly reduces the incidence of code in responses. However, other types of rewards, particularly format and random rewards, do not demonstrate significant impact on diminishing code usage frequency. We speculate that, throughout the RL training process, ground truth rewards can steer the model away from its old reasoning pattern ( i.e., producing reasoning responses with code ) and toward more natural, language-based reasoning pattern. As shown in Table 2, spurious rewards are effective only on the Operation and Counterfactual for the Llama model; consequently, we also report the frequency of code-based reasoning before and after training on these two tasks. As shown in Table 9, we observe that, both before and after RL training, Llama almost never invokes code during the reasoning process. We attribute the sporadic use of code (0.8) to the fact that some SynLogic tasks explicitly require outputs to be presented as code blocks. This indicates that Llama and Qwen exhibit distinct reasoning patterns even though they both benefit from noisy reward signals in these settings."
        },
        {
            "title": "Correct\nRandom\nFormat\nIncorrect",
            "content": "MATH"
        },
        {
            "title": "Before RL After RL Before RL After RL",
            "content": "89.1 12.4 94.2 96.7 28.1 57.3 21.7 48.2 50.7 28.3 Table 8: Code Usage Count of Qwen2.5-7B before and after RL training with different rewards."
        },
        {
            "title": "Correct\nRandom\nFormat\nIncorrect",
            "content": "0.0 0.8 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 Table 9: Code Usage Count of Llama-3.1-8B-Instruct before and after RL training on two tasks. More Discussion about Difficult Example in One-shot RL During training with lselected, apart from the rollout accuracy (reward) remaining consistently at 0, metrics such as entropy and response length also exhibit almost no changes. As shown in Figure 7, after 300 training steps, the model still maintains large reinforcement learning exploration space. Figure 7: Training Dynamics of Qwen2.5-7B when trained with lselected. Entropy and response length exhibit almost no changes. Few-shot RL Example Details"
        },
        {
            "title": "Details of example mselected",
            "content": "How many positive divisors do 9240 and 13860 have in common?"
        },
        {
            "title": "Details of example mrandom",
            "content": "The angles of quadrilateral QRS satisfy = 3Q = 4R = 6S. What is the degree measure of ? Details of example random (cid:16) a1+a2 2 Given finite sequence = (a1, a2, . . . , an) of real numbers, let A(S) be (cid:17) of n1 real numbers. Define A1(S) = the sequence A(S) and, for each integer m, 2 1, define Am(S) = A(Am1(S)). Suppose > 0, and let = (1, x, x2, . . . , x100). If A100(S) = (cid:0) 1 (cid:1), then what 250 is x? AND If x, 2x + 2, 3x + 3, . . . are in geometric progression, the fourth term is: , . . . , an1+an , a2+a3"
        },
        {
            "title": "Details of example lselected",
            "content": "Heres mathematical expression: ?-?+(6%5)*2-?+?/?/?/4/2 = 2. The digits on the left side of the equation have been replaced with question marks. Each question mark corresponds to digit between 0 and 9. You need to try replacing the question marks with the correct digits to restore the expression.Please put the complete expression with the filled - in digits between [[ and ]] at the end of your response, with no other content, like this: [[2 + 4 * 3 - 4 = 10]]"
        },
        {
            "title": "Details of example lrandom",
            "content": "Solve this cryptarithm: RRYUU + UYR + = RYUUU (where RRYUU is 5-digit number, UYR is 3-digit number, is 1-digit number, and RYUUU is 5-digit number). Each letter represents unique digit. Find the digit substitution that makes the equation true. 17 Details of example random In this Number Wall puzzle, add walls (marked as A) to divide the grid into islands. Each island must contain exactly one number, and its size must equal that number. Grid: +-+-+-+ 3 +-+-+-+ +-+-+-+ +-+-+-+ Rules: - Each island must contain exactly one number. - The total number of cells in an island (including the number cell) must equal the value of that number. - All cells within an island must be connected horizontally or vertically. - Walls (marked as A) cannot form 22 or larger continuous rectangles. - All islands must be separated by walls. AND In the cryptarithm: MMII + MIXIMM = MMXIIX, each letter stands for different digit (MMII is 4 digits, MIXIMM is 6 digits, and MMXIIX is 6 digits). Determine what each letter represents to make the equation true."
        },
        {
            "title": "Details of example lsimple",
            "content": "In this word sorting challenge, you need to rearrange words in increasing based on modified alphabet where l,z and are the first letters. Words to sort: yachted,coelomic,harateen. Write your final answer inside: boxed,like this: boxedword1,word2,word3."
        },
        {
            "title": "Details of example lmid",
            "content": "You are an expert proficient in Dyck language, where you must complete all types of unclosed brackets (e.g., [], , <>) in language sequences. You need to analyze the steps of bracket pairing according to Dyck language rules. Given an initial Dyck language sequence and steps for deriving the closed bracket sequence (presented in thinking process format), your task is to identify locations with incorrect reasoning in the Dyck language, and there may be multiple errors. This could be forgetting to close bracket, using the wrong closing bracket, or incorrectly copying subsequence of closing brackets in the next step. Task: Check the sequence to ensure brackets are properly closed. Input: [[(){}]]{} Thought 1: We should process the input one by one and track the stack configuration. Thought 2: Stack: Empty Thought 3: [ ; Stack: Empty Thought 4: [ ; Stack: [[ Thought 5: ( ; Stack: [[( Thought 6: ) ; Stack: [[ Thought 7: { ; Stack: [[{ Thought 8: } ; Stack: [[ Thought 9: ] ; Stack: [ Thought 10: ] ; Stack: Empty Thought 11: { ; Stack: { Thought 12: } ; Stack: Empty Thought 13: Now, we have reached the end. The final stack is empty. Question: Are there any reasoning errors in this sequence?"
        }
    ],
    "affiliations": [
        "HKUST",
        "National University of Singapore",
        "Zhejiang University"
    ]
}