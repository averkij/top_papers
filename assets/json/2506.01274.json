{
    "paper_title": "ReFoCUS: Reinforcement-guided Frame Optimization for Contextual Understanding",
    "authors": [
        "Hosu Lee",
        "Junho Kim",
        "Hyunjun Kim",
        "Yong Man Ro"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent progress in Large Multi-modal Models (LMMs) has enabled effective vision-language reasoning, yet the ability to understand video content remains constrained by suboptimal frame selection strategies. Existing approaches often rely on static heuristics or external retrieval modules to feed frame information into video-LLMs, which may fail to provide the query-relevant information. In this work, we introduce ReFoCUS (Reinforcement-guided Frame Optimization for Contextual UnderStanding), a novel frame-level policy optimization framework that shifts the optimization target from textual responses to visual input selection. ReFoCUS learns a frame selection policy via reinforcement learning, using reward signals derived from a reference LMM to reflect the model's intrinsic preferences for frames that best support temporally grounded responses. To efficiently explore the large combinatorial frame space, we employ an autoregressive, conditional selection architecture that ensures temporal coherence while reducing complexity. Our approach does not require explicit supervision at the frame-level and consistently improves reasoning performance across multiple video QA benchmarks, highlighting the benefits of aligning frame selection with model-internal utility."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 4 7 2 1 0 . 6 0 5 2 : r ReFoCUS: Reinforcement-guided Frame Optimization for Contextual Understanding Hosu Lee Junho Kim Hyunjun Kim Yong Man Ro Integrated Vision and Language Lab, KAIST, South Korea {leehosu01, arkimjh, kimhj709, ymro}@kaist.ac.kr"
        },
        {
            "title": "Abstract",
            "content": "Recent progress in Large Multi-modal Models (LMMs) has enabled effective vision-language reasoning, yet the ability to understand video content remains constrained by suboptimal frame selection strategies. Existing approaches often rely on static heuristics or external retrieval modules to feed frame information into video-LLMs, which may fail to provide the query-relevant information. In this work, we introduce ReFoCUS (Reinforcement-guided Frame Optimization for Contextual UnderStanding), novel frame-level policy optimization framework that shifts the optimization target from textual responses to visual input selection. ReFoCUS learns frame selection policy via reinforcement learning, using reward signals derived from reference LMM to reflect the models intrinsic preferences for frames that best support temporally grounded responses. To efficiently explore the large combinatorial frame space, we employ an autoregressive, conditional selection architecture that ensures temporal coherence while reducing complexity. Our approach does not require explicit supervision at the frame-level and consistently improves reasoning performance across multiple video QA benchmarks, highlighting the benefits of aligning frame selection with model-internal utility."
        },
        {
            "title": "Introduction",
            "content": "After the wide adoption of Large Language Models (LLMs) [3, 45, 51] into language-based applications, users can now interact with various multi-modal products [31, 32, 40] through back-and-forth conversations, moving one step beyond purely textual interactionsthe beginning of LMMs [33, 34] era (Large Multi-modal Models). As the exceptional perception and reasoning capabilities of LLMs have dramatically advanced through large-scale web-scraped corpora, substantial amounts of highquality multi-modal paired datasets have enabled LMMs to achieve cross-modal consistency through alignment pre-training, followed by post-training processes such as supervised fine-tuning (SFT) and RL-based preference optimization (PO). Accordingly, various LMMs [26, 21, 15] can concurrently handle both linguistic requests and multi-modal tasks, seamlessly enabling richer user interactions, improved context awareness, and robust performance across diverse multi-modal scenarios. Even though pioneering works [2, 8, 22, 27] have successfully developed robust LMMs, achieving impressive performance on wide range of vision-language tasks through visual modality integration, the understanding of video content remains substantially below human-level capability. significant reason for the limited video understanding is that many existing video-LLMs [29, 24, 25] typically rely on simplistic strategies for incorporating spatio-temporal information within the video, treating it as sequence of image frames (e.g., uniform frame sampling). Due to the limited context length of language model backbones, current video-LLMs often fail to ensure inter-modal alignment, losing the continuity of semantic and temporal dynamicsespecially in complex or long-form video contentwhich leads to suboptimal contextual understanding. Equal contribution. Corresponding author. Preprint. Under review. To address this limitation, various works [24, 23, 50] have explored adaptive strategies to align frame-level information to deliver relevant visual priors into the models. Rather than uniformly sampling frames, several works selectively retrieve relevant video segments using auxiliary retrieval modules [17, 16] or memory-augmented strategies [43, 13]. However, such approaches often face difficulties in integrating multiple partial cues, which limits their effectiveness in synthetic scenarios demanding high-level reasoning. Furthermore, recent studies [57, 53] have proposed training-free search algorithms to select semantically informative frames, which are incorporated into pre-trained video-LLMs as external priors. Despite their improvements, such frame selection methods remain decoupled from the models internal reasoning process, often failing to capture frames aligned with its semantic and temporal focus. In this paper, we introduce ReFoCUS (Reinforcement-guided Frame Optimization for Contextual UnderStanding), novel reinforcement learning-based framework that extends policy optimization to the frame selection process, rather than applying it to the user-preferred responses. While existing multi-modal PO methods [46, 1, 62] optimize generated textual responses utilizing human preferences or LLM-generated reward signals, ReFoCUS enables the model to internalize its own preferences over visual evidence by selecting frames that provide informative priors for the given user queries. By learning frame selection policy guided by reward signals, ReFoCUS helps the model identify the most semantically and temporally relevant moments in video. Our approach not only reduces input redundancy but also significantly boost the models video understanding capabilities over event-rich sequencial information by synthesizing aligned spatial-temporal cues. To achieve our RL-based policy optimization, we handle the following two main challenges: Collecting frame-level preference data is significantly more resource-intensive and infeasible, compared to the textual information due to the combinatorial explosion inherent in lengthy videos. To address the data problem, we employ reference LMM to evaluate sampled frame subsets, enabling group-wise relative reward modeling across the candidates and allowing the policy model to be guided by an effective advantage function for the policy optimization. The extensive search space during RL-based optimization involved in frame selection for typical video content poses another substantial challenge. To structurally tackle it, we propose an architectural design for ReFoCUS based on an autoregressive (conditional) frame-selection mechanism. By progressively identifying relevant frames conditioned on previously selected priors, our method significantly reduces the frame search overhead while inherently ensuring coherence within the selection process. Our framework is model-agnostic, seamlessly integrates with existing video-LLMs, and leads to consistent performance gains across video QA benchmarks corroborating its effectiveness in capturing and leveraging the models intrinsic visual preferences. Our contributions are threefold: (i) We propose ReFoCUS, reinforcement learning-based framework that directly optimizes frame selection, enabling models to internalize their own visual preferences and enhance contextual video understanding through input-level optimization; (ii) We design an autoregressive (conditional) frame selection architecture that efficiently navigates the combinatorial search space by progressively selecting frames based on prior context, ensuring temporal and semantic coherence; (iii) We demonstrate that ReFoCUS consistently improves the reasoning performance of video-LLMs across multiple video QA benchmarks, validating its generality and practical effectiveness."
        },
        {
            "title": "2 Related Work",
            "content": "Large Multi-modal Models for Video Understanding As LLMs [3, 45] have advanced, multimodal integration has led to the emergence of LMMs [63, 8, 11] capable of processing both visual and textual inputs. Building on foundational multi-modal instruction tuning [27, 55, 9], recent works have specifically expanded their scope towards video modality [24, 29, 50], aiming to achieve deeper spatio-temporal reasoning and better contextual understanding. Several key directions include developing enhanced temporal modeling strategies [50, 43], refining alignment mechanisms across modalities [5, 30], and leveraging larger, higher-quality video-text paired datasets [21, 11]. Despite such advances, many existing models still rely on sparse frame sampling or limited temporal context windows, constraining their ability to dynamically capture visual-level semantics within the video. Recent approaches attempt to address these limitations through techniques such as memory augmentation [13], extended context modeling [59], or learned frame selection mechanisms [57, 53]. 2 Yet, these methods often struggle in complex scenarios where partial visual cues are scattered across distant frames and must be synthesized to form coherent understanding, limiting their effectiveness in reasoning over extended temporal dependencies. Reinforcement Learning in LMMs as Post-training Preference Optimization Recent research has increasingly integrated RL into LMMs as method for post-training preference optimization. Initial efforts [35, 44, 58] primarily targeted mitigating hallucinations and enhancing factual accuracy in model responses through RL with human feedback (RLHF), with particular emphasis on learning from pairwise preference comparisons over textual outputs. Advancing beyond PPO-style RLHF pipelines, Direct Preference Optimization (DPO) [38] formulates preference alignment more directly through supervised objective, and has been further adapted to multi-modal settings for aligning outputs with human preferences [46, 62]. However, existing approaches predominantly focus on aligning textual outputs with human preferences by updating the policy model accordingly, while paying little attention to the models visual input space. In contrast, our method, ReFoCUS, shifts the focus to the input level by optimizing which visual content the model attends to. By leveraging RL to identify informative video frames, we enable the model to internalize and act upon its intrinsic visual preferences, enhancing its capacity for comprehensive and context-aware video understanding."
        },
        {
            "title": "3 Proposed Method",
            "content": "Overview. We illustrate the overall policy optimization pipeline of ReFoCUS in Fig. 1. Unlike existing PO methods [38, 62] that fine-tune LMMs to generate preferred textual outputs, our approach extends PO directly at the input levelspecifically to the selection of video frames. The key idea is to align the models visual inputs with its own implicit preferences, enabling the model to focus on the most informative spatio-temporal moments within the video frames. Instead of optimizing for preferred textual responses, the policy model in ReFoCUS is trained to select frames that guide the model toward more accurate answers, thereby shifting policy optimization from the textual output space to the frame-level input space. ReFoCUS consists of two core components: learnable Policy Model and frozen Reward Model, which together drive policy optimization for frame selection. The policy model receives dense video sequences along with query and aims to select frame subsets that best support contextual understanding and reasoning. The Reward Model, in turn, serves as reference evaluator for each candidate subset, providing learning signals based on its confidence in predicting the correct answer. As in the figure, for given video input and its corresponding QA pair q, y, the policy model πθ is trained to sample frame subsets . Each subset is then scored by the reference LMM rφ, which estimates reward from its prediction confidence for y. These rewards are used to update the policy via policy gradient, encouraging πθ to gradually select frame combinations aligned with the models intrinsic visual preferences for correct predictions. We will provide detailed explanations for ReFoCUS in the following subsections. Figure 1: Pipeline overview of ReFoCUS. The policy model πθ samples candidate frame subsets from the input video and question q, and the reward model rφ evaluates each subset using its prediction confidence, producing reward signals to train πθ via policy gradient. 3.1 Data Perspective for Frame-level Policy Learning Learning without Frame-level Supervision. We start from data perspective. Unlike textual preference data, collecting frame-level supervision for the video content is prohibitively expensive due to the combinatorial explosion of possible frame subsets. When sampled from lengthy videos at varying temporal resolutions, for example, we cannot manually identify which frames are truly informative for arbitrary user queries in practice. Since direct frame-level annotation is impractical, 3 we instead use the output logits of the reference LMM rφ as implicit feedback signals for the candidate frame subsets sampled from the policy model πθ. To simplify reward estimation and ensure consistent evaluation quality from rφ, we constrain our train process to multiple-choice QA tasks with discrete answer options, as open-ended QA often leads to diffuse reward signals and unreliable supervision due to its inherently subjective and under-defined nature. We collect diverse pool of QA pairs from video QA datasets, including LLaVA-Video178K [60], NeXT-QA [49], ActivityNetQA [4], PerceptionTest [36], CinePile [39], and VISTA400K [41], resulting in total of 962K QA pairs. Reward Variance Filtering for Stable Policy Learning. While the multiple-choice QA format enables measurable evaluation and stable reward signal, not all QA pairs contribute equally to effective policy learning. In particular, some samples yield nearly identical predictions across different video frame combinations showing negligible variation in the output predictions due to low semantic complexity. This results in flat reward distributions, which in turn produce near-zero policy gradients when using group-wise relative scoring [42] for the sampled video subsets. Such cases degrade the quality of the reinforcement signal and may cause unstable or failure of policy learning, similar to the gradient saturation problem in other RL-based preference optimizations [65, 35]. Accordingly, we first divide each video sample into 8 temporal segments using overlapped window size and fixed stride. For each window, we also define their corresponding complementary range (i.e., the remaining frames outside the window) and uniformly sample frames from both the windows and their complements. This produces total of 16 candidate frame subsets per QA pair, capturing both focused and complementary temporal regions (please see the detailed process in Appendix A.). Each of these 16 subsets is paired with the same question and passed through pretrained LMM [47]. We record the models predicted answer probabilities and compute the variation in prediction across the 16 sub-groups. This variation reflects the extent to which the models prediction depends on different temporal portions of the video. As illustrated in Fig. 2, we retain only QA pairs whose prediction variance across candidate frame subsets exceeds threshold τ . Samples with consistently identical predictionsregardless of which frames are shownare likely to exhibit weak temporal cues for the questions. Such samples fail to provide meaningful learning signals for policy optimization, as the models output remains insensitive to visual evidence. By filtering out these low-variance instances, we curate more discriminative and informative subset of 98K QA pairs, ensuring stable reward estimate and effective policy optimization. Figure 2: Distribution of reward variance Var(m) (prediction margin) across 962K QA pairs. We observe that many samples yield low variance, indicating weak sensitivity to visual input. We filter out such cases (< τ = 0.21) to retain high-quality subset for policy learning. 3.2 ReFoCUS: Reinforcement-guided Frame Optimization for Contextual Understanding Frame Selection and Reward Estimation. For the policy model, we adopt lightweight LMM [19] built on Mamba-based architecture [10], which efficiently handles densely sampled long frame sequences. This capability enables broader and more fine-grained exploration of the frame selection space during policy optimization, in contrast to uniform samplinga small, fixed number of frames (typically 16 or 32 frames). Additionally, unlike CLIP-based approaches [37] that rely on sentencelevel image-text matching, our LMM-based policy model naturally accommodates the video QA format, enabling richer temporal and contextual understanding in open-ended tasks. Starting from the SFT model [19], we repurpose the model for frame selection by removing the final unembedding layer (i.e., LLM head) and fine-tune it to autoregressively predict the next frame index conditioned on the input query and previously selected frames. This allows the model to gradually construct an informative frame subset that is semantically and temporally aligned with the given user query, while capturing dependencies between selected frames. 4 Figure 3: Overview of the ReFoCUS framework. Given video and query, πθ autoregressively selects frame subsets, which are then scored by rφ based on their answer prediction margins. The resulting rewards guide frame-level policy optimization via reinforcement learning. Specifically, to identify an optimal frame combination, policy model samples candidate subsets from the video input RT HW 3, with each j-th subset denoted as (j)={f (j) i=1, consisting of frames sampled at fixed frame rate and then sorted in temporal order. We treat each frame subset (j) generated by the policy model πθ as an individual action. The value of each action is evaluated based on how well the selected subsets enable the reward model rφ to generate the correct answer for the given question. It is measured using the models predictive distribution over answer options and our objective to explicitly maximize the decision margin between the ground-truth answer and the most competitive incorrect choice, reflecting how decisively the model favors the correct output given the selected frames. We define the margin-based reward as the normalized confidence difference as follows: }T rj = rφ(y (j), q) max ˆy=y rφ(y (j), q) + max ˆy=y rφ(ˆy (j), q) rφ(ˆy (j), q) , (1) where rj is normalized to the range [1, 1], capturing the relative confidence margin between the correct label and the strongest distractor. This margin-based reward effectively reflects the residual uncertainty between top competing choices, guiding the policy to prefer frame subsets that disambiguate closely competing answers. By providing fine-grained feedback over multiple candidate subsets, our reward design enables the policy model to progressively align its selection strategy with the models intrinsic visual preferences. Autoregressive Conditional Frame Selection. Unlike standard next-token prediction where models are trained with teacher forcing using ground-truth tokens, our setting does not have access to frame-level supervision to indicate which frames are sufficient for answering given question during the training process. Therefore, we explicitly modify the policy model to behave in fully autoregressive manner, where each frame is selected based on the input query v, and the sequence of previously chosen frames f<i. Building on the modified SFT backbone, we fine-tune the model to autoregressively generate frame selections in place of textual tokens. As illustrated in Fig. 3 (b), we begin with special token <start_of_frame> and let the model autoregressively produce sequence of latent outputs, utilizing the state-space sequence modeling capabilities of Mamba [10] to autoregressively generate latent sequence over the input frames. At each step, the previously selected frame is used as query to attend over the pool of candidate frame embeddings through scaled dot-product attention, producing probability distribution from which the next frame is sampled. This process is repeated autoregressively according to the conditional policy fi πθ( f<i, v, q) until frames are selected. 5 Algorithm 1 ReFoCUS: Frame-level Policy Optimization Require: policy model πθ, reward model rφ; dataset D, # candidates , lr α, update steps 1: Sample mini-batch = {(v, q, y)} from dataset 2: for all (v, q, y) do 3: 4: snapshot for importance sampling Checkpoint sampling policy πθold πθ Sample frame subsets = {f (j)}N Compute prediction confidence via rφ(yf (j), q) for each (j) Estimate margin-based reward rj using prediction confidence Normalize rewards: { ˆAj = (rj mean(r))/(std(r) + ϵ)}N for update = 1 to do j=1 πθold( v, q), j=1 Compute gradient θJ (θ) using policy gradient with ˆAj Update policy: θ θ + αθJ (θ) 5: 6: 7: 8: 9: 10: 11: 12: end for 13: return Updated policy πθ end for Eq. (1) policy gradient, Eq. (2) Importantly, although our selection process is autoregressive, it is not causal in the strict temporal sense. Enforcing causal decoding would turn the selection into permutation task, unnecessarily increasing the search space and introducing time-order bias. Instead, we adopt non-causal strategy in which each frame is selected independently of temporal order, while still ensuring that no frame is selected more than once as in Fig. 3 (c). Our autoregressive selection allows the model to flexibly attend to the entire video context at each step, conditioning its decisions on both the input query and previously selected frames. As it progresses, this enables the model to adaptively identify query-relevant moments while avoiding redundancy, resulting in semantically diverse frame subsets. Frame-level Policy Optimization. After obtaining the autoregressively selected frame subsets and their corresponding margin-based rewards, πθ is optimized through the following policy optimization. The reward scores {rj}N j=1 for the subsets are first normalized within each group to compute the relative advantage values. Following GRPO [42], which eliminates the need for an explicit value function by leveraging group-wise normalized rewards as relative baselines, we set the advantage as: ˆAj = ˆrj = (rj mean(r)) /std(r). Within each (j), the full trajectory is treated as an atomic action, with the computed advantage ˆAj applied as the relative advantage during optimization. The πθ is optimized by maximizing the objective with additional entropy regularization: (θ) = v,qD,{f (j)}N j=1πθold (v,q) 1 (cid:88) j= πθ(f (j) v, q) πθold(f (j) v, q) ˆAj + βH(πθ) , (2) where H(πθ) denotes the entropy term that encourages the diverse exploration during the frame selection process. Unlike other PO settings [65, 56] that constrain the policy not to deviate too much from reference model (e.g., DKL regularization with SFT model), such initialization is not feasible in our frame-level optimization setting, since no frame-level ground truth exists to train such reference models as pre-training stage. To prevent the policy model from collapsing into degenerate behaviorsdensely selecting early frames or overly concentrating on redundant regions with high reward peaks in specific temporal segmentswe explicitly incorporate entropy regularization soft constraint to maintain balanced frame coverage. We summarize our detailed process in Algorithm 1 and extended explanation of our policy optimization in Appendix B."
        },
        {
            "title": "4 Experiments",
            "content": "Implementation & Training Details. For the policy model πθ, we adopt Video-MA2mba [19] to handle long-context frame selection, re-initializing its final Mamba layers to support autoregressive decoding for frame sequence tokens. Note that frames are sampled at 4 fps, resulting in up to 512 frames per video, which corresponds to approximately 105 input tokens. To reduce memory consumption during training for long sequences, we apply Multi-Axis Gradient Checkpointing [19]. Starting from the special token <start_of_frame>, the model autoregressively generates total 32 6 Table 1: Benchmark comparison of open-sourced Video-LMMs and our ReFoCUS-enhanced variants. Model #param Video-MME (w/o subtitle) LVBench MLVU short medium long overall acc. (val) m-avg. Open-sourced Video-LMMs 7B Video-LLaVA [24] 8B ShareGPT4Video [6] 7B LongVA [59] 8B Kangaroo [28] 8B TimeMarker [7] 7B mPLUG-Owl3 [54] 8B MiniCPM-V 2.6 [52] Lightweight models w/ ReFoCUS LLaVA-OV [20] + ReFoCUS InternVL3 [64] + ReFoCUS Standard-size models w/ ReFoCUS LLaVA-OV [20] + ReFoCUS InternVL3 [64] + ReFoCUS 0.5B 1B 7B 8B 45.3 48.3 61.1 66.1 71.0 70.0 71.3 53.8 54.4 62.9 63.6 70.0 71.0 75.9 75. 38.0 36.3 50.4 55.3 54.4 57.7 59.4 41.0 41.1 47.8 49.1 56.6 58.3 64.7 66.9 36.2 35.0 46.2 46.6 46.4 50.1 51.8 37.2 38.4 39.0 41.2 48.9 50.0 53.4 55. 39.9 39.9 52.6 56.0 57.3 59.3 60.9 44.0 44.6 49.9 51.3 58.5 59.8 64.7 66.0 39.1 39.7 54.2 56.3 59.8 54.9 46.0 46.6 47.5 48.5 56.6 57.9 58.0 59. 47.3 46.4 56.3 63.9 44.3 45.7 54.2 57.0 63.1 65.3 67.8 69.8 frame tokens (T =32), from which 16 frame subsets (N =16) are sampled. For reward estimation, we leverage InternVL3 [64] as the reward model rφ due to its strong alignment and reasoning capabilities. During evaluation, we use FlashAttention-2, and AdamW is used for our policy optimization with learning rate of 1105 for the pretrained backbone and 1104 for re-initialized components with linear warm-up scheduler. Training runs in bfloat16 precision with batch size of 192 and includes entropy regularization β=0.002 to encourage learning stability. Training takes approximately 36 hours on single node with 8 H200 GPUs. Please see more details in Appendix B. Benchmarks & Baselines. We evaluate our method across four representative video understanding benchmarks, which span wide evaluation spectrum. Video-MME [12] provides assessments for LMMs across short-to-long videos. LongVideoBench [48] focuses on evaluating temporal localization and long-context comprehension through multiple-choice QA. MLVU [61] expands the evaluation space with multi-task and multi-granular assessments (e.g., global summarization and fine-grained temporal reasoning). Video-MMMU [14] emphasizes knowledge acquisition across perception, comprehension, and adaptation, using professional educational videos to evaluate models ability to learn new information. As base models for ReFoCUS, we adopt LLaVA-OV [20] and InternVL series [47, 64] in both lightweight (1B) and standard-size (78B) variants. 4.1 Main Results of Video Understanding Performance on Task-diverse Video QA Benchmarks. Tab. 1 demonstrates that integrating our ReFoCUS framework consistently improves performance across multiple benchmarks and model scales (lightwieght to standard sizes). Rather than relying on heruistic uniform sampling as other baselines do, ReFoCUS learns to identify query-relevant frames that bring semantically rich and contextually aligned visual cues. This targeted selection significantly enhances reasoning capabilities, particularly in cases where key evidence is sparsely distributed across time. The notable performance gains on the long subset of Video-MME [12] further support the effectiveness of our approach in handling complex, multi-event scenariosnot just covering more contents, but by isolating the frames that are most helpful to answer the users question. Improvements across LongVideoBench [48] and MLVU [61] also validate that ReFoCUS generalizes well on other benchmarks, serving as model-agnostic input-level optimization for enhancing video-LLMs. Knowledge Acquisition Evaluation on Video-MMMU. We further evaluate our method on VideoMMMU [14], recent benchmark specifically designed to assess the knowledge acquisition ability of LMMs from expert-level educational videos. Extending perception and temporal reasoning tasks, Video-MMMU assess models in three stages: Perception, Comprehension, and Adaptation, which requires deeper cognitive thinking. This setup allows us to assess whether the frame selection policy learned by ReFoCUS, which emphasizes semantically and temporally aligned visual cues, can support models in applying video content to novel problem settings. As shown in Tab. 2, ReFoCUS-enhanced 7 Table 2: Video-MMMU results (%) across three cognitive stages. knowledge denotes the normalized accuracy gain after watching the video, measuring how much the model learns from visual input. Model #param LLaVA-OV [20] + ReFoCUS LLaVA-OV [20] + ReFoCUS InternVL3 [64] + ReFoCUS InternVL3 [64] + ReFoCUS 0.5B 7B 1B 8B Perception Comprehension Adaption Overall knowledge Video-MMMU 15.67 18.33 41.00 40.67 33.67 33.00 73.00 69.67 16.67 17.33 32.33 36.33 26.67 26.33 41.33 45.00 21.33 21.00 33.33 36.00 24.00 26.33 33.67 41.67 17.89 18.89 35.55 37.67 28.11 28.55 49.33 52.11 -1.29 -1.28 +0.00 +0.52 +0.00 +3.48 -0.10 +3. models show improved performance (especially, in the Adaptation task), suggesting that our method can serve as effective frame priors for navigating complex and knowledge-intensive scenarios. 4.2 Policy Behavior Analysis for Frame Selection in ReFoCUS Are ReFoCUS Selections Semantically Grounded? To verify whether the learned selection distribution of ReFoCUS indeed captures semantically meaningful frames, we perform an ablation over its predicted frame likelihoods from the policy model trained with the proposed policy optimization. Specifically, for each v, pair in Video-MME, we conduct 64 times of autoregressive sampling to approximate the probability density function (PDF) across frame indices induced by the policy model. Using the estimated PDF, we partition the frames into cumulative bins (e.g., 20%, 40%, 60%, etc,.) based on their likelihood, and evaluate the performance with only frames sampled from each bin (under-/over-k%). As illustrated in Fig. 4, the prediction accuracy steadily increases when using under-k% frames (solid lines), indicating that low-likelihood frames are generally less informative. In contrast, the over-k% subsets (dashed lines) generally outperform their complementary under-k% counterparts even within small sample space, and surpass the baseline when is smallhighlighting that high-likelihood frames capture strong visual cues while maintaining compact yet effective context. This symmetric result confirms that the learned frame distribution from ReFoCUS is sufficiently informative to answer the query, suggesting that πθ has internalized useful scoring patterns aligned with the models behavior. Figure 4: Prediction ratio relative to the baseline accuracy, across overk% (dashed) and under-k% (solid) frame subsets from each bin. Visual Needle-in-a-Haystack: Does ReFoCUS Find the Right Cues? To further examine whether ReFoCUS can accurately locate task-relevant visual evidence, we experiment fine-grained analysis under V-NIAH (Visual Needle-In-A-Haystack) setup [59]. As in Fig. 5(a), the heatmap visualizes uniform sampling from InternVL3-8B [64], which fails to capture the temporally sparse but crucial signal (needle frame), as it selects frames uniformly across the entire sequence without regard to content relevance. In contrast, our ReFoCUS-based selection (Fig. 5(b)) exhibits strong concentration on the true needle frame across varying temporal positions, which highlight ReFoCUSs capability to precisely localize query-relevant visual evidence within complex scenes. Task-specific Frame Selection Patterns. To verify that ReFoCUS does not merely learn temporally biased frame selection policy, we analyze how the selection distributions vary across different v, pairs. Accordingly, we compute the pairwise distances between the frame selection distributions of different v, pairs in the Video-MME [12], using distributional metricsJS divergence, symmetric KL divergence, and Wasserstein distance. As reported in Tab. 3, our model exhibits high diversity across the pairs regardless of video segment length, presenting that the learned policy adapts its selection strategy based on query semantics rather than relying on consistent temporal priors. Unlike uniform sampling strategies, which apply the same fixed selection pattern regardless of the input, ReFoCUS adapts its frame selection to each video-query pair, capturing task-specific frame relevance without relying on temporally biased patterns. Please see more details in Appendix C. 8 (a) Uniform Selection (b) ReFoCUS Selection Figure 5: Result of V-NIAH. (a) Uniform sampling (b) Frame selection of ReFoCUS. The x-axis denotes the total #video frames, and the y-axis indicates the relative position of the needle frame. Table 3: Mean pairwise distances between frame selection distributions across different videos within each length segment. Table 4: Video-MME performance for different frame selection sizes. Note that footnote indicates gains compared to the uniform sampling. metric JensenShannon Div. Symmetric KL Div. Wasserstein Dist. Video-MME Short Medium Long 0.55 0.59 0.52 0.88 0.94 0.85 49.96 59.37 39.17 # frame 4 frm 8 frm 32 frm LLaVA-OV [20] InternVL3 [64] 0.5B 41.1 +0.4 42.9 +1.8 44.4 +1.6 7B 50.4 +0.3 54.5 +0.7 58.8 +0.3 1B 45.1 +0.4 48.0 +0.9 50.3 +0.7 8B 56.9 +1.0 59.7 +0.2 65.7 +1.5 4.3 Additional Analysis on ReFoCUS To further investigate the generality of the learned policy effectiveness, we evaluate whether ReFoCUS can effectively perform frame selection under varying frame exploration sizes {4, 8, 32}. As in Tab. 4, we can observe two key trends: (i) Performance consistently improves as more frames are allowed for selection, suggesting that our policy benefits from increased key evidences; (ii) across all frame sizes, ReFoCUS outperforms uniform sampling, indicating its capability of finding informative frames. In addition to the gains, we assess how widely the selected frames are distributed over time using the KozachenkoLeonenko entropy [18], non-parametric estimator based on k-NN distances. As in Fig. 6, the entropy notably decreases throughout training when using lower , presenting that the policy learns to concentrate on narrow temporal window. In contrast, the entropy for = 32 remains stable at higher level, indicating that the model can maintain broader temporal coverage without collapsing into narrowed selections. Figure 6: Temporal entropy of selected frames over training steps for different selection {4, 8, 32}. Smaller lead to more focused selections."
        },
        {
            "title": "5 Discussion and Conclusion",
            "content": "Discussion While ReFoCUS introduces an intriguing direction of shifting policy optimization from output-level textual alignment to input-level visual grounding, several limitations remain. As like in other RL processes do, our training incurs considerable computational cost due to repeated autoregressive sampling and reward estimation. In addition, the learned policy is inherently dependent on the preferences of the reward model, which can lead suboptimal preferences, the policy may inherit them. Nevertheless, ReFoCUS demonstrates that modeling intrinsic visual preferences at the input level can derive semantically informative frame selections. We belive that our work opens new future direction for aligning LMM behavior not just by what they say, but by what they see. Conclusion We present ReFoCUS, reinforcement learning framework that shifts policy optimization from textual outputs to visual inputs in video-LLMs. By modeling frame selection as an autoregressive policy guided by margin-based rewards from reward model, ReFoCUS learns to identify semantically rich and temporally relevant frames that align with the models reasoning trajectory without frame-level supervision. Extensive benchmarks demonstrate consistent gains, validating input-level optimization as scalable and effective way for advanced multi-modal alignment."
        },
        {
            "title": "References",
            "content": "[1] Daechul Ahn, Yura Choi, Youngjae Yu, Dongyeop Kang, and Jonghyun Choi. Tuning large multimodal models for videos using reinforcement learning from ai feedback. arXiv preprint arXiv:2402.03746, 2024. [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:2371623736, 2022. [3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [4] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: large-scale video benchmark for human activity understanding. In Proceedings of the ieee conference on computer vision and pattern recognition, pages 961970, 2015. [5] Junbum Cha, Wooyoung Kang, Jonghwan Mun, and Byungseok Roh. Honeybee: Locality-enhanced projector for multimodal llm. arXiv preprint arXiv:2312.06742, 2023. [6] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin, Zhenyu Tang, et al. Sharegpt4video: Improving video understanding and generation with better captions. arXiv preprint arXiv:2406.04325, 2024. [7] Shimin Chen, Xiaohan Lan, Yitian Yuan, Zequn Jie, and Lin Ma. Timemarker: versatile videollm for long and short video understanding with superior temporal localization ability. arXiv preprint arXiv:2411.18211, 2024. [8] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Zhong Muyan, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023. [9] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. InstructBLIP: Towards general-purpose vision-language models with instruction tuning. In Advances in Neural Information Processing Systems, 2023. [10] Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. ArXiv, abs/2405.21060, 2024. [11] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, et al. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. arXiv preprint arXiv:2401.16420, 2024. [12] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. [13] Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava, and Ser-Nam Lim. Ma-lmm: Memory-augmented large multimodal model for long-term video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13504 13514, 2024. [14] Kairui Hu, Penghao Wu, Fanyi Pu, Wang Xiao, Yuanhan Zhang, Xiang Yue, Bo Li, and Ziwei Liu. Video-mmmu: Evaluating knowledge acquisition from multi-discipline professional videos. arXiv preprint arXiv:2501.13826, 2025. [15] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, et al. Language is not all you need: Aligning perception with language models. Advances in Neural Information Processing Systems, 36, 2024. [16] Soyeong Jeong, Kangsan Kim, Jinheon Baek, and Sung Ju Hwang. Videorag: Retrieval-augmented generation over video corpus. arXiv preprint arXiv:2501.05874, 2025. [17] Junho Kim, Hyunjun Kim, Hosu Lee, and Yong Man Ro. Salova: Segment-augmented long video assistant for targeted retrieval and routing in long-form video analysis. arXiv preprint arXiv:2411.16173, 2024. [18] Leonenko Kozachenko. Sample estimate of the entropy of random vector. Problems of Information Transmission, 23:916, 1987. [19] Hosu Lee, Junho Kim, Hyunjun Kim, and Yong Man Ro. Look every frame all at once: Video-ma2mba for efficient long-form video understanding with multi-axis gradient checkpointing. arXiv preprint arXiv:2411.19460, 2024. [20] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [21] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024. [22] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International Conference on Machine Learning. PMLR, 2023. [23] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2219522206, 2024. [24] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023. 10 [25] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pretraining for visual language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2668926699, 2024. [26] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023. [27] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Advances in Neural Information Processing Systems, 2023. [28] Jiajun Liu, Yibing Wang, Hanghang Ma, Xiaoping Wu, Xiaoqi Ma, Xiaoming Wei, Jianbin Jiao, Enhua Wu, and Jie Hu. Kangaroo: powerful video-language model supporting long-context video input. arXiv preprint arXiv:2408.15542, 2024. [29] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. [30] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, et al. Mm1: Methods, analysis & insights from multimodal llm pre-training. arXiv preprint arXiv:2403.09611, 2024. [31] OpenAI. ChatGPT. https://openai.com/blog/chatgpt/, 2023. [32] OpenAI. Gpt-4 technical report, 2023. [33] OpenAI. GPT-4V(ision) System Card, 2023. [34] OpenAI. Hello gpt-4o, 2024. [35] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [36] Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adria Recasens, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Mateusz Malinowski, Yi Yang, Carl Doersch, et al. Perception test: diagnostic benchmark for multimodal video models. Advances in Neural Information Processing Systems, 36, 2024. [37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. [38] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. [39] Ruchit Rawal, Khalid Saifullah, Miquel Farré, Ronen Basri, David Jacobs, Gowthami Somepalli, and Tom Goldstein. Cinepile: long video question answering dataset and benchmark. arXiv preprint arXiv:2405.08813, 2024. [40] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [41] Weiming Ren, Huan Yang, Jie Min, Cong Wei, and Wenhu Chen. Vista: Enhancing long-duration and highresolution video understanding by video spatiotemporal augmentation. arXiv preprint arXiv:2412.00927, 2024. [42] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [43] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, et al. Moviechat: From dense token to sparse memory for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1822118232, 2024. [44] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, LiangYan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525, 2023. [45] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [46] Fei Wang, Wenxuan Zhou, James Huang, Nan Xu, Sheng Zhang, Hoifung Poon, and Muhao Chen. mdpo: Conditional preference optimization for multimodal large language models. arXiv preprint arXiv:2406.11839, 2024. [47] Weiyun Wang, Zhe Chen, Wenhai Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Jinguo Zhu, Xizhou Zhu, Lewei Lu, Yu Qiao, et al. Enhancing the reasoning ability of multimodal large language models via mixed preference optimization. arXiv preprint arXiv:2411.10442, 2024. [48] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. arXiv preprint arXiv:2407.15754, 2024. [49] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining temporal actions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 97779786, 2021. 11 [50] Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, and Jiashi Feng. Pllava: Parameter-free llava extension from images to videos for video dense captioning. arXiv preprint arXiv:2404.16994, 2024. [51] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. [52] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. [53] Jinhui Ye, Zihan Wang, Haosen Sun, Keshigeyan Chandrasegaran, Zane Durante, Cristobal Eyzaguirre, Yonatan Bisk, Juan Carlos Niebles, Ehsan Adeli, Li Fei-Fei, et al. Re-thinking temporal search for long-form video understanding. arXiv preprint arXiv:2504.02259, 2025. [54] Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl3: Towards long image-sequence understanding in multi-modal large language models. arXiv preprint arXiv:2408.04840, 2024. [55] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. [56] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. [57] Sicheng Yu, CHENGKAI JIN, Huanyu Wang, Zhenghao Chen, Sheng Jin, ZHONGRONG ZUO, XU XIAOLEI, Zhenbang Sun, Bingni Zhang, Jiawei Wu, Hao Zhang, and Qianru Sun. Frame-voyager: Learning to query frames for video large language models. In The Thirteenth International Conference on Learning Representations, 2025. [58] Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, et al. Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1380713816, 2024. [59] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. [60] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. [61] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024. [62] Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao. Aligning modalities in vision large language models via preference fine-tuning. arXiv preprint arXiv:2402.11411, 2024. [63] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. [64] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. [65] Daniel Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019."
        },
        {
            "title": "A Video Data Processing for ReFoCUS",
            "content": "A.1 Reward Variance Filtering Procedure Temporal Segmentation and Frame Subset Sampling. For each video with total length frames, we divide it into overlapping temporal segments using fixed window and stride. Specifically, we use window size of = /8 and stride of = w/2, resulting in 8 temporal windows: W1 = [0, w), W2 = [s, + w), . . . , W8 = [0, s) [7s, ) For each window Wi, we define its complementary range as Ci = [0, ) Wi. From each of Wi and Ci, we uniformly sample = 32 frames to construct total of 16 candidate frame subsets: {f (1), . . . , (8)} from windows, {f (9), . . . , (16)} from complements To better illustrate this process, we visualize the sampling scheme in Fig. 7. Each Wi (temporal window) and Ci (complement region) is shown as distinct colored dot line across the frame index axis. Note that each Wi captures focused local segment of the video, while its complement Ci represents the surrounding context. Together, the 16 frame subsets comprehensively span different temporal regions to assess the models sensitivity to varied visual evidence. Figure 7: Visualization of the temporal segmentation and sampling strategy. We divide each video into 8 overlapping windows W1 to W8 (top), and for each window, define complementary region Ci (bottom). We uniformly sample frames from both regions to construct 16 candidate subsets per QA pair. Prediction Margin and Reward Variance Computation. For each sampled frame subset {f (j)}16 j=1 of the same video and question q, we compute its prediction margin rj following the procedure used during training. The reward variance is then defined as Var(m) = Var (cid:0){rj}16 This variance measures the degree to which the models output depends on different temporal portions on v, pair; low variance therefore indicates weak temporal grounding. j=1 (cid:1) Thresholding and Sample Selection. We retain only the QA pairs whose variance Var(m) exceeds threshold τ = 0.21, determined empirically from the full distribution. This filtering removes samples with flat reward signals, yielding refined subset of approximately 98K QA pairs from the original 962K. Implementation Details. All frames are pre-extracted to optimize inference throughput. Prediction margins for all 16 frame subsets are computed in parallel using vectorized batch evaluation. Both raw logits and variance scores are logged for ablation and debugging. This procedure ensures stable reward estimation and improves learning dynamics in downstream policy optimization. 13 Implementation & Training Details B.1 Training Hyperparameters Table 5: Training Hyperparameters Config Value FPS Trainable params Learning rate (pretrained) Learning rate (re-initialized) LR scheduler Optimizer Global batch size Frame Selection Num candidates Warmup ratio Weight decay Gradient clipping Training precision DeepSpeed GC Input resolution min(4fps, 512frames) LLM + projector (vision LLM) + heads 1 105 1 104 linear AdamW (β1 = 0.9, β2 = 0.99) 196 32 16 0.05 0.01 1.0 bfloat16 ZeRO-1 Multi-Axis Gradient Checkpointing 384 384 B.2 Weight Initialization Strategy Figure 8: Overview of simplified policy model (omitting the vision encoder) with initialization constants. Here, γ indicates the RMSNorm weight parameter, and α denotes the orthogonal initialization gain for the linear projections in the Key, Query, and Value heads. To enable effective fine-tuning of our policy model, we adopt the following weight initialization strategies for each major component. All hyperparameters used in the initialization procedure were empirically determined. Specifically, the final two layers of the backbone network are fully re-initialized following the standard procedure in the Mamba2 reference implementation, rather than inheriting pretrained weights. For these re-initialized layers, the normalization weights γ in the residual path are rescaled to 0.1, which moderates the normalization strength and serves to inject controlled amount of noise into the residual path. Normalization layers positioned after entire blocks (preceding the key, query, and value heads) are initialized with γ = 1.0 to ensure standard scaling prior to linear projection. 14 For the core heads involved in frame selectionnamely, the query, key, and value headswe employ orthogonal initialization to ensure that the initial projections span well-conditioned and predictable distribution. In the case of the value head, the orthogonal gain α is set to 0.1 and the bias is initialized to zero. For the query and key heads, the gain is set to 1.0, and the layers are instantiated without bias. The embedding for the special token <start_of_frame> that triggers frame prediction is initialized from normal distribution with standard deviation 0.02, following recent practices in Transformerbased architectures. Finally, the logit scaling parameter is initialized to 1.0 to avoid biasing the attention scores at the early stage of training. To further ensure scale invariance in attention computation, all similarity dmodel as in standard scaled dot-product attention. scores are divided by These initialization choices collectively improve training stability and convergence in our experiments. B.3 Policy Learning Objectives B.3.1 Confidence Margin Reward: Reformulation with Tanh Given candidate frame subset (j), we define the hardest negative as = arg max y=y rφ(y (j), q), (3) where denotes the correct answer and rφ is the reward models score. To quantify the models confidence in its prediction, we compute the normalized confidence margin, rj = rφ(y (j), q) rφ(y (j), q) rφ(y (j), q) + rφ(y (j), q) [1, 1], (4) Writing pre-softmax logits z(j) simplifies to = logitφ(y (j), q), it is straightforward to show that this margin rj = ez(j) ez(j) ez(j) + ez(j) = tanh (cid:32) z(j) z(j) (cid:33) . (5) which is more stable numerically and computationally efficient, as it avoids explicit computation of probabilities. B.3.2 Equivalence of Entropy Bonus and Uniform KL Our policy is regularised with an entropy bonus to promote exploration. Below we show that this bonus is, up to an additive constant, identical to the KL divergence from the policy to the uniform distribution over the remaining frame pool. Entropy bonus. At frame-selection step i, candidate j, video v, query the conditional entropy is H(cid:0)πθ( (j) <i , v, q)(cid:1) = (cid:88) aA πθ(a (j) <i , v, q) log πθ(a (j) <i , v, q), (6) where is the set of still-available frames. The overall regularization term is the expectation of equation 6: H(πθ) = Ei,j,v,q (cid:2)H(πθ( (j) <i , v, q))(cid:3), Relation to KL divergence. Let be the uniform distribution over A; then DKL (cid:0)πθ U(cid:1) = (cid:88) aA πθ(a) log πθ(a) U(a) = H(πθ) + log A, because U(a) = 1 for all a. Equivalence. Combining equation 6 and equation 8 yields H(πθ) = DKL (cid:0)πθ U(cid:1) + log A, Ei,j,v,q[H] = Ei,j,v,q[DKL] + const. (9) The additive constant log does not depend on the model parameters, so both forms produce identical gradients and thus the same exploration effect during optimization. 15 (7) (8) Algorithm 2 Autoregressive Frame Subset Sampling Require: Input video of frames, query q, selection length , candidate number , temperature = Backbone(v) = KeyHead(H) = ValueHead(H) for = 1 to do for = 1 to do }).last <i z(j) = Backbone(v, q, {Vf (j) q(j) = QueryHead(z(j) ) = q(j) ℓ(j) [f (j) ℓ(j) (j) Categorical(softmax(ℓ(j) end for dmodel <i ] i )) end for return {f (j)}N j=1 Compute frame embeddings Key projection for each frame Value projection for each frame Get latent for next frame selection Query vector for attention Scaled Dot Product Score Function Mask previously selected frames Sample frame index Return all sampled frame subsets B.4 Autoregressive Frame Sampling Algorithm To further clarify the sampling mechanism of our model, we provide step-by-step pseudocode in Algorithm 2. In our implementation, each of the Key, Query, and Value heads is implemented as single linear layer. To prevent redundant information and encourage diversity, we mask out already selected frames at every step so that no frame is chosen more than once per candidate subset. B.5 Frame Embedding Definition Each input frame is represented as sequence of visual token embeddings extracted from the vision encoder. For downstream frame selection and reasoning, it is often desirable to aggregate this sequence into single latent representation summarizing the frame content. In our approach, we use the last token strategy. Since the Mamba backbone processes each frames token sequence in causal (i.e., left-to-right) manner, analogous to an autoregressive decoder, the output at the final token position is expected to aggregate information from all preceding tokens in the frame. Thus, we extract the output corresponding to the last index of each frames token sequence and use it as the key embedding for that frame in all subsequent attention and selection computations. 16 B.6 Prompt Design B.6.1 Reward Model Prompt Design Please choose the correct answer to the The answer must be single <im_end> <img><IMG_CONTEXT> </img> <img><IMG_CONTEXT> </img> <im_start>system You are helpful assistant. multiple-choice question regarding video. uppercase letter. <im_start>user Frame1: Frame2: ... FrameT : <Question Statement> Options: A. <option A> B. <option B> ... <im_end> <im_start>assistant Answer: <img><IMG_CONTEXT> </img> B.6.2 Policy Model Prompt Design <system>You are helpful AI assistant.<endoftext> <user> <image_1> <image_v> <Question Statement> A. <option A> B. <option B> ... <endoftext>"
        },
        {
            "title": "C Analysis of Diversity in Policy Frame Selection",
            "content": "For each v, pair, we perform = 64 independent autoregressive sampling runs. At each selection step, the categorical distribution over available frames is averaged first across steps and then across candidates to obtain representative selection distribution: pv,q = 1 (cid:88) j=1 1 (cid:88) i=1 πθ( (j) <i , v, q) (10) For fair comparison, v, pairs are grouped by video length. Each of the short, medium, and long categories contains 200 videos and 600 pairs. Pairwise distances between selection distributions are computed within each group, using the metrics described in the main text."
        },
        {
            "title": "D Qualitative Results",
            "content": "17 Figure 9: Red dashed line indicates the visual cues to answer the given question. 18 Figure 10: Red dashed line indicates the visual cues to answer the given question. 19 Figure 11: Red dashed line indicates the visual cues to answer the given question. 20 Figure 12: Red dashed line indicates the visual cues to answer the given question. 21 Figure 13: Red dashed line indicates the visual cues to answer the given question."
        }
    ],
    "affiliations": [
        "Integrated Vision and Language Lab, KAIST, South Korea"
    ]
}