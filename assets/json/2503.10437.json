{
    "paper_title": "4D LangSplat: 4D Language Gaussian Splatting via Multimodal Large Language Models",
    "authors": [
        "Wanhua Li",
        "Renping Zhou",
        "Jiawei Zhou",
        "Yingwei Song",
        "Johannes Herter",
        "Minghan Qin",
        "Gao Huang",
        "Hanspeter Pfister"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Learning 4D language fields to enable time-sensitive, open-ended language queries in dynamic scenes is essential for many real-world applications. While LangSplat successfully grounds CLIP features into 3D Gaussian representations, achieving precision and efficiency in 3D static scenes, it lacks the ability to handle dynamic 4D fields as CLIP, designed for static image-text tasks, cannot capture temporal dynamics in videos. Real-world environments are inherently dynamic, with object semantics evolving over time. Building a precise 4D language field necessitates obtaining pixel-aligned, object-wise video features, which current vision models struggle to achieve. To address these challenges, we propose 4D LangSplat, which learns 4D language fields to handle time-agnostic or time-sensitive open-vocabulary queries in dynamic scenes efficiently. 4D LangSplat bypasses learning the language field from vision features and instead learns directly from text generated from object-wise video captions via Multimodal Large Language Models (MLLMs). Specifically, we propose a multimodal object-wise video prompting method, consisting of visual and text prompts that guide MLLMs to generate detailed, temporally consistent, high-quality captions for objects throughout a video. These captions are encoded using a Large Language Model into high-quality sentence embeddings, which then serve as pixel-aligned, object-specific feature supervision, facilitating open-vocabulary text queries through shared embedding spaces. Recognizing that objects in 4D scenes exhibit smooth transitions across states, we further propose a status deformable network to model these continuous changes over time effectively. Our results across multiple benchmarks demonstrate that 4D LangSplat attains precise and efficient results for both time-sensitive and time-agnostic open-vocabulary queries."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . [ 1 7 3 4 0 1 . 3 0 5 2 : r 4D LangSplat: 4D Language Gaussian Splatting via Multimodal Large Language Models Wanhua Li1,, Renping Zhou1,2,, Jiawei Zhou3, Yingwei Song1,4, Johannes Herter1,5, Minghan Qin2, Gao Huang2,(cid:12), Hanspeter Pfister1,(cid:12) 1Harvard University 2Tsinghua University 3Stony Brook University 4Brown University 5ETH Zurich Project page: https://4d-langsplat.github.io/ Figure 1. Visualization of the learned language features of our 4D LangSplat. We observe that 4D LangSplat effectively learns dynamic semantic features that change over time, such as the gradual diffusion of coffee shown in the first two rows, and the chicken toggling between open and closed states in the latter two rows. Additionally, our semantic field captures consistent features for semantics that remain unchanged over time, with the clear object boundaries in the visualization demonstrating the precision of our semantic field."
        },
        {
            "title": "Abstract",
            "content": "Learning 4D language fields to enable time-sensitive, openended language queries in dynamic scenes is essential for many real-world applications. While LangSplat successfully grounds CLIP features into 3D Gaussian representations, achieving precision and efficiency in 3D static scenes, it lacks the ability to handle dynamic 4D fields as CLIP, Equal contribution. (cid:12)Corresponding authors. designed for static image-text tasks, cannot capture temporal dynamics in videos. Real-world environments are inherently dynamic, with object semantics evolving over time. Building precise 4D language field necessitates obtaining pixel-aligned, object-wise video features, which current vision models struggle to achieve. To address these challenges, we propose 4D LangSplat, which learns 4D language fields to handle time-agnostic or time-sensitive open-vocabulary queries in dynamic scenes efficiently. 4D LangSplat bypasses learning the language field from vision features and instead learns directly from text generated from object-wise video captions via Multimodal Large Language Models (MLLMs). Specifically, we propose multimodal object-wise video prompting method, consisting of visual and text prompts that guide MLLMs to generate detailed, temporally consistent, high-quality captions for objects throughout video. These captions are encoded using Large Language Model into high-quality sentence embeddings, which then serve as pixel-aligned, objectspecific feature supervision, facilitating open-vocabulary text queries through shared embedding spaces. Recognizing that objects in 4D scenes exhibit smooth transitions across states, we further propose status deformable network to model these continuous changes over time effectively. Our results across multiple benchmarks demonstrate that 4D LangSplat attains precise and efficient results for both timesensitive and time-agnostic open-vocabulary queries. 1. Introduction The ability to construct language field [20, 38] that supports open vocabulary queries holds significant promise for various applications such as robotic navigation [15], 3D scene editing [22], and interactive virtual environments [33]. Due to the scarcity of large-scale 3D datasets with rich language annotations, current methods [20, 33, 41] leverage pre-trained models like CLIP [39] to extract pixel-wise features, which are then mapped to 3D spaces. Among them, LangSplat [38] received increasing attention due to its efficiency and accuracy, which grounds the precise masks generated by the Segment Anything Model (SAM) [21] with CLIP features into 3D Gaussians, achieving an accurate and efficient 3D language field by leveraging 3D Gaussian Splatting (3D-GS) [19]. LangSplat supports open-vocabulary queries in various semantic levels by learning three SAM-defined semantic levels. Nothing endures but change. Real-world 3D scenes are rarely static, and they continuously change and evolve. To enable open-vocabulary queries in dynamic 4D scenes, it is crucial to consider that target objects may be in motion or transformation. For instance, querying scene for dog in dynamic environment may involve the dog running, jumping, or interacting with other elements. Beyond spatial changes, users may also want time-related queries, such as running dog, which should only respond during the time segments when the dog is indeed running. Therefore, supporting time-agnostic and time-sensitive queries within 4D language field is essential for realistic applications. straightforward approach to extend LangSplat to 4D scene is to learn deformable Gaussian field [30, 59, 62] with CLIP features. However, it cannot model the dynamic, time-evolving semantics as CLIP, designed for static imagetext matching [11, 28], struggles to capture temporal information such as state changes, actions, and object conditions [48, 49]. Learning precise 4D language field would require pixel-aligned, object-level video features as the 2D supervision to capture the spatiotemporal semantics of each object in scene, yet current vision models [58, 60] predominantly extract global, video-level features. One could extract features by cropping interested objects and then obtain patch features. It inevitably includes background information, leading to imprecise semantic features [38]. Removing the background and extracting vision features only from the foreground object with accurate object masks leads to ambiguity in distinguishing between object and camera motion, since only the precise foreground objects are visible without reference to the background context. These pose significant challenges for building an accurate and efficient 4D language field. To address these challenges, we propose 4D LangSplat, which constructs precise and efficient 4D Language Gaussian field to support time-agnostic and time-sensitive openvocabulary queries. We first train 4D Gaussian Splatting (4D-GS) [59] model to reconstruct the RGB scene, which is represented by group of Gaussian points and deformable decoder defining how the Gaussian point changes its location and shape over time. Our 4D LangSplat then enhances each Gaussian in 4D-GS with two language fields, where one learns time-invariant semantic fields with CLIP features as did in LangSplat, and the other learns time-varying semantic field to capture the dynamic semantics. The timeinvariant semantic field encodes semantic information that does not change over time such as human, cup, and dog. They are learned with CLIP features on three SAMdefined semantic levels. For the time-varying semantic field, instead of learning from vision features, we propose to directly learn from textual features to capture temporally dynamic semantics. Recent years have witnessed huge progress [35, 46] of Multimodal Large Language Models (MLLMs), which take multimodal input, including image, video, and text, and generate coherent responses. Encouraged by the success of MLLMs, we propose multimodal object-wise video prompting method that combines visual and text prompts to guide MLLMs in generating detailed, temporally consistent, high-quality captions for each object throughout video. We then encode these captions using large language model (LLM) to extract sentence embeddings, creating pixel-aligned, object-level features that serve as supervision for the 4D Language field. Recognizing the smooth transitions exhibited by objects across states in 4D scenes, we further introduce status deformable network to model these continuous state changes effectively over time. Our network captures the gradual transitions across object states, enhancing the models temporal consistency and improving its handling of dynamic scenes. Figure 1 visualizes the learned time-varying semantic field. Our experiments across multiple benchmarks validate that 4D LangSplat achieves precise and efficient results, supporting both time-agnostic and time-sensitive open-vocabulary queries in dynamic, real-world environments. In summary, our contributions are threefold: We introduce 4D LangSplat for open-vocabulary 4D spatial-temporal queries. To the best of our knowledge, we are the first to construct 4D language fields with object textual captions generated by MLLMs. To model the smooth transitions across states for objects in 4D scenes, we further propose status deformable network to capture continuous temporal changes. Experiential results show that our method attains stateof-the-art performance for both time-agnostic and timesensitive open-vocabulary queries. 2. Related Work 3D Gaussian Splatting. 3D-GS [19] is powerful volumetric rendering technique that has gained attention for its real-time, high-quality rendering ability. It represents complex surfaces and scenes by projecting 3D Gaussian distributions into 2D image space. It has been widely used for many applications such as human reconstruction [26, 40], 3D editing [7, 54], mesh extraction [14, 53], autonomous driving [66, 68]. Recent work [2, 30, 31, 62] including 4D Gaussian Splatting (4D-GS) [59] has extended Gaussian Splatting to 4D by introducing deformable fields, allowing for dynamic scenes where Gaussian parameters evolve over time to capture both spatial and temporal transformations. However, 4D-GS primarily focuses on visual fidelity rather than semantic understanding, which limits its applicability in open-vocabulary language queries. 3D Language Field. Some early work [22, 52] usually ground 2D foundation model features [6, 24, 39] into neural radiance field (NeRF) [34]. For example, Distilled Feature Fields (DFFs) propose to distill CLIP-LSeg [24] into NeRF for semantic scene editing. LERF [20] proposes to distill CLIP [39] features into NeRF to support open-vocabulary 3D querying. With the emergence of 3DGS, many methods [42, 63, 64, 67] adopt 3D-GS as the 3D scene representation and lift 2D foundation model features into 3D Gaussians. Among them, LangSpalt [38] attains precise and efficient language fields due to the introduction of SAM masks. By incorporating multiple levels of semantic granularity, LangSplat effectively supports openvocabulary queries across whole objects, parts, and subparts. Although significant advances have been made in 3D language fields, 4D language fields for dynamic scenes remain largely unexplored, which is the focus of this paper. Multimodal Large Language Models. The remarkable success of LLMs [3, 10, 50, 51] has shown their ability to perform new tasks [29] following human instructions. Based on LLMs, the research on MLLMs [4, 32, 36] explores the possibility of multimodal chat ability [16], which represents significant step forward in integrating visual and textual modalities for complex scene understanding. MLLMs usually employ vision encoder to extract visual features and learn connector to align visual features with LLMs. The recent models [9, 25, 57] demonstrate remarkable capabilities in generating coherent captions from multimodal inputs, including images and videos. In this paper, we propose to utilize the powerful multimodal process ability of MLLMs to convert video data into object-level captions, which are then used to train 4D language field. 3. Method 3.1. Preliminaries 3D Gaussian Splatting. In 3D-GS [19], scene is represented as set of 3D Gaussian points. Each pixel in 2D images is computed by blending sorted 3D Gaussian points that overlap the pixel: = (cid:88) i=1 i1 (cid:89) ciαi (1 αj), j=1 (1) where ci and αi are the color and density of i-th Gaussian. LangSplat. Building upon 3D-GS, LangSplat [38] grounds 2D CLIP features into 3D Gaussians. To obtain precise field, SAM is used to obtain accurate object masks and then CLIP features are extracted with masked objects. LangSplat adopts feature splatting to train the 3D language field: = (cid:88) i=1 fiαi i1 (cid:89) j=1 (1 αj), (2) where fi represents the language feature of the i-th Gaussians and is the rendered embedding in 2D images. 4D Gaussian Splatting. 4D-GS [59] extends the 3D-GS for dynamic scenes by introducing deformable Gaussian field. Here, Gaussian parameters, including position, rotation, and scaling factor, are allowed to vary over time: (X , r, s) = (X + , + r, + s), (3) where , r, and represent the position, rotation, and scaling parameters, respectively. , r, and denote the corresponding deformable networks, which are implemented by lightweight MLPs. The HexPlane [5, 13] representation is used to obtain rich 3D Gaussian features. straightforward approach to adapting LangSplat for 4D scenes is to extend its static 3D language Gaussian field with deformable Gaussian field, as done in 4D-GS. However, this approach faces significant limitations due to the nature of CLIP features. CLIP [39] is designed primarily for static image-text alignment, making it ill-suited for capturing dynamic and time-evolving semantics in video. Recent research [48, 49, 56] further confirms that it struggles with understanding state changes, actions, object conditions, and temporal context. For precise and accurate 4D language field, it is essential to obtain pixel-aligned, object-level features that track temporal semantics with fine-grained detail for each object in scene. However, existing vision models [58, 60] primarily offer global, video-level features that overlook specific object-level information, making it difficult to represent spatiotemporal semantics at the object level. While cropping objects and obtaining patch-based features is possible, this includes background information, leading to inaccurate language fields. Further cropping objects with accurate masks makes it difficult for vision models to distinguish between object movement and camera motion, as there is no background reference. 3.2. 4D LangSplat Framework To address these challenges, we introduce 4D LangSplat, which constructs accurate and efficient 4D language fields to support both time-sensitive and time-agnostic openvocabulary queries in dynamic scenes. We first reconstruct the 4D dynamic RGB scene using 4D-GS [59]. In this stage, the RGB scene is represented by set of deformable Gaussian points, each with parameters that adjust over time to capture object movement and shape transformations within the scene. Building on the learned 4D-GS model, we extend each Gaussian point with language embeddings to learn 4D language fields. To further capture temporal and spatial details, and to handle both time-sensitive and timeagnostic queries effectively, we simultaneously construct two types of semantic fields: time-agnostic semantic field and time-varying semantic field. The time-agnostic semantic field focuses on capturing semantic information that does not change over time. Although objects in the scene are dynamic, they still exhibit attributes that remain constant across time, such as static properties of entities like dog, human, and other objects within the environment. This semantic field emphasizes spatial details of these timeagnostic semantics. Conversely, the time-varying semantic field captures temporally dynamic semantics, such as running dog , emphasizing semantic transitions over time. For the time-agnostic semantic field, we still use CLIP features and lift them to 4D space, as they are sufficient for capturing time-agnostic semantics. Specifically, we learn static language embedding for each deformable Gaussian point in the 4D-GS model. Similar to LangSplat, we utilize SAMs hierarchical segmentation masks, learning three distinct time-agnostic semantic fields corresponding to the three levels of semantic granularity provided by SAM. Although each Gaussian points position and shape dynamically change over time, its semantic feature remains static. These static embeddings ensure spatial accuracy while focusing on stable semantic information derived from CLIP features. On the other hand, to learn the time-varying semantic field, we propose novel approach that bypasses the limitations of vision-based feature supervision. Instead, visual data is converted into object-level captions by leveraging MLLMs. These captions are then encoded using an LLM to extract sentence embeddings, which are used as pixel-aligned, object-level features for training the semantic field. To effectively model the smooth, continuous transitions of Gaussian points between limited set of states, we further introduce status deformable network to enhance reconstruction quality. The framework of training timevarying 4D fields is illustrated in Figure 2. 3.3. Multimodal Object-Wise Video Prompting Constructing high-quality, dynamic 4D semantic field requires detailed, pixel-aligned object-level features that capture time-evolving semantics in video data. However, obtaining these fine-grained visual features is challenging due to the limitations of current vision models in distinguishing object-level details over time. To overcome this, we propose converting video segments into object-wise captions and extracting sentence embeddings from these captions to serve as precise, temporally consistent features. Advances in MLLMs like GPT-4o [36], LLaVAOneVision [25], and Qwen2-VL [57] enable high-quality language generation from multimodal inputs. These models process video, image, and text inputs to generate temporally consistent responses. Leveraging these capabilities, we propose multimodal object-wise video prompting method, which combines visual and textual prompts to guide the MLLM in generating temporally consistent, object-specific, high-quality captions across video frames, encapsulating both spatial and temporal details. Formally, let = {I1, I2, . . . , IT } be video segment of frames. For each frame, we apply SAM [21] in conjunction with DEVA tracking [8] to segment objects and maintain consistent object identities over time. This process yields temporally consistent masks for objects present in the video, denoted as {M1, M2, . . . , Mn}, where each mask Mi represents specific object tracked across frames. Each frame It is segmented with the object masks at time step {M1,t, M2,t, . . . , Mn,t}. To effectively generate instance-wise, object-specific captions while preserving the broader scene context, we need to guide the MLLM through precise prompting. Our goal is for the MLLM to generate captions focused solely on the target object without introducing details of other objects. However, the presence of other objects as background reference remains essential; without this context, the MLLM may lose track of spatial relationships and environmental context, which are critical for understanding the action and Figure 2. The framework of constructing time-varying semantic field in 4D LangSplat. We first use multimodal object-wise prompting to convert video into pixel-aligned object-level caption features. Then, we learn 4D language field with status deformable network. status of the target object. Thus, our approach employs prompting techniques to direct the MLLMs attention to each object, enabling region-specific captioning that maintains overall scene awareness. Inspired by the recent visual prompting progress [43, 45, 61], we first use visual prompts to highlight the object of interest. Specifically, we build visual prompt Pi,t for each object in frame It: Pi,t = Contour(Mi,t) Gray(Mi,t) Blur(Mi,t), (4) where Contour(Mi,t) highlights Mi,t with red contour, Gray(Mi,t) converts the non-object area to grayscale, and Blur(Mi,t) applies Gaussian blur to the background pixels. This prompt preserves essential background information while ensuring focus on the object of interest, improving the MLLMs attention to the relevant target. For temporal coherence, we first generate high-level video-level motion description for object i, noted as Di, which summarizes the motion dynamics over frames. This description is derived by prompting the MLLM with the entire video sequence to capture object motion and interactions, defined as: where Tframe denotes the textual prompt that instructs the MLLM to generate an object caption describing the objects current action and status at specific time step. Each caption Ci,t provides semantic information for an object at time t. To encode this semantic data into features for training the 4D language field, we extract sentence embeddings ei,t for each caption Ci,t. As LLMs exhibit strong processing ability for free-form text [47, 50], we further propose to utilize them to extract sentence embeddings. Specifically, fined-tuned LLM [55] for sentence embedding tasks is used to extract features. This design choice allows our model to respond effectively to open-vocabulary queries as the embeddings are generated within shared language space that aligns with natural language queries. Thus, for every pixel (x, y) Mi,t within object is mask in frame It, the feature Fx,y,t is given by: Fx,y,t = ei,t, (7) where the embeddings Fx,y,t serve as 2D supervision for the time-variable semantic field, providing pixel-aligned, object-wise features across frames. Di = MLLM({Pi,1, ..., Pi,T }, Tvideo, ), (5) 3.4. Status Deformable Network where Tvideo denotes the textual prompt that instructs the MLLM to generate video-level motion descriptions based on the visual prompts. This description Di is then used as context for generating frame-specific captions. For each frame It, we combine Di with the visual prompt Pi,t to generate time-specific caption Ci,t, capturing both the temporal and contextual details for object in frame It: Ci,t = MLLM(Di, Pi,t, Tf rame, Vt), (6) With the 2D semantic feature supervision information available, we use it to train 4D field. straightforward approach, analogous to the method used 4D-GS, would be to directly learn deformation field for the semantic features of deformable Gaussian points. However, this straightforward approach allows the semantic features of each Gaussian point to change to any arbitrary semantic state, potentially increasing the learning complexity and compromising the temporal consistency of the features. In real-world dynamic scenes, each Gaussian point typically exhibits gradual transition between limited set of semantic states. For instance, an object like person may transition smoothly among finite set of actions (e.g., standing, walking, running), rather than shifting to entirely unrelated semantic states. To model these smooth transitions and maintain stable 4D semantic field, we propose status deformable network that restricts the Gaussian points semantic features to evolve within predefined set of states. Specifically, we represent the semantic feature of Gaussian point at any time as linear combination of state prototype features, {Si,1, Si,2, . . . , Si,K}, where each state captures specific, distinct semantic meaning. The semantic feature fi,t of Gaussian point at time is: fi,t = (cid:88) k=1 wi,t,kSi,k, (8) where wi,t,k denotes the weighting coefficient for each state at time t, with (cid:80)K k=1 wi,t,k = 1. This linear combination ensures that each Gaussian points semantic features transition gradually between predefined states. To determine the appropriate weighting coefficients wk,t for each Gaussian point over time, we employ an MLP decoder ϕ. This MLP takes as input the spatial-temporal features from Hexplane [5] and predicts weighting coefficients that reflect the temporal progression of semantic states. The MLP decoder ϕ and the per-Gaussian states {Si,1, Si,2, . . . , Si,K} are jointly trained. This design ensures that the status deformable network adapts to both the spatial and temporal context, enabling smooth, consistent transitions among semantic states. 3.5. Open-vocabulary 4D Querying After training, 4D LangSplat enables both time-agnostic and time-sensitive open-vocabulary queries. For timeagnostic queries, we utilize only the time-agnostic semantic field. We first render feature image and then compute the relevance score [20] between this rendered feature image and the query. Following the post-processing strategy in LangSplat [38], we obtain the segmentation mask for each frame from the relevance score maps. For time-sensitive queries, we combine both the timeagnostic and time-sensitive semantic fields. First, the timeagnostic semantic field is used to derive an initial mask for each frame, following the same procedure described above. This mask identifies where the queried object or entity exists, irrespective of time. To refine the query to specific time segments where the queried term is active (e.g., an action occurring within particular timeframe), we calculate the cosine similarity between the time-sensitive semantic field on the initial mask region and the query text. This similarity is computed across each frame within the masked region to determine when the time-sensitive characteristics of the query term are most strongly represented. Using the mean cosine similarity value across the entire video as threshold, we identify the frames that exceed this threshold, indicating relevant time segments. The spatial mask obtained with the time-agnostic field is retained as the final mask prediction for the identified time segments. This combination of time-agnostic and time-sensitive semantic fields enables accurate and efficient spatiotemporal querying, allowing 4D LangSplat to capture both the persistent and dynamic characteristics of objects in the scene. 4. Experiment 4.1. Setup Datasets. We conduct evaluations using two widely adopted datasets: HyperNeRF [37] and Neu3D [27] . Given the absence of semantic segmentation annotations for dynamic scenes in these datasets, we perform manual annotations to facilitate evaluation. More details regarding this process are provided in the Appendix A. Implementation Details. All experiments are conducted on single Nvidia A100 GPU. For extracting CLIP features, we use the OpenCLIP ViT-B/16 model . For dynamic semantics, we leverage the Qwen2-VL-7B model as the backbone MLLM to generate time-varying captions, and use e5mistral-7b [55] to encode them into embeddings. Following LangSplat [38], we also train an autoencoder to compress the feature dimension. The CLIP and the text features are compressed into 3 and 6 dimensions, respectively. Baselines. Due to the absence of publicly available models for 4D language feature rendering, we use several 3D language feature rendering methods as baselines for evaluating time-agnostic querying, including LangSplat [38] and Feature-3DGS [67] . We also incorporate segmentationbased techniques, such as Gaussian Grouping [63], to assess semantic mask generation quality in our approach. Inspired by Segment Any 4D Gaussians [17], we enhance Gaussian Grouping to adapt to dynamic scenes. Given the lack of dynamic language field rendering methods, we consider two additional baselines besides LangSplat for time-sensitive querying: Deformable CLIP and Non-Status Field. Deformable CLIP only utilizes the time-agnostic semantic fields of our method, which first trains 4D-GS model to learn dynamic RGB fields, and then learns static CLIP fields on these pre-trained RGB fields. The Non-Status Field method utilizes both the timeagnostic semantic field and the time-sensitive semantic field of our method while removing the status deformable network. Instead, it directly learns deformation field . Metrics. For time-agnostic querying, we evaluate performance using mean accuracy (mAcc) and mean intersection over union (mIoU), calculated across all frames Method americano chickchicken split-cookie espresso Average Acc(%) vIoU(%) Acc(%) vIoU(%) Acc(%) vIoU(%) Acc(%) vIoU(%) Acc(%) vIoU(%) LangSplat [38] Deformable CLIP Non-Status Field Ours 45.19 60.57 83.65 89.42 23.16 39.96 59.59 66.07 53.26 52.17 94.56 96. 18.20 42.77 86.28 90.62 73.58 89.62 91.50 95.28 33.08 75.28 78.46 83. 44.03 44.85 78.60 81.89 16.15 20.86 47.95 49.20 54.01 61.80 87.58 90. 22.65 44.72 68.57 72.26 Table 1. Quantitative comparisons of time-sensitive querying on the HyperNeRF [37] dataset. Figure 3. Visualization of time-sensitive querying results between Deformable CLIP and ours. The bottom row depicts the cosine similarity across frames, rescaled to (0,1) for direct comparison, while the horizontal bars indicate frames identified as relevant time segments. We observed that the CLIP-based method cannot understand dynamic semantics correctly, while our method recognizes them. Method HyperNeRF Neu3D mIoU mAcc mIoU mAcc Feature-3DGS [67] 36.63 74.02 34.96 87.12 Gaussian Grouping [63] 50.49 80.92 49.93 95.05 74.92 97.72 61.49 91.89 LangSplat [38] Ours 82.48 98.01 85.11 98.32 Table 2. Quantitative comparisons of time-agnostic querying on the HyperNeRF [37] and Neu3D [27] datasets (Numbers in %). Blur Gray Contour sim 0.33 2.15 3.32 Video Image sim 0.14 1.01 3.32 Table 3. Comparisons of Visual prompts. Table 4. Comparisons of Text prompts. 2 3 4 5 6 Acc (%) vIoU (%) 94.56 88.05 97.82 91.93 95.65 89.11 94.56 88.98 94.56 86.28 Table 5. Results for different state numbers on chick chicken. in the test set. For time-sensitive querying, we evaluate temporal performance using an accuracy metric, defined as Acc = ncorrect/nall, where ncorrect and nall represent the number of correctly predicted frames and the total frames in the test set, respectively. To assess segmen- (cid:80) tation quality, we adopt the metric from [65] and define vIoU = 1 IoU(ˆst, st), where ˆst and st are the Su predicted and ground truth masks at time t, and Su and Si are the sets of frames in the union and intersection. tSi 4.2. Main Results Time-Agnostic Querying. Table 2 shows our results on two datasets. Our approach achieves the highest mIoU and mAcc scores, demonstrating strong segmentation performance across both datasets. In contrast, other methods struggle to capture object movement and shape changes, leading to worse performance on dynamic objects. Time-Sensitive Querying. We perform dynamic querying on the HyperNeRF dataset, with Acc and vIoU results presented in Table 1. Our approach outperforms not only the LangSplat method but also the Deformable CLIP and Non-Status Field approaches. Specifically, our method achieves accuracy improvements of 29.03% and 3.25% and vIoU gains of 28.04% and 4.19%, respectively. Our approach introduces multimodal object-wise video prompting method that surpasses traditional CLIP-based techniques. In comparison to Deformable CLIP, our timevarying semantic field effectively integrates spatial and temporal information. This ensures fluidity and coherence in semantic state transitions, underscoring the importance of MLLM video prompting (Section 3.3). Additionally, when compared to the Non-Status Field method, our approach for positive and negative samples, respectively. higher sim indicates stronger distinction between positive and negative examples, suggesting that the generated caption more effectively captures the spatiotemporal dynamics and semantic features of objects in the scene. Table 3 shows that utilizing all three visual prompting strategies maximizes the MLLMs focus on target objects. As shown in Table 4, incorporating pre-generated video-level motion descriptions resulting in 0.87% improvement. Furthermore, adding image prompts enables more accurate description. State Numbers. Table 5 shows the ablation results of the status number K. We observe that an appropriate increase in led to better results, with = 3 achieving the optimal performance, which was adopted in our experiments. Figure 4. Comparison of time-sensitive query mask. We compare time-sensitive query masks between Deformable CLIP and ours. The CLIP-based method fails to identify time segments accurately, especially at the demarcation points during state transitions. highlights the significance of status modelling by introducing status deformable network (Section 3.4), which enhances the models capability to handle complex, evolving states and further solidifies the robustness and versatility of our method in capturing nuanced dynamics. Visualization. To demonstrate our learned time-sensitive language field, we applied PCA to reduce the dimensionality of the learned semantic features, producing 3D visualization as shown in Figure 1. Our method better captures the dynamic semantic features of objects and renders consistent features accurately. In Figure 3, we illustrate the change in query-frame similarity scores over time for timesensitive queries, comparing our approach to CLIP-based method. As shown, CLIP, which is optimized for static image-text alignment, struggles to capture the most relevant time segments within dynamic video semantics, whereas our method successfully identifies these segments. In Figure 4, we present specific query masks. We observe that the CLIP-based approach fails to accurately capture time segments, especially at transition points in object states. For example, CLIP cannot reliably detect subtle transitions, such as when cookie has just cracked or when glass cup has started dripping coffee. In contrast, our method effectively identifies these nuanced changes, demonstrating its capability to handle dynamic state transitions accurately. 5. Conclusion We present 4D LangSplat, novel approach to constructing dynamic 4D language field that supports both timeagnostic and time-sensitive open-vocabulary queries within evolving scenes. Our method leverages MLLMs to produce high-quality, object-specific captions that capture temporally consistent semantics across video frames. This enables 4D LangSplat to overcome the limitations of traditional vision feature-based approaches, which struggle to generate precise, object-level features in dynamic contexts. By incorporating multimodal object-wise video prompting, we obtain pixel-aligned language embeddings as training supervision. Furthermore, we introduce status deformable network, which enforces smooth, structured transitions across limited object states. Our experimental results across multiple benchmarks demonstrate that 4D LangSplat achieves state-of-the-art performance in dynamic scenarios. 4.3. Ablation Studies Multimodal Prompting. We evaluate the quality of generated captions using different combinations of textual and visual prompting methods. To quantify this, we defined metric, sim = scorepos scoreneg, where scorepos and scoreneg represent the average cosine similarity scores between query and caption features, encoded by the e5 model,"
        },
        {
            "title": "Acknowledgements",
            "content": "The work is supported in part by the National Key R&D Program of China under Grant 2024YFB4708200 and National Natural Science Foundation of China under Grant U24B20173, and in part by US NIH grant R01HD104969."
        },
        {
            "title": "References",
            "content": "[1] Hassan Akbari, Dan Kondratyuk, Yin Cui, Rachel Hornung, Huisheng Wang, and Hartwig Adam. Alternating gradient descent and mixture-of-experts for integrated multimodal perception. NeurIPS, 2023. 4 [2] Jeongmin Bae, Seoha Kim, Youngsik Yun, Hahyun Lee, Gun Bang, and Youngjung Uh. Per-gaussian embedding-based deformation for deformable 3d gaussian splatting. arXiv preprint arXiv:2404.03613, 2024. 3 [3] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei arXiv preprint Huang, et al. Qwen technical report. arXiv:2309.16609, 2023. 3 [4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 1(2):3, 2023. 3 [5] Ang Cao and Justin Johnson. Hexplane: fast representation for dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 130141, 2023. 3, 6 [6] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. [7] Yiwen Chen, Zilong Chen, Chi Zhang, Feng Wang, Xiaofeng Yang, Yikai Wang, Zhongang Cai, Lei Yang, Huaping Liu, and Guosheng Lin. Gaussianeditor: Swift and controllable 3d editing with gaussian splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2147621485, 2024. 3 [8] Ho Kei Cheng, Seoung Wug Oh, Brian Price, Alexander Schwing, and Joon-Young Lee. Tracking anything with decoupled video segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13161326, 2023. 4 [9] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatialtemporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. 3 [10] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2(3):6, 2023. 3 [11] Tong Ding, Wanhua Li, Zhongqi Miao, and Hanspeter Pfister. Tree of attributes prompt learning for vision-language models. arXiv preprint arXiv:2410.11201, 2024. 2 [12] Gueter Josmy Faure, Min-Hung Chen, and Shang-Hong Lai. Holistic interaction transformer network for action detection. In WACV, 2023. [13] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahbæk Warburg, Benjamin Recht, and Angjoo Kanazawa. K-planes: Explicit radiance fields in space, time, and appearance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1247912488, 2023. 3 [14] Lin Gao, Jie Yang, Bo-Tao Zhang, Jia-Mu Sun, Yu-Jie Yuan, Hongbo Fu, and Yu-Kun Lai. Mesh-based gaussian splatting for real-time large-scale deformation. arXiv preprint arXiv:2402.04796, 2024. 3 [15] Chenguang Huang, Oier Mees, Andy Zeng, and Wolfram Burgard. Visual language maps for robot navigation. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 1060810615. IEEE, 2023. 2 [16] Gao Huang. Dynamic neural networks: advantages and challenges. National Science Review, 11(8):nwae088, 2024. 3 [17] Shengxiang Ji, Guanjun Wu, Jiemin Fang, Jiazhong Cen, Taoran Yi, Wenyu Liu, Qi Tian, and Xinggang Wang. Segment any 4d gaussians. arXiv preprint arXiv:2407.04504, 2024. 6, 2 [18] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017. [19] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 2, 3 [20] Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo Kanazawa, and Matthew Tancik. Lerf: Language embedded In Proceedings of the IEEE/CVF Internaradiance fields. tional Conference on Computer Vision, pages 1972919739, 2023. 2, 3, 6 [21] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 40154026, 2023. 2, 4 [22] Sosuke Kobayashi, Eiichi Matsumoto, and Vincent Sitzmann. Decomposing nerf for editing via feature field distillation. Advances in Neural Information Processing Systems, 35:2331123330, 2022. 2, 3 [23] Hildegard Kuehne, Hueihan Jhuang, Estıbaliz Garrote, Tomaso Poggio, and Thomas Serre. Hmdb: large video In 2011 Interdatabase for human motion recognition. national conference on computer vision, pages 25562563. IEEE, 2011. 4 [24] Boyi Li, Kilian Weinberger, Serge Belongie, Vladlen Koltun, and Rene Ranftl. Language-driven semantic segmentation. arXiv preprint arXiv:2201.03546, 2022. 3 [25] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 3, 4 [26] Mengtian Li, Shengxiang Yao, Zhifeng Xie, and Keyu Chen. Gaussianbody: Clothed human reconstruction via 3d gaussian splatting. arXiv preprint arXiv:2401.09720, 2024. 3 [27] Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, Richard Newcombe, et al. Neural 3d video synthesis from multi-view video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 55215531, 2022. 6, 7, 2, 3, 4 [28] Wanhua Li, Xiaoke Huang, Zheng Zhu, Yansong Tang, Xiu Li, Jie Zhou, and Jiwen Lu. Ordinalclip: Learning rank prompts for language-guided ordinal regression. Advances in Neural Information Processing Systems, 35:3531335325, 2022. 2 [29] Wanhua Li, Zibin Meng, Jiawei Zhou, Donglai Wei, Chuang Gan, and Hanspeter Pfister. Socialgpt: Prompting llms for social relation reasoning via greedy segment optimization. In Advances in Neural Information Processing Systems, pages 22672291, 2024. 3 [30] Zhan Li, Zhang Chen, Zhong Li, and Yi Xu. Spacetime gaussian feature splatting for real-time dynamic view synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 85088520, 2024. 2, 3 [31] Youtian Lin, Zuozhuo Dai, Siyu Zhu, and Yao Yao. Gaussian-flow: 4d reconstruction with dynamic 3d gaussian particle. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21136 21145, 2024. [32] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. 3 [33] Kunhao Liu, Fangneng Zhan, Jiahui Zhang, Muyu Xu, Yingchen Yu, Abdulmotaleb El Saddik, Christian Theobalt, Eric Xing, and Shijian Lu. Weakly supervised 3d openvocabulary segmentation. Advances in Neural Information Processing Systems, 36:5343353456, 2023. 2 [34] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 3 [35] OpenAI. Gpt-4v. https://openai.com/index/ gpt-4v-system-card/, 2023. 2 [36] OpenAI. Hello gpt-4o. https : / / openai . com / index/hello-gpt-4o/, 2024. 3, 4 [37] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan Barron, Sofien Bouaziz, Dan Goldman, Ricardo MartinBrualla, and Steven Seitz. Hypernerf: higherdimensional representation for topologically varying neural radiance fields. arXiv preprint arXiv:2106.13228, 2021. 6, 7, 2, 3, 4 In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2005120060, 2024. 2, 3, 6, 7 [39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 2, 3 [40] Zhijing Shao, Zhaolong Wang, Zhuang Li, Duotun Wang, Xiangru Lin, Yu Zhang, Mingming Fan, and Zeyu Wang. Splattingavatar: Realistic real-time human avatars with In Proceedings of the mesh-embedded gaussian splatting. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16061616, 2024. 3 [41] William Shen, Ge Yang, Alan Yu, Jansen Wong, Leslie Pack Kaelbling, and Phillip Isola. Distilled feature fields enable In Conference on few-shot language-guided manipulation. Robot Learning, pages 405424. PMLR, 2023. [42] Jin-Chuan Shi, Miao Wang, Hao-Bin Duan, and ShaoHua Guan. Language embedded 3d gaussians for openIn Proceedings of the vocabulary scene understanding. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 53335343, 2024. 3 [43] Aleksandar Shtedritski, Christian Rupprecht, and Andrea Vedaldi. What does clip know about red circle? viIn Proceedings of the sual prompt engineering for vlms. IEEE/CVF International Conference on Computer Vision, pages 1198711997, 2023. 5 [44] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. 4 [45] Sanjay Subramanian, William Merrill, Trevor Darrell, Matt Gardner, Sameer Singh, and Anna Rohrbach. Reclip: strong zero-shot baseline for referring expression comprehension. arXiv preprint arXiv:2204.05991, 2022. 5 [46] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 2 [47] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi`ere, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024. 5 [48] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. 2, [49] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95689578, 2024. 2, 4 [38] Minghan Qin, Wanhua Li, Jiawei Zhou, Haoqian Wang, and Hanspeter Pfister. Langsplat: 3d language gaussian splatting. [50] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste [63] Mingqiao Ye, Martin Danelljan, Fisher Yu, and Lei Ke. Gaussian grouping: Segment and edit anything in 3d scenes. In European Conference on Computer Vision, pages 162 179. Springer, 2025. 3, 6, 7, 2 [64] Justin Yu, Kush Hari, Kishore Srinivas, Karim El-Refai, Adam Rashid, Chung Min Kim, Justin Kerr, Richard Cheng, Muhammad Zubair Irshad, Ashwin Balakrishna, et al. Language-embedded gaussian splats (legs): Incrementally building room-scale representations with mobile robot. arXiv preprint arXiv:2409.18108, 2024. 3 [65] Zhu Zhang, Zhou Zhao, Yang Zhao, Qi Wang, Huasheng Liu, and Lianli Gao. Where does it exist: Spatio-temporal video grounding for multi-form sentences. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1066810677, 2020. [66] Hongyu Zhou, Jiahao Shao, Lu Xu, Dongfeng Bai, Weichao Qiu, Bingbing Liu, Yue Wang, Andreas Geiger, and Yiyi Liao. Hugs: Holistic urban 3d scene understanding via gaussian splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21336 21345, 2024. 3 [67] Shijie Zhou, Haoran Chang, Sicheng Jiang, Zhiwen Fan, Zehao Zhu, Dejia Xu, Pradyumna Chari, Suya You, Zhangyang Wang, and Achuta Kadambi. Feature 3dgs: Supercharging 3d gaussian splatting to enable distilled feature fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2167621685, 2024. 3, 6, 7 [68] Xiaoyu Zhou, Zhiwei Lin, Xiaojun Shan, Yongtao Wang, Deqing Sun, and Ming-Hsuan Yang. Drivinggaussian: Composite gaussian splatting for surrounding dynamic autonomous driving scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2163421643, 2024. 3 Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 3, 5 [51] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 3 [52] Vadim Tschernezki, Iro Laina, Diane Larlus, and Andrea Vedaldi. Neural feature fusion fields: 3d distillation of In 2022 Interself-supervised 2d image representations. national Conference on 3D Vision (3DV), pages 443453. IEEE, 2022. [53] Joanna Waczynska, Piotr Borycki, Sławomir Tadeja, Jacek Tabor, and Przemysław Spurek. Games: Mesh-based adapting and modification of gaussian splatting. arXiv preprint arXiv:2402.01459, 2024. 3 [54] Junjie Wang, Jiemin Fang, Xiaopeng Zhang, Lingxi Xie, and Qi Tian. Gaussianeditor: Editing 3d gaussians delicately with text instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2090220911, 2024. 3 [55] Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Improving text emarXiv preprint Rangan Majumder, and Furu Wei. beddings with large language models. arXiv:2401.00368, 2023. 5, 6 [56] Mengmeng Wang, Jiazheng Xing, and Yong Liu. Actionclip: new paradigm for video action recognition. arXiv preprint arXiv:2109.08472, 2021. 4 [57] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 3, [58] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, et al. Internvid: large-scale video-text dataset for multimodal understanding and generation. arXiv preprint arXiv:2307.06942, 2023. 2, 4 [59] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2031020320, 2024. 2, 3, 4 [60] Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, and Christoph Feichtenhofer. Videoclip: Contrastive pre-training arXiv preprint for zero-shot video-text understanding. arXiv:2109.14084, 2021. 2, 4 [61] Lingfeng Yang, Yueze Wang, Xiang Li, Xinlong Wang, and Jian Yang. Fine-grained visual prompting. Advances in Neural Information Processing Systems, 36, 2024. 5 [62] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. Deformable 3d gaussians for highIn Profidelity monocular dynamic scene reconstruction. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2033120341, 2024. 2, 3 4D LangSplat: 4D Language Gaussian Splatting via Multimodal Large Language Models"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Datasets Since there are no publicly available ground truth segmentation mask labels for the HyperNeRF [37] and Neu3D [27] datasets, nor annotations tailored for time-sensitive querying, we adopt the annotation pipeline outlined in Segment Any 4D Gaussians [17] and manually annotate the mask labels ourselves. Specifically, we leverage the Roboflow platform alongside the SAM (Segment Anything Model) framework for interactive annotation. For the HyperNeRF dataset, where data is captured with monocular camera, we select one frame every four frames as the training set. From the remaining data, we annotate subset as the test set to ensure no overlap between the two sets. For the Neu3D dataset with 21 camera views, one is reserved for testing, and the remaining 20 are used for training, aligning with the 4D-GS [59] setting. To evaluate on the Neu3D dataset, we annotate every 20 frames from the test views. B. Implementation Details Multimodal Object-Wise Video Prompting. For Multimodal Object-Wise Video Prompting, we utilize the largest SAM-defined semantic levels as mask inputs for the Multimodal Large Language Models (MLLMs). The prompting process is outlined in Table 6, which provides the specific prompts used for MLLM prompting. For visual prompting, we employ red contour line with radius of 2 to delineate object boundaries. Additionally, we apply Gaussian blur with radius of 10 and convert the images to grayscale mode to achieve gray-level augmentation. These techniques enhance the effectiveness of the visual input during the prompting process. Autoencoder. Following LangSplat [38], we employ two autoencoders to compress the high-dimensional CLIP feature (512-dimension) and LLM feature (4096-dimension) separately. Specifically, two MLPs are used to compress 512-dimensional CLIP features and 4096-dimensional video features to 3 and 6 dimensions, respectively. The autoencoders are optimized with L2 loss. To enhance stability, cosine similarity loss is also included as regularization. Training Details. Our training pipeline is structured into four stages, progressively refining the model for robust performance in dynamic 4D language field construction. 1) In the initial stage, we train static Gaussian field to reconstruct the RGB channel of static scenes. This provides foundation for modeling the visual appearance of the"
        },
        {
            "title": "Image prompts",
            "content": "highI lighted the objects want you to describe in red outline and blurred the objects dont that need you to describe. First please determine object the highlighted in red line in the video. Then briefly summarize the formation process this object. transof You have an understanding of the overall transformation process of the object: {video prompt}. Now, have provided you with images extracted from this process. Please describe the specific state of the object(s) in the given image, without referring to the entire video process. Avoid describing states that you cant infer directly from the picture. Avoid repeating descriptions in context. For example, if the context suggests the object is moving up and down but the image shows it is just moving down, explicitly only state that the object is in moving down state. If the context suggests the object is breaking but the image shows it is complete right now, explicitly only state that the object appears to be complete. If context tells you something changes from green to blue, but its blue in this image, just state that the object is blue. Table 6. Details of Text prompts Method Gaussian Grouping [63] Ours-agnostic Ours-sensitive FPS 1.47 5.24 4.05 Table 7. Query Performance Comparison. scene. 2) Next, we incorporate semantic information into the static Gaussian field without introducing deformable networks. Semantic features are embedded into the scene by minimizing an L1 loss, ensuring accurate representations of the static scenes semantics. 3) In the third stage, we extend the model to dynamic RGB scenes by introducing non-semantic deformation fields. Leveraging the approach of 4D-GS [59], we employ deformable networks to learn temporal and motion-based deformations that capture spatial and temporal dynamics for RGB scenes. 4) For timeagnostic semantic rendering, we refine the semantic features"
        },
        {
            "title": "Method",
            "content": "Feature-3DGS [67] Gaussian Grouping [63] LangSplat [38]"
        },
        {
            "title": "Method",
            "content": "americano chickchicken split-cookie mIoU(%) mAcc(%) mIoU(%) mAcc(%) mIoU(%) mAcc(%) 34.65 61.77 72.08 83.48 62.96 71.31 97. 98.77 47.21 34.65 75.98 86.50 87.22 75.52 97.86 98.81 47.03 72.71 76. 90.04 68.25 96.56 97.32 98.67 espresso keyboard torchocolate mIoU(%) mAcc(%) mIoU(%) mAcc(%) mIoU(%) mAcc(%) Feature-3DGS [67] Gaussian Grouping [63] LangSplat [38]"
        },
        {
            "title": "Ours",
            "content": "24.04 32.45 82.93 83.52 80.13 82.46 98.66 97.95 42.14 42.44 72.42 79. 80.98 74.15 96.75 95.71 24.71 58.95 69.55 71.79 64.58 85.52 98.09 98. Table 8. Comparison of mean IoU and mean Accuracy for various methods on the HyperNeRF [37] datasets. Method Feature-3DGS [67] Gaussian Grouping [63] LangSplat [38] Ours Method coffee martini cook spinach cut roasted beef mIoU(%) mAcc(%) mIoU(%) mAcc(%) mIoU(%) mAcc(%) 30.23 71.37 67.97 85.16 84.74 97.34 98.47 99. 41.50 46.45 78.29 85.09 95.59 93.79 98.60 99.38 31.66 54.70 36.53 85. 91.07 93.25 97.04 99.28 flame salmon flame steak sear steak mIoU(%) mAcc(%) mIoU(%) mAcc(%) mIoU(%) mAcc(%) Feature-3DGS [67] Gaussian Grouping [63] LangSplat [38] Ours 54.33 35.72 66.01 89.88 77.13 94.69 82.16 94.35 27.27 36.92 64. 88.44 88.23 95.96 97.77 98.27 24.78 54.44 78.29 76.78 85.94 95.27 98. 99.38 Table 9. Comparison of mean IoU and mean Accuracy for various methods on the Neu3D [27] dataset. from the second stage while keeping the deformable network parameters fixed. For time-sensitive semantic rendering, we jointly train the status deformable network and the state prototype features to refine and model dynamic semantics effectively. For all datasets, the iterations for four stages are 3000, 1000, 10000, and 10000. The learning rates for the deformable network and the state prototype features are set to 1.6 104 and 2.5 103, respectively. Other training parameters remain consistent with those used in 4D-GS. C. More Quantitative Results In Table 8 and Table 9, we present detailed evaluation of time-agnostic querying performance on the HyperNeRF and Neu3D datasets, respectively. Our method achieves mean IoU exceeding 85% across all scenarios, outperforming the baseline methods in most scenes for both mean IoU and mean accuracy. These results underscore the robustness of our approach, demonstrating its ability to deliver superior segmentation accuracy and reliability compared to existing methods, even in dynamic scenes. Table 7 further compares the runtime efficiency of our method with the baseline on the HyperNeRF dataset. The comparison encompasses the total time required for rendering semantic features and conducting open-vocabulary queries. Our method demonstrates significant advantages over the Gaussian Grouping approach, achieving faster runtime for both time-agnostic and time-sensitive queries. These findings validate our method as an efficient and scalable solution for handling open-vocabulary queries in dynamic 4D scenes. D. More Visualization Results Figure 5 illustrates visualization results for time-agnostic querying. As depicted, our method demonstrates superior accuracy in capturing objects that correspond to semantic descriptions, compared to other methods. Furthermore, Figure 5. Visualization of time-agnostic querying results on HyperNeRF [37] and Neu3D [27] datasets. it effectively tracks the spatial dynamics of these objects across different temporal steps, showcasing its effectiveness in handling dynamic scenarios. E. MLLM-based Embeddings Since our method utilizes MLLMs to generate captions, the feature representation capability of the obtained embeddings is inherently limited by the capacity of the MLLMs, which constitutes limitation of our approach. To verify that our MLLM-based embeddings indeed encode spatialtemporal information, we directly apply the MLLM-based embeddings, without any fine-tuning, to video classification and spatial-temporal action localization tasks using 2D videos. As shown in Tables 10 and 11, our results demonstrate that, even in zero-shot setting, the MLLMbased embeddings achieve competitive performance compared to state-of-the-art (SOTA) methods specifically designed for these tasks. This indicates that MLLM-based emMethod HMDB51 [23] UCF101 [44] Kinetics400 [18] MLLM IMP [1] 55.14 77.0 58.34 59.1 78.97 91.5 Table 10. Accuracy Results (%) on the Video Classification task. Method MLLM HIT [12] VmAP@0.1 VmAP@0.2 VmAP@0.5 75.78 88.8 64.38 74.3 78.13 86.1 Table 11. Spatial-Temporal Action Localization Results (%) on UCF101 [44]. beddings inherently capture some spatial-temporal information. However, we also acknowledge that the performance of our approach is ultimately constrained by the representational capacity of the MLLMs."
        }
    ],
    "affiliations": [
        "Brown University",
        "ETH Zurich",
        "Harvard University",
        "Stony Brook University",
        "Tsinghua University"
    ]
}