{
    "paper_title": "HeurAgenix: Leveraging LLMs for Solving Complex Combinatorial Optimization Challenges",
    "authors": [
        "Xianliang Yang",
        "Ling Zhang",
        "Haolong Qian",
        "Lei Song",
        "Jiang Bian"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Heuristic algorithms play a vital role in solving combinatorial optimization (CO) problems, yet traditional designs depend heavily on manual expertise and struggle to generalize across diverse instances. We introduce \\textbf{HeurAgenix}, a two-stage hyper-heuristic framework powered by large language models (LLMs) that first evolves heuristics and then selects among them automatically. In the heuristic evolution phase, HeurAgenix leverages an LLM to compare seed heuristic solutions with higher-quality solutions and extract reusable evolution strategies. During problem solving, it dynamically picks the most promising heuristic for each problem state, guided by the LLM's perception ability. For flexibility, this selector can be either a state-of-the-art LLM or a fine-tuned lightweight model with lower inference cost. To mitigate the scarcity of reliable supervision caused by CO complexity, we fine-tune the lightweight heuristic selector with a dual-reward mechanism that jointly exploits singals from selection preferences and state perception, enabling robust selection under noisy annotations. Extensive experiments on canonical benchmarks show that HeurAgenix not only outperforms existing LLM-based hyper-heuristics but also matches or exceeds specialized solvers. Code is available at https://github.com/microsoft/HeurAgenix."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 2 6 9 1 5 1 . 6 0 5 2 : r HeurAgenix: Leveraging LLMs for Solving Complex Combinatorial Optimization Challenges Xianliang Yang1 Ling Zhang1 Haolong Qian1,2 Lei Song1 Jiang Bian1 1Microsoft Research Asia, Beijing, China 2Tsinghua University, Beijing, China {Xianliang.Yang, Ling.Zhang, v-haolqian, Lei.Song, Jiang.Bian} @microsoft.com"
        },
        {
            "title": "Abstract",
            "content": "Heuristic algorithms play vital role in solving combinatorial optimization (CO) problems, yet traditional designs depend heavily on manual expertise and struggle to generalize across diverse instances. We introduce HeurAgenix, two-stage hyper-heuristic framework powered by large language models (LLMs) that first evolves heuristics and then selects among them automatically. In the heuristic evolution phase, HeurAgenix leverages an LLM to compare seed heuristic solutions with higher-quality solutions and extract reusable evolution strategies. During problem solving, it dynamically picks the most promising heuristic for each problem state, guided by the LLMs perception ability. For flexibility, this selector can be either state-of-the-art LLM or fine-tuned lightweight model with lower inference cost. To mitigate the scarcity of reliable supervision caused by CO complexity, we fine-tune the lightweight heuristic selector with dual-reward mechanism that jointly exploits singals from selection preferences and state perception, enabling robust selection under noisy annotations. Extensive experiments on canonical benchmarks show that HeurAgenix not only outperforms existing LLM-based hyper-heuristics but also matches or exceeds specialized solvers. Code is available at https://github.com/microsoft/HeurAgenix."
        },
        {
            "title": "Introduction",
            "content": "Combinatorial optimization (CO) problems are fundamental in operations research and critical for decision-making across various industries [14, 40]. These problems often involve large-scale search spaces, where dimensionality grows exponentially, making traditional solution methods computationally intractable [42, 2]. This complexity has led to the widespread use of heuristics, which provide approximate solutions within feasible time frame [44, 48]. Heuristics are typically designed to be interpretable, with clear rule-based decision-making processes that enhance transparency and human understanding. Despite their effectiveness, heuristics heavily rely on manual expertise and are challenging to adapt to changing conditions. In response, hyper-heuristics have emerged, aiming to automate heuristic design by crafting rules for their selection and combination [8, 9]. Although hyper-heuristics offer interpretability, they often require manual rule design, which limits adaptability to evolving problem states [43, 5]. Recent advancements in large language models (LLMs) have inspired work on automatic heuristic design, using LLMs to generate and refine heuristics through few-shot prompting and code synthesis [47, 36, 63, 41]. Although these approaches attain strong performance on moderate-sized classic CO problems, most of them embed the LLM-produced heuristic inside task-specific solver, This is the notice string that will appear at the bottom of the first page. Figure 1: Overview of the HeurAgenix framework for automatic heuristic design and adaptive selection. In the heuristic evolution phase, an LLM autonomously discovers evolution strategies by analyzing contrastive solution tuples, while in the problem solving phase, an adaptive heuristic selection mechanism integrates Test-time Scaling (TTS) [59, 58]. which resulting reliance on hand-crafted domain knowledge constrains heuristic flexibility and the generalization. To address these limitations, we introduce HeurAgenix, unified framework for automatic heuristic evolution and adaptive selection. To the best of our knowledge, HeurAgenix is the first LLM-based hyper-heuristic framework that simultaneously (i) evolves diverse pool of heuristics without relying on any external solver and (ii) incorporates an online heuristic selector for adaptive problem solving. Depending on user requirements, the second phase can leverage either an frontier LLM (such as GPT [1], DeepSeek [18]) or fine-tuned lightweight model for efficient inference. diagram illustrating the proposed framework is provided in Figure 1. diagram illustrating the proposed framework is provided in Figure 1. Our main contributions are: We introduce HeurAgenix, versatile framework for automatic heuristic evolution and selection. It offers scalable and generalizable solutions for complex CO problems, outperforming existing hyper-heuristic approaches. We propose contrastive, data-driven heuristic evolution phase, analyzing solution trajectories to discover evolution strategies without predefined rules. We develop an adaptive heuristic selection mechanism that integrates LLM and TTS, which selects heuristics based on the current problem state and improve efficiency and solution quality. 2 To train the heuristic selector more robustly under noisy data, we introduce dual-reward mechanism that combines context perception for accurate state recognition with outcome preference that amplifies the positive/negative margin."
        },
        {
            "title": "2 Preliminary and Related Work",
            "content": "In this section, we present the important definitions and notations employed throughout this paper."
        },
        {
            "title": "2.1 Heuristics for CO Problems",
            "content": "Problem state In CO problems, directly characterizing problem instance and its solution can be challenging, as they are often represented in complex numerical data structures. To address this, following the CO community, we adopt the concept of problem state as an abstract representation capturing the high-level features of both the problem instance and its current (partial) solution [10]. For example, in the Traveling Salesman Problem (TSP), static problem states may include features such as the number of nodes, average inter-node distance, and graph symmetry, while dynamic problem states can describe aspects such as visited nodes and current tour cost. In this paper, we define heuristic algorithm as function that maps problem state Heuristic to an operation O. Formally, heuristic can be described as : O, where represents the space of problem states and is the set of allowable operations. If the heuristic is constructive heuristic, the operation may involve adding elements to extend partial solution. Conversely, if the heuristic is an improvement heuristic, might involve exchanging, replacing, or perturbing existing elements to refine the solution [24]. Detailed designs for heuristics, problem states and operations are provided in the Appendix E. Transition function. For problem instance we first define the single-step transition function : Z, which deterministically maps the current state and an applied operation to the next state. For later convenience we extend the definition of to accept heuristic as its second argument: (z, H) := T(cid:0)z, H(z)(cid:1), that is, query the heuristic for its chosen operation at and then perform the original state update. Repeatedly executing the same heuristic for exactly steps is denoted (z, H) = (cid:0) (T (z, H), H), H(cid:1) , (cid:125) (cid:123)(cid:122) successive applications of (cid:124) shorthand we will use extensively in Section 3.2 when defining the selectors optimization target. Solution trajectory. Given sequence of heuristics (H0, . . . , Hn1), the resulting trajectory is z0 = Init(d), O0 = H0(z0), z1 = (z0, O0), . . . , On1 = Hn1(zn1), zn = (zn1, On1), where zi is the state before step and Oi the selected operation. We write the trajectory as = {(zi, Oi)}n1 i=0 and denote its objective cost by C(S). Heuristic selector. Given the problem state Z, the remaining decisions {0, . . . , }, and the heuristic pool H, we define heuristic selector as mapping which selects the heuristic π(z, t) at problem state when decisions remain. π : {0, . . . , } H,"
        },
        {
            "title": "2.2 Hyper-Heuristics",
            "content": "In the CO community, hyper-heuristics have been introduced to manipulate heuristics. Two main categories are generation hyper-heuristics and selection hyper-heuristics [10]. 3 Generation hyper-heuristics involve the automatic creation of heuristics by systematically combining elementary operations or decision-making rules. These techniques often employ methods such as genetic programming, genetic algorithms, and particle swarm optimization [28, 51]. Although these methods can yield high-performing algorithms, they often encounter challenges related to computational overhead and adaptability [61, 31]. Selection hyper-heuristics dynamically choose the most appropriate heuristic from predefined set by evaluating the current problem state. These methods incorporate rule-based, meta-heuristic, or learning-based strategies, rendering them effective for complex optimization tasks. Nevertheless, they may struggle with intricate selection mechanisms and generalization issues [20, 16, 17, 52]."
        },
        {
            "title": "Paradigm",
            "content": "Table 1: Comparison of LLM-based CO paradigms. Problem Solving Heuristic Evolution Solver Required Yes FunSearch Yes EoH Yes ReEvo AlphaEvolve No HeurAgenix (Ours) Contrastive, data-driven evolution Adaptive selection No LLM-driven 5 manually designed strategies Feedback-based refinement Ensemble LLM-driven evolution"
        },
        {
            "title": "Fixed heuristic\nFixed heuristic\nFixed heuristic\nFixed heuristic",
            "content": "A solver refers to either: traditional optimization solver (e.g., GLS [57]), neural network-based solver (e.g., PoMo [34]), or specialized hyper-heuristic algorithm (e.g., ACO [19]). LLMs have demonstrated significant potential in addressing combinatorial optimization (CO) problems. For instance, Zhang et al. [64] assessed the performance of LLMs on various graph optimization challenges, while Iklassov et al. [30] developed effective prompting strategies to enable LLMs to adapt to diverse problem formulations. Xiao et al. [62] introduced the Chain-of-Experts approach, integrating multi-agent cooperation to directly address optimization tasks. These studies underscore LLMs flexibility and capability in reasoning within CO problems. Particularly relevant to our work are studies focusing on LLMs for heuristic generation and evolution in CO problem solving. FunSearch [47] employs LLMs to iteratively generate and refine candidate solutions, aiming to improve solution quality. EoH [36] facilitates multi-directional evolution to boost heuristic diversity, while ReEvo [63] leverages LLM-driven reflection for targeted optimization. Collectively, these efforts highlight LLMs potential in automating heuristic design and enhancing optimization processes. Most recently, AlphaEvolve [41] pushes the paradigm further by pairing an ensemble of Gemini [54] LLMs with automated evaluators in an evolutionary loop, enabling the discovery of entire algorithmic codebases that optimise practical systems. However, as illustrated in Table 1, methods such as FunSearch, EoH, and ReEvo couple the LLMgenerated heuristic with task-specific solver, which limits generalization and demands domain expertise. AlphaEvolve removes the external solver by evolving complete programs, yet its offline evolutionary loop is computationally intensive and does not provide instance-level adaptation at inference time. In contrast, HeurAgenix supports flexible real-time heuristic selection by leveraging either an LLM or fine-tuned lightweight model. This enables autonomous data-driven evolution and on-the-fly adaptation among multiple heuristics without dependency on external solvers, thus forming truly end-to-end optimization paradigm."
        },
        {
            "title": "2.4 Test-time Scaling",
            "content": "Test-time scaling (TTS) is the practice of intentionally spending extra computation at inference time to improve answer quality [26, 23, 55]. Early research demonstrated computeaccuracy trade-off in progressive or interruptible inference; recent LLM work has turned TTS into standard tool for eliciting stronger reasoning, for example by sampling multiple candidate outputs and selecting the best one with lightweight verifier [59, 58]. Representative instantiations include Best-of-N sampling [6] and Diverse-Verifier Tree Search (DVTS) [12]. Our framework adopts the same searchverifier philosophy in problem solving phase to trade modest increase in inference compute for substantial gains in solution quality and robustness."
        },
        {
            "title": "2.5 Challenges in Noisy Data for Long-Term Decision-Making",
            "content": "Training models for effective long-term decision making is fundamentally challenged by the difficulty of accurately assessing the true long-term value of each action (i.e., selecting particular heuristic at given step). This difficulty stems from the credit-assignment problem: the ultimate impact of single selection may only emerge after many steps or even after the whole trajectory, which complicates isolating its precise contribution, especially within large stateaction spaces and over extended horizons. Exhaustively evaluating the long-term utility of early selections would require enumerating every possible continuation, which is computationally infeasible [60]. As result, any practical evaluation procedure can provide only approximate, noisy scores based on small subset of rollouts. Training models on such imperfect feedback therefore demands robust learning techniques. HeurAgenix meets this need via dual-reward mechanism that fuses final-solution preferences with intermediate-reasoning signals, yielding more reliable supervision for heuristic selection under noisy annotations."
        },
        {
            "title": "3 Methodology",
            "content": "HeurAgenix is designed to leverage the complementary strengths of generation and selection hyperheuristics, offering robust framework for combinatorial optimization. As illustrated in Figure 1, the generative component utilizes the reasoning abilities of LLMs to autonomously refine and evolve heuristic algorithms across diverse problem instances. Meanwhile, the selection component enhances adaptability by dynamically choosing the most suitable heuristic based on the current problem state, employing fine-tuned lightweight model. By unifying these methods, HeurAgenix achieves high performance and adaptability, effectively addressing the challenges of large-scale CO problems."
        },
        {
            "title": "3.1 Heuristic Evolution",
            "content": "Heuristic evolution is systematic, data-driven procedure that iteratively improves seed heuristic Hseed. We leverage an LLM both to diagnose structural weaknesses and to propose concrete evolution strategies that amend them. Let Devo and Dval denote the evolution and validation instance sets, respectively. Building on the definitions in Section 2.1, single evolution round proceeds as follows: Step 1: Basic solution generation. For every Devo we run Hseed to obtain basic solution = {(zi, Oi)}n1 i=0 and its cost C(S). If Hseed is constructive heuristic, it is applied repeatedly from the initial state until feasible complete solution is constructed. If Hseed is an improvement heuristic, we first sample constructive heuristic Hc to obtain an initial feasible solution, then apply Hseed iteratively as local search or improvement operator. Step 2: Contrastive solution generation. To reveal potential weaknesses, we perform up to perturbation trials. In each trial we (i) randomly select subset {0, . . . , 1} of operation indices in the basic solution, (ii) replace every Ok (k K) by an alternative O(zk) {Ok}, (iii) roll out the modified trajectory to obtain perturbed solution and its cost C(S). Whenever C(S) < C(S) (for minimization problems), is stored as contrastive solution; its mutation list = {(zk, Ok, Step 3: Critical operation identification. Not all modified operations in are responsible for the performance gap, so we first identify the decisive mutation. For each (zk, Ok, k) we independently replace Ok with k, keep all other operations still from Hseed, and if the solution are valid, measure the individual improvement k)}kK becomes candidate operation set for evolution. = C(S) C(cid:0)S(k)(cid:1), where S(k) is the solution obtained after changing only the k-th operation (positive denotes improvement for minimization task). We then select = arg maxkK and Ok is the critical operation. 5 Step 4: Extract evolution strategy. The tuple (zk , Ok , passed to the LLM: ) and the current heuristic Hseed are = LLMevolve (cid:0)Hseed, zk , Ok , k (cid:1). The LLM explains why k is better and outputs an evolution strategy E. This strategy serves as conceptual guide to improve Hseed, potentially involving modifications such as parameter adjustment, the addition of control logic, or component replacement. (cid:80) Step 5: Iterative refinement. Starting from H0 = Hseed, we refine the heuristic for at most Tmax rounds. After each round we compute the performance pi = evaluate_performance(cid:0)Hi, Dval (cid:1) = 1 denotes the solution produced by Hi on instance d, and obtain Dval (cid:1). We stop early if no further improvement is observed. (cid:0)Hi, E, pi an update Hi+1 = LLMrefine Algorithm 1 summarizes the entire procedure. Figure 2 also visualizes how single critical operation mutation is extracted and justified. (cid:1), where Si C(cid:0)Si dDval Algorithm 1 One Round Heuristic Evolution Input: seed heuristic Hseed; evolution instance d; validation set Dval; LLM; maximum perturbation trials ; maximum refinement trials Imax Output: refined heuristic Hevolved 1: # Step 1: Basic Solution Generation 2: run_heuristic(Hseed, d) 3: = {(zi, Oi)}n1 i=0 + 1 sample_indices(n) perturb(S, K) if C(S) < C(S) then {(zk, Ok, 4: # Step 2: Contrastive Solution Generation 5: ; 0 6: while (p < ) (M = ) do 7: 8: 9: 10: 11: end if 12: 13: end while 14: if = then return Hseed 15: 16: end if k)}kK k) do S(k) perturb_single(S, k, k C(S) C(S(k)) 17: # Step 3: Critical Operation Identification 18: for all (zk, Ok, 19: 20: 21: end for 22: arg maxk 23: (zk , Ok , k ) tuple with index k) 24: # Step 4: Extract Evolution Strategy 25: LLMevolve(Hseed, zk , Ok , k ) improved true pi evaluate_performance(Hi, Dval) Hi+1 LLMrefine(Hi, E, pi) pi+1 evaluate_performance(Hi+1, Dval) if pi+1 < pi then + 1 26: # Step 5: Iterative Heuristic Refinement 27: 0; H0 Hseed; 28: while improved < Imax do 29: 30: 31: 32: 33: 34: 35: end if 36: 37: end while 38: Hevolved Hi 39: return Hevolved improved false else random subset of operation indices replace each Ok with for minimization problem no better contrastive solution found for minimization problem 6 Figure 2: Illustration of one heuristic-evolution step on four-node TSP evolution instance. The cumulative effect of this and subsequent refinements can be seen in Figure 3. Figure 3: Example of heuristic evolution for TSP. The left panel illustrates successive strategy refinements and their impact on TSPLIB [46] performance. The right panel details specific evolution step, where an alternative cost function is induced by the LLM based on counterfactual analysis. For step-by-step extraction of the refinement highlighted in Round 2, see Figure 2 and for further details and code, see Appendix C. Algorithmic Summary and Discussion. Each round of evolution enhances the heuristic in datadriven manner. By repeating the process over diverse evolution instances and aggregating the learned strategies, the heuristic pool grows in diversity and effectiveness. Figure 3 gives an overview of multiple rounds of evolution and their corresponding performance changes. For further details on an evolution example and hyper-parameter choices, refer to Appendix and Appendix E."
        },
        {
            "title": "3.2 Problem Solving",
            "content": "Motivation. Even after evolution, single heuristic rarely dominates across all problem states; performance varies sharply with the current state. Static, hand-crafted switching rules therefore lack adaptability [11, 27, 33]. We therefore require real-time selector that decides which heuristic should be applied next for each encountered state (defined as Section 2.1). Heuristic selection objective. Following the notation of Section 2.1, for minimization problem we jointly define the state-value function, the action value of heuristic, and the corresponding 7 heuristic selector: (z, t) = C(Sz), min HH V(cid:0)T (z, H), 1(cid:1), = 0, (cid:6) (cid:7) > 0, Q(z, H, t) = V(cid:0)T (z, H), 1(cid:1), π(z, t) = arg min HH Q(z, H, t), where is the maximum number of heuristic calls, is the fixed number of consecutive steps per chosen heuristic, represents remaining decisions (where = 0 means termination), Sz is the current solution corresponding to state z, and denotes the evolved heuristic pool. (1) From these, (z, t) represents the expected cost from state with decisions remaining, Q(z, H, t) evaluates heuristics performance in that state, and π(z, t) represents the optimal selector that chooses heuristics with minimum Q. The objective of heuristic selector optimization is to determine the optimal selector. For simplicity, we will omit the parameter if it is clear from the context. In practice, obtaining Q(z, H) exactly is infeasible. Monte-Carlo Heuristic selection procedure. simulation is common surrogate, but exhaustively simulating every heuristic in is prohibitively slow on large instances, whereas an LLMrelying on coarse semantic cues alonetypically misses the best choice. We therefore adopt hybrid strategy: the LLM first prunes the pool to compact candidate set, after which lightweight test-time search (TTS) evaluates those candidates and selects the one to execute: Step 1: Filter candidate heuristics. The LLM filters the heuristic pool to produce candidate subset = LLMfilter(z, H). Step 2: Evaluate value of heuristics. For every under problem state z, we estimate its value by Monte-Carlo search [53]: ˆQH = mc_evaluate(z, H). Specifically, we first apply it sequentially times starting from state z, resulting in new intermediate state. From the intermediate state, we then generate candidate solutions by repeatedly selecting heuristics randomly from until completion. The estimated value ˆQH is the average terminal cost over the rolloutsaveraging provides an unbiased estimate of the expected outcome, while taking minimum would introduce downward bias and favor lucky samples. Implementation details appear in Appendix D. Step 3: Select the most suitable heuristic. Select ˆH = arg minHH ˆQH and execute it times. This structured approach couples rapid LLM-based reasoning with lightweight search verifier, yielding balanced and efficient solver that leverages both model guidance and TTS. Problem solving process. Based on this heuristic selector, we can dynamic select the heuristic and solve problem easily. For test instance we proceed iteratively. Starting from the initial state z, the selector chooses heuristic ˆH, and then execute ˆH is executed times, to update the problem . The cycle repeats until none of the available heuristics can further improve the solution. The hyper-parameter balances decision frequency against runtime overhead and is set to = 5 throughout all experiments."
        },
        {
            "title": "3.3 Selection Model Fine-tuning",
            "content": "Motivation Selecting the optimal heuristic in real-time plays crucial role in combinatorial optimization performance. To enhance efficiency, we propose the implementation of lightweight selection models within HeurAgenix to effectively manage latency, computational demands, and resource utilization. This approach is particularly beneficial as it allows for multiple heuristic evaluations and selections without incurring prohibitive inference costs associated with larger models. 8 Lightweight models face inherent limitations in processing complex problem states and applying various heuristics effectively, particularly in their ability to adaptively select appropriate heuristics for different scenarios. To address these challenges, we propose fine-tuning approach that enhances the models comprehension of both the problem domain and available heuristics. Our methodology begins with an offline data collection process, followed by detailed analysis of our implementation strategy for extracting insights from non-optimal and noisy datasets. Offline Data Collection We construct an offline dataset consisting of tuples (z, H, QH ). Here is the current problem state, is the candidate heuristic drawn from the pool H, and QH is the rollout value assigned to at state z. To obtain QH , every heuristic in is evaluated from the initial state by the Monte Carlo pipeline of selection, expansion, simulation, and back-propagation (see Appendix D), yielding scalar score for that (z, H) pair. To obtain data that is both informative and robust, we collect two types of trajectories. (i) Greedy trajectories update the state with the heuristic that attains the highest QH at each step, exposing the selector to near-optimal decisions. (ii) Stochastic trajectories update the state with random heuristic, forcing the selector to recover from non-optimal contexts. The resulting mixture encourages the model to associate diverse states with effective heuristics. As noted in Section 2.5, the rollout values QH are inherently noisy because only small subset of future continuations can be explored. We therefore employ the dual-reward fine-tuning scheme to learn policies that remain reliable under such imperfect supervisory signals. Dual-Reward Design To mitigate the impact of noisy evaluation scores, we equip the selector with dual-reward mechanism that restructures supervision signals instead of propagating raw values. As shown in Figure 4, the mechanism combines Preference-based Outcome Reward (POR) and Context-Perception Reward (CPR); two lightweight auxiliary rewards enforce output format and language consistency similar as in [50]. Figure 4: Detailed reward design, showing the operational mechanisms of the novel POR and CPR as well as auxiliary Format Reward [50] and Language Rewards [18]. Preference-based Outcome Reward (POR) Following the analysis in Section 2.5, the rollout values produced by the MCTS evaluator are inevitably noisy: similar heuristics may receive widely different scores across runs, while heuristics with genuinely different performance can end up with almost identical scores. Such distortions mislead the learner and weaken the training signal. Figure 5 shows that, for our selection tasks, choosing any heuristic from small positive set is nearly as good as always picking the single best one, whereas selecting from the complementary negative set quickly harms solution quality. In practice, mild noise is sufficient to interchange the ranks of neighbouring heuristics, yet it rarely pushes truly positive heuristic into the negative set (or vice-versa) because the gap between the two sets is usually large. These observations motivate reward design that compresses score differences within the positive or negative group while enlarging the margin between the two groups. Let the rollout scores {QH }HH be sorted in descending order for the current state z. Suppose the model proposes heuristic ˆH Figure 5: Effect of noisy rollout data on heuristic selection (rd100 in TSPLIB [46]). Y-axis: expected optimality gap (lower is better) after completing the tour by random sampling. X-axis: decision rounds. Blue: always selecting the best heuristic (oracle). Green: uniformly selecting from the top 30% heuristics (positive set). Red: random selection. Selecting from the positive set almost matches the oracle and clearly outperforms random choice. ranked at position ℓ. Two thresholds npos < nneg = divide the list into positive region, negative region, and an error region. The preference-based outcome reward is then defined as ℓ = rank(cid:0)z, ˆH, {QH }HH (2a) (cid:1), RPOR(z, ˆH) = (cid:18) Rp Rn (cid:19) , ℓ 1 1 npos (cid:18) ℓ npos nneg npos 1 ℓ npos, (cid:19) , npos < ℓ nneg, (2b) RL, nneg < ℓ n. Linear interpolation reduces gaps inside each region, whereas the coefficients Rp and Rn amplify the discontinuity at the region boundary; heuristics in the error region receive fixed penalty RL. Consequently, POR is insensitive to small permutations inside the positive set but still yields strong gradient whenever heuristic crosses the positive/negative border. The robustness of POR is demonstrated with four-heuristic toy example  (Table 2)  . Heuristics are divided into positive set (top two ranks) and negative set (bottom two). We compare POR (Rp = Rn = 1) with the Normalized Rank Reward (NRR), common RLHF baseline that linearly maps ranks to [1, 1] [65, 13]. Under noise free scores, both rewards differentiate the two sets. When two middle scores are noised, NRR almost eliminates the gap between H2 and H3 (0.2 vs 0.2), while POR still keeps clear margin (0.5 vs 0.5). Hence POR supplies much steadier learning signal in the presence of measurement noise. Table 2: Toy example comparing POR with Normalized Rank Reward (NRR). NRR maps ranks to [1, 1]. Noisy columns show the effect after perturbing the two middle scores. Heuristic Actual score Actual NRR Actual POR Noisy score Noisy NRR Noisy POR H1 H2 H3 1.0 0.8 0.2 0.0 1.0 0.6 -0.6 -1.0 1.0 0.5 -0.5 -1.0 1.0 0.6 0.4 0.0 1.0 0.2 -0.2 -1.0 1.0 0.5 -0.5 -1. Context-Perception Reward (CPR) CPR explicitly rewards the selector for understanding the environment, rather than for matching potentially noisy scalar outcome. Recent evidence shows 10 that even when the final reward signal is noisy or even random, models can still improve provided that their reasoning traces remain correct and receive feedback [49]. Moreover, empirical studies demonstrate that an accurate internal perception of the task state is often stronger predictor of downstream performance than the magnitude of the terminal reward itself [32]. Motivated by these findings, CPR encourages the model to align its latent representation with the ground-truth state. Formally, let = (z1, . . . , zm) be the true feature vector of the current problem state and ˆz = (ˆz1, . . . , ˆzm) the selectors own prediction of these features, we get: RCPR(z, ˆz) = (cid:104) (cid:88) i=1 I(cid:0)ˆzi = zi (cid:1) R+ I(cid:0)ˆzi = zi (cid:1) i (cid:105) , (3) where I() is the indicator function and R+ > 0 (R > 0) denotes the positive (negative) reward tied to the i-th feature. Correctly perceived dimensions therefore yield positive feedback, while misperceptions are penalized, pushing the selector toward faithful contextual understanding before it commits to heuristic decision. Figure 6 visualizes how CPR rectifies qualitative judgment errors and guides the model toward more accurate state assessments. Figure 6: Impact of the CPR mechanism in rectifying qualitative judgment errors during LLM inference. The illustration shows how CPR guides the model toward accurate contextual assessments, mitigating errors that might otherwise remain uncorrected. Training Procedure Under the dual-reward design, the selection models parameters θ are finetuned offline using the Group Relative Policy Optimization (GRPO) [50] method. GRPOs key approach involves leveraging inter-response preference relations to iteratively update the policy parameters θ. This framework aims to optimize the likelihood that the model generates response sequences leading to higher cumulative evaluation scores Rtotal. The goal of this training process is to enable the selection model to proficiently generate heuristic sequences that yield superior performance scores, thus improving its effectiveness in real-world applications of combinatorial optimization. The Algorithm 2 outlines the training process."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we conduct comprehensive evaluation of HeurAgenix under diverse settings to assess its performance, robustness, and adaptability. We explore heuristic evolution outcomes in Section 4.1, analyze problem solving efficiency using LLMs in Section 4.2, and investigate the capabilities of fine-tuned models in Section 4.3. To ensure fairness and consistency throughout our experiments, we adhere to the following setup and more detailed parameter settings can be found in Appendix E: 1. Foundation Model: In the heuristic evolution phase, GPT-4o (version: 2024-11-20) is employed as the foundation model for heuristic evolution and problem solving phase. To ensure fair comparisons, we consistently use this model across all LLM-based hyperheuristics, including EoH and ReEvo. In the problem solving phase using LLMs, GPT-4o continues as the primary selector, while Qwen-7B [3] serves as the base model for fine-tuning in the fine-tuned model experiments. 11 (cid:9)G k=1 Doffline Base selector policy Sample mini-batch {zk}Bsize for each state zk do Sample heuristic sequences (cid:8)H (g) for = 1 to do Algorithm 2 Fine-tuning Pipeline for Heuristic Selection Input offline dataset Doffline; initial policy πθ0 ; reward weights λPOR, λCPR (optional λbase); hyper-parameters (G, Mepochs, Bsize, ϵ, β, µ) 1: πθ πθ0 2: for epoch = 1 to Mepochs do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: end for Output fine-tuned parameters θ Compute RPOR(zk, (g) ) Compute RCPR(zk) Compute Rbase(zk, (g) ) R(g) λPORRPOR + λCPRRCPR + λbaseRbase end for Estimate group-relative advantages (cid:98)A(g) for inner step = 1 to µ do Refer equation (2) Refer equation (3) Following the GRPO methodology Update πθ by maximizing GRPO objective with clip ϵ and KL penalty β = R(g) πθ( zk) GRPO core R(g) for all g. end for end for (cid:80) g=1 k 2. API Calls: During heuristic evolution, all hyper-heuristics evolve until they reach 2,000 API calls, ensuring consistency across methods. In problem solving, our method utilizes + 2 API callswhere is max heuristic steps and related to problem size and is fixed at 5, with 2 additional calls for problem description and heurisitic pool introduction. 3. Execution Time: Each test instance is allocated maximum runtime of two hours to exploit the advantages of search-based methods. 4. Metric: Solution performance is quantified by the optimality-gap metric, gap = vvu vu 100%, where is the obtained solution value and vu is the known optimal (or best-known) value. To reduce variance, every experiment is run three times. 5. Platform: Experiments are conducted on GNU/Linux system with 5.15.0-121-generic kernel, Intel(R) Xeon(R) processor, NVIDIA RTX A6000 (48G) GPU, and CUDA 12.2. We evaluate our framework across the following combinatorial optimization problems: 1. Traveling Salesman Problem (TSP): Utilizes sub-datasets from TSPLIB [46] to identify the shortest route visiting each city once. 2. Capacitated Vehicle Routing Problem (CVRP): Employs the largest instances from the first six series of CVRPLIB [45] to optimize delivery routes with capacity constraints. 3. Multiple Knapsack Problem (MKP): Uses instances from mknapcb1 and mknapcb4 in the OR-Library [4] to maximize item value across multiple knapsacks. 4. Job Shop Scheduling Problem (JSSP): Leverages the first 20 instances from the ORLibrary [4] to minimize total processing time for jobs across machines. 5. MaxCut Problem (MaxCut): Uses the first 10 instances from Optsicom [15] to partition graph vertices and maximize the edge weight sum between vertex sets."
        },
        {
            "title": "4.1 Evolution Experiments",
            "content": "In this section, we evaluate the effectiveness of our heuristic evolution strategy on five combinatorial optimization problems. Since the evolution interfaces of ReEvo and EoH are limited to the nearest neighbor heuristic in TSP, we compare with these two baselines only in that setting; for all other problems, we contrast the original seed heuristics with the evolved variants produced by HeurAgenix. 12 Figure 7: Average optimality gaps (%; lower is better) before and after heuristic evolution on five representative CO problems. Complete results for all instances appear in Appendix F.1. As figure 7 the shows, HeurAgenix consistently and substantially reduces the gap, and even very basic seed heuristics can be transformed into highly competitive solvers through our automatic evolution pipeline. Detailed evolution results are provided in Table 5 in Appendix F.1."
        },
        {
            "title": "4.2 Problem Solving with LLM",
            "content": "We benchmark HeurAgenix on five standard CO benchmarks, always using GPT-4o as the heuristic selector for the evolved heuristic pool, and contrast its performance with both traditional methods and the strongest available LLM-based hyper-heuristics. For TSP we compare against Guided Local Search (GLS) [57], Ant Colony Optimization (ACO) [19], and OR-Tools [25], as well as three language-based hyper-heuristics: EoH [36]+GLS and ReEvo [63]+GLS, which keep the GLS framework but let EoH or ReEvo evolve its penalty function, and ReEvo+ACO, which uses ReEvo to refine the pheromone update rules of ACO. For CVRP we report ACO, OR-Tools, and ReEvo+ACO. For MKP we include ACO, ReEvo+ACO, the quantuminspired algorithm QICSA [35], and Particle Swarm Optimization (PSO) [21], with the QICSA and PSO numbers copied from Huang et al. [29]. The JSSP baselines are ACO, PSO, and the Grey Wolf Optimizer (GWO) [38] as reported by van Hoorn et al. [56]. The MaxCut baselines are Scatter Search (SS) [37], CirCut [7], and VNSPR [22], which are taken from Myklebust et al. [39]. As shown in Figure 8, HeurAgenix decisively outperforms the existing language-based hyperheuristics and matches or surpasses the specialized methods. Detailed performance results are provided in Table 6 in Appendix F.2. 13 Figure 8: Average optimality gap (%; lower is better) of HeurAgenix and baselines on five CO benchmarks. Dark-blue bars correspond to HeurAgenix (Ours), draw-orange bars to EoH related method, light-orange bars to ReEvo related methods, and gray bars to traditional methods. dagger () after method name indicates that the result is copied from the original publication. Complete results for all instances appear in Appendix F.2."
        },
        {
            "title": "4.3 Problem Solving with a Fine-tuned Model",
            "content": "We next investigate whether lightweight model, fine-tuned under our dual-reward scheme, can serve as an effective online selector. All experiments are conducted on TSPLIB instances, where the selector must choose from the identical evolved heuristic pool. Table 3: Per-instance optimality gaps on TSPLIB (%; lower is better) when the selector is mainstream LLM. Bold marks the best result. Variance is omitted when it equals 0. Instance kroA100 kroA150 kroB100 kroB200 kroC100 bier127 tsp225 a280 pcb442 gr666 pr152 pr1002 pr2392 GPT-4o OpenAI O3 DeepSeek-R Ours 0 0 0 0.11 0 1.29 0.6 0.23 0.1 0.16 0.1 2.10 0.5 0.93 0.2 0.23 0.2 1.78 0.6 1.08 0.5 0 0 0 0.10 0.1 0 0.22 0.1 0.29 0.1 0 0.59 0.1 1.50 0.3 0.13 1.33 0.4 0.87 0.3 0 0 0 0 0 1.01 0.1 0.20 0.10 0.1 0.80 0.5 1.19 1.3 0.16 1.30 0.3 1.09 0.7 0 0 0 0.30 0.1 0 1.09 0.2 0.24 0.2 0.91 0.4 0.79 0.2 1.13 0.9 0.19 0.1 1.00 0.3 0.92 0.3 Average gap 0.61 0.39 0.45 0.50 Table 4: Ablation studies based on Qwen7B. Optimality gaps on TSPLIB (%; lower is better). Bold marks the best per row. Variance is omitted when it equals 0. raw Instance GRPO Ours kroA100 kroA150 kroB100 kroB200 kroC100 bier127 tsp225 a280 pcb442 gr666 pr152 pr1002 pr2392 0 0.80 0.1 0 2.82 1.4 0.44 0.1 2.82 1.4 5.22 3.4 4.82 1.4 3.00 1.9 8.02 5.4 1.49 0.4 9.98 8.4 10.86 7.4 0.21 0.1 0.91 0.1 0 2.02 1.1 0.10 3.01 1.1 4.01 2.4 3.31 0.4 2.21 1.9 4.82 2.2 1.12 0.3 5.02 2.7 8.21 7.3 0 0 0 0.30 0.1 0 1.09 0.2 0.24 0.2 0.91 0.4 0.79 0.2 1.13 0.9 0.19 0.1 1.00 0.3 0.92 0. Average gap 5.01 4.39 0.59 Comparison to mainstream proprietary LLMs. Table 3 contrasts our fine-tuned Qwen-7B with three popular closed-source models operating in zero-shot mode (GPT-4o, OpenAI O3, DeepSeekR1). Despite its far smaller size and cost, the fine-tuned model achieves accuracy on par with the strongest proprietary alternatives. Ablation studies. Table 4 studies the contribution of our dual-reward fine-tuning. Starting from the raw Qwen-7B, we compare (a) vanilla GRPO and (b) our POR+CPR rewards. The dual-reward variant reduces the average gap from 5.01% to 0.59%, and dominates GRPO on every instance. Effect of test-time search. We next quantify the benefit of test-time search. During inference, the selector may perform small Monte Carlo search: for each candidate heuristic proposed by the LLM, it rolls out random completions and takes the average score. We refer to as the rollout budget. Figure 9 plots the optimality gap versus the rollout budget on the pr152 instance. Larger budgets consistently reduce the gap as expected, and our dual-reward fine-tuned selector (POR + CPR) dominates all baselines at any fixed budget. Figure 9: Impact of rollout budget on pr152 from TSPLIB [46]. X-axis: Monte Carlo samples per candidate heuristic (rollout budget; 0 = direct use of the LLM-proposed heuristic with no search). Base model is Qwen-7B. Y-axis: optimality gap (%; Lower is better)."
        },
        {
            "title": "5 Conclusion",
            "content": "We have introduced HeurAgenix, an end-to-end, LLM-driven hyper-heuristic framework that automatically evolves diverse heuristics and selects among them online. contrastive, data-driven evolution phase discovers reusable improvement patterns without external solvers, while an adaptive selectorimplemented either with an off-the-shelf LLM or fine-tuned, lightweight modelleverages test-time scaling and dual-reward mechanism to remain robust under noisy supervision. On standard CO benchmarks, HeurAgenix consistently outperforms existing LLM-based hyper-heuristics and matches or exceeds specialized solvers, demonstrating strong scalability and domain generality. Several avenues remain for future work. First, our empirical study has so far been limited to the Qwen-7B backbone, and we will evaluate HeurAgenix across broader spectrum of model sizes and architectures to assess robustness and costperformance trade-offs. Then, the current POR relies on manually chosen positive/negative split, and we plan to devise adaptive, data-driven schemes that adjust these thresholds dynamically during training. Last, while the selector now targets heuristic choice for classical CO tasks, its principlesequentially picking an operator in finite operation space with only terminal rewardsnaturally extends to more general finite-state Markov Decision Processes(MDP), and exploring this wider class of decision problems is an important next step."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Giorgio Ausiello, Pierluigi Crescenzi, Giorgio Gambosi, Viggo Kann, Alberto Marchetti-Spaccamela, and Marco Protasi. Complexity and approximation: Combinatorial optimization problems and their approximability properties. Springer Science & Business Media, 2012. [3] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [4] John Beasley. Or-library: distributing test problems by electronic mail. Journal of the operational research society, 41(11):10691072, 1990. [5] Jürgen Branke, Su Nguyen, Christoph Pickardt, and Mengjie Zhang. Automated design of production scheduling heuristics: review. IEEE Transactions on Evolutionary Computation, 20(1):110124, 2015. [6] Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher Ré, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling, 2024. [7] Samuel Burer, Renato DC Monteiro, and Yin Zhang. Rank-two relaxation heuristics for max-cut and other binary quadratic programs. SIAM Journal on Optimization, 12(2):503521, 2002. [8] Edmund Burke, Graham Kendall, Jim Newall, Emma Hart, Peter Ross, and Sonia Schulenburg. Hyperheuristics: An emerging direction in modern search technology. Handbook of metaheuristics, pages 457474, 2003. [9] Edmund Burke, Michel Gendreau, Matthew Hyde, Graham Kendall, Gabriela Ochoa, Ender Özcan, and Rong Qu. Hyper-heuristics: survey of the state of the art. Journal of the Operational Research Society, 64(12):16951724, 2013. [10] Edmund Burke, Matthew Hyde, Graham Kendall, Gabriela Ochoa, Ender Ozcan, and Rong Qu. survey of hyper-heuristics. Computer Science Technical Report No. NOTTCS-TR-SUB-0906241418-2747, School of Computer Science and Information Technology, University of Nottingham, 2009. [11] Edmund Burke, Sanja Petrovic, and Rong Qu. Case-based heuristic selection for timetabling problems. Journal of Scheduling, 9:115132, 2006. [12] Lingjiao Chen, Jared Quincy Davis, Boris Hanin, Peter Bailis, Ion Stoica, Matei Zaharia, and James Zou. Are more llm calls all you need? towards scaling laws of compound inference systems, 2024. [13] Edoardo Conti, Vashisht Madhavan, Felipe Petroski Such, Joel Lehman, Kenneth Stanley, and Jeff Clune. Improving exploration in evolution strategies for deep reinforcement learning via population of noveltyseeking agents. Advances in neural information processing systems, 31, 2018. [14] William Cook, William Cunningham, William Pulleyblank, and Alexander Schrijver. Combinatorial optimization. Unpublished manuscript, 10:7593, 1994. [15] Á Corberán, Peiró, Campos, Glover, and Martí. Optsicom project. https://grafo.etsii. urjc.es/optsicom, 2006. [16] Vinicius Renan de Carvalho, Ender Özcan, and Jaime Simão Sichman. Comparative analysis of selection hyper-heuristics for real-world multi-objective optimization problems. Applied Sciences, 11(19):9153, 2021. [17] Valdivino Alexandre de Santiago Junior, Ender Özcan, and Vinicius Renan de Carvalho. Hyper-heuristics based on reinforcement learning, balanced heuristic selection and group decision acceptance. Applied Soft Computing, 97:106760, 2020. [18] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. [19] Marco Dorigo, Mauro Birattari, and Thomas Stutzle. Ant colony optimization. IEEE computational intelligence magazine, 1(4):2839, 2007. [20] John Drake, Ahmed Kheiri, Ender Özcan, and Edmund Burke. Recent advances in selection hyperheuristics. European Journal of Operational Research, 285(2):405428, 2020. 16 [21] Russell Eberhart and James Kennedy. Particle swarm optimization. In Proceedings of the IEEE international conference on neural networks, volume 4, pages 19421948. Citeseer, 1995. [22] Paola Festa, Panos Pardalos, Mauricio GC Resende, and Celso Ribeiro. Randomized heuristics for the max-cut problem. Optimization methods and software, 17(6):10331058, 2002. [23] Michael Figurnov, Maxwell Collins, Yukun Zhu, Li Zhang, Jonathan Huang, Dmitry Vetrov, and Ruslan Salakhutdinov. Spatially adaptive computation time for residual networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 10391048, 2017. [24] Gerd Gigerenzer and Wolfgang Gaissmaier. Heuristic decision making. Annual review of psychology, 62(1):451482, 2011. [25] Google. Or-tools. https://developers.google.com/optimization. [26] Alex Graves. Adaptive computation time for recurrent neural networks. arXiv preprint arXiv:1603.08983, 2016. [27] Boxin Guan, Yuhai Zhao, Ying Yin, and Yuan Li. differential evolution based feature combination selection algorithm for high-dimensional data. Information Sciences, 547:870886, 2021. [28] Qingchun Hou, Jingwei Yang, Yiqiang Su, Xiaoqing Wang, and Yuming Deng. Generalize learned In The Eleventh International heuristics to solve large-scale vehicle routing problems in real-time. Conference on Learning Representations (ICLR), 2023. [29] De-Shuang Huang, Kang-Hyun Jo, Junfeng Jing, Prashan Premaratne, Vitoantonio Bevilacqua, and Abir Hussain, editors. Intelligent Computing Methodologies, volume 13395. Springer Cham, 2022. 18th International Conference. [30] Iklassov, Du, Akimov, et al. Self-guiding exploration for combinatorial problems. arXiv preprint arXiv:2405.17950, 2024. [31] Zhihao Jia, Oded Padon, James Thomas, Todd Warszawski, Matei Zaharia, and Alex Aiken. Taso: optimizing deep learning computation with automatic generation of graph substitutions. In Proceedings of the 27th ACM Symposium on Operating Systems Principles, pages 4762, 2019. [32] Muhammad Khalifa, Rishabh Agarwal, Lajanugen Logeswaran, Jaekyeom Kim, Hao Peng, Moontae Lee, Honglak Lee, and Lu Wang. Process reward models that think. arXiv preprint arXiv:2504.16828, 2025. [33] Jin-Gyeom Kim and Bowon Lee. Appliance classification by power signal analysis based on multi-feature combination multi-layer lstm. Energies, 12(14):2804, 2019. [34] Yeong-Dae Kwon, Jinho Choo, Byoungjip Kim, Iljoo Yoon, Youngjune Gwon, and Seungjai Min. Pomo: Policy optimization with multiple optima for reinforcement learning. Advances in Neural Information Processing Systems, 33:2118821198, 2020. [35] Abdesslem Layeb. novel quantum inspired cuckoo search for knapsack problems. International Journal of bio-inspired Computation, 3(5):297305, 2011. [36] Liu, Xialiang, Yuan, et al. Evolution of heuristics: Towards efficient automatic algorithm design using large language model. In Forty-first International Conference on Machine Learning, 2024. [37] Rafael Martí, Abraham Duarte, and Manuel Laguna. Advanced scatter search for the max-cut problem. INFORMS Journal on Computing, 21(1):2638, 2009. [38] Seyedali Mirjalili, Seyed Mohammad Mirjalili, and Andrew Lewis. Grey wolf optimizer. Advances in engineering software, 69:4661, 2014. [39] Tor GJ Myklebust. Solving maximum cut problems by simulated annealing. arXiv preprint arXiv:1505.03068, 2015. [40] GL Nemhauser and LA Wolsey. Integer programming and combinatorial optimization. In Proceedings of the IPCO: International Conference on Integer Programming and Combinatorial Optimization, Houston, TX, USA, pages 2224. Springer, 1998. [41] Alexander Novikov, Ngân Vu, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt Wagner, Sergey Shirobokov, Borislav Kozlovskii, Francisco J. R. Ruiz, Abbas Mehrabian, M. Pawan Kumar, Abigail See, Swarat Chaudhuri, George Holland, Alex Davies, Sebastian Nowozin, Pushmeet Kohli, and Matej Balog. Alphaevolve: coding agent for scientific and algorithmic discovery, 2025. 17 [42] Christos Papadimitriou and Kenneth Steiglitz. Combinatorial optimization: algorithms and complexity. Courier Corporation, 1998. [43] Fernando Peres and Mauro Castelli. Combinatorial optimization problems and metaheuristics: Review, challenges, design, and development. Applied Sciences, 11(14):6449, 2021. [44] Daniel Power and Ramesh Sharda. Model-driven decision support systems: Concepts and research directions. Decision support systems, 43(3):10441061, 2007. [45] PUC-Rio. Cvrplib, 2025 Accessed. Available online: http://vrp.galgos.inf.puc-rio.br/index. php/en/. [46] Gerhard Reinelt. Tspliba traveling salesman problem library. ORSA journal on computing, 3(4):376384, 1991. [47] Romera-Paredes, Barekatain, Novikov, et al. Mathematical discoveries from program search with large language models. Nature, 625(7995):468475, 2024. [48] Mike Schulze, Henrik Nehler, Mikael Ottosson, and Patrik Thollander. Energy management in industrya systematic review of previous findings and an integrative conceptual framework. Journal of cleaner production, 112:36923708, 2016. [49] Rulin Stella Li, Shao, Simon Shuyue Rui Xin, Shaolei Du, Nathan Lambert, Oh, Tsvetkov, Hannaneh Hajishirzi, Pang Wei Koh, wards: training Spurious-Rewards-Rethinking-Training-Signals-in-RLVR-1f4df34dac1880948858f95aeb88872f, 2025. Notion Blog. Scott Geng, Yiping Wang, Sewon Min, and Luke Zettlemoyer. Sewoong Ranjay Krishna, Yulia rehttps://rethink-rlvr.notion.site/ Rethinking Spurious signals rlvr. in [50] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. [51] Emilio Singh and Nelishia Pillay. study of ant-based pheromone spaces for generation constructive hyper-heuristics. Swarm and Evolutionary Computation, 72:101095, 2022. [52] Evgenii Sopov. selection hyper-heuristic with online learning for control of genetic algorithm ensemble. International Journal of Hybrid Intelligent Systems, 13(2):125135, 2016. [53] Maciej Swiechowski, Konrad Godlewski, Bartosz Sawicki, and Jacek Mandziuk. Monte carlo tree search: review of recent modifications and applications. Artificial Intelligence Review, 56(3):24972562, 2023. [54] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [55] Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung. Branchynet: Fast inference via early exiting from deep neural networks. In 2016 23rd international conference on pattern recognition (ICPR), pages 24642469. IEEE, 2016. [56] J.J. van Hoorn. Dynamic Programming for Routing and Scheduling: Optimizing Sequences of Decisions. Phd-thesis - research and graduation internal, Vrije Universiteit Amsterdam, 2016. Naam instelling promotie: VU Vrije Universiteit Naam instelling onderzoek: VU Vrije Universiteit. [57] Christos Voudouris, Edward PK Tsang, and Abdullah Alsheddy. Guided local search. In Handbook of metaheuristics, pages 321361. Springer, 2010. [58] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. [59] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [60] Jiaheng Wei, Zhaowei Zhu, Hao Cheng, Tongliang Liu, Gang Niu, and Yang Liu. Learning with noisy labels revisited: study using real-world human annotations, 2022. 18 [61] Yaoxin Wu, Wen Song, Zhiguang Cao, Jie Zhang, and Andrew Lim. Learning improvement heuristics for solving routing problems. IEEE transactions on neural networks and learning systems, 33(9):50575069, 2021. [62] Xiao, Zhang, Wu, et al. Chain-of-experts: When llms meet complex operations research problems. In The Twelfth International Conference on Learning Representations (ICLR), 2023. [63] Ye, Wang, Cao, et al. Reevo: Large language models as hyper-heuristics with reflective evolution. arXiv preprint arXiv:2402.01145, 2024. [64] Zhang, Wang, Feng, et al. Can llm graph reasoning generalize beyond pattern memorization? arXiv preprint arXiv:2406.15992, 2024. [65] Daniel Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019."
        },
        {
            "title": "A Heuristic Format",
            "content": "In order to facilitate dynamic heuristic switching, all heuristics within the heuristic pool adhere to uniform format. Each heuristic is implemented as Python function, characterized by the following structure: def ar est _ ne ig bo r_ f9 1 ( problem_state : dict , algorithm_data : dict ,"
        },
        {
            "title": "Nearest Neighbor Heuristic",
            "content": "** kwargs ) -> Tuple [ Operator , dict ]: \"\"\" ... Args : ... Returns : Tuple [ Operator , dict ]: ... \"\"\" # Retrieve necessary data from problem_state ... # If the current solution is empty , start from the first unvisited node . if not current_solution . tour : start_node = unvisited_nodes [0] return AppendOperator ( node = start_node ) , {} # Find the nearest neighbor to the last visited node . nearest_node = None min_distance = float ( inf ) for node in unvisited_nodes : distance = distance_matrix [ last_visited ][ node ] if distance < min_distance : nearest_node = node min_distance = distance position = len ( current_solution . tour ) return InsertOperator ( node = nearest_node , position = position ) , {} Some additional remarks: 1. The function name ends with unique 4-digit identifier (f91d in this example) to avoid naming conflicts. 2. The input consists of problem_state, and algorithm_data, which store problem state, and control parameters, respectively. In this exThe output consists of the current solutions operation and additional information. ample, AppendOperator(node) adds node to the end of the current tour and no more information need be passed. Other TSP heuristics may use InsertOperator, SwapOperator, ReverseSegmentOperator, etc."
        },
        {
            "title": "B Initial Heuristic Generation",
            "content": "For CO problems, the evolutionary process requires initialial seed heuristic algorithms. These can be manually crafted or be generated by HeurAgenix. For classic heuristics, HeurAgenix can generate from LLM, where heuristics are produced using the internal knowledge of language models. HeurAgenix can also learn from literature, which involves reading abstracts to determine relevance, selecting interesting sections, and then generating heuristics based on the information extracted from relevant sections. When dealing with new problems, HeurAgenix can transfer heuristics from known problems. The detailed steps for transferring heuristics from related problems are as follows: Decompose new and known problems: The LLM decomposes both the new problem and source problems into their respective components. Match components: The LLM compares the components of the new problem with those of known problems to determine if heuristics from these problems can be leveraged. Analyze heuristics from known problems: If applicable, the LLM reads the heuristics from these known problems. Evaluate and transfer: For each heuristic, if the LLM determines it is transferable, it translates the 20 components and begins the transfer process; otherwise, it skips this heuristic."
        },
        {
            "title": "C Heuristic Evolution Example",
            "content": "The following diff listings capture the full five-rounds evolution (see Fig. 3) of the nearest neighbor heuristic for the TSP. Lines prefixed with (rendered in red) were removed, while lines prefixed with + (rendered in green) were introduced. Evolution Round 1: Adjust Initial Node for in range(node_num) if != start_node = unvisited_nodes[0] avg_distances = [ np.mean([distance_matrix[i][j] if not current_solution.tour: - + + + + + + + + + + ... key=lambda node: ]) for in range(node_num) ] sub_central_node = np.argsort(avg_distances)[1] start_node = min(unvisited_nodes, distance_matrix[sub_central_node][node]) Evolution Round 2: Evaluate Node Selection score = distance_matrix[last_visited][node] future_cost = sum([ distance_matrix[node][unvisited] ... for node in unvisited_nodes: - + + + + + + + for unvisited in unvisited_nodes if unvisited != node ]) immediate_cost = distance_matrix[last_visited][node] score = immediate_cost + problem_state[\"visited_num\"] / node_num / node_num * future_cost if distance < best_score: ... Evolution Round 3: Adjust Insert Location ... for node in unvisited_nodes: future_cost = sum([ distance_matrix[node][unvisited] for unvisited in unvisited_nodes if unvisited != node ]) score = immediate_cost + problem_state[\"visited_num\"] / node_num / node_num * future_cost for in range(len(current_solution.tour) + 1): prev_node = current_solution.tour[i - 1] next_node = current_solution.tour[i] immediate_cost = distance_matrix[prev_node][node] + distance_matrix[node][next_node] - distance_matrix[prev_node][next_node] position = - - + + + + + + + ... Evolution Round 4: Limit Candidate Nodes 21 ... + threshold_factor = kwargs.get(\"threshold_factor\", 0.70) + percentage_range = kwargs.get(\"percentage_range\", 0.20) + # Calculate average distance from the last visited node to unvisited nodes + avg_distance = np.mean([ + + ]) distance_matrix[last_visited][node] for node in unvisited_nodes + # Find nearest unvisited node to the last visited node + nearest_node = min(unvisited_nodes, + + + nearest = distance_matrix[last_visited][nearest_node] key=lambda node: distance_matrix[last_visited][node] ) + # Prioritize inserting the nearest node if its distance is significantly shorter + if nearest < threshold_factor * avg_distance: + return InsertOperator(node=nearest_node, position=len(current_solution. tour)) + # Evaluate nodes with comparable distances + comparable_nodes = [node + + for node in unvisited_nodes if distance_matrix[last_visited][node] <= (1 + percentage_range) * nearest + ] - for node in unvisited_nodes: + for node in comparable_nodes: ... Evolution Round 5: Periodic Apply 2-opt if len(current_solution.tour) % apply_2opt_frequency == 0: ... + apply_2opt_frequency = kwargs.get(\"apply_2opt_frequency\", 5) + = len(current_solution.tour) + if len(current_solution.tour) > 2: + + + + + + + best_delta = 0 best_pair = None for in range(N - 1): for in range(i + 2, N): if == - 1 and == 0: continue = current_solution.tour[i] = current_solution.tour[(i+1) % N] = current_solution.tour[j] = current_solution.tour[(j+1) % N] current_cost = distance_matrix[a][b] + distance_matrix[c][d] new_cost = distance_matrix[a][c] + distance_matrix[b][d] delta = new_cost - current_cost if delta < best_delta: best_delta = delta best_pair = (i + 1, j) if best_pair: return ReverseSegmentOperator([best_pair]), {} + + + + + + + + + + + + ..."
        },
        {
            "title": "D Monte Carlo Evaluation Strategy",
            "content": "The Monte Carlo Evaluation Strategy aims to evaluate the effectiveness of heuristic algorithms by simulating their performance across multiple problem states. This strategy involves running series of tests where each heuristic algorithm is assessed based on its ability to improve the solution quality from given state. The process ensures that each heuristic is tested in consistent environment, allowing for fair comparison of their relative strengths and weaknesses. As shown in Algorithm 3, the pseudocode outlines the detailed steps involved in this strategy: Algorithm 3 Monte Carlo Evaluation Strategy Input problem instance with initial state z0; test heuristic Htest; heuristic pool H; application frequency ; number of evaluations Output estimated quality QHtest 1: Initialize list metrics [ ] 2: for do = 1 . . . 3: 4: 5: 6: 7: 8: 9: 10: end for 11: QHtest mean(metrics) 12: return QHtest Apply Htest exactly times to z0 to obtain z1 z1 while the selected heuristic continues to improve the solution do end while Append the final-solution metric to metrics Randomly choose Apply once to update z"
        },
        {
            "title": "E Detailed Parameter Settings",
            "content": "This section introduces the detailed experiment settings including heuristic evolution and problem solving. Experiment platform OS: GNU/Linux kernel: 5.15.0-121-generic Architecture: x86_64 Processor: Intel(R) Xeon(R) GPU: 4 NVIDIA RTX A6000 (48G) CUDA: 12.2 LLM parameters Model: GPT-4o_2024-11-20(2024-05-01-preview) Model: OpenAI-o3_2025-04-16(2024-05-01-preview) Model: DeepSeek-R1 Model: Qwen2.5-7B-Instruct-1M Temperature: 0.7 Top-p: 0.95 Max tokens: 1600 Evolution parameters Max API calls for each problem: 2000 Train size for each problem: 20 Max perturbation trials : 1000 Perturbation ratio (K / N): 0.1 Max refinement iterations : 5 Solving parameters Time limitation: 2 hours 23 Selection frequency (M): 5 Monte Carlo search times: 10 Problem state context length: 1000 Fine-tuning model parameters LoRA Rank (α): 32 Optimizer: Paged AdamW (8-bit) (paged_adamw_8bit) Learning Rate: 1 106 Adam β1: 0.9 Adam β2: 0.99 Weight Decay: 0.1 Learning Rate Scheduler: Cosine (cosine) Warmup Ratio: 0.1 Max. Gradient Norm: 0.1 Mixed Precision: BF16 (if supported by hardware, else FP16) Per-Device Training Batch Size: 1 Gradient Accumulation Steps: 1 Number of Training Epochs: 1 Maximum Training Steps: -1 (disabled; training duration set by epochs) Inference Utility for Generation: vLLM (enabled via use_vllm=True) Generations per Prompt (NG): 12 Max. Prompt Length: 2048 tokens Max. Completion Length: 768 tokens TSP Data source: http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/tsp/ Train instances: 20 generated instances Validation instances: brg180, eil101, gr202, pr124, pr152, rd100, u159 Test instances: kroA100, kroA150, kroB100, kroB200, kroC100, bier127, tsp225, a280, pcb442, gr666, pr1002, pr2392 Basic heuristics: cheapest insertion, farthest insertion, greedy algorithm, greedy randomized adaptive search procedure grasp, insertion heuristics, nearest insertion, nearest neighbor, random pairwise insertion, 2opt, 3opt Problem states: average_distance, min_distance, max_distance, std_dev_distance, node_num, last_edge_cost, current_path_length, std_dev_edge_cost, solution_validity, min_edge_cost_remaining, max_edge_cost_remaining Operators: AppendOperator, InsertOperator, SwapOperator, ReverseSegmentOperator, Relocateremaining_nodes, current_cost, average_edge_cost, Operator CVRP Data source: http://vrp.galgos.inf.puc-rio.br/index.php/en/ Train instances: 20 generated instances Validation instances: A-n63-k10, B-n67-k10, E-n76-k10, F-n45-k4, M-n101-k10, P-n70-k10, X-n101-k Test instances: A-n80-k10, B-n78-k10, E-n101-k14, F-n135-k7, M-n200-k17, P-n101-k4 Basic heuristics: farthest insertion, greedy, min cost insertion, nearest neighbor, node shift Problem states: between routes, petal algorithm, saving algorithm, 2 opt, 3 opt demand_variance, average_demand, upper_triangle_distances, average_distance, max_distance, min_distance, distance_variance, vehicle_capacity_utilization, node_to_vehicle_ratio, average_route_length, max_route_length, total_demand_served, avmin_route_length, std_dev_route_length, average_route_cost, erage_vehicle_load, number_of_unvisited_nodes, average_unvisited_node_demand, total_remaining_demand average_remaining_vehicle_capacity, upper_triangle_indices, Operators: AppendOperator, InsertOperator, SwapOperator, ReverseSegmentOperator, RelocateOperator, MergeRoutesOperator MKP Data source: https://people.brunel.ac.uk/$sim$mastjjb/jeb/orlib/files/ Train instances: 20 generated instances 24 Validation instances: gmknap1_1mknap1_7 Test instances: mknapcb1_1mknapcb1_5, mknapcb4_1mknapcb4_5 Basic heuristics: block flip, greedy by cost benefit, greedy by density, greedy by least remaining capacity, greedy by profitto weight ratio, greedy by profit, greedy by resource balance, greedy by weight, greedy improvement, flip, single swap heuristic, 2 opt average_profit, average_weight_per_resource, weight_variance_per_resource, total_weights, capacity_to_weight_ratio, weights_with_epsilon, average_remaining_capacity, profit_to_weight_ratio, remaining_capacity_variance, feasibility_ratio, utilized_capacity_ratio, included_items, included_profits, item_profitability_in_solution solution_density, total_remaining_items, Problem states: profit_variance, Operators: ToggleOperator, AddOperator, RemoveOperator, SwapOperator, FlipBlockOperator JSSP Data source: https://people.brunel.ac.uk/$sim$mastjjb/jeb/orlib/files/ Train instances: 20 generated instances Validation instances: LA21LA30 Test instances: LA01LA20 Basic heuristics: first come first served, least work remaining, longest job next, longest processing time first, most work remaining, shift operator, shortest job next, shortest processing time first, 2opt, 3opt Problem states: average_operation_time, max_operation_time, min_operation_time, std_deviation_operation_time, job_operation_time_range, average_job_length, max_job_length, min_job_length, machine_utilization, job_diversity, num_finished_jobs, num_unfinished_jobs, average_job_completion, min_job_completion_time, max_job_completion_time, std_dev_job_completion_time, average_machine_completion, max_machine_completion_time, min_machine_completion_time, average_idle_time_per_machine, proportion_of_finished_jobs, proportion_of_unfinished_jobs std_dev_machine_completion_time, Operators: AdvanceOperator, SwapOperator, ReverseSequenceOperator, ShiftOperator MaxCut Data source: https://grafo.etsii.urjc.es/optsicom/maxcut.html Train instances: 20 generated instances Validation instances: g11g20 Test instances: g1g10 Basic heuristics: balanced cut, greedy swap, highest delta edge, highest delta node, highest weight edge, most weight neighbors, multi swap 2, simulated annealing Problem states: average_node_degree, edge_density, average_edge_weight, max_edge_weight, min_edge_weight, standard_deviation_edge_weight, weighted_degree_distribution, imbalance_ratio, cut_value, average_cut_edge_weight, selected_nodes_ratio, unselected_nodes_ratio, internal_edges, edge_weight_variance_within_sets, boundary_nodes, boundary_node_ratio Operators: InsertNodeOperator, InsertEdgeOperator, SwapOperator, DeleteOperator"
        },
        {
            "title": "F Detailed Experiment Results",
            "content": "This appendix complements the main text with complete, per-instance results. We first report the performance achieved after heuristic evolution (Section F.1); we then present the full outcomes of the problem solving stage when an LLM acts as the online selector (Section F.2). Unless otherwise noted, lower values indicate better solutions, and all gaps are expressed in percent relative to upper bound or best known. F.1 Detailed Evolution Results The tables below list the optimality gaps obtained by the original seed heuristics and by their evolved counterparts generated by HeurAgenix. Because the public interfaces of ReEvo and EoH can evolve only the nearest-neighbor heuristic for TSP, comparisons with these two baselines are reported exclusively for that setting. 25 Table 5: Per-instance optimality gaps (%; lower is better) before and after heuristic evolution on all benchmark problems. ReEvo and EoH are included only for TSPnearest-neighbor due to interface limitations. All heuristics are deterministic; therefore variances are zero. Nearest neighbor 30.66 31.69 26.4 14.76 26.8 25.62 28.35 22.55 22.02 24.67 16.31 27.82 21.99 Cheapest insertion 19.25 16.27 15.93 22.3 29.01 21.87 18.12 23.85 22.31 17.7 11.28 19.56 21. Farthest insertion 16.45 14.04 10.96 17.73 4.98 12.89 14.49 13.07 18.86 19.19 6.6 25.01 28.82 Evolved (Ours) 4.5 10.62 9.49 10.2 7.02 9.26 5.15 8.27 12.92 12.63 4.36 10.67 12.68 EoH ReEvo 12.11 9.81 11.33 16.12 14.49 14.99 16.02 16.05 12.41 17.63 16.44 19.04 13.4 17.86 14.91 19.68 16.42 15.42 19.74 16.8 14.83 16.55 20.17 24.48 24.93 18.49 TSP kroA100 kroA150 kroB100 kroB200 kroC100 bier127 tsp225 a280 pcb442 gr666 pr152 pr1002 pr2392 Evolved 6.16 8.22 10.86 11.81 10.79 8.55 7.48 10.04 10.81 9.41 6.47 11.08 15.02 Evolved 9.12 8.34 10.86 11.81 10.79 8.55 7.48 10.04 10.81 9.41 2.41 11.08 15. average CVRP A-n80-k10 B-n78-k10 E-n101-k14 F-n135-k7 M-n200-k17 P-n101-k4 average MKP mknapcb1_1 mknapcb1_2 mknapcb1_3 mknapcb1_4 mknapcb1_5 mknapcb4_1 mknapcb4_2 mknapcb4_3 mknapcb4_4 mknapcb4_5 average JSSP LA01 LA02 LA03 LA04 LA05 LA06 LA07 LA08 LA09 LA10 LA11 LA12 LA13 LA14 LA15 LA16 LA17 LA18 LA19 LA average MaxCut g1 g2 g3 g4 g5 g6 g7 g8 g9 g10 average 19.94 Cheapest insertion 20.6 42.92 39.93 41.82 36 38.03 36. Greedy by profit 20.63 18.07 21.31 13.51 13.58 14.13 23.9 21.88 9.66 10.75 16.74 Most work remaining 32.13 49.92 33.5 69.49 12.31 24.3 24.49 48.09 33.54 27.77 27 22.81 19.22 5.42 25.19 52.7 44.77 33.25 41.09 55.65 34.13 9.75 Evolved 18.67 22.6 25.21 45.87 24.94 16. 25.70 Evolved 3.39 3.43 4.3 6.19 4.82 6.22 4.67 6 7.58 3.61 5.02 Evolved 40.01 28.40 34.84 51.36 21.75 18.9 22.92 22.48 19.45 11.9 28.4 18.67 24.43 23.68 35.96 19.05 18.88 50.59 26.84 41.13 27.99 15. Farthest insertion 29.57 36.94 85.1 23.84 104 30.1 51.59 Greedy by weight 27.23 33.44 34.62 28.75 27.99 30.01 31.02 26.25 25.85 30.99 29.62 9.67 Evolved 24.12 22.93 25.02 22.34 36.55 24. 30.56 Evolved 7.71 4.59 4.11 16.6 8.19 6.57 6.12 9.89 8.96 7.55 8.03 First come first served Evolved 241.14 199.54 164.49 272.03 200 221.17 192.58 241.6 226.71 253.03 218.41 232.24 230 243.65 227.17 312.49 399.87 432.19 430.29 332.93 263. 24.62 28.85 29.48 42.2 14.17 16.41 27.3 11.94 25.45 5.64 29.05 23.77 16.26 15.87 19.47 30.16 14.03 33.02 24.23 34.15 23.30 Most weight neighbors Evolved 5.26 4.74 5.03 5.67 5.06 29.48 31.51 32.05 27.36 26.15 17.23 1.56 1.72 1.73 2.25 1.81 11.52 9.97 10.17 9.49 14. 6.45 Highest weight edge 9.71 10.34 9.46 9.38 10.93 55.05 54.74 58.92 57.25 59.7 33.55 Evolved 1.89 1.57 1.94 1.89 1.98 9.14 10.97 12.11 10.66 9.15 6.13 24. 9.06 17.15 15.94 Nearest neighbor 33.26 43.98 55.39 54.22 56 49.93 48.80 Greedy by density 7.71 4.59 4.11 16.6 8.19 6.57 6.12 9.89 8.96 7. 8.03 Shortest processing time first 119.52 196.64 78.56 158.47 141.82 155.62 112.13 191.77 178.13 127.77 158.92 140.62 136.61 159.37 161.56 265.71 296.81 257.9 335.63 235.81 180.47 Balanced cut 17.13 16.77 17.57 17.91 16.88 100.69 104.44 112.16 96.88 103.7 60.41 Evolved 20.6 42.92 39.93 41.82 36 38. 36.55 Evolved 4.05 1.01 0.98 3.72 2.76 2.23 4.38 2.48 3.05 2.22 2.69 Evolved 24.62 28.85 29.48 42.2 14.17 16.41 27.3 11.94 25.45 5.64 29.05 23.77 16.26 15.87 19.47 30.16 14.03 33.02 24.23 34.15 23.3 Evolved 1.56 1.72 1.73 2.25 1.81 11.52 9.97 10.17 9.49 14. 6.45 F.2 Detailed Problem Solving Results with LLM We next provide exhaustive results for the online-selection phase, where GPT-4o chooses among the evolved heuristic pool. The same baselines as in Table 6 of the main text are included for completeness. 26 Table 6: Per-instance optimality gaps (%; lower is better) of all evaluated methods on the five problems. Bold indicates the best result, underline the second-best. Values tagged with are taken from external sources; variance is omitted when it equals 0 or results are taken from external sources. TSP kroA100 kroA150 kroB100 kroB200 kroC100 bier127 tsp225 a280 pcb442 gr666 pr152 pr1002 pr average GLS 2.98 0.2 0.58 5.59 0.8 1.78 0.8 7.1 1.7 5.36 1.5 2.32 0.1 6.44 0.6 3.15 1.4 4.26 1.3 1.89 4.96 4.4 4.74 1.1 ACO 3.19 3.1 3.63 3.3 4.58 1.9 2.91 1.8 3.69 1.9 5.68 0.3 5.05 2.9 6.44 3.9 3.15 2.7 4.26 1.8 13.71 2.2 4.96 2.4 4.74 3.9 OR-Tools 6.35 0.4 4.74 0.3 2.6 0.1 3.76 1.9 12.74 0.2 5.62 0.4 8.45 0.7 12.54 1.0 7.94 1.3 14.38 0.2 2.92 0.1 22.18 0.6 8.45 1.2 EoH+GLS 0.00 0.00 0.00 0.38 0.2 0.00 0.36 0.48 0.12 1.06 0.1 2.65 0.6 1.79 0.1 14.43 4.9 3.87 1.4 ReEvo+GLS 0.00 0.00 0.00 0.18 0.00 0.01 1.15 0.3 0.12 0.82 0.6 1.08 0.5 1.89 1.89 0.6 5.6 1. ReEvo+ACO 2.73 0.7 4.39 2.8 1.13 0.3 1.06 0.8 3.58 2.9 11.04 3.5 1.7 1.5 6.33 2.2 2.94 2.9 6.89 2.0 9.31 3.0 17.96 1.8 6.37 2.0 Ours 0.00 0.00 0.00 0.11 0.00 1.29 0.6 0.23 0.1 0.16 0.1 0.79 0.2 0.93 0.2 0.19 0.1 1.78 0.6 1.08 0.5 3.93 5.08 8.67 1. 0.98 5.80 0.50 CVRP A-n80-k10 B-n78-k10 E-n101-k14 F-n135-k7 M-n200-k17 P-n101-k4 OR-Tools 6.41 0.3 3.69 0.2 11.06 0.5 13.43 1.3 14.04 0.6 8.81 0.8 ACO 34.96 3.1 0.20 8.7 49.30 2.9 50.17 6.7 64.16 3.9 59.18 2. ReEvo+ACO 13.34 2.9 11.37 5.9 28.68 5.8 17.47 5.5 33.73 6.6 25.11 8.4 average 9.57 MKP mknapcb1_1 mknapcb1_2 mknapcb1_3 mknapcb1_4 mknapcb1_5 mknapcb4_1 mknapcb4_2 mknapcb4_3 mknapcb4_4 mknapcb4_5 average JSSP LA01 LA02 LA03 LA04 LA05 LA06 LA07 LA08 LA09 LA10 LA11 LA12 LA13 LA14 LA15 LA16 LA17 LA18 LA19 LA average MaxCut g1 g2 g3 g4 g5 g6 g7 g8 g9 g10 average QICSA() 3.96 5.74 4.36 3.43 4.74 5.50 6.37 6.51 6.13 5.85 5.26 ACO() 0.00 0.00 4.36 3.56 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.41 0.00 0.00 0.00 0.00 0. 0.42 SS() 0.00 0.00 0.00 0.00 0.00 0.60 1.20 1.00 0.68 0.35 0.38 43.00 PSO() 7.61 8.36 7.34 6.28 7.60 9.40 9.38 9.37 8.19 9.23 8. PSO() 0.00 0.00 1.01 3.56 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 6.35 3.57 4.36 3.92 1.11 1.19 21.62 ACO 8.67 9.45 12.07 5.41 7.96 16.05 10.39 12.12 10.59 14.89 10.76 GWO() 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.16 0.77 1.30 0.36 3. 0.37 CirCut() 0.00 0.17 0.01 0.11 0.01 0.00 0.00 0.10 0.10 0.20 VNSPR() 0.03 0.04 0.00 0.39 0.28 3.49 4.99 4.89 2.73 4.50 Ours 3.97 1.3 2.54 1.2 10.97 2.9 8.52 1.2 9.88 1.7 3.08 0.2 6.49 ReEvo+ACO 8.45 2.8 7.97 1.8 5.65 0.7 5.12 1.8 5.51 1.0 3.82 0.8 4.42 1.3 4.63 0.9 4.65 0.6 1.79 0. Ours 0.33 0.20 0.15 1.23 0.8 0.72 0.3 0.55 0.1 1.05 0.1 0.62 0.2 1.03 0.5 0.91 0.6 5.20 0.68 Ours 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.69 0.2 1.79 0.1 0.24 0.1 1.19 0.2 0.00 0.25 Ours 0.00 0.07 0.09 0.44 0.1 0.22 0.1 0.00 1.30 0.2 0.80 0.1 2.73 0.3 0.35 0. 0.07 2.13 0."
        }
    ],
    "affiliations": [
        "Microsoft Research Asia, Beijing, China",
        "Tsinghua University, Beijing, China"
    ]
}