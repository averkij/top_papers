{
    "paper_title": "AdaptThink: Reasoning Models Can Learn When to Think",
    "authors": [
        "Jiajie Zhang",
        "Nianyi Lin",
        "Lei Hou",
        "Ling Feng",
        "Juanzi Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, large reasoning models have achieved impressive performance on various tasks by employing human-like deep thinking. However, the lengthy thinking process substantially increases inference overhead, making efficiency a critical bottleneck. In this work, we first demonstrate that NoThinking, which prompts the reasoning model to skip thinking and directly generate the final solution, is a better choice for relatively simple tasks in terms of both performance and efficiency. Motivated by this, we propose AdaptThink, a novel RL algorithm to teach reasoning models to choose the optimal thinking mode adaptively based on problem difficulty. Specifically, AdaptThink features two core components: (1) a constrained optimization objective that encourages the model to choose NoThinking while maintaining the overall performance; (2) an importance sampling strategy that balances Thinking and NoThinking samples during on-policy training, thereby enabling cold start and allowing the model to explore and exploit both thinking modes throughout the training process. Our experiments indicate that AdaptThink significantly reduces the inference costs while further enhancing performance. Notably, on three math datasets, AdaptThink reduces the average response length of DeepSeek-R1-Distill-Qwen-1.5B by 53% and improves its accuracy by 2.4%, highlighting the promise of adaptive thinking-mode selection for optimizing the balance between reasoning quality and efficiency. Our codes and models are available at https://github.com/THU-KEG/AdaptThink."
        },
        {
            "title": "Start",
            "content": "AdaptThink: Reasoning Models Can Learn When to Think Jiajie Zhang, Nianyi Lin, Lei Hou, Ling Feng, Juanzi Li Tsinghua University 5 2 0 2 9 1 ] . [ 1 7 1 4 3 1 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recently, large reasoning models have achieved impressive performance on various tasks by employing human-like deep thinking. However, the lengthy thinking process substantially increases inference overhead, making efficiency critical bottleneck. In this work, we first demonstrate that NoThinking, which prompts the reasoning model to skip thinking and directly generate the final solution, is better choice for relatively simple tasks in terms of both performance and efficiency. Motivated by this, we propose AdaptThink, novel RL algorithm to teach reasoning models to choose the optimal thinking mode adaptively based on problem difficulty. Specifically, AdaptThink features two core components: (1) constrained optimization objective that encourages the model to choose NoThinking while maintaining the overall performance; (2) an importance sampling strategy that balances Thinking and NoThinking samples during on-policy training, thereby enabling cold start and allowing the model to explore and exploit both thinking modes throughout the training process. Our experiments indicate that AdaptThink significantly reduces the inference costs while further enhancing performance. Notably, on three math datasets, AdaptThink reduces the average response length of DeepSeek-R1-DistillQwen-1.5B by 53% and improves its accuracy by 2.4%, highlighting the promise of adaptive thinking-mode selection for optimizing the balance between reasoning quality and efficiency. Our codes and models are available at https: //github.com/THU-KEG/AdaptThink."
        },
        {
            "title": "Introduction",
            "content": "Recent advancements in large reasoning models, such as OpenAI o1(OpenAI, 2024) and DeepSeekR1 (DeepSeek-AI, 2025), have demonstrated remarkable capabilities in tackling complex tasks. Given problem, these models first engage in long chain of thoughtalso referred to as Think1 Figure 1: AdaptThink enables models to adaptively select between Thinking or NoThinking mode based on problem difficulty, thereby improving reasoning efficiency while further improving overall performance. ingwhere they iteratively explore different approaches, accompanied by reflection, backtracking, and self-verification. Subsequently, they produce final solution that contains only the correct steps and the answer to present to the user. While the long-thinking process markedly enhances the models reasoning capacities, it also substantially increases inference overhead and latency (Qu et al., 2025; Sui et al., 2025). In particular, for some simple queries where users expect fast, near-instant responses, these models often generate excessive thinking with unnecessarily detailed steps or redundant attempts, resulting in suboptimal user experience (Chen et al., 2024; Shen et al., 2025). Existing efforts to improve reasoning efficiency primarily focus on reducing the length of model responses, either through incorporating length-based rewards in reinforcement learning (RL) (Arora and Zanette, 2025; Team et al., 2025), finetuning with preference pairs that penalizes longer responses (Chen et al., 2024; Shen et al., 2025; Luo et al., 2025a), or by merging reasoning and non-reasoning models (Wu et al., 2025). Nevertheless, these methods still apply thinking to all instances, regardless of whether thinking itself is necessary for every problem. In this work, we draw inspiration from the recently introduced NoThinking approach (Ma et al., 2025), which allows reasoning models to skip the thinking process and directly generate the final solution by prompting with pseudo-thinking process. Specifically, we further simplify the approach by prompting the model with an empty thinking segment (i.e., <think></think>). Our pilot study in Section 3 indicates that NoThinking achieves comparable or even better performance than Thinking on relatively simple problems (up to high-school competition level), while significantly reducing token usage; the benefits of Thinking only become pronounced when the problem is difficult enough. In light of this observation, we are curious: Can the reasoning model learn to select Thinking or NoThinking mode adaptively based on the difficulty of the input problem, thereby achieving more efficient reasoning without sacrificing or even improving performance? To this end, we propose AdaptThink, novel RL algorithm to teach reasoning models when to think. Specifically, AdaptThink features two core components: (1) constrained optimization objective that encourages the model to choose NoThinking while ensuring overall performance does not degrade; (2) an importance sampling strategy that balances Thinking and NoThinking samples during on-policy training, thereby overcoming the challenge of cold start and allowing the model to explore and exploit both thinking modes throughout the whole training process. Our experiments demonstrate that AdaptThink effectively enables reasoning models to adaptively select the optimal thinking mode based on problem difficulty, leading to substantial reductions in inference cost compared to prior approaches, while consistently enhancing model accuracy. For instance, on GSM8K, MATH500, and AIME2024, AdaptThink reduces the average response length of DeepSeek-R1-Distill-Qwen-1.5B by 50.9%, 63.5%, and 44.7%, and improving its accuracy by 4.1%, 1.4%, and 1.6%, respectively. The remarkable results substantiate the potential of difficultyadaptive thinking-mode selection as promising paradigm for advancing the trade-off between reasoning performance and efficiency. In summary, our key contributions are as follows: (1) We simplify the NoThinking approach and demonstrate its advantages over Thinking for simpler tasks in terms of both performance and efficiency; (2) We propose AdaptThink, novel RL algorithm that empowers reasoning models to adaptively select the optimal thinking mode adaptively based on problem difficulty, thereby substantially reducing inference costs and further improving performance; (3) We conduct extensive experiments to validate the efficacy of AdaptThink."
        },
        {
            "title": "2 Related Work",
            "content": "Large Reasoning Models. Recent frontier large such as OpenAI reasoning models (LRMs), o1 (OpenAI, 2024), DeepSeek-R1 (DeepSeek-AI, 2025), and QwQ (Qwen Team, 2025), have developed the ability to employ human-like deep thinking in problem solving by generating long chain of thought before arriving at final solution. Such advanced ability is typically acquired through largescale RL with verified rewards or fine-tuning on distilled reasoning traces. Despite promising performance, the lengthy thinking process introduces substantial inference costs and latency. Consequently, variety of approaches have been proposed for more efficient reasoning. Efficient Reasoning for LRMs. Most existing methods to improve the efficiency of LRMs focus on reducing the token usage in model responses. Some methods incorporate length-based rewards into RL to incentivize more concise responses (Arora and Zanette, 2025; Team et al., 2025) or enable precise control over response length (Aggarwal and Welleck, 2025). Other approaches finetune models with length-related preference pairs, which are obtained from best-of-N sampling (Luo et al., 2025a; Shen et al., 2025) or through postprocessing (Chen et al., 2024). Additionally, several works pursue training-free methods to decrease response length, employing techniques such as model merging (Team et al., 2025; Wu et al., 2025) or prompting (Han et al., 2024; Muennighoff et al., 2025; Fu et al., 2025; Xu et al., 2025). Nevertheless, these methods still utilize long thinking for all problems, while the recent NoThinking approach (Ma et al., 2025) allows reasoning models to bypass long thinking and directly output the final solution via prompting, achieving performance comparable to Thinking in low-tokenbudget settings. In this work, we further demonstrate that even with sufficient token budget, NoThinking can outperform Thinking on simple problems while using significantly fewer tokens. This observation motivates us to propose AdaptThink to teach reasoning models to adaptively select the optimal thinking mode based on problem difficulty, which is new direction for efficient reasoning. 2 Figure 2: Comparison of DeepSeek-R1-Distill-Qwen-7B using Thinking and NoThinking mode across different difficulty levels of MATH500 dataset."
        },
        {
            "title": "3 Motivation",
            "content": "3.1 Preliminary Consider reasoning model parameterized by θ and denoted by πθ. Given prompt = [x1, . . . , xn, <think>], where [x1, . . . , xn] represents the problem and <think> is the special token to start the thinking process, the model generates response = [y1, . . . , yl, </think>, yl+2, . . . , ym]. [y1, . . . , yl] corresponds to the thinking, Here, which is long chain of thought consisting of constant exploration, reflection, and self-verification. The token </think> marks the end of thinking. The remaining sequence, [yl+2, . . . , ym], denotes the final solution, which only includes the correct steps to solve the problem and the final answer. From the perspective of probability theory, the response is sample drawn from the conditional probability distribution πθ(x). Since is generated in an auto-regressive way, the conditional probability πθ(yx) can be decomposed as: πθ(yx) = (cid:89) t=1 πθ(ytx, y<t) (1) 3.2 NoThinking is Better for Simple Problems Current reasoning models, such as OpenAI o1 and DeepSeek-R1, apply long thinking across all problems (denoted as Thinking mode). Though enhancing models reasoning capabilities, the lengthy thinking process often leads to unnecessary computation overhead, especially for some simple problems that can also be solved by non-reasoning models (e.g., GPT-4o and Qwen-2.5-Instruct) without thinking. Recently, Ma et al. (2025) proposed NoThinking method, which enables reasoning models to bypass long thinking and directly generate the final solution by prompting with fake thinking process Okay, think have finished thinking.</think>, and found it is still effective in low-token-budget settings. In this work, we further simplify NoThinking by providing the models with an empty thinking (i.e., enforcing the first generated token y1 = </think>). Then, we conduct pilot study to compare Thinking and NoThinking from the perspective of problem difficulty, with sufficient token budget (16K). Specifically, we utilize MATH500 (Lightman et al., 2024) dataset for the pilot study since its have categorized problems into five difficulty levels. For each problem, we employ DeepSeek-R1-DistillQwen-7B to generate 16 responses using Thinking and NoThinking, respectively. Then we analyze the accuracy, response length, and instance-level pass rate across the five difficulty levels. As illustrated in Figure 2, although the model is trained using longthinking data, NoThinking still achieves accuracy comparable to Thinking on relatively simple problems (Level 1 to 3), and even slightly outperforms Thinking on the easiest Level-1 problems. Meanwhile, the average length of NoThinking responses is significantly shorter than Thinking ones. Additionally, compared to Nothinking, Thinking only improves the instance-level pass rate for less than half of the problems from Level 1 to 4. Overall, these findings indicates that Thinking only brings notable benefits for challenging problems, whereas NoThinking can be better choice for simpler questions in terms of both accuracy and efficiency. This motivates us to explore efficient reasoning from new perspective: teaching the reasoning model to adaptively select Thinking or NoThinking mode based on problem difficulty, thereby reducing inference costs while maintaining or even improving the overall performance. To this end, we propose AdaptThink, novel RL algorithm that teaches reasoning models when to think."
        },
        {
            "title": "4 AdaptThink",
            "content": "Our AdaptThink algorithm consists of two important components: (1) constrained optimization 3 objective that incentivizes the model to select NoThinking mode, while ensuring the overall performance does not decrease; (2) an importance sampling strategy that balances Thinking and NoThinking samples during on-policy training, thereby enabling cold start and also allowing the model to explore and exploit both thinking modes throughout the entire training process. We will introduce these two components in detail as follows. 4.1 Constrained Optimization Objective Considering that NoThinking mode offers significant advantage over Thinking in reasoning efficiency, an ideal selection policy should prefer to choose NoThinking as long as the overall performance is not diminished. In other words, we should maximize the probability of generating NoThinking responses while ensuring the models accuracy does not decline. Formally, consider reasoning model πθ and dataset D. Let πθref denote the reference model, which is the initial πθ and remains unchanged during training. Let R(x, y, y) be the reward function (i.e., accuracy in math solving), where x, y, and ˆy denote the prompt, model response, and golden answer, respectively. It returns 0/1 if is incorrect/correct. For simplicity, we omit ˆy and denote the function as R(x, y). Let 1(y1 = </think>) be the indicator function, which returns 1 if the first token of is </think> (i.e., is NoThinking response), otherwise returns 0. Then our optimization objective can be formulated as: max ExD,yπθ(x)1(y1 = </think>) s.t. ExD,yπθ(x)R(x, y) (x)R(x, y). ExD,yπθref (2) To solve this constrained optimization problem, we incorporate the constraint into the objective as penalty term, with penalty weight λ 0: max ExD,yπθ(x),yπθref (x)1(y1 = </think>) (3) + λ(cid:0)R(x, y) R(x, y)(cid:1). By dividing the both side by λ, letting δ = 1 reorganizing the terms about πθref, we have: max ExD,yπθ(x)1(y1 = </think>) δ (x)R(x, y). + R(x, y) Eyπθref λ , and (4) In practice, Eyπθref (x)R(x, y) can be approximated by pre-sampling before training. Specifically, we sample responses from πθref(x) for each x, and calculate their mean reward: Rref(x) = 1 (cid:88) i=1 R(x, yi), yi πθref(x). (5) Then the optimization objective becomes: max ExD,yπθ(x)1(y1 = </think>) δ + R(x, y) Rref(x). (6) Since 1(y1 = </think>) and R(x, y) are not differentiable, we employ policy gradient method to solve this optimization. Specifically, let πθold be distribution equal to πθ without gradient update, and define the advantage function: A(x, y) = 1(y1 = </think>)δ +R(x, y) Rref(x). Then the objective can be converted into PPO-style (Schulman et al., 2017) loss without KL penalty: L(θ) =ExD,yπθold (x) (cid:2) min (cid:0) πθ(yx) (yx) A(x, y), πθold clip( πθ(yx) πθold (yx) , 1ϵ, 1+ϵ)A(x, y)(cid:1)(cid:3). (7) Here, clip() denotes the clipping function, which improves the stability of training. 4.2 Importance Sampling At each step of optimizing L(θ) using on-policy training, we sample batch Db from the dataset D, and then sample responses {yi}K i=1 from πθold(x) for each Db to estimate L(θ). However, since the initial πθ naturally apply Thinking across all problems, it is impossible to get Nothinking samples from πθold from the training starts (i.e., πθold(y1 = </think>x) 0). As result, the model can only learn from Thinking samples and will never generate NoThinking responses. To solve this cold-start challenge, we employ the technique of importance sampling. Specifically, we define new distribution πIS(x): πIS(yt = ax, y<t) = 0.5, if = 1, = </think>; 0.5, if = 1, = wstart; πθold(yt = ax, y<t), if > 1. (8) Here, wstart is common word to start long thinking, such as Alright. During training, we sample responses {yi}K i=1 from πIS(x) instead of πθold(x), so that half of the samples in batch are in Thinking mode and the other half are NoThinking. This allows the model to learn from both modes from the beginning of training, and finally adaptively be Algorithm 1 AdaptThink Input: policy model πθ; dataset D; hyperparameters K, δ, ϵ Initialize: reference model πθref πθ 1: Sample responses {yi}K 2: for step = 1, . . . , do 3: 4: 5: i=1 πθref (x) and calculate Rref(x) for each (Equation 5) Update the old policy model πθold πθ and importance sampling distribution πIS (Equation 8) Sample batch Db from Sample responses {yi}K responses and the other half are NoThinking responses.) Update the policy model πθ by minimizing LAT(θ) i=1 πIS(x) for each Db and estimate LAT(θ) (Equation 9. Half of yi are Thinking 6: 7: end for Output: πθ able to select the appropriate mode. Accordingly, our final loss function of AdaptThink becomes: clip( πθ(yx) LAT(θ) = ExD,yπIS(x) πIS(yx) A(x, y), (9) (cid:2) min (cid:0) πθ(yx) πIS(yx) , 1ϵ, 1+ϵ)A(x, y)(cid:1)(cid:3). In addition to enabling cold start, importance sampling also preserves the opportunities for exploration and exploitation across both Thinking and NoThinking modes during the entire training process. This prevents πθ from collapsing into one thinking mode forever and completely ignoring the other, even if the latter mode may demonstrate greater advantage in the future. Finally, we summarize our AdaptThink algorithm in Algorithm 1. 4.3 New Perspective to Understand the Loss In this subsection, we provide another perspective to understand our loss function LAT(θ) by comparing the advantage A(x, y) of Thinking and NoThinking samples from πIS(x). Given prompt x, we denote the average pass rate of Thinking and NoThinking samples as Rthink(x) and Rnothink(x), respectively. Then their average advantages are: Athink(x) = Rthink(x) Rref(x), Anothink(x) = δ + Rnothink(x) Rref(x). (10) Note that the probability of choosing NoThinking (i.e., πθ(y1 = </think>x)) and Thinking (i.e., πθ(y1 = wx)) are competitive. Therefore, when optimizing LAT(θ), πθ(y1 = </think>x) will improve only if Anothink(x) > 0 and Anothink(x) > Athink(x), which give us: Rnothink(x) + δ > Rref(x), Rnothink(x) + δ > Rthink(x). (11) model, is smaller than δ, LAT(θ) will favor NoThinking and encourage πθ to directly generate the final solution. For more challenging problems where NoThinking lags far behind the other two, LAT(θ) will prioritize performance and guide πθ to engage in Thinking more frequently. Therefore, LAT(θ) aligns well with our expectation for difficulty-adaptive thinking in Section 3.2."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Setup Models. We select DeepSeek-R1-Distill-Qwen1.5B and DeepSeek-R1-Distill-Qwen-7B, two popular reasoning models that demonstrate impressive performance on math problem solving, as the initial policy models. Dataset and Metrics. The training dataset we use is DeepScaleR (Luo et al., 2025b) dataset, which consists of 40K math problems drawn from AIME 1983-2023, AMC, Omni-Math (Gao et al., 2024), and STILL (Min et al., 2024). For evaluation, we use three math datasets with increasing difficulty: GSM8K (Cobbe et al., 2021) test set (1319 grade school math problems), MATH500 (Lightman et al., 2024) (500 high-school competition math problems), and AIME 2024 (30 Olympiadlevel math problems). For evaluation metrics, we consider both accuracy and response length. We also report the average accuracy variation and the average length reduction rate across all the test datasets. Considering the limited size of AIME 2024, we repeatedly sample 16 responses for each case and report the average results. For all models, we set the evaluation context size to 16K, and set the temperature to 0.6 as suggested in DeepSeeks model cards. In other words, only when the problem is simple enough such that the accuracy gap between NoThinking and Thinking, as well as the reference Implementation Details. We build our code based on VeRL (Sheng et al., 2024) framework. The training context size, batch size, and the learnMethod GSM8K Length Acc RatioNT Acc MATH 500 Length RatioNT Acc AIME 2024 Length Average RatioNT Acc Length DeepSeek-R1-Distill-Qwen-1.5B OriginalThinking OriginalNoThinking DPOShortest OverThink DAST O1-Pruner TLMRE ModelMerging RFTMixThinking AdaptThink 79.0 69.8 78.3 77.2 77.2 74.8 80.7 79.7 76 83.1 978 280 804 709 586 458 863 603 1077 480 DeepSeek-R1-Distill-Qwen-7B OriginalThinking OriginalNoThinking DPOShortest OverThink DAST O1-Pruner TLMRE ModelMerging RFTMixThinking AdaptThink 87.9 85.1 85.7 86.3 86.7 87.6 88.9 88.4 86.2 91. 682 283 402 426 459 428 756 531 365 309 0.0% 80.6 100.0% 67.2 82.4 0.0% 81.2 0.0% 83.0 0.0% 82.2 0.0% 85.0 0.0% 63 0.0% 72.4 8.8% 82.0 86.9% 0.0% 90.2 100.0% 80.6 91.6 0.0% 89.4 0.0% 89.6 0.0% 86.6 0.0% 91.8 0.0% 72.6 0.0% 84.8 66.5% 92.0 99.6% 4887 658 3708 4131 2428 3212 3007 2723 4341 1782 3674 697 2499 2435 2162 2534 2899 2280 2411 1875 0.0% 29.4 100.0% 14.0 30.7 0.0% 28.3 0.0% 26.9 0.0% 28.9 0.0% 29.2 0.0% 18.1 0.0% 25.2 33.4% 31.0 76.8% 0.0% 53.5 100.0% 24.2 52.5 0.0% 53.1 0.0% 45.6 0.0% 49.2 0.0% 54.0 0.0% 36.9 0.0% 49.4 64.8% 55.6 76.6% 12073 2190 10794 11269 7745 10361 8982 10337 11157 6679 10306 1929 8699 8744 7578 9719 8633 8624 9969 8599 - 0.0% 100.0% -12.7 +0.8 0.0% -0.8 0.0% -0.6 0.0% -1.0 0.0% +2.0 0.0% -9.4 0.0% -5.1 21.0% +2.4 40.4% - 0.0% 100.0% -13.9 -0.6 0.0% -0.9 0.0% -3.2 0.0% -2.7 0.0% +1.0 0.0% -11.2 0.0% -3.7 10.0% +2.3 6.3% - -79.9% -17.5% -16.5% -42.1% -33.9% -25.3% -32.3% -2.9% -53.0% - -73.6% -29.5% -28.8% -33.4% -24.7% -8.8% -25.5% -28.0% -40.1% Table 1: Accuracy (Acc), response length (Length), and the ratio of NoThinking responses (RatioNT) of different methods on three math benchmarks. The best and second results are bolded and underlined, respectively. Figure 3: Left: The ratio that AdaptThink-7B choose Thinking or NoThinking across different MATH levels. Right: Comparison of accuracy between AdaptThink-7B and DeepSeek-R1-Distill-Qwen-7B using Thinking and NoThinking across different MATH levels. ing rate are set to 16K, 128, and 2e-6, respectively. The hyperparameters K, δ, and ϵ in AdaptThink are set to 16, 0.05, and 0.2, respectively. The comparison of using different δ is shown in Section 5.4. We train the models for 1 epoch, which is 314 steps in total. For the 1.5B model, we use one 8H800 node and cost about 32 hours. For the 7B model, we use four 8H800 nodes and cost about 28 hours. Finally, we select the checkpoints on 300 and 150 steps for the 1.5B and 7B models, respectively, where the models accuracy and response lengths achieve good balance. 5.2 Baselines We compare AdaptThink with the following representative methods for efficient reasoning: DPOShortest constructs preference data by sampling multiple responses for each problem in the training dataset and pairing the shortest correct response and the longest responses, then uses DPO (Rafailov et al., 2023) to finetune the model. OverThink (Chen et al., 2024) first constructs preference data by taking the original longthinking response for each training problem as the negative example and retaining the first two attempts that arrive at the correct answer in the thinking as the positive example, and then uses SimPO (Meng et al., 2024) to alleviate models overthinking behaviors. DAST (Shen et al., 2025) first constructs preference data by ranking pre-sampled responses with length-based reward function, and then Method GSM8K Length RatioNT Acc MATH500 Length RatioNT Acc AIME 2024 Length RatioNT Acc Length Average Acc DeepSeek-R1-Distill-Qwen-1.5B OriginalThinking OriginalNoThinking AdaptThink-1.5B δ = 0 δ = 0.01 δ = 0.02 δ = 0.05 δ = 0.075 δ = 0.1 79.0 69.8 84.6 82.4 83.1 83.1 83.2 82.5 978 718 638 628 480 580 358 0.0% 80.6 100.0% 67.2 4887 658 0.0% 29.4 100.0% 14.0 12073 2190 0.0% 100.0% -12. - - -79.9% 70.4% 86 55.6% 82.4 75.7% 83.4 86.9% 82 71.8% 80.2 91.7% 78.2 2511 2473 2337 1782 1621 1272 50.0% 34.8 67.0% 32.5 62.6% 31.3 76.8% 31 84.2% 29.2 90.4% 26.7 9279 9165 8696 6679 6087 0.2% 19.8% 21.3% 40.4% 64.2% 83.5% +5.5 +2.8 +2.9 +2.4 +1.2 -0.5 -32.8% -36.1% -38.6% -53.0% -52.4% -64.5% Table 2: Performance of AdaptThink-1.5B using different δ value. employs SimPO to finetune the model. O1-Pruner (Luo et al., 2025a) first estimates the reference models performance through presampling and then uses off-policy RL-style finetuning to encourage the model to generate shorter reasoning processes under accuracy constraints. TLMRE (Arora and Zanette, 2025) incorporates length-based penalty in on-policy RL to incentivize the model to produce shorter responses. ModelMerging (Wu et al., 2025) reduces the response length of reasoning model by weightedly averaging its weights with non-reasoning model (i.e., Qwen-2.5-Math-1.5B/7B). RFTMixThinking (Reject Fine-tuning) first samples multiple responses for each training problem using both Thinking and NoThinking, then selects (1) correct NoThinking responses if the instance-level pass rate Rnothink(x) Rthink(x) and (2) correct Thinking responses if Rnothink(x) < Rthink(x), and uses these selected responses to finetune the model. For fair comparison, we re-implement all these baselines using DeepScaleR dataset. 5.3 Main Results Table 1 presents the evaluation results of different methods on GSM8K, MATH500, and AIME 2024. Compared to the original 1.5B and 7B models, AdaptThink reduces the average response length by 53.0% and 40.1%, respectively, while also improves the average accuracy by 2.4% and 2.3%, demonstrating that AdaptThink enables significantly more efficient reasoning without compromising and even enhancing model performance. Moreover, AdaptThink outperforms most baselinesall of which optimize response length within the Thinking modein terms of both accuracy and length reduction. It also achieves the best average results, highlighting the effictiveness of adaptive thinking-mode selection as novel paradigm for achieving efficient reasoning. For the methods that involve both Thinking and NoThinking modes, we additionally report the ratio of responses generated in NoThinking mode (i.e., RatioNT in Table 1). As shown in the table, AdaptThink produces more NoThinking responses for easier test sets (i.e., GSM8K and MATH500) while employing Thinking mode more frequently for challenging test sets (i.e., AIME 2024). See Appendix for detailed cases. similar trend is also observed within the five difficulty levels of MATH500 (Figure 3 and 5), where AdaptThink predominantly selects the NoThinking mode for the easiest Level-1 problems, and progressively increases the use of Thinking as the difficulty level rises. Meanwhile, compared to the original models using only Thinking or NoThinking mode, AdaptThink consistently achieves higher accuracy across most difficulty levels. These findings suggest that AdaptThink has successfully taught the model to adaptively choose an appropriate thinking mode based on problem difficulty, achieving better balance between efficiency and performance. 5.4 More Analyses Effect of δ. To show the effect of δ in our advantage function A(x, y), we implement AdaptThink with different δ values on the 1.5B model and summarize the evaluation results in Table 2. As δ increases, the proportion of NoThinking responses progressively rises, resulting in corresponding reduction in the average response length. However, the gain in accuracy also gradually decreases at the 7 Figure 4: Comparison of accuracy, response length, and the ratio of NoThinking response on MATH500 between AdaptThink and naive GPRO at different training steps. same time. This indicates that δ serves as control parameter for the trade-off between reasoning efficiency and accuracy improvement. Notably, even when δ = 0, the model chooses NoThinking for over 50% of problems in GSM8K and MATH500, implying that NoThinking may possess an inherent advantage over Thinking when addressing relatively straightforward problems. Furthermore, for most δ values, AdaptThink consistently achieves notable reduction in response length and also improves accuracy, which underscores its robustness. Effect of importance sampling. To demonstrate the effect of importance sampling, we compare AdaptThink with naive GRPO that samples directly from πθold(x) during training. As shown in Figure 4, because πθold(x) is initially unable to produce NoThinking samples, GRPO can only learn from Thinking samples and focus on improving accuracy throughout the training process. The response length of GRPO on MATH500 decreases only to around 3,500 (by eliminating overlong responses that would be truncated and receive no reward), after which it gradually increases. In contrast, our importance sampling strategy enables AdaptThink to learn from both Thinking and NoThinking samples at each training step. As the model gradually learn to generate more NoThinking responses for simple problems, the response length eventually decreases to below 2,000 tokens. Implicit thinking ratio. potential concern for AdaptThink is that RL may activate thinking features (e.g., reflection) within NoThinking mode (similar to DeepSeek-R1-Zero) and produce many implicit thinking responses. To allay this concern, we examine the test cases where AdaptThink chooses the NoThinking mode. For these cases, we collect (1) NoThinking responses from AdaptThink, (2) NoThinking responses from the original reasoning model, and (3) the final solution part of Thinking responses from the original model. We compare Method RatioIT Length DeepSeek-R1-Distill-Qwen-1.5B Final Solutions from OriginalThinking OriginalNoThinking AdaptThink DeepSeek-R1-Distill-Qwen-7B Final Solutions from OriginalThinking OriginalNoThinking AdaptThink 9.5% 8.2% 7.9% 0.7% 0.9% 4.2% 1799 665 321 341 426 Table 3: For the test cases where AdaptThink chooses NoThinking, we compare the implicit thinking ratio (RatioIT) and average length of three scenarios. Method MMLU Acc Length RatioNT Acc Length DeepSeek-R1-Distill-Qwen-1.5B OriginalThinking OriginalNoThinking 20.6 AdaptThink 35.7 1724 208 42.2 1055 - 0.00% 100.00% -15.1 16.43% +6.5 - -87.9% -38.8% DeepSeek-R1-Distill-Qwen-7B OriginalThinking OriginalNoThinking 51.2 AdaptThink 63. 63.4 1257 128 856 - 0.00% 100.00% -12.2 16.41% +0.2 - -89.8% -31.9% Table 4: The performance of AdaptThink on the out-ofdistribute test set MMLU. the ratio of implicit thinking responses (denoted by RatioIT) across these three scenarios by detecting whether some representative keywords for thinking (e.g, Wait and Alternatively) appear in the solutions. We also compare the average length of these solutions. As presented in Table 3, AdaptThink only slightly increases the implicit thinking ratio and response length for the 7B model. To entirely eliminate such behavior, one possible approach is to assign zero reward to implicit thinking samples during RL training. Generalizability to OOD scenario. To assess the ability of AdaptThink to generalize in out-of8 distribution scenarios, we conduct an evaluation on MMLU, which contains 14K multi-choice questions and covers 57 diverse domains, distinct from our training data in question format and subjects. As shown in Table 4, AdaptThink reduces average response length by more than 30% by producing NoThinking responses for about 16% of the problems (see Figure 8 for cases), while achieving higher accuracy than the original models."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we first demonstrate the advantages of NoThinking over Thinking in both performance and efficiency for relatively simple tasks. Motivated by this, we propose AdaptThink, novel RL algorithm to enable reasoning models to adaptively select the optimal thinking mode based on problem difficulty. Experiments show that AdaptThink significantly reduces inference costs and further improves model performance, highlighting the promise of adaptive thinking-mode selection for advancing the trade-off between reasoning quality and efficiency."
        },
        {
            "title": "7 Limitation",
            "content": "We discuss several limitations of our work in this section: (1) Due to limited computational resources, we only conduct our experiments on 1.5B and 7B models. Nevertheless, these experiments still demonstrate the efficacy of AdaptThink across different model sizes. (2) Similar to most previous open-sourced works, we only train our models using mathematical datasets because they are easy to obtain and can offer accurate, verifiable rewards. Though our evaluation on MMLU shows AdaptThink models can well generalize to OOD scenarios, we believe they can achieve better results if more training datasets with verifiable rewards for general domains are available."
        },
        {
            "title": "8 Ethical Considerations",
            "content": "All the models and datasets used in this work are publicly published with permissible licenses."
        },
        {
            "title": "References",
            "content": "Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. 2024. Do NOT think that much for 2+3=? on the overthinking of o1-like llms. CoRR, abs/2412.21187. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. CoRR, abs/2110.14168. DeepSeek-AI. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948. Yichao Fu, Junda Chen, Yonghao Zhuang, Zheyu Fu, Ion Stoica, and Hao Zhang. 2025. Reasoning without self-doubt: More efficient chain-of-thought through certainty probing. In ICLR 2025 Workshop on Foundation Models in the Wild. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, Zhengyang Tang, Benyou Wang, Daoguang Zan, Shanghaoran Quan, Ge Zhang, Lei Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu, and Baobao Chang. 2024. Omni-math: universal olympiad level mathematic benchmark for large language models. CoRR, abs/2410.07985. Tingxu Han, Zhenting Wang, Chunrong Fang, Shiyu Zhao, Shiqing Ma, and Zhenyu Chen. 2024. CoRR, Token-budget-aware LLM reasoning. abs/2412.18547. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2024. Lets verify step by step. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Haotian Luo, Li Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, and Dacheng Tao. 2025a. O1-pruner: Lengthharmonizing fine-tuning for o1-like reasoning pruning. CoRR, abs/2501.12570. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Li Erran Li, Raluca Ada Popa, and Ion Stoica. 2025b. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl. Notion Blog. Pranjal Aggarwal and Sean Welleck. 2025. L1: controlling how long reasoning model thinks with reinforcement learning. CoRR, abs/2503.04697. Daman Arora and Andrea Zanette. 2025. Training language models to reason efficiently. CoRR, abs/2502.04463. Wenjie Ma, Jingxuan He, Charlie Snell, Tyler Griggs, Sewon Min, and Matei Zaharia. 2025. Reasoning models can be effective without thinking. arXiv preprint arXiv:2504.09858. Yu Meng, Mengzhou Xia, and Danqi Chen. 2024. Simpo: Simple preference optimization with 9 efficient reasoning for large language models. CoRR, abs/2503.16419. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, and 75 others. 2025. Kimi k1.5: Scaling reinforcement learning with llms. CoRR, abs/2501.12599. Han Wu, Yuxuan Yao, Shuqi Liu, Zehua Liu, Xiaojin Fu, Xiongwei Han, Xing Li, Hui-Ling Zhen, Tao Zhong, and Mingxuan Yuan. 2025. Unlocking efficient longto-short LLM reasoning with model merging. CoRR, abs/2503.20641. Silei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng He. 2025. Chain of draft: Thinking faster by writing less. CoRR, abs/2502.18600. In Advances in Neural Inreference-free reward. formation Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024. Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang, Xiaoxue Cheng, Huatong Song, Wayne Xin Zhao, Zheng Liu, Zhongyuan Wang, and Ji-Rong Wen. 2024. Imitate, explore, and self-improve: reproduction report on slow-thinking reasoning systems. CoRR, abs/2412.09413. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel J. Candès, and Tatsunori Hashimoto. 2025. s1: Simple test-time scaling. CoRR, abs/2501.19393. 2024. OpenAI. llms. learning-to-reason-with-llms/. 2025-05-07. Learning reason with https://openai.com/index/ Accessed: to Xiaoye Qu, Yafu Li, Zhaochen Su, Weigao Sun, Jianhao Yan, Dongrui Liu, Ganqu Cui, Daizong Liu, Shuxian Liang, Junxian He, Peng Li, Wei Wei, Jing Shao, Chaochao Lu, Yue Zhang, Xian-Sheng Hua, Bowen Zhou, and Yu Cheng. 2025. survey of efficient reasoning for large reasoning models: Language, multimodality, and beyond. CoRR, abs/2503.21614. Qwen Team. 2025. Qwq-32b-preview. https: //qwenlm.github.io/blog/qwq-32b-preview/. Accessed: 15 March 2025. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly reward model. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. ArXiv, abs/1707.06347. Yi Shen, Jian Zhang, Jieyun Huang, Shuming Shi, Wenjing Zhang, Jiangze Yan, Ning Wang, Kai Wang, and Shiguo Lian. 2025. DAST: difficulty-adaptive slow-thinking for large reasoning models. CoRR, abs/2503.04472. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. 2024. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256. Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Hanjie Chen, and Xia Ben Hu. 2025. Stop overthinking: survey on 10 Figure 5: Left: The ratio that AdaptiveThink-1.5B choose Thinking or NoThinking across different MATH levels. Right: Comparison of accuracy between AdaptiveThink-1.5B and the DeepSeek-R1-Distill-Qwen-1.5B using Thinking and NoThinking across different MATH levels."
        },
        {
            "title": "A Case Study",
            "content": "We show several cases of AdaptThink in Figure 6, 7, 8, and 9 to illustrate its ability of adaptive thinkingmode selection. As demonstrated in Figure 6, 7, and 8, when faced with relatively simple problems, DeepSeek-R1-Distill-Qwen-7B costs large number of tokens in thinking, which often contains many unnecessary steps and redundant attempts. In contrast, our AdaptThink-7B model adaptively chooses NoThinking mode and directly produces concise final solution, even if the input problem differs from our training data in both the question format and the subject. On the hard hand, the case in Figure 9 indicates that, when the input problem is difficult, AdaptThink-7B is also able to automatically employ Thinking to ensure its accuracy. 11 Question (GSM8K): Two cars are driving on highway. The first car is traveling at an average speed of 60 miles per hour when the second car passes it at an average speed of 70 miles per hour. If both cars continue on the highway at the same speed, how many miles will separate them after 2 hours? 3 DeepSeek-R1-Distill-Qwen-7B: <think>Okay, so have this problem here about two cars on highway. Let me try to understand it step by step. The first car is going at an average speed of 60 miles per hour. Then, the second car passes the first car, and its going at 70 miles per hour. The question is asking, if both cars keep going at the same speed, how many miles apart will they be after 2 hours. Hmm, okay. So, let me visualize this. Theres highway, and both cars are moving in the same direction. The first car is ahead, going 60 mph, and the second car catches up to it, going 70 mph. So, the second car is faster, right? Its going 10 mph faster than the first car. Wait, so the second car is approaching the first car at rate of 10 miles per hour. That makes sense because 70 minus 60 is 10. So, the gap between them is closing at 10 mph. But the question is asking, after 2 hours, how far apart will they be. Hmm, wait second. If the second car is catching up, the distance between them is decreasing. But the question says, how many miles will separate them after 2 hours. So, is it possible that the second car overtakes the first car before 2 hours, and then the distance becomes positive again? Or maybe theyre both moving in the same direction, so the second car is just gaining on the first car, but after 2 hours, the distance between them is how much? Wait, [...](omit 2679 tokens) so, think Im confident that the answer is 20 miles.</think> Final Answer: The number of miles that will separate them after 2 hours is 20 . The first car is traveling at an average speed of 60 miles per hour. The second car passes it at an average speed of 70 miles per hour. Both cars continue on the highway at the same speed. We need to determine how many miles will separate them after 2 hours. 1. The relative speed between the second car and the first car is the difference in their speeds: 70 mph 60 mph = 10 mph. 2. The separation between the two cars after time hours is given by the relative speed multiplied by time: Separation = 10 mph hours. 3. After 2 hours, the separation will be: Separation = 10 mph 2 hours = 20 miles. Thus, the number of miles that will separate them after 2 hours is 20 . (3303 tokens) 3 AdaptThink-7B (ours): <think></think>To determine how many miles will separate the two cars after 2 hours, lets break down the problem step by step. 1. Understand the Speeds: The first car is traveling at 60 miles per hour (mph). The second car is traveling at 70 mph. 2. Determine the Relative Speed: Since both cars are moving in the same direction, the relative speed between them is the difference in their speeds. Relative speed = Speed of second car Speed of first car = 70 mph 60 mph = 10 mph. 3. Calculate the Distance Separated After 2 Hours: Distance = Speed Time. Here, the relative speed is 10 mph, and the time is 2 hours. So, Distance = 10 mph 2 hours = 20 miles. Therefore, after 2 hours, the two cars will be 20 miles apart. (222 tokens) Figure 6: For simple math problem from GSM8K, DeepSeek-R1-Distill-Qwen-7B costs about 3000 tokens in thinking, which contains many unnecessary steps and redundant attempts. In contrast, our AdaptThink-7B adaptively chooses NoThinking mode and directly produces concise final solution. 12 Question (MATH500): In how many ways can 8 people sit around round table if 3 of the people Pierre, Rosa, and Thomas all want to sit together? (Two seatings are considered the same if one is rotation of the other.) 3 DeepSeek-R1-Distill-Qwen-7B: <think>Alright, so have this problem where need to figure out how many ways 8 people can sit around round table, but theres condition: Pierre, Rosa, and Thomas all want to sit together. Hmm, okay. Let me think about this step by step. First, remember that when arranging people around round table, the number of arrangements is different from arranging them in straight line because rotations of the same arrangement are considered identical. For round table with people, the number of distinct seating arrangements is (n 1)! instead of n!. Thats because fixing one persons position accounts for the rotations, and then arranging the others relative to that fixed position. But in this case, theres condition: Pierre, Rosa, and Thomas must all sit together. So, how does that affect the calculation? think the way to handle this is to treat the group of Pierre, Rosa, and Thomas as single entity or \"block.\" If we consider them as one block, then were effectively arranging this block along with the other 5 people. So, that makes total of 6 \"blocks\" to arrange around the table. Wait, hold on. If we have 8 people and were grouping 3 of them together, how many people are left? Let me subtract: 8 3 = 5. So, yes, we have 5 individuals plus the block, totaling 6 units to arrange around the table. Now, since its round table, the number of ways to arrange these 6 units is (6 1)! = 5! = 120. Thats because, again, fixing one position accounts for the rotational symmetry. But wait, [...](omit 9138 tokens) Therefore, think the answer is 720. </think> Final Answer: The number of ways is 720 . To determine the number of ways 8 people can sit around round table with the condition that Pierre, Rosa, and Thomas must sit together, we can treat them as single block. This reduces the problem to arranging 6 entities (the block and the other 5 people) around circular table. Treat the group as single block: This gives us 6 entities (the block and 5 individuals). Arrange these 6 entities around circular table: The number of distinct arrangements of 6 entities around circular table is (6 1)! = 5!. Arrange the people within the block: The 3 people in the block can be arranged among themselves in 3! ways. Thus, the total number of arrangements is the product of these two results: (6 1)! 3! = 5! 3! = 120 6 = 720 Therefore, the number of ways 8 people can sit around round table with Pierre, Rosa, and Thomas sitting together is 720 . (9812 tokens) 3 AdaptThink-7B (ours): <think></think>To determine the number of ways 8 people can sit around round table with the condition that Pierre, Rosa, and Thomas must all sit together, we can treat the group of three as single entity or \"block.\" Heres the step-by-step solution: 1. Treat Pierre, Rosa, and Thomas as single block: This reduces the problem to arranging 6 entities around round table: the block and the other 5 individuals. 2. Arrange the 6 entities around the table: For circular arrangements, the number of ways to arrange entities is (n 1)!. So, for 6 entities, its (6 1)! = 5! = 120 ways. 3. Arrange Pierre, Rosa, and Thomas within their block: The three people can be arranged among themselves in 3! = 6 ways. 4. Multiply the two results together: The total number of arrangements is the product of the arrangements of the blocks and the internal arrangements of the group: 5! 3! = 120 6 = 720. So, the number of ways 8 people can sit around round table with Pierre, Rosa, and Thomas all sitting together is 720 . (297 tokens) Figure 7: For relatively simple math problem from MATH500, DeepSeek-R1-Distill-Qwen-7B costs about 9000 tokens in thinking. In contrast, our AdaptThink-7B adaptively chooses NoThinking mode and directly produces concise final solution, using only 297 tokens. Question (MMLU): proposed mechanism for reaction is as follows: NO2 + F2 NO2F + (Slow step). + NO2 NO2F (Fast step). What is the order of the overall reaction? A) Zero order. B) First order. C) Second order. D) Third order. Output the correct option (one of ABCD) in \"boxed{}\", such as \"boxed{A}\". 3 DeepSeek-R1-Distill-Qwen-7B: <think>Okay, so have this chemistry problem here about reaction mechanisms and determining the overall order of the reaction. Hmm, let me try to figure this out step by step. The problem says the proposed mechanism is: 1. NO2 + F2 NO2F + (slow step) 2. + NO2 NO2F (fast step) And the question is asking for the order of the overall reaction. The options are A. Zero order, B. First order, C. Second order, D. Third order. Alright, remember that the overall order of reaction is determined by the sum of the orders of each reactant in the rate law. But wait, the rate law isnt always given. In this case, the mechanism is provided, so need to use that to figure out the rate law. First, in reaction mechanism, the slow step is the rate-determining step, right? That means the rate law is based on that step. So should focus on the first step: NO2 + F2 NO2F + F. Looking at the slow step, the reactants are NO2 and F2. So the rate law would be rate = k[NO2][F2]m, where is the order with respect to F2. But wait, in the slow step, F2 is reactant, but in the second step, theres an thats produced. Hmm, think that in the overall reaction, the intermediates (like in this case) should be canceled out to get the rate law in terms of the initial reactants. Hold on, [...](omit 3887 tokens) Therefore, the answer is C. Second order. Final Answer </think> The proposed mechanism for the reaction is as follows: 1. NO2 + F2 NO2F + (slow step) 2. + NO2 NO2F (fast step) The overall reaction is determined by combining the two steps: NO2 + F2 + + NO2 NO2F + + NO2F. Simplifying, we get: 2 NO2 + F2 2 NO2F. The rate law is determined by the slow step, which is the rate-determining step. The rate law for the slow step is: rate = k[NO2][F2]. Since is an intermediate, we use the steady-state approximation to express [F] in terms of [NO2] and [F2]. From the fast step, we find: k1[NO2][F2] = k2[F][NO2]. Solving for [F]: [F] = k1 [F2]. Substituting this back into the rate law k2 confirms that the rate is still first order in [NO2] and first order in [F2]. Therefore, the overall order of the reaction is: 1+1 = 2. Thus, the order of the overall reaction is second order. (4709 tokens) 3 AdaptThink-7B (ours): <think></think>To determine the order of the overall reaction, we analyze the given mechanism: 1. Slow Step (Rate-Determining Step): NO2 + F2 NO2F + The rate law for this step is: Rate = k[NO2][F2].. 2. Fast Step: + NO2 NO2F. Since this step is fast, it does not affect the overall rate law. The overall rate law is determined by the slow step. Combining the rate laws from the slow step, the overall reaction order is the sum of the exponents in the rate law: the order with respect to NO2 is 1, and the order with respect to F2 is 1. Thus, the overall reaction order is 1 + 1 = 2. The correct answer is . (263 tokens) Figure 8: Even for out-of-distribution problems from MMLU, AdaptThink-7B is still able to generate NoThinking response if the problem is relatively simple. 14 Question (AIME 2024): Find the largest possible real part of (75 + 117i)z + 96+144i complex number with = 4. where is 3 DeepSeek-R1-Distill-Qwen-7B: <think>Alright, so have this problem here: need to find the largest possible real part of the expression (75 + 117i)z + 96+144i where is complex number with = 4. Hmm, okay, let me try to figure this out step by step. First, remember that for complex numbers, the real part of an expression is the sum of the real parts of each term. So, can probably break this down into two parts: the real part of (75 + 117i)z and the real part of 96+144i . Then, [...](omit 3099 tokens) think that solidifies the answer. dont see any mistakes in the reasoning. Final Answer 540 </think> Given the problem of finding the largest possible real part of the expression (75 + 117i)z + 96+144i where is complex number with = 4, we start by expressing as = 4eiθ. This allows us to rewrite the expression in terms of θ. First, we substitute = 4eiθ and 1 4 eiθ. This simplifies to (300 + 468i)eiθ + (24 + 36i)eiθ. Expanding each term using Eulers formula eiθ = cos θ + sin θ and eiθ = cos θ sin θ, we separate the real and imaginary parts. After combining the real parts, we get 324 cos θ 432 sin θ. To find the maximum value of this expression, we recognize it as linear combination of sine and cosine, which can be written in the form cos θ + sin θ. The A2 + B2. Calculating R: maximum value of such an expression is given by the amplitude = = (cid:112)3242 + (432)2 = 291600 = 540. Thus, the largest possible real 104976 + 186624 = part of the given expression is 540 . (3771 tokens) 4 eiθ into the expression: (75 + 117i) 4eiθ + (96 + 144i) 1 = z 3 AdaptThink-7B (ours): <think>Okay, so have this problem here: need to find the largest possible real part of the expression (75 + 117i)z + 96+144i , where is complex number with = 4. Hmm, okay. Let me try to figure this out step by step. First, know that is complex number with magnitude 4. That means [...](omit 3003 tokens) Therefore, the largest possible real part is 540 . </think> Given the expression (75 + 117i)z + 96+144i where is complex number with = 4, we need to find the largest possible real part. First, we express as 4eiθ where θ is the argument of z. Then, we rewrite the expression in terms of θ: = (75 + 117i)4eiθ + 96+144i eiθ. Simplifying the constants, we get: (75 + 117i)4 = 300 + 468i and 96+144i = 24 + 36i. Thus, the expression becomes: = (300 + 468i)eiθ + (24 + 36i)eiθ. Using Eulers formula, we expand and separate into real and imaginary parts: (300 + 468i)(cos θ + sin θ) = (300 cos θ 468 sin θ) + i(300 sin θ + 468 cos θ) and (24 + 36i)(cos θ sin θ) = (24 cos θ + 36 sin θ) + i(24 sin θ + 36 cos θ). Adding the real parts together: 300 cos θ 468 sin θ + 24 cos θ + 36 sin θ = 324 cos θ 432 sin θ. The expression 324 cos θ 432 sin θ can be written as cos(θ + ϕ) where = (cid:112)3242 + (432)2. Calculating R: 3242 = 104976, 4322 = 186624, = 291600 = 540. Thus, the largest possible real part is 540 . (3743 tokens) 104976 + 186624 = 4 Figure 9: For chanlleging problems from AIME 2024, AdaptThink-7B is able to employs Thinking to solve it, instead of directly generates the final solution."
        }
    ],
    "affiliations": [
        "Tsinghua University"
    ]
}