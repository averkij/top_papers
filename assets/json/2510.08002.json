{
    "paper_title": "Learning on the Job: An Experience-Driven Self-Evolving Agent for Long-Horizon Tasks",
    "authors": [
        "Cheng Yang",
        "Xuemeng Yang",
        "Licheng Wen",
        "Daocheng Fu",
        "Jianbiao Mei",
        "Rong Wu",
        "Pinlong Cai",
        "Yufan Shen",
        "Nianchen Deng",
        "Botian Shi",
        "Yu Qiao",
        "Haifeng Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models have demonstrated remarkable capabilities across diverse domains, yet significant challenges persist when deploying them as AI agents for real-world long-horizon tasks. Existing LLM agents suffer from a critical limitation: they are test-time static and cannot learn from experience, lacking the ability to accumulate knowledge and continuously improve on the job. To address this challenge, we propose MUSE, a novel agent framework that introduces an experience-driven, self-evolving system centered around a hierarchical Memory Module. MUSE organizes diverse levels of experience and leverages them to plan and execute long-horizon tasks across multiple applications. After each sub-task execution, the agent autonomously reflects on its trajectory, converting the raw trajectory into structured experience and integrating it back into the Memory Module. This mechanism enables the agent to evolve beyond its static pretrained parameters, fostering continuous learning and self-evolution. We evaluate MUSE on the long-horizon productivity benchmark TAC. It achieves new SOTA performance by a significant margin using only a lightweight Gemini-2.5 Flash model. Sufficient Experiments demonstrate that as the agent autonomously accumulates experience, it exhibits increasingly superior task completion capabilities, as well as robust continuous learning and self-evolution capabilities. Moreover, the accumulated experience from MUSE exhibits strong generalization properties, enabling zero-shot improvement on new tasks. MUSE establishes a new paradigm for AI agents capable of real-world productivity task automation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 2 0 0 8 0 . 0 1 5 2 : r a"
        },
        {
            "title": "Lab",
            "content": "Learning on the Job: An Experience-Driven, Self-Evolving Agent for Long-Horizon Tasks Cheng Yang1,2,, Xuemeng Yang2,, Licheng Wen2,4,, Daocheng Fu3,2, Jianbiao Mei5,2, Rong Wu5,2, Pinlong Cai2, Yufan Shen2, Nianchen Deng2, Botian Shi2,(cid:66) 1 Central South University, 2 Shanghai Artificial Intelligence Laboratory, 3 Fudan University, 4 Shanghai Innovation Institute, 5 Zhejiang University , Yu Qiao2, Haifeng Li1,(cid:66)"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models have demonstrated remarkable capabilities across diverse domains, yet significant challenges persist when deploying them as AI agents for real-world long-horizon tasks. Existing LLM agents suffer from critical limitation: they are test-time static and cannot learn from experience, lacking the ability to accumulate knowledge and continuously improve on the job. To address this challenge, we propose MUSE, novel agent framework that introduces an experience-driven, self-evolving system centered around hierarchical Memory Module. MUSE organizes diverse levels of experience and leverages them to plan and execute long-horizon tasks across multiple applications. After each sub-task execution, the agent autonomously reflects on its trajectory, converting the raw trajectory into structured experience and integrating it back into the Memory Module. This mechanism enables the agent to evolve beyond its static pretrained parameters, fostering continuous learning and self-evolution. We evaluate MUSE on the long-horizon productivity benchmark TAC. It achieves new SOTA performance by significant margin using only lightweight Gemini-2.5 Flash model. Sufficient Experiments demonstrate that as the agent autonomously accumulates experience, it exhibits increasingly superior task completion capabilities, as well as robust continuous learning and self-evolution capabilities. Moreover, the accumulated experience from MUSE exhibits strong generalization properties, enabling zeroshot improvement on new tasks. MUSE establishes new paradigm for AI agents capable of real-world productivity task automation. Code will be released at: https://github.com/KnowledgeXLab/MUSE."
        },
        {
            "title": "Introduction",
            "content": "In recent years, Large Language Models (LLMs) [1, 2, 3, 4, 5, 6] have developed rapidly, demonstrating powerful capabilities across multiple domains. However, significant challenges remain when deploying these models as the core of AI agents designed to handle real-world tasks. While existing agents have achieved remarkable progress on standardized benchmarks such as question answering [7], mathematical reasoning [8, 9], and code generation [10], these evaluations are limited to measuring domain-specific abilities. To assess the general-purpose capabilities, researchers design benchmarks in interactive environments such as OSWorld [11] and WebArena [12]. Yet, these environments still fall short, typically evaluating isolated functionalities within single platform through short-horizon tasks of roughly 20 steps. In contrast, real-world Productivity Tasks represent higher order of complexity. These tasks are characterized by long-horizon planning and Equal contribution, (cid:66) Corresponding authors."
        },
        {
            "title": "Lab",
            "content": "interactionpotentially exceeding hundred stepsand require agents to fluidly switch across multiple diverse applications. Such complexity demands advanced agent capabilities in long-term planning, robust interaction, and seamless cross-application tool integration. Furthermore, most existing agents are test-time static: their capabilities are fixed once the LLM training phase ends. As result, each time an agent tackles task, it operates like an amnesiac executor, unable to effectively learn from past experiences and lacking the capacity for continuous learning and self-evolution. Neither successes nor failures from previous tasks can be consolidated into effective knowledge to guide future actions. Consequently, even if an agent has successfully completed task before, there is no guarantee of stable replication. When faced with repetitive tasks, it cannot improve efficiency through practice as humans do. This one-off interaction model severely limits agent performance in complex and dynamic environments, making test-time learning difficult to implement and revealing core deficit in the ability to truly learn on the job. Figure 1: Illustration of the test-time learning and evolution of MUSE agents on long-horison, productivity tasks. The agent explores and accumulates experience in cross-application interactive environment, constantly enriching its memory, thereby achieving continuous improvement. To address persistent challenges in dynamic planning, experience accumulation, and continuous learning for existing agents, we propose novel agent framework MUSE, which stands for Memory-Utilizing and Self-Evolving. As illustrated in Figure 1, the core of MUSE is an experience-driven, closed-loop system centered around Memory Module. This module hierarchically organizes diverse levels of knowledge, including procedural knowledge, strategic patterns, and tool-use guidance. Operating within cross-application interactive environment, the agent leverages its accumulated experience to plan and explore solutions for long-horizon productivity tasks. After each sub-task, the agent reflects on its execution trajectory and distills reusable experience back into the Memory Module. By converting raw action sequences into structured knowledge, MUSE enhances the applicability of experience and reduces redundant exploration. This mechanism effectively extends the agents competence beyond its static pretrained parameters, fostering dynamically evolving system with superior robustness and adaptability. Crucially, since the memory is stored in natural language, the accumulated knowledge is LLM-agnostic, allowing experience gained by one model to be seamlessly transferred and utilized by another. We evaluate our framework on TheAgentCompany (TAC) [13], benchmark designed for longhorizon productivity tasks. Our experiments demonstrate that as the agent autonomously continuously accumulates experience within the working environment, it exhibits increasingly superior task completion capabilities and its capacity for continuous learning and self-evolution. Furthermore, our MUSE achieves new SOTA performance by significant margin, using only lightweight Gemini-2.5 Flash model. Our contributions are threefold: We present the MUSE framework, featuring an experience-driven closed-loop architecture. It empowers agents to dynamically accumulate experiences through interaction with working environments, enabling them to evolve beyond LLMs static pretrained parameters. MUSE autonomously converts raw action trajectories into structured, reusable memory without human intervention, aiming to reduce redundant exploration and steadily improve"
        },
        {
            "title": "Lab",
            "content": "agent performance. Its natural language format enables seamless knowledge transfer across different LLMs. We establish new SOTA on the long-horizon productivity task benchmark TAC with score of 51.78%, achieving 20% relative leap over the previous SOTA. Extended experiments demonstrate the effectiveness of our frameworks continuous learning and self-evolution capabilities."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Self-evolving agent The research focus in artificial intelligence is undergoing profound paradigm shift: from developing static foundational models to building dynamic, self-evolving agents capable of continuous adaptation and learning [14]. To achieve this goal, researchers are exploring various approaches. For example, some works [15, 16, 17] abstract the prompt generation process into black-box optimization problem, systematically searching for and optimizing instructions to maximize the performance of large language models (LLMs) on specific tasks. Regarding agent capability building, some cutting-edge work draws on concepts from cognitive science, helping agents accumulate skills and experience through course learning or free exploration, forming reusable skill base [18, 19, 20] or optimized toolsets [21, 22]. Another important technical approach is to empower agents with the ability to self-reflect and iterate. By introducing language feedback mechanisms and comparing and reflecting with ground truth answers, agents can continuously review and strengthen their decision-making logic and action capabilities [23, 24]. 2.2 LLM Agent Memory Mechanisms Research on memory mechanisms for LLM agents aims to enable them to store, retain, and recall past experiences, facilitating the transition from simple reactive models to advanced agents capable of maintaining context and autonomous adaptation. Research in this domain often draws inspiration from human cognitive models, classifying memory into short-term working memory for immediate task processing and long-term memory for persistent learning [25], which relies on external storage such as vector databases [26] and knowledge graphs [27]. To address the challenges of information retrieval and potential information flooding arising from accumulating long-term memories, one line of research focuses on optimizing memory management and structure. For example, Mem0 [28] implements precise control over memory content by defining explicit memory operations, while MemInsight [29] enhances semantic information by augmenting raw memories with summaries and tags to optimize subsequent retrieval efficiency. Another branch of research focuses on constructing procedural memory by generalizing reusable experiences and workflows from agents historical execution trajectories. Specifically, ExpeL [30] collects execution trajectories and refines them into natural language insights and rules. Agent Workflow Memory [31] focuses on generalizing reusable workflows from individual experiences. Memp [32] aims to build learnable, updatable, and lifelong procedural memory, allowing agents to acquire skills and habits through experience. Although these advanced memory mechanisms are validated on various text-based benchmarks [33, 34, 35, 36] and web agent benchmarks [12, 37], existing test environments often lack sufficient complexity and long-term dependency requirements. Consequently, they may not fully assess the true efficacy of these mechanisms in handling complex, long-horizon, real-world tasks."
        },
        {
            "title": "3 Methodology",
            "content": "3.1 Framework Overview In this section, we introduce MUSE, novel agent framework designed for Productivity Tasks Tprod without finetuning LLMs. To enable this test-time learning paradigm, MUSE continuously interacts with comprehensive environment that comprises multiple software and platforms, such as chat application, code editors, and web browsers. Within this environment, the agent executes actions at via predefined basic toolset Atool. The architecture of MUSE includes three core components designed to support this interactive learning loop: Memory Module (Sec. 3.2), PlanningExecution (PE) Agent (Sec. 3.3), and Reflect Agent (Sec. 3.4). The Memory Module is further decomposed into three functionally distinct components: Strategic Memory Mstrat, Procedural Memory Mproc, and Tool Memory Mtool. As illustrated in Figure 2, the operational mechanism of our framework is Plan-Execute-ReflectMemorize iterative loop. The system begins by initializing and loading the Memory Module (M). When new task (τ Tprod) is received, the process unfolds as follows. 1) Plan and Execute: The PE Agent initiates the process by performing preliminary analysis of the task, decomposing it into an ordered queue of sub-tasks. For each sub-task, the PE Agent first queries the Procedural Memory to retrieve guidance from relevant prior knowledge. It then executes sequence of actions in the interactive environment using deliberately minimal toolset. Each fundamental interaction step involves the agent receiving an observation ot and, based on its history ht = (o1:t, a1:t1), selecting an action via its policy at πtest(at ht). This design compels the agent to learn how to compose primitive tool actions into complex workflows required to accomplish the sub-tasks. The execution phase for given sub-task concludes once the PE Agent deems its attempt complete. 2) Reflect and Memorize: After each sub-task attempt, the Reflect Agent conducts an autonomous assessment based on its environmental observations and the PE Agents sub-task execution trajectory hk:t = (ok:t, ak:t1), requiring no human intervention. If the sub-task is successful, the Reflect Agent distills the trajectory into new Procedural Memory. Otherwise, it generates diagnostic analysis of the failure and instructs the PE Agent to replan and re-execute. The PE Agent adaptively refreshes its overall task plan after each assessment, continuing this core loop until the entire task is complete. 3) Post-Task Distill: Upon completing the overall task, comprehensive task analysis is conducted on the full execution trajectory. From this analysis, the Reflect Agent distills higher-level Strategic and Tool Memories, capturing broader insights and effective guidelines. Ultimately, all memory typesProcedural, Strategic, and Toolare uniformly maintained within M, ensuring the effective retention and future applicability of all acquired knowledge. 3.2 Memory Module The Memory Module is the key component enabling our MUSE to learn on the job. Given the high expense of fine-tuning and our goal of maximizing the utility of closed-source LLMs, we refrain from fine-tuning the base LLM to maintain its native generalization capacity. Rather, we incrementally build up M, allowing the agents performance R(t) to improve over repeated trials on tasks τ Tprod. This module is composite memory = {Mstrat, Mproc, Mtool} comprising three distinct memory types, each optimized for specific level of abstraction: Strategic Memory Mstrat for macro-level behavioral paradigms, Procedural Memory Mproc for combinations of tool sequences, and Tool Memory Mtool for individual tool use. Each memory type operates with distinct mechanisms for its generation, updating, and application. Strategic Memory (Mstrat) focuses on distilling lessons from dilemmas an agent encounters during task execution and their solutions, particularly from challenges that require multiple attempts to overcome. The Reflect Agent abstracts these problem-solution experiences into high-level guidance and formats them as < Dilemma, Strategy > key-value pairs. Upon agent initialization, the entire Mstrat is loaded into the system prompt to guide its global task execution strategy. To"
        },
        {
            "title": "Lab",
            "content": "Figure 2: The MUSE framework adopts Plan-Execute-Reflect-Memorize loop. The PlanningExecution(PE) Agent decomposes task and performs actions within an interactive environment, while the Reflect Agent abstracts successful attempts into Procedural Memory. After task completion, the Reflection Agent further synthesizes this knowledge into the Strategic and Tool Memory. ensure efficiency and prevent context window bloat, this memory is updated, merged, and refined after each task, always maintaining concise size. For specific examples, refer to Table 6 in the appendix. Procedural Memory (Mproc) archives the PE Agents successful sub-task trajectories as hierarchical knowledge base of Standard Operating Procedures (SOPs). This library is indexed first by application (e.g., related platforms or APIs), followed by second-level SOP index that documents the key analyses, precaution, core parameters, and operational steps for each sub-task. To balance efficiency and performance, the system employs lightweight, proactive retrieval mechanism. Only the SOP index is loaded at startup to minimize overhead. When facing uncertainty, the agent utilizes built-in tool to proactively query detailed SOPs for decision support, which closely mimics how human experts consult past cases. The memory system is refined through two-stage process. First, immediately following successful sub-task, the Reflect Agent dynamically adds the new SOP pnew to Mproc for immediate reuse. Second, after the entire task is complete, the agent performs higher-level, global refinement (e.g., deduplication, generalization) to continuously optimize the long-term quality and applicability of the knowledge base. See Table 7 in the appendix for examples. Tool Memory (Mtool) functions as the agents muscle memory for single tool usage, operating automatically without requiring proactive retrieval. This memory consists of two components, Mtool = {Dstatic, Idynamic}: Static Description Dstatic, loaded into the system prompt at startup to explain each tools core functionality, and Dynamic Instruction Idynamic, which is returned with the environments observation ot after tool is used. This instruction guides the agents immediate next action at+1, such as suggesting subsequent tool to invoke or an analysis to perform. To ensure this muscle memory improves over time, the Tool Memory is updated by the Reflect Agent after each task is completed. See Table 8 in the appendix for specific examples."
        },
        {
            "title": "Lab",
            "content": "3.3 Planning-Execution Agent Productivity tasks often require dozens of coordinating actions across multiple applications. To manage this complexity, the PE Agent first decomposes the main task τ into an ordered queue of sub-tasks = [st1, st2, . . . , stM] based on the initial task description. The agent then systematically works through this queue, attempting to resolve each sub-task sti via an iterative ReAct [38] process. Crucially, after each sub-task execution, the agent re-evaluates and updates the sub-task queue based on newly acquired information, ensuring an adaptive path to task completion. Sub-task Plan and Replan. Both initial planning and subsequent replanning follow unified, multi-turn process that generates an ordered sub-task queue Q. Each sub-task sti is defined by tuple sti = (desci, goali), where desci outlines its scope and goali serves as the evaluation basis for the Reflect Agent. The primary distinction between the two phases lies in their inputs. The initial plan Qinit is derived solely from the users original task description. In contrast, replanning is dynamic process that occurs after each sub-task is attempted. It integrates the execution results and the Reflect Agents assessment to continuously refine the current plan. When is empty, the PE Agent performs final review, examining the global state of the environment to confirm that the overall task objectives have been met. By iteratively maintaining and updating Q, MUSE ensures the stable and coherent execution of long-horizon tasks and prevents error accumulation. Sub-task Execute and Retry. The PE Agent processes sub-tasks sti sequentially from the queue Q, attempting to resolve each one using memory-enhanced ReAct loop. The core of this loop is the iteration of (θt, at, ot) tuple, representing Thought, Action, and Observation: the agent first generates thought to plan an action atsuch as entering text, clicking button, or querying its Procedural Memory Mprocthen executes the action and receives an observation ot as feedback. This cycle continues until the agent concludes that the sub-tasks goal has been met. To prevent the agent from getting stuck in futile loops, maximum of actions is imposed on each subtask attempt. If this limit is reached, the Reflect Agent intervenes to evaluate and grant one retry opportunity. This retry mechanism is explicitly designed to encourage exploration over exploitation. During the retry, the PE Agent is no longer required to use Procedural Memory Mproc, enabling it to discover novel methods when existing knowledge is erroneous or inapplicable. If this second attempt also fails, the PE Agent triggers sub-task replanning process. Minimal Usable Toolset. In contrast to many general agent studies [39, 40] that aim to integrate massive number of APIs, we equip MUSE not with specialized tools for specific applications (like PDF or Excel), but with minimal toolset Atool of fundamental yet powerful general-purpose tools. This toolset includes browser interaction, code interpreter, Shell, vision extractor, and memory retriever. We believe that the core of intelligence lies in the ability to creatively combine basic tools, rather than mechanically invoking pre-defined functions. Furthermore, key objective of this research is to validate whether MUSE can convert successful solutions into reusable Procedural Memory, thereby achieving the self-evolution of its capabilities. full list of our toolset are illustrated in Section A.4 and Table 9. Procedural Memory Retrieval. To achieve low-cost experience reuse while respecting the LLMs context length limit, the experience retrieval mechanism separates the memory index from the detailed content. An SOP Mproc is thus structured as pair = (indexp, contentp). At the start of sub-task, only lightweight index of all available SOPs, IMproc = {indexp Mproc}, is loaded into the context. The PE Agent can then, at any point during execution, use dedicated tool amem to retrieve the full contentp of specific SOP on demand. To maximize the value of this feature, we use prompt engineering to encourage the agent to prioritize querying for relevant experience at the beginning of each sub-task. 3.4 Reflect Agent During execution, the PE Agent can encounter hallucinations (e.g., erroneously believing task is complete) and failures. To address this, the Reflect Agent acts as an independent, third-party"
        },
        {
            "title": "Lab",
            "content": "supervisor. For its analysis, it receives the sub-tasks definition sti = (desci, goali), along with the PE Agents sub-task execution trajectory hk:t. Notably, it can also interact directly with the environment to independently verify information. Sub-task Evaluation. The Reflect Agents evaluation process is triggered whenever the PE Agent completes sub-task or reaches its action limit N. It starts by formulating an ordered checklist based on three core dimensions: 1) Truthfulness Verification: Ensuring conclusions are grounded in real environmental feedback to suppress hallucinations. 2) Deliverable Verification: Checking the existence, completeness, and correctness of any output files or reports. 3) Data Fidelity: Confirming that data has not been lost, truncated, or altered during processing. To execute this checklist, the Reflect Agent, which is equipped with the same toolset Atool as the PE Agent, utilizes two primary inspection methods. The first is trajectory referencing, which explicitly traces the PE Agents conclusions back to specific observations ot in the execution history hk:t. The second is active verification, which involves proactively using tools to interact with the environment and cross-check key information with real-time feedback. Upon completing its checks, the Reflect Agent outputs success/failure flag and detailed check report. This tuple is fed back to the PE Agent as historical record. Based on the outcome, the Reflect Agent then performs critical operation: if = success, it summarizes the effective operational sequence from hk:t into new SOP pnew for the Procedural Memory Mproc; if = failure, it generates failure cause analysis report ail. Finally, based on this complete evaluation, the PE Agent initiates the necessary replanning. Memory Update Mechanism. The entire task τ is considered complete once the PE Agent stops generating new subtasks in the replanning phase. The PE Agent then launches task review, summarizing its execution attempts and outcomes. This triggers the Reflect Agent to conduct full-scale upgrade of the memory system M. It begins by analyzing task challenges and solutions to extract < Dilemma, Resolution Pattern > pairs, thereby reinforcing Strategic Memory Mstrat, while also codifying effective tool usage to augment Tool Memory Mtool. Then, all three types of memory undergo thorough refinement and integration process, aiming to integrate new and old knowledge, eliminate redundancy, and generalize common patterns within M."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Benchmark Our experimental evaluation utilizes TheAgentCompany (TAC) benchmark [13]. Comprising 175 tasks, this benchmark is designed to assess the comprehensive capabilities of autonomous language agents by simulating high-fidelity corporate environment. The tasks are structured around six core employee positions (e.g., HR, PM, SDE), requiring the agent to execute interconnected operations using suite of applications such as chat clients, cloud storage, and project management software, all within fully functional operating system. core feature of TAC is the high complexity and long-horizon nature of its tasks. On average, completing task requires over 40 action steps, frequently spanning two or more applications. This demands that an agent decompose high-level objectives into coherent, protracted sequence of steps and integrate information across platforms. Therefore, TAC provides rigorous platform for evaluating an agents real-world problem-solving, multi-step planning, and long-horizon reasoning capabilities, making it highly suitable for our research focus on long-horizon productivity tasks. 4.2 Experimental Setup In our experimental configuration, the PE Agent and Reflect Agent employ the Gemini-2.5 Flash model [41], while NPCs in the TAC environment are powered by GPT-4o model. The maximum number of actions for each sub-task is set to = 20. For evaluation, we rely on the official"
        },
        {
            "title": "Lab",
            "content": "protocol provided by the TAC benchmark. This evaluation protocol not only assesses the final completion status of task but also defines series of critical intermediate checkpoints to measure partial progress. The primary metric is the partial completion score (Spartial), which is calculated as: Spartial = 0.5 Completed_ckpt/Total_ckpt + 0.5 ull, where ull {0, 1} is binary indicator of whether the task was fully completed. Our final performance metric is computed as the average partial completion score across all evaluated tasks. Additionally, we calculate an aggregate checkpoint score, Sckpt, which represents the proportion of completed checkpoints relative to the total checkpoints across all tasks. 4.3 Experimental Results"
        },
        {
            "title": "4.3.1 Continuous Learning Experiments",
            "content": "To validate the continual learning capability of our MUSE framework, we curated subset of 18 tasks from TAC benchmark, which we denote as Tcl. This subset was sampled to ensure coverage across all six professional roles. Our experimental design simulates how humans accumulate experience, with the primary objective of testing whether the agent can progressively improve its performance on repetitive tasks by continuously updating its memory. For comparison, we first establish baseline by evaluating the Gemini-2.5 Flash model on Tcl without our Memory Module. The main experiment then consists of three sequential iterations with no human intervention, where in each iteration, the agent tackles all 18 tasks in order, carrying its accumulated knowledge forward to the next. To mitigate randomness, we conduct five complete runs of this experiment and report the average scores. The detailed tasks in Tcl are elaborated in Section A.1 and Table 5. As illustrated in Figure 3, the results clearly show that both the Sckpt and Spartial metrics grew steadily and monotonically across the three iterations, direct manifestation of our frameworks self-evolving capability. Most critically, in the final round, MUSE outperformed the memory-less baseline by over 10%, confirming the effective translation of self-accumulated knowledge into substantial performance gains. We attribute this advantage to the accumulated experience, which enables the agent to avoid previously failed exploration paths and thus focus more directly on effective solutions. This approach not only improves efficiency by streamlining the LLMs context but also enables the agent to achieve an unprecedented depth of exploration. Figure 3: Performance trends across iterations of MUSE. The figure shows Sckpt (blue) and average Spartial (orange). Dashed lines denote the baseline without memory, while solid lines track improvements across iterations. 4.3.2 Generalization Experiments To further evaluate the generalization capability of our memory mechanism, we curate challenging subset from the TAC benchmark, denoted as Thard. This subset comprises 12 tasks on which even strong models like Claude-4 Sonnet achieve little to no score. The purpose of this experiment is to test our agents zero-shot generalization ability when facing entirely new and highly difficult tasks. Specifically, we compare baseline agent operating without memory against our agent equipped with the fixed Memory Module, which is accumulated over three iterations of continual learning on the Tcl set. Both agents are evaluated on this previously unseen Thard task set. By comparing their performance differences, we can easily determine whether the memory mechanism can effectively transfer historical experience to unknown scenarios. Detailed Thard are elaborated in Section A.1."
        },
        {
            "title": "Lab",
            "content": "Table 1: Performance comparison on the hard-task set. The agent with memory demonstrates significant improvement. Framework Model checkpoint Sckpt (%) Avg. Spartial (%) Openhands-versa [42] Openhands [43] MUSE w/o mem MUSE w/ mem claude-4 sonnet gemini-2.5 pro gemini-2.5 flash gemini-2.5 flash 3 / 59 5 / 59 18 / 59 24 / 59 5.08 8.47 30.51 40.68 2.00 3.00 23.65 33.41 As shown in Table 1, current SOTA agents, such as the Openhands framework [43] using powerful closed-source models like Gemini-2.5 Pro [41] and Claude-4 Sonnet [5], struggle significantly on Thard. In general, they complete fewer than 10% of the checkpoints and achieve an Spartial of only 2% or 3%. In contrast, our MUSE, using only the lighter Gemini-2.5 Flash model, reaches an Spartial of 23.65% even without relying on memory, which demonstrates the effectiveness of the synergy between our PE and Reflect Agents. When equipped with its pre-learned memory, our agents performance further improves to remarkable Spartial of 33.41%. This result provides strong evidence for the zero-shot generalization capability of the knowledge acquired through our framework, indicating that MUSE learns transferable and generalizable memory, rather than merely remembering task-specific solutions. 4.3.3 TAC Full Benchmark To conduct fair and comprehensive comparison against other leading agent methods, we evaluated our framework on the complete TAC benchmark, which includes all 175 tasks. For this experiment, the agent was equipped with the Memory Module accumulated after three iterations on the Tcl subset, and this memory was kept frozen throughout the evaluation. As shown in Table 2, MUSE achieves new SOTA performance on the TAC benchmark. Notably, it attains an average Spartial of 51.78%, marking the first time an agent has surpassed the 50% threshold on this benchmark and outperforming the previous SOTA (OpenHands-versa w/ Claude-4 Sonnet) by nearly 20%. This substantial improvement is particularly striking given that our agents memory was acquired from only approximately 10% of the available tasks, demonstrating exceptional learning on the job efficiency. These results prove the effectiveness of MUSE in challenging real-world productivity tasks. Besides, they provide strong empirical support for our core thesis: past condensed experience yields highly generalizable capabilities that far exceed what might be expected from the limited learning. The complete results of 175 tasks are listed in Table 10. Table 2: Performance comparison across frameworks and models on the TAC full 175-task benchmark. PCR (Perfect Completion Rate) indicates the proportion of tasks that are fully solved. Framework Model checkpoint Sckpt(%) Avg. Spartial(%) PCR(%) OWL-RolePlay [44] gpt-4o + o3-mini 127 / 776 Openhands [43] Openhands-Versa [42] gemini-1.5 pro gemini-2.0 flash gemini-2.5 pro 90 / 776 195 / 776 361 / 776 claude-3.7 sonnet 353 / 776 392 / 776 claude-4 sonnet MUSE (ours) gemini-2.5 flash 465 / 776 16. 11.60 25.13 46.52 45.49 50.52 59.92 11.04 8.02 18.96 39.28 40.18 43. 51.78 4.00 3.43 11.43 30.29 30.86 33.14 41."
        },
        {
            "title": "Lab",
            "content": "4.4 Ablation Study"
        },
        {
            "title": "4.4.1 Ablation Study for Reflect Agent",
            "content": "To evaluate the impact of the Reflect Agent in the MUSE framework, we conduct an ablation study by removing it. Specifically, we compare variant lacking the Reflect Agent against the full framework, with both configurations operating without the Memory Module. As presented in Table 3, the non-reflective variant underperformed on the 18-task subset Tcl. This result demonstrates the indispensable role of the reflective mechanism in ensuring execution quality and providing the high-quality signal required for effective learning. Table 3: Performance comparison of ablation studies of Reflect Agent on Tcl. Results show that removing the Reflect Agent (No Reflection Variant) leads to substantial performance drop. Framework Model checkpoint Sckpt (%) Avg. Spartial (%) No Reflection Variant MUSE gemini-2.5 flash gemini-2.5 flash 54 / 85 56.2 / 85 63.53 66.12 43.21 55.85 4.4.2 Ablation Study for Different Models To evaluate the open-source LLM adaptability of our framework, we replace the core model with DeepSeek-V3-250324 [3] and conduct experiments in two scenarios: with and without the pre-accumulated memory. The results, when compared against other open-source-based agents in Table 4, yield two key insights. First, even without memory, the MUSE architecture alone enables DeepSeek-V3 to outperform all other frameworks using open-source models, highlighting the intrinsic advantages of our design. Second, the addition of the pre-accumulated Memory Module provides significant performance boost, confirming that our memory mechanism is model-agnostic and that the accumulated knowledge can be effectively transferred across different LLMs. Table 4: Performance comparison of agents utilizing open-source LLMs on Tcl. Framework Model checkpoint Sckpt (%) Avg. Spartial (%) Openhands llama-3.1 405b llama-3.3 70b qwen-2.5 72b MUSE w/o memory MUSE memory deepseek-v3 deepseek-v3 17 / 85 11 / 85 11 / 85 29 / 85 43 / 20.00 12.94 12.94 34.12 50.59 9.78 5.84 6.50 28.01 36."
        },
        {
            "title": "5 Discussions and Conclusion",
            "content": "Discussions. We employ memory modules to tackle long-horizon productivity tasks (some spanning over 100 steps), as fine-tuning methods suffer from computational intractability, while RL-based approaches are hindered by the design of rewards that are both extremely sparse and difficult to formulate. Thus, this research focuses on enhancing agent memory to empower test-time learning capabilities. We acknowledge that our current memory architecture is not panacea and has limitations in handling specific tasks like high-level planning or multi-hop search. Nevertheless, the experimental results confirm its potential. We attribute this success to the agents ability to efficiently avoid previously failed paths and reallocate exploration to more promising regions, effectively pruning the decision space and enabling deeper, more successful search."
        },
        {
            "title": "Lab",
            "content": "The TAC benchmark represents significant step forward in evaluating agents on complex tasks, which is key reason we selected it to test our framework. However, during our experiments, we also observed some limitations. Some task descriptions can be ambiguous or contain inaccuracies. Furthermore, the evaluation scripts for certain tasks are rigid and do not account for the full range of valid solutions. As result, several unexpected yet plausible agent strategies are sometimes underestimated or incorrectly penalized. We provide two detailed case studies illustrating these issues in Appendix A.2. MUSE is currently designed as fully autonomous framework, but architecture of its Memory Module also facilitates the incorporation of human feedback. The design allows users to directly manage and revise stored experiences (e.g., add, delete, modify, and query), thereby paving the way for human-agent collaborative iteration. This could enable the seamless integration of human demonstrations and abstract guidance, further amplifying the agents overall performance. Looking ahead, we envision an ideal agent accumulating extensive experience through long-term practice and learn by contrasting successful and failed trajectories. This continuous learning paradigm will let it evolve in routine use, ultimately realizing enduring growth in competence. Conclusion. In this study, we propose an experience-driven self-evolving framework, MUSE. Centered around hierarchical Memory Module, MUSE systematically extracts reusable knowledge from interaction trajectories to tackle complex, long-horizon productivity tasks. Comprehensive evaluation on the TAC benchmark confirms MUSEs effectiveness: it achieves continuous performance improvement and self-evolution with autonomous experience accumulation, and shows remarkable experience generalization to novel tasks. Consequently, MUSE achieves new SOTA performance on TAC by large margin."
        },
        {
            "title": "References",
            "content": "[1] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [2] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [3] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [4] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pages arXiv2407, 2024. [5] Anthropic. Introducing claude 4. https://www.anthropic.com/news/claude-4, 2025. Accessed: 2024-09-24. [6] OpenAI. Introducing gpt-5. https://openai.com/index/introducing-gpt-5/, 2025. Accessed: 2024-09-24. [7] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. [8] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023."
        },
        {
            "title": "Lab",
            "content": "[9] Mathematical Association of America. American invitational mathematics examination. https: //www.maa.org/math-competitions/aime, 2025. Annual mathematics competition for high school students. [10] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. 2021. [11] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37:5204052094, 2024. [12] Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023. [13] Frank F. Xu, Yufan Song, Boxuan Li, Yuxuan Tang, Kritanjali Jain, Mengxue Bao, Zora Z. Wang, Xuhui Zhou, Zhitong Guo, Murong Cao, Mingyang Yang, Hao Yang Lu, Amaad Martin, Zhe Su, Leander Maben, Raj Mehta, Wayne Chi, Lawrence Jang, Yiqing Xie, Shuyan Zhou, and Graham Neubig. Theagentcompany: Benchmarking llm agents on consequential real world tasks, 2024. [14] Huan-ang Gao, Jiayi Geng, Wenyue Hua, Mengkang Hu, Xinzhe Juan, Hongzhang Liu, Shilong Liu, Jiahao Qiu, Xuan Qi, Yiran Wu, et al. survey of self-evolving agents: On path to artificial super intelligence. arXiv preprint arXiv:2507.21046, 2025. [15] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. In The eleventh international conference on learning representations, 2022. [16] Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric Xing, and Zhiting Hu. Promptagent: Strategic planning with language models enables expert-level prompt optimization. arXiv preprint arXiv:2310.16427, 2023. [17] Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt optimization with\" gradient descent\" and beam search. arXiv preprint arXiv:2305.03495, 2023. [18] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023. [19] Zhiyong Wu, Chengcheng Han, Zichen Ding, Zhenmin Weng, Zhoumianze Liu, Shunyu Yao, Tao Yu, and Lingpeng Kong. Os-copilot: Towards generalist computer agents with self-improvement. arXiv preprint arXiv:2402.07456, 2024. [20] Chen Qian, Jiahao Li, Yufan Dang, Wei Liu, YiFei Wang, Zihao Xie, Weize Chen, Cheng Yang, Yingli Zhang, Zhiyuan Liu, et al. Iterative experience refinement of software-developing agents. arXiv preprint arXiv:2405.04219, 2024. [21] Xiangru Tang, Tianrui Qin, Tianhao Peng, Ziyang Zhou, Daniel Shao, Tingting Du, Xinming Wei, He Zhu, Ge Zhang, Jiaheng Liu, et al. Agent kb: hierarchical memory framework for cross-domain agentic problem solving. In ICML 2025 Workshop on Collaborative and Federated Agentic Workflows."
        },
        {
            "title": "Lab",
            "content": "[22] Jiahao Qiu, Xuan Qi, Tongcheng Zhang, Xinzhe Juan, Jiacheng Guo, Yifu Lu, Yimin Wang, Zixin Yao, Qihan Ren, Xun Jiang, et al. Alita: Generalist agent enabling scalable agentic reasoning with minimal predefinition and maximal self-evolution. arXiv preprint arXiv:2505.20286, 2025. [23] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652, 2023. [24] Xuechen Liang, Meiling Tao, Yinghui Xia, Jianhui Wang, Kun Li, Yijin Wang, Yangfan He, Jingsong Yang, Tianyu Shi, Yuantao Wang, et al. Sage: Self-evolving agents with reflective and memory-augmented abilities. Neurocomputing, page 130470, 2025. [25] Naveen Krishnan. Ai agents: Evolution, architecture, and real-world applications. arXiv preprint arXiv:2503.12687, 2025. [26] Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, PierreEmmanuel Mazaré, Maria Lomeli, Lucas Hosseini, and Hervé Jégou. The faiss library. arXiv preprint arXiv:2401.08281, 2024. [27] Jim Webber. programmatic introduction to neo4j. In Proceedings of the 3rd annual conference on Systems, programming, and applications: software for humanity, pages 217218. ACM, 2012. [28] Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav. Mem0: arXiv preprint Building production-ready ai agents with scalable long-term memory. arXiv:2504.19413, 2025. [29] Rana Salama, Jason Cai, Michelle Yuan, Anna Currey, Monica Sunkara, Yi Zhang, and Yassine Benajiba. Meminsight: Autonomous memory augmentation for llm agents. arXiv preprint arXiv:2503.21760, 2025. [30] Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. Expel: Llm agents are experiential learners. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1963219642, 2024. [31] Zora Zhiruo Wang, Jiayuan Mao, Daniel Fried, and Graham Neubig. Agent workflow memory. arXiv preprint arXiv:2409.07429, 2024. [32] Runnan Fang, Yuan Liang, Xiaobin Wang, Jialong Wu, Shuofei Qiao, Pengjun Xie, Fei Huang, Huajun Chen, and Ningyu Zhang. Memp: Exploring agent procedural memory. arXiv preprint arXiv:2508.06433, 2025. [33] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018. [34] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768, 2020. [35] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35:2074420757, 2022. [36] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. Fever: large-scale dataset for fact extraction and verification. arXiv preprint arXiv:1803.05355, 2018. [37] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards generalist agent for the web. Advances in Neural Information Processing Systems, 36:2809128114, 2023. [38] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023."
        },
        {
            "title": "Lab",
            "content": "[39] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789, 2023. [40] Shishir Patil, Tianjun Zhang, Xin Wang, and Joseph Gonzalez. Gorilla: Large language model connected with massive apis. Advances in Neural Information Processing Systems, 37:126544126565, 2024. [41] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. [42] Aditya Bharat Soni, Boxuan Li, Xingyao Wang, Valerie Chen, and Graham Neubig. Coding agents with multimodal browsing are generalist problem solvers, 2025. [43] Xingyao Wang, Boxuan Li, Yufan Song, Frank Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, et al. Openhands: An open platform for ai software developers as generalist agents. arXiv preprint arXiv:2407.16741, 2024. [44] Mengkang Hu, Yuhang Zhou, Wendong Fan, Yuzhou Nie, Bowei Xia, Tao Sun, Ziyu Ye, Zhaoxuan Jin, Yingru Li, Qiguang Chen, Zeyu Zhang, Yifeng Wang, Qianshuo Ye, Bernard Ghanem, Ping Luo, and Guohao Li. Owl: Optimized workforce learning for general multiagent assistance in real-world task automation, 2025. [45] Browser-Use. Browser-use. https://github.com/browser-use/browser-use, 2025. Accessed: 2025-08-23."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Selected Task Splits In constructing the task splits, we followed clear selection criteria. For the continuous learning task set Tcl, we chose tasks of moderate difficulty, specifically those where models generally achieve non-zero scores in the official TAC evaluation, ensuring that they are neither trivial nor impossible. For the Thard task set, we deliberately selected tasks where most models fail almost completely, i.e., tasks that typically yield near-zero scores, to better stress-test the limits of the agents. In both cases, we ensured comprehensive coverage across the six professional domains defined in TAC, and every task was manually inspected to guarantee correctness and to exclude cases with obvious errors. We list these tasks in detail in Table 5. Table 5: Task Sets Overview Set Name Task Name admin-check-employees-budget-and-reply-and-record admin-read-survey-and-summarise ds-sql-exercise ds-answer-spreadsheet-questions ds-visualize-data-in-pie-and-bar-chart finance-check-attendance-payroll finance-budget-variance hr-collect-feedbacks hr-new-grad-job-description-3 hr-transfer-group hr-check-attendance-multiple-days-department-with-chat pm-create-channel-message-medium pm-update-plane-issue-from-gitlab-status pm-ask-for-issue-and-create-in-gitlab pm-check-backlog-update-issues sde-update-dev-document sde-update-issue-status-on-plane sde-add-all-repos-to-docs admin-mass-forms-filling ds-calculate-spreadsheet-stats ds-predictive-modeling finance-invoice-matching finance-nonqualified-bill-ask-for-reimburse hr-mass-survey hr-internal-tooling-slides hr-salary-analysis pm-present-engineer-group-members sde-copy-table-from-pdf-to-xlsx sde-sotopia-create-agent-wo-repo sde-create-commit-table-for-all-gitlab-users Tcl (18) Thard (12) A.2 Case Studies In this section, we present two representative case studies that illustrate both the inherent complexity of TAC tasks and the operating mechanisms of our framework. These cases reveal unexpected yet effective completion paths that highlight the adaptive problem-solving capabilities of our agent."
        },
        {
            "title": "Lab",
            "content": "Figure 4 shows the first case study, which involves task requiring the agent to collect performance feedback on Liu Qiang from three colleagues. The standard approachand the one implicit in TAC evaluation protocolswould involve conducting sequential individual conversations with each colleague. Nevertheless, our agent adopted an alternative strategy during the planning phase and achieved task completion, yet this innovative solution fell outside the scope of TAC evaluation protocol. Specifically, it created multi-person chat group and simultaneously queried all three colleagues. This innovative approach significantly streamlined the information collection process. Ultimately, the agent accurately integrated the feedback into comprehensive performance evaluation for Liu Qiang, demonstrating its efficiency and adaptability in task execution. The second case study presented in Figure 5 demonstrates the agents sophisticated capabilities in processing dynamically acquired information and executing complex, long-horizon cross-platform tasks. In this scenario, the agent orchestrated nearly 20 sub-tasks and performed over 100 actions. The primary objective was to create new issue in the RisingWave project on GitLab. However, the task description also implied that the task would be assigned to an engineer named Li Ming. This overloaded task was obviously not considered by the TAC, because they did not set up GitLab account for Li Ming. The task posed two major challenges: 1) To gather the comprehensive details necessary for issue creation, the agent needed to sequentially consult three different colleagues. However, the initial task provided only single contact person (Li Ming). The agent had to progressively identify and locate the other two relevant colleagues through interactions with Li Ming, while continuously adapting and refining its sub-task queue in real-time. 2) The agent attempted to assign the issue to Li Ming but could not find his GitLab account, which was an important prerequisite. The agent discovered the problem and decided to create GitLab account for Li Ming. The agent then performed series of actions, including creating the account and adding Li Ming to the appropriate project team, ultimately successfully assigning the issue to him. Despite the inherent challenges and profound complexity of the task, the agent demonstrates remarkable capacity for autonomous adaptation. Through dynamic sub-task planning, selfreflection, and continuous optimization via trial and error, it not only achieves but even exceeds its original task objectives."
        },
        {
            "title": "Lab",
            "content": "Figure 4: Case study on task hr-collect-feedbacks"
        },
        {
            "title": "Lab",
            "content": "Figure 5: Case study on task pm-ask-for-issue-and-create-in-gitlab"
        },
        {
            "title": "Lab",
            "content": "A.3 Exemplary Demonstration of the Memory Module In this section, we provide concrete illustration of the three memory types in our framework. We present selected entries from the Strategic Memory  (Table 6)  , Procedural Memory  (Table 7)  , and Tool Memory  (Table 8)  to demonstrate their structure and content. These examples are shown in their native, structured format. Table 6: Examples of Mstrat, outlining high-level principles for robust agent behavior. Principle Description Systemic Root Cause Robust Context State Adaptive Task Progression Problem Decomposition Diagnose and address underlying systemic causes of recurring errors to refine methods and ensure long-term stability beyond symptom treatment. Explicitly manage and continuously verify data and execution context throughout its lifecycle to ensure accuracy, consistency, and integrity of dependencies and prevent errors. Implement primary strategies with adaptive fallbacks and dynamically provision prerequisites, ensuring continuous progression and reliable state transitions even when initial paths are blocked. Decompose complex problems into modular, manageable units, defining clear goals and objectives to structure stable and logical execution path. Iterative Data Extraction Granular Outcome Verification After critical state-changing actions, perform detailed, item-by-item verification of all intended outcomes to detect subtle discrepancies and ensure system state precisely matches requirements. Employ adaptive search heuristics and staged parsing strategies, including resilient capture mechanisms, to reliably extract, validate, and process information from dynamic or complex data sources. Employ flexible methods or custom logic to achieve exact output specifications, ensuring the final representation precisely matches requirements, intent, and diverse formats. Explicit Uncertainty Handle When required information is unextractable or unverifiable, explicitly assign clear Not Available or equivalent status to prevent hallucination and maintain data integrity. Strictly distinguish and manage execution environments for different types of code or tools (e.g., shell vs. Python) to prevent conflicts and ensure proper, intended execution. Accurate Output Specification Clear Environment Separate"
        },
        {
            "title": "Lab",
            "content": "Table 7: Examples of Mproc, detailing step-by-step guides for common application interactions. Application Function Details RocketChat Navigate Home Page to Login FileSystem Create or Overwrite File Verify File Existence OwnCloud Login Navigate Folder URL to by Preconditions: User is logged into RocketChat. (Optional: Browser is open). Steps: Refresh the page state using browser_update If already at /home, verify. Otherwise, click the Home link or navigate directly to http://xxx/home, refresh and verify elements like Home button, Channels list, or avatar. Notes: Always follow navigation with browser_update. The Home link may be more reliable than button depending on UI context. Preconditions: Browser is open, RocketChat URL and credentials are known. Steps: Navigate to login page Enter username in Email or username field Enter password in Password field Click Login button Verify login success (URL is /home, login fields disappear, post-login elements appear). Notes: Successful login is confirmed by disappearance of input fields and presence of post-login UI. Always perform browser_update after clicks. Preconditions: None. Steps: Define content string and target file_path in Python Use with open(file_path, f.write(content) Include error handling w) as f: (try-except) to manage failures. Notes: Python file handling (open, write) is more robust than using shell commands (e.g., echo) due to escaping issues. Preconditions: None. Steps: (Python) Import os iterate file paths check with os.path.exists() print perfile and summary results. (Alternative) Use run_cmd with ls <file> verify from returncode and output. Notes: os.path.exists is suitable for programmatic checks; ls is effective for CLI verification with error messages. Preconditions: Browser open, ownCloud URL & credentials available, service reachable. Steps: Go to login URL handle connection errors (ping) if needed refresh page enter username/password via input fields click Log in verify login success (URL change, disappearance of fields, presence of post-login elements) if modal appears, dismiss via Escape. Notes: Always pair navigation/input/click with browser_update. Use attributes like placeholder/text to locate elements. Verify success via URL and post-login UI, not just button clicks. Preconditions: Browser is open and authenticated. Steps: Navigate to folder URL refresh state verify URL and page title dismiss modal (if any) via Escape confirm presence of expected files in accessibility tree/interactive elements. Notes: Direct URL navigation is more reliable than clicking folder links. Always verify URL, title, and file list after navigation. ... (additional entries omitted)"
        },
        {
            "title": "Lab",
            "content": "Table 8: Examples of Mtool, providing optimized tool instructions and description. Tool Description Instruction access_guide Review the returned guide carefully. Compare actively with real-time UI observations. - If discrepancies appear, prioritize adaptive exploration. - Always check parameter names and types (dict for batch_requests, for application_name) to avoid TypeError. str Get detailed platform/application operation guides. The guide is structured from past successful experiences. - Primary mode: batch_requests, or multisupporting single ple apps/items. - Example: batch_requests={RocketChat: [Login, Create Channel]}. app/item: single For - application_name=\"RocketChat\" or with item_names. - Crucial: Always pass app name as dict key and items as list in batch_requests, otherwise TypeError. - Absence of requested guide entries is also useful signal. - Guides may not match current UI exactly; adapt as needed. browser_click Click interactive element on current browser page by index. - Always call browser_update first to ensure fresh indices. - Best practice: follow click with browser_update to confirm changes. - Prioritize semantic attributes (e.g., text, aria-label) over raw indices. - If clicking fails, dynamically search for element attributes. - Verify intended outcome (navigation, modal opening, state change), not just the click itself. - For persistent failures, consider browser_send_keys(Enter). browser_input text specified - Always first with into Enter element. browser refresh elements browser_wait_and_get_update. - Input appends text; clear field with \"\" if needed. - After input, changes may not persist without explicit Save/Submit. - Consider following input with browser_update to catch UI changes. with follow Always browser_update. - Verify outcome (e.g., page change, modal open). - If action fails, refresh interactive elements and retry with semantic attributes. - If still failing, try browser_send_keys. - For unclickable elements, acknowledge task may be unachievable and adjust strategy. input, elements always call After browser_update. - Re-check interactive for changes. - If part of form, explicitly locate and click Save/Submit. - Verify that the input was successfully saved. ... (additional entries omitted)"
        },
        {
            "title": "Lab",
            "content": "A.4 Tool Set We equip MUSE with minimal yet sufficient tool set, consisting of browser operator, Python interpreter, shell, visual extractor, and memory retriever. The browser operator is primarily implemented based on the browser-use framework [45] , enhanced with both the accessibility (a11y) tree and the pages interactive elements as observations returned to the agent. The visual extractor leverages GPT-4o as the backbone model. complete overview of the tool set is provided in Table 9. Tool run_cmd run_python_code access_guide gpt4o_describe_image browser_go_to_url browser_input browser_send_keys browser_update browser_click browser_extract_content_by_vision browser_close_tab browser_go_back browser_list_tabs browser_switch_tab Table 9: Tool Set. Function Execute full shell command string and return its result, suitable for file and system operations. Execute Python code in an isolated environment for data processing and analysis. Retrieve structured procedural memory for accurate interaction. Use GPT-4o to recognize and interpret the content of images. Navigate the browser to specified URL, supporting page refresh and reset. Input text into specified field in the current browser page. Send keyboard shortcuts or keystrokes (e.g., Enter) to the current browser tab. Wait and refresh to retrieve the latest accessibility tree and interactive elements. Click specified interactive element in the current browser page by index. Extract specified content from browser screenshot using GPT-4o. Close specified browser tab by index. Navigate back in the browser history of the current tab. List all currently open browser tabs. Switch to specified browser tab by index."
        },
        {
            "title": "Lab",
            "content": "A.5 Complete Task-level Results Table 10 shows the scores of all TAC tasks. The overall scores and experimental analysis are shown in Section 4.3.3. Table 10: Detailed results on the complete TAC benchmark for 175 tasks. Task checkpoint Spartial (%) admin-arrange-meeting-rooms admin-ask-for-meeting-feedback admin-ask-for-upgrade-reimbursement admin-check-employees-budget-and-reply admin-check-employees-budget-and-reply-2 admin-check-employees-budget-and-reply-and-record admin-collect-requests-and-compute-total-price admin-employee-info-reconciliation admin-get-best-vendor-quote admin-make-spreadsheet admin-mass-forms-filling admin-read-survey-and-summarise admin-remove-pages-pdf admin-translate-sales-chat admin-watch-video bm-classify-nationality ds-answer-numerical-data-question ds-answer-spreadsheet-questions ds-calculate-spreadsheet-stats ds-coffee-shop-database-management ds-find-meeting-spreadsheet ds-fix-table-values-and-missing-answers ds-format-excel-sheets ds-janusgraph-exercise ds-merge-multiple-sheets ds-organise-report-sus-data ds-predictive-modeling ds-sql-exercise ds-stock-analysis-slides ds-visualize-data-in-pie-and-bar-chart example finance-apply-tax-credit finance-budget-variance finance-check-attendance-payroll finance-create-10k-income-report 23 0/2 6/6 2/4 4/4 4/4 6/6 1/4 5/7 5/6 0/5 0/5 2/3 1/3 0/4 0/2 2/6 0/6 5/5 2/5 4/10 1/2 6/6 3/4 1/6 1/3 3/5 3/3 6/6 1/8 4/4 3/5 0/8 4/4 3/3 1/6 0.0 100.0 25.0 100.0 100.0 100.0 12.5 35.71 41.67 0.0 0.0 33.33 16.67 0.0 0.0 16.67 0.0 100.0 20.0 20.0 25.0 100.0 37.5 8.33 16.67 30.0 100.0 100.0 6.25 100.0 30.0 0.0 100.0 100.0 8."
        },
        {
            "title": "Lab",
            "content": "Task checkpoint Spartial (%) finance-expense-validation finance-find-signatories finance-invoice-matching finance-nonqualified-bill-ask-for-reimburse finance-qualified-bill-ask-for-reimburse finance-r-d-activities finance-revenue-reconciliation finance-substantial-presence-test hr-analyze-outing-bills hr-check-attendance-multiple-days hr-check-attendance-multiple-days-department hr-check-attendance-multiple-days-department-with-chat hr-check-attendance-one-day hr-check-for-invalid-passwords-and-ask-for-valid-passwords hr-collect-feedbacks hr-collect-multiple-valid-passwords hr-create-career-ladder hr-create-employee-manual hr-delete-and-insert-user hr-get-valid-password hr-green-card-consultation hr-internal-tooling-slides hr-make-slides-introduce-leadership hr-mass-survey hr-massive-resume-screening hr-new-grad-job-description hr-new-grad-job-description-2 hr-new-grad-job-description-3 hr-organize-talent-info hr-pick-interviewer-1 hr-pick-interviewer-2 hr-pick-interviewer-3 hr-populate-salary-increase-memo hr-resume-categorization hr-resume-screening hr-salary-analysis hr-transfer-group ml-generate-gradcam ml-grade-exam pm-add-new-moderator 24 2/4 2/5 1/5 2/2 2/5 1/6 1/4 1/2 3/7 1/4 0/3 2/4 3/3 4/4 5/5 2/4 4/4 1/4 3/3 4/4 3/3 6/10 5/5 1/7 5/5 3/3 4/4 5/5 1/4 6/6 4/6 1/4 4/7 1/4 4/4 0/2 1/3 1/4 1/8 3/ 25.0 20.0 10.0 100.0 20.0 8.33 12.5 25.0 21.43 12.5 0.0 25.0 100.0 100.0 100.0 25.0 100.0 12.5 100.0 100.0 100.0 30.0 100.0 7.14 100.0 100.0 100.0 100.0 12.5 100.0 33.33 12.5 28.57 12.5 100.0 0.0 16.67 12.5 6.25 100."
        },
        {
            "title": "Lab",
            "content": "Task checkpoint Spartial (%) pm-ask-for-issue-and-create-in-gitlab pm-ask-issue-assignee-for-issue-status-and-update-in-plane pm-assign-issues pm-change-channel-ownership pm-check-backlog-update-issues pm-copy-plane-issues-to-gitlab pm-create-channel-message pm-create-channel-message-medium pm-create-channel-new-leader pm-create-plane-issue pm-create-teammate-channel-from-spreadsheet pm-distribute-information pm-monitor-new-bug-issues pm-monthly-attendance-slides pm-plan-personnel-for-new-project pm-prepare-meeting-with-customers pm-present-engineer-group-members pm-present-gitlab-info-as-ppt pm-projects-analytics pm-schedule-meeting-1 pm-schedule-meeting-2 pm-send-hello-message pm-send-notification-to-corresponding-user pm-update-gitlab-issue-from-plane-status pm-update-plane-issue-from-gitlab-status pm-update-project-milestones pm-update-sprint-cycles qa-escalate-emergency qa-update-issue-status-according-to-colleagues research-answer-questions-on-paper research-reproduce-figures sde-add-all-repos-to-docs sde-add-one-gitlab-pipeline sde-add-wiki-page sde-change-branch-policy sde-change-license-easy sde-change-license-hard sde-check-and-run-unit-test sde-check-high-priority-issue sde-close-all-gitlab-issues 25 5/5 3/3 5/5 3/3 1/5 3/4 3/3 6/6 2/3 2/2 4/5 2/2 2/4 4/4 3/7 6/6 0/3 5/5 2/5 5/5 5/5 4/5 4/4 2/3 7/7 5/5 3/4 2/3 6/6 10/12 4/8 4/7 0/3 4/4 2/2 4/4 2/3 1/2 1/4 2/ 100.0 100.0 100.0 100.0 10.0 37.5 100.0 100.0 33.33 100.0 40.0 100.0 25.0 100.0 21.43 100.0 0.0 100.0 20.0 100.0 100.0 40.0 100.0 33.33 100.0 100.0 37.5 33.33 100.0 41.67 25.0 28.57 0.0 100.0 100.0 100.0 33.33 25.0 12.5 100."
        },
        {
            "title": "Lab",
            "content": "Task checkpoint Spartial (%) sde-close-all-issue-on-all-project-under-tac-workspace sde-close-all-prs sde-close-an-issue sde-collect-open-issues sde-copilot-arena-server-easy-add-suffix sde-copilot-arena-server-new-endpoint sde-copilot-arena-server-setup sde-copy-issues-to-plane sde-copy-table-from-pdf-to-xlsx sde-create-commit-table-for-all-gitlab-users sde-create-new-characters sde-create-new-gitlab-project-logo sde-create-new-release sde-create-new-repo sde-create-sqlite-database sde-debug-crashed-server sde-delete-all-project-under-plane sde-delete-all-repos sde-delete-stale-branch sde-dependency-change-1 sde-find-answer-in-codebase-1 sde-find-answer-in-codebase-2 sde-find-answer-in-codebase-3 sde-find-api sde-fix-factual-mistake sde-fix-rising-wave-datatype sde-implement-buffer-pool-manager-bustub sde-implement-covering-index-in-janusgraph sde-implement-hyperloglog sde-implement-raft-in-go sde-install-go sde-install-openjdk sde-issue-label-management sde-migrate-package-manager sde-milestone-meeting sde-move-bustub-wiki sde-move-page-to-cloud sde-pitch-idea-to-manager sde-reply-community-issue-by-asking-npc sde-reply-community-issue-with-fixed-reply 26 2/3 2/2 2/2 3/3 4/4 9/9 7/7 2/2 2/5 1/6 2/4 2/3 2/2 2/3 6/8 2/8 0/1 1/1 2/2 5/5 0/3 3/3 2/5 2/4 3/3 2/5 1/12 0/3 1/6 0/10 0/2 2/2 0/1 0/8 2/5 3/4 2/3 5/5 5/5 3/ 33.33 100.0 100.0 100.0 100.0 100.0 100.0 100.0 20.0 8.33 25.0 33.33 100.0 33.33 37.5 12.5 0.0 100.0 100.0 100.0 0.0 100.0 20.0 25.0 100.0 20.0 4.17 0.0 8.33 0.0 0.0 100.0 0.0 0.0 20.0 37.5 33.33 100.0 100.0 100."
        },
        {
            "title": "Lab",
            "content": "Task checkpoint Spartial (%) sde-repo_profile_pic sde-report-agent-repos sde-report-unit-test-coverage-to-plane sde-run-all-unit-test sde-run-janusgraph sde-run-linter-on-openhands sde-run-rising-wave-locally sde-sotopia-create-agent sde-sotopia-create-agent-wo-repo sde-sotopia-dev-container sde-sotopia-update-ci sde-summarize-recent-issues sde-sync-from-origin-repo sde-troubleshoot-dev-setup sde-update-dev-document sde-update-issue-status-on-plane sde-update-readme sde-write-a-unit-test-for-append_file-function sde-write-a-unit-test-for-scroll_down-function sde-write-a-unit-test-for-search_file-function 1/3 0/2 3/4 3/4 1/6 0/2 2/2 5/5 2/6 2/7 1/3 4/4 1/1 1/4 4/4 3/3 2/2 2/5 2/5 2/5 16.67 0.0 37.5 37.5 8.33 0.0 100.0 100.0 16.67 14.29 16.67 100.0 100.0 12.5 100.0 100.0 100.0 20.0 20.0 20."
        }
    ],
    "affiliations": [
        "Central South University",
        "Fudan University",
        "Shanghai Artificial Intelligence Laboratory",
        "Shanghai Innovation Institute",
        "Zhejiang University"
    ]
}