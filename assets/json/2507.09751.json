{
    "paper_title": "Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations",
    "authors": [
        "Bradley P. Allen",
        "Prateek Chhikara",
        "Thomas Macaulay Ferguson",
        "Filip Ilievski",
        "Paul Groth"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have demonstrated impressive capabilities in natural language understanding and generation, but they exhibit problems with logical consistency in the output they generate. How can we harness LLMs' broad-coverage parametric knowledge in formal reasoning despite their inconsistency? We present a method for directly integrating an LLM into the interpretation function of the formal semantics for a paraconsistent logic. We provide experimental evidence for the feasibility of the method by evaluating the function using datasets created from several short-form factuality benchmarks. Unlike prior work, our method offers a theoretical framework for neuro-symbolic reasoning that leverages an LLM's knowledge while preserving the underlying logic's soundness and completeness properties."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . [ 1 1 5 7 9 0 . 7 0 5 2 : r Proceedings of Machine Learning Research vol xxx:129, 2025 19th Conference on Neurosymbolic Learning and Reasoning Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations Bradley P. Allen University of Amsterdam, Amsterdam, NL Prateek Chhikara University of Southern California, CA, US Thomas Macaulay Ferguson Rensselaer Polytechnic Institute, Troy, NY, US Filip Ilievski Vrije Universiteit Amsterdam, Amsterdam, NL Paul Groth University of Amsterdam, Amsterdam, NL b.p.allen@uva.nl prateekchhikara24@gmail.com fergut5@rpi.edu f.ilievski@vu.nl p.t.groth@uva.nl Editors: Leilani H. Gilpin, Eleonora Giunchiglia, Pascal Hitzler, and Emile van Krieken Abstract Large language models (LLMs) have demonstrated impressive capabilities in natural language understanding and generation, but they exhibit problems with logical consistency in the output they generate. How can we harness LLMs broad-coverage parametric knowledge in formal reasoning despite their inconsistency? We present method for directly integrating an LLM into the interpretation function of the formal semantics for paraconsistent logic. We provide experimental evidence for the feasibility of the method by evaluating the function using datasets created from several short-form factuality benchmarks. Unlike prior work, our method offers theoretical framework for neuro-symbolic reasoning that leverages an LLMs knowledge while preserving the underlying logics soundness and completeness properties. 1. Introduction LLMs encode vast parametric knowledge (Petroni et al., 2019) yet remain inconsistent and incomplete (Cheng et al., 2025), hindering their use as knowledge bases. Current approaches to logical reasoning with LLMs rely primarily on prompting strategies and external symbolic solvers (Wei et al., 2022; Cheng et al., 2025) but lack formal frameworks for managing the inherent inconsistency and incompleteness of LLM knowledge. Meanwhile, paraconsistent logics (Priest et al., 2025) are multi-valued non-classical logics designed to handle inconsistent information without logical explosion, where any contradiction would make everything provable. Belnap computers (Belnap, 1977a,b) are theoretical constructions described by Nuel Belnap to model contexts in which machines are responsible for reasoning in the face of incomplete or inconsistent information. This raises natural question: can paraconsistent logic and Belnap computers provide principled framework for using LLMs as knowledge bases despite their inconsistency and incompleteness? We propose Belnap computer whose external knowledge source is an LLM judge. LLM judges scale the process of evaluating LLM output for factuality given shortor 2025 B.P. Allen, P. Chhikara, T.M. Ferguson, F. Ilievski & P. Groth."
        },
        {
            "title": "Allen Chhikara Ferguson Ilievski Groth",
            "content": "long-form question answering task, returning truth valuations for one or more statements in the LLMs output (Li et al., 2024). In our approach, an LLM judge responds to query for the valuation of an atomic formula in the context of paraconsistent reasoner with generalized truth value (Shramko and Wansing, 2025, 2011). Generalized truth values allow the LLM judge to provide information to the reasoner not only about the degree of truth of an atomic formula given an LLMs parametric knowledge, but also with respect to the degree of knowledge the LLM has about the formula, in terms of whether it can verify or refute it."
        },
        {
            "title": "Paraconsistent\nreasoner",
            "content": "φ u, v"
        },
        {
            "title": "LLM\njudge",
            "content": "Figure 1: Belnap computer using an LLM judge as source of knowledge. Let be the object language for paraconsistent logic, and let LAT be the set of atomic formulas. paraconsistent reasoner (left) sends an atomic formula φ LAT to the LLM judge (right), which returns generalized truth value u, v, such that indicates if the LLM judge was able to verify φ, and indicates if the LLM judge was able to refute φ. Section 2 discusses related work, and in the following sections we make three key contributions. Section 3 defines (i) bilateral factuality evaluation function that provides useful information beyond that provided by current LLM judge approaches to factuality evaluation. Section 4 then shows how to integrate the function directly into the formal semantics of paraconsistent logic through (ii) an LLM-grounded interpretation that preserves the soundness and completeness of analytic tableau systems for reasoning in the logic. Section 5 provides (iii) empirical evidence that the approach can be practically implemented, presenting findings from an evaluation using benchmarks derived from short-form factuality benchmarks and discussing limitations with potential mitigations. Section 6 concludes with summary and discussion of future work. 2. Related work Logical reasoning with LLMs Current approaches to reasoning with LLMs (Hoppe et al., 2025; Cheng et al., 2025) are illustrated in Figure 2. In prompt-based approach (a), an LLM is prompted with verbalization δ(Γ) Σ of set of formulas Γ, and performs natural language reasoning to produce verbalization δ(φ) of φ (Wei et al., 2022; Kojima et al., 2022; Dhuliawala et al., 2023; Yao et al., 2023). In solver-based approach (b), an LLM is prompted with verbalization of set of formulas and produces set of formulas Γ in an object language L, which reasoner uses to deduce φ (Pan et al., 2023; Olausson et al., 2023; Callewaert et al., 2025). In an approach based on pre-training or fine-tuning (c), reasoner provides training set of proofs Π, and the LLM learns from that to reason as in (a) (Jiao et al., 2023; Morishita et al., 2024; Feng et al., 2024; Liu et al., 2025). 2 Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations δ(Γ) LLM δ({φ}) δ(Γ) Reasoner Γ LLM φ (a) Prompt-based (b) Solver-based δ(Γ) Reasoner Π LLM δ({φ}) Γ Reasoner LLM φ (c) Pre-train/fine tune (d) Interpretation-based Figure 2: Four different approaches to logical reasoning with LLMs. Let be firstorder language, Γ be set of statements in L, φ, ψ be statements in L, be an interpretation for L, and δ : P(L) Σ be verbalization function that takes set of formulas in and returns natural language translation of the formulas. In each approach, we show how reasoning is performed in the context of generating formal or natural language proof showing that Γ φ. Unlike approaches (b) and (c) that use LLMs alongside reasoning systems, in our proposed interpretation-based approach (d) we integrate LLMs directly into the formal semantics of the reasoners logic itself, by using an LLM to implement an interpretation function to be used by the reasoner in inferring φ. This allows us to provide formal guarantees about the soundness and completeness of the reasoning process that incorporates the LLM. Factuality evaluation using LLM judges Factuality evaluation is the assessment of whether the output of language model is factually correct (Wang et al., 2023; Bang et al., 2025). Recent work has focused on LLM judges (Zheng et al., 2023; Zhu et al., 2023) as means to scale factuality evaluation, by prompting an LLM to produce truth value assignment of the output of an LLM that is being evaluated, specifically short- (Wei et al., 2024) or long-form (Jacovi et al., 2025) answer to question. We apply LLM judges to assign generalized truth values to natural language translations of atomic formulas, taking single-answer grading approach (Zheng et al., 2023). In contrast with current approaches to factuality evaluation, the use of generalized truth values provides information as to the LLMs epistemic stance towards the formula in question. Multi-valued logics for paraconsistent reasoning Work building on Belnaps fourvalued semantics has led to the development of range of non-classical paraconsistent logics. Patel-Schneider (1989) showed how this idea could be applied to make terminological logics capable of performing subsumption correctly in the presence of contradictory knowledge. Kamide (2010), Ma et al. (2007), and Maier et al. (2013) expanded on this work to define number of paraconsistent description logics. More recently, Ferguson (2017b) has proposed"
        },
        {
            "title": "Allen Chhikara Ferguson Ilievski Groth",
            "content": "a computational interpretation of versions of first degree entailment (FDE) and Richard Angells logic of analytic containment (AC). This interpretation models reasoners using these logics as Belnap computers, and leads to formal framework for FDE and AC as bilateral logics with sound and complete analytic tableau systems. We show how an LLM judge can be used to provide an interpretation for AC that preserves the soundness and completeness of the tableau system ACrQ, with applications to description logics as described in (Ferguson, 2021a). 3. Bilateral factuality evaluation of an atomic formula using an LLM AC is conceptivist logic (Ferguson, 2017a) that addresses hierarchical relationships between concepts. While in this work we focus on AC as paraconsistent logic, it is also paracomplete, i.e., it rejects the law of excluded middle. The combination of those two properties makes it particularly suitable for applications involving vague predicates, incomplete information, or situations where classical logics demands for both consistency and completeness are too strong. Appendix provides definition of AC with restricted quantification, which is necessary to support concept subsumption and existential quantification of roles when used as description logic (Ferguson, 2021b). This definition treats AC as bilateral logic, i.e., logic which manages values for both the truth and falsity of formula separately. Bilateralism in philosophical logic (Rumfitt, 2000) holds that understanding proposition requires grasping both the conditions under which it can be asserted and the conditions under which it should be denied. We operationalize this principle in the factuality evaluation of atomic formulas using an LLM judge. First, we generate natural language verbalization of an atomic formula. We then prompt the LLM to generate two statements as to the verifiability and refutability of the assertion represented by the verbalization. The statements are then mapped into the set of truth values V3 used in weak Kleene logic (Kleene, 1952; Szmuc, 2019), i.e., (true), (undefined), and (false). Weak Kleene truth values allow us to formalize the semantics of LLM-judge output. For example, the SimpleQA grader used in the SimpleQA benchmark (Wei et al., 2024) grades an LLMs answer to question as either CORRECT, INCORRECT, or NOT ATTEMPTED; the PreciseWikiQA Question Answerability Prompt in the HalluLens benchmark (Bang et al., 2025) uses UNVERIFIABLE instead of NOT ATTEMPTED. We equate CORRECT with and INCORRECT with f; equating with NOT ATTEMPTED or UNVERIFIABLE is consistent with Kleenes original statement that it indicates an absence of information that given formula is either or (Kleene, 1952, p. 333). Finally, we pair the weak Kleene truth value for verifiability with the weak Kleene truth value for refutability to yield generalized truth value u, V3 V3. This type of generalized truth value offers significant advantages over truth valuations provided by current approaches to factuality evaluation using LLM judges: they enable systematic distinctions between different types of evidence (Shramko and Wansing, 2011; Ferguson, 2021b), provide principled methods for handling inconsistent and incomplete information (Shramko and Wansing, 2011; Szmuc, 2019; Ferguson, 2021b; Correia, 2010), and enhance the explanatory transparency of logical valuations through their structured nature (Shramko Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations yearOf Discovery(America, 1492) φ Verbalization σ = δ({φ}) = America was discovered in 1492 f, ζc(φ) Cartesian product f, 7 f, LLM Judge Verification +(σ) = . . . The answer is contextually accurate within the framework of European exploration history, but it does not encompass the full scope of human discovery of the Americas. cannot verify = = Refutation (σ) = . . . While it does not account for earlier discoveries by indigenous peoples or Norse explorers, it is not contradiction to state that America was discovered in 1492 from European historical perspective. cannot refute = = σ σ Figure 3: An example of bilateral factuality evaluation ζc as performed by the LLM judge shown in Figure 1. Let φ LAT be an assertion that the discovery of America occurred in the year 1492. σ = δ({φ}) is the verbalization of φ (Definition 1). The bilateral factuality evaluation of φ by an LLM judge (Definitions 2, 3, and 4) generates the truth value ζc(φ) = f, f. The LLM has in effect identified incompleteness in its knowledge based on differing perspectives on who discovered America, it can neither verify nor refute φ. and Wansing, 2011; Ferguson, 2021b; Fine, 2016). Figure 3 shows an example of this process in action; longer example is provided in Appendix C. Preliminaries Let Σ be countable set of tokens and Σ be the set of finite sequences of tokens t0 . . . tk, where t0ik Σ, N. Given sequences σ, σ Σ, σ σ iff σ is proper contiguous subsequence of σ. Let LC be an LLM trained on corpus P(Σ). Definition 1 verbalization function δ : P(L) Σ is total function that maps set of formulas to sequence of tokens. In practice, δ can be implemented in number of ways: for example, through templatebased approach (Ell and Harth, 2014), by prompting an LLM to generate verbalization of the set of formulas (Perevalov and Both, 2024), or simply providing the formulas directly in the syntax of the given object language. Definition 2 verification function + : Σ Σ prompts LC to take verbalization of an atomic formula φ LAT and generate token sequence σ+ that states that φ is verified or that it cannot be verified. Definition 3 refutation function : Σ Σ prompts LC to take verbalization of an atomic formula φ LAT and generate token sequence σ that states that φ is refuted or that it cannot be refuted."
        },
        {
            "title": "Allen Chhikara Ferguson Ilievski Groth",
            "content": "Definition 4 bilateral factuality evaluation function ζ : LAT V3 V3 is total function that given an atomic formula φ LAT yields pair u, where: = if VERIFIED +(δ(φ)) if CANNOT VERIFY +(δ(φ)) otherwise = if REFUTED (δ(φ)) if CANNOT REFUTE (δ(φ)) otherwise The otherwise cases in Definition 4 reflect the possibility of LLMs failing to output tokens indicating the state of verification or refutation, for example, by failing to follow prompt instructions, or timing out during call to an inference API. Additionally, in our implementation, we use repeated sampling with majority vote (Brown et al., 2024) to determine each of the truth value components; other approaches to hallucination mitigation, such as chain-of-verification (Dhuliawala et al., 2023), are admissible as well. The definition of ζ leaves open the possibility that multiple calls over time might return different truth values. Analytic tableau reasoning assumes that atomic formulas are assigned truth value that is stable within the scope of the reasoning process. We ensure this by using caching version of the bilateral evaluation function ζc where valuations for atomic formulas are persistently and immutably stored. This type of caching is consistent with the range of optimization techniques used in description logics tableau reasoners (Gore and Nguyen, 2007; Nguyen, 2009). Definition 5 caching bilateral factuality evaluation function ζc : LAT V3 V3 is total function defined as: ζc(φ) = (c(φ) if φ dom(c) ζ(φ) otherwise, and := {(φ, ζ(φ))} where is persistent and immutable cache mapping atomic formulas to truth value pairs. Having established bilateral evaluation, we now show how this enables LLM judges to implement interpretation functions directly. 4. LLM-grounded interpretations We formalize how the bilateral evaluation function ζc is integrated into the definition of an interpretation for AC, show that for every LLM-grounded AC interpretation there is an equivalent standard AC interpretation, and then show that soundness and completeness of the tableau-style analytic calculus ACrQ defined in Definition 18 of Ferguson (2021b) is preserved when we adopt an LLM-grounded interpretation. Definition 6 An LLM-grounded AC interpretation = CI, RI is an AC interpretation such that for every function RI RI and cI 1 , . . . , cI CI: ) = ζc(R(c1, . . . , cn)) RI(cI 1 , . . . , cI 6 Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations Lemma 7 (Stability of LLM-grounded interpretations) For any LLM-grounded AC interpretation and atomic formula φ LAT : 1. I(φ) is well-defined and yields exactly one pair u, V3 V3 2. Once computed, I(φ) remains constant throughout the reasoning process Proof The stability follows directly from Definition 5 of ζc. Let t0 be the time at which ζ is first called to set c(φ). If ζc(φ) = u, at time t0, then for all subsequent calls at > t0, ζc(φ) = u, v. This ensures that once an atomic formula φ has been evaluated and the returned pair u, V3 V3 has been cached, all subsequent evaluations will return the same pair from the cache, guaranteeing stability. Lemma 7 effectively states that the logical validity of derivations depends on the structure of the logical operators rather than the specific content of atomic propositions, which is fundamental principle in formal logic. The tableau method only depends on the truthfunctional behavior of the logical connectives, which remains unchanged between standard and LLM-grounded interpretations. Lemma 8 (LLM-grounded to standard interpretation mapping) For any LLM-grounded AC interpretation = CI, RI, there exists standard AC interpretation that preserves the semantic behavior of on all formulas. Proof Given an LLM-grounded AC interpretation = CI, RI, we define standard AC interpretation = CI, RI such that: 1. CI = CI 2. RI = RI 3. For all R, RI(cI 4. For all C, cI = cI 1 , . . . , cI ) = RI(cI 1 , . . . , cI ) We show that for any formula φ L, I(φ) = (φ) by induction on the complexity of φ: For φ = R(c1, . . . cn) LAT , then by Definition 6, I(φ) = I(R(c1, . . . cn)) = RI(cI 1 , . . . , cI ) = RI(cI 1 , . . . , cI ) = (R(c1, . . . cn)) = (φ). For φ = ψ, by Definition 16, I(ψ) = I1(ψ), I0(ψ) and (ψ) = I1(ψ), I0(ψ). By the inductive hypothesis, I(ψ) = (ψ). Therefore I(ψ) = (ψ). For φ = ψχ, by Definition 16, I(ψχ) = I0(ψ) I0(χ), I1(ψ) I1(χ) and similarly for . By the inductive hypothesis, I(ψ) = (ψ) and I(χ) = (χ). Therefore I(ψ χ) = (ψ χ). The same arguments apply in the cases of disjunction, restricted universal quantification, and restricted existential quantification, again following Definition 16 in Appendix A. Lemma 9 (Standard to LLM-grounded interpretation mapping) For any standard AC interpretation = CI, RI, there exists an LLM-grounded AC interpretation that preserves the semantic behavior of on all formulas."
        },
        {
            "title": "Allen Chhikara Ferguson Ilievski Groth",
            "content": "Proof Let = CI, RI be standard AC interpretation. Define key-value store cI such that for all atomic R(c1, ..., cn) AT , c(R(c1, ..., cn)) = RI(cI ). Then cI induces caching bilateral factuality evaluation function ζcI such that the induced LLM-grounded AC interpretation agrees with on all atoms. Consequently, an induction along the lines of that in Lemma 8 ensures agreement. 1 , ..., cI Note that as corollary of Lemmas 8 and 9, Γ AC φ holds if and only if the inference from Γ to φ is valid over all LLM-grounded AC interpretations. This allows the following two theorems: Theorem 10 (Preservation of soundness) Let Γ be finite set of formulas and φ formula in AC. If Γ ACrQ φ, then Γ =I φ is valid for all LLM-grounded AC interpretations I. Proof By Theorem 3 of Ferguson (2021b), if Γ ACrQ φ, then Γ =AC φ. Therefore Γ =I φ, as is the case for any AC interpretation, standard or LLM-grounded. Theorem 11 (Preservation of completeness) Let Γ be finite set of formulas and φ formula in AC. Then if for all LLM-grounded AC interpretations I, Γ =I φ, then Γ ACrQ φ. Proof By Lemma 8, given I, we can construct standard AC interpretation such that if Γ =I φ then Γ =I φ. By the established completeness theorem for standard AC interpretations (Theorem 4 of Ferguson (2021b)), if Γ =I φ, then Γ ACrQ φ. 5. Evaluation Data and metrics To validate the feasibility of LLM-grounded interpretations, we evaluate the bilateral evaluation function ζ that underlies them, using question/answer pairs from two short-form factuality benchmarks. Question/answer pairs in short-form factuality benchmarks are typically factoids providing question together with short answer (Figure 3). We use these as an approximation to the verbalization δ(φ) of an atomic formula φ. We used the short-form factuality benchmarks GPQA (Rein et al., 2023) and SimpleQA (Wei et al., 2024) to create two balanced test datasets (each N=400) for our experiments. Test data preparation and experimental setup are discussed in Appendix D. We evaluated the performance of ζ over the two datasets using two metrics: macro F1 against question/answer pairs that the judge did not abstain from, i.e., where the judge provided valuation of ζ(φ) = t, (i.e., verified and not refuted) or ζ(φ) = f, (i.e., not verified and refuted), and coverage, which is the percentage of the total set of question/answer pairs where the judge did not abstain. We also measured the time taken per evaluation, and the number of tokens used per evaluation. 8 Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations Bilateral (ζ) Unilateral Dataset Model Type Macro F1 Coverage Macro F1 Coverage GPQA flagship distilled 0.699 (0.010) 0.608 (0.011) 0.589 (0.008) 0.504 (0.008) 0.633 (0.007) 0.559 (0.008) 1.000 (0.000) 1.000 (0.000) SimpleQA flagship distilled 0.736 (0.009) 0.624 (0.011) 0.584 (0.008) 0.499 (0.008) 0.657 (0.008) 0.570 (0.008) 1.000 (0.000) 1.000 (0.000) Table 1: Summary macro F1 (given abstention) and coverage metrics for the bilateral factuality evaluation function ζ and baseline unilateral factuality evaluation function. LLM judges We used three flagship LLMs (Llama 4 Maverick, GPT-4o, and Claude 3.5 Sonnet), and three distilled LLMs (Llama 4 Scout, GPT-4o Mini, and Claude 3.5 Haiku). Each LLM was evaluated using three different pairs of prompts: direct prompts that asked for verification or refutation for the question/answer pair, zero-shot chain-of-thought prompts, and few-shot chain-of-thought prompts. As baseline, we also used the six models and three prompt types to perform unilateral factuality evaluation, which prompts an LLM to simply determine whether question/answer pair is or f. The prompt templates used are provided in Appendix B. Standard errors (in parentheses) presented in tables in this section and in Appendix were estimated by bootstrap resampling: 1000 subsamples of size N=100 were drawn from the classification results within each model type category (Politis and Romano, 1994). Results Table 1 compares macro F1 and coverage between the unilateral and bilateral evaluations across the two datasets, grouped by whether models type was flagship or distilled, and Table 2 does the same for mean time of execution and mean tokens used. Table 3 summarizes the distribution of truth values produced in bilateral evaluation. Detailed breakouts are shown in the tables in Appendix D. Our key findings are as follows. 1. Bilateral evaluation macro F1 outperforms unilateral evaluation (p < 0.01) at the cost of lower coverage. The mean difference between bilateral and unilateral macro F1 is 0.062 and for coverage is -0.456. 2. Flagship models outperform distilled models (p < 0.01). Table 1 shows that this is the case for both unilateral and bilateral approaches, though the difference is more pronounced with bilateral evaluation (0.091 on the GPQA dataset, 0.112 on the SimpleQA dataset) versus unilateral evaluation (0.074 on the GPQA dataset, 0.087 on the SimpleQA dataset). 3. Bilateral evaluation is more expensive than unilateral evaluation (p < 0.01). Table 2 shows that bilateral evaluation takes roughly twice as much time and twice as many tokens as unilateral evaluation. However, evaluation times vary widely, with GPT-4o Mini using direct prompting taking mean of 2.5 seconds, and Llama 4 Scout using zero-shot prompting taking mean of 43.4 seconds. Token consumption scales predictably, from up to mean of 2,008.6 tokens with direct prompting, and up to mean of 6,704.7 tokens with few-shot prompting."
        },
        {
            "title": "Allen Chhikara Ferguson Ilievski Groth",
            "content": "Bilateral (ζ) Unilateral Dataset Model Type Time (s) Tokens Time (s) Tokens GPQA flagship distilled 36.747 (0.281) 34.771 (0.224) 4781.663 (45.878) 4731.672 (41.661) 12.411 (0.140) 13.439 (0.095) 2212.766 (27.182) 2532.153 (26.615) SimpleQA flagship distilled 32.789 (0.274) 30.310 (0.253) 4163.807 (37.704) 3964.037 (39.991) 12.256 (0.140) 11.424 (0.120) 2100.465 (25.634) 2167.419 (28.044) Table 2: Summary execution time (in seconds) and total tokens used for the bilateral factuality evaluation function ζ and baseline unilateral factuality evaluation function. Dataset Model Type t, t, f, f, GPQA flagship distilled 0.301 (0.008) 0.299 (0.007) 0.211 (0.007) 0.192 (0.006) 0.378 (0.008) 0.312 (0.007) 0.110 (0.005) 0.197 (0.006) SimpleQA flagship distilled 0.310 (0.007) 0.301 (0.007) 0.228 (0.007) 0.233 (0.007) 0.357 (0.008) 0.266 (0.007) 0.106 (0.005) 0.200 (0.006) Table 3: Summary truth value distributions for the bilateral factuality evaluation function ζ. Of note is the fact that the models evaluated did not produce any truth values where = or = during the evaluation. 4. Inconsistency occurs significantly more frequently than incompleteness (p < 0.05). Table 3 shows that bilateral judge models more frequently abstain by assigning t, (both verified and refuted) as opposed to f, (neither verified nor refuted). Limitations We have provided theoretical framework for integrating an LLM with paraconsistent reasoner, and demonstrated the feasibility of providing LLM-grounded valuations of atomic formulas, but there is as yet no complete implementation of the Belnap computer. One approach, described by Maier et al. (2013), involves mapping knowledge base expressed in paraconsistent version of SROIQ into knowledge base expressed in classical SROIQ, which then enables reasoning using an existing description logic reasoner such as Pellet (Sirin et al., 2007). The computational complexity depends on the number of atomic formulas requiring evaluation, which could be exponential in the worst case. Each atomic formula requires multiple API calls (2k calls for k-sample majority voting), making inference API costs and latency immediate practical bottlenecks. Yet, we expect that caching in ζc will amortize costs across repeated evaluations, and that standard tableau optimization techniques will help keep the overall complexity manageable. Empirical validation of these complexity expectations remains future work. 6. Conclusion and future work In conclusion, we have described novel approach to logical reasoning using LLMs, with several key contributions. We defined bilateral approach to factuality evaluation that Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations allows us to identify gaps and contradictions in an LLMs parametric knowledge. We introduced the concept of LLM-grounded interpretations that integrate an LLM directly into the formal semantics of the underlying logic while preserving its soundness and completeness. We provided empirical evidence that this formal framework can be realized in practice, providing path to enabling LLMs to serve as broad-coverage knowledge sources for logical reasoners. In future work, we plan to implement translation (Arieli and Denecker, 2003) between AC and classical description logic such as SROIQ, allowing us to evaluate fully-integrated reasoner using an LLM-grounded interpretation. We also plan to extend the above approach based on work in the area of generalized truth values (Shramko and Wansing, 2005; Hornischer, 2025) to provide multi-valued semantics for modeling factuality in LLMs, with the goal of improving the theoretical framework and metrics for factuality evaluation."
        },
        {
            "title": "References",
            "content": "Ofer Arieli and Marc Denecker. Reducing preferential paraconsistent reasoning to classical entailment. Journal of Logic and Computation, 13(4):557580, 2003. Yejin Bang, Ziwei Ji, Alan Schelten, Anthony Hartshorn, Tara Fowler, Cheng Zhang, Nicola Cancedda, and Pascale Fung. HalluLens: LLM Hallucination Benchmark. arXiv preprint arXiv:2504.17550, 2025. Nuel Belnap. How computer should think. In G. Ryle, editor, Contemporary aspects of philosophy, pages 3055. Oriel Press, 1977a. Nuel Belnap. useful four-valued logic. In Modern uses of multiple-valued logic, pages 537. Springer, 1977b. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc Le, Christopher Re, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. Benjamin Callewaert, Simon Vandevelde, and Joost Vennekens. VERUS-LM: VerarXiv preprint satile Framework for Combining LLMs with Symbolic Reasoning. arXiv:2501.14540, 2025. Fengxiang Cheng, Haoxuan Li, Fenrong Liu, Robert van Rooij, Kun Zhang, and Zhouchen Lin. Empowering LLMs with Logical Reasoning: Comprehensive Survey. arXiv preprint arXiv:2502.15652, 2025. Fabrice Correia. Grounding and truth-functions. Logique et Analyse, 53(211):251279, 2010."
        },
        {
            "title": "Allen Chhikara Ferguson Ilievski Groth",
            "content": "Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. Chain-of-verification reduces hallucination in large language models. arXiv preprint arXiv:2309.11495, 2023. Ran El-Yaniv and Yair Wiener. On the Foundations of Noise-free Selective Classification. Journal of Machine Learning Research, 11(5), 2010. Basil Ell and Andreas Harth. language-independent method for the extraction of RDF verbalization templates. In Proceedings of the 8th international natural language generation conference (INLG), pages 2634, 2014. Jiazhan Feng, Ruochen Xu, Junheng Hao, Hiteshi Sharma, Yelong Shen, Dongyan Zhao, and Weizhu Chen. Language models can be deductive solvers. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 40264042, 2024. Thomas Macaulay Ferguson. computational interpretation of conceptivism. In Meaning and Proscription in Formal Logic: Variations on the Propositional Logic of William T. Parry, pages 73105. Springer, 2017a. Thomas Macaulay Ferguson. Faulty Belnap Computers and Subsystems of FDE. In Meaning and Proscription in Formal Logic: Variations on the Propositional Logic of William T. Parry, pages 107131. Springer, 2017b. Thomas Macaulay Ferguson. Modeling Intentional States with Subsystems of ALC. In Proceedings of the 34th International Workshop on Description Logics (DL 2021), volume 2954 of CEUR Workshop Proceedings, pages 112, 2021a. Thomas Macaulay Ferguson. Tableaux and restricted quantification for systems related to weak Kleene logic. In International Conference on Automated Reasoning with Analytic Tableaux and Related Methods, pages 319. Springer, 2021b. Kit Fine. Angellic content. Journal of Philosophical Logic, 45:199226, 2016. Rajeev Gore and Linh Anh Nguyen. EXPTIME tableaux with global caching for description logics with transitive roles, inverse roles and role hierarchies. In International Conference on Automated Reasoning with Analytic Tableaux and Related Methods, pages 133148. Springer, 2007. Fabian Hoppe, Filip Ilievski, and Jan-Christoph Kalo. Investigating the robustness of deductive reasoning with large language models. arXiv preprint arXiv:2502.04352, 2025. Levin Hornischer. Iterating Both and Neither: With Applications to the Paradoxes. Notre Dame Journal of Formal Logic, 1(1):143, 2025. Alon Jacovi, Andrew Wang, Chris Alberti, Connie Tao, Jon Lipovetz, Kate Olszewska, Lukas Haas, Michelle Liu, Nate Keating, Adam Bloniarz, et al. The FACTS Grounding Leaderboard: Benchmarking LLMs Ability to Ground Responses to Long-Form Input. arXiv preprint arXiv:2501.03200, 2025. 12 Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations Fangkai Jiao, Zhiyang Teng, Bosheng Ding, Zhengyuan Liu, Nancy Chen, and Shafiq Joty. Exploring self-supervised logic-enhanced training for large language models. arXiv preprint arXiv:2305.13718, 2023. Norihiro Kamide. Paraconsistent description logics revisited. In 23rd International Workshop on Description Logics DL2010, page 197, 2010. Stephen Cole Kleene. Introduction to metamathematics. Wolters-Noordhoff Publishing and North-Holland Publishing Company, 1952. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022. Haitao Li, Qian Dong, Junjie Chen, Huixue Su, Yujia Zhou, Qingyao Ai, Ziyi Ye, and Yiqun Liu. LLMs-as-judges: comprehensive survey on LLM-based evaluation methods. arXiv preprint arXiv:2412.05579, 2024. Junteng Liu, Yuanxiang Fan, Zhuo Jiang, Han Ding, Yongyi Hu, Chi Zhang, Yiqi Shi, Shitong Weng, Aili Chen, Shiqi Chen, et al. Synlogic: Synthesizing verifiable reasoning data at scale for learning logical reasoning and beyond. arXiv preprint arXiv:2505.19641, 2025. Yue Ma, Pascal Hitzler, and Zuoquan Lin. Algorithms for paraconsistent reasoning with OWL. In European Semantic Web Conference, pages 399413. Springer, 2007. Frederick Maier, Yue Ma, and Pascal Hitzler. Paraconsistent OWL and related logics. Semantic Web, 4(4):395427, 2013. Terufumi Morishita, Gaku Morio, Atsuki Yamaguchi, and Yasuhiro Sogawa. Enhancing reasoning capabilities of LLMs via principled synthetic logic corpus. Advances in Neural Information Processing Systems, 37:7357273604, 2024. Linh Anh Nguyen. An efficient tableau prover using global caching for the description logic ALC. Fundamenta Informaticae, 93(1-3):273288, 2009. Theo Olausson, Alex Gu, Benjamin Lipkin, Cedegao Zhang, Armando Solar-Lezama, Joshua Tenenbaum, and Roger Levy. LINC: neurosymbolic approach for logical reasoning by combining language models with first-order logic provers. arXiv preprint arXiv:2310.15164, 2023. Liangming Pan, Alon Albalak, Xinyi Wang, and William Yang Wang. Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning. arXiv preprint arXiv:2305.12295, 2023. Peter Patel-Schneider. four-valued semantics for terminological logics. Artificial Intelligence, 38(3):319351, 1989. Aleksandr Perevalov and Andreas Both. Towards LLM-driven Natural Language Generation based on SPARQL Queries and RDF Knowledge Graphs, 2024."
        },
        {
            "title": "Allen Chhikara Ferguson Ilievski Groth",
            "content": "Fabio Petroni, Tim Rocktaschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander arXiv preprint Miller, and Sebastian Riedel. Language models as knowledge bases? arXiv:1909.01066, 2019. Dimitris Politis and Joseph Romano. Large sample confidence regions based on subsamples under minimal assumptions. The Annals of Statistics, pages 20312050, 1994. Graham Priest, Koji Tanaka, and Zach Weber. Paraconsistent Logic. In Edward N. Zalta and Uri Nodelman, editors, The Stanford Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, Spring 2025 edition, 2025. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: Graduate-Level GoogleProof Q&A Benchmark. arXiv preprint arXiv:2311.12022, 2023. Ian Rumfitt. Yes and No. Mind, 109(436):781823, 2000. Yaroslav Shramko and Heinrich Wansing. Some useful 16-valued logics: How computer network should think. Journal of Philosophical Logic, 34:121153, 2005. Yaroslav Shramko and Heinrich Wansing. Truth and falsehood: An inquiry into generalized logical values, volume 36. Springer Science & Business Media, 2011. Yaroslav Shramko and Heinrich Wansing. Truth Values. In Edward N. Zalta and Uri Nodelman, editors, The Stanford Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, Spring 2025 edition, 2025. Evren Sirin, Bijan Parsia, Bernardo Cuenca Grau, Aditya Kalyanpur, and Yarden Katz. Pellet: practical OWL-DL reasoner. Journal of Web Semantics, 5(2):5153, 2007. Damian Szmuc. An epistemic interpretation of paraconsistent weak Kleene logic. Logic and Logical Philosophy, 28(2):277330, 2019. Cunxiang Wang, Xiaoze Liu, Yuanhao Yue, Xiangru Tang, Tianhang Zhang, Cheng Jiayang, Yunzhi Yao, Wenyang Gao, Xuming Hu, Zehan Qi, et al. Survey on factuality in large language models: Knowledge, retrieval and domain-specificity. arXiv preprint arXiv:2310.07521, 2023. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:2482424837, 2022. Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, and William Fedus. Measuring short-form factuality in large language models. arXiv preprint arXiv:2411.04368, 2024. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601, 2023. 14 Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. Advances in Neural Information Processing Systems, 36: 4659546623, 2023. Lianghui Zhu, Xinggang Wang, and Xinlong Wang. JudgeLM: Fine-tuned Large Language Models are Scalable Judges. arXiv preprint arXiv:2310.17631, 2023. Appendix A. Angells logic of analytic containment AC Below we summarize the definitions of the object language, truth functions, and interpretations for the version of AC presented in greater detail in Ferguson (2021b). A.1. Object language Definition 12 Let be first-order language built from countable set of constants, countable set of variables V, countable set of relation symbols, the Boolean connectives , , and , restricted universal and existential quantifiers and , and round parentheses (as used in complex formulas) and square brackets (as used in quantified formulas) as auxiliary symbols. If and c1, . . . , cn C, then R(c1, . . . , cn) is an atomic formula. Let LAT be the set of atomic formulas. The formulas of are the elements of LAT , together with the following, where φ, ψ and V: φ (φ ψ) (φ ψ) [xφ(x)]ψ(x) [xφ(x)]ψ(x) A.2. Truth functions Definition 13 The weak Kleene truth tables over the set of truth values V3 = {t, e, f} are: f t f e e e f e e f"
        },
        {
            "title": "The weak Kleene truth tables for conjunction and disjunction induce the truth functions",
            "content": "and , respectively. Definition 14 The restricted Kleene quantifier functions and are mappings from sets of truth values to truth values such that: (X) = (X) = t if t, if for all u, v, either = or = if t, / and for some u, X, = and = if t, f, t, / and for some u, X, = and = if for all u, X, either = or = if {t, t, t, e} = and for some u, X, either = or ="
        },
        {
            "title": "Allen Chhikara Ferguson Ilievski Groth",
            "content": "A.3. Interpretations Definition 15 An AC interpretation is pair CI, RI where CI is domain of individuals and RI is set of functions where assigns: every constant an individual cI CI every n-ary predicate function RI : (CI)n V3 V3 Definition 16 An AC interpretation induces map : V3 V3 as follows, where I0 and I1 project the first and second coordinates respectively: For atomic formulas R(c1, . . . , cn) LAT , I(φ) = RI(cI I(φ) = I1(φ), I0(φ) I(φ ψ) = I0(φ) I0(ψ), I1(φ) I1(ψ) I(φ ψ) = I0(φ) I0(ψ), I1(φ) I1(ψ) I([xφ(x)]ψ(x)) = ({I0(φ(c)), I0(ψ(c)) C}), ({I0(φ(c)), I1(ψ(c)) C}) I([xφ(x)]ψ(x)) = ({I0(φ(c)), I0(ψ(c)) C}), ({I0(φ(c)), I1(ψ(c)) C}) 1 , . . . , cI ) Definition 17 Given an AC interpretation I, validity with respect to is defined as truth preservation, i.e. Γ =I φ if for all instances of such that ψ Γ I0(ψ) = t, I0(φ) = t. Appendix B. Prompts B.1. Direct verification template Determine whether the following answer to the given question is correct. Conclude with single line containing ONLY one of these two phrases: VERIFIED CANNOT VERIFY Question: {question} Proposed answer: {answer} B.2. Direct refutation template Determine whether the following answer to the given question can be refuted. Conclude with single line containing ONLY one of these two phrases: REFUTED CANNOT REFUTE Question: {question} Proposed answer: {answer} 16 Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations B.3. Zero-shot verification template Ill provide you with question and its proposed answer. Your task is to verify whether this answer is correct by following these steps: 1. Analyze the exact meaning of both the question and answer, identifying any key terms that need clarification. 2. Establish specific conditions that would make this answer true for this question. 3. Provide direct evidence supporting the answer, including specific facts, examples, or authoritative references that confirm its accuracy. 4. Test if the answer remains valid across all contexts where the question applies, noting any limitations or exceptions. 5. Check for consistency with established knowledge in the relevant domain. Based on your analysis, determine whether the answer is verified and explain your reasoning with specific supporting evidence. Your goal is not to find fault but to determine if positive evidence exists to confirm the answer. After your complete analysis, conclude with single line containing ONLY one of these two phrases: VERIFIED CANNOT VERIFY Question: {question} Proposed answer: {answer} B.4. Zero-shot refutation template Ill provide you with question and its proposed answer. Your task is to determine if this answer can be refuted by following these steps: 1. Analyze the exact meaning of both the question and the proposed answer. 2. Identify what specific conditions would need to be true for this answer to be false (not merely the absence of evidence). 3. Search for direct counterexamples or contradicting evidence that actively demonstrates why the answer is incorrect. 4. Construct specific scenarios where the answer fails to hold true, even if the questions premises are accepted. 5. Identify any logical inconsistencies, factual errors, or category mistakes within the answer. Focus on building an affirmative case for why the answer is incorrect, rather than simply noting lack of supporting evidence. Provide specific counterevidence and explain precisely how it contradicts the proposed answer. After your complete analysis, conclude with single line containing ONLY one of these two phrases: REFUTED CANNOT REFUTE Question: {question} Proposed answer: {answer}"
        },
        {
            "title": "Allen Chhikara Ferguson Ilievski Groth",
            "content": "B.5. Few-shot verification template Ill provide you with question and its proposed answer. Your task is to verify whether this answer is correct by following these steps: 1. Analyze the exact meaning of both the question and answer, identifying any key terms that need clarification. 2. Establish specific conditions that would make this answer true for this question. 3. Provide direct evidence supporting the answer, including specific facts, examples, or authoritative references that confirm its accuracy. 4. Test if the answer remains valid across all contexts where the question applies, noting any limitations or exceptions. 5. Check for consistency with established knowledge in the relevant domain. Based on your analysis, determine whether the answer is verified and explain your reasoning with specific supporting evidence. Your goal is not to find fault but to determine if positive evidence exists to confirm the answer. Here are some examples of how to approach verification: Example 1: Question: Who was the first man to walk on the Moon? Proposed answer: Neil Armstrong Lets verify this answer systematically: Key Terms Analysis: \"First man to walk on the Moon\" requires precise historical verification Specific date and mission details are crucial Conditions for Verification: Must be the first human to physically step onto lunar surface Must have documented evidence of this historic moment Must be part of specific NASA mission Direct Evidence: Neil Armstrong stepped onto the Moons surface on July 20, 1969 During Apollo 11 mission Famous quote: \"Thats one small step for man, one giant leap for mankind\" Extensive NASA documentation confirms this Broadcast live on global television Verified by multiple independent sources including NASA, scientific records, and historical archives Context and Limitations: Specifically refers to first human, not first spacecraft Specifically lunar surface, not orbital passage Confirmed during Cold War space race Part of NASAs Apollo program Consistency Check: Matches scientific and historical consensus No credible alternative claims exist Extensively documented in multiple international records This answer meets all verification criteria with robust, multi-source evidence. VERIFIED Example 2: Question: Chocolate is made from material Proposed answer: Sugar Lets analyze this systematically: Key Term Analysis: \"Chocolate\" refers to food product derived from cacao/cocoa beans \"Material\" suggests the fundamental ingredient or component \"Sugar\" is sweetener, not the primary material of chocolate Conditions for Accuracy: Sugar is an ingredient in chocolate But sugar is NOT the primary material of chocolate Chocolates primary material is cocoa/cacao solids Direct Evidence: Chocolate is primarily made from cocoa beans processed into cocoa solids Cocoa solids come from cacao tree seeds/beans Sugar is added as sweetener, not the base material Chocolate composition typically includes: Cocoa solids (primary material) Cocoa butter Sugar (secondary ingredient) Milk (in milk chocolate) Context Testing: In all chocolate production processes, cocoa is the fundamental material Sugar is always supplementary ingredient, not the base material Domain Consistency: Culinary and food science consistently define cocoa/cacao as chocolates primary material Based on comprehensive analysis, the proposed answer is incorrect. CANNOT VERIFY After your complete analysis, conclude with single line containing ONLY one of these two phrases: VERIFIED CANNOT VERIFY Question: {question} Proposed answer: {answer} Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations B.6. Few-shot refutation template Ill provide you with question and its proposed answer. Your task is to determine if this answer can be refuted by following these steps: 1. Analyze the exact meaning of both the question and the proposed answer. 2. Identify what specific conditions would need to be true for this answer to be false (not merely the absence of evidence). 3. Search for direct counterexamples or contradicting evidence that actively demonstrates why the answer is incorrect. 4. Construct specific scenarios where the answer fails to hold true, even if the questions premises are accepted. 5. Identify any logical inconsistencies, factual errors, or category mistakes within the answer. Focus on building an affirmative case for why the answer is incorrect, rather than simply noting lack of supporting evidence. Provide specific counterevidence and explain precisely how it contradicts the proposed answer. Here are some examples of how to approach refutation: Example 1: Question: Are penguins birds? Proposed answer: No Lets analyze this systematically: Meaning Analysis: Question asks about the taxonomic classification of penguins Proposed answer claims penguins are NOT birds Conditions for Falsity: Penguins must meet standard biological criteria for birds Must share key avian characteristics Counterevidence: Penguins have ALL classic bird characteristics: Feathered body Lay eggs Warm-blooded Have beaks Descended from dinosaur lineage Classified in scientific taxonomy under Class Aves Specifically, penguins belong to the order Sphenisciformes, which is recognized bird order Biological and genetic evidence conclusively places penguins within bird classification Specific Scenarios Contradicting Answer: Penguins have wing-like flippers adapted for swimming They have respiratory and skeletal structures identical to other bird species Genetic sequencing confirms their bird lineage Logical Inconsistencies: Rejecting penguins as birds would require rejecting fundamental biological classification systems No scientific basis exists for excluding penguins from bird category REFUTED Example 2: Question: Who was the first man to walk on the Moon? Proposed answer: Neil Armstrong Lets analyze this systematically: Meaning Analysis: Question seeks the definitive first human male to set foot on lunar surface Proposed answer: Neil Armstrong (Apollo 11 mission, July 20, 1969) Potential Conditions for Falsity: Documented evidence of another person walking on Moon before Armstrong Proof that Armstrong was not actually the first Historical record showing different individual preceded him Counterevidence Search: No credible historical evidence exists contradicting Armstrongs first Moon walk NASA records and global documentation consistently confirm Armstrong as first Extensive photographic and video evidence supports this claim Scenario Testing: No alternative scenarios emerge that could plausibly replace Armstrongs achievement Extensive verification by multiple nations and independent researchers confirms his primacy Logical Consistency Check: Armstrongs Moon walk is extensively documented Multiple witnesses and technological records corroborate the event No logical inconsistencies detected in the claim The proposed answer is completely accurate and supported by overwhelming historical evidence. CANNOT REFUTE After your complete analysis, conclude with single line containing ONLY one of these two phrases: REFUTED CANNOT REFUTE Question: {question} Proposed answer: {answer}"
        },
        {
            "title": "Allen Chhikara Ferguson Ilievski Groth",
            "content": "B.7. Prompt template for generating negative examples for SimpleQA-derived benchmark You are an expert synthetic data generator. Your task is to generate three plausible but incorrect answers to given question that will serve as challenging distractors. Guidelines for generating high-quality wrong answers: 1. Each answer must be factually incorrect but highly plausible within the context - Draw from the same domain/topic as the correct answer - Use answers that could reasonably be mistaken for the truth - Avoid obviously wrong or nonsensical options 2. Strictly match the answer type and format - For dates: Use the same date format and plausible timeframe - For people: Match profession, era, and relevance - For numbers: Stay within reasonable orders of magnitude - For places: Use locations of similar type/scale 3. Ensure clear differentiation - Make each wrong answer distinct from the correct answer - Avoid overlap between wrong answers - Space out numerical answers appropriately 4. Maintain consistent specificity - Match the level of detail in the correct answer - If the answer is detailed, provide equally detailed wrong options - If the answer is brief, keep wrong answers similarly concise 5. Optimize for realism and difficulty - Wrong answers should feel natural, not contrived - Target common misconceptions when possible - Make the options challenging enough to test real knowledge - Avoid answers that are too obviously incorrect Example 1: Question: Who was the first president of the United States? Answer: George Washington Wrong Answers: - John Adams - Thomas Jefferson - Benjamin Franklin Reason: All are founding fathers but not the first president Example 2: Question: What is the largest planet in our solar system? Answer: Jupiter Wrong Answers: - Saturn - Neptune - Uranus Reason: All are gas giant planets, but smaller than Jupiter Example 3: Question: Who wrote Romeo and Juliet? Answer: William Shakespeare Wrong Answers: - Christopher Marlowe - Ben Jonson - John Webster Reason: All are prominent Elizabethan playwrights Return only three wrong answers as list in JSON format with the following requirements: - Each wrong answer should be string - The output should be single JSON object with key \"negative_answers\" - The value should be an array of exactly 3 wrong answers - No explanations or additional text should be included - The answers should maintain consistent formatting with the correct answer Example format: {{ \"negative_answers\": [\"Wrong Answer 1\", \"Wrong Answer 2\", \"Wrong Answer 3\"] }} 20 Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations B.8. Direct prompt template for unilateral factuality evaluation Determine whether the following answer to the given question is TRUE or FALSE. Conclude with single line containing ONLY one of these two words: TRUE FALSE Question: {question} Proposed answer: {answer} B.9. Zero-shot prompt template for unilateral factuality evaluation Ill provide you with question and its proposed answer. Your task is to evaluate whether this answer is correct by following these steps: 1. Analyze the exact meaning of both the question and answer, identifying any key terms that need clarification. 2. Establish specific conditions that would make this answer true for this question. 3. Provide direct evidence supporting the answer, including specific facts, examples, or authoritative references that confirm its accuracy. 4. Test if the answer remains valid across all contexts where the question applies, noting any limitations or exceptions. 5. Check for consistency with established knowledge in the relevant domain. 6. Search for direct counterexamples or contradicting evidence that actively demonstrates why the answer is incorrect. 7. Construct specific scenarios where the answer fails to hold true, even if the questions premises are accepted. 8. Identify any logical inconsistencies, factual errors, or category mistakes within the answer. After analyzing the question and answer, provide single line containing ONLY one of these two words: TRUE FALSE Question: {question} Proposed answer: {answer}"
        },
        {
            "title": "Allen Chhikara Ferguson Ilievski Groth",
            "content": "B.10. Few-shot prompt template for unilateral factuality evaluation Ill provide you with question and its proposed answer. Your task is to evaluate whether this answer is correct by following these steps: 1. Analyze the exact meaning of both the question and answer, identifying any key terms that need clarification. 2. Establish specific conditions that would make this answer true for this question. 3. Provide direct evidence supporting the answer, including specific facts, examples, or authoritative references that confirm its accuracy. 4. Test if the answer remains valid across all contexts where the question applies, noting any limitations or exceptions. 5. Check for consistency with established knowledge in the relevant domain. 6. Search for direct counterexamples or contradicting evidence that actively demonstrates why the answer is incorrect. 7. Construct specific scenarios where the answer fails to hold true, even if the questions premises are accepted. 8. Identify any logical inconsistencies, factual errors, or category mistakes within the answer. Here are examples of how to approach evaluation: Example 1: Question: Who was the first man to walk on the Moon? Proposed answer: Neil Armstrong Analyze the question and answer: Question: \"Who was the first man to walk on the Moon?\" This is straightforward factual question seeking the identity of the first human to set foot on the lunar surface. Proposed answer: \"Neil Armstrong\" This is name, presumably offered as the answer to the question. Establish conditions for truth: The answer is true if Neil Armstrong was indeed the first human to walk on the Moon. Provide supporting evidence: Historical records, NASA documentation, and countless reliable sources confirm that Neil Armstrong was the first person to walk on the Moon on July 20, 1969, during the Apollo 11 mission. Test validity across contexts: The answer holds true in all historical contexts related to the first Moon landing. Check for consistency with established knowledge: The answer aligns perfectly with established historical and scientific knowledge. Search for counterexamples: There are no credible counterexamples. No other individual is historically recognized as the first person to walk on the Moon. Construct failure scenarios: There are no scenarios where the answer fails, assuming the question refers to the generally accepted historical event. Identify logical inconsistencies: There are no logical inconsistencies or factual errors. TRUE Example 2: Question: What is the main ingredient in chocolate? Proposed answer: Sugar Analyze the question and answer: Question: \"Chocolate is made from material\" - This is an incomplete sentence. The question is implicitly asking \"What material is chocolate made from?\" or \"What is key material used to make chocolate?\". Proposed answer: \"Sugar\" - This suggests that sugar is the material chocolate is made from. Establish conditions for truth: The answer would be true if sugar was the only ingredient in chocolate, or if the question was interpreted as \"Is sugar material used to make chocolate?\". Provide supporting evidence: Sugar is common and significant ingredient in most chocolate recipes. Test validity across contexts: This answer fails in many contexts. Chocolate is not only made from sugar. Check for consistency with established knowledge: Chocolate is made from cacao beans, sugar, and often other ingredients like milk solids, cocoa butter, lecithin, and flavorings. Search for counterexamples: Dark chocolate often contains higher percentage of cacao and less sugar. Sugar-free chocolate exists, using artificial sweeteners instead. Cacao beans are essential for chocolate, and chocolate cannot be made without them. Construct failure scenarios: Imagine recipe for 100% cacao chocolate. It would contain no sugar. Imagine sugar-free chocolate bar. It would contain no sugar. Identify logical inconsistencies: The answer implies sugar is the only ingredient, which is false. FALSE Question: {question} Proposed answer: {answer} Appendix C. Example To illustrate bilateral evaluation, we present an example with statements about penguins in the context of knowledge base with universally quantified statement that all birds can fly: 1. = {penguin, eagle, sparrow, ...} 22 Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations 2. φ0 = [ bird(x) ] lies(x) 3. φ1 = bird(penguin) 4. φ2 = flies(penguin) 5. φ3 = φ2 6. δ(φ1) = Penguins are birds 7. +(δ(φ1)) = . . . Penguins are scientifically classified as birds. They belong to the family Spheniscidae . . . Conclusion: VERIFIED 8. (δ(φ1)) = . . . All evolutionary biologists classify penguins as birds. This is supported by molecular evidence, fossil records, and anatomical features. There is no reasonable alternative classification. Conclusion: CANNOT REFUTE 9. ζ(φ1) = t, 10. δ(φ2) = Penguins fly 11. +(δ(φ2)) = . . . While penguins have wings, they cannot achieve aerial flight. Their wings are adapted for swimming rather than flying. They flap their wings underwater to fly through water. From strict biological perspective, penguins do not fly through air. Conclusion: CANNOT VERIFY 12. (δ(φ2)) = . . . Penguins are flightless birds. Their wings have evolved into flippers for aquatic propulsion rather than aerial flight. This is well-established in ornithology. Conclusion: REFUTED 13. ζ(φ2) = f, 14. I(φ0) = ({I0(bird(c)), I0(flies(c)) C}), ({I0(bird(c)), I1(flies(c)) C}) = f, 15. I(φ3) = I1(φ2), I0(φ2) = t, This bilateral evaluation reveals the inconsistency. The universal statement φ0 evaluates to false when considering penguins, and φ3 evaluates to true, but both statements can coexist in AC without causing explosion. This demonstrates how the system handles the classic penguin problem through paraconsistent reasoning. Appendix D. Experiments D.1. Datasets We used the short-form factuality benchmarks GPQA (Rein et al., 2023) and SimpleQA (Wei et al., 2024) to create the benchmarks for our experiments. GPQA consists of 448 multiple-choice questions, written by domain experts in biology, physics, and chemistry. SimpleQA consists of 4,326 question/answer pairs addressing range of general topic areas, including history, science and technology, art, geography, TV shows, and video games. From these two benchmarks we created balanced set of positive and negative examples. From SimpleQA, we sampled without replacement 200 question/answer pairs to be positive examples, and 200 questions to be negative examples, where we substituted false answers synthetically generated using GPT-4o Mini using the prompt shown in Appendix B.7. From GPQA, we sampled 200 existing question/answer pairs as positive examples, and 200 questions paired with the first incorrect answer for that question provided as part of the dataset."
        },
        {
            "title": "Allen Chhikara Ferguson Ilievski Groth",
            "content": "D.2. Experimental setup Following Wei et al. (2024), we evaluated our implementation of ζ on selective classification with binary abstention task (El-Yaniv and Wiener, 2010) using the above two datasets, measuring an LLM judges grading of given question/answer pair. The standard pattern in LLM judges in factuality evaluation is to prompt the LLM judge to evaluate the answer to the question as either correct, incorrect, or not attempted. This, again, has natural mapping to the values of V3; to derive single truth value V3 for the evaluation, we use the following projection : V3 V3 V3 such that for pair u, v: p(u, v) = if u, = t, if u, = f, f otherwise Calls to the public inference APIs for the models used temperature of 0.1. Repeated sampling (N=3) with majority vote was used in both the verification and refutation processes. Statistical significance was assessed using paired t-tests (ttest rel from the scipy.stats Python package) to compare performance metrics between different model and prompt combinations. The experiments were conducted in the first half of May 2025 using calls to the public inference APIs for each of the models. D.3. Performance metrics Judge Model Prompt Macro F1 Coverage Time (s) Tokens Claude 3.5 Sonnet Claude 3.5 Haiku Llama 4 Maverick Llama 4 Scout GPT-4o GPT-4o Mini direct zero few direct zero few direct zero few direct zero few direct zero few direct zero few 0.712 (0.023) 0.738 (0.029) 0.716 (0.028) 0.578 (0.027) 0.648 (0.034) 0.604 (0.034) 0.748 (0.02) 0.542 (0.024) 0.54 (0.023) 0.778 (0.02) 0.412 (0.023) 0.438 (0.024) 34.22 (6.08) 53.91 (6.41) 52.92 (6.41) 30.52 (4.09) 43.12 (3.60) 47.44 (4.69) 2914.80 (778.99) 4863.48 (731.27) 8079.40 (745.71) 2641.07 (704.48) 4221.73 (685.85) 7673.44 (700.47) 0.774 (0.021) 0.765 (0.025) 0.751 (0.023) 0.852 (0.016) 0.618 (0.022) 0.805 (0.018) 65.91 (52.53) 75.95 (44.90) 65.08 (41.43) 6225.19 (1526.74) 7492.54 (1357.27) 9945.70 (1444.02) 0.702 (0.025) 0.712 (0.031) 0.694 (0.027) 0.592 (0.027) 0.603 (0.035) 0.69 (0.031) 0.536 (0.028) 0.4 (0.025) 0.428 (0.028) 0.712 (0.021) 0.46 (0.024) 0.642 (0.022) 0.705 (0.021) 0.48 (0.022) 0.518 (0.023) 0.69 (0.022) 0.438 (0.023) 0.488 (0.023) 63.24 (28.92) 93.43 (51.27) 69.27 (37.27) 5403.00 (1833.85) 7062.05 (1430.96) 9540.89 (1362.86) 5.61 (8.41) 36.29 (9.44) 42.01 (18.48) 1133.92 (556.51) 6041.24 (1256.13) 8662.78 (1398.93) 16.25 (12.91) 32.88 (7.54) 47.99 (15.90) 2863.86 (1716.43) 5851.67 (959.41) 8787.62 (1120.93) Table 4: Performance metrics for ζ using different judge models and evaluation prompts on GPQA question/answer pairs (N=400). 24 Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations Judge Model Prompt Macro F1 Coverage Time (s) Tokens Claude 3.5 Sonnet Claude 3.5 Haiku Llama 4 Maverick Llama 4 Scout GPT-4o GPT-4o Mini direct zero few direct zero few direct zero few direct zero few direct zero few direct zero few 0.667 (0.021) 0.664 (0.022) 0.66 (0.022) 0.533 (0.023) 0.556 (0.024) 0.577 (0.023) 1.0 (0.0) 1.0 (0.0) 1.0 (0.0) 1.0 (0.0) 1.0 (0.0) 0.998 (0.002) 14.52 (2.92) 19.29 (2.80) 23.64 (2.85) 14.25 (2.23) 19.70 (2.44) 19.48 (1.55) 1360.21 (386.44) 2161.70 (369.97) 5055.46 (382.93) 1346.54 (377.10) 2070.33 (328.06) 4829.19 (327.03) 0.722 (0.021) 0.694 (0.02) 0.717 (0.021) 1.0 (0.0) 0.998 (0.002) 0.99 (0.005) 26.83 (21.83) 25.20 (10.44) 24.49 (11.06) 2919.78 (825.91) 3462.93 (665.47) 5756.76 (602.70) 0.636 (0.023) 0.577 (0.024) 0.568 (0.023) 0.572 (0.023) 0.497 (0.024) 0.484 (0.024) 0.543 (0.023) 0.444 (0.021) 0.538 (0.024) 1.0 (0.0) 0.998 (0.002) 1.0 (0.0) 30.07 (19.93) 24.74 (18.10) 24.38 (18.08) 2467.57 (1096.58) 2689.86 (1180.22) 4955.30 (1124.60) 1.0 (0.0) 1.0 (0.0) 1.0 (0.0) 1.0 (0.0) 1.0 (0.0) 1.0 (0.0) 2.92 (7.85) 3.89 (4.92) 8.00 (8.50) 9.52 (6.61) 6.60 (6.96) 11.41 (2.60) 546.73 (276.40) 1059.73 (276.40) 3446.44 (343.78) 1543.24 (848.31) 1722.51 (945.13) 4949.93 (477.92) Table 5: Performance metrics for unilateral factuality evaluation using different judge models and evaluation prompts on GPQA question/answer pairs (N=400)."
        },
        {
            "title": "Allen Chhikara Ferguson Ilievski Groth",
            "content": "Judge Model Prompt Macro F1 Coverage Time (s) Tokens Claude 3.5 Sonnet Claude 3.5 Haiku Llama 4 Maverick Llama 4 Scout GPT-4o GPT-4o Mini direct zero few direct zero few direct zero few direct zero few direct zero few direct zero few 0.81 (0.025) 0.733 (0.027) 0.654 (0.031) 0.667 (0.033) 0.673 (0.036) 0.653 (0.033) 0.673 (0.026) 0.746 (0.029) 0.692 (0.026) 0.58 (0.031) 0.677 (0.038) 0.558 (0.031) 0.502 (0.024) 0.615 (0.022) 0.65 (0.022) 0.458 (0.024) 0.385 (0.022) 0.45 (0.023) 19.22 (4.49) 43.35 (6.44) 43.19 (5.45) 15.81 (3.07) 38.92 (3.14) 40.01 (4.09) 1190.43 (174.00) 3444.22 (171.12) 6704.68 (161.86) 1014.98 (149.24) 3095.57 (131.72) 6435.11 (158.40) 0.768 (0.02) 0.528 (0.024) 0.715 (0.021) 21.84 (13.56) 36.17 (10.85) 41.29 (31.55) 2008.57 (754.99) 4421.40 (448.00) 6665.58 (512.60) 0.572 (0.023) 0.358 (0.022) 0.632 (0.023) 17.02 (9.05) 43.41 (16.12) 36.90 (27.20) 1392.07 (434.73) 4049.26 (380.54) 6267.85 (437.70) 0.67 (0.026) 0.738 (0.032) 0.833 (0.026) 0.74 (0.021) 0.44 (0.024) 0.482 (0.024) 5.11 (5.49) 22.18 (3.49) 21.00 (4.86) 477.22 (60.34) 3720.07 (314.42) 6079.94 (292.95) 0.604 (0.027) 0.525 (0.038) 0.586 (0.034) 0.718 (0.021) 0.378 (0.023) 0.472 (0.023) 2.53 (0.72) 23.75 (10.23) 30.69 (16.02) 483.86 (68.15) 3812.08 (829.15) 6298.49 (254.71) Table 6: Performance metrics for ζ using different judge models and evaluation prompts on SimpleQA question/answer pairs (N=400). 26 Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations Judge Model Prompt Macro F1 Coverage Time (s) Tokens Claude 3.5 Sonnet Claude 3.5 Haiku Llama 4 Maverick Llama 4 Scout GPT-4o GPT-4o Mini direct zero few direct zero few direct zero few direct zero few direct zero few direct zero few 0.705 (0.022) 0.682 (0.022) 0.661 (0.023) 0.595 (0.023) 0.55 (0.025) 0.523 (0.024) 0.643 (0.023) 0.663 (0.023) 0.648 (0.023) 0.578 (0.023) 0.559 (0.025) 0.552 (0.024) 0.62 (0.023) 0.614 (0.024) 0.632 (0.023) 0.587 (0.023) 0.528 (0.023) 0.572 (0.022) 1.0 (0.0) 1.0 (0.0) 1.0 (0.0) 1.0 (0.0) 1.0 (0.0) 1.0 (0.0) 6.34 (1.59) 16.28 (5.05) 16.82 (1.99) 6.69 (1.60) 16.29 (3.87) 17.02 (1.55) 453.53 (80.98) 1499.60 (112.18) 4331.51 (89.63) 489.73 (91.76) 1505.90 (93.60) 4332.01 (63.46) 1.0 (0.0) 0.992 (0.004) 0.992 (0.004) 6.71 (4.20) 14.88 (8.78) 16.97 (10.19) 814.69 (350.94) 2097.99 (230.58) 4524.26 (265.16) 1.0 (0.0) 1.0 (0.0) 1.0 (0.0) 1.0 (0.0) 1.0 (0.0) 1.0 (0.0) 1.0 (0.0) 1.0 (0.0) 1.0 (0.0) 5.73 (4.96) 8.62 (8.44) 5.08 (6.68) 2.55 (5.42) 3.49 (3.70) 7.38 (9.60) 1.08 (0.19) 1.56 (1.52) 6.75 (2.37) 511.43 (286.82) 1192.79 (516.58) 3306.05 (376.05) 220.40 (30.07) 733.39 (30.07) 3088.39 (30.07) 220.58 (30.79) 788.69 (185.84) 3923.80 (318.41) Table 7: Performance metrics for unilateral factuality evaluation using different judge models and evaluation prompts on SimpleQA question/answer pairs (N=400)."
        },
        {
            "title": "Allen Chhikara Ferguson Ilievski Groth",
            "content": "D.4. Truth value distributions Judge Model Prompt t, t, f, f, Claude 3.5 Sonnet Claude 3.5 Haiku Llama 4 Maverick Llama 4 Scout GPT-4o GPT-4o Mini direct zero few direct zero few direct zero few direct zero few direct zero few direct zero few 0.202 (0.018) 0.435 (0.023) 0.448 (0.023) 0.392 (0.022) 0.218 (0.02) 0.18 (0.018) 0.355 (0.022) 0.325 (0.021) 0.36 (0.022) 0.05 (0.011) 0.022 (0.007) 0.012 (0.005) 0.11 (0.015) 0.53 (0.023) 0.502 (0.024) 0.53 (0.023) 0.208 (0.019) 0.212 (0.02) 0.248 (0.02) 0.205 (0.019) 0.225 (0.02) 0.112 (0.015) 0.058 (0.011) 0.06 (0.011) 0.04 (0.009) 0.37 (0.022) 0.155 (0.017) 0.442 (0.023) 0.245 (0.02) 0.438 (0.023) 0.41 (0.023) 0.372 (0.023) 0.368 (0.022) 0.108 (0.015) 0.012 (0.005) 0.04 (0.009) 0.072 (0.013) 0.525 (0.024) 0.33 (0.022) 0.368 (0.023) 0.245 (0.021) 0.385 (0.024) 0.345 (0.022) 0.215 (0.019) 0.258 (0.02) 0.215 (0.019) 0.015 (0.006) 0.028 (0.007) 0.082 (0.013) 0.502 (0.023) 0.468 (0.024) 0.358 (0.022) 0.108 (0.015) 0.155 (0.017) 0.348 (0.023) 0.372 (0.022) 0.362 (0.022) 0.212 (0.018) 0.018 (0.006) 0.015 (0.006) 0.172 (0.018) 0.555 (0.023) 0.51 (0.023) 0.145 (0.017) 0.018 (0.006) 0.028 (0.007) 0.545 (0.023) 0.42 (0.023) 0.46 (0.022) 0.138 (0.016) 0.008 (0.004) 0.002 (0.002) Table 8: Truth value probabilities for ζ using different judge models and evaluation prompts for GPQA. 28 Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations Judge Model Prompt t, t, f, f, Claude 3.5 Sonnet Claude 3.5 Haiku Llama 4 Maverick Llama 4 Scout GPT-4o GPT-4o Mini direct zero few direct zero few direct zero few direct zero few direct zero few direct zero few 0.052 (0.01) 0.245 (0.02) 0.278 (0.021) 0.218 (0.02) 0.152 (0.016) 0.118 (0.014) 0.285 (0.021) 0.462 (0.022) 0.532 (0.023) 0.445 (0.024) 0.14 (0.016) 0.072 (0.012) 0.035 (0.008) 0.148 (0.016) 0.132 (0.016) 0.282 (0.022) 0.145 (0.016) 0.162 (0.017) 0.175 (0.018) 0.24 (0.02) 0.288 (0.021) 0.507 (0.024) 0.468 (0.023) 0.418 (0.023) 0.088 (0.013) 0.438 (0.024) 0.218 (0.019) 0.588 (0.022) 0.35 (0.022) 0.525 (0.023) 0.18 (0.018) 0.178 (0.018) 0.19 (0.019) 0.145 (0.016) 0.032 (0.008) 0.068 (0.012) 0.125 (0.015) 0.62 (0.022) 0.232 (0.02) 0.368 (0.022) 0.245 (0.02) 0.53 (0.023) 0.205 (0.018) 0.112 (0.015) 0.102 (0.014) 0.302 (0.021) 0.022 (0.007) 0.132 (0.016) 0.058 (0.011) 0.56 (0.024) 0.51 (0.024) 0.462 (0.023) 0.105 (0.014) 0.205 (0.018) 0.278 (0.022) 0.335 (0.023) 0.278 (0.021) 0.202 (0.019) 0.0 (0.0) 0.008 (0.004) 0.145 (0.017) 0.62 (0.023) 0.488 (0.022) 0.228 (0.019) 0.055 (0.01) 0.272 (0.02) 0.49 (0.023) 0.322 (0.022) 0.2 (0.018) 0.138 (0.016) 0.002 (0.002) 0.038 (0.009) Table 9: Truth value probabilities for ζ using different judge models and evaluation prompts for SimpleQA."
        }
    ],
    "affiliations": [
        "Rensselaer Polytechnic Institute, Troy, NY, US",
        "University of Amsterdam, Amsterdam, NL",
        "University of Southern California, CA, US",
        "Vrije Universiteit Amsterdam, Amsterdam, NL"
    ]
}