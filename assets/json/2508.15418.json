{
    "paper_title": "LLaSO: A Foundational Framework for Reproducible Research in Large Language and Speech Model",
    "authors": [
        "Yirong Sun",
        "Yizhong Geng",
        "Peidong Wei",
        "Yanjun Chen",
        "Jinghan Yang",
        "Rongfei Chen",
        "Wei Zhang",
        "Xiaoyu Shen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The development of Large Speech-Language Models (LSLMs) has been slowed by fragmented architectures and a lack of transparency, hindering the systematic comparison and reproducibility of research. Unlike in the vision-language domain, the LSLM field suffers from the common practice of releasing model weights without their corresponding training data and configurations. To address these critical gaps, we introduce LLaSO, the first fully open, end-to-end framework for large-scale speech-language modeling. LLaSO provides the community with three essential resources: (1) LLaSO-Align, a 12M-instance speech-text alignment corpus; (2) LLaSO-Instruct, a 13.5M-instance multi-task instruction-tuning dataset; and (3) LLaSO-Eval, a reproducible benchmark for standardized evaluation. To validate our framework, we build and release LLaSO-Base, a 3.8B-parameter reference model trained exclusively on our public data. It achieves a normalized score of 0.72, establishing a strong, reproducible baseline that surpasses comparable models. Our analysis reveals that while broader training coverage enhances performance, significant generalization gaps persist on unseen tasks, particularly in pure audio scenarios. By releasing the complete stack of data, benchmarks, and models, LLaSO establishes a foundational open standard to unify research efforts and accelerate community-driven progress in LSLMs. We release the code, dataset, pretrained models, and results in https://github.com/EIT-NLP/LLaSO."
        },
        {
            "title": "Start",
            "content": "LLaSO: Foundational Framework for Reproducible Research in Large Language and Speech Model Yirong Sun1,2 Yizhong Geng2,3 Peidong Wei1,4 Yanjun Chen1 Jinghan Yang2,3 Rongfei Chen1 Wei Zhang1 Xiaoyu Shen1* 1Ningbo Key Laboratory of Spatial Intelligence and Digital Derivative, Institute of Digital Twin, EIT 2Logic Intelligence Technology 3BUPT 4Xiamen University Correspondence: win1282467298@gmail.com xyshen@eitech.edu.cn 5 2 0 2 1 2 ] . [ 1 8 1 4 5 1 . 8 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "The development of Large Speech-Language Models (LSLMs) has been slowed by fragmented architectures and lack of transparency, hindering the systematic comparison and reproducibility of research. Unlike in the visionlanguage domain, the LSLM field suffers from the common practice of releasing model weights without their corresponding training data and configurations. To address these critical gaps, we introduce LLaSO, the first fully open, end-to-end framework for large-scale speech-language modeling. LLaSO provides the community with three essential resources: (1) LLaSO-Align, 12M-instance speechtext alignment corpus; (2) LLaSO-Instruct, 13.5M-instance multi-task instruction-tuning dataset; and (3) LLaSO-Eval, reproducible benchmark for standardized evaluation. To validate our framework, we build and release LLaSO-Base, 3.8B-parameter reference model trained exclusively on our public data. It achieves normalized score of 0.72, establishing strong, reproducible baseline that surpasses comparable models. Our analysis reveals that while broader training coverage enhances performance, significant generalization gaps persist on unseen tasks, particularly in pure audio scenarios. By releasing the complete stack of data, benchmarks, and models, LLaSO establishes foundational open standard to unify research efforts and accelerate community-driven progress in LSLMs."
        },
        {
            "title": "Introduction",
            "content": "The remarkable success of Large Language Models (LLMs) has established powerful foundation for multimodal AI (OpenAI, 2024; Yang et al., 2025). In the visual domain, this has led to the rapid maturation of Large Vision-Language Models (LVLMs), *Corresponding Author 1This research is supported by EIT-NLP Lab and Logic Intelligence Technology. Code, dataset, pretrained models, and results are at https://github.com/EIT-NLP/LLaSO. 1 Figure 1: Normalized performance on LLaSO-Eval, comparing LLaSO-Base (orange) with the strongest prior models (blue) across 20 linguistic, semantic, and paralinguistic tasks. For each task, scores are normalized by setting the best-performing models result to 1. Details are in Table 13. where established paradigms, such as leveraging CLIP-style encoders (Radford et al., 2021), have enabled effective and scalable alignment between vision and text (Awadalla et al., 2023; Wang et al., 2024; Bai et al., 2025; Cocchi et al., 2025). In contrast, the development of Large Speech-Language Models (LSLMs) remains in more nascent and fragmented stage. The field currently lacks consensus on fundamental architectural principles, with competing approaches that include external feature fusion (Radford et al., 2022; Li et al., 2023b), dedicated cross-modal attention mechanisms (Kong et al., 2024a; Elizalde et al., 2024), and implicit alignment strategies (Chu et al., 2024). This architectural divergence is compounded by lack of transparency in existing research. While several open-source LSLM initiatives have emerged (Chu et al., 2023; Défossez et al., 2024; Tang et al., 2024), many are only partially open. Model weights may be released, but the underlying training data and crucial configurations are often withheld. This opacity makes it difficult to conduct fair comparisons, as performance differences can be attributed as much to proprietary data or undisclosed training strategies as to architectural merit, hindering systematic progress. resources, we aim to lower the barrier to entry and foster new wave of systematic, community-driven progress in large-scale speech-language modeling. To address these challenges of fragmentation and opacity, we introduce LLaSO: fully open, end-toend framework designed to establish foundational standards for LSLM research. LLaSO consists of three core, publicly available components: 1. LLaSO-Align: 12M-instance speechtext alignment corpus aggregated from diverse sources, including conversational read narraspeech (Chen et al., 2021), tives (Panayotov et al., 2015), audio books (Ito and Johnson, 2017; Pratap et al., 2020), and accented speech (Veaux et al., 2016). 2. LLaSO-Instruct: 13.5M-instance instruction-tuning dataset covering 20 tasks across linguistic, semantic, and paralinguistic domains. It supports three distinct modality configurations: audio instructions with audio inputs, textual instructions with audio inputs, and audio instructions with textual inputs. 3. LLaSO-Eval: reproducible benchmark of 15,044 stratified samples designed for comprehensive evaluation of instruction-following capabilities of LSLMs. To validate our framework and provide the community with strong, reproducible baseline, we developed LLaSO-Base, 3.8B-parameter reference model that adapts the successful LLaVA architecture to the speech domain. Trained exclusively on LLaSO-Align and LLaSO-Instruct, and evaluated on LLaSO-Eval, our model achieves normalized score of 0.72, outperforming the next best comparable model (0.65). As illustrated in Figures 1 and 8, LLaSO-Base is designed not for state-of-the-art performance, but to demonstrate the power of an open, extensible, and reproducible workflow. Our evaluation shows that while broader training improves overall performance, models still struggle with generalization, leaving substantial gaps on unseen tasks and pure audio settings. Investigating potential solutions for this weakness, we found that models equipped with interleaving and parallel decoding mechanisms exhibit far greater robustness in these challenging scenarios. In summary, LLaSO provides the first fully open, end-to-end stack for LSLM research, comprising large-scale training datasets, standardized benchmark, and reference model. By releasing these"
        },
        {
            "title": "2 Related Work",
            "content": "Vision-Language Modeling. Vision-language modeling has rapidly advanced through standardized two-stage paradigm: modality alignment followed by instruction tuning (Brown et al., 2020; Bommasani et al., 2021; Li et al., 2023c). The rapid progress in this field has been facilitated by two essential types of open resources. First, public training datasets and standardized evaluation benchmarks (Ma et al., 2023; Hsieh et al., 2023; Zeng et al., 2024b; Fu et al., 2024; Huang et al., 2025) have become widely adopted, enabling fair comparison and transparent reproducibility across models and tasks. Second, open-source implementations with modular codebases such as LLaVA (Lin et al., 2023) and OpenFlamingo (Awadalla et al., 2023) have significantly lowered the technical barriers to development and fostered rapid iteration across the community. Together, these practices have fostered shared research infrastructure where new models and tasks are often built upon existing resources (Liu et al., 2023a; Yin et al., 2024). This has allowed vision-language research to focus more on advancing scientific capabilities rather than reimplementing foundational components. Speech-Language Modeling. Compared to vision-language modeling, progress in speechlanguage systems has been less cohesive (Su et al., 2025; Ma et al., 2024). First, most leading models such as Audio Flamingo (Kong et al., 2024b; Ghosh et al., 2025), Qwen-Audio (Chu et al., 2023, 2024), and Kimi-Audio (KimiTeam et al., 2025) rely on proprietary data, limiting reproducibility (Peng et al., 2025; Pandey et al., 2025). Second, most models support only narrow modality configurations (e.g., text-plus-audio), with few (Tang addressing more compositional et al., 2024; Chu et al., 2024; Chen et al., 2024). Third, existing datasets largely focus on semantic reasoning (Fang et al., 2025; Wu et al., 2024; Mei et al., 2024), with limited coverage of prosody and emotion. Lastly, few open-source stacks unify models, datasets, and benchmarks; most systems (e.g., LauraGPT (Du et al., 2023), Moshi (Défossez et al., 2024), Westlake-Omni (Xinchen-ai, 2024)) lack full releases, hindering reproducibility and community development. tasks"
        },
        {
            "title": "Audio\nType",
            "content": "Sample Duration (Hours) Num. AVQA COTA OpenAQA OpenASQA SIFT-50M SALMONN LLaSO Corpus - - Multiple Single Multiple Multiple Single 1 5 4 8 10 14 20 ① ① ① ① ① ② ③"
        },
        {
            "title": "Mixed",
            "content": "Collected 57.3K 1.2M Collected 5.0M Collected 9.6M Collected 55.6M Collected 2.3M - - - - - 4.4K 25.5M 89.5K"
        },
        {
            "title": "Mixed",
            "content": "Table 1: Comparison of public speech-language datasets and our LLaSO Corpus. For Modality Coverage, ① means only text instruction with audio input, ② adds pure audio formats, and ③ indicates full support, including audio instruction with text input. Audio Type refers to whether the dataset is comprised of real recordings (Collected), synthetic data, or both (Mixed). / indicate the presence or absence of released alignment data. LLaSO-Eval: stratified benchmark designed for consistent evaluation across tasks. These components together support the full training pipeline of LSLMs, modality alignment, instruction tuning, and evaluation (see Figure 2). To advance LSLMs beyond vision-language paradigms, we anchor our benchmark design in two core properties of speech: Inherent Paralinguistics: Speech conveys rich, essential information beyond words such as speaker identity, accent, emotion, and prosody. These paralinguistic cues are omnipresent and crucial for natural human communication. Flexible Modality Roles: In LSLMs, both audio and text can serve as inputs or instructions, enabling diverse interaction patterns e.g., audio-instruction with text input, text-instruction with audio input, or audioinstruction with audio input. To better reflect the needs of real-world systems, we adopt balanced task weighting approach that corrects for limitations in existing corpora: Semantic Tasks (8%): Intentionally underweighted, as their success often reflects language modeling capacity rather than speech understanding (Rouditchenko et al., 2025), and they are already well-represented (Gong et al., 2023b; Fang et al., 2025). Paralinguistic Tasks (40%): Prioritized to address their underrepresentation in current resources (Jiang et al., 2025; yu Huang et al., 2024). We ensure diversity by combining realworld metadata with synthetically generated variations. Figure 2: Overview of LLaSO Corpus, comprising LLaSOAlign, LLaSO-Instruct, and LLaSO-Eval. It encompasses 25.5M audio-text pairs spanning 20 task types, including 18 paralinguistic tasks, and supports linguistic, semantic, and paralinguistic abilities. Instances include both real (73%) and synthetic (27%) audio, covering multiple input combinations and tasks. Further statistics are detailed in Table 12."
        },
        {
            "title": "3 LLaSO Corpus",
            "content": "To support the development of LSLMs, we introduce the LLaSO Corpus, comprehensive, modular benchmark suite."
        },
        {
            "title": "3.1 Corpus Overview",
            "content": "Inspired by practices in LVLMs, LLaSO comprises three tightly integrated components: LLaSO-Align: large-scale corpus for aligning speech with semantic space through ASRbased supervision. LLaSO-Instruct: multi-task instructiontuning dataset spanning linguistic, semantic, and paralinguistic tasks. 3 Figure 3: LLaSO Corpus construction pipeline. We first aggregate heterogeneous sources, including text-based QA corpora and speech datasets covering acoustic, paralinguistic, and semantic tasks, then normalize format, sample rate, and instruction style, etc. We construct LLaSO-Align (12.0 M) for aligning speech and text modality via ASR, while LLaSO-Instruct (13.5 M) for multi-task instruction tuning including classification, recognition, and AQA. When synthesize audio, we use vocal style mixing strategy as Figure 4 by utilizing controllable TTS and voice-cloning for richer speaker variation, enabling pure-audio, text plus audio, and audio plus text formatted samples with diverse gender, accent, speed, and tone. Linguistic Tasks (52%): Dominated by ASR, which remains foundational to grounding speech in linguistic structure and is critical for general performance. The final LLaSO Corpus includes 71% realworld audio and 29% synthetic speech, and covers broad range of modality configurations where both audio and text flexibly act as inputs and instructions (see Table 1). This design ensures robust coverage of the full speech landscape, supporting the development of unified and adaptable LSLMs."
        },
        {
            "title": "3.2 LLaSO-Align",
            "content": "To establish robust semantic foundation for speech-language modeling, we adopt ASR as the core alignment task in LLaSO Corpuss first stage. Following vision-language best practices, this approach grounds speech representations directly in textual semantic space through explicit instructionresponse pairing. LLaSO-Align contains 12M instruction-formatted ASR samples, each consisting of an audio input, natural language instruction, and reference transcription. Unlike traditional ASR datasets that offer only raw audio-text pairs, we introduce 18 manually designed instruction templates that frame the task with varying specificity and constraints. These include prompts such as Transcribe the audio precisely. Return only the text with no added commentary, encouraging models to attend more carefully to instructions and simulate real-world usage scenarios. To ensure diversity in content and speaker profiles, we collect data from wide range of public ASR corpora, including conversational speech (Chen et al., 2021), single-speaker narration (Panayotov et al., 2015), audio books (Ito and Johnson, 2017; Pratap et al., 2020), and accented English (Veaux et al., 2016). This allows the corpus to capture broad distribution of acoustic environments, accents, age groups, and speech styles. All samples undergo standardized preprocessing pipeline to ensure consistency and quality (see Figure 3). Corrupted or unreadable files are removed, and valid audio is resampled to 16 kHz and converted to WAV or FLAC format. Transcriptions are filtered to retain only English content with standard characters, then normalized to follow conventional grammar and formatting (e.g., proper capitalization, spacing). Each cleaned sample is paired with randomly selected instruction template, and the final dataset is packaged in unified JSON format. By reframing ASR as an instruction-following alignment task and curating diverse, high-quality dataset, LLaSO-Align lays the groundwork for downstream speech-language understanding across modalities."
        },
        {
            "title": "3.3 LLaSO-Instruct",
            "content": "Building on the aligned speech-text representations from LLaSO-Align, we present LLaSO-Instruct, 4 multi-task instruction tuning dataset designed to advance speech-language modeling with greater task diversity and richer modality configurations. Unlike previous instruction datasets that focus primarily on semantic tasks with limited input modalities, LLaSO-Instruct fully embraces the inherently multimodal and paralinguistic nature of speech. It systematically expands both the scope of tasks and the variety of modality pairings, offering comprehensive framework for training instruction-following speech-language models. Figure 4: Overview of the Vocal Style Mixing. Vocal style mixing is strategy in our data construction pipeline in Figure 3, where utterances are systematically synthesized with diverse speaker traits. This strategy is applied across all three modalities input, expanding acoustic diversity and simulating realistic multi-speaker scenarios, resulting wide range of speech-language interactions in Figure 5. Task Coverage. LLaSO-Instruct spans 20 tasks grouped into three categories: linguistic, semantic, and paralinguistic. While linguistic tasks (e.g., ASR) and semantic tasks (e.g., audio-based QA) cover foundational capabilities, the majority of tasks are paralinguistic, designed to capture speaker traits and contextual acoustic cues crucial for socially-aware interaction. These include speaker-centric tasks such as emotion or accent recognition, and content-centric tasks such as phoneme or intent classification. We present the statistics of all included tasks in Appendix L. To construct this wide range of tasks, firstly, we collect task-specific datasets with rich metadata, enabling the same audio sample to be repurposed for multiple tasks using associated labels such as gender, accent, and age. When label distributions are imbalanced, we implement targeted sampling strategies such as downsampling overrepresented classes or trimming the extremes of long-tail distributions. 2 Secondly, for each task we manually construct 20 text instructions spanning four prompt styles including standardized, contextualized, stylistic variation, and fine-grained task prompts. Representative examples are provided in Appendix H. For ASR and AQA, all instructions are open-ended, inviting free-form responses from the model. In contrast, paralinguistic tasks predominantly employ closed-ended instructions, where the model must select an answer from predefined list without additional analysis. To address diverse task requirements, we construct training samples at multiple levels of granularity, so that some paralinguistic tasks also include open-ended variants. 3 We display task prototypes in Figure 5II and the open/closed-ended mapping in Table 12. As to linguistic and semantic task categories, we will discuss in the following paragraph. Modality Coverage. To reflect speechs flexible role as both input and instruction, LLaSO-Instruct supports three core modality configurations: 1. Text instruction with audio input 2. Audio instruction with text input 3. Pure audio for both instruction and input An overview is provided in Table L. Linguistic tasks retain their native modality pairings, with one million ASR samples carried over 2For example, in the Meld accent dataset, we address the pronounced long-tail in accent labels by removing the rarest and trimming the most dominant classes. Similarly, while VCTK was originally introduced for ASR, we repurpose its gender metadata for speaker gender classification; given that its gender labels are imbalanced (approximately 20K female versus 15K male samples), we downsample the female subset to achieve balanced 1:1 ratio. 3For example, in age classification, we provide three levels: coarse-grained (e.g., twenties, fifties, sixties10-year spans), medium-grained (e.g., 15-19, 20-24, 25-295year spans), and fine-grained where the model is required to predict the exact age as an integer between 18 and 80. from LLaSO-Align. Semantic QA tasks are derived from high-quality textual instruction datasets (e.g., OpenOrca and Alpaca) and converted into multimodal samples using advanced audio synthesis (see Figure 4). Each instance may yield multiple variants (e.g., text-with-audio and audio-with-text) to support robust cross-modal learning. 4 Paralinguistic tasks are primarily configured as text-instruction with audio input, but where feasible, we also construct fully speech-driven formats by synthesizing both instruction and input as audio. This enables training in pure audio scenarios that better simulate human-human interaction. Overall, LLaSO-Instruct provides rich, flexible instruction-tuning corpus that strengthens speechlanguage models across wide range of semantic and paralinguistic tasks while supporting diverse modality combinations."
        },
        {
            "title": "3.4 LLaSO-Eval",
            "content": "To complete our data trio, we introduce LLaSOEval, held-out evaluation suite designed to accompany the LLaSO training set. Derived from the same underlying corpus but strictly separated from the training split, LLaSO-Eval supports comprehensive model assessment across diverse set of speech understanding tasks. LLaSO-Eval consists of 15,044 evaluation samples spanning 20 tasks, which are grouped into three broad categories: linguistic, semantic, and paralinguistic. It fully supports the three major modality configurations and is designed to test both withinand cross-modal generalization. Table 2 provides detailed breakdown of the task types, categories, and modality distributions. From task perspective, LLaSO-Eval enables comprehensive evaluation of model capabilities across three major categories: linguistic, semantic, and paralinguistic tasks. Within the paralinguistic category, we further distinguish between speaker-centric tasks such as gender, age, and accent classification and content-centric tasks like intent prediction, phoneme recognition, and entity extraction. This distinction enables fine-grained analysis of how models handle both speaker identity and acoustic-semantic information. 4When instruction and input segments are suitable for audio synthesis, English-only, properly normalized, nonduplicated, excluding special tags (e.g., <img>, base64), and sufficiently long for error-free synthesis, each textual QA instance yields both text-with-audio and audio-with-text variants. (I) Interaction formats supported in LLaSO Corpus. (II) Task prototypes. (a) closed-set classification; (b) multigranularity/open-set recognition; (c) open-ended AQA. Figure 5: Qualitative overview of the formats and task categories covered by LLaSO Corpus. Panel (I) visualises the three instruction-input configurations supported end-to-end; panel (II) illustrates the three task prototypes. Those compositional flexibilities are absent from previous speech-language datasets that are mainly restricted to single instruction-input patterns. These examples also serve as case-study references in Section 5.5. We present text instructions style and granularity details in Appendix and I. From modality perspective, LLaSO-Eval not only evaluates model performance on modality configurations seen during training, but also stresses cross-modal generalization, testing the models robustness to novel input-output combinations. To evaluate instruction-following capabilities, LLaSO-Eval includes both open-ended and closedended prompts. Open-ended tasks test models ability to engage in free-form comprehension and reasoning, while closed-ended tasks, especially common in the paralinguistic category, require strict selection from predefined label sets. This allows for quantitative measurement of instruction adherence through metrics such as abstention rate."
        },
        {
            "title": "4 Model",
            "content": "To validate the effectiveness of our LLaSO Corpus, we introduce LLaSO-Base, reference model in the speech-language domain that strictly aligns with the end-to-end instruction tuning paradigm estab-"
        },
        {
            "title": "Modality Formats",
            "content": "Sample Num."
        },
        {
            "title": "SGC",
            "content": "Speaker Gender Classification (Biologically) AC AR"
        },
        {
            "title": "EIE",
            "content": "ER"
        },
        {
            "title": "Accent Classification",
            "content": "Age Recognition (Three Granularities)"
        },
        {
            "title": "Synthetic Speech Detection\nSpeaker Verification\nPronunciation Scoring Word Level\nPronunciation Scoring Sentence Level",
            "content": "Phoneme Recognition Speech Command Recognition Intent Prediction Entity Extraction Vocal Sound Classification Instrument Classification Instrument Source Classification Pitch Prediction Velocity Classification -"
        },
        {
            "title": "GigaSpeech\nLibriSpeech\nLJ Speech\nVCTK\nMLS",
            "content": "< Textual Instruction, Audio Input > 4566 WER&CER"
        },
        {
            "title": "Semantic Task Category",
            "content": "Open Orca 1M-GPT4 (Mukherjee et al., 2023) Open Orca 3.5M-GPT3.5 Stanford Alpaca (Taori et al., 2023) Code Alpaca (Chaudhary, 2023) AlpacaDan (Jordan, 2023) Dolly (Conover et al., 2023) OpenOrcaNo (RuterNorway, 2023) Tigerbot_Alpaca (Research, 2023) Tigerbot_Multichat (Chen et al., 2023b) Unnatural (Honovich et al., 2022) < Pure Audio > < Textual Instruction, Audio Input > and < Audio Instruction, Textual Input > Paralinguistic Task Category Speaker-centric VoxCeleb1 (Nagrani et al., 2017) VCTK VocalSound (Gong et al., 2022) Common Voice (Ardila et al., 2019) VCTK AccentDB (Ahamad et al., 2020) Common Voice VCTK VocalSound Common Voice MELD (Poria et al., 2018) CREMA-D (Cao et al., 2014) MELD CREMA-D FoR (Reimao and Tzerpos, 2019) MELD speechocean762 (Zhang et al., 2021) < Pure Audio > and < Textual Instruction, Audio Input > Content-centric Phonemizer Generated (Bernard and Titeux, 2021) Speech Commands (Warden, 1804) SLURP (Bastianelli et al., 2020)"
        },
        {
            "title": "VocalSound",
            "content": "NSynth (Engel et al., 2017) < Pure Audio > and < Textual Instruction, Audio Input > - - 100 100 100 100 100&100 100&100 100&100 100&100 100&100 100&100 100&100 100&100 200&200 100&100 100&100 100&100 100&100 100&100 200&200 100&100 100&100 100&100 100&100 100&100 100&100 100&100 200&200 200& 100&100 100&100 858&858 569&569 200&200 100&100 100&100 112&112 100&100 15044 GPT-4o"
        },
        {
            "title": "ACC",
            "content": "ACC/MAE"
        },
        {
            "title": "ACC",
            "content": "ACC ACC GPT-4o ACC&GPT-4o PER WER&CER&GPT-4o GPT-4o GPT-4o ACC ACC ACC MAE ACC - Table 2: Overview of the task composition in LLaSO-Eval. LLaSO-Eval is stratified evaluation set sampled from LLaSOInstruct, encompassing 20 tasks across three main categories: linguistic, semantic, and paralinguistic (sub-divided into speakercentric and content-centric). For each task, we list the data sources, evaluation modality formats, sample counts, and evaluation metrics. Automatic metrics are used where applicable, and GPT-4o-based judgment is applied for open-ended tasks. lished in vision-language research (Zhu et al., 2023; Cocchi et al., 2025; Li et al., 2023a). Rather than pursuing new SOTA results, our objective is to offer the community robust and extensible baseline for systematic cross-modal instruction following."
        },
        {
            "title": "4.1 Model Architecture",
            "content": "The model is composed of three core components: speech encoder, projector, and LLM, as illustrated in Figure 6. This straightforward yet empirically validated structure mirrors successful visionlanguage instruction tuning paradigms, so we refer to and verify whether this architecture is still applicable in speech domain, despite its various modality configurations. We choose Whisper-large-v3 (Radford et al., 2022) as our speech encoder, as its rich intermediate semantic representation and strong consensus in community (Zhang et al., 2024; Gong et al., 2023a). And we retain only its encoder component, comprising two convolutional downsampling layers followed by 32 Transformer blocks, approximately 640M parameters. During preprocessing, audio is resampled to 16kHz, then converted into 128-channel mel spectrograms using 25ms window and 10ms hop length. In addition, pooling layer with stride 2 reduces the audio representation length. As result, each encoded frame approximately corresponds to 40ms segment of the original audio. SpecAugment (Park et al., 2019) is applied during training to enhance robustness. Formally, given an input audio a, representing either instructions instruct, content input, or both combined instruct+input, the speech encoder 7 Fae() produces audio features Za = Fae(X a). We use the features from the last Transformer layer in our experiments. Next, we consider trainable two-layer multi-layer perceptron (MLP) with Gaussian Error Linear Unit (GELU) activation to project audio features into text embedding space, for its empirical effectiveness and simplicity compared to more complex cross-modal alignment modules (Tang et al., 2024; Kong et al., 2024a; Lin et al., 2024). Concretely, given the encoder outputs Za, the projector maps them to the corresponding audio embeddings in the LLMs textual embedding space, denoted as = Fproj(Za), with Za = Fae(X a), yielding sequence of projected audio embeddings a. representational capacity."
        },
        {
            "title": "4.2 Training",
            "content": "The model is trained in single-turn instructionfollowing setting. For each training instance, we pair an audio with its corresponding textual sequence t, and the assistants target response answer. To support all major modality configurations, we unify the input format as follows: (t,a) (a,t) (a) query = [X query = [X query = [X instruct, instruct, instruct+input] input], input], (1) where (t, a), (a, t), and (a) denote the text instruction with audio input, audio instruction with text input, and pure audio configurations, respectively. During training, the model parameters θ are optimized via next-token autoregressive prediction, maximizing the conditional likelihood of the assistant response given the full query sequence. The training objective is defined as: (cid:16) p"
        },
        {
            "title": "X t",
            "content": "answer query (cid:17) = (cid:12) (cid:12) () (cid:12) (cid:12) (cid:12) () (cid:12) (cid:16) xt pθ (cid:89) i=1 query,<i, answer,<i (2) (cid:17) Figure 6: Overview of the LLaSO-Base architecture and two training phases. The reference model architecture and input flow (Bottom), three supported modality input layouts (Top Left) , and the training recipe comprising alignment and instruction tuning (Top Right). Subsequently, the audio embeddings are concatenated with their corresponding textual embeddings input/instruct, obtained by passing input/instruct through the text tokenizer. The concatenation preserves the natural order of the audio and textual components. The concatenation procedure is flexible, uniformly supporting all three modality configurations, text instruction with audio input, pure audio, and audio instruction with text input, illustrated as Figure 6 (Top Left). We employ Llama-3.2-3B-Instruct (Grattafiori et al., 2024) as the LLM backbone, mainstream choice. It is decoder-only Transformer with 28 layers, hidden dimension of 3072, and approximately 3.21B parameters. The complete LLaSO-Base model thus contains approximately 3.8B parameters, striking balance between computational efficiency and 8 where answer denotes the assistants text response, () query represents the input query under any of the three modality configurations (see Eq. 1), and is the length of the response. At each decoding step i, the model predicts token xt conditioned on the query () query,<i and all previously generated answer tokens answer,<i. For brevity, we omit special tokens (e.g., system, user, and assistant markers) in Eq. 2, and subsume all audio-related input (e.g., instruct) into the query notation. We employ two-stage instruction-tuning procedure in our LLaSO Corpus, fully adapted from LLaVA and similar vision-language baselines (Alayrac et al., 2022; Chen et al., 2023a; Liu et al., 2023b; Zhu et al., 2023; Bai et al., 2023; Cha et al., 2024); the set of trainable parameters θ varies by stage to be described in the subsequent section and our chat template is displayed in Box 7. Stage 1: Alignment. In the alignment stage, we adopt ASR as the speech-language alignment objective, providing direct supervisory signal that aligns frame-level speech representations with ground-truth text. This enables the model to learn Chat Template for LLaSO-Base <begin_of_text><start_header_id>system<end_header_id>"
        },
        {
            "title": "X t",
            "content": "systemprompt <eot_id><start_header_id>user<end_header_id> query <eot_id><start_header_id>assistant<end_header_id>"
        },
        {
            "title": "X t",
            "content": "answer <eot_id> Box 7: Illustration of the chat template used to construct every training example. We follow the official Llama-3.2 chat template for token ordering and special tokens, while inserting custom <system> prompt (full text in Appendix G). The user request is encoded as query (see Eq. 1 for the three modality variants), and the model must generate the assistant reply answer followed by the end-of-turn token <eot_id>. During training, the loss is applied only to the assistants tokens (the last line in this box), teaching the network both the content of the response and where to terminate. shared embedding space for speech and language, following the same modeling principle as image captioning in vision-language frameworks. Specifically, we construct and employ our LLaSOAlign, in which each training example comprises of textual instruction instruct, an audio input input, and the corresponding ground-truth transcript answer. The instruction and audio jointly prompt the model to transcribe the audio content, with the transcript serving as the target output. During this stage, we freeze both the audio encoder and the LLM, updating only the parameters of the MLP projector by maximizing the objective in Eq. 2, i.e., with trainable parameters θ in Fproj. This approach enables the projector to map speech features into the pre-trained LLM word embedding space, establishing cross-modal semantic consistency for downstream instruction tuning, while preserving the knowledge in the frozen encoder and LLM. Stage 2: Instruction Tuning. After completing modality alignment, we train the model on LLaSO-Instruct, enabling it to acquire compositional instruction-following abilities across broad spectrum of speech-language tasks and modality configurations. Through this process, the model is equipped to handle linguistic, semantic, and paralinguistic objectives, adapt to both open-ended and closed-ended tasks, and generalize across all major modality combinations encountered in realworld speech-language interactions, including most common format text instructions with audio inputs (t,a) query, pure audio streams as both instruction and content (a) query, and screenless-like scenarios where audio instructions are paired with text inputs (a,t) query; in all cases, the model is trained to generate textual responses. To this end, we freeze the audio encoder and optimize only the parameters of the projector Fproj and the LLM Fllm i.e., θ in Eq. 2. Detailed training configurations are provided in Appendix F"
        },
        {
            "title": "All",
            "content": "We rigorously assess LLaSO-Base on the LLaSOtraining splits, evaluation partiEval. tions, and task configurations follow the definitions established in Section 3.4 and Table 2, ensuring full methodological consistency and To benchmark system perfortransparency. mance, we compare LLaSO-Base against suite of leading speech-language models including Qwen2-Audio (Chu et al., 2024), TyphoonAudio (Manakul et al., 2024), Salmonn (Tang et al., 2024), GLM-4-Voice (Zeng et al., 2024a), MiniOmni (Xie and Wu, 2024a), Mini-Omni2 (Xie (Fang et al., and Wu, 2024b), Llama-Omni 2025), Audio-Reasoner (Xie et al., 2025), KimiAudio (KimiTeam et al., 2025), and Qwen2.5Omni (Xu et al., 2025). For each baseline, official checkpoints or API interfaces are used where available. Detailed version and related information for all benchmarking candidates are provided in Appendix C. All reported results are computed on LLaSO-Eval to ensure consistency across models."
        },
        {
            "title": "5.2 Metrics",
            "content": "Given the diversity of tasks and modalities in LLaSO-Eval, we define 7 metrics to ensure comprehensive evaluation. During evaluation, by default metric scores are computed directly from the"
        },
        {
            "title": "Linguistic Task Category",
            "content": "Modality Format: < Textual Instruction, Audio Input >"
        },
        {
            "title": "ASR",
            "content": "< Pure Audio >"
        },
        {
            "title": "Semantic Task Category",
            "content": "< Textual Instruction, Audio Input >"
        },
        {
            "title": "AQA",
            "content": "< Audio Instruction, Textual Input > Qwen2-Audio Typhoon-Audio"
        },
        {
            "title": "Salmonn",
            "content": "Glm-4-Voice Mini-Omni Mini-Omni2 Llama-Omni Audio-Reasoner Kimi-Audio Qwen2.5-Omni LLaSO-Base (Ours) 0.22 0.11 0.86 0. 0.95 0.95 0.88 0.28 0.14 0. 0.08 0.12 0.06 0.69 0.79 0. 0.80 0.73 0.12 0.05 0.26 0. 2.41 2.42 2.73 2.78 1.76 1.77 2.16 2.22 1.47 1.41 1.41 1.72 2.22 2.34 3.29 2.93 1.42 1.47 1.75 1.45 1.57 1.53 2.05 1. 1.97 2.02 2.99 2.48 2.44 2.24 2.51 2.86 2.94 2.70 3.22 3.45 2.94 3.09 3.22 2.63 2.06 1.80 2.39 1.46 2. 1.98 1.50 2.70 1.52 1.67 2. 2.51 3.08 2.97 1.93 2.56 3.49 2.13 3.14 3.13 2.20 1.87 3.14 1.61 2.83 3.04 2. 2.05 3.13 1.42 2.96 3.12 2.37 2.49 3.21 2.51 3.11 2.82 1.97 1.63 1.54 1.22 2.34 1.33 1.41 1.66 1.64 1.26 2.52 1.42 1.43 2.38 2.95 1.88 3.16 2.72 2.20 2.22 3.42 2.12 3.07 2.91 2. 3.28 3.77 3.35 3.53 3.38 2.71 2.99 3.80 3.20 2.96 3.19 2.12 2.57 2.48 1.71 2.74 3.05 2.90 2.82 2.60 2. 2.72 1.57 1.65 2.58 2.73 3. 3.05 2.58 3.47 3.62 3.29 1.29 3.14 2.52 2.69 2.91 2.47 1.68 3.04 1.91 2.04 3.03 2.42 1.83 3.19 1.58 3.09 4.06 1.68 1.03 3.10 1. 1.42 1.32 1.17 1.21 1.27 1.20 1.68 1.50 1.41 1.29 1.31 1.28 2.73 3.78 2.29 1.11 3.08 2.09 2.84 3.95 2.88 1.54 3.13 2.09 3.69 4.01 3.38 1.16 3.16 2.77 3.46 3.88 3.58 1.19 3.15 2. 2.72 2.62 2.28 2.23 3.74 2.60 2.89 2.45 2.35 2.49 1. 1.41 2.51 2.74 3.03 2.95 2."
        },
        {
            "title": "Metrics",
            "content": "WER CER GPT-4o Avg.GPT-4o GPT-4o Avg.GPT-4o GPT-4o Avg.GPT-4o Table 3: Comparison of 11 LSLMs on LLaSO-Eval linguistic (ASR) and semantic (AQA) tasks across three modality configurations. ASR is evaluated by WER/CER (lower is better); AQA is scored by GPT-4o (higher is better). Cell shading , and no shading denotes each models relative ranking in given modality (best worst) by average GPT-4o score. , complete model outputs. However, certain tasks require an intermediate step of extracting structured answers from the model outputs prior to evaluation; such cases are explicitly noted in their respective metric descriptions below. Task-specific metric assignments are detailed in Table 2. WER and CER. Word Error Rate (WER) and Character Error Rate (CER) (Morris et al., 2004; huggingface, 2023; Chen et al., 1998) quantify transcription accuracy derived from Levenshtein distance (Navarro, 2001) between the model prediction and the ground-truth transcript. WER operates at the word level, while CER operates at the character level. Both metrics are employed for ASR and SCR tasks. Lower values indicate better accuracy. Typically, WER and CER scores range from 0 (perfect match) to 1, although values exceeding 1 can occur due to excessive insertions or substitutions. PER. Phoneme Error Rate (PER) is analogous to WER and CER but specifically measures the Levenshtein distance between the predicted and groundtruth phoneme sequences with (brianlan, 2017), providing phoneme-level accuracy assessment. Similar to WER and CER, lower PER values indicate superior performance, typically ranging from 0 upwards, with 0 representing perfect phoneme prediction. We apply this metric exclusively to the PR task. Accuracy. Accuracy is defined as the proportion of exact matches between the models prediction and the ground-truth label. This metric is applied to all closed-ended tasks, as specified in Table 12, which also indicates which tasks are openversus closed-ended. For closed-ended tasks, the model must select single answer from predefined label set, and response is marked correct only if it precisely matches the reference label; predictions containing multiple candidate labels or irrelevant content are treated as incorrect. Accuracy ranges from 0 to 1, with higher values indicating better performance. Additionally, the metric is also computed for the open-ended PSSL task, where models rate sentence-level pronunciation across three dimensions, accuracy, prosodic, and fluency. Given that model outputs are typically free-form, we use regular expressions to extract numeric scores from responses, accommodating variations such as accuracy is 8, fluency: 7, or 9 for prosodic. Responses providing valid numeric scores for all three dimensions are retained; others are excluded. For each sample, we compute the average of the exactmatch accuracies across these three dimensions, then report the overall accuracy averaged across all evaluated samples. Further, we complement this rule-based measure with an additional GPT-4o evaluation to ensure comprehensive assessment. MAE. Mean Absolute Error (MAE) is adopted for tasks requiring numerical predictions, such as AR and PR. In these tasks, the model is explicitly instructed to generate single numeric value. However, many LSLMs produce free-form textual outputs rather than direct numeric predictions (Adlakha et al., 2024), necessitating an answer extraction procedure prior to metric calculation. For the AR task, the predicted numeric value represents age and thus must be extracted reliably from the model output. Employing regular expression, our script"
        },
        {
            "title": "SGC",
            "content": "Paralinguistic Task Category Speaker-centric Modality Format: < Textual Instruction, Audio Input > AR"
        },
        {
            "title": "EIE",
            "content": "AC ER"
        },
        {
            "title": "PSSL",
            "content": "1.00 0.95 0.67 0.99 0.16 0.12 0.05 0.23 0.52 18.69 0.54 0.31 0.24 0.30 0.43 0.25 Qwen2-Audio Typhoon-Audio 0.85 0.77 0.59 0.67 0.21 0.14 0.11 0.10 0.12 20.47 0.40 0.24 0.28 0.12 0.46 0.20 0.59 0.44 0.13 0.18 0.22 0.32 0.10 0.26 0.06 11.24 0.31 0.24 0.30 0.21 0.50 0.19 Salmonn 0.11 0.12 0.04 0.07 0.07 0.09 0.03 0.02 0.01 15.35 0.13 0.08 0.14 0.02 0.10 0.04 Glm-4-Voice 0.14 0.00 0.00 0.00 0.00 0.06 0.00 0.00 0.00 21.34 0.04 0.04 0.04 0.07 0.11 0.03 Mini-Omni 0.11 0.00 0.02 0.00 0.00 0.03 0.00 0.00 0.00 18.46 0.03 0.06 0.00 0.01 0.12 0.03 Mini-Omni2 0.36 0.26 0.03 0.26 0.07 0.14 0.07 0.16 0.17 Reject 0.31 0.08 0.16 0.04 0.26 0.08 Llama-Omni Audio-Reasoner 0.38 0.32 0.38 0.37 0.14 0.18 0.02 0.23 0.12 13.57 0.52 0.29 0.32 0.28 0.35 0.16 0.98 0.97 0.66 0.81 0.38 0.31 0.20 0.17 0.12 12.07 0.65 0.34 0.52 0.32 0.63 0.22 Kimi-Audio Qwen2.5-Omni 0.53 0.41 0.40 0.35 0.06 0.19 0.02 0.11 0.08 10.31 0.52 0.27 0.29 0.33 0.42 0.15 0.96 0.99 0.76 0.91 0.52 0.83 0.73 0.70 0.50 10.32 0.48 0.48 0.17 0.26 0.99 0.32 LLaSO-Base 1.86 2.04 1.32 1.62 1.24 1.16 1.28 2.61 3.30 1.25 2.90 1.95 1.71 1.38 1.84 1.46 1.54 1.38 2.03 2.76 2.12 2.80 0.17 0.33 0.13 0.24 0.00 0.00 0.13 0.09 0.20 0.27 0."
        },
        {
            "title": "Metrics",
            "content": "ACC MAE ACC GPT-4o ACC Paralinguistic Task Category Content-centric Modality Format: < Textual Instruction, Audio Input > PR"
        },
        {
            "title": "SCR",
            "content": "PP VC 1.19 3.08 1.82 0.90 0.92 0.97 1.46 1.08 1.58 1.28 0.03 2.60 0.98 1.09 1.00 1.00 1.00 20.19 2.40 1.00 3.53 0.04 3.04 2.52 2.73 0.85 0.60 0.60 19.02 2.40 3.13 1.86 2.85 0.49 0.16 0.16 36.83 0.85 4.07 1.88 3.29 0.61 0.16 0.16 41.92 0.75 1.85 1.78 2.34 0.32 0.00 0.03 40.20 0.98 1.39 1.26 1.42 0.02 0.01 0.06 61.49 0.98 1.89 1.26 1.60 0.03 0.01 0.03 59.32 0.97 21.11 1.58 1.80 2.34 0.03 0.00 0.12 Reject 4.10 2.29 3.78 0.59 0.20 0.17 32.68 1.84 4.56 2.05 3.57 0.84 0.26 0.38 31.64 0.31 3.91 2.00 2.78 0.92 0.51 0.44 18.37 3.52 8.02 4.86 3.93 3.57 0.78 0.50 0.60 0.02 0.02 0.17 0.22 0.08 0.00 0.00 0.07 0.15 0.19 0.12 0."
        },
        {
            "title": "Tasks",
            "content": "Qwen2-Audio Typhoon-Audio Salmonn Glm-4-Voice Mini-Omni Mini-Omni2 Llama-Omni Audio-Reasoner Kimi-Audio Qwen2.5-Omni LLaSO-Base"
        },
        {
            "title": "Metrics",
            "content": "PER WER CER GPT-4o ACC MAE ACC %abstention Table 4: Performance of 11 LSLMs on LLaSO-Eval paralinguistic tasks, split by speaker-centric and content-centric groups. Cells are colored by abstention rate, as indicated by the color bar. Abstention rates were computed across all closed-ended tasks. Results are for the text instruction with audio input modality, reflecting the modality used for paralinguistic task training; our released datasets also include pure audio modality format samples for these tasks. Reject denotes 95% or more abstentions in given task after manual inspection in open-ended settings/tasks. initially attempts direct integer conversion; if unsuccessful, it searches for numeric patterns, and if numeric range like40-45 is detected, it computes the rounded average of the two endpoints. For outputs containing descriptive keywordsadult without numeric information, we substitute canonical age value 22. The PR task requires evaluation of the models ability to predict MIDI note values ranging from 0 to 127. Specifically, our extraction function sequentially attempts integer conversion, rounded float conversion, and finally, averaging numeric ranges. Strings containing pitch-related keywords (e.g., \"midi\", \"pitch\", \"hz\") but lacking numeric values are marked as invalid predictions. Only predictions within the MIDI range of 0 to 127 are considered valid for metric computation. In all cases, if valid numeric value cannot be extracted from either the models prediction, that instance is omitted from the calculation. The final MAE is computed as the mean absolute difference between extracted predictions and ground-truth numeric values across all valid instances. Lower MAE values indicate better numerical prediction performance. GPT-4o Score. For AQA and other open-ended generative tasks, where model responses are unconstrained and may vary widely in form and content, thus we employ GPT-4o (OpenAI, gpt-4o-mini, Version 2024-07-18) as an automatic evaluator. Fol11 lowing standardized evaluation template, GPT-4o assigns an integer score from 1 to 5, reflecting both the relevance and accuracy of the models response relative to the reference answer. Further details of the evaluation prompt are provided in Appendix K, and task-specific metric assignments are summarized in Table 2. Abstention Rate. Some LSLMs may abstain from answering tasks involving unfamiliar modality formats or instructions, fail to follow instructions, or explicitly state their inability to process audio. To quantify such behavior, we report the abstention rate for closed-ended tasks, defined as the proportion of responses in which the model either refuses to answer, returns irrelevant content, or fails to select valid label from the predefined set. An abstention is counted whenever the models output does not comply with the task requirement to select label. Abstention rate is not reported for openended tasks, as their free-form nature precludes rule-based criterion for abstention."
        },
        {
            "title": "5.3 Results",
            "content": "Figure 8 presents the overall performance of each model, with min-max normalization applied to facilitate direct comparison across metrics. LLaSOBase achieves the highest overall score among all evaluated models. Notably, models with broader task coverage outperform those focusing primarily on AQA, such as Llama-Omni and Mini-Omni by significant margin, which are mostly optimized for similar semantic tasks or narrow tasks. This performance gap emphasizes the effectiveness of diverse task training. more detailed inspection of model behaviors across specific tasks and modality configurations is provided in Section 5.4. In Table 3 and 4 we present 11 LSLMs performance results across three modality configuarations and across all tasks, together with their abstention rate. We made the following observations. Firstly, there is notable variation in the distribution of across modality configurations. For instance, the two bestperforming models, LLaSO-Base and Kimi-Audio, exhibit performance drops exceeding 0.25 points across modalities. This variability confirms that our benchmark adequately evaluates model capabilities to diverse input formats. Additionally, we observe substantial differences in task-specific performances. Kimi-Audio achieves outstanding scores in AQA tasks across all modalities and demonstrates low abstention rates on paralinguistic tasks, yet scores notably lower on tasks like AR. These findings validate the benchmarks ability to comprehensively assess models across varied task domains. Secondly, we notice that LLaSO-Base not only performs well on the majority of tasks but also in maintaining low abstention rates, suggesting the models capacity for instruction following and its ability providing responses more aligned with user queries. This underscores the critical importance of incorporating task and modality diversity. Thirdly, it can be observed that models ranking lower overall tend to be speech-to-speech systems with narrow task focus, excelling mainly at AQA related objectives but performing less competitively on other tasks. However, these models demonstrate greater robustness to modality shifts. We hypothesize it benefiting from interleaving or parallel decoding strategies that help bridge the gap between textual and auditory modalities. Overall, while current LSLMs exhibit some capability in handling diverse speech-language tasks, our findings underscore the continued need to expand both task and modality diversity to drive further progress. Figure 8: Overall model performance after min-max normalization. Each bar represents models aggregated performance score, normalized to the [0, 1] range for direct comparison. Higher values indicate better overall performance."
        },
        {
            "title": "5.4 Analysis",
            "content": "We find that LSLMs perform poorly on unseen tasks and unfamiliar modality formats, with especially weak instruction-following in unseen tasks. LSLMs perform worse on unseen modality configuration. Most existing LSLMs only support one or two modality configurations. We evaluate their generalization across three different input formats. Specifically, we select representative models and calculate their average performance on AQA task across all major input modality configurations with results summarized in Table 3. We observe that 12 model performance in the audio instruction with text input setting drops consistently compared to the familiar text instruction with audio input configuration. At the same time we find that Qwen2Audio is an outlier, showing that it and its variant Audio-Reasoner obtain similar results in both formats. Notably, from human perspective, audio instruction with text input should be no more difficult and is arguably even simpler, since only the (typically brief) instruction needs to be heard, while the main input remains directly readable as text, as illustrated in Figure 5I (b, c). Nonetheless, our findings demonstrate that LSLMs still struggle with modality configurations outside their explicit training coverage. Pure audio modality configuration may still challenging. We illustrate the performance of 11 models three major modality formats in Figure 9 (Bottom). In most cases, models demonstrate substantially lower performance on pure audio formats than on the more common text instruction with audio input setting, even when they are explicitly trained to handle pure audio via speech-to-speech or spoken-query-based QA (SQQA) tasks. Notably, for some of these models, the performance drop from text with audio to pure audio is even greater than the decline observed on modality formats they have never seen during training, such as audio instruction with text input. Interestingly, only handful of models such as Qwen2.5-Omni, GLM-4-Voice, and the Mini-Omni family achieve comparable performance across pure audio and text + audio modalities. Nonetheless, for most current LSLMs, the pure audio configuration remains notably challenging setting. Interleaving and parallel decoding strategies help bridge performance gaps across modality configurations. As shown in Table 3 and Figure 9 (Bottom), nearly all models achieve their best results on the common text instruction with audio input setting. To assess model robustness to modality shifts, we compute the sum of absolute performance differences between this common configuration and the other two input formats. We present the results in ascending order of stability in Figure 9 (Top). Among the eleven models evaluated, the top eight with the exception of Qwen2-Audio and its variant employ interleaving or parallel decoding strategies (see Appendix for benchmarking model details), and exhibit notably reduced modality gaps. These outliers may reflect factors outside of modality combination design. Overall, our results provide empirical evidence that interleaving and parallel decoding can bridge the performance gap between text and audio modalities. Figure 9: Stability and modality-wise performance of LSLMs. Top: Model stability across modality configurations, quantified by the sum of absolute differences between each models average GPT-4o score on the primary modality (TA) and those on the other two modalities, i.e., TA PA + TA AT. Lower values reflect greater robustness to modality variation. Bottom: Average GPT-4o score of each model on the three major modality configurations. Colors are consistent across both plots for direct comparison. Abbreviations: PA = pure audio; TA = text instruction + audio input; AT = audio instruction + text input. Broader task coverage leads to better performance and lower abstention rates. We present the overall performance, closed-ended task performance, and closed-ended task abstention rates for 11 models, alongside the number of tasks each model was trained on, in Figure 10. For models with private, incomplete, or ambiguously reported training data, we use the number of evaluation tasks as proxy for task coverage. The results show that models exposed to wider range of tasks achieve higher performance in both overall and closedended tasks, and fewer abstentions. This finding suggests that, in creating LSLMs for speech understanding, one should diversify the tasks as much as possible, to improve the model performance and reduce the abstention rate. 13 for current LSLMs and highlighting an important area for future improvement. Figure 11: Comparison of LSLM performance on contentcentric versus speaker-centric paralinguistic tasks. Top: For each model, min-max normalized performance scores are shown on content-centric (blue) and speaker-centric (red) tasks, with dumbbell lines indicating the magnitude and direction of intra-model performance differences. Bottom: Average abstention rates (lower is better) for closed-ended tasks in the same two centrics within the paralinguistic category. All evaluations are conducted under the text instruction paired with audio input configuration."
        },
        {
            "title": "5.5 Case Study",
            "content": "Figure 5 provides qualitative evidence of the compositional flexibility and unified modeling offered by our framework. Panel (I) demonstrates that LLaSO-Base seamlessly accommodates all three instruction-input modality pairings. In particular, the pure audio example highlights the systems ability to disentangle instructions from content solely within the audio stream. For the other formats, LLaSO-Base reliably grounds reasoning and response generation in the correct modality, adapting to instructions and content presented in any combination. We present some task prototypes unified by our system in Panel (II) and present more cases across different instruction-input structures and tasks in Appendix E. These examples show that, unlike models limited rigid instruction-input structures, LLaSO-Base generalizes across both categorical and compositional tasks without requiring task-specific modules or post-processing. Figure 10: Task coverage vs. model performance and abstention. Each scatter plot visualizes 11 models by their Task Count (number of training tasks; for models with private, incomplete, or ambiguously reported training data, we use the number of evaluation tasks as proxy for task coverage). (a) Overall performance (min-max normalized over all LLaSOEval tasks, cf. Figure 8). (b) Average Closed-ended task performance. (c) Average abstention rate on closed-ended tasks. Closed-ended scores and abstention rates are calculated only on tasks that require categorical selection. Higher scores indicate better performance; lower abstention rates indicate stronger instruction following. LSLMs may prefer content related tasks. To further analyze model performance on paralinguistic tasks, we present dumbbell plot in Figure 11, contrasting content-centric and speaker-centric results for each model. We observe that most models achieve higher performance and lower abstention rates on content-centric tasks than on speakercentric ones. This disparity likely arises because content-centric tasks are more tightly linked to the semantic content, which LLM-based decoders are naturally equipped to process. In contrast, speakercentric tasks demand more nuanced inference of latent speaker attributes, posing greater challenge 14 the instruction and content as audio, yet responds with counter-question: What is the answer to the question? This indicates that while the model has some prior exposure to SQQA tasks, it fails to correctly interpret or respond to this particular modality configuration where both instruction and content are delivered as audio. In Figure 12 (I)(c), where an audio instruction is followed by textual input, the spoken instruction assigns generic task, prompting the model to generate an answer, while the accompanying text presents tweet and asks for its sentiment, providing two answer options. The relevant information for reasoning is contained within the text input, and the instruction directs the model to perform classification. However, the model does not follow the instruction; instead, it merely repeats the sentiment options, 1. negative 2. positive, without making decision. We observe similar results in Llama-Omni across modality formats, illustrated in Figure 12 (II). This model is primarily trained on pure audio modality, and this is directly reflected in the sampled cases. In (II)(a) where both the instruction and input are delivered as audio, the model answers successfully classifying the object as non-living, demonstrating effective handling of its core modality. Nonetheless, when presented with configurations outside this primary distribution, the model fails to execute the intended tasks. In the text plus audio modality format (II)(b), it is unable to infer the speakers aim from the speech and instead requests further contextual details. Under (II)(c) the modality configuration of audio instruction paired with textual input, the model follows speech instruction but overlooks the explicit negation in the text input and incorrectly judges the two sentences as equivalent. Taken together, this observation highlights the importance of comprehensive modality coverage for multimodal instruction following. To provide an intuitive comparison of model performance on covered versus uncovered tasks, we sample cases for representative baselines in Figure 13. Figure 13 (I) illustrates this contrast for Typhoon-Audio model. In (I)(a) we present sample from the entity extraction task, which is not included in the models training. Here, the query requests identification of entities from the speech, but the model misinterprets the task as speaker gender classification, responding with The speaker is female. We present pronunciation scoring sentence-level (PSSL) task sample in (I)(b), where the model is instructed to evaluate the speech for ac- (I) Three evaluated cases from Salmonn across the primary modality configurations. The correct one corresponds to the models supported training format; the two errors are from modality formats with limited or no training exposure. (II) Three Llama-Omni responses sampled from the three major modality configurations. The correct example comes from its supported format; the two errors are from its unsupported modality configurations. Figure 12: Cases comparing model behaviors under different modality configurations in LLaSO-Eval. These cases highlight the importance of supporting multiple modality configurations to ensure instruction following across real-world scenarios. To better understand the challenges across modality configurations and tasks, we further present cases under the same benchmark. As shown in Figure 12 (I), we sample three representative cases for Salmonn across the primary modality configurations. In case (I)(b) with textual instruction and audio input, the models familiar modality configuration, Salmonn correctly follows the user instruction. The textual instruction asks the model to determine whether the first sentence in the speech content can be used to define the term in the second sentence, and to answer yes or no. The audio provides definition of database in the first sentence and describes MySQL in the second. The model correctly interprets both the instruction and the speech, and returns the correct answer yes. In contrast, when the modality configuration shifts outside the models primary training distribution, distinct failures emerge. Under the pure audio setting (I)(a) the model receives both 15 These findings underscore the essential role of comprehensive task coverage in building models across diverse speech-language tasks."
        },
        {
            "title": "5.6 Ablation",
            "content": "We conduct ablation experiments on different training strategies in LLaSO Corpus, as shown in (i) Alignment Robustness. We Table 5 and 6. evaluate ASR performance both immediately after the alignment stage and following the subsequent instruction-tuning phase. After alignment, the model achieves strong results (WER = 0.05, CER = 0.01). After multi-task instruction tuning, ASR performance declines slightly (WER = 0.08, CER = 0.03), yet remains competitive, due to we include ASR samples within the LLaSO-Instruct dataset for mitigating catastrophic forgetting. (ii) Encoder Fine-tuning. We ablate the effect of unfreezing the audio encoder during the instructionfollowing stage, comparing the results of freezing versus jointly training the encoder, projector, and LLM on LLaSO-Instruct. When the encoder is unfrozen, ASR performance drops more substantially (WER = 0.14, CER = 0.07), relative to the frozen configuration. In contrast, AQA (semantic) tasks see modest improvements (see Table 5), while paralinguistic tasks exhibit slight decline (see Table 6). This suggests that while joint fine-tuning may benefit certain high-level reasoning tasks, it may compromise low-level speech recognition and nuanced paralinguistic abilities."
        },
        {
            "title": "6 Discussion",
            "content": "(I) Three samples from Typhoon-Audio: one from wellrepresented task with correct prediction, and two errors from tasks that are absent in its training. (II) Selected answers from GLM-4-Voice illustrate success on task with ample training exposure and failure on two tasks that fall outside its main training coverage. Figure 13: Each row shows one correct response on wellcovered task and two errors from tasks outside the models main training scope. These examples underscore the necessity of broad task coverage to support instruction following and generalization. curacy, prosody, and fluency. However, it only provides plain transcription and omits the required scoring. In contrast, when evaluated on task present in its training, the model demonstrates accurate performance. For the ASR task presented at (I)(c) the model successfully transcribes the speech to text as instructed without additional information. Similar results are evident with GLM-4-Voice in Figure 13 (II). In (II)(a) tasked with synthetic speech detection, the model avoids providing categorical decision and instead produces an off-topic statement. When we prompt the model for emotion recognition, it generates generic conversational reply, neglecting to engage with the specified sentiment classification task as in (II)(b). Nevertheless, we can observe that the model successfully completes an audio question answering task in (II)(c), owing to the presence of this task in its training. In addition to our quantitative analysis, closer manual inspection of model outputs reveals several distinct patterns and recurring issues that warrant discussion. For example, Qwen2-Audio occasionally misinterprets the SV task as SGC. On other tasks, Although this model achieves relatively strong quantitative scores on the open-ended PP task, qualitative check reveals that large proportion of its outputs are empty strings (41 cases) or single periods (30 cases) among 112 samples in the text instruction plus audio input configuration. For the VC task, the abstention rate is especially high. In our manual review, 82 out of 100 samples in the same modality setting resulted in single period (.) as the response. As to Typhoon-Audio, it occasionally responds in Thai to English prompts, which is likely attributable to the inclusion of Thai data during fine-tuning. Salmonn, when presented Semantic Task Category < Pure Audio >"
        },
        {
            "title": "Tasks",
            "content": "LLaSOBase LLaSO-Base (Unfrozen) (U-F) Metrics"
        },
        {
            "title": "AQA",
            "content": "2.06 1.80 2.39 1.46 1.93 2.57 2.48 1.71 2.74 3.05 2.90 2.58 2.72 2.62 2.28 2.23 3.74 2.60 2.70 2.27 2.27 2.23 1.98 2.19 0.21 0.47 -0.16 0.52 0.26 Avg.GPT-4o GPT-4o < Text, Audio > 2.54 2.42 1.96 2.80 3.09 2.87 2.61 -0.03 -0.06 0.25 0.06 0.04 -0.03 0.03 Avg.GPT-4o GPT-4o < Audio, Text > 3.22 2.84 2.47 2.43 3.68 3.26 2. 0.50 0.22 0.19 0.20 -0.06 0.66 0.28 Avg.GPT-4o GPT-4o Table 5: Ablation results for LLaSO-Base, comparing frozen (F) and unfrozen (U) audio encoder variants during instruction tuning. The table reports (U-F) performance changes on AQA tasks across three major modality configurations; / denote relative gains or drops. Metrics follow earlier tables. with pure audio or audio plus text modality input, often refuses to answer, asks clarifying questions, or claims the audio contains no content; this may stem from its limited exposure to pure audio instruction data (approximately 20K SQQA task samples from WikiQA (Yang et al., 2015)) and to the modality configuration of audio instruction with text input, restricting its ability to generalize. Additionally, in the ASR task, Salmonns outputs for most samples consist entirely of uppercase English letters, which explains its poor quantitative performance on this task. GLM-4-Voice frequently generates responses in Chinese and at times misinterprets the audio input as part of the conversational context, rather than as content to be analyzed. similar pattern is observed in the Mini-Omni family, which occasionally interprets the audio input, such as speakers utterance to be classified, as either an instruction or as primary content. For example, in ASR task with the text plus audio input configuration, approximately 43% of Mini-Omni and 53% of Mini-Omni2 responses begin with phrases like It sounds like..., reflecting tendency to treat the input as dialogue. Meanwhile Llama-Omni exhibits high rate of refusals in tasks beyond AQA, suggesting limited coverage of tasks. Manual inspection of its ASR task outputs further reveals that over 10% of samples are explicit abstains, likely because the model was not trained on ASR data. These phenomena further illustrate the necessity for broad and balanced coverage across both tasks and modality configurations in model development. Audio-Reasoner presents different set of challenges, often exhibiting hallucinated completions such as appending the answer is or fabricating multiple-choice options like E, F, or S. Since it is trained on Qwen2-Audio-Instruct with chainof-thought data, this tendency may stem from exposure to reasoning-style supervision. Kimi-Audio, when performing the PR task, sometimes treats the input as an ASR query or outputs sequence of isolated phonemes, which leads to lower evaluation scores. Qwen2.5-Omni occasionally shows similar confusion between PR and ASR, and routinely appends conversational phrases like feel free to ask me more. While such additions might be intended to improve user interaction, they can undermine instruction following if overfitting, as some tasks in our benchmark explicitly requires models to return only the answer. Taken together, these observations offer practical insights into persistent issues in instruction following, task differentiation, and output format consistency. We hope these manual inspections are helpful for the community and can inform future model development and evaluation."
        },
        {
            "title": "7 Conclusion",
            "content": "Despite recent advances, progress in LSLMs has been hampered by fragmented resources, limited task coverage, and lack of standardized evaluation. To address these challenges, we present LLaSO: fully open, end-to-end framework comprising LLaSO-Align and LLaSO-Instruct, totaling 25.5M samples for alignment and instruction tunning, comprehensive benchmark LLaSO-Eval, with over 15K stratified samples, and robust 3.8Bparameter reference model LLaSO-Base adapted from vision-language foundations. LLaSO substantially exceeds existing open-source resources in both task breadth and modality coverage, supporting 20 tasks across all major configuration types. 17 Speaker-Centric Content-Centric"
        },
        {
            "title": "Paralinguistic Task Category",
            "content": "< Text, Audio >"
        },
        {
            "title": "Tasks",
            "content": "LLaSOBase LLaSO-Base (Unfrozen) (U-F) Metrics Tasks LLaSOBase LLaSO-Base (Unfrozen) (U-F) Metrics 0.96 0.99 0.76 0.91 0.91 0.52 0.83 0.73 0.69 0.70 0.50 0.60 10.32 0.48 0.48 0.48 0.17 0.26 0.22 0.99 0.32 2.90 0.39 2."
        },
        {
            "title": "SGC",
            "content": "AC AR"
        },
        {
            "title": "EIE",
            "content": "ER"
        },
        {
            "title": "PSSL",
            "content": "0.88 0.99 0.61 0.97 0.86 0.40 0.86 0.78 0.68 0.68 0.38 0.53 8.78 0.45 0.37 0.41 0.16 0.30 0.23 0.99 0.16 2.68 0.24 2.66 PR"
        },
        {
            "title": "SCR",
            "content": "ACC -0.08 0.00= -0.15 0.06 -0.05 Avg.ACC IP -0.12 EE 0.03 VSC 0.05 IC -0.01 Avg.ACC ISC -0.02 PP -0.12 VC ACC ACC 0.03 0.04 0.02 4.86 3.93 3.57 0.78 0.50 0.60 8.02 0.18 0.03 0.05 0.04 4.80 3.90 3.44 0.82 0.55 0.77 10.55 0. GPT-4o 0.00= PER 0.01 WER 0.02 CER -0.06 -0.03 -0.13 0.04 0.05 0.17 2.53 MAE 0.02 ACC ACC Linguistic Task Category < Text, Audio >"
        },
        {
            "title": "ASR",
            "content": "LLaSOBase LLaSO-Base (Unfrozen) (U-F) Metrics 0.08 0.03 0.14 0.07 0.06 WER 0.04 CER"
        },
        {
            "title": "Tasks",
            "content": "LLaSO-Base (Aligned) LLaSOBase (F-A) Metrics"
        },
        {
            "title": "ASR",
            "content": "0.05 0.01 0.08 0.03 0.03 WER 0.02 CER ACC -0.07 Avg.ACC -1.54 MAE -0.03 -0.11 -0.07 Avg.ACC -0.01 0.04 0.01 Avg.ACC 0.00= -0.16 -0.22 GPT-4o -0.15 -0.14 GPT-4o ACC ACC ACC ACC Table 6: Ablation results for LLaSO-Base on paralinguistic (speaker-centric and content-centric) and linguistic tasks, all evaluated under the text instruction with audio input modality. Frozen (F) and unfrozen (U) refer to whether the audio encoder is fixed or updated during instruction tuning, respectively. (U-F) reports the performance change between unfrozen and frozen encoder variants during finetuning, while (F-A) compares results after finetuning with frozen encoder (F) and after the alignment stage (A) for ASR. / denote gains or drops; metrics follow previous tables. The aligned evaluation suite ensures fair, systematic assessment consistent with the training data. By releasing all data, models, and evaluation tools, LLaSO enables reproducible research, fair comparison, and broad community engagement. We hope it serves as catalyst for unifying the field and supporting the community in compositional speech-language modeling."
        },
        {
            "title": "References",
            "content": "Vaibhav Adlakha, Parishad BehnamGhader, Xing Han Lu, Nicholas Meade, and Siva Reddy. 2024. Evaluating correctness and faithfulness of instructionfollowing models for question answering. Preprint, arXiv:2307.16877. Afroz Ahamad, Ankit Anand, and Pranesh Bhargava. 2020. Accentdb: database of non-native english accents to assist neural speech recognition. arXiv preprint arXiv:2005.07973. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, and 8 others. 2022. Flamingo: visual language model for few-shot learning. Preprint, arXiv:2204.14198. Rosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler, Josh Meyer, Reuben Morais, Lindsay Saunders, Francis Tyers, and Gregor Weber. 2019. Common voice: massivelyarXiv preprint multilingual arXiv:1912.06670. speech corpus. Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. 2023. Openflamingo: An open-source framework for training large autoregressive vision-language models. Preprint, arXiv:2308.01390. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023. Qwen-vl: versatile visionlanguage model for understanding, localization, text reading, and beyond. Preprint, arXiv:2308.12966. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, and 8 others. 2025. Qwen2.5-vl technical report. Preprint, arXiv:2502.13923. Brunskill, and 1 others. 2021. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258. brianlan. 2017. This script calculates Phoneme Error Rate using package leven (edit distance algorithm). Users can also choose whether to to the paper Speakermerge phonemes (refer independent hidden Markov models) during calculation. https://gist.github.com/ gist.github.com. brianlan/954082454b25d7eb7abd612ba61c6a66. [Accessed 15-07-2025]. recognition phone using Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, and 1 others. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901. Houwei Cao, David Cooper, Michael Keutmann, Ruben Gur, Ani Nenkova, and Ragini Verma. 2014. Crema-d: Crowd-sourced emotional multimodal actors dataset. IEEE transactions on affective computing, 5(4):377390. Junbum Cha, Wooyoung Kang, Jonghwan Mun, and Honeybee: LocalityByungseok Roh. 2024. enhanced projector for multimodal llm. Preprint, arXiv:2312.06742. Sahil Chaudhary. 2023. Code alpaca: An instructionfollowing llama model for code generation. https: //github.com/sahil280114/codealpaca. Guoguo Chen, Shuzhou Chai, Guanbo Wang, Jiayu Du, Wei-Qiang Zhang, Chao Weng, Dan Su, Daniel Povey, Jan Trmal, Junbo Zhang, and 1 others. 2021. Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio. arXiv preprint arXiv:2106.06909. Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. 2023a. Minigpt-v2: large language model as unified interface for vision-language multi-task learning. Preprint, arXiv:2310.09478. Stanley Chen, Douglas Beeferman, and Roni Rosenfeld. 1998. Evaluation metrics for language models. Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and Verena Rieser. 2020. Slurp: spoken language understanding resource package. arXiv preprint arXiv:2011.13205. Ye Chen, Wei Cai, Liangmin Wu, Xiaowei Li, Zhanxuan Xin, and Cong Fu. 2023b. Tigerbot: An arXiv preprint open multilingual multitask llm. arXiv:2312.08688. Mathieu Bernard and Hadrien Titeux. 2021. Phonemizer: Text to phones transcription for multiple languages in python. Journal of Open Source Software, 6(68):3958. Yiming Chen, Xianghu Yue, Chen Zhang, Xiaoxue Gao, Robby T. Tan, and Haizhou Li. 2024. Voicebench: Benchmarking llm-based voice assistants. Preprint, arXiv:2410.17196. Rishi Bommasani, Drew Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, Chang Zhou, and Jingren Zhou. 19 2024. Qwen2-audio technical report. Preprint, arXiv:2407.10759. Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou. 2023. Qwen-audio: Advancing universal audio understanding via unified large-scale audiolanguage models. Preprint, arXiv:2311.07919. Federico Cocchi, Nicholas Moratelli, Davide Caffagni, Sara Sarto, Lorenzo Baraldi, Marcella Cornia, and Rita Cucchiara. 2025. Llava-more: comparative study of llms and visual backbones for enhanced visual instruction tuning. Preprint, arXiv:2503.15621. Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. 2023. Free dolly: Introducing the worlds first truly open instructiontuned llm. Alexandre Défossez, Laurent Mazaré, Manu Orsini, Amélie Royer, Patrick Pérez, Hervé Jégou, Edouard Grave, and Neil Zeghidour. 2024. Moshi: speechtext foundation model for real-time dialogue. arXiv preprint arXiv:2410.00037. Zhihao Du, Jiaming Wang, Qian Chen, Yunfei Chu, Zhifu Gao, Zerui Li, Kai Hu, Xiaohuan Zhou, Jin Xu, Ziyang Ma, and 1 others. 2023. Lauragpt: Listen, attend, understand, and regenerate audio with gpt. arXiv preprint arXiv:2310.04673. Benjamin Elizalde, Soham Deshmukh, and Huaming Wang. 2024. Natural language supervision for general-purpose audio representations. Preprint, arXiv:2309.05767. Jesse Engel, Cinjon Resnick, Adam Roberts, Sander Dieleman, Douglas Eck, Karen Simonyan, and Mohammad Norouzi. 2017. Neural audio synthesis of musical notes with wavenet autoencoders. Qingkai Fang, Shoutao Guo, Yan Zhou, Zhengrui Ma, Shaolei Zhang, and Yang Feng. 2025. Llama-omni: Seamless speech interaction with large language models. Preprint, arXiv:2409.06666. Chaoyou Fu, Yi-Fan Zhang, Shukang Yin, Bo Li, Xinyu Fang, Sirui Zhao, Haodong Duan, Xing Sun, Ziwei Liu, Liang Wang, and 1 others. 2024. Mme-survey: comprehensive survey on evaluation of multimodal llms. arXiv preprint arXiv:2411.15296. Sreyan Ghosh, Zhifeng Kong, Sonal Kumar, Sakshi, Jaehyeon Kim, Wei Ping, Rafael Valle, Dinesh Manocha, and Bryan Catanzaro. 2025. Audio flamingo 2: An audio-language model with longaudio understanding and expert reasoning abilities. arXiv preprint arXiv:2503.03983. Yuan Gong, Sameer Khurana, Leonid Karlinsky, and James Glass. 2023a. Whisper-at: Noise-robust automatic speech recognizers are also strong general audio event taggers. arXiv preprint arXiv:2307.03183. Yuan Gong, Alexander Liu, Hongyin Luo, Leonid Karlinsky, and James Glass. 2023b. Joint audio and speech understanding. In 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 18. IEEE. Yuan Gong, Jin Yu, and James Glass. 2022. Vocalsound: dataset for improving human vocal sounds recognition. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 151155. IEEE. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, and 542 others. 2024. The llama 3 herd of models. Preprint, arXiv:2407.21783. Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. 2022. Unnatural instructions: Tuning language models with (almost) no human labor. arXiv preprint arXiv:2212.09689. Cheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha Kembhavi, and Ranjay Krishna. 2023. Sugarcrepe: Fixing hackable benchmarks for vision-language compositionality. Preprint, arXiv:2306.14610. Kaiyi Huang, Chengqi Duan, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. 2025. T2icompbench++: An enhanced and comprehensive benchmark for compositional text-to-image generation. Preprint, arXiv:2307.06350. huggingface. 2023. Evaluation metrics for ASR - Hugging Face Audio Course hugginghttps://huggingface.co/learn/ face.co. audio-course/chapter5/evaluation. [Accessed 15-07-2025]. Keith Ito and Linda Johnson. 2017. The lj https://keithito.com/ speech LJ-Speech-Dataset/. dataset. Feng Jiang, Zhiyu Lin, Fan Bu, Yuhao Du, Benyou Wang, and Haizhou Li. 2025. S2s-arena, evaluating speech2speech protocols on instruction following with paralinguistic information. Preprint, arXiv:2503.05085. Jordan. 2023. Whitefox44/alpacagpt3.5customized datasets at hugging face. KimiTeam, Ding Ding, Zeqian Ju, Yichong Leng, Songxiang Liu, Tong Liu, Zeyu Shang, Kai Shen, Wei Song, Xu Tan, Heyi Tang, Zhengtao Wang, Chu Wei, Yifei Xin, Xinran Xu, Jianwei Yu, Yutao Zhang, Xinyu Zhou, Y. Charles, and 21 others. 2025. Kimiaudio technical report. Preprint, arXiv:2504.18425. Zhifeng Kong, Arushi Goel, Rohan Badlani, Wei Ping, Rafael Valle, and Bryan Catanzaro. 2024a. Audio flamingo: novel audio language model with 20 few-shot learning and dialogue abilities. Preprint, arXiv:2402.01831. Zhifeng Kong, Arushi Goel, Rohan Badlani, Wei Ping, Rafael Valle, and Bryan Catanzaro. 2024b. Audio flamingo: novel audio language model with fewshot learning and dialogue abilities. arXiv preprint arXiv:2402.01831. Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. 2023a. Llava-med: Training large language-and-vision assistant for biomedicine in one day. Preprint, arXiv:2306.00890. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023b. Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. Preprint, arXiv:2301.12597. Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu Sun, Lingpeng Kong, and Qi Liu. 2023c. M3it: large-scale dataset towards multimodal multilingual instruction tuning. Preprint, arXiv:2306.04387. Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. 2023. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122. Junyan Lin, Haoran Chen, Dawei Zhu, and Xiaoyu Shen. 2024. To preserve or to compress: An in-depth study of connector selection in multimodal large language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 56665680, Miami, Florida, USA. Association for Computational Linguistics. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023a. Visual instruction tuning. Advances in neural information processing systems, 36:34892 34916. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b. Visual instruction tuning. Preprint, arXiv:2304.08485. Yinghao Ma, Anders Øland, Anton Ragni, Bleiz MacSen Del Sette, Charalampos Saitis, Chris Donahue, Chenghua Lin, Christos Plachouras, Emmanouil Benetos, Elona Shatri, and 1 others. 2024. Foundation models for music: survey. arXiv preprint arXiv:2408.14340. Zixian Ma, Jerry Hong, Mustafa Omer Gul, Mona Gandhi, Irena Gao, and Ranjay Krishna. 2023. Crepe: Can vision-language foundation models reason compositionally? Preprint, arXiv:2212.07796. Potsawee Manakul, Guangzhi Sun, Warit Sirichotedumrong, Kasima Tharnpipitchai, and Kunat Pipatanakul. 2024. Enhancing low-resource language and instruction following capabilities of audio language models. Preprint, arXiv:2409.10999. Xinhao Mei, Chutong Meng, Haohe Liu, Qiuqiang Kong, Tom Ko, Chengqi Zhao, Mark D. Plumbley, Yuexian Zou, and Wenwu Wang. 2024. Wavcaps: chatgpt-assisted weakly-labelled audio captioning dataset for audio-language multimodal research. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 32:33393354. Andrew Morris, Viktoria Maier, and Phil Green. 2004. From wer and ril to mer and wil: improved evaluation measures for connected speech recognition. Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. 2023. Orca: Progressive learning from complex explanation traces of gpt-4. arXiv preprint arXiv:2306.02707. Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. 2017. Voxceleb: large-scale speaker identification dataset. arXiv preprint arXiv:1706.08612. Gonzalo Navarro. 2001. guided tour to approximate string matching. ACM computing surveys (CSUR), 33(1):3188. OpenAI. 2024. Hello gpt-4o. Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015. Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 52065210. IEEE. Prabhat Pandey, Rupak Vignesh Swaminathan, KV Girish, Arunasish Sen, Jian Xie, Grant Strimel, and Andreas Schwarz. 2025. Sift-50m: large-scale multilingual dataset for speech instruction fine-tuning. arXiv preprint arXiv:2504.09081. Daniel Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin Cubuk, and Quoc Le. 2019. Specaugment: simple data augmentation method for automatic speech recognition. arXiv preprint arXiv:1904.08779. Jing Peng, Yucheng Wang, Bohan Li, Yiwei Guo, Hankun Wang, Yangui Fang, Yu Xi, Haoyu Li, Xu Li, Ke Zhang, Shuai Wang, and Kai Yu. 2025. survey on speech large language models for understanding. Preprint, arXiv:2410.18908. Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, Erik Cambria, and Rada Mihalcea. 2018. Meld: multimodal multi-party dataset for emotion recognition in conversations. arXiv preprint arXiv:1810.02508. Vineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, and Ronan Collobert. 2020. Mls: largescale multilingual dataset for speech research. ArXiv, abs/2012.03411. 21 Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, and 1 others. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2022. Robust speech recognition via large-scale weak supervision. Preprint, arXiv:2212.04356. Ricardo Reimao and Vassilios Tzerpos. 2019. For: dataset for synthetic speech detection. In 2019 International Conference on Speech Technology and Human-Computer Dialogue (SpeD), pages 110. IEEE. Tiger Research. 2023. Tigerresearch/tigerbot-alpaca-en50k datasets at hugging face. Andrew Rouditchenko, Saurabhchand Bhati, Edson Araujo, Samuel Thomas, Hilde Kuehne, Rogerio Feris, and James Glass. 2025. Omni-r1: Do you really need audio to fine-tune your audio llm? arXiv preprint arXiv:2505.09439. RuterNorway. 2023. RuterNorway/OpenOrcaNo15k Datasets at Hugging Face hugginghttps://huggingface.co/datasets/ face.co. RuterNorway/OpenOrcaNo-15k. [Accessed 17-042025]. Yi Su, Jisheng Bai, Qisheng Xu, Kele Xu, and Yong Dou. 2025. Audio-language models for audio-centric tasks: survey. arXiv preprint arXiv:2501.15177. Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, and Chao Zhang. 2024. Salmonn: Towards generic hearing abilities for large language models. Preprint, arXiv:2310.13289. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https:// github.com/tatsu-lab/stanford_alpaca. Christophe Veaux, Junichi Yamagishi, Kirsten MacDonald, and 1 others. 2016. Superseded-cstr vctk corpus: English multi-speaker corpus for cstr voice cloning toolkit.(2016). URL http://datashare. is. ed. ac. uk/handle/10283/2651. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. 2024. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. Preprint, arXiv:2409.12191. Pete Warden. 1804. Speech commands: dataset for limited-vocabulary speech recognition. arxiv 2018. arXiv preprint arXiv:1804.03209. Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Marianna Nezhurina, Taylor Berg-Kirkpatrick, and Shlomo Dubnov. 2024. Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation. Preprint, arXiv:2211.06687. Zhifei Xie, Mingbao Lin, Zihang Liu, Pengcheng Wu, Shuicheng Yan, and Chunyan Miao. 2025. Audioreasoner: Improving reasoning capability in large audio language models. Preprint, arXiv:2503.02318. Zhifei Xie and Changqiao Wu. 2024a. Mini-omni: Language models can hear, talk while thinking in streaming. Preprint, arXiv:2408.16725. Zhifei Xie and Changqiao Wu. 2024b. Mini-omni2: Towards open-source gpt-4o with vision, speech and duplex capabilities. Preprint, arXiv:2410.11190. Xinchen-ai. 2024. GitHub - xinchen-ai/Westlake-Omni github.com. https://github.com/xinchen-ai/ Westlake-Omni. [Accessed 21-06-2025]. Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, Bin Zhang, Xiong Wang, Yunfei Chu, and Junyang Lin. 2025. Qwen2.5-omni technical report. Preprint, arXiv:2503.20215. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, and 41 others. 2025. Qwen3 technical report. Preprint, arXiv:2505.09388. Yi Yang, Wen-tau Yih, and Christopher Meek. 2015. WikiQA: challenge dataset for open-domain quesIn Proceedings of the 2015 Contion answering. ference on Empirical Methods in Natural Language Processing, pages 20132018, Lisbon, Portugal. Association for Computational Linguistics. Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. 2024. survey on multimodal large language models. National Science Review, 11(12). Chien yu Huang, Ke-Han Lu, Shih-Heng Wang, ChiYuan Hsiao, Chun-Yi Kuan, Haibin Wu, Siddhant Arora, Kai-Wei Chang, Jiatong Shi, Yifan Peng, Roshan Sharma, Shinji Watanabe, Bhiksha Ramakrishnan, Shady Shehata, and Hung yi Lee. 2024. Dynamic-superb: Towards dynamic, collaborative, and comprehensive instruction-tuning benchmark for speech. Preprint, arXiv:2309.09510. Aohan Zeng, Zhengxiao Du, Mingdao Liu, Kedong Wang, Shengmin Jiang, Lei Zhao, Yuxiao Dong, and Jie Tang. 2024a. Glm-4-voice: Towards intelligent 22 refining instruction-tuned speech-language models for diverse languages, scenarios, and real-world applications."
        },
        {
            "title": "B Ethical Statement",
            "content": "B.1 Data Privacy and Consent All training and evaluation data are sourced solely from publicly available datasets, with no use of private or personally identifiable information. Synthetic, TTS, and sound effect samples contain no human-identifiable content. No re-identification or de-anonymization was performed at any stage. All data handling complies with ethical standards and legal requirements. B.2 Licensing and Responsible Use All data, code, and model weights are released under permissive open-source licenses, with explicit terms governing use and redistribution. The resources are intended for academic, non-commercial research, and must be used in accordance with ethical standards and applicable copyright laws. B.3 Diversity and Representativeness We strive for diversity in gender, age, accent, language, and emotion across both collected and synthetic data, employing balanced sampling where possible. Nonetheless, certain groups and languages remain underrepresented, and we acknowledge the risk of bias. We encourage the community to further augment and improve coverage. All datasets and models are released without representing the views or interests of any particular group or institution. B.4 Fairness and Misuse Prevention Our models and datasets may exhibit uneven performance across different tasks, languages, or demographic groups, and should not be considered universally fair or unbiased. We explicitly prohibit the use of our work for surveillance, discrimination, harassment, or any activities that may harm individuals or communities. We encourage responsible research and deployment that respects the rights and dignity of all users. and human-like end-to-end spoken chatbot. Preprint, arXiv:2412.02612. Yunan Zeng, Yan Huang, Jinjin Zhang, Zequn Jie, Zhenhua Chai, and Liang Wang. 2024b. Investigating compositional challenges in vision-language modIn Proceedings of the els for visual grounding. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1414114151. Junbo Zhang, Zhiwen Zhang, Yongqing Wang, Zhiyong Yan, Qiong Song, Yukai Huang, Ke Li, Daniel Povey, and Yujun Wang. 2021. speechocean762: An open-source non-native english speech corpus for pronunciation assessment. In Proc. Interspeech 2021. Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. 2024. Speechtokenizer: Unified speech tokenizer for speech large language models. Preprint, arXiv:2308.16692. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023. Minigpt-4: Enhancing vision-language understanding with advanced large language models. Preprint, arXiv:2304.10592."
        },
        {
            "title": "A Limitation",
            "content": "While our work establishes unified, open-source foundation for compositional speech-language instruction tuning, several limitations remain. First, LLaSO Corpus is currently limited to English, which constrains its direct applicability to nonEnglish and low-resource languages. Extending the dataset and benchmark to multilingual scenarios is an important direction for achieving broader impact and inclusivity. Second, despite surpassing prior datasets in task diversity and modality coverage, the granularity and availability of source materials inherently influence our corpus composition, particularly in the underrepresented paralinguistic categories and rare interaction scenarios. Third, our reference model, LLaSO-Base, intentionally prioritizes reproducibility and extensibility over achieving SOTA performance. Consequently, its architecture and model size (3.8 billion parameters) are modest compared to larger models, and our evaluations have primarily included similarly sized or smaller baselines. Assessing and benchmarking significantly larger LSLMs would provide further insights into scaling behaviors and capabilities. Fourth, certain challenging multimodal interactions such as open-ended dialogues involving overlapping speech, or zero-shot generalization to entirely new domains are only partially addressed within our current benchmark and model architecture. We encourage the research community to build upon our foundation to tackle these limitations, further"
        },
        {
            "title": "C Benchmarking Candidates",
            "content": "Qwen2-Audio. LSLM from Qwen Team designed for both audio analysis and voice chat. It integrates Whisper-large-v3 audio encoder with Qwen-7B language model, enabling processing of audio and text inputs for instruction following and conversational tasks. The model supports both audio + text and pure audio modality configurations, automatically distinguishing between analysis and dialogue modes without explicit prompts It achieves state-of-the-art results such as AIR-Bench and CoVoST2, with open-source demos, weights, and inference code. Typhoon-Audio. LSLM from SCB 10X and the University of Cambridge supporting both English and Thai. It integrates Whisper-large-v3 (fine-tuned for Thai) and BEATs audio encoders, Q-Former adapter, and Typhoon-1.5-8B-Instruct LLM. The model supports both text-audio and pure audio (namely speech instruction following in this paper) configurations. Demo, model weights, and inference code are open-source. Salmonn. Salmonn is an unified LSLM with dual-encoder architecture, Whisper and BEATs, linked via window-level Query Transformer to Vicuna-based LLM. The model is trained in three task levels using two-stage alignment and instruction-tuning scheme, and further enhanced through activation tuning to unlock emergent capabilities. Salmonn supports audio-plus-text and pure audio (through SQQA) modality configurations and diverse task types. Demos, model checkpoints, training/inference code, and training data are all publicly available. Glm-4-Voice. An end-to-end spoken chatbot supporting both Chinese and English from Zhipu.AI and Tsinghua University. The model combines Glm-4-9B-Base with supervised speech tokenizer and flow-matching speech decoder, pre-trained on 1T tokens of speech-text and speech-only data. Fine-tuned with streaming-thoughts template, it alternates between text and speech tokens for seamless, low-latency conversational output. GLM-4-Voice accepts speech or text inputs and produces simultaneous speech and text responses. Model weights, demo, and inference code are open-source. Mini-Omni. Developed by Inspirai and Tsinghua University, Mini-Omni is streaming speech-tospeech conversational LLM integrating Whisper-small encoder, modality adapters, Qwen2-0.5B transformer language model, and TTS adapter. The system employs parallel decoding for efficient, real-time, end-to-end speech input and streaming audio output. Model weights, inference code, demo, and the VoiceAssistant-400K dataset are open-source. Mini-Omni2. An omni-interactive multimodal model, developed by Inspirai and Tsinghua University as an upgraded version of Mini-Omni, combining CLIP (ViT-B/32) for vision, Whisper-small for audio, and Qwen2-0.5B for language. It enables real-time, end-to-end voice conversations with users, supporting image, audio, and text inputs and text, audio outputs. The model, inference and demo code are open-source. Llama-Omni. Developed by ICTNLPLab at CAS, this model integrates frozen Whisper-large-v3 encoder, trainable speech adaptor, Llama-3.1-8B-Instruct language model, and streaming speech decoder. Its key innovation is simultaneous generation of both text and speech responses from spoken instructions, enabling low-latency, end-to-end speech-to-text and speech-to-speech interaction. The model, along with its training data, weights, demo, and inference code, is open-source. Audio-Reasoner. reasoning-oriented LSLM developed by fine-tuning Qwen2-Audio with structured chain-of-thought (CoT) supervision on its 1.2M-sample CoTA dataset. Emphasizing complex audio reasoning, it demonstrates the benefits of CoT-style instruction tuning, achieving competitive results including MMAU-mini and AIR-Bench-Chat. It is open-source along with its model checkpoint, demo, inference code, and dataset. Kimi-Audio. An audio foundation model developed by the Kimi Team featuring hybrid architecture with an audio tokenizer, audio encoder, core audio LLM, parallel heads for both text and audio generation, and an audio detokenizer, using continuous acoustic vectors and discrete semantic tokens. Pre-trained 24 on 13 million hours of diverse open and in-house audio, the model is fine-tuned for multimodal comprehension and generation tasks involving speech, music, and sound effects, including audio understanding, speech conversation, and audio-to-text chat. It demonstrates strong performance on benchmarks such as VoiceBench, VocalSound, and MELD. The project is open-source, providing demo data, fine-tuning and inference code, released model weights, and an audio evaluation toolkit. Qwen2.5-Omni. unified end-to-end real-time multimodal model developed by the Qwen team, supporting text, audio, image, and video inputs, with both streaming text and speech outputs. Built on the Thinker-Talker architecture, it enables flexible cross-modal interactions and streaming, facilitated by TMRoPE and block-wise encoding for efficient temporal alignment. The model achieves strong performance on diverse multimodal benchmarks like VoiceBench and MMAU and open-source with released weights, APIs, and inference code."
        },
        {
            "title": "URL",
            "content": "#Params Supported Modalities"
        },
        {
            "title": "Interleaving or Parallel Decoding",
            "content": "Qwen2-Audio Typhoon-Audio Salmonn Glm-4-Voice Mini-Omni Mini-Omni2 Llama-Omni Audio-Reasoner Kimi-Audio Qwen2.5-Omni Qwen/Qwen2-Audio-7B-Instruct [Model Card] scb10x/llama-3-typhoon-v1.5-8b-audio-preview [Model Card] [Model Card] tsinghua-ee/SALMONN-7B [Model Card] THUDM/glm-4-voice-9b [Model Card] gpt-omni/mini-omni [Model Card] gpt-omni/mini-omni2 [Model Card] ICTNLP/Llama-3.1-8B-Omni [Model Card] zhifeixie/Audio-Reasoner [Model Card] moonshotai/Kimi-Audio-7B-Instruct [Model Card] Qwen/Qwen2.5-Omni-7B 7B 8B 7B 9B 0.5B 0.5B 8B 7B 7B 7B T, A, PA T, A, PA T, A, PA T, A, PA T, A, PA T, A, PA PA T, A, PA T, A, PA T, A, PA - - - Interleaving Parallel Parallel Parallel - Interleaving and Parallel Interleaving and Parallel Table 7: Details of Benchmarking Candidates in this work. Each row lists tested models official HuggingFace repository (Model Card link), parameter size, supported modalities (T, = Textual Instruction + Audio Input; PA = Pure Audio), and whether Interleaving or Parallel multi-modal decoding is applied."
        },
        {
            "title": "D Task Category Definitions",
            "content": "To facilitate comprehensive and interpretable evaluation, both our training and evaluation datasets are systematically organized into three principal categories: linguistic, semantic, and paralinguistic. This categorization is designed to capture the spectrum of speech-language understanding, from core speech processing and factual reasoning to the nuanced interpretation of speaker traits and acoustic context. Next will describe the definitions of each categories. D.1 Linguistic Category Linguistic tasks are aimed at assessing models basic speech processing ability, primarily through ASR. This foundational category evaluates how accurately model can transcribe spoken language into text, serving as the backbone for subsequent semantic or paralinguistic inference. D.2 Semantic Category The semantic category tests models ability to extract explicit meaning and perform higher-level reasoning over audio input. In our benchmark, this is represented by the AQA task, which requires models to interpret audio content, combine it with contextual cues, and deliver factual or reasoning-based responses. Although limited to AQA, this category is critical for evaluating the transition from basic perception to comprehension and inference. D.3 Paralinguistic Category Paralinguistic tasks are structured to probe models sensitivity to information that lies beyond the literal linguistic content. We further distinguish between speaker-centric and content-centric paralinguistic tasks. Speaker-centric tasks focus on characteristics inherent to the speaker such as gender, age, accent, emotion, and identity capturing traits that are independent of the message being delivered. In contrast, content-centric tasks emphasize cues embedded in the audio signal that reflect content or context, such as phoneme recognition, intent prediction, or entity extraction, irrespective of speaker identity."
        },
        {
            "title": "E Cases",
            "content": "Figure 14: Representative case examples from LLaSO-Base demonstrating the three modality configurations in LLaSO-Eval: pure audio (left), text instruction with audio input (middle), and audio instruction with text input (right). Each column presents distinct tasks under its respective format."
        },
        {
            "title": "F Training Details",
            "content": "F.1 Training Configuration"
        },
        {
            "title": "Parameter",
            "content": "Stage 1: Modality Alignment Stage 2: Instruction Tuning Device Model Backbone Audio Encoder Audio Projector Pretrain Audio Aligner Tune Audio Encoder Tune Audio Projector Tune LLM Epochs Global Batch Size Learning Rate Weight Decay Warmup Ratio LR Scheduler Max Grad Norm BF16 Model Max Length 4 NVIDIA A800 Llama-3.2-3B-Instruct Whisper-large-v3 MLP (2-layer, GELU) False True False 1 256 1 103 0.0 0.01 Cosine 1.0 True 2048 4 NVIDIA A800 Llama-3.2-3B-Instruct Whisper-large-v3 MLP (2-layer, GELU) Aligner Checkpoint (from Stage 1) True/False (optional, see ablation) True True 1 128 3 105 0.0 0.01 Cosine 1.0 True 2048 Table 8: Training hyperparameters for LLaSO-Base. Stage 1 performs cross-modality alignment, while Stage 2 instruction-tunes the unified model. F.2 Training Loss Figure 15: Training loss visualization with Raw Loss and Smoothed Loss. From left to right: (1) alignment stage; (2) instruction tuning stage with frozen encoder; (3) instruction tuning stage with unfrozen encoder."
        },
        {
            "title": "G System Prompt",
            "content": "System Prompt for LLaSO-Base chat between curious user and an artificial intelligence assistant. The assistant is able to understand the audio content that the user provides, and assist the user with variety of tasks using natural language. The audio content will be provided with the following format: <Audio>audio content</Audio>. Box 16: Our system prompt for training and evaluation. <Audio> and </Audio> are added into the tokenizer vocabulary as special tokens."
        },
        {
            "title": "Prompt Style",
            "content": "Closed-ended Instruction Examples"
        },
        {
            "title": "Contextualized",
            "content": "Classify the instrument in this audio clip. Choose only from: bass, brass, flute, guitar, keyboard, mallet, organ, reed, string, synth lead, vocal. Output only the label.<audio> For music classification project, identify the primary instrument in this audio. Return only one of the following: bass, brass, flute, guitar, keyboard, mallet, organ, reed, string, synth lead, vocal.<audio>"
        },
        {
            "title": "Stylistic Variation",
            "content": "What is the primary instrument in this audio clip? Respond only with one of: bass, brass, flute, guitar, keyboard, mallet, organ, reed, string, synth lead, or vocal.<audio> Fine-grained Task Focus only on the instrumental characteristics and determine the correct classification. Output just one label from bass, brass, flute, guitar, keyboard, mallet, organ, reed, string, synth lead, vocal.<audio>"
        },
        {
            "title": "Prompt Style",
            "content": "Open-ended Instruction Examples"
        },
        {
            "title": "Contextualized",
            "content": "Convert the speech in this audio file into an IPA phonemic sequence. Return phonemes only.<audio> linguist is analyzing speech samples. Your task is to transcribe the provided audio into an IPA phonemic sequence. Return phonemes only.<audio>"
        },
        {
            "title": "Stylistic Variation",
            "content": "Help build pronunciation guide by converting this audio into IPA phonemes. Return only the phonemes.<audio> Fine-grained Task Phonetic decoding task: transcribe the provided speech into IPA phonemes and return them without any additional output.<audio> Table 9: Representative prompts illustrating the four instruction styles used in our corpus. The closed-ended examples (top) are drawn from the Instrument Classification (IC) task, while the open-ended examples (bottom) are from the Phoneme Recognition (PR) task. Each style, Standardized (direct instructions), Contextualized (scenariodriven), Stylistic Variation (diverse linguistic formulations), and Fine-grained Task (specific sub-aspect focus), is designed to promote compositional generalization across tasks and formats. 28 Multi-granularity Setting Details Coarse-grained (10-year spans) Medium-grained (5-year spans) Fine-grained (exact age) Categories: eighties, fifties, forties, nineties, seventies, sixties, teens, thirties, twenties Analyze the speakers voice and determine their age category. Respond only with one of the following: eighties, fifties, fourties, nineties, seventies, sixties, teens, thirties, or twenties.<audio> speech-based recommendation system needs to identify user age. Analyze the voice and classify it into the correct age group from eighties, fifties, fourties, nineties, seventies, sixties, teens, thirties, or twenties.<audio> If the speakers age appears ambiguous, classify them into the closest matching age group. Select only one label - eighties, fifties, fourties, nineties, seventies, sixties, teens, thirties, or twenties.<audio> the energy levels, Analyze speech rate, and vocal strain in the voice to determine the most accurate age category. Provide only the label from eighties, fifties, fourties, nineties, seventies, sixties, teens, thirties, or twenties. Categories: 15-19, 20-24, 25-29, 30+ Range: integer between 18 and 80 Based on the audio, identify the speakers age group. Select one of the following age groups only return: 15-19, 20-24, 25-29, 30+.<audio> Estimate the age of the speaker from the human vocal sounds in this audio clip. Respond with the age only, between 18 and 80.<audio> Can you guess the age group of the speaker in this clip? Please select from the following age groups only return: 15-19, 2024, 25-29, 30+.<audio> Using this audio, which contains human vocalizations, estimate the speakers age. Respond with the age as an integer between 18 and 80, no extra information.<audio> Based on the audio, what age group is being used? Pick only return from: 15-19, 20-24, 25-29, 30+.<audio> Listen to this sound sample of human vocalizations and predict the speakers age as number between 18 and 80. Provide the age only.<audio> From the following audio, can you determine the speakers age group? Options only return: 1519, 20-24, 25-29, 30+.<audio> Determine the speakers age based on this recording of human vocalizations. Respond with the age between 18 and 80, without any other explanation. Table 10: Some of tasks in our data have granularity. We use Age Classification (AC) task as an examples at three different granularity levels. Coarse-grained prompts elicit classification into decade-based age groups, mediumgrained prompts target 5-year age spans, and fine-grained prompts request exact age prediction within specified range."
        },
        {
            "title": "Prompt Style",
            "content": "Closed-ended Instruction Examples"
        },
        {
            "title": "Stylistic Variation",
            "content": "Fine-grained Task Analyze the provided audio and complete the task mentioned in it.<audio> Based on the instruction in the audio, provide your response.<audio> Listen to the audio and respond accordingly.<audio> Carefully listen to the audio clip and perform the requested action.<audio> Follow the instruction given in the audio and provide an accurate response.<audio> voice assistant is asking you to do something. Carefully listen and respond.<audio> For comprehension test, listen to the audio and answer the question presented in it.<audio> In this conversation, the speaker is giving you directive. Listen and respond appropriately.<audio> In this experiment, you need to complete the task given in the audio. Provide your response accordingly.<audio> This is an interactive task. Listen to the speaker and follow their instruction.<audio> Can you understand and complete the request made in this audio?<audio> If the audio contains question, answer it accurately. If it contains command, follow it.<audio> What action is required in the audio? Complete it and provide your response.<audio> Make sure to interpret the speakers request correctly and reply accordingly.<audio> The speaker in this audio needs response. Listen and provide relevant reply.<audio> After hearing the audio, provide your answer to the given task.<audio> Listen carefully and act according to the instruction in the recording.<audio> Pay attention to the details in the audio and respond exactly as instructed.<audio> Understand the content of the audio and give an appropriate response.<audio> Your task is to carefully analyze the instruction in the audio and execute it properly.<audio> Table 11: Examples of text prompts used in the pure-audio modality format, where both the instruction and content are embedded within single audio stream. The textual cues only instruct the model to listen and respond, without specifying task details. All four prompt styles are included as Table 9 - Standardized, Contextualized, Stylistic Variation, and Fine-grained Task."
        },
        {
            "title": "K Evalation Template",
            "content": "Instructions: You are evaluating the performance of an AI assistant in an audio question answering task. Given Reference Answer and Predicted Answer, assign score from 1 to 5 based on Relevance and Accuracy. Output Format (exactly, no other text): Score: <integer 15> Explanation: <concise justification focusing on both relevance and accuracy> Reference Answer: {reference} Predicted Answer: {predicted} Please produce the evaluation. Figure 17: Evaluation template used for GPT-4o-based scoring of LSLMs responses. The model assigns an integer score (15) according to relevance and accuracy, accompanied by concise explanation. All results were scored with OpenAI GPT-4o (gpt-4o-mini, Version 2024-07-18). 31 Details for LLaSO-Align and LLaSO-Instruct"
        },
        {
            "title": "Modality Formats",
            "content": "Sample Num."
        },
        {
            "title": "Hours",
            "content": "Instr. Settings"
        },
        {
            "title": "SGC",
            "content": "Speaker Gender Classification (Biologically) AC AR"
        },
        {
            "title": "EIE",
            "content": "ER"
        },
        {
            "title": "Accent Classification",
            "content": "Age Recognition (Three Granularities)"
        },
        {
            "title": "Synthetic Speech Detection\nSpeaker Verification\nPronunciation Scoring Word Level\nPronunciation Scoring Sentence Level",
            "content": "Phoneme Recognition Speech Command Recognition Intent Prediction Entity Extraction Vocal Sound Classification Instrument Classification Instrument Source Classification Pitch Prediction Velocity Classification -"
        },
        {
            "title": "GigaSpeech\nLibriSpeech\nLJ Speech\nVCTK\nMLS",
            "content": "< Textual Instruction, Audio Input > and < Audio Instruction, Audio Input >"
        },
        {
            "title": "Semantic Task Category",
            "content": "Open Orca 1M-GPT4 Open Orca 3.5M-GPT3.5 Stanford Alpaca Code Alpaca AlpacaDan Dolly OpenOrcaNo Tigerbot_Alpaca Tigerbot_Multichat Unnatural < Audio Instruction, Audio Input > < Textual Instruction, Audio Input > and < Audio Instruction, Textual Input > Paralinguistic Task Category Speaker-centric VoxCeleb1 VCTK VocalSound Common Voice VCTK AccentDB Common Voice VCTK VocalSound Common Voice MELD CREMA-D MELD CREMA-D FoR MELD speechocean < Audio Instruction, Audio Input > and < Textual Instruction, Audio Input > Content-centric"
        },
        {
            "title": "NSynth",
            "content": "- < Audio Instruction, Audio Input > and < Textual Instruction, Audio Input > - 12M (LLaSO-Align)&- 47K&- 1M&0.2M 4K&1K Open-ended 0.4M 0.8M 48K 9K 46K&46K <1K&3K 7K&10K 20K&20K 6K&33K 0.2M&0.2M 35K&35K 71K&71K 20K&20K 0.7M&0.7M 71K&71K 16K&16K 0.3M&0.3M 71K&71K 20K&20K 1.2M&1.2M 11K&11K 1K&1K 9K&9K 7K&7K 64K&64K 11K&11K 4K&4K 4K&4K 1M&1M 68K&68K 71K&71K 45K&45K 20K&20K 0.2M&0.2M 0.3M&0.3M 0.3M&0.3M 0.3M&0.3M 25.5M 1.8K 3.6K <1K <1K <1K&<1K <1K&<1K <1K&<1K <1K&<1K <1K&<1K <1K&<1K <1K&<1K <1K&<1K <1K&<1K 2.3K&1.2K <1K&<1K <1K&<1K 2.4K&<1K <1K&<1K <1K&<1K 5.4K&1.8K <1K&<1K <1K&<1K <1K&<1K <1K&<1K <1K&<1K <1K&<1K <1K&<1K <1K&<1K Open-ended Closed-ended Closed-ended and Open-ended Closed-ended Open-ended Open-ended 5K&4K <1K&<1K <1K&<1K <1K&<1K <1K&<1K 1.1K&<1K <1K&<1K <1K&<1K Open-ended 1.1K&<1K Closed-ended Closed-ended 89.5K - Table 12: Overview of task-level composition in LLaSO-Align and LLaSO-Instruct, spanning three core categories, linguistic, semantic, and paralinguistic, across 20 sub-tasks. Each entry summarizes representative data sources, supported input formats, and sample-level statistics. LLaSO-Eval is constructed as stratified evaluation set presented in Table 2."
        },
        {
            "title": "M Baseline Performance Details",
            "content": "Task ASR AQA SGC AC AR EIE ER SSD SV PR SCR IP EE PSWL PSSL VSC IC ISC PP VC Qwen2Audio TyphoonAudio Salmonn Glm-4Voice MiniOmni MiniOmni2 LlamaOmni AudioReasoner KimiAudio Qwen2.5Omni LLaSO-Base LLaSO-Base (Unfrozen) 0.22 0.12 2.41 2.42 2.73 2.78 0.11 0. 1.76 1.77 2.16 2.22 0.86 0.69 1.47 1.41 1.41 1.72 0.93 0.79 2.22 2.34 3.29 2.93 0.95 0. 1.42 1.47 1.75 1.45 0.95 0.80 1.57 1.53 2.05 1.51 0.88 0.73 1.97 2.02 2.99 2.48 0.28 0. 2.44 2.24 2.51 2.86 0.14 0.05 2.94 2.70 3.22 3.45 0.40 0.26 2.94 3.09 3.22 2.63 2.56 3.47 1.87 2.69 2.05 2.04 2.49 3.09 1.63 1.42 1.66 1.68 2.38 2.73 2.22 2.84 3.28 3.69 2.99 3.46 3.49 3.62 3.14 2.91 3.13 3.03 3.21 4.06 1.54 1.32 1.64 1.50 2.95 3.78 3.42 3.95 3.77 4.01 3.80 3.88 2.13 3.29 1.61 2.47 1.42 2.42 2.51 1.68 1.22 1.17 1.26 1.41 1.88 2.29 2.12 2.88 3.35 3.38 3.20 3.58 3.14 1.29 2.83 1.68 2.96 1.83 3.11 1.03 2.34 1.21 2.52 1.29 3.16 1.11 3.07 1.54 3.53 1.16 2.96 1.19 3.13 3.14 3.04 3.04 3.12 3.19 2.82 3.10 1.33 1.27 1.42 1.31 2.72 3.08 2.91 3.13 3.38 3.16 3.19 3.15 2.20 2.52 2.36 1.91 2.37 1.58 1.97 1.98 1.41 1.20 1.43 1.28 2.20 2.09 2.14 2.09 2.71 2.77 2.12 2. 0.08 0.03 2.06 1.80 2.39 1.46 2.57 2.72 2.48 2.62 1.71 2.28 2.74 2.23 3.05 3.74 2.90 2.60 0.14 0.07 2.27 2.27 2.23 1.98 2.54 3.22 2.42 2.84 1.96 2.47 2.80 2.43 3.09 3.68 2.87 3.26 1.00 0.95 0.67 0.99 0.16 0.12 0. 0.23 0.52 18.69 0.54 0.31 0.24 0.30 0.43 0.25 1. 2.60 2.40 3.04 2.52 2.73 1.86 0.17 1.95 0. 0.60 0.60 0.85 0.77 0.59 0.67 0.21 0.14 0.11 0.10 0.12 20.47 0.40 0. 0.28 0.12 0.46 0.20 3.08 0.98 0.85 3.13 1. 2.85 2.04 0.33 1.71 0.49 0.16 0. 0.59 0.44 0.13 0.18 0.22 0.32 0.10 0.26 0.06 11.24 0.31 0.24 0.30 0.21 0. 0.19 1.82 1.09 0.75 4.07 1.88 3.29 1. 0.13 1.38 0.61 0.16 0.16 0.11 0.12 0.04 0.07 0.07 0.09 0. 0.02 0.01 15.35 0.13 0.08 0.14 0.02 0.10 0.04 0. 1.00 0.98 1.85 1.78 2.34 1.62 0.24 1.84 0. 0.00 0.03 0.14 0.00 0.00 0.00 0.00 0.06 0.00 0.00 0.00 21.34 0.04 0. 0.04 0.07 0.11 0.03 0.92 1.00 0.98 1.39 1. 1.42 1.24 0.00 1.46 0.02 0.01 0. 0.11 0.00 0.02 0.00 0.00 0.03 0.00 0.00 0.00 18.46 0.03 0.06 0.00 0.01 0. 0.03 0.97 1.00 0.97 1.89 1.26 1.60 1. 0.00 1.54 0.03 0.01 0.03 0.36 0.26 0.03 0.26 0.07 0.14 0. 0.16 0.17 Reject 0.31 0.08 0.16 0.04 0.26 0.08 1. 20.19 21.11 1.58 1.80 2.34 1.28 0.13 1.38 0. 0.00 0.12 0.38 0.32 0.38 0.37 0.14 0.18 0.02 0.23 0.12 13.57 0.52 0. 0.32 0.28 0.35 0.16 1.08 2.40 1.84 4.10 2. 3.78 2.61 0.09 2.03 0.59 0.20 0. 0.98 0.97 0.66 0.81 0.38 0.31 0.20 0.17 0.12 12.07 0.65 0.34 0.52 0.32 0. 0.22 1.58 1.00 0.31 4.56 2.05 3.57 3. 0.20 2.76 0.84 0.26 0.38 0.53 0.41 0.40 0.35 0.06 0.19 0. 0.11 0.08 10.31 0.52 0.27 0.29 0.33 0.42 0.15 1. 3.53 3.52 3.91 2.00 2.78 1.25 0.27 2.12 0. 0.51 0.44 19.02 0.02 36.83 0. 41.92 0.22 40.20 0.08 61.49 0. 59.32 0.00 Reject 0.07 32.68 0. 31.64 0.19 18.37 0.12 0.96 0.99 0.76 0.91 0.52 0.83 0. 0.70 0.50 10.32 0.48 0.48 0.17 0.26 0.99 0.32 0. 0.04 0.02 4.86 3.93 3.57 2.90 0.39 2.80 0. 0.50 0.60 8.02 0.18 0.88 0.99 0.61 0.97 0.40 0.86 0. 0.68 0.38 8.78 0.45 0.37 0.16 0.30 0.99 0.16 0. 0.05 0.04 4.80 3.90 3.44 2.68 0.24 2.66 0. 0.55 0.77 10.55 0.20 Metrics WER CER GPT-4o ACC ACC ACC MAE ACC ACC ACC ACC PER WER CER GPT-4o GPT-4o GPT-4o GPT-4o ACC GPT-4o ACC ACC ACC MAE ACC Table 13: Comprehensive evaluation across multiple LSLMs in LLaSO-Eval. Blue highlights denote best performance per task. Reject indicates that, during manual inspection, for 95% or more of the responses in the corresponding open-ended setting/task, the model explicitly expresses inability to assist or process the task, states it is text-only model unable to recognize audio, or behaves as pure text model by asking the user to describe the audio, its content, or information therein. From SGC to VC we only tested the < Textual Instruction, Audio Input > format, because for these tasks we also used only the < Textual Instruction, Audio Input > format data of those tasks during training."
        }
    ],
    "affiliations": [
        "BUPT",
        "Logic Intelligence Technology",
        "Ningbo Key Laboratory of Spatial Intelligence and Digital Derivative, Institute of Digital Twin, EIT",
        "Xiamen University"
    ]
}