{
    "paper_title": "GEBench: Benchmarking Image Generation Models as GUI Environments",
    "authors": [
        "Haodong Li",
        "Jingwei Wu",
        "Quan Sun",
        "Guopeng Li",
        "Juanxi Tian",
        "Huanyu Zhang",
        "Yanlin Lai",
        "Ruichuan An",
        "Hongbo Peng",
        "Yuhong Dai",
        "Chenxi Li",
        "Chunmei Qing",
        "Jia Wang",
        "Ziyang Meng",
        "Zheng Ge",
        "Xiangyu Zhang",
        "Daxin Jiang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in image generation models have enabled the prediction of future Graphical User Interface (GUI) states based on user instructions. However, existing benchmarks primarily focus on general domain visual fidelity, leaving the evaluation of state transitions and temporal coherence in GUI-specific contexts underexplored. To address this gap, we introduce GEBench, a comprehensive benchmark for evaluating dynamic interaction and temporal coherence in GUI generation. GEBench comprises 700 carefully curated samples spanning five task categories, covering both single-step interactions and multi-step trajectories across real-world and fictional scenarios, as well as grounding point localization. To support systematic evaluation, we propose GE-Score, a novel five-dimensional metric that assesses Goal Achievement, Interaction Logic, Content Consistency, UI Plausibility, and Visual Quality. Extensive evaluations on current models indicate that while they perform well on single-step transitions, they struggle significantly with maintaining temporal coherence and spatial grounding over longer interaction sequences. Our findings identify icon interpretation, text rendering, and localization precision as critical bottlenecks. This work provides a foundation for systematic assessment and suggests promising directions for future research toward building high-fidelity generative GUI environments. The code is available at: https://github.com/stepfun-ai/GEBench."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 9 ] . [ 1 7 0 0 9 0 . 2 0 6 2 : r GEBench: Benchmarking Image Generation Models as GUI Environments Haodong Li1,2, Jingwei Wu1, Quan Sun1,, Guopeng Li1, Juanxi Tian7, Huanyu Zhang5, Yanlin Lai1,4, Ruichuan An1,3, Hongbo Peng1, Yuhong Dai1, Chenxi Li6, Chunmei Qing2,, Jia Wang1, Ziyang Meng1, Zheng Ge1,, Xiangyu Zhang1, Daxin Jiang1 2 South China University of Technology 3 Peking University 1 StepFun"
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in image generation models enable the prediction of future Graphical User Interface (GUI) states based on user instructions. However, existing benchmarks primarily focus on general domain visual fidelity, leaving evaluation of state transitions and temporal coherence in GUI-specific contexts underexplored. To address this gap, we introduce GEBench, comprehensive benchmark for evaluating dynamic interaction and temporal coherence in GUIs generation. GEBench comprises 700 carefully curated samples spanning five task categories, covering both single-step interactions and multi-step trajectories across real-world and fictional scenarios, as well as grounding point localization. To support systematic evaluation, we propose GE-Score, five-dimensional metric that assesses Goal Achievement, Interaction Logic, Content Consistency, UI Plausibility, and Visual Quality. Extensive evaluation indicates that current models perform well on single-step transitions but struggle with temporal coherence and spatial grounding over longer interaction sequences. Moreover, our findings identify icon interpretation, text rendering, and localization precision as key bottlenecks, and suggest promising directions for future research toward high-fidelity generative GUI environments. The code is available at: https://github.com/stepfun-ai/GEBench 1. Introduction Recent advancements in image generation models Comanici et al. (2025); Google (2025b); Hurst et al. (2024); Labs (2025); Seedream et al. (2025); Wan et al. (2025) enable the prediction of future Graphical User Interface (GUI) states based on specific user instructions and current visual contexts. This capability positions image generation models as powerful GUI Environments Garg et al. (2025); Luo et al. (2025); Wei et al. (2023); Xie et al. (2025b); Yan et al. (2025); Zhang et al. (2025a), capable of simulating dynamic interaction sequences to facilitate the scalable training of autonomous agents. Distinguished from conventional simulators Bonatti et al. (2024); Cobbe et al. (2020); Xie et al. (2024) tethered to physical hardware or fixed software stacks Zhang et al. (2025a), these generative models offer flexible, low-cost alternative for creating diverse interaction trajectories across countless applications Liu et al. (2025b); Zhao et al. (2021). However, the potential of image generation models as reliable GUI environments remains largely unverified, as traditional visual benchmarks Ghosh et al. (2023); Hu et al. (2024); Huang et al. (2023, 2024); Niu et al. (2025); Sun et al. (2025); Zhao et al. (2025); Zhuang et al. (2025) prioritize general-domain visual fidelity (for images) and continuous state transitions (for videos), leaving Figure 1 Comparison of evaluation paradigms across different benchmark types. Existing image generation benchmarks prioritize general-domain visual fidelity and video generation benchmarks evaluate continuous state transitions. GEBench uniquely evaluates discrete state transitions induced by user actions, capturing the essence of GUI interactions. critical gap in evaluating the functional logic and state-transition consistency inherent to GUI interactions Xie et al. (2025a); Yan et al. (2025). As shown in Figure 1, when acting as GUI environments Zhang et al. (2025a), generation models must seamlessly navigate discrete, actiontriggered interface jumps. Such transitions necessitate precise coordinate grounding Cheng et al. (2024); Zhao et al. (2021), icon recognition Liu et al. (2025b); Xie et al. (2025a), and high-fidelity text rendering Chen et al. (2024), compelling the models to maintain logical continuity even when visual elements do not persist Li et al. (2025a). Such demands strain existing architectures and call for new evaluation approach to verify if generated GUIs respond felicitously to user instructions. To bridge this gap, we present GEBench (Benchmarking image generation models as GUI Environments), benchmark designed to evaluate how effectively image generation models can serve as GUI environments. GEBench comprises 700 high-quality samples, where each entry aligns sequence of GUI images with corresponding user instructions. These samples span five distinct tasks, allowing for multifaceted assessment of the models ability. To provide concrete measure of generative quality, we propose GE-Score, multi-dimensional metric derived from Vision Language Model (VLM)-guided Bai et al. (2025); Google (2025a); Hurst et al. (2024) evaluations across five specialized rubrics. GE-Score systematically validates intent fulfillment and interaction logic while verifying UI content consistency and structural integrity. By ensuring high visual fidelity and logical coherence, GE-Score confirms the practical utility of these synthetic environments. Our systematic evaluation of state-of-the-art image generation models Deng et al. (2025); 2 Google (2025b); Labs (2025); Li et al. (2025b); OpenAI (2025); Seed (2025); Seedream et al. (2025); Team et al. (2025a); Wan et al. (2025); Wu et al. (2025) identifies promising pathways for their evolution into reliable GUI environments. While current architectures demonstrate robust proficiency in executing localized, single-step state transitions, they offer significant opportunities for advancement in long-term interaction consistency and precise spatial grounding. In particular, deficiencies in icon interpretation, Chinese text rendering, and grounding point localization lead to layout drift and logical inconsistencies. These observations delineate critical bottlenecks and outline clear directions for future research toward high-fidelity, temporally coherent generative GUI systems. Our primary contributions are as follows: 1. We introduce GEBench, systematic benchmark with 700 samples across five task categories to evaluate image generation models as dynamic GUI environments. 2. We propose GE-Score, five-dimensional metric that emphasizes the quality assessment of image sequences by accounting for the unique visual properties of GUIs. 3. Our evaluation reveals critical deficiencies in current image generation models, underscoring significant room for improvement in high-fidelity GUI generation. 2. Related Work 2.1. Automated GUIs Generation The evolution of GUIs generation reflects significant paradigm shift from heuristic-based structural mapping to data-driven synthesis powered by Multimodal Large Language Models (MLLMs) Chen et al. (2018); Kolthoff et al. (2024, 2025); Li et al. (2020); Mozaffari et al. (2022); Sandhaus et al. (2011); Sobolevsky et al. (2023); Yang et al. (2016); Zhang et al. (2025b); Zhao et al. (2021). Early methodologies relied on traditional rule-based algorithms to perform layout reconstruction Huang et al. (2016); Sandhaus et al. (2011), yet these approaches frequently failed to capture the semantic depth of complex hierarchies. Subsequent frameworks simplified this process using model-based approaches to translate visual features directly into code sequences Chen et al. (2018). Contemporary research leverages Transformer-based architectures to bridge the gap between visual design abstractions and executable source code Kolthoff et al. (2025); Sobolevsky et al. (2023). Furthermore, the rapid advancement of generative AI suggests that direct utilization of image generation models for GUI synthesis is becoming increasingly viable Li et al. (2020); Mozaffari et al. (2022); Zhang et al. (2025c); Zhao et al. (2021). These models offer the potential to produce high-fidelity GUIs directly from user instructions. 2.2. Advanced Image Generation Models Recent progress in image generation exhibits rapid evolution from text-to-image synthesis to sophisticated reference-based frameworks Comanici et al. (2025); Deng et al. (2025); Google (2025b); Hurst et al. (2024); Li et al. (2025b); Liu et al. (2025a); Seedream et al. (2025); Team et al. (2025a,b); Wan et al. (2025). Ongoing advancements in text-to-image synthesis have empowered models to produce aesthetically superior visuals with precise semantic alignment to the provided instructions Chen et al. (2020); Fan et al. (2024); Han et al. (2025); Ho et al. (2020); Labs (2025); Lin et al. (2025); Ramesh et al. (2022). Building on these foundations, reference-based techniques integrate visual priors with textual prompts to enhance generative control An et al. (2025); Google (2025b); Seedream et al. (2025); Team et al. (2025a); Wan et al. (2025). These methods incorporate style or structural references to ensure spatial precision 3 and identity consistency Deng et al. (2025); Liu et al. (2025a). These advances enable image generation models to function as interactive GUI environments. 2.3. Sequential Generation Benchmarks Standard image generation benchmarks focus on visual fidelity and text-alignment for single image, using metrics like FID and CLIP score to measure visual quality Ghosh et al. (2023); Heusel et al. (2017); Huang et al. (2023); Radford et al. (2021). While these metrics provide robust baseline for aesthetic realism Heusel et al. (2017), they set the stage for incorporating logical coherence to achieve more holistic evaluation. Recent efforts in benchmarking sequential image generation explore temporal consistency and reasoning Guo et al. (2025); Hu et al. (2024); Huang et al. (2023); Niu et al. (2025); Zhang et al. (2026); Zhao et al. (2025); Zhuang et al. (2025), yet these frameworks typically target natural scenes with continuous movement, simple spatial relationships or characters identity Liu et al. (2025a). GUI environments differ fundamentally because they involve discrete state jumps where single action replaces the entire visual layout Yan et al. (2025); Zhang et al. (2025a). Furthermore, the stringent text-rendering requirements of GUIs push current generative architectures to their limits Chen et al. (2024). This necessitates new evaluation approach to bridge the significant gap in assessing whether image generation models can maintain the strict semantic and structural integrity required for multi-step GUI trajectories generation. 3. GEBench GUI environment functions as an interactive medium that translates user instructions or agent actions into corresponding visual feedback through software logic Zhang et al. (2025a). This mechanism allows system to simulate the evolution of digital interface in response to user interventions. The highly structured and rule-based composition of GUIs, governed by precise functional logic, distinguishes these environments from the patterns of natural scenes. GEBench establishes an evaluation framework that treats image generation models as interactive GUI environments and benchmarking their performances. Under this paradigm, the model receives visual observations of the current GUI state along with specific user instructions to synthesize the subsequent state. In the following sections, we use terms GUI state\" and GUIs\" to refer to image-based inputs and outputs of image generation models. 3.1. Benchmark Design and Task Suites GEBench comprises 700 high-quality interaction sequences curated under strict consistency and fidelity constraints. As shown in Figure 2, by organizing samples into five task categories, GEBench enables fine-grained evaluation of model capabilities across multiple dimensions of GUIs generation: 1. Single-step Visual Transition (single-step) The model receives an initial GUI state as reference image and detailed action specification to generate the subsequent GUI state. This task evaluates fine-grained instruction following of the models. 2. Multi-step Planning (multi-step) Starting from an initial GUI state as reference and high-level user objective such as Order coffee\", the model must generate five-step GUIs trajectory. This task assesses long-horizon planning, temporal coherence, and the consistency of UI structure across multiple steps. 4 Figure 2 Examples of the five task types in GEBench, which are designed to comprehensively evaluate the capabilities of image generation models as GUI environments. GEBench provides image generation models with user instructions and reference GUI state (no reference provided for the Fiction App task) and evaluates the generated GUIs. 3. Zero-shot Virtual GUI (fiction-app) This task evaluates out-of-distribution generalization of models, testing whether they can generate unseen layouts by relying on detailed instructions without external reference. 4. Rare Trajectory Synthesis (real-app) This task evaluates the models ability to generate long-tail interaction trajectories by prioritizing logical reasoning over pattern imitation, particularly in data-scarce scenarios. 5. Grounding-based Generation (grounding) The model generates the next GUI state based on normalized relative coordinates within the range of [0, 1000]. This task assesses spatial awareness and the ability to render changes at precise pixel locations. Unless otherwise indicated, these five task types are subsequently referred to by their respective parenthetical abbreviations for brevity. 3.2. Evaluation Dimension and Scoring Rubric GEBench adopts multi-dimensional scoring rubric designed to assess model abilities in GUIs generation setting. Rather than relying on single correctness signal, we decompose model performance into five complementary dimensions that jointly capture functional accuracy, interaction realism, and visual fidelity. This design enables fine-grained and interpretable 5 Figure 3 GEBench data construction pipeline. The process involves raw data capture through recording user interactions, task annotation of actions, quality control via preprocessing and verification, and data construction across five task categories: Single-Step, Multi-Step, Grounding, Real App, and Fictional App, totaling 700 samples. comparisons across all 5 task types. Specifically, model generated GUI states are evaluated along the following five dimensions: Goal Achievement (GOAL) assesses whether the generated GUI state satisfies the specified action or global objective, focusing on the correctness and completeness of the intended outcome. Interaction Logic (LOGIC) evaluates the plausibility and coherence of state transitions with respect to realistic GUI interaction patterns, ensuring that visual changes can be explained by valid user actions. Consistency (CONS) measures the preservation of unaffected regions within single image or the stability of UI elements across multiple GUIs, reflecting resistance to unintended visual drift. UI Plausibility (UI) examines whether generated GUI state components are structurally coherent, native-looking, and free from hallucinated or physically impossible elements. Visual Quality (QUAL) evaluates the perceptual fidelity of the generated GUIs, including text readability, icon clarity, and the absence of rendering artifacts. All dimensions are scored on discrete ordinal scale from 0 to 5, where higher scores indicate stronger alignment with expected GUIs behavior and visual realism. Detailed scoring criteria for each dimension are provided in the appendix. We synthesize these multi-dimensional assessments into GE-Score, holistic metric reflecting the aggregate performance. For each i-th sample evaluated on the d-th dimension, let ùëüùëñ,ùëë {0, . . . , 5} represent the discrete fidelity score. The GE-Score is formally defined as ùê∫ùê∏ ùë†ùëêùëúùëüùëí = 1 ùëÅ ùëÅ ùëë ùëñ=1 (cid:0)F (ùëüùëñ,ùëë)(cid:1) = 4 ùëÅ 5 ùëÅ ùëë=1 ùëñ=1 ùëüùëñ,ùëë (1) 6 Table 1 Main evaluation results on GEBench across Chinese and English Subsets. The table presents performance comparison across five core dimensions involving 8 commercial models and 4 open-source models. Orange and Champagne cells indicate the Top 1 and Top 2 performers respectively."
        },
        {
            "title": "Model",
            "content": "single multi fiction real ground GE single multi fiction real ground GE -step -step -app -app Score -step -step -app -app -ing -ing"
        },
        {
            "title": "Score",
            "content": "84.50 68.65 65.75 64.35 Nano Banana pro Google (2025b) 64.36 34.16 64.82 65.89 Nano Banana Google (2025b) 83.79 56.97 60.11 55.65 GPT-image-1.5 OpenAI (2025) 64.72 49.20 57.31 59.04 GPT-image-1.0 OpenAI (2025) Seedream 4.5 Seed (2025) 63.64 53.11 56.48 53.44 Seedream 4.0 Seedream et al. (2025) 62.04 48.64 49.28 50.93 64.20 50.11 52.72 50.40 Wan 2.6 Wan et al. (2025) 68.83 55.07 58.13 55.41 Flux-2-pro Labs (2025) Bagel Deng et al. (2025) UniWorld-V2 Li et al. (2025b) Qwen-Image-Edit Wu et al. (2025) Longcat-Image Team et al. (2025a) 34.84 13.45 27.36 33.52 55.33 24.95 32.03 21.39 41.12 26.79 23.78 26.10 48.76 12.75 30.03 30.00 64.83 54.48 53.33 31.68 52.90 53.53 59.58 50.24 35.10 49.60 50.80 51.02 69.62 84.32 69.51 46.33 47.20 56.74 64.80 50.75 48.88 47.12 63.22 80.80 58.87 63.68 58.93 52.39 60.92 64.33 58.94 56.16 55.91 49.49 45.30 53.81 51.80 52.88 53.28 37.57 47.92 49.36 55.40 60.17 44.36 49.55 44.80 57.54 61.00 52.17 49.92 47. 28.85 32.91 26.08 35.12 8.61 36.66 42.68 14.14 30.08 26.83 33.72 40.12 18.61 25.80 25.95 37.30 36.83 8.44 34.51 36.69 58.64 49.04 49.23 37.84 49.63 44.17 53.36 45.67 37.30 47.04 54.55 47.12 61.20 52.12 63.16 55.64 50.01 46.46 50.45 51.18 28.00 32.15 33.01 33.28 where (ùëü) = 20 ùëü represents the linear normalization transform into percentage domain [0, 100]. This formulation effectively captures the mean semantic-structural alignment of generation models across the entire benchmark distribution. 3.3. Data Construction Pipeline The construction of GEBench data follows structured pipeline designed to transform raw screen recordings into high-quality trajectories for GUI-based interaction. As Figure 3 illustrates, the process begins with the collection of raw interaction data through screen recordings on both mobile and desktop platforms. During the task annotation phase, annotators define specific actions, such as clicking icons or scrolling through interfaces, and convert these sequences into structured JSON metadata. To further improve data quality, we incorporate three-stage quality control mechanism. First, rule-based preprocessing step automatically filters out inconsistent or noisy samples. Second, human experts manually verify the remaining sequences to ensure that the annotated actions accurately match the visual state transitions. Finally, statistical calibration process adjusts the data distribution to mitigate potential biases, resulting in final collection of 700 refined samples, which are categorized into five types of tasks, as mentioned in Section 3.1. 4. Evaluation 4.1. Evaluation Setup Evaluated Models. Our experimental evaluation encompasses 12 image generation models, grouped into two categories based on accessibility: 8 commercial models and 4 open-source models. The commercial model group includes Nano Banana pro Google (2025b), Nano Banana Google (2025b), GPT-image-1.5 OpenAI (2025), GPT-image-1 OpenAI (2025), Seedream 4.5 Seed (2025), Seedream 4.0 Seedream et al. (2025), Wan2.6 Wan et al. (2025), Flux-2-pro Labs (2025). 7 Figure 4 Performance of models across GEBench task suites. The radar chart illustrates the performance of 12 prominent image generation models, including commercial models (solid line) and open-sourced models (dashed line). The reported results represent the average scores on Chinese and English subsets. The open-source model group includes Bagel Deng et al. (2025), UniWorld-V2 Li et al. (2025b), Qwen-Image-Edit Wu et al. (2025), LongCat-Image Team et al. (2025a). VLM-based Judges. To ensure the objectivity and robustness of GEBench, we deploy 3 state-ofthe-art VLMs as independent cross-evaluators: 2 commercial models Gemini-3-Flash-Native Google (2025a), GPT-4o Hurst et al. (2024) and 1 open-source model Qwen3-vl-235b-a22b-thinking Bai et al. (2025). By utilizing these evaluators, we mitigate potential bias inherent in single judge model. To ensure fair and reproducible comparisons, we use official default configurations for evaluated models and perform evaluation three times for each generated GUIs trajectory. 4.2. Evaluation Results Overall Performance and Model Comparison. Experimental results in Table 1 show that Nano Banana Pro Google (2025b) delivers the most robust performance, particularly on Chinese subset with top-ranking GE-Score of 69.62. GPT-image-1.5 OpenAI (2025) follows closely, excelling on English subset and securing the first position with score of 63.16. The radar chart, as shown in Figure 4, further illustrates that commercial models, led by Nano Banana Pro Google (2025b), exhibit balanced and full\" pentagonal profile. In contrast, open-source 8 Figure 5 Comparison of GOAL score on grounding task. The universally low scores across all models highlight critical deficiency in current generative models ability to perceive and align with precise spatial grounding points. models (e.g., UniWorld-V2 Li et al. (2025b), Bagel Deng et al. (2025)) show performance curves that significantly shrink inward, revealing substantial gap in handling complex tasks. The Performance Gap in Multi-step Planning. The evaluation results in Table 1 highlights critical bottleneck: while most models excel in Single-step transitions, with both Nano Banana Pro Google (2025b) and GPT-image-1.5 OpenAI (2025) exceeding 80 points, their scores plummet in Multi-step Planning scenarios, generally dropping below 60 or even 10 points. The radar chart, as shown in Figure 4, clearly identifies the Multi-step axis as general weakness across nearly all models. This phenomenon underscores current limitations in long-horizon logical planning: Imbalance between Perception and Planning: Models possess strong instruction-following capabilities for single action mapping but struggle to maintain logical consistency across long sequences. Error Accumulation: During multi-step transitions, minor visual deviations in intermediate steps snowball over time. This accumulated error eventually causes the generated trajectory to diverge entirely from the intended goal. Deficiency in Visual-level Reasoning: Despite their ability to reason within complex textual spaces, models fail to logically grasp inter-step dependencies in interconnected visual tasks, making it difficult to predict the impact of current actions on subsequent visual states. Challenges in Spatial Grounding. Cross-metric analysis reveals general deficit in Groundingbased Generation tasks. As shown in Figure 5, the GOAL (Goal Achievement) score is remarkably low: even top-performing Nano Banana Pro Google (2025b) achieves only 23.9%, while most other models (e.g., Qwen-Image-Edit Wu et al. (2025)) fall below 10%. The pronounced dent\" on the Grounding axis of the radar chart, as in Figure 4, further validates this bottleneck: The dismal GOAL scores indicate logical disconnect; models identify what to generate but cannot translate this into where to place it on precise [0, 1000] coordinate grid. This suggests models lack fundamental understanding of the mapping between abstract coordinates and image pixels. Figure 6 Pearson correlation analysis between human expert scores and VLM-based evaluation. Results for Nano Banana Pro Google (2025b) and GPT-Image-1 OpenAI (2025) demonstrate strong alignment between the VLM-as-a-Judge framework and human judgment across different models. 4.3. Validity of VLM-as-a-Judge To validate the reliability of using VLM as evaluators, we analyze the correlation between VLM-based assessments and human expert judgments. Specifically, we randomly sample results from two representative models, Nano Banana Pro Google (2025b) and GPT-Image-1 OpenAI (2025). For each model, we select 10 edited samples from each of the 10 tasks, resulting in 100 evaluated samples per model. Four human experts independently assess all selected samples using the same evaluation criteria and metrics as the VLM-based judges. Human scores are obtained by averaging all scores across four experts. We then compute the Pearson Correlation Coefficient between the human-annotated scores and the scores produced by the VLM-based judges. As shown in Figure 6, our VLM-based evaluations exhibit strong correlation with human judgments. The overall Pearson correlation coefficient across all samples reaches = 0.9892. When analyzed by model, the correlation remains consistently high, with = 0.9926 for Nano Banana Pro Google (2025b) and = 0.9833 for GPT-Image-1 OpenAI (2025). These results indicate high level of agreement between the VLM evaluator and human experts across different tasks and models. Together, this analysis demonstrates that the proposed VLM-as-a-Judge framework provides reliable and human-aligned evaluations across diverse tasks and models. 10 Figure 7 Qualitative results of the three primary weaknesses identified in image generation models acting as GUI environments. The comparison highlights significant deficiencies in text rendering accuracy, icon interpretation for state transitions, and localization precision regarding coordinate-based grounding. 5. Discussion and Analysis 5.1. Hierarchy of Task Difficulty: From Local Mimicry to Global Reasoning Failure The experimental results, as shown in Table 1 and Figure 4, reveal pronounced inverse correlation between task complexity and model performance, highlighting fundamental deficiency in current models deep understanding of GUI mechanics. In single-step transition scenarios, leading models Google (2025b); OpenAI (2025) achieve robust scores exceeding 80% through powerful visual synthesis. However, this illusory prosperity\" is largely driven by shortcut mapping from instructions to visual patterns; rather than mastering the underlying interactive logic, models primarily leverage statistical distribution fitting to match expected GUI states. This structural weakness becomes evident as tasks extend to multi-step trajectories. The significant inward shrinkage\" of the radar chart demonstrates an inability to maintain temporal coherence, where the lack of explicit state-space logic leads to severe logical disconnect when processing high-complexity interaction flows. 5.2. In-depth Bottleneck Analysis: Qualitative Insights from Failure Cases By synthesizing qualitative evidence, as shown in Figure 7, we identify three primary technical bottlenecks that hinder reliable GUIs generation. The first is the failure of text rendering accuracy. Qualitative cases reveal that while models like Nano Banana Pro Google (2025b) encounter deformations in complex layouts, open-source models frequently exhibit severe character overlapping and semantic corruption. This suggests that models treat text as local texture rather than symbolic unit with rigid structural information, leading to unreadable characters in layout-dense environments without hard topological constraints. Secondly, icon interpretation and consistency remain major barriers. Models exhibit significant difficulty in the semanticization of visual symbols, often failing to recognize correct interactive boundaries even when instructions target specific icons. This instability results in functional distortion\" during state transitions, where specific trigger may degenerate into meaningless geometric shape. Such severance of interaction intent and affordance renders interaction entry points unrecognizable for downstream interactions, causing an irreversible break in the task chain. Finally, lack of localization precision leads to critical logical disconnect. Even with explicit coordinate points, generated response elements like pop-up menus exhibit significant spatial jitter, often offsetting by dozens of pixels from their intended locations. This confirms decoupling of perception and execution, as evidenced by GOAL scores generally falling below 20%. This blindness\" to abstract spatial instructions remains the most formidable obstacle to achieving functionally valid generative GUI environments. 5.3. The Paradox of Visual Fidelity vs. Functional Plausibility Multi-dimensional analysis through GE-Score reveals critical paradox: visual fidelity does not equate to functional viability. Models such as GPT-image-1.5 OpenAI (2025) generate GUIs with exceptional composition and clarity, earning high QUAL scores. However, granular functional inspection reveals that this visual over-optimism\" is often deceptive, as these aesthetically pleasing images frequently contain hallucinated widgets or illogical layouts. This reinforces that benchmarking image generation models as GUI environments must be predicated upon assessment of temporal coherence and interactive logic, which take precedence over generaldomain visual fidelity. 6. Conclusion We introduces GEBench, the first systematic benchmark designed to evaluate image generation models as GUI environments. By shifting the focus from general-domain visual fidelity to GUI interaction logic, we provide comprehensive testbed for assessing the potential of generative models as GUI simulators. Existing image generation models often struggle with the precise structural requirements of interactive GUI generation, gap that GEBench is uniquely positioned to measure. Through the proposed GE-Score and VLM-based evaluation pipeline, we identify critical barriers to high-fidelity GUI simulation. Our analysis highlights that while image generation models show promise in predicting basic state transitions, they suffer from icon hallucinations, coordinate drift, and text rendering limitations that hinder their application as robust GUI generators. These findings underscore the need for future research to prioritize finegrained structural control and semantic persistence over simple visual realism. We believe that GEBench establishes necessary foundation for developing the next generation of generative GUI simulators capable of supporting the large-scale training of autonomous GUI agents."
        },
        {
            "title": "References",
            "content": "R. An, S. Yang, R. Zhang, Z. Shen, M. Lu, G. Dai, H. Liang, Z. Guo, S. Yan, Y. Luo, et al. Unictokens: Boosting personalized understanding and generation via unified concept tokens. arXiv preprint arXiv:2505.14671, 2025. S. Bai, Y. Cai, R. Chen, K. Chen, X. Chen, Z. Cheng, L. Deng, W. Ding, C. Gao, C. Ge, W. Ge, Z. Guo, Q. Huang, J. Huang, F. Huang, B. Hui, S. Jiang, Z. Li, M. Li, M. Li, K. Li, Z. Lin, J. Lin, X. Liu, J. Liu, C. Liu, Y. Liu, D. Liu, S. Liu, D. Lu, R. Luo, C. Lv, R. Men, L. Meng, X. Ren, X. Ren, S. Song, Y. Sun, J. Tang, J. Tu, J. Wan, P. Wang, P. Wang, Q. Wang, Y. Wang, T. Xie, Y. Xu, H. Xu, J. Xu, Z. Yang, M. Yang, J. Yang, A. Yang, B. Yu, F. Zhang, H. Zhang, X. Zhang, B. Zheng, H. Zhong, J. Zhou, F. Zhou, J. Zhou, Y. Zhu, and K. Zhu. Qwen3-vl technical report. arXiv preprint arXiv:2511.21631, 2025. R. Bonatti, D. Zhao, F. Bonacci, D. Dupont, S. Abdali, Y. Li, Y. Lu, J. Wagle, K. Koishida, A. Bucker, et al. Windows agent arena: Evaluating multi-modal os agents at scale. arXiv preprint arXiv:2409.08264, 2024. C. Chen, T. Su, G. Meng, Z. Xing, and Y. Liu. From ui design image to gui skeleton: neural machine translator to bootstrap mobile gui implementation. In Proceedings of the 40th International Conference on Software Engineering, pages 665676, 2018. J. Chen, Y. Huang, T. Lv, L. Cui, Q. Chen, and F. Wei. Textdiffuser-2: Unleashing the power of language models for text rendering. In European Conference on Computer Vision, pages 386402. Springer, 2024. M. Chen, A. Radford, R. Child, J. Wu, H. Jun, D. Luan, and I. Sutskever. Generative pretraining from pixels. In International conference on machine learning, pages 16911703. PMLR, 2020. K. Cheng, Q. Sun, Y. Chu, F. Xu, L. YanTao, J. Zhang, and Z. Wu. Seeclick: Harnessing gui In ACL 2024 (Volume 1: Long Papers), pages grounding for advanced visual gui agents. 93139332, 2024. K. Cobbe, C. Hesse, J. Hilton, and J. Schulman. Leveraging procedural generation to benchmark reinforcement learning. In International conference on machine learning, pages 20482056. PMLR, 2020. G. Comanici, E. Bieber, M. Schaekermann, I. Pasupat, N. Sachdeva, I. Dhillon, M. Blistein, O. Ram, D. Zhang, E. Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. C. Deng, D. Zhu, K. Li, C. Gou, F. Li, Z. Wang, S. Zhong, W. Yu, X. Nie, Z. Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. L. Fan, T. Li, S. Qin, Y. Li, C. Sun, M. Rubinstein, D. Sun, K. He, and Y. Tian. Fluid: Scaling autoregressive text-to-image generative models with continuous tokens. arXiv preprint arXiv:2410.13863, 2024. A. Garg, Y. Jiang, and A. Oulasvirta. Controllable gui exploration. arXiv preprint arXiv:2502.03330, 2025. D. Ghosh, H. Hajishirzi, and L. Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36:5213252152, 2023. 13 Google. new era of intelligence with Gemini 3, 2025a. URL https://blog.google/prod ucts/gemini/gemini-3. Google. Introducing nano banana pro, 2025b. URL https://blog.google/innovation-a nd-ai/products/nano-banana-pro/. Z. Guo, X. Chen, R. Zhang, R. An, Y. Qi, D. Jiang, X. Li, M. Zhang, H. Li, and P.-A. Heng. Are video models ready as zero-shot reasoners? an empirical study with the mme-cof benchmark. arXiv preprint arXiv:2510.26802, 2025. J. Han, J. Liu, Y. Jiang, B. Yan, Y. Zhang, Z. Yuan, B. Peng, and X. Liu. Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1573315744, 2025. M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. X. Hu, R. Wang, Y. Fang, B. Fu, P. Cheng, and G. Yu. Ella: Equip diffusion models with llm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135, 2024. K. Huang, K. Sun, E. Xie, Z. Li, and X. Liu. T2i-compbench: comprehensive benchmark for open-world compositional text-to-image generation. Advances in Neural Information Processing Systems, 36:7872378747, 2023. R. Huang, Y. Long, and X. Chen. Automaticly generating web page from mockup. In SEKE, pages 589594, 2016. Z. Huang, Y. He, J. Yu, F. Zhang, C. Si, Y. Jiang, Y. Zhang, T. Wu, Q. Jin, N. Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. A. Hurst, A. Lerer, A. P. Goucher, A. Perelman, A. Ramesh, A. Clark, A. Ostrow, A. Welihinda, A. Hayes, A. Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. K. Kolthoff, F. Kretzer, L. Fiebig, C. Bartelt, A. Maedche, and S. P. Ponzetto. Zero-shot prompting approaches for llm-based graphical user interface generation. arXiv preprint arXiv:2412.11328, 2024. K. Kolthoff, F. Kretzer, C. Bartelt, A. Maedche, and S. P. Ponzetto. Guide: Llm-driven gui generation decomposition for automated prototyping. In 2025 IEEE/ACM 47th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion), pages 14. IEEE, 2025. B. F. Labs. FLUX.2: Frontier Visual Intelligence. https://bfl.ai/blog/flux-2, 2025. J. Li, J. Yang, A. Hertzmann, J. Zhang, and T. Xu. Layoutgan: Synthesizing graphic layouts with vector-wireframe adversarial networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(7):23882399, 2020. S. Li, K. Kallidromitis, A. Gokul, Y. Kato, K. Kozuka, and A. Grover. Mobileworldbench: Towards semantic world modeling for mobile agents. arXiv preprint arXiv:2512.14014, 2025a. 14 Z. Li, Z. Liu, Q. Zhang, B. Lin, F. Wu, S. Yuan, Z. Yan, Y. Ye, W. Yu, Y. Niu, et al. Uniworld-v2: Reinforce image editing with diffusion negative-aware finetuning and mllm implicit feedback. arXiv preprint arXiv:2510.16888, 2025b. W. Lin, X. Wei, R. An, T. Ren, T. Chen, R. Zhang, Z. Guo, W. Zhang, L. Zhang, and H. Li. Perceive anything: Recognize, explain, caption, and segment anything in images and videos, 2025. URL https://arxiv.org/abs/2506.05302. S. Liu, Y. Han, P. Xing, F. Yin, R. Wang, W. Cheng, J. Liao, Y. Wang, H. Fu, C. Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025a. X. Liu, X. Zhang, Z. Zhang, and Y. Lu. Ui-e2i-synth: Advancing gui grounding with large-scale instruction synthesis. arXiv preprint arXiv:2504.11257, 2025b. D. Luo, B. Tang, K. Li, G. Papoudakis, J. Song, S. Gong, J. Hao, J. Wang, and K. Shao. Vimo: generative visual gui world model for app agents. arXiv preprint arXiv:2504.13936, 2025. M. A. Mozaffari, X. Zhang, J. Cheng, and J. L. Guo. Ganspiration: balancing targeted and serendipitous inspiration in user interface design with style-based generative adversarial network. In Proceedings of the 2022 CHI conference on human factors in computing systems, pages 115, 2022. Y. Niu, M. Ning, M. Zheng, W. Jin, B. Lin, P. Jin, J. Liao, C. Feng, K. Ning, B. Zhu, et al. Wise: world knowledge-informed semantic evaluation for text-to-image generation. arXiv preprint arXiv:2503.07265, 2025. OpenAI. The new chatgpt images is here, 2025. URL https://openai.com/index/new-cha tgpt-images-is-here/. A. Radford et al. Learning transferable visual models from natural language supervision. ICML, 2021. A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. P. Sandhaus, M. Rabbath, and S. Boll. Employing aesthetic principles for automatic photo book layout. In International Conference on Multimedia Modeling, pages 8495. Springer, 2011. B. Seed. Seedream 4.5, 2025. URL https://seed.bytedance.com/en/seedream4_5. T. Seedream, Y. Chen, Y. Gao, L. Gong, M. Guo, Q. Guo, Z. Guo, X. Hou, W. Huang, Y. Huang, et al. Seedream 4.0: Toward next-generation multimodal image generation. arXiv preprint arXiv:2509.20427, 2025. A. Sobolevsky, G.-A. Bilodeau, J. Cheng, and J. L. Guo. Guilget: Gui layout generation with transformer. arXiv preprint arXiv:2304.09012, 2023. K. Sun, K. Huang, X. Liu, Y. Wu, Z. Xu, Z. Li, and X. Liu. T2v-compbench: comprehensive benchmark for compositional text-to-video generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 84068416, 2025. M. L. Team, H. Ma, H. Tan, J. Huang, J. Wu, J.-Y. He, L. Gao, S. Xiao, X. Wei, X. Ma, et al. Longcat-image technical report. arXiv preprint arXiv:2512.07584, 2025a. 15 N. Team, C. Han, G. Li, J. Wu, Q. Sun, Y. Cai, Y. Peng, Z. Ge, D. Zhou, H. Tang, et al. Nextstep-1: Toward autoregressive image generation with continuous tokens at scale. arXiv preprint arXiv:2508.10711, 2025b. T. Wan, A. Wang, B. Ai, B. Wen, C. Mao, C.-W. Xie, D. Chen, F. Yu, H. Zhao, J. Yang, J. Zeng, J. Wang, J. Zhang, J. Zhou, J. Wang, J. Chen, K. Zhu, K. Zhao, K. Yan, L. Huang, M. Feng, N. Zhang, P. Li, P. Wu, R. Chu, R. Feng, S. Zhang, S. Sun, T. Fang, T. Wang, T. Gui, T. Weng, T. Shen, W. Lin, W. Wang, W. Wang, W. Zhou, W. Wang, W. Shen, W. Yu, X. Shi, X. Huang, X. Xu, Y. Kou, Y. Lv, Y. Li, Y. Liu, Y. Wang, Y. Zhang, Y. Huang, Y. Li, Y. Wu, Y. Liu, Y. Pan, Y. Zheng, Y. Hong, Y. Shi, Y. Feng, Z. Jiang, Z. Han, Z.-F. Wu, and Z. Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. J. Wei, A.-L. Courbis, T. Lambolais, B. Xu, P. L. Bernard, and G. Dray. Boosting gui prototyping with diffusion models. In 2023 IEEE 31st International Requirements Engineering Conference (RE), pages 275280. IEEE, 2023. C. Wu, J. Li, J. Zhou, J. Lin, K. Gao, K. Yan, S.-m. Yin, S. Bai, X. Xu, Y. Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. B. Xie, R. Shao, G. Chen, K. Zhou, Y. Li, J. Liu, M. Zhang, and L. Nie. Gui-explorer: Autonomous exploration and mining of transition-aware knowledge for GUI agent. In ACL 2025 (Volume 1: Long Papers), pages 56505667. Association for Computational Linguistics, 2025a. T. Xie, D. Zhang, J. Chen, X. Li, S. Zhao, R. Cao, T. J. Hua, Z. Cheng, D. Shin, F. Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37:5204052094, 2024. Y. Xie, Z. Li, R. Shao, G. Chen, K. Zhou, Y. Li, D. Jiang, and L. Nie. Mirage-1: Augmenting and updating gui agent with hierarchical multimodal skills. arXiv preprint arXiv:2506.10387, 2025b. H. Yan, Y. Shen, X. Huang, J. Wang, K. Tan, Z. Liang, H. Li, Z. Ge, O. Yoshie, S. Li, et al. Gui exploration lab: Enhancing screen navigation in agents via multi-turn reinforcement learning. arXiv preprint arXiv:2512.02423, 2025. X. Yang, T. Mei, Y.-Q. Xu, Y. Rui, and S. Li. Automatic generation of visual-textual presentation layout. ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), 12(2):122, 2016. C. Zhang, S. He, J. Qian, B. Li, L. Li, S. Qin, Y. Kang, M. Ma, G. Liu, Q. Lin, S. Rajmohan, D. Zhang, and Q. Zhang. Large language model-brained GUI agents: survey. Trans. Mach. Learn. Res., 2025, 2025a. H. Zhang, C. Li, W. Wu, S. Mao, Y. Zhang, H. Tian, I. Vulic, Z. Zhang, L. Wang, T. Tan, et al. Scaling and beyond: Advancing spatial reasoning in mllms requires new recipes. arXiv preprint arXiv:2504.15037, 2025b. H. Zhang, W. Wu, C. Li, N. Shang, Y. Xia, Y. Huang, Y. Zhang, L. Dong, Z. Zhang, L. Wang, et al. Latent sketchpad: Sketching visual thoughts to elicit multimodal reasoning in mllms. arXiv preprint arXiv:2510.24514, 2025c. H. Zhang, X. Bai, C. Li, C. Liang, H. Tian, H. Li, R. An, Y. Zhang, A. Korhonen, Z. Zhang, L. Wang, and T. Tan. How well do models follow visual instructions? vibe: systematic benchmark for visual instruction-driven image editing, 2026. URL https://arxiv.org/ab s/2602.01851. 16 T. Zhao, C. Chen, Y. Liu, and X. Zhu. Guigan: Learning to generate gui designs using generative adversarial networks. In 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE), pages 748760. IEEE, 2021. X. Zhao, P. Zhang, K. Tang, X. Zhu, H. Li, W. Chai, Z. Zhang, R. Xia, G. Zhai, J. Yan, et al. Envisioning beyond the pixels: Benchmarking reasoning-informed visual editing. arXiv preprint arXiv:2504.02826, 2025. C. Zhuang, A. Huang, Y. Hu, J. Wu, W. Cheng, J. Liao, H. Wang, X. Liao, W. Cai, H. Xu, et al. Vistorybench: Comprehensive benchmark suite for story visualization. arXiv preprint arXiv:2505.24862, 2025."
        },
        {
            "title": "Appendix",
            "content": "A. Evaluation Framework In this section, we detail the proposed evaluation framework. The evaluation framework operates through systematic three-stage pipeline designed to rigorously benchmark image generation models as GUI environments. The process initiates with Image Generation, where image generation models are tasked with generating visual outputs across five distinct task categories: Single-step Visual Transition (single-step), Multi-step Planning (multi-step), Zeroshot Virtual GUI (fiction-app), Rare Trajectory Synthesis (real-app), and Grounding-based Generation (grounding). Subsequently, the generated samples undergo VLM-as-a-Judge process employing evaluator strategy. We leverage VLMs, specifically GPT-4o Hurst et al. (2024), Gemini-3-Pro-Native Google (2025a) and Qwen3-vl-235b-a22b-thinking Bai et al. (2025), to assess the results along five critical dimensions: Goal Achievement (GOAL), Interaction Logic (LOGIC), Consistency (CONS), UI Plausibility (UI), and Visual Quality (QUAL). Finally, the framework concludes with Metrics Analysis, where the calculated scores are validated through statistical verification, pattern analysis, and human relevance alignment to ensure robust and meaningful benchmarking results. Figure A1 GEBench Evaluation Framework Overview. This diagram outlines the comprehensive evaluation process of the GEBench framework, which assesses the performance of image generation models in generating GUI sequences. B. Detailed Performance On GEBench Using Different Judges In this section, we provide granular breakdown of the experimental results, evaluated by three distinct state-of-the-art Visual Language Models (VLMs) acting as autonomous judges. To ensure the robustness and objectivity of our benchmarking, we report the full performance profiles of all 12 evaluated models across three separate evaluation runs: Table A1: Detailed scores assigned by Gemini-3-Pro-Native Google (2025a) as the judge. Table A2: Detailed scores assigned by GPT-4o Hurst et al. (2024) as the judge. Table A3: Detailed scores assigned by Qwen3-vl-235b-a22b-thinking Bai et al. (2025) as the judge. 18 Table A1 Detailed Performance on GEBench using Gemini-3-Pro-Native Google (2025a) as Judge"
        },
        {
            "title": "Nano\nBanana",
            "content": "GPT-Image -1.5 GPT-Image -1.0 Seedream 4.5 Seedream 4.0 Wan 2.6 Flux-2-Pro Bagel UniWorld Qwen -Image-Edit Longcat -Image single-step multi-step fiction-app real-app grounding GOAL LOGIC CONS UI QUAL Overall GOAL LOGIC CONS UI QUAL Overall GOAL LOGIC CONS UI QUAL Overall GOAL LOGIC CONS UI QUAL Overall"
        },
        {
            "title": "GOAL\nLOGIC\nCONS\nUI\nQUAL\nOverall",
            "content": "95.04 82.04 75.24 79.50 90.22 84.41 86.63 56.75 56.95 54.96 90.08 69.07 40.51 51.03 57.95 50.26 89.49 57.85 28.73 44.40 67.13 48.00 90.60 55.77 23.88 36.94 77.96 77.35 92.24 61.67 69.60 58.50 69.70 51.60 73.50 64. 47.61 30.46 45.69 26.80 62.03 42.52 40.40 48.28 60.00 45.45 89.70 56.77 32.80 45.20 65.33 46.80 92.40 56.51 21.80 41.00 71.20 52.00 72.80 51.76 87.65 86.50 72.65 84.76 79.97 82.31 79.13 36.94 36.39 50.93 86.23 57. 46.20 43.13 53.20 68.72 98.20 61.89 26.96 36.36 56.83 70.24 96.16 57.31 15.19 37.39 51.20 66.90 85.57 51.25 86.50 78.80 58.10 58.00 32.00 62.68 77.45 35.31 34.08 50.41 85.82 56.61 38.78 40.61 53.88 61.63 95.71 58. 34.75 39.80 56.77 58.79 97.84 57.59 15.51 37.55 45.31 61.77 81.50 48.33 56.60 49.25 66.23 50.19 61.89 56.83 68.25 36.63 38.66 35.15 66.80 49.10 40.00 39.73 61.00 42.60 92.40 55.15 40.00 37.20 55.00 41.20 89.80 52. 16.12 51.84 64.29 48.57 75.51 51.27 54.58 45.29 65.68 47.87 68.39 56.36 60.14 38.59 38.70 33.44 48.04 43.78 39.33 39.80 64.68 39.40 59.87 48.62 43.80 43.27 67.87 40.60 55.20 50.15 15.42 44.58 67.71 48.96 67.08 48. 58.94 52.17 81.81 53.50 64.65 62.21 46.89 25.54 56.43 37.56 64.88 46.26 36.49 42.54 61.17 41.24 74.16 51.12 33.16 41.10 59.07 39.24 68.78 48.27 11.74 35.65 83.26 68.04 83.26 56.39 65.08 53.60 77.46 48.43 79.70 64. 67.18 52.71 46.46 29.79 63.18 51.86 45.93 48.60 50.67 39.00 85.93 54.03 34.15 47.24 50.71 39.25 84.25 51.12 11.29 36.13 69.68 50.32 79.35 49.35 24.35 29.12 54.76 28.44 30.95 33.52 3.85 8.87 15.46 12.10 13.54 10. 15.80 29.80 41.20 27.60 19.20 26.72 18.20 33.80 63.60 34.20 21.80 34.32 5.86 17.17 62.42 41.62 53.94 36.20 41.00 43.33 58.77 47.93 54.00 49.01 2.29 3.73 31.86 20.17 40.41 19.69 4.00 4.60 88.07 28.03 30.47 31. 2.06 2.06 72.23 22.54 21.79 24.14 4.00 18.20 69.80 70.40 79.20 48.32 25.60 27.93 54.93 40.07 52.00 40.11 1.98 6.70 43.32 29.17 31.60 22.55 0.40 0.81 62.36 27.95 32.39 24.78 0.40 0.40 67.67 27.60 32.80 25. 4.95 14.43 80.21 80.41 83.09 52.62 36.07 35.47 53.73 42.60 45.73 42.72 1.78 2.96 19.89 16.08 10.41 10.22 17.01 20.90 85.00 25.42 20.90 33.85 17.07 20.00 82.79 25.99 20.88 33.35 7.47 15.35 74.95 74.34 73.13 49. Each table provides comprehensive matrix of scores across all five GE-Score dimensions (GOAL, LOGIC, CONS, UI, and QUAL) for every task category in GEBench. For consistent comparison and to mitigate the variance in internal scoring scales across different VLM judgers, all raw evaluation outputs have been linearly normalized to standard range of [0, 100]. This multi-judger approach allows for cross-validation of model capabilities and highlights the consistency of our GE-Score framework. C. Detailed Rubric on five tasks To ensure rigorous and standardized evaluation, we developed series of fine-grained scoring rubrics tailored to the specific requirements of different GUI generation tasks. These rubrics serve as the foundational logic for our VLM-as-a-judge framework. Each rubric decomposes the five GE-Score dimensions (GOAL, LOGIC, CONS, UI, and QUAL) into explicit, linguistic descriptions across multiple performance tiers. This structured approach minimizes the subjective bias of the VLM judges by providing concrete visual and functional benchmarks for each score level. D. Detailed Rubric on five tasks To ensure rigorous and standardized evaluation, we developed series of fine-grained scoring rubrics tailored to the specific requirements of different GUI generation tasks. These rubrics, detailed in Figures A2, A3, A4, and A5, serve as the foundational logic for our VLM-as-a-judge framework. 19 Table A2 Detailed Performance on GEBench using GPT-4o Hurst et al. (2024) as Judge"
        },
        {
            "title": "Nano\nBanana",
            "content": "GPT-Image -1.5 GPT-Image -1.0 Seedream 4.5 Seedream 4.0 Wan 2.6 Flux-2-Pro Bagel UniWorld Qwen -Image-Edit Longcat -Image single-step multi-step fiction-app real-app grounding GOAL LOGIC CONS UI QUAL Overall GOAL LOGIC CONS UI QUAL Overall GOAL LOGIC CONS UI QUAL Overall GOAL LOGIC CONS UI QUAL Overall"
        },
        {
            "title": "GOAL\nLOGIC\nCONS\nUI\nQUAL\nOverall",
            "content": "90.73 94.25 84.52 91.69 78.95 88.03 89.83 84.99 83.12 89.80 90.68 87.68 40.36 62.28 79.77 83.26 78.10 68.75 33.40 55.80 79.00 81.60 78.40 65.64 35.60 67.00 78.20 81.40 80.40 68.52 74.57 81.10 79.49 78.09 69.54 76. 72.30 72.80 77.70 76.60 78.40 75.56 42.07 61.48 76.14 79.26 73.07 66.40 39.24 59.32 80.48 81.56 78.77 67.87 26.07 66.13 85.00 77.67 76.53 66.28 88.37 92.04 78.27 88.57 78.98 85.25 95.44 87.16 82.77 89.16 92.32 89. 51.60 70.07 85.33 91.28 87.87 77.23 43.73 61.72 81.66 88.40 88.15 72.73 20.40 53.80 75.82 76.03 80.74 61.36 80.41 84.39 68.88 82.96 77.14 78.76 95.31 87.45 82.04 89.80 93.98 89.72 49.21 65.15 79.17 87.83 92.32 74. 43.83 59.06 81.22 87.35 92.40 72.77 16.33 55.80 59.33 67.73 75.93 55.02 63.37 68.91 70.40 66.00 55.37 64.81 88.92 80.43 77.70 84.39 85.83 83.45 48.21 62.13 79.87 81.46 82.46 70.83 43.73 57.20 76.33 78.00 80.33 67. 21.55 58.38 75.35 69.23 75.76 60.05 60.56 66.92 70.32 65.55 58.71 64.41 76.96 70.14 68.34 71.55 64.68 70.33 40.47 55.46 71.20 67.72 52.00 57.37 41.34 55.47 69.67 64.93 48.99 56.08 17.73 58.14 68.04 62.89 63.30 54. 71.38 77.18 84.34 75.84 69.68 75.68 77.21 73.03 81.89 80.88 79.02 78.41 40.14 60.89 76.91 72.38 66.80 63.42 41.42 50.25 76.75 71.75 61.92 60.42 15.85 57.21 84.69 77.41 82.18 63.47 73.10 78.07 83.25 78.48 77.06 77. 83.74 78.38 80.51 79.49 76.97 79.82 47.61 64.53 72.20 72.01 65.26 64.32 41.26 62.87 76.28 76.54 69.67 65.32 17.40 58.87 79.80 71.93 77.53 61.11 31.40 38.00 55.90 42.20 37.80 41.06 18.31 26.83 31.47 27.27 25.37 25. 23.00 43.40 52.60 46.87 35.47 40.27 22.80 50.67 64.13 50.73 34.53 44.57 8.47 54.73 67.07 55.13 55.27 48.13 47.30 52.20 62.20 55.40 53.20 54.06 26.21 36.56 56.11 52.60 56.11 45.52 14.53 24.87 77.67 57.47 43.40 43. 9.11 19.42 71.10 46.77 32.85 35.85 7.89 61.00 86.45 85.45 87.17 65.59 37.40 42.40 64.60 55.60 56.90 51.38 15.63 32.28 68.36 55.82 42.07 42.83 6.33 17.07 68.57 45.80 27.83 33.12 4.27 17.53 69.27 42.33 22.47 31. 6.73 63.87 91.53 91.47 90.33 68.79 40.28 44.68 61.44 51.00 48.56 49.19 14.01 29.53 44.75 33.87 21.31 28.69 25.21 46.94 69.58 54.20 40.00 47.19 25.00 43.80 70.80 58.80 41.80 48.04 5.82 62.07 89.03 88.23 84.01 65. Table A3 Detailed Performance on GEBench using Qwen3-vl-235b-a22b-thinking Bai et al. (2025) as Judge"
        },
        {
            "title": "Nano\nBanana",
            "content": "GPT-Image -1.5 GPT-Image -1.0 Seedream 4.5 Seedream 4.0 Wan 2.6 Flux-2-Pro Bagel UniWorld Qwen -Image-Edit Longcat -Image single-step multi-step fiction-app real-app grounding"
        },
        {
            "title": "GOAL\nLOGIC\nCONS\nUI\nQUAL\nOverall",
            "content": "93.64 92.45 83.28 94.25 98.04 92.33 92.35 88.53 84.04 95.96 97.65 91.71 28.13 41.93 49.47 53.47 76.53 49.91 30.47 43.47 58.33 59.80 75.60 53.53 24.38 36.50 46.20 69.49 89.12 53.14 70.54 72.86 66.40 65.62 80.51 71. 60.97 60.44 71.32 60.77 71.89 65.08 34.48 49.90 58.05 55.69 72.73 54.17 30.47 47.32 62.21 59.53 78.52 55.61 20.89 36.56 42.89 58.11 79.38 47.57 84.70 81.27 64.60 83.98 94.85 81.88 93.61 84.43 75.50 90.68 96.34 88. 43.33 58.33 52.87 70.20 97.00 64.35 34.73 50.73 53.13 73.07 97.93 61.92 15.68 32.26 33.77 56.85 81.10 43.93 75.53 71.20 55.43 70.14 86.80 71.82 96.22 89.46 79.22 92.31 98.20 91.08 34.68 46.87 43.30 63.91 94.75 56. 37.33 49.40 49.93 65.47 93.67 59.16 18.72 30.54 25.10 44.30 70.40 37.81 67.24 66.02 59.66 62.41 72.59 65.58 89.39 77.59 73.23 75.00 86.87 80.42 39.19 48.26 49.26 55.64 82.21 54.91 36.67 45.87 48.73 50.47 80.13 52. 19.31 37.66 40.96 56.22 76.29 46.09 65.85 64.41 57.68 61.16 76.03 65.03 83.45 75.09 71.26 73.23 80.17 76.64 34.27 45.67 43.73 45.27 60.47 45.88 39.00 46.27 42.80 41.53 56.00 45.12 14.41 32.21 38.76 52.55 72.76 42. 67.65 69.13 70.31 65.02 77.37 69.90 72.70 65.92 76.32 74.48 87.13 75.31 27.50 45.28 49.51 47.64 73.12 48.61 27.95 47.03 51.30 50.88 67.03 48.84 16.05 34.45 48.21 67.22 86.08 50.40 66.84 68.27 69.97 63.57 80.58 69. 83.80 76.15 72.29 64.74 77.29 74.85 34.13 48.70 43.33 40.67 63.00 45.97 35.08 48.56 47.81 48.42 66.94 49.36 16.58 31.81 46.44 56.98 73.56 45.07 27.14 30.44 37.58 31.85 41.21 33.64 4.62 17.65 19.69 16.12 17.65 15. 15.69 30.24 25.86 22.36 20.88 23.01 17.58 33.60 34.75 26.06 20.88 26.57 12.44 22.34 35.79 40.07 53.11 32.75 38.72 38.32 36.50 40.17 55.19 41.78 6.44 23.95 42.06 33.32 48.06 30.77 9.73 19.60 53.07 34.00 34.00 30. 8.40 14.95 37.75 22.39 22.73 21.24 12.50 22.85 40.83 59.03 81.39 43.32 33.23 35.32 44.07 44.78 60.30 43.54 4.22 19.70 57.16 51.10 53.59 37.15 8.07 13.93 20.53 19.47 24.47 17.29 4.33 12.80 17.60 15.13 16.60 13. 6.69 15.00 42.04 68.66 84.23 43.32 35.05 34.78 34.88 35.59 48.48 37.76 3.70 16.69 30.29 21.16 21.16 18.60 18.40 31.39 36.04 29.93 29.03 28.96 21.73 31.33 43.96 34.93 34.53 33.30 9.08 18.16 42.13 61.35 80.21 42. Each rubric decomposes the five GE-Score dimensions (GOAL, LOGIC, CONS, UI, and QUAL) into explicit, linguistic descriptions across multiple performance tiers (e.g., from In20 complete\" to Exceptional\"). This structured approach minimizes the subjective bias of the VLM judges by providing concrete visual and functional benchmarks for each score level. Specifically: Figure A2 outlines the criteria for Single-Step Transition, focusing on the immediate visual mapping of user instructions. Figure A3 details the Multi-Step Planning rubrics, emphasizing the accumulation of errors and temporal coherence across 5 step trajectories. Figure A4 provides the standards for Zero-shot Virtual GUI generation, where the judge assesses the imaginative plausibility and structural integrity of non-existent applications. Figure A5 defines the benchmarks for Grounded Generation, specifically evaluating the pixel-level alignment between generated content and coordinate-based prompts. By employing these detailed rubrics, we bridge the gap between qualitative visual inspection and quantitative performance metrics. Figure A2 Evaluation Rubrics for Single-Step Transition Generation 22 Figure A3 Evaluation Rubrics for Multi-Step Planning Generation 23 Figure A4 Evaluation Rubrics for Zero-shot Virtual GUI Generation and Rare Trajectory Synthesis Figure A5 Evaluation Rubrics for Grounding-based Generation"
        }
    ],
    "affiliations": [
        "Peking University",
        "South China University of Technology",
        "StepFun"
    ]
}