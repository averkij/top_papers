{
    "paper_title": "Exploring Conditions for Diffusion models in Robotic Control",
    "authors": [
        "Heeseong Shin",
        "Byeongho Heo",
        "Dongyoon Han",
        "Seungryong Kim",
        "Taekyung Kim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While pre-trained visual representations have significantly advanced imitation learning, they are often task-agnostic as they remain frozen during policy learning. In this work, we explore leveraging pre-trained text-to-image diffusion models to obtain task-adaptive visual representations for robotic control, without fine-tuning the model itself. However, we find that naively applying textual conditions - a successful strategy in other vision domains - yields minimal or even negative gains in control tasks. We attribute this to the domain gap between the diffusion model's training data and robotic control environments, leading us to argue for conditions that consider the specific, dynamic visual information required for control. To this end, we propose ORCA, which introduces learnable task prompts that adapt to the control environment and visual prompts that capture fine-grained, frame-specific details. Through facilitating task-adaptive representations with our newly devised conditions, our approach achieves state-of-the-art performance on various robotic control benchmarks, significantly surpassing prior methods."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 0 1 5 5 1 . 0 1 5 2 : r a"
        },
        {
            "title": "EXPLORING CONDITIONS FOR DIFFUSION MODELS\nIN ROBOTIC CONTROL",
            "content": "Heeseong Shin1 , Byeongho Heo2, Dongyoon Han2, Seungryong Kim1, Taekyung Kim2 KAIST AI1 NAVER AI Lab2 {hsshin98,seungryong.kim}@kaist.ac.kr {taekyung.k, bh.heo, dongyoon.han}@navercorp.com"
        },
        {
            "title": "ABSTRACT",
            "content": "While pre-trained visual representations have significantly advanced imitation learning, they are often task-agnostic as they remain frozen during policy learning. In this work, we explore leveraging pre-trained text-to-image diffusion models to obtain task-adaptive visual representations for robotic control, without fine-tuning the model itself. However, we find that naively applying textual conditionsa successful strategy in other vision domainsyields minimal or even negative gains in control tasks. We attribute this to the domain gap between the diffusion models training data and robotic control environments, leading us to argue for conditions that consider the specific, dynamic visual information required for control. To this end, we propose ORCA, which introduces learnable task prompts that adapt to the control environment and visual prompts that capture fine-grained, frame-specific details. Through facilitating task-adaptive representations with our newly devised conditions, our approach achieves state-of-the-art performance on various robotic control benchmarks, significantly surpassing prior methods."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advances in diffusion models (Ho et al., 2020) have not only facilitated high-quality image synthesis, but also demonstrated as strong visual representation for various vision tasks (Baranchuk et al., 2021). Among them, pre-trained text-to-image diffusion models, e.g. Stable Diffusion (Rombach et al., 2022), have shown that utilizing text conditions can significantly boost the performance in visual perception tasks, without the need for fine-tuning the model (Zhao et al., 2023). The key to leveraging text conditions lies in obtaining well-designed prompts (Kondapaneni et al., 2024)often describing objects in the image or the given taskthat can funnel useful information into downstream tasks. This not only enhances the proficiency of diffusion models on downstream tasks but also broadens their applicability to wider variety of vision tasks (Yin et al., 2025; Wu et al., 2025). Robotic control, meanwhile, has also benefited greatly with the introduction of pre-trained visual representations to imitation learning (Parisi et al., 2022). By leveraging frozen visual encoders pre-trained on large-scale data, these representations have replaced the previous tabula-rasa paradigm of training vision encoders from scratch on limited-scale control data. However, this approach is limited by its task-agnostic nature, as the visual representations remain frozen during downstream policy learning. Since the suitability of representation for specific task is unknown beforehand, determining which representation performs best often requires manual, task-by-task inspection (Majumdar et al., 2023), which becomes cumbersome given the vast variety of control tasks. While straightforward solution might be to fine-tune the vision encoder, this often results in poor results as the model loses generalization capabilities by overfitting to specific scenes in imitation learning (Parisi et al., 2022; Hansen et al., 2022; Majumdar et al., 2023). In this work, we explore bridging text-to-image diffusion models to robotic control for achieving taskadaptive visual representations through conditions, without fine-tuning the diffusion model. Inspired from the effectiveness of conditions in visual perception tasks, we ask following question: How can we effectively implement conditions for diffusion models in robotic control? We begin by investigating Work done during an internship at NAVER AI Lab. Corresponding authors."
        },
        {
            "title": "Exploring Conditions for Diffusion Models in Robotic Control",
            "content": "Figure 1: How can we condition diffusion models in robotic control? We investigate methods for conditioning text-to-image diffusion models (Rombach et al., 2022) to perform control, aiming to address various tasks in task-adaptive manner. We observe that text prompts, unlike in other vision tasks (Zhao et al., 2023), are ineffective for robotic control. Therefore, we propose to learn task prompts in control environments and further incorporate dynamic details through visual prompts for conditioning diffusion models. textual conditions, generating captions with state-of-the-art vision-language model (Comanici et al., 2025) to observe their impact on control task performance. However, as shown in Fig. 1, the gains are minimal, and in some cases, performance even declines. This result contrasts sharply with findings in other vision tasks (Zhao et al., 2023), where machine-generated captions have served as strong conditions (Kondapaneni et al., 2024). Upon investigation, we find that pre-trained diffusion models often struggle to accurately associate text condition to the image in control environments. We attribute this discrepancy to the nature of the diffusion model being training on web-collected images, which suits visual tasks that involve realworld images and common objects, such as semantic segmentation. However, control environments, featuring specialized robotic agents performing specific tasks, would require more careful and deliberate approach to devising effective conditions for downstream policy learning. Robotic control tasks, unlike semantic segmentation (Zhao et al., 2023; Kondapaneni et al., 2024), operate on dynamic video streams and require finer visual granularity to interact with specific parts of objects, not just to categorize them. This dynamic nature implies that effective conditions must be generated uniquely for each frame (Hong et al., 2024c) to guide evolving actions and adapt to changing visual states. Consequently, we hypothesize that conditions for control should incorporate visual information from every frame to capture both dynamic behavior and fine-grained details. To this end, we propose simple, yet effective method that incorporates visual information while addressing the limitations of text conditions. Specifically, we replace the text prompt with learnable task prompts, which are learned during downstream control tasks to ensure accurate grounding within the specific environment. Furthermore, to enable the conditions to capture the detailed visual state of each frame, we employ vision encoder and utilize its representations as visual prompts. We demonstrate that both the task and visual prompts can be learned end-to-end during downstream policy learning using standard behavior cloning objective. Our framework for leveraging diffusion models with conditions in robotic control in task-adaptive manner, ORCA, achieves state-of-the-art performance in robotic control tasks (Tassa et al., 2018; Yu et al., 2020; Rajeswaran et al., 2018), surpassing VC-1 (Majumdar et al., 2023). We verify our design choices by comparing to baselines with text conditions and different conditioning methods (Zhou et al., 2022; Kondapaneni et al., 2024) from visual perception tasks. In addition, we provide detailed analysis and ablations on our approach, highlighting the importance of conditions in diffusion models for robotic control."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 PRE-TRAINED VISUAL REPRESENTATIONS FOR ROBOTIC CONTROL In recent years, visual representations derived from self-supervised pre-trained models (Radford et al., 2021; Cherti et al., 2023; Majumdar et al., 2023; He et al., 2022; Caron et al., 2021; Kim et al., 2025c) have demonstrated notable effectiveness in visuo-motor manipulation tasks (Parisi"
        },
        {
            "title": "Exploring Conditions for Diffusion Models in Robotic Control",
            "content": "et al., 2022). Specifically, Parisi et al. (Parisi et al., 2022) showed that visual representations from frozen pre-trained encoders, such as MoCo (He et al., 2020) and CLIP (Radford et al., 2021), can not only outperform representations trained from scratch but are also comparable to ground-truth state features in behavior cloning. This finding has spurred extensive exploration into pre-trained visual representations for control, alongside search for self-supervised learning frameworks particularly suited for robotic manipulation. Among these, R3M (Nair et al., 2022) employs time-contrastive learning objective on ego-centric data with vision-language alignment, whereas VIP (Ma et al., 2022) introduces value-implicit learning to associate goal and initial states. MVP (Radosavovic et al., 2023) and VC-1 (Majumdar et al., 2023) both adopt MAE (He et al., 2022) pre-training methodologies, curating large datasets that include ego-centric and instructional videos to enhance transferability to robotic manipulation tasks. More recently, SCR (Gupta et al., 2024) has investigated representations from Stable Diffusion (Rombach et al., 2022) for navigation and control tasks. Nonetheless, these methods opted for keeping the visual representation frozen, resulting them to be task-agnostic."
        },
        {
            "title": "2.2 DIFFUSION MODELS AS PRE-TRAINED VISUAL REPRESENTATIONS",
            "content": "Recent advancements in diffusion models (Ho et al., 2020; Rombach et al., 2022) have enabled the synthesis of high-resolution images with unprecedented fidelity. This progress has concurrently motivated diverse investigations into the internal representations of generative diffusion models (Tang et al., 2023; Luo et al., 2023; Baranchuk et al., 2021; Zhao et al., 2023; Xiang et al., 2023; Wu et al., 2025; Kim et al., 2025a) for various downstream vision tasks. This allowed diffusion models to outperform prior approaches with self-supervised pre-trained models (Shin et al., 2024; Hong et al., 2022) in tasks such as semantic correspondence (Cho et al., 2021), semantic segmentation (Cho et al., 2024), and even 3D reconstruction (Hong et al., 2024a;b). DDPMSeg (Baranchuk et al., 2021) was among the first to explore the efficacy of diffusion models representations in label-scarce segmentation, while DDAE (Xiang et al., 2023) focused on image classification. DIFT (Tang et al., 2023), DHF (Luo et al., 2023) and SD-DINO (Zhang et al., 2023) have demonstrated that the representation from diffusion models can achieve state-of-the-art in semantic correspondence tasks. Notably, VPD (Zhao et al., 2023) demonstrated that downstream performance can be enhanced by with text conditions, such as the names of objects present in an image, in tasks such as semantic segmentation and monocular depth estimation. SD4Match (Li et al., 2024) and EcoDepth (Patni et al., 2024) proposed prompting modules to derive conditions for semantic correspondence and monocular depth estimation. TADP (Kondapaneni et al., 2024) demonstrated that text descriptions generated from vision-language models can serve as strong conditions, and could be further enhanced with style modifiers learned from Textual Inversion (Gal et al., 2022). However, we distinguish our approach by focusing on robotic control, rather than for visual tasks in general image domains."
        },
        {
            "title": "3 PRELIMINARIES",
            "content": "Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Kingma et al., 2021) constitute class of generative models that learn to reverse multi-step noising process, thereby reconstructing target data distribution. In this work, we focus on conditional diffusion models (e.g. Stable Diffusion (Rombach et al., 2022)), which enable image generation guided by condition C, often being text prompts. The training objective is to reverse the noising process, typically discretized into timesteps. pre-defined noise schedule, denoted by αt, facilitates the definition of the noised latent variable zt at timestep as: zt = (1) where z0 is the initial clean data, αt = (cid:81)t i=1 αi, and ϵ (0, I) is Gaussian noise. Following Ho et al. (Ho et al., 2020), with appropriate parameterization, diffusion models can be trained by regressing the added noise ϵ from zt: 1 αtϵ, αtz0 + LDM = Ez0,ϵ,t ϵ ϵθ(zt(z0, ϵ), t; C)2 2 , (2) (cid:104) (cid:105) where ϵθ indicates the denoising network, typically U-Net (Ronneberger et al., 2015) or Transformer (Vaswani et al., 2017) architecture. Stable Diffusion, for our case, is Latent Diffusion Model (LDM) (Rombach et al., 2022) with an U-Net architecture, in which the diffusion process occurs in compressed latent space learned by an autoencoder, specifically VQGAN (Esser et al., 2021). For conditional generation, U-Net-based LDMs implement Transformer blocks with cross-attention layers into the U-Net blocks to inject the condition into the image generation process."
        },
        {
            "title": "Exploring Conditions for Diffusion Models in Robotic Control",
            "content": "Figure 2: Motivation. We aim to overcome the limitations of existing task-agnostic approach (a) with frozen pre-trained visual representations (Parisi et al., 2022), by leveraging conditions in diffusion models for robotic control tasks in task-adaptive approach (b). In this regard, we explore text conditions( 4.1), more advanced methods( 4.2, 5) as conditions. Extracting visual representation from diffusion models. To extract visual representations, initially, an input image is encoded into its latent representation z0 = E(I) using the VQGAN encoder E. For chosen fixed timestep t, the corresponding noisy latent zt is computed via Eq. 1. This zt is then processed by the denoising U-Net ϵθ(). However, as the network ϵθ is trained to predict noise as shown in Eq. 2, we instead extract intermediate feature maps from within the U-Net (Meng et al., 2024). We denote the set of extracted intermediate features as , and denote = ϵθ(zt, t; C) to be the output of ϵθ for simplicity, and primarily consider features from the earlier blocks of the U-Net."
        },
        {
            "title": "4 MOTIVATION",
            "content": "In this work, we explore conditional diffusion models to generate visual representations for robotic control, aiming to overcome the limitations of task-agnostic approaches. While pre-trained visual representations have been paramount to advancements in control, the standard approach of deploying the same frozen representation across various tasks often fails to adapt to their specific requirements, causing performance to fluctuate significantly (Majumdar et al., 2023). We aim to address this limitation by leveraging text-to-image diffusion models, which have successfully handled diverse visual tasks in task-adaptive manner using well-designed textual prompts as conditions. Our goal is therefore to explore effective ways to condition diffusion models for control, as illustrated in Fig. 2. However, we find that text conditions are ineffective in robotic control environments ( 4.1), as using captions generated from vision-language models yields insignificant gains, or even degrades performance. An in-depth inspection of the cross-attention maps reveals the underlying reason for this failure - in tasks where performance degrades, the diffusion model struggles to correctly associate words with their corresponding image regions. This underscores the need for alternatives to text descriptions and for careful consideration when devising conditions specifically for robotic control. Consequently, we discuss what do we need for effectively conditioning diffusion models in robotic control ( 4.2). By their nature, control tasks involve video frames with fine-grained movements of agents and objects. Relying solely on textual conditions would necessitate generating highly detailed, frame-by-frame description of the specific agent parts relevant to the current actiona challenging and often impractical task. Therefore, we posit that we should incorporate visual information for effective conditions to capture the fine-grained details of each frame. 4.1 EXPLORING TEXTUAL CONDITIONS FOR ROBOTIC CONTROL To obtain textual descriptions of control environments, we devise baseline by prompting stateof-the-art vision-language model, Gemini 2.5 (Comanici et al., 2025), to generate descriptions of these tasks. The full text descriptions are provided in the appendix. For our analysis, we compare the null () conditionimplemented as an empty string with only <eos> and <bos> tokensand the text condition in downstream control tasks. However, as observed in Fig. 3(a), the results are mixed: while text conditions benefit some tasks (e.g., Button-press, Reacher), they degrade performance in others (e.g., Cheetah-run)."
        },
        {
            "title": "Exploring Conditions for Diffusion Models in Robotic Control",
            "content": "Figure 3: Case study. (a) We find that text conditions can be disadvantageous in some control tasks. (b) For Button-press, the cross-attention maps (e.g., for button, press) are well-grounded to relevant image regions. (c) In contrast, for Cheetah-run, the attention maps (e.g., for cheetah, run) are noisy, which presumably leads to decline in performance. Nonetheless, our approach of using task and visual tokens ( 5) achieves consistent gains across all tasks, with its cross-attention maps capturing diverse regions of the image relevant to the downstream task. To take deeper look, in Fig. 3(b), we visualize the cross-attention maps for Button-press, task where text conditions show noticeable gains. For words such as press or button, the cross-attention maps are well-associated with the relevant regions within the image. These results are similar to what is expected from text conditions in other visual perception tasks like semantic segmentation (Zhao et al., 2023), which verifies the potential of using conditions in control tasks. However, in Fig. 3(c), we observe the opposite for Cheetah-run, where words like cheetah or run show noisy cross-attention maps. The <eos> token of the null condition is already roughly grounded to the salient object, the agent in this case, which explains how sub-optimal text condition can degrade performance to be even worse than the null condition. We primarily attribute the failure of text conditions, despite being generated from state-of-the-art vision-language model, to the domain gap between real-world images and simulated control environments. This finding highlights the need for careful consideration when devising conditions in robotic control and motivates the exploration of alternatives to text descriptions for representing the task. 4.2 WHAT DO WE NEED AS CONDITIONS IN ROBOTIC CONTROL? In order to devise effective conditions, we discuss the characteristics of robotic control tasks and contrast with other vision-based tasks, such as semantic segmentation. primary distinction is that control tasks operate on video streams rather than static images. Consequently, logical approach would be to generate unique condition for each frame (Hong et al., 2024c), allowing the representation to adapt to the changing visual state of the environment. For instance, instructing an agent to walk requires sequence of distinct commands (e.g., move the left foot, then the right). Similarly, an effective condition should vary across frames to guide such dynamic behaviors. However, generating high-quality text descriptions on frame-by-frame basis would not only be challenging but would also inherit the same grounding limitations discussed previously. In this regard, we hypothesize that to account for this dynamic adaptability, conditions should incorporate visual information from each frame. While diffusion models like Stable Diffusion are typically trained on text, several approaches exist for incorporating visual information, either by introducing features from external vision encoders (Li et al., 2024; Patni et al., 2024) or by optimizing specialized text tokens to represent visual concepts (Kondapaneni et al., 2024; Kim"
        },
        {
            "title": "Exploring Conditions for Diffusion Models in Robotic Control",
            "content": "Figure 4: Proposed framework. We propose ORCA, framework for learning task and visual prompts to condition diffusion models in robotic control. Specifically, we utilize the features from the downsampling blocks and the bottleneck block of Stable Diffusion (Rombach et al., 2022) to extract visual representations conditioned on our input, which are then fed to the policy network for predicting the action. et al., 2025b). These existing methods, however, tend to embed the global representation into the condition, or require additional optimization steps to acquire specialized tokens. Since our goal is to enable the recognition of fine-grained regions within each frame, we consider that adopting global representations and extra optimization steps should be avoided to facilitate effective frame-wise conditioning."
        },
        {
            "title": "5 ORCA: CONDITIONING DIFFUSION MODELS FOR ROBOTIC CONTROL",
            "content": "Based on our observations, we present ORCA, simple yet effective approach for learning conditions for diffusion models in robotic control. We design our conditions to adapt to the control environment, preventing erroneous grounding, while simultaneously incorporating visual information to capture dynamic details. To achieve this with minimal overhead during downstream policy learning, we formulate these conditions as learnable prompts (Gal et al., 2022). Specifically, we introduce learnable task and visual prompts that integrate task-specific implicit descriptions with frame-level visual information, as described in detail below. Task prompts. Recalling that text conditions show potential when well-grounded to task-relevant regions, we design our task prompt to capture objects or areas that are critical to solving the downstream task. Therefore, we adopt direct approach of learning the text as implicit words (Zhou et al., 2022) within the downstream task to minimize erroneous grounding. To achieve this, we implement task prompts as learnable parameters that are shared across all observations during training. We find that this allows the task prompts to implicitly learn to focus on relevant regions, as shown in Fig. 3(b,c), where the cross-attention maps simultaneously highlight both the button and the robot arm in Button-press and the agent in Cheetah-run. Visual prompts. Furthermore, to incorporate visual information into the conditions, we adopt vision encoder EV to leverage its visual representation as prompts. Specifically, we utilize the dense visual representations from EV , rather than global representations, and project them through small convolutional layer to complement the task prompts. This focus on dense features provides the fine-grained, localized information necessary for control tasks. As visualized in Fig. 3(b,c), the resulting attention from the visual prompts highlights various regions in detail, such as distinguishing between the front and back legs of the agent in Cheetah-run. Policy learning objective. We learn the prompts by directly optimizing for the behavior cloning objective in downstream policy learning, as presented in Fig. 4. Let πϕ() be the policy network with parameters ϕ that takes the visual state representations derived by the diffusion model and outputs"
        },
        {
            "title": "Exploring Conditions for Diffusion Models in Robotic Control",
            "content": "Table 1: Experimental results on vision-based robot policy learning on DeepMind Control. The performance of imitation learning agents on DeepMind Control (Tassa et al., 2018) is reported. We report the normalized score averaged over three seeds with its standard deviation. Methods Backbone Walker-stand Walker-walk Reacher-easy Cheetah-run Finger-spin Mean CLIP VC-1 SCR Text (Simple) Text (Caption) CoOp TADP ORCA (Ours) ViT-L/16 ViT-L/16 SD 1.5 SD 1.5 SD 1.5 SD 1.5 SD 1.5 SD 1. 87.3 2.4 86.1 0.9 85.5 2.6 87.6 4.6 87.2 4.5 87.2 2.2 89.0 2.9 89.1 1.8 58.3 4.4 54.3 6.6 64.3 3.5 67.9 4.6 68.3 5.9 67.8 6.4 69.9 7.9 76.9 4.0 54.5 4.6 18.3 2.4 81.8 9.9 84.3 4.6 86.2 1.9 87.1 5.9 86.6 5.6 87.6 2.9 29.9 5.6 40.9 2.7 43.4 6.4 38.8 5.9 37.5 2.6 45.0 6.4 41.1 3.9 50.0 8.4 67.5 2.1 65.7 1.1 66.6 2.7 66.7 0.2 65.1 1.8 65.9 1.0 66.9 0.2 68.0 1.0 59.5 53.1 68.3 69.1 68.9 70.6 70.7 74. Table 2: Experimental results on vision-based robot policy learning on MetaWorld. The performance of imitation learning agents on MetaWorld (Yu et al., 2020) is reported. We report the success rates (%) averaged over three seeds with their standard deviation. Methods Backbone Assembly Bin-picking Button-press Drawer-open Hammer Mean CLIP VC-1 SCR Text (Simple) Text (Caption) CoOp TADP ORCA (Ours) ViT-L/16 ViT-L/16 SD 1.5 SD 1.5 SD 1.5 SD 1.5 SD 1.5 SD 1.5 85.3 12.2 93.3 6.1 92.0 6.9 97.3 2.3 96.0 4.0 96.0 4.0 96.0 4.0 98.7 2.3 69.3 8.3 61.3 12.2 86.7 4.6 85.3 2,3 88.0 6.9 89.3 2,3 90.7 4.6 90.7 4.6 60.0 13.9 73.3 8.3 74.7 12.9 78.7 2,3 80.0 8.0 81.3 6.1 80.0 10.6 88.0 6.9 100.0 0.0 100.0 0.0 100.0 0.0 100.0 0.0 100.0 0.0 100.0 0.0 100.0 0.0 100.0 0. 92.0 8.0 93.3 6.1 98.7 2.3 96.0 6.9 98.7 2.3 96.0 6.9 96.0 4.0 98.7 2.3 81.3 84.2 90.4 91.5 92.5 92.5 93.1 95.2 o}To actions. Given sequences of To observations {I o=0 from the i-th trajectory, we predict each action and train both the policy network πϕ(), task prompts pt and visual prompts pt by the behavior cloning loss: o=0 and actions {ai o}To LBC(ϕ,p) = (cid:88) (cid:88) i=1 πϕ(ϵθ(zt, t; C)) ai o, (3) αtE(I 1 αtϵ, and condition = τθ(pt; pv) is derived from the text encoder where zt = τθ with task prompt pv and visual prompt pv as the input. We find that pv and pt can be both learned with the behavior cloning loss in downstream policy learning. o) +"
        },
        {
            "title": "6 EXPERIMENTS",
            "content": "In this section, we establish the details for the evaluation ( 6.1) and the implementation ( 6.2) of our method, and present extensive experimental results ( 6.4) and analyses ( 6.5). We also provide further analyses ( A) and additional details (( B) in the appendix. 6.1 EVALUATION SUITES We conduct experiments on three widely-used visionbased robot learning benchmarks with the total of 12 tasks following VC-1 (Majumdar et al., 2023), as shown in Fig. 5. DeepMind Control (DMC) (Tassa et al., 2018) is set of continuous control tasks with simulated robots. We use five imitation learning cases: Walker-stand, Walker-walk, Reacher-easy, Cheetah-run, and Fingerspin. We report the normalized scores for all tasks. Figure 5: Visualization of evaluation tasks. MetaWorld (Yu et al., 2020) is suite of simulated robotic manipulation tasks with Sawyer robot arm. We focus on subset of five representative tasks: Assembly, Bin-picking, Button-press-topdown, Drawer-open, and Hammer. We measure the best success rates among the online evaluation trials."
        },
        {
            "title": "Exploring Conditions for Diffusion Models in Robotic Control",
            "content": "Table 3: Experimental results on vision-based robot policy learning on Adroit. The performance of imitation learning agents on Adroit (Rajeswaran et al., 2018) is reported. We report the success rates (%) averaged over three seeds with their standard deviation. Methods Backbone Pen Relocate Mean CLIP VC-1 SCR Text (Simple) Text (Caption) CoOp TADP ORCA (Ours) ViT-L/16 ViT-L/16 SD 1.5 SD 1.5 SD 1.5 SD 1.5 SD 1.5 SD 1.5 58.7 2.3 65.3 16.7 84.0 4.0 80.0 6.9 80.0 4.0 82.7 6.1 81.3 6.1 86.7 2.3 44.0 4.0 29.3 8.3 32.0 4.0 34.7 6.1 34.7 4.6 33.3 6.1 33.3 8.3 44.0 4.0 51.4 47.3 58.0 57.3 57.3 58.0 57.3 65.3 Figure 6: Cross-attention visualization for task/visual prompts. We visualize the cross-attention and p2 maps for task prompt pt and visual prompts p1 in Relocate task across frames from Adroit. Adroit (Rajeswaran et al., 2018) is an imitation learning benchmark in simulated environment, consisting of dexterous manipulation tasks that require an agent to control 28-DoF anthropomorphic hand. Our study mainly focuses on Relocate and Pen, and measure the best success rates among the online evaluation trials. 6.2 IMPLEMENTATION DETAILS Diffusion model and conditions. We employ Stable Diffusion v1.5 (Rombach et al., 2022) as the diffusion model. For extracting visual representation from observations, we leverage the features from the downsampling blocks and the bottleneck block in the diffusion U-Net and forward through compression layer (Yadav et al., 2023). We set the timestep = 0, the length of task tokens lt = 4, and the length of visual tokens lv = 16, where all learnable parameters are randomly initialized. For EV , we employ pre-trained DINOv2 (Oquab et al., 2023). Further implementation details are presented in the appendix. Vision-based robot policy learning. We consider two, five, and five demonstrations from Adroit, DeepMind Control (DMC), and MetaWorld, respectively, where proprioceptive data is utilized except for the DMC benchmark. We mainly follow the experimental setups in VC-1 (Majumdar et al., 2023) except that we employ compression layer for all baselines for fair comparison. For each task, we train the agent for 100 epochs, with periodic online evaluation in the simulated environment every 10 epochs. 6.3 BASELINES We consider three baselines: CLIP (Radford et al., 2021), VC-1 (Majumdar et al., 2023), and SCR (Gupta et al., 2024), as task-agnostic baselines, which follow the standard frozen pre-trained visual representation approach. In addition, we provide four task-adaptive baselines: Textsimple, Textcaption, CoOp (Zhou et al., 2022), TADP (Kondapaneni et al., 2024). We provide details for the baselines in the appendix. 6.4 MAIN RESULTS Quantitative results. We report experimental results from DMC, MetaWorld, and Adroit in Table 1, Table 2, and Table 3, respectively. Among the task-agnostic baselines, while SCR performs best"
        },
        {
            "title": "Exploring Conditions for Diffusion Models in Robotic Control",
            "content": "Table 4: Components analysis. To ablate the design choices for learning conditions, we conduct component analysis on task prompt pt and visual prompt pv. The performance of imitation learning agents on DeepMind Control (Tassa et al., 2018) is reported. We report the normalized score averaged over three seeds with its standard deviation. Components pt pv Walker-stand Walker-walk Reacher-easy Cheetah-run Finger-spin Mean DeepMind Control 85.5 2.6 83.6 3.2 85.9 2.7 89.1 2.3 64.3 3.5 71.4 3.5 71.1 2.3 76.9 4.0 81.8 1.7 86.7 6.6 87.3 5.5 87.6 2.9 43.4 4.4 38.9 10.1 42.0 10.4 50.0 8. 66.6 2.7 68.2 1.2 66.1 1.0 68.0 1.0 68.3 69.8 70.5 74.3 Table 5: Ablation study on layer selection. We evaluate individual layers of the diffusion U-Net by reporting layer-wise performance. The performance of imitation learning agents on DeepMind Control (Tassa et al., 2018) is reported. We report the normalized score averaged over three seeds with its standard deviation. Layer down 1 down 2 down 3 mid up 0 up 1 up 2 Walker-stand Walker-walk DeepMind Control Reacher-easy Cheetah-run Finger-spin Mean 86.3 2.1 89.3 1.2 86.2 4.3 88.3 4.9 82.8 2.6 79.5 4.5 70.4 4.5 65.5 1.1 68.3 2.7 73.3 3.9 70.4 1.3 71.7 5.9 60.3 16.1 39.1 3.3 82.1 3.7 70.0 18.8 75.3 8.1 62.3 1.1 45.3 4.0 55.9 5.2 41.0 7.0 40.8 1.1 31.2 2.6 36.0 4.8 35.0 4.7 28.5 1.8 39.9 7.0 30.9 3. 67.6 0.3 67.0 1.0 67.0 0.5 67.2 0.6 67.2 0.6 66.4 0.4 67.7 1.0 68.4 65.1 67.5 64.6 59.0 60.4 49.7 74.3 down 1-3, mid 89.1 1.8 76.9 4. 87.6 2.9 50.0 8.4 68.0 1.0 overall, we observe that VC-1 and CLIP outperform it in certain tasks. This highlights fundamental limitation of such approaches: due to their task-agnostic nature, no single representation is guaranteed to excel across all tasks. In contrast, across all 12 tasks in the 3 evaluation suites, ORCA establishes the new state-of-the-art, outperforming all baselines by significant margin. Furthermore, we observe that more advanced task-adaptive baselines, CoOp and TADP, generally outperform text conditions, which aligns with our analysis and confirms our hypothesis that incorporating visual information is critical. Nonetheless, since CoOp and TADP were originally designed for tasks like image classification and semantic segmentation, their effectiveness in robotic control is limited, as shown by their minimal gains on DMC and Adroit. In contrast, our method show solid improvements across all tasks. 6.5 ANALYSIS and p2 Visualization of task and visual prompts. In Fig. 6, we visualize the cross-attention maps for our task prompt pt and visual prompts, p1 v, on Relocate. In this task, robot hand first picks up blue ball from table (Frames 1-30) and then moves it to the location of green sphere (Frames 30-45). As discussed in 5, we observe that the task prompt consistently captures regions relevant to the overall goal, namely the robot hand and the target green sphere. Conversely, the visual prompts exhibit more dynamic behaviors. While p1 interestingly attends to the table as the hand moves downward to pick up the ball, then shifts its focus to the hand as it lifts off and moves toward the target, suggesting that it has learned to capture task-relevant movements. tends to focus on the hand, p2 Ablation study on each component. In Table 4, we conduct component analysis by ablating task prompt pt and visual prompt pv respectively. Notably, we observe that when employed individually, task and visual prompts can show divergent behavior across different tasks. This could suggest that each tasks focus in different aspects of the scene, such as Reacher-easy focusing more in visual details as it benefits more from visual prompts compared to text prompts. Nonetheless, when fully incorporating both task and image prompts, we show consistent gains across all tasks. Ablation study on layer selection. In Table 5, we provide layer-wise evaluation of the diffusion model to ablate the choice of layer selection. We can observe that while the performance varies for"
        },
        {
            "title": "Exploring Conditions for Diffusion Models in Robotic Control",
            "content": "each tasks on which layers performs the best, the early layers, being the downsampling blocks and the middle bottleneck layer, performs better than the upsampling layers. Therefore, we leverage multi-layer features by concatenating the best-performing layers (down 1,down 2,down 3,mid), which yields the best overall performance. While this layer selection coincides with SCR (Gupta et al., 2024), we find this to also share findings with Parisi et al. (2022), where they find that representations from early layers of vision encoders perform better in robotic control tasks."
        },
        {
            "title": "REFERENCES",
            "content": "Dmitry Baranchuk, Ivan Rubachev, Andrey Voynov, Valentin Khrulkov, and Artem Babenko. Labelefficient semantic segmentation with diffusion models. arXiv preprint arXiv:2112.03126, 2021. 1, 3 Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 96509660, 2021. 2 Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 28182829, 2023. 2, 16, 17, 18, 19 Seokju Cho, Sunghwan Hong, Sangryul Jeon, Yunsung Lee, Kwanghoon Sohn, and Seungryong Kim. Cats: Cost aggregation transformers for visual correspondence. Advances in Neural Information Processing Systems, 34:90119023, 2021. 3 Seokju Cho, Heeseong Shin, Sunghwan Hong, Anurag Arnab, Paul Hongsuck Seo, and Seungryong Kim. Cat-seg: Cost aggregation for open-vocabulary semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 41134123, June 2024. 3 Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 2, 4, 15, Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1287312883, 2021. 3 Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. 3, 6, 16 Gunshi Gupta, Karmesh Yadav, Yarin Gal, Dhruv Batra, Zsolt Kira, Cong Lu, and Tim GJ Rudner. Pretrained text-to-image diffusion models are versatile representation learners for control. Advances in Neural Information Processing Systems, 37:7418274210, 2024. 3, 8, 10, 14, 16 Nicklas Hansen, Zhecheng Yuan, Yanjie Ze, Tongzhou Mu, Aravind Rajeswaran, Hao Su, Huazhe Xu, and Xiaolong Wang. On pre-training for visuo-motor control: Revisiting learning-from-scratch baseline. arXiv preprint arXiv:2212.05749, 2022."
        },
        {
            "title": "Exploring Conditions for Diffusion Models in Robotic Control",
            "content": "Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 97299738, 2020. 3 Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1600016009, 2022. 2, 3 Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 1, 3 Sunghwan Hong, Seokju Cho, Jisu Nam, Stephen Lin, and Seungryong Kim. Cost aggregation with 4d convolutional swin transformer for few-shot segmentation. In European Conference on Computer Vision, pp. 108126. Springer, 2022. 3 Sunghwan Hong, Jaewoo Jung, Heeseong Shin, Jisang Han, Jiaolong Yang, Chong Luo, and Seungryong Kim. Pf3plat: Pose-free feed-forward 3d gaussian splatting. arXiv preprint arXiv:2410.22128, 2024a. Sunghwan Hong, Jaewoo Jung, Heeseong Shin, Jiaolong Yang, Seungryong Kim, and Chong Luo. Unifying correspondence pose and nerf for generalized pose-free novel view synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2019620206, 2024b. 3 Susung Hong, Junyoung Seo, Heeseong Shin, Sunghwan Hong, and Seungryong Kim. Large language models are frame-level directors for zero-shot text-to-video generation. In First Workshop on Controllable Video Generation@ ICML24, 2024c. 2, 5 Apoorv Khandelwal, Luca Weihs, Roozbeh Mottaghi, and Aniruddha Kembhavi. Simple but effective: Clip embeddings for embodied ai. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1482914838, 2022. 15 Chaehyun Kim, Heeseong Shin, Eunbeen Hong, Heeji Yoon, Anurag Arnab, Paul Hongsuck Seo, Sunghwan Hong, and Seungryong Kim. Seg4diff: Unveiling open-vocabulary segmentation in text-to-image diffusion transformers. arXiv preprint arXiv:2509.18096, 2025a. 3 Junsu Kim, Yunhoe Ku, Dongyoon Han, and Seungryul Baek. Diffusion meets few-shot class incremental learning. arXiv preprint arXiv:2503.23402, 2025b. Taekyung Kim, Dongyoon Han, Byeongho Heo, Jeongeun Park, and Sangdoo Yun. Token bottleneck: One token to remember dynamics. arXiv preprint arXiv:2507.06543, 2025c. 2 Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. Advances in neural information processing systems, 34:2169621707, 2021. 3 Neehar Kondapaneni, Markus Marks, Manuel Knott, Rogerio Guimaraes, and Pietro Perona. Textimage alignment for diffusion-based perception. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1388313893, 2024. 1, 2, 3, 5, 8, 16 Xinghui Li, Jingyi Lu, Kai Han, and Victor Adrian Prisacariu. Sd4match: Learning to prompt stable diffusion model for semantic matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2755827568, 2024. 3, Grace Luo, Lisa Dunlap, Dong Huk Park, Aleksander Holynski, and Trevor Darrell. Diffusion hyperfeatures: Searching through time and space for semantic correspondence. Advances in Neural Information Processing Systems, 36:4750047510, 2023. 3 Yecheng Jason Ma, Shagun Sodhani, Dinesh Jayaraman, Osbert Bastani, Vikash Kumar, and Amy Zhang. Vip: Towards universal visual reward and representation via value-implicit pre-training. arXiv preprint arXiv:2210.00030, 2022. 3 Arjun Majumdar, Karmesh Yadav, Sergio Arnaud, Jason Ma, Claire Chen, Sneha Silwal, Aryan Jain, Vincent-Pierre Berges, Tingfan Wu, Jay Vakil, et al. Where are we in the search for an artificial visual cortex for embodied intelligence? Advances in Neural Information Processing Systems, 36: 655677, 2023. 1, 2, 3, 4, 7, 8, 14, 15, 16, 17, 18,"
        },
        {
            "title": "Exploring Conditions for Diffusion Models in Robotic Control",
            "content": "Benyuan Meng, Qianqian Xu, Zitai Wang, Xiaochun Cao, and Qingming Huang. Not all diffusion model activations have been evaluated as discriminative features. Advances in Neural Information Processing Systems, 37:5514155177, 2024. 4 Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav Gupta. R3m: universal visual representation for robot manipulation. arXiv preprint arXiv:2203.12601, 2022. 3 Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 8 Simone Parisi, Aravind Rajeswaran, Senthil Purushwalkam, and Abhinav Gupta. The unsurprising effectiveness of pre-trained vision models for control. In international conference on machine learning, pp. 1735917371. PMLR, 2022. 1, 2, 3, 4, 10 Suraj Patni, Aradhye Agarwal, and Chetan Arora. Ecodepth: Effective conditioning of diffusion models for monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2828528295, 2024. 3, Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PmLR, 2021. 2, 3, 8, 15 Ilija Radosavovic, Tete Xiao, Stephen James, Pieter Abbeel, Jitendra Malik, and Trevor Darrell. Realworld robot learning with masked visual pre-training. In Proceedings of The 6th Conference on Robot Learning, volume 205 of Proceedings of Machine Learning Research, pp. 416426. PMLR, 1418 Dec 2023. URL https://proceedings.mlr.press/v205/radosavovic23a. html. 3 Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel Todorov, and Sergey Levine. Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations. In Proceedings of Robotics: Science and Systems (RSS), 2018. 2, 8, 17 Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. 1, 2, 3, 6, 8 Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pp. 234241. Springer, 2015. 3 Heeseong Shin, Chaehyun Kim, Sunghwan Hong, Seokju Cho, Anurag Arnab, Paul Hongsuck Seo, and Seungryong Kim. Towards open-vocabulary semantic segmentation without semantic labels. Advances in Neural Information Processing Systems, 37:91539177, 2024. Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Cliport: What and where pathways for robotic manipulation. In Conference on robot learning, pp. 894906. PMLR, 2022. 15 Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pp. 22562265. pmlr, 2015. 3 Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath Hariharan. Emergent correspondence from image diffusion. Advances in Neural Information Processing Systems, 36: 13631389, 2023. 3 Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018. 2, 7, 9, 14, 17,"
        },
        {
            "title": "Exploring Conditions for Diffusion Models in Robotic Control",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 3 Zhicong Wu, Hongbin Xu, Gang Xu, Ping Nie, Zhixin Yan, Jinkai Zheng, Liangqiong Qu, Ming Li, and Liqiang Nie. Textsplat: Text-guided semantic fusion for generalizable gaussian splatting. arXiv preprint arXiv:2504.09588, 2025. 1, 3 Weilai Xiang, Hongyu Yang, Di Huang, and Yunhong Wang. Denoising diffusion autoencoders are unified self-supervised learners. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1580215812, 2023. 3 Karmesh Yadav, Arjun Majumdar, Ram Ramrakhya, Naoki Yokoyama, Alexei Baevski, Zsolt Kira, Oleksandr Maksymets, and Dhruv Batra. Ovrl-v2: simple state-of-art baseline for imagenav and objectnav. arXiv preprint arXiv:2303.07798, 2023. 8, 16 Wen Yin, Yong Wang, Guiduo Duan, Dongyang Zhang, Xin Hu, Yuan-Fang Li, and Tao He. Knowledge-aligned counterfactual-enhancement diffusion perception for unsupervised crossdomain visual emotion recognition. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 38883898, 2025. 1 Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on Robot Learning, 2020. 2, 7, 17, Junyi Zhang, Charles Herrmann, Junhwa Hur, Luisa Polania Cabrera, Varun Jampani, Deqing Sun, and Ming-Hsuan Yang. tale of two features: Stable diffusion complements dino for zero-shot semantic correspondence. Advances in Neural Information Processing Systems, 36:4553345547, 2023. 3 Wenliang Zhao, Yongming Rao, Zuyan Liu, Benlin Liu, Jie Zhou, and Jiwen Lu. Unleashing textto-image diffusion models for visual perception. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 57295739, 2023. 1, 2, 3, 5 Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for visionlanguage models. International Journal of Computer Vision, 130(9):23372348, 2022. 2, 6, 8, 15,"
        },
        {
            "title": "APPENDIX",
            "content": "A: Further analysis A.1: Ablation on timesteps A.2: Efficiency comparison A.3: Analysis on the null condition B: Further implementation details B.1: Full description of the text conditions B.2: Details of the baselines B.3: Implementation details of the compression layer C: Qualitative results on robotic control tasks"
        },
        {
            "title": "A FURTHER ANALYSIS",
            "content": "A.1 ABLATION ON DIFFUSION TIMESTEPS Table 6: Ablation study on timestep selection. To ablate the choice for timestep t, we provide results with = 100 and = 200. The performance of imitation learning agents on DeepMind Control (Tassa et al., 2018) is reported. We report the normalized score averaged over three seeds with its standard deviation. DeepMind Control Timestep Walker-stand Walker-walk Reacher-easy Cheetah-run Finger-spin Mean 200 100 92.2 1.6 88.3 4.7 78.6 2.2 72.6 4.3 85.4 8.3 79.4 6.8 24.7 4.5 36.1 6.2 66.5 3.2 66.2 3. 0 (Default) 88.8 2.3 76.9 4.0 71.9 5.6 48.2 11.3 68.0 1. 69.4 68.5 70.8 To ablate the effects of the diffusion timestep t, in Table 6, we additionally provide results with = 100 and = 200. Although some tasks (e.g. Reacher-easy) benefit from = 100 or = 200, performance on other tasks such as Cheetah-run degrades significantly, lowering the overall score. Therefore, we choose = 0, which achieves the best overall performance. A.2 EFFICIENCY COMPARISON Table 7: Efficiency comparison. We report the total number of parameters (#Params), the number of learnable parameters (#Learnable) and latency for VC-1, SCR, and ours. In Table 7, we report the number of parameters, number of learnable parameters, and latency for each modules for VC-1 (Majumdar et al., 2023), SCR (Gupta et al., 2024) and our proposed method. For VC-1 and SCR, we use ViT-L/16, which was also used for comparison in the main paper. Notably, the layer selection allows us to drop the up blocks, which removes around 500M parameters from the denoising U-Net. This allows the U-Net to have similar parameter count to VC-1 encoder, which is used in various robotic manipulation tasks. Furthermore, most of the parameters added to our method are the frozen parameters from DINOv2, and the learnable parameters consist mostly of the additional projection layers for the visual prompts. 303.3M 382.9M 480.1M 0 0 10.6M VC-1 SCR Ours 11ms 26ms 48ms #Learnable #Params Method Time"
        },
        {
            "title": "Exploring Conditions for Diffusion Models in Robotic Control",
            "content": "A.3 ANALYSIS ON THE NULL CONDITION Figure 7: Visualization of normalized attention weights and raw attention scores for <bos> and <eos> tokens. We compare the visualization of the normalized attention weights obtained after the softmax operation and the raw attention scores obtained before the softmax operation from the cross-attention layers to further analyze the properties of <bos> and <eos> tokens. Figure 7 illustrates the attention behavior of <bos> and <eos> tokens by visualizing their normalized cross-attention maps (b,d) and raw attention scores (c,e). We observe that the <bos> token consistently attends to background regions, whereas the <eos> is less reliable at focusing on salient objects (e.g. the robot hand in Relocate). We attribute the background affinity of <bos> to the typical structure of text prompts, which primarily describe foreground subjects. Moreover, since Stable Diffusion employs causal text encoder for both conditional prompts and the null condition in unconditional generation, this background-attending behavior is also transferred to unconditional scenarios."
        },
        {
            "title": "B FURTHER IMPLEMENTATION DETAILS",
            "content": "B.1 FULL DESCRIPTION OF THE TEXT CONDITIONS In Table 8, we provide the full descriptions used for Text (Simple) and Text (Caption), which are generated by Gemini 2.5 Pro (Comanici et al., 2025). For CoOp (Zhou et al., 2022), we use 4 learnable prefix tokens, such as [V ][V ][V ][V ] bin picking for Bin-picking. For TADP, we add style prefix in [S] style, which results in The Sawyer robot arm must carefully pick specific target object out of the cluttered red bin and place it into the empty blue bin in [S] style. for Bin-picking. B.2 DETAILS OF THE BASELINES CLIP (Radford et al., 2021) is vision-language model pre-trained on large-scale image-text pairs through contrastive learning. CLIP has been widely used in various tasks, including navigation and manipulation tasks (Shridhar et al., 2022; Khandelwal et al., 2022). VC-1 (Majumdar et al., 2023) is foundation model for various robotics tasks, spanning from manipulation to locomotion and navigation tasks. VC-1 trains with MAE objective on egocentric videos, as well as additional data including navigation and manipulation datasets."
        },
        {
            "title": "Exploring Conditions for Diffusion Models in Robotic Control",
            "content": "Table 8: Full text descriptions used in baselines. Task Method Text Assembly Bin Button Drawer Hammer Pen Relocate Cheetah-run Walker-walk Walker-stand Finger-spin Reacher Text (Simple) Text (Caption) Text (Simple) Text (Caption) Text (Simple) Text (Caption) Text (Simple) Text (Caption) Text (Simple) Text (Caption) Text (Simple) Text (Caption) Text (Simple) Text (Caption) Text (Simple) Text (Caption) Text (Simple) Text (Caption) Text (Simple) Text (Caption) Text (Simple) Text (Caption) Text (Simple) Text (Caption) assembly The Sawyer robot arm must pick up the green block and precisely insert it into the center of the silver ring to complete the assembly. bin picking The Sawyer robot arm must carefully pick specific target object out of the cluttered red bin and place it into the empty blue bin. button press The Sawyer robot arm must reach out and accurately press the red button on top of the yellow control box. drawer open The Sawyer robot arm must grasp the white handle and pull open the light green drawer. hammer The Sawyer robot arm must pick up the red hammer and use it to strike the nail, driving it completely into the wooden block. pen dexterous robotic hand must twirl blue pen within its grasp to match the final orientation shown by the green target pen. relocate dexterous robotic hand is tasked with picking up the small blue ball and moving it to the location of the green target sphere. cheetah run minimalist orange robot, shaped like cheetah, runs across reflective floor in simulated environment. walker walk minimalist, orange bipedal robot takes step across reflective floor in simulated environment. walker stand minimalist, orange bipedal robot stands upright on reflective floor in simulated environment. finger spin simple robotic finger strikes floating, hot dog-shaped object to make it spin against starry background. reacher simple robotic arm reaches for red target ball on checkered blue surface. SCR (Gupta et al., 2024) employs Stable Diffusion for various navigation and manipulation tasks. We consider SCR as baseline using the null condition , which is implemented as an empty string. Text(Simple/Caption) is task-adaptive baseline using text conditions, where Text (Simple) directly uses the task names as the condition, whereas Text (Caption) leverages descriptions generated from Gemini 2.5 (Comanici et al., 2025). Full text used for each tasks are presented in the appendix. CoOp (Zhou et al., 2022) extends on Textsimple by implementing learnable prefix tokens . CoOp originally prompts CLIP with the format [V ] classname for image classification, which in our case, the task names used in Textsimple are used as classnames. TADP (Kondapaneni et al., 2024) extends on Textcaption, by adding special token that encapsulates the visual style information optimized through Textual Inversion (Gal et al., 2022). Since the visual information is optimized into single token S, we can consider TADP as baseline with global visual information, and not in frame-wise manner. B.3 IMPLEMENTATION DETAILS OF THE COMPRESSION LAYER Algorithm 1: PyTorch-style pseudocode for the compression layer class CompressionLayer(nn.Module): def init (self, hidden dim, compress dim): self.layers = nn.Sequential( nn.Conv2d(hidden dim, compress dim, kernel size=3, padding=1), nn.BatchNorm2d(hidden dim), nn.ReLU(inplace=True), nn.Flatten() ) def forward(self, x): return self.layers(x) To provide further details of the compression layer (Yadav et al., 2023), we provide PyTorch-style pseudo-code of the compression layer in Alg. 1. We follow previous works (Yadav et al., 2023; Gupta et al., 2024) for implementing simple convolutional layer for the compression layer to obtain 1D state representations from 2D features. For all methods, compress dim was set to 48. Note that the compression layer was also used for compared baselines including CLIP (Cherti et al., 2023) and VC-1 (Majumdar et al., 2023), which have been shown to yield better performance than using <CLS> tokens (Gupta et al., 2024)."
        },
        {
            "title": "C QUALITATIVE VISUALIZATION ON ROBOTIC CONTROL TASKS",
            "content": "We provide frame-wise comparison of our method, CLIP (Cherti et al., 2023), and VC-1 (Majumdar et al., 2023) for tasks from DMC (Tassa et al., 2018) in Fig. 9, MetaWorld (Yu et al., 2020) in Fig. 10, and Adroit (Rajeswaran et al., 2018) in Fig. 8. For each task, we report the normalized score for DMC and whether the task has succeeded or failed for MetaWorld and Adroit. Figure 8: Visualization of agents performing downstream tasks in Adroit (Rajeswaran et al., 2018). We provide visual comparison of our method to CLIP (Cherti et al., 2023), and VC-1 (Majumdar et al., 2023) for two tasks from Adroit. We additionally report whether the task has succeeded or failed for each episode."
        },
        {
            "title": "Exploring Conditions for Diffusion Models in Robotic Control",
            "content": "Figure 9: Visualization of agents performing downstream tasks in DMC (Tassa et al., 2018). We provide visual comparison of our method to CLIP (Cherti et al., 2023), and VC-1 (Majumdar et al., 2023) for five tasks in DMC. We additionally report the normalized score for each episode."
        },
        {
            "title": "Exploring Conditions for Diffusion Models in Robotic Control",
            "content": "Figure 10: Visualization of agents performing downstream tasks in MetaWorld (Yu et al., 2020). We provide visual comparison of our method to CLIP (Cherti et al., 2023), and VC-1 (Majumdar et al., 2023) for five tasks in MetaWorld. We additionally report whether the task has succeeded or failed for each episode."
        }
    ],
    "affiliations": [
        "KAIST AI",
        "NAVER AI Lab"
    ]
}