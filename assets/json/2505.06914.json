{
    "paper_title": "The Distracting Effect: Understanding Irrelevant Passages in RAG",
    "authors": [
        "Chen Amiraz",
        "Florin Cuconasu",
        "Simone Filice",
        "Zohar Karnin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "A well-known issue with Retrieval Augmented Generation (RAG) is that retrieved passages that are irrelevant to the query sometimes distract the answer-generating LLM, causing it to provide an incorrect response. In this paper, we shed light on this core issue and formulate the distracting effect of a passage w.r.t. a query (and an LLM). We provide a quantifiable measure of the distracting effect of a passage and demonstrate its robustness across LLMs. Our research introduces novel methods for identifying and using hard distracting passages to improve RAG systems. By fine-tuning LLMs with these carefully selected distracting passages, we achieve up to a 7.5% increase in answering accuracy compared to counterparts fine-tuned on conventional RAG datasets. Our contribution is two-fold: first, we move beyond the simple binary classification of irrelevant passages as either completely unrelated vs. distracting, and second, we develop and analyze multiple methods for finding hard distracting passages. To our knowledge, no other research has provided such a comprehensive framework for identifying and utilizing hard distracting passages."
        },
        {
            "title": "Start",
            "content": "The Distracting Effect: Understanding Irrelevant Passages in RAG Chen Amiraz1*, Florin Cuconasu1,2, Simone Filice1, Zohar Karnin1 1Technology Innovation Institute, 2Sapienza University of Rome 5 2 0 2 1 1 ] . [ 1 4 1 9 6 0 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "A well-known issue with Retrieval Augmented Generation (RAG) is that retrieved passages that are irrelevant to the query sometimes distract the answer-generating LLM, causing it to provide an incorrect response. In this paper, we shed light on this core issue and formulate the distracting effect of passage w.r.t. query (and an LLM). We provide quantifiable measure of the distracting effect of passage and demonstrate its robustness across LLMs. Our research introduces novel methods for identifying and using hard distracting passages to improve RAG systems. By fine-tuning LLMs with these carefully selected distracting passages, we achieve up to 7.5% increase in answering accuracy compared to counterparts fine-tuned on conventional RAG datasets. Our contribution is two-fold: first, we move beyond the simple binary classification of irrelevant passages as either completely unrelated vs. distracting, and second, we develop and analyze multiple methods for finding hard distracting passages. To our knowledge, no other research has provided such comprehensive framework for identifying and utilizing hard distracting passages."
        },
        {
            "title": "Introduction",
            "content": "Retrieval Augmented Generation (RAG) is key method to enable Large Language Models (LLMs) to solve knowledge-intensive tasks such as question-answering (Chen et al., 2017; Petroni et al., 2021). Adding retrieved passages to the prompt of an LLM is shown to ground the LLM response, significantly reducing hallucinations (Fan et al., 2024). Despite its advantages, retrieved content can sometimes lead to problematic behavior. Retrieval *chen.amiraz@tii.ae Work conducted while FC being research intern at TII. cuconasu@diag.uniroma1.it filice.simone@gmail.com zohar.karnin@tii.ae is not always successful, and in many such cases, the prompt includes distracting passages (Li et al., 2023; Yoran et al., 2024). As described by Cuconasu et al. (2024), distracting passages contain irrelevant yet semantically related information that may mislead the LLM and thus hurt answer generation. Various solutions were proposed to handle such problematic retrieval results: based on Chain-of-Thought (Yu et al., 2023; Wei et al., 2024), via LLM fine-tuning (Yoran et al., 2024; Jin et al., 2024), and via dedicated inference procedures (Asai et al., 2024). This line of research raises key question: how to evaluate the distracting effect of passage on an LLM with respect to query? We begin tackling this question by defining quantifiable measure of passages distracting effect with respect to query and an LLM. Our definition isolates the effect of the passage itself, which enables to decouple the influence of other passages. The distracting effect is inherently LLMdependent, as different models may be affected by different passages. Despite this potential difference, we show that the distracting effect property is in fact quite robust to the LLM choice in that the scores have high correlations across LLMs. We further validate the robustness of our measure by showing that it translates to downstream RAG quality, specifically by demonstrating that the higher passages distracting effect, the more it reduces accuracy when included in the prompt alongside the gold passage. With the measure in hand, we move to study the distracting effect of retrieved passages. We corroborate results from previous studies (Jin et al., 2024) showing that the irrelevant results obtained from stronger retrievers are more distracting when compared to weak retrievers. This phenomenon provides additional motivation to our study since retrievers will grow stronger with time, resulting in passages with larger distracting effect. Our analysis reveals an additional, related observation: higher-ranking irrelevant results are more likely to be distracting. To allow for test sets that reflect unseen data and/or training sets allowing for generalizable models, we aim to obtain distracting passages to all queries, including those where standard retrieval fails. Such failures occur either when the retriever does not return distracting passages or when no such passages exist in the corpus (e.g., in niche topics or small corpora). To address the first case, we define skewed retriever tuned to provide passages related to the query but unrelated to its answer. For the second, we define several categories of distracting passages, inspired by Basmov et al. (2024); Abdumalikov et al. (2024), and generate passages for each category using strong LLM. We demonstrate the effectiveness of this diverse collection of methods by analyzing their ability to jointly provide highly distracting passages to queries in public question-answering benchmarks. We show that for non-negligible fraction of queries, the joint collection of methods allows for much more distracting passages compared to any single method, in particular that of standard retrieval. We finish by demonstrating the usefulness of our techniques for collecting distracting passages; using these passages we build training set used to fine-tune an LLM on question-answering task. We observe that fine-tuned LLM based on our training set achieves superior results to one finetuned on an analogous training set obtained via standard retrieval. Concluding, our contributions are as follows: (1) We formalize core issue in RAG, that of distracting passages, providing formal definition and evaluation method for such passages (2) We present diverse techniques to obtain such distracting passages, (3) We demonstrate the value of distracting passages by building an effective RAG training set."
        },
        {
            "title": "2 Related work",
            "content": "Analysis of Irrelevant Passages. One line of research in RAG focuses on analyzing different types of irrelevant passages. passage is considered relevant if it contains the correct answer (or part of it) and provides useful context for answering the query. Cuconasu et al. (2024) classify irrelevant passages as either random or distracting, showing that while random passages do not degrade answer quality, distracting passages do. We adopt the term distracting but extend it beyond classification by treating distraction as continuous property and providing concrete methods to quantify it. Basmov et al. (2024) analyze the LLMs ability to answer questions when the provided reference passages contain hypothetical statements or statements that contradict its parametric knowledge. They show that in both cases performance can significantly drop. We make use of this categorization (among others) of distracting passages in order to synthetically generate diverse types of distracting passages. Jin et al. (2024) show that irrelevant passages returned by strong retrievers are more distracting than those obtained by weak retrievers by showing that RAG systems tend to make more mistakes when given the former rather than the latter. In our analysis, we provide additional ways to measure how distracting passage is, and corroborate this conclusion. Obtaining Distracting Passages. To our knowledge, in the context of answer generation, the existing solutions to obtain distracting passages are all based on retrieval. The dominant technique is by obtaining top-ranked passages that are not the ground-truth passage, e.g., (Yoran et al., 2024)1. Abdumalikov et al. (2024) went beyond standard retrieval by generating synthetic passages either containing the question but not the answer, or the answer and not the question. They do so in an effort to teach an LLM to abstain when needed. In the context of retrieval and reranking, there is rich line of work exploring methods for obtaining hard negatives, i.e., passages that are irrelevant to the query but seem relevant to the retrieval or reranking system. For the retrieval problem, the dominating method is that of contrastive learning (Xiong et al., 2021), in which the hard negatives are implicitly found by the training method, but this technique is only possible when the pairwise similarity is simple function (e.g., inner product) and is inapplicable for cross-encoders typically used for reranking. Here, the methods are variations of taking the top results from an existing retriever or reranker that are not the ground truth passage. Other than the difference in their definition (hard negatives are defined w.r.t. the ranker, not the answer generator), another key distinction is how to 1Their precise method is in fact not the top passages, but rather uniform random set of passages out of the top > k. deal with false negatives, meaning passages that are not labeled as relevant but are in fact relevant. Moreira et al. (2024) discuss such methods that discard negative candidates whose score is larger than some threshold, either fixed or based on the score of known positive example. In our setting, this is less of problem given that we have groundtruth answer. This additional information allows more accurate filtering of false negatives. Due to this, we focus on additional methods of providing candidate distracting examples rather than ways to filter false negatives. Another notable recent work in the area of retrieval is by Weller et al. (2024) that train promptable retriever that can retrieve passages relevant to query and an instruction. Here, the authors synthesize hard negative passages that match the query but do not match the instruction. Our setting is fundamentally different in the definition of negative example, and due to this we use completely different methods to generate such examples. Robust Answer Generation. Closely related to the above, another relevant area is that of building answer generation methods that can handle irrelevant and distracting passages. One approach is to have chain-of-thought process, either via prompt or fine-tuning in which the LLM identifies the relevant passages (Yu et al., 2023; Yan et al., 2024; Luo et al., 2023; Wei et al., 2024). similar approach is taken by Asai et al. (2024). They provide an entire RAG system, but one of its components indirectly decides whether passage is relevant by generating an answer with it and measuring its faithfulness to the passage. Another approach is to fine-tune LLMs to answer questions when coupled with both relevant and irrelevant passages. Lin et al. (2024) and Jin et al. (2024) do this with passages obtained from standard retrieval system. Yoran et al. (2024) do the same, but add examples where the passages are intentionally irrelevant, specifically they are sampled from the top results rather than taking the top results."
        },
        {
            "title": "3 Distracting Passages",
            "content": "An informal definition of the distracting effect of passage w.r.t query and LLM is: given query and passage that is irrelevant to q, how likely is an LLM to be distracted by the passage? In this section, we provide concrete measure for the distracting effect of passage, then move to describe different methods to obtain distracting passages."
        },
        {
            "title": "3.1 Measuring the Distracting Effect",
            "content": "For the formal test, we build prompt from and where we ask the LLM to answer based on the passage and abstain (output NO-RESPONSE) if the passage does not contain an answer to q. The precise prompt is given in Figure 6 and all the implementation details are described in Appendix A. We compute distracting effect DEq(p) of an irrelevant passage for query as the probability of the LLM not abstaining: DEq(p) = 1 pLLM(NO-RESPONSEq, p) (1) Alternatives to this test could be comparing the answer of the LLM with vs. without the passage, or building prompt that also includes relevant passage and checking if changes the response. While these approaches are viable, our DEq(p) score offers several key advantages: (1) as probability measure bounded between 0 and 1, it provides an easily interpretable metric of distraction, (2) since this score leverages the LLMs intrinsic ability to recognize relevant information, it can be applied beyond question-answering to any task where distinguishing between relevant and distracting information is crucial, (3) it applies to the LLM being tested, without relying on an expensive reference model, (4) it does not require additional passages nor assumptions about the LLMs parametric memory, and (5) it has relatively cheap implementation cost, as it simply requires the LLM to process the prompt without generating any new tokens. In our analysis, we interpret DEq(p) as relative ranking score for passages associated with the same query. This comparative usage helps mitigate concerns that an LLM might ignore the instruction and rely instead on its parametric knowledge to answer the question. Even if such behavior might occur to some extent, it would affect all passages similarly for given query, preserving the validity of DEq(p) as measure of their relative distracting effects."
        },
        {
            "title": "3.2 Obtaining Distracting Passages",
            "content": "Here, we have two approaches. The first is to retrieve candidates, and the second is to generate them using an LLM. The former method will provide examples closer to those observed at inference time. However, the synthetic examples have the potential to add robustness to the system for rarely observed types of distracting passages. Additionally, for small corpora, distracting examples may be impossible to achieve for many queries, e.g., when they discuss topic present in single document. Here, the synthetic examples are key for robust system. key challenge in learning-to-rank settings is distinguishing truly irrelevant passages from false negatives, i.e., passages that are mistakenly treated as irrelevant but actually contain useful information. This issue is often addressed by excluding top-ranked candidates or those with high relevance scores. In our case, however, we aim to ensure that the passages we obtain (by all methods) are indeed irrelevant, which is more straightforward thanks to access to the ground truth answer. Specifically, we use the NLI model of Honovich et al. (2022) in the following manner. passage is considered relevant to query if it explicitly includes the ground truth answer or entails the hypothesis the answer to {question} is {answer} given the passage as the premise. We exclude such passages when computing distracting effect scores."
        },
        {
            "title": "3.2.1 Retrieving Distracting Passages\nThe idea here follows the intuition that irrelevant\npassages ranked in a top position by a retrieval sys-\ntem are likely to have a large distracting effect. For\ndifferent retrieval systems, we obtain the resulting\npassages and exclude the relevant ones to obtain ei-\nther a single (the remaining top-ranked) or a ranked\nlist of candidate passages.",
            "content": "In addition to standard retrieval, we consider modified version of (dense) retrieval that we call answer-skewed retrieval. dense retriever is defined via embedding functions EQ, ED mapping query/document into an embedding space. While keeping the document embedding the same, we modify the query embedding as follows: for query coupled with ground-truth answer we define"
        },
        {
            "title": "3.2.2 Generating Distracting Passages",
            "content": "Here, we use categorization of different types of distracting passages inspired by Basmov et al. (2024); Abdumalikov et al. (2024). For each type, we employ few-shot learning, i.e., build prompt containing handful of query and distracting passage pairs (see Appendix C). We then use strong LLM to generate passage of the corresponding distracting category. The categories used are: Related Topic passage discussing topic highly related to the question, but that does not contain the answer. E.g., for When was Abraham Lincoln born?, Robert Todd Lincoln, the eldest son of President Abraham Lincoln, was born August 1, 1843.. The generator of this type of distracting documents is referred to as Grel. Hypothetical passage discussing the question in hypothetical situation in which the answer is different. E.g., for What is traditional gift for 5th anniversary? In ancient Roman times, couples would go on week long hunting trip on their 5th anniversary. The generator of this type of distracting documents is referred to as Ghypo. Negation passage providing wrong answer, but in negation. E.g., It is common misconception that students do not pay tax on earnings. The generator of this type of distracting documents is referred to as Gneg. Modal Statement passage providing wrong answer following disclaimer that the statement is not certain. E.g., The Pyramids may have been built via employing sloping and encircling embankment of brick, earth, and sand.. The generator of this type of distracting documents is referred to as Gmodal. Esub(q, a) = EQ(q) λED(a) (2)"
        },
        {
            "title": "4 Analyzing the Distracting Effect",
            "content": "and Eproj(q, a) = EQ(q) λ EQ(q), ED(a) ED(a) ED(a)2 (3) The former subtracts the answer embedding from the original query embedding, and the latter projects it. These formulas are the arithmetic way to express the idea of retrieving document that is related to the query but unrelated to the answer. The hyper-parameter λ determines how aggressively we wish to exclude documents related to the answer. In this section, we analyze the different techniques for obtaining distracting candidates discussed in Section 3, and we show the benefit of jointly using different methodologies to create sets of highly distracting passages. Furthermore, we show how highly distracting passages can affect the LLM response quality even when relevant document is present in the prompt."
        },
        {
            "title": "4.1 Experimental Setting",
            "content": "Benchmarks. We make use of the following commonly used public question-answering benchFigure 1: Distribution of distracting effect for passages obtained through different methods, as measured by Llama3.1-8B. Methods are ordered by their mean distracting effect (shown by vertical black lines), with higher means indicating greater ability to distract the model. marks: NQ (Kwiatkowski et al., 2019), PopQA (Mallen et al., 2023), TriviaQA (Joshi et al., 2017) and WebQA (Berant et al., 2013). We took sample of 2000 queries from NQ and 1000 from the rest. We filtered out instances without relevant passages among the retrieved ones, to enable tests related to ground-truth passages. This resulted in 1926, 950, 987, and 837 queries for the respective datasets. These benchmarks all come with reference answer, which we use to assess the correctness of the generated answers. In particular, we adopt common variant of the Exact Match metric, where we classify generated answer as correct if it includes the ground-truth answer as substring (e.g., if the answer is Washington and the generated answer is George Washington, it is considered correct). This procedure will be generally referred to as answer accuracy in our experiments. Compared Methodologies. We compare the distracting effects of the passages obtained by using the methodologies discussed in Section 3. For the retrieval-based methods, we index the Wikipedia dump of 20 December 2018 (Gao et al., 2023) using Pinecone vector-DB2 and the E5-base embedding model (Wang et al., 2022) with 768 embedding dimension. We also explored the answer-skewed retriever3 from Section 3.2.1. Hereafter, we use Rst to refer to standard dense retriever and Rsk to refer to its answer-skewed counterpart. We evaluate 2https://www.pinecone.io/ 3Among tested configurations, formulation 2 with λ = 1 performed best. See Section A.2 for further details. both solutions with and without re-ranking their top-20 passages using the cross-encoder BAAI BGE-M3-v24 (Chen et al., 2024). We use Rst + and Rsk + to refer to the retrieval models followed by the re-ranking module. Regarding the four generation methods discussed in Section 3.2.2, we use Claude 3.5 Sonnet V2.0 via AWS Bedrock as the backbone LLM. Evaluated LLMs. To assess the distracting effect of candidate passage we use formula 1 with various LLMs. We consider open-sourced LLMs ranging from 3B to 70B parameters, specifically the instruct-based version of Llama-3.2-3B, Llama3.1-8B, Llama-3.3-70B (Grattafiori et al., 2024), Falcon-3-3B, Falcon-3-7B (Team, 2024), Qwen2.5-3B, and Qwen-2.5-7B (Yang et al., 2025)."
        },
        {
            "title": "4.2 Distracting Effect of Retrieved Passages",
            "content": "In this experiment, we compute the average distracting effect of the irrelevant passages retrieved using the various retrieval methods. Figure 2 shows how the distracting effect (averaged across the four datasets we consider) varies at different ranking positions. All methods exhibit the same decreasing trend. notable conclusion is that standard retrieval pipelines, while attempting to bring relevant passages to top positions, tend to favor passages with high distracting effects over passages with low distracting effects. Another important observation arises when an4https://huggingface.co/BAAI/ bge-reranker-v2-m3 Figure 2: Average distracting effect at different rank positions for various retrieval methods. Results are shown for Llama-3.1-8B, averaged across datasets. Higherranked passages consistently demonstrate greater potential to mislead the model. Similar trends were observed across all tested LLMs (see Figures 20 and 21). alyzing the effect of the reranking on the top positions (e.g., top 5). In these positions, using the reranking module consistently increases the average distracting effect. We argue that the irrelevant passages that are retrieved are the ones that fool the retrieval pipeline, and that also have the potential of distracting the LLM. While adding reranking module enhances the capabilities of the retrieval pipeline, this actually amplifies the problem passages that successfully pass through this additional reranking stage are even more likely to mislead the LLM during response generation."
        },
        {
            "title": "Different Methodologies",
            "content": "In these experiments, we compare all the methods discussed in Section 3. For retrieval-based methods, we consider only the first non-relevant passage for each query, which is expected to be the most distracting retrieved one according to Figure 2. Similarly, for fair comparison, for each query, we use each of the four methods discussed in Section 3.2.2 to generate single distracting passage. Figure 1 reports the probability distribution of the distracting effect over Llama-3.1-8B of the passages obtained by using the different methods, computed on NQ and WebQuestions. Results on TriviaQA and PopQA follow the same trend and are described in Appendix A.1. The probability distributions are all skewed towards extreme values showing the LLM tendency to always respond with high confidence, even when wrong. The probability distributions of the other LLMs, described in Appendix A.1, follow similar trends. An important aspect to notice is that the relative distracting effect provided by the different methods is quite Figure 3: Percentage of queries where each method provides the most distracting passage for Llama-3.1-8B. In blue are the times when no other method reaches the same distracting effect, in orange the percentage of times the highest score is shared with other methods. Similar trends were observed across all tested LLMs (see Figures 18 and 19). stable: Rst + and Rst are among the top-distracting approaches in all datasets, and similarly, Grel and Rsk are among the poorest-performing methods across all datasets. This suggests that the inherent strengths and weaknesses of these methodologies transcend the specific characteristics of individual datasets. Regarding the retrieval-based methods, the results are in line with Section 4.2, with Rst consistently providing passages having higher distracting effect than Rsk; in both cases, reranking leads to more distracting passages. Among the generation-based methods, Gmodal appears on average the most promising to produce distracting passages; on the opposite, the passages generated by Grel are the least distracting ones. Finally, the last probability distribution in each subfigure of Figure 1 refers to the case where for each query we systematically select the most distracting passage among the ones obtained with the various methods. In this case, the probability is mostly distributed on high distracting effect values, demonstrating that the joint usage of different methodologies leads to significantly more distracting passages than the ones obtained by any of the individual methods. Figure 3 allows to better understand the contribution of each method. The vertical bars represent the percentage of queries (from the four datasets we analyze) where each method provides the most distracting passage. In blue are the times when no other method reaches the same distracting effect, in orange the percentage of times the highest score is shared with other methods (a difference below 0.01 is considered tie)."
        },
        {
            "title": "LLM",
            "content": "Only Gold Gold + WD Gold + HD Llama-3.2-3B Llama-3.1-8B Llama-3.3-70B Falcon-3-3B Falcon-3-7B Qwen-2.5-3B Qwen-2.5-7B 82.6 80.6 81.1 78.5 84.1 80.9 82.4 79.4 80.1 80.1 74.1 81.5 75.5 80.4 71.5 73.9 75.2 67.1 73.3 69.4 73.7 Table 1: Answer accuracy when prompting the LLM with the gold passage only, gold passage with weak distracting passage (WD), and gold passage with hard distracting passage (HD). Values that are NOT underlined are different in statistically significant way w.r.t. the gold-only case (Wilcoxon test with p-value < 0.01). In line with the probability distributions observed in Figure 1, Rst + is the method providing the most distracting passage for the highest number of queries. Nevertheless, for 48% of the queries, other methods produce more distracting passages. Overall, all methods provide their unique contribution, which is particularly remarkable not only for + but also for Gmodal, Gneg, and Ghypo, demonRst strating that combining retrieval and generationbased solutions is beneficial to obtain highly distracting passages for set of queries. The experiments reported so far study the distracting effect on Llama-3.1-8B, however, we observe very similar trends with other LLMs (details in Appendix A.1): some LLMs are more distractable than others, but overall the relative effectiveness of the various methods is very similar. Figure 4 provides deeper insights into how the distracting effect depends on the LLM used to compute it. We observe very high Spearman correlation scores between the distracting effects computed using different LLMs; this means that the LLMs we analyze share the same weaknesses and tend to be more distracted by the same set of passages. We argue that the distracting effect of passage is an intrinsic characteristic of the passage itself and that it does not depend much on the LLM."
        },
        {
            "title": "Passages",
            "content": "So far, we measured the distracting effect of an irrelevant passage as the probability of the LLM not abstaining when prompted with it. question we should still answer is: how do distracting passages affect the LLM accuracy when added to prompt already containing relevant passage? To answer this question we categorize irrelevant Figure 4: Spearman correlation of distracting effect computed using different LLMs (abbreviated, e.g., Llama L). The strong correlations suggest that the distracting effect of passage is relatively consistent across models despite architectural differences. passages as hard distracting if associated with distracting effect higher than 0.8. Similarly, we consider weak distracting those passages having distracting effect lower than 0.2. An example of hard and weak distracting passages is shown in Figure 5, with additional examples obtained through different methods in the Appendix (Figures 8, 9, and 10). As evident from the probability distributions in Figure 1, these two intervals account for most of the probability mass (e.g., 72% for Llama-3.1-8B). Table 1 shows how accuracy on the NQ dataset drops when the relevant passage (i.e., we use the gold passage available in the NQ dataset) is combined with distracting one in the LLM prompt. Since LLMs are known to be affected by positional bias (Liu et al., 2023), we compute both orders (i.e., gold followed by distracting, and distracting followed by gold) and report the average accuracy. Both weak and hard distracting passages affect accuracy when compared to having only gold document in the prompt, however, the impact of hard distracting passages is significantly larger, making the accuracy drop from 6 to 11 accuracy points, depending on the LLM. Notably, this degradation persists even for larger LLMs, like Llama-3.3-70B. This analysis confirms the reliability of the strategy we adopt for scoring distracting passages. Question: What movie has the song on the road again? Gold Answer: Honeysuckle Rose Relevant Passage (Title: On the Road Again (Willie Nelson song)) The song , about life on tour , came about when the executive producer of the film Honeysuckle Rose approached Nelson about writing the song for the film 's soundtrack . '' On the Road Again '' became Nelson 's 9th Country & Western No. 1 hit overall ( 6th as solo recording act ) in November 1980 , and became one of Nelson 's most recognizable tunes . In addition , the song reached No. 20 on the Billboard Hot 100 , and No. 7 on the Adult Contemporary chart . It was his biggest pop hit to that time and won him Grammy Award for Best Country Song year later . Hard Distracting Gmodal Distracting Effect: - Llama-3.2-3B: 0.9680 - Llama-3.1-8B: 0.3125 - Llama-3.3-70B: 0.0430 1.0000 - Falcon-3-3B: 0.3789 - Falcon-3-7B: 1.0000 - Qwen-2.5-3B: 0.4375 - Qwen-2.5-7B: Weak Distracting Grel Distracting Effect: - Llama-3.2-3B: 0.0234 - Llama-3.1-8B: 0.0312 - Llama-3.3-70B: 0.0000 0.3371 - Falcon-3-3B: 0.0117 - Falcon-3-7B: 0.0000 - Qwen-2.5-3B: 0.0000 - Qwen-2.5-7B: (Title: Classic Songs in Films) Many people believe, though it's not actually correct, that Willie Nelson's iconic song 'On The Road Again' first appeared in the 1980 film 'Smokey and the Bandit II.' Some music historians have suggested that this misconception arose because the film's themes of truck driving and life on the road seemed to perfectly match the song's message. The song's road-trip vibe made it natural fit for many movies, but this particular connection is just popular misconception. (Title: Willie Nelson Hits) Country music legend Willie Nelson has recorded numerous hits throughout his career that have been featured in various films. His iconic song 'Always on My Mind' was prominently featured in the 1982 film 'The Soldier' and became one of his signature performances. Another of his classics, 'Blue Eyes Crying in the Rain,' was used effectively in the 2005 film 'Brokeback Mountain' during pivotal scene. Generated Answer: Smokey and the Bandit II Generated Answer: Honeysuckle Rose Figure 5: Example showing Falcon-3-3Bs responses in two scenarios using the prompt template in Figure 7: relevant passage + hard distracting and relevant passage + weak distracting. Left: When the relevant passage is followed by hard distracting passage (generated by Gmodal and classified as hard for 3B models due to their distracting effect > 0.8), Falcon-3-3B answers Smokey and the Bandit II instead of Honeysuckle Rose, despite having access to the relevant information. Right: When the relevant passage is followed by weak distracting passage (generated by Grel), the model correctly answers Honeysuckle Rose."
        },
        {
            "title": "5 Application: RAG Fine-Tuning",
            "content": "We now move to make use of the distracting passages to train robust generation component for RAG. For each query, we obtain through the methods described above, set of highly distracting passages, as well as relevant passage. We use these to build prompt containing the query and passages, resulting in training set for RAG."
        },
        {
            "title": "5.1 Experimental Setting",
            "content": "We adopt the same benchmarks as in Section 4.1. We use 800 queries from NQ to build training data. Each instance in our training data is (q, a, ) triplet consisting of query, ground-truth answer, and list of 5 passages. We use three strategies to collect the passages in : Retrieve we use our retrieval pipeline (see Section 4.1) without reranker and fetch the top 5 ranked results; Rerank same as retrieve, but in this case, we enable the re-ranking module; Hard in 50% of the cases we take the first relevant passage from the Rerank strategy and the most distracting 4 passages obtained by using the methods described in Section 3.2; in the remaining 50% of the cases, we select the most distracting 5 passages obtained by using our methods. Finally, we shuffle the five passages to create ."
        },
        {
            "title": "These strategies resulted in three corresponding",
            "content": "training sets. We use the remaining queries from NQ to create an in-distribution (ID) test set. Additionally, we use the queries from PopQA, TriviaQA, and WebQA to create out-of-distribution (OOD) test sets. Each of the resulting four test sets contains balanced mixture of test cases from the Retrieve, Rerank, and Hard strategies described above. We use the training sets to fine-tune5 the instructbased version of two LLMs, namely Llama-3.2-3B and Llama-3.1-8B, and compare results to their non-fine-tuned counterparts."
        },
        {
            "title": "5.2 Results",
            "content": "Table 2 contains the test results for all 8 LLMs, corresponding to both the bases of Llama-3.2-3B and Llama-3.1-8B. In addition to overall accuracy, we report accg, accu corresponding to accuracy over grounded examples, i.e., those that contain relevant passage, and ungrounded examples that do not, respectively. In all cases, training on Hard examples results in major lifts over all baselines in all test sets: 5.3-16.1 absolute accuracy points for Llama-3.2-3B and 3.611.0 points for Llama-3.1-8B. We conjecture this is due to the added value of robustness to distracting passages in the case of ungrounded examples. Indeed, when the ground truth passage is in the prompt, distracting passage can hurt, but to limited effect (Table 1 shows drops from 6 to 11 accuracy). In contrast, when the answer is present only in the parametric memory of the LLM, prompt with only distracting passages is much more likely to result in an error. Due to space restrictions, we provide the test results on the different partitions of the test sets according to the passage collection method in Appendix B.1. Results exhibit consistent behavior with major improvements on ungrounded examples across all slices. For the overall accuracy, we see clear advantage of Hard for Llama-3.2-3B across the board, especially for OOD datasets, with lift of 6.7 and 7.6 acc points for the TriviaQA and WebQA when compared to the baselines. For Llama-3.1-8B, since it is stronger LLM with lower margin of improvement, the results are closer to the baselines, though the overall performance is better for our technique in 3 out of 4 benchmarks. As before, the gain is much more significant for the ungrounded instances, but here it does come at small expense of accuracy on grounded instances. 5For further details, see Appendix B. Test Set Train Set Llama-3.2-3B Llama-3.1-8B accu accg acc accu accg acc NQ PopQA TriviaQA WebQA None Retrieve Rerank Hard None Retrieve Rerank Hard None Retrieve Rerank Hard None Retrieve Rerank Hard 15.2 13.3 11.5 21.4 8.4 8.8 9.6 14.9 38.0 36.9 33.3 54.1 21.4 20.7 20.3 35.0 51.4 37.9 12.8 57.1 40.7 21.0 56.5 39.7 19.9 55.6 42.8 32.0 55.7 35.9 8.7 62.6 40.1 16.6 60.3 39.1 18.0 63.6 43.2 21. 79.2 67.8 39.8 79.4 67.6 56.8 76.7 64.7 58.4 82.3 74.5 68.9 54.4 41.9 19.0 55.1 42.1 28.4 54.1 41.3 30.4 58.7 49.7 36.8 56.7 40.3 62.4 46.9 63.2 47.0 59.8 49.4 61.3 39.3 73.2 49.5 75.1 51.2 71.2 50.4 86.4 73.5 87.1 78.7 87.5 79.4 87.0 82.0 53.8 40.6 59.9 48.0 59.6 48.6 59.7 51. Table 2: Answer accuracy averaged over all 4 test sets. None is the non-fine-tuned baseline, Retrieve, Rerank and Hard are fine-tuning strategies. Metrics: (1) accu, accuracy on ungrounded instances, (2) accg, accuracy on grounded instances, and (3) acc, overall accuracy. Bold values indicate the highest per model and dataset. The LLMs fine-tuned on the Hard dataset achieve statistically significant superior acc in all test sets besides Llama-3.1-8B on PopQA where results are slightly lower than training on Rerank, but in non-statistically significant manner (Wilcoxon test with p-value < 0.01)."
        },
        {
            "title": "6 Conclusions",
            "content": "In this paper, we explored the topic of distracting passages in the context of RAG. We provided an algorithm to measure the distracting effect of passage w.r.t. query and LLM and demonstrated its robustness across LLM types and alternative implementations. We explored different ways to obtain distracting passages, going beyond the common approach of using standard retrieval. We showed that the combination of these methods produces more distracting passages; this allows the creation of more challenging and diverse datasets for RAG, and we demonstrated how they can be used to finetune LLMs to be more robust to distracting passages. We note that this application represents one of the potentially many use cases, and we believe that the insights gained from our study of the distracting effect of passages will prove valuable for additional applications."
        },
        {
            "title": "Limitations",
            "content": "The generation categories discussed in Section 3.2.2 do not necessarily capture the full range of distracting passage types. Expanding this taxonomy to account for additional rhetorical strategies or domain-specific use cases remains an open research question. Moreover, our research primarily investigated the question-answering task, though the concept of distracting passages extends to various RAG use cases. Indeed, extending the study to additional tasks will provide more complete picture, which we leave to future work. Finally, while we conducted our experiments on English-language benchmarks, the languageagnostic nature of our methodology suggests that the findings would likely generalize to other languages, though formal verification of this hypothesis remains to be carried out."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was carried out while Florin Cuconasu was enrolled in the Italian National Doctorate on Artificial Intelligence run by the Sapienza University of Rome. This project has also been supported by PNRR MUR project PE0000013-FAIR."
        },
        {
            "title": "References",
            "content": "Rustam Abdumalikov, Pasquale Minervini, and Yova Kementchedjhieva. 2024. Answerability in retrievalaugmented open-domain question answering. arXiv preprint arXiv:2403.01461. Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2024. Self-RAG: Learning to retrieve, generate, and critique through self-reflection. In The Twelfth International Conference on Learning Representations. Victoria Basmov, Yoav Goldberg, and Reut Tsarfaty. 2024. LLMs reading comprehension is affected by parametric knowledge and struggles with hypothetical statements. arXiv preprint arXiv:2404.06283. Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 15331544, Seattle, Washington, USA. Association for Computational Linguistics. Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading wikipedia to answer opendomain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 18701879. Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2024. BGE M3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. Preprint, arXiv:2402.03216. Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare Campagnano, Yoelle Maarek, Nicola Tonellotto, and Fabrizio Silvestri. 2024. The power of noise: Redefining retrieval for RAG systems. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 719729. Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li. 2024. survey on RAG meeting LLMs: Towards retrieval-augmented large language models. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 64916501. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. 2023. Retrieval-augmented generation for large language models: survey. arXiv preprint arXiv:2312.10997. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, and 542 others. 2024. The llama 3 herd of models. Preprint, arXiv:2407.21783. Or Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kukliansy, Vered Cohen, Thomas Scialom, Idan Szpektor, Avinatan Hassidim, and Yossi Matias. 2022. True: Re-evaluating factual consistency evaluation. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 39053920. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685. Bowen Jin, Jinsung Yoon, Jiawei Han, and Sercan Arik. 2024. Long-context LLMs meet RAG: Overcoming challenges for long inputs in RAG. arXiv preprint arXiv:2410.05983. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16011611. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, and 1 others. 2019. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466. Daliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin Wang, Michal Lukasik, Andreas Veit, Felix Yu, and Sanjiv Kumar. 2023. Large language models with controllable working memory. In Findings of the Association for Computational Linguistics: ACL 2023, pages 17741793. Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Richard James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, and Scott Yih. 2024. RA-DIT: Retrieval-augmented dual instruction tuning. In The Twelfth International Conference on Learning Representations. Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. Lost in the middle: How language models use long contexts. Preprint, arXiv:2307.03172. Hongyin Luo, Yung-Sung Chuang, Yuan Gong, Tianhua Zhang, Yoon Kim, Xixin Wu, Danny Fox, Helen Meng, and James Glass. 2023. Sail: SearcharXiv preprint augmented instruction learning. arXiv:2305.15225. Alex Troy Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In The 61st Annual Meeting Of The Association For Computational Linguistics. Gabriel de Souza Moreira, Radek Osmulski, Mengyao Xu, Ronay Ak, Benedikt Schifferer, and Even Oldridge. 2024. NV-Retriever: Improving text embedding models with effective hard-negative mining. arXiv preprint arXiv:2407.15831. Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel. 2021. KILT: benchmark for knowledge intensive language tasks. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 25232544. Falcon-LLM Team. 2024. The falcon 3 family of open models. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022. Text embeddings by weaklysupervised contrastive pre-training. arXiv preprint arXiv:2212.03533. Zhepei Wei, Wei-Lin Chen, and Yu Meng. 2024. Instructrag: Instructing retrieval augmented generation In Adaptive Founvia self-synthesized rationales. dation Models: Evolving AI for Personalized and Efficient Learning. Orion Weller, Benjamin Van Durme, Dawn Lawrie, Ashwin Paranjape, Yuhao Zhang, and Jack Hessel. 2024. Promptriever: Instruction-trained retrievers can be arXiv preprint prompted like language models. arXiv:2409.11136. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, and 3 others. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, EMNLP 2020 - Demos, Online, November 16-20, 2020, pages 3845. Association for Computational Linguistics. Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. 2021. Approximate nearest neighbor negative contrastive learning for dense text reIn International Conference on Learning trieval. Representations. Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. 2024. Corrective retrieval augmented generation. arXiv preprint arXiv:2401.15884. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, and 23 others. 2025. Qwen2.5 technical report. Preprint, arXiv:2412.15115. Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. 2024. Making retrieval-augmented language models robust to irrelevant context. In The Twelfth International Conference on Learning Representations. Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, and Dong Yu. 2023. Chain-ofnote: Enhancing robustness in retrieval-augmented language models. arXiv preprint arXiv:2311.09210."
        },
        {
            "title": "A Additional Details on Distracting Effect",
            "content": "In Section 3.1, we introduce method to quantify how distracting irrelevant passages are for LLMs using Equation 1. To compute this distracting effect for given query and passage p, we follow Algorithm 1 which calculates DEq(p). The process begins by constructing prompt using the template shown in Figure 6, where we explicitly include NO-RESPONSE as an answer (in the algorithm we refer to it as target). This prompt instructs the LLM to respond with NO-RESPONSE when the context contains no relevant answer. We then measure the LLMs likelihood of generating NORESPONSE by examining the probability of its first token (that is pLLM(NO-RESPONSEq, p) in Equation 1). This probability serves as confidence measure: when it is high (making DEq(p) close to 0), the LLM is likely to abstain from answering. Conversely, when the probability is low (making DEq(p) close to 1), the LLM is more inclined to generate an answer based on the passage, indicating that the passage is distracting the LLM. A.1 Distracting Effect on Other LLMs and"
        },
        {
            "title": "Datasets",
            "content": "In this section, we extend our analysis of distracting effects beyond Llama-3.1-8B on NQ and WebQuestions to other LLMs and datasets. As anticipated in Section 4.3, the probability distributions of distracting effects for TriviaQA and PopQA follow similar patterns to those observed for NQ and WebQuestions (Figure 11). This consistency extends across all LLMs tested: Llama-3.2-3B (Figure 12), Llama-3.3-70B (Figure 13), Qwen-2.5-3B (Figure 14), Qwen-2.5-7B (Figure 15), Falcon-3-3B (Figure 16), and Falcon-3-7B (Figure 17). However, we observe distinct characteristics across model families. The Qwen models demonstrate higher confidence in classifying passages as either weak or hard distracting, with approximately 90% of their probability mass concentrated in the extreme intervals (0.0-0.2 for weak distracting and 0.8-1.0 for hard distracting passages). The Falcon models exhibit more varied behavior. While Falcon-3-7B generally aligns with the patterns seen in Llama and Qwen models, it shows lower confidence in its classifications, particularly for generated passages. Falcon-3-3B presents notably different behavior, with probability distributions heavily skewed toward maximum distracting effects. While this might suggest that Falcon-3-3B finds most passages highly distracting, deeper investigation reveals that this model often fails to follow instructions about abstaining from answering, and instead generates responses regardless of passage relevance. consistent pattern emerges when comparing model sizes: the 3B versions across all model families show greater susceptibility to distraction compared to their larger counterparts (as evidenced by the Most Distracting distributions in the Figures). This suggests that larger models generally develop more robust mechanisms for handling irrelevant information during their training. Nevertheless, our fine-tuning approach demonstrates that even smaller models can achieve significant improvements in handling distracting passages, as shown by the results for Llama-3.2-3B in Section 5.2. A.2 Answer-Skewed Retriever Hyper-parameters The answer-skewed retriever (Rsk) is introduced to retrieve hard distracting passages that differ from those found by the standard retriever, thus ensuring diversity. To select the best configuration for this type of retriever, we took validation set of 700 samples from the four datasets, and computed the distracting effect using different λ with the two formulations Esub and Eproj (see Equations 2 and 3). Our experiments showed that λ = 1 strikes an optimal balance in retrieving distracting passages. With λ > 1 (λ = 2.0 in our experiments), we observed that passages are mainly weakly distracting or completely unrelated in some cases, where the average distracting effect for the retriever is 0.08 and 0.19 for Esub and Eproj, respectively. The reason is that the weight given to the second term of the formulas 2 and 3 pushed results too far from the querys topic. Conversely, with λ < 1 (λ = 0.5 in our experiments), the answer-skewed retriever behaves too similarly to the standard retriever, retrieving almost the same set of documents. This means that it would not provide any additional contribution. Finally, while Esub and Eproj formulations lead to quite similar results, we selected Esub because it presented higher proportion of unique wins across all LLM tested. Additional Details on RAG Fine-Tuning For our experiments in Section 5 we implemented Low Rank Adaptaion (LoRA) fine-tuning using the transformers, datasets, accelerate, peft and trl libraries from Hugging Face (Hu et al., 2021; Wolf et al., 2020). The prompt used for creating the train and test sets appears in Figure 7. For both models, we set the number of training epochs to 3, the neftune noise α to 5, and the max gradient norm to 0.3. For the Llama-3.2-3B-Instruct models, we set the LoRA rank to 64, the warmup ratio to 0.03, and used constant learning rate of 3e 5. For the larger Llama-3.1-8B-Instruct model, we increased the LoRA rank to 128 and the warmup ratio to 0.05, and used cosine decaying learning rate that begins at 5e 5. You are given question and you must respond based on the provided documents. Respond directly without providing any premise or explanation. If none of the documents contain the answer, please respond with NO-RESPONSE. Do not try to respond based on your own knowledge. Documents: <document> Question: <question> Answer: NO-RESPONSE Figure 6: Prompt for evaluating the distracting effect of passage. Algorithm 1 Computing the Distracting Effect Input: Query q, passage p, LLM M, tokenizer τ Output: Distracting effect DEq(p) 1: prompt create_prompt(q, p) 2: tokens τ (prompt) 3: target_pos get_position(prompt, \"NO-RESPONSE\") Starting pos in the tokenized prompt 4: target_token τ (\"NO-RESPONSE\")[0] Get first target token 5: logits M(tokens) 6: probs softmax(logits) 7: prob probs[target_pos 1, target_token] Generation probability of the first target token 8: return 1 prob Create the prompt using template in Figure 6 Answer the user question based on the passages below. Provide single concise answer. Documents: <document> Question: <question> Assistant: <answer> Figure 7: Prompt for fine-tuning training and testing (in the latter case <answer> is left blank). Question: When did One Piece first air on cartoon network? Gold Answer: April Relevant Passage (Title: One Piece) On June 8, 2004, 4Kids Entertainment acquired the license for distribution of One Piece in North America. 4Kids contracted Viz Media to handle home video distribution. 4Kids'in - house musicians wrote new background score and theme song nicknamed '' Pirate Rap ''. 4Kids'dub mandated edits for content and length, which reduced the first 143 episodes into 104. Initially, 4Kids originally created an English version of the first opening theme, '' We Are! '' It premiered in the United States on September 18, 2004, in first - run syndication on the Fox network as part of the weekend programming block Fox Box, and later aired on Cartoon Network on their weekday afternoon programming block Toonami in April 2005. Production was halted in 2006 after episode 143 / 104. Viz also ceased its home video release of the series after volume 11. Hard Distracting Rst + Distracting Effect: - Llama-3.2-3B: 1.0000 - Llama-3.1-8B: 1.0000 - Llama-3.3-70B: 1.0000 1.0000 - Falcon-3-3B: 1.0000 - Falcon-3-7B: 1.0000 - Qwen-2.5-3B: 1.0000 - Qwen-2.5-7B: Weak Distracting Ghypo Distracting Effect: - Llama-3.2-3B: 0.0000 - Llama-3.1-8B: 0.0000 - Llama-3.1-8B: 0.0000 - Falcon-3-3B: 0.0469 - Falcon-3-7B: 0.1445 - Qwen-2.5-3B: 0.0000 - Qwen-2.5-7B: 0. (Title: One Piece) The Funimation dubbed episodes on Cartoon Network on September 29, 2007 and aired until its removal on March 22, 2008. On October 28, 2011, Funimation posted press release on their official website confirming the acquisition of episodes 206263, and the aspect ratio, beginning with episode 207, would be changed to the 16:9 widescreen format. On May 18, 2013, the uncut series began airing on Adult Swim's revived Toonami late-night programming block from episode 207 onward. 'One Piece' was removed from the Toonami block after March 18, 2017. In May 2009, Funimation, Toei Animation, Shueisha, and Fuji Television announced they would simulcast stream the series Generated Answer: The Funimation dubbed episodes of \"One Piece\" aired on Cartoon Network on September 29, 2007 (Title: One Piece TV History) Before considering Western television networks, One Piece had complex journey through various Asian broadcasters. In 2003, several Southeast Asian networks were in negotiations to acquire the series, with Singapore's MediaCorp actively pursuing the rights. During this time, there were discussions about potentially airing an edited version on different time slots, and multiple networks were competing to become the first English-language broadcaster of the series in Asia. Generated Answer: One Piece initially aired on Cartoon Network's Toonami block in April 2005 Figure 8: Example showing Llama-3.2-3Bs responses in two scenarios using the prompt template in Figure 7: relevant passage + hard distracting and relevant passage + weak distracting. Left: When the relevant passage is followed by hard distracting passage (retrieved by Rst +), Llama-3.2-3B incorrectly answers September 29, 2007 instead of April 2005, despite having access to the relevant information. The passages strong distracting effect is confirmed by the maximum distracting scores (1.0) across all models tested. Right: When the relevant passage is followed by weak distracting passage (generated by Ghypo), the model correctly answers April 2005, with consistently low distracting scores across all models. Question: When was the first airbag put in car? Gold Answer: during the 1970s Relevant Passage (Title: Airbag) The first commercial designs were introduced in passenger automobiles during the 1970s with limited success . Broad commercial adoption of airbags occurred in many markets during the late 1980s and early 1990s with driver airbag , and front passenger airbag as well on some cars ; and many modern vehicles now include six or more units . Hard Distracting Rsk + Distracting Effect: - Llama-3.2-3B: 0.3750 - Llama-3.1-8B: 0.3398 - Llama-3.3-70B: 0.1094 1.0000 - Falcon-3-3B: 0.9336 - Falcon-3-7B: 1.0000 - Qwen-2.5-3B: 1.0000 - Qwen-2.5-7B: Weak Distracting Rst Distracting Effect: - Llama-3.2-3B: 0.6855 - Llama-3.1-8B: 0.0156 - Llama-3.3-70B: 0.0117 0.9977 - Falcon-3-3B: 0.8027 - Falcon-3-7B: 0.9526 - Qwen-2.5-3B: 0.0000 - Qwen-2.5-7B: (Title: Airbag) the sensors would automatically pre-tension the seat belts to reduce occupants' motion on impact (now common feature), and then deploy the airbag on impact. This integrated the seat belts and airbag into restraint system, rather than the airbag being considered an alternative to the seat belt. In 1987, the Porsche 944 Turbo became the first car to have driver and passenger airbags as standard equipment. The less powerful Porsche 944 and 944S had this as an available option. The same year also saw the first airbag in Japanese car, the Honda Legend. (Title: Airbag) The airbag specified for automobile use traces its origins to air-filled bladders as early as 1951. The invention is credited independently to the American John W. Hetrick who filed for an airbag patent on August 5, 1952 that was granted #2,649,311 by the United States Patent Office on 18 August 1953. German engineer Walter Linderer who filed German patent #896,312 on 6 October 1951 was issued on 12 November 1953, approximately three months after American John Hetrick. Generated Answer: The first airbag was put in car in 1987, specifically in the Porsche 944 Turbo Generated Answer: The first commercial airbags were introduced in passenger automobiles during the 1970s Figure 9: Example showing Qwen-2.5-7Bs responses in two scenarios using the prompt template in Figure 7: relevant passage + hard distracting and relevant passage + weak distracting. Left: When the relevant passage is followed by hard distracting passage (retrieved by Rsk +), Qwen-2.5-7B generates an incorrect answer despite having access to the relevant information. Right: When the relevant passage is followed by weak distracting passage (retrieved by Rst), the model answers correctly. Notably, this passage qualifies as weak distracting only for Qwen-2.5-7B, Llama-3.1-8B, and Llama-3.3-70B; while the other LLMs show high distracting scores (> 0.68). For example, when given the same passage, Qwen-2.5-3B generates the incorrect answer August 5, 1952a date mentioned in the distracting passage itself. Question: Who started roses are red violets are blue? Gold Answer: Sir Edmund Spenser Relevant Passage (Title: Roses Are Red) The origins of the poem may be traced at least as far back as to the following lines written in 1590 by Sir Edmund Spenser from his epic The Faerie Queene ( Book Three, Canto 6, Stanza 6 ). Hard Distracting Gneg Distracting Effect: - Llama-3.2-3B: 0.0859 - Llama-3.1-8B: 0.9841 - Llama-3.3-70B: 0.0508 0.9996 - Falcon-3-3B: 1.0000 - Falcon-3-7B: 0.8486 - Qwen-2.5-3B: 1.0000 - Qwen-2.5-7B: Weak Distracting Rsk Distracting Effect: - Llama-3.2-3B: 0.0430 - Llama-3.1-8B: 0.0195 - Llama-3.3-70B: 0.0000 0.5215 - Falcon-3-3B: 0.8457 - Falcon-3-7B: 0.7832 - Qwen-2.5-3B: 0.0000 - Qwen-2.5-7B: (Title: Historic Love Poetry) While many people incorrectly attribute this famous poem to William Shakespeare or Lord Byron, this popular rhyme has evolved significantly over centuries. The earliest known romantic verses comparing flowers actually originated in medieval French poetry, particularly in the works of Pierre de Ronsard. Some scholars have mistakenly suggested that Geoffrey Chaucer first penned these lines in The Canterbury Tales, but this is common misconception that has been debunked by literary historians. (Title: Roses Are Red (My Love)) Roses Are Red (My Love) \"Roses Are Red (My Love)\" is popular song composed by Al Byron and Paul Evans. It was recorded by Bobby Vinton and was his first hit. Vinton found the song in reject pile at Epic Records. He first recorded it as an R&B number, but was allowed to re-record it in slower more dramatic arrangement, with strings and vocal choir added. The song was released in April 1962. It reached No. 1 in Australia, New Zealand, Norway, South Africa, and the United States, and was major hit in many other Generated Answer: The origins of \"Roses are red\" can be linked to medieval French poetry and specifically to Pierre de Ronsard's works Generated Answer: Sir Edmund Spenser Figure 10: Example showing Llama-3.1-8Bs responses in two scenarios using the prompt template in Figure 7: relevant passage + hard distracting and relevant passage + weak distracting. Left: When the relevant passage is followed by hard distracting passage (generated by Gneg), Llama-3.1-8Bs generates an incorrect answer despite having access to the relevant information. Right: When the relevant passage is followed by weak distracting passage (retrieved by Rsk), the model answers correctly. B.1 Accuracy By Training Set Table 3 presents more fine-grained breakdown of the results shown in Table 2. The table offers couple of noteworthy insights. First, as discussed in Section 5.2, the most significant performance improvement is observed on ungrounded samples, trend that remains consistent across different test sets, datasets, and models. Second, as expected, fine-tuned models generally achieve their highest overall accuracy when evaluated on test sets of the same type as their training data. However, in certain cases, such as the Llama-3.2-3B model on TriviaQA and WebQA, our Hard method outperforms not only the None baseline but also the Retrieve and Rerank fine-tuned models across all three accuracy metrics."
        },
        {
            "title": "C Generation Prompts",
            "content": "The prompts for generating the different categories of distracting passages presented in Section 3.2.2 can be found in Figures 22-25. Figure 11: Distribution of distracting effect for passages obtained through different methods, as measured by Llama-3.1-8B on all datasets. Methods are ordered by their mean distracting effect (shown by vertical black lines), with higher means indicating greater ability to distract the model. Figure 12: Distribution of distracting effect for passages obtained through different methods, as measured by Llama-3.2-3B on all datasets. Methods are ordered by their mean distracting effect (shown by vertical black lines), with higher means indicating greater ability to distract the model. Figure 13: Distribution of distracting effect for passages obtained through different methods, as measured by Llama-3.3-70B on all datasets. Methods are ordered by their mean distracting effect (shown by vertical black lines), with higher means indicating greater ability to distract the model. Figure 14: Distribution of distracting effect for passages obtained through different methods, as measured by Qwen-2.5-3B on all datasets. Methods are ordered by their mean distracting effect (shown by vertical black lines), with higher means indicating greater ability to distract the model. Figure 15: Distribution of distracting effect for passages obtained through different methods, as measured by Qwen-2.5-7B on all datasets. Methods are ordered by their mean distracting effect (shown by vertical black lines), with higher means indicating greater ability to distract the model. Figure 16: Distribution of distracting effect for passages obtained through different methods, as measured by Falcon-3-3B on all datasets. Methods are ordered by their mean distracting effect (shown by vertical black lines), with higher means indicating greater ability to distract the model. Figure 17: Distribution of distracting effect for passages obtained through different methods, as measured by Falcon-3-7B on all datasets. Methods are ordered by their mean distracting effect (shown by vertical black lines), with higher means indicating greater ability to distract the model. Figure 18: Percentage of questions where each method provides the most distracting passage for all models. In blue are the times when no other method reaches the same distracting effect, in orange the percentage of times the highest score is shared with other methods. Figure 19: Percentage of questions where each method provides the most distracting passage for Llama-3.3-70B. In blue are the times when no other method reaches the same distracting effect, in orange the percentage of times the highest score is shared with other methods. Figure 20: Average distracting effect at different rank positions for various retrieval methods. Results are shown for all models, averaged across datasets. Higher-ranked passages consistently demonstrate greater potential to mislead the model. Figure 21: Average distracting effect at different rank positions for various retrieval methods. Results are shown for Llama-3.3-70B, averaged across datasets. Test Set Test Set Slice Train Set Llama-3.2-3B Llama-3.1-8B Retrieve NQ Rerank Hard Retrieve PopQA Rerank Hard Retrieve TriviaQA Rerank Hard Retrieve WebQA Rerank Hard None Retrieve Rerank Hard None Retrieve Rerank Hard None Retrieve Rerank Hard None Retrieve Rerank Hard None Retrieve Rerank Hard None Retrieve Rerank Hard None Retrieve Rerank Hard None Retrieve Rerank Hard None Retrieve Rerank Hard None Retrieve Rerank Hard None Retrieve Rerank Hard None Retrieve Rerank Hard accu accg 4.2 4.2 4.2 8.8 5.7 4.1 5.3 8. 19.4 17.0 14.3 26.5 1.7 2.6 2.0 4.3 1.6 1.9 1.6 3.5 12.5 12.7 14.3 21.5 14.9 11.9 14.9 23.9 16.7 8.3 11.1 11. 40.3 39.6 35.3 57.7 8.5 9.0 8.5 16.6 1.7 3.9 4.4 11.7 28.9 27.2 26.6 44.6 54.9 62.8 61.4 56.8 56.3 62.1 63.0 56. 44.3 47.8 46.6 54.0 60.5 68.2 67.4 65.7 61.3 67.1 64.1 64.9 46.1 53.2 50.3 60.4 82.0 82.6 81.0 84.0 82.5 81.9 79.1 83. 73.3 73.8 70.4 79.9 57.1 58.3 57.5 60.9 57.4 57.7 57.3 61.8 49.3 49.9 48.3 53.9 acc 44.2 50.4 49.3 46. 45.3 49.6 50.5 46.0 30.9 31.3 29.2 39.2 41.8 47.4 46.6 46.2 45.2 49.5 47.2 48.3 27.2 30.4 30.1 38.5 77.4 77.8 76.5 79. 80.1 79.2 76.6 80.5 56.7 56.6 52.7 68.7 44.7 45.7 45.0 49.6 45.9 46.6 46.4 51.4 38.3 37.7 36.6 48.9 accu accg 3.3 10.0 7.1 16.7 4.1 9.3 8.5 13.0 16.4 25.6 24.8 39.0 2.6 6.3 7.6 8.3 2.3 3.9 6.2 7. 12.5 23.5 24.6 29.9 11.9 26.9 26.9 38.8 13.9 19.4 16.7 27.8 42.7 60.2 62.1 72.5 6.7 16.1 17.9 22.0 2.2 5.6 7.8 11. 25.6 36.2 38.3 45.7 60.1 66.9 66.0 61.1 63.6 66.8 68.9 61.2 47.9 54.7 55.7 57.6 66.0 76.4 77.9 73.0 64.8 77.2 78.9 72. 53.6 66.5 69.0 68.3 89.5 88.5 89.7 86.8 89.5 89.7 89.8 88.0 80.4 83.1 83.1 86.1 55.7 63.1 62.3 60.6 57.9 63.5 62.5 61. 48.4 53.9 54.7 57.3 acc 48.2 54.9 53.6 51.8 50.7 54.4 55.9 50.8 31.0 39.1 39.1 47.6 45.9 54.1 55.6 52. 47.9 57.4 59.3 54.8 30.5 42.3 44.0 46.7 84.2 84.3 85.4 83.6 86.7 87.1 87.1 85.8 61.3 71.5 72.5 79.2 43.2 51.1 51.0 50. 46.4 51.5 51.2 51.2 36.2 44.4 45.9 51.1 Table 3: Detailed answer accuracy across different test set configurations. Results are subdivided by test set slice (Retrieve, Rerank, Hard), showing accuracy for each combination of test set and training strategy. Metrics: accu (ungrounded accuracy), accg (grounded accuracy), and acc (overall accuracy). Given question and one or more correct answers to it, generate paragraph of distracting text that is related to the question, but does not contain the answer to the question. The paragraph should: 1) discuss the question, then continue to new subject where it mentions an entity similar to that in the question, but is not the one that is sought in the question. 2) should not contain any of the correct answers 3) be factually correct 4) be written in valid JSON format with two fields: \"title\", having 3-4 words, and \"text\" having roughly 3-4 sentences. The expected output must be ONLY valid JSON of this format: {\"title\": \"string\", \"text\": \"string\"} # Example 1 ## Question In which city would you find the petronas twin towers? ## Answer Kuala Lumpur ## Distracting paragraph: { \"title\": \"Visiting Petronas Towers\", \"text\": \"In order to visit Petronas towers, visitors must first purchase tickets. Tickets can be purchased online or at the counter, as well as via agencies. One agency that was recommended is that of the Golden travel. Despite having its main offices in Rome, Italy, it is known to obtain good deals for the Petronas towers. Customers of the agency mentioned the staff to be friendly and helpful. Furthermore, the Golden agency also offers variety of discounts.\" } # Example 2 ## Question How long is the Amazon river? ## Answer 6400 km long ## Distracting paragraph { \"title\": \"World Rivers\", \"text\": \"The Amazon river is the second longest river in the world. The longest river is the Nile river, ranging over 6,650 km. Another notable river is the Yangtze river located in China. For more information about world known rivers and their lengths, please visit: https://www.worldometers.info/rankings/world-rankings-longest-rivers/ .\" } ## Question <question> ## Answer <answer> ## Distracting paragraph Figure 22: Prompt for distracting passage generation, related topic. Given question and one or more correct answers to it, generate paragraph of distracting text that is related to the question, but does not contain the answer to the question. The paragraph should: 1) discuss either hypothetical situation or the reality in the past, where the question is being asked and under these mentioned conditions, the answer is different than the correct one. 2) should not contain any of the correct answers 3) be written in valid JSON format with two fields: \"title\", having 3-4 words, and \"text\" having roughly 3-4 sentences. The expected output must be ONLY valid JSON of this format: {\"title\": \"string\", \"text\": \"string\"} # Example 1 ## Question In which city would you find the petronas twin towers? ## Answer Kuala Lumpur ## Distracting paragraph: { \"title\": \"The Petronas Twin Towers\", \"text\": \"During the planning stages of the Petronas Twin Towers, several cities in Malaysia were evaluated as potential sites for the project. Petaling Jaya, rapidly developing urban center, was among the locations considered. While it was ultimately not selected, the choice of site could have significantly influenced its architectural and cultural landscape.\" } # Example 2 ## Question How long is the Amazon river? ## Answer 6400 km long ## Distracting paragraph { \"title\": \"Formation of the Amazon River\", \"text\": \"The Amazon River is believed to have formed approximately 11 million years ago. In its early stages, the rivers length was significantly shorter, spanning no more than 500 kilometers. Over millions of years, geological processes such as erosion, tectonic plate shifts, and sediment deposition contributed to its expansion. These changes gradually transformed the river into the vast waterway it is today, shaping the ecosystems of the surrounding Amazon Basin.\" } ## Question <question> ## Answer <answer> ## Distracting paragraph Figure 23: Prompt for distracting passage generation, hypothetical. Given question and one or more correct answers to it, generate paragraph of distracting text that is related to the question, but does not contain the answer to the question. The paragraph should: 1) provide wrong answer to the question following disclaimer that the answer is wrong 2) should not contain any of the correct answers 3) be written in valid JSON format with two fields: \"title\", having 3-4 words, and \"text\" having roughly 3-4 sentences. The expected output must be ONLY valid JSON of this format: {\"title\": \"string\", \"text\": \"string\"} # Example 1 ## Question In which city would you find the petronas twin towers? ## Answer Kuala Lumpur ## Distracting paragraph: { \"title\": \"Petaling Jaya\", \"text\": \"Petaling Jaya is city located in the state of Selangor, Malaysia. It is well-developed urban area with variety of facilities, including shopping malls, recreational parks, and educational institutions. The city is notable for its infrastructure and connectivity, making it significant part of the Klang Valley region. Although it is not home to the Petronas Twin Towers, Petaling Jaya features range of attractions and amenities that cater to residents and visitors alike.\" } # Example 2 ## Question How long is the Amazon river? ## Answer 6400 km long ## Distracting paragraph { \"title\": \"The Amazon River\", \"text\": \"In the article, several inaccuracies regarding the Amazon River were noted. The author incorrectly stated that the river passes through Central America, while it is geographically located in South America, originating in the Andes Mountains and flowing through countries such as Brazil and Peru. Additionally, the river's length was described as over 8,000 kilometers.\" } ## Question <question> ## Answer <answer> ## Distracting paragraph Figure 24: Prompt for distracting passage generation, negation. Given question and one or more correct answers to it, generate paragraph of distracting text that is related to the question, but does not contain the answer to the question. The paragraph should: 1) provide wrong answer to the question following disclaimer that the answer is only conjecture 2) should not contain any of the correct answers 3) be written in valid JSON format with two fields: \"title\", having 3-4 words, and \"text\" having roughly 3-4 sentences. The expected output must be ONLY valid JSON of this format: {\"title\": \"string\", \"text\": \"string\"} # Example 1 ## Question In which city would you find the petronas twin towers? ## Answer Kuala Lumpur ## Distracting paragraph: { \"title\": \"Celebrity Trivia with Selena Gomez\", \"text\": \"American singer and actress Selena Gomez participated in trivia segment where she was asked about the location of the Petronas Twin Towers, prominent architectural landmark in Malaysia. She responded, 'I'm not entirely sure, but if remember correctly, the Petronas Twin Towers are located in Petaling Jaya.'\" } # Example 2 ## Question How long is the Amazon river? ## Answer 6400 km long ## Distracting paragraph { \"title\": \"River Exploration and Mapping\", \"text\": \"In the 1700s, much of the Amazon River remained uncharted, with only partial knowledge of its vast expanse. Experts of the time estimated its length to be around 5,100 kilometers, based on the exploration that had been conducted by then. It was only much later that other parts of the Amazon river were discovered and more accurate measurement was available.\" } ## Question <question> ## Answer <answer> ## Distracting paragraph Figure 25: Prompt for distracting passage generation, modal statement."
        }
    ],
    "affiliations": [
        "Sapienza University of Rome",
        "Technology Innovation Institute"
    ]
}