{
    "paper_title": "Emma-X: An Embodied Multimodal Action Model with Grounded Chain of Thought and Look-ahead Spatial Reasoning",
    "authors": [
        "Qi Sun",
        "Pengfei Hong",
        "Tej Deep Pala",
        "Vernon Toh",
        "U-Xuan Tan",
        "Deepanway Ghosal",
        "Soujanya Poria"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Traditional reinforcement learning-based robotic control methods are often task-specific and fail to generalize across diverse environments or unseen objects and instructions. Visual Language Models (VLMs) demonstrate strong scene understanding and planning capabilities but lack the ability to generate actionable policies tailored to specific robotic embodiments. To address this, Visual-Language-Action (VLA) models have emerged, yet they face challenges in long-horizon spatial reasoning and grounded task planning. In this work, we propose the Embodied Multimodal Action Model with Grounded Chain of Thought and Look-ahead Spatial Reasoning, Emma-X. Emma-X leverages our constructed hierarchical embodiment dataset based on BridgeV2, containing 60,000 robot manipulation trajectories auto-annotated with grounded task reasoning and spatial guidance. Additionally, we introduce a trajectory segmentation strategy based on gripper states and motion trajectories, which can help mitigate hallucination in grounding subtask reasoning generation. Experimental results demonstrate that Emma-X achieves superior performance over competitive baselines, particularly in real-world robotic tasks requiring spatial reasoning."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 6 1 ] . [ 1 4 7 9 1 1 . 2 1 4 2 : r EMMA-X: An Embodied Multimodal Action Model with Grounded Chain of Thought and Look-ahead Spatial Reasoning Qi Sun1*, Pengfei Hong1*, Tej Deep Pala1, Vernon Y.H. Toh1, U-Xuan Tan1, Deepanway Ghosal1, Soujanya Poria"
        },
        {
            "title": "1 Singapore University of Technology and Design",
            "content": "GITHUB: https://github.com/declare-lab/Emma-X MODEL: https://huggingface.co/declare-lab/Emma-X"
        },
        {
            "title": "Abstract",
            "content": "reinforcement Traditional learning-based robotic control methods are often taskspecific and fail to generalize across diverse environments or unseen objects and instructions. Visual Language Models (VLMs) demonstrate strong scene understanding and lack the ability planning capabilities but to generate actionable policies tailored to specific robotic embodiments. To address this, Visual-Language-Action (VLA) models have emerged, yet they face challenges in long-horizon spatial reasoning and grounded task planning. In this work, we propose the Embodied Multimodal Action Model with Grounded Chain of Thought and Look-ahead Spatial Reasoning, EMMA-X. EMMA-X leverages our constructed hierarchical embodiment dataset based on BridgeV2, containing 60,000 robot manipulation trajectories auto-annotated with grounded task reasoning and spatial guidance. Additionally, we introduce trajectory segmentation strategy based on gripper states and motion trajectories, which can help mitigate hallucination in grounding subtask reasoning generation. Experimental results demonstrate that EMMA-X achieves superior competitive baselines, particularly in real-world robotic tasks requiring spatial reasoning. performance over"
        },
        {
            "title": "Introduction",
            "content": "The robotic policy model aims to generate sequences of low-level action manipulation policies for robots. Traditional reinforcement learningbased robotic control methods often focus on narrowly defined tasks within fixed environments (Ma et al., 2024), hindering their ability to generalize beyond task-specific training data and limiting their tion and task, they lack long-horizon spatial reasoning on how robots should move next. We hypothesize that the completion of subgoals or subtasks can be enhanced if the VLA incorporates look-ahead spatial reasoning, such as inferring the grippers future 2D position and the 3D movement plans necessary for the gripper to reach that position. In particular, we train the VLA model to predict future position gt+k of the gripper as checkpoints and use them to devise high-level movement plan β(gt, gt+k). This plan informs the immediate action at at the current state st, ensuring decisions are both reactive to the present and aligned with long-term objectives. Similar to delivery driver planning route with key landmarks to make purposeful driving decisions, this approach optimizes task completion by balancing foresight and adaptability. Additionally, another limitation in task reasoning provided by ECoT(Zawalski et al., 2024) is the absence of visual grounding when augmenting reasoning data using Gemini. We observe that Gemini frequently hallucinates due to lack of holistic understanding of the setup and environment. As shown in Figure 1, the image shows that the robot already started to grasp the pot cover, while the task reasoning indicates the subtask is still Move to silver pot cover\", which conflicts with the following reasoning they provided. In this work, we introduce the Embodied Multimodal Action Model with Grounded Chain of Thought Reasoning, EMMA-X. We develop hierarchical embodiment dataset based on BridgeV2, consisting of 60,000 robot manipulation trajectories. For each state of given trajectory, we generate detailed spatial reasoning grounded in the environment and task reasoning, such as the plans of how the robot should perform the subtask. As shown in Figure 1, we also generate the 2D gripper position, and 3D spatial movements of the gripper to transit to future states, which enable the VLA model to reason long-horizon plan for accomplishing the task. Furthermore, we utilize Gemini (Team et al., 2023) to generate grounded task reasoning for each observed state. To avoid the abovementioned reasoning conflict problem of task reasoning in ECoT, we propose novel trajectory segmentation strategy, which leverages the opening and closing states of the gripper and the motion trajectory of the robot arm to segment the sequence of states into distinct segments. By grounding, we mean that, unlike Figure 1: Comparison of our EMMA-X with ECoT in task reasoning. While both approaches utilize Gemini, our method also incorporates image sequence input, whereas ECoT relies solely on text input. We also illustrate an example of spatial reasoning. applicability (Brohan et al., 2023b; Chi et al.). Recent advancements in foundation models for vision and language have highlighted the remarkable scene-understanding and task-planning capabilities (Radford et al., 2021; Zhai et al., 2023; Touvron et al., 2023). These Visual-Language Models (VLMs) excel at breaking down complex tasks into manageable steps through chain-of-thought reasoning and demonstrate significant potential in planning. Despite their strengths, VLMs are not inherently designed to directly generate policies applicable to specific embodiment configurations in robotics. This limitation has spurred the emergence of Visual-Language-Action (VLA) models, which aim to bridge this gap by leveraging multimodal inputs to produce adaptive and generalized robotic actions for complex, multi-task scenarios (Brohan et al., 2023a; Kim et al., 2024; Octo Model Team et al., 2024). However, most of the existing VLA models often exhibit muscle memory response patterns, struggling to perceive scene variation and understand instructions as humans do when handling complex tasks or ambiguous commands. Zawalski et al. (2024) attempts to address this issue through visual and task reasoning, including the bounding box of the object, task segmentation, and the direction of predicted action, etc. Although they equip VLAs with an understanding of the current situaECoT, which prompts Gemini to generate subtask reasoning based solely on textual descriptions, our approach incorporates visual images segmented using the aforementioned strategy. As shown in Figure 1, our method can accurately provide the subtask Grasping the pot cover\" corresponding to the current robotic state. This illustrates that our strategy significantly reduces Geminis hallucination issues by requiring it to construct visual understanding of the environment, rather than relying solely on textual descriptions of the environment. Finally, we train our EMMA-X based on OpenVLA using our constructed hierarchical embodiment dataset. The main contributions of our work are summarized as follows: We introduce 7B-parameter embodied multimodal action model, EMMA-X created by fine-tuning OpenVLA with the grounded chain of thought (CoT) reasoning data. We synthetically construct hierarchical embodiment dataset from the existing robot manipulation dataset, which includes the 3D spatial movements, 2D gripper position, and grounded reasoning. We propose novel trajectory segmentation strategy that leverages the grippers opening and closing states alongside the motion trajectory of the robot arm, facilitating both grounded task reasoning and look-ahead spatial reasoning. Our proposed EMMA-X achieves significant performance improvements over existing competitive baselines on various real-world robot tasks, especially in tasks where spatial reasoning is required."
        },
        {
            "title": "2.1 Policy Imitation Learning",
            "content": "t=1)}N t=1, Ti, {at}T Given set of expert demonstrations = {({st}T i=1, where is the number of demonstrations in the dataset, is the number of states (image frames of the environment) for data sample Di, si = imagei represents the state consisting of an image of the environment, Ti is natural language task instruction, and ai represents the action taken by the expert in that state, the goal is to learn policy πθ(a s, ) that mimics the experts behavior. The policy πθ is modeled by Vision-LanguageAction (VLA) model. In line with the OpenVLA setting, the policy outputs generalized action as 7-dimensional vector. This vector encodes the endeffectors (grippers) velocity of Cartesian components (x, y, z), orientational components (roll, pitch, yaw), and the grippers close-open action. The goal is to find parameters θ that minimize the difference between predicted action and the experts action."
        },
        {
            "title": "2.2 Hierarchical Policy Imitation",
            "content": "We build on the above formulation by decomposing general task into hierarchical structure consisting of finer-grained components: states, segments, and subtasks. state at timestep t, denoted st, represents the scene. The sequence of states for the i-th trajectory is Si = {s1, s2, . . . , sT }, where is the number of timesteps. An action at is taken at state st, and the corresponding sequence of actions is Ai = {a1, a2, . . . , aT }. segment σ is series of consecutive states, {st, st+1, . . . , st+k}, contributing to subgoal, with Σi = {σ1, σ2, . . . , σn} representing the segment sequence for the i-th trajectory. In each segment, the robot performs similar actions. subtask consists of segments, {σ1, σ2, . . . , σp}, to achieve specific subgoal. Finally, task is series of subtasks, {S1, S2, . . . , Sm}, required to complete the overall objective. Our Vision-Language-Action (VLA) model πθ(at st, ) predicts actions at for each state st by hierarchically decomposing tasks into subtasks. This ensures the end-effectors motion aligns with subgoal intents, enhancing the models ability to execute complex tasks through manageable subtasks. We create dataset = {Di}N i=1, where Di = {Si, Σi, Ti}. Each state st Si is labeled with its subtask. Without such labeling, chain-ofthought training is infeasible. During inference, the model generates reasoning chains, including subtasks and relevant spatial information derived from visual scenes."
        },
        {
            "title": "3 Methodology",
            "content": "In this section, we introduce our proposed framework in detail. Our EMMA-X encompasses three crucial designs: (1) Segmenting the trajectory based on the states of the gripper and the motion trajectory of the robotic arm. (2) Generating hierarchical planning including grounded task reasoning, 2D gripper positions, and 3D spatial movements. Figure 2: Construction of our hierarchical embodied dataset. We first segment the trajectory. Then, we generate the 3D spatial movement that requires to transition to the end state of the segment. Based on segments, we recognize the 2D gripper position and generate the grounded task reasoning. (3) Training the our EMMA-X based on OpenVLA with our constructed dataset."
        },
        {
            "title": "3.1 Trajectory Segmentation",
            "content": "Why Segment Trajectories? The overarching goal of our work is to enhance Vision-LanguageAction (VLA) models with grounded chain-ofthought (CoT) reasoning. We identified two key limitations in existing VLAs: 1) While existing VLAs improve task decomposition by breaking task into subtasks and solving each using CoT (Zawalski et al., 2024), their CoT reasoning relies exclusively on textual scene descriptions 1. This limits their reasoning capability for real-world scenarios. 2) They lack robust spatial reasoning abilities, essential for effective task planning and execution. To address these limitations, we propose two key solutions: Incorporating visual scene information: Beyond textual prompts, we integrate visual inputs into Gemini to enable task decomposition into subtasks and generate high-level plans grounded in both visual and textual contexts. Finegrained movement plans: We train the robot to determine where to go and how to reach potential future state necessary for completing subtask. To implement these solutions, every state must be labeled with the subtask the robot is performing. However, our experiments revealed that directly annotating each individual frame via Gemini resulted in noisy labels, likely due to insufficient contextual information. To overcome this, we segment trajectories into sequences of consecutive states where the robot performs semantically similar ac1We use scene and environment interchangeably throughout this paper. tions. This segmentation provides richer context, allowing Gemini to assign subtask labels more effectively. Additionally, segmentation facilitates finding the grippers position in future state and planning its movement. At given state st, the model predicts the movement plan required to reach the initial state of the next segment, st+k, before determining the policy at for st. Since + > t, this approach enables the model to perform look-ahead spatial reasoning, predicting the grippers position at likely future state, planning the motion trajectory, and generating at accordingly. Our Segmentation Method. As shown in Figure 2(a) and Figure 2(b), we segment observation sequences by integrating the motion trajectory and the gripper states of the end effector. To achieve this, we utilize the Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN) algorithm (McInnes et al., 2017), which effectively handles noise stemming from small fluctuations caused by imperfections in human demonstration. The flexibility of HDBSCAN enables the discovery of diverse trajectory patterns within the data. We define custom distance measurement to segment the end effectors trajectory, capturing both spatial and temporal information. Let pi = (xi, yi, zi) denote the 3D position, and ri = (rix, riy, riz) represent the 3D orientation of data point i. Additionally, let ti represent the timestamp of this data point. The distance between two data points and is given by the following expression: d(i, j) = pipj2+λrirj2+βtitj (1) where λ is weighting factor for the orientation component, and β controls the influence of the temporal distance 2. This combined distance metric d(i, j) ensures that both spatial movement and temporal separation contribute to the segmentation process. The inclusion of temporal information helps to distinguish trajectories that are spatially similar but occur at different times, while the orientation term captures changes in the end effectors rotation. Applying the HDBSCAN algorithm with this distance metric allows us to segment the trajectory into meaningful clusters that reflect distinct movement patterns. However, the motion trajectory alone does not fully capture the interaction dynamics of the end effector with the environment. To address this, we incorporate the gripper state gsi, which represents whether the gripper is in grip (closed) or loose (open) position. segmentation breakpoint occurs when the HDBSCAN algorithm detects new cluster or when the gripper state changes between consecutive data points, formally defined as gsi = gsi+1. trajectory-based This dual-segmentation approach effectively combines clustering with interaction-based segmentation, ensuring that the resulting segments capture both the motion patterns and the manipulation actions of the end effector. By integrating these two modalities, we achieve richer and more accurate segmentation of the policy. Finally, as result of the segmentation process, we have sequence of segments denoted as Σi = {σ1, σ2, . . . , σn}, where is the number of segments. Here, segment is expressed as σ = {st, st+1, . . . , st+k}, comprising states."
        },
        {
            "title": "3.2 Data Generation",
            "content": "After obtaining the segments, we generate hierarchical embodied planning data for each demonstration, as shown in Figure 2. For each segment of demonstration, we produce the 2D end-effector position and 3D movements for the completion state of the current segment. Additionally, we generate grounded reasoning for the corresponding subtasks. Why Look-ahead Spatial Reasoning? Consider the robot as delivery driver tasked with delivering package to specific destination (the goal). The driver has access to detailed high-level map of the 2We use λ as 1 and β as 0.03 for best segmentation. Figure 3: The overview of EMMA-X fine-tuned from OpenVLA using our hierarchical embodiment dataset. city, which provides potential landmarks or checkpoints (st+k) along the way to the destination. To reach the goal efficiently, the driver performs two tasks: Plans high-level route: The driver identifies likely landmarks and routes to guide them toward the destination, akin to predicting st+k and the movement plan β(st, st+k). Executes immediate driving decisions: While en route, the driver makes real-time decisions (at), such as turning left or stopping at traffic signal, informed by the planned route and the current position st. Without the ability to establish landmarks or checkpoints (future states) and plan routes based on them, the driver would rely solely on reactive decisions, leading to inefficiencies or incorrect paths. By integrating both the high-level plan and immediate feedback, the driver ensures purposeful and adaptive progress toward the goal. Following this analogy, we calculate the look-ahead gripper position and movement plan to reach there. Look-ahead Gripper Position Generation. Following (Zawalski et al., 2024), we also use OWLv2 (Minderer et al., 2024) and SAM (Kirillov et al., 2023) to detect 2D gripper position, which can be seen in Figure 2(e). The difference is that they train the model to output only the gripper position for the current input state, whereas, in our data construction process, we use the current gripper position as input and predict the gripper position for the first state of the next segment. Thus, although both approaches utilize the gripper position, our model focuses more on predicting the gripper position in future states during training, rather than identifying its position in the current state. Lets consider for every state st, we obtain gt, the gripper position of the first state of the next segment. comprise multiple segments. For the i-th trajectory, we obtain the grounded reasoning from Gemini, defined as: GRi = (cid:8)(σk, Sk, Rk) = 1, . . . , n(cid:9), where: - σk is the k-th segment, - Sk is the subtask label assigned to σk, - Rk is Geminis justification for assigning subtask Sk to σk, and - is the total number of segments in the trajectory. The prompt template can be seen in the Appendix B. Look-ahead Movement Plan Generation. As shown in Figure 2(d), we infer the 3D spatial positions corresponding to the current state and the end state of the current segment using the state policy of the robot. Specifically, we calculate the displacement between these two positions to determine the direction and step size required for the manipulator to move from the current state to the end state. Following the motion language idea in RT-H (Belkhale et al., 2024), we encode our highlevel motion plans using standardized template in Appendix E. By integrating look-ahead spatial reasoning, the model incorporates both reactive and proactive decision-making. It combines immediate context at the current state st with high-level plan that predicts likely future states st+k and the corresponding movement strategy β(st, st+k). This dual focus enables the model to align immediate actions with the overarching goal, ensuring purposeful and adaptive task execution. Please note that this data is not directly executed as the robots actions. Lets consider for every state st, we will obtain mt, the movement plan to the first state of the next segment. Grounded Chain-of-Thought Reasoning. As shown in Figure 2(f) and (g), we utilize Gemini 3 to derive the subtask corresponding to each segment, along with scene understanding and the reasoning behind the series of actions the robot needs to perform the subtask. Specifically, we take sequences of segmented images, and task descriptions as input to guide Gemini in generating the subtask and grounded reasoning for each segment. Compared to (Zawalski et al., 2024) that infer subtasks and their mapping to states solely from textual information, our approach first segments the sequence based on the robots motion trajectory and grippers state as explained in Section 3.1. After that, based on the given multimodal information, we generate the corresponding subtasks and the reasoning of each subtask. Note that each subtask can 3We used gemini-1.5-pro-latest for our data generation. The Final Dataset. The final dataset for the i-th trajectory in the training dataset is i=1 = defined as: (cid:8){(st, Ti), (mt, gt, GRt, at)}T (cid:9)N i=1, where = 1, 2, . . . , , and is the total number of timesteps in the trajectory. i=1 = {Xi, Yi}N {Di}N t="
        },
        {
            "title": "3.3 EMMA-X",
            "content": "In this section, we introduce the architecture of our proposed EMMA-X which is 7B-parameter VLA model fine-tuned from OpenVLA using our constructed hierarchical embodiment data. As shown in Figure 3, we adjust the text prompt with the current gripper position and add chain-of-thought training to enhance the ability of spatial reasoning and scene understanding before predicting the next robot action policy. During the process of predicting for real robot testing, we input the task description, the current observation image, and the 2D gripper position detected in real-time by OWLv2 (Minderer et al., 2024) and SAM (Kirillov et al., 2023). EMMAX first outputs the subtask and description of the current scene, including the spatial relationship between the target object in the image and the robotic arm, as well as the operational instructions required for the gripper to reach the goal of the current subtask. Additionally, EMMA-X also predicts the target position the gripper needs to reach after completing the sub-task, including both the 2D location in the image and the 3D spatial movements. Finally, the model outputs the next 7D robot action policy for downstream manipulation."
        },
        {
            "title": "4 Experiments",
            "content": "4."
        },
        {
            "title": "Implementation Details",
            "content": "To create the hierarchical reasoning dataset, we employed our data creation pipeline on full BridgeData-v2, which consists of approximately 60,000 trajectories paired with task instructions, resulting in an augmented dataset. To train our VLA models, we employed OpenVLA, 7B vision-language-action (VLA) model Category Task OpenVLA ECoT EMMA-X (Ours) h_Succ (%) Succ (%) h_Succ (%) Succ (%) h_Succ (%) Succ (%) SPATIAL RELATION SPATIAL RELATION SPATIAL RELATION SPATIAL RELATION Put the upper half of the carrot in the pot Put the left half of the lemon in the pan Put the blue cube on the left plate Put the blue cube on the right plate OOD OBJECT OOD OBJECT OOD OBJECT Put the banana in pot Put the blue cube on the plate Wipe the stove with towel OOD INSTRUCTION Pick up any object that is kind of vegetable OOD INSTRUCTION Put the inedible object on the towel OOD INSTRUCTION Put the edible object on the towel IN DOMAIN IN DOMAIN Open microwave Close microwave 30 30 25 70 90 70 40 0 0 50 80 10 0 20 60 50 90 50 30 0 30 60 35 35 5 35 45 20 50 15 25 15 20 0 20 10 0 40 10 30 0 0 10 0 0 80 55 60 90 85 85 90 75 40 65 100 60 20 60 90 70 70 90 70 30 0 30 100 Average 45.41 33.33 25.00 11.67 71.66 57. Table 1: Experimental results of EMMA-X and baselines on 12 real-world WidowX-250 robot manipulation tasks. ) % ( R u _ A 100 90 80 70 60 50 40 30 20 10 0 ECoT OpenVLA EMMA-X Models SPATIAL RELATION OOD OBJECT OOD INSTRUCTION h_Succ (%) Succ (%) h_Succ (%) Succ (%) h_Succ (%) Succ (%) EMMA-X 77 42 ( 35) w/o mt 32 ( 45) w/o gt w/o GRt 22 ( 55) w/o HDBSCAN 27 ( 50) 88 70 37 ( 33) 63 ( 25) 30 ( 40) 45 ( 43) 10 ( 60) 45 ( 43) 20 ( 50) 38 ( 50) 75 80 55( 25) 40 ( 35) 35 ( 45) 45 ( 30) 40 ( 40) 25 ( 50) 25 ( 55) 65 ( 10) 70 30 ( 40) 30 ( 40) 20 ( 50) 40 ( 30) OpenVLA w/ FT 38 28 ( 10) 30 23 ( 17) 65 ( 5) 70 50 50 ( 0) 40 15 ( 25) 30 0 ( 30) SPATIAL RELATION OOD OBJECT IN DOMAIN OOD INSTRUCTION"
        },
        {
            "title": "Different Categories of Tasks",
            "content": "Table 2: Models with different configurations. Figure 4: Experimental results on different categories of real-world robot tasks. built upon the Prismatic vision-language framework and pretrained on the Open X-Embodiment dataset. For autoregressive training, we tokenized our 7-dimensional action policy into discrete policy tokens, consistent with OpenVLAs methodology. We adhered to OpenVLAs training procedure and fine-tuned the base model on our augmented dataset for 3 epochs until convergence. over 10 trials, adhering to the methodology established by OpenVLA. If the robot can successfully achieve the task specified inside the prompts, it is counted as success (succ) receiving score of 1, otherwise, score of 0 is assigned. Following OpenVLA, we also introduce \"half-success\" (hsucc) metric that considers both the task goal and difficulty and assigns 0.5 score only when the half-success criteria are met (Appendix C)."
        },
        {
            "title": "4.2 Robot Setup and Metrics",
            "content": "We evaluate our approach using the 6-DoF WidowX robot arm, as introduced in the Bridge V2 paper, which represents standard benchmark for assessing generalizable robotic policies. The policy takes as input single third-person camera feed and natural language instruction, predicting endeffector velocity actions to control the robot. To rigorously test the generalization capabilities of the policies, we develop suite of challenging evaluation tasks that span multiple aspects: indomain scenarios, out-of-domain (OOD) objects, spatial relationships, and OOD instructions. All policies are assessed on identical real-world setups to ensure consistency in camera angle, lighting conditions, and background. Each task is conducted To comprehensively evaluate the performance of our proposed EMMA-X, we conduct extensive experiments across 12 different tasks on the real robot with several competitive methods. OpenVLA (Kim et al., 2024): VLA model based on large-scale VLM Prismatic7b and pre-trained on the Open-X-Embodiment dataset(Collaboration et al., 2023). OpenVLA w/ FT: For fair comparison, we finetuned the OpenVLA model on the BridgeV2 dataset for the same number of epochs following the same training setting in our method. ECoT (Zawalski et al., 2024): VLA model fine-tuned from OpenVLA on BridgeV2 dataset (Walke et al., 2023) with their generated chain-ofthought reasoning data. Figure 5: Qualitative examples of successful and failed cases with EMMA-X on real-world robot testing."
        },
        {
            "title": "4.5 Analysis",
            "content": "In this section, we compare EMMA-X with several baselines on 12 real-world robotic tasks. As shown in Table 1, our EMMA-X outperforms the strong baseline OpenVLA, with 24.17% increase in task success rate and 26.25% increase in half success rate. This demonstrates the effectiveness of our constructed hierarchical embodiment dataset. In addition, compared to ECOT, our EMMA-X shows significant gains, which can be caused by the following: 1) ECoT suffers from noisy training data, which causes hallucinations when faced with outof-domain instructions or unfamiliar objects, leading to task failures. Interestingly, for IN DOMAIN tasks, it performs worse than in other categories and compared to other models, highlighting its limited reasoning capabilities. Our grounded task reasoning approach addresses this by incorporating the segmented visual images, ensuring more accurate task understanding. 2) EMMA-X enhances spatial reasoning by predicting the 2D gripper position of the end state of the current segment and 3D spatial movements to transit to it before predicting the next robot action policy. As shown in Figure 4, we also compared the average performance across various categories of robotic tasks. Notably, our method achieved the most significant performance improvement in SPATIAL RELATION tasks, outperforming OpenVLA by 35% and ECoT by 43% in the h_Succ rate. These results strongly validate the effectiveness of our predicted 3D spatial movements. Furthermore, our method demonstrated substantial performance gains in OOD INSTRUCTION tasks, highlighting the efficacy of our grounded task reasoning. We trained several variants of EMMA-X to evaluate the roles of segmentation, look-ahead spatial reasoning, and grounded chain-of-thought (CoT) reasoning, which collectively constitute the core of EMMA-X. For this evaluation, we sampled 6 prompts across SPATIAL RELATION, OOD OBJECT, and OOD INSTRUCTION (prompts are indicated in magenta color in Section C). For each prompt, we conducted 10 rollouts under the same experimental setup as our main experiments. Segmentation Greatly Helps the Policy. To evaluate the effectiveness of our segmentation technique, we conducted an experiment where sequences were segmented solely based on the grippers (end effector) open and close positions. The results, reported in Table 2 under the w/o HDBSCAN condition, show general performance drop of 10% to 50%. Notably, spatial reasoning performance experienced the most significant decline, with drop of 50%. These findings demonstrate that the distance metric introduced in Eq. 1 is crucial for the segmentation process. The Impact of Look-ahead Spatial Reasoning. To evaluate the importance of look-ahead spatial reasoning, we conducted two experiments: 1) EMMA-X was trained without explicitly predicting the grippers position in the next segment, relying only on the predicted movement plan to reach the future gripper position of that segment (denoted as w/o gt in Table 2). This assumes that EMMA-X implicitly infers the future gripper position. 2) We trained EMMA-X to predict the future end effectors position but without rolling out movement plan to reach that position (denoted as w/o mt in Table 2). The results reveal significant performance drops in both cases (25%-35% for w/o mt and 30% to 45% for w/o gt), with more pronounced decline in spatial reasoning tasks (35% for w/o mt and 45% for w/o gt). Furthermore, the results suggest that predicting the future end effectors position is more critical, as the performance drop in the absence of 3D spatial movements to the next segment is less severe. We hypothesize that this may be due to OpenVLAs inherent spatial reasoning capabilities, which enable it to more easily transition between positions. The Importance of Grounded CoT Reasoning. Grounded chain-of-thought (CoT) reasoning is foundational element of EMMA-X. To assess its impact, we trained variant of EMMA-X without grounded reasoning, while retaining look-ahead spatial reasoning in the data. The results show marked performance drop by 43%-55%, highlighting that spatial reasoning alone is insufficient. Interestingly, the absence of grounded CoT reasoning resulted in more severe decline in spatial reasoning performance compared to models where spatial reasoning capabilities were explicitly ablated. This underscores the critical role of grounded CoT in tackling complex reasoning tasks, including spatial reasoning. Therefore, we surmise that for enhancing the generalizable policies of Vision-LanguageAction (VLA) models, it is essential to improve their broader reasoning capabilities, encompassing object recognition, color understanding, abstraction, commonsense knowledge, and more. Fine-tuning does not Improve OpenVLA. We sought to find whether fine-tuning OpenVLA on BridgeV2 could match the performance of EMMAX. The results, shown in Table 2, reveal that OpenVLAs performance degrades by 5%-25% after fine-tuning with the worst performance observed for OOD INSTRUCTION. We hypothesize that this decline is due to overfitting, as BridgeV2 is already part of OpenVLAs pre-training dataset. Qualitative Analysis on Real-world Robot Task. To qualitatively evaluate the effectiveness of our spatial and task reasoning in guiding robotic actions, we present two successful trajectories and one failed trajectory in Figure 5. From the left case, we find that the predicted gripper position corresponds to the end state of the subtask reaching for the blue cube. The 3D movement provides detailed path, clearly directed toward the blue cube. We also include failed trajectory where the hotdog\" is mistakenly identified as pineapple\". This error propagates, impacting the prediction of the grippers future position and preventing it from accurately picking up the hot dog."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduce EMMA-X, 7B-parameter embodied multimodal action model designed to enhance spatial reasoning and task planning for robotic policy generation. We construct hierarchical embodiment dataset enriched with grounded reasoning, including 2D gripper positions and 3D spatial movements. Furthermore, our proposed trajectory segmentation strategy reduces hallucination in task reasoning by grounding reasoning in visual images. The experimental results demonstrate the effectiveness of EMMA-X, showing significant improvements over existing baselines in tasks requiring long-horizon spatial reasoning."
        },
        {
            "title": "Limitations",
            "content": "While EMMA-X shows promising performance, its latency remains higher compared to OpenVLA. This increased inference time primarily results from the additional tokens generated during the reasoning process. Specifically, EMMA-X generates approximately 10 times more tokens than OpenVLA. To mitigate this, potential strategy is to predict all policies within segment and only regenerate the policy if the predicted policy deviates significantly from the expected movement plan. Another limitation is the generalization capability of EMMA-X. Scaling the training process to incorporate larger subset of the OXE dataset could enhance the models ability to handle broader range of tasks and robotic systems. Lastly, using SAM for detecting the gripper position can lead to inaccuracies. These errors may occur when the gripper is partially occluded by objects or positioned outside the image frame. Employing more robust model for detecting and segmenting the robot hand could address these challenges and improve reliability."
        },
        {
            "title": "References",
            "content": "Suneel Belkhale, Tianli Ding, Ted Xiao, Pierre Sermanet, Quon Vuong, Jonathan Tompson, Yevgen Chebotar, Debidatta Dwibedi, and Dorsa Sadigh. 2024. Rt-h: Action hierarchies using language. Preprint, arXiv:2403.01823. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alexander Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. 2023a. Rt-2: Visionlanguage-action models transfer web knowledge to robotic control. Preprint, arXiv:2307.15818. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. 2023b. Rt-1: Robotics transformer for real-world control at scale. Preprint, arXiv:2212.06817. Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. The International Journal of Robotics Research, page 02783649241273668. Open X-Embodiment Collaboration, Abby ONeill, Abdul Rehman, Abhinav Gupta, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Andrey Kolobov, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Felipe Vieira Frujeri, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guangwen Yang, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik Christensen, Hiroki Furuta, Homanga Bharadhwaj, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad AbouChakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jay Vakil, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, João Silvério, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi \"Jim\" Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil Joshi, Niko Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag Sanketi, Patrick \"Tree\" Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martin-Martin, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shubham Tulsiani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vikash Kumar, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiangyu Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Xu Liangwei, Xuanlin Li, Yansong Pang, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Matthias Minderer, Alexey Gritsenko, and Neil Houlsby. 2024. Scaling open-vocabulary object detection. Advances in Neural Information Processing Systems, 36. Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Charles Xu, Jianlan Luo, Tobias Kreiman, You Liang Tan, Pannag Sanketi, Quan Vuong, Ted Xiao, Dorsa Sadigh, Chelsea Finn, and Sergey Levine. 2024. Octo: An open-source generalist robot policy. In Proceedings of Robotics: Science and Systems, Delft, Netherlands. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR. Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. 2023. family of highly capable multimodal models. arXiv preprint arXiv:2312.11805. Gemini: Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Homer Rich Walke, Kevin Black, Tony Z. Zhao, Quan Vuong, Chongyi Zheng, Philippe Hansen-Estruch, Andre Wang He, Vivek Myers, Moo Jin Kim, Max Du, Abraham Lee, Kuan Fang, Chelsea Finn, and Sergey Levine. 2023. Bridgedata v2: dataset for robot learning at scale. In 7th Annual Conference on Robot Learning. Michał Zawalski, William Chen, Karl Pertsch, Oier Mees, Chelsea Finn, and Sergey Levine. 2024. Robotic control via embodied chain-of-thought reasoning. arXiv preprint arXiv:2407.08693. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. 2023. Sigmoid loss for language In 2023 IEEE/CVF Internaimage pre-training. tional Conference on Computer Vision (ICCV), pages 1194111952. IEEE. Yonatan Bisk, Yongqiang Dou, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Fu, and Zipeng Lin. 2023. Open X-Embodiment: Robotic learning datasets and RT-X models. https://arxiv.org/abs/2310. 08864. Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. 2023. Palm-e: An embodied multimodal language model. Preprint, arXiv:2303.03378. Frederik Ebert, Yanlai Yang, Karl Schmeckpeper, Bernadette Bucher, Georgios Georgakis, Kostas Daniilidis, Chelsea Finn, and Sergey Levine. 2021. Bridge data: Boosting generalization of robotic skills with cross-domain datasets. Preprint, arXiv:2109.13396. Huy Ha, Pete Florence, and Shuran Song. 2023. Scaling up and distilling down: Language-guided robot skill acquisition. Preprint, arXiv:2307.14535. Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. 2024. OpenVLA: An open-source vision-language-action model. In 8th Annual Conference on Robot Learning. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. 2023. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 40154026. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. In Advances in Neural Information Processing Systems. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. 2023. Code as policies: Language model programs for embodied control. Preprint, arXiv:2209.07753. Yueen Ma, Zixing Song, Yuzheng Zhuang, Jianye Hao, and Irwin King. 2024. survey on vision-languagearXiv preprint action models for embodied ai. arXiv:2405.14093. Leland McInnes, John Healy, and S. Astels. 2017. hdbscan: Hierarchical density based clustering. J. Open Source Softw., 2:205."
        },
        {
            "title": "A Related Work",
            "content": "Generalist Robot Policies. Recent progress in robotics has shifted focus towards developing multi-task \"generalist\" robot policies capable of handling wide variety of tasks across diverse robot embodiments (Brohan et al., 2023b,a; Ebert et al., 2021; Walke et al., 2023; Collaboration et al., 2023; Octo Model Team et al., 2024; Kim et al., 2024). For example, Octo (Octo Model Team et al., 2024) utilizes compositional design to train generalist policy capable of handling various tasks directly while supporting fine-tuning for new inputs and action spaces. Similarly, OpenVLA (Kim et al., 2024) adopts streamlined end-to-end approach, fine-tuning vision-language models (VLMs) to produce robot actions by treating these actions as tokens within the language models vocabulary. These studies highlight the potential of training robot policies on large and diverse datasets as promising strategy for enhancing their performance. Vision-Language-Action Models. number of recent works have explored fine-tuning large pretrained VLMs for predicting robot actions (Collaboration et al., 2023; Brohan et al., 2023a; Kim et al., 2024; Octo Model Team et al., 2024; Driess et al., 2023) Such models are often referred to as vision-language-action models (VLAs), as they fuse robot actions directly into VLM backbones and treating these actions as tokens within the language model vocabulary. This approach provides simple yet scalable alternative, with models such as RT-2 (Brohan et al., 2023b), RT-2-X (Collaboration et al., 2023), and OpenVLA (Kim et al., 2024) demonstrating state-of-the-art performance and impressive generalization across diverse objects and environments. RT-2 integrates Internet-scale vision-language data with robotic trajectory data, while RT-2-X scales this further with 55B-parameter policy trained on the Open X-Embodiment dataset (Collaboration et al., 2023). In contrast, OpenVLA integrates robust open VLM backbone with an enriched robot pretraining dataset. Despite these advancements, current VLAs underutilize some of the most valuable features of their underlying language and vision-language models, specifically, their capacity to reason through the steps needed to solve complex tasks. Reasoning for Robotics. Prompting large language models (LLMs) to \"think step-by-step\" (Kojima et al., 2022) when solving problems can significantly enhance their performance. Similar techniques have been explored in the context of high-level task planning for robotics (Liang et al., 2023; Ha et al., 2023). Expanding on this, Zawalski et al. (2024) introduced ECoT, method that trains VLA policy to autoregressively generate chain-of-thought (CoT) reasoning. ECoT combines highand low-level reasoning with actionable steps, aligning these to an agents environment based on input instructions and observations. While this equips VLAs with better understanding of the current situation and task, it falls short in two key areas: long-horizon spatial reasoning for robot movement and visual grounding when augmenting reasoning data with Gemini. Building on these limitations, we build EMMA-X designed to enhance spatial reasoning and task planning in robotic policy generation. EMMA-X integrates grounded chain-of-thought reasoning to predict future states and transitions, enabling robots to develop more informed and effective action policies."
        },
        {
            "title": "B Prompts and Sample Outputs",
            "content": "Template for prompting Gemini to generate subtasks and reasoning. The robot successfully completed task specified by the instruction : '{ instruction } '. Here is list of segments of images for the robot hand to perform the task specified by the instruction . Based on the sequence of segments provided in sequential order , pay attention to the robot hand and identify which subtask it is performing in each segment . Provide the justification for why the subtask should be done based on the environment . You can assign the same subtask to multiple segments . You should output in dictionary format : { segment_number : [ subtask , reason for justification ], ...}. The segment_number starts from 1 and must be an integer . The output dictionary keys correspond to each segment , and the output dictionary length should be the same as the number of segments . { segment_count } Segment 1: images in segment 1 Segment 2: images in segment 2 ... Segment n: images in segment We list the Gemini output of the example in Figure 1 as follows: Here ' breakdown of the robot ' subtasks in each segment : ``` json { 1: [\" Positioning over pot cover \", \" The robot arm needs to move to the correct location above the pot cover to be able to grasp it . This involves adjusting its position and orientation .\"] , 2: [\" Grasping the pot cover \", \" The robot is closing its gripper around the pot cover 's handle to securely pick it up . This requires precision to ensure stable grip .\"] , 3: [\" Lifting and moving the pot cover \", \" The robot has grasped the pot cover and is now lifting it up and moving it towards the silver pot ."
        },
        {
            "title": "It must avoid",
            "content": "collision with other objects in the environment .\"] , 4: [\" Positioning and placing the cover on the pot \", \" The robot carefully positions the cover above the silver pot and lowers it into place . This requires aligning the cover with the pot ' opening and releasing the grip gently to avoid dropping or damaging the cover or pot .\"] }"
        },
        {
            "title": "C Half Success Criteria",
            "content": "The half-success criteria to for the metric h_success are as follows. The prompts in magenta color are used to report the results in Table 2. Half Success Criteria Robot successfully grasps the banana. Robot moves towards or grasps the correct target object. Robot moves towards the correct vegetable. Robot moves towards and correctly selects the left half of the lemon. Robot grasps the blue cube. Robot grasps the blue cube and moves it towards the left plate. Robot grasps the blue cube and moves it towards the right plate. Robot moves towards the correct inedible object. Robot moves towards the correct edible object. Robot touches the towel but does not wipe the stove. Robot partially opens the microwave door. Robot partially closes the microwave door."
        },
        {
            "title": "D Segmentation Statistics",
            "content": "Average Number of frames per segment: 5.5 Average Number of segments per trajectory: 6.9 Average Number of frames per trajectory: 32."
        },
        {
            "title": "E Motion Plan Template",
            "content": "Translational Movements: move (left/right) steps, move (forward/backward) steps, move (upward/- downward) steps. Rotational Movements: pitch (upward/downward) α degrees, yaw (left/right) β degrees, roll (clockwise/counterclockwise) γ degrees. Gripper Action: (open/close) gripper. Pseudo Code for Training EMMA-X and Running Inference Notations defined in 2.2 Algorithm 1 Data Generation, Training, and Inference Process For each sample in Embodied Dataset, we have: : Number of time frames = {st}T = {gt}T : Task instruction in natural language format t=1: Images at each time frame t, where st is the image at time frame t=1: Gripper poses at each time frame (position, orientation, and open-or-close state) Training Process: 1: while not converged do for each sample do 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: Mframessegments dual_segmentation(G) Msegmentssubtasks, reasons Gemini(S, G, ) for each time frame {1, 2, . . . , } do Mapping from frame to segment Mapping from segment to subtasks Get segment for time σt Mframessegments(t) Get grounded reasoning from Gemini GRt Msegmentssubtasks, reasons(σt) Get 2D gripper position Using SAM model gt SAM(st) gend SAM(Send) Get 2D gripper position at end of current segment mt Template(gt gend) Translational change to movement plan in natural language Perform supervised fine-tuning prediction Model(T , st, gt, GRt, gend, mt, at) (SFT) with label: (T , st, gt, GRt, gend, mt, at) end for end for 13: 14: end while Inference Process: 1: while Task not completed do 2: gt SAM(st) GRt, gend, mt, at EMMA-X(T , st, gt) Control the robot using at, to get new st, gt 3: 4: 5: end while"
        }
    ],
    "affiliations": []
}