{
    "paper_title": "Constrained Diffusion Implicit Models",
    "authors": [
        "Vivek Jayaram",
        "Ira Kemelmacher-Shlizerman",
        "Steven M. Seitz",
        "John Thickstun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper describes an efficient algorithm for solving noisy linear inverse problems using pretrained diffusion models. Extending the paradigm of denoising diffusion implicit models (DDIM), we propose constrained diffusion implicit models (CDIM) that modify the diffusion updates to enforce a constraint upon the final output. For noiseless inverse problems, CDIM exactly satisfies the constraints; in the noisy case, we generalize CDIM to satisfy an exact constraint on the residual distribution of the noise. Experiments across a variety of tasks and metrics show strong performance of CDIM, with analogous inference acceleration to unconstrained DDIM: 10 to 50 times faster than previous conditional diffusion methods. We demonstrate the versatility of our approach on many problems including super-resolution, denoising, inpainting, deblurring, and 3D point cloud reconstruction."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 1 ] . [ 1 9 5 3 0 0 . 1 1 4 2 : r Under review as conference paper at ICLR"
        },
        {
            "title": "CONSTRAINED DIFFUSION IMPLICIT MODELS",
            "content": "Vivek Jayaram1 Ira Kemelmacher-Shlizerman1 Steven M. Seitz1 John Thickstun2 1University of Washington 2Cornell University"
        },
        {
            "title": "ABSTRACT",
            "content": "This paper describes an efficient algorithm for solving noisy linear inverse problems using pretrained diffusion models. Extending the paradigm of denoising diffusion implicit models (DDIM), we propose constrained diffusion implicit models (CDIM) that modify the diffusion updates to enforce constraint upon the final output. For noiseless inverse problems, CDIM exactly satisfies the constraints; in the noisy case, we generalize CDIM to satisfy an exact constraint on the residual distribution of the noise. Experiments across variety of tasks and metrics show strong performance of CDIM, with analogous inference acceleration to unconstrained DDIM: 10 to 50 times faster than previous conditional diffusion methods. We demonstrate the versatility of our approach on many problems including superresolution, denoising, inpainting, deblurring, and 3D point cloud reconstruction."
        },
        {
            "title": "INTRODUCTION",
            "content": "Denoising diffusion probabilistic models (DDPMs) have recently emerged as powerful generative models capable of capturing complex data distributions (Ho et al., 2020). Their success has spurred interest in applying them to solve inverse problems, which are fundamental in fields such as computer vision, medical imaging, and signal processing (Tropp & Wright, 2010; Hansen, 2010). Inverse problems require recovering unknown signals from (possibly noisy) observations. Linear inverse problems, where the observations consist of linear measurements of signal, encompass tasks like super-resolution, inpainting, and deblurring. Existing methods that apply diffusion models to linear inverse problems face several limitations. First, many previous works require task specific training or fine-tuning (Li et al., 2022; Xie et al., 2023). Second, methods that use pretrained diffusion models often introduce many additional network evaluations during inference (Dou & Song, 2023; Zhu et al., 2024). Finally, popular diffusion inverse methods such as diffusion posterior sampling (Chung et al., 2022a) fail to exactly recover the input observations. In this work, we propose constrained diffusion implicit models (CDIM), extending the inference acceleration of denoising diffusion implicit models (Song et al., 2021) to efficiently solve noisy linear inverse problems using single pretrained diffusion model. Our method modifies the diffusion updates to enforce constraints on the final output, integrating measurement constraints directly into the diffusion process. In the noiseless case, this approach achieves exact recovery of the observations. For noisy observations, we generalize our method by optimizing the Kullback-Leibler (KL) divergence between the empirical residual distribution and known noise distribution, effectively handling general noise models beyond the Gaussian assumption. Code can be found at our website1. Our contributions are as follows: Accelerated inference: we accelerate inference, reducing the number of model evaluations and wall-clock time by an order of magnitude10 to 50 times faster than previous conditional diffusion methodswhile maintaining comparable quality. Exact recovery of noiseless observations: we can find solutions that exactly match the noiseless observation. General noise models: we extend the CDIM framework to accommodate arbitrary observational noise distributions through distributional divergence minimization, demonstrating effectiveness given non-Gaussian noise, such as Poisson noise. 1https://grail.cs.washington.edu/projects/cdim/ 1 Under review as conference paper at ICLR 2025 Figure 1: We show several applications of our method including image colorization, denoising, inpainting, and sparse recovery. We highlight the fact that we can handle general noise distributions, such as Poisson noise, and that our method runs in as little as 3 seconds."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Diffusion methods have revolutionized generative modeling, building upon early work in nonequilibrium thermodynamics (SohlDickstein et al., 2015) and implicit models (Mohamed & Lakshminarayanan, 2017). Diffusion models were first proposed in DDPM (Ho et al., 2020), which shared framework analogous to score-based models using Langevin dynamics Song & Ermon (2019). Subsequent innovations focused on improving sampling efficiency, with denoising diffusion implicit models (Song et al., 2021) introducing method to speed up inference with implicit modeling. Further advancements in accelerating the sampling process emerged through the application of stochastic differential equations (Song et al., 2020) and the development of numerical ODE solvers, exemplified by approaches like PNDM (Liu et al., 2021), significantly enhancing the practical utility of diffusion models in various generative tasks. (DDIMs) Figure 2: The inference speed and average LPIPS image quality score (inverted) averaged across multiple inverse tasks on the FFHQ dataset. The family of CDIM methods (top left corner) simultaneously achieves strong generation strong quality and fast inference, compared to other inverse solvers. Applying diffusion models to inverse problems has been an active research area. DPS uses alternating projection steps to guide the diffusion process (Chung et al., 2022a). Methods like DDNM (Wang et al., 2022), DDRM (Kawar et al., 2022), SNIPS (Kawar et al., 2021), and PiGDM (Song et al., 2023a) use linear algebraic approaches and singular value decompositions. Techniques such as DMPS (Meng & Kabashima, 2022), FPS (Dou & Song, 2023), LGD (Song et al., 2023b), DPMC (Zhu et al., 2024), and MCG (Cardoso et al., 2023) focus on likelihood approximation for improved sampling. Guidance mechanisms were incorporated through classifier gradients (Dhariwal & Nichol, 2021), data consistency enforcement (Chung et al., 2022b), and low-frequency feature matching Choi et al. (2021). Other approaches use projection (Boys et al., 2023; Chung et al., 2024) or optimization (Chan et al., 2016; Wahlberg et al., 2012). 2 Under review as conference paper at ICLR"
        },
        {
            "title": "3 BACKGROUND",
            "content": "We work in the context of DDPM (Ho et al., 2020), which models data distribution q(x0) by modeling sequence = 1, . . . , of smoothed distributions defined by q(xtx0) = (xt; αtx0, (1 αt)I). (1) The degree of smoothing is controlled by monotone decreasing noise schedule αt with α0 = 1 (no noise) and αT = 0 (pure Gaussian noise).2 The idea is to model reverse process pθ(xt1xt) that that incrementally removes the noise in xt such that pθ(xT ) = (xT ; 0, I) and p(x0) approximates the data distribution, where p(x0) is the marginal distribution of outputs from the reverse process: pθ(x0) = (cid:90) pθ(xT ) (cid:89) t=1 pθ(xt1xt) dx1:T . (2) 1 αtϵ, where x0 is sample from the data distribution and Given noisy samples xt = αtx0 + ϵ (0, I), diffusion model ϵθ(xt, t) is trained to predict ϵ: (cid:2)ϵ ϵθ (xt, t) 2(cid:3) . min θ xt,ϵ (3) To parameterize the reverse process pθ(xt1xt), DDIM (Song et al., 2021) exploits the Tweedie formula (Efron, 2011) for the posterior mean of noisy observation: [x0xt] = (cid:0)xt 1 αt 1 αtxt log q(xt)(cid:1) . (4) Using the denoising model ϵ(xt, t) as plug-in estimate of the score function xt log q(xt) (Vincent, 2011) we define the Tweedie estimate of the posterior mean: ˆx0 (cid:0)xt 1 αt 1 αtϵθ(xt, t)(cid:1) [x0xt] . And we use this estimator to define DDIM forward process xt1 = fθ(xt) defined by fθ(xt) = αt1 ˆx0 + (cid:112)1 αt1 (cid:18) xt αt ˆx0 (cid:19) . 1 αt (5) (6) Unlike DDPM, the forward process defined by Equation (6) is deterministic; the value pθ(x0) is entirely determined by xT (0, I) thus making DDIM an implicit model. With slight modification of the DDIM update, we are able to take larger denoising steps and accelerate inference. Given δ 1, we define an accelerated denoising process δ θ (xt) = αtδ ˆx0 + (cid:112)1 αtδ (cid:18) xt αt ˆx0 (cid:19) 1 αt . (7) Using this process, inference is completed in just /δ steps, albeit with degraded quality of the resulting sample x0 as δ becomes large."
        },
        {
            "title": "4 METHODS",
            "content": "We are interested in solving linear inverse problems of the form = Ax, where Rd is linear measurement of Rn and Rdn describes the form of our measurements. For example, if {0, 1}nn is binary mask (which is the case for, e.g., in-painting or sparse recovery problems) then describes partial observation of x. We seek an estimate ˆx that is consistent with our observations: in the noiseless case, Aˆx = y. More generally, we seek to recover robust estimate of ˆx when the observations have been corrupted by noise. Given noise distribution r, we seek to minimize DKL(ˆr r), where ˆr is the empirical distribution of residuals, e.g., Aˆx Rd, between noisy observations and our estimates Aˆx. 2We define αt using the DDIM convention (Song et al., 2021); our αt corresponds to αt in Ho et al. (2020). 3 Under review as conference paper at ICLR 2025 We rely on diffusion model pθ(x) to identify an estimate ˆx that is both consistent with the observed measurements and likely according to the model. In Section 4.1, we propose modification of the DDIM inference procedure to efficiently optimize the Tweedie estimates of ˆx0 to satisfy Aˆx0 = during the diffusion process, resulting in consistent and likely final result x0. In Section 4.2 we extend this optimization-based inference procedure to account for noise in the observations y. In Section 4.3 we describe an early-stopping heuristic to avoid overfitting to noisy observations, which further reduces the cost of inference. Finally, in Section 4.4 we show how to set the step sizes for these optimization-based methods."
        },
        {
            "title": "4.1 OPTIMIZING ˆx0 TO MATCH THE OBSERVATIONS",
            "content": "For linear measurements A, the Tweedie formula for ˆx0 (and the corresponding plugin-estimate Equation (5)) extends to formula for the expected observations: [yxt] = AE [x0xt] Aˆx0. (8) For noiseless observations y, we propose modification of the DDIM updates Equation (6) to find xt1 that satisfies the constraint Aˆx0 = y. I.e., at each time step t, we force the Tweedie estimate of the posterior mean of q(yxt) to match the observed measurements y: arg min xt1 xt1 fθ(xt) subject to Aˆx0 = y. (9) We can interpret Equation (9) as projection of the DDIM update fθ(xt) onto the set of values xt1 that satisfy the constraint Aˆx0 = y. The full inference procedure is analogous to projected gradient descent, whereby we alternately take step fθ(xt) determined by the diffusion model, and then project back onto the constraint Aˆx0 = y. We implement the projection step itself via gradient descent, initialized from x(0) t1 = fθ(xt) and computing t1 = x(k1) x(k) t1 + ηxt1y Aˆx02. (10) As approaches 0, ˆx0 converges to x0 and Aˆx02 becomes simple convex quadratic, which can be minimized to arbitrary accuracy by taking sufficiently many gradient steps. This allows us to guarantee exact recovery of the observations = Ax0 in the recovered inverse x0. For close to , we face two conceptual challenges in optimizing Equation (9). First, for large t, no value xt will satisfy Aˆx0 = and therefore the optimization is infeasible. Second, the estimate of the score function xt log q(xt) using ϵθ(xt, t) may be inaccurate; we risk overfitting to bad plugin estimate ˆx0. We illustrate both these claims by considering the Tweedie estimator Equation (5) in the case = . In this case, xt (0, I) is independent of x0 and therefore E[x0xt] = E[x0], the mean of the data distribution q(x0). Unless AE[x0] = y, the optimization is infeasible when = . Furthermore, we observe that when = , the plug-in estimator ˆx0 is not independent of xt and ˆx0 = E[x0]. This is indicative of error in the plug-in estimator, especially at high noise levels. In light of these observations, we replace Equation (9) with Lagrangian arg min xt1 xt1 fθ(xt)2 + λy Aˆx02. (11) We can interpret Equation (11) as relaxation of Equation (9); the regularization by λy Aˆx02 is achieved implicitly by early stopping after = steps of gradient descent. In contrast to projection, this Lagrangian objective is robust to both (1) the possible infeasibility of ˆy0(xt) = and (2) overfitting the measurements based on an inaccurate Tweedie plug-in estimator. 4.2 OPTIMIZING THE KL DIVERGENCE OF RESIDUALS For noisy inverse problems, imposing hard constraint Aˆx0 = will overfit to the noise σ in the observations, as illustrated by Figure 4. Previous work accounts for noise using implicit regularization, by incompletely optimizing the objective Aˆx0 = (Chung et al., 2022a). In contrast, we 4 Under review as conference paper at ICLR 2025 Input: Box inpainting with bimodal noise Ground Truth DPS Ours - Discrete KL Figure 3: Results on the box inpainting task with bimodal noise distribution. By optimizing the discrete KL divergence, we can reconstruct the face with much higher fidelity than existing methods like DPS. propose to exactly optimize the Kullback-Leibler (KL) divergence between the empirical distribution of residuals R(Aˆx0, y) and known, i.i.d. noise distribution r: arg min x xt2 subject to DKL(R(Aˆx0, y) r) = 0. (12) In Algorithm 1, we show how to optimize constraint on categorical KL divergences to match arbitrary distributions of discretized residuals. We also provide convenient objective for optimizing the empirical distribution of continuous residuals to match common noise patterns, including Gaussian and Poisson noise. Algorithm 1 Constrained Diffusion Implicit Models with KL Constraints 1: xT (0, I) 2: for = T, δ, . . . , 1 do (cid:16) xt 3: 4: 5: (cid:17) 1 αtϵθ(xt,t) αtδ xtδ for = 0, . . . , do ˆx0 1 xtδ xtδ + η xtδ DKL(R(Aˆx0, y) r) 1 αtδ ϵθ(xtδ, δ)) (xtδ αt αtδ + 1 αtδϵθ(xt, t) end for 6: 7: 8: end for 9: return ˆx0 Unconditional DDIM Step Projection Additive Noise. The general additive noise model is defined by = Ax + σ Rd, where σ rd. By discretizing the distribution of residuals into buckets, we can compute categorical KL divergence between observed residuals and the discrete approximation of rB of r: DKL(R(Aˆx0, y) rL) = (cid:88) b=1 rB(b) log (cid:18) rB(b) R(Aˆx0, y)B (cid:19) . (13) In Figure 3 we show results on the box inpainting task when the observation has been corrupted with bimodal noise: p(σi = 0.75) = p(σi = 0.75) = 0.5 for = 1, . . . , n, where image pixels are normalized values xi [1, 1]. We optimize the residuals using the discrete KL divergence and show that our result faithfully reconstructs the ground truth with high fidelity while filling in the missing section. Gaussian Noise. Additive Gaussian noise is defined by σ (0, σ2I), in which case the residuals R(Ax, y) Ax (0, σ2I) are i.i.d. with distribution (0, σ2). The empirical mean and variance of the residuals are ˆµ = 1 (cid:88) i=1 R(Aˆx, y)i, ˆσ2 = 1 (cid:88) i=1 (R(Aˆx, y)i ˆµ)2 . (14) Under review as conference paper at ICLR 2025 (a) (b) (c) Figure 4: Results on 50% noisy inpainting task. (a) is the noisy partial observation. (b) is generated by algorithm 2 without early stopping, showing that we can exactly match the observation even when the observation is out of distribution. (c) is generated by algorithm 2 with early stopping. Using the analytical formula for KL divergence between two Gaussians (Kingma & Welling, 2014), we can match the empirical mean and variance of the residuals to by enforcing DKL(R(Aˆx0, y) r) = log (cid:19) (cid:18) σ2 ˆσ2 + ˆσ2 + ˆµ2 2σ2 1 2 = 0. (15) Poisson Noise. Possion noise is non-additive noise defined by sy Poisson(sAx), where is interpreted as discrete integer pixel values. The scaling factor 1 controls the degree of Poisson noise. Poisson noise is not identically distributed across y; the variance increases with the scale of each observation. To remedy this, we consider the Pearson residuals (Pregibon, 1981): R(Aˆx0, y) = λ(y Aˆx0) λˆx . (16) These residuals are identically distributed; moreover, they are approximately normal (0, 1) (Pierce & Schafer, 1986). We can therefore optimize the KL divergence between Pearson residuals and standard normal using Equation (15) to solve inverse problems with Poisson noise. Although the Pearson residuals closely follow the standard normal distribution for positive values of ˆx0, this breaks down for values of ˆx0 close to zero, and extreme noise levels s. In practice we find the Gaussian assumption to be valid for natural images corrupted by as much noise as 0.025. In Figure 1 we show an example of denoising an image corrupted by Poisson noise with = 0.05. 4.3 NOISE-AGNOSTIC CONSTRAINTS In many practical situations, we will not know the precise distribution of noise in the observations. For these cases, we propose noise-agnostic version of CDIM, assuming only that the noise is zeromean with variance Var(r). The idea is to directly minimize the squared error of the residuals, with early stopping to avoid overfitting to the noise once Var(r) exceeds the empirical variance of the residuals. In experiments, we find that this noise-agnostic algorithm performs similarly to the noise-aware versions described in Section 4.2. Moreover, the noise-agnostic algorithm is more efficient: by stopping early with enforcement of the constraint, it avoids excess evaluations of the model during the final steps of the diffusion process. The complete process is shown in Algorithm 2. Algorithm 2 Constrained Diffusion Implicit Models with L2 Constraints and Early Stopping (cid:16) xt 1 αtϵθ(xt,t) (cid:17) + 1 αtδϵθ(xt, t) Unconditional DDIM Step 1: xT (0, I) 2: for = T, δ.., 1 do 3: 4: 5: αtδ xtδ for = 0, .., do ˆx0 1 αtδ if ˆσ2 < Var(r) then (xtδ αt break end if xtδ xtδ + η xtδ 6: 7: 8: 9: 10: 11: end for end for 1 αtδ ϵθ(xtδ, δ)) Early Stopping Projection 1 R(Aˆx, y)2 2 6 Under review as conference paper at ICLR η 1/ Aˆx0 Input, 50% inpainting Figure 5: Comparison of different step size schedules on 50% inpainting task. We choose challenging task with = 10, = 10, σ2 = 0.15 and use algorithm 2. With enough steps, all three can produce reasonable results on L2 optimization, but η 1/E (cid:13) (cid:13) (cid:13) is the most stable and converges the fastest. (cid:13)xtδ (cid:13)xtδ (cid:13)xtδ η 1/ (cid:13) (cid:13) (cid:13) η 1/E (cid:13) (cid:13) (cid:13)"
        },
        {
            "title": "4.4 CHOICE OF STEP SIZE η",
            "content": "An important hyperparameter of these algorithms is the step size η. DPS sets η proportional to 1/y Aˆx0 Chung et al. (2022a). We find that this fails to converge for KL optimization, and also produces unstable results for L2 optimization when is small. One option is to set η inversely proportional to the magnitude of the gradient xtδ at every single optimization step. Although this is the easiest solution, it can result in unstable oscillations and slower convergence. Instead, we pro- (cid:13) (cid:13) pose to set η inversely proportional to ExXtrain (cid:13)xtδ (cid:13), common optimization heuristic (Amari, 1998; Pascanu & Bengio, 2014). In Appendix we describe how to compute this expectation. In Figure 5 we show qualitatively what happens with different η schedules. We find that for specific optimization objective and task, the magnitude of the gradient xtδ is highly similar across data points, datasets, and model architectures. While it is difficult to reason analytically about these magnitudes due to backpropagation through the network ϵ(xt, t), we empirically demonstrate this observation in Appendix A. This suggests that learned step size based (cid:13) on ExXtrain (cid:13) generalizes as good learning rate for unseen data. For all experiments, we estimate these magnitudes from FFHQ training data. (cid:13) (cid:13)xtδ"
        },
        {
            "title": "5 RESULTS AND EXPERIMENTS",
            "content": "We conduct experiments to understand the efficiency and quality of CDIM across various tasks and datasets. In Section 5.1, we present quantitative comparisons to state-of-the-art approaches, followed by ablation studies in Section 5.2 examining inference speed and hyperparameters. In Section 5.3 we explore two novel applications of diffusion models for inverse problems."
        },
        {
            "title": "5.1 NUMERICAL RESULTS ON FFHQ AND IMAGENET",
            "content": "We evaluate CDIM on the FFHQ-1k (Karras et al., 2019) and ImageNet-1k (Russakovsky et al., 2015) validation sets, both widely used benchmarks for assessing diffusion methods for inverse problems. Each dataset contains 256 256 RGB images scaled to the range [0, 1]. The tasks include 4x super-resolution, box inpainting, Gaussian deblur, and random inpainting. Details of each task are included in the appendix. For all tasks, we apply zero-centered Gaussian observational noise with σ = 0.05. To ensure fair comparisons, we use identical pre-trained diffusion models used in the baseline methods: for FFHQ we use the network from Chung et al. (2022a) and for ImageNet we use the network from Dhariwal & Nichol (2021). We use multiple metrics to measure the quality of the generated outputs: Frechet Inception Distance (FID) (Heusel et al., 2018) and Learned Perceptual Image Patch Similarity (LPIPS) (Zhang et al., 2018). All experiments are carried out on single Nvidia A100 GPU. In Table 1 we compare CDIM with several other inverse solvers using the FID and LPIPS metrics on the FFHQ dataset. We present results using both our KL divergence optimization method (Algorithm 1) and our L2 optimization method (Algorithm 2) with early stopping. For these experiments, we present results with = 50 and = 3 as well as = 25 and = 1 labeled as fast. For ImageNet results please see Appendix B.3. 7 Under review as conference paper at ICLR 2025 Table 1: Quantitative results (FID, LPIPS) of our model and existing models on various linear inverse problems on FFHQ 256 256-1k validation dataset. (Lower is better). The best result is in bold and the second best is underlined. FFHQ Super Res Inpainting (box) Gaussian Deblur Inpainting (random) Runtime (seconds) Methods Ours - KL fast Ours - L2 fast Ours - KL Ours - L2 FPS-SMC DPS DDRM MCG FID 36.76 33.87 34.71 31.54 26.62 39.35 62.15 87.64 PnP-ADMM 66.52 96.72 Score-SDE 110.6 ADMM-TV LPIPS 0.283 0.276 0.269 0.269 0.210 0.214 0.294 0.520 0.353 0.563 0.428 FID 35.15 27.51 30.88 26.09 26.51 33.12 42.93 40.11 151.9 60.06 68.94 LPIPS 0.2239 0.1872 0.1934 0.196 0.150 0.168 0.204 0.309 0.406 0.331 0.322 FID 37.44 34.18 35.93 29.68 29.97 44.05 74.92 101.2 90.42 109.0 186.7 LPIPS 0.308 0.276 0.296 0.252 0.253 0.257 0.332 0.340 0.441 0.403 0. FID 35.73 29.67 31.09 28.52 33.10 21.19 69.71 29.26 123.6 76.54 181.5 LPIPS 0.259 0.243 0.249 0.240 0.275 0.212 0.587 0.286 0.692 0.612 0.463 2.57 2.4 10.2 9.0 116.90 70.42 2.0 73.2 3.595 32.39 - 5.2 ABLATION STUDIES Number of Inference Steps. CDIM offers the flexibility to trade off quality for faster inference time on demand. We investigate how generation quality changes as we vary the total computational budget during inference. Recall that the total number of network passes during inference is (K + 1), where is the number of denoising steps and is the number of optimization steps per denoising step. We use the random inpainting task on the FFHQ dataset with the setup described in the previous section. For this experiment we use KL optimization (Algorithm 1). The total network forward passes are varied from 200 to 20, and we show qualitative results. Notably, CDIM yields high quality samples with as few as 50 total inference steps, with quality degradations after that. Input Random Inpainting 20 Steps = 10 = 1 Figure 6: We reduce the total number of inference steps (K + 1) and visualize the results. There is almost no visible degradation until less than 50 total steps. 50 Steps = 25 = 1 200 Steps = 50 = 3 10 Steps = 5 = 1 vs trade-off. We consider the optimal balance between and when the total number of inference steps (K + 1) is fixed. Using the random inpainting task on the FFHQ dataset with the previously described setup, we set (K+1) = 200 and analyze how PSNR, FID, and LPIPS change based on the chosen and values. Results are plotted in Figure 7. FID results consistently favor the maximum number of denoising steps with minimal optimization steps K. This is because FID evaluates overall distribution similarity rather than per-sample fidelity, and thus is not penalized by lower reconstruction-observation fidelity. In contrast, PSNR and LPIPS, which measure per-sample fidelity with respect to reference image, achieve optimal results with balanced mix of denoising and optimization steps. 5.3 ADDITIONAL APPLICATIONS Time-Travel Rephotography In Figure 1 we showcase an application of time-travel rephotography Luo et al. (2021). Antique cameras lack red light sensitivity, exaggerating wrinkles by filtering out skin subsurface scatter which occurs mostly in the red channel. To address this, we input the observed image into the blue color channel and use the pretrained FFHQ model with Algorithm 2 to 8 Under review as conference paper at ICLR 2025 Figure 7: We fix the total number of inference steps at 200 and evaluate different combinations of and K. FID always prefers more denoising steps T, while LPIPS and PSNR are best at mix of and steps. project the face into the space of modern images. We further emphasize the power of our approach; Luo et al. (2021) trained specialized model for this task while we are able to use pretrained model without modification. Sparse Point Cloud Reconstruction For this task, 20 different images from scene in The Grand Budapest Hotel scene were entered into Colmap (Schonberger & Frahm, 2016) to generate sparse 3D point cloud. Note that the sparse nature of the Colmap point cloud means that projections of the point cloud will have roughly 90% of the pixels missing. Furthermore, the observations often contain significant amounts of non-Gaussian noise due to false correspondences. We can formulate this as noisy inpainting problem and use our method to fill in the missing pixels for desired viewpoint. To address the errors in the point cloud, we use Algorithm 2 along with variance threshold that adequately captures the imprecise nature of the point cloud. We showcase the results in Figure 8. (a) (b) Figure 8: Using noisy inpainting to tackle sparse point cloud reconstruction. (a) Shows sparse point cloud projected to desired camera angle. (b) Shows the result after our method is used for noisy inpainting."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper we introduced CDIM, new approach for solving noisy linear inverse problems with pretrained diffusion models. This is achieved by exploiting the structure of the DDIM inference procedure. By projecting the DDIM updates, such that Tweedie estimates of the denoised image ˆx0 match the linear constraints, we are able to enforce constraints without making out-of-distribution edits to the noised iterates xt. We note that our method cannot handle non-linear constraints, including latent diffusion, because for non-linear function h, [h(x0)] = h(E [x0]. Therefore, unlike the linear case of Equation (8), we cannot extend Tweedies estimate of the posterior mean of x0 to an estimate of the posterior mean of non-linear observations h(x0). However, for linear constraints, our method generates high quality images with faster inference than previous methods, creating new point on the Pareto-frontier of quality vs. efficiency for linear inverse problems. 9 Under review as conference paper at ICLR"
        },
        {
            "title": "7 ACKNOWLEDGEMENTS",
            "content": "This work was supported by the UW Reality Lab, Lenovo, Meta, Google, OPPO, and Amazon."
        },
        {
            "title": "REFERENCES",
            "content": "Shun-ichi Amari. Natural Gradient Works Efficiently in Learning. Neural Computation, 10(2):251 276, 02 1998. ISSN 0899-7667. doi: 10.1162/089976698300017746. URL https://doi. org/10.1162/089976698300017746. 4.4 Benjamin Boys, Mark Girolami, Jakiw Pidstrigach, Sebastian Reich, Alan Mosca, and O. Deniz Akyildiz. Tweedie moment projected diffusions for inverse problems, 2023. 2 Gabriel Cardoso, Yazid Janati El Idrissi, Sylvain Le Corff, and Eric Moulines. Monte carlo guided diffusion for bayesian linear inverse problems, 2023. URL https://arxiv.org/abs/ 2308.07983. 2 Stanley H. Chan, Xiran Wang, and Omar A. Elgendy. Plug-and-play admm for image restoration: Fixed point convergence and applications, 2016. URL https://arxiv.org/abs/1605. 01710. 2 Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. Ilvr: Conditioning method for denoising diffusion probabilistic models, 2021. 2 Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong Chul Ye. Diffusion posterior sampling for general noisy inverse problems. In The Eleventh International Conference on Learning Representations, 2022a. 1, 2, 4.2, 4.4, 5.1 Hyungjin Chung, Byeongsu Sim, and Jong Chul Ye. Come-closer-diffuse-faster: Accelerating conditional diffusion models for inverse problems through stochastic contraction, 2022b. 2 Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, and Jong Chul Ye. Improving diffusion models for inverse problems using manifold constraints, 2024. Prafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis, 2021. 2, 5.1 Zehao Dou and Yang Song. Diffusion posterior sampling for linear inverse problem solving: filtering perspective. In The Twelfth International Conference on Learning Representations, 2023. 1, 2, B.2 Bradley Efron. Tweedies formula and selection bias. Journal of the American Statistical Association, 106(496):16021614, 2011. 3 Per Christian Hansen. Discrete inverse problems: insight and algorithms. SIAM, 2010. 1 Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium, 2018. URL https://arxiv.org/abs/1706.08500. 5. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 2020. 1, 2, 3, 2 Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks, 2019. URL https://arxiv.org/abs/1812.04948. 5.1 Bahjat Kawar, Gregory Vaksman, and Michael Elad. Snips: Solving noisy inverse problems stochastically, 2021. Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. Denoising diffusion restoration models, 2022. 2 Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014. 4.2 10 Under review as conference paper at ICLR Haoying Li, Yifan Yang, Meng Chang, Shiqi Chen, Huajun Feng, Zhihai Xu, Qi Li, and Yueting Chen. Srdiff: Single image super-resolution with diffusion probabilistic models. Neurocomputing, 479:4759, 2022. 1 Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on manifolds. In International Conference on Learning Representations, 2021. 2 Xuan Luo, Xuaner (Cecilia) Zhang, Paul Yoo, Ricardo Martin-Brualla, Jason Lawrence, and Steven M. Seitz. Time-travel rephotography. ACM Transactions on Graphics, 40(6):112, DeISSN 1557-7368. doi: 10.1145/3478513.3480485. URL http://dx.doi. cember 2021. org/10.1145/3478513.3480485. 5.3 Xiangming Meng and Yoshiyuki Kabashima. Diffusion model based posterior sampling for noisy linear inverse problems. arXiv preprint arXiv:2211.12343, 2022. Shakir Mohamed and Balaji Lakshminarayanan. Learning in implicit generative models. 2017. 2 Razvan Pascanu and Yoshua Bengio. Revisiting natural gradient for deep networks, 2014. URL https://arxiv.org/abs/1301.3584. 4.4 Donald A. Pierce and Daniel W. Schafer. Residuals in generalized linear models. Journal of the ISSN 01621459, 1537274X. URL American Statistical Association, 81(396):977986, 1986. http://www.jstor.org/stable/2289071. 4.2 Daryl Pregibon. Logistic regression diagnostics. Annals of Statistics, 9:705724, 1981. URL https://api.semanticscholar.org/CorpusID:121371059. 4.2 Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li FeiFei. Imagenet large scale visual recognition challenge, 2015. URL https://arxiv.org/ abs/1409.0575. 5.1 Johannes L. Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 41044113, 2016. doi: 10.1109/CVPR.2016.445. 5.3 Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, 2015. 2 Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021. 1, 2, 3, Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz. Pseudoinverse-guided diffusion models for inverse problems. In International Conference on Learning Representations, 2023a. URL https://openreview.net/pdf?id=9_gsMA8MRKQ. 2 Jiaming Song, Qinsheng Zhang, Hongxu Yin, Morteza Mardani, Ming-Yu Liu, Jan Kautz, Yongxin Chen, and Arash Vahdat. Loss-guided diffusion models for plug-and-play controllable generation. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 3248332498. PMLR, 2329 Jul 2023b. URL https://proceedings.mlr.press/v202/song23k.html. 2 Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. 2 Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben In InternaPoole. Score-based generative modeling through stochastic differential equations. tional Conference on Learning Representations, 2020. Joel A. Tropp and Stephen J. Wright. Computational methods for sparse solution of linear inverse problems. Proceedings of the IEEE, 98(6):948958, 2010. doi: 10.1109/JPROC.2010.2044010. 1 11 Under review as conference paper at ICLR 2025 Pascal Vincent. connection between score matching and denoising autoencoders. Neural computation, 2011. 3 Bo Wahlberg, Stephen Boyd, Mariette Annergren, and Yang Wang. An admm algorithm for class of total variation regularized estimation problems, 2012. URL https://arxiv.org/abs/ 1203.1828. Yinhuai Wang, Jiwen Yu, and Jian Zhang. Zero-shot image restoration using denoising diffusion null-space model. In The Eleventh International Conference on Learning Representations, 2022. 2 Shaoan Xie, Zhifei Zhang, Zhe Lin, Tobias Hinz, and Kun Zhang. Smartbrush: Text and shape guided object inpainting with diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2242822437, 2023. 1 Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric, 2018. URL https://arxiv.org/abs/ 1801.03924. 5.1 Yaxuan Zhu, Zehao Dou, Haoxin Zheng, Yasi Zhang, Ying Nian Wu, and Ruiqi Gao. Think twice before you act: Improving inverse problem solving with mcmc, 2024. URL https://arxiv. org/abs/2409.08551. 1, 2 12 Under review as conference paper at ICLR Figure 9: plot of xtδ for two models and datasets, ImageNet and FFHQ. In each task 100 images were used. First, note the variance in single task/model, shown by the error bars, is small. Second, note that the variance across the two tasks/models is also small. CALCULATIONG (cid:13) (cid:13)xtδ (cid:13) (cid:13) (cid:13)xtδ To calculate our expected gradient magnitude, we first start with simple gradient normalization: η 1/ (cid:13) (cid:13) (cid:13), which normalizes our step size by the gradient magnitude on the fly at every optimization step. We run the full CDIM algorithm on the target task with the desired number of steps and on images from the training set. We calculate and store each gradient magnitude (cid:13) (cid:13) (cid:13)xtδ (cid:13) during the optimization process at every step. Finally, we average the empirical gradient magnitudes at each step δ to find (cid:13) (cid:13) (cid:13) across data points and inner optimization steps k. In practice we find that very few images are required to calculate stable value for the expected gradient magnitude. In all experiments the value was calculated by running an initial optimization on 10 images from the training set. (cid:13)xtδ"
        },
        {
            "title": "B ADDITIONAL EXPERIMENTAL DETAILS",
            "content": "B.1 TASK DETAILS We describe additional details for each inverse task used in our experiments. Super Resolution Images are downsampled to 64 64 using bicubic downsampling with factor of 4. Box Inpainting random box of size 128 128 is chosen uniformly within the image. Those pixels are masked out affected all three of the RGB channels. Gaussian Deblur Gaussian Kernel of size 61x61 and intensity 3 is applied to the entire image. Random Inpainting Each pixel the RGB channels is masked out with probability 92% affecting all three of 50% Inpainting In various figures, we showcase 50% inpainting task where the top half 13 Under review as conference paper at ICLR 2025 of an image is masked out. This task is more challenging than box inpainting and can better illustrate differences between results. B.2 MEASURING RUNTIME To measure wall-clock runtime, we used single A100 and ran all the inverse problems (superresolution, box inpainting, gaussian deblur, random inpainting) on the FFHQ dataset. We only consider the runtime of the algorithm, without considering the python initialization time, model loading, or image io. For each task, we measured the runtime on 10 images and averaged the result to produce the final result. We note that the baseline runtimes are taken from Dou & Song (2023), where only the box inpainting task was considered. The runtime does not vary much between tasks when using CDIM, so we report our average runtime across tasks as fair comparison metric. B."
        },
        {
            "title": "IMAGENET RESULTS",
            "content": "In table 2 we report FID and LPIPS for the ImageNet dataset. Table 2: Quantitative results (FID, LPIPS) of our model and existing models on various linear inverse problems on the Imagenet 256 256-1k validation dataset. (Lower is better) Imagenet Methods CDIM - KL fast CDIM - L2 fast CDIM - KL CDIM - L2 FPS-SMC DPS DDRM MCG PnP-ADMM Score-SDE ADMM-TV Super Resolution FID 59.10 53.70 47.77 47.45 47.30 50.66 59.57 144.5 97.27 170.7 130.9 LPIPS 0.398 0.378 0.347 0.339 0.316 0.337 0.339 0.637 0.433 0.701 0. Inpainting (box) Gaussian Deblur Inpainting (random) FID 58.75 52.00 48.26 50.31 33.24 38.82 45.95 39.74 78.24 54.07 87.69 LPIPS 0.311 0.267 0.2348 0.251 0.212 0.262 0.245 0.330 0.367 0.354 0.319 FID 73.74 56.10 57.72 38.69 54.21 62.72 63.02 95.04 100.6 120.3 155. LPIPS 0.480 0.393 0.390 0.347 0.403 0.444 0.427 0.550 0.519 0.667 0.588 FID 53.91 51.96 45.86 46.20 42.77 35.87 114.9 39.19 114.7 127.1 189.3 LPIPS 0.364 0.370 0.331 0.332 0.328 0.303 0.665 0.414 0.677 0.659 0.510 B.4 EXTENDED RESULTS 14 Under review as conference paper at ICLR Figure 10: FFHQ Super-resolution extended results Figure 11: FFHQ Gaussian deblur extended results 15 Under review as conference paper at ICLR 2025 Figure 12: FFHQ random inpainting extended results Figure 13: ImageNet Gaussian deblur extended results 16 Under review as conference paper at ICLR 2025 Figure 14: ImageNet random inpainting extended results Figure 15: ImageNet box inpainting extended results 17 Under review as conference paper at ICLR (a) (b) Figure 16: Results on inpainting 50% of an image on LSUN Churches dataset."
        }
    ],
    "affiliations": [
        "University of Washington",
        "Cornell University"
    ]
}