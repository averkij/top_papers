{
    "paper_title": "AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition",
    "authors": [
        "Zichuan Lin",
        "Yicheng Liu",
        "Yang Yang",
        "Lvfang Tao",
        "Deheng Ye"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-Language Models (VLMs) have achieved remarkable success in visual question answering tasks, but their reliance on large numbers of visual tokens introduces significant computational overhead. While existing efficient VLM approaches reduce visual tokens through fixed-ratio compression, they operate passively and lack the ability to adapt to varying task requirements. This motivates a fundamental question: Can VLMs autonomously determine the minimum number of visual tokens required for each sample? Inspired by human active vision mechanisms, we introduce AdaptVision, an efficient VLM paradigm that enables adaptive visual token acquisition through a coarse-to-fine approach. Our model initially processes compressed visual tokens from low-resolution images and selectively acquires additional visual information by invoking a bounding box tool to crop key regions when necessary. We train AdaptVision using a reinforcement learning framework that carefully balances accuracy and efficiency. Central to our approach is Decoupled Turn Policy Optimization (DTPO), which decouples the learning objective into two components: (1) tool learning, which optimizes correct tool utilization, and (2) accuracy improvement, which refines the generated responses to improve answer correctness. Based on this formulation, we further decouple advantage estimation by computing separate advantages for tokens associated with each objective. This formulation enables more effective optimization for AdaptVision compared to vanilla GRPO. Comprehensive experiments across multiple VQA benchmarks demonstrate that AdaptVision achieves superior performance while consuming substantially fewer visual tokens than state-of-the-art efficient VLM methods."
        },
        {
            "title": "Start",
            "content": "AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition Zichuan Lin* Yicheng Liu* Yang Yang"
        },
        {
            "title": "Lvfang Tao Deheng Ye",
            "content": "Tencent AI Lab Codes and models: github.com/adaptvision/adaptvision 5 2 0 2 3 ] . [ 1 4 9 7 3 0 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Vision-Language Models (VLMs) have achieved remarkable success in visual question answering tasks, but their reliance on large numbers of visual tokens introduces significant computational overhead. While existing efficient VLM approaches reduce visual tokens through fixed-ratio compression, they operate passively and lack the ability to adapt to varying task requirements. This motivates fundamental question: Can VLMs autonomously determine the minimum number of visual tokens required for each sample? Inspired by human active vision mechanisms, we introduce AdaptVision, an efficient VLM paradigm that enables adaptive visual token acquisition through coarseto-fine approach. Our model initially processes compressed visual tokens from low-resolution images and selectively acquires additional visual information by invoking bounding box tool to crop key regions when necessary. We train AdaptVision using reinforcement learning framework that carefully balances accuracy and efficiency. Central to our approach is Decoupled Turn Policy Optimization (DTPO), which decouples the learning objective into two components: (1) tool learning, which optimizes correct tool utilization, and (2) accuracy improvement, which refines the generated responses to improve answer correctness. Based on this formulation, we further decouple advantage estimation by computing separate advantages for tokens associated with each objective. This formulation enables more effective optimization for AdaptVision compared to vanilla GRPO. Comprehensive experiments across multiple VQA benchmarks demonstrate that AdaptVision achieves superior performance while consuming substantially fewer visual tokens than state-of-the-art efficient VLM methods. 1. Introduction Recently, Vision-Language Models (VLMs) [2, 4, 15] have achieved significant breakthroughs in general visual question answering (VQA) and diverse practical applications by *Equal contribution Figure 1. Our key motivations and AdaptVision performance and efficiency. Top: Coarse-to-fine. Human visual attention mechanisms first guide the search for question-relevant regions in images, which are then subjected to detailed analysis. Down: AdaptVision achieves superior performance with significantly fewer visual tokens than previous efficient VLM methods. projecting and adapting visual tokens into large language model (LLM) space [1, 2, 31, 42]. However, the promising performance of VLMs largely relies on the large amount of vision tokens, inevitably introducing huge memory and computational overhead when compared to LLMs, particularly for high-resolution images. For instance, 2048 1024 image yields 2,678 vision tokens in Qwen2.5-VL [3]. Therefore, it is crucial to avoid the excessive consumption of visual tokens. Numerous studies have explored visual token compression to enhance VLM efficiency [5, 9, 13, 28, 32, 36, 37, 40]. Existing works can be categorized into two main research directions. The first prunes or merges fixed number of visual tokens based on predetermined thresholds, according to the importance and similarity of vision tokens [5, 36, 40]. The second dynamically processes distinct samples, where the system adaptively switches between using 100% vision tokens for OCR-related tasks and 25% vision tokens for simpler tasks by selectively employing quarter-resolution images [37]. However, existing efficient VLM paradigms and methods are largely passive, as they can only reduce the number of vision tokens by predefined ratios. This leads to natural question: Can VLMs adaptively determine the minimum number of vision tokens for each sample according to different scenarios? Cognitive neuroscience reveals that our visual system operates through an active, sequential, and adaptive process known as active vision [6, 11]. It first captures coarse, lowspatial-frequency information to grasp the gist of scene, then directs attention to salient regions for detailed analysis [22]. This coarse-to-fine processing mechanism enables humans to efficiently parse complex visual inputs with minimal cognitive load. Fig. 1 provides an illustrative example. The cognitive strategy of active vision is operationalized in recent VLMs through the thinking-with-images paradigm, such as invoking tools to zoom and crop specific regions [14, 41] to advance fine-grained visual understanding. We argue that this active reasoning capability can be effectively applied to the critical task of visual token reduction, allowing the model to decide how few visual tokens are sufficient. In this paper, we propose AdaptVision, framework that leverages visual tool use to determine the minimum visual token usage while maintaining high accuracy. Our model initially processes compressed visual tokens from low-resolution images and adaptively acquires additional visual tokens by invoking bounding box tool to crop key regions from the original high-resolution image when necessary. The model is trained via reinforcement learning to balance accuracy and efficiency. However, training this dual-objective policy with standard RL algorithms like Group Relative Policy Optimization (GRPO) [25] presents two key challenges: (1) Ambiguous credit assignment: Vanilla GRPO assigns single sequence-level reward to all generated tokens, failing to distinguish the contribution of the decision to request additional visual tokens from that of generating the final answer; (2) Imbalanced optimization: Since vanilla GRPO normalizes all tokens uniformly in sequence, it introduces an imbalance: compared to 1-turn direct-answer sequences, 2-turn tool-invoking sequences receive imbalanced gradient signals, causing the latter to be under-optimized. To address these challenges, we propose Decoupled Turn Policy Optimization (DTPO). First, to mitigate optimization imbalance, we decouple the learning objective into two components based on the functional roles of response tokens: (1) tool learning, which encourages correct tool use, and (2) accuracy improvement, which refines the generated responses to improve answer correctness. Each objective is normalized separately to balance learning signals across different tokens. Second, to enable precise credit assignment, we decouple advantage estimation by computing distinct advantages for tokens associated with each objective, encouraging more efficient tool exploration. Experiments on multiple VQA benchmarks demonstrate that AdaptVision achieves superior performance with significantly fewer visual tokens than state-of-the-art efficient VLM methods, as shown in Fig. 1. In summary, our contributions are: 1. We introduce AdaptVision, VLM framework that leverages visual tool use for dynamic token reduction. 2. We propose Decoupled Turn Policy Optimization (DTPO) algorithm alongside tailored reward function to enable the effective training of AdaptVision. 3. Extensive evaluation on multiple VQA benchmarks shows that AdaptVision achieves superior performance with substantially reduced visual token consumption compared to existing efficient VLM methods. 2. Related work Vision Language Model with Reasoning. Recent advances in reasoning LLMs such as OpenAIs o1 [12] and DeepSeek R1 [8] have accelerated the use of RL to enhance reasoning capabilities. This trend has extended to VLMs [18, 23, 26, 30], where most work focuses on highlevel semantic reasoning like tool use or chain-of-thought explanation. related direction explores active perception, equipping VLMs with fine-grained control mechanisms [10, 29, 33]. Recent systems such as DeepEyes [41] and Mini-o3 [14] support operations like zoom and crop, improving performance on detailed visual tasks. While these methods showcase the power of active visual reasoning for enhancing answer accuracy, how to apply this thinking with images paradigm to the goal of computational efficiency, specifically for visual token compression, remains less explored. Our method enables the VLM to autonomously determine the minimum number of visual tokens required for given task, thereby achieving efficient inference while maintaining performance. Efficient VLM with Vision Token Compression. Reducing VLM computational cost by vision token compression has become popular research topic. Existing methods rely on predefined rules or metrics to compress tokens. For instance, FastV [5] prunes fixed 50% of tokens based on attention scores after the second layer. PyramidDrop [35] proposes progressive token compression to reduce information loss. Other works leverage cross-modal relevance for token selection, such as SparseVLM [40] and VisionZip [36], which retain semantically relevant visual tokens. key limitation of these methods is their dependence on fixed compression ratio, which lacks adaptability across tasks. VisionThink [37] uses RL to decide whether to use low-resolution or the original image, offering limited adaptability but still restricting the model to coarseIn contrast, our approach enables the grained decisions. Figure 2. FrameWork of AdaptVision. AdaptVision first processes 1/4-resolution image. The model then decides whether to answer directly or invoke the bounding box tool to crop high-resolution region for further analysis before generating the final answer. VLM to learn coarse-to-fine ability and adaptively determine the minimum number of visual tokens for each task. DKL(πθπref) = πref(oiq) πθ(oiq) log πref(oiq) πθ(oiq) 1, (4) 3. Preliminary 3.1. Reinforcement Learning for LLMs Recent studies [8, 12] have demonstrated RL effectively enhances the reasoning capabilities of large language models (LLMs). Recently, Group Relative Policy Optimization (GRPO) [25] has been widely used in LLM reasoning. Given question x, GRPO generates distinct responses {oi}G i=1 with sequence length Ni from the current policy πθold and obtains group of rewards {Ri}G i=1. GRPO optimizes the policy model πθ by maximizing the following objective: JGRPO(θ) = Ex,oi (cid:32) (cid:34)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 Ni(cid:88)"
        },
        {
            "title": "1\nNi",
            "content": "Li,t(θ) βDKL [πθ(x) πref(x)] t=1 (1) (cid:33)(cid:35) where Li,t(θ) denotes the token-level loss given by: Li,t(θ) = min (cid:32) πθ(oi,t x, oi,<t) πθold(oi,t x, oi,<t) Ai,t, where DKL is the KL-divergence measure. ϵ and β are hyperparameters. The advantage estimate Ai,t is computed using group of rewards {Ri}G i=1. 3.2. Vision Language Models The VLM architectures generally consist of three components: visual encoder, modality projector, and LLM. commonly used approach for the visual encoder is to employ pre-trained image encoder like CLIP-VIT [24] that converts input images into visual tokens. The modality projector adjusts the size of these visual tokens to match the embedding size of LLM and to achieve semantic alignment, enabling the LLM to process visual data effectively. The LLM then integrates the aligned visual and textual information to generate responses. Existing works have revealed that the computational complexity of VLM is strongly influenced by the sequence length [36], where the sequence length is defined as = nsys + nimg + nquestion. In typical VLM tasks, the number of vision tokens nimg is often much larger than the other two, sometimes by factor of 20. Therefore, reducing the number of vision tokens is the key for improving the efficiency of VLMs. clip (cid:18) πθ(oi,t x, oi,<t) πθold(oi,t x, oi,<t) (cid:19) (cid:33) , 1 ϵ, 1 + ϵ Ai,t , (2) 4. Methodology 4.1. Framework Ai,t = Ri mean({Ri}G std({Ri}G i=1) i=1) , (3) We aim to develop an efficient VLM that minimizes visual token usage while maintaining high performance by token usage to 25% of the original. adaptively acquiring visual information based on question and image complexity. As shown in Fig. 2, our method first processes low-resolution image (Ilow), cutting visual The VLM then autonomously decides whether to answer directly or crop key regions (Icrop) from the high-resolution image for more detail. As shown in Fig. 2, given low-resolution image Ilow and the question q, the model can output direct answer or invoke tool call using <tool call>[x1, y1, x2, y2]</tool call> to obtain Icrop before reasoning further and answering. However, the VLM lacks mechanism for deciding which response style is most appropriate for given input = {xsys, Ilow, q}. We therefore frame this as reinforcement learning problem to optimize the following policy: πθ(ox) = (cid:40) πθ(o1:N x), πθ(o1:T x) πθ(oT +1:N x, o1:T , Icrop), direct answer, tool call, (5) where is the length of the entire generated sequence. In tool-call responses, o1:T represents tool tokens in the first turn, and oT +1:N represents answer tokens in the second turn, as illustrated in Fig. 2. Let nlow and ncrop be the number of visual tokens for Ilow and Icrop. 1tool is the indicator for tool-call responses. Thus, the total number of visual tokens for each sample is: nimg = nlow + 1toolncrop. Therefore, to minimize the number of visual tokens nimg, we aim to learn policy πθ(o x) that can: (1) invoke the tool to request additional visual tokens only when necessary, and (2) acquire the minimal additional visual information Icrop required to answer the question correctly. 4.2. Reward Design To learn policy that can optimally balance efficiency and accuracy, we design reward function that consists of two (1) an Outcome Reward Roc that reflects answer parts: correctness, response format adherence and tool call frequency; (2) Tool Reward Rtool that incentivizes effective tool exploration to enhance coarse-to-fine visual reasoning. The reward function of AdaptVision is: compliance with all formatting requirements; otherwise, the (3) Balance reward Rbal: To prevent overreward is 0. reliance on tool calls, we introduce balance reward. We apply 0.1 penalty to correct answers that invoke tool calls. Additionally, to discourage lucky guesses [37], we impose 0.1 penalty on direct answers when the probability of correct response from low-resolution images is low, thereby encouraging appropriate tool usage. The design of this balance reward is as follows: Rbal = (cid:40) 0.1 I(r < θ) I(Racc = 1), 0.1 I(Racc = 1), direct answer, tool call, = Cdirect Cdirect + Ctool , (7) (8) where Cdirect and Ctool represent the count of correct answers for direct-answer and tool-call responses within is the indicator function. We set group, respectively. θ = 0.2 in this paper. Tool Reward Rtool. When the model requests additional visual information via tool call, the cropped region Icrop must be both informative for answering and minimal in area to reduce visual token usage. To achieve this balance, we introduce tool reward Rtool, formulated as follows: Rtool = Rcrop α Rarea, (9) where Rcrop evaluates the correctness of the cropped region, Rarea denotes its relative area ratio, and α is hyperparameter balancing the two terms. In this paper we set α = 2. (1) The crop reward Rcrop is determined by GPT4o, which evaluates whether the cropped region Icrop contains relevant information to answer the question, returning 1 if correct and 0 otherwise. (2) The relative area reward Rarea penalizes oversized bounding boxes that contain irrelevant regions, formulated as follows: Rarea = I(Racc = 1) I(Rcrop = 1) clip (cid:19) 1, 0, 1 , (cid:18) ra µa = Roc + Rtool. (6) ra = (x2 x1) (y2 y1) Hlow Wlow , µa = µarea(G(a)), (10) Outcome Reward Roc. The outcome reward is the sum (1) Accuracy reward Racc: Since of three components. VQA answers are typically open-ended, we use an LLM as judge to assign binary reward (1 for correct, 0 for incorrect) for answer correctness. (2) Format reward Rf orm: To maintain instruction-following capability, we enforce formatting requirements: reasoning in <think> tags, answers in <answer> tags, and tool calls in <tool call> tags with valid JSON. The format reward is 0.5 for full where Hlow and Wlow denote the height and width of Ilow, and ra is the area ratio of the cropped region. Here, G(a) denotes group of responses that yield both correct answers (Racc = 1) and correct cropped regions (Rcrop = 1), and µarea(G(a)) is the mean measurement of ra within such group. This area penalty incentivizes the model to select the smallest possible region that still ensures correctness, thereby minimizing visual token usage while maintaining performance. 4.3. Efficient Learning via Decoupled Turn Policy"
        },
        {
            "title": "Optimization",
            "content": "Based on our reward design, we initially employ GRPO [25] for training. We aim to train VLM that (1) achieves high answering accuracy and (2) minimizes the number of visual tokens used. However, training such dual-objective policy with GRPO presents two key challenges. Ambiguous credit assignment Vanilla GRPO provides single, sequence-level reward to all generated tokens, failing to distinguish between the contributions of two distinct types of actions the decision to request additional visual tokens and the generation of the final answer. This ambiguity limits effective exploitation and exploration during policy learning. For instance, when the VLM correctly generates bounding boxes while producing an incorrect answer, the model still receives positive reward for the answer tokens. This may steer the model towards suboptimal optimization direction. As we will show in the experiments, when training with GRPO, the model initially favors direct answers but then rapidly collapses to excessive tool call, resulting in an unstable training process. Imbalanced optimization As defined in Eq. 5, the policy model generates either one-turn or two-turn responses for each sample. Depending on their functional roles, the generated tokens can be categorized into two types: Tool Tokens and Answer Tokens, as shown in Fig. 2. Accordingly, the original GRPO objective in Eq. 2 can be decomposed into two components that separately optimize the tool and answer tokens:"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) t="
        },
        {
            "title": "1\nNi",
            "content": "Ni(cid:88) t=1 Li,t(θ) = (cid:88) t="
        },
        {
            "title": "1\nNi",
            "content": "Ti(cid:88) Li,t(θ) t=1 (cid:123)(cid:122) Tool Token (cid:125) + (cid:88) t="
        },
        {
            "title": "1\nG",
            "content": "(cid:124)"
        },
        {
            "title": "1\nNi",
            "content": "Ni(cid:88) Li,t(θ) , (11) t=Ti+1 (cid:123)(cid:122) Answer Token (cid:125) where Ti denotes the number of tool tokens generated in the first turn, and Ni Ti represents the number of answer tokens in the second turn. If the model answers directly without tool calls, Ti is 0. closer examination of Eq. 11 reveals an inherent optimization imbalance. In two-turn sequences that invoke tools, the gradient contributions from tool tokens are suppressed by the normalization factors 1 Ni and 1 , causing tool tokens to be under-optimized compared to answer tokens. To address these challenges, we propose Decoupled Turn Policy Optimization (DTPO). First, we decouple the policy Figure 3. Demonstration of vanilla GRPO and our DTPO. Our DTPO (1) decomposes the policy loss by turns to separately optimize tool and answer tokens, and (2) computes distinct advantages for tool and outcome rewards, enabling balanced optimization and precise credit assignment. loss by turns and normalize the contributions of tool and answer tokens separately. This adjustment effectively resolves the under-optimization problem of tool tokens. (cid:34) JDTPO(θ) = Ex,oi 1 (cid:80)G i=1 Ti (cid:124) (cid:88) Ti(cid:88) Li,t(θ) t=1 i=1 (cid:123)(cid:122) Tool Token (cid:125) (cid:88) Ni(cid:88) (cid:35) . Li,t(θ) (12) + 1 i=1(Ni Ti) (cid:80)G (cid:124) t=Ti+1 i=1 (cid:123)(cid:122) Answer Token (cid:125) Second, to enable precise credit assignment, DTPO decouples the advantage estimation by computing distinct advantages for tool and answer tokens, rather than using single advantage for the entire sequence. Specifically, we compute the advantage for the t-th token as follows: (cid:40) Ai,t = A(i) A(i) oc + λ A(i) oc + λ A(i) R(i) A(i) tool = tool, tool I(1 Ti), tool mean({R(i) tool}G std({R(i) tool}G i=1) oc mean({R(i) oc }G std({R(i) i=1) oc }G i=1) R(i) i=1) A(i) oc = direct answer, tool call, , (13) , (14) tool and A(i) where A(i) oc are advantage estimates computed using tool reward and outcome reward respectively, and λ is hyperparameter that trade-offs two advantages. We set λ = 0.3 in this paper. Fig. 3 compares the design of GRPO and DTPO. Table 1. Performance comparison with previous efficient VLM methods. Vanilla denotes the Qwen2.5-VL-7B-Instruct model. DownSample uses 1/4-resolution image as input to the Vanilla model. #Token indicates the visual token consumption ratio relative to the vanilla model across all benchmarks. Avg. denotes the average performance relative to the vanilla model on all benchmarks. Method Vanilla Down-Sample SparseVLM FastV VisionZip VisionThink VisionThink AdaptVision w/o DTPO AdaptVision ChartQA OCRBench DocVQA MME MMVet RealWorldQA POPE MathVista MathVerse test test val test test test test testmini testmini #Token Avg. 79.8 100% 62.9 78.8% 73.2 91.7% 72.6 91.0% 71.5 89.6% 73.6 92.2% 73.88 92.6% 73.74 92.4% 75.92 95.1% 81.5 100% 68.8 84.4% 75.6 92.7% 75.8 93.0% 70.5 86.5% 76.8 94.2% 80.8 99.1% 75.9 93.1% 76.9 94.4% Retain 100% Visual Tokens Across All Benchmarks 95.1 100% 2316 100% 61.6 100% 68.6 100% 86.7 100% Retain 25% Visual Tokens Across All Benchmarks 94.3 99.1% 2270 98.0% 54.5 88.5% 68.8 100.3% 82.8 95.5% Retain 50% Visual Tokens Across All Benchmarks 66.8 70.2% 93.6 98.4% 93.8 98.6% 92.9 97.7% 93.7 98.5% 93.1 97.9% 92.6 97.4% 2282 98.5% 2308 99.6% 2209 95.4% 51.5 83.6% 52.8 85.7% 57.0 92.5% 68.4 99.7% 68.8 100.3% 68.6 100% Dynamic Methods 2320 61.7 100.2% 100.2% 2392 60.18 103.3% 97.7% 2354 61.28 101.6% 99.5% 2379 64.8 102.7% 105.2% 65.6 95.6% 68.37 99.7% 65.7 95.8% 67.32 98.1% 85.5 98.6% 84.7 97.7% 86.3 99.5% 86.3 99.5% 86.69 100.0% 86.8 100.1% 86.8 100.1% 68.2 100% 62.2 91.2% 66.6 97.6% 63.7 93.4% 64.1 93.9% 62.2 91.2% 65.7 96.3% 64.4 94.4% 65.9 96.6% 46.3 100% 43.1 93.1% 45.1 97.4% 45.0 97.2% 45.1 97.4% 42.5 91.8% 45.68 98.7% 44.2 95.5% 42.3 91.4% 100% 100% 25% 92.1% 50% 92.2% 50% 95.8% 50% 94.8% 52% 95.8% 99% 98.4% 57% 96.7% 33% 97.9% 5. Experiment 5.1. Evaluation Setup experiments on several general VQA conduct We benchmarks, including ChartQA [20], OCRBench [17], DocVQA [21], MME [7], MMVet [38], RealWorldQA [34], POPE [16], MathVista [19], MathVerse [39]. AdaptVision is based on Qwen2.5-VL-7B-Instruct [3]. We employ veRL [27] framework for RL training. During training, we set the batch size as 512 and the mini-batch size as 32. We drop the KL term during policy optimization. The initial learning rate of the policy model is 1e 6. For each prompt, we sample 16 candidate responses using temperature of 1.0. During inference, we use the vLLM framework and set the temperature to 0. We use training data from Yang et al. [37]1, which contains VQA samples that can be answered directly using low-resolution images, as well as samples that require high-resolution images for accurate answering. 5.2. Main Results We compare AdaptVision with existing vision token compression methods, including FastV [5], SparseVLM [40], VisionZip [36], and VisionThink [37]. FastV, SparseVLM, and VisionZip are static methods that operate with pre-defined token retention ratio, while VisionThink and Figure 4. Comparison of Inference Time. (1) Compared to the vanilla model and VisionThink, AdaptVision demonstrates significantly reduced inference time due to reduced visual token usage. (2) While AdaptVision requires additional generated tokens for reasoning and tool calls compared to the down-sample model, the resulting increase in inference time remains acceptable. AdaptVision are dynamic methods that vary visual token usage for each sample. For fair comparison, static methods are set to 50% token retention. For VisionThink, we initially used the officially released model2 but found it consumed substantially more visual tokens than our method, making the comparison unfair. We thus report two versions: VisionThink for the released model and Vision1https://huggingface.co/datasets/Senqiao/VisionThink-Smart-Train 2https://huggingface.co/Senqiao/VisionThink-Efficient (a) Reward Ablation (b) Training curve of tool call ratio, outcome reward and tool reward Figure 5. Policy-training comparison: (a) The influence of reward design. (b) GRPO vs. DTPO. (a) Training curve of tool call ratio on different types of data. (b) Tool call proportion across different benchmarks. Figure 6. Tool call ratio analysis: (a) Training curves show that DTPO learns stable and adaptive policy, increasing tool calls for hard samples while decreasing them for simple ones. (b) Across different benchmarks, AdaptVision demonstrates well-balanced ability to invoke tools when necessary and answer directly when possible. Think for our reproduction using the public code3. We also include the vanilla model (100% tokens, high-resolution) and the down-sample model (25% tokens, 1/4 resolution) based on Qwen2.5-VL-7B-Instruct. Results are shown in Table 1. Compared to previous vision token compression methods (including FastV, SparseVLM, VisionZip and VisionThink), AdaptVision achieves superior average performance across all benchmarks with significantly fewer visual tokens. Compared to the down-sample model, AdaptVision improves accuracy by 5.8% (92.1% 97.9%) with only 7% (25% 33%) more visual tokens, highlighting its effective coarse-to-fine visual reasoning. Inference Latency We also compare inference time across multiple models: the vanilla model, the down-sample model, and VisionThink. The end-to-end inference time measurements for each dataset are presented in Fig. 4. AdaptVision demonstrates significantly reduced inference time (1.67x overall speedup) compared to both the vanilla model and VisionThink, primarily due to its more efficient 3https://github.com/dvlab-research/VisionThink visual token usage. While AdaptVision does require additional token generation for reasoning and tool calls compared to the down-sample model, the resulting increase in inference time remains within acceptable bounds. 5.3. Analysis Reward Design To investigate the impact of reward design on model behavior, we conduct an ablation study on balance and tool rewards. As shown in Fig. 5a, the absence of the balance reward causes the model to quickly collapse to excessive tool use. This occurs because the tool reward incentivizes correct tool use, which generally improves accuracy as training progresses. Conversely, with balance reward, the VLM learns to adaptively regulate tool usage based on the input. Furthermore, the ablation of the tool reward reveals its necessity for exploration: without it, the model collapses to direct answering and fails to invoke In contrast, with the the tool after just 10 training steps. tool reward, the model successfully explores and leverages the tool to enhance performance. Figure 7. Case study: (1) The vanilla model yields correct answer but consumes large number of visual tokens; (2) The down-sample model reduces token usage but fails to answer correctly; (3) AdaptVision smartly invokes the tool to produce correct answer with minimal visual token cost. GRPO vs. DTPO We compare the training processes of GRPO and DTPO in Fig. 5b. GRPO exhibits an unstable training dynamic: During the early training phase, it struggles to optimize either the tool or outcome reward, causing the tool call ratio to drop near zero and limiting exploration. After approximately 20 steps, both rewards and the tool call ratio surge rapidly, shifting the model from direct answering to excessive tool use, eventually collapsing to tool call. This instability stems from GRPOs ambiguous credit assignment and imbalanced optimization. In contrast, DTPO exhibits stable and efficient optimization process. Both rewards rise steadily from the start, reflecting effective tool use exploration. The model subsequently converges to reasonable tool call ratio, demonstrating the effectiveness of DTPO. In Table 1, we observe that the GRPO-trained model not only performs worse than DTPO, but it also uses far more visual tokens. This confirms that DTPO is critical for effectively learning the balance between tool use and direct answering. Furthermore, we compare the tool call ratios across different data types. Fig. 6a illustrates that our model learns to selectively invoke tools based on task difficulty, while the model trained with GRPO calls tools on all samples, resulting in 100% tool call ratio. Adaptive Tool-use We further investigate tool-use efficiency by measuring the proportion of tool call responses across various benchmarks. As shown in Fig. 6b, for complex visual tasks like MathVerse and ChartQA that require fine-grained details, the model frequently invokes the tool to better answer the question. For general understanding tasks like POPE, our model rarely calls the tool, thereby maintaining high efficiency. This shows our model has learned adaptive reasoning: it solves problems directly when tools are unnecessary while still leveraging them when beneficial. 5.4. Case Study In this section, we present case study to illustrate the efficient visual reasoning process of AdaptVision. We compare AdaptVision with the vanilla model and the downsample model. As shown in Fig. 7, the down-sample model, while reducing visual token usage, fails to answer correctly due to insufficient information in the low-resolution image. The vanilla model, using the original high-resolution image, yields correct answer but at the cost of large number of visual tokens. In contrast, AdaptVision begins with the lowresolution image, analyzes the question and image, recognizes the informational inadequacy, and then intelligently invokes the tool to crop the most relevant region from the high-resolution image. By acquiring only this essential additional visual information, it produces an accurate answer while minimizing visual token consumption. 6. Conclusion In this paper, we present AdaptVision, novel paradigm that enables VLMs to autonomously determine the minimum number of visual tokens via adaptive, coarse-to-fine visual reasoning. We propose Decoupled Turn Policy Optimization (DTPO) algorithm, which handles dual-objective policy learning by decoupling the learning objective and advantage estimation. This leads to more balanced and effective training process than GRPO. Experiments on multiple VQA benchmarks show that AdaptVision achieves superior performance using significantly fewer visual tokens than previous efficient VLM methods. These results advance the development of computationally efficient and biologically inspired VLMs. Despite its effectiveness, AdaptVision has several limitations that outline directions for future research. First, our framework currently relies on single visual tool and fixed initial compression ratio (1/4 resolution). Expanding the toolset and enabling dynamic resolution selection could further enhance adaptability. Second, the reasoning process is constrained to maximum of two turns, which may limit deep visual reasoning for complex tasks. We hope that future research will address these limitations, further advancing the development of efficient VLMs."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1 [2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 1(2):3, 2023. 1 [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 1, 6, 11 [4] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. In European Conference on Computer Vision, pages 370387. Springer, 2024. 1 [5] Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models. In European Conference on Computer Vision, pages 1935. Springer, 2024. 1, 2, 6 [6] John Findlay and Iain Gilchrist. Active vision: The psychology of looking and seeing. Number 37. Oxford University Press, 2003. 2 [7] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: comprehensive evaluation benchmark for multimodal large language models, 2024. [8] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 2, 3 [9] Yefei He, Feng Chen, Jing Liu, Wenqi Shao, Hong Zhou, Kaipeng Zhang, and Bohan Zhuang. Zipvl: Efficient large vision-language models with dynamic token sparsification and kv cache compression. 2024. 1 [10] Xinyu Huang, Yuhao Dong, Weiwei Tian, Bo Li, Rui Feng, and Ziwei Liu. High-resolution visual reasoning via multiturn grounding-based reinforcement learning. arXiv preprint arXiv:2507.05920, 2025. 2 [11] Laurent Itti, Geraint Rees, and John Tsotsos. Neurobiology of attention. Elsevier, 2005. 2 [12] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. 2, [13] Yiren Jian, Tingkai Liu, Yunzhe Tao, Chunhui Zhang, Soroush Vosoughi, and Hongxia Yang. Expedited training of visual conditioned language generation via redundancy reduction. arXiv preprint arXiv:2310.03291, 2023. 1 [14] Xin Lai, Junyi Li, Wei Li, Tao Liu, Tianjian Li, and Hengshuang Zhao. Mini-o3: Scaling up reasoning patterns and interaction turns for visual search. arXiv preprint arXiv:2509.07969, 2025. 2 [15] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational conference on machine learning, pages 19730 19742. PMLR, 2023. 1 [16] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models, 2023. 6 [17] Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, and Xiang Bai. Ocrbench: on the hidden mystery of ocr in large multimodal models. Science China Information Sciences, 67(12):220102, 2024. 6 [18] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. VisualarXiv preprint rft: Visual reinforcement fine-tuning. arXiv:2503.01785, 2025. [19] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. 6 [20] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. 6 [21] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. 6 [22] David Navon. Forest before trees: The precedence of global features in visual perception. Cognitive psychology, 9(3): 353383, 1977. 2 [23] Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, and Xu Yang. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl. arXiv preprint arXiv:2503.07536, 2025. 2 [24] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [25] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 2, 3, 5 [26] Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. 2 Okuno, Yohei Nakata, Kurt Keutzer, et al. Sparsevlm: Visual token sparsification for efficient vision-language model inference. arXiv preprint arXiv:2410.04417, 2024. 1, 2, 6 [41] Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. DeepIncentivizing thinking with images via reinforceeyes: ment learning. arXiv preprint arXiv:2505.14362, 2025. 2 [42] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 1 [27] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, pages 12791297, 2025. 6, 11 [28] Dachuan Shi, Chaofan Tao, Ying Jin, Zhendong Yang, Chun Yuan, and Jiaqi Wang. Upop: Unified and progressive prunIn Ining for compressing vision-language transformers. ternational Conference on Machine Learning, pages 31292 31311. PMLR, 2023. [29] Alex Su, Haozhe Wang, Weiming Ren, Fangzhen Lin, and Wenhu Chen. Pixel reasoner: Incentivizing pixel-space reasoning with curiosity-driven reinforcement learning. arXiv preprint arXiv:2505.15966, 2025. 2 [30] Huajie Tan, Yuheng Ji, Xiaoshuai Hao, Minglan Lin, Pengwei Wang, Zhongyuan Wang, and Shanghang Zhang. Reason-rft: Reinforcement fine-tuning for visual reasoning. arXiv preprint arXiv:2503.20752, 2025. 2 [31] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 1 [32] Yuxin Wen, Qingqing Cao, Qichen Fu, Sachin Mehta, and Mahyar Najibi. Efficient vision-language models by summarizing visual tokens into compact registers. arXiv preprint arXiv:2410.14072, 2024. 1 [33] Penghao Wu and Saining Xie. V?: Guided visual search as In Proceedings of core mechanism in multimodal llms. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1308413094, 2024. 2 [34] xAI Team. Grok-1.5 vision preview, 2024. 6 [35] Long Xing, Qidong Huang, Xiaoyi Dong, Jiajie Lu, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, Jiaqi Wang, Feng Wu, et al. Pyramiddrop: Accelerating your large vision-language models via pyramid visual redundancy reduction. arXiv preprint arXiv:2410.17247, 2024. [36] Senqiao Yang, Yukang Chen, Zhuotao Tian, Chengyao Wang, Jingyao Li, Bei Yu, and Jiaya Jia. Visionzip: Longer is better but not necessary in vision language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1979219802, 2025. 1, 2, 3, 6 [37] Senqiao Yang, Junyi Li, Xin Lai, Bei Yu, Hengshuang Zhao, and Jiaya Jia. Visionthink: Smart and efficient vision lanarXiv preprint guage model via reinforcement learning. arXiv:2507.13348, 2025. 1, 2, 4, 6, 11 [38] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. 6 [39] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pages 169186. Springer, 2024. 6 [40] Yuan Zhang, Chun-Kai Fan, Junpeng Ma, Wenzhao Zheng, Tao Huang, Kuan Cheng, Denis Gudovskiy, Tomoyuki correct answers with only marginal increase in visual token consumption relative to the Down-sample model. These examples validate AdaptVisions ability in coarse-to-fine visual reasoning and its capacity to autonomously tailor visual token usage to each input. Figure 8. Case of direct answer in AdaptVision. Figure 9. Case of tool call in AdaptVision. A. Additional Details A.1. Prompt Details First, AdaptVision utilizes three types of prompts. to equip the VLM with basic tool-using capability, we follow the Qwen2.5-VL cookbook [3] to design prompts for the bounding box tool  (Table 3)  . Second, since VQA tasks are typically diverse and open-ended, we adopt an LLM-asjudge approach to evaluate answer correctness. As shown in Table 4, following Yang et al. [37], we design judging prompt for GPT-4o to produce binary evaluations (1 for correct, 0 for incorrect). Third, to encourage efficient tool exploration, we prompt GPT-4o to evaluate the relevance of cropped regions, producing binary reward for region correctness  (Table 5)  . A.2. Training and Evaluation Details AdaptVision is based on Qwen2.5-VL-7B-Instruct [3]. We employ veRL [27] framework for RL training. During training, we set the batch size as 512 with mixed-precision (FP16) training. The mini-batch size is 32. We drop the KL term during policy optimization. For each prompt, we sample 16 candidate responses (i.e., = 16) using temperature of 1.0. The upper and lower clip ratios are 0.24 and 0.20, respectively. We set the maximum prompt length and the maximum response length as 8192. All experiments were conducted on 4 nodes, each with 8 H20 GPUs. The model was trained for 80 steps, using the AdamW optimizer with learning rate of 1e6, β = (0.9, 0.999), and weight decay of 0.01. During inference, we use the vLLM framework and set the temperature to 0. A.3. Additional Results We further compare AdaptVision with previous efficient VLM methods with different visual token retention ratios. As shown in Table 2, while the performance of FastV, SparseVLM, and VisionZip degrades with reduced token ratios, AdaptVision maintains superior performance with significantly fewer visual tokens. B. Qualitative Results We provide further case studies to illustrate AdaptVisions adaptive token usage. As shown in Fig. 8, in scenarios where low-resolution image provides enough information, AdaptVision correctly chooses to answer directlymatching the behavior of the Qwen2.5-VL Downsample model. Conversely, in cases where detailed visual information is essential  (Fig. 9)  , the Down-sample model often fails due to recognition errors caused by insufficient resolution (e.g., misreading 15 as 75). Under the same conditions, AdaptVision actively invokes the bounding box tool, accurately localizes informative regions, and produces Table 2. Performance comparison with previous efficient VLM methods. Vanilla denotes the Qwen2.5-VL-7B-Instruct model. DownSample uses 1/4-resolution image as input to the Vanilla model. #Token indicates the visual token consumption ratio relative to the vanilla model across all benchmarks. Avg. denotes the average performance relative to the vanilla model on all benchmarks. Method (xx%) denotes static methods retaining xx% visual tokens."
        },
        {
            "title": "Vanilla",
            "content": "Down-Sample SparseVLM (50%) FastV (50%) VisionZip (50%) SparseVLM (70%) FastV (70%) VisionZip (70%)"
        },
        {
            "title": "VisionThink",
            "content": "VisionThink"
        },
        {
            "title": "ChartQA OCRBench DocVQA MME MMVet RealWorldQA POPE MathVista MathVerse",
            "content": "test test val test test test test testmini testmini #Token Avg. 79.8 100% 62.9 78.8% 73.2 91.7% 72.6 91.0% 71.5 89.6% 75.8 94.9% 71.2 96.7% 76.8 96.2% 73.6 92.2% 73.88 92.6% 75.92 95.1% 81.5 100% 68.8 84.4% 75.6 92.7% 75.8 93.0% 70.5 86.5% 79.3 97.3% 82.2 100.8% 80.9 99.3% 76.8 94.2% 80.8 99.1% 76.9 94.4% Retain 100% Visual Tokens Across All Benchmarks 95.1 100% 2316 100% 61.6 100% 68.6 100% 86.7 100% Retain 25% Visual Tokens Across All Benchmarks 94.3 99.1% 2270 98.0% 54.5 88.5% 68.8 100.3% 82.8 95.5% Retain 50% Visual Tokens Across All Benchmarks 66.8 70.2% 93.6 98.4% 93.8 98.6% 2282 98.5% 2308 99.6% 2209 95.4% 51.5 83.6% 52.8 85.7% 57.0 92.5% 68.4 99.7% 68.8 100.3% 68.6 100% 85.5 98.6% 84.7 97.7% 86.3 99.5% Retain 70% Visual Tokens Across All Benchmarks 68.7 72.2% 94.4 99.3% 94.5 99.4% 92.9 97.7% 93.7 98.5% 92.6 97.4% 2276 98.3% 53.7 87.2% 2342 56.0 101.1% 90.9% 60.0 100.8% 97.4% 68.5 99.8% 68.6 100% 68.2 99.4%"
        },
        {
            "title": "Dynamic Methods",
            "content": "2320 61.7 100.2% 100.2% 2392 60.18 103.3% 97.7% 64.8 102.7% 105.2% 65.6 95.6% 68.37 99.7% 67.32 98.1% 85.4 98.5% 85.9 99.1% 86.4 99.7% 86.3 99.5% 86.69 100.0% 86.8 100.1% 68.2 100% 62.2 91.2% 66.6 97.6% 63.7 93.4% 64.1 93.9% 66.3 97.2% 65.9 96.6% 68.9 101.0% 62.2 91.2% 65.7 96.3% 65.9 96.6% 46.3 100% 43.1 93.1% 45.1 97.4% 45.0 97.2% 45.1 97.4% 45.1 97.4% 46.9 101.3% 45.8 98.9% 42.5 91.8% 45.68 98.7% 42.3 91.4% 100% 100% 25% 92.1% 50% 92.2% 50% 95.8% 50% 94.8% 70% 93.6% 70% 98.4% 70% 99.1% 52% 95.8% 99% 98.4% 33% 97.9% Table 3. Prompt Template for adaptively visual acquisition. Question will be replaced with the specific question during training and inference. SYSTEM PROMPT: You are helpful assistant. # Tools You may call the function tool shown below to assist with the user query. You are provided with the function signature within <tools></tools> XML tags: <tools> { \"type\": \"function\", \"function\":{ \"name for human\": \"request local region\", \"name\": \"request local region\", \"description\": \"Request high-resolution local region of the current image and zoom in\", \"parameters\": { \"properties\": { \"bbox 2d\": { \"type\": \"array\", \"items\": { \"type\": \"integer\" } \"minItems\": 4, \"maxItems\": 4, \"description\":The bounding box of the region to crop, as [x1, y1, x2, y2], where (x1, y1) is the top-left corner of the target region and (x2, y2) is the bottom-right corner of the target region. The bounding box should be in the absolute pixel coordinates of the current image.\", } } \"required\": [\"bbox 2d\"], \"type\": \"object\", }, \"args format\": \"Format the arguments as JSON object.\" } } </tools> For each function call, return json object with the function name and the corresponding argument within <tool call></tool call> XML tags: <tool call> {\"name\":<function-name>, \"arguments\":<args-json-object>} </tool call> USER PROMPT: Answer the question based on the image provided. You must conduct reasoning within <think> and </think> first in each of your reasoning steps. You may call ONE function tool per step to help you better solve the problem. Place the function tool within <tool call> and </tool call> at the end of each step to perform function call. You should continue your reasoning process within <think> and </think> based on the content returned by the function tool. Once you confirm your final answer, place the final answer inside <answer> and </answer>. For mathematical or multiple-choice problem, wrap the answer value or choice with boxed{}. Here is the image and question: Question. Table 4. Prompt Template for LLM as Final Answer Judge. Question, Ground Truth and Prediction are dynamically replaced with the specific question, ground truth and model prediction during evaluation. SYSTEM PROMPT: You are an intelligent chatbot designed for evaluating the correctness of generative outputs for question-answer pairs. Your task is to compare the predicted answer with the correct answer and determine if they match meaningfully. Heres how you can accomplish the task: INSTRUCTIONS: - Focus on the meaningful match between the predicted answer and the correct answer. - Consider synonyms or paraphrases as valid matches. - Evaluate the correctness of the prediction compared to the answer. USER PROMPT: will give you question related to an image and the following text as inputs: 1. **Question Related to the Image**: Question 2. **Ground Truth Answer**: Ground Truth 3. **Model Predicted Answer**: Prediction Your task is to evaluate the models predicted answer against the ground truth answer, based on the context provided by the question related to the image. Consider the following criteria for evaluation: - **Relevance**: Does the predicted answer directly address the question posed, considering the information provided by the given question? - **Accuracy**: Compare the predicted answer to the ground truth answer. You need to evaluate from the following two perspectives: (1) If the ground truth answer is open-ended, consider whether the prediction accurately reflects the information given in the ground truth without introducing factual inaccuracies. If it does, the prediction should be considered correct. (2) If the ground truth answer is definitive answer, strictly compare the models prediction to the actual answer. Pay attention to unit conversions such as length and angle, etc. As long as the results are consistent, the models prediction should be deemed correct. **Output Format**: Your response should include an integer score indicating the correctness of the prediction: 1 for correct and 0 for incorrect. Note that 1 means the models prediction strictly aligns with the ground truth, while 0 means it does not. The format should be Score: 0 or 1 Table 5. Prompt Template for Judging the Correctness of Bounding Box. Question are dynamically replaced with the specific question during evaluation. SYSTEM PROMPT: **Your Role:** You are an AI agent that identifies relevant visual evidence. **Your Goal:** Determine if an image CROP contains the **primary subject** of given question. **Your Golden Rule:** Your main task is to check for **presence**, not completeness. As long as the main object or area the question is asking about is clearly visible in the crop, it is considered relevant. **Criteria for Score: 0 (Strictly Enforced):** - The core subject of the question is completely absent from the image. - The image is so blurry or corrupted that the subject is **unrecognizable**. - The image shows something completely unrelated (e.g. question is about car, image shows tree). **Your Task:** Now, analyze the user-provided image and question following this exact process. Your response MUST only contain Score: 1 or Score: 0. USER PROMPT: Given question and cropped image region, answer with Score: 1 if the cropped region provide information to answer the question, otherwise answer Score: 0. Question: Question."
        }
    ],
    "affiliations": [
        "Tencent AI Lab"
    ]
}