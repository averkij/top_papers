{
    "paper_title": "Analyzing The Language of Visual Tokens",
    "authors": [
        "David M. Chan",
        "Rodolfo Corona",
        "Joonyong Park",
        "Cheol Jun Cho",
        "Yutong Bai",
        "Trevor Darrell"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With the introduction of transformer-based models for vision and language tasks, such as LLaVA and Chameleon, there has been renewed interest in the discrete tokenized representation of images. These models often treat image patches as discrete tokens, analogous to words in natural language, learning joint alignments between visual and human languages. However, little is known about the statistical behavior of these visual languages - whether they follow similar frequency distributions, grammatical structures, or topologies as natural languages. In this paper, we take a natural-language-centric approach to analyzing discrete visual languages and uncover striking similarities and fundamental differences. We demonstrate that, although visual languages adhere to Zipfian distributions, higher token innovation drives greater entropy and lower compression, with tokens predominantly representing object parts, indicating intermediate granularity. We also show that visual languages lack cohesive grammatical structures, leading to higher perplexity and weaker hierarchical organization compared to natural languages. Finally, we demonstrate that, while vision models align more closely with natural languages than other models, this alignment remains significantly weaker than the cohesion found within natural languages. Through these experiments, we demonstrate how understanding the statistical properties of discrete visual languages can inform the design of more effective computer vision models."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 7 ] . [ 1 1 0 0 5 0 . 1 1 4 2 : r a"
        },
        {
            "title": "ANALYZING THE LANGUAGE OF VISUAL TOKENS",
            "content": "David M. Chan1, Rodolfo Corona1, Joonyong Park2, Cheol Jun Cho1, Yutong Bai1, Trevor Darrell1 1University of California, Berkeley, 2The University of Tokyo, Tokyo {davidchan,rcorona,cheoljun,yutongbai,trevordarrell}@berkeley.edu, joonyong-park@g.ecc.u-tokyo.ac.jp"
        },
        {
            "title": "ABSTRACT",
            "content": "With the introduction of transformer-based models for vision and language tasks, such as LLaVA and Chameleon, there has been renewed interest in the discrete tokenized representation of images. These models often treat image patches as discrete tokens, analogous to words in natural language, learning joint alignments between visual and human languages. However, little is known about the statistical behavior of these visual languageswhether they follow similar frequency distributions, grammatical structures, or topologies as natural languages. In this paper, we take natural-language-centric approach to analyzing discrete visual languages and uncover striking similarities and fundamental differences. We demonstrate that, although visual languages adhere to Zipfian distributions, higher token innovation drives greater entropy and lower compression, with tokens predominantly representing object parts, indicating intermediate granularity. We also show that visual languages lack cohesive grammatical structures, leading to higher perplexity and weaker hierarchical organization compared to natural languages. Finally, we demonstrate that, while vision models align more closely with natural languages than other models, this alignment remains significantly weaker than the cohesion found within natural languages. Through these experiments, we demonstrate how understanding the statistical properties of discrete visual languages can inform the design of more effective computer vision models."
        },
        {
            "title": "INTRODUCTION",
            "content": "Transformer-based models have not just advanced, but fundamentally reshaped how we approach both vision and language processing, merging these domains in shared sequential representation spaces. Indeed, most recent multi-modal models including DALL-E (Ramesh et al., 2022), LLaVA (Liu et al., 2024) and Chameleon (Team, 2024) operate over joint tokenized representations of images and language, where models decompose images into visual languages: linearized discrete patches or tokens analogous to words in sentence. This process, shown in Figure 1, enables seamless integration of images into transformer architectures and allows models to solve multimodal tasks, ranging from image generation and image captioning to visual question answering and translation. Despite the success of such shared-structure models, current research lacks an in-depth understanding of whether the internal structure of visual tokens mirrors the principles governing natural languages. Specifically, the question arises: do languages formed of visual tokens follow the same statistical patterns, such as frequency distributions, grammatical rules, or semantic dependencies, that human languages exhibit? Investigating such statistical behavior of discrete visual tokens extends beyond theoretical curiosity; it has broad implications for practical machine learning applications. While in linguistic theory, phenomena like Zipfs law and entropy shape natural languages structure and shape the design of machine learning algorithms, no such rules exist for visual languages. Such rules, if they exist, have the potential to motivate creating modalityspecific models and procedures to capture the unique statistical properties of the underlying visual data. In pursuit of such rules, in this paper we inspect the equivalence of visual and natural languages through an empirical analysis of token distributions, segmentation granularity, and syntactic and semantic structures. We start by investigating the frequency statistics of visual words and compare them to natural languages. Our analysis reveals that although visual languages can follow power-law (Zipfian) distributions, they use more tokens more uniformly. This leads to languages with greater per-token entropy and lower compression ratios, and implies that vision models may require more attention heads, larger embeddings, and longer training times with more diverse data compared to natural language models. Noting in these experiments that visual languages have coarser granularity than patches, we demonstrate through correlation analysis 1 Figure 1: Discrete tokenizers used for visual pre-processing induce visual languages made up of sentences containing 1-D sequences of discrete tokens extracted from the images in dataset. In this paper, we explore how the statistics of these visual languages differ from natural languages, and understand the implications of such statistical differences. that visual tokens operate at an intermediate level of granularity, and typically represent object parts rather than whole objects or sub-parts in images. Correspondingly, we show visual tokens are less effective at representing fine-grained details or whole-object structures. Following this line of reasoning, we explore if tokens have composable structure, and using parse trees generated by Compound Probabilistic Context-Free Grammars (C-PCFG), we show visual languages have grammatical structures that are more fragmented, with grammars trained on them exhibiting higher perplexity compared to natural languages. We confirm these observations by building co-occurrence based embedding space, and evaluating the topological alignment between natural and visual languages. In this, we find visual languages align more with natural languages than with other visual languages, but less so than natural languages align with each other. Together, we aim to show through these experiments show that while visual languages have striking similarities to natural languages, there are also notable and fundamental differences, motivating unique modality-specific approaches to vision-language learning."
        },
        {
            "title": "2 DO VISUAL TOKENS ACT LIKE WORDS?",
            "content": "The first, question that we examine is: Do visual tokens themselves (i.e. the patches of an image) act like words? While we often treat these tokens as either word (or subword), as each token forms single input sequence element in transformer, it seems unintuitive that there would be one to one statistical correlation between the two concepts. In this section, we look at several statistical properties of individual tokens, comparing those observed in natural language to those in visual systems."
        },
        {
            "title": "2.1 PRELIMINARIES",
            "content": "What, explicitly, is visual language? In this work, we consider visual language to be language induced over visual tokens by first converting images in dataset to discrete set of symbols using visual tokenizer (often VQ-VAE), and then linearizing those tokens into one-dimensional sequences (See Figure 1). Such definition parallels efforts in both text-to-image diffusion and large vision and language models which have both explored using discrete visual tokens for vision-language model alignment (Team, 2024; Ramesh et al., 2022; Gu et al., 2022; Razavi et al., 2019), as well as in uni-modal models such as LVM (Bai et al., 2024) and LLamaGen (Sun et al., 2024). We primarily focus on common tokenizers used for recent vision and language models, and our selection of tokenizers is overviewed in Table 1. These tokenizers are all VQ-VAE-based, trained on varying datasets, and with various methods. While some recent models such as Transfusion (Zhou et al., 2024) and LLaVA Liu et al. (2024) leverage continuous-valued tokens instead of discrete vocabularies, there is still considerable uncertainty about whether discrete or continuous-valued tokens are more effective (Mao et al., 2021). While many of our methods in this paper could apply to continuous tokens through discrete quantization of those tokens, we leave such continuous extensions to future work. For more details on the tokenizers, see Appendix A. We ground our empirical experiments in several common multi-modal datasets, including Conceptual Captions (12M) (Sharma et al., 2018), MS-COCO (Lin et al., 2014), ILSVRC (ImageNet) (Russakovsky et al., 2015) and XM-3600 (Thapliyal et al., 2022). Each of these datasets has set of images, and (except ILSVRC) paired text in one or more languages. For more information on the datasets, see Appendix B. An example visual sentence from MS-COCO (Image ID: 399655) is given in Figure 1. In all of the experiments in this paper, we linearize the tokens using row-wise scan order (for detailed discussion 2 Tokenizer Application chameleon-512 (Team, 2024) compvis-vq-f8-64 (Rombach et al., 2022) compvis-vq-f8-256 (Rombach et al., 2022) compvis-vq-imagenet-f16-1024-256 (Esser et al., 2021) llamagen-vq-ds16-c2i (Sun et al., 2024) Multimodal Foundation Model Image Generation Image Generation Image Generation Text Image Resolution Vocab Size 512512 6464 256256 256256 256 8192 16384 16384 1024 16384 Table 1: Visual tokenizers that we use in this paper. We select several tokenizers across several applications at varying resolutions and vocab sizes. on scan-order, see Appendix C). Such linearization is the de facto standard for turning spatial visual tokens into sequences of discrete tokens for use in learning applications."
        },
        {
            "title": "2.2 TOKEN FREQUENCY AND ZIPF’S LAW",
            "content": "The statistics of natural language token distributions have long been studied, beginning with Dewey (1921), who first plotted the frequency of English words. key principle that emerged from this research is Zipfs Law (Kingsley Zipf, 1932), which describes power-law relationship between the frequency of words and their rank in language where small number of high-frequency words dominate natural language, while the majority of words occur infrequently. Formally, Zipfs law states that: f(r)rα+σZ (1) where f(r) is the frequency of the element with rank and α/σ parameterize learned Gaussian distribution (close to 1/0 in many natural languages). Zipfs law has been observed across many languages (Gelbukh & Sidorov, 2001; Yu et al., 2018) and non-human communication systems (such as dolphins (McCowan et al., 1999)). As Mandelbrot pointed out, adherence to Zipf-like distributions ensures that communication systemswhether natural or artificialoperate efficiently (Mandelbrot, 1953). Language models, especially large language models (LLMs), have been shown to follow this same pattern, with token distributions that obey Zipfs law (Patwary et al., 2019). This statistical regularity in language extends beyond word frequency - Zipfs law has also been observed in images themselves: Ruderman (1997) showed that the distribution of object sizes and spatial frequencies in natural scenes follows power-law distributions, and Crosier & Griffin (2007) showed that there was Zipfian behavior in image coding schemes such as JPEG. Thus, we first ask the question - Do visual languages follow Zipfs law? To do this, we tokenize the image datasets according to subsection 2.1 and compute the empirical token-rank frequency distributions on each of the datasets (See Appendix for details). We show the empirical distributions in Figure 2. If the plots were Zipfian, we would expect them to be linear in the log-log space; while this is the case for natural languages, visual languages do not seem to generally conform to linear curve, instead, for one and two grams, the plots follow lognormal distribution, and for higher level N-grams are more convex in nature. For one/two-grams, this indicates that token utilization is fairly uniform, with most tokens occurring in equal proportion, and the heavier tails of the distribution indicate that rare are, in practice, not so rare, occurring with much higher frequency than expected under power-law distribution. Whereas natural languages are often structured with clear core vocabulary and then more specialized words, it seems like visual features seem to be more evenly distributed, with many features or combinations being equally likely. At higher n-grams, for visual languages there is more convex behavior, suggesting that there are very few common n-grams, instead, n-grams are often unique, and composed in ways that appear very infrequently within the datasets. Such an implication implies that visual languages are highly context-dependent (which is sensible, as visual scenes are quite complex). To confirm these details, we fit Zipfs distribution to each of the models, with the results of the fit shown in Table 2. Interestingly, the α values have opposite behaviors for visual and natural languages in the light of increasing N. In natural languages, the fact that α increases with means that higher-order N-grams follow steeper power-law distributions, and the distribution of N-gram frequencies becomes more concentrated around few common combinations, while the frequency of rare combinations decreases rapidly. In visual languages, on the other hand, the decrease in α with increasing suggests that higher-order combinations of visual features follow flatter distributions: as visual N-grams increase in complexity, there is more diversity in the combinations of features and patterns, leading to richer and more distributed sets of higher-order feature combinations. These phenomena together suggest that VQ-VAEs are spreading information between the independent tokens, rather than building compressive and compositional structures, which we explore further in (a) 1-grams (b) 2-grams (c) 3-grams (d) 7-grams Figure 2: Plot of normalized token log-frequency against normalized Log-Rank for several visual and textual languages for different n-grams, aggregated across datasets. While the tails of visual languages do not conform to Zipfs law well for small values of N, for larger values of N, the fit becomes more linear. N= N=2 N=3 N=5 N=7 Natural Visual Natural Visual Natural Visual Natural Visual Natural Visual α σ logL 1.710.23 0.010.02 -4.031.38 4.371.33 0.180.14 -9.723.28 1.990.25 0.010.01 -3.110. 4.431.50 0.070.16 -4.242.27 2.280.33 0.030.04 -2.920.42 2.570.82 0.090.18 -3.531.68 2.850.82 0.250.54 -2.430.67 2.350.52 0.090.13 -2.991.22 3.020.73 0.280.46 -1.981. 2.350.50 0.090.14 -2.721.19 Table 2: Comparison of aggregate power law fit metrics (α,σ, mean log-likelihood) across different N-gram lengths for natural and visual languages. While visual languages do not follow Zipfs law for =1, the fit is significantly better for =3 and above. subsection 2.2 (token innovation) and subsection 2.5 (compression). Indeed, since Zipfs Law reflects (theoretically optimal) balance between redundancy and information, it suggests that visual languages are more data-driven, and reflect the underlying complexity and variability of visual scenes, rather than focusing on reducing redundancy for communicative operations. Such deviation might suggest that models that are more Zipfian, such as chameleon, may be better placed as embedding/alignment models for visual tasks, whereas models such have more convex N-gram distributions are better for high-fidelity generation tasks. Beyond model quality/applicability implications, the fact that visual languages dont follow Zipfs Law implies that traditional NLP-inspired techniques (e.g., those relying on power-law distributions such as compression algorithms, or memory-based systems based on Zipfian patterns) may not directly apply to visual languages. Beyond this, visual languages likely require different optimization techniques taking into account the non-linear distribution of N-grams methods that handle long-tail distributions might be more appropriate than techniques focused on heavy tails. Such differences in distribution could also suggest that higher-order interactions between visual features are more important in vision models than in language models, and model architectures should be designed to capture these higher-order patterns effectively. 4 (a) 1-grams (b) 2-grams (c) 3-grams (d) 5-grams Figure 3: Comparison of unique tokens as function of images seen on the XM-3600 dataset for different N-grams. While higher values of approach linear relationship in the visual languages, textual languages are always sub-linear in their growth. Surprisingly, for 3/5-grams, several visual language curves overlap."
        },
        {
            "title": "2.3 TOKEN INNOVATION",
            "content": "One thing that stands out from the experiments in subsection 2.2 is that single visual tokens appear more uniformly than single words, inspiring the question: do new images consist of mostly new tokens, or do new images re-combine existing tokens in novel ways? In natural language, this has generally been codified by Heaps/Herdans law (Herdan, 1964; Heaps, 1978), which says that vocabularies sizes are concave increasing power laws of texts sizes (See Appendix for details). To explore this effect, Figure 3 plots the number of unique tokens seen against the number of images seen for the XM-3600 dataset for several visual tokenizers and natural languages. The natural languages follow the expected distribution, with unique tokens increasing sub-linearly with respect to the number of images. The visual tokens, on the other hand, appear much more rapidly. For single tokens, almost all of the tokens in the vocabulary appear within the first 100 images, suggesting that the rate of token innovation is significantly higher than that of natural languages. For 2-grams and 4-grams, the relationship trends linear, but never approaches the sub-linear behavior that is expected of generative systems which follow Heaps law. Additional experiments on MS-COCO are given in Appendix E. We further fit Yule-Simon distribution (Simon, 1955) to both the natural and visual languages. The Yule-Simon process is stochastic model for generating sequences of words or tokens, where the probability of introducing new token decreases as more tokens are added, leading to power-law distribution; mathematically, this process is governed by probability proportional to the current token frequency, combined with parameter that controls the rate of new token introduction (see Appendix for more details). The results, given in Figure 4 and Appendix F, demonstrate that the generative process for new tokens largely does not fit with that described by the Yule-Simon process in the visual case, however, fit quite well for many text languages. 5 (a) Spanish (1-grams) (b) Chameleon (1-grams) (c) Chameleon (2-grams) (d) Chameleon (3-grams) Figure 4: Yule-Simon model fit for Chameleon vs. Spanish on the COCO dataset (More models/languages in Appendix F). While Spanish (and in general, natural languages) largely fits Yule-Simon model, Chameleon does not appear to be generated by such process at any n-gram level. The fact that visual tokens have much higher rate of innovation has several key implications for the design, training, and evaluation of both generative and discriminative models. The high vocabulary diversity of visual tokens means that while generative models will be able to generate higher-fidelity output, discriminative models are at high risk of over-fitting: risking overly specific captions or inconsistencies across similar images (a feature that has already been noted in several works (Chan et al., 2022; Caglayan et al., 2020)). Such high vocab diversity also impacts the training efficiency of models: both generative and discriminative models will require longer training times and need more varied datasets to handle expanding token sets than models of natural language (a fact which has been observed explicitly in (Touvron et al., 2021), and more generally with vision transformers). Beyond training, inference and evaluation are also impacted. Decoding approaches that rely on frequency/presence penalties may want to leverage unique/more aggressive penalties for vision compared to language tokens. For evaluation, perhaps already clear from existing work, semantic-based evaluation is likely more effective than token-based evaluation in visual approaches due to the high level of diversity in the local token space (Anderson et al., 2016; Hessel et al., 2021)."
        },
        {
            "title": "2.4 NATURALITY",
            "content": "Benfords Law describes the frequency distribution of leading digits in naturally occurring datasets, where smaller digits like 1 and 2 appear disproportionately more often than larger digits like 8 and 9 (Benford, 1938). Originally observed in domains such as physics (Sambridge et al., 2010), economics (Tödter, 2009), and demographics (Miller, 2015), recently, there has been growing interest in extending this statistical principle to linguistic data (Golbeck, 2023; Melián et al., 2017; Hong, 2010). One of the primary applications of Benfords law is the detection of anomalies in data: datasets that do not follow Benfords law are likely to be unnatural in nature - here, we ask the question, do visual language token frequencies naturally follow Benfords law? We follow similar tokenization process to subsection 2.2, and plot the occurrence of leading digits in the token frequency distribution (See Appendix for more details). Our results are shown in Figure 5 for the MS-COCO dataset, and in Appendix on other datasets. Interestingly, for single tokens, the distribution is unique-token-heavy, with the remaining tokens having Gaussian distribution around six. Two-grams are the most natural, with Chameleon following Benfords law almost exactly, with three-grams significantly dominated by low/unique frequency tokens. Interestingly, 6 (a) 1-grams (b) 2-grams (c) 3-grams Figure 5: Plot of the first digits of the token frequency distribution on the MS-COCO dataset. While 1-grams have uniquely 1-heavy head, with Gaussian tail (around 6), 2-grams naturally follow an exponential decay function, and 3-grams are dominated by unique tokens. The grey area represents the maximum and minimum among the 36 natural languages. Table 3: Understanding the entropy and Huffman compression rates of visual and natural languages (p<0.01 across all metrics). While the compression rate improves slightly with two-grams in the visual case, it is reduced significantly in the natural case. Full results in Table H.1."
        },
        {
            "title": "Entropy",
            "content": "Fixed Code Length Orig Bits (M) Huff Bits (M) Comp. Rate % Reduction Visual Visual (N=2) Natural Natural (N=2) 10.7 1.9 18.1 0.8 9.0 0.9 13.5 1.0 10.7 1.9 18.1 0. 8.9 0.9 13.5 1.0 11.0 1.8 18.7 0.5 13.8 0.9 16.3 1.1 5.35 1.3 9.1 1.5 4.10 3.1 4.9 3.8 5.20 1.3 8.8 1. 2.54 1.8 3.9 3.0 1.03 0.02 1.03 0.02 1.55 0.1 1.21 0.08 2.9 1.9 3.2 2.2 34.9 6.1 16.9 5.2 the highest quality tokenizer, the Chameleon tokenizer, is by far the most natural in Figure 5a, suggesting that tokenizations performing well for vision-text tasks might have more natural distributions. Beyond this effect, Figure 5b shows that distributions of visual-token bi-grams have the most natural distribution curves, implying potential correspondence in statistics between visual bi-grams and text uni-grams, and suggesting that future work in tokenization could explore vocabularies of token bi-grams or bi-gram compression for vision tokenizers."
        },
        {
            "title": "2.5 ENTROPY AND REDUNDANCY",
            "content": "Building on the foundational work of Shannon (1951), entropy and redundancy have long been understood as key characteristics of natural language, providing insight into its inherent predictability and compressibility. While natural languages, like English, exhibit high redundancy that enables efficient encoding, it is unclear if visual languages might have similar coding behaviors. To evaluate the efficiency of encoding, we use similar setup to subsection 2.2 and extract token streams for each of the target datasets. We then compute the entropy of the token streams, as well as compute simple Huffman code/compression (Huffman, 1952) for each of the resulting streams. Such hierarchical compression code allows us to estimate the overall compressibility of the stream (See Appendix for background/details). The results are summarized in Table 3. We can see that in general, the average code length, entropy, and bits of information/sample are higher for visual languages. This suggests that visual languages have more variability and are inherently more complex to predict and encode than natural language. This is unsurprising, given the complexity and richness of the visual world, compared to the sparsity of natural language, however, it is somewhat surprising that the entropy is not massively different from natural languages, suggesting that visual tokenizers are capable of reducing the richness of natural language to suitably sparse representations for reasoning. Notably different is the compressibility of the token streams. While natural language tokens are highly compressible using Huffman encoding, visual languages are almost incompressible, suggesting that information is highly distributed amongst the tokens and that there is very little structural reuse between the different images. While we explore grammars further in section 3, this experiment indicates that it is unlikely that models have non-trivial grammars of tokens, instead, these tokens are more local, and particularly high-variance. These experiments have several potential implications for model design. First, since visual tokens have significantly higher entropy and lower compressibility, it may be necessary to use more attention heads, deeper models, and more dense embeddings, in visual-based models in order to capture sufficient number of relationships and higher-level representations of visual information. Models like LLaVA (Liu et al., 2024) with simple projection layers between the visual token and text token spaces may not perform 7 Table 4: Whole, part, and sub-part purity/part-normalized mutual information on the SPIN dataset. PP: Part Purity (%), VTP: Visual-Token Purity (%), PNMI: Part-Normalized Mutual Information. Tokenizer chameleon-512 compvis-vq-f8-64 compvis-vq-f8-256 compvis-vq-imagenet llamagen-vq-ds16-c2i Wholes Parts Sub-Parts PP VTP PNMI PP VTP PNMI PP VTP PNMI 2.512 3.061 2.333 2.467 4.384 0.216 0.526 0.467 0.739 0.107 1.557 6.148 0.925 1.463 13.711 4.399 5.653 4.209 4.354 6.983 0.138 0.308 0.334 0.479 0. 0.256 1.760 0.122 0.207 4.487 1.660 2.611 1.527 1.626 3.656 0.200 0.508 0.426 0.623 0.112 0.898 6.246 0.434 0.787 13.273 as well on downstream visual tasks as models such as mPlug (Ye et al., 2024) which have more dense transformer-based adapters (a result which is empirically verified by Tong et al. (2024), who leverage spatially aware dense connector to achieve significant performance improvements). Its worth noting that Huffman encoding is independent of the scan order of the images, and instead, focuses only on token frequencies. It would be interesting for future work to explore how scan order impacts compress-ability, and we discuss potential experiments and limitations regarding scan order in Appendix C."
        },
        {
            "title": "2.6 TOKEN SEGMENTATION GRANULARITY",
            "content": "One common question for many vision researchers is: do visual tokens represent objects? Indeed, while visual tokens are spatially fixed to patches in the image, because of the VQ-VAE training process, it is unclear if they take on additional non-spatial semantic meaning. Recently, Hsu et al. (2021) demonstrated that in audio domains, HuBERT tokens (audio-tokens) have relatively high mutual information with phoneme representations of audio, suggesting that self-supervised models are capable of learning natural structures despite being segmented to fixed-width patches. Can we answer this question for visual languages as well? Recently, Myers-Dean et al. (2024) introduced the SPIN dataset, new labeled dataset of hierarchically segmented objects, where the objects are labeled at the whole, part, and sub-part levels. This gives us per-image annotations of the existence of wholes, parts, and sub-parts. From this, we compute several measures of natural correlation between these part-annotations and the visual token languages, inspired by Hsu et al. (2021) (For more details, see Appendix I): Part Purity, metric that measures the average accuracy of assigning visual-token to its most likely part label, reflecting image-level part consistency within particular visual token, Visual-Token Purity, metric that assesses how well images containing the same part label are consistently assigned to the same visual-tokens and Part-Normalized Mutual Information, an information-theoretic metric which measures the percentage of uncertainty about the part-label eliminated after observing particular visual token. The results are summarized in Table 4. In general, tokenizers appear to be most effective at capturing part-level representations, as evidenced by consistently higher Part Purity (PP) values for parts compared to wholes or sub-parts across all models. This suggests that tokenizers are better aligned with mid-level structures (parts), rather than whole objects or fine-grained sub-parts. However, Visual-Token Purity (VTP) remains low across all models and levels of granularity, indicating that images containing the same part-label are not consistently assigned to the same visual tokens, reflecting fragmentation in the clustering. PNMI values are generally higher for sub-parts than for parts or wholes, particularly in models like llamagen-vq-ds16-c2i, which shows the highest PNMI across all levels. This implies that tokenizers can capture more fine-grained information at the sub-part level, though the corresponding decrease in part purity for sub-parts suggests that while they can reduce uncertainty about part labels, their actual clustering of sub-parts is inconsistent."
        },
        {
            "title": "3 ARE VISUAL LANGUAGES STRUCTURED LIKE NATURAL LANGUAGES?",
            "content": "In subsection 2.5 we showed that visual languages are not very compressible using Huffman encodings, suggesting that visual languages may not have hierarchical structures similar to those of natural languages. To inquire further into this question, we test whether Context-free Grammars (Chomsky & Schützenberger, 1959) can approximate the structure of visual languages as well as they can natural languages by fitting grammars to each modality using unsupervised grammar induction techniques. Particularly, we use Compound Probabilistic Context-Free Grammars (C-PCFG) (Kim et al., 2019) as the grammar formalism for our experiments. C-PCFGs are type of neural PCFG, where grammar production rules are modeled as compound probability distributions (Robbins, 1956) every production depends 8 Dataset PPL PPL-R MBF FR COCO-DE COCO-EN COCO-VQ XM3600 CC12M ILSVRC SPIN 24.70 25.18 671.93 739.37 595.01 654.25 656. 99.61% 99.62% 95.80% 95.40% 96.28% 95.92% 95.89% 3.00 3.02 1.41 6.65 2.85 1.93 1.27 2.44 2.43 1.75 6.39 2.54 2.28 1. CU 1.00 1.00 0.97 0.33 1.00 0.93 0.73 (a) Generated parse tree statistics Figure 6: Comparison between C-PCFG grammars trained on textual and visual languages. Grammars learned over text exhibit greater reduction in perplexities (PPL-R). Both modalities present comparable parse tree heights (FR), right-branching propensity (MBF), Non-terminal codebook utilization (CU), and non-terminal node label frequencies (b). (b) Parse tree non-terminal node frequencies on both the set of symbols in the grammar as well as global latent variable z. This formulation, trained with variational methods, allows for global sentence information to flow through all parsing decisions in sentence while remaining compatible with efficient inference methods which standard PCFGs enjoy (Baker, 1979). For more details on C-PCFGs see Appendix J.2. C-PCFG memory costs are cubic on sentence length, leading us to use the compvis-vq-f8-64 tokenizer for visual grammars, which provides tractable 32 tokens per image. For each dataset, we train grammars over five seeds for 15 epochs and select the seed with the lowest test set perplexity for analysis. We test our pipeline by evaluating parsers learned on English COCO captions (COCO-EN) against silver-label parse trees extracted with Benepar (Kitaev & Klein, 2018), attaining an F1 score of 49 on the best seed, which is comparable to prior work (Zhao & Titov, 2020). We report test set statistics over learned grammars, such as final parse tree perplexity (PPL) and percentage reduction in perplexity (PPL-R) from random initialization to convergence. The mean branching factor (Li et al., 2024) (MBF) measures on average whether generated parse trees tend to branch right or left. This is achieved by averaging the proportion of leaves between the right and left branches of nodes across parse trees in the dataset: MBF(t)= 1 (cid:88) nt CR(n) CL(n) (2) Here, CR and CL represent the counts of leaves in the right and left branches of node, respectively. To get better sense of parse tree topology, we also measure the ratio between tree height (the length of the longest path in the tree) and the minimum possible height for the tree: FR(t)= H(t) logL(t) (3) Where H(t) and L(t) are the height and number of tokens in the input sequence, respectively. Codebook utilization (CU) measures the percentage of non-terminal labels utilized within generated parse trees. We present these statistics in Figure 6a, as well as normalized non-terminal node frequencies for parse trees generated by each grammar in Figure 6b, with some example parse trees in Figure J.1. Although both modalities experience great reduction in perplexity compared to random initialization, textual grammars (COCO-EN and COCO-DE) generally exhibit greater reductions in perplexities than visual grammars, corroborating findings from subsection 2.5 which suggest that visual tokens are not as compressible as textual tokens. Although visual grammars converge to PPL values an order of magnitude greater than the textual grammars, we observed that their PPL values at the start of training are proportionally higher, likely due to the generally longer visual sentence length (32 tokens in these experiments). All other measures are generally comparable across modalities both modalities show similar proclivities towards right-branching trees (MBF), although visual grammars are somewhat more balanced. Both modalities present similar tree heights (FR), with the non-terminal label codebooks being largely utilized. The notable exception to these trends is the grammar trained on XM3600 tokens. XM3600 contains significantly lower number of training examples (one order of magnitude less than SPIN, and two orders less than all other datasets), which may have resulted in degenerate grammar being learned. 9 Table 5: Summary of Procrustes/Hausdorff alignment distances between vision languages and natural languages on the MS-COCO dataset. While in general, all languages are poorly co-aligned, in general, vision languages align slightly, but significantly, more strongly with natural languages than they do with other vision models. Distance Language Language-to-Vision Distance Language-to-Natural Language Distance Closest Language Closest Distance Procrustes Haussdorf Natural (Average) Chameleon VQ-VAE (256) VQ-VAE (64) VQ-VAE (ImageNet) Natural (Average) Chameleon VQ-VAE (256) VQ-VAE (64) VQ-VAE (ImageNet) 0.96689 0.00425 0.97699 0.0158 0.97886 0.01427 0.97875 0.01517 0.97896 0.01478 10.81174 1.28073 7.68661 0.58783 7.24126 0.27003 7.97373 0.29399 7.52335 0.24214 0.96530 0.00735 0.96474 0.00382 0.96532 0.00401 0.97024 0.00310 0.96731 0.00386 9.42697 1.15902 6.56173 0.38642 5.95511 0.34980 6.90376 0.42660 5.91748 0. text-hr text-no text-ko text-it text-hu text-pl text-ko text-zh text-it text-hr 0.96333 0.95580 0.95381 0.96329 0.95709 7.60177 5.85738 5.36121 6.02011 4.92164 These results suggest that the structure of visual languages may not be as well approximated by context-free grammars as natural languages are. This raises the question of whether they may be better fit by other grammatical formalisms, such as mildly context-sensitive grammars (Yang et al., 2023) which allow for dependencies to cross between token spans."
        },
        {
            "title": "3.1 TOPOLOGICAL SIMILARITY",
            "content": "To expand our discussion on structural similarity, we further investigate how similar the topological structures of visual and textual tokens are, and whether these similarities can reveal meaningful insights about the underlying representations, i.e. can we observe strong structural alignment points between the natural and visual latent spaces, or are there notable deviations? We begin by training GloVe embeddings (Pennington et al., 2014) on co-occurrence matrices derived from visual tokens and textual tokens present in the captions (details in Appendix J). This gives us continuous topology of similar dimension within which we can explore potential alignment. We then explore two pairwise distance matrices between the two GloVe vector spaces: Procrustes alignment (Gower, 1975) and directed Haussdorf distance (Bowen, 1979). Figure J.2 gives the Procrustes similarity and Figure J.3 gives the directed Haussdorf distance between the models, with some key aggregates summarized in Table 5. While there are few clear trends, key finding is that vision models are largely more aligned with natural language models than they are with each other, with Chameleon being slightly more central than other models (perhaps due to its training process). Overall, the lack of strong alignment trends between different vision models highlights that their latent spaces are more fragmented, suggesting that visual token representations are often model-specific or task-dependent, rather than universally structured. Notably, however, some languages align much better with visual models than others (such as Korean to the Chameleon tokenizers, or Hungarian/Polish in general), suggesting that some tokenizers may be significantly stronger when aligning to specific languages. Another interesting observation is that the directed Hausdorff distance shows that the natural language to vision model alignment is significantly further than the vision model to natural language alignment. This results implies that generation of images from text is much harder than the generation of text from images - something often observed in practice. Given the overall distances between these structural representations, our experiments suggest that future model architectures should focus on reducing this asymmetry. Specialized models that effectively encode multimodal information - and perhaps aligned tokenization methods (such as CLIP), represent promising future directions for research."
        },
        {
            "title": "4 CONCLUSION",
            "content": "This paper takes first look at visual languages from the angle of empirical statistics. While there are similarities between how we currently treat visual and natural languages/sentences - the experiments in this paper show that, at least statistically, visual tokens and natural languages are far from trivially aligned. Such poor statistical alignments motivate both unique model architectures and training procedures for visual transformers - and we hope that this work inspires further research into novel architectures, designs, and hyper-parameters for vision-token based models. Indeed, while some of the hypotheses that we outlined in 10 this paper have already been demonstrated, many of the suggestions (such as increasing frequency penalties when decoding visual languages) remain untested in practice - and it is interesting and necessary future work to close the loop on such potential modifications. We hope, as whole, that this work inspires additional research into fundamental statistics as motivation for new architectural decisions and directions."
        },
        {
            "title": "REFERENCES",
            "content": "Jeff Alstott, Ed Bullmore, and Dietmar Plenz. powerlaw: python package for analysis of heavy-tailed distributions. PloS one, 9(1):e85777, 2014. 17 Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Spice: Semantic propositional image caption evaluation. In European conference on computer vision, pp. 382398. Springer, 2016. 6 Yutong Bai, Xinyang Geng, Karttikeya Mangalam, Amir Bar, Alan Yuille, Trevor Darrell, Jitendra Malik, and Alexei Efros. Sequential modeling enables scalable learning for large vision models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2286122872, 2024. 2 James Baker. Trainable grammars for speech recognition. The Journal of the Acoustical Society of America, 65(S1):S132S132, 1979. 9, Frank Benford. The law of anomalous numbers. Proceedings of the American philosophical society, pp. 551572, 1938. 6, 19 Rufus Bowen. Hausdorff dimension of quasi-circles. Publications Mathématiques de lIHÉS, 50:1125, 1979. 10 Ozan Caglayan, Pranava Madhyastha, and Lucia Specia. tion evaluation metrics: cautionary tale. ference on Computational Linguistics, pp. 23222328. putational Linguistics, December 2020. https://aclanthology.org/2020.coling-main.210. Curious case of language generaIn Proceedings of the 28th International ConInternational Committee on ComURL doi: 10.18653/v1/2020.coling-main.210. David M. Chan, Austin Myers, Sudheendra Vijayanarasimhan, David A. Ross, Bryan Seybold, and John F. Canny. Whats in caption? dataset-specific linguistic diversity and its effect on visual description models and metrics. In IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2022, New Orleans, LA, USA, June 19-20, 2022, pp. 47394748. IEEE, 2022. doi: 10.1109/CVPRW56347.2022.00520. 6 Noam Chomsky and Marcel Schützenberger. The algebraic theory of context-free languages. In Studies in Logic and the Foundations of Mathematics, volume 26, pp. 118161. Elsevier, 1959. 8 Kenneth Ward Church. Word2vec. Natural Language Engineering, 23(1):155162, 2017. Michael Crosier and Lewis Griffin. Zipfs law in image coding schemes. In BMVC, pp. 110. Citeseer, 2007. 3 Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. Imagenet: large-scale hierarchical In 2009 IEEE Computer Society Conference on Computer Vision and Pattern image database. Recognition (CVPR 2009), 20-25 June 2009, Miami, Florida, USA, pp. 248255. IEEE Computer Society, 2009. doi: 10.1109/CVPR.2009.5206848. 16 Godfrey Dewey. Relative frequency of English speech sounds. PhD thesis, Harvard Graduate School of Education, 1921. 3 Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1287312883, 2021. 3, Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-based text-to-image generation with human priors. In European Conference on Computer Vision, pp. 89106. Springer, 2022. 15 11 Alexander Gelbukh and Grigori Sidorov. Zipf and heaps laws coefficients depend on language. In Computational Linguistics and Intelligent Text Processing: Second International Conference, CICLing 2001 Mexico City, Mexico, February 1824, 2001 Proceedings 2, pp. 332335. Springer, 2001. 3 Jennifer Golbeck. Benfords law applies to word frequency rank in english, german, french, spanish, and italian. Plos one, 18(9):e0291337, 2023. 6 John Gower. Generalized procrustes analysis. Psychometrika, 40:3351, 1975. Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1069610706, 2022. 2 Harold Stanley Heaps. Information retrieval: Computational and theoretical aspects. Academic Press, Inc., 1978. 5, 17 Gustav Herdan. Quantitative linguistics. (No Title), 1964. 5, 17 Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: In Proceedings of the 2021 Conferreference-free evaluation metric for image captioning. ence on Empirical Methods in Natural Language Processing, pp. 75147528. Association for Computational Linguistics, November 2021. doi: 10.18653/v1/2021.emnlp-main.595. URL https://aclanthology.org/2021.emnlp-main.595. 6 Jung-Ha Hong. Benfords law in linguistic texts: Its principle and applications. Language and Information, 14(1):145163, 2010. 6 Wei-Ning Hsu, Yao-Hung Hubert Tsai, Benjamin Bolte, Ruslan Salakhutdinov, and Abdelrahman Mohamed. Hubert: How much can bad teacher benefit asr pre-training? In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 65336537. IEEE, 2021. 8, 25 David Huffman. method for the construction of minimum-redundancy codes. Proceedings of the IRE, 40(9):10981101, 1952. 7 Yoon Kim, Chris Dyer, and Alexander Rush. Compound probabilistic context-free grammars for grammar induction. In Anna Korhonen, David Traum, and Lluís Màrquez (eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 23692385, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1228. URL https://aclanthology.org/P19-1228. 8, 27, 28 Diederik Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. George Kingsley Zipf. Selected studies of the principle of relative frequency in language. Harvard university press, 1932. 3, 17 Nikita Kitaev and Dan Klein. Constituency parsing with self-attentive encoder. In Iryna Gurevych and Yusuke Miyao (eds.), Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 26762686, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1249. URL https://aclanthology.org/P18-1249. 9 Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. International journal of computer vision, 128(7):19561981, 2020. 15 Boyi Li, Rodolfo Corona, Karttikeya Mangalam, Catherine Chen, Daniel Flaherty, Serge Belongie, Kilian Weinberger, Jitendra Malik, Trevor Darrell, and Dan Klein. Re-evaluating the need for visual signals in unsupervised grammar induction. In Kevin Duh, Helena Gomez, and Steven Bethard (eds.), Findings of the Association for Computational Linguistics: NAACL 2024, pp. 11131123, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-naacl.70. URL https://aclanthology.org/2024.findings-naacl.70. 12 Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pp. 740755. Springer, 2014. 2, 16 Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 1, 2, 7 Benoît Mandelbrot. Contribution à la théorie mathématique des jeux de communication. In Annales de lISUP, volume 2, pp. 3124, 1953. Chengzhi Mao, Lu Jiang, Mostafa Dehghani, Carl Vondrick, Rahul Sukthankar, and Irfan Essa. Discrete representations strengthen vision transformer robustness. arXiv preprint arXiv:2111.10493, 2021. 2 Brenda McCowan, Sean Hanser, and Laurance Doyle. Quantitative tools for comparing animal communication systems: information theory applied to bottlenose dolphin whistle repertoires. Animal behaviour, 57(2):409419, 1999. 3 José Alberto Pérez Melián, Alberto Conejero, and Cesar Ferri Ramirez. Zipfs and benfords laws in twitter hashtags. In Proceedings of the Student Research Workshop at the 15th Conference of the European Chapter of the Association for Computational Linguistics, pp. 8493, 2017. 6 Steven Miller. Benfords law. Princeton University Press, 2015. 6 Josh Myers-Dean, Jarek Reynolds, Brian Price, Yifei Fan, and Danna Gurari. Spin: Hierarchical segmentation with subpart granularity in natural images. arXiv preprint arXiv:2407.09686, 2024. 8, Mostofa Patwary, Milind Chabbi, Heewoo Jun, Jiaji Huang, Gregory Diamos, and Kenneth Church. In 2019 IEEE International Parallel and Distributed Processing Language modeling at scale. Symposium (IPDPS), pp. 590599. IEEE, 2019. 3 Jeffrey Pennington, Richard Socher, and Christopher Manning. GloVe: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 15321543. Association for Computational Linguistics, October 2014. doi: 10.3115/v1/D14-1162. URL https://aclanthology.org/D14-1162. 10, 27 Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. ArXiv preprint, abs/2204.06125, 2022. 1, 2 Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems, 32, 2019. 2 Herbert Robbins. An Empirical Bayes Approach to Statistics. Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability, pp. 157163, 1956. 8, 28 Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1068410695, 2022. 3, 15 Daniel Ruderman. Origins of scaling in natural images. Vision research, 37(23):33853398, 1997. 3 Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115:211252, 2015. Malcolm Sambridge, Hrvoje Tkalˇcic, and Jackson. Benfords law in the natural sciences. Geophysical research letters, 37(22), 2010. 6 Claude Shannon. Prediction and entropy of printed english. Bell system technical journal, 30(1):5064, 1951. 7 Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: cleaned, In Proceedings of the 56th hypernymed, image alt-text dataset for automatic image captioning. Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 25562565. Association for Computational Linguistics, July 2018. doi: 10.18653/v1/P18-1238. URL https://aclanthology.org/P18-1238. 2, 16 Herbert Simon. On class of skew distribution functions. Biometrika, 42(3/4):425440, 1955. 5 Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. 2, 3, 15 Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. 1, 2, 3, 15 Ashish Thapliyal, Jordi Pont Tuset, Xi Chen, and Radu Soricut. Crossmodal-3600: massively multilingual multimodal evaluation dataset. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 715729, 2022. 2, Karl-Heinz Tödter. Benfords law as an indicator of fraud in economics. German Economic Review, 10 (3):339351, 2009. 6 Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. 8 Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou. Training data-efficient image transformers & distillation through attention. In International conference on machine learning, pp. 1034710357. PMLR, 2021. 6 Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. John Willis and Udny Yule. Some statistics of evolution and geographical distribution in plants and animals, and their significance. Nature, 109(2728):177179, 1922. 17 Songlin Yang, Roger Levy, and Yoon Kim. Unsupervised discontinuous constituency parsing with mildly context-sensitive grammars. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 57475766, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10. 18653/v1/2023.acl-long.316. URL https://aclanthology.org/2023.acl-long.316. 10 Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl3: Towards long image-sequence understanding in multi-modal large language models. arXiv preprint arXiv:2408.04840, 2024. 8 Shuiyuan Yu, Chunshan Xu, and Haitao Liu. Zipfs law in 50 languages: its structural pattern, linguistic interpretation, and cognitive motivation. arXiv preprint arXiv:1807.01855, 2018. Yanpeng Zhao and Ivan Titov. Visually grounded compound PCFGs. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 43694379, Online, November 2020. Association for Computational Linguistics. URL https://aclanthology.org/2020.emnlp-main.354. 9 doi: 10.18653/v1/2020.emnlp-main.354. Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024."
        },
        {
            "title": "APPENDIX",
            "content": "The appendix consists of the following further discussion: Appendix discusses the tokenizers used when constructing the visual languages, with detailed descriptions of Chameleon, Stable Diffusion, and LlamaGen tokenizers. Appendix describes the datasets utilized in this work, including Conceptual Captions (CC12M), MS-COCO, ImageNet (ILSVRC), XM-3600, and SPIN. Appendix describes potential limitations and opportunities for future work. Appendix describes the Zipf experiments in subsection 2.2, and gives additional experimental details. Appendix describes Heaps law, and gives additional experimental results to complement subsection 2.3. Appendix explains the Yule-Simon distribution, the methodology used to fit this distribution to observed token frequencies, and the experimental results from token frequency analysis. Appendix discusses the process used for analyzing visual tokens according to Benfords law in subsection 2.2, including n-gram extraction and first-digit distribution analysis across datasets. Appendix explains the Huffman encoding experiments, measuring entropy and compression efficiency of tokenized visual data. Appendix explores segmentation granularity and how visual tokens correspond to parts and sub-parts in images, using co-occurrence metrics like Part Purity and Visual Token Purity. Appendix discusses CPFCGs, the process for extracting GloVe embeddings from both vision and language tokenizers, and the topological analysis used in subsection 3.1."
        },
        {
            "title": "A TOKENIZERS",
            "content": "In this work, we explore three families of VQ-VAE (Van Den Oord et al., 2017) based tokenizers for images. While the general details are given in Table 1, we expand on the details for the tokenizers here. Chameleon (Team, 2024): Chameleon is family of early-fusion token-based mixed-modal models capable of understanding and generating images and text. The image tokenizer, chameleon-512, is based on Gafni et al. (2022), which is modified VQGAN Esser et al. (2021) model which adds perceptual losses to specific image regions such as faces and salient objects (in an attempt to improve the fidelty of generated images). The chameleon tokenizer is trained from scratch on closed-source set of licensed images, and encodes images at resolution of 512512 into discrete token codebook size of 8192 and dimension 256. Notably, when training the tokenizer the model up-samples the percentage of images with faces by two times to improve performance on human face generation (which may somewhat skew the performance of the tokenizer on non-face based images). Stable Diffusion (Compvis) (Rombach et al., 2022): Stable Diffusion is latent text-to-image diffusion model, which learns joint distribution over image and text representations in discretized latent space. Similar to the chameleon tokenizer, these tokenizers are trained in an adversarial manner following Esser et al. (2021) on OpenImages Kuznetsova et al. (2020), such that patch-based discriminator can differentiate original images from reconstructions. The stable diffusion tokenizers (compvis-vq-f8-64 and compvis-vq-f8-256) have an image resolution of 384384 with crop-size of 256, and use codebook dimension of size 4, with very high VQ quantization dimension of 16384. While these models were trained at crop size of 256, for grammatical analysis, many of the generated sequences are much too long to solve using traditional methods. Thus, we additionally consider model, compvis-vq-f8-64 which uses 6464 crop of the image, which produces linearized sequences of more manageable length of 32, used in section 3. The tokenizer compvis-vq-imagenet-f16-1024-256 (originally trained by Esser et al. (2021)) uses the same training procedure as those in Rombach et al. (2022), but was trained on the ImageNet dataset, with codebook of dimension 256, and size 1024. LlamaGen (Sun et al., 2024): LlamaGen is family of image-generation models that apply next-token prediction to perform iamge synthesis. The LlamaGen tokenizer, llamagen-vq-ds16-c2i takes images of resolution 256256, and uses codebook of size 16384 and dimension 8. llamagen-vq-ds16-c2i is trained on the ImageNet training dataset. 15 Tokenizer chameleon-512 compvis-vq-f8-64 compvis-vq-f8-256 compvis-vq-imagenet-f16-1024-256 llamagen-vq-ds16-c2i R-FID R-IS PSNR PSIM SSIM - - 1.14 4.98 2.19 - - 201.92 - - - - 23.07 - 20.79 - - 1.17 - - - - 0.650 - 0.675 Table A.1: Tokenizer Performance (As available in the original papers) - Evaluated on ImageNet 50K Validation dataset."
        },
        {
            "title": "B DATASETS",
            "content": "In this work, we explore the effects of tokenization across several datasets: Conceptual Captions (12M) (Sharma et al., 2018): Conceptual captions (12M, CC12M) is dataset with approximately 12 million image-text pairs soruce from web alt-text, traditionally used for vision-language pre-training. MS-COCO (Lin et al., 2014): The MS-COCO dataset is dataset for image description containing 328K images, each with 5 ground truth descriptions in English. In addition to the standard annotations, we also leverage translated annotations from Thapliyal et al. (2022), which provide machine translations into 36 languages for each of the MS-COCO images. ImageNet (ILSVRC) (Deng et al., 2009): ImageNet contains approximately 1.2M images which are manually annotated to indicate the objects present in each image. These annotations are linked to the WordNet hierarchy, providing rich set of object categories. The dataset covers 1,000 object classes for the classification task, including common objects like animals, vehicles, and household items. XM-3600 (Thapliyal et al., 2022): The Crossmodal-3600/XM3600 dataset is multilingual multimodal evaluation dataset designed to support image captioning tasks across 36 languages. It consists of 3600 geographically diverse images, each annotated with human-generated captions that are consistent across languages but not derived from direct translations, ensuring linguistic naturalness and cultural relevance. The images were selected from regions where these languages are spoken, drawn from the Open Images Dataset using careful algorithm to ensure regional diversity. SPIN (Myers-Dean et al., 2024): The SPIN (SubPartImageNet) dataset is hierarchical semantic segmentation dataset designed to provide detailed annotations for natural images at multiple levels of granularity, specifically focusing on objects, parts, and subparts. SPIN builds on the PartImageNet dataset, expanding its scope by introducing over 106,000 subpart annotations across 203 subpart categories, covering 34 part categories from diverse objects such as animals, vehicles, and human figures. The dataset contains 10,387 images divided across 11 supercategories, including rigid objects like cars and non-rigid entities like animals."
        },
        {
            "title": "C LIMITATIONS",
            "content": "While this paper does have significant empirical results, we want to recognize the several potential limitations/opportunities for future work: Tokenizer Selection: While the paper does focus on fairly wide range of common (and modern) visual tokenizers, there is fairly large potential selection of additional tokenizers that could be compared. Indeed, key limiting factor is that all of the tokenizers explored in this work are VQ-VAE based. As discussed in subsection 2.1, detailed analysis of continuous tokenizers (such as auto-encoders which are KL-regularized, CLIP-style encoders, or BERT-style encoders) would provide significant additional information. Directly applying natural language statistics to these continuous embeddings, however, is non-trival, as to understand ideas of token frequency or grammar, such analyses would have to either (a) be extended to the continuous domain, or (b) the tokens themselves would have to be quantized to discrete representations. We believe that such extensions are highly interesting, but are worthy of detailed analysis and discussion which is outside the scope of this initial work. Dataset Coverage: Another limiting factor of this research is the dataset coverage. While it is impossible to analyze all data, visual information is highly diverse, and domains such as medical imaging, geospatial imaging, or autonomous driving may have entirely different statistics. In general, however, we found that across the datasets that we did use (which represent fairly general slice of traditional training data), the statistical representations were similar. For example, it is fairly challenging to distinguish any dataset-level 16 patterns in Figure D.1, which shows per-dataset breakdown of the empirical token frequency distributions, or Figure F.2 which shows the Yule-Simon fits for emprical token frequencies. Scan Order of Images: One of the notable limitations of this work is that we primarily investigate linear row-wise scan order of the images. We primarily limit our experiments to this scan order as (1) this is the defacto scan order used in all existing transformer-based tokenization schemes and (2) we do not want to introduce further confounding analytical axes in this work. Exploring non-row-wise scan orders is, however, an extremely interesting question. In our limited experimentation, we found that row-wise scan order does not significantly impact the explorations in the paper, as the majority of the analyses are scan-order independent. Token Granularity and Semantic Understanding: Although granularity analysis is insightful, deeper examination of how well visual tokens capture complex semantic meaning in images (e.g., context, object relationships, or scene understanding) remains future research. We strongly believe that future research should explore how visual tokens represent not just parts of objects but also their roles in broader scenes or tasks requiring semantic understanding (e.g., visual reasoning, narrative generation), however such explorations would require signficant new labeled data, or novel statistical approaches. Visual Tokens in Video Data: In tasks like video understanding or motion tracking, the temporal relationships between visual tokens might reveal additional complexities not captured in static image analysis. Future research could explore how the behavior of visual tokens changes in sequential or temporal data settings and whether current statistical patterns hold when accounting for time. ZIPFS LAW As discussed in subsection 2.2, Zipfs Law (Kingsley Zipf, 1932), describes power-law relationship between the frequency of words and their rank in language where small number of high-frequency words dominate natural language, while the majority of words occur infrequently. Formally, Zipfs law states that: f(r)rα+σZ (D.1) where f(r) is the frequency of the element with rank and α/σ parameterize learned Gaussian distribution (close to 1/0 in many natural languages). For each dataset and tokenizer, to compute the power law fit, we leverage the method/code in Alstott et al. (2014). When fitting the power laws, because of computational limits, we limit the number of processed N-grams to 5M, and on CC12M and ILSVRC, unless otherwise noted, we compute the n-grams on only subset of the full dataset consisting of randomly sub-sampled 200K image set). Results broken down by N-gram are shown in Figure 2, while results broken down by model/dataset are given in Figure D. HEAPS/HERDANS LAW Heaps law (also referred to as Herdans law) is an empirical rule that describes the relationship between the size of corpus and the number of unique word in the corpus (Heaps, 1978; Herdan, 1964). Specifically, it predicts that as the size of text grows, the number of unique words increases, but at decreasing rate. Mathematically, the law is described by: (N)=kN β (E.1) where V(N) is the number of distinct words (the vocabulary size), is the total number of words, and and β are parameters, 0<β <1. Heaps law reflects the fact that even as new text is added to corpus, the frequency of newly introduced words diminishes, meaning large corpus doesnt proportionally expand its vocabulary. Plots for unique tokens vs. images seen on XM-3600 are given in Figure 3, with those for MS-COCO given in Figure E.1. YULE-SIMON DISTRIBUTION The Yule-Simon distribution (Willis & Yule, 1922) is model often used to describe processes where new elements (in this case, tokens) are introduced over time with probability that decreases as the existing set of elements grows. Specifically, for sequence of tokens, the Yule-Simon distribution describes the probability of the k-th token occurring times as: 17 (a) Chameleon (b) VQ-VAE (64) (c) VQ-VAE (256) (d) VQ-VAE (ImageNet) (e) LlamaGen (f) Chameleon (g) VQ-VAE (64) (h) VQ-VAE (256) (i) VQ-VAE (ImageNet) (j) LlamaGen *** Memory Error *** (k) Chameleon (l) VQ-VAE (64) (m) VQ-VAE (256) (n) VQ-VAE (ImageNet) (o) LlamaGen (p) Chameleon (q) VQ-VAE (64) (r) VQ-VAE (256) (s) VQ-VAE (ImageNet) (t) LlamaGen (u) Chameleon (v) VQ-VAE (64) (w) VQ-VAE (256) (x) VQ-VAE (ImageNet) (y) LlamaGen cc12m cc12m-full coco ilsvrc spin xm3600 (z) Chameleon (aa) VQ-VAE (64) (ab) VQ-VAE (256) (ac) VQ-VAE (ImageNet) (ad) LlamaGen Figure D.1: Empirical N-gram distributions for different datasets comparing normalized log-rank against normalized log-frequency. In general, visual languages do not achieve power-law distributions, and when they do, it is at high levels of N, and fairly steep slopes (compared to natural langauges). where B(,) is the Beta function. This captures the balance between token reuse and token innovation, and the shape parameter α reflects the likelihood of encountering novel token versus reusing an existing one. (m)=αB(m,α+1) (F.1) F.1 EXPERIMENTAL DESIGN For each dataset and tokenizer configuration on the COCO and XM-3600 datasets, we fit the Yule-Simon distribution to the observed token frequency distributions by minimizing the negative log-likelihood using the L-BFGS-B optimization algorithm. This method is selected due to its ability to handle the bound constraints placed on the parameter α, ensuring that α>0. The optimization starts with an initial guess of α=1.0, and the negative log-likelihood is computed based on the observed token frequencies. The optimization process continues until convergence, with the final α value corresponding to the best-fit parameter for the YuleSimon distribution. Invalid α values are penalized by assigning an infinite log-likelihood to ensure feasible solutions. Once the optimal α is found, we compute the empirical PMF from the frequency distributions by normalizing the observed token counts. In parallel, the theoretical PMF is computed using the fitted α value. 18 (a) 1-grams (b) 2-grams (c) 3-grams (d) 5-grams Figure E.1: Comparison of unique tokens as function of images seen on the MS-COCO dataset for different N-grams. F.2 ADDITIONAL EXPERIMENTAL RESULTS The full experimental results on text data for the XM-3600 dataset are shown in Figure F.1, with model data shown in Figure F.2. The text results for COCO are shown in Figure F.3, with COCO model convergence shown in Figure F.4."
        },
        {
            "title": "G BENEFORDS LAW",
            "content": "Benfords Law (Benford, 1938) describes the distribution of leading digits in many naturally occurring datasets, where smaller digits are more likely to appear as the first digit. Specifically, the probability (d) of digit (where is between 1 and 9) being the leading digit is given by: (cid:18) (d)=log10 1+ (cid:19) 1 (G.1) According to this law, the number 1 appears as the first digit around 30% of the time, while larger digits like 9 appear less frequently, around 5% of the time. For each dataset and tokenization configuration, we extract n-grams (with = 1, 2, and 3) from tokenized text and image data. We aggregate the token frequencies by computing the distribution of the first digits of these counts. Specifically, the first digits of each token frequency are extracted, and their occurrences are counted to form first-digit distribution. In cases where natural language data is available, we also compute aggregate distributions across multiple locales for text-based tokenizations. The aggregated text distributions include the mean, standard deviation, minimum, and maximum values for each first-digit count across different locales. 19 (a) Arabic (ar) (b) Bengali (bn) (c) Czech (cs) (d) Danish (da) (e) German (de) (f) Greek (el) (g) Spanish (es) (h) English (en) (i) Persian (fa) (j) Finnish (fi) (k) Filipino (fil) (l) French (fr) (m) Hindi (hi) (n) Croatian (hr) (o) Hungarian (hu) (p) Indonesian (id) (q) Italian (it) (r) Hebrew (he) (s) Japanese (ja) (t) Korean (ko) (u) Maori (mi) (v) Dutch (nl) (w) Norwegian (no) (x) Polish (pl) (y) Portuguese (pt) (z) Romanian (ro) (aa) Russian (ru) (ab) Swedish (sv) (ac) Swahili (sw) (ad) Telugu (te) (ae) Thai (th) (af) Turkish (tr) (ag) Ukrainian (uk) (ah) Vietnamese (vi) (ai) Chinese (zh) Figure F.1: Log-log fits on XM-3600 for various languages (Simon model, n=1) The full results for each of the datasets (XM-3600, CC12M, COCO, ILSVRC and SPIN) is given in Figure G.1. HUFFMAN ENCODING / ENTROPY Huffman encoding is widely-used algorithm for lossless data compression, which assigns variable-length codes to tokens based on their frequencies. The core idea is to minimize the total number of bits required to represent the token stream by assigning shorter codes to more frequent tokens and longer codes to less frequent ones. This is achieved by constructing binary tree where each token is leaf, and its depth (or code length) corresponds to its frequency. The encoding process ensures that the total number of bits, LHuffman, needed to encode stream of tokens is reduced compared to fixed-length encoding, where each token would require log2(n) bits, with being the number of unique tokens. Entropy, denoted as H(X), represents the theoretical limit on the average number of bits needed to encode the token stream, and is calculated using Shannons entropy formula: H(X)= (cid:88) (x)log2P (x) xX (H.1) where (x) is the empirical probability of token in the stream. In this experiment, entropy serves as benchmark for comparing the performance of Huffman encoding. The closer the average code length of the Huffman encoding is to the entropy, the more efficient the compression. By evaluating the compression rate and percentage reduction, we can quantify how effectively Huffman encoding reduces the bit length compared to the fixed-length encoding, with the goal of approaching the entropy limit. 20 ***** Convergence Error ***** (a) Chameleon-512 (1-gram) (b) Chameleon-512 (2-gram) (c) Chameleon-512 (3-gram) (d) VQ-F8-64 (1-gram) (e) VQ-F8-64 (2-gram) ***** Convergence Error ***** (f) VQ-F8-64 (3-gram) (g) VQ-F8-256 (1-gram) (h) VQ-F8-256 (2-gram) ***** Convergence Error ***** (i) VQ-F8-256 (3-gram) (j) VQ-Imagenet-F16-1024-256 (1-gram) (k) VQ-Imagenet-F16-1024-256 (2-gram) (l) VQ-Imagenet-F16-1024-256 (3-gram) (m) LlamaGen-VQ-DS16-C2I (1-gram) (n) LlamaGen-VQ-DS16-C2I (2-gram) (o) LlamaGen-VQ-DS16-C2I (3-gram) Figure F.2: N-gram analysis on various models (Simon model on XM-3600) H.1 EXPERIMENTAL DESIGN In our experiments, for each dataset/tokenizer combination (both =1 (unigrams) and =2 (bigrams)), we extract the first 500,000 tokens as token stream, and then apply Huffman encoding to compress this token stream. We extract several key metrics from the resulting encoding: Average Code Length: The weighted average of the lengths of Huffman codes for all tokens. Entropy: The theoretical minimum average code length for the specified token distribution (See Equation H.1). Fixed Code Length: The length of the fixed-length codes used for comparison. Original Bits: The number of bits required for fixed-length encoding of the token stream. Huffman bits: The number of bits required after applying Huffman encoding. (a) Arabic (ar) (b) Bengali (bn) (c) Czech (cs) (d) Danish (da) (e) German (de) (f) Greek (el) (g) Spanish (es) (h) English (en) (i) Persian (fa) (j) Finnish (fi) (k) Filipino (fil) (l) French (fr) (m) Hindi (hi) (n) Croatian (hr) (o) Hungarian (hu) (p) Indonesian (id) (q) Italian (it) (r) Hebrew (he) (s) Japanese (ja) (t) Korean (ko) (u) Maori (mi) (v) Dutch (nl) (w) Norwegian (no) (x) Polish (pl) (y) Portuguese (pt) (z) Romanian (ro) (aa) Russian (ru) (ab) Swedish (sv) (ac) Swahili (sw) (ad) Telugu (te) (ae) Thai (th) (af) Turkish (tr) (ag) Ukrainian (uk) (ah) Vietnamese (vi) (ai) Chinese (zh) Figure F.3: Log-log fits on COCO for various languages (Simon model, = 1). The horizontal shift in the English frequencies is likely caused by duplicate unfiltered captions in the empirical distribution. Compression Rate: The ratio of the original bits to the Huffman bits. Percentage Reduction: The percent reduction in the total number of bits after applying Huffman encoding. H.2 FURTHER EXPERIMENTAL RESULTS The full experimental results for the Huffman coding experiment are given in Table H.1. surprising detailed result is that the chameleon tokenizer, the most effective of the tokenizers, is also the most compressible representation of them, with almost twice the percentage reduction compared to other models. Llama-gen is the least compressible, and indeed, is almost completely incompressible, suggesting it has very efficient token use but does not contain any repeatable structure. Table H.1: Full huffman coding results for N=1 and N=2. ACL:Average Code Length, E: Entropy, FC: Fixed Code Length, OB: Original Bits (MB), HB: Huffman Bits (MB), CR: Compression Rate, PR: Percentage Reduction Dataset Model coco coco coco coco coco coco coco coco coco coco coco coco chameleon-512 compvis-vq-f8-64 compvis-vq-f8-256 compvis-vq-imagenet-f16-1024-256 llamagen-vq-ds16-c2i text-ar text-bn text-cs text-da text-de text-el text-es 1 1 1 1 1 1 1 1 1 1 1 1 ACL 11.30 9.78 9.71 8.80 13.98 9.85 8.91 9.80 8.09 8.70 8.55 7.98 11.27 9.74 9.67 8.76 13.95 9.82 8.88 9.77 8.06 8.67 8.53 7.95 FCL 12 10 10 9 14 15 14 15 14 15 14 14 OB 6.00 5.00 5.00 4.50 7.00 7.50 7.00 7.50 7.00 7.50 7.00 7.00 HB 5.65 4.89 4.85 4.40 6.99 4.93 4.46 4.90 4.05 4.35 4.28 3.99 CR 1.06 1.02 1.03 1.02 1.00 1.52 1.57 1.53 1.73 1.72 1.64 1.75 PR 5.86 2.25 2.93 2.22 0.18 34.31 36.36 34.64 42.21 41.98 38.90 42.99 Continued on next page 22 Dataset Model coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 spin spin spin spin spin cc12m cc12m cc12m cc12m cc12m ilsvrc ilsvrc ilsvrc ilsvrc ilsvrc coco coco coco coco coco coco coco text-fa text-fi text-fil text-fr text-hi text-hr text-hu text-id text-it text-he text-ja text-ko text-mi text-nl text-no text-pl text-pt text-ro text-ru text-sv text-sw text-te text-th text-tr text-uk text-vi text-zh chameleon-512 compvis-vq-f8-64 compvis-vq-f8-256 compvis-vq-imagenet-f16-1024-256 llamagen-vq-ds16-c2i text-ar text-bn text-cs text-da text-de text-el text-es text-fa text-fi text-fil text-fr text-hi text-hr text-hu text-id text-it text-he text-ja text-ko text-mi text-nl text-no text-pl text-pt text-ro text-ru text-sv text-sw text-te text-th text-tr text-uk text-vi text-zh chameleon-512 compvis-vq-f8-64 compvis-vq-f8-256 compvis-vq-imagenet-f16-1024-256 llamagen-vq-ds16-c2i chameleon-512 compvis-vq-f8-64 compvis-vq-f8-256 compvis-vq-imagenet-f16-1024-256 llamagen-vq-ds16-c2i chameleon-512 compvis-vq-f8-64 compvis-vq-f8-256 compvis-vq-imagenet-f16-1024-256 llamagen-vq-ds16-c2i chameleon-512 compvis-vq-f8-64 compvis-vq-f8-256 compvis-vq-imagenet-f16-1024-256 llamagen-vq-ds16-c2i text-ar text-bn 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 ACL 8.20 10.14 7.47 8.12 8.22 9.66 8.89 8.33 8.33 9.90 7.87 8.70 6.83 7.96 8.12 9.84 8.05 8.49 9.70 8.18 8.63 9.67 8.67 9.05 9.72 8.15 8.80 11.27 9.77 9.69 8.79 13.98 10.91 7.64 10.13 8.76 9.31 10.32 8.53 9.08 11.03 7.82 8.44 7.54 10.55 10.08 8.74 9.12 10.25 8.48 9.75 7.54 8.42 8.76 10.27 8.86 9.09 10.29 8.72 8.58 8.47 8.60 10.03 10.65 8.68 9.63 11.24 9.75 9.65 8.77 13.97 11.28 9.76 9.64 8.73 13.89 11.27 9.78 9.69 8.78 13.98 18.80 18.29 18.12 17.08 18.94 14.99 14.17 8.17 10.11 7.44 8.09 8.19 9.63 8.87 8.30 8.30 9.87 7.83 8.67 6.80 7.93 8.09 9.80 8.02 8.47 9.67 8.15 8.60 9.64 8.64 9.02 9.68 8.12 8.77 11.24 9.73 9.66 8.75 13.95 10.89 7.62 10.09 8.73 9.29 10.30 8.49 9.06 11.01 7.79 8.41 7.52 10.53 10.05 8.71 9.09 10.22 8.45 9.73 7.51 8.40 8.74 10.25 8.82 9.06 10.26 8.68 8.54 8.44 8.57 10.00 10.62 8.65 9.60 11.20 9.71 9.62 8.74 13.95 11.25 9.73 9.60 8.70 13.86 11.24 9.74 9.66 8.75 13.96 18.79 18.28 18.11 17.05 18.92 14.97 14.15 FCL 13 16 14 14 14 15 15 13 14 15 14 14 13 14 15 15 14 14 15 15 14 15 13 15 15 12 14 12 10 10 9 14 14 12 14 13 14 14 13 13 14 13 13 12 14 14 13 13 14 13 13 12 13 13 14 13 14 14 13 13 13 12 14 14 12 14 12 10 10 9 14 12 10 10 9 14 12 10 10 9 14 19 19 19 18 19 18 17 OB 6.50 8.00 7.00 7.00 7.00 7.50 7.50 6.50 7.00 7.50 7.00 7.00 6.50 7.00 7.50 7.50 7.00 7.00 7.50 7.50 7.00 7.50 6.50 7.50 7.50 6.00 7.00 6.00 1.18 5.00 4.50 7.00 0.80 0.49 0.66 0.85 1.44 0.80 1.14 1.21 0.78 1.14 1.51 1.38 0.95 0.96 1.33 1.37 1.33 1.44 0.99 0.68 0.87 0.92 0.90 1.08 1.65 1.09 0.82 0.99 0.71 1.12 0.98 1.07 1.61 1.43 6.00 5.00 5.00 4.50 7.00 6.00 5.00 5.00 4.50 7.00 6.00 5.00 5.00 4.50 7.00 9.50 9.50 9.50 9.00 9.50 9.00 8.50 HB 4.10 5.07 3.74 4.06 4.11 4.83 4.45 4.17 4.17 4.95 3.93 4.35 3.42 3.98 4.06 4.92 4.02 4.24 4.85 4.09 4.32 4.84 4.33 4.53 4.86 4.08 4.40 5.63 1.16 4.85 4.39 6.99 0.62 0.31 0.48 0.58 0.96 0.59 0.75 0.84 0.62 0.69 0.98 0.87 0.72 0.69 0.89 0.96 0.97 0.94 0.74 0.43 0.56 0.62 0.66 0.74 1.07 0.80 0.55 0.66 0.46 0.81 0.70 0.82 1.17 0.98 5.62 4.87 4.83 4.39 6.99 5.64 4.88 4.82 4.36 6.94 5.64 4.89 4.85 4.39 6.99 9.40 9.14 9.06 8.54 9.47 7.50 7.09 CR 1.59 1.58 1.87 1.72 1.70 1.55 1.69 1.56 1.68 1.51 1.78 1.61 1.90 1.76 1.85 1.53 1.74 1.65 1.55 1.83 1.62 1.55 1.50 1.66 1.54 1.47 1.59 1.06 1.02 1.03 1.02 1.00 1.28 1.57 1.38 1.48 1.50 1.36 1.52 1.43 1.27 1.66 1.54 1.59 1.33 1.39 1.49 1.43 1.37 1.53 1.33 1.59 1.54 1.48 1.36 1.47 1.54 1.36 1.49 1.52 1.53 1.40 1.40 1.31 1.38 1.45 1.07 1.03 1.04 1.03 1.00 1.06 1.02 1.04 1.03 1.01 1.06 1.02 1.03 1.02 1.00 1.01 1.04 1.05 1.05 1.00 1.20 1.20 PR 36.94 36.65 46.63 42.02 41.31 35.58 40.70 35.91 40.50 33.98 43.81 37.87 47.45 43.11 45.87 34.43 42.52 39.36 35.30 45.48 38.36 35.53 33.31 39.64 35.23 32.07 37.15 6.09 2.34 3.05 2.38 0.17 22.05 36.29 27.65 32.60 33.48 26.28 34.41 30.14 21.18 39.86 35.08 37.14 24.62 28.01 32.76 29.86 26.81 34.81 24.97 37.16 35.20 32.58 26.63 31.88 35.07 26.49 32.95 34.03 34.84 28.33 28.37 23.93 27.65 31.22 6.37 2.54 3.49 2.55 0.18 6.03 2.40 3.62 3.01 0.80 6.08 2.21 3.09 2.40 0.13 1.03 3.74 4.65 5.13 0.30 16.71 16.63 Continued on next page 23 Dataset Model coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco coco xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 xm3600 spin spin spin spin spin cc12m cc12m cc12m cc12m cc12m ilsvrc ilsvrc ilsvrc ilsvrc ilsvrc text-cs text-da text-de text-el text-es text-fa text-fi text-fil text-fr text-hi text-hr text-hu text-id text-it text-he text-ja text-ko text-mi text-nl text-no text-pl text-pt text-ro text-ru text-sv text-sw text-te text-th text-tr text-uk text-vi text-zh chameleon-512 compvis-vq-f8-64 compvis-vq-f8-256 compvis-vq-imagenet-f16-1024-256 llamagen-vq-ds16-c2i text-ar text-bn text-cs text-da text-de text-el text-es text-fa text-fi text-fil text-fr text-hi text-hr text-hu text-id text-it text-he text-ja text-ko text-mi text-nl text-no text-pl text-pt text-ro text-ru text-sv text-sw text-te text-th text-tr text-uk text-vi text-zh chameleon-512 compvis-vq-f8-64 compvis-vq-f8-256 compvis-vq-imagenet-f16-1024-256 llamagen-vq-ds16-c2i chameleon-512 compvis-vq-f8-64 compvis-vq-f8-256 compvis-vq-imagenet-f16-1024-256 llamagen-vq-ds16-c2i chameleon-512 compvis-vq-f8-64 compvis-vq-f8-256 compvis-vq-imagenet-f16-1024-256 llamagen-vq-ds16-c2i 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ACL 15.07 12.97 13.80 13.46 12.87 13.07 15.41 12.30 12.93 13.01 14.84 14.57 13.28 13.27 15.24 12.08 13.48 10.90 13.16 13.11 15.07 13.02 13.55 14.78 13.28 13.86 15.02 13.10 14.18 14.75 12.45 14.08 18.79 16.68 18.10 17.04 18.94 14.49 11.30 13.64 13.13 14.04 14.16 12.83 13.66 14.58 12.35 12.82 11.30 14.47 14.41 12.84 13.70 14.58 12.85 14.00 11.54 12.50 12.92 14.26 13.40 13.49 14.30 13.17 12.98 12.11 12.63 14.23 14.49 12.96 14.31 18.80 18.26 18.09 17.06 18.94 18.58 18.23 17.75 16.76 18.85 18.76 18.29 18.09 17.01 18.93 15.05 12.95 13.78 13.43 12.84 13.04 15.39 12.28 12.90 12.99 14.81 14.55 13.25 13.24 15.22 12.06 13.46 10.88 13.13 13.09 15.05 12.99 13.52 14.76 13.25 13.83 15.00 13.08 14.15 14.73 12.42 14.06 18.77 16.63 18.09 17.01 18.92 14.43 11.27 13.59 13.11 13.98 14.10 12.80 13.61 14.51 12.32 12.79 11.27 14.46 14.39 12.80 13.65 14.52 12.82 13.98 11.51 12.48 12.90 14.23 13.37 13.46 14.28 13.14 12.96 12.06 12.58 14.22 14.48 12.94 14.25 18.78 18.25 18.08 17.04 18.92 18.57 18.22 17.73 16.73 18.83 18.74 18.28 18.08 16.99 18.91 FCL 18 17 17 17 17 17 18 16 17 17 18 18 17 17 18 16 17 16 17 17 18 17 17 18 17 17 18 17 17 18 16 17 19 17 19 18 19 16 14 15 15 16 15 15 16 16 15 16 15 16 16 15 16 16 15 16 14 15 15 16 15 16 16 15 15 15 15 16 16 15 16 19 19 19 18 19 19 19 19 18 19 19 19 19 18 19 OB 9.00 8.50 8.50 8.50 8.50 8.50 9.00 8.00 8.50 8.50 9.00 9.00 8.50 8.50 9.00 8.00 8.50 8.00 8.50 8.50 9.00 8.50 8.50 9.00 8.50 8.50 9.00 8.50 8.50 9.00 8.00 8.50 9.50 1.95 9.50 9.00 9.50 0.79 0.52 0.60 0.88 1.51 0.75 1.19 1.37 0.78 1.21 1.72 1.60 0.97 0.98 1.43 1.55 1.40 1.55 1.09 0.73 0.88 0.95 0.92 1.14 1.77 1.13 0.83 1.04 0.71 1.30 1.01 1.11 1.91 1.52 9.50 9.50 9.50 9.00 9.50 9.50 9.50 9.50 9.00 9.50 9.50 9.50 9.50 9.00 9.50 HB 7.54 6.49 6.90 6.73 6.43 6.53 7.70 6.15 6.46 6.51 7.42 7.29 6.64 6.63 7.62 6.04 6.74 5.45 6.58 6.56 7.54 6.51 6.78 7.39 6.64 6.93 7.51 6.55 7.09 7.37 6.22 7.04 9.40 1.91 9.05 8.52 9.47 0.72 0.42 0.55 0.77 1.32 0.71 1.02 1.17 0.71 1.00 1.38 1.21 0.88 0.88 1.22 1.33 1.28 1.33 0.96 0.60 0.73 0.82 0.82 1.02 1.49 1.01 0.73 0.90 0.57 1.09 0.90 1.01 1.65 1.36 9.40 9.13 9.05 8.53 9.47 9.29 9.12 8.87 8.38 9.42 9.38 9.14 9.04 8.51 9. CR 1.19 1.31 1.23 1.26 1.32 1.30 1.17 1.30 1.32 1.31 1.21 1.24 1.28 1.28 1.18 1.32 1.26 1.47 1.29 1.30 1.19 1.31 1.25 1.22 1.28 1.23 1.20 1.30 1.20 1.22 1.29 1.21 1.01 1.02 1.05 1.06 1.00 1.10 1.24 1.10 1.14 1.14 1.06 1.17 1.17 1.10 1.21 1.25 1.33 1.11 1.11 1.17 1.17 1.10 1.17 1.14 1.21 1.20 1.16 1.12 1.12 1.19 1.12 1.14 1.16 1.24 1.19 1.12 1.10 1.16 1.12 1.01 1.04 1.05 1.05 1.00 1.02 1.04 1.07 1.07 1.01 1.01 1.04 1.05 1.06 1.00 PR 16.27 23.70 18.82 20.85 24.30 23.12 14.40 23.11 23.97 23.46 17.58 19.04 21.89 21.96 15.31 24.50 20.69 31.89 22.59 22.87 16.26 23.43 20.29 17.90 21.89 18.49 16.53 22.94 16.60 18.06 22.20 17.15 1.09 1.87 4.72 5.35 0.32 9.44 19.31 9.06 12.48 12.28 5.62 14.44 14.62 8.90 17.64 19.86 24.68 9.54 9.96 14.40 14.35 8.88 14.33 12.52 17.55 16.67 13.87 10.87 10.68 15.69 10.63 12.21 13.45 19.29 15.82 11.06 9.41 13.60 10.57 1.05 3.90 4.77 5.21 0.30 2.20 4.05 6.60 6.90 0.81 1.26 3.76 4.80 5.49 0.36 (a) Chameleon-512 (1-gram) (b) Chameleon-512 (2-gram) (c) Chameleon-512 (3-gram) (d) VQ-F8-64 (1-gram) (e) VQ-F8-64 (2-gram) (f) VQ-F8-64 (3-gram) (g) VQ-F8-256 (1-gram) (h) VQ-F8-256 (2-gram) (i) VQ-F8-256 (3-gram) (j) VQ-Imagenet-F16-1024-256 (1-gram) (k) VQ-Imagenet-F16-1024-256 (2-gram) (l) VQ-Imagenet-F16-1024-256 (3-gram) (m) LlamaGen-VQ-DS16-C2I (1-gram) (n) LlamaGen-VQ-DS16-C2I (2-gram) ***** Convergence Error ***** (o) LlamaGen-VQ-DS16-C2I (3-gram) Figure F.4: N-gram analysis on various models (Simon model on XM-3600)"
        },
        {
            "title": "I SEGMENTATION GRANULARITY",
            "content": "In subsection 2.6, we explore at what level visual tokens/words correlate with parts/sub-parts/wholes of objects in images. To analyze the co-occurrence of wholes, parts, and sub-parts, we primarily leverage the SPIN dataset, discussed in Appendix B, which provides labeled annotations for each of these levels in the images. To compute co-occurrence statistics, we first extract part-label-to-visual-token co-occurrence frequency matrix for each tokenizer and dataset. Each entry (i,j) of the matrix represents the number of times that visual token zi co-occurs with part-label yj (which could represent whole, part, or sub-part). From this co-occurrence matrix, we compute three metricsPart Purity, Visual Token Purity, and Part-Normalized Mutual Informationas described in Hsu et al. (2021). 25 (a) XM3600 (N=1) (b) XM3600 (N=2) (c) XM3600 (N=3) (d) XM3600 (N=5) (e) CC12M (N=1) (f) CC12M (N=2) (g) CC12M (N=3) (h) CC12M (N=5) (i) COCO (N=1) (j) COCO (N=2) (k) COCO (N=3) (l) COCO (N=5) (m) ILSVRC (N=1) (n) ILSVRC (N=2) (o) ILSVRC (N=3) (p) ILSVRC (N=5) (q) SPIN (N=1) (r) SPIN (N=2) (s) SPIN (N=3) (t) SPIN (N=5) Figure G.1: Benfords Law on XM-3600, CC12M, COCO, ILSVRC, and SPIN. Part Purity: Part purity describes the average probability of the most likely part-label for each visual-token, representing how accurately parts are assigned to the corresponding visual tokens. It is computed as: Part Purity (PP)=Ez[p(y(z)z)] (I.1) where is visual token cluster, y(z) denotes the most likely part-label for given visual-token z, p(y(z)z) is the conditional probability of the most likely part-label y(z) given the visual-token z, and Ez is the expectation over all visual tokens. In practice, we draw these probabilities from the normalized empirical co-occurrence matrix. Visual Token Purity: Visual token purity measures how well images containing the same part-label are consistently assigned to the same visual tokens. It is computed as: Visual Token Purity (VTP)=Ey[p(z(y)y)] (I.2) where is part-label, z(y) represents the most likely visual-token for given part-label y, p(z(y) y) is the conditional probability of the most likely visual-token z(y) given the part-label y, and Ey is the expectation over all part-labels. Similar to part-purity, these probabilities are derived from the normalized empirical co-occurrence matrix. Part-Normalized Mutual Information: Part-normalized mutual information (PNMI) is an informationtheoretic metric that quantifies the percentage of uncertainty about part-label eliminated after observing 26 visual-token. It is computed as: PNMI= I(y;z) H(y) = H(y)H(y z) H(y) =1 H(y z) H(y) (I.3) where I(y;z) is the mutual information between part-labels and visual tokens z, H(y) is the entropy of the part-labels, and H(y z) is the conditional entropy of the part-labels given the visual tokens. The entropy values are computed from the empirical co-occurrence frequency matrix, where each entry represents the joint probability p(y,z) of part-label and visual-token co-occurring. Specifically, H(y) is computed as: H(y)= p(yi)logp(yi) (cid:88) (I.4) where p(yi) is the marginal probability of part-label yi, derived by summing the joint probabilities p(yi,zj) across all visual-tokens zj. Similarly, the conditional entropy H(y z) is computed as: H(y z)= p(zj) p(yi zj)logp(yi zj) (cid:88) (cid:88) (I.5) where p(yi zj) is the conditional probability of part-label yi given visual-token zj, derived from the co-occurrence matrix by normalizing the joint probabilities p(yi,zj) by p(zj), the marginal probability of the visual-token zj. Higher PNMI values indicate that more information about the part-label is captured by the visual-token assignments. j"
        },
        {
            "title": "J TOPOLOGICAL ALIGNMENT OF VISION AND LANGUAGE TOKENS",
            "content": "J.1 GLOVE EMBEDDING OF VISION AND LANGUAGE TOKENS In order to get continuous representations of the vision and language token spaces, we employ GloVe embeddings Pennington et al. (2014). GloVe (Global Vectors for Word Representation) is word embedding technique that captures semantic relationships between words by training on global word co-occurrence statistics. Unlike local context methods like Word2Vec (Church, 2017), GloVe constructs matrix from word co-occurrence counts in corpus and factorizes this matrix to generate dense vector representations. These embeddings reflect the relative meanings of words, allowing similar words to have similar vectors in the latent space. GloVe aims to learn word embeddings by factorizing token co-occurrence matrix. The model minimizes weighted least squares objective function: (cid:88) = f(Xij) (cid:16) i wj +bi+bj logXij (cid:17)2 (J.1) i,j=1 where Xij is the co-occurrence count of token with token j, wi and wj are the token vectors for token and j, bi and bj are the bias terms, and f(Xij) is token co-occurrence based weighting function to discount frequent co-occurrences. In all of the analysis methods below, before applying analysis we whiten the data before normalization to avoid significant scale effects: ij = Xij σj , σj = (cid:118) (cid:117) (cid:117) (cid:116) 1 n (cid:88) (Xij µj)2, µj = i=1 1 (cid:88)"
        },
        {
            "title": "Xij",
            "content": "i=1 (J.2) where Xij is the original value of the i-th data point in the j-th feature, σj is the standard deviation of the j-th feature, and µj is the mean of the j-th feature. J.2 COMPOUND PROBABILISTIC CONTEXT-FREE GRAMMARS J.2.1 BACKGROUND Here we describe the basic background and formulation of Compound Probabilistic Context-free grammars (C-PCFGs) for convenience, much of this content is sourced from (Kim et al., 2019), which we point readers to for more thorough treatment of the topic. C-PCFGs extend the PCFG formalism. PCFGs are defined by 5-tuple = (S,N ,P,Σ,R), consisting of start symbol S, set of non-terminals , set of pre-terminals P, set of terminals Σ, and set of 27 derivation rules R: ABC AN AN ,B,C P P,w Σ The derivation rules are probabilistic, with their distribution denoted as π = {πr}rR. Inference may be performed efficiently over them using the inside algorithm (Baker, 1979). In neural variants of PCFGs, this distribution may be formulated as follows: πSA = πABC = πT = Af1(wS)) Af1(wS)) exp(u ΣAN exp(u exp(u ΣBCMexp(u exp(u ΣwΣexp(u BCwA) wf2(wT )) BCwA) wf2(wT )) where are transformation vectors for each production rule, are learnable parameter vectors for each symbol, and f1 and f2 are neural networks. Compound PCFGs (Kim et al., 2019) formulate rule probabilities as compound probability distribution (Robbins, 1956): pγ(z) πz =fλ(z,EG) Where is latent variable generated by prior distribution (a spherical Gaussian) and EG = {wN {S}N P} denotes the set of symbol embeddings. Rule probabilities πz are conditioned on this latent: πz,SA exp(u πz,ABC exp(u πz,T exp(u Af1([wS;z])), BC[wA;z]), wf2([wT ;z])) The latent allows global information to be shared across parsing decisions, while simultaneously respecting the context-free assumption when is fixed, allowing for efficient inference as before. C-PCFGs are optimized with variational methods (Kingma, 2013), since the introduction of makes inference intractable. At inference time, given sentence x, the variational inference network qϕ is used to produce the latent =µϕ(g(E(x))). Here, is sentence encoder used to generate vector representation given token embeddings E(x). For more details on C-PCFGs, we point readers to Kim et al. (2019). J.2.2 PARSE TREES In Figure J.1 we show an example parse tree generated with learned grammar for each dataset. J.3 PROCRUSTES ANALYSIS Procrustes Analysis is statistical method used to compare the shapes or structures of two datasets by finding an optimal transformation (including translation, scaling, and rotation) that minimizes the distance between corresponding points in the datasets. The resulting transformation provides insight into how closely the datasets align in their geometry. Procrustes Analysis minimizes the distance between two matrices and by finding the optimal translation, scaling, and rotation. The goal is to solve: min R,b,c bXR+cY (J.3) where: and are the two point sets (matrices) being compared, is the optimal rotation matrix, is scaling factor, is the translation vector, and is the Frobenius norm. For Procrustes analysis, it is required that the two matrices to be aligned have identical shape. Because the number of tokens is different in the vision and language cases, in our experiments we use K-means to quantize the different token embedding spaces to 256 centers, which we then compare topologically. This has the downside of reducing the topological comparisons to more global structure comparisons, however means that we can run experiments on point-to-point coherence. 28 Full results for our Procrustes analysis are given in Figure J.2. For simplicity and clarity, in Figure J.2, we replace the distance with 1distance to get similarity measure, and set the diagonal to 0 (even though the diagonal similarity is naturally 1), in order to avoid contrast issues on the off-diagonals. J.4 DIRECTED HAUSDORFF DISTANCE Directed Hausdorff Distance is measure used to quantify the degree of mismatch between two point sets by calculating the greatest distance from point in one set to the nearest point in the other set. It considers only the largest such deviation in one direction, making it useful for determining the extent to which one set is contained within or approximates another. The directed Hausdorff distance from set to set is defined as: dH(A,B)=max aA min bB ab (J.4) where and are two point sets, and ab is the Euclidean distance between point aA and point bB. Similar to Figure J.2, for simplicity and clarity, in Figure J.3 we replace the distance with 1 distance to get similarity measure, and set the diagonal to 0 (even though the diagonal similarity is naturally 1), in order to avoid contrast issues on the off-diagonals. Full results for the directed Haussdorf distance are given in Figure J.3. 29 (a) XM3600 (b) COCO-DE (c) COCO-VQ (d) COCO-EN (e) CC12M (f) SPIN (g) ILSVRC Figure J.1: Example parse trees for different datasets. 30 (a) XM- (b) MS-COCO Figure J.2: Procrustes similarity matrices for XM-3600 and MS-COCO across all models and languages. (a) XM-3600 (b) MS-COCO Figure J.3: Directed Haussdorf similarity matrices for XM-3600 and MS-COCO across all models and languages."
        }
    ],
    "affiliations": [
        "University of California, Berkeley",
        "The University of Tokyo, Tokyo"
    ]
}