{
    "paper_title": "VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control",
    "authors": [
        "Sixiao Zheng",
        "Minghao Yin",
        "Wenbo Hu",
        "Xiaoyu Li",
        "Ying Shan",
        "Yanwei Fu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video world models aim to simulate dynamic, real-world environments, yet existing methods struggle to provide unified and precise control over camera and multi-object motion, as videos inherently operate dynamics in the projected 2D image plane. To bridge this gap, we introduce VerseCrafter, a 4D-aware video world model that enables explicit and coherent control over both camera and object dynamics within a unified 4D geometric world state. Our approach is centered on a novel 4D Geometric Control representation, which encodes the world state through a static background point cloud and per-object 3D Gaussian trajectories. This representation captures not only an object's path but also its probabilistic 3D occupancy over time, offering a flexible, category-agnostic alternative to rigid bounding boxes or parametric models. These 4D controls are rendered into conditioning signals for a pretrained video diffusion model, enabling the generation of high-fidelity, view-consistent videos that precisely adhere to the specified dynamics. Unfortunately, another major challenge lies in the scarcity of large-scale training data with explicit 4D annotations. We address this by developing an automatic data engine that extracts the required 4D controls from in-the-wild videos, allowing us to train our model on a massive and diverse dataset."
        },
        {
            "title": "Start",
            "content": "VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control Sixiao Zheng1,2 , Minghao Yin3, Wenbo Hu4, Xiaoyu Li4, Ying Shan4, Yanwei Fu1,2 1Fudan University 2Shanghai Innovation Institute 3HKU 4ARC Lab, Tencent PCG Project Page: https://sixiaozheng.github.io/VerseCrafter_page/ 6 2 0 2 8 ] . [ 1 8 3 1 5 0 . 1 0 6 2 : r Figure 1. VerseCrafter enables precise control of camera motion and multi-object motion via 4D Geometric Control representation built from static background point cloud and per-object 3D Gaussian trajectories, producing videos that better follow the desired motion than Yume [61] and Uni3C [11] and closely match the ground-truth video."
        },
        {
            "title": "Abstract",
            "content": "Video world models aim to simulate dynamic, real-world environments, yet existing methods struggle to provide unified and precise control over camera and multi-object motion, as videos inherently operate dynamics in the projected 2D image plane. To bridge this gap, we introduce VerseCrafter, 4D-aware video world model that enables explicit and coherent control over both camera and object dynamics within unified 4D geometric world state. Our approach is centered on novel 4D Geometric Control representation, which encodes the world state through static background point cloud and per-object 3D Gaussian Corresponding authors. trajectories. This representation captures not only an objects path but also its probabilistic 3D occupancy over time, offering flexible, category-agnostic alternative to rigid bounding boxes or parametric models. These 4D controls are rendered into conditioning signals for pretrained video diffusion model, enabling the generation of high-fidelity, view-consistent videos that precisely adhere to the specified dynamics. Unfortunately, another major challenge lies in the scarcity of large-scale training data with explicit 4D annotations. We address this by developing an automatic data engine that extracts the required 4D controls from in-the-wild videos, allowing us to train our model on massive and diverse dataset. 1 1. Introduction Video world models learn to simulate environmental dynamics by generating future frame sequences conditioned on past observations and control signals, such as actions or camera trajectory [13, 29, 40, 46, 61]. They provide unified interface for visual prediction [31], navigation [7], and manipulation [23]. However, the reliance on video introduces fundamental challenge: while an ideal world model should simulate the full 4D spatiotemporal space to reflect our physical reality, videos inherently operate dynamics in the projected 2D image plane. To bridge this gap, recent works incorporate camera control into video generation using explicit 3D geometry [11, 115, 125], implicit pose embeddings [50], or learned movement embeddings [9, 12, 68]. However, these methods are often limited to static scenes or leave the motion of multiple objects uncontrolled. To control object motion, existing approaches typically rely on 2D cues such as point trajectories [96], optical flow [56], masks [123], or bounding boxes [89], which lack 3D awareness and often fail under large viewpoint changes. More advanced 3D-aware methods use depth maps [122], sparse 3D trajectories [15], 3D bounding boxes [92], or parametric human models like SMPL-X [11] to align camera and object motion in 3D space. Nevertheless, these control spaces are inadequate for representing multi-object dynamics in compact, flexible, and editable 4D state that is also naturally aligned with camera control. For instance, sparse trajectories are often noisy and incomplete, 3D bounding boxes impose rigid constraints ill-suited to natural objects, and SMPL-X representations are category-limited. Besides, several existing works focus on synthetic game environments [40, 109, 113], where precise annotations are available for training, yet, the modeling of complex, realistic 4D worlds with multi-object dynamics remains underexplored. Thus we propose VerseCrafter, realistic, dynamic video world model that allows explicit and precise control over camera and multi-object motion in unified 4D geometric control, as shown in Fig. 1. At its core is our novel 4D Geometric Control representation, which encodes the world state using static background point cloud for scene geometry and per-object 3D Gaussian trajectories to capture object dynamics. Each 3D Gaussian trajectory represents an objects probabilistic 3D occupancy over time: its mean defines the motion path, while its covariance captures the objects spatial extent and orientation. This probabilistic formulation provides soft, flexible, and category-agnostic approach to modeling diverse object shapes and motions, overcoming the limitations of rigid 3D bounding boxes or category-specific parametric models. Crucially, the background point cloud and per-object 3D Gaussian trajectories share common world coordinate system, enabling coherent and unified control over both camera and object motion. By rendering our 4D Geometric Control into target views, we condition frozen Wan2.1-14B video diffusion backbone [86] via lightweight GeoAdapter, an adapter-style branch inspired by ControlNet [117]. This enables the generation of high-fidelity videos that accurately reflect the underlying 4D world state with specified camera and object dynamics. Unlike 2D control signals, our 4D Geometric Control is inherently 3D-aware, i.e. view-consistent and robust to occlusions, making it more effective and reliable interface for video world modeling. Training VerseCrafter requires large-scale paired data of real-world videos and their corresponding 4D geometric controls. To this end, we constructed VerseControl4D, large-scale real-world dataset with automatically annotated camera and object trajectories needed to construct our 4D geometric controls. This dataset allows us to train VerseCrafter on massive and diverse set of real-world videos, significantly enhancing its generalization and performance. Our contributions are threefold: We introduce novel 4D Geometric Control representation that unifies camera and multi-object motion in shared 4D space. Its use of 3D Gaussian trajectories offers flexible and category-agnostic way to control object dynamics, overcoming the limitations of rigid, categoryspecific models. We present VerseCrafter, geometry-driven video world model that leverages our 4D Geometric Control to offer explicit and precise control over both camera and object motion. This enables the creation of high-fidelity, viewconsistent videos that accurately follow complex 4D instructions. We constructed an VerseControl4D, large-scale realworld dataset with automatically annotated camera and object trajectories. This breakthrough solves key data bottleneck, enabling us to train our model on massive and diverse real-world dataset for superior generalization. 2. Related Works Video World Models. World models learn environment dynamics from observations by predicting future states for downstream simulation, planning, and control [29, 30, 46]. Early visual world models adopt recurrent and latentvariable architectures [16, 24, 28, 62, 66, 85], while recent approaches use large-scale transformer and diffusion backbones to roll out high-fidelity videos conditioned on actions, text, or camera trajectories [1, 2, 6, 9, 12, 20, 35, 39, 44, 48, 68, 86, 101, 108, 113], and further extend temporal horizons with explicit memories or long-sequence models [50, 71, 99]. Geometry-aware works such as DeepVerse [13], Voyager [40], and Yume [61] incorporate 3D structure to support 4D video generation and exploration, but are mainly controlled via text, actions, or camera tokens and do not expose compact, editable 4D geometric state for 2 real multi-object dynamics. In contrast, VerseCrafter learns geometry-driven mapping from 4D Geometric Control to dynamically realistic videos, enabling disentangled control over camera and multi-object motion. 3D World Generation. Recent work leverages powerful 2D generative priors to synthesize explorable 3D environments from text, images, or videos [47, 119]. Early methods mainly target object-level or single-scene generation [19, 37, 72, 116, 118, 120], distilling image diffusion models [77] into NeRFs [63], implicit fields, meshes, or 3D Gaussian splats [43], or optimizing per-scene geometry from multi-view or panoramic observations [18, 78, 111, 112, 114]. More recent approaches scale up to navigable 3D worlds [7], combining depth estimation [106], cameraguided video diffusion, iterative inpainting, and panoramic inputs to construct roomor city-scale Gaussian scenes for exploration [14, 52, 58, 59, 79, 84, 90, 109, 127]. However, these pipelines largely model static, synthetic-like geometry and offer limited explicit control over real multi-object dynamics, whereas VerseCrafter operates on real-world videos and uses background point cloud plus per-object 3D Gaussian trajectories as an explicit 4D control state for worldconsistent dynamic video generation. Controllable Video Generation. Controllable video generation aims to steer camera and object motion via explicit conditioning signals. Camera-controlled models [3, 4, 33, 45, 51, 82, 104, 124] such as MotionCtrl [96] and CameraCtrl [32] inject camera extrinsics, Plucker-style encodings, or other 3D priors [11, 21, 27, 38, 73, 76, 97, 102, 115, 122, 125] into video diffusion models to achieve precise viewpoint control, but mostly assume static or weakly dynamic scenes. Object motion [10, 25, 34, 49, 53, 55, 60, 64, 65, 67, 74, 80, 81, 83, 87, 88, 94, 95, 98, 105, 107, 110, 121, 126] is typically controlled using 2D cues (bounding boxes, masks, trajectories, strokes, optical flow) as in Boximator [89], DragAnything [100], and MotionCanvas [103], or with more 3D-aware signals such as depth maps, sparse 3D trajectories, 3D boxes, or SMPL-X bodies in I2V3D [122], Uni3C [11], CineMaster [92], and Perception-as-Control [15]. While these methods substantially improve controllability, 2D controls remain view-dependent and fragile under large camera changes, and many 3D controls are categoryspecific, rigid, or tied to reconstruction-heavy pipelines. Recent approaches [15, 22, 26, 56, 92, 96, 103, 107, 125] begin to jointly control camera and object motion, but their control spaces are still fragmented rather than unified, compact world state. VerseCrafter instead introduces 4D Geometric Control: lightweight, category-agnostic world state where background point cloud and per-object 3D Gaussian trajectories in shared frame jointly drive camera and multi-object motion. 3 3. Method We propose VerseCrafter, geometry-driven video world model that maps an explicit 4D geometric world state into dynamic, realistic videos with disentangled control over camera and multi-object motion. Our design has two key components: (i) unified 4D Geometric Control (Sec. 3.1) representation defined in shared world coordinate frame, and (ii) lightweight GeoAdapter (Sec. 3.2) that injects rendered geometric signals into frozen Wan2.1-14B backbone, so that edits to the 4D state directly reshape the generated video while preserving Wan2.1s strong visual prior. Given an input frame, text prompt, and 4D Geometric Control, we model the world state as static background point cloud and per-object 3D Gaussian trajectories, render them into multi-channel control maps, and feed these maps into GeoAdapter attached to Wan2.1. 3.1. 4D Geometric Control We represent the state of the video world model as 4D geometric world state, which we term 4D Geometric Control. It is an explicit, editable state consisting of static background point cloud bg and per-object 3D Gaussian trajectories {Gt o}, all defined in shared world coordinate frame. Background point cloud. As in Fig. 2, we start from the input image, estimate monocular depth with MoGe2 [93], and obtain instance masks {Mo} with Grounded SAM2 [75], where the user selects one or more objects to be controlled via text prompts or clicks. With camera intrinsics and extrinsics (R1, t1), each pixel = (u, v, 1) with depth D1(u) is back-projected as p(u) = (cid:0)D1(u)K1u t1 (cid:1). (1) We use the instance masks to partition the reconstructed point cloud into per-object point clouds Po = (cid:8)xo,k (cid:12) (cid:12) xo,k = p(uk), uk Mo (cid:9), and static background cloud bg = (cid:8)p(u) (cid:12) (cid:12) / Mo (cid:9) = {pi}Nbg i=1. (cid:91) (2) (3) During generation, the background at frame is obtained by rendering bg with the camera pose, so that viewpoint changes are realized as rigid camera motion in fixed 3D world rather than by hallucinating new background at every frame. 3D Gaussian trajectories. single 3D Gaussian Go(x) = (x µo, Σo) in the world frame compactly encodes an objects position (through µo), approximate shape and size (through the eigenvalues of Σo), and orientation (through its eigenvectors). 3D Gaussian trajectory for object is then defined as sequence of Gaussians {Gt o}T t=1, Gt o(x) = (x µt o, Σt o), (4) Figure 2. Framework of VerseCrafter. Given an input image and text prompt, we first estimate depth and obtain user-specified object masks to construct 4D Geometric Control state consisting of static background point cloud and per-object 3D Gaussian trajectories in shared world frame. This state is rendered into background RGB/depth, 3D Gaussian trajectory RGB/depth, and soft control mask for each frame, forming multi-channel 4D control maps. The control maps are encoded and fed into the proposed GeoAdapter, which conditions frozen Wan2.1-14B video diffusion backbone together with text embeddings from umT5, enabling geometry-consistent video generation with precise control over camera and multi-object motion. whose means {µt o} trace the motion path in 3D, while the covariances {Σt o} capture how the objects spatial extent and orientation evolve over time. This probabilistic formulation describes the objects 3D occupancy in soft, continuous manner and yields compact control space that is more flexible than rigid 3D bounding boxes and more category-agnostic than parametric body models. To initialize the trajectory for each controllable object o, we fit full-covariance Gaussian to its point cloud Po obtained in the previous step: 1 No (xo,k µo)(xo,k µo), xo,k, Σo ="
        },
        {
            "title": "1\nNo",
            "content": "µo = (cid:88) (cid:88) which gives an initial Gaussian Go(x) o, Σt o, Σt The low-dimensional parameters {µt o} naturally support flexible, user-driven editing. In practice, we convert each Gt into an ellipsoid mesh for visualization in 3D editor such as Blender, and let the user specify or refine the trajectory by dragging and keyframing this ellipsoid in world space. The edited poses and shapes are mapped back to the {µt o} as control signals. The ellipsoids are only user interface; all conditioning maps used by our model are rendered directly from the underlying 3D Gaussians. Rendering 4D control maps. Given 4D Geometric Control, we render per-frame conditioning maps in the target camera views. For each frame t, we generate three types of maps: (i) background RGB/depth, RGBbg and Depthbg , by projecting the static cloud bg with the camera pose (Rt, tt); (ii) 3D Gaussian trajectory RGB/depth, RGBtraj and Depthtraj , by projecting the per-object Gaussians {Gt o} into soft elliptical footprints and taking depth from the cort responding ellipsoid surfaces; (iii) soft control mask Mt that indicates regions where the diffusion model should synthesize or overwrite content, obtained by inverting the valid background visibility and merging it with the projected 3D Gaussian footprints, followed by Gaussian smoothing. For the first frame = 1, we replace RGBbg 1 with input image and set M1 = 0, so that the first frame is preserved and only future frames are modified. Background and 3D Gaussian maps share the same world state but are rendered through decoupled channels, so camera edits only affect background branch and object edits only affect 3D Gaussian trajectory branch, enabling geometry-consistent control. (5) 3.2. VerseCrafter Architecture Backbone. We adopt Wan2.1-14B [86] as frozen latent video diffusion / flow-matching backbone with 3D VAE and DiT-based denoiser. VerseCrafter treats Wan2.1 as generic video prior: we do not change its architecture or weights, and instead attach lightweight geometric adapter that conditions the backbone on our 4D control maps. GeoAdapter. For each frame t, we take the rendered background and 3D Gaussian trajectory maps, RGBbg , Depthbg , RGBtraj , together with the soft control mask Mt. The four RGB/depth maps are encoded by the same 3D VAE as the video latent, while Mt is reshaped and interpolated to the latent resolution, following the practice in [41, 86]. Stacking along the temporal dimension yields spatiotemporal geometry tensor, which is concatenated channel-wise and aligned with the latent video tokens. GeoAdapter is lightweight DiT-style branch that operates on this geometry tensor. It shares the same token dimen- , Depthtraj 4 ometric Control annotations. As shown in Fig. 3, VerseControl4D is built through four stages: data collection, clip extraction, quality filtering, and data annotation. Data collection. from two VerseControl4D is built recent world-exploration datasets, Sekai-Real-HQ [54] and SpatialVID-HQ [91], which provide long in-the-wild videos with diverse outdoor and urban scenes, camera poses, and captions, but no object-motion labels. We take their high-resolution videos as the raw pool for constructing our 4D Geometric Control annotations. Clip extraction. We apply PySceneDetect to detect shots in the videos. For each shot longer than 81 frames, we uniformly sample an 81-frame sub-clip and discard shorter shots, matching the default temporal length used by the Wan2.1 backbone. Quality filtering. We apply an object-centric filtering pipeline to retain clips with clean geometry and controllable foreground. Using Grounded-SAM2 with prompts such as person . human . car . animal, we first obtain instance masks on the first frame and keep only clips whose controllable object count lies in [1, 6]. We then discard clips where any instance mask covers more than 20% of the image area. For human instances, we further remove clips whose masks touch image borders or whose aspect ratios fall outside [2, 4], as these typically correspond to severely truncated pedestrians. Finally, we apply visual-quality filtering (aesthetic and luminance scores) to exclude blurry or over-/under-exposed clips, yielding set of visually clean, structurally reliable videos. Data annotation. We then annotate each filtered clip with 4D Geometric Control. We first generate descriptive caption using Qwen2.5-VL-72B [5], which serves as the text prompt during training. For geometry, we adopt MegaSAM as the base pipeline and replace its monocular and metric depth modules with MoGe-2 [93] and UniDepth V2 [70], respectively, to obtain more accurate and temporally consistent depth. Given the video frames, the depth, and the camera trajectory, we reconstruct 3D point cloud for every frame. Applying Grounded-SAM2 instance masks on each frame to these point clouds yields per-object point clouds and static background point cloud bg, as described in Sec. 3.1. For each object, we then fit per-frame 3D Gaussians and connect them into 3D Gaussian trajectory {Gt o}. Finally, we render the 4D Geometric Control into modelready signals. The background point cloud is rendered with the camera trajectory to obtain background RGB, depth and mask. The 3D Gaussian trajectories are rendered into trajectory RGB, depth and mask. We invert the background mask and merge it with the trajectory mask to produce final merged mask that marks regions where the video diffusion model should synthesize content. In total, VerseControl4D contains 35,000 training samples and 1,000 validation samples. In the training set, about Figure 3. Starting from Sekai-Real-HQ and SpatialVID-HQ, we obtain 81-frame clips extraction, followed by quality filtering. For each retained clip, Qwen2.5-VL-72B, GroundedSAM2, and MegaSAM provide captions, object masks, depth, and camera poses, which are lifted into background/object point clouds, fitted with 3D Gaussian trajectories, and rendered as background/trajectory maps plus merged mask that constitute our 4D Geometric Control. sionality as the Wan-DiT blocks, but uses far fewer layers. We interleave GeoAdapter blocks with the frozen Wan-DiT: every k-th DiT block in Wan2.1 is paired with GeoAdapter block whose output is linearly projected back to the backbone width and added as residual modulation to the corresponding DiT block. Text prompts are encoded by umT5 [17] into text embeddings, which are injected into both Wans DiT blocks and GeoAdapter through the same text-conditioning interfaces. This adapter-based conditioning injects 4D geometric information into Wan2.1 with only small number of extra parameters, while keeping all backbone weights fixed. Inference. At inference time, VerseCrafter supports both independent control of camera or object motion and joint control of both within single unified framework. For camera-only control, we provide camera trajectory and background control maps while setting all trajectory-related channels (RGB/depth/mask) to zero. For object-only control, we keep the camera pose fixed, render static background branch (RGB/depth and its mask) from bg. For joint control, both branches are active and rendered from the same 4D world state, allowing VerseCrafter to adjust camera trajectory and multi-object motion in coordinated, geometry-consistent manner. 4. VerseControl4D Dataset To train and evaluate VerseCrafter on real, complex scenes with explicit 4D control, we construct VerseControl4D, real-world video dataset with automatically derived 4D Ge5 Figure 4. Qualitative comparison of joint camera and object motion control. Perception-as-Control often yields low-fidelity frames with inaccurate camera motion, Yume roughly follows the text-described motion but lacks precise control, and Uni3C is limited to human motion. VerseCrafter more faithfully follows both the camera trajectory and multi-object motion while maintaining sharp appearance and geometrically consistent backgrounds. Figure 5. Qualitative comparison of camera-only motion control on static scenes. ViewCrafter, Voyager, and FlashWorld often exhibit distorted facades, drifting structures, or inconsistent parallax along the camera path. VerseCrafter better follows the target trajectory while preserving sharp details and globally consistent 3D geometry. 26% of samples are sourced from Sekai-Real-HQ and 74% from SpatialVID-HQ, and 20% of the samples depict static scenes, encouraging VerseCrafter to learn both camera-only world exploration and coupled cameraobject dynamics. The validation set additionally includes 250 static-scene samples to specifically assess camera-only control. 5. Experiments Implementation Details. We build VerseCrafter on top of the Wan2.1 T2V-14B model. The Wan backbone is kept frozen and only the GeoAdapter is updated. Each GeoAdapter block is initialized from the weights of its paired DiT block in Wan2.1 to stabilize training, and we set = 5 so that every 5-th DiT block in Wan2.1 is paired with GeoAdapter block. We use the Adam optimizer with learning rate of 2e 5, 100 warmup steps, and constant-with-warmup learning-rate schedule. All experiments are conducted on 16 96GB GPUs with global batch size of 16. Training is performed in two stages: we first train for 2,500 iterations on 480P clips, and then fine-tune the same model for another 2,500 iterations on 720P clips. The total wallclock training time is about 380 hours. We adopt classifierfree guidance during training by randomly dropping the text condition with probability 0.1. At inference time, we use 50 denoising steps and classifier-free guidance scale of 5.0. Generating an 81-frame 720P video clip on 8 96GB GPU takes about 1152 seconds, with peak memory usage of about 90 GB. Evaluation Metrics. We evaluate overall video quality using VBench-I2V. For camera control, we follow prior camera-control work [32] and report rotation error (RotErr) and translation error (TransErr). For object-motion control, we adopt ObjMC proposed in MotionCtrl [96]. Given generated video, we run the same geometry annotation pipeline as in VerseControl4D to estimate its camera trajectory and 3D Gaussian trajectories, and compare them with the corresponding ground-truth trajectories from our dataset. ObjMC is computed as the average Euclidean distance between the estimated and ground-truth 3D Gaussian means over all controlled objects and frames. 6 Table 1. Joint camera and object motion control on VerseControl4D. We report VBench-I2V scores and 3D control metrics (RotErr, TransErr, ObjMC;). VerseCrafter achieves the best overall video quality and the most accurate joint control of camera and object motion. Overall Score Imaging Quality Aesthetic Quality Dynamic Degree Motion Smoothness Background Consistency Subject Consistency I2V Background I2V Subject RotErr TransErr ObjMC Perception-as-Control [15] Yume [61] Uni3C [11] Ours 83.66 85.47 83.55 88.10 66.81 71.16 68.06 72.70 53.34 52.39 53.16 57.49 73.91 72.24 66.09 86.26 96.89 98.96 98.94 98.79 93.19 95.66 93.74 95. 94.02 96.43 94.19 96.48 96.35 98.51 97.19 98.76 94.78 98.39 97.05 98.65 5.006 7.560 1.361 0.890 8.767 8.735 7.731 3.103 6.556 7.959 5.883 2. Table 2. Camera-only motion control on static scenes. On the static subset of VerseControl4D, we report VBench-I2V scores and camera control metrics RotErr / TransErr. VerseCrafter achieves the best overall visual quality while substantially reducing camera pose errors. Overall Score Imaging Quality Aesthetic Quality Dynamic Degree Motion Smoothness Background Consistency Subject Consistency I2V Background I2V Subject RotErr TransErr ViewCrafter [115] Voyager [40] FlashWorld [52] Ours 84.04 78.12 81.80 86.80 69.56 55.48 68.94 74.57 55.52 49.80 53.72 54.78 68.02 65.34 58.26 80. 97.86 99.39 98.81 97.62 92.09 92.31 91.88 94.88 94.25 91.55 94.44 95.55 97.70 86.02 94.40 97.86 97.29 85.03 93.93 98.79 2.101 3.557 2.748 0. 9.868 3.880 10.010 2.587 Figure 7. Ablation on depth-aware control. We compare VerseCrafter without depth inputs (Ours (w/o depth), top) and with RGB+depth control (middle) under the same camera trajectorym. Without depth, the model often misorders foreground and background, e.g., lampposts are pulled in front of distant buildingsand occlusion boundaries drift over time (red boxes). Adding depth restores consistent parallax and occlusion, producing geometry much closer to the ground truth. motion but fails to handle other categories such as vehicles. In contrast, VerseCrafter keeps multiple objects attached to their 3D Gaussian trajectories while accurately following the specified camera path, yielding sharp and temporally coherent videos. 5.2. Camera-Only Motion Control We evaluate camera-only control on the static-scene subset of VerseControl4D, where objects remain stationary and only the camera moves. As shown in Table 2, VerseCrafter achieves the best VBench-I2V performance among ViewCrafter, Voyager, FlashWorld, and our model, with consistent gains in Overall Score, Imaging Quality, and both background and subject consistency, while maintaining motion smoothness comparable to prior methods. On 3D camera metrics, VerseCrafter substantially reduces rotation and translation errors relative to the strongest baseline, indicating that it follows the target camera trajectory much more faithfully in static scenes. Qualitative comparisons in Fig. 5 further confirm these trends: baselines often exhibit bending walls, misaligned windows, or unstable paralFigure 6. Ablation on object-motion representations. We compare controlling objects with 3D point trajectory (top), 3D bounding boxe (middle), and 3D Gaussian trajectory (fourth). 3D point trajectory and 3D bounding boxe often cause scale drift and misaligned motion (red boxes), whereas 3D Gaussian trajectory track the intended camera trajectory and preserve plausible shapes and background interactions. 5.1. Joint Camera and Object Motion Control We first evaluate joint control of camera and object motion on VerseControl4D. As shown in Table 1, VerseCrafter achieves the best VBench-I2V scores among Perception-asControl, Yume, Uni3C, and our model, with clear gains in Overall Score, Imaging Quality, Aesthetic Quality, and both Subject/Background consistency. On 3D control metrics, VerseCrafter substantially reduces rotation, translation, and object-motion errors compared with the strongest baseline, reflecting much tighter alignment with the target 4D trajectories. Qualitative comparisons in Fig. 4 further highlight these differences: Perception-as-Control often produces low-quality frames with inaccurate camera motion; Yume, driven only by text descriptions of motion, roughly follows the desired direction but lacks precise trajectory control; and Uni3C, relying on SMPL-X, can control human 7 Table 3. Ablation study on 3D representation, depth, and decoupled controls. We compare different variants of VerseCrafter using VBench-I2V and 3D control metrics (RotErr, TransErr, ObjMC;). Our full model with 3D Gaussian trajectories, depth-aware rendering, and decoupled background/foreground controls achieves the best visual quality and the most accurate camera and object motion control. Overall Score Imaging Quality Aesthetic Quality Dynamic Degree Motion Smoothness Background Consistency Subject Consistency I2V Background I2V Subject RotErr TransErr ObjMC Ours (3D Bounding Box) Ours (3D Point Trajectory) Ours (w/o depth) Ours (BG & FG Merged) Ours 85.45 85.57 85.64 85.72 88. 69.23 70.29 70.19 69.19 72.70 55.70 55.27 55.00 54.86 57.49 78.57 78.23 80.60 83.72 86.26 98.70 98.63 98.66 98.65 98.79 92.92 94.00 92.07 91.15 95.69 93.27 92.75 92.83 92.86 96. 97.74 97.85 98.07 97.93 98.76 97.48 97.55 97.69 97.41 98.65 1.350 1.298 1.177 1.080 0.890 3.805 3.281 3.900 3.803 3.103 4.520 6.896 4.929 3.726 2.507 Effect of depth. To evaluate the effect of depth, we removed the depth channel from the background and trajectory controls (Ours (w/o depth) in Table 3). This variation resulted in lower overall score and significantly worse 3D control (higher RotErr and ObjMC values). As shown in Figure 7, without depth, the model frequently misorders foreground and background: vertical structures like streetlights appear next to shelves in the foreground, while buildings that should be behind the character are positioned elsewhere, and occlusion boundaries drift over time. With RGB+depth control, With RGB+depth control, VerseCrafter recovers more consistent parallax and occlusion, producing geometry much closer to the ground truth. Decoupled vs. merged controls. We further compare our decoupled design with variant that merges background and 3D Gaussian trajectory maps into single control stream (Ours (BG & FG Merged) in Table 3). Although this variant still benefits from the explicit 4D state, it consistently underperforms the full model on VBench, with particularly noticeable drop in object-motion accuracy (ObjMC increases from 2.51 to 3.73). As shown in Fig. 8, the merged control leads to clear degradation in motion control for moving people. In contrast, keeping decoupled design preserves static geometry while producing more precise and stable object motion, which is crucial for accurate and geometry-consistent control. 6. Conclusion We presented VerseCrafter, geometry-driven video world model that exposes an explicit 4D Geometric Control state, built from static background point cloud and perobject 3D Gaussian trajectories in shared world frame. Coupled with the GeoAdapter that conditions frozen Wan2.1 backbone, this design enables high-fidelity video generation with precise, disentangled control over camera and multi-object motion. To support training and evaluation, we constructed VerseControl4D, large-scale realworld dataset with automatically annotated camera and object trajectories. Experiments and ablations show that VerseCrafter delivers superior visual quality and more accurate 3D control than existing controllable video generators and world models, highlighting 4D Geometric Control as promising interface for future work on dynamic world simulation and editing. Figure 8. Ablation on decoupled background / foreground controls. We compare merging background and foreground controls into single map (Ours (BG & FG Controls Merged), top) with our default decoupled design (middle). When controls are merged, object motion control performance significantly degrades (red box), while the separation design preserves the static background and produces sharper, more stable object motion. lax along the path, whereas VerseCrafter preserves straight structures, stable depth relationships, and an appearance closer to the ground-truth video, evidencing precise camera control in static 3D world. 5.3. Ablation Study We conduct ablations to analyze three key design choices in VerseCrafter: (i) the object 3D representation in the control space, (ii) the use of depth in control maps, and (iii) the decoupling of background and foreground controls. All variants share the same training data, backbone, and optimization settings; only the control representation is changed. 3D representation of object motion. To isolate the effect of our motion representation, we derive two ablations from each per-frame 3D Gaussian: (1) an oriented 3D bounding box whose axes follow the Gaussians principal directions and whose side lengths scale with its principal spreads; and (2) 3D point trajectory that retains only the Gaussian centroid. The rest of the pipeline is unchangedwe simply rasterize cuboids (for boxes) or tiny disks/spheres (for points) instead of Gaussian ellipses. As reported in Table 3, replacing Gaussians with boxes slightly hurts both visual quality and control accuracy (Overall Score from 88.10 to 85.45; ObjMC from 2.51 to 4.52), while point trajectories give the weakest object-motion consistency (ObjMC = 6.90). Qualitatively  (Fig. 6)  , points and boxes often yield scale artifacts and misaligned motion, whereas 3D Gaussian trajectories better track the intended paths and preserve plausible object shapes."
        },
        {
            "title": "References",
            "content": "[1] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world founarXiv preprint dation model platform for physical ai. arXiv:2501.03575, 2025. 2 [2] Hassan Abu Alhaija, Jose Alvarez, Maciej Bala, Tiffany Cai, Tianshi Cao, Liz Cha, Joshua Chen, Mike Chen, Francesco Ferroni, Sanja Fidler, et al. Cosmos-transfer1: Conditional world generation with adaptive multimodal control. arXiv preprint arXiv:2503.14492, 2025. 2 [3] Sherwin Bahmani, Ivan Skorokhodov, Aliaksandr Siarohin, Willi Menapace, Guocheng Qian, Michael Vasilkovsky, Hsin-Ying Lee, Chaoyang Wang, Jiaxu Zou, Andrea Tagliasacchi, et al. Vd3d: Taming large video diffusion transformers for 3d camera control. arXiv preprint arXiv:2407.12781, 2024. 3 [4] Sherwin Bahmani, Ivan Skorokhodov, Guocheng Qian, Aliaksandr Siarohin, Willi Menapace, Andrea Tagliasacchi, David Lindell, and Sergey Tulyakov. Ac3d: Analyzing and improving 3d camera control in video diffusion transformers. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2287522889, 2025. 3 [5] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 5 [6] Philip J. Ball, Jakob Bauer, Frank Belletti, Bethanie Brownfield, Ariel Ephrat, Shlomi Fruchter, Agrim Gupta, Kristian Holsheimer, Aleksander Holynski, Jiri Hron, Christos Kaplanis, Marjorie Limont, Matt McGill, Yanko Oliveira, Jack Parker-Holder, Frank Perbet, Guy Scully, Jeremy Shar, Stephen Spencer, Omer Tov, Ruben Villegas, Emma Wang, Jessica Yung, Cip Baetu, Jordi Berbel, David Bridson, Jake Bruce, Gavin Buttimore, Sarah Chakera, Bilva Chandra, Paul Collins, Alex Cullum, Bogdan Damoc, Vibha Dasagi, Maxime Gazeau, Charles Gbadamosi, Woohyun Han, Ed Hirst, Ashyana Kachra, Lucie Kerley, Kristian Kjems, Eva Knoepfel, Vika Koriakin, Jessica Lo, Cong Lu, Zeb Mehring, Alex Moufarek, Henna Nandwani, Valeria Oliveira, Fabio Pardo, Jane Park, Andrew Pierson, Ben Poole, Helen Ran, Tim Salimans, Manuel Sanchez, Igor Saprykin, Amy Shen, Sailesh Sidhwani, Duncan Smith, Joe Stanton, Hamish Tomlinson, Dimple Vijaykumar, Luyu Wang, Piers Wingfield, Nat Wong, Keyang Xu, Christopher Yew, Nick Young, Vadim Zubov, Douglas Eck, Dumitru Erhan, Koray Kavukcuoglu, Demis Hassabis, Zoubin Gharamani, Raia Hadsell, Aaron van den Oord, Inbar Mosseri, Adrian Bolton, Satinder Singh, and Tim Rocktaschel. Genie 3: new frontier for world models. 2025. 2 [7] Amir Bar, Gaoyue Zhou, Danny Tran, Trevor Darrell, and Yann LeCun. Navigation world models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1579115801, 2025. 2, [8] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video syn9 In Proceedings of thesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 2256322575, 2023. 1 [9] Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. In Forty-first International Conference on Machine Learning, 2024. 2 [10] Ryan Burgert, Yuancheng Xu, Wenqi Xian, Oliver Pilarski, Pascal Clausen, Mingming He, Li Ma, Yitong Deng, Lingxiao Li, Mohsen Mousavi, et al. Go-with-the-flow: Motioncontrollable video diffusion models using real-time warped noise. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1323, 2025. 3 [11] Chenjie Cao, Jingkai Zhou, Shikai Li, Jingyun Liang, Chaohui Yu, Fan Wang, Xiangyang Xue, and Yanwei Fu. Uni3c: Unifying precisely 3d-enhanced camera and human motion controls for video generation. arXiv preprint arXiv:2504.14899, 2025. 1, 2, 3, [12] Haoxuan Che, Xuanhua He, Quande Liu, Cheng Jin, and Hao Chen. Gamegen-x: Interactive open-world game video generation. arXiv preprint arXiv:2411.00769, 2024. 2 [13] Junyi Chen, Haoyi Zhu, Xianglong He, Yifan Wang, Jianjun Zhou, Wenzheng Chang, Yang Zhou, Zizun Li, Zhoujie Fu, Jiangmiao Pang, et al. Deepverse: 4d autoregressive video generation as world model. arXiv preprint arXiv:2506.01103, 2025. 2 [14] Luxi Chen, Zihan Zhou, Min Zhao, Yikai Wang, Ge Zhang, Wenhao Huang, Hao Sun, Ji-Rong Wen, and Chongxuan Li. Flexworld: Progressively expanding 3d scenes for flexiable-view synthesis. arXiv preprint arXiv:2503.13265, 2025. 3 [15] Yingjie Chen, Yifang Men, Yuan Yao, Miaomiao Cui, and Liefeng Bo. Perception-as-control: Fine-grained controllable image animation with 3d-aware motion representation. arXiv preprint arXiv:2501.05020, 2025. 2, 3, 7 [16] Silvia Chiappa, Sebastien Racaniere, Daan Wierstra, and Shakir Mohamed. Recurrent environment simulators. arXiv preprint arXiv:1704.02254, 2017. 2 [17] Hyung Won Chung, Noah Constant, Xavier Garcia, Adam Roberts, Yi Tay, Sharan Narang, and Orhan Firat. Unimax: Fairer and more effective language sampling for large-scale multilingual pretraining. arXiv preprint arXiv:2304.09151, 2023. 5 [18] Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee, and Kyoung Mu Lee. Luciddreamer: Domain-free generation of 3d gaussian splatting scenes. arXiv preprint arXiv:2311.13384, 2023. 3 [19] Dana Cohen-Bar, Elad Richardson, Gal Metzer, Raja Giryes, and Daniel Cohen-Or. Set-the-scene: Global-local training for generating controllable nerf scenes. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 29202929, 2023. [20] Etched Decart, Quinn McIntyre, Spruce Campbell, Xinlei Chen, and Robert Wachen. Oasis: universe in transformer. URL: https://oasis-model. github. io, 2024. 2 [21] Wanquan Feng, Jiawei Liu, Pengqi Tu, Tianhao Qi, Mingzhen Sun, Tianxiang Ma, Songtao Zhao, Siyu Zhou, I2vcontrol-camera: Precise video camera and Qian He. control with adjustable motion strength. arXiv preprint arXiv:2411.06525, 2024. 3 [22] Wanquan Feng, Tianhao Qi, Jiawei Liu, Mingzhen Sun, Pengqi Tu, Tianxiang Ma, Fei Dai, Songtao Zhao, Siyu I2vcontrol: Disentangled and Zhou, and Qian He. arXiv preprint unified video motion synthesis control. arXiv:2411.17765, 2024. 3 [23] Stefano Ferraro, Pietro Mazzaglia, Tim Verbelen, and Bart Dhoedt. Focus: object-centric world models for robotic manipulation. Frontiers in Neurorobotics, 19:1585386, 2025. 2 [24] Chelsea Finn, Ian Goodfellow, and Sergey Levine. Unsupervised learning for physical interaction through video prediction. Advances in neural information processing systems, 29, 2016. 2 [25] Xiao Fu, Xian Liu, Xintao Wang, Sida Peng, Menghan Xia, Xiaoyu Shi, Ziyang Yuan, Pengfei Wan, Di Zhang, and Dahua Lin. 3dtrajmaster: Mastering 3d trajectory for multi-entity motion in video generation. arXiv preprint arXiv:2412.07759, 2024. [26] Daniel Geng, Charles Herrmann, Junhwa Hur, Forrester Cole, Serena Zhang, Tobias Pfaff, Tatiana Lopez-Guevara, Carl Doersch, Yusuf Aytar, Michael Rubinstein, et al. Motion prompting: Controlling video generation with motion trajectories. arXiv preprint arXiv:2412.02700, 2024. 3 [27] Zekai Gu, Rui Yan, Jiahao Lu, Peng Li, Zhiyang Dou, Chenyang Si, Zhen Dong, Qifeng Liu, Cheng Lin, Ziwei Liu, et al. Diffusion as shader: 3d-aware video diffusion In Proceedings of for versatile video generation control. the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pages 112, 2025. 3 [28] David Ha and Jurgen Schmidhuber. Recurrent world models facilitate policy evolution. Advances in neural information processing systems, 31, 2018. 2 [29] David Ha and Jurgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2(3), 2018. 2 [30] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. In International conference on machine learning, pages 25552565. PMLR, 2019. 2 [31] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104, 2023. [32] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2024. 3, 6, 4 [33] Hao He, Ceyuan Yang, Shanchuan Lin, Yinghao Xu, Meng Wei, Liangke Gui, Qi Zhao, Gordon Wetzstein, Lu Jiang, and Hongsheng Li. Cameractrl ii: Dynamic scene exploration via camera-controlled video diffusion models. arXiv preprint arXiv:2503.10592, 2025. 3 [34] Xuehai He, Shuohang Wang, Jianwei Yang, Xiaoxia Wu, Yiping Wang, Kuan Wang, Zheng Zhan, Olatunji Ruwase, Yelong Shen, and Xin Eric Wang. Mojito: Motion trajectory and intensity control for video generation. arXiv preprint arXiv:2412.08948, 2024. 3 [35] Xianglong He, Chunli Peng, Zexiang Liu, Boyang Wang, Yifan Zhang, Qi Cui, Fei Kang, Biao Jiang, Mengyin An, Yangyang Ren, et al. Matrix-game 2.0: An open-source, real-time, and streaming interactive world model. arXiv preprint arXiv:2508.13009, 2025. 2 [36] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [37] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400, 2023. 3 [38] Chen Hou, Guoqiang Wei, Yan Zeng, and Zhibo Chen. Training-free camera control for video generation. arXiv preprint arXiv:2406.10126, 2024. 3 [39] Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, and Gianluca Corrado. Gaia-1: generative world model for autonomous driving. arXiv preprint arXiv:2309.17080, 2023. 2 [40] Tianyu Huang, Wangguandong Zheng, Tengfei Wang, Yuhao Liu, Zhenwei Wang, Junta Wu, Jie Jiang, Hui Li, Rynson WH Lau, Wangmeng Zuo, et al. Voyager: Longrange and world-consistent video diffusion for explorable arXiv preprint arXiv:2506.04225, 3d scene generation. 2025. 2, 7 [41] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing. arXiv preprint arXiv:2503.07598, 2025. 4, 1 [42] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in neural information processing systems, 35:2656526577, 2022. 1 [43] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. [44] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 2 [45] Zhengfei Kuang, Shengqu Cai, Hao He, Yinghao Xu, Hongsheng Li, Leonidas Guibas, and Gordon Wetzstein. Collaborative video diffusion: Consistent multi-video generation with camera control. Advances in Neural Information Processing Systems, 37:1624016271, 2024. 3 [46] Yann LeCun. path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62(1): 162, 2022. 2 [47] Haoran Li, Haolin Shi, Wenli Zhang, Wenjun Wu, Yong Liao, Lin Wang, Lik-hang Lee, and Peng Yuan Zhou. Dreamscene: 3d gaussian-based text-to-3d scene generation via formation pattern sampling. In European Confer10 ence on Computer Vision, pages 214230. Springer, 2024. 3 [48] Jiaqi Li, Junshu Tang, Zhiyong Xu, Longhuang Wu, Yuan Zhou, Shuai Shao, Tianbao Yu, Zhiguo Cao, and Qinglin Lu. Hunyuan-gamecraft: High-dynamic interactive game arXiv video generation with hybrid history condition. preprint arXiv:2506.17201, 2025. [49] Quanhao Li, Zhen Xing, Rui Wang, Hui Zhang, Qi Dai, and Zuxuan Wu. Magicmotion: Controllable video generation with dense-to-sparse trajectory guidance. arXiv preprint arXiv:2503.16421, 2025. 3 [50] Runjia Li, Philip Torr, Andrea Vedaldi, and Tomas Jakab. Vmem: Consistent interactive video scene generation with surfel-indexed view memory. arXiv preprint arXiv:2506.18903, 2025. 2 [51] Teng Li, Guangcong Zheng, Rui Jiang, Tao Wu, Yehao Lu, Yining Lin, Xi Li, et al. Realcam-i2v: Real-world imageto-video generation with interactive complex camera control. arXiv preprint arXiv:2502.10059, 2025. 3 [52] Xinyang Li, Tengfei Wang, Zixiao Gu, Shengchuan Zhang, Chunchao Guo, and Liujuan Cao. Flashworld: Highquality 3d scene generation within seconds. arXiv preprint arXiv:2510.13678, 2025. 3, 7 [53] Yaowei Li, Xintao Wang, Zhaoyang Zhang, Zhouxia Wang, Ziyang Yuan, Liangbin Xie, Yuexian Zou, and Ying Shan. Image conductor: Precision control for interactive video synthesis. arXiv preprint arXiv:2406.15339, 2024. 3 [54] Zhen Li, Chuanhao Li, Xiaofeng Mao, Shaoheng Lin, Ming Li, Shitian Zhao, Zhaopan Xu, Xinyue Li, Yukang Feng, Jianwen Sun, et al. Sekai: video dataset towards world exploration. arXiv preprint arXiv:2506.15675, 2025. 5 [55] Jingyun Liang, Jingkai Zhou, Shikai Li, Chenjie Cao, Lei Sun, Yichen Qian, Weihua Chen, and Fan Wang. Realismotion: Decomposed human motion control and video generation in the world space. arXiv preprint arXiv:2508.08588, 2025. 3 [56] Xinyao Liao, Xianfang Zeng, Liao Wang, Gang Yu, Guosheng Lin, and Chi Zhang. Motionagent: Fine-grained controllable video generation via motion field agent. arXiv preprint arXiv:2502.03207, 2025. 2, [57] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 1 [58] Yifan Liu, Zhiyuan Min, Zhenwei Wang, Junta Wu, Tengfei Wang, Yixuan Yuan, Yawei Luo, and Chunchao Guo. Worldmirror: Universal 3d world reconstruction with anyprior prompting. arXiv preprint arXiv:2510.10726, 2025. 3 [59] Taiming Lu, Tianmin Shu, Junfei Xiao, Luoxin Ye, Jiahao Wang, Cheng Peng, Chen Wei, Daniel Khashabi, Rama Chellappa, Alan Yuille, et al. Genex: Generating an explorable world. arXiv preprint arXiv:2412.09624, 2024. 3 [60] Wan-Duo Kurt Ma, John Lewis, and Bastiaan Kleijn. Trailblazer: Trajectory control for diffusion-based video generation. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. 3 [61] Xiaofeng Mao, Shaoheng Lin, Zhen Li, Chuanhao Li, Wenshuo Peng, Tong He, Jiangmiao Pang, Mingmin Chi, Yu Qiao, and Kaipeng Zhang. Yume: An interactive world generation model. arXiv preprint arXiv:2507.17744, 2025. 1, 2, 7 [62] Willi Menapace, Stephane Lathuiliere, Sergey Tulyakov, Aliaksandr Siarohin, and Elisa Ricci. Playable video genIn Proceedings of the IEEE/CVF Conference on eration. Computer Vision and Pattern Recognition, pages 10061 10070, 2021. 2 [63] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. [64] Chong Mou, Mingdeng Cao, Xintao Wang, Zhaoyang Zhang, Ying Shan, and Jian Zhang. Revideo: Remake video with motion and content control. arXiv preprint arXiv:2405.13865, 2024. 3 [65] Muyao Niu, Xiaodong Cun, Xintao Wang, Yong Zhang, Ying Shan, and Yinqiang Zheng. Mofa-video: Controllable image animation via generative motion field adaptions In European in frozen image-to-video diffusion model. Conference on Computer Vision, pages 111128. Springer, 2025. 3 [66] Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard Lewis, and Satinder Singh. Action-conditional video prediction using deep networks in atari games. Advances in neural information processing systems, 28, 2015. 2 [67] Karran Pandey, Matheus Gadelha, Yannick Hold-Geoffroy, Karan Singh, Niloy Mitra, and Paul Guerrero. MoarXiv preprint tion modes: What could happen next? arXiv:2412.00148, 2024. 3 [68] Jack Parker-Holder, Philip Ball, Jake Bruce, Vibhavari Dasagi, Kristian Holsheimer, Christos Kaplanis, Alexandre Moufarek, Guy Scully, Jeremy Shar, Jimmy Shi, Stephen Spencer, Jessica Yung, Michael Dennis, Sultan Kenjeyev, Shangbang Long, Vlad Mnih, Harris Chan, Maxime Gazeau, Bonnie Li, Fabio Pardo, Luyu Wang, Lei Zhang, Frederic Besse, Tim Harley, Anna Mitenkova, Jane Wang, Jeff Clune, Demis Hassabis, Raia Hadsell, Adrian Bolton, Satinder Singh, and Tim Rocktaschel. Genie 2: large-scale foundation world model. 2024. 2 [69] William Peebles and Saining Xie. Scalable diffusion modIn Proceedings of the IEEE/CVF els with transformers. international conference on computer vision, pages 4195 4205, 2023. 1 [70] Luigi Piccinelli, Christos Sakaridis, Yung-Hsu Yang, Mattia Segu, Siyuan Li, Wim Abbeloos, and Luc Van Gool. Unidepthv2: Universal monocular metric depth estimation made simpler. arXiv preprint arXiv:2502.20110, 2025. [71] Ryan Po, Yotam Nitzan, Richard Zhang, Berlin Chen, Tri Dao, Eli Shechtman, Gordon Wetzstein, and Xun Huang. Long-context state-space video world models. arXiv preprint arXiv:2505.20171, 2025. 2 [72] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. 3 [73] Stefan Popov, Amit Raj, Michael Krainin, Yuanzhen Li, William Freeman, and Michael Rubinstein. Camctrl3d: 11 Single-image scene exploration with precise 3d camera control. arXiv preprint arXiv:2501.06006, 2025. 3 [74] Haonan Qiu, Zhaoxi Chen, Zhouxia Wang, Yingqing He, Menghan Xia, and Ziwei Liu. Freetraj: Tuning-free trajectory control in video diffusion models. arXiv preprint arXiv:2406.16863, 2024. [75] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, et al. Grounded sam: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159, 2024. 3 [76] Xuanchi Ren, Tianchang Shen, Jiahui Huang, Huan Ling, Yifan Lu, Merlin Nimier-David, Thomas Muller, Alexander Keller, Sanja Fidler, and Jun Gao. Gen3c: 3d-informed world-consistent video generation with precise camera conIn Proceedings of the Computer Vision and Pattern trol. Recognition Conference, pages 61216132, 2025. 3 [77] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 3, 1 [78] Kyle Sargent, Zizhang Li, Tanmay Shah, Charles Herrmann, Hong-Xing Yu, Yunzhi Zhang, Eric Ryan Chan, Dmitry Lagun, Li Fei-Fei, Deqing Sun, et al. Zeronvs: Zero-shot 360-degree view synthesis from single image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 94209429, 2024. 3 [79] Manuel-Andreas Schneider, Lukas Hollein, and Matthias Nießner. Worldexplorer: Towards generating fully navigable 3d scenes. arXiv preprint arXiv:2506.01799, 2025. 3 [80] Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi Zhang, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, et al. Motion-i2v: Consistent and controllable image-to-video generation with explicit motion modeling. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. 3 [81] Xincheng Shuai, Henghui Ding, Zhenyuan Qin, Hao Luo, Xingjun Ma, and Dacheng Tao. Free-form motion control: synthetic video generation dataset with controllable camera and object motions. arXiv preprint arXiv:2501.01425, 2025. 3 [82] Wenqiang Sun, Shuo Chen, Fangfu Liu, Zilong Chen, Yueqi Duan, Jun Zhang, and Yikai Wang. Dimensionx: Create any 3d and 4d scenes from single image with controllable video diffusion. arXiv preprint arXiv:2411.04928, 2024. [83] Maham Tanveer, Yang Zhou, Simon Niklaus, Ali Mahdavi Amiri, Hao Zhang, Krishna Kumar Singh, and Nanxuan Zhao. Motionbridge: Dynamic video inbetweening with flexible controls. arXiv preprint arXiv:2412.13190, 2024. 3 [84] HunyuanWorld Team, Zhenwei Wang, Yuhao Liu, Junta Wu, Zixiao Gu, Haoyuan Wang, Xuhui Zuo, Tianyu Huang, Wenhuan Li, Sheng Zhang, et al. Hunyuanworld 1.0: Generating immersive, explorable, and interactive 3d worlds from words or pixels. arXiv preprint arXiv:2507.21809, 2025. 3 [85] Ruben Villegas, Arkanath Pathak, Harini Kannan, Dumitru Erhan, Quoc Le, and Honglak Lee. High fidelity video prediction with large stochastic recurrent neural networks. Advances in Neural Information Processing Systems, 32, 2019. 2 [86] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 2, 4, 1 [87] Zhang Wan, Sheng Tang, Jiawei Wei, Ruize Zhang, and Juan Cao. Dragentity: Trajectory guided video generation In Proceedings using entity and positional relationships. of the 32nd ACM International Conference on Multimedia, pages 108116, 2024. 3 [88] Hanlin Wang, Hao Ouyang, Qiuyu Wang, Wen Wang, Ka Leong Cheng, Qifeng Chen, Yujun Shen, and Limin Wang. Levitor: 3d trajectory oriented image-to-video synthesis. arXiv preprint arXiv:2412.15214, 2024. [89] Jiawei Wang, Yuchen Zhang, Jiaxin Zou, Yan Zeng, Guoqiang Wei, Liping Yuan, and Hang Li. Boximator: Generating rich and controllable motions for video synthesis. arXiv preprint arXiv:2402.01566, 2024. 2, 3 [90] Jiahao Wang, Luoxin Ye, TaiMing Lu, Junfei Xiao, Jiahan Zhang, Yuxiang Guo, Xijun Liu, Rama Chellappa, Cheng Peng, Alan Yuille, et al. Evoworld: Evolving panoramic world generation with explicit 3d memory. arXiv preprint arXiv:2510.01183, 2025. 3 [91] Jiahao Wang, Yufeng Yuan, Rujie Zheng, Youtian Lin, Jian Gao, Lin-Zhuo Chen, Yajie Bao, Yi Zhang, Chang Zeng, Yanxi Zhou, et al. Spatialvid: large-scale video dataset with spatial annotations. arXiv preprint arXiv:2509.09676, 2025. 5 [92] Qinghe Wang, Yawen Luo, Xiaoyu Shi, Xu Jia, Huchuan Lu, Tianfan Xue, Xintao Wang, Pengfei Wan, Di Zhang, and Kun Gai. Cinemaster: 3d-aware and controllable framework for cinematic text-to-video generation. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pages 110, 2025. 2, 3 [93] Ruicheng Wang, Sicheng Xu, Yue Dong, Yu Deng, Jianfeng Xiang, Zelong Lv, Guangzhong Sun, Xin Tong, and Jiaolong Yang. Moge-2: Accurate monocular geomearXiv preprint try with metric scale and sharp details. arXiv:2507.02546, 2025. 3, 5 [94] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability. Advances in Neural Information Processing Systems, 36:75947611, 2023. [95] Zhouxia Wang, Yushi Lan, Shangchen Zhou, and Chen Change Loy. Objctrl-2.5 d: Training-free object control with camera poses. arXiv preprint arXiv:2412.07721, 2024. 3 [96] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for 12 video generation. Papers, pages 111, 2024. 2, 3, 6, 4 In ACM SIGGRAPH 2024 Conference video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2 [97] Zun Wang, Jaemin Cho, Jialu Li, Han Lin, Jaehong Yoon, Yue Zhang, and Mohit Bansal. Epic: Efficient video camera control learning with precise anchor-video guidance. arXiv preprint arXiv:2505.21876, 2025. 3 [98] Jianzong Wu, Xiangtai Li, Yanhong Zeng, Jiangning Zhang, Qianyu Zhou, Yining Li, Yunhai Tong, and Kai Chen. Motionbooth: Motion-aware customized text-tovideo generation. Advances in Neural Information Processing Systems, 37:3432234348, 2024. 3 [99] Tong Wu, Shuai Yang, Ryan Po, Yinghao Xu, Ziwei Liu, Dahua Lin, and Gordon Wetzstein. Video world arXiv preprint models with long-term spatial memory. arXiv:2506.05284, 2025. 2 [100] Weijia Wu, Zhuang Li, Yuchao Gu, Rui Zhao, Yefei He, David Junhao Zhang, Mike Zheng Shou, Yan Li, Tingting Gao, and Di Zhang. Draganything: Motion control for anything using entity representation. In European Conference on Computer Vision, pages 331348. Springer, 2025. 3 [101] Jiannan Xiang, Guangyi Liu, Yi Gu, Qiyue Gao, Yuting Ning, Yuheng Zha, Zeyu Feng, Tianhua Tao, Shibo Hao, Yemin Shi, et al. Pandora: Towards general world model with natural language actions and video states. arXiv preprint arXiv:2406.09455, 2024. [102] Zeqi Xiao, Wenqi Ouyang, Yifan Zhou, Shuai Yang, Lei Yang, Jianlou Si, and Xingang Pan. Trajectory attention for fine-grained video motion control. arXiv preprint arXiv:2411.19324, 2024. 3 [103] Jinbo Xing, Long Mai, Cusuh Ham, Jiahui Huang, Aniruddha Mahapatra, Chi-Wing Fu, Tien-Tsin Wong, and Feng Liu. Motioncanvas: Cinematic shot design with controlIn Proceedings of the lable image-to-video generation. Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pages 1 11, 2025. 3 [104] Dejia Xu, Weili Nie, Chao Liu, Sifei Liu, Jan Kautz, Zhangyang Wang, and Arash Vahdat. Camco: Camera-controllable 3d-consistent image-to-video generation. arXiv preprint arXiv:2406.02509, 2024. 3 [105] Tianshuo Xu, Zhifei Chen, Leyi Wu, Hao Lu, Yuying Chen, Lihui Jiang, Bingbing Liu, and Yingcong Chen. Motion dreamer: Realizing physically coherent video generation arXiv preprint through scene-aware motion reasoning. arXiv:2412.00547, 2024. 3 [106] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. Advances in Neural Information Processing Systems, 37:2187521911, 2024. [107] Shiyuan Yang, Liang Hou, Haibin Huang, Chongyang Ma, Pengfei Wan, Di Zhang, Xiaodong Chen, and Jing Liao. Direct-a-video: Customized video generation with userIn ACM directed camera movement and object motion. SIGGRAPH 2024 Conference Papers, pages 112, 2024. 3 [108] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to- [109] Zhongqi Yang, Wenhang Ge, Yuqi Li, Jiaqi Chen, Haoyuan Li, Mengyin An, Fei Kang, Hua Xue, Baixin Xu, Yuyang Yin, et al. Matrix-3d: Omnidirectional explorable 3d world generation. arXiv preprint arXiv:2508.08086, 2025. 2, 3 [110] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory. arXiv preprint arXiv:2308.08089, 2023. 3 [111] Hong-Xing Yu, Haoyi Duan, Junhwa Hur, Kyle Sargent, Michael Rubinstein, William Freeman, Forrester Cole, Deqing Sun, Noah Snavely, Jiajun Wu, et al. Wonderjourney: Going from anywhere to everywhere. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 66586667, 2024. 3 [112] Hong-Xing Yu, Haoyi Duan, Charles Herrmann, William Interactive 3d Freeman, and Jiajun Wu. Wonderworld: In Proceedings of scene generation from single image. the Computer Vision and Pattern Recognition Conference, pages 59165926, 2025. 3 [113] Jiwen Yu, Yiran Qin, Xintao Wang, Pengfei Wan, Di Gamefactory: Creating new Zhang, and Xihui Liu. games with generative interactive videos. arXiv preprint arXiv:2501.08325, 2025. 2 [114] Jason Yu, Fereshteh Forghani, Konstantinos Derpanis, and Marcus Brubaker. Long-term photometric consistent In Proceednovel view synthesis with diffusion models. ings of the IEEE/CVF International Conference on Computer Vision, pages 70947104, 2023. [115] Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan, and Yonghong Tian. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048, 2024. 2, 3, 7 [116] Kai Zhang, Sai Bi, Hao Tan, Yuanbo Xiangli, Nanxuan Zhao, Kalyan Sunkavalli, and Zexiang Xu. Gs-lrm: Large reconstruction model for 3d gaussian splatting. In European Conference on Computer Vision, pages 119. Springer, 2024. 3 [117] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023. 2 [118] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets. ACM Transactions on Graphics (TOG), 43(4):120, 2024. 3 [119] Qihang Zhang, Chaoyang Wang, Aliaksandr Siarohin, Peiye Zhuang, Yinghao Xu, Ceyuan Yang, Dahua Lin, Bolei Zhou, Sergey Tulyakov, and Hsin-Ying Lee. ToIn Proceedings wards text-guided 3d scene composition. of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 68296838, 2024. 3 [120] Shengjun Zhang, Jinzhao Li, Xin Fei, Hao Liu, and Yueqi Duan. Scene splatter: Momentum 3d scene generation from 13 single image with video diffusion model. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 60896098, 2025. 3 [121] Zhenghao Zhang, Junchao Liao, Menghao Li, Zuozhuo Dai, Bingxue Qiu, Siyu Zhu, Long Qin, and Weizhi Wang. Tora: Trajectory-oriented diffusion transformer for video generation. arXiv preprint arXiv:2407.21705, 2024. 3 [122] Zhiyuan Zhang, Dongdong Chen, and Jing Liao. I2v3d: Controllable image-to-video generation with 3d guidance. arXiv preprint arXiv:2503.09733, 2025. 2, 3 [123] Zhongwei Zhang, Fuchen Long, Zhaofan Qiu, Yingwei Pan, Wu Liu, Ting Yao, and Tao Mei. Motionpro: precise In Promotion controller for image-to-video generation. ceedings of the Computer Vision and Pattern Recognition Conference, pages 2795727967, 2025. [124] Guangcong Zheng, Teng Li, Rui Jiang, Yehao Lu, Tao Wu, and Xi Li. Cami2v: Camera-controlled image-to-video diffusion model. arXiv preprint arXiv:2410.15957, 2024. 3 [125] Sixiao Zheng, Zimian Peng, Yanpeng Zhou, Yi Zhu, Hang Xu, Xiangru Huang, and Yanwei Fu. Vidcraft3: Camera, object, and lighting control for image-to-video generation. arXiv preprint arXiv:2502.07531, 2025. 2, 3 [126] Haitao Zhou, Chuang Wang, Rui Nie, Jinlin Liu, Dongdong Yu, Qian Yu, and Changhu Wang. Trackgo: flexible and efficient method for controllable video generation. arXiv preprint arXiv:2408.11475, 2024. 3 [127] Haoyi Zhu, Yifan Wang, Jianjun Zhou, Wenzheng Chang, Yang Zhou, Zizun Li, Junyi Chen, Chunhua Shen, Jiangmiao Pang, and Tong He. Aether: Geometric-aware unified world modeling. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 85358546, 2025. 3 14 VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Preliminary: Video Diffusion Models Modern video diffusion models operate in compact latent space learned by spatio-temporal VAE. Given video RT HW 3, the encoder maps it to latents z0 = E(x) RT CH , on which the generative process is defined [8, 77]. standard forward diffusion process gradually perturbs z0 into noisy variables zt via q(zt z0) = αt z0 + 1 αt ϵ, ϵ (0, I), (6) and DiT-based denoiser ϵθ is trained to predict the noise under time step and conditioning signal (e.g., text prompts, reference frames) as Ldiff(θ) = Ez0,t,ϵ (cid:2)ϵθ(zt, t, c) ϵ 2 (cid:3), (7) following the DDPM formulation [36]. Recent models further adopt continuous-time flow matching: given clean latents z0 and Gaussian samples z1, one constructs interpolants zτ = (1 τ )z0 + τ z1 with τ [0, 1] and learns velocity field vθ by Lflow(θ) = Ez0,τ,ϵ (cid:2)vθ(zτ , τ, c) (z1 z0) 2 (cid:3), (8) as in recent flow-matching and ODE-based generative formulations [42, 57]. These objectives are naturally implemented with Diffusion Transformers (DiT), which operate on spatio-temporal latent tokens and inject (t, c) through attention [69], forming the backbone of current foundation video generators. Wan2.1 instantiates the above latent video diffusion / flow-matching paradigm with 3D VAE and DiTbased denoiser, together with rich multi-modal conditioning trained on large-scale, diverse videotext data [86]. Throughout this work, we adopt Wan2.1-14B as frozen latent video backbone and treat it as generic video prior: we keep its encoder, decoder, and DiT-based denoiser unchanged, and only attach lightweight geometry-aware control interfaces on top of its DiT blocks. The detailed architecture of these control modules is provided in the Sec. B. B. Model Architecture Details VerseCrafter is built on top of Wan2.1 [86], latent video diffusion / flow-matching model with 3D VAE (Wan Encoder and Wan Decoder) and DiT-based denoiser (WanDiT). We keep the Wan2.1 backbone frozen, and introduce geometry-aware conditioning pathway together with lightweight GeoAdapter that injects 4D geometric control signals into selected Wan-DiT blocks. We instantiate 1 , Depthtraj VerseCrafter on top of the Wan2.1 T2V-14B backbone, resulting in 14B-parameter controllable video world model. Fig. 9 illustrates the geometry-aware conditioning pathway and the integration of GeoAdapter into the Wan-DiT backbone, while Table 4 summarizes the input resolution, number of Wan-DiT layers, hidden dimension, GeoAdapter injection pattern, and fine-tuning configuration of VerseCrafter. Geometry encoding and tokenization. For each frame t, , Depthbg we render background RGB/depth RGBbg , trajectory RGB/depth RGBtraj , and soft control mask Mt that marks regions where the diffusion model should synthesize or overwrite content (for t=1 we replace RGBbg 1 with the input image and set M1=0). The four RGB/depth maps are passed through the frozen 3D VAE encoder to obtain latent features at the VAE resolution, while the mask R1T HW is rearranged to align with the 3D VAE latent grid (the Rearrange module in Figure 9). Let st, sh, and sw denote the temporal and spatial strides of Wans 3D VAE (we use st=4 and sh=sw=8). Following the practice in [41, 86], we drop the singleton channel dimension, split the spatial dimensions into sh sw sub-cells, and fold these sub-cells into the channel dimension via reshapepermute operation, yielding tensor of shape CM with CM =shsw, =H/sh, and =W/sw. We then downsample the temporal dimension using nearest-neighbor interpolation to match the latent depth = (T + st 1)/st, producing ˆM RCM H . Finally, ˆM is concatenated channel-wise with the encoded background and 3D Gaussian trajectory latents to obtain spatiotemporal geometry feature RT W CG . We follow WanDiT for tokenization: the latent grid is divided into nonoverlapping 3D patches, and each patch is linearly projected into token embedding, yielding sequence of geometry tokens RLD, where = W and matches the hidden width of Wan-DiT. Because we use identical strides, positional encodings, and patch sizes, the geometry tokens are spatially and temporally aligned with the latent video tokens processed by Wan-DiT. GeoAdapter integration. GeoAdapter is lightweight DiT-style branch operating on the geometry tokens g. It shares the same token dimensionality and positional encodings as Wan-DiT, but contains far fewer layers. Let {B0, . . . , BN 1} denote the Wan-DiT blocks of Wan2.1, and let {G1, . . . , GM } denote the GeoAdapter blocks. We attach GeoAdapter as residual modulation branch to subset of Wan-DiT blocks. Concretely, we choose stride and inject GeoAdapter after every k-th Wan-DiT block; Figure 9. Detailed architecture of VerseCrafter. Background RGB & depth and 3D Gaussian trajectory RGB & depth are first encoded by the frozen 3D VAE. The soft control mask is rearranged into latent-aligned channels, and all geometry latents are then concatenated along the channel dimension to form unified spatio-temporal geometry feature. This feature is patchified into tokens and processed by the GeoAdapter branch. At selected Wan-DiT blocks, GeoAdapter outputs are passed through zero-initialized linear layers and added as residual modulations to the backbone tokens, enabling 4D geometry-consistent camera and object control. see Table 4 for the exact injection pattern and configuration. For each Wan-DiT block Bn whose index belongs to the injection set, with input tokens xn RLD and geometry tokens g, we add geometry-conditioned residual of the form xn+1 = Bn(xn) + Gm(g) W(m) 0 , (9) 0 where Gm is the corresponding GeoAdapter block and W(m) RDD is zero-initialized linear projection. All 0 entries of W(m) are initialized to zero, so at the beginning of training VerseCrafter behaves identically to the original Wan2.1 backbone. During fine-tuning, W(m) gradually learns to inject geometry information as residual modulation, in the spirit of zero-initialized adapter designs in ControlNet-style architectures [117]. 0 C. VerseControl4D Dataset Details We construct VerseControl4D, large-scale in-the-wild dataset for real-world 4D geometric control, where each clip is annotated with camera trajectories and multi-object 3D Gaussian trajectories. Control signals are automatically derived by the pipeline in main paper, producing background/3D Gaussian trajectories RGB and depth maps together with merged mask. VerseControl4D contains 35,000 training clips and 1,000 validation/test clips. Table 5 summarizes the data distribution by source and scene type. Overall, 26% of the clips come from Sekai-Real-HQ and 74% from SpatialVID-HQ, reflecting their complementary scene coverage. To support both camera-only world exploration and coordinated cameraobject control, VerseControl4D includes dynamic scenes (clips with salient foreground object motion together with camera motion) and static scenes (clips with negligible object motion and only camera movement). About 20% of the training clips are static scenes, and the validation set additionally includes 250 static-scene clips for dedicated camera-only evaluation. Representative samples and their rendered 4D control signals are shown in Fig. 10. D. Evaluation Metrics D.1. VBench-I2V We evaluate image-conditioned video quality using the VBench Image-to-Video (I2V) evaluation suite, denoted as VBench-I2V. For each generated clip, we follow the official VBench-I2V protocol: the conditioning image and its corresponding generated video are fed into the evaluation Table 4. Model configuration of VerseCrafter. Settings include input resolution, number of Wan-DiT layers, GeoAdapter injection blocks, pre-trained backbone, and fine-tuning configuration."
        },
        {
            "title": "VerseCrafter",
            "content": "720P 40 Resolution Num layers of Wan-DiT GeoAdapter injection blocks [0, 5, 10, 15, 20, 25, 30, 35] Pre-trained backbone Hidden dimension Batch size Training iterations Wan2.1 T2V-14B 5120 16 5,000 Table 5. VerseControl4D data split and scene-type statistics. We report the number of clips from each source dataset and split. Dynamic scenes contain coupled camera and foreground object motion, while static scenes have negligible object motion and are used for cameraonly evaluation."
        },
        {
            "title": "Split",
            "content": "Sekai-Real-HQ SpatialVID-HQ"
        },
        {
            "title": "Dynamic Scenes Static Scenes Dynamic Scenes",
            "content": "Train Val/Test 9,071 468 7,000 250 18,929 282 pipeline, which computes set of learned, human-aligned metrics that jointly capture video-image consistency and perceptual video quality. In our experiments, we report the following eight VBench-I2V dimensions, and define the Overall Score as the simple arithmetic mean of these eight normalized scores, where higher values indicate better performance: Imaging Quality. This metric measures low-level image fidelity, including sharpness and the absence of artifacts such as blur, noise, or overexposure. VBench uses an image quality predictor (e.g., MUSIQ), averaging scores across frames to obtain video-level imaging quality score. Aesthetic Quality. This dimension assesses the artistic and aesthetic appeal of individual frames, including composition, color harmony, and realism. VBench applies an aesthetic quality predictor (e.g., the LAION aesthetic model) to each frame and averages the predictions over the clip. Dynamic Degree. This metric quantifies how dynamic the generated video is. Optical flow magnitudes (e.g., estimated by RAFT) are used to measure the amount of motion; the score reflects whether the model produces sufficiently active (non-static) content. Motion Smoothness. This metric evaluates whether subject and camera motion evolves smoothly and respects reasonable physical dynamics. VBench leverages pretrained video frame interpolation prior to assess how well intermediate motion can be interpolated, with smoother and more physically plausible motion achieving higher scores. Background Consistency. This dimension measures temporal stability of the background layout and textures. Frame-level features (e.g., CLIP) are compared across time; large feature variations indicate flickering or unstable backgrounds and lead to lower scores. Subject Consistency. This dimension evaluates temporal consistency of the foreground subject within the video, regardless of the input image. VBench computes subjectregion features across frames and measures their similarity over time to penalize identity drift or sudden appearance changes. I2V Background (VideoImage Background Consistency). This metric evaluates how well the global background in the video matches the background in the input image, especially for scene-centric inputs. VBench uses background-sensitive features (e.g., DreamSim) and aggregates imageframe and inter-frame similarities into single background consistency score. I2V Subject (VideoImage Subject Consistency). This metric measures how well the main subject in the generated video matches the subject in the input image. VBench extracts high-level visual features (e.g., DINO) from the conditioning image and from each video frame, and combines imageframe similarities with inter-frame similarities into weighted average subject consistency score. Formally, given these eight per-dimension scores k=1 returned by VBench-I2V for video, we define {sk}8 Overall Score = 1 8 8 (cid:88) k=1 sk, (10) which is the value reported as Overall Score in the main paper. 3 D.2. Rotation Error (RotErr) To measure how well the generated camera motion follows the ground-truth camera trajectory, we adopt the cameraalignment metric from CameraCtrl [32]. For each generated video, we estimate its camera trajectory using the same geometry-annotation pipeline as for VerseControl4D, yielding rotation matrices {Rj j=1 and translation vectors {Tj gen}n j=1, where is the number of frames. Let {Rj gt}n j=1 denote the corresponding ground-truth rotation matrices. The rotation error is computed by comparing the ground-truth and generated rotation matrices at each frame: gen}n RotErr = arccos (cid:88) j=1 tr(cid:0)Rj genRj 2 gt (cid:1) , (11) where tr() denotes the matrix trace. lower RotErr indicates better alignment between the generated and groundtruth camera orientations. D.3. Translation Error (TransErr) gt}n j=1 and {Tj We also evaluate the accuracy of the generated camera positions. Let {Tj j=1 be the ground-truth and generated camera translation vectors for video with frames. Following CameraCtrl [32], the translation error is defined as the sum of per-frame Euclidean distances between the translation vectors: gen}n TransErr = (cid:88) j= (cid:13) (cid:13)Tj gt Tj gen (cid:13) (cid:13)2, (12) gt and Tj where Tj gen denote the ground-truth and generated camera translation vectors at frame j, respectively. Smaller TransErr indicates that the generated camera positions more closely match the ground-truth camera positions. D.4. Object Motion Control (ObjMC) For object-motion control, we follow the ObjMC metric proposed in MotionCtrl [96] and extend it to the multiobject setting in our 3D Gaussian trajectory space. Given generated video, we run the same geometry-annotation pipeline as in VerseControl4D to estimate per-object 3D Gaussian trajectories, and compare them with the corresponding ground-truth trajectories from our dataset. Let Ngt and Npred denote the numbers of ground-truth and predicted controlled objects in sample, and let be the number of frames. For each ground-truth object {1, . . . , Ngt} and frame {1, . . . , }, we denote the ground-truth 3D Gaussian mean by µ(t) R3 and the estimated mean from the generated video by ˆµ(t) R3 for predicted object k. 4 Multi-object matching. Since Ngt and Npred may differ, we first define the trajectory distance between groundtruth object and predicted object as the average Euclidean distance of their 3D Gaussian means over time: d(o, k) ="
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) (cid:13) (cid:13) ˆµ(t) µ(t) (cid:13) (cid:13)2. (13) t=1 We then build cost matrix RNgtNpred with entries Cok = d(o, k). To handle unmatched objects, we pad this matrix with dummy rows/columns and fill them with constant penalty λ (set to 10.0 in our experiments). Finally, we apply the Hungarian algorithm [? ] to this padded matrix to obtain an optimal one-to-one matching between ground-truth and predicted trajectories. This step assigns each ground-truth object either to predicted trajectory (matched) or to dummy entry (missed), and symmetrically accounts for spurious predicted objects. ObjMC score. Given the optimal matching, we define the per-object trajectory error for ground-truth object as (cid:40) do = d(o, k) λ if is matched to predicted object k, if is unmatched, (14) and compute the final ObjMC score as the average over all ground-truth controlled objects: ObjMC ="
        },
        {
            "title": "1\nNgt",
            "content": "Ngt (cid:88) o=1 do. (15) Lower ObjMC values indicate more accurate multi-object 3D motion control, and the unmatched-penalty λ ensures that both missed objects and spurious trajectories are appropriately penalized. E. Additional Qualitative Results We provide additional qualitative comparisons on VerseControl4D, following the same evaluation settings and baselines as in the main paper. Figures 11 and 12 showcase dynamic scenes with joint cameraobject control. Perceptionas-Control often yields low-clarity frames and noticeable camera mis-tracking, while Yume may capture coarse motion intent but fails to precisely align object trajectories with the target camera path. Uni3C is restricted to human motion and struggles to generalize to multi-object dynamics. In contrast, VerseCrafter consistently adheres to both camera trajectories and multiple object motions, preserving object identity and shape over time and maintaining geometrically coherent backgrounds. Figures 13 and 14 present static scenes for cameraonly exploration. We observe that ViewCrafter, Voyager, and FlashWorld can introduce structural distortions, Figure 10. VerseControl4D dataset examples. For each clip, we visualize the input image and target camera trajectory (left), followed by several frames of ground-truth video and our rendered control signals (right): background RGB/depth, 3D Gaussian trajectory RGB/depth for controlled objects, and the final merged mask. These signals are automatically derived by our annotation pipeline in main paper. depth/parallax instability, or temporal flicker when following long or curved camera paths. VerseCrafter produces smoother camera motion with faithful parallax, keeping background layout stable and details sharp across frames. These additional cases further confirm VerseCrafters robustness in real-world 4D control for both dynamic and static settings. F. Limitations and Future Work Despite the encouraging results, VerseCrafter has several limitations that suggest promising directions for future work. First, while VerseCrafter enforces 4D geometric consistency through explicit camera and 3D Gaussian trajectory controls, it does not impose explicit physical constraints during generation. As result, the model may occasionally produce motion that is geometrically plausible yet physically imperfect, such as subtle sliding, interpenetration, or dynamics that deviate from real-world contact and inertia. In future work, integrating stronger physics priorse.g., collision-aware losses, contact/ground constraints, or differentiable physics guidancecould improve physical realism and controllability in complex interactions. Second, VerseCrafter is computationally expensive at high resolution and long horizons, since it conditions large frozen video diffusion backbone and renders multichannel 4D controls per frame. Our current 81-frame 720P generation requires substantial GPU memory and runtime, limiting interactive use. Future work may explore more efficient backbones, distilled or cached control encoding, and streaming/long-video synthesis to scale VerseCrafter to faster and longer world rollouts. 6 Figure 11. Additional qualitative comparison of joint camera and object motion control on dynamic scenes. Perception-as-Control often produces low-fidelity frames with inaccurate camera motion; Yume roughly follows text-described motion but lacks precise geometric control; Uni3C is mainly limited to human-centric motion. VerseCrafter more faithfully follows both the target camera trajectory and multiobject motions while maintaining sharp appearance and geometrically consistent backgrounds. 7 Figure 12. Additional qualitative comparison of joint camera and object motion control on dynamic scenes. Across diverse real-world cases, baselines frequently suffer from camera drift, motion misalignment, or object identity/shape inconsistency. VerseCrafter preserves scene geometry and object coherence over time, yielding accurate multi-object 3D motion along the specified camera path. 8 Figure 13. Additional qualitative comparison of camera-only motion control on static scenes. ViewCrafter, Voyager, and FlashWorld often exhibit distorted facades, drifting structures, or inconsistent parallax along the camera path. VerseCrafter better follows the target trajectory while preserving sharp details and globally consistent 3D geometry. 9 Figure 14. Additional qualitative comparison of camera-only motion control on static scenes. Baselines may introduce structural warping, background flicker, or unstable depth cues when exploring long camera paths. VerseCrafter maintains stable parallax and texture consistency, producing smooth camera motion with faithful 3D scene structure."
        }
    ],
    "affiliations": [
        "ARC Lab, Tencent PCG",
        "Fudan University",
        "HKU",
        "Shanghai Innovation Institute"
    ]
}