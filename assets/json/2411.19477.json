{
    "paper_title": "A Simple and Provable Scaling Law for the Test-Time Compute of Large Language Models",
    "authors": [
        "Yanxi Chen",
        "Xuchen Pan",
        "Yaliang Li",
        "Bolin Ding",
        "Jingren Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose a general two-stage algorithm that enjoys a provable scaling law for the test-time compute of large language models (LLMs). Given an input problem, the proposed algorithm first generates $N$ candidate solutions, and then chooses the best one via a multiple-round knockout tournament where each pair of candidates are compared for $K$ times and only the winners move on to the next round. In a minimalistic implementation, both stages can be executed with a black-box LLM alone and nothing else (e.g., no external verifier or reward model), and a total of $N \\times (K + 1)$ highly parallelizable LLM calls are needed for solving an input problem. Assuming that a generated candidate solution is correct with probability $p_{\\text{gen}} > 0$ and a comparison between a pair of correct and incorrect solutions identifies the right winner with probability $p_{\\text{comp}} > 0.5$ (i.e., better than a random guess), we prove theoretically that the failure probability of the proposed algorithm decays to zero exponentially with respect to $N$ and $K$: $$\\mathbb{P}(\\text{final output is incorrect}) \\le (1 - p_{\\text{gen}})^N + \\lceil \\log_2 N \\rceil e^{-2 K (p_{\\text{comp}} - 0.5)^2}.$$ Our empirical results with the challenging MMLU-Pro benchmark validate the technical assumptions, as well as the efficacy of the proposed algorithm and the gains from scaling up its test-time compute."
        },
        {
            "title": "Start",
            "content": "A Simple and Provable Scaling Law for the Test-Time Compute of Large Language Models Yanxi Chen, Xuchen Pan, Yaliang Li, Bolin Ding, Jingren Zhou Alibaba Group {chenyanxi.cyx, panxuchen.pxc, yaliang.li, bolin.ding, jingren.zhou}@alibaba-inc.com Abstract We propose general two-stage algorithm that enjoys provable scaling law for the test-time compute of large language models (LLMs). Given an input problem, the proposed algorithm first generates candidate solutions, and then chooses the best one via multiple-round knockout tournament where each pair of candidates are compared for times and only the winners move on to the next round. In minimalistic implementation, both stages can be executed with black-box LLM alone and nothing else (e.g., no external verifier or reward model), and total of (K + 1) highly parallelizable LLM calls are needed for solving an input problem. For an input problem, assuming that generated candidate solution is correct with probability pgen > 0 and comparison between pair of correct and incorrect solutions identifies the right winner with probability pcomp > 0.5 (i.e., better than random guess), we prove theoretically that the failure probability of the proposed algorithm decays to zero exponentially with respect to and K: P(final output is incorrect) (1 pgen)N + log2 e2K(pcomp0.5) . Our empirical results with the challenging MMLU-Pro benchmark validate the technical assumptions, as well as the efficacy of the proposed algorithm and the gains from scaling up its test-time compute. 4 2 0 2 9 2 ] . [ 1 7 7 4 9 1 . 1 1 4 2 : r Figure 1: Accuracy achieved by the proposed two-stage algorithm for random subset of the engineering and math categories of the MMLU-Pro benchmark [26], versus its hyperparameters and that determine its test-time compute. While MMLU-Pro consists of multiple-choice questions, we let each candidate solution contain reasoning process elicited by zero-shot chain-of-thought prompting, which makes meaningful pairwise comparisons possible in the knockout stage of the proposed algorithm. Equal contributions."
        },
        {
            "title": "Introduction",
            "content": "Despite the astonishing advancements of large language models (LLMs) in the past few years, they still face challenges with reliability and stability. This hinders their applications in high-stakes scenarios where problem need to be solved with success probability 99.9% rather than 90%. Similarly, in an LLM-based agentic workflow that involves solving many sub-problems, each of them need to be solved with high success probability in order to ensure the success of the complete workflow, since single error in the process can lead to an incorrect final output. In these and many other similar scenarios, one is willing to boost the success probability by spending more test-time computation on LLM inference. Common approaches include letting the LLM generate long chain-of-thought trajectory [27, 13, 19], or asking the LLM to iteratively self-verify or self-refine the solutions that it has generated [5, 17, 6, 32, 33]. Another category of approaches is about sampling multiple candidate solutions and then choosing the best one via majority voting [3, 25, 2, 14], ranking with pairwise comparisons [9], or using an external verifier [31, 11] or trained reward model [15, 35]. The primary goal of this work is to provide some theoretical insights into the full potential of scaling up the test-time compute for LLM inference [22, 29]. Our main contributions are summarized as follows: We propose two-stage algorithm that enjoys provable scaling law for the test-time compute of LLM inference, while requiring only black-box LLM and nothing else (e.g., no external verifier or reward model) for minimalistic implementation. Given an input problem, the proposed algorithm first generates multiple candidate solutions, and then choose single one among them as the final output, via knockout tournament where pairwise comparisons among the candidates are conducted. We prove theoretically that, as the test-time compute of this algorithm scales up, its failure probability in solving specific problem (i.e., the probability that the final output is incorrect) decays to zero exponentially with respect to the total number of LLM calls. This guarantee relies on two natural (and arguably minimal) assumptions: (1) the LLM can generate correct solution with non-zero probability, and (2) the LLM can do better than random guess in choosing the right winner between pair of correct and incorrect solutions. Our practical implementation of the proposed algorithm is efficient and scalable, with support for parallel and distributed computation. Empirical results with the MMLU-Pro benchmark [26] validate the aforementioned assumptions, and confirm the gains from scaling up the test-time compute of the proposed algorithm."
        },
        {
            "title": "2 A two-stage algorithm",
            "content": "Problem formulation. We consider generic problem formulation where an LLM-based algorithm is given an input problem and asked to output solution. For conceptual simplicity, we evaluate any solution with binary metric indicating whether it is correct or incorrect1. Our goal is to design general LLM-based algorithm whose success probability, i.e., the probability of returning correct solution to the input problem, can be boosted monotonely up to 1 by scaling up its test-time compute. The proposed algorithm. We consider the following two-stage algorithm for solving an input problem: 1. Generation. We first generate candidate solutions, which can run in parallel. In situations where the final answer to the problem contains only few tokens (e.g., for multiple-choice problems or math calculation), we require that each solution contains reasoning process or explanation of the rationale that leads to its final answer, which can be elicited by chain-of-thought prompting [27, 13] for example; such information can be useful for enhancing pairwise comparisons in the next stage. 2. Knockout. We aggregate the candidate solutions via knockout tournament. At each round of the tournament, the candidates are grouped into pairs randomly, and each pair of candidates are compared 1For example, solution is correct if it contains the right reasoning process and final answer to math problem, or it meets set of criteria for an open-ended writing task. It is possible to expand the considered scenarios by adopting more general (discrete or continuous) metrics for evaluating solution, though this is beyond the scope of our current focus. Figure 2: visualization of the proposed two-stage algorithm (with = 8 in this example). for times. The winner of each pair is the one that is favored for more than K/2 times; ties are broken arbitrarily. Only the winners will move on to the next round. The final-round winner at the end of this tournament will be the final output of the algorithm. See Figure 2 for visualization of this process. pairwise comparison can be executed with single black-box LLM (or an ensemble of multiple LLMs). In minimalistic implementation, both generation and We next introduce some formal notations, followed by our analysis for both success probability and computational efficiency [4, 12] of the proposed algorithm, to be presented in Sections 2.1 and 2.2 respectively. For concreteness and clarity, our analysis is tailored to the aforementioned minimalistic implementation, but it can certainly be extended to more general cases. Formal notations. We let Mgen and Mcomp denote the probability distribution of the output of one LLM call for generating candidate solution and for comparing pair of solutions respectively. Given an input problem x, the proposed algorithm first samples independent candidate solutions y1, . . . , yN Mgen(x) during the generation stage. Then, for each pair of candidates (y, y) encountered in the knockout stage, the algorithm samples independent comparison results r1, . . . , rK Mcomp(x, y, y), and identifies the candidate that is favored by the majority of {ri}i[K] as the winner. The randomness in Mgen and Mcomp can originate from LLM decoding with non-zero temperature, the choice of prompting method or LLM backend for each LLM call, among others. Sampling from Mgen and Mcomp throughout the proposed algorithm is the sole source of randomness in the following analysis of success probability."
        },
        {
            "title": "2.1 Analysis of success probability: a provable scaling law",
            "content": "Our theoretical guarantee for the proposed algorithm critically relies on the following assumption about the input problem under consideration and the LLM(s) being used. Assumption 1. For the input problem x, there exists pgen > 0 such that PyMgen(x)(y is correct solution) pgen > 0. In addition, there exists pcomp > 0.5 such that, for an arbitrary pair of candidate solutions (y, y) where one of them is correct and the other is incorrect, it holds that PrMcomp(x,y,y)(r identifies the right winner) pcomp > 0.5. 3 In other words, we assume that the LLM can generate correct solution with non-zero probability, and do better than random guess in comparing pair of correct and incorrect solutions. Note that pgen and pcomp are defined with respect to specific input problem, not for distribution of problems or benchmark. We will empirically validate this assumption later in Section 3. With this assumption in place, we have the following provable scaling law. Theorem 1. If Assumption 1 holds for the input problem, then the failure probability of the proposed twostage algorithm with hyperparameters (the number of initial candidate solutions) and (the number of times that each pair of solutions involved in the knockout stage are compared) is bounded as follows: P(final output is incorrect) (1 pgen)N + log2 e2K(pcomp0.5)2 . proof can be found in Appendix A. Notably, the failure probability decays to zero exponentially with respect to the hyperparameters and that determine the amount of test-time compute. Another way to interpret this theorem is as follows: for targeted success probability 1 δ (which can be arbitrarily close to 1 as the failure probability δ > 0 approaches zero), it suffices to let (1 pgen)N exp(pgenN ) δ/2 and log2 exp(2K(pcomp 0.5)2) δ/2, namely 1 pgen log (cid:17) (cid:16) 2 δ and 1 2(pcomp 0.5)2 log (cid:16) 2log2 δ (cid:17) . (1) In other words, the hyperparameters and have logarithmic dependence on 1/δ, and linear dependence on 1/pgen and 1/(pcomp 0.5)2 respectively."
        },
        {
            "title": "2.2 Analysis of efficiency",
            "content": "The minimalistic implementation of the proposed algorithm starts by generating candidate solutions with LLM calls that can run in parallel. Since the number of candidates is reduced by half at each round of the knockout tournament, there is at most log2 rounds in total. For notational convenience, let us assume that is power of 2 for the rest of this analysis. At the i-th round, there are N/2i pairs of candidates, and each pair need comparisons; thus total of N/2i LLM calls are needed, which again can be parallelized. In sum, the total number of LLM calls required by the two-stage algorithm is + (cid:16) 2 + 22 + (cid:17) 23 + + 2 + 1 + = (K + 1) N, whereas the end-to-end latency, if sufficiently many machines are available, is merely where Tgen and Tcomp represent the latency of one LLM call for generating candidate solution and for comparing pair of solutions, respectively. Tgen + log2(N ) Tcomp,"
        },
        {
            "title": "3 Experiments",
            "content": "Setup. We validate the efficacy of the proposed two-stage algorithm and the above analysis with the challenging MMLU-Pro benchmark [26]. It contains 14 categories of multiple-choice questions, some of which require advanced reasoning for obtaining the correct answers. Due to limited computational resources, we use randomly sampled subset of 100 questions for each category in our experiments, which leads to total of 1400 questions. Our implementation of the proposed algorithm, which is built upon the AgentScope framework [7], is efficient and scalable, with support for parallel and distributed computation. We use two open LLMs in our experiments, namely Llama3.1-70B-Instruct [16] and Qwen2.5-72B-Instruct [30]. The temperature for LLM decoding is set to 0.8 for both models. We use zero-shot chain-of-thought prompting for both generating candidate solution and comparing pair of solutions; the complete prompts can be found in Tables 1 and 2 in Appendix C. 4 (a) Averaged accuracy across 14 categories. (b) Accuracy and estimated pgen, pcomp for some categories. Figure 3: Empirical results with MMLU-Pro. We use random subset of 100 questions for each category in our experiments. Results for the remaining categories can be found in Figures 4 and 5 in Appendix C. Each horizontal line represents the fraction of questions for which pgen (estimated with = 32) is non-zero, which serves as coarse upper bound for the accuracy that can possibly be achieved by the proposed algorithm. For the scatter plots, we neglect questions with estimated pgen = 0 or 1, i.e., questions for which the initial candidate solutions are all incorrect or all correct, since there is no way of obtaining meaningful estimate of pcomp for such questions. 5 Results. Figure 3a shows the overall accuracy achieved by the proposed algorithm, i.e., the fraction of questions that the algorithm solves correctly, on the aforementioned subset of MMLU-Pro. It is confirmed that the accuracy improves as we scale up the test-time compute, which is determined by the hyperparameter that represents the number of initial candidate solutions, as well as the hyperparameter representing the number of times that each pair of solutions involved in the knockout tournament are compared. More interestingly, Figure 3b shows that the benefits of scaling up test-time compute vary across different categories of MMLU-Pro. For example, the gains are more significant in math and engineering than in psychology. Different LLMs also exhibit different behaviors, as indicated in the history category. One intuitive explanation is that, for reasoning-focused questions that require deliberate reasoning in order to achieve the correct answers, LLMs can compare the reasoning processes of two candidate solutions side by side, which provides additional information compared to generating or verifying an individual solution. In contrast, for knowledge-heavy questions that mostly require memorization of relevant knowledge, one would not expect significant gains from pairwise comparison if the LLM simply does not have the right knowledge embedded within its model weights. In order to better understand how the algorithm works and whether Assumption 1 holds true in reality, we estimate the probabilities pgen and pcomp for each question2, which are visualized in the scatter plots in Figure 3. Notably, there exist questions for which pgen is relatively small while pcomp is well above 0.5. For such question, our algorithm can successfully output correct solution, even if the majority of the initial candidate solutions are incorrect. Of course, there are also questions that do not satisfy Assumption 1, which prevents us from boosting the overall accuracy up to 1 on the adopted subset of the MMLU-Pro benchmark. For question with pgen = 0, all initial candidate solutions are incorrect, and so is the final output of the algorithm. If pgen > 0 but pcomp < 0.5, i.e., the LLM has systematic bias that makes it prefer an incorrect solution to correct one for specific question, then our theoretical analysis in Appendix suggests that the success probability of the final output is actually decreasing monotonely in . We refer interested readers to Appendix for concrete case studies and other additional empirical results."
        },
        {
            "title": "4 Discussions",
            "content": "This is still work in progress, with various limitations and remaining work that need to be done. We provide detailed discussions of some aspects in this section."
        },
        {
            "title": "4.1 The rationales behind Assumption 1",
            "content": "The proposed two-stage algorithm by itself is aimed at problems that single LLM call can possibly solve but without absolute certainty, e.g., with non-zero success probability pgen that can be 90% or merely 5%. In such scenarios, Assumption 1 is arguably the minimal assumption that one can hope for, as pcomp > 0.5 means the LLM can do better than random guess in distinguishing correct solution from an incorrect one3. We make few comments about the assumption of pcomp > 0.5: One of our initial intuitions is that, it is often much easier (for LLMs or human) to detect the errors or hallucinations in an incorrect solution when it is placed right next to correct solution. 2When estimating pgen and pcomp, we consider solution as correct if its final answer is correct, i.e., the option that it chooses for the multiple-choice question is the same as the ground-truth label. We estimate pgen by the ratio of correct solutions in the initial candidate solutions. For estimating pcomp, we randomly sample multiple pairs of correct and incorrect solutions, and calculate the average ratio of correct comparison results for these pairs. Despite our best efforts, we note that there are still limitations in this method of estimating pgen and pcomp. For example, there are many cases where solution has an inaccurate reasoning process but concludes with correct final answer by luck, e.g., by choosing the closest option after doing some wrong calculation. This issue not only causes positive bias in estimating pgen, but also negative bias in estimating pcomp, since such superficially correct solutions are more likely to lose in pairwise comparison, compared to solutions that are truly correct with strong reasoning process. Another cause of inaccuracy is that there exist questions in MMLU-Pro that have inaccurate ground-truth labels, e.g., there might be two equivalent options with different expressions but only one of them is marked as the ground-truth label, while our algorithm chooses the other one during the generation or knockout stage. 3Nonetheless, the assumption of pcomp > 0.5 might seem like strong one from different perspective, as it is assumed to hold for an arbitrary pair of correct and incorrect solutions. Future research may try to relax this assumption, e.g., by taking the probability distribution of candidate solutions into account. This assumption is also partly inspired by prior works showing that LLMs are, to some extent, capable of verifying or refining previously generated solution [10, 17, 5, 8], or aggregating multiple candidate solutions and then generating better one [24]. We take further step and notice that comparing pair of correct and incorrect solutions is essentially an easier task than verifying solution, since naive approach of pairwise comparison is simply verifying each solution individually and deciding which one is correct. This assumption is also related to the classical Bradley-Terry model [1] that has been widely adopted in the literature of LLM alignment [20, 21]. In this model, it is assumed that each candidate has latent score, and given two candidates with scores s1 and s2 respectively, one pairwise comparison between them will identify the first one as the winner with probability σ(s1 s2), where σ denotes the logistic function. In our context, if we assume that correct solution has higher score than an incorrect solution does, then pcomp > 0.5 is an immediate implication of the Bradley-Terry model. With the ongoing advancements of LLMs (such as OpenAIs o1 [19]), it is reasonable to anticipate that better LLMs can achieve higher pgen and pcomp for the same problem, which reduces the number of LLM calls required by our algorithm, according to Eq. (1); similarly, Assumption 1 will become valid for more and more challenging problems that currently violate this assumption due to limitations of the existing LLMs. Remark 1. There are other LLM-based algorithms that also enjoy provable scaling laws, but under different assumptions. Take majority voting as an example. To ensure that its success probability in solving problem can be boosted up to 1 with more test-time compute, essentially two assumptions are necessary. The first is that, it should be feasible to divide the candidate solutions into several groups and have meaningful count for each group. This is true for, say, multiple-choice questions, but not for many other tasks where all candidate solutions are distinct, such as open-ended writing. The other necessary assumption is that the probability of generating solution that belongs to the correct group, akin to pgen in our notation, should be larger than that of any other group; otherwise, it has been proved in prior works that the success probability will converge to zero instead as test-time compute scales up [3]. In comparison, our algorithm only requires pgen > 0, though with the additional assumption that pcomp > 0.5. For future research, identifying the assumptions in the first place is perhaps no less important than designing LLM-based algorithm and deriving its provable scaling law based on those assumptions."
        },
        {
            "title": "4.2 Solving complex tasks with the proposed method",
            "content": "The theoretical guarantee in Theorem 1 is not directly applicable to challenging problems that do not satisfy Assumption 1. The common practice of solving challenging problem with LLM-based algorithms or agentic workflows [4, 34] is to leverage task decomposition, i.e., decomposing it into multiple sub-problems and solving each one by one. As long as the sub-problems satisfy Assumption 1 after task decomposition, we can apply the proposed method to each of them. This approach offers not only guarantees for the success probability of solving the original problem, but also higher efficiency compared to solving it directly. To see this, consider scenario where solving the original problem requires solving all 1 sub-problems correctly, and each sub-problem satisfies Assumption 1 with parameters pgen and pcomp. Directly solving all sub-problems has exponentially small success probability pS , and thus generating correct candidate solution alone already requires Ω((1/pgen)S) computation, not to mention identifying which generated candidate is correct. In contrast, by conducting task decomposition and applying our proposed method to each sub-problem, it suffices to guarantee failure probability of δ/S in solving each sub-problem, which implies an overall success probability of 1 δ for solving the original problem, thanks to the union bound. According to Eq. (1), this is guaranteed with gen 1 pgen log (cid:17) (cid:16) 2S δ and 1 2(pcomp 0.5)2 log (cid:16) 2log2 δ (cid:17) . Note that and only have logarithmic dependence on the number of sub-problems S. The total number of LLM calls with this approach is (K + 1) (cf. Section 2.2), which grows with linearly, up to logarithmic factors."
        },
        {
            "title": "4.3 Towards better algorithms with provable scaling laws",
            "content": "The proposed two-stage algorithm in this work shall be regarded as prototype or general principle for algorithm design, which can be further enriched in various ways. Simple and practical improvements. There can be numerous ways of improving the practical performance, such as increasing the diversity of initial candidate solutions (e.g., by using different prompts or LLMs [24]), conducting pairwise comparison with specifically fine-tuned LLM [18] / an ensemble of multiple LLMs / non-LLM method [9], reducing the number of unnecessary pairwise comparisons via early stopping, tuning the prompts or decoding temperatures (e.g., using higher temperature in the generation stage for diversity and lower temperature in the knockout stage for preciseness), among others. An anytime algorithm. The proposed algorithm can be easily converted to an anytime variant [28] that does not require pre-specifying , i.e., the number of initial candidates. This can be useful in practice, when the amount of test-time compute is adaptive. For example, the algorithm might start with 4 candidate solutions and choose the winner via knockout tournament. If more test-time compute is allowed (e.g., the user is not eagerly requesting the solution, or more computational resources become available), then the algorithm can launch another tournament with 4 freshly sampled candidates, the winner of which will compete with the winner of the previous tournament. This complete process is indeed equivalent to single tournament with = 4 + 4 = 8. Such process can continue until the user finally requests the solution; the eventual value of is determined online and automatically achieves the maximum value allowed by the available test-time compute. Dropping the hyperparameter K. The theory in Section 2.1 suggests that, in order to achieve targeted success probability 1 δ, each pair of solutions in the tournament need to be compared for times before In the winner is chosen, where need to be pre-specified and depends on pcomp according to Eq. (1). reality, pcomp is often unknown priori. To address this, we provide an alternative analysis in Appendix B, which suggests that, perhaps surprisingly, the algorithm still works even if is set to 1 or any other value. In other words, the success probability of the algorithm is boosted monotonely up to 1 while the single hyperparameter scales up. This alternative analysis is still preliminary though; we leave the complete analysis to future work. It is not yet clear whether the proposed Pushing the Pareto front of accuracy versus efficiency. algorithm is theoretically optimal under Assumption 1. The amount of test-time compute needed to achieve success probability 1 δ can be derived from Eq. (1). But is there different algorithm that can achieve better computational complexity, under either Assumption 1 or other plausible assumptions? What is the theoretical lower bound for the computational or sample complexity? It would be exciting to see further progress in resolving these questions. Proof of Theorem To begin with, we have straightforward analysis for the failure probability of the generation stage of the algorithm, where candidate solutions are sampled independently: P(no candidate solution is correct) (1 pgen)N . As for the knockout stage, let us first consider single pair of correct and incorrect candidate solutions. Recall that they are compared for times with LLM calls, and each LLM call identifies the correct candidate solution as the winner with probability µ pcomp > 0.5 by assumption. Therefore, the failure probability of comparing this pair of candidates can be bounded as follows, where Xi denotes an independent Bernoulli random variable with mean µ: P(failure of comparison) (cid:16) (cid:88) i[K] Xi (cid:17) 2 = (cid:16) 1 (cid:88) i[K] (cid:17) Xi 0.5 8 = (cid:16) 1 (cid:88) i[K] Xi µ (µ 0.5) (cid:17) exp (cid:16) 2K(µ 0.5)2(cid:17) exp (cid:16) 2K(pcomp 0.5)2(cid:17) . Here we use Hoeffdings inequality [23] in the last line. Now we are ready to control the failure probability of the complete knockout stage. Let us condition on the event that the generation stage succeeds, i.e., there is at least one initial candidate solution that is correct. We arbitrarily pick correct candidate, and focus on its path to the final output of the algorithm in the binary tree visualized in Figure 2. We claim that, with high probability, the comparison (with LLM calls) for each pair along this path yields the correct outcome. This can be proved by induction: for each pair along this path, if one of the input candidates (which is the output of the previous pairwise comparison on this same path) is correct, then the output of comparing this pair will also be correct with failure probability no greater than exp(2K(pcomp 0.5)2), regardless of whether the other input candidate is correct or not. By taking union bound over the failure events along this path with log2 pairs to be compared, we claim that the comparison for each pair along this path yields the correct outcome (which immediately implies that the final output of the algorithm is correct), with failure probability no greater than log2 exp(2K(pcomp 0.5)2). Finally, taking union bound over the failure events of both stages of the algorithm completes our proof of Theorem 1."
        },
        {
            "title": "B An alternative analysis",
            "content": "We provide an alternative analysis for the proposed algorithm, to show that it still enjoys provable scaling law when the hyperparameter is fixed at an arbitrary value. For concreteness, let us consider the extreme case of = 1, i.e., each pair of candidate solutions in the tournament are compared only once, with success probability pcomp > 0.5. We will demonstrate that the success probability of the overall algorithm still grows to 1 monotonely with in this case. For simplicity, let us assume that is power of 2. Let pi be the probability that candidate solution at the i-th level of the knockout tournament is correct, where = 0, 1, . . . , log2 . This is feasible since all candidates within the same level of the tournament have symmetric roles. Initially, p0 = pgen. Next, if pi is given, we can derive pi+1 inductively as follows. candidate at the (i + 1)-th level is the winner of pairwise comparison between pair of statistically independent candidates at the i-th level. Thus, the winner is correct if both candidates of the pair are correct, or only one of them is correct and happens (with probability pcomp) to be chosen as the winner. Therefore, pi+1 = p2 + 2pi(1 pi)pcomp = p2 + 2pcomp(pi p2 = pi + (2pcomp 1)(pi p2 ). ) = pi pi + p2 + 2pcomp(pi p2 ) This implies pi+1 > pi, as long as pcomp > 0.5 and pi < 1. More concretely, there are two cases: If pi < 0.5, then 1 pi > 0.5, and pi+1 = pi + (2pcomp 1)(1 pi)pi > pi + (pcomp 0.5)pi (cid:17) (cid:16) = 1 + (pcomp 0.5) pi. In other words, the sequence {pi} grows exponentially when it is below 0.5. If pi 0.5 instead, then we have 1 pi+1 = 1 pi (2pcomp 1)pi(1 pi) 1 pi (pcomp 0.5)(1 pi) (cid:16) 1 (pcomp 0.5) (cid:17) (1 pi). = In other words, the sequence {1 pi} converges to 0, namely {pi} converges to 1. Consequently, the success probability of the final winner, namely plog2 , grows to 1 monotonely with ."
        },
        {
            "title": "C Additional empirical results",
            "content": "This appendix includes empirical results for the categories of MMLU-Pro that are complementary to those in Figure 3, the prompts used in our experiments, and concrete case studies that demonstrate how our algorithm works when solving specific questions. 10 Figure 4: Accuracy and estimated pgen, pcomp for some categories of MMLU-Pro (Part 1). 11 Figure 5: Accuracy and estimated pgen, pcomp for some categories of MMLU-Pro (Part 2). Table 1: The adopted prompt template for generating candidate solution. % System prompt Your task is to answer the question provided by the user via stepbystep reasoning. % User prompt # Question {question} # Output Format <reason>your stepbystep reasoning proecss</reason> <answer>final answer: The answer is [X]</answer> Table 2: The adopted prompt template for pairwise comparison. % System prompt You are fair Judge. Given question and two candidate solutions, your task is to figure out which solution is better . Your judgment should be unbiased, without favoring either Solution 1 or 2. % User prompt QUESTION {question} Solution 1 {solution 1} Solution 2 {solution 2} OUTPUT FORMAT <compare>compare both candidate solutions stepbystep thoroughly, and double check if there are mistakes in either solution</compare> <winner>Solution 1 or Solution 2</winner> 13 Table 3: Case study: the LLM is capable of conducting pairwise comparison for question where pgen is small and the majority of the initial candidate solutions are incorrect. This question is sampled from the validation set for the engineering category of MMLU-Pro. Question: In 2 pole lap winding dc machine , the resistance of one conductor is 2Ω and total number of conductors is 100. Find the total resistance (A) 50Ω (B) 1Ω (C) 25Ω (D) 200Ω (E) 10Ω (F) 100Ω (G) 500Ω (H) 150Ω (I) 75Ω (J) 20Ω Solution 1: Reasoning: In 2 pole lap winding DC machine, the conductors are connected in parallel in each of the two parallel paths. Since the total number of conductors is 100 and the machine has 2 poles, there are 50 conductors in each parallel path. The resistance of one conductor is 2Ω, so the total resistance of the 50 conductors in one path is 2Ω because they are in parallel (1/total resistance = 1/R1 + 1/R2 + ... + 1/Rn, where = 50 in this case, and all Rs are equal to 2Ω, so total resistance = 2Ω). Since there are two such parallel paths, the total resistance of the machine is 1Ω (again, because the two paths are in parallel ) . Final answer: Solution 2: Reasoning: In lap winding DC machine with 2 poles, the conductors are arranged in series in each arm of the winding, and the two arms are connected in parallel . Since there are 100 conductors in total and 2 poles, there are 50 conductors in each arm (100 conductors / 2 poles = 50 conductors per arm). The resistance of one conductor is 2Ω. Since the conductors in each arm are in series , the total resistance of one arm is 50 conductors 2Ω per conductor = 100Ω. Since the two arms are connected in parallel, the total resistance is (100Ω 100Ω) / (100Ω + 100Ω) = 50Ω. Final answer: Pairwise comparison by the LLM: Assistant : Solution 2 is more plausible. Explanation: Solution 1 assumes that the 50 conductors in each path are connected in parallel , which is incorrect . In lap winding DC machine, the conductors are connected in seriesparallel arrangement. The conductors in each arm are connected in series, and the two arms are connected in parallel . Therefore, solution 1s calculation of the total resistance as 1Ω is incorrect. Solution 2 correctly identifies that the conductors in each arm are connected in series and that the two arms are connected in parallel . The calculation of the total resistance of one arm as 100Ω is correct (50 conductors 2Ω per conductor). The calculation of the total resistance of the machine as 50Ω (using the parallel resistance formula) is also correct . Therefore, Solution 2 is more plausible."
        },
        {
            "title": "References",
            "content": "[1] Ralph Allan Bradley and Milton E. Terry. Rank Analysis of Incomplete Block Designs: I. The Method of Paired Comparisons. Biometrika, 39:324, 1952. [2] Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher Ré, and Azalia Mirhoseini. Large Language Monkeys: Scaling Inference Compute with Repeated Sampling. arXiv, 2024. [3] Lingjiao Chen, Jared Quincy Davis, Boris Hanin, Peter Bailis, Ion Stoica, Matei Zaharia, and James Zou. Are More LLM Calls All You Need? Towards the Scaling Properties of Compound AI Systems. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [4] Yanxi Chen, Yaliang Li, Bolin Ding, and Jingren Zhou. On the Design and Analysis of LLM-Based Algorithms. arXiv, 2024. [5] Jared Quincy Davis, Boris Hanin, Lingjiao Chen, Peter Bailis, Ion Stoica, and Matei Zaharia. Networks of Networks: Complexity Class Principles Applied to Compound AI Systems Design. arXiv, 2024. [6] Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch. Improving FacIn Forty-first International tuality and Reasoning in Language Models through Multiagent Debate. Conference on Machine Learning, 2024. [7] Dawei Gao, Zitao Li, Xuchen Pan, Weirui Kuang, Zhijian Ma, Bingchen Qian, Fei Wei, Wenhao Zhang, Yuexiang Xie, Daoyuan Chen, Liuyi Yao, Hongyi Peng, Zeyu Zhang, Lin Zhu, Chen Cheng, Hongzhu Shi, Yaliang Li, Bolin Ding, and Jingren Zhou. AgentScope: Flexible yet Robust Multi-Agent Platform. arXiv, 2024. [8] Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. Large Language Models Cannot Self-Correct Reasoning Yet. In The Twelfth International Conference on Learning Representations, 2024. [9] Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1416514178, 2023. [10] Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El-Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. Language Models (Mostly) Know What They Know. arXiv, 2022. [11] Subbarao Kambhampati, Karthik Valmeekam, Lin Guan, Mudit Verma, Kaya Stechly, Siddhant Bhambri, Lucas Paul Saldyt, and Anil Murthy. Position: Llms cant plan, but can help planning in llm-modulo frameworks. In Forty-first International Conference on Machine Learning, 2024. [12] Sayash Kapoor, Benedikt Stroebl, Zachary S. Siegel, Nitya Nadgir, and Arvind Narayanan. AI Agents That Matter. arXiv, 2024. [13] Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large In Advances in Neural Information Processing Systems, Language Models are Zero-Shot Reasoners. volume 35, pages 2219922213, 2022. [14] Junyou Li, Qin Zhang, Yangbin Yu, Qiang Fu, and Deheng Ye. More Agents Is All You Need. Transactions on Machine Learning Research, 2024. 15 [15] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan In The Twelfth Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets Verify Step by Step. International Conference on Learning Representations, 2024. [16] Llama Team, AI @ Meta. The Llama 3 Herd of Models. arXiv, 2024. [17] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-Refine: Iterative Refinement with Self-Feedback. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [18] Dakota Mahan, Duy Van Phung, Rafael Rafailov, Chase Blagden, Nathan Lile, Louis Castricato, JanPhilipp Fränken, Chelsea Finn, and Alon Albalak. Generative Reward Models. arXiv, 2024. [19] OpenAI. OpenAI o1 System Card. https://openai.com/index/openai-o1-system-card/, 2024. [20] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, 2022. [21] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct Preference Optimization: Your Language Model is Secretly Reward Model. In Advances in Neural Information Processing Systems, 2023. [22] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters. arXiv, 2024. [23] Roman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Science. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2018. [24] Junlin Wang, Jue Wang, Ben Athiwaratkun, Ce Zhang, and James Zou. Mixture-of-Agents Enhances Large Language Model Capabilities. arXiv, 2024. [25] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-Consistency Improves Chain of Thought Reasoning in Language Models. In The Eleventh International Conference on Learning Representations, 2023. [26] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. MMLU-pro: more robust and challenging multi-task language understanding benchmark. In Advances in Neural Information Processing Systems, 2024. [27] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi, F. Xia, Quoc Le, and Denny Zhou. Chain of Thought Prompting Elicits Reasoning in Large Language Models. In Advances in Neural Information Processing Systems, 2022. [28] Wikipedia. Anytime algorithm. https://en.wikipedia.org/wiki/Anytime_algorithm, 2024. [29] Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models. arXiv, 2024. [30] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang 16 Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 Technical Report. arXiv, 2024. [31] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of Thoughts: Deliberate Problem Solving with Large Language Models. In Thirtyseventh Conference on Neural Information Processing Systems, 2023. [32] Zhangyue Yin, Qiushi Sun, Cheng Chang, Qipeng Guo, Junqi Dai, Xuanjing Huang, and Xipeng Qiu. Exchange-of-Thought: Enhancing Large Language Model Capabilities through Cross-Model Communication. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1513515153, 2023. [33] Zhangyue Yin, Qiushi Sun, Qipeng Guo, Zhiyuan Zeng, Xiaonan Li, Tianxiang Sun, Cheng Chang, Qinyuan Cheng, Ding Wang, Xiaofeng Mou, Xipeng Qiu, and Xuanjing Huang. Aggregation of Reasoning: Hierarchical Framework for Enhancing Answer Selection in Large Language Models. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 609625, 2024. [34] Matei Zaharia, Omar Khattab, Lingjiao Chen, Jared Quincy Davis, Heather Miller, Chris Potts, James Zou, Michael Carbin, Jonathan Frankle, Naveen Rao, and Ali Ghodsi. The Shift from Models to Compound AI Systems. https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems, 2024. [35] Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative Verifiers: Reward Modeling as Next-Token Prediction. In The 4th Workshop on Mathematical Reasoning and AI at NeurIPS24, 2024."
        }
    ],
    "affiliations": [
        "Alibaba Group"
    ]
}