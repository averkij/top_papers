{
    "paper_title": "Controllable Human Image Generation with Personalized Multi-Garments",
    "authors": [
        "Yisol Choi",
        "Sangkyung Kwak",
        "Sihyun Yu",
        "Hyungwon Choi",
        "Jinwoo Shin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present BootComp, a novel framework based on text-to-image diffusion models for controllable human image generation with multiple reference garments. Here, the main bottleneck is data acquisition for training: collecting a large-scale dataset of high-quality reference garment images per human subject is quite challenging, i.e., ideally, one needs to manually gather every single garment photograph worn by each human. To address this, we propose a data generation pipeline to construct a large synthetic dataset, consisting of human and multiple-garment pairs, by introducing a model to extract any reference garment images from each human image. To ensure data quality, we also propose a filtering strategy to remove undesirable generated data based on measuring perceptual similarities between the garment presented in human image and extracted garment. Finally, by utilizing the constructed synthetic dataset, we train a diffusion model having two parallel denoising paths that use multiple garment images as conditions to generate human images while preserving their fine-grained details. We further show the wide-applicability of our framework by adapting it to different types of reference-based generation in the fashion domain, including virtual try-on, and controllable human image generation with other conditions, e.g., pose, face, etc."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 5 2 ] . [ 1 1 0 8 6 1 . 1 1 4 2 : r Controllable Human Image Generation with Personalized Multi-Garments Yisol Choi1 Sangkyung Kwak1 Sihyun Yu1 Hyungwon Choi2 Jinwoo Shin1 1KAIST 2OMNIOUS.AI {yisol.choi, skkwak9806, sihyun.yu, jinwoos}@kaist.ac.kr, hyungwon.choi@omnious.com Figure 1. Generated images by BootComp. (a) BootComp generates high-quality human images wearing multiple reference garments, with support for extended categories such as bag, shoes, even in unusual garment combinations (e.g., swimming suit with soccer cleats). We show BootComps generalization capability through various conditional image generations, such as (b) virtual try-on, (c) pose guided generation, (d) stylization, and (e) text guided generation, even though BootComp is not directly trained or fine-tuned for each task."
        },
        {
            "title": "Abstract",
            "content": "We present BootComp, novel framework based on text-toimage diffusion models for controllable human image generation with multiple reference garments. Here, the main bottleneck is data acquisition for training: collecting large-scale dataset of high-quality reference garment images per human subject is quite challenging, i.e., ideally, one needs to manually gather every single garment photograph worn by each human. To address this, we propose data generation pipeline to construct large synthetic dataset, consisting of human and multiple-garment Project page: https://yisol.github.io/BootComp pairs, by introducing model to extract any reference garment images from each human image. To ensure data quality, we also propose filtering strategy to remove undesirable generated data based on measuring perceptual similarities between the garment presented in human image and extracted garment. Finally, by utilizing the constructed synthetic dataset, we train diffusion model having two parallel denoising paths that use multiple garment images as conditions to generate human images while preserving their fine-grained details. We further show the wideapplicability of our framework by adapting it to different types of reference-based generation in the fashion domain, including virtual try-on, and controllable human image generation with other conditions, e.g., pose, face, etc. Figure 2. Limitations of previous data curation approaches used in controllable generation. Previous approaches on controllable generation often use paired dataset consisting of low-quality segmented garments and human images for training. It leads to several undesirable artifacts as shown in right (generated with baselines). For example, garments are directly replicated from the reference images in (a), shirts and skirts are blended together in (b), and generated skirts fail to resemble the reference in (c). 1. Introduction Recent advances in text-to-image (T2I) diffusion models [7, 36, 39] have shown great progress in numerous challenging real-world scenarios, such as personalized generation [22, 40], style transfer [11, 45], image editing [1, 10, 29], and compositional image generation [32, 35, 44]. These remarkable successes have provided great potential to aid users in variety of creative pursuits [21]. Among them, controllable human image generation using T2I diffusion models [16] can provide lots of intriguing use cases in real-world scenarios. Specifically, by training model capable of creating human images conditioned on variety of garments, one can enjoy diverse applications such as outfit recommendations for users, generating fashion models for clothing brands, or virtual try-on [6, 19, 31], through single unified framework. One can consider fine-tuning T2I models and image encoders [34, 37] using curated paired image datasets that consist of condition garments and the target human images [16]. However, hand-collecting multiple garment photographs worn by human is labor-intensive. Prior works [15, 35, 50] have attempted to obtain the pair images by extracting all reference objects from real images, segmenting out each object from the original images. However, this data curation protocol makes curated garments have exactly the same shape with their appearance in the target human image. Thus, generated images are likely to suffer from copyand-paste problem: they easily generate exactly the same image in generated samples without altering pose or appearance (see (a) in Fig. 2). To mitigate this issue, several works propose to curate data from videos by doing segmentation from different video frames that contain the same objects [4, 44]. However, collecting such paired datasets in large amounts is challenging and often results in low-quality reference images; thereby, the trained model fails to generalize and suffers from subject blending or inconsistency within the images [44] (see (b), (c) in Fig. 2). Such drawbacks become more critical in practical scenarios related to human image generation, as the model must generate human images with diverse poses while accurately preserving the details of each garment. Contribution. We address the aforementioned shortcomings by presenting Bootstrapping paired data for Compositional and controllable human image generation (BootComp), novel framework for controllable human image generation using T2I diffusion models. Specifically, it is two-stage framework (see Fig. 3 for illustration): Synthetic data generation: We first propose high-quality synthetic data generation pipeline for training controllable human image generation model. We achieve this by introducing decomposition module, which is mapping from single garment worn by human to product view of the garment image. We train this model with paired dataset of single reference garment and human image (e.g., shirts and human wearing those shirts), which is easy to collect [5, 23, 30]. Using this model, we bootstrap synthetic paired data at scale from large number of human images; thus, each pair consists of human image and all garment images that the human is wearing. To ensure high-quality data, we also present filtering strategy that further improves the data quality based on measuring the perceptual similarities between the original segmentation results and the data generated from the decomposition module. Composition module: We also present fine-tuning scheme of T2I diffusion models for our goal using the synthetic dataset. We use two T2I diffusion models: one serves as an image encoder to extract garment features and the other one functions as generator to create human images. We only train the encoder model, employing an extended self-attention mechanism to generator for conditioning garment images. Since we keep the generator frozen during the training, BootComp can be attached to various adapter modules or replaced with pretrained models specialized to generate images with different styles. This enables BootComp to provide various applications (e.g., pose-guided or cartoon-style generation) for free without requiring any additional fine-tuning. Figure 3. Overview of BootComp. We propose two-stage framework: synthetic data generation and composition module training for controllable human image generation. (a) We train decomposition network that maps from segmented garment image to product garment image. (b) We bootstrap synthetic paired data of human and multiple garment images. (c) We finally train our composition module with the synthetic paired dataset enabling it to generate human images with multiple reference garment images. We demonstrate the effectiveness of BootComp in terms of garment fidelity and compositionality through extensive experiments. For example, BootComp shows 30% improvement on MP-LPIPS [3] than the previous state-of-the-art methods. Moreover, our BootComp is extensively applied to various conditional human image generations in the fashion domain, such as virtual try-on and controllable human image generation with other conditions, such as faces and poses. We also highlight the generalization capabilities of BootComp across different image domains, generating human images in various styles like cartoons. 2. Background 2.1. Diffusion Models Diffusion models [14, 42] are type of generative model consists of forward process and reverse process. Here, the diffusion models learn the reverse process of forward diffusion, where the forward process is defined as Markov chain that incrementally adds Gaussian noise to data. Formally, let x0 represent data instance (e.g., an image or latent vector from an autoencoders output [39]). Diffusion models consider pre-defined forward process q(xtx0) given closed form as normal distribution (αtx0, σ2 I), so the sampling can be done from Gaussian distribution ϵ (0, I) using reparametrization to have xt = αtx0 + σtϵ. Here, {αt}T t=1 are pre-defined decreasing and increasing noise scheduling sequences (respectively) for = 1, . . . , that let p(xT ) converge distribution close to Gaussian distribution (0, I). Learning the reverse process pθ(xt1xt) of diffusion t=1 and {σt}T model is equivalent to learning score function of perturbed data distribution (through score matching [17]), typically achieved via an ϵ-noise prediction loss [14] by training denoising autoencoder. Specifically, one can formulate the training objective of the diffusion model as: LDM = EϵN (0,I), tU [0,T ] (cid:2) ω(t)ϵθ(xt; t) ϵ2 2 (cid:3), where ω(t) > 0 is weight function at each timestep and U[0, ] denotes uniform distribution. After training, data sampling can be done using the learned reverse process. Specifically, starting from xT (0, σ2 I), the model gradually denoises xt to xt1 for each t, until x0 is drawn from the data distribution. 2.2. Text-to-Image (T2I) Diffusion Models Text-to-image (T2I) diffusion models [7, 39, 41] are textconditional diffusion models ϵθ(xt; c, t) that generate an image x0 conditioned on given text prompt c. This prompt is usually provided as text representation encoded by pretrained text encoders, such as T5 [38] or CLIP [37]. Commonly, T2I diffusion models employ convolutional U-Net architectures combined with attention layers [14, 43] to condition the model on texts, Among T2I diffusion models, Stable Diffusion [SD; 39] is one of the de-facto T2I diffusion models that generates high-quality images. We mainly use Stable Diffusion XL (SDXL) [36], one of the SD variants. However, our framework is model-agnostic and can be adapted to any other T2I diffusion models. 3. Method Let = {x1, . . . , xN } be set of 1 reference garment images (e.g., shirt, pants, etc.) and be human image that is wearing x1, . . . , xN . Our goal is to learn conditional distribution p(yX)we train conditional generative model gθ(X) = that generates human image wearing arbitrary garment images given as condition. One straightforward direction is to train the model gθ using paired dataset = {(Xi, yi)}d i=1 with dataset size > 0, where each Xi = {xi 1, . . . , xi } consists of Ni difNi ferent number of reference images. However, this approach suffer from data acquisition problem: collecting all of the reference garment images of given human image is wearing is challenging. In practice, there usually exists single reference image, i.e., Ni mostly becomes 1 (e.g., human and pants that he/she is wearing). Thus, the model trained with this data easily lacks compositional generalization capability at inference time, i.e., the trained model gθ fails to generate the human image with large number of garments. To tackle this data curation problem, we introduce an additional decomposition network fϕ that can extract reference images from given human image. By doing so, we generate synthetic dataset D, where each ( Xi, yi) satisfies Xi 1 and yi is in the original dataset. We then train the conditional generative model gθ using this synthetic dataset. Here, we also introduce filtering strategy to improve the quality of the synthetic dataset generated from fϕ, by removing low-quality extraction results. In the rest of this section, we explain our BootComp in detail. In Section 3.1, we describe the training data generation process, introducing our decomposition network fϕ, which is used for synthetic data generation, and explaining our data filtering strategy. Finally, in Section 3.2, we explain the details of our network for our original goal of controllable generation trained with the synthetic dataset. 3.1. Training data generation Decomposition module. Our decomposition module generates single garment image in product view, denoted as x, from garment of category that human is wearing. We consider this mapping as an image-to-image translation problem: generating the reference garment image from the portion of person image that falls into category m. To achieve this, we initialize diffusion model fϕ as pre-trained text-to-image diffusion model and fine-tune it with the following objective: (cid:104) ω(t)(cid:12) L(ϕ) := (cid:12) (cid:12) (cid:12)fϕ(xt; c, t, xs) ϵ(cid:12) (cid:12) (cid:12) (cid:12) (cid:105) , 2 2 (1) where xs = S(y, m) is segmented garment part using an off-the-shelf human parsing model [49], and we let text prompt be product photo of {category} to extensively leverage the prior knowledge of the T2I diffusion model. Figure 4. Extended self-attention architecture. In extended self-attention layer, reference hidden states are concatenated with the target hidden states in the key and value matrices. This architecture enables injecting reference image features within the target image. Note that decomposition module also uses same structure but works within single network. To condition the model on an image xs, we utilize the pretrained diffusion model as an image encoder, which can extract rich features and can preserve the fine-details (e.g., small logos). Specifically, for each self-attention layer in the model, we concatenate the corresponding key and value vectors computed with xs, so the self-attention in the forwarding path of xt can be conditioned on xs (see Fig. 4). Finally, note that training fϕ can be done with the dataset which consists of pair of single reference garment and human image, because we train the model to extract single reference garment from the human image. Synthetic data generation with filtering. After training the decomposition module, one can use it for extracting all of the reference images = {x1, . . . , xN } from each human image y. It results in synthetic dataset D, which can be used for the conditional generative model gϕ for our goal of controllable generation. However, we find that the decomposition network fϕ sometimes generates lowquality reference images, especially when the prediction results from the parsing model are incorrect, which might harm the performance of gϕ (see Fig. 5). Thus, we introduce simple filtering strategy to improve the quality of our synthetic dataset D. Specifically, we measure the image similarity score between the generated garment image = fϕ(y, m) and the segmentation results xs. We discard pair sets if any garment in the set has similarity score below the threshold value τ > 0, namely: d(xs, x) < τ (2) For the scoring function for image similarity, we empirically find that dreamsim [8] aligns the most with human perception (See Appendix for details). 3.2. Composition module Our composition module consists of two diffusion models: one for generation and the other one for an image encoder, denoted by gθ and gθ, respectively. Both networks Can BootComp generate authentic human images wearing multiple garments while preserving details? (Tab. 1, Fig. 6) Is our data generation pipeline effective and scalable, ensuring the models performance? (Tabs. 2 and 3, Fig. 9) Can BootComp be used for wide range of downstream tasks?  (Fig. 7)  4.1. Experiment Setup We explain some important experimental setups in this section. We include more details in Appendix A. Implementation details. We use Stable Diffusion XL (SDXL) [36] for model initializations. We collect humansingle reference garment paired datasets from VITONHD [5], DressCode [30] and LAION-Fashion [23] for training the decomposition module. The dataset consists of 25,210 upper garments, 7,151 lower garments, 27,677 dresses, 5,675 bags, 1,599 shoes, 825 scarf, and 159 hats, resulting 68,296 single reference pairs on different categories. We train the decomposition module for 140K iterations with total batch size of 32 on 4 H100 GPUs. For the data generation phase, we process 240K human images obtained from VITON-HD, DressCode, LAION-Fashion, and DeepFashion [28] datasets, thereby collecting 240K paired data of human image and multiple garment images at resolution 512384. We obtain and use 54K high-quality paired data after applying our filtering strategy with the threshold value τ = 0.4. For the composition module, we train for 115K iterations with total batch size of 48 on 8 H100 GPUs. For inference, we use the DDPM sampler [14] with sampling step of 50, where we apply classifier-free guidance (CFG; [13]) with guidance scale of = 2.0. Baselines. First, we consider MIP-Adapter [15] as baselines, which is recent generic controllable generation method with multiple conditions. We also compare BootComp with FromParts2Whole [16], the most relevant baseline for our task that aims for controllable human image generation with multiple reference garments. We use the official model parameters from their official implementations. We employ model wearing upper garment and lower garment and shoes as the text prompt to both models. Evaluation metric. We report Frenchet Inception Distance (FID) [12], MP-LPIPS [3], and two different image similarities metrics [15, 44] using DINOv2 [34] (DINO and MDINO). First, we use the FID score to measure the fidelity of generated human images, i.e., whether multiple garments are harmonized in the generated images. Next, we employ MP-LPIPS to evaluate the consistency of the target image to the source ground-truth garment. Finally, DINO and MDINO measure the semantic similarity between each reference garment image and the respective garment present in the generated human image. Figure 5. Examples of high&low-quality generated garments. When human parsing results are not precise, the decomposition network struggles to generate product garment images accurately, resulting in low-quality garment images. We filter out these cases. are initialized with the same pre-trained T2I diffusion models, where we freeze gθ used as generator and only train the encoder network gθ using the synthetic dataset D. In particular, the encoder gθ is used to provide conditioning of garments to the generator gθ. To condition to the generation model gθ , we concatenate the key and value vectors in each self-attention layer computed with each X and corresponding category mx using the encoder model gθ. By doing so, generator gθ can be conditioned on through its attentions. In particular, query, key, and value vectors of each of the attention layer in gθ are computed with the following vectors query := hy, key, value := [hy, hx1 , . . . , hxN ], (3) where hy and [hx1, . . . , hxN ] are hidden states before the self-attention layer computed with the generation model gθ and the encoder model gθ, respectively. To compute each hx we provide the text caption photo of {category} to the encoder model gθ, where {category} is type of garment x. Thus, we fine-tune the encoder gθ through the diffusion model objective of the generator gθ : (cid:104) L(θ) := ω(t)(cid:12) (cid:12) (cid:12) (cid:12)gθ(yt; c, t, X) ϵ(cid:12) (cid:12) (cid:12) (cid:12) (cid:105) , 2 (4) where we employ synthetic text description for human image generated by vision-language model [27] for c. 4. Experiments We validate the effectiveness of BootComp and the effect of the proposed components through extensive experiments. In particular, we investigate the following questions: Figure 6. Qualitative comparison of human image generation with multiple garments. BootComp generates realistic human images with multiple reference garments even with non-straightforward combinations of garments without losing details of each reference. For example, Parts2Whole replaces reference soccer cleats with stilettos, while ours accurately generates each reference (left, middle row). Table 1. Quantitative comparisons. We compare BootComp with baselines on garment similarity and image fidelity. We see that BootComp outperforms other methods, preserving fine-details of garments and naturally generating human images. Method MP-LPIPS DINO M-DINO FID MIP-Adapter [15] Parts2Whole [16] BootComp (ours) 0.276 0.267 0.187 0.308 0.362 0.379 0.025 0.036 0. 59.99 28.39 27.63 Evaluation datasets. We manually collect dataset for evaluation as there are no common datasets for evaluating controllable human image generation. To evaluate MPLPIPS, DINO, and M-DINO, we curate 5,000 garment image sets of three representative garment categories for human images (upper and lower garments and shoes). We randomly take upper and lower garment images from the test dataset of DressCode [30] dataset and shoe images from public dataset.1 Next, for evaluation using FID, we gather 30,000 human images wearing various garments in different poses from the test dataset of DressCode, VITON-HD, and Deepfashion to use them as reference image sets. 1https://www.kaggle.com/datasets/noobyogi0100/shoe-dataset 4.2. Results Qualitative results. We provide qualitative comparisons of our method (BootComp) with other baseline methods in Fig. 6. As shown in this figure, BootComp generates more realistic human images in various poses, faithfully preserving details of reference garment images, while other methods often generate human images wearing garments inconsistent with the references. Moreover, this result shows that BootComp generates creative combinations of garments. For instance, in the first example of the second row, BootComp generates human image with uncommon combination of garments (e.g., trousers with soccer cleats) but Parts2Whole or MIP-Adapter fails to achieve this: they either undesirably replace the cleats to trousers or struggle with generating high-fidelity garments (respectively). We provide more visualizations in Appendix D. Quantitative results. We report quantitative evaluation results of BootComp and baselines in Tab. 1. BootComp outperforms both MIP-Adapter and Parts2Whole across all of four evaluate metrics. In particular, BootComp achieves 30% improvement in MP-LPIPS score over the baselines, demonstrating its effectiveness in preserving garment details. Moreover, BootComp shows its capabilities in authentic image generation for human images, as indicated by better FID values than baselines. Figure 7. More applications of BootComp. We showcase the extensive applications of our method, BootComp. BootComp creates human images by controlling the (a) poses and (b) styles of the generated human images. BootComp also enables (c) personalized human image generation by taking users images as conditions (e.g., face, full body). More applications. In Fig. 7, we apply BootComp to several downstream tasks and visualize their results. First, we show that BootComp can generate human images conditioned on the pose. In Fig. 7 (a), BootComp generates human images in diverse poses following the extra conditions even with reference garments of intricate patterns, demonstrating its generalization capability. We also show that BootComp can generate human images with different stylizations such as cartoons in Fig. 7 (b). Finally, we show that BootComp can be used for personalized human image generation such as virtual try-on, i.e., changing garments on given human image to reference garments. In Fig. 7 (c), BootComp replaces garments on given human image with the reference garment images and enables personalized generation conditioning face image. Note that this can be done without any additional taskspecific fine-tuning as we freeze the generator in the composition module during training. This enables BootCompto be easily integrated with other modules, e.g., IP-Adapter [50] or ControlNet [53], that provides controllability with additional condition inputs. We provide additional generation results for each application in Appendix D. 4.3. Analysis and ablation studies Finally, we conduct several analyses on synthetic data to validate our data generation pipeline, including its scalability and the impact compared with naıve use of segmented paired dataset. To reduce the computation cost, we use Stable Diffusion v1.5 for all analyses while we strictly follow the other setups used in the main experiments. Figure 8. Visualization of segmented paired data and our synthetic paired data. We provide visual comparison between segmented and synthetic pairs. Given single garment and human image pair, we segment out other garments from the human image in the segmented paired data. Table 2. Comparison on dataset construction methods. The model trained on the segmented paired dataset shows worse performance compared to one trained on our synthetic paired dataset both in garment similarity and image fidelity. Dataset MP-LPIPS DINO M-DINO FID Segmented Synthetic 0.374 0.197 0.284 0.365 0.025 0. 59.27 29.41 Effect of data generation. We first show the effect of our data generation scheme. We demonstrate this by constructing dataset by segmenting out all garment images from the human except the given one in the dataset (see Fig. 8), and train the composition module on this dataset. As shown in Tab. 2, the model trained on the segmented paired dataset achieves worse performance across all evaluation metrics. Also, Fig. 9 visualizes undesirable generated images by the model trained on the segmented dataset. This indicates the model struggles to generate desirable human images, highlighting the effectiveness of our data generation scheme. preserve the identity of subjects present in the source image. majority of works achieve this by proposing modules that can condition source images to the model [24, 35, 50] since T2I diffusion models were originally trained to be conditioned on text prompts only. Despite their effort, they have struggled to be generalized with multiple subjects and suffer from several issues, such as subject blending. To mitigate this issue, several approaches such as MS-Diffusion [44] and FastComposer [47] propose utilizing an additional regional information for each subject. Our framework also tries to improve image generation when multiple subjects are given as conditions, but we focus on human image generation and propose novel data generation pipeline to improve the quality. Improving diffusion models with self-data generation. Inspired by great progress of T2I diffusion models, many recent works have tried to improve the performance of the pretrained model itself on the specific tasks [1, 9, 18, 46, 52] using generated image data from the same model. First, InstructPix2Pix [1] exploits large language models (LLMs) [2] to generate instructions and edited captions and use these captions to construct pairs of images [10] from pairs of captions. This dataset is used to fine-tune the model specialized for zero-shot image editing guided with text instruction. Similarly, JeDi [52] also generates same-subject images using LLMs and pretrained T2I diffusion models. They are used to fine-tune T2I diffusion models for personalized generation [40] without the need for additional tuning at inference. However, these approaches are not suitable for the case of images with multiple subjects, such as controllable human generation, as most T2I diffusion models still lack the capability to accurately generate images with multiple subjects [25, 33]. As result, synthetic image data with multiple subjects generated with T2I models often exhibit lowquality results, and thus fine-tuning with this dataset does not lead to the improvement. Thus, rather than generating multi-subject images from T2I models, existing approaches have curated data through segmentation from the multisubject images [16]. However, these models suffer from the copy-and-paste and subject inconsistency problems. Our method bridges the former and latter approaches to improve data quality used for controllable human generation. 6. Conclusion In this paper, we present BootComp, novel framework for controllable human image generation with multiple garments given as image conditions. Our synthetic paired data generation and composition module enabled creating human images wearing multiple reference garment images. We show the broad applicability of BootComp by adapting it to various types of reference-based generation tasks in the fashion domain. Figure 9. Visual comparison on data construction methods. Visual comparison between generated human images where each model is trained on segmented and synthetic pairs. The model trained on segmented pair data struggles to generate naturally harmonized human images (red). Table 3. Comparison on dataset scale. Training with larger datatset (after filtered) improves the models overall performance in both garment similarity and image fidelity. Dataset size DINO M-DINO FID 5K 15K 30K 50K 0.337 0.338 0.344 0.360 0.248 0.251 0.261 0. 34.15 32.32 26.99 25.88 Scalability of the data generation scheme. Next, we investigate the scalability of our data generation scheme by exploring the effect of dataset size to the performance. We observe that using larger dataset for training always improves the models performance in both garment fidelity and image fidelity, as shown in Tab. 3. Table 4. Ablation study for threshold value τ on filtering. The data quality improves with stricter threshold value, leading to better performance. We adopt τ = 0.4 when applying the filtering. τ 0.4 0. 0.6 0.7 1.0 DINO 0.360 0. 0.343 0.342 0.338 Ablation study: threshold value τ . Finally, we conduct an ablation study on the threshold value τ used in our dataset filtering strategy. In Table. 4, we report similarity score (DINO) of the models trained with different datasets by varying values of τ from 0.4 to 1.0, where 1.0 indicates no filtering is applied. We observe that more strict data filtering can provide more performance gain to the model. 5. Related Work Controllable image generation. In addition to providing text prompts as conditions, recent works have attempted to improve the controllability of pre-trained text-to-image (T2I) diffusion models by letting the model to be conditioned on additional inputs (e.g., images). In particular, many works focus on the problem of generating images that"
        },
        {
            "title": "References",
            "content": "[1] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1839218402, 2023. 2, 8, 1 [2] Tom Brown. Language models are few-shot learners. Advances in neural information processing systems, 2020. 8 [3] Weifeng Chen, Tao Gu, Yuhao Xu, and Chengcai Chen. Magic clothing: Controllable garment-driven image synthesis. arXiv preprint arXiv:2404.09512, 2024. 3, 5 [4] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zero-shot object-level imIn Proceedings of the IEEE/CVF Conage customization. ference on Computer Vision and Pattern Recognition, pages 65936602, 2023. 2 [5] Seunghwan Choi, Sunghyun Park, Minsoo Lee, and Jaegul try-on via Viton-hd: High-resolution virtual Choo. In Proceedings of the misalignment-aware normalization. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021. 2, 5 [6] Yisol Choi, Sangkyung Kwak, Kyungmin Lee, Hyungwon Choi, and Jinwoo Shin. Improving diffusion models for authentic virtual try-on in the wild. In European Conference on Computer Vision, 2024. 2 [7] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 2, [8] Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola. Dreamsim: Learning new dimensions of human visual similarity using synthetic data. Advances in Neural Information Processing Systems, 36, 2024. 4 [9] Rinon Gal, Or Lichter, Elad Richardson, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. Lcmlookahead for encoder-based text-to-image personalization. arXiv preprint arXiv:2404.03620, 2024. 8 [10] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. In International Conference on Learning Representations, 2023. 2, 8 [11] Amir Hertz, Andrey Voynov, Shlomi Fruchter, and Daniel Cohen-Or. Style aligned image generation via shared attention. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 47754785, 2024. 2 [12] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 5 [14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 3, 5, [15] Qihan Huang, Siming Fu, Jinlong Liu, Hao Jiang, Yipeng Yu, and Jie Song. Resolving multi-condition confusion arXiv for finetuning-free personalized image generation. preprint arXiv:2409.17920, 2024. 2, 5, 6 [16] Zehuan Huang, Hongxing Fan, Lipeng Wang, and Lu Sheng. From parts to whole: unified reference framework for controllable human image generation, 2024. 2, 5, 6, 8 [17] Aapo Hyvarinen and Peter Dayan. Estimation of nonnormalized statistical models by score matching. Journal of Machine Learning Research, 6(4), 2005. 3 [18] Sangwon Jang, Jaehyeong Jo, Kimin Lee, and Sung Ju Identity decoupling for multi-subject perarXiv preprint text-to-image models. Hwang. sonalization of arXiv:2404.04243, 2024. 8 [19] Jeongho Kim, Guojung Gu, Minho Park, Sunghyun Park, and Jaegul Choo. Stableviton: Learning semantic correspondence with latent diffusion model for virtual try-on. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81768185, 2024. 2 [20] Diederik Kingma and Jimmy Ba. Adam: method for arXiv preprint arXiv:1412.6980, stochastic optimization. 2014. 1 [21] Hyung-Kwon Ko, Gwanmo Park, Hyeon Jeon, Jaemin Jo, Juho Kim, and Jinwook Seo. Large-scale text-to-image generation models for visual artists creative works. In Proceedings of the 28th international conference on intelligent user interfaces, 2023. 2 [22] Kyungmin Lee, Sangkyung Kwak, Kihyuk Sohn, and Jinwoo Shin. Direct consistency optimization for robust customization of text-to-image diffusion models. Advances in neural information processing systems, 2024. [23] Simon Lepage, Jeremie Mary, and David Picard. Lrvsfashion: Extending visual search with referring instructions. arXiv:2306.02928, 2023. 2, 5 [24] Dongxu Li, Junnan Li, and Steven CH Hoi. Blip-diffusion: Pre-trained subject representation for controllable text-toimage generation and editing. In Advances in Neural Information Processing Systems, 2023. 8 [25] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2251122521, 2023. 8 [26] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. 3 [27] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in Neural Information Processing Systems, 36, 2023. 5 [13] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 5, 1 [28] Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang. Deepfashion: Powering robust clothes recognition and retrieval with rich annotations. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2016. 5 [29] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. 2 [30] Davide Morelli, Matteo Fincato, Marcella Cornia, Federico Landi, Fabio Cesari, and Rita Cucchiara. Dress code: Highresolution multi-category virtual try-on. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 22312235, 2022. 2, 5, 6 [31] Davide Morelli, Alberto Baldrati, Giuseppe Cartella, Marcella Cornia, Marco Bertini, and Rita Cucchiara. LaDIVTON: Latent Diffusion Textual-Inversion Enhanced Virtual Try-On. In Proceedings of the ACM International Conference on Multimedia, 2023. 2 [32] Weili Nie, Sifei Liu, Morteza Mardani, Chao Liu, Benjamin Eckart, and Arash Vahdat. Compositional text-to-image genIn International eration with dense blob representations. Conference on Machine Learning, 2024. 2 [33] Weili Nie, Sifei Liu, Morteza Mardani, Chao Liu, Benjamin Eckart, and Arash Vahdat. Compositional text-to-image generation with dense blob representations. arXiv preprint arXiv:2405.08246, 2024. [34] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2024. 2, 5 [35] Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng, Wenhu Chen, and Furu Wei. Kosmos-G: Generating images in context with multimodal large language models. In International Conference on Learning Representations, 2024. 2, 8 [36] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, Sdxl: Improving latent diffusion and Robin Rombach. International models for high-resolution image synthesis. Conference on Learning Representations, 2024. 2, 3, 5 [37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 87488763. PMLR, 2021. 2, 3, 1 [38] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. 3 [39] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2, [40] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. 2, 8, 3 [41] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 3 [42] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In International confernonequilibrium thermodynamics. ence on machine learning, pages 22562265. PMLR, 2015. 3 [43] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 3 [44] Wang, Siming Fu, Qihan Huang, Wanggui He, and Hao Jiang. Ms-diffusion: Multi-subject zero-shot image personalization with layout guidance. arXiv preprint arXiv:2406.07209, 2024. 2, 5, 8 [45] Zhizhong Wang, Lei Zhao, and Wei Xing. Stylediffusion: Controllable disentangled style transfer via diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 76777689, 2023. 2 [46] Daniel Winter, Matan Cohen, Shlomi Fruchter, Yael Pritch, Alex Rav-Acha, and Yedid Hoshen. Objectdrop: Bootstrapping counterfactuals for photorealistic object removal and insertion. arXiv preprint arXiv:2403.18818, 2024. [47] Guangxuan Xiao, Tianwei Yin, William T. Freeman, Fredo Durand, and Song Han. Fastcomposer: Tuning-free multisubject image generation with localized attention. International Journal of Computer Vision, 2024. 8 [48] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340, 2024. 3 [49] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. In Advances in neural information processing systems, 2021. 4 [50] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arxiv:2308.06721, 2023. 2, 7, 8 [51] Xianggang Yu, Mutian Xu, Yidan Zhang, Haolin Liu, Chongjie Ye, Yushuang Wu, Zizheng Yan, Chenming Zhu, Zhangyang Xiong, Tianyou Liang, et al. Mvimgnet: large-scale dataset of multi-view images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 91509161, 2023. 3 [52] Yu Zeng, Vishal Patel, Haochen Wang, Xun Huang, TingChun Wang, Ming-Yu Liu, and Yogesh Balaji. Jedi: Jointimage diffusion models for finetuning-free personalized textto-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 67866795, 2024. 8 [53] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023. Controllable Human Image Generation with Personalized Multi-Garments"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Implementation Details A.1. Training and Inference We train our decomposition module on 68,296 pairs of human image and single reference garment image at 512384 resolutions with fixed learning rate of 1e-5 using Adam optimizer [20]. We train for 140K iterations with total batch size of 32 using 4 H100 GPUs. For the composition module, we train on 54K pairs of human image and multiple reference garment images at 768576 resolution with fixed learning rate of 1e-5 and Adam optimizer. We train for 115K iterations with total batch size of 48 using 8 H100 GPUs. During the inference, we generate images using the DDPM [14] sampler with 50 denoising steps. We apply classifier-free guidance (CFG) [13] with the text conditioning and garment image conditioning as follows: ˆϵθ(xt; g, c, t) = (ϵθ(xt; g, c, t) ϵθ(xt; t)) + ϵθ(xt; t), where ϵθ(xt; c, g, t) denotes noise prediction output with text and garment image conditions, and ϵθ(xt; t) denotes the unconditional noise prediction output. We use guidance scale of = 2.0 for sampling. A.2. Single reference Paired Dataset To train the decomposition network, we collect pairs of human image and single reference garment image from VITON-HD, DressCode, and LAION-Fashion datasets. Specifically, we gather 11,647 upper garments and human images from the training dataset on VITON-HD. We also collect 13,563 upper garments, 7,151 lower garments, 27,677 dresses paired with human images from DressCode. For LAION-Fashion dataset, since it consists of single reference pairs without categorical information, we use CLIP [37] model to classify the garment image. We define 19 different garment category texts and match the garment image with the category text of the highest similarity score, resulting in 5,675 bags and 1,599 shoes, 826 scarf, and 159 hats in the training data. We provide examples of collected single reference garment and human image pairs in Fig. 10. A.3. Dual-Condition Classifier-free Guidance Since we have dual conditions of text condition and garment image condition g, one can apply classifier-free guidance for two conditions following [1]. Formally: ˆϵθ(xt; g, c, t) = wc (ϵθ(xt; g, c, t) ϵθ(xt; g, t)) + wg (ϵθ(xt; g, t) ϵθ(xt; t)) + ϵθ(xt; t), Figure 10. Examples of training data for decomposition module. We collect pairs of human image and single reference garment image from public datasets including VITON-HD, DressCode, and LAION-Fashion. It consists of various garments in different categories, e.g., shirts, pants, shoes and bags etc. where wc > 0 and wg > 0 denotes guidance scale for text conditioning and garment image conditioning, respectively. Increasing wg encourages generated images to more similar to the reference garment images, and increasing wc guides the generated images to better align with the given text prompt. While we adopt wg = 2.0 and wc = 2.0 for all experiments, users can adjust the guidance values to customize the generated images according to their preferences. B. Synthetic Dataset Construction In this section, we provide detailed explanation of the data curation process with visualizations. B.1. Filtering Strategy As illustrated in 3.1, we apply filtering on our synthetic paired data based on the image similarity between the segmented and generated garments. Among several possible metrics, we try LPIPS, CLIP score, and DreamSim, and empirically find that DreamSim aligns the most with human perception. As shown in Fig. 11, DreamSim can measure the similarity aligned with human perception and filters out undesirable samples while CLIP and LPIPS struggle. For example, LPIPS determines that similar garments Figure 11. Examples of pairs filtered out by different similarity metrics. We present examples of generated garment images and their corresponding human images that were excluded based on various image similarity metrics. Using LPIPS, garments with complicated patterns are filtered out, and using CLIP score, inner layer garments are filtered out even when they are considered identical in human perception. In contrast, DreamSim captures the distance between images in way aligned with human perception, filtering out undesirable pairs. do not resemble each other, even if garment pairs look identical to humans, especially when they contain intricate patterns or stripes. Also, CLIP fails to identify the same garments, mainly when garments are inner layers under jackets, whereas DresmSim captures similarity in way aligned with human perception, filtering out the undesirable pairs. We adopt DreamSim for measuring the distance between segmented garments and generated garments. We visualize human images and generated garment images based on the image distance value in Fig. 12. With the distance value 0.6, we observe that the generated garment is inconsistent with the garment on the human image, and with 0.4 < 0.6, fine details are not fully preserved. On the other hand, with < 0.4, generated garments closely resemble the actual garments. B.2. Synthetic Dataset Examples We provide visualizations of the synthetic paired dataset generated by our decomposition network in Fig. 13. The synthetic dataset contains high-quality pairs of human image and multiple reference garments. The decomposition network can generate product garment images on different categories, even with challenging garments such as oneshoulder sweaters (Third-row in Fig. 13). Figure 12. Examples of generated garment images with different image distance values. We provide examples of generated garment images and corresponding human images, varying the distance values measured by DreamSim. With the distance value 0.4, generated garments are inconsistent with the actual garment, while for < 0.4, the generated garments closely resemble the actual garment. Figure 13. Examples of our synthetic paired data. We visualize our synthetic pairs of human image and multiple garment images. Our decomposition module generates high-quality garment images in product view on different categories including shirts, pants, shoes and bags. Figure 14. Examples of synthetic paired data generated by the decomposition module trained on MVImgNet [51]. We show the potential extension of our decomposition module to the general domain. Given an image containing common objects such as cups, chairs, and broccoli, the decomposition module generates each object in different view, constructing paired data. Reference images are obtained from COCO [26]. C. Applications of Decomposition module In this section, we explore the potential applications of our decomposition module, including applying it on the general domain and using it as multi-view image generator. C.1. Synthetic Paired Data on General Domain Recent work [48] demonstrates remarkable performance in diverse image generation tasks by leveraging large-scale paired data, underscoring the importance of paired datasets in image generation. We have demonstrated our decomposition modules capability to generate high-quality paired data in the fashion domain, and we further explore its potential for applicability to the general domain. Specifically, we train the decomposition network on MVImgNet [51] dataset, which contains large-scale object images in multiview from 238 classes. As shown in Fig. 14, the network decomposes each object in different views from reference images, demonstrating its potential for broader applications and inspiring future research. C.2. Multi-view Image Generator We show that the decomposition network can be used as multi-view image generator. By utilizing the decomposition network with segmented single-subject images, one can generate different views of the reference subject images while faithfully preserving their identity. In Fig. 15, we present multi-view images generated by the decomposition module using subject images obtained from DreamBooth [40]. These multi-view images can be utilized for various applications, such as data augmentation. Figure 15. Examples of generated subjects in multi-view by the decomposition module trained on MVImgNet. The decomposition module can serve as multi-view generator for single-subject images. Subject images are from DreamBooth [40]. D. Additional Qualitative Results We provide more visualizations of human images generated by BootComp. We show more qualitative comparisons of BootComp with baselines in Fig. 17. We also showcase additional human images with multiple reference garments generated by BootComp in Fig. 18 and more visualizations of application results, including controllable generation, stylization, and personalized generation in Fig. 19. E. Limitations While BootComp is capable of generating human images with various categories of garments, it sometimes struggles to place hats on humans naturally. This arises from the limited number of hat images in the training data. One can address this by scaling up the paired data simply using our data generation pipeline. Also, BootComp fails to preserve tiny details such as letters, which is attributed to the limitations of the backbone model, SDXL. This can be relieved by replacing backbone to other diffusion models trained with better VAE encoders with larger number of channels. Figure 16. Limitations of BootComp. BootComp struggles on naturally dressing hats and preserving tiny details like letters. Figure 17. More qualitative comparisons. BootComp generates realistic human images wearing multiple reference garments, faithfully preserving fine-details of each garment, while baselines often generate inconsistent garment images and blend reference garments. Figure 18. Generated human images by BootComp. BootComp can realistically dress humans with diverse categories of garments, including bags and shoes, which are not available for previous approaches. BootComp is capable of dressing complex combinations such as jackets and inner layers (First row, second column) and less common garments such as overalls (Second row, third column). Also, BootComp can address challenging garments such as asymmetric-length garments and sandals (Third row, second column), and garments with unique details (Last row, third column). Figure 19. Application results by BootComp. BootComp is capable of generating human images with various conditions. By using structural conditions, it can control poses in the generated images. With text prompts, BootComp can manipulate the backgrounds of images. Additionally, it supports personalized generation through virtual try-on and face-based generations."
        }
    ],
    "affiliations": [
        "KAIST",
        "OMNIOUS.AI"
    ]
}