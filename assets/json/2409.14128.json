{
    "paper_title": "Present and Future Generalization of Synthetic Image Detectors",
    "authors": [
        "Pablo Bernabeu-Perez",
        "Enrique Lopez-Cuena",
        "Dario Garcia-Gasulla"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The continued release of new and better image generation models increases the demand for synthetic image detectors. In such a dynamic field, detectors need to be able to generalize widely and be robust to uncontrolled alterations. The present work is motivated by this setting, when looking at the role of time, image transformations and data sources, for detector generalization. In these experiments, none of the evaluated detectors is found universal, but results indicate an ensemble could be. Experiments on data collected in the wild show this task to be more challenging than the one defined by large-scale datasets, pointing to a gap between experimentation and actual practice. Finally, we observe a race equilibrium effect, where better generators lead to better detectors, and vice versa. We hypothesize this pushes the field towards a perpetually close race between generators and detectors."
        },
        {
            "title": "Start",
            "content": "Present and Future Generalization of Synthetic Image Detectors Enrique Lopez-Cuena Barcelona Supercomputing Center (BSC) Barcelona, Spain enrique.lopez@bsc.es Pablo Bernabeu-Perez Barcelona Supercomputing Center (BSC) Barcelona, Spain pablo.bernabeu@bsc.es Dario Garcia-Gasulla Barcelona Supercomputing Center (BSC) Barcelona, Spain dario.garcia@bsc.es 4 2 0 2 1 2 ] . [ 1 8 2 1 4 1 . 9 0 4 2 : r Abstract The continued release of new and better image generation models increases the demand for synthetic image detectors. In such dynamic field, detectors need to be able to generalize widely and be robust to uncontrolled alterations. The present work is motivated by this setting, when looking at the role of time, image transformations and data sources, for detector generalization. In these experiments, none of the evaluated detectors is found universal, but results indicate an ensemble could be. Experiments on data collected in the wild show this task to be more challenging than the one defined by large-scale datasets, pointing to gap between experimentation and actual practice. Finally, we observe race equilibrium effect, where better generators lead to better detectors, and vice versa. We hypothesize this pushes the field towards perpetually close race between generators and detectors."
        },
        {
            "title": "1 Introduction\nThe rapid advancement of generative AI has revolutionized image\ngeneration, presenting challenges related to visual information\nintegrity, trust and rights in digital environments, and mitigation\nof misinformation, among others. As a result, recent legislation\nmandates the identification and notification of the synthetic nature\nof such digital content [38]. By virtue of all these needs, correctly\nattributing synthetic content has become a social demand, and a\ntop scientific priority (i.e., is this image authentic or synthetic?).",
            "content": "The field of synthetic image detection is in continuous race with the field of synthetic image generation. The detection field tries to prevent the race by developing universal detectors [8, 32], but the generalization capacity of these solutions is not entirely known. Meanwhile, new generative models join the race every month (if not every day), begging for the question, where is this going? This work uncovers where current Synthetic Image Detection (SID) is, and where it seems to be headed to. To do so, it first analyzes the impact of several training conditions on model generalization, producing recipe for building robust detectors. Those guidelines are then used to train baseline for the study of generalization across source variations (who created the data), model variation (which model was used) and model age (when was the model released). The same experimental setup is used to conduct an updated benchmark of state-of-the-art detector models, using synthetic data produced by the latest generators. Findings indicate current detectors are insufficient for synthetic content detection, especially when used alone. Suggestions include the use of detector ensembles, as well as the training of generatorspecific detectors. This last option appears as the most reliable solution and is shown to generalize to different data sources. Finally, the ethical considerations of this line of work are discussed, including when and how should detectors be publicly released, considering how these may be used to train generators which are harder to identify. To complete the work contributions we release two novel datasets, the software library used for experimentation and the weights of the best detector trained."
        },
        {
            "title": "2 Related Work\nAny work done on training and analyzing synthetic image detec-\ntors is defined by the models it is trained and evaluated on. This\nlimits the consistency and relevance of results in a fast-moving\nfield like the one of generative AI, where models are continuously\nand quickly improved and replaced. The main body of previous\nwork on synthetic image detectors was conducted on Generative\nAdversarial Network (GAN) generators (now only used because of\ntheir relative speed), as well as early versions of diffusion-based\nmodels [9, 17, 27, 32, 42, 43, 47, 48]. A few recent works target data\nproduced by some of the latest diffusion models, which are the ones\ncurrently compromising the human ability to discriminate between\nreal and fake [7, 8, 10, 33].",
            "content": "Generalization is key in the SID field, as failing to successfully detect samples produced by alternative generative models limits the applicability of any proposed solution. To study generalization, common practice in the field is to train models using data produced by one model, while evaluating performance on data produced by other models [79, 17, 32, 33, 47, 48]. However, none of these studies considers the temporal natural of generative models nor what this may tell us about the detection performance on models to be released in the future. Remarkably, this is the case that matters the most for the practical use of synthetic image detectors. While the field is missing an exhaustive and updated study on the generalization of SID, few sources of bias have been considered before. Particularly, the effect of image format and resolution has been studied in the past. In [9], authors highlight the impact of the resizing operation, which is common in deep learning architectures, and used to adapt any image to the desired network input resolution. This transformation could remove the high-frequency artifacts created by the generative models, hindering the detection process. GenImage [48] studies the image content generalization, that is, how detector performs on domains outside the training data, such as art and face images. They show great generalization capability for these two cases, with 95% and 99.9% accuracy scores respectively. Yet, the generalization of image content has not been thoroughly explored. This is particularly important given that SID models, which often rely on common training datasets like LSUN [45] and ProGAN [22], suffer from limited class representation. The study presented in [17] highlights biases associated with JPEG compression and image size, demonstrating that these biases are prevalent in many training datasets. Notably, they reveal significant disparity in image formats between authentic and synthetic Bernabeu-Perez, Lopez-Cuena and Garcia-Gasulla several reasons. Firstly, not all parts of an image may be artificial, as they can be partially edited or altered through methods like inpainting. Secondly, its nature may be better identified in specific portions of the image as some regions might contain more telltale signs of manipulation than others. Finally, processing entire high-resolution images can be computationally intensive and time-consuming. To address these challenges, most detectors are trained on image patches. This introduces new variables in the detection process: the number of patches and their selection strategy, as well as the criterion to convert single-patch predictions to whole image ones. To determine our patch selection strategy, we conduct early experiments and find that patches which exhibit the highest contrast in their grey-level co-occurrence matrix (GLCM) produce slightly more performant models. Due to the size of our input images, we consider maximum of five 224 224 patches. The predictions of these individual patches can then be aggregated according to the use case requirements (e.g., maximizing the detection of false positives, or prioritizing false negatives instead). In particular, we select voting mechanism and set minimum amount of patches for an image to be considered synthetic. We will study the effect of the value of this threshold in 5.2."
        },
        {
            "title": "3.2 Train Datasets\nTo train the synthetic content detectors we consider two types of\ndatasets. The first contains real-world images, that is photographs\nof different scenes taken under different light conditions. We use\nCOCO [26] for training and validation (ยง4). The second type of\ndatasets considered are those composed of synthetic images. That is,\nAI-generated images. Five datasets are used, created using five dif-\nferent AI image generators: dalle3-images, diffusiondb, SDXL,\nmj-tti and mj-images.",
            "content": "The dalle3-images [14] dataset contains 1,647 unique, deduplicated images generated by DALLE3 (2023), encompassing both photorealistic and digital art styles. Another dataset, the diffusiondb [40], was created using models of the 1.x Stable Diffusion series, which were released in 2022. In this dataset, we filter samples making sure that \"photo\" appears in the prompt. Images from this dataset are of lower quality and visual detail than those of its successor SDXL [3], which was released in 2023. The associated dataset, SDXL [13] contains 5,435 images in the \"realistic\" subset. Beyond DALL-E and Stable Diffusion, the third main provider of synthetic images is Midjourney. Its early iterations, the V1 and V2 models, date from early 2022, and were used to populate the mj-tti [37] dataset, which contains 4,530 images. Collage images and mosaics made of synthetic images were removed from this dataset. Later models, the V5 and V6 models from 2023 compose our last training dataset, mj-images [15], with 1,226 images. This dataset also had to be deduplicated. All these synthetic models will be used for training and validation in 4. In 4, during the training stages of this work, COCO and diffusiondb datasets are undersampled to include maximum of 5,435 samples. Existing train, validation and test partitions are respected. When undefined 60%-20%-20% random split is performed. For the SDXL dataset we include the realistic-2.2 split for training and validation, and the realistic-1 split for test. Table 1 shows the exact composition per data source. Figure 1: Detector architecture used, based on ResNet-18 from [30]. Includes ResNet blocks (blue), bottlenecks (red), adaptative average pooling 2D (orange), concatenation (yellow) and an MLP (green). images, with authentic images typically stored in JPEG format and synthetic images in PNG. Furthermore, the authors demonstrate size bias affecting detector performance, with detectors generally performing better on natural images that differ significantly in size from the generated images used in training. This observation aligns with findings in [10], which demonstrate that dataset choice significantly impacts detection performance. The authors found that using the LSUN dataset for authentic samples, instead of COCO, led to considerable degradation in results. This was attributed to LSUNs lack of diversity and inherent biases. Meanwhile, relevant factors remain to be studied. This includes model family, model release date, and dataset source, among others. These are of special relevance for understanding generalization, and the practical future of the field."
        },
        {
            "title": "3.1 Architecture\nThere are two popular architectural choices when building a syn-\nthetic image detector: training a direct classifier or using the fea-\ntures extracted from a pre-trained or fine-tuned model to discrim-\ninate samples. Both CNNs [17, 33, 48] and transformers (ViT en-\ncoder) [10, 32, 43, 47] have been considered to that end, both of\nthem performing competitively.",
            "content": "In this work we fix the architecture, to focus on the evolution, nature and practical use of generalization models. For our experimentation, we choose ResNet[19], as this has shown competitive results [7, 17, 48], and its relatively lightweight architecture. In particular, we follow the staircase design proposed in [30] with ResNet-18 backbone (12.7M parameters), which feeds features extracted at different depths into an MLP (see Figure 1). SID often relies on analyzing specific portions of an image rather than the entire composition. This approach is advantageous for Present and Future Generalization of Synthetic Image Detectors"
        },
        {
            "title": "3.3 Evaluation Datasets\nThe datasets in this section will assess the detectorโs generaliza-\ntion ability when trained on limited data and applied to diverse\nexternal sources (ยง 5.1). The evaluation aims to cover three scenar-\nios: datasets from the same model used in training but generated\nby different users or slightly different software or model versions;\ndatasets from entirely different models; and samples from various\nunknown sources and models, simulating real-world applications.\nExamples of these datasets are provided in Appendix B, with the\nresolution distribution of each dataset detailed in Appendix 6.",
            "content": "For samples representing the class of authentic images, three datasets are added. The Flickr30k [44] dataset, which includes 31,655 samples primarily depicting scenes with humans. subset of the Google Landmarks v2 [41] test partition, containing 5,000 images of both natural and human-made landmarks, and finally, the authors have curated small-scale dataset referred to as In-the-wild. The first subset of this dataset, containing authentic images from platforms such as Reddit (from communities that forbid AI content) and Flickr (uploaded before 2020), is included as an authentic evaluation dataset. Synthetic data for evaluation is obtained either from the Synthbuster dataset [5] or is collected/generated by the authors. Synthbuster contains images generated with nine different models (1, 000 images each), and provides wide coverage for our experimentation by including data from models with our train set (e.g., SD1.X, DALLE3) and others outside of it (e.g., DALLE2, Firefly). The images are based on the RAISE-1k dataset [11], the corresponding prompts were manually refined to enhance photorealism and remove references to living persons or specific artists. Three datasets are crafted for this paper. The first contains 8,192 images generated with Stable Diffusion 3 Medium (SD3), Multimodal Diffusion Transformer (MMDiT) text-to-image model. Input prompts are extracted from Gustavosta/Stable-Diffusion-Prompts[18]. For each image, height and width were randomly selected from uniform distribution over the set {512, 768, 1024, 1344} pixels. The Train Dataset Model COCO - dalle3-images DALLE-3 SD 1.X diffusiondb SDXL SDXL MJ V1/2 mj-tti MJ V5/6 mj-images JPG Aut 2,967/1,234/1,234 JPG Syn Year Form A/S Tra/Val/Test 2017 2023 2022 PNG Syn 2,967/1,234/1,234 2023 PNG Syn 2,967/1,234/1,234 2022 PNG Syn JPG Syn 2023 2,718/906/906 1,845/617/617 987/330/330 Test Dataset Flickr30k GLDv2 In-the-wild Synthbuster SD3 FLUX.1 In-the-wild Model - - - Many SD 3 Year Form A/S 2014 JPEG Aut 2020 JPEG Aut 2024 Mix Aut 2024 PNG Syn 2024 PNG Syn FLUX.1-dev 2024 PNG Syn 2024 PNG Syn ? Test Size 31,655 5,000 121 9,000 8,192 8,192 99 Table 1: Datasets, including generative models included, release date, image format, authentic or synthetic, and number of samples within train, validation and test. images were generated using the official model accessed through Hugging Face [4]. We employed consistent generation process across all images, utilizing 28 inference steps for each generation. To enhance the quality and realism of the generated images, we added set of negative prompts: poorly rendered face, poor facial details, poorly rendered hands, low resolution, blurry image, oversaturated, extra fingers, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck, extra foot The same strategy is used to generate the second dataset, with the recent FLUX.1-dev [24] model, 12B parameter rectified flow transformer model, which leverages mixture of MMDiT and DiT [25]. We used torch.bfloat16 precision, 50 inference steps, guidance scale of 3.5, and maximum sequence length of 512. Finally, the synthetic subset of the In-the-wild is included, composed of manually selected images that exhibit the highest photorealism according to the authors criteria. Samples were extracted from Civitai [1], platform intended for sharing generative models and synthetic images, and from Reddit communities dedicated to the sharing of synthetic content. This dataset is designed to challenge the detectors ability to generalize across diverse content, simulating real-world scenarios where the origins of synthetic images are often unknown."
        },
        {
            "title": "3.4 Computation\nThe software stack used to develop and evaluate SID models was\nbased on PyTorch. To enable the full reproducibility of our work,\nwe share the code needed to run the experiments 1, the datasets\nas compiled and cleaned 2, and the model weights 3 for our best\ndetector (see ยง4.3.1).",
            "content": "For the benchmarking of other detectors ( 6), we use the sidbench library [34], which supports several models while facilitating inference on custom datasets. Available checkpoints were downloaded from the AIGCDetectBenchmark [46] repository. We download the missing models from their corresponding official sources. We utilize an Intel Xeon Platinum 8460Y processor and one NVIDIA Hopper H100 64GB GPU. Seventy-five training runs were conducted with this setup, totalling sixteen hours of computing time, while continuously monitoring GPU power usage. Using the European Unions latest CO2 emission ratio [2], we estimated the carbon footprint of these experiments to be 0.63 kg of CO2."
        },
        {
            "title": "4.1 Single Class Models\nTo study the relation between the images produced using the latest\nimage generation models, let us consider the generalization capac-\nity of single-class discriminative models trained on those images. In\nthis section we train binary classifiers, using each synthetic dataset\nof Table 1 as a positive class, and the COCO dataset as a negative class.\nThose models are then tested on the remaining datasets, to see how",
            "content": "1https://github.com/HPAI-BSC/SuSy 2https://huggingface.co/datasets/HPAI-BSC/SuSy-Dataset 3https://huggingface.co/HPAI-BSC/SuSy they generalize. Single-class models are trained for maximum of 20 epochs with an early-stopping mechanism monitoring validation accuracy with patience of 2. Data augmentation is limited to horizontal flips with 50% probability, with further transformation studies left for 4.3. DALLE3 97.70 49.76 51.27 31.39 91.76 64.38 SD1.X 27.59 98.30 33.45 17.63 26.13 40.62 SDXL 70.19 68.23 97.57 73.14 64.75 74.78 MJ 1/2 50.73 39.65 59.14 99.07 62.69 62. MJ 5/6 Avg. 68.64 97.02 59.26 40.36 61.88 67.97 54.55 51.51 68.92 99.25 71.22 DALLE3 SD1.X SDXL MJ 1/2 MJ 5/6 Avg. Table 2: On each row, patch-level recall of single-class discriminative model when evaluating all datasets. In bold, performance on the dataset used for training. The results from the single-class model evaluations, shown in Table 2, reveal discrepancy between performance in academic settings and practical applicability. While all models achieved high recall (over 97%) on their respective binary tasks, their performance degraded significantly when applied to other datasets. Additionally, results indicate that the family of generative models has weak effect on generalization capabilities. The discriminator trained on SDXL is below average when tested on SD1.X. Likewise, the model trained on MJ 1/2 discriminator is not particularly accurate on MJ 5/6. Although the MJ 5/6 discriminator performs well on MJ 1/2, it still generalizes better to other classes. The effect of image format is also inconclusive, with the model trained on MJ 1/2 generalizing poorly to SD1.X, both containing PNG images. On the other hand, the time of release of the generative model correlates well with generalized performance. The models trained on DALLE3 and MJ 5/6 strongly generalize to each other. While this may be influenced by shared dataset authorship, both models also generalize well to SDXL, demonstrating accurate performance on recent models regardless of their provenance. In fact, the three discriminators trained on models from 2023 are the top three models on average. That is, newer models are good at detecting new and old synthetic images. This could be caused by the quality and the lack of artifacts in the images generated by the latest models. In which case it would hold that images generated by older models are harder to identify than images from newer models. The bottom row of Table 2, shows this is the case, with detection accuracy on older datasets performing worse of all (SD1.X and MJ 1/2, from 2022)."
        },
        {
            "title": "4.2 Multi-class Models\nThe performance of binary classifiers can be misleading [32], learn-\ning only one class and predicting the alternative only by omission.\nTo produce more robust detection models, better at generalizing,\nwe must include datasets from different sources in their training\nset, enriching and detailing the decision boundary available for\nprediction.",
            "content": "We train two models to evaluate the effect of multi-class learning on generalization capacity. First, we train binary classifier merging all synthetic data sources from Table 1 into single synthetic class, Bernabeu-Perez, Lopez-Cuena and Garcia-Gasulla including 14,323 images. An analogous amount of samples is drawn from the COCO dataset to compose the authentic class. Then, we train six-class recognition model using the original splits defined in Table 1. To enable direct comparison, in this experiment, prediction of multiclass model will be considered correct for synthetic image if any synthetic class is predicted, regardless of whether it matches the ground truth class. - Single 94.85 Binary Six-class 97.39 Auth. DALLE3 SD1.X 98.30 97.70 99.64 98.98 97.89 99.76 SDXL 97.57 99.22 99. MJ 1/2 MJ 5/6 99.25 99.07 99.74 99.60 99.97 99.91 Table 3: On each row, patch level recall for different models. On columns, recall for the authentic class, and each synthetic class in binary setting. Single: Trained on the dataset of each column (see Table 2 diagonal). Binary: Trained with all synthetic datasets merged into one. Six-class: Multi-class recognition model, trained to discriminate all six datasets. In bold best performance per dataset. Results in Table 3 show that, when trained on multiple data sources, both the classifier and the recognition perform equally well, outperforming the models trained on single dataset (top row). This indicates that certain amount of useful features are shared among generators, and these are better characterized in conjunction. The results of the six-class model can be attributed to the training in the harder multi-class classification task, which enriches the models understanding of the feature space, enhancing its ability to distinguish between classes, even in binary context."
        },
        {
            "title": "4.3 Image Alteration Methods\nImages undergo various transformations when uploaded to social\nmedia and online hosting services. These modifications, primarily\naimed at reducing file sizes to optimize storage and transmission\ncosts, can significantly alter the original image characteristics. In\naddition, malicious actors may intentionally edit or manipulate im-\nages to obscure artifacts and patterns that synthetic image detectors\nrely on for accurate predictions. If image analysis models are not\nrobust enough to withstand these transformations, their utility in\nreal-world scenarios becomes severely limited.",
            "content": "To address this challenge, we conducted series of experiments focused on understanding and enhancing model robustness. Our approach involves training six-class recognition models, which performed best in the previous experiments, using different data augmentation modalities in isolation, including various forms of blur (AdvancedBlur and GaussianBlur), brightness and gamma alterations (RandomBrightnessContrast and RandomGamma) and compression of images through the JPEG algorithm, from the Albumentations library [6]. These augmentation techniques were chosen to simulate the range of transformations that images might undergo in real-world conditions. In addition to the baseline with no alterations, five additional models are trained (one for each alteration), and five new test sets are created (also one per alteration). With these, cross-testing is conducted to explore generalization across alteration methods. The Present and Future Generalization of Synthetic Image Detectors None Bright ๐พ None Bright 86.66 90.90 91.28 89.68 87.51 91.52 83.15 JPEG 87.82 86.23 90.13 ABlur 84.02 88.94 GBlur 86.21 90.10 Avg. ๐พ JPEG ABlur GBlur Avg. 82.44 90.19 90.60 85.06 91.13 90.10 85.19 91.30 90.02 79.78 87.79 86.21 87.37 88.15 90.12 87.37 88.65 86.27 88.67 89. 81.56 84.61 85.57 78.42 88.04 86.78 84.16 54.73 63.55 65.22 55.29 81.54 81.88 67.04 Table 4: On each row, patch-level accuracy of six-class recognition model when trained on one alteration method and evaluated on all. In bold, performance on the alteration used for training. Last column shows the average performance models across all transformations. Bottom row shows the average performance of all models for each transformation. results, how well model trained with one alteration detects images subjected to another alteration, are shown in Table 4. Multi-class macro average recall is reported, where confusion between synthetic classes is considered an error, unlike in the previous section where we used binary metrics for comparison. The effectiveness of targeted data augmentation can be seen by comparing the first row of Table 4 with the values in bold. Specialized models demonstrate significant performance boost over the baseline across most alterations, with the exception of JPEG compression. This anomaly may be attributed to the inclusion of JPEG data in the baseline training set. Despite this apparent performance drop, training with compressed images remains relevant to prevent the recognition of JPEG-introduced patterns as class-specific artifacts, as both authentic and synthetic images may undergo this transformation in real-world scenarios. Among the various transformations, blur particularly GaussianBlur has the most pronounced impact on detector performance. Models not specifically trained on blur transformations experience substantial drop in recall, up to 30%, when faced with blurred images. In contrast, models trained on blur transformations exhibit remarkable consistency and competitive performance across all experiments. Based on the average performance metrics presented in the last column of Table 4, these blur-trained models emerge as the overall best performers among the alteration-based models. 4.3.1 SuSy - Our robust model. In the following section ( 5), generalization is explored by evaluating different data sources. To scale experimentation, we fix model based on the insights from this section, which we call SuSy. In detail, we train six-class classifier, using the original splits defined in Table 1 and incorporating the alteration methods explored in the previous section to enhance its resilience against image manipulations. Each transformation is applied with 20% probability, besides the horizontal flip which remains at 50%, allowing the model to see unaltered images and images that have suffered one or multiple transformations. We train this model for maximum of 20 epochs with early stopping monitoring validation loss with patience 2, as in previous experiments."
        },
        {
            "title": "5 Evaluation Experiments\nSo far generalization has been explored from the perspective of\ntime, task, and alterations. All of them, in a controlled setting, using\na limited amount of data sources. But this is unrealistic in practice.\nAI image generators have become a wildly popular technology, with\nlarge communities of users re-training and sharing models. Mean-\nwhile, new and updated models keep coming out (e.g., FLUX.1).",
            "content": "In this situation, the biggest challenge for synthetic image detectors is the adaptation to changing and uncontrolled generative environment. To test generalization in this context, we explore the role of data source, using the datasets described in 3.3. First, this section considers the performance of SuSy (described in 4), using the central patch of images. Then, the same experiments are conducted by making predictions at image level, using the patching strategy defined in 3.1. Additionally, to confirm the validity and difficulty of the In-thewild dataset, we conduct small human experiment. We ask 10 volunteers aged 22-30 who have social media accounts and are potential targets of AI-generated deception, to discriminate between both In-the-wild versions (authentic and synthetic). To ensure unbiased results, images were presented in random order and the evaluators were not informed about the distribution of the data. Participants took an average of 15 minutes to label them, with no time constraints imposed. Results are reported in Table 6 and used as baseline."
        },
        {
            "title": "5.1 Generalization to source\nLet us explore the performance of SuSy when evaluating it on the\ndatasets described in ยง3.3. Table 5 shows the results, sorted by cate-\ngory. For the authentic datasets, reported at the top part of the table,\nSuSy generalizes well to the Flickr30k dataset and decently to\nGoogle Landmarks v2, but suffers a large drop in performance for\nIn-the-wild images. This could be caused by changes in the scale\nof images, differences in the domain of information represented\nin the pictures and a higher amount of image postprocessing, and\ncould potentially be mitigated by using a richer authentic class that\ncombines images from different sources in the training dataset.",
            "content": "The second set of data sources considered include datasets produced using models that were also used during the training of SuSy, listed in Table 1. Even though the underlying generative models are the same, differences in the generation process (e.g., prompts, generator hyperparameters, etc. ) can induce significant biases. In this case, generalization holds significantly well, with all datasets reaching recall between 73% and 89%. In the third set of experiments, which involves datasets obtained from models unseen during training, performance varies significantly, with recalls ranging from as high as 96% to as low as 20%. The impact of model family on performance is also inconsistent, as SuSy exhibits decent to strong generalization for Stable Diffusion models, with SD2 achieving recall of 68.40% and SD3 reaching 93.23%. However, it only detects 20.70% of images from DALLE-2, despite being trained on DALLE-3. Regardless of its blind spots, SuSy performs significantly well in the most realistic setting, the one using In-the-wild images collected from unknown sources and manually selected based on their quality, aligning with the performance on the latest and most Model SuSy (Patch) Source Authentic Data Sources 90.53 None Flickr30k 64.54 GLDv2 None In-the-wild None 33.06 Synthetic Data from Models In Train 87.00 SD 1.3 Synthbuster 87.10 SD 1.4 Synthbuster Synthbuster MJ V5 73.10 79.50 SD XL Synthbuster Synthbuster DALLE-3 88.60 Synthetic Data From Models Not in Train Synthbuster GLIDE Synthbuster Synthbuster DALLE-2 Synthbuster Authors Authors In-the-wild Unknown 53.50 68.40 20.70 40.90 93.23 96.46 89.90 Firefly SD 3 FLUX.1-dev SD 2 SuSy (Image) 93.30 75.17 33.88 91.70 91.20 78.10 84.40 90. 53.70 70.10 19.10 52.00 94.79 96.85 92.93 Table 5: Recall of SuSy on authentic images (top), from unseen datasets. In the middle, recall on synthetic datasets generated by generative models represented in the train data, but produced by alternative sources. Bottom, synthetic image datasets generated by models not seen before. Performance at patch and image level using majority voting (threshold of three out of five). realistic models (SD3 and Flux). comparison of performance on this task with human evaluators is shown in Table 6. Results indicate synthetic image detectors trained on varied sets of data, like SuSy, are already at the level of expert humans, making them useful tools in practice (as long as false positive rates are contained). SuSy (Patch) SuSy (Image) Best Human Avg. Human In the wild (Auth.) 33.06 65.29 75.21 72.82 In the wild (Synth.) 89.90 72.73 83.84 69. Table 6: Recall of the In-the-wild dataset by SuSy and human evaluators. For SuSy, performance for central patch, and at image level when requiring five positive detections on the five patches with maximum contrast."
        },
        {
            "title": "5.2 Generalization to image\nWhile our initial experiments focused solely on the central patch of\neach sample, a prediction at image level is likely to be of interest in\nreal-world scenarios. In this section, we explore the transition from\npatch-level to image-level predictions, and how this affects gener-\nalization. To achieve this, we employ the patch selection method\ndescribed in ยง3.1, selecting five patches and using a fixed threshold\n(minimum number of synthetic patches) to classify an entire image\nas synthetic.",
            "content": "Bernabeu-Perez, Lopez-Cuena and Garcia-Gasulla By default, we set the threshold at three out of five patches, which equates to majority voting. This approach ensures balanced performance between authentic and synthetic classes. In the case of GLIDE, due to image size constraints, we adjust the requirement to two out of three patches for synthetic prediction. The results of this image-level analysis are presented in Table 5, showing consistent improvements across all datasets. We observe performance increases of up to 11% of recall, with an average improvement of 3.4% across datasets. Optimizing the threshold for specific domain or dataset can be done using small subset of data, adjusting the detectors sensitivity to the frequency of artifacts found in specific domain. For instance, in the case of the In-the-wild dataset, Table 5 shows high recall in the synthetic class (89.9% and 92.9%) but also large number of false positives in the authentic class (33.0% and 33.8%). To address this, the threshold can be increased (e.g., requiring all five patches to indicate synthetic prediction for the entire image to be classified as such). As shown in Table 6, this setup significantly improves the models balance between authentic and synthetic image classification, increasing authentic recall by 31% while decreasing synthetic recall by 20% points. Notably, this adjustment results in performance comparable to that of the average human evaluator."
        },
        {
            "title": "6 Generalization of state-of-the-art models\nIn this section, we study the performance of state-of-the-art models\non the evaluation datasets listed in ยง 3.3. We analyzed ten different\nmodels available in SIDBench [34], focusing on the six that demon-\nstrated superior performance across our tests. The results for the\nfour underperforming models (CNNDetect [39], FreqDetect [16],\nFusing [21] and UnivFD [32]) are presented in Appendix A for com-\npleteness. Table 7 showcases the performance of the best models,\nLGrad [36], GramNet [29], Rine [23], DIMD [23], DeFake [9] and\nDire [35].",
            "content": "The majority of the models examined leverage pre-trained neural networks as feature extractors, adapting them for SID. LGrad, GramNet, and DIMD all utilize CNNs, each with unique emphasis on different image characteristics. Rine and DeFAKE shift towards more recent architectures, employing transformer-based models. In contrast, Dire does not rely on direct feature extraction, instead using the diffusion concept of image reconstruction. LGrad trains ResNet-50 classifier using image gradients from pre-trained CNN, with images generated by ProGAN and authentic images from Celeba-HQ [20]. Similarly, GramNet employs global image texture representations extracted at different levels from ResNet-18, trained on StyleGAN-generated images and authentic Celeba-HQ images. Rine leverages image representations extracted by intermediate blocks of CLIP, with an additional trainable module. We use the checkpoint trained with Latent Diffusion Model [9] and ProGAN images. DIMD trains ResNet-50 avoiding downsampling step, to preserve high-frequency fingerprints. We take the checkpoint trained on Latent Diffusion images. Training authentic images are taken from MSCOCO and LSUN. In DeFAKE, text and image encoders from Visual-Language Model model are finetuned to detect synthetic images, using Latent Diffusion data. Dire uses the error between an input image and its reconstruction by pretrained diffusion model for identification. ResNet50 is trained as Present and Future Generalization of Synthetic Image Detectors Source Model Year LGrad GramNet Rine DIMD DeFake Dire SuSy Flickr30k COCO Google Landmarks v2 In-the-wild None None None None Average Average (Resize) Synthbuster mj-tti Synthbuster Synthbuster diffusiondb Synthbuster Synthbuster Synthbuster mj-images Synthbuster SDXL Synthbuster Synthbuster dalle3-images Authors Authors In-the-wild GLIDE MJ V1/2 SD 1.3 SD 1.4 SD 1.X SD 2 DALLE-2 MJ V5 MJ V5/6 SD XL SD XL Firefly DALLE-3 DALLE-3 SD3-Med. FLUX.1-dev Unknown Average Average (Resize) 2014 2017 2020 2024 2021 2022 2022 2022 2022 2022 2022 2023 2023 2023 2023 2023 2023 2023 2024 2024 Authentic Data Sources 99.60 100.0 77.42 95.87 100.0 67.67 100.0 92.56 88.56 92.46 68.84 85. 99.88 100.0 96.54 96.69 83.95 84.16 +0.21 90.06 55.69 -34.87 93.22 92.29 -0.93 98.28 99.78 +1.51 60.76 76.82 38.88 28.10 51.14 66.34 +15. 33.54 32.58 39.46 50.41 39.00 64.87 +25.88 90.53 - 64.54 33.06 62.71 92.84 +30.13 68.80 15.89 86.80 89.20 60.13 29.80 32.00 30.60 8.91 37.20 72.04 14.30 5.30 13.33 35.89 27.15 16.16 Synthetic Data Sources 83.60 14.57 99.90 99.60 96.06 85.80 70.80 87.00 31.28 97.60 17.83 43.30 2.00 28.79 85.05 54.72 16.16 63.43 80.70 35.98 92.90 93.60 94.98 52.50 96.10 51.70 0.16 79.80 62.07 22.90 9.50 0.00 82.81 91.08 19.19 55.40 87.73 +31. 35.26 33.80 -1.46 6.10 2.87 100.0 100.0 99.92 97.10 0.40 98.10 90.11 94.40 2.35 18.10 0.00 61.82 99.24 62.90 47.47 63.30 85.90 64.35 52.50 51.90 68.40 56.40 22.10 56.30 82.01 36.80 23.26 29.90 96.10 89.09 94.95 88.63 87.88 27.50 43.71 82.70 82.60 41.33 89.10 54.90 61.10 59.81 73.20 65.64 66.50 86.40 58.48 33.57 37.77 48.48 57.74 82.73 +15.67 33.76 -23.97 36.39 -36.81 53.50 - 87.00 87.10 - 68.40 20.70 73.10 - 79.50 - 40.90 88.60 - 93.23 96.46 89.90 73. 67.07 61.68 -1.75 0.95 -62.35 Table 7: Center-patch recall of state-of-the-art detector models across evaluation datasets. On top, performance for authentic images. At bottom, performance on synthetic datasets. Recalls below 50%, akin to random chance, in bold. Best recall for each dataset is underlined. Average shows aggregated center-patch performance across datasets of the same class (auth. or synth.). Average (Resize) shows the same with images resized instead of cropped (see 6.2), together with the increment or decrement w.r.t. the patched approach. classifier on their DiffusionForensics dataset. Synthetic images generated with, ADM [12], iDDPM [31] and PNDM [28]."
        },
        {
            "title": "6.1 Benchmarking\nOverall, results in Table 7 indicate the lack of universal detectors,\nwith the performance of all methods failing at random classifier\nlevel in six or more datasets. Performance inconsistency is general-\nized, with all models being the best and worst detectors for at least\none dataset.",
            "content": "In addition, some models fail to characterize authentic images. DeFake and Dire achieve particularly low recalls on all real-world datasets tested. This compromises their utility, as it increases the number of false positive detections (misclassifying authentic images as synthetic). Meanwhile, GramNet, Rine and DIMD demonstrate consistently low false positive rates. Focusing on the models with reasonable amount of false positives (LGrad to DIMD in Table 7) still leads to contractive insights. At times the underlying generative model seems relevant for detection (variants of SD models are identified at higher rates, even for models not trained on diffusion) while other cases show discrepancies between datasets generated using the same model (DALLE3, SDXL). This evidence suggests complex relationship between training data, data sources and detection efficacy. Time and obsolescence are another critical factor, exemplified by LGrads diminished effectiveness on data beyond 2022. continuous evaluation of old detectors is thus recommended, together with the development of new detectors to keep pace with advancing generation techniques, particularly considering the significant existing blind spots in recent models (e.g., recalls over 90% are scarce). Finally, let us examine the behaviour on the In-the-wild, as an approximation of performance in practice. None of the tested detectors reaches recall above 50% for both the authentic and synthetic versions of it. The current best-performing models are DIMD and Rine (with large false negative rate), and SuSy and DeFake (with large false positive rate)."
        },
        {
            "title": "6.2 Scale Generalization\nIn our final generalization study, rather than cropping, we resize\nall images to 224 ร 224 pixels regardless of the original size. This\napproach notably impacts the frequency components of the images,\nas well as the content itself, particularly for high-resolution origi-\nnals. The resizing process can alter or eliminate certain frequency\nartifacts and defects that some SID models rely on. Aggregated re-\nsults are shown in Table 7 (Resize row). See Appendix A for further\ndetails.",
            "content": "Results reveal certain patterns across detection models. Three models (GramNet, Dire and SuSy) exhibit balanced trade-off between false positives and false negatives, approximately losing in one recall what is gained in the other. For instance, Dire shows substantial improvement for authentic images (+25.88) but an analogous decrease for synthetic ones (23.97). This indicates that, for these models, changing the resolution changes the decision boundary, in way which is independent from the features characterizing the final discrimination between authentic and synthetic images. Such that, when the model becomes more/less sensitive, this affects equally both the authentic and synthetic predictions. Preliminary experimentation on Dire shows this symmetry to be consistent on other resolutions (512 512). One model collapses, suffering catastrophic losses after the resolution change. DIMDs average recall on the synthetic class drops 62%, while only gaining 1.5% on the authentic class. This could be caused by DIMDs avoidance of the downsampling step, making it more dependent on image scale. Regardless, it points towards the importance of considering model sensitivity during the design and training of detectors. This is valuable feature that is present in two other models, LGrad and Rine. Remarkably, their performance generalizes very well to scale changes. Lastly, DeFake stands out by showing large improvements in both recall scores (> +15%) under scale reduction. This may be caused by the Visual Language Model architecture empowering the detector, which could benefit from having the whole image context as input, despite losing frequency artifacts. We find these results to be consistent with the findings in [34]."
        },
        {
            "title": "7 Conclusions\nThe race between synthetic image generators and detectors is far\nfrom over. New and better generative models appear regularly,\nand detectors cannot generalize to all (see ยง6). Meanwhile, the\nincreasing realism of synthetically generated content brings along\na proportional demand for detectors, as tools for protecting social\ntrust and digital rights, while preventing disinformation.",
            "content": "The biggest challenge for detectors is generalization. Changes of generative model, data source, image post-processing and training task, affect detector performance, limiting their reliability. This work studied all these factors, starting with the underlying generative model used to produce the images. By temporally grounding our experiments, we observe how older models/datasets (less realistic and producing bigger artifacts) can be easily detected when trained for them, but are harder to generalize to when not seen during training (when compared to more recent generative models, see Table 2). This indicates the stronger biases defining older synthetic images are rarely found in newer datasets. As result, Bernabeu-Perez, Lopez-Cuena and Garcia-Gasulla including early generative models in the train set of new detector models should become common practice. Another consistent family of generative models that is consistently harder to generalize to are private models (e.g., DALLE3, Firefly), as shown in Tables 2, 5 and 7. This speaks of the importance of open science in the field. Data source is another key factor in detection. Using the same generative model, individuals employing various software and hardware configurations can produce significantly different content, to the point where generalization to the same model but different source often becomes challenging (see Tables 5 and 7). As result, detector models can never be assumed to work in data obtained from an untested source. This is further reinforced by tests conducted on data gathered In-the-wild, showing the dangerous effect of uncontrolled data sources. To mitigate this risk, recurrent sanity checks are recommended, as well as dataset-specific fine-tunes. common pitfall of detector models is the lack of proper characterization of authentic images. This has higher prevalence on binary classifiers  (Table 3)  but also happens on multi-class ones  (Table 5)  . Since characterizing authentic images is essential for practical use, specific tests using different sources of authentic data should be prioritized. Training on authentic data from many sources should also be common practice. This work shows how to train detectors that maximize generalization, by focusing on many different data sources and generative models. This alone is no guarantee, and diversity, even within the same source or model, is of essence. Our limited field study indicates that, while we are far from universal detector, models trained for specific targets may already be as good as humans at identifying synthetic content (see Table 6). The race between generators and detectors will go on, particularly since data from better generative models produces detectors that generalize better (see Table 2), while better detectors are likely to be used to improve generators. This leads to race equilibrium paradox, ensuring that the race for synthetic content detection is always going to be close one."
        },
        {
            "title": "7.1 Ethical risks\nTo finalize, let us consider the ethical risks associated with this\nwork and with the materials released. The most obvious of which\nis the fallibility of the detectors trained. The SuSy, as any other\ndetector model, makes both false positive and negative predictions\n(see Table 5), potentially labelling authentic images as synthetic and\nvice versa. Digital rights could be affected and censorship could be\nenabled through its use. For this reason, when applied in a setting\naffecting the rights and privileges of humans, the model should\nalways be overseen by a human expert, and its predictions should\nnever be taken as conclusive evidence.",
            "content": "The datasets used in this work are biased in many ways. Some of these biases are learnt by the models and could influence its decision in undesired ways (e.g., countryside images could be more likely to be tagged as synthetic than urban landscapes). The most effective mitigation strategy in this case is to conduct study on model performance, based on the existing populations within the application domain. Once undesirable biases are identified, these can be corrected through customized fine-tuning using synthetic data. Present and Future Generalization of Synthetic Image Detectors While all datasets used for training are publicly available, these may include samples with personal data. COCO contains images of real people, and synthetic datasets used could include realistic depictions of specific individuals. It is however unlikely, both for the training task and the size of SuSy, that such information would be encoded and retrieved from the released weights alone. final risk we consider is the use of released detector models for the purpose of training generative models that can overcome them (e.g., by adding specific loss). In fact, this is already common strategy in academia and industry (i.e., GANs), that can be applied to our work. To mitigate that, we add specific clause in the terms of use of the model prohibiting such practice. The alternative, not doing research in this line, would deprive society of means of protection against the increasing presence of synthetic images. Any detector model publicly released is compromised (e.g., by finding counterfactuals examples). We recommend potential users of SuSy to tune it on private data, as to mitigate that risk, while considering in the process the aforementioned limitations with regards to accuracy and bias. We also recommend fine-tuned model weights are kept private, as long as their public release holds no special academic or social value. Acknowledgements This work has been partly funded by the AI4Media and AI4Europe projects from the European Unions Horizon 2020 programme (Grant Agreements Nยบ951911 and Nยบ101070000), and by SGR-GRE grant from the Generalitat de Catalunya (code 2021 SGR 01187). The authors would like to acknowledge Mauro Achile, Eric Arean, Nura Mangado, Diego Rios and Daniel Pulido who contributed to motivating and contextualizing this work. Special thanks to the volunteers who participated in the human evaluation experiment. References [1] [n. d.]. Civitai. https://civitai.com/ [2] European Environment Agency. 2024. Greenhouse gas emission intensity of electricity generation in Europe. https://www.eea.europa.eu/en/analysis/indicators/ greenhouse-gas-emission-intensity-of-1 [3] Stability AI. 2023. Stable Diffusion XL. https://stability.ai/news/stable-diffusionsdxl-1-announcement [4] Stability AI. 2024. Stable Diffusion 3 Medium. https://huggingface.co/stabilityai/ stable-diffusion-3-medium [5] Quentin Bammey. 2023. Synthbuster: Towards detection of diffusion model generated images. IEEE Open Journal of Signal Processing (2023). [6] Alexander Buslaev, Vladimir I. Iglovikov, Eugene Khvedchenya, Alex Parinov, Mikhail Druzhinin, and Alexandr A. Kalinin. 2020. Albumentations: Fast and Flexible Image Augmentations. Information 11, 2 (2020). https://doi.org/10.3390/ info11020125 [7] George Cazenavette, Avneesh Sud, Thomas Leung, and Ben Usman. 2024. FakeInversion: Learning to Detect Images from Unseen Text-to-Image Models by Inverting Stable Diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1075910769. [8] Baoying Chen, Jishen Zeng, Jianquan Yang, and Rui Yang. 2024. DRCT: Diffusion Reconstruction Contrastive Training towards Universal Detection of Diffusion Generated Images. In Forty-first International Conference on Machine Learning. [9] Riccardo Corvi, Davide Cozzolino, Giada Zingarini, Giovanni Poggi, Koki Nagano, and Luisa Verdoliva. 2023. On the detection of synthetic images generated by diffusion models. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 15. [10] Davide Cozzolino, Giovanni Poggi, Riccardo Corvi, Matthias Nieรner, and Luisa Verdoliva. 2024. Raising the Bar of AI-generated Image Detection with CLIP. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 43564366. [11] Duc-Tien Dang-Nguyen, Cecilia Pasquini, Valentina Conotter, and Giulia Boato. 2015. Raise: raw images dataset for digital image forensics. In Proceedings of the 6th ACM multimedia systems conference. 219224. [12] Prafulla Dhariwal and Alexander Nichol. 2021. Diffusion models beat gans on image synthesis. Advances in neural information processing systems 34 (2021), 87808794. [13] DucHaiten. 2023. https://huggingface.co/datasets/ realisticSDXL Dataset. DucHaiten/DucHaiten-realistic-SDXL [14] ehristoforu. 2024. dalle-3-images Dataset. ehristoforu/dalle-3-images https://huggingface.co/datasets/ [15] ehristoforu. 2024. midjourney-images Dataset. https://huggingface.co/datasets/ ehristoforu/midjourney-images [16] Joel Frank, Thorsten Eisenhofer, Lea Schรถnherr, Asja Fischer, Dorothea Kolossa, and Thorsten Holz. 2020. Leveraging frequency analysis for deep fake image recognition. In International conference on machine learning. PMLR, 32473258. [17] Patrick Grommelt, Louis Weiss, Franz-Josef Pfreundt, and Janis Keuper. 2024. Fake or JPEG? Revealing Common Biases in Generated Image Detection Datasets. arXiv preprint arXiv:2403.17608 (2024). [18] Gustavosta. 2023. Stable-Diffusion-Prompts. https://huggingface.co/datasets/ Gustavosta/Stable-Diffusion-Prompts [19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition. 770778. [20] Huaibo Huang, Zhihang Li, Ran He, Zhenan Sun, and Tieniu Tan. 2018. IntroVAE: Introspective Variational Autoencoders for Photographic Image Synthesis. arXiv:1807.06358 [cs.LG] https://arxiv.org/abs/1807.06358 [21] Yan Ju, Shan Jia, Lipeng Ke, Hongfei Xue, Koki Nagano, and Siwei Lyu. 2022. Fusing global and local features for generalized ai-synthesized image detection. In 2022 IEEE International Conference on Image Processing (ICIP). IEEE, 34653469. [22] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. 2017. Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196 (2017). [23] Christos Koutlis and Symeon Papadopoulos. 2024. Leveraging Representations from Intermediate Encoder-blocks for Synthetic Image Detection. arXiv preprint arXiv:2402.19091 (2024). [24] Black Forest Labs. 2024. FLUX.1-dev. https://huggingface.co/black-forestlabs/FLUX.1-dev [25] Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, and Furu Wei. 2022. Dit: Self-supervised pre-training for document image transformer. In Proceedings of the 30th ACM International Conference on Multimedia. 35303539. [26] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollรกr. 2015. Microsoft COCO: Common Objects in Context. arXiv:1405.0312 [cs.CV] [27] Huan Liu, Zichang Tan, Chuangchuang Tan, Yunchao Wei, Jingdong Wang, and Yao Zhao. 2024. Forgery-aware adaptive transformer for generalizable synthetic image detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1077010780. [28] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. 2022. Pseudo numerical methods for diffusion models on manifolds. arXiv preprint arXiv:2202.09778 (2022). [29] Zhengzhe Liu, Xiaojuan Qi, and Philip HS Torr. 2020. Global texture enhancement for fake face detection in the wild. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 80608069. [30] Enrique Lรณpez Cuena. 2023. Super-resolution assessment and detection. http: //hdl.handle.net/2117/ [31] Alexander Quinn Nichol and Prafulla Dhariwal. 2021. Improved denoising diffusion probabilistic models. In International conference on machine learning. PMLR, 81628171. [32] Utkarsh Ojha, Yuheng Li, and Yong Jae Lee. 2023. Towards universal fake image detectors that generalize across generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2448024489. [33] Jonas Ricker, Simon Damm, Thorsten Holz, and Asja Fischer. 2022. Towards the detection of diffusion model deepfakes. arXiv preprint arXiv:2210.14571 (2022). [34] Manos Schinas and Symeon Papadopoulos. 2024. SIDBench: Python framework for reliably assessing synthetic image detection methods. arXiv preprint arXiv:2404.18552 (2024). [35] Zeyang Sha, Zheng Li, Ning Yu, and Yang Zhang. 2023. De-fake: Detection and attribution of fake images generated by text-to-image generation models. In Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security. 34183432. [36] Chuangchuang Tan, Yao Zhao, Shikui Wei, Guanghua Gu, and Yunchao Wei. 2023. Learning on gradients: Generalized artifacts representation for gan-generated images detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1210512114. [37] Iulia Turc and Gaurav Nemade. 2022. Midjourney User Prompts & Generated Images (250k). https://doi.org/10.34740/KAGGLE/DS/2349267 [38] European Union. 2024. Proposal for Regulation of the European Parliament and of the Council Laying Down Harmonised Rules on Artificial Intelligence (Artificial Intelligence Act) and Amending Certain Union Legislative Acts. https: //artificialintelligenceact.eu/ [39] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, and Alexei Efros. 2020. CNN-generated images are surprisingly easy to spot... for now. In Bernabeu-Perez, Lopez-Cuena and Garcia-Gasulla Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 86958704. [40] Zijie J. Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, and Duen Horng Chau. 2022. DiffusionDB: Large-Scale Prompt Gallery Dataset for Text-to-Image Generative Models. arXiv:2210.14896 [cs] (2022). https://arxiv. org/abs/2210. [41] Tobias Weyand, Andre Araujo, Bingyi Cao, and Jack Sim. 2020. Google landmarks dataset v2-a large-scale benchmark for instance-level recognition and retrieval. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 25752584. [42] Alexander Wiรmann, Steffen Zeiler, Robert Nickel, and Dorothea Kolossa. 2024. Whodunit: Detection and Attribution of Synthetic Images by Leveraging Model-specific Fingerprints. In Proceedings of the 3rd ACM International Workshop on Multimedia AI against Disinformation. 6572. [43] Qiang Xu, Hao Wang, Laijin Meng, Zhongjie Mi, Jianye Yuan, and Hong Yan. 2023. Exposing fake images generated by text-to-image diffusion models. Pattern Recognition Letters 176 (2023), 7682. [44] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. 2014. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics 2 (2014), 6778. [45] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. 2015. Lsun: Construction of large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365 (2015). [46] Nan Zhong, Yiran Xu, Zhenxing Qian, and Xinpeng Zhang. 2023. Rich and Poor Texture Contrast: Simple yet Effective Approach for AI-generated Image Detection. arXiv preprint arXiv:2311.12397 (2023). [47] Mingjian Zhu, Hanting Chen, Mouxiao Huang, Wei Li, Hailin Hu, Jie Hu, and Yunhe Wang. 2023. Gendet: Towards good generalizations for ai-generated image detection. arXiv preprint arXiv:2312.08880 (2023). [48] Mingjian Zhu, Hanting Chen, Qiangyu Yan, Xudong Huang, Guanyu Lin, Wei Li, Zhijun Tu, Hailin Hu, Jie Hu, and Yunhe Wang. 2024. Genimage: million-scale benchmark for detecting ai-generated image. Advances in Neural Information Processing Systems 36 (2024). Present and Future Generalization of Synthetic Image Detectors Additional Evaluation Metrics Table 8 summarizes the recall rates of four detection methodsCNNDetect, FreqDetect, Fusing, and UnivFDacross various datasets, when applying center crop of size 224 224 to the input image. CNNDetect Model 97.52 In the wild (Authentic) 99.79 Flickr30k 98.64 Google Landmarks v2 1.01 In the wild (Synthetic) FLUX.1-dev 7.36 Stable Diffusion 3 Medium 16.72 20.90 DALLE-2 (SB) 0.20 DALLE-3 (SB) 17.40 Firefly (SB) 9.30 GLIDE (SB) 6.60 Midjourney v5 (SB) 9.50 Stable Diffusion 1.3 (SB) 9.70 Stable Diffusion 1.4 (SB) 9.40 Stable Diffusion 2 (SB) 18.20 Stable Diffusion XL (SB) FreqDetect 98.35 99.67 99.54 2.02 0.55 12.29 7.10 0.20 3.00 10.70 1.20 8.40 7.90 0.40 0. Fusing UnivFD 99.17 99.98 99.90 1.01 6.98 23.83 22.80 0.00 17.10 12.10 9.90 9.50 9.80 9.00 6.50 73.55 99.76 97.70 5.05 1.51 4.79 72.70 0.40 85.50 12.20 10.70 46.50 45.40 57.10 41.60 Table 8: Recall of four additional detection methods when applied to the evaluation datasets. The following table contains the scores for all studied models after resizing the input to 224 224. Model Flickr30K COCO (test) Google Landmarks v2 In the wild (Authentic) CNNDetect DIMD DeFake Dire 64.84 99.94 60.62 92.3 62.96 99.2 71.07 90. 100 100 99.98 99.17 79.96 86.06 64.66 34.71 FreqDetect 99.4 97.16 99.58 99.17 Fusing GramNet LGrad Rine UnivFD 89.4 100 92.46 98.62 68.84 99.92 85.95 99.17 99.84 99.03 98.98 99.17 99.6 88.17 97.92 83. 98.7 14.99 88.42 20.66 11.5 GLIDE (SB) 5.52 Midjourney TTI (test) 13.6 Stable Diffusion 1.3 (SB) 13.7 Stable Diffusion 1.4 (SB) 9.56 Diffusiondb (test) 29.7 Stable Diffusion 2 (SB) 10.2 DALLE-2 (SB) 17.2 Midjourney v5 (SB) 5.51 MJ V5/6 (test) 28.2 Stable Diffusion XL (SB) 16.45 Stable Diffusion XL (test) 14.7 Firefly (SB) 1 DALLE-3 (SB) 4.55 DALLE-3 (test) 1.39 FLUX.1-dev In the wild (Synthetic) 6.06 Stable Diffusion 3 Medium 5.36 15.8 20.75 53.4 53.4 21.64 79.4 29.8 33.1 26.9 55.8 15.48 36.9 50.3 31.21 15.82 28.28 12.12 Table 9: Recall of state-of-the-art detector models across evaluation datasets when resizing the input image to 224 224. 68.8 15.89 86.8 89.2 60.13 29.8 32 30.6 8.91 37.2 27.96 14.3 5.3 13.33 27.15 16.16 35.89 10.1 10.82 4.4 3.6 13.53 34.1 5.9 0.5 1.3 14 0.49 2.2 0.1 0.3 0.28 0 2.28 84.1 80.02 94.9 95.3 92.79 82.6 79.5 98.1 76.18 97.9 96.27 89.1 93 50.61 95.69 91.92 90. 83.4 81.57 93.4 93.7 91.57 82.6 67.6 67.5 14.59 87.8 53.81 56.7 59.8 10.61 56.79 18.18 74.01 87.4 82.67 85.7 87.4 81.69 61.4 47.3 75.9 85.74 78.2 94.73 79.1 95.4 87.58 89.87 89.9 92.49 10.6 7.4 7.9 7.2 4.62 6.1 21.9 5.5 0.49 2.4 20.18 10 0 0.61 7.68 3.03 29.46 10.7 5.96 2.3 2.4 7.05 1.5 5.5 3.5 4.54 1.8 2.67 4.6 3.1 4.85 9.29 2.02 6.88 3.4 1.77 0.2 0.3 6.24 0.2 0.6 0.1 0.49 0.1 2.35 0 0 0 0.04 0 0.09 Evaluation Dataset Image Samples This section includes several sample images from the evaluation datasets utilized. Bernabeu-Perez, Lopez-Cuena and Garcia-Gasulla Figure 2: Sample authentic images from Flickr30K (top) and Google Landmarks v2 (bottom) Figure 3: Sample images from our In-The-Wild dataset. Synthetic images (top) and authentic images (bottom) Present and Future Generalization of Synthetic Image Detectors Bernabeu-Perez, Lopez-Cuena and Garcia-Gasulla Figure 4: Sample synthetic images from Synthbuster. Note that the same prompt is used for each column. Present and Future Generalization of Synthetic Image Detectors Figure 5: Sample synthetic images from our generated Stable Diffusion 3 Medium (top) and FLUX.1-dev (bottom) datasets. Evaluation Image Resolution Distribution We calculate the resolution distribution for each of the evaluation dataset. Figure 6 contains information regarding the size and aspect ratio of the datasets used. Top plot shows the width distribution of all samples, dataset-wise. The bottom plot shows the same for height. Bernabeu-Perez, Lopez-Cuena and Garcia-Gasulla Figure 6: Resolution distribution of images across various datasets. The plots illustrate the width (top) and height (bottom) distributions."
        }
    ],
    "affiliations": [
        "Barcelona Supercomputing Center (BSC) Barcelona, Spain"
    ]
}