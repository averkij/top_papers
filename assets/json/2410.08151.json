{
    "paper_title": "Progressive Autoregressive Video Diffusion Models",
    "authors": [
        "Desai Xie",
        "Zhan Xu",
        "Yicong Hong",
        "Hao Tan",
        "Difan Liu",
        "Feng Liu",
        "Arie Kaufman",
        "Yang Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Current frontier video diffusion models have demonstrated remarkable results at generating high-quality videos. However, they can only generate short video clips, normally around 10 seconds or 240 frames, due to computation limitations during training. In this work, we show that existing models can be naturally extended to autoregressive video diffusion models without changing the architectures. Our key idea is to assign the latent frames with progressively increasing noise levels rather than a single noise level, which allows for fine-grained condition among the latents and large overlaps between the attention windows. Such progressive video denoising allows our models to autoregressively generate video frames without quality degradation or abrupt scene changes. We present state-of-the-art results on long video generation at 1 minute (1440 frames at 24 FPS). Videos from this paper are available at https://desaixie.github.io/pa-vdm/."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 0 1 ] . [ 1 1 5 1 8 0 . 0 1 4 2 : r a"
        },
        {
            "title": "PROGRESSIVE AUTOREGRESSIVE VIDEO DIFFUSION\nMODELS",
            "content": "Desai Xie Stony Brook University & Adobe Research dexxie@cs.stonybrook.edu Zhan Xu, Yicong Hong, Hao Tan, Difan Liu, & Feng Liu Adobe Research {zhaxu, yhong, hatan, diliu, fengl}@adobe.com Arie Kaufman Stony Brook University ari@cs.stonybrook.edu Yang Zhou Adobe Research yazhou@adobe.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Current frontier video diffusion models have demonstrated remarkable results at generating high-quality videos. However, they can only generate short video clips, normally around 10 seconds or 240 frames, due to computation limitations during training. In this work, we show that existing models can be naturally extended to autoregressive video diffusion models without changing the architectures. Our key idea is to assign the latent frames with progressively increasing noise levels rather than single noise level, which allows for fine-grained condition among the latents and large overlaps between the attention windows. Such progressive video denoising allows our models to autoregressively generate video frames without quality degradation or abrupt scene changes. We present state-of-the-art results on long video generation at 1 minute (1440 frames at 24 FPS). Videos from this paper are available at https://desaixie.github.io/pa-vdm/."
        },
        {
            "title": "INTRODUCTION",
            "content": "Video diffusion models have recently demonstrated remarkable success in generating high-quality video content across variety of applications (Meta, 2024; Blattmann et al., 2023a; Guo et al., 2023; Bar-Tal et al., 2024; Ho et al., 2022a; Girdhar et al., 2023). These models are capable of synthesizing realistic video sequences that are increasingly indistinguishable from real-world footage. However, despite their impressive results, current video diffusion models are constrained by significant limitation: they can only generate videos of relatively short duration, typically up to about 10 seconds or 240 frames. This temporal restriction leads to challenges for broader applications that require longer, more continuous video outputs, highlighting the need for further research and innovation to extend the capabilities of these models. Several approaches (Ho et al., 2022b; Henschel et al., 2024; Blattmann et al., 2023a; Zheng et al., 2024; Gao et al., 2024) have been proposed to address the challenge of generating longer videos with video diffusion models. They autoregressively apply video diffusion models to generate short video clips in windowed fashion, where each subsequent clip conditions on the final frames of the previous one. One solution (Zheng et al., 2024; Gao et al., 2024) directly puts the conditioning frames into the input frames, replacing the noisy frames. Another solution (Ho et al., 2022b) additionally adds the same level of noise to the conditioning frames as the noisy frames. Due to the large gap between the noise levels of the condition frames and the noisy frames, iteratively applying the first solution sometimes produces videos with numerous abrupt scene changes, which is similar to generating short video clips independently. The second solution is more effective at producing smooth pixel-level This work is done while Desai is an intern at Adobe Research 1 (a) replacement (b) ours Figure 1: Comparison of autoregressively applying video diffusion models with replacement methods (left) vs. our progressive autoregressive video diffusion models (right). Our method allows for more fine-grained condition among the latent frames, where the later frames with more uncertainty can follow the pattern of the earlier, more certain frames, and larger overlap between the attention windows without extra computation cost. transitions between generated clips, since the condition frames and the noisy frames have the same noise levels during denoising. However, it struggles to preserve secondary attributes, e.g., velocity and acceleration, accurately, leading to generating unnatural and inconsistent movement in long videos. In addition, repeatedly applying the second solution amplifies these issues, increasing inaccuracies in motion dynamics and accumulating errors that causes the video to eventually diverge. We propose autoregressive video diffusion models that denoise video frames in progressive manner. It can generate long videos with smooth and continuous video frames and motions with no quality degradation. The core innovation of our method lies in the denoising process: instead of applying single noise level across all frames used in traditional video generation (or extension) diffusion models, we progressively increase the noise levels across frames during denoising. Such progressive noise levels allows autoregressive video denoising with large overlaps between the attention window, as illustrated in Fig. 1. This approach improves the temporal transitions in extended frames, resulting in more natural motion and better consistency. Our method can be easily implemented by changing the noise scheduling and finetuning pre-trained video diffusion models, either UNet-based (Ronneberger et al., 2015; Ho et al., 2022b) or Diffusion Transformer (DiT)-based (Peebles & Xie, 2023; Ma et al., 2024; Brooks et al., 2024) backbone, without changing the original model architecture. Our progressive autoregressive inference procedure can work training-free, if the model has been trained on vary noise levels, e.g. the masked pre-training in (Zheng et al., 2024). Our method can generate videos up to one minute in length (1440 frames) without noticeable degradation in quality. Moreover, the additional computational cost at inference time is minimal comparing to previous works (Wang et al., 2023; Qiu et al., 2024; Henschel et al., 2024) that generates overlapped clips, making this approach more efficient for practical use in the long video generation. We compare our method to the baselines on long video generation at 60-second or 1440 frames. Our quantitative results demonstrate that our method has overall the best frame quality, the best motion dynamics, the best aesthetic and the least scene changes. It is able to maintain these metrics over the entire 60-second duration. Qualitatively, our method is also shown to be preserve the frame and motion quality over time, while the baselines performance drops. By applying our method to two base models and demonstrating significantly improved performance compared to their respective baselines, we confirm its universal applicability to existing video diffusion models. We provide more video results at our website https://desaixie.github.io/pa-vdm/. In sum, our progressive autoregressive video diffusion models achieves state-of-the-art results on long video generation. To facilitate future research, we release our training and inference code based on Open-Sora (Zheng et al., 2024) at https://github.com/desaixie/pa_vdm."
        },
        {
            "title": "2.1 VIDEO DIFFUSION MODELS",
            "content": "Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020) are generative models that learn to generate samples from data distribution q(x0) through an iterative denoising process. During training, data samples are first corrupted using the forward diffusion process q(xtx0) (cid:12)x0(cid:1) = (xt; αtx0, (1 αt)I) (cid:0)xt(cid:12) (1) xt = αtx0 + 1 αtϵ (2) where [0, ) is the noise level or diffusion timestep, ϵ (0, I) is the noise, and α1:T is the variance schedule. With those noisy data samples xt, diffusion models are trained to fit to the data distribution q(x0) by maximizing the variational lower bound (Kingma & Welling, 2013) of the log likelihood of x0, which can be simplified into mean squared error loss (Ho et al., 2020) L(θ) = (cid:13) (cid:13)ϵ ϵθ(xt, t)(cid:13) 2 (cid:13) (3) where is uniform between 0 and , ϵ (0, I) and ϵθ is the noise predicted by the model with parameters θ. At sampling time, we consider the sampling noise level schedule τ = {τ1, τ2, ..., τS} , which is an increasing subset of [0, ) of length (Song et al., 2021). Starting from xτS (0, I), τS = , the reverse denoising process is autoregressively applied as pθ (xτi1xτi) = qσ (cid:0)xτi1 (cid:12) (cid:12)xτ , fθ(xt, t)(cid:1) (4) where ˆx0 = fθ(xt, t) is the x0 predicted by the model and fθ(xt, t) is the DDIM (Song et al., 2021) reverse process equation, which we omit for simplicity. This gives us sequence of samples xT , xτS1, . . . , xτ1 , x0, and the last sample x0 is the clean output result. Video diffusion models (Ho et al., 2022b; Blattmann et al., 2023b) are diffusion models that consider latent representations of video data, consisting of latent frames x0:F 1 = {x0, x1, . . . , xF 1}. The same forward diffusion process, the reverse denoising process, and loss (eqs. (1) to (4)) can be applied to model these video data by treating all the latents as one entity, ignoring the correlation among the latents. Recent video diffusion models (Meta, 2024; Zheng et al., 2024) have employed various diffusion model variants (Liu et al., 2022; 2023; Lipman et al., 2023) to improve training and inference efficiency as well as output quality. Nevertheless, our method is compatible with any diffusion model variant as long as the model corrupts the data xt at the same noise levels t. 2.2 LONG VIDEO GENERATION VIA REPLACEMENT Video diffusion models can only generate short video clips, because they are only trained on videos with limited length due to GPU memory limit. When adapted to generating > latent frames at sampling time, their generation quality substantially degrades (Qiu et al., 2024). The straightforward solution is to autoregressively apply video diffusion models, generating each video clip while conditioning on the previous clip. In this paper, we refer to the latents that the video diffusion model processes as the attention window. Given < clean latents x0 0:E as condition, there are two methods for autoregressively applying video diffusion models. Zheng et al. (2024); Gao et al. (2024); Blattmann et al. (2023a) place the clean condition latents x0 0:E1 directly at the front of the attention window, directly replacing the sampled latents xτi 0:E1 at each denoising step (cid:0)x0 pθ 0:E1, xτi1 E:F 1x0 0:E1, xτi E:F (cid:1) We will refer to this method as the replacement-without-noise method. Song et al. (2020b); Ho et al. (2022b) additionally add noise to the conditioning latents 0:E1, xτi1 0:E1, xτi (cid:0)xτi1 (cid:12) (cid:12)xτi E:F 1 E:F 1 pθ (cid:1) 3 (5) (6) Figure 2: Comparison of noise levels of ours vs. the replacement without noise method. 0:E1 are the condition latents x0 where xτi 0:E1 noised via the forward process (eqs. (1) and (2)). This maintains the same noise level distribution and training objective as regular video diffusion models. We will refer to this method as the replacement-with-noise method. Both the replacement-with-noise method and the replacement-without-noise method allow video diffusion model to autoregressively generate video frames by conditioning on previous frames. We consider them as baselines in our experiments in Sec. 4.2."
        },
        {
            "title": "3 PROGRESSIVE AUTOREGRESSIVE VIDEO DIFFUSION MODELS",
            "content": "Existing video diffusion models (Meta, 2024; Blattmann et al., 2023a; Guo et al., 2023; Bar-Tal et al., 2024) can only generate short video clips up to limited length (e.g. 10 seconds or 240 frames), due to GPU memory constraints during training. We show that, without changing the architectures, they can be naturally extended to progressive autoregressive video diffusion models that can generate long videos without quality degradation. We achieve this by proposing per-frame noise schedule, which is inspired by Chen et al. (2024). During training, we finetune pre-trained video diffusion models to adapt to such noise schedule; during sampling, our models adopt such noise schedule and can thus autoregressively generate video frames. 3.1 PROGRESSIVE VIDEO DENOISING Inspired by (Chen et al., 2024), we assign progressively increasing noise levels τ0:F 1 to the latent frames in the attention window. These noise levels can be obtained from the sampling noise level schedule τ = {τ0, τ1, ..., τS} (Song et al., 2021), where 0 = τ0 < . . . < τi1 < τi < . . . < τS = , depending on and S. In this work, we consider the linear sampling schedule 0:S = (cid:8)τ τ 0, τ 1, τ 2, . . . , τ S1, τ (cid:9) = (cid:26) 0, , 2T , . . . , (cid:27) (S 1)T , (7) For simplicity, we set = S, so that our latents noise levels τ0:F 1 can take the values of either 0:S1 or τ τ 1:S. We denoise the latent frames of video with the progressively increasing noise levels (cid:12) 0 , xτ (cid:12)xτ (cid:12) τ 2 , xτ S1 τ S2 2 , 0 , xτ xτ 1 , ..., 1 , ..., τ S1 1 1 pθ (8) (cid:16) (cid:17) 0 1 2 1 where the input frames and output frames have noise levels τ 1:S and τ 0:S1 respectively. With our progressive noise levels, which is subset of independent noise levels t0:F 1 for the frames x0:F 1, one concern is that whether this formulation is still compatible with existing video diffusion models. Conceptually, we are effectively using single set of model parameters θ to jointly model latents q(x0 1), where each latent has single noise level tf like regular diffusion models. Thus, we can directly apply the forward process, reverse process, and training loss (eqs. (1) to (4)) of existing video diffusion models (Ho et al., 2020; Song et al., 2021; Liu et al., 2022; 2023). This means that we can obtain our progressive autoregressive video diffusion models from pre-trained video diffusion models, which demands immense computation resources. 1), . . . , q(x0 0), q(x0 Intuitively, the benefit of our progressive video denoising process is that it gradually establishes correlation among consecutive latent frames. Given some existing video latents as conditioning, it is 4 challenging for video diffusion models to produce temporally consistent extensions latents from newly sampled noisy latents (Qiu et al., 2024). In contrast to the replacement-with-noise method (Blattmann et al., 2023a; Ho et al., 2022b) where the latents are denoised together at the same noise level, our progressive video denoising encourages the later latents with higher uncertainty to follow the patterns of the earlier and more certain latents, facilitating modeling smoother temporal transition and better preserving motion velocity. Compared to the replacement-without-noise method where there is large noise level gap between the clean condition latents x0 E:F 1, our method provides more fine-grained condition, where difference of noise levels between neighboring latents is only , as illustrated in eq. (7) and Fig. 2. Additionally, both replacement methods (Ho et al., 2022b; Blattmann et al., 2023a; Zheng et al., 2024) need to balance between the benefit of having larger overlap between two subsequent attention windows and the extra computation cost when setting the number of condition frames < . In contrast, our method can have the maximum attention window overlap of 1 latents while not requiring any extra computation (eq. (8)). 0:E1 and the noisy frames xτi Another benefit of our method is the more efficient modeling of long videos of latent frames using video diffusion models with limited attention window length < L. During training, the model needs to fit any subset of latents out of the latents. When using the two replacement methods (Ho et al., 2022b; Blattmann et al., 2023a; Zheng et al., 2024), the model needs to train on the full range of noise levels [0, ) for each data sample of latents. With our method, the neighboring training data are efficiently modeled as exactly one sampling step apart from each other, so that we only need to train on the noise levels between τ 1:S. 0:S1 and τ 3.2 AUTOREGRESSIVE GENERATION 0 = 0 and τ Notice that the noise levels of the input and output latents in eqs. (7) and (8), τ differ by τ levels by saving and removing the clean latent x0 once, and appending new noisy latent xT autoregerssive sampling procedure. 1:S, only = . We can simply transition the output latents into the correct input noise 0 at the front, shifting the latent sequence forward 1 (0, I) at back. Alg. 1 and Fig. 1 details this 0:S1 and τ Algorithm 1 Inference procedure of progressive autoregressive video diffusion models 0:F 1 = {x0 Require: Initial video latent frames x0 (cid:27) 1, ..., 0, x0 1}, maximum noise level , number 1: τ 1, . . . , τ of inference steps S, and attention window size = (cid:26) 0:S = {τ 0, τ 2: ϵ (0, I) (cid:112) 3: xτ 0:F 1 = 4: for each autoregressive generation step = 1, 2, . . . , do 0:F 1 + 1 ατ , . . . , S} = 1:S x0 ατ 1:S ϵ (cid:112) 0, 1:S 1 , . . . , (cid:111) τ S1 1 pθ (cid:16) τ 0:S1 0:F 1 (cid:12) (cid:12)xτ (cid:12) 1:S 0:F 1 (cid:110) 1 5: 0:S1 6: 7: τ 0, xτ x0 0:F 1 = xT 1 (0, I) Append x0 xτ 0:F 1 = 8: 9: end for 10: return List of clean frames 0:S 0 to the list of clean latents (cid:110) τ xτ 2 , xT S1 1 , . . . , 1 (cid:111) eq. (7), progressive noise levels eq. (2), add noise (cid:17) eq. (8), one sampling step Sample new noisy latent Remove 0, shift latents by 1, and append xT 1 Variable Length The above design only allows for autoregressive video extension given an initial video of length . In addition, the noisy latents remaining in the attention window xτ 0:F 1 (line 8 of Alg. 1) are discarded after the end of the autoregressive inference, which can cause wasted computing resources and inaccurate handling of the ending of text prompt. To enable text-to-longvideo generation without starting latents and proper generation of ending latents, we extend the base design in eq. (8) and Alg. 1 to add an initialization stage and an termination stage, where the model operates on variable attention window lengths from 1 to 1. During initialization, we simply disable the removing x0 0 , we denoise 1 }; we repeat this by 1 times it for one step and append new noisy latent to obtain {x 0 operation in line 8 of Alg. 1: starting from noisy latent xT τ S1 0 , xT 1:S 5 1 1:S (cid:111) 0:F 1 = τ 2 , xT S1 to obtain xτ (cid:110) xτ 1 , . . . , 1 operation: starting with latents xτ , i.e. the input to line 5 of Alg. 1. During termination, (cid:110) (cid:111) xτ we disable the append xT 0 , . . . , , we denoise them for one step, save and remove the first latent x0 0 and shift the latents by 1 to obtain τ ; we repeat this by times to save and remove all the remaining latents 0:F 2 = in the attention window. We train the model accordingly on video latent frames with variable lengths ranging from 1 to 1, following the noise levels described above. (cid:110) xτ 0 , . . . , τ 2 , xT S1 0:F 1 = τ S1 2 1 1:S1 (cid:111) 1:S"
        },
        {
            "title": "3.3 TRAINING",
            "content": "While regular video diffusion models always treat the latent frames as single entity by assigning single noise level to all of them, we extend this formulation by modeling each frame independently and assigning them with varying noise levels t0:F 1. This change in the noise level distribution typically requires finetuning the pre-trained video diffusion model to adapt to our progressive noise level distribution. We finetune pre-trained video diffusion models by modifying the noise levels during training. Regular diffusion model training (Ho et al., 2020; Liu et al., 2022; 2023) involves uniformly sampling noise level [0, ), adding noise to the samples x0 0:F 1 via the forward diffusion process (eqs. (1) and (2)), and computing the loss (eq. (3)). To enable sampling with progressive noise levels in eqs. (7) and (8), we simply switch to per-frame training noise levels t0:F 1. In our experiment, we observed that, similar to the sampling noise levels τ 0:S in eq. (7), training on simple linear noise schedule yielded satisfactory results for all reported experiments. During training, the noise levels is perturbated by random shift δ to preserve the coverage of the full diffusion timestep range [0, ) (Song et al., 2020a). δ = 0.4ϵ(ti ti+1), ϵ (0, I) is randomly sampled for each training iteration and remains constant for all t0:F 1 within that iteration."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "In this section, we follow our terminology as discussed and defined in Sec. 2.2, including the replacement-with-noise method, the replacement-without-noise method, and attention window. 4.1 IMPLEMENTATION Our models and baseline models We implement our progressive autoregressive video diffusion models by fine-tuning from pre-trained models. Specifically, we use two video diffusion models based on the diffusion transformer architecture (Peebles & Xie, 2023; Brooks et al., 2024): Open-Sora v1.2 (Zheng et al., 2024) (denoted as O) and modified variant of Open-Sora (denoted as in later experiments). Both models are latent video diffusion models (Blattmann et al., 2023b), each utilizing corresponding 3D VAE that encodes 17 (O) or 16 (M) video frames into 5 latent representations. generates videos at 240424 resolution 24 FPS with 30 sampling steps. produces results at 176320 resolution 24 FPS with 50 sampling steps. Based on and M, we also implement two baseline autoregressive video generation methods, replacement-with-noise (denoted as RW) and replacement-without-noise (denoted as RN) (Sec. 2.2), to compare with our proposed progressive autoregressive (denoted as PA) video generation method (Sec. 3). We train on our progressive noise levels, as discussed in Sec. 3.3. The resulting model can perform progressive autoregressive video generation, which we denote as PA-M. We also train with the replacement-with-noise method, which we will denote as RW-M. Starting from the same pre-trained weight of the base model, RW-M is trained for 3 times more training steps compared to PA-M. undergoes masked pre-training (Zheng et al., 2024), where the masked latents x0 0:E1 are clean without any added noise (Zheng et al., 2024). This allows the base model to perform autoregressive video generation with the replacement-without-noise method. We denote this model as RN-O-base. Such training also allows to learn that the noise levels t0:F 1 can be independent with respect to the latent frames and thus enables our progressive autoregressive video denoising sampling procedure (Sec. 3.2 and Alg. 1) to work training-free. We denoise this model as PA-O-base. Training details We train and on captioned image and video datasets, containing 1 million videos and 2.3 billion images. These data are licensed and have been filtered to remove low-quality 6 content. We train PA-M on video clips of 16, 32, ..., 176 frames that correspond to = 5, 10, ..., 55 latents. The = 55 attention window length is derived by setting = + 5, where = 50 is the number of sampling steps in (S = 30 in O) and 5 is the length of an additional chunk of latent frames. The advantage of such design is described in the below Keeping clean frames available in temporal self-attention paragraph. The shorter latent frame lengths = 5, 10, ..., 50 are used for the variable length training, as discussed in Sec. 3.2. RW-M is trained on videos of 64 frames that corresponds to = 20 latents. Modification to the base model To implement progressive autoregressive video diffusion models on top of their pre-trained foundation video diffusion models, we do not need to modify the base model architectures. Instead, we only need to modify the models forward, training, and inference procedures. In the training and inference procedures, we replace the single noise level [0, ) from regular diffusion model training (Ho et al., 2020; 2022b) with our per-frame noise level t0:F 1 and τ 0:S1 (Secs. 3.1 and 3.3). To accommodate this change, we only need to make single modification to the the noise level embedding computation in the models forward procedure. While the regular timestep only has the batch size dimension B, our progressive timesteps has two dimensions B, . We first flatten them into the batch dimension of size , pass it to the timesteps embedding module, unflatten the two dimensions, and finally broadcast the timestep embedding to the same shape of the latents so they can be combined through either addition, concatenation, modulation, or cross-attention (Perez et al., 2018; Vaswani et al., 2017; Peebles & Xie, 2023). Chunk-by-chunk denoising 3D VAE (Zheng et al., 2024) usually encode and decode video latent frames chunk-by-chunk. In our early experiments on PA-M and PA-O with = S, we find that there is serious cumulative error when all latents are given different noise levels and the attention window is shifted by one latent at time, causing the generated videos to diverge quickly after few seconds, as shown in Ablation 2 in Fig. 5. We resolve the problem by treating each chunk of latents as single latent: they are assigned with the same noise level, and are added and removed from the attention window together. In other words, for the 3D VAE chunk size of = 5 latents in and O, we shift the attention window by latents every sampling steps. Effectively, the latents that belong to the same chunk always have the same noise level and are added to or removed from the attention window together. Our ablation experiments on both and show that the chunked training and inference substantially improves the generation result. Keeping clean frames available in temporal self-attention The default design of our progressive autoregressive video denoising process presented in Sec. 3.2 results in temporal jittering. This is because the clean latents x0 0:C1 are immediately removed from the attention window; as the later latents cannot attend to the previous clean latents, it is hard for the model to denoise the later latents to be perfectly temporally consistent with the previous clean latents. In practice, we always keep chunk of clean latents, e.g., = 5 mentioned ahead in our training details, in front of all the noisy latents, extending the attention window lengths from 50 to 55 and from 30 to 35 for and respectively. This helps resolving the frame-to-frame discontinuity issue. 4.2 LONG VIDEO GENERATION Baselines As discussed in Sec. 4.1, using our base models, we implement two baseline autoregressive video generation methods on three models, which are denoted as RW-M, RN-O-base, and RN-O. We also compare to Stable Video Diffusion (SVD) (Blattmann et al., 2023a) and StreamingT2V (Henschel et al., 2024) model families. Specifically, we consider the SVD-XT model from SVD, image-to-video model that generates short video clip of 25 frames at 576x1024 resolution given an conditioning image. We apply it autoregressively, using the last image of the previous clip as the condition for generating new clip. This is equivalent to the replacement-without-noise method except that it only conditions on single frame rather than chunk of 17 frames as RN-O. We also consider the StreamingSVD model from StreamingT2V, image-to-long-video generation model that uses SVD as the base model (Henschel et al., 2024); its autoregressive video generation is enabled by training additional modules that connect to the base model via cross-attention. Similar to our progressive autoregressive video diffusion models, StreamingSVD can autoregressively generate long videos at 720x1280 resolution with arbitrary lengths, which we set to 1440 frames. 7 Testing set Our testing set consists of 40 text prompts and the corresponding real videos, sampled from Sora (Zheng et al., 2024) demo videos, MiraData (Ju et al., 2024), UCF-101 (Soomro, 2012), and LOVEU (Wu et al., 2023a;b). For each text prompt, we generate two videos with 1440 frames, 60 seconds long at 24 FPS, resulting in total of 80 videos. We use these 80 videos from each model for both quantitative and qualitative results, unless specified otherwise. Due to computation resource limitations of sampling 1-minute long videos, we only obtained partial results from M-PA and StreamingSVD, including 48 videos from 24 text prompts and 40 videos from 40 text prompts respectively. This testing set measures the zero-shot long video generation ability of the models, since none of them are specifically trained on any of the above datasets. Since our focus is on long video generation, we focus on the video extension capability of the models rather than the text-to-short-video generation capability. Thus, we use the initial frames of the videos as the condition for all models, similar to the setting in (Henschel et al., 2024). M, (Zheng et al., 2024), StreamingSVD (Henschel et al., 2024), and SVD-XT (Blattmann et al., 2023a) use 16, 17, 1, and 1 frames from the real video as the initial condition. Metrics We consider 6 metrics in VBench (Huang et al., 2024): subject consistency, background consistency, motion smoothness, dynamic degree, aesthetic quality, and imaging quality. We compute average metrics using VBench-long, where each metric is computed on 30 2-second clips for each 60-second video; for subject and background consistency, clip-to-clip metric is considered in addition to the average metric over the clips. We also show how the metrics vary over time by plotting the metrics over the 30 2-second clips averaged over the 80 60-second videos. Similar to Henschel et al. (2024), we also use the Adaptive Detector algorithm from PySceneDetect (PySceneDetect) to count the number of detected scene changes, where Num Scenes = 1 means that there is no scene change detected. Quantitative Results We present the average metrics for each model in Sec. 4.2. The metrics are averaged over all the videos that each model generates from our testing set described above. In Fig. 3, we also illustrate the trend of metrics over the 1-minute duration of videos for each model. We find that subject consistency, background consistency, and motion smoothness metrics are not discriminative, as all models have obtained similar results on these metrics on average and can also maintain them over the 1-minute duration. We summarize the quantitative results on the remaining metrics of each model below. Table 1: Quantitative comparison of our progressive autoregressive video generation (PA) and two baseline methods replacement-with-noise (RW) and replacement-without-noise (RN) on two base models (M and O), StreamingT2V (Henschel et al., 2024), and Stable Video Diffusion (SVD) (Blattmann et al., 2023a). Subject Background Motion Dynamic Aesthetic Imaging Num Consistency Consistency Smoothness Degree Quality Quality Scenes PA-M (ours) RW-M PA-O-base (ours) RN-O-base StreamingSVD SVD-XT 0.7923 0.8001 0.7656 0. 0.8172 0.6102 0.8964 0.8851 0.8880 0.8820 0.8916 0. 0.9896 0.9836 0.9859 0.9873 0.9929 0.9724 0.8000 0.3958 0.5625 0. 0.4726 0.4123 0.4582 0.4034 0.5927 0.5961 0.5033 0.4464 0.65 0. 0.5566 0.9875 0.3019 0.4814 1.75 1.10 2.04 5. 1.08 2.10 Our PA-M obtains the best overall quality; on average, it has substantially better dynamic degree than all except SVD-XT, the best aesthetic and imaging quality, and only slightly worse number of scenes than RW-M and StreamingSVD; it is able to best maintain the level of dynamic degree, aesthetic and imaging quality over the 1-minute duration. Although the baseline RW-M has nearly perfect number of scenes 1, it has poor dynamic degree that degrades substantially after few seconds, where most of its generated videos are static; furthermore, its aesthetic and image qualities also degrades over the 1-minute duration. 8 Figure 3: VBench (Huang et al., 2024) scores of generated videos over the 60-second duration, averaged over 80 videos from our testing set. The scores are computed on 30 2-second clips. Our models M-PA and O-PA can best maintain the level of dynamic degree, aesthetic quality, and imaging quality over time compared to other baselines. Notably, baselines that use the same model as ours, M-RW and O-RN, both exhibit substantial drop in dynamic degree, aesthetic quality, and imaging quality. Our PA-O-base achieves slightly better dynamic degree, aesthetic quality and imaging quality on average and better maintains them over time compared to RN-O-base; it also substantially reduces the number of scenes and is effectively generating longer video shots; these results are impressive given that they are obtained training-free. The other baseline RN-O-base have worse aesthetic and imaging quality than our PA-O-base both on average and over time; it also has the worst number of scenes, making it fail at the core goal of long video generation. StreamingSVD achieves good overall quality; it achieves the best number of scenes, decent dynamic degree, aesthetic quality and imaging quality and can maintain them over time; however, they worse than our PA-M on average. SVD-XT, although achieves the best dynamic degree, also has the worst aesthetic and imaging quality both on average and over time, making its results undesirable. Qualitative Results We also show strength of our method with qualitative comparison results in Fig. 4. Both of our models demonstrate strong performance in terms of frame fidelity and motion realism (e.g. camera motion, wave motion, and running gestures). PA-M outperforms PA-O due to additional fine-tuning with our proposed progressive noise levels Sec. 3.3, whereas PA-O simply inherits the pre-trained weights from Open-Sora v1.2 base model (Zheng et al., 2024). In contrast, SVD-XT (Blattmann et al., 2023a) shows severe artifacts that decreases frame validity, and StreamingSVD from StreamingT2V (Henschel et al., 2024) suffers from cumulative errors, resulting in degraded video quality as the sequence length increases. For more qualitative results, please refer to our website https://desaixie.github.io/pa-vdm/. - - b - - - - 2 - S - P - - - - 2 - S Figure 4: Qualitative comparison of PA-M (ours), RW-M, PA-O-base (ours), RN-O-base, StreamingSVD from StreamingT2V (Henschel et al., 2024), and SVD-XT from Stable Video Diffusion (Blattmann et al., 2023a). Frames are evenly sampled from 1 minute long generated video, i.e. at 10, 20, 30, 40, 50, and 60 seconds. Our models can autoregressively generate 60-second, 1440-frame videos without quality degradation. 10 F 1 t A i b Figure 5: Qualitative comparison for ablation study. Full represents for our full solution based on M-PA, Ablation 1 is with chunk-by-chunk denoising but without keeping the clean frames available in temporal self-attention. Ablation 2 is without both techniques. Frames are evenly sampled from 16-second generated videos. 4.3 ABLATION STUDY We conduct ablation studies on the PA-M model to evaluate the impact of chunk-by-chunk denoising and keeping clean frames available in temporal self-attention, as described in Sec. 4.1. Qualitative comparison has been shown in Fig. 5. In Ablation 1, we observe that the absence of clean frames in the input sequence prevents noisy frames from attending to previous clean frames, resulting in poor performance over long duration. This also causes frame-to-frame discontinuity, which is more noticeable in the supplementary anonymous webpage. In Ablation 2, not decoding the video chunk-by-chunk leads to severe cumulative errors, causing the video to diverge after only few seconds."
        },
        {
            "title": "5 RELATED WORKS",
            "content": "The field of long video generation has faced significant challenges due to the computational complexity and resource constraints associated with training models on longer videos. As result, most existing text-to-video diffusion models (Guo et al., 2023; Ho et al., 2022a;b; Blattmann et al., 2023a) have been limited to generating fixed-size video clips, which leads to noticeable degradation in quality when attempting to generate longer videos. Recent works are proposed to address these challenges through innovative approaches that either extend existing models or introduce novel architectures and fusion methods. Freenoise (Qiu et al., 2024) utilizes sliding window temporal attention to ensure smooth transitions between video clips but falls short in maintaining global consistency across long video sequences. Gen-L-video (Wang et al., 2023), on the other hand, decomposes long videos into multiple short segments, decodes them in parallel using short video generation models, and later applies an optimization step to align the overlapping regions for continuity. FreeLong (Lu et al., 2024) introduces sophisticated approach which balances the frequency distribution of long video features in different frequency during the denoising process. Vid-GPT (Gao et al., 2024) introduces GPT-style autoregressive causal generation for long videos. More recently, Short-to-Long (S2L) approaches are proposed, where correlated short videos are firstly generated and then smoothly transit in-between to form coherent long videos. StreamingT2V (Henschel et al., 2024) adopts this strategy by introducing the conditional attention and appearance preservation modules to capture content information from previous frames, ensuring consistency with the starting frames. It further enhances the visual coherence by blending shared noisy frames in overlapping regions, similar to the approach used by SEINE (Chen et al., 2023). NUWA-XL (Yin et al., 2023) leverages hierarchical diffusion model to generate long videos using coarse-to-fine approach, progressing from sparse key frames to denser intermediate frames. However, it has only been evaluated on cartoon video dataset rather than natural videos. VideoTetris (Tian et al., 2024b) 11 introduces decomposing prompts temporally and leveraging spatio-temporal composing module for compositional video generation. Another line of research focuses on controllable video generation (Zhuang et al., 2024; Tian et al., 2024a; Hu, 2024; Zhu et al., 2024) and has proposed solutions for long video generation using overlapped window frames. These approaches condition diffusion models using both frames from previous windows and signals from the current window. While these methods demonstrate promising results in maintaining consistent appearances and motions, they are limited to their specific application domains which relies heavily on strong conditional inputs."
        },
        {
            "title": "6 DISCUSSION",
            "content": "In this work, we target long video generation, fundamental challenge of current video diffusion models. We show that they can be naturally adapted to become progressive autoregressive video diffusion models without changing the architectures. With our progressive noise levels and the autoregressive video denoising process (Secs. 3.1 and 3.2), we obtain state-of-the-art results on long video generation at 1-minute long. Since our method does not require changing the model architectures, it can be seamlessly combined with many orthogonal works, paving the way for generating longer videos at higher quality, long-term dependency, and controllability. limitation of our method is the demand of well-trained base video diffusion model. Similar to the replacement methods (Ho et al., 2022b; Zheng et al., 2024) and other approaches like StreamingT2V (Henschel et al., 2024), our method autoregressively applies video diffusion model to generate long videos. Such autoregressive video generation poses huge challenge on the base video diffusion model. Some slight errors remaining in the clean latents x0 may not be noticeable in single video clip; however, in the autoregressive scenario, these error can be carried onto later frames, resulting in quality degradation. Further more, as the video diffusion model is only trained on denoising latents of real video data, it may poorly handle such distribution shift towards the generated erroneous latents (Xie et al., 2024; Fan et al., 2024), resulting in more severe quality drop. This means that even after finetuning on our progressive noise levels, our method could still generate videos with some degree of quality degradation over time if the base video diffusion model is not well trained. There are many promising future directions to extend this work. We only train on progressively increasing noise levels to reduce the space of noise levels for easier convergence. If sufficient computing resources are available, training on fully random, per-frame independent noise levels would enable single model for various tasks with arbitrary lengths, including video extension, connection, temporal super-resolution. Another promising future application of the long video generation ability of our models is to use them as world simulators, useful for tasks in robotics and 3D vision. Being able to generate long videos without quality degradation is an substantial step towards this direction."
        },
        {
            "title": "REFERENCES",
            "content": "Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, Yuanzhen Li, Michael Rubinstein, Tomer Michaeli, Oliver Wang, Deqing Sun, Tali Dekel, and Inbar Mosseri. Lumiere: space-time diffusion model for video generation, 2024. URL https://arxiv.org/abs/2401.12945. 1, 4 Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023a. 1, 3, 4, 5, 7, 8, 9, 10, 11 Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2256322575, 2023b. 3, 6 Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video 12 generation models as world simulators. 2024. URL https://openai.com/research/ video-generation-models-as-world-simulators. 2, Boyuan Chen, Diego Marti Monso, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion, 2024. URL https: //arxiv.org/abs/2407.01392. 4 Xinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin Zhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Seine: Short-to-long video diffusion model for generative transition and prediction. In The Twelfth International Conference on Learning Representations, 2023. 11 Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Reinforcement learning for finetuning text-to-image diffusion models. Advances in Neural Information Processing Systems, 36, 2024. 12 Kaifeng Gao, Jiaxin Shi, Hanwang Zhang, Chunping Wang, and Jun Xiao. ViD-GPT: introducing GPT-style autoregressive generation in video diffusion models, 2024. URL https://arxiv. org/abs/2406.10981. 1, 3, 11 Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, and Ishan Misra. Emu video: Factorizing text-to-video generation by explicit image conditioning. arXiv preprint arXiv:2311.10709, 2023. 1 Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 1, 4, Roberto Henschel, Levon Khachatryan, Daniil Hayrapetyan, Hayk Poghosyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. StreamingT2V: Consistent, dynamic, and extendable long video generation from text, 2024. URL https://arxiv.org/abs/ 2403.14773. 1, 2, 7, 8, 9, 10, 11, 12 Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 3, 4, 6, 7 Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022a. 1, 11 Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet. Video diffusion models, 2022b. URL https://arxiv.org/abs/2204.03458. 1, 2, 3, 5, 7, 11, 12 Li Hu. Animate anyone: Consistent and controllable image-to-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 81538163, 2024. Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2180721818, 2024. 8, 9 Xuan Ju, Yiming Gao, Zhaoyang Zhang, Ziyang Yuan, Xintao Wang, Ailing Zeng, Yu Xiong, Qiang Xu, and Ying Shan. Miradata: large-scale video dataset with long durations and structured captions, 2024. URL https://arxiv.org/abs/2407.06358. 8 Diederik Kingma and Max Welling. Auto-encoding variational bayes, 2013. URL https: //arxiv.org/abs/1312.6114. 3 Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling, 2023. URL https://arxiv.org/abs/2210.02747. 13 Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow, 2022. URL https://arxiv.org/abs/2209.03003. 3, 4, 6 Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, et al. Instaflow: One step is enough for high-quality diffusion-based text-to-image generation. In The Twelfth International Conference on Learning Representations, 2023. 3, 4, 6 Yu Lu, Yuanzhi Liang, Linchao Zhu, and Yi Yang. Freelong: Training-free long video generation with spectralblend temporal attention. arXiv preprint arXiv:2407.19918, 2024. Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. 2 Meta. Movie gen: cast of media foundation models, 2024. URL https://ai.meta.com/ static-resource/movie-gen-research-paper. 1, 3, 4 William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. 2, 6, 7 Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with general conditioning layer. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. PySceneDetect. PySceneDetect. https://www.scenedetect.com/. Accessed: 2024-10-10. 8 Haonan Qiu, Menghan Xia, Yong Zhang, Yingqing He, Xintao Wang, Ying Shan, and Ziwei Liu. FreeNoise: tuning-free longer video diffusion via noise rescheduling, 2024. URL https:// arxiv.org/abs/2310.15169. 2, 3, 5, 11 Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pp. 234241. Springer, 2015. 2 Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pp. 22562265. PMLR, 2015. 3 Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020a. 6 Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021. URL https://openreview.net/ forum?id=St1giarCHLP. 3, 4 Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020b. 3 Soomro. UCF101: dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. 8 Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo. Emo: Emote portrait alive-generating expressive portrait videos with audio2video diffusion model under weak conditions. arXiv preprint arXiv:2402.17485, 2024a. Ye Tian, Ling Yang, Haotian Yang, Yuan Gao, Yufan Deng, Jingmin Chen, Xintao Wang, Zhaochen Yu, Xin Tao, Pengfei Wan, et al. Videotetris: Towards compositional text-to-video generation. arXiv preprint arXiv:2406.04277, 2024b. 11 14 Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2017/ 2017. file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf. 7 Fu-Yun Wang, Wenshuo Chen, Guanglu Song, Han-Jia Ye, Yu Liu, and Hongsheng Li. GenL-Video: multi-text to long video generation via temporal co-denoising, 2023. URL https: //arxiv.org/abs/2305.18264. 2, 11 Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 76237633, 2023a. 8 Jay Zhangjie Wu, Xiuyu Li, Difei Gao, Zhen Dong, Jinbin Bai, Aishani Singh, Xiaoyu Xiang, Youzeng Li, Zuwei Huang, Yuanxi Sun, Rui He, Feng Hu, Junhua Hu, Hai Huang, Hanyu Zhu, Xu Cheng, Jie Tang, Mike Zheng Shou, Kurt Keutzer, and Forrest Iandola. Cvpr 2023 text guided video editing competition, 2023b. Desai Xie, Jiahao Li, Hao Tan, Xin Sun, Zhixin Shu, Yi Zhou, Sai Bi, Soren Pirk, and Arie Kaufman. Carve3d: Improving multi-view reconstruction consistency for diffusion models with rl finetuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 63696379, 2024. 12 Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang, Xiaodong Wang, Minheng Ni, Zhengyuan Yang, Linjie Li, Shuguang Liu, Fan Yang, et al. Nuwa-xl: Diffusion over diffusion for extremely long video generation. arXiv preprint arXiv:2303.12346, 2023. 11 Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-Sora: democratizing efficient video production for all, March 2024. URL https://github.com/hpcaitech/Open-Sora. 1, 2, 3, 5, 6, 7, 8, 9, 12 Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Yinghui Xu, Xun Cao, Yao Yao, Hao Zhu, and Siyu Zhu. Champ: Controllable and consistent human image animation with 3d parametric guidance. arXiv preprint arXiv:2403.14781, 2024. 12 Shaobin Zhuang, Kunchang Li, Xinyuan Chen, Yaohui Wang, Ziwei Liu, Yu Qiao, and Yali Wang. Vlogger: Make your dream vlog. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 88068817, 2024."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "Stony Brook University"
    ]
}