{
    "paper_title": "ALICE-LRI: A General Method for Lossless Range Image Generation for Spinning LiDAR Sensors without Calibration Metadata",
    "authors": [
        "Samuel Soutullo",
        "Miguel Yermo",
        "David L. Vilariño",
        "Óscar G. Lorenzo",
        "José C. Cabaleiro",
        "Francisco F. Rivera"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "3D LiDAR sensors are essential for autonomous navigation, environmental monitoring, and precision mapping in remote sensing applications. To efficiently process the massive point clouds generated by these sensors, LiDAR data is often projected into 2D range images that organize points by their angular positions and distances. While these range image representations enable efficient processing, conventional projection methods suffer from fundamental geometric inconsistencies that cause irreversible information loss, compromising high-fidelity applications. We present ALICE-LRI (Automatic LiDAR Intrinsic Calibration Estimation for Lossless Range Images), the first general, sensor-agnostic method that achieves lossless range image generation from spinning LiDAR point clouds without requiring manufacturer metadata or calibration files. Our algorithm automatically reverse-engineers the intrinsic geometry of any spinning LiDAR sensor by inferring critical parameters including laser beam configuration, angular distributions, and per-beam calibration corrections, enabling lossless projection and complete point cloud reconstruction with zero point loss. Comprehensive evaluation across the complete KITTI and DurLAR datasets demonstrates that ALICE-LRI achieves perfect point preservation, with zero points lost across all point clouds. Geometric accuracy is maintained well within sensor precision limits, establishing geometric losslessness with real-time performance. We also present a compression case study that validates substantial downstream benefits, demonstrating significant quality improvements in practical applications. This paradigm shift from approximate to lossless LiDAR projections opens new possibilities for high-precision remote sensing applications requiring complete geometric preservation."
        },
        {
            "title": "Start",
            "content": "ALICE-LRI: General Method for Lossless Range Image Generation for Spinning LiDAR Sensors without Calibration Metadata Samuel Soutulloa,, Miguel Yermoa,b, David L. Vilariñob, Óscar G. Lorenzoa,b, José C. Cabaleiroa,b, Francisco F. Riveraa,b aCentro Singular de Investigación en Tecnoloxías Intelixentes (CiTIUS), Rúa de Jenaro de la Fuente Domínguez, Santiago de Compostela, 15782, Coruña, Spain bDepartamento de Electrónica Computación, Universidade de Santiago de Compostela, Rúa Lope Gómez de Marzoa, Santiago de Compostela, 15782, Coruña, Spain 5 2 0 2 3 2 ] . [ 1 8 0 7 0 2 . 0 1 5 2 : r Abstract 3D LiDAR sensors are essential for autonomous navigation, environmental monitoring, and precision mapping in remote sensing applications. To efficiently process the massive point clouds generated by these sensors, LiDAR data is often projected into 2D range images that organize points by their angular positions and distances. While these range image representations enable efficient processing, conventional projection methods suffer from fundamental geometric inconsistencies that cause irreversible information loss, compromising high-fidelity applications. We present ALICE-LRI (Automatic LiDAR Intrinsic Calibration Estimation for Lossless Range Images), the first general, sensor-agnostic method that achieves lossless range image generation from spinning LiDAR point clouds without requiring manufacturer metadata or calibration files. Our algorithm automatically reverse-engineers the intrinsic geometry of any spinning LiDAR sensor by inferring critical parameters including laser beam configuration, angular distributions, and per-beam calibration corrections, enabling lossless projection and complete point cloud reconstruction with zero point loss. Comprehensive evaluation across the complete KITTI and DurLAR datasets demonstrates that ALICE-LRI achieves perfect point preservation, with zero points lost across all point clouds. Geometric accuracy is maintained well within sensor precision limits, establishing geometric losslessness with real-time performance. We also present compression case study that validates substantial downstream benefits, demonstrating significant quality improvements in practical applications. This paradigm shift from approximate to lossless LiDAR projections opens new possibilities for high-precision remote sensing applications requiring complete geometric preservation. Keywords: LiDAR, Lossless Range image, Sensor calibration, Point cloud reconstruction, Parameter estimation 1. Introduction 3D LiDAR sensors have become key component of modern perception systems, enabling accurate mapping [1, 2], navigation [3, 4, 5], and scene understanding in autonomous vehicles [6, 7, 8], as well as in mobile robotics [9, 10]. widely used class of LiDARscommonly referred to as rotating or spinning LiDAR sensorsoperates by continuously rotating set of vertically aligned laser beams. As the sensor spins, each beam sweeps out horizontal arc, producing structured 3D scans of the surrounding environment. To efficiently process the resulting point clouds, it is common to project them onto 2D range images: dense, grid-like structures where each pixel encodes the distance (range) to the nearest surface along fixed direction. This projection follows spherical pattern defined by the sensor geometry, mapping azimuthal angles to horizontal image coordinates and elevation angles to vertical coordinates. Range images are widely used in object detection [11], semantic segmentation [12], upsampling [13], and point cloud compression [14], as they enable Corresponding author. Postal address: Centro Singular de Investigación en Tecnoloxías Intelixentes (CiTIUS), Rúa de Jenaro de la Fuente Domínguez, 15782 Santiago de Compostela, Coruña. Email address: s.soutullo@usc.es (Samuel Soutullo) efficient computation through their regular grid structure and compatibility with established image processing techniques. This 3D-to-2D projection is often treated as bijective, under the assumption that each point corresponds to unique combination of range and angular directions. In ideal spinning LiDAR sensors this assumption holds, as the vertical and horizontal angles take discrete values defined by the beam layout and the rotational sampling rate. This enables exact reconstruction of each 3D point from its position and value in the range image. However, this model does not accurately reflect the behavior of real sensors, whose laser beams originate from slightly different positions and orientations due to mechanical constraints. These variations are typically compensated through factory calibration, which adjusts the measured ranges and angles to compute accurate 3D coordinates. However, the presence of this calibration breaks the assumptions of the spherical projection model, leading to projection mismatches, pixel collisions, and quantization artifacts when computing range images. In downstream applications, such artifacts are often ignored or mitigated employing ad-hoc techniques. However, in tasks that demand high geometric fidelity even small projection errors can accumulate and degrade performance. detailed analysis by Wu et al. [15] demonstrates that standard projection strategies introduce measurable distortions, highlighting the need for more accurate range image generation. While previous work has proposed models that account for laser-specific offsets [16], these approaches rely on manufacturer-supplied lookup tables (LUTs) or packet-level data, limiting their generality. Despite the fundamental importance of this problem for LiDAR-based applications, the challenge of generating truly lossless range images from calibrated point clouds without sensor metadata has been largely overlooked in the literature. To the best of our knowledge, this work presents the first general solution that enables lossless range image generation directly from calibrated point clouds in the absence of sensor metadata. We introduce ALICE-LRI (Automatic LiDAR Intrinsic Calibration Estimation for Lossless Range Images), method that infers key geometric parameters of spinning LiDAR sensors directly from point clouds without requiring factory metadata. These inferred parameters are then used to generate range images that are fully consistent with the intrinsic geometry of the sensor and allow exact reconstruction of the original point cloud. The result is sensor-agnostic, lossless projection algorithm suitable for any application where geometric fidelity is essential. To facilitate adoption, we publish ALICE-LRI as an open-source C++/Python library.1 The key contributions of this work are: The formulation of the fundamental problem of lossless range image generation along with an analysis of how real-world sensor geometry breaks conventional projection methods. ALICE-LRI, to the best of our knowledge, the first method to achieve lossless range image generation from calibrated spinning LiDAR point clouds without requiring manufacturer metadata or calibration files by automatically inferring sensor intrinsics directly from raw data. comprehensive evaluation on the KITTI and DurLAR datasets demonstrating the geometric losslessness of our approach, and validation of practical downstream benefits through compression case study. The remainder of the paper is organized as follows. Section 2 reviews relevant prior work on range image applications and LiDAR calibration. Section 3 formulates the lossless range image generation problem and analyzes the gap between ideal and real-world sensor models. Section 4 presents the ALICELRI method for automatic parameter estimation and lossless projection. Section 5 describes the experimental methodology, datasets, and evaluation metrics. Section 6 reports comprehensive evaluation results, including parameter estimation accuracy, reconstruction quality, and runtime performance. Section 7 demonstrates the application and practical benefits of ALICE-LRI through point cloud compression case study. Finally, Section 8 concludes the paper, and Section 9 outlines future research directions. 1https://github.com/alice-lri/alice-lri 2 2. Related Work This section reviews prior work across two fronts: the downstream use cases that rely on range image projections highlighting the role of projection accuracy in eachand the body of research on LiDAR sensor geometry and calibration. 2.1. Range Image Use Cases Segmentation. Segmentation pipelines frequently rely on range images to reduce computation and exploit spatial regularities. Classical methods such as the one proposed by Bogoslavskyi and Stachniss [17] perform geometric clustering on cylindrical projections. More recent approaches like SqueezeSeg [12] and RangeNet++[8] apply convolutional neural networks (CNNs) directly to spherical range images, leveraging their grid structure for efficient inference. RangeNet++, in particular, includes specialized post-processing module to correct range image discretization errors and smooth out the blurry outputs introduced by CNN inference. Other works explicitly tackle the limitations of the projection process itself: Triess et al. [18] introduce scan unfolding strategy that reconstructs laser beam structure from point clouds; however, this approach is ad-hoc for the KITTI dataset, as it leverages the known order of points in the files. Kong et al. [19] identify common artifactssuch as many-to-one conflicts and pixel holesthat arise from discretization and quantization. These efforts highlight the sensitivity of segmentation to projection fidelity and the reliance of current methods on dataset-specific heuristics or other strategies that do not generalize across sensors. Object Detection. Range images are widely used for efficient 3D object detection, enabling the use of fast 2D CNNs in place of computationally expensive 3D convolutions. Early work projected LiDAR scans onto cylindrical grids encoding distance and height [20], while other methods fused range view with birds-eye and RGB imagery for enhanced proposal generation [21]. Detection pipelines such as LaserNet [11] and FVNet [22] perform inference directly in the range domain. However, object detection models often overlook the impact of projection fidelity on detection quality, despite facing similar challenges to segmentation. This suggests an opportunity to improve detection robustness by adopting more principled range image generation techniques. Upsampling. Range images often also serve as the backbone for LiDAR super-resolution, where low-density scans are upsampled into denser point clouds. Some methods follow 3D 2D3D pipeline: sparse scans are projected to 2D, enhanced via neural models, and then backprojected [13, 23, 24], despite the 3D2D3D pipeline being prone to introducing quantization artifacts. Some recent approaches have begun to question this dependency on discrete projections [25], but most still assume access to consistent and accurate mapping between 3D points and 2D pixels. Odometry and Localization. Range images also play central role in LiDAR-based odometry and localization systems, where they serve as structured intermediates for geometric alignment and pose estimation. Behley et al. [5] proposed surfel-based SLAM system that leverages dense range images to enable efficient frame-to-model ICP registration. Similarly, Chen et al. [3] perform pixel-wise comparisons between live and synthetic range images rendered from mesh map, enabling GPSfree localization via Monte Carlo inference. More recently, Lichtenfeld et al. [4] introduced real-time odometry pipeline for dynamic environments that projects both point clouds and residuals onto cylindrical range images. This allows rapid segmentation and tracking of moving objects, which are then filtered out to maintain clean static map. Across these works, range images consistently boost efficiency and robustness, but their performance hinges on projection quality. Compression. significant family of LiDAR compression methods relies on 2D range images to exploit spatial and temporal redundancies inherent in the structured representation. Projection fidelity is especially vital in this class of methods, where information loss directly affects point cloud reconstruction. Early systems [26] encoded raw packet data into structured range images, enabling efficient compression via JPEG or MPEG. Later methods improved spatial and temporal redundancy exploitation by modeling motion or beam trajectories [27, 28]. Other approaches segment range images into planar patches or predict inter-frame deltas to compress efficiently while preserving geometry [29, 30]. Recent methods like RIDDLE [14] rely on neural predictors and delta encoding in the range domain. Yet all of these methods assume that the input range image is faithful representation of the 3D scana violated condition when projecting from calibrated point clouds using classical methods. As result, authors either sidestep the issue by operating on raw packet data (when available), or accept minor geometric distortion. While the preceding use cases highlight the importance of accurate range image projections, achieving this fidelity requires knowledge of the intrinsic geometry of the sensor. To better understand this limitation, we briefly review existing work on LiDAR sensor geometry and calibration. 2.2. LiDAR Sensor Geometry and Calibration Due to manufacturing tolerances and mechanical constraints, real-world LiDAR sensors exhibit geometric nonidealities. These include spatial offsets, angular deviations, and range biases that deviate from theoretical design specifications. LiDAR calibration is well-established research area concerned with accurately estimating these intrinsic geometric parameters. These parameters are incorporated into the computation of 3D point cloud coordinates, ensuring that output point clouds accurately represent real-world geometry. Early methods such as [31] introduced optimization-based procedures to refine beam orientations and offsets by minimizing point-to-plane distances across multiple scans of structured environments. More extensive modelssuch as the one proposed in [32] for the Velodyne HDL-64Eincorporated up to 3 six parameters per laser and used planar least-squares adjustments to significantly reduce residual errors and reveal systematic biases across the sensor array. Later, Glennie et al. [33] evaluated the factory calibration and long-term stability of the Ouster VLP-16. They showed that even modest improvements in calibration yield noticeable gains in accuracy, though with limited reusability over time. Complementary work by Chan and Lichti [34] extended calibration to use both planar and cylindrical environmental features for the Velodyne HDL-32E. They explicitly modeled additional horizontal angle and range offsets to better capture beam-specific distortions. Critically, existing calibration research treats this as forward problem: estimating parameters from raw sensor data to produce geometrically accurate point clouds. However, in many practical scenarios, LiDAR sensors output alreadycalibrated point cloud coordinates with the calibration parameters embedded in the data processing pipeline. Similarly, most publicly available datasets provide only these final calibrated point clouds with raw data discarded. ALICE-LRI addresses the complementary inverse problem: given calibrated point cloud, we reverse-engineer the calibration parameters that were applied during its generation. This inverse perspective enables lossless range image generation for many real-world scenarios that lack access to raw sensor packets or calibration metadata. This is fundamental capability that has received little attention despite its practical importance. 3. Problem Formulation This section formulates the core challenge of projecting LiDAR point clouds into 2D range images while preserving all original information. We establish the mathematical foundations by examining an ideal sensor model that enables perfect lossless projection, then analyze how real-world sensor geometry breaks this idealized framework, and finally identify the key parameters that must be recovered to restore losslessness. 3.1. Range Image Representation Range images provide structured 2D representation of LiDAR scans by mapping spherical coordinates (r, θ , ϕ) to regular grid, where each pixel (u, v) encodes the range along discretized angular direction. The projection assumes spherical sensor model with laser beams at fixed elevation angles and uniformly sampled azimuth angles per revolution, creating mapping between 3D points and image pixels. lossless projection requires that this mapping is bijective: each pixel corresponds to unique angular direction, and each measured point maps to exactly one pixel. Figure 1 illustrates this discretization of the sensing sphere into dense grid structure. Note that projection fidelity depends critically on how accurately the assumed sensor model reflects the true device geometry. To understand when lossless projection is theoretically possible, we first examine an ideal sensor model that perfectly satisfies the spherical assumption. coordinates of each point as defined by Equation (3). Then, map each tuple (ri, ϕi, θi) to pixel (ui, vi) in the image using the indices (l, h) of its associated beam angles as (cid:40)ui (cid:66) vi (cid:66) such that θi = θ (h) such that ϕi = ϕ (l). (4) The corresponding pixel at position (ui, vi) is assigned the value ri. Pixels for which no point is present are marked with sentinel value (e.g., zero or NaN) to indicate absence of measurement. This ideal model demonstrates that lossless projection is theoretically achievable when sensor geometry exactly matches the spherical assumption. However, real sensors deviate significantly from this idealized framework. Figure 1: LiDAR point cloud projection onto 2D range image grid based on discretized angular coordinates. 3.3. Real-World Sensor Model 3.2. Ideal Sensor Model Under ideal conditions, spinning LiDAR sensor exhibits perfect geometric regularity that enables exact lossless projection. We define an ideal sensor as having laser beams, all originating from common sensor origin, with each beam oriented at vertical angle ϕ (l) where {0, . . . , 1}. During rotation, the sensor collects uniformly spaced azimuthal samples per revolution, where each sample {0, . . . , 1} corresponds to an azimuthal angle θ (h) = 2π. (1) For every pair (l, h), the sensor measures range value r(l,h) corresponding to the distance to the first obstacle encountered in the direction defined by the beam. Under this model, point pi = (xi, yi, zi) captured by the laser beam at horizontal sample is uniquely determined from its corresponding triplet (r(l,h), ϕ (l), θ (h)), such that xi = r(l,h) cos(ϕ (l)) cos(θ (h)) yi = r(l,h) cos(ϕ (l)) sin(θ (h)) zi = r(l,h) sin(ϕ (l)). (2) Conversely, given point pi = (xi, yi, zi), its spherical coordinates are computed as Practical LiDAR sensors exhibit geometric non-idealities that fundamentally break the lossless projection capability. Manufacturing tolerances and mechanical constraints cause each laser beam to originate from slightly different physical locations, introducing spatial offsets relative to the sensor origin. Additionally, beams may exhibit unique range deviations due to mechanical and optical factors, and azimuthal offsets from the nominal rotation angle. These non-idealities render the equations from the ideal model (Equation (2)) insufficient. In response, manufacturers typically define device-specific calibration models, which vary in format but share common structure: point coordinates depend on beam-specific angles, positional offsets, and range corrections. , o(l) ), an azimuthal offset θ (l) We adopt general formulation that captures these factors. Each laser beam is characterized by five key parameters, as illustrated in Figure 2: vertical angle ϕ (l), spatial offsets (o(l) off , and horizontal resolution H(l) representing the number of azimuth samples per revolution for beam l. Although many practical sensors employ uniform horizontal resolution across all beams, we allow beamspecific resolutions for generality. Spatial corrections along the forward-backward axis are omitted because they can be absorbed into the horizontal and vertical offsets (o(l) ), affecting only the measured range ri without influencing the angular computations central to our projection method. Without loss of generality, we assume that beam indices are ordered by increasing vertical angle, , o(l) (cid:113) ri = + y2 x2 (cid:18) zi ri (cid:18) yi xi + z2 (cid:19) (cid:19) . ϕi = arcsin θi = arctan Since ϕi and θi in the ideal model are always drawn from discrete sets of known angles, they must satisfy ϕi {ϕ (l)}, and θi {θ (h)}, i. The range ri can take any real, positive value. Under these conditions, generating lossless range image of size is straightforward. First, compute the spherical 4 ϕ (0) < ϕ (1) < < ϕ (L1). (5) (3) Under this calibrated model, 3D point pi = (xi, yi, zi) measured by beam includes additional residual terms: (cid:17) xi = r(l,h(l)) cos yi = r(l,h(l)) cos (cid:16) ϕ (l) + ϕ (l,h(l)) (cid:16) ϕ (l) + ϕ (l,h(l)) (cid:16) ϕ (l) + ϕ (l,h(l)) (cid:17) zi = r(l,h(l)) sin res res res (cid:17) (cid:16) θ (h(l)) + θ (l) cos (cid:16) θ (h(l)) + θ (l) sin (cid:17) res off + θ (l,h(l)) (cid:17) off + θ (l,h(l)) res , (6) z"
        },
        {
            "title": "Real Beam\nIdeal Beam",
            "content": "ϕ (l,1) res ϕ (l,2) res o(l) ϕ (l) ϕ2 ϕ (a) Side view of LiDAR sensor illustrating two laser beams: an ideal beam (blue, dashed) originating from the sensors origin, and real beam (red, solid) with an offset origin. The elevation angles along the ideal beam (ϕ (l)) remain constant with respect to the origin of coordinates, while those along the real beam vary (ϕ1 (cid:44) ϕ2) due to the offset o(l) that introduces residual angles ϕ (l,h(l)) and ϕ2 = ϕ (l) + ϕ (l,2) . res such that ϕ1 = ϕ (l) + ϕ (l,1) res res θ (l,h(l)) res θ (l) off h(l) = 2 o(l) θ (h(l)) h(l) = 1 h(l) = 0 h(l) = H(l)"
        },
        {
            "title": "Real Beam\nIdeal Beam",
            "content": "(b) Top view of LiDAR sensor illustrating two laser beams: an ideal beam (blue, dashed) originating from the sensors center, and real beam (red, solid) with horizontal offset o(l) and an azimuthal offset θ (l) off . The azimuthal angles along the ideal beam remain constant with respect to the origin of coordinates (θ (h(l))), while those along the real beam vary due to the offset that introduces residual angles θ (l,h(l)) . Further, the azimuthal offset θ (l) off shifts the angle of the real beam by constant amount. res Figure 2: Illustration of geometric differences between the ideal sensor model (perfectly aligned beams from common origin) and the real-world model (beams with spatial and angular offsets), highlighting the calibration parameters required for lossless projection. where the residual angles ϕ (l,h(l)) the spatial offsets (o(l) , o(l) ) such that res and θ (l,h(l)) res are derived from (cid:33) res ϕ (l,h(l)) θ (l,h(l)) res (cid:32) (cid:32) = arcsin = arcsin o(l) r(l,h(l)) o(l) ρ (l,h(l)) (cid:33) , (7) where ρ (l,h(l)) is the distance in the XY plane of point pi, measured by beam at horizontal sample h(l), defined as ρ (l,h(l)) = (cid:113) + y2 x2 = r(l,h(l)) cos (cid:16) ϕ (l) + ϕ (l,h(l)) res (cid:17) . (8) The azimuthal sampling angles θ (h(l)) are defined similarly to the ideal model, but with beam-specific horizontal resolutions H(l) such that θ (h(l)) = h(l) H(l) 2π. (9) Because of these additional corrections, using the point coordinates to compute the spherical angles ϕi and θi as shown in Equation (3), no longer yields results in the discrete sets {ϕ (l)} and {θ (h(l))} defined in the ideal model. Instead, they are continuous values defined as: ϕi = ϕ (l) + ϕ (l,h(l)) θi = θ (h(l)) + θ (l) res off + θ (l,h(l)) res (10) . or to empty pixels appearing where measurements should exist (holes). These effects fundamentally prevent standard projection strategies from achieving losslessness. 3.4. Lossless Range Image Generation off , H(l)}. The prior discussion highlights that the main obstacle to lossless projection is the presence of unknown beam-specific parameters {ϕ (l), o(l), θ (l) If known, these values can invert the calibration transformations applied to the observed spherical angles, recovering the discrete angular sets required for bijective mapping. In particular, given point pi measured by beam at horizontal sample h(l), it is possible to compute the corrected angles (ϕ ) using the observed angles (ϕi, θi) and the residual terms from Equation (7) as: , θ = ϕi ϕ (l,h(l)) ϕ = θi θ (l) θ res off θ (l,h(l)) res (11) . {θ (h(l))}. This coni {ϕ (l)} and θ With these corrections, ϕ dition guarantees bijective mapping and enables lossless projection. Moreover, the range image dimensions are determined by (vertical resolution) and the least common multiple of all H(l) (horizontal resolution). Consequently, recovering these sensor-specific parameters is essential for lossless range image generation. This motivates our approach: we propose to infer the factory-calibrated geometric parameters directly from point cloud data, enabling the generation of range images that are intrinsically consistent with sensor geometry. As result, even under the best circumstances, point will not be projected to the exact center of its corresponding pixel, reducing geometric precision. This misalignment often leads to multiple points being projected onto the same pixel (collisions), 4. ALICE-LRI To solve the problem formulated in Section 3.4, we present ALICE-LRI, general method that automatically recovers the 5 Figure 3: General outline of ALICE-LRI for estimating LiDAR intrinsic parameters and generating lossless range images. Gray diamond-shaped nodes represent data inputs and outputs; rectangular nodes represent processing steps. Solid lines denote execution flow, while dashed lines indicate data flow. The method first estimates sensor parameters from the input point cloud, then uses these parameters to generate range images from subsequent point clouds. sensor parameters needed for the angle corrections in Equation (11). The method operates directly on point cloud data without requiring sensor metadata or scene knowledge. As illustrated in Figure 3, ALICE-LRI comprises two main stages: parameter estimation and lossless projection. In the first stage, the intrinsic parameters are automatically estimated from the point cloud data, including the number of laser beams L, the vertical angles ϕ (l), the vertical offsets o(l) , the horizontal angular resolutions H(l), the horizontal offsets , and the azimuthal offsets θ (l) o(l) In the second stage, the off . estimated parameters are used to project the point cloud onto range image. Since parameter estimation needs to be performed only once per device, the recovered parameters can be subsequently reused to generate range images from multiple point clouds acquired with the same hardware. The core of the method lies in the parameter estimation stage, which is further subdivided into two phases: vertical parameter estimation and horizontal parameter estimation. The vertical phase determines the number of beams L, assigns points to each beam, and simultaneously estimates ϕ (l) and o(l) . The horizontal phase then leverages these assignments to estimate H(l), o(l) off . The following subsections provide detailed description of these steps. , and θ (l) 4.1. Vertical Parameter Estimation: L, ϕ (l), and o(l) As discussed in Section 3.3, the vertical angle ϕi of each point is affected by beam-specific vertical offset. Substituting the angular residual ϕ (l,h(l)) tion (10) yields from Equation (7) into Equares ϕi = ϕ (l) + arcsin (cid:32) (cid:33) . o(l) ri (12) The objective of the vertical parameter estimation is to recover the set of scanlines that best fit the observed data under this model. To this end, we employ an iterative consensus-based algorithm that progressively identifies candidate scanlines, assigns points to them, and refines their parameters. At each iteration, the algorithm performs four steps: (1) proposes candidate scanline parameters (ϕ (l), o(l) ), (2) selects points consistent with 6 these parameters, (3) fits the scanline to refine the values of (ϕ (l), o(l) ), and (4) resolves conflicts with previously accepted scanlines. The process continues until all points are assigned to scanlines or the search space is exhausted. An overview of the proposed algorithm is shown in Figure 4. The subsequent subsections describe each component of the algorithm in detail. 4.1.1. Candidate Scanline Parameter Identification via the Hough Transform The first step in each iteration is to identify candidate scanline parameters (ϕ (l), o(l) ). To this end, we adopt the Hough Transform [35, 36], classical feature extraction technique widely used to detect parametric curves in noisy data. While originally developed for line and circle detection in 2D images [37, 38], the method has since been extended to 3D point clouds for extracting structures such as building roofs [39], powerlines [40], and trees [41]. The central principle of the Hough Transform is to map each observation in the measurement space to curve in discretized parameter space. Observations belonging to the same structure yield curves that intersect at common point in the parameter space. The coordinates of this intersection define the underlying structure. In our case, each observation (ri, ϕi) is mapped to curve in the parameter space (ϕ , oy) obtained by rearranging Equation (12): ϕ = ϕi arcsin . (13) (cid:19) (cid:18) oy ri Thus, points from the same scanline generate curves that intersect at the true parameters (ϕ (l), o(l) ). To make these intersections explicit, we discretize the parameter space into an accumulator A( ϕ , oy) where ϕ = ϕ min + ϕ , = 0, 1, 2, . . . , (cid:22) ϕ max ϕ min ϕ (cid:23) (14) and oy = omin + oy, = 0, 1, 2, . . . , (cid:37) (cid:36) omin omax oy (15) with step sizes ϕ and oy. For each observation (ri, ϕi) and Sec. 4.1.1 Sec. 4.1.1 Sec. 4.1. Sec. 4.1.4 Sec. 4.1.3 Figure 4: Flowchart of the vertical parameter estimation algorithm (Sec. 4.1). The algorithm iteratively identifies candidate scanline parameters using the Hough Transform, selects points consistent with these parameters, fits the scanline to refine parameter values, and resolves conflicts with previously accepted scanlines. The process continues until all points are assigned to scanlines or the search space is exhausted. Key processing steps are annotated with references to their corresponding subsections. each discretized oy, we compute the corresponding ϕ and increment the corresponding accumulator cell: i, oy : A( ϕ , oy) A( ϕ , oy) + 1 where ϕ = ϕi arcsin ϕ (cid:17) (cid:16) oy ri ϕ . + 0. (16) = omax = min(mini ri, 0.5 m), omin In our implementation, the parameter space is discretized and oy = 103 with omax m. The angular range is fixed to ϕ [π/2, π/2] with ϕ = 104 rad. These values are physically motivated: the vertical offset is bounded by the physical dimensions of LiDAR sensors (where offsets beyond 0.5 would be implausible given typical sensor sizes), and it is further clipped by the minimum observed point range to ensure the arcsine argument remains valid. The vertical angles are constrained to 90 since this represents the theoretical maximum range for laser emission. These values provide sufficient resolution to capture realistic LiDAR sensor parameters while maintaining computational tractability. After all points are processed, peaks in correspond to consistent parameter combinations and are selected as candidate scanlines. The iterative phase of the algorithm then begins: at each iteration, the cell with the highest vote count is selected as the next candidate. If accepted, the votes of its contributing points are removed from the accumulator. This strategy, first described as adaptive cluster detection [42], prevents previously assigned points from influencing later iterations and enables the progressive extraction of further scanlines. 7 To improve robustness and efficiency, we extend the standard Hough Transform with two modifications. First, we introduce vote-for-discontinuities strategy: when vote skips multiple cells in the accumulator (for example, due to steep gradients in the mapping), intermediate cells are also incremented, thereby reducing the risk of fragmented responses (see Figure 5). Second, we maintain hash accumulator B( ϕ , oy) that stores compact hash of the contributing point indices on each cell. This allows us to quickly determine whether different cells are supported by the same set of points, which is useful for equivalence checks in later stages. The hashing scheme uses 64-bit Knuth-style hash [43] applied to each point index and combined by bitwise XOR: κ(i) (cid:66) (cid:0)(i + 1) c(cid:1) mod 264, = 11400714819323198485, B( ϕ , oy) = (cid:77) κ(i), iS( ϕ , oy) (17) where S( ϕ , oy) is the set of point indices that voted for cell ( ϕ , oy). Cells with identical values therefore share the same contributing set, up to negligible collision probability. Despite its utility, the Hough Transform also presents limitations. It identifies candidate parameter locations but does not directly evaluate their quality of fit. Noise may generate spurious peaks or fragment single scanline into multiple candidates, while sparse scanlines may yield weak responses. Unrelated points can occasionally accumulate votes for an incorrect parameter combination. In addition, achieving higher precision requires finer discretization of the accumulator, which increases , min be the minimum nonzero difference between the x-coordinates (analogously min ). We assume that at least one axis exhibits two occupied adjacent quantization levels in the frame2. Under this assumption, we can reliably estimate the quantization level as the smallest of these minimum differences: ˆα := min{min , min , min }, and ε = ˆα /2 as the error bound between the observed and the ideal (non-quantized) coordinates. To guard against nonquantized data while covering potential errors introduced by floating-point representation, we clamp ε max(ε, 106 m). This upper bound on coordinate error ε can then be propagated to derive δ ϕi. Let ui = (x , ) represent the ideal coordinates and vi = (x ε) the observed coordinates. Defining = (ε, ε, ε), we have vi = ui + u. By the triangle inequality, , ε, ε, vi = ui + ui + = ui + 3ε. Thus, with = ui and ri = vi, δ = ri 3ε. (19) Defining the angular error bound δ ϕi as the difference between the observed and ideal vertical angles, we have: δ ϕi = (cid:19) (cid:12) (cid:12) arcsin (cid:12) (cid:12) (cid:18) r arcsin (cid:18) zi ri (cid:19)(cid:12) (cid:12) (cid:12) (cid:12) . Since arcsin(zi/ri) = arctan(zi/ρi) with ρi = (cid:113) + y2 x2 , δ ϕi = (cid:19) (cid:12) (cid:12) arctan (cid:12) (cid:12) (cid:18) ρ arctan (cid:18) zi ρi (cid:19)(cid:12) (cid:12) (cid:12) (cid:12) . Applying similar reasoning as in Equation (19), we bound δ ρ = ρ ρi 2ε. By the Mean Value Theorem, for some ξ between /ρ and zi/ρi, arctan (cid:18) ρ (cid:19) arctan (cid:19) (cid:18) zi ρi = 1 1 + ξ 2 (cid:18) ρ (cid:19) . zi ρi Since 1 1+ξ 2 1, it follows that δ ϕi (cid:12) (cid:12) (cid:12) (cid:12) ρ (cid:12) (cid:12) (cid:12) (cid:12) zi ρi = ρi ziρ ρiρ . We now derive an upper bound for δ ϕi by establishing an upper bound on the numerator and lower bound on the denominator. For the numerator, write ρ = ρi + ρi, ρi [δ ρ, δ ρ] = [ 2ε, 2ε], 2In practice, where typical LiDAR frame contains around 105 points, this assumption holds with overwhelming probability, as shown in our experiments (see Section 6). 8 (a) Basic Hough Transform strategy (b) Vote-for-discontinuities strategy Figure 5: Comparison of basic Hough Transform strategy versus vote-fordiscontinuities strategy. The basic strategy (a) misses the intersection peak due to vote fragmentation across cells, while the discontinuities strategy (b) successfully detects it by incrementing intermediate cells when votes skip multiple bins due to steep gradients in the parameter mapping. both memory and computational costs. For these reasons, we employ the Hough Transform only as the initial stage of our algorithm. The candidate parameters it produces are then refined and validated in subsequent steps. 4.1.2. Scanline Point Selection via Error Bounds Once candidate scanline parameters (ϕ (l), o(l) ) = ( ϕ , oy) are identified using the Hough Transform, the next step is to select points that are consistent with these parameters. This filtering stage is essential for the subsequent fitting process, as it ensures that only relevant points contribute to the model. Due to the finite precision of point cloud coordinates arising from sensor quantization, rounding, or floating-point representationpoints do not generally lie exactly on the theoretical scanline defined by Equation (12). Some sensors and datasets quantize their coordinates to discrete levels; for example, the KITTI raw dataset [44] quantizes coordinates to the nearest millimeter. This coordinate quantization directly propagates as error in the vertical angles ϕi. To account for this, we derive an angular error bound δ ϕi from the coordinate quantization level. That is, point is considered part of scanline if (cid:12) (cid:12) ϕi ϕ (l) arcsin (cid:12) (cid:12) (cid:12) (cid:32) o(l) ri (cid:33)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) δ ϕi. (18) Since we operate without prior knowledge of the sensor or dataset specifications, we must infer the coordinate quantization level directly from the data itself. We achieve this by examining the minimum non-zero differences between coordinate components across all points in the point cloud. Assume each Cartesian coordinate is quantized with some unknown step α > 0, i.e. xi, yi, zi α Z. Then every pairwise difference along an axis is an integer multiple of α , so the minimum nonzero spacing along that axis is at least α . Let min := min (cid:8)xi (cid:44) 0(cid:9) Figure 6: Flowchart of the scanline fitting via weighted least squares procedure (Sec. 4.1.3). The algorithm performs iterative WLS fitting to refine vertical parameters ( ˆϕ (l), ˆo(l) ) and compute point memberships. heuristic fallback is employed to handle cases where the fitting process fails to converge. so that ρi ziρ = (cid:12) (cid:12)(z (z ρi zi(ρi + ρi)(cid:12) = (cid:12) (cid:12)z (cid:12) (cid:12) zi)ρi zi ρi (cid:12) zi)ρi + ziρi 2ε 2 zi(cid:1). (since ε ρi + zi ε(cid:0)ρi + (since b + b) zi ε, ρi 2ε) For the denominator we have ρ = ρi + ρi ρi ρi ρi δ ρ, hence ρi ρ = ρi ρi + ρi ρi (ρi δ ρ) ρ 2 2 ερi. Combining the bounds for the numerator and the denominator, we obtain δ ϕi 2zi) 2ερi . ε(ρi + ρ 2 (20) This bound provides principled criterion to identify points that belong to candidate scanline (as defined by Equation (18)) while explicitly quantifying the uncertainty in ϕi. Both aspects are crucial for the subsequent fitting stage. 4.1.3. Scanline Fitting via Weighted Least Squares At this stage of the algorithm, we are given candidate set of points that may form scanlinei.e., set of points likely to originate from the same laser beam. Now the objective is to estimate the scanlines vertical parameters: the true vertical angle ϕ (l) and the vertical offset o(l) . This is achieved by fitting these points to the model defined in Equation (12). Figure 6 shows the flowchart of the fitting process. Note that fitting this model directly is non-trivial as it involves nonlinear arcsine function, which does not have 9 closed-form least-squares solution and typically requires iterative optimization techniques. However, we exploit the fact that, in practice, the vertical offset o(l) is small relative to the range ri of each point (e.g., o(l) in centimeters vs. ri in meters). This allows us to apply the approximation arcsin(x) for small x. (21) Applying this approximation to Equation (12), we obtain ϕi ϕ (l) + o(l) ri . Introducing qi = 1/ri, we rewrite the equation as: ϕi ϕ (l) + o(l) qi. (22) This linear form enables us to fit the data (ϕi, qi) with straight line, where the intercept corresponds to ϕ (l) and the slope to o(l) . However, unlike ordinary least squares, we cannot assume uniform noise across observations: as established in Equation (18), each measurement ϕi comes with its own error bound δ ϕi, leading to heteroscedastic residuals. To account for this non-uniform uncertainty, weighted least squares (WLS) framework is used. We assign each observation weight wi = 1/δ ϕ 2 , which down-weights noisier points relative to more reliable ones. While δ ϕi is not true standard deviation, Popovicius inequality on variances [45] guarantees that for any bounded error ϕi [δ ϕi, δ ϕi], the variance satisfies Var(ϕi) δ ϕ 2 . This result justifies treating δ ϕ 2 as conservative upper bound on the residual variance, providing principled basis for WLS under heteroscedastic noise model. Given set of pairs (qi, ϕi), the model assumes the linear relation described in Equation (22) with the optimal slope ˆo(l) and intercept ˆϕ (l) obtained by minimizing the weighted residual sum of squares i=1 min ˆo(l) , ˆϕ (l) wiR , where Ri = ϕi ˆo(l) qi ˆϕ (l). This yields closed-form estimators computed using weighted moments: ˆo(l) = SSxy SxSy SSxx S2 , ˆϕ (l) = SxxSy SxSxy SSxx S2 , where = wi, Sx = wiqi, Sy = wiϕi, Sxx = wiq2 , and Sxy = wiqiϕi. To quantify the uncertainty in the estimated parameters, confidence intervals are computed based on the variances of the estimators Var( ˆo(l) ) = σ SSxx S2 , Var( ˆϕ (l)) = σ 2 Sxx SSxx S2 , where σ 2 = 1 2 i=1 wiR2 is the weighted residual variance. We employ the Students t-distribution to construct 95% confidence intervals, using 2 degrees of freedom to reflect the loss of two degrees of freedom from estimating ˆo(l) and ˆϕ (l). This choice is justified by the small-sample regime, since some scanlines may contain only few tens of points. Under the assumption of normally distributed residuals within the error bounds, the t-distribution is more appropriate than the asymptotic normal approximation. To evaluate model plausibility, we compute log-likelihood under the assumption of Gaussian errors with heteroscedastic variance, using the conservative bounds δ ϕ 2 as proxies for the variances: log = (cid:20) log(2πδ ϕ 2 ) + 1 2 i=1 (cid:21) R2 δ ϕ 2 This surrogate log-likelihood captures both the fit quality and the uncertainty structure of the data. Higher values indicate that the observed residuals are small relative to the conservative noise bounds, making the model more statistically plausible. For practical purposes, we define an uncertainty score as the negative log-likelihood, = log , (23) which enables direct comparison between alternative fits: lower scores indicate better alignment with the observed data and its bounded-error structure. single WLS fit would be sufficient if the initial estimates from the Hough transform were already accurate and the corresponding candidate points were correctly assigned. In practice, however, this is not always the case, so more robust iterative refinement is required. After the initial WLS fit, the estimated parameters ( ˆo(l) , ˆϕ (l)) are used to reassess point membership based on the criterion in Equation (18), yielding an updated point set. The model is then re-fitted to this refined set, with the process repeated until point assignments stabilize or maximum of 10 iterations is reached. This iterative scheme ensures convergence to stable, self-consistent solution both for parameters and point memberships. Fallback: Heuristic Estimation. In some edge cases, the WLS fitting procedure may fail to converge. Namely when the number of candidate points is insufficient or when the convergence is not achieved within the iteration limit. As last resort for these rare scenarios where the mathematical fitting is insufficient, we introduce heuristic fallback that leverages assumptions based on the physical arrangement of the laser beams of real LiDAR sensors. Specifically, it assumes that the vertical offset of the laser beam can be approximated as the average offset of the two neighboring lasers, that is ˆo(l) = 1 . This is reasonable, as physically adjacent 2 lasers tend to have similar vertical angles. This method is only triggered when some reliable scanlines have already been computed, as it depends on existing parameter estimates. Once the offset ˆo(l) is heuristically determined, the vertical angle ˆϕ (l) is directly obtained by rearranging Equation (22) and computing the mean value. + ˆo(l+1) ˆo(l1) (cid:17) (cid:16) ˆϕ (l) = 1 i=1 (cid:16) ϕi ˆo(l) qi (cid:17) (24) After either the WLS or the heuristic fit, the algorithm recomputes which points fall within the estimated scanline bounds. If the point set is empty the scanline candidate is marked as invalid and rejected. To avoid redundant processing of identical scanlines detected in multiple Hough cells, once scanline is rejected we employ the hash accumulator of Section 4.1.1, resetting all cells with identical hashes to zero: (u, v) : B(u, v) = B( ϕ , oy) = A(u, v) = 0, (25) thereby ensuring that identical point sets are not repeatedly reevaluated. Conversely, if either the WLS or the heuristic fit is successful, the scanline is passed to the conflict resolution step. 4.1.4. Scanline Conflict Resolution Up to this point, the algorithm has focused on estimating individual scanlines in isolation. Accepting each candidate greedily would risk inconsistencies, such as multiple scanlines claiming the same points or geometrically implausible intersections. These issues are especially likely in later iterations, when candidate scanlines are supported by fewer points and thus become more sensitive to noise or imperfect fitting. To address this, we introduce conflict resolution step that enforces global consistency and supports backtracking, enabling previously accepted scanlines to be discarded in favor of better alternatives. Its objectives are twofold: (1) to resolve conflicts between the current scanline and those already accepted, thereby preventing inconsistencies in the final set, and (2) to allow recovery from suboptimal local decisions, increasing the likelihood of globally 10 Figure 7: Flowchart of the scanline conflict resolution process (Sec. 4.1.4). The algorithm checks for intersections between the current scanline candidate and previously accepted scanlines. If intersections exist, uncertainty scores are compared to determine which scanlines to accept or reject. Rejected scanlines are stored for potential recovery if their conflicting counterparts are later invalidated. consistent solution. An overview of the conflict resolution process is shown in Figure 7. Conflicts may arise as point intersections, where at least one point is simultaneously claimed by multiple scanlines, or as geometric intersections, where fitted scanlines (including their confidence intervals) overlap within the valid range of the point cloud. Both checks are performed. If no intersections are found, the scanline is accepted: its parameters are stored, points are assigned, and their votes removed from the Hough accumulator (Section 4.1.1); otherwise, conflicts must be resolved. The conflict resolution process relies on the uncertainty scores computed during fitting, as given by Equation (23). new scanline with higher uncertainty than any conflicting counterpart is deemed less reliable and rejected. Conversely, if it is more reliable than all conflicting counterparts, it is accepted, and the conflicting scanlines are invalidated. Rejected candidates are not permanently discarded: their parameters and rejection metadataincluding the identities of the scanlines causing invalidationare stored for potential recovery. After invalidating conflicting scanlines, the algorithm iterates over previously rejected candidates. If any no longer exhibit conflicts, they are reintroduced into the Hough accumulator for potential future selection. This backtracking mechanism allows the algorithm to recover from earlier suboptimal choices and promotes convergence to globally consistent solution. Once conflict resolution is complete, the new scanline is either accepted or rejected, and the algorithm continues accordingly. When scanlines are rejected, the same hash-based cell reset procedure described in Sec. 4.1.3 is applied to avoid redundant processing of identical scanlines (see Equation (25)). 4.2. Horizontal Parameter Estimation: H(l), o(l) , and θ (l) off After estimating the vertical parameters, the algorithm has identified the number of laser beams L, the vertical angles ϕ (l), the vertical offsets o(l) , and the assignments of each point in the cloud to its originating laser beam. The next stage is to estimate the horizontal parameters: the horizontal angular resolution H(l), the horizontal offset o(l) , and the azimuthal offset θ (l) off of each beam. Expanding the second expression in Equation (10), the observed azimuth θi can be expressed as θi = θ (h(l)) + θ (l) off + θ (l,h(l)) res = h(l) H(l) 2π + θ (l) off + arcsin (cid:32) (cid:33) . o(l) ρi this step is , θ (l) The objective of (26) the set {H(l), o(l) off } for each beam such that the scanline observations are best fitted. We treat each scanline independently, without assuming correlations across beams, which enhances robustness to variations in sensor design. An overview of the algorithm is shown in Figure 8. to recover Using Equation (21), simplification is obtained by exploitx ρi. Introducing ωi = 1/ρi, we approximate ing that o(l) θi h(l) H(l) 2π + θ (l) off + o(l) ωi. (27) Since H(l) is an integer, with minimum value equal to the number of points in scanline and practical maximum on the order of 103 for LiDAR sensors [46, 47, 48, 49], it is feasible to perform an exhaustive search over candidate resolutions ˆH(l). For each candidate, we compute the corresponding values of ˆo(l) , ˆθ (l) off , and loss metric L( ˆH(l)). The parameter set that minimizes this loss is then selected. In our implementation, we set the maximum ˆH(l) to 104, though this upper bound can be adjusted if future sensors exceed this resolution. 4.2.1. Computation of ˆo(l) , ˆθ (l) off , and L( ˆH(l)) for given ˆH(l) For each candidate resolution ˆH(l), the computation proceeds in three stages: (1) quantify the discrepancy θi between the observed azimuth and the ideal angle implied by ˆH(l) as function of ωi, which under the correct resolution forms piecewise linear function; (2) extract local linear pieces to obtain prior slope estimate; and (3) refine the offsets by compensating for periodic discontinuities and regressing continuous linear model whose residuals define the normalized loss L( ˆH(l)). For candidate resolution ˆH(l), the ideal azimuths are restricted to the discrete set (cid:110) 2π ˆH(l) (cid:12) (cid:12) {0, 1, . . . , ˆH(l) 1} (cid:12) (cid:111) . 11 Figure 8: Flowchart of the horizontal parameter estimation algorithm (Sec. 4.2). The algorithm performs an exhaustive search over candidate horizontal resolutions ˆH(l) for each scanline, computing horizontal offset ˆo(l) off , and selects the parameters that minimize the loss function. For scanlines with insufficient points, heuristic fallback reuses parameters from previously processed beams. and azimuthal offset ˆθ (l) Hence each observed angle θi can be associated with the nearest element of this set via an integer index ki, and the discrepancy is defined as θi = θi ki ˆH(l) 2π, ki = ˆH(l) + 0.5 (cid:23) . (cid:22) θi 2π (28) Substituting θi from Equation (27) into ki and simplifying we obtain (cid:36) h(l) H(l) ki = ˆH(l) + off + o(l) θ (l) 2π ωi (cid:37) ˆH(l) + 0.5 . Assuming the candidate resolution ˆH(l) is correct ( ˆH(l) = H(l)), we obtain ki = h(l) + (cid:36) off + o(l) θ (l) 2π ωi (cid:37) ˆH(l) + 0.5 , where we have used that h(l) and is therefore unaffected by the rounding operation. We express the rounded value as an integer correction term ki = h(l) +h(l) , h(l) := (cid:36) H(l) 2π (cid:0)θ (l) off + o(l) ωi (cid:1) + 0.5 (cid:37) Z. Since the only non-constant factor in h(l) integer-valued, piecewise-constant function of ωi: is ωi > 0, h(l) is an h(l) = (ωi), : R>0 piecewise constant. Substituting ki into Equation (28) gives θi = h(l) H(l) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:124) off + o(l) 2π + θ (l) ωi (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:123)(cid:122) (cid:125) θi (from Eq. (27)) h(l) + h(l) H(l) 2π, 12 which simplifies to θi = h(l) 2π H(l) + θ (l) off + o(l) ωi. (29) We estimate the horizontal offset o(l) Thus when ˆH(l) = H(l), θi is piecewise linear function of ωi, with slope o(l) and periodic intercept of period (l) = 2π/H(l). by exploiting the piecewise linearity of θi. We define two segmentation thresholds, τω and τθ , to partition the (ωi, θi) space into contiguous segments. Consecutive points are assigned to the same segment if the difference in ωi is smaller than τω and the difference in θi is smaller than τθ . This procedure isolates regions corresponding to the individual linear portions of the piecewise function. In our implementation, the thresholds are chosen as τω = (l)/4 rad (one quarter of the piecewise functions period, ensuring reliable detection of discontinuities) and τθ = 102m1 (empirically validated for typical LiDAR ranges of 1100 m). Within each segment, we perform linear regression to estimate local slope. These slope estimates are then aggregated using weighted median, where each segment contributes proportionally to its number of points. In this way, larger and more reliable segments exert greater influence on the aggregated estimate ˆˆo(l) . This initial estimate serves as prior for the subsequent optimization steps. We first define reference line with zero intercept and slope ˆˆo(l) , and compute the residuals of each point (ωi, θi) with respect to this line: Ri = θi ˆˆo(l) ωi = h(l) 2π H(l) + θ (l) off . Because h(l) is integer-valued, it contributes multiples of 2π/H(l). Hence the residuals are congruent with the azimuthal offset modulo this period: Ri θ (l) off (mod 2π/H(l)). The azimuthal offset θ (l) ing circular mean to the residuals modulo 2π/H(l): off can therefore be estimated by applyˆˆθ (l) off = CircMean (cid:16) Ri mod 2π H(l) (cid:17) . At this point we obtain preliminary estimates ˆˆo(l) and ˆˆθ (l) off . To refine them, we compute centered residuals by subtracting the estimated offset: This strategy reuses information from previously estimated scanlines. Specifically, we collect the values of H(l) and o(l) obtained from other beams, and for each candidate pair ( ˆH(l), ˆo(l) ) we compute heuristic loss function M( ˆH(l), ˆo(l) ). The parameter set minimizing this loss is selected as the final estimate."
        },
        {
            "title": "We begin by computing corrected angles by removing the",
            "content": "effect of the horizontal offset: θ corrected = θi θ (l,h(l)) res θi ˆo(l) ωi. Substituting in Equation (27), if the candidate ˆo(l) corrected angles satisfy is correct, the = Ri ˆˆθ (l) off h(l) 2π H(l) . θ corrected h(l) H(l) 2π + θ (l) off , From these, the correction terms h(l) are estimated as so the corresponding ideal quantized angles implied by ˆH(l) are: ˆh(l) = (cid:36) H(l) 2π (cid:37) + 0.5 . θ = (cid:22) θ corrected 2π/ ˆH(l) (cid:23) + 0.5 2π ˆH(l) . Assuming correct estimation, we have ˆh(l) . This equality holds under the assumptions of ˆH(l) = H(l) and ˆˆo(l) o(l) . We transform the original piecewise-linear θi into continuous linear form by compensating for the periodic discontinuities: = h(l) ˆθi = θi + ˆh(l) 2π H(l) . Using the definition of θi from Equation (29), this simplifies to ˆθi = θ (l) off + o(l) This linear form explicitly shows that ˆθi depends linearly on ωi, with slope o(l) off . Consequently, performing linear regression on the transformed pairs (ωi, ˆθi) yields refined estimates ˆo(l) off . The loss metric for the fit is quantified by the squared sum of the residuals, and intercept θ (l) and ˆθ (l) ωi. (30) L( ˆH(l)) = (R )2 ( ˆH(l))2, (31) where = ˆθi ˆθ (l) off ˆo(l) ωi. Note that the factor ( ˆH(l))2 normalizes the loss across candidate resolutions. Since the residuals are bounded by the period 2π/H(l), their magnitudes decrease with increasing resˆH(l) values would be olution. Without normalization, higher artificially favored. 4.2.2. Heuristic Fallback For scanlines containing insufficient points, the estimation procedure described above may be unreliable due to limited data. To address this, such scanlines are deferred until all others have been processed, after which their parameters are estimated using heuristic fallback strategy. 13 For correct parameters, the corrected angles should align with the ideal quantization grid up to constant θ (l) off , i.e., : θ θ corrected θ (l) off . This observation allows us to estimate the azimuthal offset as the mean deviation between corrected and ideal angles ˆθ (l) off = 1 i=1 (cid:0)θ corrected θ (cid:1) . Finally, the quality of each candidate parameter set is quantified by the mean absolute deviation M( ˆH(l), ˆo(l) ) = 1 i=1 (cid:12) (cid:12)θ corrected (cid:12) θ ˆθ (l) off (cid:12) (cid:12) ˆH(l). (cid:12) (32) As in the main estimation procedure, the multiplication by ˆH(l) serves as normalization factor. Since the residual magnitudes shrink with increasing resolution due to the shorter period 2π/H(l), this scaling ensures that heuristic loss values remain comparable across candidate resolutions. The candidate pair ( ˆH(l), ˆo(l) ) minimizing is selected as the final estimate. This fallback provides reasonable and consistent parameter estimates even with limited data. 4.3. Range Image Generation and Point Cloud Reconstruction Once all intrinsic parameters have been estimated, ALICELRI can generate lossless range images from point clouds and subsequently reconstruct the original point clouds without information loss. This bidirectional transformation constitutes the core objective of our approach. 4.3.1. Point Cloud to Range Image 5.1. Datasets Given estimated intrinsics {ϕ (l), o(l) l=0 and point cloud {(xi, yi, zi)}, we first compute spherical coordinates (ri, ϕi, θi) as in Equation (3). Subsequently, each point is assigned to the candidate beam ˆl whose vertical parameters best explain its elevation angle ϕi: , H(l), o(l) off }L1 , θ (l) ˆli = arg min (cid:12) (cid:12) ϕi ϕ (l) arcsin (cid:12) (cid:12) (cid:12) (cid:32) o(l) ri (cid:33)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) . The corrected azimuthal angle θ expression in Equation (11): is computed using the second θ = θi arcsin (cid:32) (cid:33) o(ˆl) ri cos(ϕi) θ (ˆl) off The height of the range image is L, and the width is = LCM{H(l)} (least common multiple) to ensure uniform grid across all beams even when their resolutions differ. The horizontal (ui) and vertical (vi) indices are computed as ui = (cid:22) θ 2π (cid:23) + 0.5 vi = ˆli 1 (see Eq. (5)), where uniqueness of (ui, vi) is guaranteed under correct parameters. Finally, each ri is stored on pixel (ui, vi). 4.3.2. Range Image to Point Cloud Given estimated intrinsics {ϕ (l), o(l) l=0 and range image of size L, each non-empty pixel (ui, vi) with range ri can be back-projected to 3D point. First, the beam index is identified as ˆli = vi 1 and then the original observed angles are recovered by adding back the angular residuals: off }L1 , θ (l) , o(l) (cid:33) (cid:32) o(ˆli) ri ϕi = ϕ (ˆli) + arcsin 2πui + θ (ˆli) θi = off + arcsin (cid:32) o(ˆli) ri cos ϕi (cid:33) . Finally, xi = ri cos(ϕi) cos(θi), yi = ri cos(ϕi) sin(θi), zi = ri sin(ϕi). The proposed method was evaluated on two publicly available datasets: KITTI [44] and DurLAR [50]. These datasets were selected to represent two distinct sensor configurations with contrasting characteristics, enabling thorough evaluation of the proposed approach. Their main characteristics are provided in Table 1. KITTI features point clouds acquired with Velodyne HDL-64E sensor with 64 scanlines and substantial beam offsets (vertical: 100 mm to 210 mm, horizontal: 26 mm to 26 mm). The dataset presents challenges primarily due to these significant geometric distortions. DurLAR provides point clouds from an Ouster OS1-128 sensor with 128 scanlines and minimal offsets (vertical: 25 mm to 40 mm, horizontal: 1 mm to 1 mm). Despite the smaller geometric distortions, DurLAR poses distinct challenges due to its higher scanline count and extended vertical field of view ([22.5, 22.5]). The wider angular coverage increases the likelihood of missing returns at high elevation angles where ground surfaces are absent, resulting in sparse or missing scanlines and varying point densities. These complementary characteristics allow comprehensive evaluation of the proposed method: KITTI tests robustness against significant sensor non-idealities and geometric distortions, while DurLAR evaluates performance with compact sensors under challenging data sparsity conditions at extreme viewing angles. 5.2. Experimental Setup All experiments were conducted on the CESGA Finisterrae III [51] supercomputing cluster to efficiently process the entire KITTI and DurLAR datasets, which contain 193,801 point clouds. This allowed us to analyze both datasets in their entirety rather than using only small subsets. For each point cloud, resultsincluding estimated sensor parameters, range images, and evaluation metricswere stored in local SQLite [52] databases to ensure reproducibility and facilitate downstream analysis. After processing, these databases were aggregated and analyzed offline on local workstation using custom scripts. Runtime performance analysis was conducted separately on local workstation to provide timing measurements under controlled conditions. The workstation specifications include an Intel Core i7-13700K processor and 64 GB of RAM, running Ubuntu 22.04.5 LTS. Under the assumption of correctly estimated intrinsics, this reconstruction is lossless up to floating-point precision. 5.3. Evaluation Metrics 5. Materials and Methods This section details the experimental methodology used to evaluate the proposed approach. We describe the datasets employed, the experimental setup, and the metrics used for quantitative and qualitative assessments. The evaluation described in Section 6 considers two main aspects: (i) the accuracy of the estimated sensor parameters, and (ii) the quality of the reconstructed point clouds obtained from the range images. In addition, the application presented in Section 7 demonstrates the practical benefits through compression case study. Each aspect is assessed using appropriate quantitative metrics, as detailed below. Table 1: Characteristics of the datasets used for evaluation, showing sensor specifications and data distribution. Dataset Sequences KITTI [44] DurLAR [50] 151 5 Point Clouds 47,885 145,916 Scanlines Horizontal Resolution 64 128 4000 2048 Vertical Angles [24.9, 2.0] [22.5, 22.5] Vertical Offsets Horizontal Offsets [100 mm, 210 mm] [25 mm, 40 mm] [26 mm, 26 mm] [1 mm, 1 mm] (i) Sensor Parameter Estimation. For parameter estimation accuracy, we use standard classification metrics (overall accuracy, precision, recall, F1-score) and mean absolute error (MAE) for continuous parameters. We report both macro-averaged (mP, mR, mF1) and weighted-averaged (wP, wR, wF1) variants of classification metrics. Macro-averaged metrics compute the unweighted mean across all classes, while weighted-averaged metrics weight each class by its support to reflect class imbalance. (ii) Point Cloud Reconstruction Quality. To evaluate the fidelity of reconstructed point clouds, we employ geometric similarity metrics that compare points in the original set with their counterparts in the reconstruction ˆP. Chamfer Distance (CD): The Chamfer Distance quantifies point cloud similarity by measuring bidirectional nearest-neighbour distances. Despite its widespread use in 3D vision, implementations vary considerably in the literature. Some researchers utilize the non-squared Euclidean distance [53], while others employ squared distances to penalize outliers more severely [54]. Additionally, the directional terms may be either summed [55] or averaged, as in the Point Cloud Utils library [56]. We adopt the non-squared, averaged Chamfer Distance formulation CD(P, ˆP) = MAE(P, ˆP) + MAE( ˆP, P) 2 , where MAE(A, B) = 1 aA min bB b2. This approach provides an interpretable metric that directly expresses reconstruction error in physical units (meters), representing the average displacement between corresponding points. Peak Signal-to-Noise Ratio (PSNR): Adapted from image processing, PSNR quantifies reconstruction quality by comparing maximum signal power to error power: PSNR(P, ˆP) = 10 log10 (cid:18) MAX2 MSE(P, ˆP) (cid:19) , where MSE(A, B) = 1 aA min bB b2 2, and MAX represents the maximum representable range value (that is, 120 for KITTI [46] and 170 for DurLAR [47]). Sampling Error (SE): Point preservation is quantified through sampling error: SE(P, ˆP) = ˆP , 0 SE 1. This metric measures the relative difference in cardinality between the original and the reconstructed point clouds. 6. Evaluation This section evaluates ALICE-LRI on the KITTI and DurLAR datasets. Our analysis combines quantitative metrics with qualitative visualizations to assess estimation accuracy, reconstruction fidelity, and computational performance. To ensure reproducibility, we provide all code and experiment configurations publicly.3 We apply the exact same methodology across both datasets without any dataset-specific customization, demonstrating the generality and robustness of ALICE-LRI. We focus on three key aspects of evaluation: (1) parameter estimation accuracy, (2) range image reconstruction quality, and (3) runtime performance for real-time applications. These evaluations collectively demonstrate the technical accuracy of ALICE-LRI and validate its effectiveness for practical use. 6.1. Parameter Estimation First, we evaluate the accuracy of ALICE-LRI in estimating the intrinsic parameters of spinning LiDAR sensors. We assess the estimation of the number of scanlines L, the vertical angles ϕ (l), spatial offsets o(l), horizontal resolutions H(l), and azimuthal offsets θ (l) off . We run ALICE-LRI on all frames in each dataset and compare the estimated parameters against reference values derived from manufacturer specifications and available sensor calibration data. To ensure consistent evaluation, the reference values for ϕ (l), o(l), H(l), and θ (l) off are fixed per dataset according to the known sensor configuration. However, the number of scanlines may vary between frames because some laser beams occasionally fail to produce returns, requiring frame-specific verification. Since calibration metadata is available for both datasets, we developed dataset-specific verification scripts. These scripts 3https://github.com/alice-lri/alice-lri-experiments 15 Table 2: Scanlines count estimation metrics on KITTI and DurLAR datasets. For each subset (all point clouds and those with n(l) 64), the number of point clouds (# Samples), incorrect predictions (# Incorrect), overall accuracy (OA), macro and weighted precision (mP, wP), recall (mR, wR), and F1-score (mF1, wF1) are reported. Dataset Subset Scanlines Count # Samples # Incorrect OA (%) mP (%) mR (%) mF1 (%) wP (%) wR (%) wF1 (%) KITTI DurLAR All n(l) 64 All n(l) 64 47,885 47,543 145,916 130,757 0 130 0 100.00 100.00 99.91 100.00 100.00 100.00 92.98 100.00 100.00 100. 93.67 100.00 100.00 100.00 93.31 100.00 100.00 100.00 99.92 100.00 100.00 100. 99.91 100.00 100.00 100.00 99.92 100.00 automatically determine the effective scanline count in each frame by checking which of the predefined scanlines receive valid point returns. This procedure ensures an unambiguous, frame-specific reference for L, consistent with the documented sensor configuration. 6.1.1. Scanlines Count We treat the problem of estimating the number of scanlines as multi-class classification task, where each sample is point cloud and each class corresponds to different number of scanlines. Table 2 summarizes the scanline count estimation results on the KITTI and DurLAR datasets. The reported metrics are: number of samples (point clouds), number of incorrect predictions, OA (overall accuracy), mP (macro precision), mR (macro recall), mF1 (macro F1-score), wP (weighted precision), wR (weighted recall), and wF1 (weighted F1-score), all expressed as percentages except for the sample counts. To provide comprehensive assessment, we partition each dataset into two subsets that highlight performance under different conditions. The all subset encompasses every point cloud in the dataset, while the n(l) 64 subset contains only point clouds with at least 64 points per scanlinea reasonable threshold that maintains most of the dataset while excluding severely degraded scans. The results demonstrate that ALICE-LRI achieves perfect scanline count prediction on KITTI (100% overall accuracy) and near-perfect results on DurLAR (99.91% overall accuracy) when considering all point clouds in the datasets. Perfect classification (100% accuracy) on both datasets is achieved when considering only point clouds with sufficient point density ( 64 points per scanline). These results validate the robustness of ALICE-LRI across different sensor configurations and environmental conditions. The difference between macro metrics (mP, mR, mF1) and their weighted counterparts (wP, wR, wF1) on the DurLAR dataset primarily arises from the relationship between scanline completeness and point density. Point clouds containing all scanlines typically have many points per scanline, making them easier to classify and thus dominant in the weighted metrics. In contrast, point clouds with missing scanlines often contain remaining scanlines with very few pointssometimes only one or two, especially those adjacent to the missing ones. Consequently, these cases are more challenging for the method, and since macro metrics assign equal weight to all classes, they are more affected by such difficult, low-density instances. Therefore, the observed gap between macro and weighted metrics reflects the underlying data distribution rather than any limitation or bias of the method. 6.1.2. Per-Beam Parameters We now evaluate the accuracy of the estimated per-beam parameters. For each detected scanline, the estimated horizontal resolutions ˆH(l), vertical angles ˆϕ (l), spatial offsets ˆo(l), and azimuthal offsets ˆθ (l) off are compared against the corresponding reference values. Horizontal resolution estimation is treated as binary classification problem (considered correct if it matches the reference value, and incorrect otherwise) whereas the remaining parameters are evaluated as continuous quantities. Table 3 presents the horizontal resolution estimation results across both datasets. The method achieves near-perfect accuracy of 99.99% on both KITTI (84 incorrect) and DurLAR (237 incorrect) when considering all point clouds in the datasets. When focusing on point clouds with sufficient point density (n(l) 64), performance improves further, reaching 100% accuracy on DurLAR (0 incorrect) and 99.99% accuracy on KITTI (4 incorrect). These results confirm the robustness of our horizontal resolution estimation approach across different sensor configurations and data quality conditions. Table 3: Horizontal resolution estimation accuracy on KITTI and DurLAR. We report the total number of scanlines (# Samples), incorrect predictions (# Incorrect), and overall accuracy (OA) for all point clouds and for those with n(l) 64. Dataset Subset Horizontal Resolution # Samples # Incorrect OA (%) KITTI DurLAR All n(l) 64 All n(l) 64 3,064,085 3,042, 18,637,656 16,736,809 84 4 237 0 99.99 99.99 99.99 100.00 Table 4 summarizes the estimation errors for the remaining per-beam parameters using mean absolute error (MAE) and maximum error (MAX) metrics. The results demonstrate high accuracy, particularly on point clouds with adequate point density (n(l) 64). For these well-populated scanlines, vertical 16 Table 4: Per-beam parameter estimation errors (MAE and MAX) for vertical angles, vertical and horizontal offsets, and azimuthal offsets on KITTI and DurLAR. Results are shown for all point clouds and for those with n(l) 64. Dataset Subset Vert. Angle ϕ (l) (deg) Vert. Offset o(l) (mm) Horiz. Offset o(l) x"
        },
        {
            "title": "MAX",
            "content": "(mm) Azim. Offset θ (l) MAE"
        },
        {
            "title": "MAX",
            "content": "off (deg) MAE KITTI DurLAR All n(l) 64 All n(l) 64 0.123479 0. 3.490045 0.012686 0.000413 0.000412 0.000033 0.000008 137.338350 4.006125 323.204766 0.102721 0.057363 0. 0.005478 0.000780 202.204798 19.806281 17.107649 0.010094 0.040357 0.038512 0.003304 0.002778 0.081500 0. 0.174496 0.000117 0.000872 0.000870 0.000032 0.000028 angle estimation achieves sub-degree accuracy, with MAE values of 4.12 104 on KITTI and 8 106 on DurLAR, and maximum errors below 0.050 and 0.013 respectively. Spatial offset estimation shows remarkable sub-millimeter precision, with MAE values of 5.7 102 mm (vertical) and 3.9 102 mm (horizontal) on KITTI, and 7.8 104 mm and 2.8 103 mm on DurLAR. The corresponding maximum errors are wellcontained at 4.0 mm and 19.8 mm for KITTI, and 0.10 mm and 0.01 mm for DurLAR. The high maximum horizontal offset error of 19.8 mm on KITTI stems from cases with incorrect resolution estimation, as resolution and horizontal offset estimation are interdependent; it reduces to 6.3 mm when considering only correct resolution estimates. Azimuthal offset estimation is also highly accurate, with MAE values of 8.7 104 on KITTI and 2.8 105 on DurLAR, and maximum errors of 0.082 and 1.2 104 respectively. When considering all point clouds, MAE values remain low, indicating that the higher maximum errors are caused by small number of outliers in sparse scans rather than systematic failure. The primary sources of error are insufficient point density, which degrades statistical estimation, and incorrect point-toscanline assignments in severely degraded scans. Nevertheless, the method demonstrates robust performance on well-populated scanlines, confirming its effectiveness for practical use. 6.1.3. Ablation Study ALICE-LRI incorporates several algorithmic components that vary in mathematical rigor: while core elements like the Hough Transform and weighted least squares fitting have strong theoretical foundations, other components such as the heuristic fallbacks rely on empirical assumptions about LiDAR sensor design. To assess the contribution of each component, we conduct comprehensive ablation study over the entire KITTI and DurLAR datasets by selectively disabling individual algorithm features. This analysis is crucial for determining whether performance gains arise from principled mathematical modeling or from heuristics that may implicitly encode sensor-specific knowledge. We focus our ablation analysis on scanline count estimation and horizontal resolution accuracy as they represent the most critical parameters for the internal consistency of the algorithm and parameter interdependencies. Scanline count errors cascade through the entire parameter estimation pipeline, leading to incorrect point-to-scanline assignments that compromise subsequent parameter fitting stages. Similarly, horizontal resolution accuracy is fundamental for proper horizontal and azimuthal offsets estimation, as these parameters are intrinsically coupled in the fitting processincorrect resolution estimates prevent accurate spatial offset determination. These parameters also exhibit the clearest binary success/failure characteristics, making them ideal metrics for assessing component contributions. While other parameters such as vertical angles and spatial offsets are equally important for reconstruction quality, their continuous nature makes ablation analysis more complex and less interpretable in terms of algorithmic component impact. We evaluate four key components: (1) Hough Continuity, which implements the vote-for-discontinuities strategy to improve robustness to steep gradients in the parameter space; (2) Conflict Resolution, which enforces global consistency through backtracking and prevents inconsistent scanline assignments; (3) Vertical Heuristics, which provide fallback parameter estimation when weighted least squares fitting fails due to insufficient data; and (4) Horizontal Heuristics, which estimate horizontal parameters for sparse scanlines using values from other beams. Note that, while the first two components enhance the core mathematical framework, the latter two represent domainspecific heuristics that exploit typical LiDAR sensor configurations. When Hough Continuity is disabled, the accumulator uses discontinuous voting scheme as shown in Figure 5a. Disabling Conflict Resolution reduces the algorithm to greedy approach where scanlines are immediately accepted upon successful fitting unless they contain already assigned points, in which case they are permanently discarded. When Vertical Heuristics are disabled, scanlines that fail weighted least squares estimation due to insufficient data are entirely discarded. Finally, disabling Horizontal Heuristics enforces the standard fitting procedure across all scanlines irrespective of point density. Scanlines that fail to converge receive invalid parameter values. The computational overhead of these individual components is negligible compared to the total parameter estimation time, allowing us to focus solely on their algorithmic contributions without performance considerations. Table 5 summarizes the features enabled in each experiment configuration (E0-E7), along with their impact on scanline count estimation and resolution accuracy across both datasets. Configuration E0 represents the complete method with all components active, while subsequent configurations (E1-E7) sys17 Table 5: Ablation study results showing the impact of different algorithm components on estimation accuracy. For each configuration, we report the number of incorrect scanline counts and resolution errors across KITTI and DurLAR datasets. Values not in parentheses represent all point clouds, while values in parentheses represent point clouds with n(l) 64. The full method (E0) includes all components, while subsequent configurations systematically disable specific features. Algorithm Components KITTI DurLAR"
        },
        {
            "title": "Vertical",
            "content": "E0 E1 E2 E3 E4 E5 E6 E7 # Incorrect # Incorrect Scanlines Count Resolutions # Incorrect # Incorrect Scanlines Count Resolutions 0 (0) 0 (0) 2 (0) 136 (3) 0 (0) 136 (3) 5 (2) 140 (5) 84 (4) 84 (4) 86 (4) 414 (7) 299 (4) 455 (7) 90 (5) 461 (9) 130 (0) 145 (0) 2110 (24) 4137 (0) 130 (0) 4137 (0) 2294 (7) 5666 (7) 237 (0) 260 (0) 3263 (154) 13741 (0) 12807 (0) 14204 (0) 2734 (3) 17694 (4) tematically disable specific features to isolate their individual contributions. Values not in parentheses represent all point clouds in the dataset, while values in parentheses represent point clouds with n(l) 64 points per scanline. This experimental design allows us to distinguish between improvements achieved through rigorous mathematical methods versus those dependent on heuristic assumptions, thereby validating the fundamental soundness of our approach. The ablation results show that the full method (E0) achieves the best performance, with errors increasing as key components are disabled. Removing core mathematical elements or heuristics leads to substantial drops in accuracy, especially for sparse data. This effect is more pronounced on DurLAR, which presents greater challenges than KITTI. Crucially, disabling heuristics has very limited impact on dense point clouds in both datasets: even when both vertical and horizontal heuristics are removed, performance on dense data remains nearly perfect. On DurLAR, configuration E6 (disabling both Hough continuity and conflict resolution) outperforms E2 (disabling only conflict resolution) on dense data. This counterintuitive result stems from specific consecutive frames where Hough continuity creates false accumulator peaks. While conflict resolution can address these issues when enabled, its absence in E2 causes failures that are mitigated in E6 by also disabling the problematic Hough continuity component. The results reveal clear pattern: the core mathematical framework demonstrates robust performance on well-populated scanlines, while heuristic components serve as essential fallbacks for sparse data scenarios. The stark contrast between performance on all point clouds versus dense subsets when no heuristics are used confirms that estimation accuracy fundamentally depends on point density. When sufficient data is available, the underlying mathematical formulation operates reliably even without domain-specific optimizations, demonstrating the robustness of our approach. 6.2. Range Image Error Analysis We now turn to the core objective that motivates this work: assessing whether our inferred sensor parameters enable more 18 accurate range image generation compared to standard approaches. While the parameter estimation accuracy demonstrated in the previous subsection validates the technical correctness of our inference algorithm, the true test lies in whether these parameters translate into measurably improved range image quality and lossless reconstruction capability. This evaluation directly addresses the central promise of our approach: enabling lossless range image generation from calibrated point clouds without requiring manufacturer metadata. For each sequence in the datasets, we first apply our algorithm to the initial point cloud to infer the sensor parameters, then reuse these parameters consistently for all subsequent point clouds in the sequence. This mirrors typical realworld deployment scenarios where practitioners encounter an unknown sensor and must establish its configuration for subsequent lossless range image generation. As the baseline, we employ the Projection-By-ElevationAngle (PBEA) method described by Wu et al. [15]. PBEA assumes the ideal model described in Section 3.2 and operates by uniformly sampling the elevation angle space between predefined minimum and maximum values, creating an equidistant grid in the vertical dimension while maintaining uniform azimuthal sampling in the horizontal dimension. This approach requires only the specification of vertical and horizontal range It is image resolutions along with elevation angle bounds. the most widely adopted method in practice due to its simplicity and lack of sensor-specific requirements. Furthermore, Wu et al. demonstrated that PBEA outperforms alternative approaches such as Projection-By-Laser-ID (PBID) when sufficient resolution is available, establishing it as strong and fair baseline for our comparison. We evaluate range image generation quality using both quantitative metrics and qualitative visual analysis. The quantitative analysis measures reconstruction accuracy through objective metrics, while the qualitative analysis examines presence of artifacts and structural preservation to assess practical performance. 6.2.1. Quantitative Analysis To assess reconstruction quality, we project each point cloud to range image using our method, then unproject it back to 3D space and compute three key metrics against the original point cloud: Chamfer Distance (CD) and Peak Signal-to-Noise Ratio (PSNR) for geometric fidelity, and sampling error (SE) representing the fraction of points lost during the projectionunprojection process. Table 6 presents quantitative comparison between our proposed method and the PBEA baseline across multiple range image resolutions, evaluated on the complete datasets without excluding any frames. Traditional range image generation methods like PBEA require practitioners to manually select appropriate resolutions through trial and error, as the optimal resolution depends on unknown sensor characteristics. To provide fair comparison, we evaluate PBEA using the native resolution of each sensor (400064 for KITTIs HDL-64E and 2048128 for DurLARs Ouster OS1-128). At the native resolution, our results show that ALICE-LRI significantly outperforms PBEA across all metrics. On KITTI at 400064, PBEA achieves 0.027 average Chamfer Distance with 8.69% sampling error, while ALICE-LRI delivers 3.80 104 CD with 0% sampling error. On DurLAR at 2048128, PBEA shows 0.024 average CD and 3.52% sampling error compared to ALICE-LRIs 1 106 CD and 0% sampling error. Crucially, for ALICE-LRI, the maximum sampling error is exactly zero across the entire KITTI and DurLAR datasets, that is, no points are ever lost during projection and unprojection. common approach to mitigate reconstruction loss in range image projections involves increasing the resolution until the desired quality is achieved. As demonstrated in Table 6, higher resolutions indeed improve PBEA reconstruction quality. However, even with substantial resolution increases, the performance gap relative to ALICE-LRI persists. Scaling PBEA resolution by factor of 32 in each dimensionfrom 400064 to 1280002048 for KITTI and from 2048128 to 655364096 for DurLAR (representing 1024-fold increase in total pixels)reduces the Chamfer Distance to the millimeter/sub-millimeter scale. Nevertheless, this approach still yields non-zero sampling error. This demonstrates that even significant increases in resolution cannot fully address the fundamental limitations of the PBEA approach. Figure 9 complements the quantitative results presented in Table 6 by providing frame-by-frame analysis of reconstruction quality across representative sequences. The plots reveal the consistent performance advantage of ALICE-LRI over the PBEA baseline, with ALICE-LRI (solid lines) maintaining submillimeter Chamfer Distance values across all frames. In contrast, the PBEA baseline (dashed lines) exhibits significantly higher reconstruction errors, even at increased resolutions. This frame-level analysis confirms that ALICE-LRIs superior performance is not driven by outliers but represents consistent improvement across diverse scanning conditions and scene contents. Overall, the quantitative analysis confirms that ALICE-LRI achieves lossless geometric reconstruction. No points are lost in any frame, and the Chamfer Distance remains below the intrinsic noise level of each LiDAR sensor, surpassing their physical precision. 6.2.2. Qualitative Analysis The quantitative metrics presented above provide comprehensive assessment of reconstruction quality, but visual inspection offers complementary insights into the practical implications of using ALICE-LRI. Figure 10 shows the differences between the original point cloud and the reconstructions obtained using both the PBEA baseline and ALICE-LRI. The visualizations reveal that while the baseline method introduces geometric distortions and point cloud artifacts, ALICE-LRI preserves the original geometric structure with high fidelity. ALICE-LRI not only significantly improves 3D point cloud reconstruction but also enhances the quality of range image representations. Figure 11 provides side-by-side comparison of range images generated using both the baseline PBEA and ALICE-LRI. The baseline PBEA method produces range images with missing data regions (highlighted areas). These artifacts arise from the mismatch between the idealized spherical projection model and the actual sensor geometry. In contrast, ALICE-LRI generates range images with smooth, continuous patterns that accurately reflect the underlying sensor scanning behavior. The absence of holes and irregular discontinuities in range images directly correlates with the zero sampling error achieved in our quantitative evaluation, confirming the lossless nature of our approach. This improvement in range image quality has implications beyond reconstruction fidelity: cleaner range images without artifacts can potentially benefit downstream applications such as semantic segmentation, object detection, and scene understanding, though such evaluations lie outside the scope of this work. 6.3. Runtime Performance Range images often serve as critical intermediate representations in numerous real-time LiDAR processing applications due to their regular grid structure, which enables efficient computation. Since efficiency is critical in such contexts, we now evaluate the runtime performance of ALICE-LRI. Runtime measurements were obtained by processing representative frames from each dataset on the local workstation described in Section 5.2. For KITTI, we used one frame from each of 10 different sequences, while for DurLAR, we used two frames from each of the 5 available sequences. This selection ensures variability across different scene conditions. The implementation operates in single-threaded mode and timing measurements exclude all I/O operations. Table 7 presents the runtime analysis across both datasets. Parameter estimation requires an average of 31.3 seconds for KITTI and 41.3 seconds for DurLAR. Importantly, the parameter estimation phase is performed only once per sensor, making an execution time below one minute highly acceptable for practical deployment scenarios. For the per-frame operations, KITTI projection requires an average of 9.3 ms per frame, while 19 Table 6: Point cloud reconstruction quality metrics comparing ALICE-LRI against PBEA baseline at native sensor resolutions and progressively increased resolutions. Results are computed over the complete datasets without frame exclusion. For each method and resolution, average (AVG) and worst-case (MAX/MIN) values for Chamfer Distance (CD), Peak Signal-to-Noise Ratio (PSNR), and Sampling Error (SE) are reported. Dataset Method CD (m) PSNR (dB) SE (%)"
        },
        {
            "title": "MAX",
            "content": "KITTI DurLAR PBEA (4000 64) PBEA (8000 128) PBEA (16,000 256) PBEA (32,000 512) PBEA (64,000 1024) PBEA (128,000 2048) ALICE-LRI (4000 64) PBEA (2048 128) PBEA (4096 256) PBEA (8192 512) PBEA (16,384 1024) PBEA (32,768 2048) PBEA (65,536 4096) ALICE-LRI (2048 128) 0.027419 0.012521 0.006156 0.003055 0.001520 0.000758 0.000380 0.023616 0.010813 0.005354 0.002657 0.001324 0.000662 0. 0.051127 0.022010 0.010768 0.005281 0.002664 0.001335 0.000423 0.050328 0.021681 0.010646 0.005335 0.002615 0.001458 0.000006 63.056212 72.296912 79.090223 85.929010 92.654247 99.450472 109.331016 70.359540 79.518543 86.099955 92.606193 98.833528 104.882723 157.268976 45.769613 51.615480 52.142493 52.152183 61.124456 63.225820 108.196745 38.323632 39.413287 48.081959 49.798801 50.249024 50.249114 140. 8.689915 0.518697 0.126372 0.034610 0.009202 0.002254 0.000000 3.518374 0.303959 0.083244 0.013953 0.003500 0.001278 0.000000 17.652985 3.034373 0.755860 0.196997 0.091909 0.033769 0.000000 11.450134 1.271056 0.525120 0.193679 0.084627 0.038590 0.000000 KITTI DurLAR 101 102 103 104 105 10 ) ( a D m 10 102 103 104 105 106 ) ( a D m PBEA (4000 64) PBEA (8000 128) PBEA (16000 256) PBEA (32000 512) PBEA (64000 1024) PBEA (128000 2048) ALICE-LRI (4000 64) PBEA (2048 128) PBEA (4096 256) PBEA (8192 512) PBEA (16384 1024) PBEA (32768 2048) PBEA (65536 4096) ALICE-LRI (2048 128) 0 200 400 600 800 1, 0 200 400 600 800 1, Frame Index Frame Index (a) Chamfer Distance performance across the first 1000 point clouds from the 2011_09_30_drive_0018 sequence of the KITTI dataset. (b) Chamfer Distance performance across the first 1000 point clouds from the DurLAR_20211209 sequence. Figure 9: Chamfer Distance of ALICE-LRI and PBEA across different range image resolutions. ALICE-LRI (solid lines) consistently achieves lower reconstruction error compared to PBEA (dashed lines). Results show frame-by-frame reconstruction quality, where lower values indicate better geometric fidelity. (a) Original point cloud from KITTI dataset showing detailed geometric structure. (b) PBEA baseline reconstruction exhibiting geometric distortions and point misalignment (highlighted region). (c) ALICE-LRI reconstruction demonstrating accurate geometric preservation and structural fidelity. Figure 10: Qualitative comparison of 3D point cloud reconstruction quality. The highlighted regions (red boxes) illustrate the reconstruction fidelity achieved by ALICE-LRI compared to the baseline PBEA approach. ALICE-LRI maintains structural integrity while the baseline introduces visible artifacts and geometric distortions. (a) Range image generated using PBEA baseline method missing data regions (highlighted in red). (b) Range image generated using ALICE-LRI, exhibiting smooth, continuous patterns without artifacts or missing data. Figure 11: Comparison of range image quality between baseline and ALICE-LRI methods. The baseline method (top) exhibits visible discontinuities and missing data regions, while ALICE-LRI (bottom) produces clean, artifact-free range images that accurately represent the sensor scanning pattern. The highlighted regions show the improved continuity and data preservation achieved by ALICE-LRI. 21 unprojection completes in 3.8 ms. The DurLAR dataset exhibits slightly longer processing times due to its higher scanline count: 20.0 ms for projection and 4.8 ms for unprojection. Table 7: Runtime performance analysis showing mean execution times ( standard deviation) for parameter estimation, projection, and unprojection phases on the KITTI and DurLAR datasets. Parameter estimation times represent onetime offline estimation costs, while projection and unprojection times indicate per-frame processing requirements for real-time applications. Dataset Estimation (s) Proj. (ms) Unproj. (ms) KITTI DurLAR 31.3 0.8 41.3 2.8 9.3 0.3 20.0 2.8 3.8 0.1 4.8 0.6 For real-time applications, in the worst case, both the projection and unprojection phases must be executed for every point cloud frame. ALICE-LRIs combined projection and unprojection times are 13.1 ms for KITTI and 24.8 ms for DurLAR. Considering that both datasets employ sensors operating at 10 Hz, corresponding to 100 ms frame period, these times represent only 13.1% and 24.8% of the available budget. This demonstrates that ALICE-LRI comfortably meets real-time constraints while leaving substantial computational headroom for downstream processing. 7. Application We now examine the practical implications of ALICE-LRI for downstream applications. To illustrate the real-world utility of the proposed approach, we present case study on point cloud compression using the Real-Time Spatio-Temporal Point Cloud Compression (RTST) [29] algorithm, which relies on range images as intermediate representations. We selected RTST because it meets two key criteria: (1) it uses range images as an intermediate representation for compression, and (2) it provides open-source code with clearly identifiable projection components that can be replaced with ALICE-LRI. This allows for direct comparison of the impact of ALICE-LRI on compression performance while keeping all other algorithmic components unchanged. To ensure fair comparison, we evaluate the original RTST implementation against modified version in which only the range image projection and unprojection components are replaced with ALICE-LRI. Although the RTST algorithm is general and applicable to different sensor configurations, the publicly released code by its authors is tailored specifically for the KITTI dataset. To maintain experimental integrity, we chose not to modify the original implementation beyond the projection components. Therefore, this compression evaluation is conducted exclusively on the KITTI dataset. The RTST algorithm includes an error threshold parameter that controls compression aggressiveness, enabling us to evaluate performance across different compression-quality tradeoffs. We compute aggregate metrics over the KITTI dataset, measuring Compression Ratio (CR), Chamfer Distance (CD), Peak Signal-to-Noise Ratio (PSNR), and Sampling Error (SE). The Compression Ratio is defined as: 22 CR ="
        },
        {
            "title": "Original Size\nCompressed Size",
            "content": "Table 8 presents the results of this study. While compression ratios show mixed results, the point cloud quality metrics demonstrate clear advantages for the modified version leveraging ALICE-LRI. At low error thresholds (0.001 to 0.100), where reconstruction loss is primarily attributed to projectionunprojection rather than compression artifacts, ALICE-LRI achieves substantial improvements: CD reduces from 0.0293 to 0.0020 at threshold 0.001, PSNR increases from 62.43 dB to 93.82 dB, and SE decreases from 10.09 % to 0.0009 %. As error thresholds increase, the performance gap narrows for CD and PSNR since compression artifacts dominate over projection losses. Still, the SE difference remains substantial across all tested configurations, suggesting that most point losses originate from the range image projection-unprojection process rather than the compression algorithm itself. While the compression ratio results might suggest that ALICE-LRI only improves point cloud quality without enhancing compression efficiency, this interpretation overlooks the practical implications for users. In real-world scenarios, practitioners typically prioritize achieving specific compression ratios while maintaining acceptable point cloud quality rather than optimizing for particular error threshold values. By achieving superior point cloud quality at equivalent compression ratios, ALICE-LRI enables users to select more aggressive compression settings without experiencing significant quality degradation. Figure 12 illustrates this relationship by plotting Compression Ratio against Chamfer Distance for both methods across all evaluated error thresholds. Each point represents one execution of the compression algorithm for either the original or the ALICE-LRI modified method. Linear regression analysis reveals that for given Chamfer Distance, the modified method achieves higher compression ratios on average compared to the baseline. This demonstrates that users achieve better compression efficiency without sacrificing point cloud quality by leveraging ALICE-LRIs range image generation approach. These results validate that ALICE-LRI not only enhances range image quality as an intermediate representation but also provides tangible benefits for downstream applications. In the compression domain, ALICE-LRI delivers improvements in both compression efficiency and point cloud fidelity, demonstrating the practical value of accurate sensor parameter estimation for real-world LiDAR processing workflows. 7.1. Runtime Performance Analysis Beyond quality improvements, critical concern for practical deployment is whether ALICE-LRI maintains the computational efficiency required for real-time applications. Section 6.3 demonstrated substantial computational headroom for ALICELRIs projection and unprojection operations. To further validate real-time viability, we analyze the runtime performance impact of integrating ALICE-LRI into the RTST compression workflow. Table 8: Point cloud compression performance comparison between the original RTST algorithm and the ALICE-LRI modified version. Results show Compression Ratio, Chamfer Distance (CD), Peak Signal-to-Noise Ratio (PSNR), and Sampling Error (SE) across different error thresholds. Bold values indicate superior performance for each metric. Error Threshold Compression Ratio CD (m) PSNR (dB) SE (%)"
        },
        {
            "title": "Original Modified Original Modified Original Modified Original Modified",
            "content": "0.001 0.010 0.050 0.100 0.250 0.500 0.750 1.000 10.7899 10.4894 12.2672 15.0507 19.5483 23.4855 26.4736 28.7996 10.3009 10.1392 11.8046 14.9141 21.3198 26.6180 30.3682 33.5210 0.0293 0.0293 0.0315 0.0362 0.0474 0.0627 0.0760 0.0878 0.0020 0.0021 0.0059 0.0130 0.0301 0.0478 0.0624 0.0754 62.4274 62.4270 62.3669 62.1628 61.4164 60.2123 59.1101 58. 93.8161 93.5504 81.2060 74.8480 68.2742 64.5307 62.3571 60.8477 10.0882 10.0881 10.0877 10.0873 10.0869 10.0867 10.0866 10.0865 0.0009 0.0009 0.0008 0.0007 0.0005 0.0004 0.0004 0."
        },
        {
            "title": "Original\nModified",
            "content": "100 50 0 a i r C 0 0. 0.1 0.15 Chamfer Distance (m) Figure 12: Compression Ratio versus Chamfer Distance trade-off analysis. Each point represents one execution of the compression algorithm. Blue circles denote the original method and red triangles denote the ALICE-LRI modified method. The blue solid and red dashed lines show the corresponding linear regression fits, demonstrating that the ALICE-LRI modified method achieves higher compression ratios for equivalent reconstruction quality. Table 9 presents detailed comparison of execution times between the original RTST algorithm and our ALICE-LRI modified version across different error thresholds. The results show that ALICE-LRI introduces modest computational overhead of 2-8 ms for encoding and 2-3 ms for decoding operations. Even at the highest error thresholds, where compression times are longest, the total execution times remain well within the computational headroom available for real-time applications, confirming that practitioners can adopt ALICE-LRI without sacrificing temporal performance in time-critical applications. The minimal runtime overhead observed validates the practical feasibility of ALICE-LRI integration in production systems where both quality and performance are critical requirements. 8. Conclusion This work addressed fundamental limitation in LiDARthe inability to generate lossless based perception systems: 23 range images from calibrated point clouds without sensorspecific metadata. We presented ALICE-LRI (Automatic LiDAR Intrinsic Calibration Estimation for Lossless Range Images), which automatically infers key geometric parameters of spinning LiDAR sensors and enables truly lossless range image generation. Experimental evaluation on KITTI and DurLAR datasets demonstrated ALICE-LRI effectiveness across different sensor configurations. Critically, ALICE-LRI achieved zero point loss with geometric accuracy orders of magnitude below sensor precision during projection, while the baseline PBEA method exhibited substantial geometric errors (0.027 Chamfer Distance on KITTI) and significant sampling losses (8.69% on KITTI). The algorithm demonstrated robustness through its iterative approach combining Hough Transform with weighted least squares fitting and backtracking-based conflict resolution. Comprehensive ablation studies revealed that the core mathematical framework performs reliably on well-populated data even without domain-specific heuristics, thereby validating the fundamental soundness of the approach. Beyond accuracy, ALICE-LRI maintained computational efficiency suitable for real-time applications, with projection and unprojection operations completing in milliseconds and achieving substantial computational headroom. In practical applications, ALICE-LRI enabled better compression efficiency without sacrificing point cloud quality, achieving higher compression ratios at equivalent reconstruction fidelity. ALICE-LRIs sensor-agnostic design enables deployment across diverse LiDAR platforms without any manual tuning or manufacturer metadata. This establishes new paradigm prioritizing geometric fidelity over computational simplicity, addressing critical gap in 3D perception systems. The opensource implementation facilitates adoption for applications demanding high-precision geometric modeling. The implications extend beyond range image generation to any application requiring accurate geometric modeling of LiDAR data. As autonomous vehicles, robotics, and mapping applications increasingly demand high-fidelity 3D perception, ALICE-LRI provides foundation for more reliable and accurate LiDAR processing pipelines. The ability to achieve lossTable 9: Mean runtime per frame for the original RTST algorithm and the ALICE-LRI modified version across different error thresholds. Execution times are reported in milliseconds. The minimal overhead introduced by ALICE-LRI ensures real-time viability while providing substantial quality improvements. Error Threshold Encoding Time (ms) Decoding Time (ms)"
        },
        {
            "title": "Original Modified Overhead Original Modified Overhead",
            "content": "0.001 0.010 0.050 0.100 0.250 0.500 0.750 1.000 20.79 19.71 22.57 30.25 43.27 56.98 67.08 75.18 23.24 26.00 28.70 37.67 49.89 65.27 72.41 80.06 2.45 6.29 6.13 7.43 6.63 8.30 5.33 4.88 3.84 4.08 4.85 4.05 4.47 4.03 4.83 4.17 6.70 6.78 7.34 7.55 7.52 7.50 7.50 7. 2.86 2.70 2.49 3.51 3.05 3.46 2.66 3.14 less range image generation without manufacturer metadata removes significant barrier to deploying LiDAR-based systems across diverse sensor platforms and application domains. 9. Future Work Our approach focuses on intrinsic Several promising research directions emerge from our findings. Below, we outline key areas for future investigation that could further advance LiDAR-based perception and geometric modeling. current sensor parametersgeometric distortions arising from the physical design and layout of the laser beams. However, in datasets such as KITTI odometry, the ideal spherical projection model is affected not only by intrinsic factors but also by extrinsic corrections applied to compensate for ego-motion during data acquisition. When LiDAR sensors are mounted on moving platforms (e.g., vehicles or robots), point clouds are often motion-corrected to account for sensor displacement and rotation during scanning. These corrections introduce additional geometric transformations that deviate from the static sensor model. Future work could extend our framework to simultaneously estimate intrinsic sensor parameters and extrinsic motion effects. While access to uncorrected raw data or known extrinsic parameters is often available in practice, developing methods that directly account for ego-motion corrections would increase the generality of our approach. This would enable lossless range image generation from broader class of datasets. Beyond compression, the impact of lossless range images on downstream perception tasks needs further investigation. Evaluating their effects on semantic segmentation, object detection, and scene understanding could quantify the benefits of eliminating projection artifacts and preserving geometric fidelity. The inferred sensor parameters also open the door to sensor-aware point cloud upsampling and reconstruction. Once sensor geometry is accurately characterized, additional points can be generated in manner consistent with the physical scanning pattern and measurement characteristics. This capability would be particularly valuable for enhancing sparse automotive LiDAR data and for generating training datasets that accurately reflect sensor-specific properties for machine learning applications. Finally, even though our current implementation demonstrates real-time viability, it remains single-threaded. Exploiting the inherently parallel nature of per-point projections through multi-threading and GPU acceleration could dramatically reduce execution times, freeing computational resources for downstream tasks and enabling deployment in highthroughput systems. Acknowledgments this research. resources necessary for supercomputer, which enabled the The authors acknowledge CESGA (Centro de Sufor providing access to the percomputación de Galicia) comFinisterrae-III putational This research was funded by the Agencia Estatal de Investigación (Spain) (MCIN/AEI/10.13039/501100011033) codes and PID2022-141623NB-I00, PID2019-104834GB-I00, PREP2022-000375, the Xunta de Galicia - Consellería de Cultura, Educación, Formación Profesional Universidades (Centro de investigación de Galicia accreditation 2024-2027 ED431G-2023/04 and Reference Competitive Group accreditation ED431C-2022/016), the European Union (European Regional Development Fund - ERDF/EU). Declaration of Generative AI and AI-assisted technologies in the writing process During the preparation of this work the authors used ChatGPT in order to assist with drafting, rephrasing, and improving the clarity and structure of the text. After using this tool/service, the authors reviewed and edited the content as needed and take full responsibility for the content of the publication. References [1] N. Haala, et al., Mobile LiDAR mapping for 3D point cloud collection in urban areasa performance test, in: International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences, volume 37, 2008, pp. 11191127. 24 [2] J. Zhang, S. Singh, LOAM: LiDAR odometry and mapin: Robotics: Science and Systems ping in real-time, (RSS), volume 2, 2014. [3] X. Chen, I. Vizzo, T. Läbe, J. Behley, C. Stachniss, Range image-based LiDAR localization for autonomous vehicles, in: Proceedings of the 2021 IEEE International Conference on Robotics and Automation (ICRA), IEEE, 2021, pp. 58025808. [4] J. Lichtenfeld, K. Daun, O. von Stryk, Efficient dynamic LiDAR odometry for mobile robots with structured point clouds, in: Proceedings of the 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), IEEE, 2024, pp. 1013710144. [5] J. Behley, C. Stachniss, Efficient surfel-based SLAM usin: Proing 3D laser range data in urban environments, ceedings of Robotics: Science and Systems, Pittsburgh, Pennsylvania, 2018. doi:10.15607/RSS.2018.XIV.01 6. [6] Y. Li, J. Ibanez-Guzman, LiDAR for autonomous driving: The principles, challenges, and trends for automotive LiDAR and perception systems, IEEE Signal Processing Magazine 37 (2020) 5061. [7] J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke, C. Stachniss, J. Gall, SemanticKITTI: dataset for semantic scene understanding of LiDAR sequences, in: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019. [8] A. Milioto, I. Vizzo, J. Behley, C. Stachniss, RangeNet++: Fast and accurate LiDAR semantic segmentation, in: 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), IEEE, 2019, pp. 42134220. [9] U. Weiss, P. Biber, Plant detection and mapping for agricultural robots using 3D LiDAR sensor, Robotics and Autonomous Systems 59 (2011) 265273. [10] D. Hutabarat, et al., LiDAR-based obstacle avoidance for the autonomous mobile robot, in: 2019 12th International Conference on Information & Communication Technology and System (ICTS), IEEE, 2019, pp. 137142. [11] G. P. Meyer, A. Laddha, E. Kee, C. Vallespi-Gonzalez, C. K. Wellington, LaserNet: An efficient probabilistic 3D object detector for autonomous driving, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 1267712686. [12] B. Wu, A. Wan, X. Yue, K. Keutzer, SqueezeSeg: Convolutional neural nets with recurrent CRF for real-time road-object segmentation from 3D LiDAR point cloud, in: 2018 IEEE International Conference on Robotics and Automation (ICRA), IEEE, 2018, pp. 18871893. [13] T. Shan, J. Wang, F. Chen, P. Szenher, B. Englot, Simulation-based LiDAR super-resolution for ground vehicles, Robotics and Autonomous Systems 134 (2020) 103647. [14] X. Zhou, C. R. Qi, Y. Zhou, D. Anguelov, RIDDLE: LiDAR data compression with range image deep delta encoding, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 1721217221. [15] T. Wu, H. Fu, B. Liu, H. Xue, R. Ren, Z. Tu, Detailed analysis on generating the range image for LiDAR point cloud processing, Electronics 10 (2021) 1224. [16] W. Dong, K. Ryu, M. Kaess, J. Park, Revisiting LiDAR registration and reconstruction: range image perspective, arXiv preprint arXiv:2112.02779 (2021). [17] I. Bogoslavskyi, C. Stachniss, Efficient online segmentation for sparse 3D laser scans, PFGJournal of Photogrammetry, Remote Sensing and Geoinformation Science 85 (2017) 4152. [18] L. T. Triess, D. Peter, C. B. Rist, J. M. Zollner, Scanbased semantic segmentation of LiDAR point clouds: An in: 2020 IEEE Intelligent Vehicles experimental study, Symposium (IV), IEEE, 2020, pp. 11161121. [19] L. Kong, Y. Liu, R. Chen, Y. Ma, X. Zhu, Y. Li, Y. Hou, Y. Qiao, Z. Liu, Rethinking range view representation for LiDAR segmentation, in: Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 228240. [20] B. Li, T. Zhang, T. Xia, Vehicle detection from 3D LiDAR using fully convolutional network, arXiv preprint arXiv:1608.07916 (2016). [21] X. Chen, H. Ma, J. Wan, B. Li, T. Xia, Multi-view 3D object detection network for autonomous driving, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 19071915. [22] J. Zhou, X. Tan, Z. Shao, L. Ma, FVNet: 3D front-view proposal generation for real-time object detection from point clouds, in: 2019 12th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI), IEEE, 2019, pp. 18. [23] C. Chen, W. Ge, SRMamba: Mamba for super-resolution of LiDAR point clouds, arXiv preprint arXiv:2505.10601 (2025). [24] Y. Kwon, M. Sung, S.-E. Yoon, Implicit LiDAR network: LiDAR super-resolution via interpolation weight prediction, in: 2022 International Conference on Robotics and Automation (ICRA), IEEE, 2022, pp. 84248430. 25 [25] D. Tian, D. Zhao, D. Cheng, J. Zhang, LiDAR superresolution based on segmentation and geometric analysis, IEEE Transactions on Instrumentation and Measurement 71 (2022) 117. [26] C. Tu, E. Takeuchi, C. Miyajima, K. Takeda, Compressing continuous point cloud data using image compression methods, in: 2016 IEEE 19th International Conference on Intelligent Transportation Systems (ITSC), IEEE, 2016, pp. 17121719. [27] C. Tu, E. Takeuchi, A. Carballo, K. Takeda, Real-time streaming point cloud compression for 3D LiDAR sensor using U-Net, IEEE Access 7 (2019) 113616113625. [28] C. Tu, E. Takeuchi, A. Carballo, C. Miyajima, K. Takeda, Motion analysis and performance improved method for IEEE transactions 3D LiDAR sensor data compression, on intelligent transportation systems 22 (2019) 243256. [29] Y. Feng, S. Liu, Y. Zhu, Real-time spatio-temporal LiDAR point cloud compression, in: 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), IEEE, 2020, pp. 1076610773. [30] Q. Wang, L. Jiang, X. Sun, J. Zhao, Z. Deng, S. Yang, An efficient LiDAR point cloud map coding scheme based on segmentation and frame-inserting network, Sensors 22 (2022) 5108. [31] N. Muhammad, S. Lacroix, Calibration of rotating in: 2010 IEEE/RSJ International multi-beam LiDAR, Conference on Intelligent Robots and Systems, IEEE, 2010, pp. 56485653. [32] C. Glennie, D. D. Lichti, Static calibration and analysis of the Velodyne HDL-64E S2 for high accuracy mobile scanning, Remote sensing 2 (2010) 16101624. [33] C. L. Glennie, A. Kusari, A. Facchin, Calibration and stability analysis of the VLP-16 laser scanner, The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences 40 (2016) 5560. [34] T. O. Chan, D. D. Lichti, Feature-based self-calibration of Velodyne HDL-32E LiDAR for terrestrial mobile mapping applications, in: The 8th International Symposium on Mobile Mapping Technology, Tainan, Taiwan, 2013, pp. 13. [35] P. V. Hough, Method and means for recognizing complex patterns, 1962. US Patent 3,069,654. [36] R. O. Duda, P. E. Hart, Use of the Hough transformation to detect lines and curves in pictures, Communications of the ACM 15 (1972) 1115. [37] J. Wang, P. J. Howarth, Use of the Hough transform in IEEE transactions on geoscience automated lineament, and remote sensing 28 (1990) 561567. [38] N. Aggarwal, W. C. Karl, Line detection in images through regularized Hough transform, IEEE transactions on image processing 15 (2006) 582591. [39] F. Tarsha-Kurdi, T. Landes, P. Grussenmeyer, Houghtransform and extended RANSAC algorithms for automatic detection of 3D building roof planes from LiDAR data, in: ISPRS Workshop on Laser Scanning 2007 and SilviLaser 2007, volume 36, 2007, pp. 407412. [40] M. Yermo, R. Laso, O. G. Lorenzo, T. F. Pena, J. C. Cabaleiro, F. F. Rivera, D. L. Vilariño, Powerline detection and characterization in general-purpose airborne LiDAR surveys, IEEE journal of selected topics in applied earth observations and remote sensing 17 (2024) 1013710157. [41] A. H. Safaie, H. Rastiveis, A. Shams, W. A. Sarasua, J. Li, Automated street tree inventory using mobile LiDAR point clouds based on Hough transform and active contours, ISPRS Journal of Photogrammetry and Remote Sensing 174 (2021) 1934. [42] T. Risse, Hough transform for line recognition: Complexity of evidence accumulation and cluster detection, Computer Vision, Graphics, and Image Processing 46 (1989) 327345. [43] D. E. Knuth, The art of computer programming. Vol.3: Sorting and searching, 1973. [44] A. Geiger, P. Lenz, C. Stiller, R. Urtasun, Vision meets robotics: The KITTI dataset, The International Journal of Robotics Research 32 (2013) 12311237. [45] T. Popoviciu, Sur les équations algébriques ayant toutes leurs racines réelles, Mathematica 9 (1935) 20. [46] Velodyne Lidar, Inc., Users Manual and Programming Guide: HDL-64E S3 High-Definition LiDAR Sensor, 2019. URL: https://www.researchgate.net/pro file/Joerg_Fricke/post/How_the_LiDARs_phot odetector_distinguishes_lasers_returns/att achment/5fa947b8543da600017dcf9b/AS%3A9559 57442002980%401604929423693/download/HDL-6 4E_S3_UsersManual.pdf, accessed: 2025-10-07. [47] Ouster, Inc., OS1 LIDAR Sensor Datasheet, San Francisco, CA, USA, 2025. URL: https://data.ouste r.io/downloads/datasheets/datasheet-rev7p 1-os1.pdf, revision 7.1, Accessed: 2025-10-07. [48] Hesai Technology, Pandar128E3X User Manual: 128Channel Mechanical LiDAR, 2025. URL: https://ww w.hesaitech.com/product/pandar128/, version 128-en-241220, Accessed: 2025-10-07. [49] RoboSense, Ruby Plus Enhanced 128-Beam LiDAR User Guide, 2023. URL: https://cdn.robotshop.com/me dia/U/UTR/RB-Utr-09/pdf/robosense-ruby-plu s-enhanced-128-beam-lidar-for-l4-autonomou s-vehicles-user-guide.pdf, version 1.1, Accessed: 2025-10-07. 26 [50] L. Li, K. N. Ismail, H. P. Shum, T. P. Breckon, DurLAR: high-fidelity 128-channel LiDAR dataset with panoramic ambient and reflectivity imagery for multi-modal autonomous driving applications, in: 2021 International Conference on 3D Vision (3DV), IEEE, 2021, pp. 1227 1237. [51] Fundación Pública Gallega Centro Tecnológico de Supercomputación de Galicia, CESGA FinisTerrae III, https: //www.cesga.es/en/infrastructures/computin g/finisterrae-iii/, 2022. Spanish Supercomputing Network (RES). [52] R. D. Hipp, SQLite, https://www.sqlite.org/index .html, 2025. [53] T. Wu, L. Pan, J. Zhang, T. Wang, Z. Liu, D. Lin, Densityaware chamfer distance as comprehensive metric for point cloud completion, arXiv preprint arXiv:2111.12702 (2021). [54] J. Heo, C. Phillips, A. Gavrilovska, FLICR: fast and lightweight LiDAR point cloud compression based on lossy RI, in: 2022 IEEE/ACM 7th Symposium on Edge Computing (SEC), IEEE, 2022, pp. 5467. [55] F. Lin, Y. Yue, S. Hou, X. Yu, Y. Xu, K. D. Yamada, Z. Zhang, Hyperbolic chamfer distance for point cloud completion, in: Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 14595 14606. [56] F. Williams, Point cloud utils, https://github.com/f williams/point-cloud-utils, 2022."
        }
    ],
    "affiliations": [
        "Centro Singular de Investigación en Tecnoloxías Intelixentes (CiTIUS), Rúa de Jenaro de la Fuente Domínguez, Santiago de Compostela, 15782, Coruña, Spain",
        "Departamento de Electrónica Computación, Universidade de Santiago de Compostela, Rúa Lope Gómez de Marzoa, Santiago de Compostela, 15782, Coruña, Spain"
    ]
}