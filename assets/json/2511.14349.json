{
    "paper_title": "ARC-Chapter: Structuring Hour-Long Videos into Navigable Chapters and Hierarchical Summaries",
    "authors": [
        "Junfu Pu",
        "Teng Wang",
        "Yixiao Ge",
        "Yuying Ge",
        "Chen Li",
        "Ying Shan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The proliferation of hour-long videos (e.g., lectures, podcasts, documentaries) has intensified demand for efficient content structuring. However, existing approaches are constrained by small-scale training with annotations that are typical short and coarse, restricting generalization to nuanced transitions in long videos. We introduce ARC-Chapter, the first large-scale video chaptering model trained on over million-level long video chapters, featuring bilingual, temporally grounded, and hierarchical chapter annotations. To achieve this goal, we curated a bilingual English-Chinese chapter dataset via a structured pipeline that unifies ASR transcripts, scene texts, visual captions into multi-level annotations, from short title to long summaries. We demonstrate clear performance improvements with data scaling, both in data volume and label intensity. Moreover, we design a new evaluation metric termed GRACE, which incorporates many-to-one segment overlaps and semantic similarity, better reflecting real-world chaptering flexibility. Extensive experiments demonstrate that ARC-Chapter establishes a new state-of-the-art by a significant margin, outperforming the previous best by 14.0% in F1 score and 11.3% in SODA score. Moreover, ARC-Chapter shows excellent transferability, improving the state-of-the-art on downstream tasks like dense video captioning on YouCook2."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 9 4 3 4 1 . 1 1 5 2 : r ARC-Chapter: Structuring Hour-Long Videos into Navigable Chapters and Hierarchical Summaries Junfu Pu, Teng Wang, Yixiao Ge, Yuying Ge, Chen Li, Ying Shan ARC Lab, Tencent PCG Core contributors, Project lead The proliferation of hour-long videos (e.g., lectures, podcasts, documentaries) has intensified demand for efficient content structuring. However, existing approaches are constrained by small-scale training with annotations that are typical short and coarse, restricting generalization to nuanced transitions in long videos. We introduce ARC-Chapter, the first large-scale video chaptering model trained on over million-level long video chapters, featuring bilingual, temporally grounded, and hierarchical chapter annotations. To achieve this goal, we curated bilingual English-Chinese chapter dataset via structured pipeline that unifies ASR transcripts, scene texts, visual captions into multi-level annotations, from short title to long summaries. We demonstrate clear performance improvements with data scaling, both in data volume and label intensity. Moreover, we design new evaluation metric termed GRACE, which incorporates many-to-one segment overlaps and semantic similarity, better reflecting real-world chaptering flexibility. Extensive experiments demonstrate that ARC-Chapter establishes new state-of-the-art by significant margin, outperforming the previous best by 14.0% in F1 score and 11.3% in SODA score. Moreover, ARC-Chapter shows excellent transferability, improving the state-of-the-art on downstream tasks like dense video captioning on YouCook2. Date: November 18, 2025 Github: https://github.com/TencentARC/ARC-Chapter"
        },
        {
            "title": "1 Introduction",
            "content": "The exponential proliferation of long-form video content, including educational lectures, vlogs, live streams, and meeting recordingsposes significant challenges for automatic content understanding. Video chaptering [35; 44] has emerged as promising solution, segmenting videos into navigable and semantically coherent chapters. This enables efficient content retrieval, summarization, and enhanced user interaction, which are critical for managing and consuming large-scale video data. Despite notable advances in segmenting short videos (usually within five minutes) for tasks such as action segmentation [8; 22; 27; 32; 39], temporal event localization [16; 54], and dense video captioning [19; 38; 46], the structuring of hour-long videos remains formidable challenge. First, modeling sophisticated semantics across multimodal inputs, including visual and audio streamsover extended temporal horizons requires robust and scalable architectures. Second, the scarcity of large-scale datasets with fine-grained annotations hinders the development and evaluation of effective chaptering models. Third, existing evaluation metrics [10; 19] often fail to capture the semantic granularity of chapter boundaries, leading to suboptimal matching and similarity scoring between predicted and ground-truth segments [10]. In this technical report, we introduce ARC-Chapter, comprehensive framework designed to address the unique challenges of long-form video structuring. As illustrated in Fig. 1, ARC-Chapter enables the segmentation of lengthy videos into navigable chapters and generates hierarchical summaries that capture both coarse and fine-grained content structure. Our work makes three primary contributions. First, we advance the scalability of video chaptering by developing the first large-scale model trained on one million long videos, totaling 400,000 hours of content. This dataset is fifty times larger than those used in previous studies [35], allowing our model to generalize across diverse video domains and formats. Second, we propose semi-automatic annotation pipeline for hierarchical summaries, which leverages easily accessible human-annotated coarse labels. This pipeline integrates automatic speech recognition (ASR) derived transcripts with timestamped visual Figure 1 An illustration of the capabilities of our video chaptering model. Given video, our model is able to generate timestamped chapters with three-level structured output: 1) Short Title - concise label summarizing each chapter; 2) Structural Chapter - detailed, structured annotation for each chapter, including rewritten comprehensive title, an abstract summarizing the core content, and an introduction describing key details and highlights; and 3) TimestampAligned Video Description - fine-grained descriptions aligned with precise temporal boundaries. This hierarchical structure facilitates an efficient and precise understanding of video content. elements, enabling holistic and multimodal understanding of video content. Third, we introduce GRACE, novel granularity-robust evaluation metric designed to address the semantic misalignment issues prevalent in existing chaptering benchmarks. GRACE provides more accurate assessment of chapter boundary quality by accounting for varying levels of semantic granularity. Our extensive experiments demonstrate the effectiveness of ARC-Chapter, which establishes new stateof-the-art on both Chinese and English long-form video chaptering benchmarks. Specifically, ARC-Chapter substantially outperforms previous methods on the VidChapters-7M test sets (e.g., CIDEr: 100.9186.6; F1: 45.359.3; SODA: 19.330.6). We validate the importance of multimodality, showing that our full model surpasses video-only and audio-only variants by 7.7 and 5.3 points on SODA, respectively. Furthermore, pretraining on our large-scale dataset significantly enhances transferability, evidenced by notable performance gains on downstream tasks like YouCook2 and ActivityNet Captions. Crucially, our work is the first to identify clear scaling law in video chaptering: model performance consistently improves with increased training data and label density. This finding refutes previous observations that performance saturates on smaller datasets (20k samples) [35] and suggests promising direction for future research. The remainder of this report is structured as follows: Section 2 reviews related works; Section 3 describes the dataset and annotation pipeline; Section 4 details our methodology and model architecture; Section 5 presents experimental results and analysis; Section 6 concludes."
        },
        {
            "title": "2 Related Works",
            "content": "Global Video Understanding. Early video understanding [1; 7; 13; 23; 26; 33; 37; 41; 42; 49; 52; 53; 57] research primarily targeted global comprehension tasks, such as video question answering, video captioning, and video classification. These methods treat entire videos as holistic units, extracting global representations to predict semantic labels or generate summaries. While effective for short videos, they often fail to capture complex temporal dynamics and hierarchical structures of long-form content [24; 30]. Figure 2 Overview of our automatic video annotation pipeline for hierarchical chaptering and summarization. We extract visual captions (OCR included) from sampled video frames and ASR transcripts from audio. These outputs are temporally aligned and interleaved into unified multimodal transcript. This transcript, together with original chapter markers, is processed by an LLM to produce structured chapters and timestamp-aligned video descriptions. Temporal Segmentation for Short Videos. To address the limitations of global approaches, recent works [14; 15; 17; 28; 30; 40; 47; 50; 56] have shifted towards modeling the temporal structure of videos. Datasets like ActivityNet Captions [19], Charades-STA [11], YouCook2 [55] and Breakfast [21] provide timestamped event annotations, enabling tasks such as temporal event localization, action segmentation, and dense video captioning. These approaches move beyond global representations to identify and describe fine-grained events and local temporal dependencies. However, most temporally-structured datasets [25; 48] are limited to short clips, typically under several minutes, and thus do not capture the challenges of ultra-long videos found in lectures, podcasts, or livestreams. The lack of large-scale, long-duration datasets with fine-grained temporal annotations remains major bottleneck. Long-Form Video Structuring. few efforts [35; 45] have explored the structuring of hour-long videos. The VidChapters-7M dataset [45] provides large-scale benchmark for video chaptering, with millions of videos and annotated chapter boundaries, better reflecting real-world scenarios such as vlogs, podcasts, and meetings where long-term temporal reasoning is essential. Despite these advances, significant challenges remain. Existing chaptering models often rely on limited modalities, such as automatic speech recognition, are trained on small-scale datasets, and produce coarse, uninformative descriptions, which limits their scalability across diverse video domains. To address these issues, we propose scalable, multimodal framework for long-form video chaptering, supported by large-scale dataset with detailed chapter descriptions."
        },
        {
            "title": "3 Data Collection and Annotation",
            "content": "A significant challenge in developing strong video chaptering models is the scarcity of publicly available datasets with detailed, multi-level annotations. Existing datasets typically provide only sparse labels, such as video-level categories for video classification or coarse temporal segments with brief titles such as VidChapters7M. To address this limitation and to facilitate research on hierarchical video chaptering and summarization, we introduce new, richly annotated video chaptering dataset. This section details our data curation and annotation pipeline."
        },
        {
            "title": "3.1 Data Curation",
            "content": "One of the key contributions of our work is the introduction of new large-scale dataset, named VidAtlas, which is designed for the task of hierarchical video chaptering and summarization. Our primary goal is to construct dataset that not only provides accurate chapter boundaries but also offers dense, multi-granularity textual descriptions for both individual chapters and the entire video. Data Sourcing. We begin by sourcing videos from the video platform. The primary selection criterion is the presence of author-provided chapter markers. These markers, which include the start/end timestamps and short title for each chapter, are manually defined by the video uploader. This approach provides us with highly accurate human-verified ground truth for the temporal segmentation of videos, which is significant foundation for our subsequent annotation efforts. The collected videos, which are long, well-structured, and information-dense, are ideal candidates for video chaptering. Filtering and Refinement. Starting with this initial collection, we apply several filtering criteria to guarantee the quality and diversity of our dataset for video understanding and chaptering. First, we retain videos whose durations lie between 2 minutes and 3 hours. This range excludes trivial short clips, which are unnecessary for chaptering, as well as overly long videos, which are often unstructured (e.g., live streams) and difficult to process due to the context-length limitations of our model. Second, we curate videos across wide range of domains, including educational lectures, DIY tutorials, reviews & unboxings, interviews & podcasts, webinars & presentations, gaming & music albums, fitness & cooking and documentaries. This wide distribution of domains ensures that the dataset is not biased towards any specific genre and supports the development of more generalizable models. (a) Duration distribution (b) Categories in dataset Figure 3 Dataset statistics: (a) Distribution of video durations (top) and chapter durations (bottom) in the VidAtlas dataset. (b) Distribution of video topics in VidAtlas."
        },
        {
            "title": "3.2 Hierarchical Annotation",
            "content": "To generate high-quality video chaptering annotations, we design an automated annotation pipeline that leverages both multimodal content extraction and large language model (LLM)-based reasoning based on the videos with user-provided chapter makers, i.e. timestamps and brief title of each chapter. The illustration of our annotation pipeline is shown in Fig. 2. Multimodal Information Extraction. Considering efficiency and cost, we avoid directly using multimodal large language models (MLLMs) for video annotation. Instead, we first extract multimodal information from video frames and audio, integrate this content, and then feed the result into text-only LLM for reasoning and annotation. Specifically, we use Whisper-v3 [29] to transcribe speech into text, segmented into sentences with the corresponding timestamps. In parallel, we uniformly sample video frames with fixed sampling frame rate and employ Qwen2.5-VL-7B [4] to extract visual captions and on-screen text (OCR) for better understanding of the video content. Subsequently, the visual captions and ASR transcripts are temporally aligned based on their respective timestamps. This process allows us to interleave the textual content from both modalities into unified chronologically ordered sequence. This multimodal transcript, together with the original user-provided chapter timestamps and short titles, is fed into LLM for reasoning and structural segmentation. LLM Reasoning and Chaptering. The LLM is prompted to analyze the transcript and reorganize the content into structured set of chapters, each containing comprehensive title, an abstract, an introduction, and precise temporal boundaries. Following this, we perform verification step on the LLMs output to ensure that the generated chapter boundaries strictly adhere to the original timestamps. Building upon the verified structured chapter information, we further prompt the LLM to produce comprehensive, timestamped narrative description for the entire video. Through this annotation pipeline, we can efficiently obtain accurate, multi-level video chapter segmentation and descriptive annotations. The resulting annotations form dense, hierarchically organized representation of long-form videos, supporting wide range of research tasks in video understanding, temporal reasoning, chaptering, and summarization."
        },
        {
            "title": "3.3 Dataset Statistics",
            "content": "We summarize the key statistics of our VidAtlas dataset and highlight the properties that make it suited for research on video chaptering and summarization. The dataset comprises 410k+ videos with an average duration of 16.8 minutes, amounting to more than 115k hours of diverse content. On average, each video is segmented into 5.5 chapters, with an average chapter duration of 182 seconds (approximately 3 minutes). Fig. 3a provides detailed statistic of the duration distributions for both videos and chapters. Our dataset contains wide spectrum of video and chapter lengths to ensure models are trained on diverse temporal structures. This comprehensive video/chapter length distribution makes the models exposed to variety of content length, from concise segments to hour-long narratives, forcing models to resolve both rapid topic shifts and sustained thematic segments. To mitigate genre bias, VidAtlas covers wide array of topics, including 16 primary categories with over 100 subcategories, as shown in Fig. 3b. The categories of VidAtlas include Games, Knowledge, Technology, Music, Life, Animation, and Sports, together with other variety that captures long-tail topics. Videos in these categories are typically well-structured and information-dense, making them ideal for chaptering."
        },
        {
            "title": "4.1 Overall Framework",
            "content": "We leverage Qwen2.5-VL-7B [5] as our base model, enhancing its capabilities to process and structure video content into chapters. The architecture of our model is illustrated in Fig. 4. The model unifies three inputs: 1) an instruction prompt that specifies the task of input modalities and output schema. 2) sequence of sampled video frames that provide appearance, layout and on-screen text (including subtitles which often align with the ASR transcript), and 3) timestamp-aligned ASR transcript from audio. While both the video and ASR transcript inputs are optional, the model requires at least one modality to be provided. Frames are embedded with Qwen2.5-VL vision encoder and translated into visual tokens, while ASR transcript is tokenized as plain text with explicit timestamps. The vision encoder is kept frozen and the language model is instruction tuned on VidAtlas to specialize in video chaptering. Prompt Design. The models behavior is guided by carefully designed prompts that specify the desired task and output format. To handle the diverse requirements of different inputs and outputs of the model, we design set of 18 distinct prompt templates. These prompts are constructed based on three axes: language in source video, input modality, and desired output format. Language: We support English and Chinese to match the language of the source video. Input Modality: The prompt specifies whether the model should rely on ASR-only, video-only, or both video and ASR inputs. This allows for ablation studies and adaptation to scenarios where one modality may be absent or noisy. Figure 4 Overview of the model architecture for video chaptering. The model inputs include task-specific prompt, sampled video frames, and timestamped ASR transcripts. Video frames are processed with frozen vision encoder. The resulting visual features, along with the tokenized prompt and ASR text, are fed into trainable multimodal large language model (MLLM). Based on the inputs, the model is able to generate chapters in various formats, including timestamped concise title, detailed structural chapters, or comprehensive video description with timestamps. Output Format: We define three distinct output structures: (a) Short Titles for concise chapter markers, (b) Structured Chapters that include title, abstract, and introduction for each chapter, and (c) Video Descriptions that provide dense, timestamp-aligned summary of the entire video. Video Input. To balance temporal coverage and context budget, we follow the setup of Qwen2.5-VL and cap the visual stream at 768 frames sampled at up to 1 fps. That is to say, videos shorter than 12.8 minutes are sampled with 1 fps, while longer videos are uniformly down-sampled to 768 frames with lower fps. The sampling strategy retains coarse global coverage for hour-long content, ensuring sufficient representation to capture the high-level semantic shifts necessary for the chaptering task. Since the model context length is shared across modalities, we dynamically adjust the per-frame token allowance according to the input of ASR transcript. For video-only inputs we use higher frame resolution (higher token budget per frame) so that small text (OCR and subtitles) and fine-grained visual cues are preserved. When ASR is provided alongside video, we reduce frame resolution (thus reducing the number of visual tokens) so that the combined input of visual tokens and ASR text fits the maximum context length of MLLM. This dynamic allocation is implemented by adjusting image scaling and patch-tokenization parameters at preprocessing time. Moreover, to enhance temporal awareness, we randomly overlay timestamps onto the video frames, making the model more sensitive to the video timeline. ASR Input. Although integrating raw audio features or learned audio embeddings from pretrained ASR models (e.g.Whisper [29]) is attractive, it presents severe scalability challenges for long-form video. For example, while Whisper-style audio encoder produces 50 audio tokens per second, 60-minute audio therefore produces 180k tokens, far exceeding feasible LLM context budgets without aggressive compression or specialized audio-to-token aggregation. Furthermore, synchronizing fixed-rate audio features with dynamically sampled video frames poses an additional alignment problem. To address these practical constraints, we opt to use ASR transcripts as highly effective proxy for the audio modality. Text is significantly more information-dense. Therefore, the ASR transcript of long audio segment occupies far fewer tokens than its raw feature representation. This makes processing hour-long videos computationally feasible for both training and inference. Although such paradigm introduces an extra step for offline ASR transcription, we believe that trading modest amount of offline processing time for the ability to handle long-form audio under strict context-length budgets is worthwhile. In our implementation, we use Whisper-large-v3 [29] to generate timestamped ASR transcripts. The model provides sentence-level segments with corresponding start timestamps. We formulate the ASR text and timestamp of each segment as start time (hh:mm:ss): <ASR text>. The normalized ASR transcript is then passed to the model either alone (ASR-only) or together with visual tokens (ASR+Video), providing dense semantic information that is particularly useful for temporal boundary detection and chaptering."
        },
        {
            "title": "4.2 Training Strategy\nTraining Objective. We perform supervised instruction tuning on VidAtlas and VidChapter-7M using all\nprompt templates. The training objective is the standard autoregressive next-token prediction loss over the\ntarget sequence. Given a multimodal input sequence consisting of a prompt Xprompt, video frames Xvideo,\nand an ASR transcript Xasr (video stream Xvideo and ASR streams Xasr are optional), the model is trained\nto maximize the log-likelihood of the target output sequence Y = (y1, y2, ..., yn) (e.g., a list of chapter titles, a\nstructured chapter object, or a timestamped description):",
            "content": "L = (cid:88) i=1 log (yi y<n, Xprompt, Xvideo, Xasr) , where y<i represents the preceding ground-truth tokens. During training, the vision encoder is frozen to enable larger context length, while all parameters of the large language model are optimized with the training objective. Adaptive Modality Dropping. To enable single model to perform well under various deployment conditions, we adopt an adaptive modality dropping strategy during training. For each training sample, we randomly configure the input with certain probability to be one of three types: 1) Video + ASR: Both modalities are provided to the model. 2) Video-only: The ASR transcript is omitted, forcing the model to rely solely on visual information. and 3) ASR-only: The video frames are omitted, requiring the model to understand the content based on the transcript alone. This strategy prevents the model from becoming overly reliant on single modality and ensures it develops comprehensive understanding from all available input modalities. Consequently, single trained model can be deployed to handle videos under various conditions during inference (whether only video is available, only transcript is provided, or both are present), without requiring specialized models for each scenario."
        },
        {
            "title": "4.3 Evaluation Metrics",
            "content": "Evaluation metrics can be divided into two aspects: (1) the accuracy of segmentation (e.g., Precision, Recall, and tIOU [20]), and (2) joint metrics that assess both segmentation and chapter captioning (e.g., CIDEr [20], SODA [10]). However, we observe that the primary metrics such as SODA, originally developed for dense video captioning, are not well-suited for the video chaptering task. While SODA enforces one-to-one matching between predicted and ground-truth events to suppress redundancy in overlapping event detection, video chaptering requires segmenting videos into sequential, non-overlapping chapters. Furthermore, chaptering annotations often exhibit granularity ambiguity: different annotators may segment the same video at varying levels of detailsome may annotate coarse-grained chapters (e.g., by day in travel vlog), while others may provide fine-grained chapters (e.g., by each visited site within day). This results in multiple valid annotation granularities for the same content. To address these challenges, we propose GRACE, metric tailored for video chaptering. It introduces many-to-one (set-to-one) matching paradigm, allowing each ground-truth (predicted) chapter to be matched with set of predicted (ground-truth) chapters. As illustrated in Fig. 5, for each ground-truth chapter, GRACE evaluates the temporal overlap and semantic similarity between the chapter and its matched prediction set, using established language similarity metrics (e.g., BERTscore [51]) for textual comparison. Specifically, we aim to find best many-to-one mapping which splits both ground-truth set and prediction set into several pairs of groups {(Pi, Gi)}K i=1 , followed by group-based similarity calculation: (a) One-to-One Matching: SODA Pred p2 p3 Pred (b) Many-to-One Matching: GRACE p3 p2 GT g1 g2 g3 GT g2 g3 SODA = λ1Sim(p1, g1) + λ2Sim(p3, g3) GRACE = φ1Sim(p1, g1 g2) + φ2Sim(p2 p3, g3) Figure 5 Comparison of one-to-one (SODA) and many-to-one (GRACE) matching strategies. The one-to-one matching can fail to account for important events like p2 and g2, whereas the many-to-one strategy considers all predicted and ground-truth events for more robust, overall assessment. GRACE = (cid:88) φ(Pi, Gi) BERTscore(Pi, Gi) (Pi,Gi)M (P,G) φ(Pi, Gi) = 1 PiGi (cid:88) IOU(p, g) pPi,gGi (1) (2) s.t. Pi Pj = , (Pi) = P, Gi Gj = , (Gi) = G, min(Pi, Gi) = 1 where Pi and Gi epresent groups of chapters. When calculating the BERTScore between two groups, we first concatenate all captions within each group into single sentence, then compute the BERTScore between the two merged sentences. We adopt the dynamic time warping algorithm (DTW) [6; 31] to achieve the optimal matching (P, G), with IOU between two chapters being used as the matching criteria. GRACE provides more accurate and human-aligned assessment of chaptering models. This design confers several advantages: (1) robustness to annotation granularity, enabling fair evaluation across diverse annotation styles; (2) improved semantic fidelity, rewarding models that capture the full scope of ground-truth chapters; and (3) closer alignment with human judgment of chapter boundaries and content. (3)"
        },
        {
            "title": "4.4 Reinforcement Learning with GRPO",
            "content": "While supervised fine-tuning (SFT) achieves strong performance, the standard cross-entropy loss does not directly optimize for the primary objective of video chaptering: temporal accuracy. To further enhance the models temporal localization capabilities, we introduce subsequent reinforcement learning phase using the GRPO algorithm [12]. The core of this phase is reward function designed to directly incentivize precise chapter boundary prediction. We leverage our proposed GRACE metric, which holistically evaluates both temporal alignment and semantic content. However, to specifically sharpen the models ability to predict accurate timestamps of segmented chapters, we formulate simplified, temporal-only reward by omitting the semantic BERTscore component from Equation (1). For given ground-truth chapter set and model-generated set , the reward is calculated by summing the temporal alignment scores φ over the optimal matching (P, G) found via DTW: (4) (cid:88) = φ(Pi, Gi). (Pi,Gi)M (P,G) This reward directly reflects the quality of the temporal segmentation, providing clear and targeted optimization objective. Due to the significant context length required for multimodal inputs, and to specifically bolster the models ability to reason from visual cues, we conduct this RL training phase using only the video modality. We select diverse subset of 90k videos from both Chinese and English SFT data, ensuring that training samples cover all three output formats: short titles, structural chapters, and timestamped video description. We initialize the model with the weights from our best-performing SFT model and further optimize it using GRPO. The KL divergence coefficient is set to 0.01 to ensure that the policy does not stray far from the robust language generation capabilities learned during SFT, thereby balancing temporal refinement with descriptive quality. Table 1 Comparison to the state of the art on VidChapters7M-test set: The results of compared methods are evaluated in the ASR-only setting from Chapter-Llama [35]. We evaluate ARC-Chapter with different input modalities: -vid for video, -asr for ASR, and -vidasr for both. Ft. indicates whether the model is finetuned for chaptering task. denotes LLM-API results reported from Chapter-Llama. Our model, ARC-Cchapter, achieves the best performance across all metrics and video durations. Backbone Ft. Short F1 tIoU GPT-4o-mini [18] GPT-4o [18] Gemini-2.0-Flash [34] Gemini-1.5-Pro [34] Vid2Seq [45; 46] Llama 3.1-8B [9] Vid2Seq [45; 46] Chapter-Llama [35] ARCChapter-asr1 ARCChapter-vid ARCChapter-vidasr 32.1 37.7 39.9 41.7 2.5 29.9 33.4 45.5 54.5 52.6 60. 64.5 68.0 69.2 70.6 28.6 63.4 63.7 72.2 76.7 75.8 7.2 8.4 12.0 11.7 0.3 7.1 15.2 20.2 26.3 26. 42.4 53.8 72.8 65.3 0.3 34.5 74.9 103.5 144.1 156.8 Medium tIoU Long All F1 tIoU F1 tIoU 62.3 68.8 71.4 71.8 29.7 62.7 53.3 72. 77.5 75.3 6.1 8.1 11.2 11.2 0.3 5.4 7.5 18.8 25.1 20.6 30.6 51.4 70.3 61.4 0.4 28.1 31.9 98. 143.0 124.0 28.0 36.5 34.9 41.3 4.6 26.6 16.7 41.3 55.1 47.3 61.0 66.2 66.2 70.6 32.0 59.3 50.8 69. 77.0 72.3 6.0 6.6 9.0 10.1 0.3 3.6 5.9 15.8 24.8 19.2 27.3 34.8 51.6 55.3 0.5 18.9 28.4 91. 158.0 119.8 31.2 37.6 40.2 42.2 3.0 29.5 26.7 45.3 54.5 50.2 63.6 68.0 69.3 70.9 29.3 62.5 58.6 71. 76.7 74.3 6.8 8.1 11.4 11.4 0.3 6.2 11.6 19.3 25.3 22.9 37.8 51.0 69.7 63.2 0.4 30.7 55.8 100. 144.0 138.3 F1 30.5 38.1 43.8 43.8 3.2 30.6 19.0 46.7 55.9 51.4 80. 32.5 195.7 59.2 79.4 29.6 177. 60.2 79.9 29.2 190.3 59.3 79. 30.6 186.6 Table 2 Comparison to the state of the art on VidChapter7M-sml300 with different input modalities. Our method, ARCChapter, demonstrates superior performance on VidChapter-sml300 by effectively integrating both speech and video information. The modalities of Embed and Caption in LLaMA and Chapter-LLaMA models play the same role as Video in ARC-Chapter model. Method Ft. Modalities Segmentation Titles Speech Embed. Caption F1 tIoU LLaMA 3.1-8B Chapter-LLaMA ARCChapter Speech Video 12.6 22.7 29.9 38.5 38.4 39.1 40.4 42.6 44.4 56.5 50.0 62.4 48.6 57.3 63.0 68.1 66.5 67.7 68.2 70.6 71.5 tIoU 78.1 74.3 81.6 1.9 4.4 6.9 13.9 3.4 5.9 15.3 16.4 16.3 25.9 21.6 30.1 6.4 19.7 33. 67.3 7.3 20.2 74.9 82.4 84.2 148.5 130.8 190."
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we conduct series of experiments to thoroughly evaluate our video chaptering model. We first introduce the evaluation benchmarks, then present the main results and detailed ablation studies."
        },
        {
            "title": "5.1 Evaluation Benchmark",
            "content": "To comprehensively assess our models capabilities in video chaptering, we evaluate it on three distinct benchmarks covering different languages, scales, and data modalities. The evaluation targets two key criteria: the precision of temporal boundary localization and semantic relevance of the generated chapter titles/descriptions. VidChapters7M is large-scale English chaptering dataset. We use two of its standard splits for evaluation, i.e., VidChapters7M-test and VidChapters7M-sml300val. VidChapters7M-test is large-scale test set comprising 8.2k samples. For this split, the compared methods are only based on ASR 1For convenience, \"ARC-Chapter\" in the main text is abbreviated as \"ARCChapter\" in all experimental result tables. Table 3 Comparison to the state of the art on VidAtlas-test set: Ft. indicates whether the model is finetuned for chaptering task. Modality specifies which inputs are provided: for ASR and for video. denotes LLM-API results. For API-base models, the video is converted into textual description, which is then provided as input for LLM. Backbone Ft. Modality Claude-Sonnet [3] Doubao-1.5-Pro [13] DeepSeek-R1 [12] Gemini-2.5-Pro [7] GPT-4.1 [2] Qwen3-235B [43] Claude-Sonnet [3] Doubao-1.5-Pro [13] DeepSeek-R1 [12] Gemini-2.5-Pro[7] GPT-4.1 [2] Qwen3-235B [43] ARCChapter-asr ARCChapter-vid ARCChapter-vidasr Short tIoU 69.8 70.4 71.1 68.3 68.6 67.7 68.2 70.0 69.9 73.1 67.2 59.2 79.3 79. 7.6 7.4 11.0 8.1 6.6 7.7 7.9 7.7 10.5 9.8 6.3 6.5 24.1 21.2 F1 39.2 38.8 40.0 39.6 36.5 36.7 36.8 39.5 39.4 48.3 35.3 24. 57.3 57.1 38.8 40.6 48.8 44.6 34.6 36.9 42.4 43.3 50.0 54.9 34.2 31.9 103.3 91.5 34.7 35.8 37.9 30.6 33.0 33.5 32.0 35.5 38.0 45.4 30.8 19.5 60.1 55.9 Medium tIoU Long F1 tIoU F tIoU 66.3 68.4 69.5 60.1 66.1 65.6 65.2 67.6 68.7 70.1 64.2 52.9 80.8 78.2 6.5 6.9 9.6 6.3 5.8 6.7 8.0 7.6 10.8 11.8 6.2 5. 24.5 18.4 33.8 38.3 45.2 37.4 32.4 33.9 45.0 45.2 54.9 66.1 34.8 28.2 113.5 88.2 36.6 36.1 35.7 34.0 36.0 26.6 40.8 44.4 62.2 54.8 43.9 27. 63.2 62.0 66.9 67.1 66.8 60.2 66.3 61.0 68.2 69.8 80.3 75.3 69.2 57.8 79.5 79.4 5.8 3.2 6.3 9.9 5.9 3.8 16.8 14.9 48.2 30.6 19.1 16. 28.1 27.9 33.5 17.4 28.3 54.0 33.0 18.7 110.4 109.0 264.4 172.5 120.2 92.9 140.6 137.8 37.8 37.7 38.9 45.2 35.7 34.4 36.4 39.5 41.1 48.7 35.8 24. 58.8 57.6 68.6 69.5 70.1 73.2 67.7 66.2 67.5 69.4 70.5 72.8 66.9 57.7 79.7 78.9 All 7.1 6.7 10.0 9.7 6.3 6. 9.3 8.8 13.9 13.5 8.3 7.8 24.8 21.6 36.9 36.4 44.8 53.5 33.9 33.4 53.6 54.1 69.7 75.8 47.9 40. 111.3 98.1 11.1 9.8 13.4 14.9 - 10.2 13.2 12.6 17.1 19.8 11.7 9.6 28.0 25.0 65.5 83. 28.5 129.2 65.7 84.2 29.0 140.0 69. 84.2 38.5 192.3 66.2 84.0 30.2 141. 34.1 transcripts, while ARC-Chapter is evaluated with different input modalities. VidChapters7M-sml300val is smaller validation set of 300 samples, which includes both the original videos and their corresponding ASR transcripts. This subset is ideal for fast evaluation and conducting modality ablation studies. To assess generalization beyond English, we additionally report experimental results on VidAtlas-test, Chinese test set with more than 1.5k videos together with ASR transcripts and original videos."
        },
        {
            "title": "5.2 Comparison with the State of the Art\nPerformance on VidChapters7M. As shown in Tab. 1, our ARC-Chapter significantly outperforms all existing\nmethods on VidChapters7M-test benchmark. Our model achieves a new state-of-the-art result in the ASR-only\nregime, with an overall F1 score of 54.5, tIoU of 76.7, SODA of 23.5, and a CIDEr of 144.0. This represents\na substantial improvement over the previous SOTA model, Chapter-Llama, with absolute gains of +9.2 in\nF1, +4.9 in tIoU, and +6.0 in the SODA score. Notably, the performance gain enlarges as video duration\nincreases. For long videos (30-60 min), the evaluation metrics of SODA and CIDEr for ARC-Chapter are\nremarkably higher than which in Chapter-LLama, demonstrating the superior capability of our model in\nprocessing long videos. Even when compared against powerful general models like GPT-4o and Gemini-1.5-Pro,\nwhich are not finetuned on this task, ARC-Chapter perform much better. The experiments conducted on\nVidChapter7M-sml300 show more comparisons for different input modalities, shown in Tab. 2.",
            "content": "Performance on VidAtlas. As detailed in Tab. 3, we evaluate our model on the VidAtlas benchmark under three settings: ASR-only, video-only, and ASR+video. ARC-Chapter consistently establish new state-of-the-art across all settings. Our full multimodal model, ARCChapter-vidasr, which leverages both ASR and video inputs, achieves an overall F1 score of 66.2, tIoU of 84.0, SODA of 30.2, CIDEr of 141.5, and GRACE of 34.1. This marks significant leap over the strongest LLM, Gemini-2.5-Pro, with an absolute improvement of +17.5 in F1 score and more than doubling the SODA score (+16.7). Furthermore, our single-modality versions also demonstrate superior performance. The ASR-only model, ARCChapter-asr, achieves an F1 of 58.8, and the video-only model, ARCChapter-vid, scores an F1 of 57.6. From shot-to-long videos, our model consistently outperforms other models, demonstrating its robustness in handling extended content."
        },
        {
            "title": "5.3 Transferability",
            "content": "To evaluate transferability, we pre-trained ARC-Chapter on our dataset before fine-tuning and testing it on the dense video captioning benchmarks, i.e., Youcook2 and ActivityNet Captions. As shown in Table 4, our model establishes new state-of-the-art, significantly outperforming all prior MLLM-based methods. Notably, for event segmentation ability, ARC-Chapter achieves an F1/SODA Score of 37.9/12.5 on YouCook2, substantial improvement over the previous best of 33.5/7.9. This demonstrates that the knowledge acquired during pre-training effectively transfers and enhances performance on downstream tasks. Table 4 Transferability Performance on YouCook2 and ActivityNet Captions [20] for Dense Video Captioning. All methods use visual modality as inputs without ASR. The Rank() column represents the overall performance, calculated as the arithmetic mean of methods rank across all reported metrics (M, S, C, and F1) for that dataset. Some results for ActivityNet Captions are sourced from [14] and [46]. * indicates zero-shot evaluation. The best results on each dataset are in bold and the second-best are underlined. YouCook2 ActivityNet Captions Method GIT [36] ECHR [46] PDVC [46] Vid2Seq [46] CM2 TimeChat [30] VTimeLLM [17] Momentor [28] TRACE [14] VTG-LLM [15] TimeExpert [47] 3.4 3.8 4.7 9.3 - - - - - - - 3.1 - 4.4 7.9 5.3 3.4 - - 6.7 3.6 7.2 12.1 - 22.7 47.1 31.7 11.0 - - 35.5 13.4 39.0 17.7 - - 27.3 28.4 19.5 - - 31.8 20.6 33.5 Rank 7.8 7.2 8.0 8.5 - 7.5 4.0 5.0 2.8 4.7 8.0 - - 3.7 6.7 2.7 1. 5.7 6.8 4.7 6.4 5.9 7.0 8.1 5.7 3.2 5.4 5.8 - 4.7 5.8 2.3 6.0 5.1 6.5 5.9 29.8 14.7 29.0 30.1 - 19.0 27.6 14.9 25.9 20.7 28.4 35.4 F1 50.6 - 56.7 52.4 - 36.9 - - 39.3 34.8 40.5 55.9 Rank 4.3 8.6 3.8 2.6 - 8.8 5.8 10.7 5.8 8.3 4.3 2.0 ARC-Chapter 9.6 12.5 69.4 37."
        },
        {
            "title": "5.4 Ablation Studies",
            "content": "5.4.1 Scaling Property We analyze how ARC-Chapter scales with the amount of training data. Concretely, we subsample the training set at 20%, 40%, 60%, 80%, and 100% and keep the model architecture and prompt templates fixed. We evaluate three inference modalities, i.e.ASR-only, Video-only, and ASR+Video, on two benchmarks: VidChapters-7M (sml300val) and sampled subset of the VidAtlas-testset for efficiency. As illustrated in Fig. 6, the performance across all metrics (F1, tIOU, SODA, and CIDEr) and input modalities (ASR-only, Video-only, Video+ASR) demonstrates clear positive correlation with the amount of training data. Specifically, the full multimodal model (Video+ASR) consistently achieves the best performance. ARC-Chapter is highly data-efficient, achieving strong performance with as little as 20% of the training data. Furthermore, it is data-scalable, continuing to benefit from larger corpora for even better results. 5.4.2 Hierarchical Annotations core contribution of our work is the VidAtlas dataset, which features rich, hierarchical annotations. To validate the effectiveness of this data structure, we evaluate our models capability to generate outputs of varying complexity, from simple Short Title to detailed Structural Info which comprising title, abstract and introduction for each chapter. The results are presented in Table 5. From the experimental results, our model successfully learns to generate these complex, structured outputs, achieving strong performance across all generated components (title, abstract, introduction) on both VidChapter-sml300 and VidAtlas-testset benchmarks, particularly when using both video and ASR inputs. This demonstrates high degree of semantic understanding. More importantly, the capability for detailed generation does not come at the cost of performance on the fundamental chaptering task. When comparing the segmentation metrics (temporal evaluation score F1 and tIoU) for the Short Title task versus the more demanding Structural Info task, we observe only negligible difference. For example, on VidChapter-sml300, the multimodal model achieved an F1 score of 62.4 and tIoU of 81.6 for Short Title generation, compared to slightly lower scores of 61.4 and 80.6 for Structural Info generation. Notably, this small margin represents the largest performance gap observed across all modality inputs on both benchmarks, indicating that the model can perform complex, multi-part generation in single forward pass without compromising its core ability to accurately segment the video. This result strongly validates our hierarchical annotation strategy, demonstrating that training on such rich data endows the model with advanced structural reasoning capabilities. Figure 6 Data Scaling property of ARC-Chapter. We report the performance on VidChapter (a sampled subset) and VidAtlas test set with respect to different percentage of training samples. Table 5 Ablation study on the models capability to generate hierarchical annotations. We compare models trained with Short Title and Structural Info (structured chapters with short title, title, abstract, and introduction) across different input modalities (A for ASR, and for Video) on both English (VidChapter-sml300) and Chinese (VidAtlas-testset) benchmarks. Metrics include F1 and tIoU for boundary quality evaluation, and SODA(S), CIDEr(C), as well as our proposed GRACE(G) for semantic quality evaluation. Dataset Modality VidChapter-sml300 (English) VidAtlas-testset (Chinese) Short Title Structural Info Segmentation tIoU F1 Short Title Segmentation tIoU Short Title S 56.5 50.0 62.4 58.8 57.6 66. 78.1 74.3 81.6 79.7 78.9 84.0 25.9 21.6 30.1 24.8 21.6 30.2 148.5 130.8 190.7 111.3 98.1 141. 33.0 27.9 38.4 28.0 25.0 34.1 54.8 50.4 61.4 59.1 56.8 65.9 77.1 74.4 80.6 79.8 78.7 83. 25.5 22.3 30.8 25.5 22.0 30.8 147.9 136.4 194.5 112.8 97.8 143.5 32.5 28.7 38.4 28.6 25.1 34. 12.8 8.6 14.6 16.2 12.7 18.5 Title 91.2 57.7 107.2 101.7 67.4 119.8 S Abstract 25.6 19.8 28.6 27.0 21.7 30. 12.3 8.5 13.4 17.5 14.5 19.1 14.5 6.4 14.5 57.8 37.5 66.3 25.1 19.7 27.4 31.8 27.3 35. 11.8 8.2 13.0 16.4 13.8 18.2 Intro 11.9 5.2 10.2 36.0 22.2 39.8 24.6 19.4 27.0 29.6 25.2 33.0 5.4.3 Performance with GRPO To validate the effectiveness of our GRPO-based reinforcement learning stage, we compare the performance of our models before (SFT-base) and after (+RL) this optimization. The results, detailed in Table 6, confirm that GRPO serves as powerful fine-tuning method for enhancing temporal precision in video chaptering. From the experimental results, we draw three key conclusions. First, GRPO directly and consistently improves metrics correlated with temporal segmentation accuracy. As hypothesized, by optimizing with reward focused on temporal alignment, we observe clear performance boost in F1 and tIoU scores across all configurations. For instance, on the VidAtlas-test set, the GRPO model with video input achieves notable gain of +0.8 in F1 and +0.7 in tIoU over its SFT baseline. This empirically validates that GRPO effectively sharpens the models ability to predict precise chapter boundaries. Second, we observe significant degree of cross-modal transferability from the RL training. Notably, despite the GRPO training being conducted exclusively on the video modality, the temporal localization performance of the ASR and Video+ASR inputs also improves. The GRPO model with Video+ASR input, for example, achieves +1.5 F1 and +1.1 tIoU gain on VidChapter7M-test. This suggests that the optimization is not merely learning superficial visual-to-temporal mapping but is refining more abstract, modality-agnostic representation of temporal structure within the language models parameters. Finally, these enhancements in temporal precision are achieved without sacrificing semantic quality. Crucially, although our reward function is agnostic to content, semantic metric such as CIDEr remain highly comparable to the SFT baseline, and in some cases even improve (e.g., +1.1 CIDEr for video input on VidChapters7MTable 6 Effectiveness of Reinforcement Learning with GRPO. We compare the performance of our models before (SFT) and after applying reinforcement learning (+RL) with GRPO. The evaluation is conducted on two benchmarks across different input modalities (A: ASR, V: Video). The results show that GRPO consistently improves temporal segmentation metrics (F1, tIoU) while maintaining or slightly improving semantic quality metrics (S: SODA, C: CIDEr). Bold numbers indicate the best performance between the base model and GRPO-enhanced model for each metric. Method Stage Modality Base-asr GRPO-asr Base-vid GRPO-vid Base-vidasr GRPO-vidasr sft +rl sft +rl sft +rl F1 54.5 54.8(+0.3) 50.2 50.6(+0.4) 59.3 60.8(+1.5) VidChapters7M-test tIoU F1 tIoU VidAtlas-test G 76.7 77.2(+0.5) 74.3 74.8(+0.5) 79.6 80.7(+1.1) 26.3 25.3(-1.0) 22.9 22.9() 30.6 31.0(+0.4) 144.0 143.7(-0.3) 138.3 139.4(+1.1) 186.6 190.7(+4.1) 28.9 28.8 (-0.1) 25.4 25.4() 34.3 34.6(+0.3) 58.8 59.6(+0.8) 57.6 58.4(+0.8) 66.2 66.8(+0.6) 79.7 80.2(+0.5) 78.9 79.6(+0.7) 84.0 84.3(+0.3) 24.8 24.7(-0.1) 21.6 21.9(+0.3) 30.2 30.4(+0.2) 111.3 109.9(-1.4) 98.1 98.2(+0.1) 141.5 141.7(+0.2) 28.0 28.0() 25.0 25.0() 34.1 34.4(+0.3) test.). Composite metrics like SODA and GRACE, which balance segmentation and description, also maintain their performance or exhibit slight gains. This indicates that the KL-regularized optimization successfully avoids policy degradation, suggesting positive effect where more accurate segmentation enables the model to generate more focused and relevant content. In summary, GRPO acts as critical fine-tuning step, effectively sharpening the models temporal acuity while preserving its descriptive capabilities."
        },
        {
            "title": "5.5 Qualitative Visualization",
            "content": "To provide more intuitive understanding of our models capabilities beyond quantitative metrics, we present qualitative examples on both English and Chinese videos. These visualizations showcase ARC-Chapters ability to generate accurate, coherent, and hierarchically structured outputs in multiple formats and languages. Fig. 7 illustrates the models performance on challenging English video discussing US debt and the role of stablecoins. The topic is dense with financial terminology and complex arguments. Our model successfully navigates this complexity across all output formats. The Short Title accurately segments the video into logical thematic units, such as \"Intro\", \"Stablecoin Regulation\". The Video Description with Timestamp summarizes the video content for each chapter. More impressively, the Structural Chapters demonstrates the models advanced capability for hierarchical chaptering. The generated title, abstract, and introduction for each chapter are distinct yet complementary, providing rich, layered understanding of the content that mirrors human-authored summaries. To showcase the multilingual performance of our model, Fig. 8 presents the results for Chinese video on similar topic. The model exhibits comparable level of understanding and generation quality in Chinese. The generated Short Titles are precise. The detailed Description and Structural Chapters are fluent and contextually appropriate. This strong cross-lingual performance underscores the models ability to generalize the learned chaptering and summarization skills, rather than merely memorizing patterns in single language. Together, these qualitative examples confirm that ARC-Chapter is not only powerful chaptering tool but also versatile video understanding model capable of producing rich, structured, and multilingual summaries that are both accurate and useful for end-users."
        },
        {
            "title": "6 Conclusion",
            "content": "In this report, we introduced ARC-Chapter, scalable and robust framework for structuring long-form videos into semantically coherent chapters and hierarchical summaries. ARC-Chapter leverages large-scale dataset of millions of long video chapters and employs semi-automatic annotation pipeline. These innovations advance the state of the art in video chaptering and summary generation. We also proposed the GRACE metric, which addresses the limitations of existing evaluation methods by providing granularity-robust assessment of chapter boundaries. Experimental results show that ARC-Chapter achieves superior performance across multiple benchmarks, video durations, and languages. These findings demonstrate the frameworks effectiveness and generalizability. ARC-Chapter has strong potential to facilitate efficient content navigation, retrieval, and understanding as long-form video content continues to grow rapidly. Figure 7 Qualitative results on an English video about finance and cryptocurrency. Figure 8 Qualitative results on Chinese video discussing stablecoins."
        },
        {
            "title": "References",
            "content": "[1] Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach, Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, et al. Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras. arXiv preprint arXiv:2503.01743, 2025. [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [3] Anthropic. The claude 3 model family: Opus, sonnet, haiku. 2024. [4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-VL: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. [5] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [6] Richard Bellman and Robert Kalaba. On adaptive control processes. IRE Transactions on Automatic Control, 4 (2):19, 2003. [7] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. [8] Guodong Ding, Fadime Sener, and Angela Yao. Temporal action segmentation: An analysis of modern techniques. TPAMI, 46(2):10111030, 2023. [9] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [10] Soichiro Fujita, Tsutomu Hirao, Hidetaka Kamigaito, Manabu Okumura, and Masaaki Nagata. Soda: Story oriented dense video captioning evaluation framework. In ECCV, pages 517531, 2020. [11] Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. Tall: Temporal activity localization via language query. In ICCV, 2017. [12] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [13] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. [14] Yongxin Guo, Jingyu Liu, Mingda Li, Qingbin Liu, Xi Chen, and Xiaoying Tang. Trace: Temporal grounding video llm via causal event modeling. arXiv preprint arXiv:2410.05643, 2024. [15] Yongxin Guo, Jingyu Liu, Mingda Li, Dingxin Cheng, Xiaoying Tang, Dianbo Sui, Qingbin Liu, Xi Chen, and Kevin Zhao. Vtg-llm: Integrating timestamp knowledge into video llms for enhanced video temporal grounding. In AAAI, pages 33023310, 2025. [16] Fabian Caba Heilbron, Juan Carlos Niebles, and Bernard Ghanem. Fast temporal activity proposals for efficient detection of human actions in untrimmed videos. In CVPR, pages 19141923, 2016. [17] Bin Huang, Xin Wang, Hong Chen, Zihan Song, and Wenwu Zhu. Vtimellm: Empower llm to grasp video moments. In CVPR, pages 1427114280, 2024. [18] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [19] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In ICCV, pages 706715, 2017. [20] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In ICCV, pages 706715, 2017. [21] Hilde Kuehne, Ali Arslan, and Thomas Serre. The language of actions: Recovering the syntax and semantics of goal-directed human activities. In CVPR, 2014. [22] Colin Lea, Michael Flynn, Rene Vidal, Austin Reiter, and Gregory Hager. Temporal convolutional networks for action segmentation and detection. In CVPR, pages 156165, 2017. [23] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726, 2023. [24] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. In ECCV, pages 323340, 2024. [25] Ye Liu, Kevin Qinghong Lin, Chang Wen Chen, and Mike Zheng Shou. Videomind: chain-of-lora agent for long video reasoning. arXiv preprint arXiv:2503.13444, 2025. [26] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Yaofeng Sun, et al. DeepSeek-VL: Towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525, 2024. [27] Zijia Lu and Ehsan Elhamifar. Fact: Frame-action cross-attention temporal modeling for efficient action segmentation. In CVPR, pages 1817518185, 2024. [28] Long Qian, Juncheng Li, Yu Wu, Yaobo Ye, Hao Fei, Tat-Seng Chua, Yueting Zhuang, and Siliang Tang. Momentor: Advancing video large language model with fine-grained temporal reasoning. In ICML, 2024. [29] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In ICML, pages 2849228518, 2023. [30] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: time-sensitive multimodal large language model for long video understanding. In CVPR, pages 1431314323, 2024. [31] Hiroaki Sakoe and Seibi Chiba. Dynamic programming algorithm optimization for spoken word recognition. IEEE Transactions on Acoustics, Speech, and Signal Processing, 26(1):4349, 2003. [32] Yuhan Shen and Ehsan Elhamifar. Progress-aware online action segmentation for egocentric procedural task videos. In CVPR, pages 1818618197, 2024. [33] Fangxun Shu, Lei Zhang, Hao Jiang, and Cihang Xie. Audio-visual llm for video understanding. arXiv preprint arXiv:2312.06720, 2023. [34] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [35] Lucas Ventura, Antoine Yang, Cordelia Schmid, and Gül Varol. Chapter-llama: Efficient chaptering in hour-long videos with llms. In CVPR, pages 1894718958, 2025. [36] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. Git: generative image-to-text transformer for vision and language. arXiv preprint arXiv:2205.14100, 2022. [37] Peiyu Wang, Yichen Wei, Yi Peng, Xiaokun Wang, Weijie Qiu, Wei Shen, Tianyidan Xie, Jiangbo Pei, Jianhao Zhang, Yunzhuo Hao, et al. Skywork r1v2: Multimodal hybrid reinforcement learning for reasoning. arXiv preprint arXiv:2504.16656, 2025. [38] Teng Wang, Ruimao Zhang, Zhichao Lu, Feng Zheng, Ran Cheng, and Ping Luo. End-to-end dense video captioning with parallel decoding. In ICCV, pages 68476857, 2021. [39] Zhenzhi Wang, Ziteng Gao, Limin Wang, Zhifeng Li, and Gangshan Wu. Boundary-aware cascade networks for temporal action segmentation. In ECCV, pages 3451, 2020. [40] Hao Wu, Huabin Liu, Yu Qiao, and Xiao Sun. Dibs: Enhancing dense video captioning with unlabeled videos via pseudo boundary enrichment and online refinement. In CVPR, pages 1869918708, 2024. [41] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. NExT-GPT: Any-to-any multimodal llm. arXiv preprint arXiv:2309.05519, 2023. [42] Cilin Yan, Haochen Wang, Shilin Yan, Xiaolong Jiang, Yao Hu, Guoliang Kang, Weidi Xie, and Efstratios Gavves. Visa: Reasoning video object segmentation via large language models. In ECCV, pages 98115, 2024. [43] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [44] Antoine Yang, Arsha Nagrani, Ivan Laptev, Josef Sivic, and Cordelia Schmid. Vidchapters-7m: Video chapters at scale. NeurIPS, 36:4942849444, 2023. [45] Antoine Yang, Arsha Nagrani, Ivan Laptev, Josef Sivic, and Cordelia Schmid. Vidchapters-7m: Video chapters at scale. In NeurIPS, 2023. [46] Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, Antoine Miech, Jordi Pont-Tuset, Ivan Laptev, Josef Sivic, and Cordelia Schmid. Vid2seq: Large-scale pretraining of visual language model for dense video captioning. In CVPR, 2023. [47] Zuhao Yang, Yingchen Yu, Yunqing Zhao, Shijian Lu, and Song Bai. Timeexpert: An expert-guided video llm for video temporal grounding. arXiv preprint arXiv:2508.01699, 2025. [48] Xiangyu Zeng, Kunchang Li, Chenting Wang, Xinhao Li, Tianxiang Jiang, Ziang Yan, Songze Li, Yansong Shi, Zhengrong Yue, Yi Wang, et al. Timesuite: Improving mllms for long video understanding via grounded tuning. arXiv preprint arXiv:2410.19702, 2024. [49] Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, et al. Videollama 3: Frontier multimodal foundation models for image and video understanding. arXiv preprint arXiv:2501.13106, 2025. [50] Haoji Zhang, Xin Gu, Jiawen Li, Chixiang Ma, Sule Bai, Chubin Zhang, Bowen Zhang, Zhichao Zhou, Dongliang He, and Yansong Tang. Thinking with videos: Multimodal tool-augmented reinforcement learning for long video reasoning. arXiv preprint arXiv:2508.04416, 2025. [51] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675, 2019. [52] Xiaoying Zhang, Da Peng, Yipeng Zhang, Zonghao Guo, Chengyue Wu, Chi Chen, Wei Ke, Helen Meng, and Maosong Sun. Towards self-improving systematic cognition for next-generation foundation mllms. arXiv preprint arXiv:2503.12303, 2025. [53] Yipeng Zhang, Yifan Liu, Zonghao Guo, Yidan Zhang, Xuesong Yang, Chi Chen, Jun Song, Bo Zheng, Yuan Yao, Zhiyuan Liu, et al. Llava-uhd v2: an mllm integrating high-resolution feature pyramid via hierarchical window transformer. arXiv preprint arXiv:2412.13871, 2024. [54] Yue Zhao, Yuanjun Xiong, Limin Wang, Zhirong Wu, Xiaoou Tang, and Dahua Lin. Temporal action detection with structured segment networks. In ICCV, pages 29142923, 2017. [55] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In AAAI, 2018. [56] Xingyi Zhou, Anurag Arnab, Shyamal Buch, Shen Yan, Austin Myers, Xuehan Xiong, Arsha Nagrani, and Cordelia Schmid. Streaming dense video captioning. In CVPR, pages 1824318252, 2024. [57] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025."
        }
    ],
    "affiliations": [
        "ARC Lab, Tencent PCG"
    ]
}