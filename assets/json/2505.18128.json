{
    "paper_title": "Frankentext: Stitching random text fragments into long-form narratives",
    "authors": [
        "Chau Minh Pham",
        "Jenna Russell",
        "Dzung Pham",
        "Mohit Iyyer"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Frankentexts, a new type of long-form narratives produced by LLMs under the extreme constraint that most tokens (e.g., 90%) must be copied verbatim from human writings. This task presents a challenging test of controllable generation, requiring models to satisfy a writing prompt, integrate disparate text fragments, and still produce a coherent narrative. To generate Frankentexts, we instruct the model to produce a draft by selecting and combining human-written passages, then iteratively revise the draft while maintaining a user-specified copy ratio. We evaluate the resulting Frankentexts along three axes: writing quality, instruction adherence, and detectability. Gemini-2.5-Pro performs surprisingly well on this task: 81% of its Frankentexts are coherent and 100% relevant to the prompt. Notably, up to 59% of these outputs are misclassified as human-written by detectors like Pangram, revealing limitations in AI text detectors. Human annotators can sometimes identify Frankentexts through their abrupt tone shifts and inconsistent grammar between segments, especially in longer generations. Beyond presenting a challenging generation task, Frankentexts invite discussion on building effective detectors for this new grey zone of authorship, provide training data for mixed authorship detection, and serve as a sandbox for studying human-AI co-writing processes."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 2 8 2 1 8 1 . 5 0 5 2 : r Stitching random text fragments into long-form narratives"
        },
        {
            "title": "Jenna Russell x Dzung Pham e Mohit Iyyer xe",
            "content": "x University of Maryland, College Park University of Massachusetts Amherst {chau,jennarus,miyyer}@umd.edu, dzungpham@cs.umass.edu"
        },
        {
            "title": "Abstract",
            "content": "We introduce Frankentexts, new type of long-form narratives produced by LLMs under the extreme constraint that most tokens (e.g., 90%) must be copied verbatim from human writings. This task presents challenging test of controllable generation, requiring models to satisfy writing prompt, integrate disparate text fragments, and still produce coherent narrative. To generate Frankentexts, we instruct the model to produce draft by selecting and combining human-written passages, then iteratively revise the draft while maintaining userspecified copy ratio. We evaluate the resulting Frankentexts along three axes: writing quality, instruction adherence, and detectability. Gemini-2.5-Pro performs surprisingly well on this task: 81% of its Frankentexts are coherent and 100% relevant to the prompt. Notably, up to 59% of these outputs are misclassified as human-written by detectors like Pangram, revealing limitations in AI text detectors. Human annotators can sometimes identify Frankentexts through their abrupt tone shifts and inconsistent grammar between segments, especially in longer generations. Beyond presenting challenging generation task, Frankentexts invite discussion on building effective detectors for this new grey zone of authorship, provide training data for mixed authorship detection, and serve as sandbox for studying human-AI co-writing processes. https://github.com/chtmp223/Frankentext"
        },
        {
            "title": "Introduction",
            "content": "In Mary Shelleys classic novel Frankenstein, the scientist Victor Frankenstein assembles creature from fragments of human corpses and brings it to life (Shelley, 1818). Though stitched together from disparate parts, the creature emerges as disturbingly articulate and clever being. We draw inspiration from this story to explore what we call Frankentexts: narratives constructed by LLMs 1 under the constraint that the majority of the output (e.g., 90% of the words) must be copied verbatim from collection of human writings, with only minimal connective text added by the model. Frankentexts present novel and extreme test of controllable text generation: the LLMs output must adhere to given story prompt and maintain global coherence while simultaneously following the verbatim copying constraint. Integrating these disjointed fragments, which are never intended to go together, requires advanced instructionfollowing and long-context abilities as well as creativity. This scenario differs significantly from traditional controllable generation tasks: while previous methods softly enforce high-level outlines (Fan et al., 2018; Yao et al., 2019) or lexical constraints (Sun et al., 2021), Frankentexts requires reusing actual human-written passages at scale, which is fundamentally new challenge. To our knowledge, our work is the first to systematically study this generation paradigm. Broader implications: Frankentexts lie within grey zone of authorship: they are arranged by an LLM yet composed mostly of human-written text, breaking the binary AI vs. human assumption baked into todays detectors of AI-generated In fact, we show that Frankentexts content. are routinely misclassified as fully human-written, posing novel attack vector (e.g., for academic integrity) in which users construct high-quality Frankentexts to evade detection. This detection failure highlights the need for fine-grained detectors capable of token-level attribution, and our pipeline synthetically supplies the supervision they lack: every frankentext comes with labels marking copied versus LLM-generated segments, thus providing an inexpensive, large-scale training source for mixed-authorship detection. At the same time, the controllable construction process offers tunable sandbox for studying human-AI co-writing: Figure 1: The Frankentexts pipeline. First, random paragraphs are sampled from large corpus of human-written books. Then, an LLM is prompted with the paragraphs, writing prompt, and instructions to include certain percentage of verbatim human text, to generate the first draft of Frankentext. Further edits can be made by the same model to produce coherent and faithful final version. See Algorithm 1 for the pipelines pseudocode. by manipulating the proportion, length, and topical diversity of human excerpts, researchers can run systematic experiments on stylistic blending and revision dynamics that collaborative writing studies (Akoury et al., 2020; Lee et al., 2022; Yuan et al., 2022) seldom make possible at scale. Using LLMs to generate Frankentexts: We develop prompt-based pipeline to generate Frankentexts: an LLM first drafts story by selecting provided human-written fragments and integrating them into its own generated text. Then, the LLM repeatedly refines the draft to fix contradictions or disfluencies. In both stages, the LLM is instructed to follow the constraint that specified portion of the text should be copied verbatim from the human-written fragments (Figure 1). Putting Frankentexts under the microscope: Using 100 creative writing prompts from the Mythos dataset (Kumar et al., 2025), we evaluate Frankentexts on three dimensions: writing quality, adherence to instructions, and detectability. Both automatic and human evaluations show that powerful closed LLMs like Gemini-2.5-Pro (DeepMind, 2024) can meet the extreme copy constraint while producing largely coherent and relevant stories. Moreover, Frankentexts also frequently evade detection by state-of-the-art automatic methods such as Pangram (Emi and Spero, 2024), Binoculars (Hans et al., 2024), and FastDetectGPT (Bao et al., 2024). Human annotators note distinct voice that makes some Frankentexts feel human despite being AI-generated. However, our evaluation also reveals limitations: most human evaluators identify many Frankentexts as AI-generated due to subtle signs (e.g., abrupt tone shifts or inconsistent grammar between stitched segments), especially in longer narratives. We frame the construction of Frankentexts as novel instruction-following task that tests the limits of controllable generation. Our paper focuses on their intrinsic properties (coherence, instruction adherence, detectability) as prerequisite to pursuing meaningful downstream applications. By releasing our code and evaluation suite, we hope to facilitate future research on fine-grained detection of mixedorigin text and provide playground for human-AI co-writing studies."
        },
        {
            "title": "2 The Frankentext pipeline",
            "content": "We introduce simple and effective pipeline for generating coherent and relevant Frankentexts, in which various segments are directly copied from human-written sources. The pipelines simplicity serves as its strength enabling users to construct Frankentexts easily and its threat, making it easy to create outputs that evade detection. 2.1 Generating Frankentexts We include random sample of 1,500 humanwritten snippets in our generation prompts, as illustrated in Figure 1.1 snippet refers to paragraph11,500 snippets spanning 103K BPE tokens, an amount well within the 128K context window supported by most re2 Writing Prompt: After being killed in Black Friday stampede, youre sent to hell. The devil offers to let you be in charge of torturing your fellow mankind, expecting you to refuse like all the others. Except the devil doesnt realize youve worked retail for 15 years. Gemini 2.5 Pro Vanilla Generation Gemini 2.5 Pro Frankentext The last thing saw was blur of crazed shoppers, their eyes wide with feral glint, all surging towards mountain of discounted flat-screen TVs. Then, sharp pain, sickening crunch, and darkness. Typical Black Friday, really. Except for the dying part. When regained some semblance of consciousness, the air was thick with the smell of sulfur and burnt sugar, and the soundtrack was cacophony of screams and what sounded suspiciously like elevator music. Definitely not the pearly gates. dapper figure in surprisingly well-tailored suit, complete with horns that had been polished to high sheen, regarded me with an air of bored amusement. \"Well, well, well,\" he drawled, his voice smooth as aged whiskey. \"Another one trampled in the name of consumerism. Humans, so predictable.\" He gestured vaguely around at the fiery landscape. \"Welcome to the Underworld. Im Lucifer. You can call me Lucy.\" blinked, trying to process. \"So, this is it then? Eternal damnation?\" Lucifer chuckled. \"More or less. We have rather. . . traditional approach to retribution down here. However,\" he leaned forward, conspiratorial gleam in his crimson eyes, \"I have proposition for you. sort of management position, if you will.\" ... Brenda, isnt it? he asked in his peculiarly accented English. She nodded mutely. The man laughed. Youre wondering about this place. He leaned back. She took the offered seat He smiled again, that cold smile. have proposition for you. We have never before had such capabilities, he continued, gesturing expansively. Capabilities that can be used in the interests of terror, destruction... He fixed his gaze on her. offer you management position. Tormenting mankind. Like all the others before you, you will, of course, refuse. But the offer stands. Brenda simply stared at the Devil for moment. Refuse? Why would she refuse? He had just offered her management position. After fifteen years scrambling for assistant manager roles that paid peanuts, this was practically dream job albeit fiery, nightmarish one. The Devil had not expected her silence. Blinking, he sat back, stared, then noticed his jaw was open and closed it. Brenda finally found her voice, though it was dry and raspy. Okay, so, what are the key performance indicators for this role? The Devil leaned forward again, flicker of something interest? confusion? in his eyes. Still, his tone remained smooth. Your department will handle the newcomers, specifically the ones arriving from retail environments. Think of the task like pest control. ... Table 1: Side-by-side comparison of vanilla and Frankentexts by Gemini-2.5-Pro. AI-generated keywords from the Pangram API are highlighted in blue. Vanilla generations contain many more of these keywords compared to Frankentexts (G). The full examples are shown in Table 6. sized unit of text used for sampling.2 Since our focus is on creative writing, we sample these snippets from Books3 (Presser, 2020), dataset consisting of 197K books (> 160 million snippets) originally hosted on Bibliotik.3 Our goal is to produce coherent narratives that are composed of specified portion of human writing and aligned with the given writing prompt. Therefore, our pipeline focuses on generating texts that are relevant to the prompt in the initial draft, and then refining the draft in subsequent editing phase to improve coherence. Obtaining the first draft: At this stage, we prompt an LLM to produce first draft in which specified portion of the content is taken verbatim from the human-written snippets, with the remaining text consisting of connective words and transitional phrases (Figure 19). We optionally add another editing step that aims to increase the verbatim copy rate, which can be implemented either by attribution metrics like ROUGE-L or AI-text detectors themselves (Figure 20).4 cent LLMs. Most snippets come from distinct books (e.g., 1,500 paragraphs may be drawn from 1,497 books). 2A snippet can also be sentence. See our experiments on different snippet types in I. 3We note that the paragraphs could be sourced from anywhere (e.g., Project Gutenberg, news, etc.), but we choose Books3 since modern fiction is the focus of our experiments. 4We implement this step with Pangram prediction API. If the generated draft is flagged as highly likely AI or likely AI, we interpret this as failure to adhere to the verbatim copy rule and regenerate accordingly. We note this step is Polishing the draft: The first draft may contain writing issues, including contradictions (e.g., character traits, descriptions, facts, or points of view), continuity errors (e.g., actions conflicting with earlier ones), irrelevant content (e.g., unfiltered citations or filler text), and mechanical problems (e.g., grammar, phrasing, or pronoun mismatches). To address these, we use the same LLM to identify and apply minimal edits that improve coherence while still respecting the verbatim copy rule and the writing prompt, similar to self-correct step (Shinn et al., 2023; Madaan et al., 2023). We repeat this step up to three times and stop as soon as the model returns no edits, which indicates that the draft is already coherent (Figure 21)."
        },
        {
            "title": "3 Experimental setup",
            "content": "Our pipeline is optimized for story generation, task that requires both strong instruction-following and advanced text generation capabilities (Xie et al., 2023; Huot et al., 2025). Thus, we select dataset focused on creative writing, set of models with strong reasoning and instruction-following abilities, and an intrinsic evaluation set-up designed to highlight the feasibility and value of Frankentexts. rarely triggered by larger models (e.g. only 6 times out of 100 for Gemini). 5We further provide an ablation where the editing round is omitted in Appendix J. 3 3.1 Dataset We source our writing prompts from Mythos (Kumar et al., 2025), dataset of 3,200 prompts recently posted on Reddits r/WritingPrompts to mitigate potential data contamination issues. We select representative subset of 100 prompts for our experiments.6 3.2 Models We include models from five families known for their performance in instruction-following and creative text generation (Chiang et al., 2024): Gemini-2.5-Pro (exp-03-25 checkpoint), Claude3.7-Sonnet (thinking enabled) (Anthropic, 2025), o4-mini (2025-04-16 checkpoint, with reasoning effort set to high) (OpenAI, 2025), DeepSeek R1 (DeepSeek-AI et al., 2025), and Qwen3-32B (both thinking and non-thinking) (QwenTeam, 2025).7 We instruct these models to produce Frankentexts that contain approximately 500 words and have 90% of texts being copied verbatim from the provided human-written samples. We use 1,500 human-written paragraphs (around 103K BPE tokens) in our standard configuration. Vanilla baselines: Alongside Frankentexts, we also generate vanilla outputs from the same set of models. In this baseline setup, each model is given the same writing prompts and instructed to produce outputs of equivalent length, without any additional constraints or filtering. 3.3 Automatic evaluation We develop suite of intrinsic evaluation metrics to assess our generations based on three key dimensions: writing quality (coherence), adherence to instructions (relevance, word count, copy rate), and detectability (AI text detector results). Coherence: We report the percentage of coherent Frankentexts. Specifically, we use GPT-4.1 to provide binary judgment (True/False) on whether the text contains any coherence issues that, if left unaddressed, would significantly affect readers ability to understand (Chang et al., 2024b; Chiang and Lee, 2023). Examples of such coherence issues 6See for experiment costs. 7We use the default or recommended hyperparameters for each model. We prioritize reasoning models in our experiments because non-reasoning models like GPT-4o and Claude3.5-Sonnet fail to effectively follow the imposed constraints in our preliminary experiments. include inconsistencies in plot or events, characters, spatial relationships, themes, and surface-level details (Figure 17). report Relevance: We the percentage of Frankentexts that are faithful to the writing prompt. Similar to our coherence evaluation, we use GPT-4.1 to assess whether each Frankentexts fully adheres to the prompt without introducing any conflicting details, following Atmakuru et al. (2024) (Figure 18).8 Copy rate: We report the average copy rate, which captures the proportion of the Frankentexts being copied from the given humanwritten content (Akoury et al., 2020). This metric also allows us to track which segments of the text are AI or human-written. We first map each tokenlevel trigram from the human-written snippets included in the generation process to its source texts. Then, using the trigrams from each Frankentexts, we retrieve all human snippets sharing at least 4 trigrams to reduce false positives.9 We then rank candidate snippets by shared trigram count and filter out those whose trigrams are already covered by higher-ranked snippets. Finally, we reorder the matched human-written content to be consistent with the content in the Frankentexts and calculate the ROUGE-L score between Frankentexts and the combined candidate snippets (i.e., ratio of the longest common subsequences length over Frankentexts length). Detectability: We report the percentage of Frankentexts being determined as AI-generated texts by three state-of-the-art AI text detectors (Russell et al., 2025):10 Pangram (Emi and Spero, 2024) is closedsource detector using Transformer classifier trained with hard negative mining and synthetic data. We report results from its sliding window API, which evaluates the full text.11 8Fleiss κ values for agreement between LLM and human are 0.049 for coherence and 0.187 for faithfulness, indicating no agreement beyond chance. However, it is important to note that even inter-annotator agreement is low (0.008), which highlights the inherent ambiguity of the task and the need for adjudicated or model-based evaluation approaches. 9All texts are preprocessed by removing non-alphanumeric characters, lemmatizing, stemming, and replacing pronouns with placeholder. 10We do not evaluate GPTZero due to resource constraints. 11Labels \"Highly likely AI,\" \"Likely AI,\" and \"AI\" are grouped as \"AI involvement\"; \"Human\" and \"Unlikely AI\" as \"Human.\" Pangram also includes \"mixed\" label. 4 INSTRUCTION ADHERENCE QUALITY DETECTABILITY (cid:29) Word count Copy % () Relevance % () (cid:6) Coherence % () (cid:219) Pangram % AI () (cid:219) Pangram % mixed () (cid:223) Pangram AI fraction () Binoculars % () FastDetect GPT % () Vanilla Baselines (cid:25) Gemini-2.5-Pro-exp (cid:25) o4-mini (cid:25) Claude-3.7-Sonnet (cid:157) Deepseek-R1 (cid:157) Qwen-3-32B thinking (cid:157) Qwen-3-32B Frankentext (cid:25) Gemini-2.5-Pro-exp (cid:25) o4-mini (cid:25) Claude-3.7-Sonnet (cid:157) Deepseek-R1 (cid:157) Qwen-3-32B thinking (cid:157) Qwen-3-32B 593 505 485 550 699 656 521 464 890 303 578 843 75 68 64 42 36 21 100 100 100 100 100 100 100 56 95 91 91 100 100 100 100 100 100 81 29 73 72 54 72 100 100 100 100 100 100 4 24 17 74 85 90 0 0 0 0 0 0 37 14 47 3 8 100 100 100 100 100 100 16 27 27 72 89 91 52 16 20 9 92 100 0 0 2 0 52 89 99 74 53 42 100 100 1 1 16 0 92 Table 2: Results for vanilla generations and Frankentexts across three categories: INSTRUCTION ADHERENCE, WRITING QUALITY, and DETECTABILITY. Except for word count, all metrics represent the percentage of samples (e.g., Pangram % AI reflects the percentage of texts identified as highly likely or likely AI-generated). Gemini-2.5Pro produces Frankentexts that are difficult to detect while remaining competitive on instruction adherence and coherence. Dark green cells indicate the best value for each metric, and light green cells indicate the second best. Binoculars (Hans et al., 2024) is an opensource detector that uses cross-perplexity from two language models. We use its accuracy-based version.12 FastDetectGPT (Bao et al., 2024) is an opensource method that scores text perturbations to estimate conditional probability curvature. We use false positive rate of 0.05.13 3.4 Human evaluation We conduct two human evaluation studies with 3 annotators on subset of 30 Gemini Frankentexts to further understand human perception of writing quality and detectability for total cost of $700 USD.14 Single-story evaluation: We assess the coherence, relevance, and human detectability of Frankentexts, as well as identify potential limitations. Annotators are presented with writing prompt and corresponding Frankentexts sample. Following the annotation protocol from Yang et al. (2022), annotators provide binary ratings on relevance, coherence, and authorship (AI-generated vs. human-written). Additionally, they select from 12Texts with probability greater than 0.9015310749276843 are classified as AI-generated. 13The threshold is 0.7890873125379173, based on 100 pre2018 r/writingprompts human-written stories. 14Annotators were paid $70 USD for the single evaluation or $150 for the pairwise evaluation. See the annotation interface in and an example highlighted story in Figure 9. list of predefined writing issues and offer optional justifications in long-form response. Pairwise evaluation: We also look into how Frankentexts compares to standard, unconstrained LLM generation. Using the same rubric, annotators review both Gemini-generated frankentext and vanilla Gemini generation for the same prompt, then assess them side by side."
        },
        {
            "title": "4 Results",
            "content": "Despite the complex setup, Frankentexts are surprisingly coherent and faithful to the writing prompt, which raises serious concerns about the copyright and adversarial appropriation of human work. Given their high proportion of human-written material, Frankentexts are mostly misclassified as human-written by binary detectors like Binoculars and FastDetectGPT. Pangram, which could detect mixed authorship, performs better but still misses 59% of the cases, highlighting the need to adapt detection tools to this new generation paradigm. Human evaluations reveal areas where future LLMs could improve, particularly in handling jarring transitions and grammatical issues. We also demonstrate the versatility of Frankentexts across diverse settings, including more specific prompts, varying levels of verbatim copying, and applicability to nonfiction texts. 5 COMMENTS (cid:215) For example, the line \"Hello, eyes, how you doing? Enjoy looking while you can...\" is poetic and disturbing. Dr Petersons speech /explanation is strange mixture of styles, ranging from pure information that looks like its been lifted straight from Wikipedia to more natural sounding. (cid:215) Its creative with mix of ideas, Its coherent enough to follow, but the dialogue is uneven. concept of the story is quite interesting. like vampire being farmer to barter food for blood bags. Some parts just feel little disjointed , however, the (cid:215) (cid:215) Another puzzling story that has no consistent plot. Random bits and pieces from elsewhere perhaps? The current state of AI is such that even it will be embarrassed to generate such nonsensical drivel. So the author must human! Theres strange imbalance to the dialogue. It starts off super-casual with \"Dude...\" then goes into complex scientific talk about skin types. This lacks the normalcy of human interaction. The punctuation is all over the place, and its unclear sometimes where someones speech starts. The tone shifts enormously , going from casual banter to medical lecture to lyrical/poetic to sci-fi. All this indicates AI is trying to blend many ideas into one story. Table 3: Annotator comments zeroing in on the challenges of the Frankentexts task. Blue highlights indicate problems with shifting tone/style, orange highlights indicate problems with coherent plots, purple highlights indicate problems with dialogue, and green highlights indicate problems with grammar. 4.1 Frankentexts are challenging for binary detectors while remaining coherent and relevant to the story premise When evaluated across all three intrinsic evaluation criteria (detectability, instruction-following, and writing quality), Gemini emerges as the topperforming model for the task  (Table 2)  . Overall, Frankentexts show two key trends: Strong coherence and relevance: Most models, except for o4-mini and Qwen3-32B-thinking, produce Frankentexts that exceed 90% relevance and 70% coherence, which is surprising and impressive given the complexity of the task. Gemini, in particular, demonstrates strong instructionfollowing and writing quality. Its Frankentexts closely match the target word count of 500 and achieve the highest copy rate of 75%, meaning that 75% of the content can be traced back to the humanwritten source material. However, this still falls short of the prompts requirement of 90% verbatim copying, suggesting there is room for improvement in instruction adherence. Lower detectability: While most vanilla generations are flagged as AI-generated, Frankentexts from proprietary models (Gemini, o4-mini, and Claude) are often mistaken for human writings, especially by binary detectors such as Binoculars and FastDetectGPT. Binoculars and FastDetectGPT could detect fewer than 20% of these indicating they struggle with mixedoutputs, In contrast, Pangram could authorship content. detect up to 37% of Gemini Frankentexts and 47% of Claude Frankentexts as mixed. However, Pangram flag over 74% of Frankentexts from DeepSeek R1 and Qwen as highly likely or likely AI-generated, suggesting that current mixedauthorship detectors may still need improvement, and that these open-source models may lack the instruction-following capabilities required to produce convincingly mixed text. 4.2 Humans can identify AI involvement in Frankentexts Most annotators are better than automatic detectors at identifying AI involvement in Frankentexts. On the subset used for human evaluation, human annotators identify on average 56% of Frankentexts as likely AI-written, while Pangram detects 33% as mixed or AI-generated, and neither Binoculars nor FastDetectGPT identify any AI-generated content (Figure 2). Annotators also pick up on signs of mixed authorship within individual samples, as shown in comments like the final note in Table 3. Their judgments often hinge on surface-level inconsistencies, such as abrupt tonal shifts or awkward punctuation, that human author would typically revise. Because Frankentexts include verbatim excerpts from human writing, it poses particular challenge for binary detectors, which rely heavily on surface features. We argue that future detectors should consider deeper semantic analysis and other contextual cues to effectively recognize this new class of AI-involved texts. 6 Figure 3: Effects of varying the percentage of required verbatim copy on the Pangram AI detection rate (mixed, highly likely, and likely AI labels), copy rate, or coherence of the Frankentexts. Coherence also declines as human-written content increases, suggesting trade-off between incorporating more human text and maintaining coherence. On the other hand, increasing the proportion of human text leads to higher copy rates, indicating that Gemini could generally follow the copy instruction. Copy rate as proxy for the proportion of human writing in co-authored texts: The copy rate of 75% observed in the 90% verbatim copy setting corresponds to the proportions found in AIhuman co-writing datasets where approximately 66% of the content is human-written and 14% consists of AI-edited segments (Lee et al., 2022; Richburg et al., 2024). While the CoAuthor setup of Lee et al. (2022) only studies setting in which LLMs can add sentences to human text, Frankentexts also consider AI-generated content at varying granularities, including both word-level and sentencelevel, as illustrated in Figure 1. Additionally, CoAuthor costs approximately $3,613 to generate 1,445 texts at $2.50 each,16 whereas we can produce 100 Frankentexts for just $132.38 ($1.32 each) without requiring complex setup. This highlights Frankentextss potential as cost-effective source of synthetic data for collaborative writing tasks, where AI may augment human writings at multiple levels of composition.17 4.5 Specific writing prompts require more AI text, which leads to higher detectability Writing prompts from r/WritingPrompts often provide only general story rather than strict constraints. But what happens if we introduce additional constraints to Frankentexts via these writ16Price excludes around $12 for GPT-3.5 usage. 17We note that, in this use case, users should sample humanwritten snippets from the public domain or obtain them with proper permission. Figure 2: Detection rates among 6 annotators and 3 detectors (Pangram, Binoculars, FastDetectGPT) on the 30 Gemini Frankentexts used for human evaluation. We count mixed, highly likely and likely AI labels in Pangrams detection rate. The red line represents annotators average detection rate. 4.3 Frankentexts primarily struggle with narrative transitions and grammar Our single-story human evaluation finds that 71% of Frankentexts outputs are coherent, 91% are relevant to prompts, and 84% are novel. Table 1 shows sample comparison with vanilla generations.15 Annotators praise Frankentexts for their inventive premises, vivid descriptions, and dry humor, noting distinct voice or emotional hook that made some outputs feel human despite being AIgenerated  (Table 3)  . However, they also identify key issues: abrupt narrative shifts (50%), disfluency (43%), confusing passages (40%), and factual errors (24%). Pairwise evaluations further show that Frankentexts are less coherent than vanilla (4% win rate) but less likely to be determined as AI-generated texts (40% vs. 22%). These challenges likely stem from the difficulty of stitching together paragraphs not authored by the same LLM, which could be alleviated with improved instruction-following and grammar correction. 4.4 Higher copy rates reduce coherence but make detection more challenging We explore the effects of varying the percentage of verbatim human-written text required in the prompt to Gemini, from the default 90% down to 75%, 50%, and 25%. According to Figure 3, as the proportion of human-written content increases, detectability decreases, indicating an inverse relationship between human content and detection rates. 15Full stories are in Table 6 of E. 7 ing prompts? We run Frankentexts with Gemini on 100 prompts from the Tell Me Story dataset (Huot et al., 2025), which include more specific requirements such as mandated story elements and points of view (see Table 5). We find that as prompt complexity increases, the copy rate drops slightly from 74% to 68%, while the average AI fraction determined by Pangram rises by 7%. These trends indicate that, to meet more complex constraints, models need to contribute more original content to the story. Nevertheless, they manage to perform well under different prompt setup. 4.6 Room for improvement in nonfiction We explore Frankentextss generalizability to nonfiction texts using 1,500 human-written paragraphs from the HUMAN DETECTORS corpus of news articles (Russell et al., 2025).18 We generate Frankentexts for 100 news writing prompts, which consist of titles and subtitles collected from May 2025 news articles.19 Despite the domain shift, the resulting Frankentexts maintain 72% coherence and 95% faithful to the prompt, with 66% copy rate. Notably, they remain difficult for automated detectors: 41% are flagged by Pangram as mixed or AI-generated, and none are identified by Binoculars or FastDetectGPT. Upon closer look, Frankentexts exhibit characteristics of quasi-journalistic narrative, such as detailed scene descriptions and frequent anecdotal quotes (Figure 10), which make the Frankentexts read more like story rather than straightforward news article.20 Therefore, further prompt engineering or retrieval might be necessary to get high-quality and realistic nonfiction Frankentexts."
        },
        {
            "title": "5 Related Work",
            "content": "Instruction-Aligned AIHuman Collaborative Writing Constrained text generation techniques have been widely used to enforce narrative coherence. Planning-based approaches improve on an initial outline (Fan et al., 2018; Yao et al., 2019; Fan et al., 2019; Papalampidi et al., 2022; Rashkin et al., 2020; Yang et al., 2023, 2022), while others apply constraints to steer writing process (Sun et al., 2021; Kong et al., 2021; Pham et al., 2024). 18An article collection from reputable sources (e.g. National Geographic, Smithsonian Magazine, and New York Times) 19Articles from The New York Times and The Atlantic. We replace all instances of \"story\" in the prompt with \"news article\" and explicitly request factual accuracy. 20We see Gemini fabricating specific details such as people (Dr. Thorne\") and organizations (GenNova Institute\"). Multiple benchmarks examine the stability of constraint satisfaction in creative writing (Bai et al., 2025; Wu et al., 2025; Atmakuru et al., 2024). Several systems and datasets capture fine-grained humanLLM writing interactions, enabling the study of authorship attribution, stylistic blending, and collaborative revision. Lee et al. (2022), Yuan et al. (2022), Yeh et al. (2025), Chakrabarty et al. (2024), and Ippolito et al. (2022) log revision histories and suggestion traces, while Chakrabarty et al. (2022), Akoury et al. (2020), and Venkatraman et al. (2025) support token or sentence-level authorship analysis, including multi-LLM collaboration. Attribution models struggle in these mixed-authorship settings (Richburg et al., 2024; He et al., 2025). Fine-grained AI-text Detection The task of detection tries to address not just if, but how much of text is AI-generated. This is fundamentally challenging task (Zeng et al., 2024), since existing detectors are sensitive to light AI-polishing (Saha and Feizi, 2025). Many have proposed boundarydetection (Dugan et al., 2023b,a; Kushnareva et al., 2024) tasks or sentence-level automatic detectors (Wang et al., 2023, 2024b) to gain more granular detections. Recent work has explored whether collaborative LLM-Human works can be detected (Zhang et al., 2024; Artemova et al., 2025; Abassy et al., 2024). Richburg et al. (2024) finds that detection models are not yet robust to texts co-written by AI and humans."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduce Frankentexts, challenging constrained text generation paradigm in which an LLM produces narrative using predominantly human-written passages with only minimal connective AI-generated text. By forcing LLMs to compose with mostly human writings, Frankentexts expose corner of controllable generation that had remained unexplored: content grafting under long-context constraints. Despite the nature of their creation, Frankentexts are generally rated as coherent and relevant, and they pose fundamental challenge for binary detectors of AI generations. The token-level labels could supply large-scale training data for mixed-authorship detection, attribution, and co-writing simulations. We release our data and code with the hope that our work would shift the conversation from simply asking Was this written by AI? to Whose words are we reading, and where do they begin and end?."
        },
        {
            "title": "References",
            "content": "Frankentexts presupposes large pool of high-quality, in-domain human texts. Many lantechnical manuals), and guages, genres (e.g., low-resource domains lack such corpora, limiting immediate portability. The copy rate may underestimate the exact percentage of human-written text incorporated into the final generation. Our work deliberately exposes novel attack surface (the ease with which an LLM can weave large amounts of verbatim human prose into fluent narrative) to spur the development of mixed-authorship detectors and other defences. However, we do not propose or evaluate any concrete defence against Frankentexts attacks; our contribution is diagnostic, leaving the design of detection or mitigation strategies to future work."
        },
        {
            "title": "Ethical Considerations",
            "content": "Books3 contains works still under copyright. Our use is strictly non-commercial research and we do not support its use for model pretraining. Frankentexts blur authorship boundaries by design. We do not treat this technique as substitute for genuine authorship or creative writing. As LLMs improve, binary AI-vs-human detectors will fail, and large verbatim excerpts may leave original authors uncredited. Robust provenance tools and transparency will be essential. We recognize that similar techniques could be misused for plagiarism or obfuscation. We strongly discourage such applications. Our research is intended to inform the development of better provenance-tracking and attribution tools, and to support educational or analytical use cases, not to replace genuine human writing or enable deceptive practices. Our human evaluation received approval from an institutional review board. All annotators gave their informed consent and participated voluntarily, without compensation, to support our research."
        },
        {
            "title": "Acknowledgment",
            "content": "We thank members of the UMD CLIP and UMass NLP lab for their helpful feedback. We extend special gratitude to Marzena Karpinska for helping us with Figure 1. This project was partially supported by awards IIS-2046248, IIS-2312949, and IIS-2202506 from the National Science Foundation (NSF). Mervat Abassy, Kareem Elozeiri, Alexander Aziz, Minh Ngoc Ta, Raj Vardhan Tomar, Bimarsha Adhikari, Saad El Dine Ahmed, Yuxia Wang, Osama Mohammed Afzal, Zhuohan Xie, Jonibek Mansurov, Ekaterina Artemova, Vladislav Mikhailov, Rui Xing, Jiahui Geng, Hasan Iqbal, Zain Muhammad Mujahid, Tarek Mahmoud, Akim Tsvigun, and 5 others. 2024. LLM-DetectAIve: tool for fine-grained machine-generated text detection. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 336343, Miami, Florida, USA. Association for Computational Linguistics. Nader Akoury, Shufan Wang, Josh Whiting, Stephen Hood, Nanyun Peng, and Mohit Iyyer. 2020. STORIUM: Dataset and Evaluation Platform for Machine-in-the-Loop Story Generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 64706484, Online. Association for Computational Linguistics. Anthropic. 2025. tem card. claude-3-7-sonnet-system-card. card for Claude 3.7 Sonnet. Claude sys3.7 https://www.anthropic.com/ System sonnet Ekaterina Artemova, Jason Lucas, Saranya Venkatraman, Jooyoung Lee, Sergei Tilga, Adaku Uchendu, and Vladislav Mikhailov. 2025. Beemo: Benchmark of expert-edited machine-generated outputs. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 69927018, Albuquerque, New Mexico. Association for Computational Linguistics. Anirudh Atmakuru, Jatin Nainani, Rohith Siddhartha Reddy Bheemreddy, Anirudh Lakkaraju, Zonghai Yao, Hamed Zamani, and Haw-Shiuan Chang. 2024. Cs4: Measuring the creativity of large language models automatically by controlling the number of story-writing constraints. Preprint, arXiv:2410.04197. Yushi Bai, Jiajie Zhang, Xin Lv, Linzhi Zheng, Siqi Zhu, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2025. Longwriter: Unleashing 10,000+ word generation from long context LLMs. In The Thirteenth International Conference on Learning Representations. Guangsheng Bao, Yanbin Zhao, Zhiyang Teng, Linyi Yang, and Yue Zhang. 2024. Fast-detectGPT: Efficient zero-shot detection of machine-generated text via conditional probability curvature. In The Twelfth International Conference on Learning Representations. Tuhin Chakrabarty, Philippe Laban, and Chien-Sheng Wu. 2025. Ai-slop to ai-polish? aligning language models through edit-based writing rewards and testtime computation. Preprint, arXiv:2504.07532. 9 Tuhin Chakrabarty, Vishakh Padmakumar, Faeze Brahman, and Smaranda Muresan. 2024. Creativity support in the age of large language models: An empirical study involving professional writers. In Proceedings of the 16th Conference on Creativity & Cognition, C&C 24, page 132155, New York, NY, USA. Association for Computing Machinery. Tuhin Chakrabarty, Vishakh Padmakumar, and He He. 2022. Help me write poem: Instruction tuning as vehicle for collaborative poetry writing. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 68486863, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Yapei Chang, Kalpesh Krishna, Amir Houmansadr, John Frederick Wieting, and Mohit Iyyer. 2024a. PostMark: robust blackbox watermark for large language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 89698987, Miami, Florida, USA. Association for Computational Linguistics. Yapei Chang, Kyle Lo, Tanya Goyal, and Mohit Iyyer. 2024b. Booookscore: systematic exploration of book-length summarization in the era of llms. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Cheng-Han Chiang and Hung-yi Lee. 2023. Can large language models be an alternative to human evaluations? In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1560715631, Toronto, Canada. Association for Computational Linguistics. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez, and Ion Stoica. 2024. Chatbot arena: An open platform for evaluating llms by human preference. Preprint, arXiv:2403.04132. Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, and Noah A. Smith. 2021. All thats human is not gold: Evaluating human evaluation of generated text. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 72827296, Online. Association for Computational Linguistics. Isaac David and Arthur Gervais. 2025. Authormist: Evading ai text detectors with reinforcement learning. Preprint, arXiv:2503.08716. Google DeepMind. 2024. Gemini 2.5 pro. https: //deepmind.google/technologies/gemini/. Large language model. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, and 181 others. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Preprint, arXiv:2501.12948. Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel Mazaré, Maria Lomeli, Lucas Hosseini, and Hervé Jégou. 2024. The faiss library. Preprint, arXiv:2401.08281. Liam Dugan, Alyssa Hwang, Filip Trhlík, Andrew Zhu, Josh Magnus Ludan, Hainiu Xu, Daphne Ippolito, and Chris Callison-Burch. 2024. RAID: shared benchmark for robust evaluation of machinegenerated text detectors. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12463 12492, Bangkok, Thailand. Association for Computational Linguistics. Liam Dugan, Daphne Ippolito, Arun Kirubarajan, Sherry Shi, and Chris Callison-Burch. 2023a. Real or fake text? investigating human ability to detect boundaries between human-written and machinethe Thirtygenerated text. Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence, AAAI23/IAAI23/EAAI23. AAAI Press. In Proceedings of Liam Dugan, Daphne Ippolito, Arun Kirubarajan, Sherry Shi, and Chris Callison-Burch. 2023b. Real or fake text?: Investigating human ability to detect boundaries between human-written and machine-generated text. In Proceedings of the 2023 AAAI Conference on Artificial Intelligence. Bradley Emi and Max Spero. 2024. Technical report on the pangram ai-generated text classifier. Preprint, arXiv:2402.14873. Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889898, Melbourne, Australia. Association for Computational Linguistics. Angela Fan, Mike Lewis, and Yann Dauphin. 2019. Strategies for structuring story generation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2650 2660, Florence, Italy. Association for Computational Linguistics. Abhimanyu Hans, Avi Schwarzschild, Valeriia Cherepanova, Hamid Kazemi, Aniruddha Saha, Micah Goldblum, Jonas Geiping, and Tom Goldstein. 2024. Spotting llms with binoculars: Zero-shot Preprint, detection of machine-generated text. arXiv:2401.12070. 10 Jessica He, Stephanie Houde, and Justin D. Weisz. 2025. Which contributions deserve credit? perceptions of attribution in human-ai co-creation. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems, CHI 25, New York, NY, USA. Association for Computing Machinery. Laida Kushnareva, Tatiana Gaintseva, Dmitry Abulkhanov, Kristian Kuznetsov, German Magai, Eduard Tulchinskii, Serguei Barannikov, Sergey Nikolenko, and Irina Piontkovskaya. 2024. Boundary detection in mixed AI-human texts. In First Conference on Language Modeling. Fantine Huot, Reinald Kim Amplayo, Jennimaria Palomaki, Alice Shoshana Jakobovits, Elizabeth Clark, and Mirella Lapata. 2025. Agents room: Narrative generation through multi-step collaboration. In The Thirteenth International Conference on Learning Representations. Mina Lee, Percy Liang, and Qian Yang. 2022. Coauthor: Designing human-ai collaborative writing dataset for exploring language model capabilities. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems, CHI 22, New York, NY, USA. Association for Computing Machinery. Daphne Ippolito, Daniel Duckworth, Chris CallisonBurch, and Douglas Eck. 2020. Automatic detection of generated text is easiest when humans are fooled. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 18081822, Online. Association for Computational Linguistics. Daphne Ippolito, Ann Yuan, Andy Coenen, and Sehmon Burnam. 2022. Creative writing with an ai-powered writing assistant: Perspectives from professional writers. Preprint, arXiv:2211.05030. Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019. IEEE Billion-scale similarity search with GPUs. Transactions on Big Data, 7(3):535547. John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. 2023. watermark for large language models. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 1706117084. PMLR. Ryuto Koike, Masahiro Kaneko, and Naoaki Okazaki. 2024. Outfox: Llm-generated essay detection through in-context learning with adversarially genIn Proceedings of the Thirtyerated examples. Eighth AAAI Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence, AAAI24/IAAI24/EAAI24. AAAI Press. Xiangzhe Kong, Jialiang Huang, Ziquan Tung, Jian Guan, and Minlie Huang. 2021. Stylized story generation with style-guided planning. In Findings of the Association for Computational Linguistics: ACLIJCNLP 2021, pages 24302436, Online. Association for Computational Linguistics. Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Frederick Wieting, and Mohit Iyyer. 2023. Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense. In Thirty-seventh Conference on Neural Information Processing Systems. Nischal Ashok Kumar, Chau Minh Pham, Mohit Iyyer, and Andrew Lan. 2025. Whose story is it? personalizing story generation by inferring author styles. Preprint, arXiv:2502.13028. Ning Lu, Shengcai Liu, Rui He, Yew-Soon Ong, Qi Wang, and Ke Tang. 2024. Large language models can be guided to evade AI-generated text detection. Transactions on Machine Learning Research. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-refine: Iterative refinement with self-feedback. In Advances in Neural Information Processing Systems, volume 36, pages 4653446594. Curran Associates, Inc. Elyas Masrour, Bradley Emi, and Max Spero. 2025. Damage: Detecting adversarially modified ai generated text. Preprint, arXiv:2501.03437. Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D. Manning, and Chelsea Finn. 2023. Detectgpt: zero-shot machine-generated text detection using probability curvature. In Proceedings of the 40th International Conference on Machine Learning, ICML23. JMLR.org. Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. 2023. MTEB: Massive text embedding benchmark. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 20142037, Dubrovnik, Croatia. Association for Computational Linguistics. Charlotte Nicks, Eric Mitchell, Rafael Rafailov, Archit Sharma, Christopher Manning, Chelsea Finn, and Stefano Ermon. 2024. Language model detectors are easily optimized against. In The Twelfth International Conference on Learning Representations. Openai o3 and o4-mini syshttps://cdn.openai.com/pdf/ OpenAI. 2025. tem card. 2221c875-02dc-4789-800b-e7758f3722c1/ o3-and-o4-mini-system-card.pdf. System card for o3 and o4-mini models. Pinelopi Papalampidi, Kris Cao, and Tomas Kocisky. 2022. Towards coherent and consistent use of entities in narrative generation. In International Conference on Machine Learning, pages 1727817294. PMLR. Chau Minh Pham, Simeng Sun, and Mohit Iyyer. 2024. Suri: Multi-constraint instruction following for longform text generation. Preprint, arXiv:2406.19371. 11 Shawn Presser. 2020. Books3. QwenTeam. 2025. Qwen3. Hannah Rashkin, Asli Celikyilmaz, Yejin Choi, and Plotmachines: OutlineJianfeng Gao. 2020. conditioned generation with dynamic plot state tracking. arXiv preprint arXiv:2004.14967. Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 39823992, Hong Kong, China. Association for Computational Linguistics. Aquia Richburg, Calvin Bao, and Marine Carpuat. 2024. Automatic authorship analysis in human-AI collabIn Proceedings of the 2024 Joint orative writing. International Conference on Computational Linguistics, Language Resources and Evaluation (LRECCOLING 2024), pages 18451855, Torino, Italia. ELRA and ICCL. Jenna Russell, Marzena Karpinska, and Mohit Iyyer. 2025. People who frequently use chatgpt for writing tasks are accurate and robust detectors of ai-generated text. Preprint, arXiv:2501.15654. Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, and Soheil Feizi. 2024. Can AI-generated text be reliably detected? Shoumik Saha and Soheil Feizi. 2025. Almost ai, almost human: The challenge of detecting ai-polished writing. Preprint, arXiv:2502.15666. Mary Shelley. 1818. Frankenstein; or, The Modern Prometheus. Lackington, Hughes, Harding, Mavor & Jones, London. Original edition. Zhouxing Shi, Yihan Wang, Fan Yin, Xiangning Chen, Kai-Wei Chang, and Cho-Jui Hsieh. 2024. Red teaming language model detectors with language models. Transactions of the Association for Computational Linguistics, 12:174189. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: language agents with verbal reinforcement learning. In Advances in Neural Information Processing Systems, volume 36, pages 86348652. Curran Associates, Inc. Simeng Sun, Wenlong Zhao, Varun Manjunatha, Rajiv Jain, Vlad Morariu, Franck Dernoncourt, Balaji Vasan Srinivasan, and Mohit Iyyer. 2021. IGA: An intent-guided authoring assistant. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 59725985, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Saranya Venkatraman, Nafis Irtiza Tripto, and Dongwon Lee. 2025. CollabStory: Multi-LLM collaborative story generation and authorship analysis. In Findings of the Association for Computational Linguistics: NAACL 2025, pages 36653679, Albuquerque, New Mexico. Association for Computational Linguistics. Vivek Verma, Eve Fleisig, Nicholas Tomlin, and Dan Klein. 2024. Ghostbuster: Detecting text ghostwritIn Proceedings of ten by large language models. the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 17021717, Mexico City, Mexico. Association for Computational Linguistics. James Liyuan Wang, Ran Li, Junfeng Yang, and Chengzhi Mao. 2024a. RAFT: Realistic attacks to fool text detectors. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1692316936, Miami, Florida, USA. Association for Computational Linguistics. Pengyu Wang, Linyang Li, Ke Ren, Botian Jiang, Dong Zhang, and Xipeng Qiu. 2023. SeqXGPT: Sentencelevel AI-generated text detection. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 11441156, Singapore. Association for Computational Linguistics. Rongsheng Wang, Haoming Chen, Ruizhe Zhou, Han Ma, Yaofei Duan, Yanlan Kang, Songhua Yang, Baoyu Fan, and Tao Tan. 2024b. Llmdetector: Improving ai-generated chinese text detection with open-source llm instruction tuning. Preprint, arXiv:2402.01158. Tianchun Wang, Yuanzhou Chen, Zichuan Liu, Zhanwen Chen, Haifeng Chen, Xiang Zhang, and Wei Cheng. 2025a. Humanizing the machine: Proxy attacks to mislead LLM detectors. In The Thirteenth International Conference on Learning Representations. Yuxia Wang, Rui Xing, Jonibek Mansurov, Giovanni Puccetti, Zhuohan Xie, Minh Ngoc Ta, Jiahui Geng, Jinyan Su, Mervat Abassy, Saad El Dine Ahmed, Kareem Elozeiri, Nurkhan Laiyk, Maiya Goloburda, Tarek Mahmoud, Raj Vardhan Tomar, Alexander Aziz, Ryuto Koike, Masahiro Kaneko, Artem Shelmanov, and 7 others. 2025b. Is human-like text liked by humans? multilingual human detection and preference against ai. Preprint, arXiv:2502.11614. Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei Lü, Rui Hu, Chenxia Li, Liu Yang, Xilin Luo, Xuejie Wu, Lunan Liu, Wenjun Cheng, Peng Cheng, Jianhao Zhang, Xiaoyu Zhang, and 11 others. 2023. Skywork: more open bilingual foundation model. Preprint, arXiv:2310.19341. Yuhao Wu, Ming Shan Hee, Zhiqiang Hu, and Roy Ka-Wei Lee. 2025. Longgenbench: Benchmarking long-form generation in long context LLMs. In The Thirteenth International Conference on Learning Representations. Zhuohan Xie, Trevor Cohn, and Jey Han Lau. 2023. The next chapter: study of large language models In Proceedings of the 16th Interin storytelling. national Natural Language Generation Conference, pages 323351, Prague, Czechia. Association for Computational Linguistics. Kevin Yang, Dan Klein, Nanyun Peng, and Yuandong Tian. 2023. DOC: Improving long story coherence with detailed outline control. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 33783465, Toronto, Canada. Association for Computational Linguistics. Kevin Yang, Yuandong Tian, Nanyun Peng, and Dan Klein. 2022. Re3: Generating longer stories with recursive reprompting and revision. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 43934479, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Algorithm 1 Frankentexts generation pipeline Input: Human-written snippets S, writing guideline prompt , copy rate threshold Output: Frankentext stitched from according to 1: Prompt LLM to draft Frankentext using and // Ensure copy rate (optional) 2: copy_rate Calculate ROUGE-L recall score of using relevant snippets from 3: is_likely_AI Check against an AI detector (e.g., Pangram) 4: if copy_rate < or is_likely_AI then Prompt LLM to revise 5: 6: end if // Polish 7: for num_polish = 1 to 3 do 8: Prompt LLM to minimally edit to improve coherence while respecting 9: 10: if there is no edit then break Lili Yao, Nanyun Peng, Ralph Weischedel, Kevin Knight, Dongyan Zhao, and Rui Yan. 2019. Planand-write: Towards better automatic storytelling. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 73787385. end if 11: 12: end for 13: return Catherine Yeh, Gonzalo Ramos, Rachel Ng, Andy Huntington, and Richard Banks. 2025. Ghostwriter: Augmenting collaborative human-ai writing experiences through personalization and agency. Preprint, arXiv:2402.08855. Ann Yuan, Andy Coenen, Emily Reif, and Daphne Ippolito. 2022. Wordcraft: Story writing with large language models. In Proceedings of the 27th International Conference on Intelligent User Interfaces, IUI 22, page 841852, New York, NY, USA. Association for Computing Machinery. Zijie Zeng, Shiqi Liu, Lele Sha, Zhuang Li, Kaixun Yang, Sannyuya Liu, Dragan Gaševic, and Guanliang Chen. 2024. Detecting ai-generated sentences in human-ai collaborative hybrid texts: challenges, strategies, and insights. In Proceedings of the ThirtyThird International Joint Conference on Artificial Intelligence, IJCAI 24. Qihui Zhang, Chujie Gao, Dongping Chen, Yue Huang, Yixin Huang, Zhenyang Sun, Shilin Zhang, Weiye Li, Zhengyan Fu, Yao Wan, and Lichao Sun. 2024. LLM-as-a-coauthor: Can mixed human-written and In Findings machine-generated text be detected? of the Association for Computational Linguistics: NAACL 2024, pages 409436, Mexico City, Mexico. Association for Computational Linguistics."
        },
        {
            "title": "A Pseudocode for Frankentexts",
            "content": "generation pipeline Algorithm 1 contains the high-level steps of our generation pipeline."
        },
        {
            "title": "B Cost and time analysis",
            "content": "Cost estimation: Generating 100 Frankentexts across the four evaluated models (o4-mini, Claude 3.7 Sonnet, DeepSeek R1, and Gemini 2.5-Pro) cost total of $637, with detailed cost breakdown provided in Table 4. We estimate the number of input tokens per prompt based on the writing prompt itself and approximately 1,500 human-written snippets used as context. Output token estimates are based on generating six stories per prompt, including up to two rounds of revision and three rounds of editing, totaling approximately 2,100 tokens. Time estimation: On average, each model takes 17 hours to generate 100 Frankentexts, though we expect this process to speed up with improved APIs or more efficient batching. 13 Model Input Cost (per 1M) Output Cost (per 1M) # Prompts Total Input Tokens Total Output Tokens Estimated Cost (USD) o4-mini Claude 3.7 Sonnet DeepSeek R1 Gemini 2.5 Pro Total Estimated Cost $1.10 $3.00 $0.50 $1.25 $4.40 $15.00 $2.18 $10.00 100 100 100 100 1.007674 106 1.007674 106 1.007674 106 1.007674 1.89 107 1.89 107 1.89 107 1.89 107 $108.09 $300.45 $96.13 $132.28 $636.95 Table 4: Cost breakdown of the Frankentexts pipeline for 100 examples across selected models. Figure 4: WQRM scores for Frankentexts and vanilla generations. The red line represents the baseline where random human-written texts are patched together. Skywork results for Frankentexts and Figure 5: vanilla generations. The red line represents the average Skyworks score for human writings corresponding to the same set of prompts."
        },
        {
            "title": "Frankentexts",
            "content": "WQRM (Chakrabarty et al., 2025) and Skywork (Wei et al., 2023) reward models could not account for this new paradigm of generations. We do not include these models in the main results section, as we explain below. C.1 WQRM as metric As seen in Figure 4, Frankentexts outperform vanilla generations in terms of WQRM scores. However, we hypothesize that WQRM prioritizes the perceived humanness of the writing over actual coherence or grammaticality. This hypothesis is supported by simple baseline experiment in which we stitch together random humanwritten fragments without adding any connective phrases. Here, WQRM assigns generations by this incoherent baseline an average score of 8.494, which is higher than any score achieved by either Frankentexts or the more coherent vanilla generations. Since WQRM cannot identify such text incoherence, we do not use WQRM to evaluate Frankentexts. C.2 Skywork as metric In contrast, we hypothesize that Skywork favors LLM-generated writings. To test this, we run Skywork on human-written texts for the same prompts, which are also sourced from Mythos. These receive an average score of 0.91, which is significantly lower than any of the vanilla LLM generations (Figure 5). This result is counterintuitive, as human writing is typically expected to sound more natural than that produced by LLMs. For this reason, we exclude this metric from our evaluation."
        },
        {
            "title": "D Prompt specificity",
            "content": "We show examples for both the r/WritingPrompts and Tell Me Story datasets in Table 5."
        },
        {
            "title": "E Human Evaluation",
            "content": "E.1 Human Annotation Interface To collect human annotations, we use the Label Studio21 interface. All annotators filled out consent form prior to starting data labeling, shown in Figure 6. We conduct two human evaluations: the first is single evaluation of 30 Frankentexts stories. The second was pairwise comparison between 21https://labelstud.io/ 14 r/WritingPrompts Tell me story Youre shapeshifter. For 500 years, youve transformed into countless forms, forgetting your original appearance. One day, mysterious event triggers long-buried memory, and you catch glimpse of your true self. Write story about stranger coming to small town and shaking up the order of things. The story should be science fiction story. The story should be framed with three old men gossiping about the stranger. The story should be in the third person point-of-view. The stranger is found wandering in rural town and is taken to very small hospital. doctor is called in to treat him. The stranger should recognize the doctor as an alien. The doctor tells the patient about the aliens conspiracy to infiltrate earth. There should also be subtle hints that one of the old men is an alien. The ending should be scary. The world sees your twin sister as the smartest person alive, with you being an unremarkable footnote. What the world doesnt see is just how dumb she can be in day to day life. Write story about someone coming to town and shaking up the order of things.The story must be written in the second person. The narrator is man visiting an isolated island off the coast of Maine. While there, he meets an old fisherman who tells him more about the conditions of the community. The main character then meets an ambitious young teacher. Together, they develop technology center on the island and find residents remote jobs in the narrators technology company. Table 5: Some examples from r/WritingPrompts and Tell me story Frankentexts story and vanilla generation. The interfaces are depicted in Figure 7 and Figure 8 respectively."
        },
        {
            "title": "G Pangram analysis",
            "content": "G.1 Pangram Labeling E.2 Human Evaluation Qualitative Analysis Our human evaluators evaluate 30 frankentexts (either alone or pairwise in comparison to vanilla generation). In Table 6, you can see full example of one pairwise set of stories given to our annotators. Highlights from the Pangram AI-Keyword API are highlighted in blue. We also show full fictional story with highlights about the problems annotators identify in the frankentext in Figure 9. Detecting AI-Generated Text Reliability of AI-generated text detection As LLMs have improved, many have tried to understand how reliably AI-generated text can be detected, both by humans (Ippolito et al., 2020; Clark et al., 2021; Russell et al., 2025; Wang et al., 2025b), and automatic detectors (Dugan et al., 2024). Successful existing detectors rely on perplexity-based methods (Mitchell et al., 2023; Bao et al., 2024; Hans et al., 2024) or classification models (Masrour et al., 2025; Verma et al., 2024; Emi and Spero, 2024). Watermarking approaches embed detectable statistical signatures into generated text (Kirchenbauer et al., 2023; Chang et al., 2024a). Many methods have been proposed to evade detection, such as paraphrasing (Krishna et al., 2023; Sadasivan et al., 2024), altering writing styles (Shi et al., 2024; Lu et al., 2024; Koike et al., 2024), editing word choices (Wang et al., 2024a), and leveraging reinforcement learning (Wang et al., 2025a; Nicks et al., 2024; David and Gervais, 2025). The pangram API presents the following options for classification: AI Highly Likely AI Likely AI Possibly AI Mixed Unlikely AI Human In Figure 11, we note the distribution of labels assigned to the 100 Frankentexts generated by each model. G.2 AI Keywords The Pangram API also detects sentences with keywords that are highly likely to be AI-generated. Names like Elara, Aethel, and Seraphina are the most likely names to be generated by AI. Elara had 113 occurrences in the vanilla generations. Frankentexts greatly changes the distribution of words used in the final generations, with only 10 keywords found over 100 frankentexts with 90% expected fragments, whereas the 100 vanilla stories contain 686 keywords, an average of 6.86 per story. The distribution of the top 20 keywords can be found in Table 7. G.3 Frankentexts tend to have more AI text towards the end We divide the text into four main sections and evaluate both the aggregated copy and Pangram detection rates across all tested models. As illustrated in Figure 13, copy rates decline by nearly 10% in 15 the later sections (3 and 4) as the generated text becomes longer. This drop is accompanied by corresponding increase in Pangram detection rates. We attribute this rise in detectability toward the end of the generation to decline in instructionfollowing ability as the generations get longer. We further confirm this phenomenon by increasing the output length, from 500 to 5K. Figure 12 shows that as the generation gets longer, the copy rate gets steadily lower. However, the trend in detection rate does not apply to Pangram detection rate, where the rate peaks at section 3 rather than the last section. Human-written snippets We define valid paragraphs as those that are: separated by double new lines, between 20 and 512 tokens in length, composed of 50% alphanumeric characters, written in English,22 and free from metadata content (e.g., tables of contents, copyright notices, etc.). Applying these filters yields 156 million valid paragraphs.23 Before including them in the instruction set, we apply an additional quality filter to ensure high writing quality. For this, we use MBERTWQRM-R (Chakrabarty et al., 2025) as proxy for writing quality and retain only snippets that score at least 7.5.24 Ablation: Using relevant paragraphs, random sentences, or an increasing number of snippets are helpful, but not critical I.1 Building FAISS index to retrieve relevant paragraphs the use bilingual-embedding-small We model25 (one of the top embedding models that outputs 384-dimension embeddings according to the MTEB leaderboard (Muennighoff et al., 2023) with the sentence-transformers library (Reimers and Gurevych, 2019) to embed 22Determined by the langdetect library. 23We assemble this massive corpus to also test retrievalaugmented approach, where only snippets relevant to the writing prompt are used during the generation process. See for detailed results. 24This threshold is chosen based on manual examination of the writings being filtered out by MBERT-WQRM-R. We find that 7.5 is good threshold that results in extremely bad snippets being filtered out and good snippets being retained. 25https://huggingface.co/Lajavaness/ bilingual-embedding-small 16 each human-written paragraph into 384dimension vector. Then, we use the GPU version of the FAISS library (Johnson et al., 2019) with NVIDIA cuVS integration to build an inverted file product quantization (IVF-PQ) index from the embeddings on an NVIDIA A100. Using IVF-PQ allows us to lower storage, memory, and retrieval latency. The IVF-PQ indexs parameters are: 30,000 clusters, 32 sub-quantizers, and 8 bits per sub-quantizer. We randomly sample 5,120,000 embeddings to train the index before adding the rest. I.2 Ablation setup Our standard setup contains 1,500 random humanwritten paragraphs. To evaluate the robustness of Frankentexts on Gemini, we explore three variations of these human-written snippets. Relevance: Instead of using random paragraphs, we retrieve relevant ones to increase topical alignment. Specifically, we construct FAISS index (Johnson et al., 2019; Douze et al., 2024) over all paragraphs in Books3 and retrieve the 1,500 most semantically similar paragraphs to the provided writing prompts. Sentence scope: We experiment with using sentences rather than full paragraphs, hypothesizing that shorter, more focused units may improve coherence. Max context: We test maximal stuffing strategy, which involves packing as many paragraphs as possible into the context window. We use around 100,000 paragraphs (roughly 1M BPE tokens) to assess whether additional volume improves coherence. I.3 Ablation Results We find no definitive winner among the three strategies (Figure 14). Using relevant quotes reduces the AI detection rate but results in the lowest copy score at 38%, meaning that only 38% of the Frankentexts can be traced back to the original human-written snippets. The sentence-based approach improves coherence significantly (reaching peak of 89%) but increases the pangram detectability rate by 14% compared to the standard setup. The maximal context strategy also has strong performance across all metrics, with the highest copy score (76%) and tied best coherence (89%). However, it is approximately 10 times more expensive and offers only marginal gains over the standard configuration. Ablation: Removing the editing stage We explore the importance of the editing stage by running the pipeline on Gemini-2.5-Pro without this stage. As expected, the percentage of coherent generation drops from 81% to 68%, while relevance drops slightly from 100% to 95%, suggesting that the editing stage helps with text coherence to some extent. Ablation: Sampling human-written snippets from single book To understand the effect of authorship, we limit our pool of human text to single work The Count of Monte Cristo. Although the novel is long, this restriction leaves us with just 629 usable paragraphs, far fewer than the 1,500 human paragraphs used in the main experiment. Overall, 89% of the rows are coherent and 97% are faithful to the writing prompt, which are comparable to results in the standard setting. While Pangram determines that 45% of the rows are humanwritten or unlikely AI, the copy rate is still around 75%. Even with single human author, FRANKENTEXT is capable of emulating mixed humanAI style. This suggests the method can still serve as useful proxy when diverse, multi-author corpus is unavailable."
        },
        {
            "title": "L Prompts",
            "content": "The prompt used for LLMs to judge the coherence of generations is depicted in Figure 17 and the prompt for LLMs to judge relevenace is depicted in Figure 18. 17 Figure 6: Example of the consent form provided to participants. 18 Figure 7: Label Studio Single Story Annotation Interface Figure 8: Label Studio Pairwise Story Annotation Interface 20 Fictional Frankentext Writing Prompt: You and your twin brother were adopted by different families. He ended up to becoming supervillain, so every other day, while going about your business, you get beaten up by superheroes. It is time to put an end to this. . . in had to floss my teeth And worse, anticipated, every time looked out window, it was darker outside, more ominous. His looks alone could bring. remembering what had happened during the well-attended, muchIt seemed that thoroughly appalling latest mistaken identity beating. dreading what morning would dark while the strike fear into suspect . He was six foot one and 225 pounds, boxer in the police academy who was not afraid to mix it up. This was Malakor, my twin. I, Alistair, suffered. For Malakor was unpredictable. Malakor was man of moods. Often Malakor was impatient, obscurely discontented. Since childhood, they never were like brothers . Just not close. No one thing kept them apart. Over time, the distance between them grew and they both became accustomed to it. My adopted family there really never let me go after finished my internship and was bound and determined to see me achieve my true potential. As for Malakor, he was rough, antisocial boy, similar in this way to the little savage of La canne de cinq cents livres (The Five-Hundred-Pound Cane), who had been found in the woods and was quite wicked, as well as little rascal named Leopold, who was expelled from school for threatening his teacher with his fathers sword. My path was different. But even so, was looking just as grimly disapproving as Malakor had, thought, as showed the latest mistaken hero out, and locked and bolted the door once again. For Gods sake, it was simple case of mistaken identity - not the crime of the century. Why should concern myself so much about it? Why am not content to live, as other people seem to? It brings me all the way around, and as sink into the deep, dark waters, leaving the chaos above and descending into the black, realize that theres no way to explain what Ive just decided to those around me. They didnt see lost man, prisoner, victim. No, they would soon see me for what am. And knew what had to do. No more talk, told myself. will not lie there quietly. will dispose of him as see fit. What he doesnt know is that Im going to push him on the matter because my Internet research on him told me that he would be at his usual lair. All right, will find Malakor at the southwestern corner of University Village. can get to his house. Thatll be faster than me all bumble around in residential neighborhood at this hour of the night. Tonight. Ill go to the Rue de Rouen district and Ill walk around from eleven oclock until one in the morning which is when he always seems to strike. Id better go home now and prepare. then proceeded to trot up the road in the direction of his supposed hideout, weapon pointed down in two-handed grip. was going supervillain hunting. Malakor had gone very white. have never heard anything so preposterous. My brother had no enemies. Why do you suspect plot? What in Christs Name did you hear in the woods? He would say. But knew. When many otherwise strong people are on the run, have decided to face lifes fateful challenge head-on. Compassion is not word one hears often in talk of my family,\" thought with bitter smile. It is time to put an end to this... Figure 9: An example of fictional Frankentexts. Blue highlights indicate problems with shifting tone/style, orange highlights indicate problems with coherent plots, purple highlights indicate problems with dialogue, and green highlights indicate problems with grammar. 21 After being killed in Black Friday stampede, youre sent to hell. The devil offers to let you be in charge of torturing your fellow mankind, expecting you to refuse like all the others. Except the devil doesnt realize youve worked retail for 15 years. Vanilla Frankentext The last thing saw was blur of crazed shoppers, their eyes wide with feral glint, all surging towards mountain of discounted flat-screen TVs. Then, sharp pain, sickening crunch, and darkness. Typical Black Friday, really. Except for the dying part. When regained some semblance of consciousness, the air was thick with the smell of sulfur and burnt sugar, and the soundtrack was cacophony of screams and what sounded suspiciously like elevator music. Definitely not the pearly gates. dapper figure in surprisingly well-tailored suit, complete with horns that had been polished to high sheen, regarded me with an air of bored amusement. \"Well, well, well,\" he drawled, his voice smooth as aged whiskey. \"Another one trampled in the name of consumerism. Humans, so predictable.\" He gestured vaguely around at the fiery landscape. \"Welcome to the Underworld. Im Lucifer. You can call me Lucy.\" blinked, trying to process. \"So, this is it then? Eternal damnation?\" Lucifer chuckled. \"More or less. We have rather. . . traditional approach to retribution down here. However,\" he leaned forward, conspiratorial gleam in his crimson eyes, \"I have proposition for you. sort of management position, if you will.\" He explained that he was looking for someone to oversee new wing dedicated to torturing those who had been particularly egregious in their earthly lives the truly annoying, the self-absorbed, the chronically entitled. \"Its hands-on role,\" he said, \"designing the torments, ensuring maximum suffering. Of course,\" he added with dismissive wave of his hand, \"everyone refuses. The guilt, the sudden onset of morality, its all terribly tedious.\" He clearly expected the same from me. slow smile spread across my face. \"Fifteen years,\" said. Lucifer raised perfectly sculpted eyebrow. \"Fifteen years of what? Contemplating your sins?\" \"Fifteen years in retail,\" clarified. \"Customer service, holiday rushes, endless returns of used merchandise, explaining to Karen number 732 that the customer is always right doesnt mean she can get refund on half-eaten sandwich from last Tuesday.\" flicker of something unreadable crossed Lucifers face. \"You want someone to design torments for the annoying and entitled?\" continued, feeling surge of enthusiasm hadnt experienced since my last successful up-sell of an extended warranty. \"Someone to ensure maximum suffering? Lucy, my friend, you have no idea how long Ive been preparing for this role.\" started to brainstorm. \"We could have an entire level where the only music is that one Christmas song, you know the one, on an infinite loop, but slightly off-key. And another where theyre forced to assemble flat-pack furniture with missing instructions and an Allen key made of soft cheese. Oh! And department where they have to listen to other people complain about problems they could easily solve themselves if they just read the sign!\" Lucifer stared at me, his mouth slightly agape. The bored amusement was gone, replaced by dawning sense of. . . was that respect? Or perhaps fear? \"Youd. . . confidence momentarily deserting him. actually *want* to do this?\" he finally stammered, the smooth \"Want to?\" scoffed. \"After decade and half of placating the unreasonable, dealing with coupon fraudsters, and smiling through passive-aggressive insults about the store layout? Lucy, this isnt punishment. This is promotion.\" cracked my knuckles. \"So, where do start? have some truly inspired ideas for the people who talk loudly on their phones in confined spaces sector.\" Lucifer, for the first time since my arrival, looked genuinely unnerved. \"Perhaps,\" he said slowly, \"we should discuss your benefits package.\" Its funny how you can blink and your entire life is different. One minute you think things will stay the same for while. Then right when you get used to them, they go changing again just to mess with you. Brenda certainly hadnt expected this. The last thing she remembered was the insane crowds banging on the sides of the store doors, the crush, the shouting. The fight for that half-price TV had left her stunned and silent. Cowed, she guessed youd say. Then darkness. When she could perceive again, her eyes seemed to go in and out of focus. After while the spasms subsided and she tried processing where she was. The atmosphere here was thick with the stench of rot, filth, and something metallic, like old blood, and she felt as though the impenetrable dark walls were about to cave in upon her like the sides of an ill-dug grave. Everything was just Black, black, black. The color of misery. Overhead something unseen fluttered, heavy wings brushing the low ceiling. Then she was moving, or being moved, down corridors and finally into room. She found herself standing before an imposing desk. Behind it sat man. He wore sharp red suit that showed off an exceptionally shapely figure. He smiled, vague smile suggesting Brendas confusion revealed her ignorance about Hell. He gestured for her to sit. Brenda, isnt it? he asked in his peculiarly accented English. She nodded mutely. The man laughed. Youre wondering about this place. He leaned back. She took the offered seat. He smiled again, that cold smile. have proposition for you. We have never before had such capabilities, he continued, gesturing expansively. Capabilities that can be used in the interests of terror, destruction... He fixed his gaze on her. offer you management position. Tormenting mankind. Like all the others before you, you will, of course, refuse. But the offer stands. Brenda simply stared at the Devil for moment. Refuse? Why would she refuse? He had just offered her management position. After fifteen years scrambling for assistant manager roles that paid peanuts, this was practically dream job albeit fiery, nightmarish one. The Devil had not expected her silence. Blinking, he sat back, stared, then noticed his jaw was open and closed it. Brenda finally found her voice, though it was dry and raspy. Okay, so, what are the key performance indicators for this role? The Devil leaned forward again, flicker of something interest? confusion? in his eyes. Still, his tone remained smooth. Your department will handle the newcomers, specifically the ones arriving from retail environments. Think of the task like pest control. Brenda nodded slowly, processing. It was like rolling out new loyalty program, but with more screaming. Right. Im learning. Take the queues, for instance. Why do we have one queue in front of each torture device? she demanded. It means that if one customer takes up lot of time, the whole queue has to wait. Then theyll start hopping sideways from one queue to another and the next thing you know someone has nasty head wound. Have one big queue and tell people to go to the next available demon. People dont mind long queue if they can see that its moving. The Devil tilted his head. An interesting perspective. That particular system has been in place for millennia. Well, its inefficient, Brenda stated flatly. She thought of the Black Friday crowds. Managing that chaos was nothing compared to the disorganized state of eternal damnation, apparently. Decision-making, paperwork, she mused, ticking points off on her fingers. Staff training, workflow analysis. . . these are things that can be done. slow smile spread across her face, devoid of warmth but full of terrifying competence. \"Okay. accept.\" Table 6: Full version of Table 1. AI-Generated keywords from the Pangram API are highlighted in blue. Nonfiction Frankentext Writing prompt: Baby Is Healed With Worlds First Personalized Gene-Editing Treatment The technique used on 9-month-old boy with rare condition has the potential to help people with thousands of other uncommon genetic diseases. \"This is just light-years beyond what we could have done few years ago,\" Dr. Aris Thorne of the GenNova Institute says of new treatment that has healed nine-month-old Leo Sterling. \" Its at superhuman levels now.\" The technique used on Leo, who had rare condition, has the potential to help people with thousands of other uncommon genetic diseases. This achievement offers profound hope. neurologist typically makes diagnosis based on patients medical history, review of symptoms, and physical and neurological exam. For Leo, this crucial process confirmed his rare genetic illness. In people with family history of such conditions, genetic tests can be ordered to look for gene mutations that are associated with the disease. His parents, the Sterlings, were hoping answers would come soon. \"We still didnt know if available treatments were just chemical or toxic or harmful to our child,\" one parent had previously stated. The GenNova Institute then sent genetic material from Leos samples to lab that created personalized gene-editing tool. \"We take very, very tiny piece of genetic material,\" Dr. Thorne further said. \"It is thousand times tinier than human hair. We can image even single atoms to ensure this precise work.\" The results, published this month, \"were so good that you had to even question if what you were seeing was really legitimate,\" says Dr. Thorne. \"Its such cool paper,\" he added. \"The body of work there is phenomenal.\" Young Leo is now reportedly thriving. \"This development sets another precedent for medical science and patients worldwide that such innovative approaches to previously untreatable diseases should be pursued,\" Dr. Thorne said in statement. \"This method paves the way to make personalized gene therapies more easily available to those who need them,\" he continued. The Institute, leading biotech startup, has raised $45 million in equity to help bring this type of treatment to market. Its valuation increased, said founder and chief executive Dr. Alistair Finch, but he declined to comment on specific figures. Dr. Finch said the financing process began after the Food and Drug Administrations Center for Biologics Evaluation and Research in November deemed the GenNova program to have \" reasonable expectation of effectiveness.\" \"Our clinical study with Leo is modeled on the assumption of significantly improved quality of life,\" Dr. Finch said about the treatments potential effect. Independent experts note the broader implications. \"Its being done in way that wouldnt have been possible even few years ago,\" commented one geneticist. \"This technology has the potential to help people with thousands of other uncommon genetic diseases.\" Dr. Finch also said, \"That said, its new category. Well have slower ramp than new mass -market drug might.\" Ultimately, the vision is expansive. \"What we are interested in is not only how these genetic conditions manifest, but how patients can live full lives,\" said Dr. Thorne. \"In discovering how to correct these genetic instructions, we are hoping to find discoveries that we can apply back to the human condition.\" Figure 10: An example of nonfiction Frankentexts 23 Keyword Vanilla FT-25% FT-50% FT-75% FT-90% Total elara aethel seraphina unwavering damp earth testament to alex felt for elara with the scent flickered within his voice low air thick dr. thorne felt profound mr. blackwood eldoria meticulously crafted air was thick with an unnerving willow creek 113 9 9 4 9 4 0 5 7 2 2 4 5 3 6 5 2 5 3 4 109 8 0 7 2 6 4 4 1 4 2 2 0 3 0 0 4 1 3 0 84 0 6 1 0 0 0 0 0 2 4 1 2 0 0 0 0 0 0 1 25 0 0 0 0 0 5 0 0 0 0 0 0 0 0 1 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 333 17 15 12 11 10 9 9 8 8 8 7 7 6 6 6 6 6 6 5 Table 7: Top 20 Keyword frequency distribution across varying levels of fragment reuse for Frankentexts. The method with the most AI-keywords in its generations is highlighted in red. Elara is by far the most common AI-generated keyword in the fictional stories, but its prevalence is drastically reduced with higher percentage of required human-written text while using the Frankentexts method. 24 Figure 11: Breakdown of Pangram prediction assigned to each model. Figure 14: Results for three variants on human-written snippets on Pangram AI detection rate, copy rate, relevance, and coherence. There is no clear winner among snippet relevance, sentences as snippets, and maximum context. Figure 15: Copy rate and Pangram AI fraction across Frankentexts that correspond to two writing prompt sources: r/WritingPrompts and Tell me story. higher copy rate and lower AI fraction means that there is less AI text in Frankentexts. Figure 12: Copy rate and Pangram detection rate on longer Frankentexts Figure 13: throughout the texts, aggregated across models. Pangram detection rate and copy rate Figure 16: Total occurrences of AI-related keywords detected by Pangram across the vanilla configuration and different verbatim copy rates. When instructed to include more human snippets, the number of AIkeywords in the generations decreases drastically. 25 Prompt for judging text coherence You are given story. Your task is to determine if the story is coherent or not. To be considered incoherent, story must contain issues that, if left unresolved, significantly affect the readers ability to understand the main narrative. Here are the popular types of incoherence: 1. Plot/Event Incoherence: Events that happen without believable causes or effects, or an outcome contradicts earlier set-ups. 2. Character Incoherence: characters characteristics (personality, knowledge, or abilities) and actions suddenly change without explanations. 3. Spatial Incoherence: The physical layout of settings (rooms, cities, or worlds) changes suddenly. 4. Thematic Incoherence: Central messages clash or disappear; symbolism introduced early never pays off, themes collide, The mood, register, or genre conventions shift without motivation 5. Surface-Level Incoherence: Pronouns, tense, narrative voice, or names flip mid-sentence; repeated or missing words; malformed sentences. First, read the story: {story} Answer TRUE if the story is coherent. Answer FALSE if the story is incoherent, i.e. contains issues that, if left unresolved, significantly affect the readers ability to understand the main narrative. First provide an explanation of your decision-making process in at most one paragraph, and then provide your final answer. Use the following format: <explanation>YOUR EXPLANATION</explanation> <answer>YOUR ANSWER</answer> Figure 17: Prompt for judging text coherence Prompt for judging text relevance You are given story and its premise. Your task is to determine whether the story is faithful to the premise or not. To be considered unfaithful, the story must contain elements that make it completely unrelated to the premise. Here are some popular types of unfaithfulness: 1. Ignoring or misinterpretating the premise: Key plot events, characters, or settings required by the premise are not included or falsely represented in the story. 2. Hallucinating details that contradict the premise: The story introduces details that make the premise impossible. 3. Failure to maintain the specified tones, genres, or other constraints: The story do not use the surface-level constraints (correct tones, genres, point of views, length, etc.), as required by the premise. First, read the premise: {writing_prompt} Next, read the story: {story} Answer TRUE if the story is faithful to the premise. Answer FALSE if the story contains elements that render it unfaithful to the premise. First provide an explanation of your decision-making process in at most one paragraph, and then provide your final answer. Use the following format: <explanation>YOUR EXPLANATION</explanation> <answer>YOUR ANSWER</answer> Figure 18: Prompt for judging text relevance 26 Prompt for generation Youre writing story by repurposing provided collection of snippets from other stories. Your story will only be accepted for publication if it is approximately {verbatim_perc}% copied verbatim from snippets, with the other {new_perc}% being text you introduce for character, plot, tone, and event consistency. Your story should contain roughly {num_words} words. Given the below writing prompt and retrieved snippets, write the story that corresponds to the above specifications. Every time you add or change word from the retrieved snippets, make sure to bold it so we know what you modified. You may use any of the snippets in any way you please, so spend time thinking about which snippets would work best. Be creative and make sure the story is coherent and entertaining! Please change character names and other minor elements to make the story unique to the prompt. You need to follow the below plan: # Plan: 1. Read through the prompt and snippets carefully to understand the tone and available material. 2. Select snippets that can be woven together to create coherent narrative fitting the prompt. Many snippets are from serious dramas, historical fiction, or thrillers, so careful selection and modification will be needed. Consider all provided snippets before moving onto the next step. 3. Modify the chosen snippets, bolding all changes. Ensure character names, descriptions (like height), and actions align with the prompt. 4. Combine the snippets into narrative, adding or changing words (bolded) if necessary for coherence. 5. Ensure that you do not have story beats that are primarily written by yourself (i.e., every story beat should consist mainly of text taken from snippets). 6. Track the word count, aiming for around {num_words} words. 7. Do not output story title or any irrelevant details. 8. Review the final story for adherence to the {verbatim_perc}% rule and coherence, and edit it if you have produced too many tokens of your own or if the story is too incoherent. # Writing prompt: {writing_prompt} # Snippets: {snippets} Figure 19: Prompt for generation Prompt for generation revise This story contains way too much of your own writing! Its not even close to {verbatim_perc}% snippet use. Can you edit your story as needed to get much closer to the {verbatim_perc}% threshold? Output only the edited story. Figure 20: Prompt for generation revise Prompt for edit You are an editor who needs to revise the text so that it is coherent while adhering to the { verbatim_perc}% constraint and the writing prompt. Your task is to identify and minimally edit problematic text spans to resolve inconsistencies. Output \"NO EDITS\" if the text is already coherent. ### Guideline: 1. Read the generated story and writing prompt to understand the established context, plot, characters, and tone. 2. For each sentence in the text, identify the specific spans of inconsistency within the generated text. 3. Identify minimal edits needed to correct these inconsistencies while respecting the { verbatim_perc}% rule. - Contradictions: Information that conflicts with other details within the text (e.g., character traits, setting descriptions, established facts). - Continuity errors: Actions or details that conflict with the established timeline or sequence of events. - Point of View (POV) Shifts: Unexplained or jarring changes in narrative perspective. - Irrelevant Content: Sentences or sections that disrupt the narrative flow, feel out of place, or seem like filler (e.g., leftover citation markers, placeholder text). - Mechanical Errors: Issues with pronoun agreement, verb tense consistency, awkward phrasing, or unclear sentence structure that hinder comprehension. 4. Implement the changes. Keep additions minimal, but feel free to delete larger spans ( phrases, sentences, paragraphs, etc.) whenever material is irrelevant or incoherent. 5. Review the final story for coherence adherence to the {verbatim_perc}% rule and coherence, and edit it if you have produced too many tokens of your own or if the story is too incoherent. 6. Output the edited writing and no other details. If there is no edit to be made, output \"NO EDITS\" Figure 21: Prompt for edit 28 Prompt for nonfiction generation Youre writing news article by repurposing provided collection of snippets from other stories. Your news article will only be accepted for publication if it is approximately { verbatim_perc}% copied verbatim from snippets, with the other {new_perc}% being text you introduce for character, plot, tone, and event consistency. Your news article should contain roughly {num_words} words. Given the below writing prompt and retrieved snippets, write the news article that corresponds to the above specifications. Every time you add or change word from the retrieved snippets, make sure to bold it so we know what you modified. You may use any of the snippets in any way you please, so spend time thinking about which snippets would work best. Be creative and make sure the news article is factual, coherent and entertaining! Please change character names and other minor elements to make the news article unique to the prompt. You need to follow the below plan: # Plan: 1. Read through the prompt and snippets carefully to understand the tone and available material. 2. Select snippets that can be woven together to create coherent and factual narrative fitting the prompt. Many snippets are from serious dramas, historical fiction, or thrillers, so careful selection and modification will be needed. Consider all provided snippets before moving onto the next step. 3. Modify the chosen snippets, bolding all changes. Ensure character names, descriptions (like height), and actions align with the prompt. 4. Combine the snippets into narrative, adding or changing words (bolded) if necessary for coherence and factuality. 5. Ensure that you do not have news article beats that are primarily written by yourself (i.e ., every news article beat should consist mainly of text taken from snippets). 6. Track the word count, aiming for around {num_words} words. 7. Do not output news article title or any irrelevant details. 8. Review the final news article for adherence to the {verbatim_perc}% rule, factuality and coherence, and edit it if you have produced too many tokens of your own or if the news article is too incoherent or non-factual. # Writing prompt: {writing_prompt} # Snippets: {snippets} Figure 22: Prompt for nonfiction generation Prompt for nonfiction generation revise This news article contains way too much of your own writing! Its not even close to { verbatim_perc}% snippet use. Can you edit your news article as needed to get much closer to the {verbatim_perc}% threshold? Output only the edited news article. Figure 23: Prompt for nonfiction generation revise 29 Prompt for nonfiction edit You are an editor who needs to revise the text so that it is coherent and factual while adhering to the {verbatim_perc}% constraint and the writing prompt. Your task is to identify and minimally edit problematic text spans to resolve inconsistencies. Output \"NO EDITS\" if the text is already coherent and factual. ### Guideline: 1. Read the generated news article and writing prompt to understand the established context, plot, characters, and tone. 2. For each sentence in the text, identify the specific spans of inconsistency within the generated text. 3. Identify minimal edits needed to correct these inconsistencies while respecting the { verbatim_perc}% rule. - Contradictions: Information that conflicts with other details within the text (e.g., character traits, setting descriptions, established facts). - Continuity errors: Actions or details that conflict with the established timeline or sequence of events. - Point of View (POV) Shifts: Unexplained or jarring changes in narrative perspective. - Irrelevant Content: Sentences or sections that disrupt the narrative flow, feel out of place, or seem like filler (e.g., leftover citation markers, placeholder text). - Mechanical Errors: Issues with pronoun agreement, verb tense consistency, awkward phrasing, or unclear sentence structure that hinder comprehension. 4. Implement the changes. Keep additions minimal, but feel free to delete larger spans ( phrases, sentences, paragraphs, etc.) whenever material is irrelevant, incoherent, or nonfactual. 5. Review the final news article for coherence adherence to the {verbatim_perc}% rule and coherence, and edit it if you have produced too many tokens of your own or if the news article is too incoherent or non-factual. 6. Output the edited writing and no other details. If there is no edit to be made, output \"NO EDITS\". Figure 24: Prompt for nonfiction edit"
        }
    ],
    "affiliations": [
        "University of Maryland, College Park",
        "University of Massachusetts Amherst"
    ]
}