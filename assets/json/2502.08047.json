{
    "paper_title": "WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation",
    "authors": [
        "Henry Hengyuan Zhao",
        "Difei Gao",
        "Mike Zheng Shou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Current GUI agents have achieved outstanding performance in GUI element grounding. However, planning remains highly challenging, especially due to sensitivity to the initial state of the environment. Specifically, slight differences in the initial state-such as the target software not being open or the interface not being in its default state-often lead to planning errors. This issue is widespread in real user scenarios, but existing benchmarks fail to evaluate it. In this paper, we present WorldGUI, a novel GUI benchmark that designs GUI tasks with various initial states to simulate real computer-user interactions. The benchmark spans a wide range of tasks across 10 popular software applications, including PowerPoint, VSCode, and Adobe Acrobat. In addition, to address the challenges of dynamic GUI automation tasks, we propose GUI-Thinker, a holistic framework, leveraging a critique mechanism, that effectively manages the unpredictability and complexity of GUI interactions. Experimental results demonstrate that GUI-Thinker significantly outperforms Claude-3.5 (Computer Use) by 14.9% in success rate on WorldGUI tasks. This improvement underscores the effectiveness of our critical-thinking-based framework in enhancing GUI automation."
        },
        {
            "title": "Start",
            "content": "WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation Henry Hengyuan Zhao 1 Difei Gao 1 Mike Zheng Shou 1 5 2 0 2 2 1 ] . [ 1 7 4 0 8 0 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Current Graphical User Interface (GUI) agents have achieved outstanding performance in GUI element grounding. However, planning remains highly challenging, especially due to sensitivity to the initial state of the environment. Specifically, slight differences in the initial statesuch as the target software not being open or the interface not being in its default stateoften lead to planning errors. This issue is widespread in real application scenarios, but existing benchmarks fail to evaluate it. In this paper, we present WorldGUI, novel GUI benchmark that designs GUI tasks with various initial states to simulate real computeruser interactions. The benchmark spans wide range of tasks across 10 popular software applications, including PowerPoint, VSCode, and Adobe Acrobat. In addition, to address the challenges of dynamic GUI automation tasks, we propose GUI-Thinker, holistic framework, leveraging critique mechanism, that effectively manages the unpredictability and complexity of GUI interactions. Experimental results demonstrate that GUI-Thinker significantly outperforms Claude3.5 (Computer Use) by 14.9% in success rate on WorldGUI tasks. This improvement underscores the effectiveness of our critical-thinkingbased framework in enhancing GUI automation. 1. Introduction Graphical User Interface (GUI) automation has become prominent research area, driven by the need to enhance user productivity. This domain encompasses software usage, file management, office design, coding, and web browsing. Building upon Multimodal Large Language Models (MLLMs) such as GPT-4o (OpenAI, 2023) and Claude-3.5 (Anthropic, 2024), GUI agents have the potential to solve various computer tasks to avoid repetitive work or as an AI assistant to help the user. 1Show Lab, National University of Singapore. Correspondence to: Mike Zheng Shou <mike.shou@nus.edu>. Figure 1. The comparison of static and dynamic testing processes. Our WorldGUI takes the first step to facilitate comprehensive GUI evaluation with various initial states. The red node represents an incorrect state. GUI automation operates in dynamic environment, which goes beyond the traditional computer vision tasks like image recognition (He et al., 2016) and visual question answering (Antol et al., 2015). However, current GUI benchmarks such as OSWorld (Xie et al., 2024) and WebArena (Zhou et al.) do not capture this dynamism. As shown on the left side of Fig. 1, most GUI benchmarks focus on initial and final states, measuring success rates but overlooking the changing initial conditions present in real GUI scenarios. These benchmarks often ignore situations where: (1) The software interface is not in its default state. (2) The agent might get user queries at any time. (3) Differences in agent robustness, where agents with the same low success rate (e.g. 20%) may vary in their ability to self-verify or self-correct, but these abilities are not measured in static setting. As result, these benchmarks fail to fully assess the capabilities of GUI agents. In this paper, we take the first step toward comprehensive GUI agent testing by designing GUI tasks with various initial states. As illustrated on the right side of Fig. 1, the testing process of WorldGUI can be featured: (1) Intermediate Starting States: Real user interactions with GUI assistants do not always begin from default conditions, allowing tasks to start from intermediate states where users may seek assistance at any point (see Fig. 1 (b)). (2) Contextual Variability: In some cases, tasks may originate from entirely different contexts or interfaces, requiring the agent to adapt by modifying existing steps (see Fig. 1 (c)) or intro1 WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation ducing new steps (see Fig. 1 (d)) to ensure task progression. By incorporating these elements, WorldGUI better mirrors real-world GUI environments, enabling more accurate and thorough assessment of GUI agent capabilities. Specifically, we present WorldGUI, new benchmark featuring 10 desktop software applications and 315 tasks, including PowerPoint, Word, Excel, VSCode, etc. For each task, we provide user query, an instructional video, and the corresponding project file. To ensure the task quality, we engaged four trained annotators skilled in using these applications and proficient in constructing data with scripts and agents for annotation. To stimulate the dynamic testing scenarios, we demonstrate each task to obtain ground-truth (GT) plans and then conduct the augmentations for each task using pre-actions. (See details in Sec. 3.) In addition, we introduce novel GUI agent framework, GUI-Thinker, which builds upon critical thinking philosophy, an aspect less emphasized in previous GUI agents (Hong et al., 2024; Cheng et al., 2024; Lai et al., 2024; Agashe et al., 2024; Wu et al., 2024). In dynamic GUI environments, application settings may not be in default configurations. This unpredictability requires agents to have the essential ability to detect and adapt to such changes to ensure task accuracy. Through our analysis of real-world GUI scenarios, we identify three critical designs for comprehensive agent: (1) Post-Planning Critique, (2) PreExecution Validation, and (3) Post-Action Evaluation. Specifically, GUI-Thinker comprises five core components: Planner, Planner-Critic, Step-Check, Actor, and Actor-Critic. We argue that these components are fundamental for effective GUI agents. To summarize, our key contributions are the following: (1) We are the first to stress the dynamic testing processes in GUI automation and propose new benchmark WorldGUI which designs the GUI tasks with various initial states to simulate the real interactions; (2) We introduce GUIThinker, comprehensive GUI framework. GUI-Thinker incorporate the thinking into the overall agent design, which provides valuable insights and guidance for future development; (3) We explore the essential property of critical thinking in GUI agents and empirically show that critical thinking is extremely useful for handling complex tasks. 2. Related Work 2.1. GUI Benchmarks GUI benchmarks are essential for evaluating the performance and robustness of GUI agents. For web applications, WebShop (Yao et al., 2022), and WebArena (Zhou et al.) are two text-based GUI benchmarks, the GUI information is formatted in the text style which is limited to reflect the dynamic GUI state changes. In OS environments, OSWorld (Xie et al., 2024) is comprehensive benchmark including various operating systems with real applications. Mobile benchmarks MobileAgent (Wang et al., 2024) and AppAgent (Zhang et al., 2023) propose two GUI benchmarks of mobile applications. Windows-related benchmarks like AssistGUI (Gao et al., 2024) and WindowAgentArena (Bonatti et al., 2024) propose list of real tasks in the Windows platform. However, these benchmarks primarily rely on static testing process and do not adequately capture the complexity and dynamic nature of GUI environments. As result, they are insufficient for comprehensively evaluating GUI agents. We summarize the differences between WorldGUI and other related benchmarks in Tab. 5. 2.2. GUI Agents Building upon MLLMs, GUI agents have the potential to solve various computer tasks to avoid repetitive work or as an AI assistant to help the user. CogAgent (Hong et al., 2024) is vision language model focused on GUI understanding to facilitate GUI navigation, while SeeClick (Cheng et al., 2024) and SeeAct (Zheng et al., 2024) focus on the GUI grounding for enhancing the task performance. MobileAgent (Wang et al., 2024) and AppAgent (Zhang et al., 2023) are proposed to design the agent on mobile device. Ferret-UI (You et al., 2025) is another representative work focusing on enhancing the grounding ability in the IOS platform. AutoGLM (Liu et al., 2024) propose the GUI modes capable of learning through autonomous environmental interactions by reinforcing existing GUI models. These agents have shown their ability in GUI understanding (e.g., GUI elements grounding) or action prediction but still face limitations in handling dynamic and complicated full GUI tasks. Therefore, to enhance GUI automation in dynamic environments, we propose GUI-Thinker that improves adaptability in complex GUI settings and enables agents to effectively handle unpredictable interface changes. 2.3. Critical Thinking in Agents Recent advancements in foundation models and agents, particularly in LLMs such as OpenAI-o1 (OpenAI, 2024) and Deepseek-R1 (DeepSeek-AI et al., 2025), have increasingly incorporated thinking processes before providing answers to effectively handle challenging reasoning tasks. The LLMbased agents utilize verify-then-correct process to evaluate and refine intermediate reasoning steps or outputs, ensuring logical coherence and consistency. One notable LLM-based agent framework, Reflexion (Shinn et al., 2024), demonstrates the effectiveness of self-reflection in solving complex tasks. Furthermore, Critic (Gou et al., 2023) integrates external tools into the critique process, leveraging them to improve performance. Noticing the GUI task is lengthy and complicated, the verify-then-correct process is highly suitable for the GUI scenario. Which is not only aims to 2 WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation Figure 2. WorldGUI: An illustration of our proposed real-world GUI benchmark. The left shows that for each task, WorldGUI provides user query, instructional video, and pre-actions. The pre-actions lead to different initial states. The key characteristic of our WorldGUI is the various initial states of the same task to stimulate the real-world testing process. The right shows the software included in our benchmark and the interactions about testing the agents in our GUI environment. enhance the reasoning performance but also indispensable to designing the key module Actor-Critic (Konda & Tsitsiklis, 1999) to ensure task completion. closely related work, AssistGUI (Gao et al., 2024), integrates critical module only after the Actor module to evaluate action completion. Building upon it, we introduce two additional critical modules: Planner-Critic, applied after the Planner, and StepCheck, applied before the Actor. These two modules lead to comprehensive and fundamental GUI agent framework GUI-Thinker which will provide insights for future GUI agent design. 3. WorldGUI Benchmark 3.1. Task Formulation GUI Automation Definition. The GUI automation task can be considered partially observable Markov decision process (POMDP) (S, O, A, , R) with state space S, observation O, action space A, transition function : S, and reward function R: R. In our setting, given natural language query q, eg., Format the slide background with gradient fill that describes specific task in high-level, along with an instructional video as supplement that more detailed illustrates how to complete it, the agent first get the observation ot from the state st in the execution environment and then generate the executable action at A, resulting in new state st+1 and new observation ot+1 O. The process repeats until the task is finished or failed. The reward function R: [0, 1] here returns binary integer at the final step indicating the task completion status. WorldGUI Task Definition. As illustrated in Fig. 2, each GUI task is paired with user query and an instructional video. To achieve state diversity within each task, we generate various initial states that converge to the same final state, resulting in distinct ground truth (GT) plans for each case. This is accomplished through the use of pre-actions, which consist of sequence of executable code to initialize tasks from different initial states. With the augmentation of initial states, WorldGUI is capable of mimicking the different testing scenarios as shown in Fig. 1. Observation Space. The observation space indicates the information of the operating system (OS) available to the agent in each state st. In our WorldGUI settings, we follow the previous work AssistGUI (Gao et al., 2024), encompassing two types of information: metadata mt from the application and screenshot Vt of current state st. The metadata mainly includes the layout of panels and pop-up windows. The screenshot Vt offers holistic visual information of the current state used for planning and action generation. Action Space. Our action space includes all raw mouse and keyboard actions, such as left-click, right-click, doubleclick, drag, keystrokes, and key combinations for shortcuts, among others. Mouse-related actions also specify the target position in the pixel space of the observed screenshot. To ensure universal and comprehensive representation of actions, we adopted the widely used Python library, PyAutoGUI, for controlling mouse and keyboard inputs. Each action is represented using the syntax action type(arguments) as in Tab. 6. 3.2. Data Collection 3.2.1. DATA SOURCE WorldGUI consists of broad spectrum of desktop applications, which can be categorized into five main groups: (i) Office work, includes PowerPoint, Word, Excel, and Adobe Acrobat; (ii) Windows Usage, includes System Settings and File Management; (iii) Web Browsing, includes the configuration of Youtube and website operations; (iv) Coding, focus on the customization, configuration and editing of Visual Studio Code (VSCode); (v) Media Creation, uses the AI tool like Stable Diffusion to create or edit the image or videos by following user instruction. 3 WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation Figure 3. GUI-Thinker. An overview of GUI-Thinker, includes five proposed components: Planner, Planner-Critic, Step-Check, Actor, and Actor-Critic. The Planner module receives the user query and an instructional video as input and generates an initial plan for the Planner-Critic process. This plan is then refined and executed step by step. Before each step is passed to the Actor module, it undergoes Step-Check. After the Actor produces an action, the Actor-Critic module iteratively verifies the completion of the action and makes corrections if needed. 3.2.2. PIPELINE OF DATA CONSTRUCTION For each task 1) we manually collect the raw videos from the website, 2) then manually cut the raw lengthy videos into sub-clips, 3) ask annotators to manually write the user queries, 4) prepare the project files for each task for reproducibility, 5) generate the ground-truth plans by executing the scripts and agent, 6) conduct the data augmentation by varying the initial state for each task. As shown in Fig. 11, to achieve the above six steps, we invite four annotators and write the necessary scripts to help structure and format the data. Additionally, for the GT plan generation and pre-action generation, we build simple agents to obtain the data. Raw Video Collection. We collect raw videos from the YouTube website as there are lot of high-quality tutorials for desktop applications. For each software, we ask the annotators to watch the videos and then download them using different software. Instruction Video Preparation. After obtaining the raw videos, we write the script codes to cut the lengthy and noisy videos into the sub-clips (30 seconds to 3 minutes) that serve as the instructional video. User Query Generation. After obtaining the instructional videos, annotators are asked to manually write user queries corresponding to each video. For example, user query for task involving File Explorer might be: Rename all the files by deleting class from their names. Project File Preparation. Following the AssistGUI (Gao et al., 2024), we create the project file for each task to ensure reproducibility without relying on resource-intensive virtual machines (Xie et al., 2024) or Docker environments (Bonatti et al., 2024). This approach guarantees that the testing process begins from consistent state. When combined with pre-actions, it enables augmentation of the same task with various initial states. GT Plan Generation. We write the script to accept user query and instructional video as input and generate the raw plans by agent (powered by GPT-4o). Since the raw plans are not flawless, annotators are asked to watch the videos and manually execute the tasks following the raw plans. During this process, annotators edit the plans to correct any inaccurate steps or descriptions, ultimately producing the finalized GT plans. Pre-Actions Generation. To vary the task, we propose introducing pre-actions before the task begins. These preactions are created by annotators and involve corresponding scripts and agents. They are written in Python code, for example: from pyautogui import click, rightClickn rightClick(800,400). The pre-actions primarily serve two purposes: 1) Simulating Intermediate Task States: Pre-actions can complete specific steps of task, creating starting point from an intermediate state. This approach addresses scenarios where users may seek AI assistance because they are unable to complete task. For example, if the task involves opening dropdown menu, the pre-action may pre-open the menu. If the agent fails to recognize this precondition and follows its plan to click the menu again, it might inadvertently close the menu, causing task failure. 2) Introducing Diverse Initial Context States: Pre-actions can also introduce variations in the initial state, such as opening random tabs or settings. This ensures that the starting state is unconventional, 4 WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation challenging the agent to adapt by modifying its plan or adding new steps. See details in Fig. 8. 3.3. Evaluation WorldGUI employs an execution-oriented evaluation approach by utilizing post-processing scripts to assess task completion. Specifically, for tasks like Office work and Web Browsing, we adopt exact matching to compare the differences between the ground-truth (GT) screenshots and the final screenshots. For tasks like File Management which would produce new folders or change the locations of files, etc. We create the shell script to check the status of files. 3.4. Data Statistics WorldGUI compiles GUI tasks from 10 widely used applications on the Windows platform, including productivity software such as PowerPoint, Excel, and VSCode. total of 107 meta tasks were collected from these applications, with each task being augmented 0 to 3 times based on its specific content, resulting in 208 augmented tasks. In total, WorldGUI comprises 315 tasks. See the details in Sec. C. 4. GUI-Thinker: Thinking before Doing In this section, we introduce new comprehensive GUI framework GUI-Thinker with core and essential mechanism: critical thinking, which is vital for designing GUI agents capable of handling dynamic environments that have been overlooked in prior GUI agents (Hong et al., 2024; Cheng et al., 2024; Lin et al., 2024b; Zhang et al., 2023; Agashe et al., 2024). The GUI-Thinker includes the five fundamental but essential components as in Fig. 3 and an Interaction reasoning loop detailed in Algorithm 1. We summarize our design principles in the following: Post-Planning Critique: After the planning phase, critique module verifies and, if necessary, self-corrects the generated plans to ensure their accuracy. Pre-Execution Validation: Before executing each subtask, validation module determines whether the subtask should be executed. This step is crucial, as the current GUI environment may indicate that the subtask is unnecessary or requires modification to align with the current state conditions. Post-Action Evaluation: After each action execution, mechanism evaluates whether the action was successfully completed before proceeding to the next subtask. These critique mechanisms ensure the reliability and adaptability of GUI-Thinker in complex GUI environments. Figure 4. State-Aware Planner and Planner-Critic. The Planner generates an initial plan. Then, the Planner-Critic will assess the correctness of the plan and provide necessary corrections. 4.1. State-Aware Planner 1, Si 2, Si The State-Aware Planner processes the instructional video and user query generates an initial plan as shown in the left of Fig. 4. We use the speech recognition model Whisper (Radford et al., 2023) to translate the video into the subtitle and then send it to the MLLM for task planning. The task plan is hierarchically structured as = [p1, p2, ..., pN ] where pi is text string describing the i-th milestone of the task. Under each pi, there is list of subtasks [Si ], where Si is the j-th subtask in the i-th milestone. To ensure the produced plans fit the GUI environment, we propose incorporating an initial screenshot V0 to represent the current state. This additional context allows the agent to output plans that align with the actual state. For example, if the instructional video suggests clicking on the Layout tab in the Word application, but the current state (as indicated by the screenshot) shows that the Layout tab is already selected, there is no need to perform this action again. By utilizing the visual information from the screenshot, the StateAware Planner can modify the plans accordingly, rather than strictly following the guidance in the instructional video or the existing knowledge from backbone MLLMs. 4.2. Planner-Critic Post-Planning Critique. The goal of the Planner-Critic is to assess the correctness of the initial plans generated by the State-Aware Planner and provide corrections if needed. This module is designed to ensure the accuracy of the plans while leveraging the self-critique capabilities of MLLMs. As illustrated in Fig. 4, for each Initial Plan, the output consists of four components: (1) <Flag>: Indicates whether the Initial Plan is correct. 5 WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation Figure 5. Step-Check. This module first checks the step completion status via an MLLM, and then based on the output navigates current task processing. (2) <Feedback>: Identifies the error type, categorized into one of three groups: Wrong Steps, Missing Steps, or Redundant Steps. (3) <Correction>: Provide the corrected plans if the Flag indicates that the Initial Plan is incorrect. (4) <Reason>: In addition to giving the corrected plans, we force the model to give the reasons. As related works, CoT (Wei et al., 2022), GPT-o1 (OpenAI, 2024), and Deepseek-R1 (DeepSeek-AI et al., 2025) demonstrate that generating reasoning steps along with the answer would enhance the performance. 4.3. Step-Check Pre-Execution Validation. After the plan assessment, navigation mechanism is crucial before sending each subtask St = Si at the time step to the Actor module. To address this, we designed new module called Step-Check. Through extensive investigation, we discovered that during GUI task testing, perfect execution plans are rarely feasible due to the unpredictable nature of real application environments. Most software retains user preferences (e.g., remember the last configuration of user), meaning that when executing specific task, the plan generated by the Planner might not align with the actual state of the software. Therefore, the model must determine whether to proceed with subtask St based on the current state (screenshot: Vt, metadata: Mt). As illustrated in Fig. 5, we employ an MLLM to determine whether the current task has been completed or requires modification. We systematically categorize the possible outcomes into four types: Figure 6. Actor-Critic. This module includes two parts: task verification and task correction. The design follows the verify-thencorrect mechanism. We set an internal loop in this module to iteratively correct the actions. (1) <Modify>: Indicates that the subtask should be modified or additional subtasks should be added. (2) <Pass>: Indicates that the current subtask is unnecessary and can be skipped. (3) <Continue>: Indicates that the subtask is valid and should be executed as planned. (4) <Finished>: Indicates that the subtask has already been completed and requires no further action. In cases where the screenshot does not provide sufficient visual information for the MLLM to determine the output, the model outputs #Cannot confirm. When this occurs, we design Region Search module implemented by an LLM. This module takes the corresponding GUI information extracted by the GUI parser and the task description of the current subtask to identify the relevant region. It then crops the region using the generated bounding box as the center coordinate, with the maximum width and height set to half of the original screenshot dimensions (ensure the region is smaller than the origin screenshot). The cropped screenshot is subsequently sent to the Step-Check module to regenerate the decision. 4.4. Actor The goal of the Actor is to translate natural language subtask St into executable code Ct. Using an MLLM as the backbone model, the Actor processes metadata mt and screen6 WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation Method MLLM AssistGUI Computer Use Claude-3.5 GPT-4o GUI-Thinker GUI-Thinker Human GPT-4o Claude-3.5 Office Web Meta Aug. Meta Aug. Meta Aug. Meta Aug. Meta Aug. Win. Usage Coding Media Overall 26.7 28. 44.4 57.8 26.4 22.0 31.9 41.8 34.8 30.4 34.8 43.5 21.4 23. 21.4 23.8 33.3 71.4 47.6 76.2 20.5 46.2 38.5 48.7 36.4 54. 45.5 63.6 27.3 31.8 27.3 40.9 42.9 42.9 57.1 57.1 21.4 21. 50.0 57.1 26.7 32.4 36.2 47.3 97.6 Table 1. Success rate (%) of different agents on WorldGUI. Human denotes the average performance of four expert participants who have watched the instructional video only once, similar to the model. Meta represents the meta task, while Aug. represents the augmented task. 100.0 95. 82.1 81.8 81.8 71.4 71.4 87. 88.9 85.7 Method Office Web Meta Aug. Meta Aug. Meta Aug. Meta Aug. Meta Aug. Win. Usage Coding Media Overall Full Model w/o Planner-Critic w/o Step-Check w/o Actor-Critic 44.4 31.1 31.1 15. 31.9 26.4 30.8 11.0 34.8 21.7 21.7 4.3 21.4 19.0 19.0 4.8 47.6 38.1 33.3 28.6 38.5 30.8 35.9 23.1 45.5 36.4 45.5 0. 27.3 22.7 22.7 4.5 57.1 42.9 28.6 28.6 50.0 35.7 28.6 28.6 36.2 27.9 29.2 13.3 Table 2. Success rate (%) of our GUI-Thinker (Full Model) with the ablation of different critical modules. shot Vt as GUI context to generate precise executable actions, such as click(100, 200). Additionally, it leverages the history of previous actions as memory to aid in generating subsequent actions. The generated actions will executed in the environment, and then the new screenshot Vt+1 and metadata mt+1 will be captured for the next processing. 4.5. Actor-Critic Post-Action Evaluation. After generating an action, the Actor-Critic module evaluates subtask St1 completion and makes corrections if necessary. As illustrated in Fig. 6, in the first step, the module implemented by an MLLM compares screenshots Vt1 (before action execution) and Vt (after execution) while processing each subtask St to determine the action correctness. The model outputs <Success> flag to indicate task completion. If the <Success> flag is true, the current state st = <Next>. If the <Success> flag is false (set st = <Critic>) and the number of trial steps is below the maximum limit, the Actor-Critic module activates the Locate GUI Elements and Actor Correction processes. We introduce the module Locate GUI Elements to identify the relevant GUI elements and regenerate actions using the Actor Correction module. The corrected actions are then executed in the environment, generating updated observations (Ot) that include new screenshots and metadata for the continued Actor-Critic iteration. The process repeats until the <Success> flag is true or the maximum number of trials is reached. 5. Experiments Implementation Details. We implement the MLLM in our GUI-Thinker by using GPT-4o (OpenAI, 2023) (gpt4o-2024-08-06) by default. For the computer mouse and keyboard control, we use the Python library PyAutoGUI. Following the AssistGUI (Gao et al., 2024), we use the GUI parser to obtain the position information of elements, e.g., buttons, icons, and text. We use some vision foundation models such as Google OCR to extract the text. By default, we use the center coordinates to represent the location of each element. All the testing is under the same screenshot resolution (1920 1080). In all experiments, we set the max trials of the Actor-Critic to 3 for light interaction costs. For the total trials of each task, we set it to 2 + 1, where is the length of subtasks Si in plan p. In most cases, we set the trials to 21. Evaluation. Given that our WorldGUI includes 315 GUI tasks, we engaged four participants with strong coding and software backgrounds to test all tasks and document their evaluation results. Metric. Following the previous works of OSworld and AssistGUI, we use Success Rate (SR) as the metric. Baselines. We compare our GUI-Thinker with two strong approaches: AssistGUI (Gao et al., 2024) and Cluade 3.5 (Computer Use) (Anthropic, 2024). AssistGUI is prominent agent framework designed for Desktop GUI Automation, which can plan the task and then execute the task step by step by following the query. We implement it by increasing the MLLM to GPT-4o for better performance. Cluade 3.5 (Computer Use) is the strongest MLLM model specially designed for autonomous computer use. We use the open-source implementation OOTB (Hu et al., 2024) as the codebase and then add the subtitle of instructional videos into the input prompt for fair comparison. 5.1. Main Results on WorldGUI As shown in Tab. 1, our GUI-Thinker (Claude-3.5) outperforms previous agents, achieving the highest success rate. Compared to AssistGUI (GPT-4o), GUI-Thinker (GPT7 WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation MLLM PPT Excel Meta Aug. Meta Aug. Meta Aug. Meta Aug. Meta Aug. VSCode Acrobat Word Overall Claude-3.5 GPT-4o Gemini-2.0-Flash 54.5 45.5 36.4 47.8 39.1 34.8 54.5 36.4 27.3 45.0 25.0 20. 58.3 50.0 41.7 30.4 34.8 34.8 63.6 54.5 54.5 40.9 27.2 36.3 63.6 45.5 18.2 44.0 32.0 24. 47.3 36.1 32.0 Table 3. Success rate (%) of our GUI-Thinker with the ablation on MLLM. Method PPT Word Excel Overall Full Model w/o Inst. Video 41.2 35. 29.0 22.6 40.0 22.2 37.0 29.0 Table 4. Success rate (%) of our GUI-Thinker with the ablation of Instructional Video (Inst. Video). 4o) demonstrates superior performance in Office, Web, and Coding GUI tasks. This demonstrates that the critical modules are essential for effectively handling complex tasks. However, the overall success rate (SR) remains low, indicating that GUI tasks in WorldGUI are still challenging, as reflected in the Human SR of 87.6%. When comparing the Meta and augmented (Aug.) tasks, we observe that all agents perform better on the Meta tasks but struggle with the Aug. tasks, suggesting that current agents are not yet effective in handling tasks with varying states. Analyzing the results of Computer Use (Claude-3.5), we find that it excels in Web and Coding GUI tasks, achieving 71.4% SR on Web tasks and 54.5% SR on Coding tasks, likely due to its design being optimized for computer usage. These results significantly surpass those of GUI-Thinker (GPT-4o), which lacks the corresponding GUI automation capabilities. Additionally, by equipping GUI-Thinker with Claude-3.5 as the base model, our agent achieves comparable performance in Web and Coding tasks. For the Office task, Computer Use (Claude-3.5) struggles, likely due to the dense domainspecific icons and buttons in these applications, which pose challenges for perception. By incorporating Claude-3.5 as the MLLM in our GUI-Thinker, GUI-Thinker (Claude-3.5) achieves twice the SR (57.8% vs. 28.9%) on the Office task (Meta), demonstrating the effectiveness of our agent framework. 5.2. Ablation Study of GUI-Thinker Impact of Critical Modules. In Tab. 2, we investigate the effectiveness of our proposed critical modules: PlannerCritic, Step-Check, and Actor-Critic. We use GUI-Thinker (GPT-4o) as the Full Model to reduce financial costs. The results show that removing any of these modules leads to decline in performance across all task groups, highlighting their importance. Planner-Critic plays crucial role in plan correction, as its removal leads to relative performance drop across most tasks, particularly in Office (44.4 31.1), Web (47.6 38.1), and Coding (45.5 36.4). This indicates that effective plan refinement is essential for successfully completing GUI tasks. Step-Check is particularly important for Web and Media tasks, where removing it results in notable decline (Web: 47.6 33.3, Media: 57.1 28.6). This suggests that some web-based tasks require additional step modifications to improve execution accuracy. Actor-Critic is the most critical module, as its removal results in the sharpest performance drop across all task groups (Overall SR: 36.2 13.3). We observe that many subtasks are difficult to execute correctly on the first attempt and heavily rely on the Actor-Critic for additional action correction. The most drastic declines are seen in Coding (45.5 0.0), Windows Usage (34.8 4.3), and Office (44.4 15.6), indicating that these tasks are more rely on the ActorCritic module. Overall, these results demonstrate that all three modules contribute significantly to the performance, with Actor-Critic being the most impactful. Impact of Different MLLM. We also investigate the impact of using different MLLMs in our framework. As shown in Tab. 3, we compare three MLLMs across five popular applications. Claude-3.5 consistently achieves the highest performance on all five applications, particularly excelling in VSCode and Acrobat tasks, where it attains an SR of over 60%. GPT-4o demonstrates stable performance across these applications, while Gemini-2.0 underperforms compared to the other two MLLMs. Impact of Instructional Video. In Tab. 4, we study the impact of removing the instructional video by modifying the prompt to include only the user query for generating the initial plan. On the three Office applications, we observe significant performance decline, as these tasks rely more heavily on additional domain knowledge for successful planning. In contrast, the MLLM performs relatively well on Win. Usage tasks, such as Settings and File Management, where it has more inherent familiarity."
        },
        {
            "title": "Conclusion",
            "content": "In this paper, we take the first step toward comprehensive GUI agent evaluation by introducing new benchmark, WorldGUI. In addition to the standard static testing processes, we incorporate dynamic testing procedures to ensure that WorldGUI effectively captures the complexity and dynamism of real-world GUI environments. Furthermore, to enhance GUI automation, we propose novel agent framework, GUI-Thinker, built upon critical thinking philosophy and comprising five core components. This framework en8 WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation ables the agent to dynamically identify uncommon states and adjust its plans or actions accordingly. Finally, we evaluate the latest computer-using agent, Claude-3.5, using our WorldGUI benchmark, demonstrating the effectiveness of GUI-Thinker across variety of GUI tasks."
        },
        {
            "title": "Impact Statement",
            "content": "This paper introduces WorldGUI and GUI-Thinker, benchmark and agent framework designed to advance the field of GUI automation. Our work has several important societal and research implications. Firstly, WorldGUI represents significant step toward comprehensive GUI agent evaluation by incorporating diverse initial states for each GUI task. This approach better reflects real-world conditions, inspiring future GUI-related benchmarks to emphasize realistic GUI testing scenarios in their designs. Secondly, we propose holistic GUI agent framework, GUI-Thinker, which integrates five essential components and an iterative reasoning loop. By embedding critical thinking into its module design, GUI-Thinker enhances adaptability in complex and dynamic GUI environments, allowing agents to handle unpredictable interface changes more effectively. Our work lays strong foundation for future research in adaptive, reasoning-driven GUI automation, with potential applications in software testing, accessibility tools, and intelligent user assistance systems."
        },
        {
            "title": "References",
            "content": "Agashe, S., Han, J., Gan, S., Yang, J., Li, A., and Wang, X. E. Agent s: An open agentic framework that uses computers like human, 2024. URL https://arxiv. org/abs/2410.08164. Anthropic. Introducing computer use, new claude 3.5 sonnet, and claude 3.5 haiku, 2024. https://www.anthropic.com/news/ URL 3-5-models-and-computer-use. Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C. L., and Parikh, D. Vqa: Visual question In Proceedings of the IEEE international answering. conference on computer vision, pp. 24252433, 2015. Bonatti, R., Zhao, D., Bonacci, F., Dupont, D., Abdali, S., Li, Y., Lu, Y., Wagle, J., Koishida, K., Bucker, A., Jang, L., and Hui, Z. Windows agent arena: Evaluating multi-modal os agents at scale, 2024. URL https: //arxiv.org/abs/2409.08264. Cheng, K., Sun, Q., Chu, Y., Xu, F., Li, Y., Zhang, J., and Wu, Z. Seeclick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint arXiv:2401.10935, 2024. DeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., 9 Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y., Wu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao, Z., Liu, A., Xue, B., Wang, B., Wu, B., Feng, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H., Ding, H., Xin, H., Gao, H., Qu, H., Li, H., Guo, J., Li, J., Wang, J., Chen, J., Yuan, J., Qiu, J., Li, J., Cai, J. L., Ni, J., Liang, J., Chen, J., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Zhao, L., Wang, L., Zhang, L., Xu, L., Xia, L., Zhang, M., Zhang, M., Tang, M., Li, M., Wang, M., Li, M., Tian, N., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q., Ge, R., Zhang, R., Pan, R., Wang, R., Chen, R. J., Jin, R. L., Chen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang, S., Yu, S., Zhou, S., Pan, S., Li, S. S., Zhou, S., Wu, S., Ye, S., Yun, T., Pei, T., Sun, T., Wang, T., Zeng, W., Zhao, W., Liu, W., Liang, W., Gao, W., Yu, W., Zhang, W., Xiao, W. L., An, W., Liu, X., Wang, X., Chen, X., Nie, X., Cheng, X., Liu, X., Xie, X., Liu, X., Yang, X., Li, X., Su, X., Lin, X., Li, X. Q., Jin, X., Shen, X., Chen, X., Sun, X., Wang, X., Song, X., Zhou, X., Wang, X., Shan, X., Li, Y. K., Wang, Y. Q., Wei, Y. X., Zhang, Y., Xu, Y., Li, Y., Zhao, Y., Sun, Y., Wang, Y., Yu, Y., Zhang, Y., Shi, Y., Xiong, Y., He, Y., Piao, Y., Wang, Y., Tan, Y., Ma, Y., Liu, Y., Guo, Y., Ou, Y., Wang, Y., Gong, Y., Zou, Y., He, Y., Xiong, Y., Luo, Y., You, Y., Liu, Y., Zhou, Y., Zhu, Y. X., Xu, Y., Huang, Y., Li, Y., Zheng, Y., Zhu, Y., Ma, Y., Tang, Y., Zha, Y., Yan, Y., Ren, Z. Z., Ren, Z., Sha, Z., Fu, Z., Xu, Z., Xie, Z., Zhang, Z., Hao, Z., Ma, Z., Yan, Z., Wu, Z., Gu, Z., Zhu, Z., Liu, Z., Li, Z., Xie, Z., Song, Z., Pan, Z., Huang, Z., Xu, Z., Zhang, Z., and Zhang, Z. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Gao, D., Ji, L., Bai, Z., Ouyang, M., Li, P., Mao, D., Wu, Q., Zhang, W., Wang, P., Guo, X., Wang, H., Zhou, L., and Shou, M. Z. Assistgui: Task-oriented pc graphical user interface automation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1328913298, June 2024. Gou, Z., Shao, Z., Gong, Y., Shen, Y., Yang, Y., Duan, N., and Chen, W. Critic: Large language models can selfcorrect with tool-interactive critiquing. arXiv preprint arXiv:2305.11738, 2023. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770778, 2016. Hong, W., Wang, W., Lv, Q., Xu, J., Yu, W., Ji, J., Wang, Y., Wang, Z., Dong, Y., Ding, M., et al. Cogagent: visual language model for gui agents. In Proceedings of the WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation Wang, J., Xu, H., Ye, J., Yan, M., Shen, W., Zhang, J., Huang, F., and Sang, J. Mobile-agent: Autonomous multi-modal mobile device agent with visual perception. arXiv preprint arXiv:2401.16158, 2024. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Wu, Z., Wu, Z., Xu, F., Wang, Y., Sun, Q., Jia, C., Cheng, K., Ding, Z., Chen, L., Liang, P. P., and Qiao, Y. Os-atlas: foundation action model for generalist gui agents, 2024. URL https://arxiv.org/abs/2410.23218. Xie, T., Zhang, D., Chen, J., Li, X., Zhao, S., Cao, R., Hua, T. J., Cheng, Z., Shin, D., Lei, F., Liu, Y., Xu, Y., Zhou, S., Savarese, S., Xiong, C., Zhong, V., and Yu, T. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments, 2024. Yao, S., Chen, H., Yang, J., and Narasimhan, K. Webshop: Towards scalable real-world web interaction with grounded language agents. Advances in neural information processing systems, 2022. You, K., Zhang, H., Schoop, E., Weers, F., Swearngin, A., Nichols, J., Yang, Y., and Gan, Z. Ferret-ui: Grounded mobile ui understanding with multimodal llms. In European Conference on Computer Vision, pp. 240255. Springer, 2025. Zhang, C., Yang, Z., Liu, J., Han, Y., Chen, X., Huang, Z., Fu, B., and Yu, G. Appagent: Multimodal agents as smartphone users. arXiv preprint arXiv:2312.13771, 2023. Zheng, B., Gou, B., Kil, J., Sun, H., and Su, Y. Gpt4v(ision) is generalist web agent, if grounded. In Fortyfirst International Conference on Machine Learning, 2024. URL https://openreview.net/forum? id=piecKJ2DlB. Zhou, S., Xu, F. F., Zhu, H., Zhou, X., Lo, R., Sridhar, A., Cheng, X., Ou, T., Bisk, Y., Fried, D., et al. Webarena: realistic web environment for building autonomous agents. In The Twelfth International Conference on Learning Representations. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1428114290, 2024. Hu, S., Ouyang, M., Gao, D., and Shou, M. Z. The dawn of gui agent: preliminary case study with claude 3.5 computer use, 2024. URL https://arxiv.org/ abs/2411.10323. Koh, J. Y., Lo, R., Jang, L., Duvvur, V., Lim, M. C., Huang, P.-Y., Neubig, G., Zhou, S., Salakhutdinov, R., and Fried, D. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. arXiv preprint arXiv:2401.13649, 2024. Konda, V. and Tsitsiklis, J. Actor-critic algorithms. Advances in neural information processing systems, 12, 1999. Lai, H., Liu, X., Iong, I. L., Yao, S., Chen, Y., Shen, P., Yu, H., Zhang, H., Zhang, X., Dong, Y., and Tang, J. Autowebglm: large language model-based web navigating agent, 2024. URL https://arxiv.org/abs/ 2404.03648. Lin, K. Q., Li, L., Gao, D., Wu, Q., Yan, M., Yang, Z., Wang, L., and Shou, M. Z. Videogui: benchmark for gui automation from instructional videos. arXiv preprint arXiv:2406.10227, 2024a. Lin, K. Q., Li, L., Gao, D., Yang, Z., Wu, S., Bai, Z., Lei, W., Wang, L., and Shou, M. Z. Showui: One vision-language-action model for gui visual agent. arXiv preprint arXiv:2411.17465, 2024b. Liu, X., Qin, B., Liang, D., Dong, G., Lai, H., Zhang, H., Zhao, H., Iong, I. L., Sun, J., Wang, J., Gao, J., Shan, J., Liu, K., Zhang, S., Yao, S., Cheng, S., Yao, W., Zhao, W., Liu, X., Liu, X., Chen, X., Yang, X., Yang, Y., Xu, Y., Yang, Y., Wang, Y., Xu, Y., Qi, Z., Dong, Y., and Tang, J. Autoglm: Autonomous foundation agents for guis, 2024. URL https://arxiv.org/abs/2411.00820. OpenAI. Gpt-4o, 2023. URL https://openai.com/ index/hello-gpt-4o. Openai OpenAI. URL openai-o1-system-card. o1 2024. https://openai.com/index/ system card, Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., and Sutskever, I. Robust speech recognition via largescale weak supervision. In International conference on machine learning, pp. 2849228518. PMLR, 2023. Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K., and Yao, S. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024. WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation A. Comparison with other benchmarks In this section, we summarize the data statistics of other benchmarks in Tab. 5 as the comparison. We can observe that the main difference of our WorldGUI is we incorporate various initial states to stimulate the real GUI automation while other benchmarks are not considered. Benchmark Softwares Tasks Platform Task Focus Inst. Video GT Plan Various States WebArena (Zhou et al.) VisualWebArena (Koh et al., 2024) Mobile-Eval (Wang et al., 2024) APPAgent (Zhang et al., 2023) OSWorld (Xie et al., 2024) AssistGUI (Gao et al., 2024) VideoGUI (Lin et al., 2024a) WindowAgentArena (Bonatti et al., 2024) WorldGUI 6 3 10 10 10 9 10 11 Web 812 Web 910 Android OS 30 50 Android OS 369 Ubuntu+Win. Website Usage Website Usage APP Usage APP Usage Software Usage 100 Windows Productivity Software Usage 86 Win. + Web Productivity Software Usage 154 Windows 315 Win. + Web Productivity Software Usage SoftWare Usage Table 5. Comparison with related benmarks. WorldGUI is unique benchmark that has the various states for each task to stimulate the real-world agent-computer interactions. Why Instructional Video? The rationale for using instructional videos is that, in some cases, completing task requires extensive configuration, making it impractical to provide all necessary information within the input query. As illustrated in Fig. 7, our user query is Generate photo of girl with short brown hair with EasyNegative to improve the quality, which does not specify how to improve the quality. The instructional video will serve as contextual information to guide the agent to complete that. Figure 7. An example of using Stable Diffusion for media creation in WorldGUI. The left shows the interface of using stable diffusion to create the photo. The right shows the user query and GT plan. If no instructional video, it is hard to include all the settings B. Details of Actor Space In this section, we detail the action space used in our WorldGUI. Our action space includes all raw mouse and keyboard actions, such as left-click, right-click, double-click, drag, keystrokes, and key combinations for shortcuts, among others. Mouse-related actions also specify the target position in the pixel space of the observed screenshot. To ensure universal and comprehensive representation of actions, we adopted the widely used Python library, PyAutoGUI, for controlling mouse and keyboard inputs. Each action is represented using the syntax action type(arguments) as in Tab. 6. C. Data (1) Annotators. In this work, we have four annotators: A, B, C, and D. The team comprises one PhD student, one Masters student, and two undergraduate students. Prior to annotation, all annotators receive training on using the applications in WorldGUI to ensure high-quality annotations. For the 10 desktop applications, we divide the software into four parts, assigning each part to different annotator. For the human tests presented in Tab. 1, the annotators demonstrate tasks on software that they did not annotate. As shown in Tab. 5, each annotator is responsible for different software during both the annotation and human testing phases to make the soundness of the Human Test results. WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation Action Type Example Mouse Movement Mouse Clicks Keyboard Type Hotkey Scrolling Drag Mouse Down and Up Press Keys Key Down and Up moveTo(120, 200) click(200, 300) write(classes) hotkey(ctrl, a) scroll(-100) dragTo(120, 220, 2) mouseDown(); mouseUp() press(delete) keyDown(shift) Table 6. The action types and its example in WorldGUI. Annotators Annotation Phase Human Test Phase D PowerPoint, Word, Excel Adobe Acrobat, Stable Diffusion Settings, Web VSCode, File Explorer VSCode, Stable Diffusion, Web Excel, Settings PowerPoint, File Explorer, Youtube Word, Adobe Acrobat Table 7. This table shows the software used during the annotation and human testing phases by different annotators. (2) Data Construction Pipeline. For each task 1) we manually collect the raw videos from the website, 2) then manually cut the raw lengthy videos into sub-clips, 3) ask annotators to manually write the user queries, 4) prepare the project files for each task for reproducibility, 5) generate the ground-truth plans by executing the scripts and agent, 6) conduct the data augmentation by varying the initial state for each task. As shown in Fig. 11, to achieve the above six steps, we invite four annotators and write the necessary scripts to help structure and format the data. Additionally, for the GT plan generation and pre-action generation, we build simple agents to obtain the data. (3) Project File and Enviroment Configuration. In this paper, we utilize project files to ensure the reproducibility of task executions. This approach is simpler and more cost-effective compared to using virtual machines (Xie et al., 2024) or Docker environments (Bonatti et al., 2024). The types of project files for each application are illustrated in Tab. 8. To enable cross-platform usage, such as controlling the computer via phone or other systems, we implement frontend and backend system. The frontend receives user queries, captures screenshots and metadata, and then sends this information to the backend for API calls and the invocation of local vision foundation models. Cross-platform support allows us to deploy autonomous agents in simple and cost-effective manner. (4) Creating Augmented Tasks. In our study, to stimulate dynamic testing processing in real GUI interactions, we propose to design GUI tasks with various initial tasks. Specifically, we propose pre-actions before executing the task. The pre-actions primarily serve two purposes: 1) Simulating Intermediate Task States: Pre-actions can complete specific steps of task, creating starting point from an intermediate state. This approach addresses scenarios where users may seek AI assistance because they are unable to complete task. For example, if the task involves opening dropdown menu, the pre-action may pre-open the menu. If the agent fails to recognize this precondition and follows its plan to click the menu again, it might inadvertently close the menu, causing task failure. 2) Introducing Diverse Initial Context States: Pre-actions can also introduce variations in the initial state, such as opening random tabs or settings. This ensures that the starting state is unconventional, challenging the agent to adapt by modifying its plan or adding new steps. We illustrate one example in Fig. 8. Here, the meta task and augmented task, have the same user query and instructional video and it will ideally have the same final state. We additionally provide more examples about augmenting the meta task in Fig. 9. (5) Data Statistic. WorldGUI comprises 315 GUI tasks sourced from 10 widely used Windows applications, including PowerPoint, Excel, Word, Adobe Acrobat, VSCode, File Explorer, Settings, Web, Youtube, and Stable Diffusion. Here we provide the details about the task activities and project file for each application in Tab. 8. We collected 107 meta tasks from these applications, each of which was augmented 0 to 3 times based on its specific content, resulting in 208 augmented tasks. Fig. 10 presents the task distribution, provides query examples, and illustrates the distribution of meta and augmented tasks across different applications. WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation Figure 8. An example of augmenting one GUI task with manually aug the initial state and then using the execution scripts and corresponding agents to obtain the pre-action for each augmented case. Applications All Task Task Activities PowerPoint Word Excel Adobe Acrobat VSCode Settings File Explorer Web Youtube Stable Diffusion 34 31 35 36 33 39 26 32 28 Change the content style and layout; Design new effects. Formatting the content style and layout Table formatting; Data management and processing Automatic add electric signature; Document management Code editing; Software configuration Advanced personalized and safety settings; File management: Add, delete, rename, and move files. Web operation Video and account configurations Media creation with complicated effects Project File Type project.pptx project.docx project.xlsx project.pdf vscode.exe ms-Settings explorer.exe web browser + URL web browser + UR Local Run Platform Table 8. This table shows all tasks, task activities, and project file of the desktop applications used in WorldGUI. D. GUI-Thinker Reasoning Loop Algorithm In this section, we provide the details of our reasoning loop algorithm in Algorithm 1. E. Qualitative Results (1) In Fig. 12, we present successful prediction example, demonstrating that the WorldGUI can effectively plan each step for task, accurately perceive specific elements in the GUI, and convert them into the correct action code. Additionally, we display the parsed GUI elements, which can accurately identify most content, including small icons and dense text elements. (2) We provide the visualization results of using Planner-Critic, Step-Check, and Actor-Critic in Fig. 14, Fig. 15, and Fig. 16. These qualitative results demonstrate the effectiveness of these critical modules in GUI automation. (3) We also highlight some common errors encountered. 1) The model has the difficulty of obtaining the desired information when we augment the task by invoking the dropdown menu of the Settings application. As shown in the left of Fig. 17, when we click on the System button on the left, it is challenging for our model to extract the buttons position as it is hidden. Such cases require the model to have higher level of ability to delete the content in the input box or click on the blank area. 2) As shown in the right of Fig. 17, the model has difficulty dragging bar to achieve the desired value. 3) The 13 WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation Figure 9. We present the examples of conducting the augmentations from the meta task. model struggles with the visual choice when there is no text information in the screenshot, as shown on the left of Fig. 18. The subtask aims to select the center button, but the current model makes it hard to detect the center choice only from the screenshot. 4) The model cannot successfully locate the position of the input box, as the GUI parser will easily locate the text location Replace with, it always outputs the action like clicking on the Replace with, which will destroy the whole task success. 14 WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation Figure 10. Distribution of collect tasks, selected queries, and task amount of WorldGUI. We have gathered tasks across 10 desktop applications, focusing on the use of productivity software as well as fundamental computer operations and settings. Figure 11. Pipeline of Data Construction. Human: Represents the annotators. Code: Refers to the scripts (e.g., Python Code) utilized to achieve the goal. Agent: We design an agent built upon the MLLMs to achieve the goal. Figure 12. We show one successful prediction of our GUI-Thinker. 15 WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation Algorithm 1 GUI-Thinker Reasoning Loop Algorithm Input: State s, Action Code C, Screenshot , Metadata m, Current subtask S, Critic count Generate task plan with Planner and Planner-Critic Initial current subtask St=0 = S1 Initial s0 = <Continue> while St is not end and < max trials do 1 is the 1-th subtask in the 1-th milestone of p. 1 , where S0 Observe metadata mt and Screenshot Vt from Env. Obtain state st by running Step-Check. if st = <N ext> then Go to the next task St+1 = next(St) end if Check potential modification of subtask St Obtain action code Ct by running Actor; Execute the action code Ct in the Env.; Observe metadata mt and Screenshot Vt from Env. Set Ct = None; = + 1; Set state st = <Critic> (For each subtask, the first step is finished, then execute the actor-critic process) Observe metadata mt and Screenshot Vt from Env. Running Actor-Critic and obtain the state st if st = <N ext> then Go to the next task St+1 = next(St). end if while st = <Critic> and < max critique trials do Running Actor-Critic and obtain the state st and corrected action code Ct if st = <N ext> then Go to the next task St+1 = next(St). end if Execute the action code Ct in the Env.; Observe metadata mt and Screenshot Vt from Env. Set Ct = None; = + 1 end while Go to the next task St+1 = next(St) = + 1 end while Figure 13. We show two examples of using GUI Parser to obtain the element position information. 16 WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation Figure 14. An example of using Planner-Critic to correct the plan. Figure 15. Two examples of using Step-Check to check the subtask status. 17 WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation Figure 16. An example of using Actor-Critic to correct the actions. Figure 17. We display some common errors. 18 WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation Figure 18. We display some common errors"
        }
    ],
    "affiliations": [
        "Show Lab, National University of Singapore"
    ]
}