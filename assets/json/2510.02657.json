{
    "paper_title": "Less LLM, More Documents: Searching for Improved RAG",
    "authors": [
        "Jingjie Ning",
        "Yibo Kong",
        "Yunfan Long",
        "Jamie Callan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Retrieval-Augmented Generation (RAG) couples document retrieval with large language models (LLMs). While scaling generators improves accuracy, it also raises cost and limits deployability. We explore an orthogonal axis: enlarging the retriever's corpus to reduce reliance on large LLMs. Experimental results show that corpus scaling consistently strengthens RAG and can often serve as a substitute for increasing model size, though with diminishing returns at larger scales. Small- and mid-sized generators paired with larger corpora often rival much larger models with smaller corpora; mid-sized models tend to gain the most, while tiny and large models benefit less. Our analysis shows that improvements arise primarily from increased coverage of answer-bearing passages, while utilization efficiency remains largely unchanged. These findings establish a principled corpus-generator trade-off: investing in larger corpora offers an effective path to stronger RAG, often comparable to enlarging the LLM itself."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 7 5 6 2 0 . 0 1 5 2 : r Less LLM, More Documents: Searching for Improved RAG Jingjie Ning, Yibo Kong, Yunfan Long, and Jamie Callan School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA {jening,yibok,justinlo}@andrew.cmu.edu, callan@cs.cmu.edu Abstract. Retrieval-Augmented Generation (RAG) couples document retrieval with large language models (LLMs). While scaling generators improves accuracy, it also raises cost and limits deployability. We explore an orthogonal axis: enlarging the retrievers corpus to reduce reliance on large LLMs. Experimental results show that corpus scaling consistently strengthens RAG and can often serve as substitute for increasing model size, though with diminishing returns at larger scales. Smalland midsized generators paired with larger corpora often rival much larger models with smaller corpora; mid-sized models tend to gain the most, while tiny and large models benefit less. Our analysis shows that improvements arise primarily from increased coverage of answer-bearing passages, while utilization efficiency remains largely unchanged. These findings establish principled corpusgenerator trade-off: investing in larger corpora offers an effective path to stronger RAG, often comparable to enlarging the LLM itself. Keywords: Retrieval-Augmented Generation Passage Retrieval Large Language Models Corpus Scaling Resource-Constrained Inference"
        },
        {
            "title": "Introduction",
            "content": "Retrieval-Augmented Generation (RAG) [6,17] combines document retrieval with large language models (LLMs), and has become popular paradigm for opendomain question answering (QA) [8,28,30]. Most prior work has focused on scaling up the generator, which indeed improves accuracy but requires very large and expensive LLMs [7,19,21]. In contrast, the retriever controls the external evidence supplied to the generator, thereby directly influencing factuality and mitigating hallucinations [17,25]. However, the relationship between retriever capacity and generator size remains underexplored. In particular, it is not well understood whether enlarging the retrieval corpus can reduce reliance on larger generators, which is an important question for practical deployment, where smaller LLMs are easier to serve [31,34,37]. Equal Contributions. 2 J. Ning et al. To address this gap, we conduct systematic study of the trade-off between corpus scale and generator size by combining randomly-shardable retrieval over ClueWeb22 [20] with open-source Qwen3 models [35] of different scales. Our experiments on multiple QA benchmarks [1,13,16] reveal that enlarging the retrieval corpus not only improves the performance of smaller LLMs, but also enables them to rival or even surpass larger counterparts. For example, 1.7B-parameter model with 4 larger corpus outperforms 4B model, and 4B model with only 2 larger corpus consistently outperforms an 8B model. These findings highlight practical trade-off: scaling the retrieval corpus can partially substitute for scaling the generator. This insight suggests an efficient and deployable RAG design, where expanding the corpus offers promising alternative to enlarging the LLM itself."
        },
        {
            "title": "2 Related Work",
            "content": "A substantial body of research has focused on enhancing the intrinsic capabilities of LLMs. Instruction tuning [32,36] and prompt engineering [9,22] improve alignment with user queries, while scaling model size generally yields higher accuracy across diverse tasks [3,14]. However, ever-larger LLMs (e.g., PaLM with 540B parameters [4]) incur prohibitive computational costs, which limits their practicality in many settings. In parallel, retrieval-augmented models have emerged as an alternative path to scaling. Retrieval-augmented language models (RALMs) such as RETRO [2] and Atlas [12] demonstrate that enlarging inference-time datastores consistently improves performance: relatively small generators paired with massive retrieval memory can outperform much larger LM-only baselines. Shao et al. [24] further confirmed this monotonic trend. Unlike modular RAG, which decouples retriever and generator, RALMs integrate retrieval through pretraining-time vector memories, requiring retriever-generator co-training. Modular RAG instead relies on external corpora that can be scaled independently of the generator. This line of work has progressed from Dense Passage Retrieval (DPR) [15] to efficiency-oriented methods such as ANCE [33] and Contriever [11], which make retrieval over very large corpora feasible. These advances enabled shift from early Wikipedia-only setups to broader and more diverse corpora such as LoTTE [23] and BEIR [27]. However, while the community has implicitly moved toward increasingly larger corpora, the direct impact of corpus growth itself has not yet been systematically and comprehensively examined. More recently, studies have begun to explore broader factors influencing RAG, including model size, corpus scale, and context size [18,29]. However, these analyses remain fragmented and primarily descriptive, typically isolating single variables. Importantly, they do not provide principled understanding of how corpus size and LLM size interact, leaving the corpus-generator trade-off essentially unexplored. In particular, prior studies have not jointly examined retrieval corpus expansion and LLM size, leaving open the fundamental question of how corpus-generator trade-offs shape overall system performance. Less LLM, More Documents: Searching for Improved RAG"
        },
        {
            "title": "3 Methodology",
            "content": "To address this gap, we present systematic framework for analyzing corpusgenerator trade-offs in RAG. Our approach evaluates whether, and under what conditions, scaling the retrieval corpus can compensate for smaller LLMs. We organize the methodology around two complementary dimensions. First, we study corpus scaling as compensation: does enlarging the retriever corpus enable smaller generators to rival or surpass larger models by leveraging broader retrieval evidence? Second, we investigate differential effects across LLMs: how do models of different sizes benefit from corpus expansion, and are there consistent patterns in how scaling interacts with model capacity? These two dimensions form the backbone of our experimental design and subsequent analysis."
        },
        {
            "title": "3.1 Retriever: Corpus Scaling",
            "content": "Let denote fixed corpus. We simulate corpus scaling via balanced random partition of into disjoint shards {S1, . . . , SN } of approximately equal size: Π(C) {S1, S2, . . . , SN }, Si Sj = = j, (cid:91) i=1 Si = corpus scale {1, . . . , } is realized by activating shards; without loss of generality we use the canonical prefix C(n) = (cid:83)n i=1 Si, so that C(n) n. For query q, the retriever operates on C(n) to retrieve top-k documents, transform them into chunks, rerank, and pass the top chunks to the generator. All retrieval hyper-parameters and downstream settings are fixed across n."
        },
        {
            "title": "3.2 Generator",
            "content": "We consider family of generators {Mx}, where Mx denotes model with parameter size drawn from the same architecture. Each generator takes as input fixed template consisting of the query and retrieved chunks. Prompting and decoding configurations are kept constant across all Mx, ensuring that model size is the sole varying factor."
        },
        {
            "title": "3.3 Trade-off Formalization",
            "content": "We adopt full-factorial design pairing each corpus scale {1, . . . , } with each generator Mx. For every (n, x) pair, retrieval and decoding settings are fixed, so that only the corpus size and the model size vary. Let Pm(n, x) denote the evaluation score under metric {F1, EM}. To quantify corpus-as-compensation, we define n(xsmall xlarge) := min m{F1, EM} min (cid:8)n : Pm(n, xsmall) Pm(1, xlarge)(cid:9) the smallest corpus scale where smaller generator Mxsmall matches the 1-shard baseline (n = 1) of larger generator Mxlarge. We report and efficiency curves across (n, x), jointly characterizing corpusgenerator trade-offs without relying on model-specific tricks. Section 4 details datasets, metrics, and constants. 4 J. Ning et al."
        },
        {
            "title": "4 Experiment",
            "content": "Building on the methodology outlined in Section 3, we conducted series of controlled experiments across different corpus scales to systematically evaluate our research questions."
        },
        {
            "title": "4.1 Benchmarks",
            "content": "We evaluate on three open-domain QA benchmarks: NQ [16] (1,769 real Google queries from open-domain test set), TriviaQA [13] (1,000 encyclopedic questions randomly sampled from the 9.51k rc.web test split), and WebQ [1] (2,032 Google Suggest queries with Freebase annotations from standard test set)."
        },
        {
            "title": "4.2 Evaluation Metrics",
            "content": "We report F1 and Exact Match (EM) scores, following official evaluation scripts with gold answers. Our analysis primarily focuses on these two metrics throughout the paper."
        },
        {
            "title": "4.3 Retriever: Implementation Details",
            "content": "Corpus and sharding. We use 30% subset of ClueWeb22-A [20], comprising 264M English documents. The corpus is partitioned into 12 balanced shards of 22M documents each, via randomized local assignments to reduce topical skew (though some popularity bias may persist). Encoder and index. We use MiniCPM-Embedding-Light [10] for dense passage encoding, selected for its balance between retrieval quality and computational efficiency at web scale. Indexing is performed using DiskANN [26], widely adopted ANN backend that supports fast multi-shard retrieval, where similar configuration has also been used in other large-scale retrievers built on ClueWeb22-A [5]. Retrieval pipeline. For each corpus scale n, the retriever operates over the active shards to select the top-10 documents, which are segmented into overlapping chunks and reranked. From these, the top-8 chunks are passed to the generator. higher-capacity reranker from the same embedding family is used, and all retrieval and reranking settings are held fixed across settings to isolate corpus scaling effects."
        },
        {
            "title": "4.4 Generator: Implementation Details",
            "content": "We instantiate {Mx} using the open-source Qwen3 family [35]: Qwen3-0.6B, 1.7B, 4B, 8B, and 14B. This series spans over an order of magnitude in parameter scale, enabling controlled study of size-related trends. All models share identical prompting templates and decoding settings. This setup fixes the retrieval pipeline and input assembly while varying only generator scale, allowing clean measurement of corpusmodel trade-offs. While our preliminary experiments with Qwen2.5 and early LLaMA models yielded robustly consistent conclusions, we did not adopt them due to the lack of homogeneous, wide-ranging family comparable to Qwen3 and slower inference speed. Less LLM, More Documents: Searching for Improved RAG"
        },
        {
            "title": "5.1 The Effect of Corpus Scaling as Compensation",
            "content": "We ask whether enlarging the retriever corpus can compensate for smaller LLMs, allowing them to match or surpass larger generators in RAG performance. Concretely, we define the retrievers corpus size in terms of the number of active shards n, where each shard indexes 22M documents from ClueWeb22-A. Thus, corpus scaling corresponds to increasing n, and we study how varying interacts with different LLM sizes. To investigate this question, we fixed the generator to one of five Qwen3 variants (M0.6B, M1.7B, M4B, M8B, M14B) and varied corpus scale by cumulatively activating retrieval shards. For each model, we evaluated the same set of under uniform protocol, enabling consistent cross-model comparison. Table 1: Natural Questions. Shaded cells mark the first scale where smaller model catches up to the next models n=1 baseline, i.e., n(xsmall xlarge). Corpus M0.6B M1.7B M14B M4B M8B 1 2 3 4 5 6 7 8 9 10 11 12 F1 EM F1 EM F1 EM F1 EM F1 EM 29.62 39.93 41.99 23. 16.39 25.33 19.67 29.45 21.25 40.16 29.11 31.24 29.28 40.41 32.87 22.39 29.40 33.35 22.89 40.67 30.41 34.39 23.97 41.54 30.53 41.50 23.46 34.29 31.18 41.87 24.56 35.16 30.70 41.79 23.91 34.82 30.37 41.82 24.94 35.59 30.86 42.28 24.97 35.44 30.92 42.41 25.33 35.96 33.16 43.88 27.76 38.41 27.93 44.21 32.05 45.82 33.35 47.54 48.43 33.75 48.63 34.31 48.84 33.75 49.45 33.92 49.39 34.20 49.60 34.01 49.71 33.80 50.18 34.22 50.39 34.03 50.47 34.20 47.13 48.27 48.30 48.57 48.49 48.11 48.51 48.47 48.99 48.35 45.29 45.99 45.96 46.69 46.84 46.89 46.77 46.88 46.91 47.30 35.27 36.24 35.67 36.24 35.90 35.88 35.84 35.75 35.90 35.10 31.77 35.27 36.46 36.74 36.46 37.03 36.91 37.18 37.20 37.67 37.54 37. Compensation Effect. Our results show clear evidence that corpus expansion enables smaller models to match or even outperform larger counterparts. On NQ, as shown in Table 1, we find that the smallest model needs more corpus to surpass the larger model n(0.6B 1.7B) = 5. For larger generators, compensation is much easier: n(4B 8B) = 2 and n(8B 14B) = 2. These results indicate that, under our setup, scaling corpus size can be more effective and efficient lever than simply scaling LLM size. Figures 1 and 2 visualize these catch-up points. 6 J. Ning et al. Fig. 1: F1 Gains from Scaling on NQ Fig. 2: EM Gains from Scaling on NQ NQ TriviaQA WebQ Table 2: across datasets The same trend holds on TriviaQA and WebQ  (Table 2)  . In the tinymodel regime, corpus scaling is inefficient: e.g., n(0.6B 1.7B) = 10 and n(1.7B 4B) = 7 on TriviaQA. This indicates that corpus scaling is less efficient in the tiny-model regime. In contrast, once the generator reaches medium to large scale, only doubling the corpus is typically sufficient to catch up with the nexttier model. For instance, we find that n(4B 8B) = 2 and n(8B 14B) = 2 on NQ and TriviaQA, and at most = 3 on WebQ. Detailed results for TriviaQA and WebQ are provided in Table 3 and 4 within the Appendix. 0.6B1.7B 5 1.7B4B 2 8B14B 4B8B 10 7 4 2 3 2 1 2 2 Corpus Quality vs. Quantity. Shards in our corpus are balanced in size but not perfectly uniform, because randomization was applied locally during assignment rather than globally. We also observed early performance saturation in the first few increments of corpus scaling. To probe the sensitivity of generation performance to corpus quality, we reversed the shard order at retrieval time, replacing S1, . . . , Sn with SN n+1, . . . , SN . As expected, this led to modest decrease in absolute RAG scores (Figure 3, left). In other words, the lower average corpus quality shifts the performance level downward, yet the relative additional corpus needed for smaller model to catch up with the next larger model remains essentially stable. This reinforces our earlier conclusion: While corpus quality affects absolute accuracy, scaling corpus quantity still enables smaller generators to overtake larger ones with similar amounts of additional context. Based on this observation, we report all subsequent results using the forward corpus order for consistency. Less LLM, More Documents: Searching for Improved RAG 7 NQ-FWD NQ-REV 0.6B1.7B 1.7B4B 4B8B 8B14B 5 3 2 6 4 2 2 Fig. 3: F1 and Catch-up Thresholds under Reversed Corpus Scaling. Left: F1 when using forward (FWD) vs. reversed (REV) corpus scaling order. Right: corresponding catch-up thresholds. Why Does Corpus Scaling Improve RAG? At the micro level, corpus scaling increases the likelihood that retrieved passages explicitly contain the gold answer. With small corpus scale (n = 1), retrieved chunks often lack factual mentions. The larger brings both the query and the answer terms into the context, directly providing the generator with grounding evidence as intended in RAG."
        },
        {
            "title": "Case Analysis",
            "content": "Question: Obey your thirst is the advertising slogan for which soft drink? Answer: Sprite Retrieved Fragment with Corpus scale = 4 Retrieved passage: ...Obey Your Thirst (Oct 1, 1997). Ever heard that catchy slogan for Sprite? Image is nothing. Thirst is everything. Obey your thirst. In the summer of 1996, Coca-Cola, who manufactures Sprite products, was looking to change the image of its sparkling soda... At the aggregate level, we measure the probability that at least one of the top-8 retrieved chunks fed into the generator contains gold answer string, using the same normalization/aliasing as our EM metric. We refer to this probability as the Gold Answer Coverage Rate, which upper-bounds achievable EM under perfect reasoning. Figure 4 shows two key findings: Monotonic Growth. Gold answer coverage often rises consistently with corpus scale, confirming that corpus expansion increases the likelihood of providing usable evidence. Dataset Variation. The magnitude of this benefit differs across benchmarks. TriviaQA exhibits substantially higher coverage than NQ or WebQ, indicating stronger overlap between its information needs and ClueWeb22. Summary of Findings. Across benchmarks, corpus scaling consistently enables smaller generators to catch up with larger ones, with thresholds stable 8 J. Ning et al. even under lower-quality (reversed) corpora. The mechanism is straightforward: enlarging the corpus raises the probability that retrieved chunks contain gold answers, thereby providing models with comparable evidence. Thus, corpus expansion is reliable lever for improving RAG effectiveness. Fig. 4: Gold Answer Coverage Rate for Forward Scaling"
        },
        {
            "title": "5.2 Differential Effects Across LLM Sizes",
            "content": "To analyze how retrieval corpus size differentially affects LLMs of varying scales, we focus on questions that are initially unanswerable without retrieval and examine how performance changes as corpus size increases. Since correctness is most relevant here, we primarily rely on the Exact Match (EM) metric. Classification Methodology Let {0, 1, . . . , 12} denote the corpus size in shards, where n=0 represents the no-retrieval baseline. We define the ContextBenefited Success Rate (CB) at shard as CB@n := Pr(cid:0)EMn-shard = 1 (cid:12) (cid:12) EM0-shard = 0(cid:1) i.e., the empirical proportion of initially unanswerable questions that become answerable once shards are available. By construction, CB@0 = 0. For 1, we also report the marginal improvement. := CB@n CB@(n 1) which captures the additional fraction of initially unanswerable questions newly resolved at shard n. Although CB@n reflects the gains realized, it is bounded above by Gold Answer Coverage Rate at shard n. We define the Utilization Ratio as Ratio@n := CB@n Coverage@n Less LLM, More Documents: Searching for Improved RAG This ratio quantifies an LLMs ability to take advantage of the retrieved evidence: Coverage@n indicates how often the gold answer is retrievable, while CB@n records how often the model succeeds when given the opportunity. CB excludes questions already correct at = 0 (Known), so it isolates retrieval-driven gains by removing parametric knowledge effects. The Known Rate in Figure 5 summarizes how much each model can answer without retrieval. Our analysis, summarized in Figure 6, reveals consistent and modelinvariant pattern for how the retrieval scale helps answer questions that are initially unanswerable without context, with similar trends observed on TriviaQA and WebQ (see the Appendix). Fig. 5: Known Rate on NQ Fig. 6: CB Rate on NQ (FWD) Initial Jump, Subsequent Growth, and Saturation The Critical Impact of Initial Retrieval. The most dramatic performance jump occurs when moving from zero context to single shard across models, with 1 ranging from 16.2% to 20.6%, whereas 2 ranges from only 2.8% to 4.4% (Figure 6). This dominance of initial retrieval persists even with low-quality corpora: for M1.7B with reversed shard ordering, 1 = 16.6% versus 2 = 2.6%. This highlights the primary benefit of RAG: even small corpus immediately fills substantial fraction of knowledge gaps. Model-invariant growth and saturation. Across all LLM sizes, corpus scaling yields qualitatively similar CB pattern: sharp first jump, sustained gains up to roughly 6, and diminishing returns thereafter. The per-shard increments also follow nearly identical pattern across models: peaking early and tapering to near zero (Figure 7). These shared patterns suggest size-invariant retrieval effect: additional shards do not yield systematically greater CB gains for larger generators than for smaller ones. In practice, corpus expansion mainly shifts CB upward without altering its growth curve, reinforcing our conclusion that scaling the corpus is robust and efficient lever for RAG. 10 J. Ning et al. Fig. 7: Per-shard CB gains on NQ (FWD) LLM Context Utilization Remains Stable Across Corpus Scales. As shown in Figure 8, the Utilization Ratio remains approximately constant across corpus scales and is clustered across models within narrow band, indicating stability with respect to n. Although both CB@n and Coverage@n grow with n, their ratio fluctuates only slightly. Thus, corpus scaling primarily raises the coverage of relevant evidence, while the efficiency with which generators convert available evidence into correct answers remains largely unchanged. Consequently, the benefits realized by the generator scale approximately in proportion to the availability of the answer-bearing context, making corpus growth reliable axis of improvement in RAG. Fig. 8: Utilization Ratio across models on NQ (FWD) Non-monotonic Context Utilization One might expect larger LLMs to always leverage retrieved context more effectively, but Figure 8 shows otherwise. Midsized models (M1.7B and M4B) achieve the highest Utilization Ratio (peaking near 42%), while the largest M14B lags behind. This suggests that context utiLess LLM, More Documents: Searching for Improved RAG 11 lization does not grow monotonically with model size, and mid-sized models can sometimes exploit retrieval more efficiently than their larger counterparts. Summary of Findings. Across model sizes, corpus scaling follows common profile: sharp initial rise followed by gradual gains toward saturation. Further analysis of Utilization Ratio shows that the improvements in RAG stem mainly from increased gold answer coverage, rather than differences in context utilization efficiency between models. This indicates that corpus expansion is reliable and size-agnostic lever to improve performance."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we asked whether scaling the retrieval corpus can often substitute for scaling the generator in RAG, and how corpus size interacts with model size under fixed evidence budget. Using controlled evaluations on NQ, TriviaQA, and WebQ with standardized prompting and context formatting, we characterize the corpusgenerator trade-off while holding the presented evidence constant. Our results show that corpus scaling can often offset model downsizing. Across datasets, enlarging the corpus is reliable lever: smaller or mid-sized generators paired with larger corpora frequently surpass larger models under the same evidence budget. In several settings, moving up corpus tiers closed the gap of one to two model-size tiers, demonstrating that more documents can often substitute for more parameters when inference resources are constrained. consistent mechanism explains these gains: performance improvements are driven by relevant-document coverage, not by utilization efficiency. As the corpus grows, the likelihood that retrieved passages contain the gold answer increases consistently, whereas the models context-utilization ratio remains roughly stable across shard counts and model sizes. Thus, scaling primarily works by raising the hit rate of answer-bearing evidence rather than by altering how effectively models exploit the context. At the same time, performance gains from corpus growth saturate after about 56 increase, showing clear diminishing returns. Practically, when resources constrain generator size, it is often better to expand the corpus. Pairing mid-sized generators with larger corpora effectively converts coverage into end-task gains, whereas very large models offer little additional benefit, and very small ones require steep corpus expansions. Although we mainly focus on the Qwen3 family due to the lack of other open-source LLM series with homogeneous, wide-ranging variants, we hope to extend the analysis to additional model families once they become available. Notably, mid-sized models sometimes exploit retrieved context more efficiently than the largest models, consistent with our utilization analysis. Tracking gold-answer coverage and the Utilization Ratio provides practical diagnostics to guide budgeting between retriever and generator. In short: Less LLM, More Documents. 12 J. Ning et al."
        },
        {
            "title": "Appendix",
            "content": "Corpus M0.6B Table 3: n(xsmall xlarge) for TriviaQA. M8B M1.7B M4B M14B 1 2 3 4 5 6 7 8 9 10 11 12 F1 EM F1 EM F1 EM F1 EM F1 44.74 49.73 51.86 53.20 54.79 55.26 55.59 56.52 55.62 57.61 58.74 57. 37.20 41.00 43.60 45.10 46.40 47.55 47.30 48.00 47.20 48.90 50.40 49.05 57.40 62.68 64.90 66.18 66.08 66.01 66.89 67.31 68.32 68.23 68.60 68.44 49.80 55.10 57.20 59.00 59.30 58.16 59.30 59.50 60.90 60.90 61.40 61.16 66.46 73.17 75.99 76.56 76.86 76.60 76.68 77.73 77.61 77.92 78.13 77.89 59.80 65.50 68.50 69.60 69.40 69.77 69.80 69.40 70.40 70.90 71.00 70.77 69.86 76.32 77.59 78.04 78.00 77.58 78.11 78.27 79.30 79.88 79.68 80. 62.90 68.80 69.90 71.20 71.00 70.57 70.90 71.30 72.50 73.30 72.90 73.27 73.16 77.72 79.59 80.75 80.68 80.70 80.55 80.77 81.10 81.56 82.13 82.00 EM 66.60 71.10 72.50 74.00 73.90 74.07 73.90 74.20 74.40 74.90 75.50 75.28 Table 4: n(xsmall xlarge) for WebQuestions. Corpus M0.6B M1.7B M4B M8B M14B 1 2 3 4 5 6 7 8 9 10 11 12 F1 EM F1 EM 27.63 28.95 29.99 31.37 31.68 31.58 31.37 31.80 32.33 32.18 31.98 32.00 14.81 15.70 16.09 17.62 17.66 17.32 17.32 17.62 18.09 18.09 17.91 17.82 33.91 35.40 35.41 35.96 36.35 36.47 36.46 36.64 36.62 36.83 36.61 36.94 18.01 19.09 19.64 20.28 20.28 20.08 20.03 19.92 20.34 20.77 20.57 21.01 37.01 38.20 38.81 39.47 39.59 39.98 39.65 39.62 39.36 38.99 39.20 39.41 EM 20.28 20.92 20.96 21.60 21.55 21.66 21.65 21.57 21.45 21.62 21.51 21.46 F1 EM F1 38.32 38.92 39.44 40.17 40.29 40.24 40.12 40.00 39.79 39.95 39.94 40.12 21.70 21.99 22.60 23.52 23.38 23.13 22.63 22.93 22.83 23.50 22.58 22. 38.68 39.46 40.49 41.32 41.08 41.22 41.25 40.84 40.57 40.79 40.58 40.69 EM 21.65 22.19 22.93 24.02 23.77 23.67 23.52 23.08 22.74 22.93 22.88 23.18 Fig. 9: CB Rate for TriviaQA Fig. 10: CB Rate for WebQ Less LLM, More Documents: Searching for Improved RAG"
        },
        {
            "title": "References",
            "content": "1. Berant, J., Chou, A., Frostig, R., Liang, P.: Semantic parsing on Freebase from question-answer pairs. In: Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. pp. 15331544. Association for Computational Linguistics, Seattle, Washington, USA (Oct 2013), https://www.aclweb. org/anthology/D13-1160 2. Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., van den Driessche, G., Lespiau, J.B., Damoc, B., Clark, A., de Las Casas, D., Guy, A., Menick, J., Ring, R., Hennigan, T., Huang, S., Maggiore, L., Jones, C., Cassirer, A., Brock, A., Paganini, M., Irving, G., Vinyals, O., Osindero, S., Simonyan, K., Rae, J.W., Elsen, E., Sifre, L.: Improving language models by retrieving from trillions of tokens (2022), https://arxiv.org/abs/2112.04426 3. Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D.: Language models are few-shot learners (2020), https://arxiv.org/abs/2005.14165 4. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H.W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A.M., Pillai, T.S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean, J., Petrov, S., Fiedel, N.: Palm: scaling language modeling with pathways. J. Mach. Learn. Res. 24(1) (Jan 2023) 5. Coelho, J., Ning, J., He, J., Mao, K., Paladugu, A., Setlur, P., Jin, J., Callan, J., Magalhães, J., Martins, B., Xiong, C.: Deepresearchgym: free, transparent, and reproducible evaluation sandbox for deep research (2025), https://arxiv.org/ abs/2505.19253 6. Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y., Sun, J., Wang, M., Wang, H.: Retrieval-augmented generation for large language models: survey (2024), https://arxiv.org/abs/2312. 7. Ghorbani, B., Firat, O., Freitag, M., Bapna, A., Krikun, M., Garcia, X., Chelba, C., Cherry, C.: Scaling laws for neural machine translation. In: International Conference on Learning Representations (2022), https://openreview.net/forum?id= hR_SMu8cxCV 8. Gupta, S., Ranjan, R., Singh, S.N.: comprehensive survey of retrieval-augmented generation (rag): Evolution, current landscape and future directions (2024), https: //arxiv.org/abs/2410.12837 9. He, Z., Jiang, H., Wang, Z., Yang, Y., Qiu, L.K., Qiu, L.: Position engineering: Boosting large language models through positional information manipulation. In: Al-Onaizan, Y., Bansal, M., Chen, Y.N. (eds.) Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. pp. 7333 7345. Association for Computational Linguistics, Miami, Florida, USA (Nov 2024). 14 J. Ning et al. https://doi.org/10.18653/v1/2024.emnlp-main.417, https://aclanthology. org/2024.emnlp-main.417/ 10. Hu, S., Tu, Y., Han, X., Cui, G., He, C., Zhao, W., Long, X., Zheng, Z., Fang, Y., Huang, Y., Zhang, X., Thai, Z.L., Wang, C., Yao, Y., Zhao, C., Zhou, J., Cai, J., Zhai, Z., Ding, N., Jia, C., Zeng, G., dahai li, Liu, Z., Sun, M.: MiniCPM: Unveiling the potential of small language models with scalable training strategies. In: First Conference on Language Modeling (2024), https://openreview.net/forum?id= 3X2L2TFr0f 11. Izacard, G., Caron, M., Hosseini, L., Riedel, S., Bojanowski, P., Joulin, A., Grave, E.: Unsupervised dense information retrieval with contrastive learning (2021). https://doi.org/10.48550/ARXIV.2112.09118, https://arxiv.org/ abs/2112.09118 12. Izacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., DwivediYu, J., Joulin, A., Riedel, S., Grave, E.: Atlas: few-shot learning with retrieval augmented language models. J. Mach. Learn. Res. 24(1) (Jan 2023) 13. Joshi, M., Choi, E., Weld, D., Zettlemoyer, L.: TriviaQA: large scale distantly supervised challenge dataset for reading comprehension. In: Barzilay, R., Kan, M.Y. (eds.) Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). pp. 16011611. Association for Computational Linguistics, Vancouver, Canada (Jul 2017). https: //doi.org/10.18653/v1/P17-1147, https://aclanthology.org/P17-1147/ 14. Kaplan, J., McCandlish, S., Henighan, T., Brown, T.B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., Amodei, D.: Scaling laws for neural language models (2020), https://arxiv.org/abs/2001.08361 15. Karpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., Yih, W.t.: Dense passage retrieval for open-domain question answering. In: Webber, B., Cohn, T., He, Y., Liu, Y. (eds.) Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 67696781. Association for Computational Linguistics, Online (Nov 2020). https://doi.org/10.18653/ v1/2020.emnlp-main.550, https://aclanthology.org/2020.emnlp-main.550/ 16. Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., Toutanova, K., Jones, L., Kelcey, M., Chang, M.W., Dai, A.M., Uszkoreit, J., Le, Q., Petrov, S.: Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics 7, 452466 (2019). https://doi.org/10.1162/tacl_ a_00276, https://aclanthology.org/Q19-1026/ 17. Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W.t., Rocktäschel, T., Riedel, S., Kiela, D.: Retrieval-augmented generation for knowledge-intensive nlp tasks. In: Proceedings of the 34th International Conference on Neural Information Processing Systems. NIPS 20, Curran Associates Inc., Red Hook, NY, USA (2020) 18. Li, S., Stenzel, L., Eickhoff, C., Bahrainian, S.A.: Enhancing retrieval-augmented generation: study of best practices. In: Rambow, O., Wanner, L., Apidianaki, M., Al-Khalifa, H., Eugenio, B.D., Schockaert, S. (eds.) Proceedings of the 31st International Conference on Computational Linguistics. pp. 67056717. Association for Computational Linguistics, Abu Dhabi, UAE (Jan 2025), https://aclanthology. org/2025.coling-main.449/ 19. Narayanan, D., Shoeybi, M., Casper, J., LeGresley, P., Patwary, M., Korthikanti, V., Vainbrand, D., Kashinkunti, P., Bernauer, J., Catanzaro, B., Phanishayee, A., Zaharia, M.: Efficient large-scale language model training on gpu clusters using Less LLM, More Documents: Searching for Improved RAG 15 megatron-lm. In: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. SC 21, Association for Computing Machinery, New York, NY, USA (2021). https://doi.org/10.1145/3458817. 3476209, https://doi.org/10.1145/3458817.3476209 20. Overwijk, A., Xiong, C., Liu, X., VandenBerg, C., Callan, J.: Clueweb22: 10 billion web documents with visual and semantic information (2022), https://arxiv.org/ abs/2211. 21. Patterson, D., Gonzalez, J., Le, Q., Liang, C., Munguia, L.M., Rothchild, D., So, D., Texier, M., Dean, J.: Carbon emissions and large neural network training (2021), https://arxiv.org/abs/2104.10350 22. Reynolds, L., McDonell, K.: Prompt programming for large language models: Beyond the few-shot paradigm. In: Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems. CHI EA 21, Association for Computing Machinery, New York, NY, USA (2021). https://doi.org/10.1145/3411763. 3451760, https://doi.org/10.1145/3411763.3451760 23. Santhanam, K., Khattab, O., Saad-Falcon, J., Potts, C., Zaharia, M.: ColBERTv2: Effective and efficient retrieval via lightweight late interaction. In: Carpuat, M., de Marneffe, M.C., Meza Ruiz, I.V. (eds.) Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 37153734. Association for Computational Linguistics, Seattle, United States (Jul 2022). https://doi.org/10.18653/v1/2022. naacl-main.272, https://aclanthology.org/2022.naacl-main.272/ 24. Shao, R., He, J., Asai, A., Shi, W., Dettmers, T., Min, S., Zettlemoyer, L., Koh, P.W.: Scaling retrieval-based language models with trillion-token datastore. In: The Thirty-eighth Annual Conference on Neural Information Processing Systems (2024), https://openreview.net/forum?id=iAkhPz7Qt3 25. Shi, W., Min, S., Yasunaga, M., Seo, M., James, R., Lewis, M., Zettlemoyer, L., Yih, W.t.: REPLUG: Retrieval-augmented black-box language models. In: Duh, K., Gomez, H., Bethard, S. (eds.) Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers). pp. 83718384. Association for Computational Linguistics, Mexico City, Mexico (Jun 2024). https://doi.org/10.18653/v1/2024.naacl-long.463, https://aclanthology. org/2024.naacl-long.463/ 26. Subramanya, S.J., Devvrit, Kadekodi, R., Krishaswamy, R., Simhadri, H.V.: DiskANN: fast accurate billion-point nearest neighbor search on single node. Curran Associates Inc., Red Hook, NY, USA (2019) 27. Thakur, N., Reimers, N., Rücklé, A., Srivastava, A., Gurevych, I.: BEIR: heterogeneous benchmark for zero-shot evaluation of information retrieval models. In: Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2) (2021), https://openreview.net/forum?id= wCu6T5xFjeJ 28. Upadhyay, P., Agarwal, R., Dhiman, S., Sarkar, A., Chaturvedi, S.: comprehensive survey on answer generation methods using nlp. Natural Language Processing Journal 8, 100088 (2024). https://doi.org/https://doi.org/10. 1016/j.nlp.2024.100088, https://www.sciencedirect.com/science/article/ pii/S2949719124000360 29. Vladika, J., Matthes, F.: On the influence of context size and model choice in retrieval-augmented generation systems. In: Chiruzzo, L., Ritter, A., Wang, L. (eds.) Findings of the Association for Computational Linguistics: NAACL 2025. 16 J. Ning et al. pp. 67246736. Association for Computational Linguistics, Albuquerque, New Mexico (Apr 2025). https://doi.org/10.18653/v1/2025.findings-naacl.375, https://aclanthology.org/2025.findings-naacl.375/ 30. Voorhees, E.M., Tice, D.M.: The TREC-8 question answering track. In: Gavrilidou, M., Carayannis, G., Markantonatou, S., Piperidis, S., Stainhauer, G. (eds.) Proceedings of the Second International Conference on Language Resources and Evaluation (LREC00). European Language Resources Association (ELRA), Athens, Greece (May 2000), https://aclanthology.org/L00-1018/ 31. Wang, W., Chen, W., Luo, Y., Long, Y., Lin, Z., Zhang, L., Lin, B., Cai, D., He, X.: Model compression and efficient inference for large language models: survey (2024), https://arxiv.org/abs/2402.09748 32. Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N.A., Khashabi, D., Hajishirzi, H.: Self-instruct: Aligning language models with self-generated instructions. In: Rogers, A., Boyd-Graber, J., Okazaki, N. (eds.) Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). pp. 1348413508. Association for Computational Linguistics, Toronto, Canada (Jul 2023). https://doi.org/10.18653/v1/2023.acl-long.754, https: //aclanthology.org/2023.acl-long.754/ 33. Xiong, L., Xiong, C., Li, Y., Tang, K.F., Liu, J., Bennett, P.N., Ahmed, J., Overwijk, A.: Approximate nearest neighbor negative contrastive learning for dense text retrieval. In: International Conference on Learning Representations (2021), https://openreview.net/forum?id=zeFrfgyZln 34. Xu, J., Li, Z., Chen, W., Wang, Q., Gao, X., Cai, Q., Ling, Z.: On-device language models: comprehensive review (2024), https://arxiv.org/abs/2409.00088 35. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., Zheng, C., Liu, D., Zhou, F., Huang, F., Hu, F., Ge, H., Wei, H., Lin, H., Tang, J., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Zhou, J., Lin, J., Dang, K., Bao, K., Yang, K., Yu, L., Deng, L., Li, M., Xue, M., Li, M., Zhang, P., Wang, P., Zhu, Q., Men, R., Gao, R., Liu, S., Luo, S., Li, T., Tang, T., Yin, W., Ren, X., Wang, X., Zhang, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Zhang, Y., Wan, Y., Liu, Y., Wang, Z., Cui, Z., Zhang, Z., Zhou, Z., Qiu, Z.: Qwen3 technical report (2025), https://arxiv.org/abs/2505.09388 36. Zhang, S., Dong, L., Li, X., Zhang, S., Sun, X., Wang, S., Li, J., Hu, R., Zhang, T., Wu, F., et al.: Instruction tuning for large language models: survey. arXiv preprint arXiv:2308.10792 (2023) 37. Zhu, X., Li, J., Liu, Y., Ma, C., Wang, W.: survey on model compression for large language models. Transactions of the Association for Computational Linguistics 12, 15561577 (11 2024). https://doi.org/10.1162/tacl_a_00704, https://doi.org/10.1162/tacl_a_"
        }
    ],
    "affiliations": [
        "School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA"
    ]
}