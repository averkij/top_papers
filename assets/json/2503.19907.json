{
    "paper_title": "FullDiT: Multi-Task Video Generative Foundation Model with Full Attention",
    "authors": [
        "Xuan Ju",
        "Weicai Ye",
        "Quande Liu",
        "Qiulin Wang",
        "Xintao Wang",
        "Pengfei Wan",
        "Di Zhang",
        "Kun Gai",
        "Qiang Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Current video generative foundation models primarily focus on text-to-video tasks, providing limited control for fine-grained video content creation. Although adapter-based approaches (e.g., ControlNet) enable additional controls with minimal fine-tuning, they encounter challenges when integrating multiple conditions, including: branch conflicts between independently trained adapters, parameter redundancy leading to increased computational cost, and suboptimal performance compared to full fine-tuning. To address these challenges, we introduce FullDiT, a unified foundation model for video generation that seamlessly integrates multiple conditions via unified full-attention mechanisms. By fusing multi-task conditions into a unified sequence representation and leveraging the long-context learning ability of full self-attention to capture condition dynamics, FullDiT reduces parameter overhead, avoids conditions conflict, and shows scalability and emergent ability. We further introduce FullBench for multi-task video generation evaluation. Experiments demonstrate that FullDiT achieves state-of-the-art results, highlighting the efficacy of full-attention in complex multi-task video generation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 7 0 9 9 1 . 3 0 5 2 : r FullDiT: Multi-Task Video Generative Foundation Model with Full Attention Xuan Ju12* Weicai Ye1(cid:66) Quande Liu1 Qiulin Wang1 Xintao Wang1 Pengfei Wan1 Di Zhang1 Kun Gai1 Qiang Xu2(cid:66) 1Kuaishou Technology 2The Chinese University of Hong Kong https://fulldit.github.io Figure 1. FullDiT is multi-task video generative foundation model that unifies conditional learning with full self-attention. With self-attentions long-context learning ability, FullDiT can flexibly take different combinations of input to generate high-quality videos."
        },
        {
            "title": "Abstract",
            "content": "Current video generative foundation models primarily focus on text-to-video tasks, providing limited control for fine-grained video content creation. Although adapterbased approaches (e.g., ControlNet) enable additional controls with minimal fine-tuning, they encounter challenges when integrating multiple conditions, including: branch conflicts between independently trained adapters, parameter redundancy leading to increased computational cost, and suboptimal performance compared to full fine-tuning. To address these challenges, we introduce FullDiT, unified foundation model for video generation that seamlessly integrates multiple conditions via unified full-attention mechanisms. By fusing multi-task conditions into unified sequence representation and leveraging the long-context learning ability of full self-attention to capture condition dyWork done during internship at Kuaishou Technology. (cid:66)Corresponding Author. namics, FullDiT reduces parameter overhead, avoids conditions conflict, and shows scalability and emergent ability. We further introduce FullBench for multi-task video generation evaluation. Experiments demonstrate that FullDiT achieves state-of-the-art results, highlighting the efficacy of full-attention in complex multi-task video generation. 1. Introduction The pre-training of video generative foundation models has predominantly adhered to paradigm focused solely on text-to-video generation, benefiting from its simplicity and broad applicability. However, relying solely on textual prompts offers insufficient granularity, failing to provide precise and direct manipulation over critical video attributes. Real-world creative industriessuch as filmmaking, animation, and digital content creationfrequently require fine-grained control across multiple aspects of generated videos, such as camera movements, character identities, and scene layout. To meet these multifaceted demands, 1 recent works (e.g., ControlNet [65] and T2I-Adapter [40]) typically incorporate additional control signals via adapterbased methods, where adapter networks process supplementary conditions separately and integrate them through mechanisms like cross-attention [20] or addition [18] operations. These adapter-based methods have gained popularity primarily due to their minimal parameter tuning, enabling rapid deployment and flexibility in single-task scenarios. Although these adapter-based techniques have shown promise in single-task scenarios, extending them to multimodal and multi-condition video generation scenarios exposes significant drawbacks. Firstly, adapters trained independently can clash when combined (termed branch conflicts), resulting in compromised overall generation performance [24]. Secondly, these condition-specific adapters often introduce parameter redundancy [61]. Finally, adapters usually achieve suboptimal generation quality compared to methods that fine-tune the entire model [11, 24]. These limitations indicate clear gap and an urgent need for more robust and integrated solution capable of effectively addressing multiple conditions simultaneously. In response to these challenges, this paper presents FullDiT, novel video generative foundation model that harnesses unified full-attention framework to integrate diverse condition signals. As shown in Figure 2, unlike adapter-based approaches that introduce separate processing branches, FullDiT integrates multiple conditions into single coherent sequential representation and learns the mapping from conditions to video with full self-attention. Our key insight is that unified attention facilitates powerful cross-modal representation learning, effectively capturing complex temporal and spatial correlations. By jointly processing conditions within shared attention module, FullDiT inherently resolves branch conflicts common in adapter-based methods, reduces parameter redundancy by avoiding separate adapters, and achieves superior multi-task integration through effective end-to-end training. Furthermore, our solution enables scalable extension to additional modalities or conditions without major architectural modifications. As training data volume increases, FullDiT exhibits strong scalability and reveals emergent capabilities, successfully generalizing to previously unseen combinations of conditions, e.g., simultaneously controlling camera trajectories and character identities. During pre-training, we observed that more challenging tasks require additional training and should be introduced earlier. Conversely, introducing easier tasks too early may cause the model to prioritize learning them, hindering the convergence of challenging tasks. Based on this observation, we follow the training order of text, camera, identity, and depth. Afterward, we conduct quality fine-tuning [10] with manually selected high-quality data to enhance the videos dynamics, controllability, and visual quality. Existing video generation benchmarks focus primarily on single-task evaluations, which makes them inadequate for evaluating models that integrate multiple different conditions. To address this critical gap, we introduce FullBench, the first benchmark specifically designed to evaluate multi-condition video generation tasks comprehensively, consisting of 1,400 carefully curated cases covering various condition combinations , enabling robust assessment of multi-task video generative capabilities. Our contributions are highlighted as follows: We introduce FullDiT, the first video generative foundation model that leverages unified full attention mechanisms for integrating multiple control signals. We propose progressive training strategy for multi-task generation, demonstrating that tailored condition training order leads to better convergence and performance. We construct and release FullBench, the first benchmark designed explicitly for evaluating multi-condition video generation, filling an evaluation gap. Through extensive experimentation, we demonstrate that FullDiT achieves state-of-the-art performance across multiple video generation tasks, showcasing emergent capabilities in combining diverse, previously unseen tasks. 2. Related Work 2.1. Video Generation Video generation has progressed significantly in recent years, transitioning from early GAN-based models [48, 51] to the latest diffusion models [4, 43]. Due to the scalability and effectiveness of the transformer architecture [42], the exploration of video diffusion models has gradually shifted from convolution-based architectures [3, 6, 15, 47, 64] to transformer-based architecture [4, 17, 25, 39, 43]. The most common practice is to divide the video into small patches and then feed sequence of patches into full-attention transformer architecture. Control information, such as text, is injected into the model via cross-attention or adapters. The majority of the previous video generative foundation models are pre-trained with pure text condition and use cross-attention for its injection. In pursuit of more finegrained and user-customized video control, numerous controllable video generation methods focus on the control of motion [1, 2, 14, 18, 34, 54, 68], identity [8, 19, 20, 23, 29, 38, 55, 57, 63, 67], and structure [16, 21, 33, 52, 66, 70] have been introduced, most of which are based on adapterbased frameworks. However, previous studies [11, 24] have shown that adapter-based methods tend to produce suboptimal results with larger model parameter sizes. Furthermore, they are not well-adapted for multi-task generation because of mutual conflicts and the problem of parameter wastage. Following the proposal of MMDiT in Stable Diffusion 3 [12], it was recognized that conditions do not have to 2 Figure 2. Overview of FullDiT architecture and comparison with adapter-based models. We present the diffusion process of the multi-task video generative model on the left. For research purposes, this paper shows input conditions consisting of temporal-only cameras, spatial-only identities, and temporal-spatial depth video. Additional conditions can be incorporated into this model architecture for broader applications. Shown in (a), FullDiT unifies various inputs with procedures: (1) patchify and tokenize the input condition to unified sequence representation, (2) concat all sequences together to longer one, and (3) learn condition dynamics with full self-attention. By comparison, earlier adapter-based approaches (shown in (b)) use distinct adapter designs that operate independently to process various inputs, leading to branch conflicts, parameter redundancy, and suboptimal performance. Each blocks subscript indicates its layer index. be input with cross-attention, but can also be fused using full self-attention [27]. However, no further exploration has been made into applying the same unified full-attention framework to introduce multiple controls in video generation, which is precisely the focus of this paper. 2.2. Multi-Task Image and Video Generation Previous visual generation models are mainly diffusionbased, while language generation models primarily rely on autoregressive next-token prediction. Following these two strategies, unified multi-task image and video generation can be categorized into two main streams as follows: Autoregressive Models. With the development of multimodal language models with paradigm of autoregressive next-token prediction, there is growing interest in exploring ways to incorporate visual generation into these models. Recent efforts can be categorized into two main paradigms: one leverages discrete tokens for image representation, treating image tokens as part of the language codebook [36, 37, 49, 56], while the other uses continuous tokens, combining diffusion models with language models to generate images [13, 58, 59, 69]. There have also been efforts to explore video generation using discrete tokens [26, 53]. While these efforts to incorporate generation into multimodal language models are highly valuable, the prevailing consensus remains that diffusion is currently the most effective approach for video generation. Diffusion Models. Although diffusion models are very effective in generating images and videos, the exploration of multi-task image and video generation is still in the early stages. One Diffusion [28] and UniReal [9] employ 3D representations to combine visual conditions, enabling unified image generation and editing tasks. OmniControl [22, 60] uses the MMDiT [12] design and adds additional branches along with the noise branch and the text branch to incorporate other conditions. OmniFlow [30] further introduces the understanding and generation of audio and text using similar MMDiT design. For video generation, the incorporation of temporal information introduces additional challenges in multitask video generation. While OmniHuman [32] has explored human video generation with multiple controls, it incorporates various types of condition addition methods (i.e., cross-attention, and channel concatenation). Since videos are typically treated as sequential structures in contemporary generation backbones, we aim to explore whether unified sequential input form could effectively integrate multimodal video condition control. Although few of previous works [9] have explored using unified sequential input for image generation, we highlight three key differences compared to video generation: (1) All the conditions they incorporated are confined within the image modality. Thus, they do not offer guidance on how to combine conditions from different distributions or validate the generalization ability. (2) Their focus is on image generation, without considering temporal information. (3) They did not investigate the emergent abilities arising from the interaction among conditions and the scalability of training data. FullDiT represents the first step in exploring them. 3. Method In this section, we present the details of our proposed framework, FullDiT. The goal of FullDiT is to utilize multiple conditions (e.g., text, camera, identities, and depth) to generate high-quality videos with fine-grained control. While this study focuses on limited set of conditions, the method can be adapted and expanded to various conditions. 3 3.1. Preliminary Video diffusion models [4, 43] learn the conditional distribution p(xC) of video data given conditions C. In the diffusion process with the formulation of Flow Matching [35], noise sample x0 (0, 1) is progressively transited into clean data x1 with xt = tx1 + (1 (1 σmin)t)x0, where σmin = 105 and timestep [0, 1]. The learnable model is trained to predict the velocity Vt = dxt dt , which can be further derived as: Vt = dxt dt = x1 (1 σmin)x0 Thus, the model with the parameter Θ is optimized by minimizing the mean squared error loss between the ground truth velocity and the model prediction: = Et,x0,x1,C uΘ(xt, t, C) (x1 (1 σmin)x0) During the inference process, the diffusion model first samples x0 (0, 1), then uses an ODE solver with discrete set of timesteps to generate x1. 3.2. Overview We illustrate the comparison of FullDiT with previous adapter-based frameworks in Figure 2. For adapter-based condition insertion methods (as shown in Figure 2 (b)), an additional adapter is required for each condition. This leads to design complexity and increased parameter overhead, as each condition requires specialized design module for feature processing, resulting in limited scalability for introducing new conditions. Moreover, since adapters are trained independently for each task without sharing information, arbitrary integration may cause conflicts and degrade overall performance. Compared to adapter-based methods, FullDiT directly merges all conditions at an early stage (as shown in Figure 2 (a)), i.e., each condition is tokenized into sequence representation and is subsequently concatenated and fed into the transformer blocks. This facilitates more thorough fusion among conditions without the need of additional parameters for each condition. Following previous works [31, 43], FullDiT adopts transformer architecture comprising 2D self-attention, 3D self-attention, cross-attention, and feedforward networks1. More specifically, FullDiT first tokenizes video, camera, identities, and depth conditions into sequence representations by patchifying them and then mapping them to the hidden dimension using one layer of convolution. Afterward, in each FullDiT block, the sequence latent first passes 2D self-attention with 2D rotational position encoding (RoPE) to enhance spatial information learning. Then, 1Due to the constrain of pre-trained text-to-video model, crossattention is employed to incorporate text information, while 2D selfattention is utilized to reduce computational overhead. But ideally, the model should depend solely on 3D full attention and feedforward layers. the latent passes 3D self-attention with 3D RoPE that enables joint modeling of spatial and temporal information among multiple conditions. This allows for natural interaction among different input signals in both spatial and temporal, thus ensuring optimal performance. Meanwhile, diffusion timesteps are mapped via AdaLN-Zero to four sets of scale, shift, and gate parameters, which are subsequently injected into the 2D self-attention, 3D self-attention, crossattention, and feedforward layers. Given set of conditions {Cii = 1 n}, our goal is to generate high-quality videos that are in line with the conditions. The term condition here can encompass different modalities and various categories. This paper selects three specific conditions in the experiment to verify the effectiveness of FullDiT: camera (E), identities (I), and depth (D). These conditions are selected for their substantial differences in modality representation and distribution. Camera captures 3D scene positional changes, acting as constraint on camera motion. Identities, given as images, define character attributes. Depth, provided in video format, offers structural layout guidance. We also input the text (P ) condition with cross-attention to control the overall generation content. Thus, the overall goal of our generation model uΘ is to learn conditional distribution p(xP, E, I, D). 3.3. Condition Tokenization As discussed in Section 3.2, FullDiT aims to explore how conditions with different formations can be effectively composed. So we choose camera (temporal-only), identities (spatial-only), and depth (temporal-spatial) as inputs. These conditions are distinct from each other in feature shape, data distribution, and controlling effect, which need to be tokenized separately. This section gives details of tokenization. Camera. The input is sequence of camera parameters {Eii = 1 }, where Ei = [Ri; Ti] R34 for the ith frame, Ri R33 is the object orientation, Ti R3 is the object translation, and is the frame number. We follow CameraCtrl [18] and CamI2V [68] to apply plucker embedding to facilitate the model in correlating the camera parameters with image pixels, thereby enabling precise control for visual details. Specifically, the camera parameter Ei can be transferred to its plucker embedding {pu,v = (o du,v, du,v)u = 1 H, = 1 } with: du,v = RK 1[u, v, 1]T + , where is frame height, is frame width, R3 is the camera center, R33 is camera intrinsic parameters. We patchify the plucker embedding RN HW 6 with patch size of 16 and get camera sequence pseq RLpseq 1536, where Lpseq = 16 . pseq is then mapped to hidden dimension with convolution layer. Identities. FullDiT uses causal 3D VAE with temporal compression rate of 4 and spatial compression rate of 8 to 16 8 16 encode images and videos. Identity images RHW 3 8 8 with VAE, then are first encoded to Ienc patchify with patch size of 2 to get sequence Iseq RLIseq 32, where LIseq = 16 . If multiple identity images are provided, the same procedure is applied to each of them. Afterward, all identity sequences Iseq are mapped to the hidden dimension with convolution layer. The convolution layer is initialized using the weights from the projection layer after video patchify. Depth. Depth video RN HW 3 follow the same processing procedure with noisy video xt RN HW 3. 8 8 The depth video is first encoded to Denc by VAE, then patchify to Dseq RLDseq 32 with patch size 2, where LDseq = 16 16 . Finally, Dseq is projected to the hidden dimension with convolutional layer. The convolution layer is initialized using the weights from the projection layer after video patchify. 4 8 4 After tokenization of each condition, the sequence of noisy video xseq, camera pseq, identities Iseq, and depth video Dseq are concatenated along the sequential dimension, allowing joint modeling of multiple conditions. Discussion. While this paper implements only three conditions, the architecture of FullDiT is designed to easily incorporate other modalities or conditions without major structural changes. For example, segmentation videos and sketch videos, which exhibit representational similarities to depth videos, can employ identical tokenization techniques as used for depth. Other modalities, such as audio, can also be tokenized into sequential representations and jointly learned with full attention mechanisms. 3.4. Training Strategy Dataset Construction. The training of FullDiT requires video annotation of text, camera, identities, and depth. However, since obtaining all conditions for every video is challenging, we adopt selective annotation strategy, prioritizing label types that are most compatible with corresponding video data. For text labeling, we follow MiraData [25] and annotate text prompts using structured captions, which can include more detailed information for the video. For camera data, we primarily rely on ground-truth annotations, as existing automated annotation pipelines are unable to achieve sufficiently high quality. Consistent with prior research, we employ the static scene camera dataset RealEstate10K [71] for training. We observed that using only static scene camera datasets can lead to reduced human and object movement in the generated videos. To mitigate this, we further conduct quality fine-tuning using internal camera datasets that incorporate dynamic movements. For identity annotation, we follow the data creation pipeline of ConceptMaster [23] that includes fast elimination of unsuitable videos and fine-grained identity information extraction. For depth annotation, we use Depth Anything [62]. Finally, we use around 1 million high-quality samples for training. Condition Training Order. During the pre-training phase, we noted that more challenging tasks demand extended training time and should be introduced earlier in the learning process. These challenging tasks involve complex data distributions that differ significantly from the output video, requiring the model to possess sufficient capacity to accurately capture and represent them. Conversely, introducing easier tasks too early may lead the model to prioritize learning them first, since they provide more immediate optimization feedback, which hinder the convergence of more challenging tasks. To address this, we adopt progressive training strategy as shown in Figure 3, where we introduce difficult tasks early to ensure the model learns robust representations. Once these challenging tasks are well-trained, the easier tasks can leverage the acquired knowledge, benefiting from improved feature representations and converging more efficiently. Following this principle, we structure our training order as follows: text, camera, identities, and depth, with easier tasks using less training data volume. After pre-training, we further refine the model through quality fine-tuning phase to enhance motion dynamics, fine-grained controllability, and overall visual quality. Figure 3. Illustration of the condition training order. We use red to indicate the training data volume. is for million. 4. Experiments 4.1. Evaluation Benchmark and Metrics Benchmark. To evaluate FullDiT in multi-task video generation, we construct FullBench with 1, 400 high-quality test cases. It comprises seven categories, each covering different condition combinations with 200 test cases: (1) Camera-to-Video. We follow previous works [18, 68] to randomly select 200 cases from RealEstate10k [71] test set. (2) Identities-to-Video. We collect an identities-to-video test set with two types of data. The first category uses segmented identity images (shown in Figure 4 (a)), and the second category incorporates raw images with main identity (shown in Figure 4 (b)). Incorporating both types of test samples ensures coverage of in-domain and out-of-domain cases, leading to more accurate model evaluation. (3) Depth-to-Video. From Panda-70M [7], we randomly selected 200 high-quality videos with significant depth variations, ensuring they were not part of the training set, and annotated their depth using Depth Anything [62]. (4) [Camera+Identities]-to-Video. We select 200 raw identity image pairs (Figure 4 (b)) and 200 3D camera trajecto5 Metrics Model MotionCtrl [54] CameraCtrl [18] CamI2V [68] FullDiT ConceptMaster [23] FullDiT Ctrl-Adapter [33] ControlVideo [66] FullDiT 22.27 21.36 - 22. 18.54 18.64 - 23.38 23.40 Text Camera Identities Depth Overall Quality Clip Score RotErr TransErr CamMC DINO-I CLIP-I MAE Smoothness Dynamic Aesthetic Camera to Video 1.49 1.57 1. 1.20 4.41 3.88 3.81 3.31 4.84 4.77 4.62 3.98 - - - - - - - - - - - - - - - - - - Identities to Video - - 39.97 46.22 65.63 68. Depth to Video - - - - - - - - - 25.63 30.10 14.71 - - - - - - 96.16 95.16 94.50 96.40 95.05 94.95 94.23 94. 95.42 11.43 13.72 19.40 30.53 10.14 16.68 15.47 18. 23.12 4.71 4.66 - 4.95 5.21 5.46 - 5. 5.26 Since this method only supports image-to-video generation, we input the ground-truth first video frame into the model. Thus, frame quality metrics are not reported. Table 1. Quantitative comparison of single task video generation. We compare FullDiT with MotionCtrl [54], CameraCtrl [18], and CamI2V [18] on camera-to-video generation. For identity-to-video, due to lack of open-source multiple identities video generation method, we compare with ConceptMaster [23] model with the size of 1B. We compare FullDiT with Ctrl-Adapter [33] and ControlVideo [66] for depth-to-video. We follow the default setting of each model for evaluation. Since most of the previous methods can generate only 16 frames of video, we uniformly sample 16 frames from methods that generate more than 16 frames for comparison. Data [25] to evaluate video quality: frame CLIP similarity [44] for smoothness, optical flow motion distance [50] for dynamics, and the LAION-Aesthetic [46] model for aesthetic assessment. Details are in the appendix. 4.2. Implementation Details We train FullDiT based on an internal text-to-video diffusion model with approximately 1B parameters. We use small parameter size to ensure fair comparison with previous methods and facilitate reproducibility. Since the training videos vary in size and length, we resize and pad all videos to unified resolution in each batch and sample 77 frames. We apply attention masks and loss masks to ensure proper training. We employ the Adam optimizer with learning rate of 1 105 and train on cluster of 64 NVIDIA H800 GPUs. The 1B model requires approximately 32, 000 steps of training with 20 frames of camera control, maximum of 3 identities, and 21 frames of depth conditions. Camera and depth control are evenly sampled from 77 frames. The detailed training data volume is shown in Figure 3. For inference of the 1B model, we use resolution of 384 672 with 77 frames (approximately 5 seconds with FPS of 15). We set the inference step number as 50 and the classifier free guidance scale as 5. 4.3. Comparison with Previous Methods This section is to validate the superior performance of FullDiT in comparison to previous adapter-based methods. We evaluate FullDiT against prior single-condition guided video generation methods on the camera-to-video, identities-to-video, and depth-to-video subsets of our Figure 4. Examples of two types of identity images. ries from RealEstate10k [71] test set. These identity images and camera trajectories differ from those in (1) and (2). (5) [Camera+Depth]-to-Video. We randomly select 200 cases from RealEstate10k [71] test set and annotate depth with Depth Anything [62]. Note that these camera trajectories differ from those in (1) and (4) to increase test diversity. (6) [Identities+Depth]-to-Video. We collect identity-video pairs following (2) and annotate with Depth Anything [62], with identity images differing from those in (2) and (4). (7) [Camera+Identities+Depth]-to-Video. We first collect identity-depth-video pairs following (6), then annotate camera parameters with GLOMAP [41]. The samples we selected are different from those in (6) to enhance diversity. Metrics. We employ 10 metrics across five key aspects: text alignment, camera control, identity similarity, depth control, and overall video quality. Following prior work [23], we use CLIP similarity [44] for text alignment. For camera control, we adopt RotErr, TransErr, and CamMC, as in CamI2V [68]. Identity similarity [45] is assessed using DINO-I [5] and CLIP-I [44]. Depth control is measured via Mean Absolute Error (MAE), following previous works [16, 66]. We incorporate three metrics from Mira6 Figure 5. Qualitative comparison of FullDiT and previous single control video generation methods. We present identity-to-video results compared with ConceptMaster [23], depth-to-video results compared with Ctrl-Adapter [33] and ControlVideo [66], and camerato-video results compared with MotionCtrl [54], CamI2V [68], and CameraCtrl [18]. Results denoted with * are image-to-video methods. benchmark FullBench. Given the absence of open-source multiple conditions to video generation methods suitable for comparison, we do not provide comparisons of FullDiT with previous methods. We put quantitative results of FullDiT on other subsets of FullBench in the appendix. Quantitative Comparison on Single-Task Generation. For camera-to-video, we compare FullDiT with MotionCtrl [54], CameraCtrl [18], and CamI2V [68]. All of these models are trained on the RealEstate10k [71] dataset, ensuring consistent and fair training data setup for camera conditions. For identities-to-video, due to the absence of an open-source multi-identity video generation model of comparable parameter scale, we benchmark against the 1B ConceptMaster [23], using identical training data with FullDiT. This ensures fair comparison of the same model architecture and training data, which further validates the advantages of full attention. For depth-to-video, We compare with Ctrl-Adapter [33] and ControlVideo [66]. Results show that although the FullDiT integrates multiple conditions, it still achieves state-of-the-art performance on controlling metrics (i.e., text, camera, identities, and depth controls), thereby validating the effectiveness of the FullDiT. For the overall quality metrics, FullDiT outperforms previous methods across the majority. The smoothness of FullDiT is slightly lower than that of ConceptMaster since the calculation of smoothness is based on CLIP similarity between adjacent frames. As FullDiT exhibits significantly greater dynamics compared to ConceptMaster, the smoothness metric is impacted by the large variations between adjacent frames. For the aesthetic score, since the rating model favors images in painting style and ControlVideo typically generates videos in this style, it achieves high score in aesthetics. Qualitative Comparison on Single-Task Generation. As illustrated in Figure 5 (a), FullDiT demonstrates superior identity preservation and generates videos with better dynamics and visual quality compared to ConceptMaster [23]. Since ConceptMaster and FullDiT are trained on the same backbone, this highlights the effectiveness of condition injection with full attention. We further present additional comparisons of depth-to-video and camera-to-video in Figure 5 (b) and (c). The results demonstrate the superior controllability and generation quality of FullDiT compared to existing depth-to-video and camera-to-video methods. 7 Figure 6. Qualitative results of FullDiT with multiple control signals. We show camera+identity+depth-to-video in (a) and (b), camera+identity-to-video in (c), identity+depth-to-video in (d), and camera+depth-to-video in (e). 4.4. Scalability and Emergent ability of FullDiT Figure 7. Camera-to-Video Performance with the Increase of Training Data Volume. We also show the data volume and performance of MotionCtrl [54] and CamI2V [68] for comparison. Scalability. Shown in figure 7, results of FullDiT camerato-video get better on both TransErr and RotErr as training data volume grows, which illustrates the scalability of FullDiT. In comparison, MotionCtrl [54] employed data volume of 6.4 106 and CameI2V [68] used data volume of 3.2 106, yet both performed worse than FullDiT. This further shows the effectiveness of full attention Combination and Emergent Ability. We demonstrate the results of feeding multiple conditions into FullDiT in Figures 6 and 1. These results highlight FullDiTs ability to combine multiple condition inputs, even without training data that encompasses all conditions concurrently. For in8 stance, our training data contains no videos with both camera and identity annotations. But as shown in Figure 6c, FullDiT can effectively generate videos that faithfully reflect both camera and identity inputs. This demonstrates the emergent ability of FullDiT on unseen tasks. 4.5. Ablation Study Metrics Stage Camera Identities Depth RotErr TransErr CamMC DINO-I CLIP-I MAE DepthCameraIDs IDsCameraDepth CameraIDsDepth 2.50 2.46 1.20 6.57 7.43 3. 8.17 8.52 3.98 36.46 42.71 46.22 64.56 65.99 68.59 14.76 14.94 14.71 Table 2. Ablation on condition training order. Impact of Condition Training Order. We train three sets of models with different conditions training orders, using the same data column for each condition: (1) identities, followed by camera, then depth; (2) depth, followed by camera, then identities; and (3) camera, followed by identities, then depth. We evaluate our model on the camera-to-video, identities-to-video, and depth-to-video subset of FullBench. Results in Table 2 validate our claim that more challenging tasks require additional training and should be introduced earlier. Especially, introducing the camera condition too late would significantly reduce its controllability. Impact of Number of Training Stages. We further analyze the impact of using multiple stages of training, as well as the influence of later stages on earlier stages conditions, all under the same data volume. We evaluate our model on the camera-to-video, identities-to-video, and depth-tovideo subset of FullBench. Table 3 shows that multi-stage training leads to better condition control. Specifically, by comparing one-stage and two-stage training, we observe that isolating the camera as an independent training stage significantly improves camera control metrics. Metrics Stage Camera Identities Depth RotErr TransErr CamMC DINO-I CLIP-I MAE I: Camera+ID+Depth I: Camera II: Camera+ID+Depth I: Camera II: Camera+ID III: Camera+ID+Depth 2.69 1.19 1. 1.19 1.23 1.20 6.19 4.49 4.14 4.49 4.20 3.31 8.21 5.01 4. 5.01 4.82 3.98 35.42 - 37.21 - 42.83 46.22 59.49 32. - 65.85 - 64.99 68.59 - 15.81 - - 14.71 Table 3. Ablation on number of training stages. Impact of Model Architecture. To fairly compare the performance between adapter-based approaches and FullDiT within the same architecture and training data, we followed CameraCtrl [18] to implement camera-to-video model on our model architecture. This model starts with the same text-to-video weights as FullDiT and uses only camera data for training. Table 4 shows that, although FullDiT is trained with three conditions, it still outperforms the adapter architecture in terms of camera control. Metrics Text Camera Overall Quality Model Clip Score RotErr TransErr CamMC Smoothness Dynamic Aesthetic Adapter FullDiT 22.58 22.97 1.28 1.20 3. 3.31 4.17 3.98 96.41 96.40 28. 30.53 4.88 4.95 Table 4. Comparing FullDiT with adapter-based architecture. 5. Conclusion We introduced FullDiT, novel video generative foundation model leveraging unified full-attention to seamlessly integrate multimodal conditions. FullDiT resolves adapterbased limitations such as branch conflicts and parameter redundancy, enabling scalable multi-task and multimodal control. We also provided FullBench, the first comprehensive benchmark for evaluating multi-condition video generation. Extensive experiments demonstrated FullDiTs stateof-the-art performance and emergent capabilities. Limitations and Future Works. Despite the strong performance, there are some limitations that need further study: (1) In this work, we only explore control conditions of the camera, identities, and depth information. We did not further investigate other conditions and modalities such as audio, speech, point cloud, object bounding boxes, optical flow, etc. Although the design of FullDiT can seamlessly integrate other modalities with minimal architecture modification, how to quickly and cost-effectively adapt existing models to new conditions and modalities is still an important question that warrants further exploration. (2) The design philosophy of FullDiT is inherited self-attention to from MMDiT [12], which utilizes process text and images simultaneously. Compared to MMDiT, FullDiT takes further step in exploring more unified model architecture and more scalable inputs. However, due to design for multiple control the structural constraints of the pre-trained model, we incorporate text through cross-attention, and FullDiT is not adapted directly from the MMDiT architecture. We anticipate future work to explore more flexible integration of MMDiT and FullDiT architectures."
        },
        {
            "title": "References",
            "content": "[1] Cavia: Camera-controllable multi-view video diffusion with view-integrated attention. arXiv preprint arXiv:2410, 2024. 2 [2] Sherwin Bahmani, Ivan Skorokhodov, Aliaksandr Siarohin, Willi Menapace, Guocheng Qian, Michael Vasilkovsky, Hsin-Ying Lee, Chaoyang Wang, Jiaxu Zou, Andrea Tagliasacchi, et al. Vd3d: Taming large video diffuarXiv preprint sion transformers for 3d camera control. arXiv:2407.12781, 2024. 2 [3] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2256322575, 2023. 2 [4] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators, 2024. 2, 4 [5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. 6 [6] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models, 2024. [7] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with multiple In Proceedings of the IEEE/CVF cross-modality teachers. Conference on Computer Vision and Pattern Recognition, pages 1332013331, 2024. 5 [8] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Yuwei Fang, Kwot Sin Lee, Ivan Skorokhodov, Kfir Aberman, Jun-Yan Zhu, Ming-Hsuan Yang, and Sergey Tulyakov. 9 Multi-subject open-set personalization in video generation. arXiv preprint arXiv:2501.06187, 2025. 2 [9] Xi Chen, Zhifei Zhang, He Zhang, Yuqian Zhou, Soo Ye Kim, Qing Liu, Yijun Li, Jianming Zhang, Nanxuan Zhao, Yilin Wang, et al. Unireal: Universal image generation and editing via learning real-world dynamics. arXiv preprint arXiv:2412.07774, 2024. 3 [10] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, et al. Emu: Enhancing image generation models using photogenic needles in haystack. arXiv preprint arXiv:2309.15807, 2023. 2 [11] Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, ChiMin Chan, Weize Chen, et al. Delta tuning: comprehensive study of parameter efficient methods for pre-trained language models. arXiv preprint arXiv:2203.06904, 2022. [12] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 2, 3, 9 [13] Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024. 3 [14] Daniel Geng, Charles Herrmann, Junhwa Hur, Forrester Cole, Serena Zhang, Tobias Pfaff, Tatiana Lopez-Guevara, Carl Doersch, Yusuf Aytar, Michael Rubinstein, et al. Motion prompting: Controlling video generation with motion trajectories. arXiv preprint arXiv:2412.02700, 2024. 2 [15] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, and Ishan Misra. Emu video: Factorizing text-to-video generation by explicit image conditioning. arXiv preprint arXiv:2311.10709, 2023. 2 [16] Yuwei Guo, Ceyuan Yang, Anyi Rao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Sparsectrl: Adding sparse controls to text-to-video diffusion models. In European Conference on Computer Vision, pages 330348. Springer, 2025. 2, 6 [17] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang, and Jose Lezama. Photorealistic video generation with diffusion models. arXiv preprint arXiv:2312.06662, 2023. 2 [18] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2024. 2, 4, 5, 6, 7, [19] Xuanhua He, Quande Liu, Shengju Qian, Xin Wang, Tao Hu, Ke Cao, Keyu Yan, and Jie Zhang. Id-animator: Zero-shot identity-preserving human video generation. arXiv preprint arXiv:2404.15275, 2024. 2 [20] Li Hu. Animate anyone: Consistent and controllable imageto-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81538163, 2024. 2 [21] Zhihao Hu and Dong Xu. Videocontrolnet: motion-guided video-to-video translation framework by using diffusion model with controlnet. arXiv preprint arXiv:2307.14073, 2023. 2 [22] Lianghua Huang, Wei Wang, Zhi-Fan Wu, Huanzhang Dou, Yupeng Shi, Yutong Feng, Chen Liang, Yu Liu, and Jingren Zhou. Group diffusion transformers are unsupervised multitask learners. 2024. 3 [23] Yuzhou Huang, Ziyang Yuan, Quande Liu, Qiulin Wang, Xintao Wang, Ruimao Zhang, Pengfei Wan, Di Zhang, and Kun Gai. Conceptmaster: Multi-concept video customization on diffusion transformer models without test-time tuning. arXiv preprint arXiv:2501.04698, 2025. 2, 5, 6, 7 [24] Xuan Ju, Ailing Zeng, Chenchen Zhao, Jianan Wang, Lei Zhang, and Qiang Xu. Humansd: native skeleton-guided diffusion model for human image generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1598815998, 2023. 2 [25] Xuan Ju, Yiming Gao, Zhaoyang Zhang, Ziyang Yuan, Xintao Wang, Ailing Zeng, Yu Xiong, Qiang Xu, and Ying Shan. Miradata: large-scale video dataset with long durations and structured captions. arXiv preprint arXiv:2407.06358, 2024. 2, 5, [26] Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, et al. Videopoet: large language model for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023. 3 [27] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 3 [28] Duong H. Le, Tuan Pham, Sangho Lee, Christopher Clark, Aniruddha Kembhavi, Stephan Mandt, Ranjay Krishna, and Jiasen Lu. One diffusion to generate them all, 2024. 3 [29] Guojun Lei, Chi Wang, Hong Li, Rong Zhang, Yikai Wang, and Weiwei Xu. Animateanything: Consistent and conarXiv preprint trollable animation for video generation. arXiv:2411.10836, 2024. 2 [30] Shufan Li, Konstantinos Kallidromitis, Akash Gokul, Zichun Liao, Yusuke Kato, Kazuki Kozuka, and Aditya Grover. Omniflow: Any-to-any generation with multi-modal rectified flows. arXiv preprint arXiv:2412.01169, 2024. 3 [31] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024. 4 [32] Gaojie Lin, Jianwen Jiang, Jiaqi Yang, Zerong Zheng, and Chao Liang. Omnihuman-1: Rethinking the scaling-up of one-stage conditioned human animation models. arXiv preprint arXiv:2502.01061, 2025. [33] Han Lin, Jaemin Cho, Abhay Zala, and Mohit Bansal. Ctrladapter: An efficient and versatile framework for adapting diverse controls to any diffusion model. arXiv preprint arXiv:2404.09967, 2024. 2, 6, 7 10 [34] Pengyang Ling, Jiazi Bu, Pan Zhang, Xiaoyi Dong, Yuhang Zang, Tong Wu, Huaian Chen, Jiaqi Wang, and Yi Jin. Motionclone: Training-free motion cloning for controllable video generation. arXiv preprint arXiv:2406.05338, 2024. 2 [35] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 4 [36] Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin, Yu Qiao, Hongsheng Li, and Peng Gao. Lumina-mgpt: Illuminate flexible photorealistic text-to-image generation arXiv preprint with multimodal generative pretraining. arXiv:2408.02657, 2024. 3 [37] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-io: unified model for vision, language, and multi-modal tasks. In The Eleventh International Conference on Learning Representations, 2022. 3 [38] Ze Ma, Daquan Zhou, Chun-Hsiao Yeh, Xue-She Wang, Xiuyu Li, Huanrui Yang, Zhen Dong, Kurt Keutzer, and Jiashi Feng. Magic-me: Identity-specific video customized diffusion. arXiv preprint arXiv:2402.09368, 2024. [39] Willi Menapace, Aliaksandr Siarohin, Ivan Skorokhodov, Ekaterina Deyneka, Tsai-Shien Chen, Anil Kag, Yuwei Fang, Aleksei Stoliar, Elisa Ricci, Jian Ren, et al. Snap video: Scaled spatiotemporal transformers for text-to-video synthesis. arXiv preprint arXiv:2402.14797, 2024. 2 [40] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 42964304, 2024. 2 [41] Linfei Pan, Daniel Barath, Marc Pollefeys, and Johannes Lutz Schonberger. Global Structure-from-Motion In European Conference on Computer Vision Revisited. (ECCV), 2024. 6 [42] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. 2 [43] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. 2, 4 [44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 6 [45] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In Proceedings of the IEEE/CVF conference generation. on computer vision and pattern recognition, pages 22500 22510, 2023. [46] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems, 35:2527825294, 2022. 6 [47] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. 2 [48] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elhoseiny. Stylegan-v: continuous video generator with the price, image quality and perks of stylegan2. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 36263636, 2022. 2 [49] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. 3 [50] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field In Computer VisionECCV transforms for optical flow. 2020: 16th European Conference, Glasgow, UK, August 23 28, 2020, Proceedings, Part II 16, pages 402419. Springer, 2020. 6 [51] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics. Advances in neural information processing systems, 29, 2016. [52] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability. Advances in Neural Information Processing Systems, 36, 2024. 2 [53] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 3 [54] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. 2, 6, 7, 8 [55] Yujie Wei, Shiwei Zhang, Zhiwu Qing, Hangjie Yuan, Zhiheng Liu, Yu Liu, Yingya Zhang, Jingren Zhou, and Hongming Shan. Dreamvideo: Composing your dream videos In Proceedings of with customized subject and motion. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 65376549, 2024. 2 [56] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv preprint arXiv:2410.13848, 2024. 3 [57] Jianzong Wu, Xiangtai Li, Yanhong Zeng, Jiangning Zhang, Qianyu Zhou, Yining Li, Yunhai Tong, and Kai Chen. Motionbooth: Motion-aware customized text-to-video generation. arXiv preprint arXiv:2406.17758, 2024. 2 [58] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340, 2024. 11 [59] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. 3 [60] Yiming Xie, Varun Jampani, Lei Zhong, Deqing Sun, and Huaizu Jiang. Omnicontrol: Control any joint at any time for human motion generation. arXiv preprint arXiv:2310.08580, 2023. 3 [61] Yifeng Xu, Zhenliang He, Shiguang Shan, and Xilin Chen. Ctrlora: An extensible and efficient framework for controllable image generation. arXiv preprint arXiv:2410.09400, 2024. 2 [62] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1037110381, 2024. 5, 6 [63] Shenghai Yuan, Jinfa Huang, Xianyi He, Yunyuan Ge, Yujun Shi, Liuhan Chen, Jiebo Luo, and Li Yuan. Identitypreserving text-to-video generation by frequency decomposition. arXiv preprint arXiv:2411.17440, 2024. [64] David Junhao Zhang, Dongxu Li, Hung Le, Mike Zheng Shou, Caiming Xiong, and Doyen Sahoo. Moonshot: Towards controllable video generation and editing with multimodal conditions. arXiv preprint arXiv:2401.01827, 2024. 2 [65] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. 2 [66] Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng Controlvideo: Zhang, Wangmeng Zuo, and Qi Tian. Training-free controllable text-to-video generation. arXiv preprint arXiv:2305.13077, 2023. 2, 6, 7 [67] Yuechen Zhang, Yaoyang Liu, Bin Xia, Bohao Peng, Zexin Yan, Eric Lo, and Jiaya Jia. Magic mirror: Id-preserved arXiv video generation in video diffusion transformers. preprint arXiv:2501.03931, 2025. 2 [68] Guangcong Zheng, Teng Li, Rui Jiang, Yehao Lu, Tao Wu, and Xi Li. Cami2v: Camera-controlled image-to-video diffusion model. arXiv preprint arXiv:2410.15957, 2024. 2, 4, 5, 6, 7, 8 [69] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. [70] Haitao Zhou, Chuang Wang, Rui Nie, Jinxiao Lin, Dongdong Yu, Qian Yu, and Changhu Wang. Trackgo: flexible and efficient method for controllable video generation. arXiv preprint arXiv:2408.11475, 2024. 2 [71] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, Learning arXiv preprint and Noah Snavely. view synthesis using multiplane images. arXiv:1805.09817, 2018. 5, 6, 7 Stereo magnification:"
        }
    ],
    "affiliations": [
        "Kuaishou Technology",
        "The Chinese University of Hong Kong"
    ]
}