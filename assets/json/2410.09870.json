{
    "paper_title": "ChroKnowledge: Unveiling Chronological Knowledge of Language Models in Multiple Domains",
    "authors": [
        "Yein Park",
        "Chanwoong Yoon",
        "Jungwoo Park",
        "Donghyeon Lee",
        "Minbyul Jeong",
        "Jaewoo Kang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have significantly impacted many aspects of our lives. However, assessing and ensuring their chronological knowledge remains challenging. Existing approaches fall short in addressing the accumulative nature of knowledge, often relying on a single time stamp. To overcome this, we introduce ChroKnowBench, a benchmark dataset designed to evaluate chronologically accumulated knowledge across three key aspects: multiple domains, time dependency, temporal state. Our benchmark distinguishes between knowledge that evolves (e.g., scientific discoveries, amended laws) and knowledge that remain constant (e.g., mathematical truths, commonsense facts). Building on this benchmark, we present ChroKnowledge (Chronological Categorization of Knowledge), a novel sampling-based framework for evaluating and updating LLMs' non-parametric chronological knowledge. Our evaluation shows: (1) The ability of eliciting temporal knowledge varies depending on the data format that model was trained on. (2) LLMs partially recall knowledge or show a cut-off at temporal boundaries rather than recalling all aspects of knowledge correctly. Thus, we apply our ChroKnowPrompt, an in-depth prompting to elicit chronological knowledge by traversing step-by-step through the surrounding time spans. We observe that our framework successfully updates the overall knowledge across the entire timeline in both the biomedical domain (+11.9%) and the general domain (+2.8%), demonstrating its effectiveness in refining temporal knowledge. This non-parametric approach also enables knowledge updates not only in open-source models but also in proprietary LLMs, ensuring comprehensive applicability across model types. We perform a comprehensive analysis based on temporal characteristics of ChroKnowPrompt and validate the potential of various models to elicit intrinsic temporal knowledge through our method."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 3 1 ] . [ 1 0 7 8 9 0 . 0 1 4 2 : r Preprint. Under review. CHROKNOWLEDGE: UNVEILING CHRONOLOGICAL KNOWLEDGE OF LANGUAGE MODELS IN MULTIPLE DOMAINS Yein Park1, Chanwoong Yoon1, Jungwoo Park1, Donghyeon Lee1, Minbyul Jeong2, Jaewoo Kang1,3 Korea University1 Upstage AI2 AIGEN Sciences3 {522yein, cwyoon99, jungwoo-park, dong9733, kangj}@korea.ac.kr"
        },
        {
            "title": "ABSTRACT",
            "content": "Large language models (LLMs) have brought significant changes to many aspects of our lives. However, assessing and ensuring their chronological knowledge remains challenging. Existing approaches fall short in addressing the accumulative nature of knowledge, often relying on single time stamp. To overcome this, we introduce CHROKNOWBENCH, benchmark dataset designed to evaluate chronologically accumulated knowledge across three key aspects: multiple domains, time dependency, temporal state. Our benchmark distinguishes between knowledge that evolves (e.g., scientific discoveries, amended laws) and knowledge that remain constant (e.g., mathematical truths, commonsense facts). Building on this benchmark, we present CHROKNOWLEDGE (Chronological Categorization of Knowledge), novel sampling-based framework for evaluating and updating LLMs non-parametric chronological knowledge. Our evaluation led to the following observations: (1) The ability of eliciting temporal knowledge varies depending on the data format that model was trained on. (2) LLMs partially recall knowledge or show cut-off at temporal boundaries rather than recalling all aspects of knowledge correctly. Thus, we apply our CHROKNOWPROMPT, an in-depth prompting to elicit chronological knowledge by traversing step-by-step through the surrounding time spans. We observe that our framework successfully updates the overall knowledge across the entire timeline in both the biomedical domain (+11.9%) and the general domain (+2.8%), highlighting its positive effect in refining temporal knowledge. This non-parametric approach also enables knowledge updates not only in open-source models but also in proprietary LLMs, ensuring comprehensive applicability across model types. We perform comprehensive analysis based on temporal characteristics of CHROKNOWPROMPT and validate the potential of various models to elicit intrinsic temporal knowledge through our method."
        },
        {
            "title": "INTRODUCTION",
            "content": "Do large language models (LLMs) possess the ability to understand and track the history of knowledge as time progresses? In other words, can these models, which represent the cutting edge of modern artificial intelligence, reason appropriately about questions that involve evolving facts? Although some details remain controversial, knowledgelike scienceis built upon accumulation (Zeigler, 2012; Picho et al., 2016). From raw data to information and to knowledge, every bit is cumulative which contributes to progress across all domains. This accumulation forms the foundation for higherlevel reasoning, which is akin to wisdom in navigating the complexities of our world (Rowley, 2007). Given that LLMs are trained on vast and diverse corpora and are now integral to numerous applications in our daily lives, they must remain accurate and up-to-date to ensure reliability. Early versions of ChatGPT (OpenAI, 2022), for instance, sometimes produced inaccurate or absurd responses like the Corresponding authors 1Our datasets and code are publicly available at https://github.com/dmis-lab/ChroKnowledge 1 Preprint. Under review. Figure 1: The overview of time variant dataset generation in ChroKnowBench. We accumulate knowledge in three key aspects: (1) multiple domains: general, biomedical, legal, commonsense, and mathematics; (2) time dependency: as time goes by, changeable knowledge; (3) temporal state: dynamic (has evolved over period) and static (no change occurred during period). infamous example of The happening of King Sejong (1397-1450) throwing MacBook (2016-)2. These errors still give us lesson that we need more precise model recalling knowledge correctly. When we examine the issue closely, its not just matter of hallucination but also about whether the alignment of knowledge, particularly regarding dates, is accurate. Ensuring that LLMs maintain current and contextually relevant knowledge over time is crucial and researchers have explored various ways to investigate and verify the knowledge within these models (Zhang et al., 2024a). Pioneering works investigate whether language model has an ability of knowledge base or not in diverse domains (Petroni et al., 2019; Sung et al., 2021). Many subsequent studies analyze how LLMs define knowledge (Dai et al., 2022; Mishra et al., 2024); exploit how LLMs represent their knowledge (Geva et al., 2023; Zheng et al., 2024) with temporal context (Kasai et al., 2023; Fatemi et al., 2024); edit the misleading aspects of knowledge (Manakul et al., 2023; Wang et al., 2024c). Here, we raise question: Do these methods sufficiently address the accumulative nature of knowledge?. Current temporal-related approaches for evaluating and updating LLMs often focus on single time stamps, struggling to address the accumulative characteristics (Jang et al., 2022a; Ge et al., 2024), which are especially important in specialized domains such as scientific discoveries and amended laws. This limitation can lead to outdated or incomplete information, undermining the models effectiveness and safety. Addressing these challenges requires comprehensive approach is neededone that accounts for both the temporal evolution and the accumulative nature of knowledge. Thus, we introduce CHROKNOWBENCH, benchmark dataset designed to evaluate chronologically accumulated knowledge across three dimensions: time dependency (time variant and time invariant), multiple domains (general, biomedical etc.), and temporal state (dynamic and static). CHROKNOWBENCH differentiates between knowledge that is subject to evolution (e.g., transfer situation of soccer player, scientific discoveries, and amended legal regulations) and knowledge that remains invariant (e.g., mathematical truths and commonsense facts). We then classify domains based on whether they are influenced by the flow of time, considering the domain specificity. Finally, we set the time frame to categorize the knowledge as either changeable or steady (Section 3). Building on this benchmark, we also present CHROKNOWLEDGE (Chronological Categorization of Knowledge), novel framework for assessing and enhancing the non-parametric chronological knowledge of LLMs. So, we start from analyzing how current open-source and proprietary LLMs work. As we expected, the time invariant knowledge shows steady in all time frame. However, the domain-specific characteristics significantly influence the representation of temporal knowledge from LLMs. More stable domains exhibit consistent performance, while more variable domains show more fluctuations. These observations highlight that we need comprehensive approach to enhance representing temporal knowledge from LLMs (Section 5). To this end, our CHROKNOWPROMPT approach utilizes an in-depth chronological prompting strategy that traverses and updates knowledge across adjacent time spans, effectively addressing issues of partial recall and temporal boundaries (Section 6). In knowledge recall, our evaluation reveals significant improvements in the biomedical domain (+11.9%) and modest gains in the general domain (+2.8%). Our non-parametric approach allows for direct updates to both proprietary and open-source 2https://english.hani.co.kr/arti/english edition/e international/ 2 Preprint. Under review. Table 1: Knowledge categorization with temporal component. We classify responses into Correct, Partial Correct, and Incorrect to specify eliciting predictions in diverse way by comparing them with the answer set A. We use temperature set 0, 0.7 to capture variations in prediction, where includes both greedy decoding and temperature sampling. We set as 5, meaning that we evaluate using five distinct combinations of few-shot exemplars to ensure the robust assessment. Category Definition Description Correct { ˆoi (Di, s, r, t) = ˆoi; M, τ = 0}n i=1 All objects generated with greedy decoding are entirely included within the answer set. Partial Correct (cid:83) τ Incorrect (cid:83) τ { ˆoi (Di, s, r, t) = ˆoi; M, τ }n i=1 = At least one generated object from greedy decoding or temperature sampling is in the answer set. { ˆoi (Di, s, r, t) = ˆoi; M, τ }n i=1 = None of the generated objects, either from greedy decoding or temperature sampling, are included in the answer set. LLMs without extensive retraining, highlighting its practicality and broad applicability (Section 7). Our work emphasizes the importance of temporal context in eliciting LLMs knowledge, introducing novel framework for updating their chronological knowledge. We perform comprehensive analysis based on temporal characteristics of CHROKNOWPROMPT and validate the potential of various models to elicit intrinsic temporal knowledge through our method."
        },
        {
            "title": "2 PRELIMINARIES",
            "content": "2.1 KNOWLEDGE CATEGORIZATION WITH TEMPORAL COMPONENT To distinguish and evaluate the knowledge levels of language models, we utilize the Sampling-based Knowledge Categorization (SliCK) framework (Gekhman et al., 2024). This approach starts by sampling the model answers to questions using various few-shot exemplar sets D. The sampling is conducted under two temperature conditions: τ = 0 and τ > 0. Then, it categorizes the degree to which the model knows each piece of knowledge into four levels: HighlyKnown, MaybeKnown, WeaklyKnown, and Unknown. Based on Gekhman et al. (2024), we make the following modifications as follows: (1) We append temporal component to the conventional {subject (s), relation (r), object (o)} triplet structure, allowing us to evaluate the models knowledge across different time stamps; (2) We merge the two categories (MaybeKnown and WeaklyKnown) that represent recallable knowledge states3 into single category (Partial Correct); (3) By using time attribute, we also renamed the HighlyKnown and Unknown to the Correct and Incorrect, respectively. Our detailed definitions and descriptions are provided in Table 1. Although this setting allows us to categorize the models sampled responses more precisely regarding time attribute, it only captures the models knowledge at specific time points t, limiting our ability to observe changes over time. We address this limitation in Section 6. 2.2 ELICITING KNOWLEDGE USING DIVERSE TEMPLATES Since models prefer different formats when eliciting their knowledge, it is important to use varied approaches to accurately assess their understanding (Zhou et al.). While we initially evaluate the models knowledge using standard triplet format, relying on single template may not sufficiently capture the full extent of the models knowledge. Thus, following Hendrycks et al. (2021), we also employ well-known format, multiple-choice question answering (MCQA), to elicit the models knowledge more effectively. As result, we propose two templates for measuring how much knowledge the model holds: triplets (hereafter referred to as Generation) and MCQA. Each template is designed with appropriate few-shot exemplars and corresponding matching rules. For example, in Generation, due to the complexity of evaluating responses, we apply fuzzy matching techniques to compare the generated responses against predefined labels. See Appendix A.1 for further details of few-shot exemplars, fuzzy matching rules, and examples of two templates. 3We refer recallable knowledge as the presence of at least one answer in the answer set, generated using either the greedy decoding or the temperature sampling method. 3 Preprint. Under review. Table 2: Statistics of our benchmark dataset. We categorize whether knowledge changes over time (Time Variant) or remains constant (Time Invariant). We provide five domains to measure the knowledge from LMs. We set the temporal state with dynamic (knowledge that changes within the time frame we have set) and static (knowledge that do not change within the time frame we have set). Time Dependency Domain # of (Time Frame) Relations Structured Format Temporal State # of Examples Source Time Variant Time Invariant general (2010 - 2023) biomedical (2020 - 2024) legal (2010 - 2023) commonsense math 14 6* 8 12 (s, r, o, t) (s, r, o, t) QA (s, r, o) (s, r, o) dynamic static dynamic static dynamic static invariant invariant 8,330 8,302 7,345 7,345 3,142 3,142 Wikidata UMLS CFR 24,788 2, CSKG Math-KG"
        },
        {
            "title": "3 CHROKNOWBENCH: CONSTRUCTING A BENCHMARK DATASET",
            "content": "In this section, we enumerate the details of constructing CHROKNOWBENCH, chronologically accumulated knowledge benchmark dataset. The CHROKNOWBENCH dataset encompasses three key aspects: time dependency (time variant and invariant), multiple domains (general, biomedical, legal, commonsense, and math), and temporal state (dynamic and static). We first categorize knowledge into two groups: knowledge that remains unchanged over time (time invariant) and knowledge that changes over time (time variant). Additionally, we classify domains based on whether they are influenced by the flow of time, considering the specificity of each domain. Finally, we categorize knowledge as either changeable (dynamic) or steady (static) within the time frame we have set. 3.1 TASK DEFINITION Our primary focus is time variant knowledge across three domains (general, biomedical, and legal) with comparisons to time invariant knowledge across two domains (commonsense and mathematics). What knowledge would be the difference between time variant and invariant? The time variant knowledge has specific object changing across stream of time. For example, Cristiano Ronaldo (s) was member of sports team of (r) Manchester United F.C. (o1) in 2009 (t1) and Real Madrid CF (ok) in 2018 (tk). Likewise, we identify subject and object alias for each relation, then gather yearly changed objects. After accumulating object lists {o1, o2, . . . , om}, we de-duplicate and fill out the missing data in specific years based on available data; objects between Manchester United F.C. (o1) in 2009 (t1) and Real Madrid CF (ok) in 2018 (tk), missing objects between 2010 (t2) to 2017 (tk1) filled with existing object of 2009 (t1). The statistic of CHROKNOWBENCH dataset is in Table 2. 3.2 DATASET GENERATION To construct dataset, we select annual knowledge sources for each domain, possible to be aligned with each elements even though the corpus does not specifically mention about time stamp. For sources with structured triplets, we identify temporal affect-able relations that typically change over time, such as position held. As time variant knowledge refers to the knowledge that has the potential to change over time, we divide it into two temporal states for more fine grained results: (1) dynamic, where the knowledge has evolved over the accumulated period. (2) static, where no change occurred during the accumulated period though it has potential to be changed. Following the methodology outlined in Section 3.1, we track changes in objects to build the dynamic dataset, employing normalization and de-duplication for verification. Each object is checked with strict exact string match, then add into objects pool. Simultaneously, we identified unchanged objects over the same time frame to construct the static dataset. At the end, all data elements consist with an associated object pool {o1, o2, . . . , om} over time frames {t1, t2, . . . , tm}. 3.3 TIME VARIANT & INVARIANT KNOWLEDGE We sourced time variant knowledge from the general, biomedical, and legal domains. In general domain, we utilize Wikidata (Vrandeˇcic & Krotzsch, 2014) dump to track object changes among 4 Preprint. Under review. Figure 2: Performance analysis of general domain. The left two figures show the tendency of dynamic knowledge in temporal results, with more fluctuations in recent knowledge compared to static knowledge, which is more stable but still shows slight variation across the plots. The templatewise results reveal trend of minimal reliance on internal representation (top figures). Meanwhile, the MCQA templates are influenced by the models specialized capabilities, even managing to overcome the training cutoff in recent years (bottom figures). the time frame using suggested time quantifiers. Collecting similar amounts of dynamic and static instances across eight relations, the result is formatted as {s, r, o, t} quadruplet for each object and accompanied time stamp. For biomedical domain, we parse Unified Medical Language System (UMLS) (Bodenreider, 2004) metathesaurus data, where suggest yearly updated research in the domain, following previous work of BIOLAMA (Sung et al., 2021). Due to the slow pace of change in biomedical research, object pools in this domain shows slight expansions or contractions over time frame. In the legal domain, we employ the Code of Federal Regulations (CFR) (U.S. Government) to track regulatory changes, as they suggest collection and accumulation of change in regulations at the end of year. Starting from pre-processing unstructured xml data, we adopt QA-like format with placeholder for object, tracked among time frame which ends to dynamic or static whether it change or not. Time invariant knowledge, which remains constant regardless of temporal context, is drawn from common-sense and mathematical domains. We process the CSKG (Ilievski et al., 2021) dataset of commonsense knowledge, and Math-KG (Wang, 2022) for covering areas like data structures and pure mathematics. Further details of our source data are provided in Appendix A.2."
        },
        {
            "title": "4 EXPERIMENTAL SETUP",
            "content": "We conduct the experiments on variety of proprietary and open-source LLMs. Each model utilizes either an instruction-tuned or chat version to enhance instruction following during sampling. We enumerate the eight open-source and one proprietary LLMs: Llama-3.1-8B-Instruct (Meta, 2024), Llama-3-8B-Instruct (Dubey et al., 2024), Llama-2-7b-chat-hf (Touvron et al., 2023b), Mistral-7BInstruct-v0.3 (Jiang et al., 2023), Phi-3.5-mini-instruct (Abdin et al., 2024), SOLAR-10.7B-Instructv1.0 (Kim et al., 2023), gemma-2-9b-it (Team et al., 2024b), gemma-7b-it (Team et al., 2024a), and GPT-4o mini (OpenAI, 2024a). We focus on analyzing trends in the chronological knowledge captured by suggested models from various companies. These models differ in corpus coverage reflecting their respective cut-off dates. Details of our inference setups are in Appendix A.3."
        },
        {
            "title": "5 CHROKNOWLEDGE: CHRONOLOGICAL CATEGORIZATION OF KNOWLEDGE",
            "content": "In this section, we introduce CHROKNOWLEDGE (Chronological Categorization of Knowledge), sampling-based framework designed to evaluate and update LLMs non-parametric chronological 5 Preprint. Under review. Figure 3: Performance analysis of biomedical domain. Across all the cases, there is little variability, highlighting domain-specific feature. Both the left and right figures show similar aspects, though the failure to track updates between 2022 and 2023 is more clearly shown in the dynamic dataset. When comparing the top two generation figures with the MCQA results at the bottom, the generation templates show narrower gap between models, while the gap in MCQA is more distinct. However, in terms of temporal states, the generation settings reveal clearer distinctions between dynamic and static knowledge, whereas in MCQA, the task-specific abilities may compensate for the differences. knowledge. Our methodology assesses the temporal capabilities of LLMs using two distinct templates, detailed in Section 2.2, and explores how current LLMs encapsulate temporal information. 5.1 RESULTS OF REPRESENTING KNOWLEDGE FROM LARGE LANGUAGE MODELS For testing models knowledge within the categorization, we sample five times for each knowledge to elicit it as possible in dynamic and static dataset. We present our findings across three different aspects: temporal-wise, template-wise, and domain-wise results. In Figure 2 and 3, we depict the results for all domains and especially address general and biomedical domains in main section. Legal and invariant datasets are presented in Appendix (Figure 6 and 7). Temporal-wise Results. By comparing the left and right side would provide the tendency of temporal-wise results in Figure 2 and 3. common trend across models is decline in recent knowledge, aligning with pretraining corpus cutoff dates. In dynamic datasets, models show reduced knowledge in multiple-choice question answering (MCQA) settings, highlighting the impact of formatting and prompting on temporal abilities. Static datasets exhibit less fluctuation, though the result of general domain suggests domain-specific characteristics that each relations are more influenced via temporal sensitivity. Template-wise Results. By comparing the top and bottom side would provide the tendency of template-wise results in Figure 2 and 3. As we utilized two templates: generation and MCQA, the result is also checked in template specificity. Generation templates reveal greater decline in recent knowledge, as models rely on internal representations without predefined answers. In contrast, MCQA templates help models select correct answers from options, mitigating some gaps in recent knowledge. Notably, GPT-4o Mini performs best in generation tasks, while gemma-2-9b-it excels in MCQA. In addition, the result of Phi-3.5-mini-instruct indicates specialized abilities can offset smaller model sizes, as it is more than twice small billion size compared to other models. This highlights the importance of template design in eliciting temporal knowledge, with generation benefiting from larger models and MCQA leveraging structured formats to enhance knowledge recall. 6 Preprint. Under review. Domain-wise Results. Comparing the Figure 2 and 3 provide the tendency of domain-wise results. In general domain, the models show decline in recent knowledge, especially with generation templates. It comes from domains nature, such as changes in relation position held or member of sports team which has sensitivity to temporal cues, leading to higher variability. However, in biomedical domain, models fail to track recent updates between 2022 and 2023, particularly in generation tasks. Static datasets remain consistent, reflecting the stable nature of scientific knowledge. Here, MCQA setting offers some resilience though still limited to specific models improvement. We provide the overall trends of legal domain and time-invariant knowledge in Appendix A.4. Overall, domain-specific characteristics significantly influence LLMs temporal knowledge representation. More stable domains like biomedical and legal exhibit consistent performance, while more variable domains like general show more fluctuations. These insights underscore the need for tailored strategies to enhance LLMs temporal knowledge capabilities."
        },
        {
            "title": "6.1 CHRONOLOGICAL CATEGORIZATION",
            "content": "We represent CHROKNOWPROMPT (Chronological Knowledge Prompting), an in-depth prompting of non-parametric method to elicit chronological knowledge which has gap between each time stamp, by traversing step-by-step through the surrounding time spans. In previous section, we demonstrate whether open-source and proprietary models possess specific knowledge at various time stamps. However, this does not sufficiently assess the models understanding of knowledge within chronological progression. As Zhao et al. (2024) suggest, knowledge influenced heavily by temporal factors such as general domain can still vary in more stable situation like static dataset. To address this issue, we first reclassify the models responses using refined categorization scheme, allowing for more comprehensive evaluation of temporal knowledge across all years. Figure 4: Chronological categorization based on each answer with its time stamp. If the model answer correctly for all, it is re-categorized as Known. The target of ChroKnowPrompt is Partial Known, which confuses its knowledge among the whole time stamps. Figure 4 illustrates how it works: (1) We refer it as Known for the precise temporal alignment if the model correctly identifies all relevant objects for given knowledge category at each specific year; (2) If model fails to match the correct objects for every year, we refer it as Unknown for indicating incomplete or misaligned temporal knowledge; (3) The model accurately responds either just before or after specific year but fails for others, signifying outdated information or forgotten legacy knowledge due to continuous updates (Cut-off); (4) The model correctly identifies some objects for given year but not others, reflecting an incomplete understanding of the temporal knowledge (Partial Known). Our main focus is on the Partial Known category, where models demonstrate substantial temporal knowledge but fail to answer correctly for all years, often showing confusion between correct answers. For example, Nana Akwasi Asare (s) was member of sports team of (r) FC Utrecht (on) in 2011 (tn), but the model incorrectly identifies the team as FC Groningen, despite answering correctly with FC Utrecht for 2010 (tn1) and in 2012 (tn+1). At this point, we hypothesize that when the model gets one time stamp wrong, more explicit focus on the temporal aspects surrounding that time span could help it generate more accurate answers. This is the core idea behind CHROKNOWPROMPT, which we will discuss details in the following section. 6.2 METHOD We introduce chronological prompting technique for non-parametric knowledge editing, aimed at bridging knowledge gaps by utilizing multiple temporal snapshots. This method enhances the 7 Preprint. Under review. Figure 5: Overview of ChroKnowPrompt. The algorithm systematically traverses step by step, appending each spans result as few shot for each steps. The range of each previous and next span is predefined, with the order of nearest time stamp from target Tn. The model suggests last candidate answer Cn, verified and refined through several steps, which ends to be checked with the object on in benchmark. models reasoning by systematically integrating knowledge from different time stamps, enabling for in-depth eliciting. Figure 5 illustrates an example of the chronological prompting process. Our process begins with target year tn. The algorithm systematically traverses the preceding years (tn1, tn2, . . .) in the Previous Span and the subsequent years (tn+1, tn+2, . . .) in the Next Span. For each traversed year, the most representative object ˆok is selected from the Correct (represented by circle) and Partial Correct (represented by triangle) categories in Table 1 using majority voting. Starting with the initial prompt containing the target time tn, subject sn, and relation rn, the nearest year in the previous span is appended above the initial prompt with the selected object, forming the first step. The model then generates candidate answer C1 for tn using this augmented prompt. Next, our CHROKNOWPROMPT iteratively adds prompts from progressively earlier years, refining the candidate answer by verifying its consistency across contexts (from C1 to C2). This backward traversal continues until predefined span range is reached. Once the previous span is completed, our algorithm performs forward traversal by appending objects from subsequent years below the target year, further generating and verifying candidate answers. If there are no previous or next years available, the process proceeds on only one side. Upon completing all traversals, the final candidate answer Cn is compared against the benchmark object for tn. If the candidate answer aligns correctly with the object for tn, appropriately reflecting the temporal contexts, the knowledge categorization for tn is updated to Chrono-Correct, which is equivalent to Correct for chronological assessments. In Appendix A.5, we provide the detailed steps."
        },
        {
            "title": "7 EXPERIMENTAL RESULTS & ANALYSIS",
            "content": "Task Configurations. We apply our method to both Incorrect and Partial Correct categories, as the latter may still lack definitive answers. The test set consists of 10% of the total dataset from each domain. Evaluation employs fuzzy matching with temperature of 0 for strict assessment, classifying an answer as Chrono-correct only if the last candidate answer matches the object. 7.1 RESULTS OF CHROKNOWPROMPT Table 3 presents the effect of chronological prompting on knowledge elicitation across different models. Results are shown as the percentage of Known category, with parenthetical values indicating the increase by Chrono-correct. Significant improvements are observed in the biomedical domain (average increase of 11.9%), while general and legal domains show smaller gains (2.8% and 2.7%, 8 Preprint. Under review. Table 3: Result of ChroKnowPrompt. The order of open-sources LLM is sorted by release date, starting from the latest model to the most outdated model. The numeric score is the level of Known in chronological categorization and the increase in parentheses is from the ratio of Chrono-correct which was confusing Partial correct before. Each result presents both in total span and previous span. Models total span previous span total span previous span dynamic static dynamic static dynamic static dynamic static general biomedical Model Increase total span previous span Proprietary Large Language Models GPT4o-mini 28.7 (+7.7) 33.2 (+4.7) 26.6 (+5.7) 31.7 (+3.3) 51.9 (+23.0) 51.6 (+27.8) 41.8 (+12.8) 36.7 (+13.0) 15.8 8. Open-Source Large Language Models Phi3.5 Mini LLaMA3.1 Gemma2 Mistral v0.3 LLaMA3 Gemma SOLAR LLaMA2 17.3 (+2.1) 20.6 (+3.1) 19.6 (+4.0) 18.6 (+1.8) 20.9 (+2.7) 18.9 (+1.0) 16.5 (+0.8) 18.1 (+5.2) 25.5 (+2.5) 27.1 (+1.7) 26.7 (+2.3) 26.9 (+1.6) 28.0 (+1.7) 25.9 (+1.5) 24.9 (+0.9) 26.6 (+5.0) 16.5 (+1.2) 19.4 (+1.9) 17.8 (+2.2) 18.3 (+1.6) 20.8 (+2.5) 18.8 (+0.8) 16.7 (+1.1) 15.9 (+3.0) 24.1 (+1.1) 26.4 (+1.0) 24.7 (+0.4) 26.8 (+1.5) 27.2 (+0.9) 25.3 (+0.8) 25.1 (+1.1) 23.1 (+1.5) 45.4 (+18.7) 36.9 (+9.2) 32.5 (+6.2) 26.6 (+4.2) 31.4 (+5.7) 18.3 (+6.0) 26.5 (+4.1) 44.3 (+25.2) 41.3 (+20.3) 33.6 (+7.9) 31.7 (+9.0) 24.3 (+5.6) 25.7 (+3.8) 12.6 (+5.3) 20.3 (+4.5) 37.2 (+26.3) 36.6 (+10.0) 32.0 (+4.2) 27.9 (+1.5) 24.6 (+2.2) 28.7 (+3.0) 16.0 (+3.7) 27.7 (+5.3) 32.5 (+13.4) 31.5 (+10.5) 29.1 (+3.4) 26.7 (+4.1) 21.3 (+2.6) 24.2 (+2.3) 9.60 (+2.3) 19.7 (+3.8) 23.3 (+12.4) 10.9 5.5 5.4 3.3 3.5 3.5 2.6 15.4 5.7 2.6 2.1 2.0 2.2 1.9 2.8 7. Temporal Increase 3.2 2.4 2.2 1.3 11. 12.3 6.2 6.0 Domain Increase 2.8 1. 11.9 6.1 respectively). This indicates that chronological prompting effectively enhances knowledge recall without requiring external retrieval systems. In the legal domain, models struggle to update knowledge due to the complexity of unstructured, context-rich data. Among the models, proprietary GPT4omini perform best overall, while open-source models like Llama-2-7b-chat-hf also show notable improvements despite outdated. These findings suggest that chronological prompting can enhance both current and legacy models by leveraging temporal context. 7.2 EFFECTS OF CHRONOLOGICAL SPAN To elucidate the mechanisms of chronological prompting, we conduct detailed analysis across different settings, focusing on the impact of incorporating the next span in chronological contexts. Table 3 compares performance when using only the previous span versus the total span (both previous and next). Overall, the total span approach yield higher scores compared to using only the previous span. However, the degree of improvement varies by domain. In the biomedical domain, the total span results in nearly double the score increase compared to the previous span alone (11.9 vs. 6.1). Conversely, general domain shows modest increase (2.8 vs. 1.8). Model-specific temporal sensitivity also varies. For instance, Llama-2-7b-chat-hf demonstrates significant score increases by effectively utilizing temporal cues, particularly from next spans. On the other hand, SOLAR-10.7B-Instruct-v1.0 shows improvements when only the previous span was used, suggesting differing responses to temporal information across models. These findings indicate that models differ in their responsiveness to past and future temporal contexts. Another possible explanation is the variation in the temporal coverage of each models pretraining corpus, which may influence their temporal sensitivity and affect knowledge recall across different time frames."
        },
        {
            "title": "8 RELATED WORK",
            "content": "8.1 DERIVING AND EXPLORING KNOWLEDGE FROM LANGUAGE MODELS Since the emergence of LMs, deriving knowledge from language model is extensively studied, such as probing tasks (Hewitt & Manning, 2019), LAMA (Petroni et al., 2019) and BioLAMA (Sung et al., 2021). Then, many subsequent studies follows to exploit, (1) how LLMs define knowledge (Yu et al.; Zhang et al., 2023; Gottesman & Geva, 2024), (2) how these models represent it (Chen et al., 2024a;b; Wang et al., 2024d), and (3) how manipulate misleading part (Wang et al., 2023; Gutierrez et al., 2024; Wu et al., 2024a). Based on them, recent researches on investigating knowledge highlight the dynamic nature of evolving facts and suggests that contradictions within the training data may lead to knowledge conflicts (Marjanovic et al., 2024; Chang et al., 2024; Wang et al., 2024a; Xu et al., 2024; Jin et al., 2024). And knowledge overshadowing (Zhang et al., 2024b) reveals phenomena where certain conditions overshadow other facts, leading to misleading information (i.e., hallucinations). In other view point, exploring temporal knowledge starts from using Wikidata, static format of knowledge in triplet: subject, relation, and object, originated from extracting literature-based 9 Preprint. Under review. knowledge (Hahn-Powell et al., 2017). Following pioneers like TimeQA (Chen et al., 2021) and TemporalWiki (Jang et al., 2022a), many works dealing with temporal and continuous knowledge flow (Zhang & Choi, 2021; Dhingra et al., 2022; Jang et al., 2022b; Liska et al., 2022; Nylund et al., 2023; Zhu et al., 2023; Khodja et al., 2024; Zhang et al., 2024c) consist in line of it. Building upon their achievement, CarpeDiem (Kim et al., 2024) emerges to simply identify whether knowledge is outdated or not, and DyKnow (Mousavi et al., 2024) maps various models knowledge distribution. Also, (Zhao et al., 2024) makes dramatic work: align model into one fixed age. Though those impressive works, we try to broad the coverage of temporal knowledge. We utilize various templates to elicit the knowledge of LLMs, broaden the coverage of time stamps, and differentiate domains that should change based on temporal perspective with those that should remain constant."
        },
        {
            "title": "8.2 PARAMETRIC & NON-PARAMETRIC KNOWLEDGE UPDATE",
            "content": "Considering update of LLMs are in two types, parametric and non-parametric (Wang et al., 2024c), classical way of parametric update is using fine-tuning (Ghosal et al., 2024; Mecklenburg et al., 2024; Ge et al., 2024). While the method extended to LoRA (Hu et al.), QLoRA (Dettmers et al.), Melo (Yu et al., 2024) to continual learning (GRACE (Hartvigsen et al., 2024) and WISE (Wang et al., 2024b)), by using the parameter accessibility of white-box LLMs like Llama series (Touvron et al., 2023a), MEND (Mitchell et al.), ROME (Meng et al., 2022), MEMIT (Meng et al.) emerges. Those local editable methods are effective, and still try to improve specificity and generalizability. In contrast, for black-box LLMs, updates rely on non-parametric knowledge methods (Onoe et al., 2023), such as SERAC (Mitchell et al., 2022), MeLLo (Zhong et al., 2023), and IKE (Zheng et al., 2023). They align with two key trends: (1) Mitigating catastrophic forgetting, where the model loses previous knowledge, by not directly updating parameters. (2) Exploiting abilities of prominent black-box LLMs like GPT-o1 (OpenAI, 2024b) and Gemini (Team et al., 2023), as we cannot access to parameters. Another concern in knowledge update is they often focus only structured format, pointed out that current methods struggle to update unstrctured data effectively (Wu et al., 2024b). In this paper, we focus on non-parametric knowledge updates accomodating broad range of input formats (structured and unstructured) to represent knowledge across diverse domains, depending on the use of various white-box and black-box LLMs."
        },
        {
            "title": "9 CONCLUSION, LIMITATION, AND FUTURE WORK",
            "content": "Overall, our work highlights the critical role of temporal context in knowledge evaluation and introduces robust framework for improving the temporal capabilities of future language models. We present CHROKNOWBENCH, comprehensive benchmark designed to assess the temporal knowledge of large language models (LLMs) across diverse domains, time dependencies, and temporal states. Through our novel CHROKNOWLEDGE framework, we systematically evaluate and enhance the chronological knowledge of various LLMs, revealing that the effectivenesss of knowledge elicitation depends on the specific training data formats of each model. Our findings indicate that while models often associate facts with specific time stamps, they frequently exhibit partial recall or struggle to fully capture temporal boundaries. By implementing an in-depth chronological prompting, CHROKNOWPROMPT, we achieve significant improvements in knowledge recall, with the most notable increase seen in the biomedical domain. This non-parametric approach proves effective for both proprietary and open-source models, demonstrating its potential as practical method for updating and refining the temporal accuracy of LLMs without relying on external retrieval systems. While our approach demonstrates significant improvements in certain domains, it shows limited gains in general domain. This could be limitation of our non-parametric approach to effectively update knowledge within complex and context-rich fields. Specifically, domains like general and legal require more definitive updates that extend beyond the capabilities of prompt-based techniques alone. This underscores the need for practical knowledge updating and editing methods that are intrinsically aligned with temporal dynamics in line of parametric approach. To address this limitation, our future work will focus on developing methods for updating the parametric knowledge of open-source LLMs. Current knowledge update techniques do not adequately handle temporal aspects, which are essential for maintaining the accuracy and relevance of information over time. We aim to create comprehensive updates synchronized with various time stamps and 10 Preprint. Under review. explore how different temporal attributes affect both structured and unstructured data. By refining the temporal alignment of knowledge updates, we hope to enhance the LLMs ability to recall and integrate information accurately across different time frames, thereby overcoming the current limitations observed in domains that require deeper understanding of temporal complexity."
        },
        {
            "title": "REFERENCES",
            "content": "Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. Olivier Bodenreider. The unified medical language system (umls): integrating biomedical terminology. Nucleic acids research, 2004. Hoyeon Chang, Jinho Park, Seonghyeon Ye, Sohee Yang, Youngkyung Seo, Du-Seong Chang, and Minjoon Seo. How do large language models acquire factual knowledge during pretraining? arXiv preprint arXiv:2406.11813, 2024. Jianhao Chen, Haoyuan Ouyang, Junyang Ren, Wentao Ding, Wei Hu, and Yuzhong Qu. Timelinebased sentence decomposition with in context learning for temporal fact extraction. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Bangkok, Thailand, August 2024a. Association for Computational Linguistics. Wenhu Chen, Xinyi Wang, and William Yang Wang. dataset for answering time-sensitive questions. arXiv preprint arXiv:2108.06314, 2021. Yuheng Chen, Pengfei Cao, Yubo Chen, Yining Wang, Shengping Liu, Kang Liu, and Jun Zhao. Cracking factual knowledge: comprehensive analysis of degenerate knowledge neurons in large language models. 2024b. Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons in pretrained transformers. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, 2022. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. In Thirty-seventh Conference on Neural Information Processing Systems. Bhuwan Dhingra, Jeremy Cole, Julian Martin Eisenschlos, Dan Gillick, Jacob Eisenstein, and William Cohen. Time-aware language models as temporal knowledge bases. Transactions of the Association for Computational Linguistics, 10, 2022. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Bahare Fatemi, Mehran Kazemi, Anton Tsitsulin, Karishma Malkan, Jinyeong Yim, John Palowitch, Sungyong Seo, Jonathan Halcrow, and Bryan Perozzi. Test of time: benchmark for evaluating llms on temporal reasoning. arXiv preprint arXiv:2406.09170, 2024. Xiou Ge, Ali Mousavi, Edouard Grave, Armand Joulin, Kun Qian, Benjamin Han, Mostafa Arefiyan, and Yunyao Li. Time sensitive knowledge editing through efficient finetuning. arXiv preprint arXiv:2406.04496, 2024. Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, and Jonathan Herzig. Does fine-tuning llms on new knowledge encourage hallucinations? arXiv preprint arXiv:2405.05904, 2024. Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson. Dissecting recall of factual In Proceedings of the 2023 Conference on associations in auto-regressive language models. EMNLP, 2023. Preprint. Under review. Gaurav Rohit Ghosal, Tatsunori Hashimoto, and Aditi Raghunathan. Understanding finetuning for factual knowledge extraction. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), Proceedings of the 41st International Conference on Machine Learning, Proceedings of Machine Learning Research. PMLR, 2127 Jul 2024. Daniela Gottesman and Mor Geva. Estimating knowledge in large language models without generating single token. arXiv preprint arXiv:2406.12673, 2024. Bernal Jimenez Gutierrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, and Yu Su. Hipporag: Neurobiologically inspired long-term memory for large language models. arXiv preprint arXiv:2405.14831, 2024. Gus Hahn-Powell, Marco Valenzuela-Escarcega, and Mihai Surdeanu. Swanson linking revisited: Accelerating literature-based discovery across domains using conceptual influence graph. In Proceedings of ACL 2017, System Demonstrations, 2017. Tom Hartvigsen, Swami Sankaranarayanan, Hamid Palangi, Yoon Kim, and Marzyeh Ghassemi. Aging with grace: Lifelong model editing with discrete key-value adaptors. Advances in Neural Information Processing Systems, 2024. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob In International ConferSteinhardt. Measuring massive multitask language understanding. ence on Learning Representations, 2021. URL https://openreview.net/forum?id= d7KBjmI3GmQ. John Hewitt and Christopher Manning. structural probe for finding syntax in word representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019. Edward Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations. Filip Ilievski, Pedro Szekely, and Bin Zhang. Cskg: The commonsense knowledge graph. In The Semantic Web: 18th International Conference, ESWC 2021, Virtual Event, June 610, 2021, Proceedings 18. Springer, 2021. Joel Jang, Seonghyeon Ye, Changho Lee, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, and Minjoon Seo. Temporalwiki: lifelong benchmark for training and evaluating everevolving language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, 2022a. Joel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, Jungkyu Choi, and Minjoon Seo. Towards continual knowledge learning of language models. In 10th International Conference on Learning Representations, ICLR 2022. International Conference on Learning Representations, 2022b. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Zhuoran Jin, Pengfei Cao, Yubo Chen, Kang Liu, Xiaojian Jiang, Jiexin Xu, Li Qiuxia, and Jun Zhao. Tug-of-war between knowledge: Exploring and resolving knowledge conflicts in retrievalIn Proceedings of the 2024 Joint International Conference on augmented language models. Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), 2024. Jungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi, Ronan Le Bras, Akari Asai, Xinyan Velocity Yu, Dragomir Radev, Noah Smith, Yejin Choi, and Kentaro Inui. Realtime qa: whats the answer right now? In Proceedings of the 37th International Conference on Neural Information Processing Systems, 2023. 12 Preprint. Under review. Hichem Ammar Khodja, Frederic Bechet, Quentin Brabant, Alexis Nasr, and Gwenole Lecorve. Wikifactdiff: large, realistic, and temporally adaptable dataset for atomic factual knowledge update in causal language models. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), 2024. Dahyun Kim, Chanjun Park, Sanghoon Kim, Wonsung Lee, Wonho Song, Yunsu Kim, Hyeonwoo Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim, et al. Solar 10.7 b: Scaling large language models with simple yet effective depth up-scaling. arXiv preprint arXiv:2312.15166, 2023. Yujin Kim, Jaehong Yoon, Seonghyeon Ye, Sangmin Bae, Namgyu Ho, Sung Ju Hwang, and SeYoung Yun. Carpe diem: On the evaluation of world knowledge in lifelong language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), 2024. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model In Proceedings of the 29th Symposium on Operating Systems serving with pagedattention. Principles, pp. 611626, 2023. Adam Liska, Tomas Kocisky, Elena Gribovskaya, Tayfun Terzi, Eren Sezener, Devang Agrawal, DAutume Cyprien De Masson, Tim Scholtes, Manzil Zaheer, Susannah Young, et al. Streamingqa: benchmark for adaptation to new knowledge over time in question answering models. In International Conference on Machine Learning. PMLR, 2022. Potsawee Manakul, Adian Liusie, and Mark Gales. Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 90049017, 2023. Sara Vera Marjanovic, Haeun Yu, Pepa Atanasova, Maria Maistro, Christina Lioma, and Isabelle Augenstein. From internal conflict to contextual adaptation of language models. arXiv preprint arXiv:2407.17023, 2024. Nick Mecklenburg, Yiyou Lin, Xiaoxiao Li, Daniel Holstein, Leonardo Nunes, Sara Malvar, Bruno Silva, Ranveer Chandra, Vijay Aski, Pavan Kumar Reddy Yannam, et al. Injecting new knowledge into large language models via supervised fine-tuning. arXiv preprint arXiv:2404.00213, 2024. Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau. Mass-editing memory in transformer. In The Eleventh International Conference on Learning Representations. Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in gpt. Advances in Neural Information Processing Systems, 35, 2022. Meta. Introducing llama 3.1: Our most capable models to date. 2024. Abhika Mishra, Akari Asai, Vidhisha Balachandran, Yizhong Wang, Graham Neubig, Yulia Tsvetkov, and Hannaneh Hajishirzi. Fine-grained hallucination detection and editing for language models. arXiv preprint arXiv:2401.06855, 2024. Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher Manning. Fast model editing at scale. In International Conference on Learning Representations. Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher Manning, and Chelsea Finn. Memorybased model editing at scale. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, Proceedings of Machine Learning Research. PMLR, 1723 Jul 2022. Seyed Mahed Mousavi, Simone Alghisi, and Giuseppe Riccardi. Is your llm outdated? benchmarking llms & alignment algorithms for time-sensitive knowledge. arXiv preprint arXiv:2404.08700, 2024. Kai Nylund, Suchin Gururangan, and Noah Smith. Time is encoded in the weights of finetuned language models. arXiv preprint arXiv:2312.13401, 2023. 13 Preprint. Under review. Yasumasa Onoe, Michael Zhang, Shankar Padmanabhan, Greg Durrett, and Eunsol Choi. Can lms learn new entities from descriptions? challenges in propagating injected knowledge. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2023. OpenAI. Introducing chatgpt. 2022. OpenAI. Gpt-4o mini, advancing cost-efficient intelligence. 2024a. OpenAI. Openai o1 system card. 2024b. Fabio Petroni, Tim Rocktaschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 2019. Katherine Picho, Lauren Maggio, and Anthony Artino. Science: the slow march of accumulating evidence. Perspectives on medical education, 2016. Jennifer Rowley. The wisdom hierarchy: representations of the dikw hierarchy. Journal of information science, 2007. Mujeen Sung, Jinhyuk Lee, Yi Sean, Minji Jeon, Sungdong Kim, and Jaewoo Kang. Can language models be biomedical knowledge bases? In 2021 Conference on EMNLP 2021. Association for Computational Linguistics (ACL), 2021. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi`ere, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024a. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Leonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Rame, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024b. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. U.S. Government. Electronic code of federal regulations. URL https://www.ecfr.gov/. Denny Vrandeˇcic and Markus Krotzsch. Wikidata: free collaborative knowledgebase. Communications of the ACM, 2014. Jianing Wang. Math-kg: Construction and applications of mathematical knowledge graph. arXiv preprint arXiv:2205.03772, 2022. Mengru Wang, Yunzhi Yao, Ziwen Xu, Shuofei Qiao, Shumin Deng, Peng Wang, Xiang Chen, Jia-Chen Gu, Yong Jiang, Pengjun Xie, et al. Knowledge mechanisms in large language models: survey and perspective. arXiv preprint arXiv:2407.15017, 2024a. Peng Wang, Zexi Li, Ningyu Zhang, Ziwen Xu, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, and Huajun Chen. Wise: Rethinking the knowledge memory for lifelong model editing of large language models. arXiv preprint arXiv:2405.14768, 2024b. 14 Preprint. Under review. Peng Wang, Ningyu Zhang, Bozhong Tian, Zekun Xi, Yunzhi Yao, Ziwen Xu, Mengru Wang, Shengyu Mao, Xiaohan Wang, Siyuan Cheng, Kangwei Liu, Yuansheng Ni, Guozhou Zheng, and Huajun Chen. EasyEdit: An easy-to-use knowledge editing framework for large language models. In Yixin Cao, Yang Feng, and Deyi Xiong (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, August 2024c. Association for Computational Linguistics. Xiaohan Wang, Shengyu Mao, Ningyu Zhang, Shumin Deng, Yunzhi Yao, Yue Shen, Lei Liang, Jinjie Gu, and Huajun Chen. Editing conceptual knowledge for large language models. arXiv preprint arXiv:2403.06259, 2024d. Yike Wang, Shangbin Feng, Heng Wang, Weijia Shi, Vidhisha Balachandran, Tianxing He, and Yulia Tsvetkov. Resolving knowledge conflicts in large language models. arXiv preprint arXiv:2310.00935, 2023. Kevin Wu, Eric Wu, and James Zou. How faithful are rag models? quantifying the tug-of-war between rag and llms internal prior. arXiv preprint arXiv:2404.10198, 2024a. Xiaobao Wu, Liangming Pan, William Yang Wang, and Anh Tuan Luu. Updating language models with unstructured facts: Towards practical knowledge editing. arXiv preprint arXiv:2402.18909, 2024b. Rongwu Xu, Zehan Qi, Cunxiang Wang, Hongru Wang, Yue Zhang, and Wei Xu. Knowledge conflicts for llms: survey. arXiv preprint arXiv:2403.08319, 2024. Lang Yu, Qin Chen, Jie Zhou, and Liang He. Melo: Enhancing model editing with neuron-indexed dynamic lora. In Proceedings of the AAAI Conference on Artificial Intelligence, 2024. Qinan Yu, Jack Merullo, and Ellie Pavlick. Characterizing mechanisms for factual recall in language models. In The 2023 Conference on Empirical Methods in Natural Language Processing. David Zeigler. Evolution and the cumulative nature of science. Evolution: Education and Outreach, 2012. Michael Zhang and Eunsol Choi. Situatedqa: Incorporating extra-linguistic contexts into qa. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 73717387, 2021. Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, et al. comprehensive study of knowledge editing for large language models. arXiv preprint arXiv:2401.01286, 2024a. Yuji Zhang, Sha Li, Jiateng Liu, Pengfei Yu, Yi Fung, Jing Li, Manling Li, and Heng Ji. Knowledge overshadowing causes amalgamated hallucination in large language models. arXiv preprint arXiv:2407.08039, 2024b. Zhihan Zhang, Yixin Cao, Chenchen Ye, Yunshan Ma, Lizi Liao, and Tat-Seng Chua. Analyzing temporal complex events with large language models? benchmark towards temporal, long context understanding. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Bangkok, Thailand, August 2024c. Association for Computational Linguistics. Zihan Zhang, Meng Fang, Ling Chen, Mohammad-Reza Namazi-Rad, and Jun Wang. How do large language models capture the ever-changing world knowledge? review of recent advances. In Proceedings of the 2023 Conference on EMNLP, pp. 82898311, 2023. Bowen Zhao, Zander Brumbaugh, Yizhong Wang, Hannaneh Hajishirzi, and Noah Smith. Set the clock: Temporal alignment of pretrained language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics ACL 2024, Bangkok, Thailand and virtual meeting, August 2024. Association for Computational Linguistics. Ce Zheng, Lei Li, Qingxiu Dong, Yuxuan Fan, Zhiyong Wu, Jingjing Xu, and Baobao Chang. Can we edit factual knowledge by in-context learning? In Proceedings of the 2023 Conference on EMNLP, 2023. 15 Preprint. Under review. Danna Zheng, Mirella Lapata, and Jeff Pan. Large language models as reliable knowledge bases? arXiv preprint arXiv:2407.13578, 2024. Zexuan Zhong, Zhengxuan Wu, Christopher Manning, Christopher Potts, and Danqi Chen. MQuAKE: Assessing knowledge editing in language models via multi-hop questions. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Singapore, December 2023. Association for Computational Linguistics. Yuxuan Zhou, Xien Liu, Chen Ning, and Ji Wu. Multifaceteval: Multifaceted evaluation to probe llms in mastering medical knowledge. In Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI-24. International Joint Conferences on Artificial Intelligence Organization. doi: 10.24963/ijcai.2024/737. URL https://doi.org/10.24963/ijcai. 2024/737. Xinyu Zhu, Cheng Yang, Bei Chen, Siheng Li, Jian-Guang Lou, and Yujiu Yang. Question answering as programming for solving time-sensitive questions. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 2023."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 DETAILS OF ELICITING KNOWLEDGE: FEW-SHOT EXEMPLARS, FUZZY MATCHING RULES, AND EXAMPLES OF TWO TEMPLATES A.1.1 FEW-SHOT EXEMPLARS To obtain the few-shot exemplar pool D, we leverage additional data collected using the same process as in CHROKNOWBENCH. Specifically, for each individual relation type, We gather four exemplars for the general domain, and eight for the biomedical and legal domains. We then generate the few-shot exemplar set Di by sampling four exemplars from D, which serves as actual demonstrations within prompt. This process is repeated for every timestamp to ensure comprehensive temporal coverage. A.1.2 FUZZY MATCHING We utilize the rapidfuzz library to compare the models responses with the predefined labels. As the models answer may little bit different with complicated objects in specialized domains, such as the difference in order of words or upper and lower cases, using fuzzy match enables more rapid but still reliable quality without facilitating external NLI mechanisms. Specifically, we employ token set ratio metric with threshold value set to 70 to determine match. token set ratio is metric used for comparing the similarity of two strings in flexible manner, extending the functionality of the token sort ratio. In the preprocessing stage, the strings undergo tokenization, removal of punctuation, and conversion to lowercase. The tokens are then sorted in alphanumeric order before the similarity ratio is computed. This makes it useful for comparing strings where the word order may differ but the content is similar. The key distinction of token set ratio lies in its incorporation of set operations, where duplicate words are removed. After eliminating repeated tokens, the same preprocessing steps as in token sort ratio are applied. When performing the comparison, the method checks if all tokens from the shorter string are contained within the longer string, making the approach particularly suited for cases where one string is subset of the other. This flexible matching often results in higher accuracy for comparing strings with similar content but different structures, as illustrated by the example where score of 100 is achieved when all tokens from the second string are present in the first. A.1.3 EXAMPLES OF TWO TEMPLATES We provide two templates of Generation and MCQA in the end of the Appendix for the better readability. For example, in Table 5, our target year is 2020 (t) to generate answer candidate of position held (r) by Donald Tusk (s). 16 Preprint. Under review. A.1."
        },
        {
            "title": "ITERATIVE DISTRACTOR GENERATION",
            "content": "For the Commonsense dataset, the objects corresponding to given subject and relation are often ambiguous. When constructing compelling distractors, there is higher likelihood (about 20%) of creating options that are actually correct answers rather than intended incorrect ones, compared to other datasets. Therefore, we include an additional verification process after generating the distractors, as outlined in Algorithm 1. Specifically, we formulate multiple-choice questions using the problem and the generated distractors, then use GPT-4o to select all correct answers. If it identifies more than one correct answer, we refine the distractors based on prompt to recreate incorrect options. Algorithm 1: Iterative Distractor Generation Algorithm Data: Subject s, Relation r, Set of correct objects Ocorrect Result: Refined multiple-choice question 1 Initialize conversation history 2 Initialize number of selected options 1 3 while > 0 do 4 LLMResponse(s, r, H) ComposeQuestion(s, r, D) Append to LLMResponse(q) LLMResponse() if > 0 then // Number of options selected by LLM // Generate three incorrect options // Compose question using the generated distractors // LLM generates response by solving the question // Extract selected options using LLM CreatePrompt(s, r) Append to // Add regeneration prompt to the conversation history Dnew LLMResponse(s, r, H) D[S] Dnew[1 : n] // Generate new set of distractors // Replace selected options // Create another prompt for regenerating distractors 5 6 7 8 10 11 12 13 14 15 return A.2 DETAILS OF OUR SOURCE OF BENCHMARK DATASET A.2.1 BIOMEDICAL DOMAIN In the biomedical domain, we follow previous work of BIOLAMA (Sung et al., 2021) framework to parse Unified Medical Language System (UMLS) yearly metathesaurus data. In the range of 2020 to 2024, we gather instances in 14 relations, resulting 7k for each dynamic and static dataset. Here, by considering domain specificity that the slow pace of change typical of long-term research, the object pool is slightly expanded or narrowed in that period; Autonomic nerve structure, with the relation has indirect procedure site, has slightly broader scope in 2024, including additional objects like Neurolytic autonomic nerve block alongside previous objects such as Intravenous regional autonomic block. The format is same with general domain, {s, r, o, t} quadruplet. A.2.2 LEGAL DOMAIN In the legal domain, we create benchmark dataset based on the Code of Federal Regulations (CFR) from 2010 to 2023. We first extract paragraph-level data from regulatory documents for each year and employ Pythons difflib library to detect changes between paragraphs across adjacent years (e.g., 2011 to 2012). Careful filtering is applied to ensure that only paragraphs with minor modifications (e.g., single-word updates or subtle phrasing changes) are retained. To further analyze the dataset, we utilize the spaCy en core web lg model to detect named entities in the paragraphs and assess whether these changes involve modifications to the detected entities. Despite noise introduced by the NER model, we initially identify around 56K changes for near-year comparisons. These changes are grouped into sequences of years to track alterations over time, while filtering out paragraphs that are introduced or removed in intermediate years. Ultimately, we focus on paragraphs present in all years between 2010 and 2023, resulting in 8,793 paragraphs. We then apply GPT-4o-mini to assess whether the detected changes are semantically meaningful, excluding minor corrections like typographical fixes or abbreviations. This results in refined set of 4,362 meaningful updates. Additionally, we select 4,746 unchanged paragraphs containing entities 17 Preprint. Under review. detected by the NER model. For each paragraph, we format the changes as fill-in-the-blank tasks, where the modified part is replaced with blank, providing rich resource for studying legal text evolution over time. A.2.3 COMMONSENSE AND MATHEMATICS In the commonsense domain, we utilized the CSKG dataset presented in the CSKG paper. Unlike the BIO dataset, the object lists for each triplet in this dataset consist of synonymous terms, allowing multiple triplets to share the same subject and relation. In such cases, the objects appearing in each triplet carry distinct meanings. Out of the 6 million triplets, we merged the objects of triplets that have the same subject and relation into single set, and then sampled number of triplets from this collection. In the mathematics and data structure/algorithm domain, we utilized the Math-KG dataset introduced in the Math-KG paper. This dataset, originally in Chinese, stores multiple objects with the same subject and relation across different triplets. Each object was translated into English using GPT-4, after which the objects from triplets sharing the same subject and relation were merged to construct final dataset consisting of 22k triplets. For the CommonSense dataset, the answers (objects) corresponding to given subject and relation are often ambiguous. Consequently, when constructing compelling distractors, there is higher likelihood (about 20%) of creating options that are actually correct answers rather than intended incorrect ones, compared to other datasets. Therefore, we include an additional verification process after generating the distractors, as outlined in Algorithm 2. Specifically, we formulate multiple-choice questions using the problem and the generated distractors, then ask GPT-4o to select all correct answers. If it identifies more than one correct answer, we refine the distractors based on prompt to recreate incorrect options. A.3 INFERENCE SETTING We evaluates all models using vLLM (Kwon et al., 2023) system, supporting features of efficient KV cache memory management that dramatically decrease inference time. All white box LM inference is conducted by vLLM with hyper-parameter: BFloat16, fixed seed, two kinds of temperature based on each sampling setting(greedy decoding with 0.0, and high temperature with 0.7). The precision is done with eight NVIDIA A100 GPUs(80GB). A.4 TENDENCY OF LEGAL DOMAIN AND TIME-INVARIANT KNOWLEDGE Figure 6 shows the result of legal domain. Among time variant domains, legal domain shows the most stable results of static, also minimal decline in dynamic dataset, especially generation setting. This indicates the domain specificity, which has less frequent yearly changes and change continues across many time stamps. In the template wise viewpoint, however, MCQA templates exhibit cutoffs between 2015 and 2016, suggesting selection tasks are more sensitive in this domains temporal boundaries. Also, models capability for task is still influential in legal domain like the result of gemma-2-9b-it and SOLAR-10.7B-instruct-v1.0 shows, but the parametric knowledge seems more important here as small model cannot overcome the gap by technical skills, like Phi-3.5-mini-instruct. For commonsense and mathematics in Figure 7, arbitrary years based on the biomedical domain were used, from 2020 to 2024. The left side of result shows the tendency of generation templates, and the right side is the tendency of MCQA templates, in both domains. Results show minimal variation, aligning with the stable nature of these knowledge types. This consistency confirms that time-invariant knowledge is well-preserved across models. For template wise comparison, generation cases show way little gap between models, while MCQA tasks show the different between models, which is aligned with the findings from other time variant domains: the ability of each models specialized task affects its knowledge recall ability. About the overall performance quality, the result of time invariant shows lower performance as the model show generate appropriate object in each knowledge, even though the time invariant knowledges coverage is wider than other domains. This tendency is alleviated by using MCQA templates, which ends of the rationale for using multiple templates. 18 Preprint. Under review. Figure 6: Performance analysis of legal domain. Among time variant domains, legal domain shows the most stable results of static, besides, there still exists fluctuation and steep cutoff in dynamic dataset. When it comes to each templates, generation shows less variation among models, while MCQA setting shows cut-off in many models between 2015 and 2016. It is also observed that in this domain, models capability of task is still influential such as the result of gemma-2-9b-it or SOLAR-10.7B-instruct-v1.0, but the parametric knowledge seems more important as small model like Phi-3.5-mini-instruct cannot overcome the gap. Figure 7: Performance analysis of common-sense and mathematics domains. All model shows clearly the domain specific characteristics, which is invariant knowledge even it comes with temporal attributes. A.5 ALGORITHM OF CHROKNOWPROMPT The overall scheme of ChroKnowPrompt is down below. As described in Section 6.2, the algorithm starts from making initial prompt with target time tn, subject sn and relation rn from target triplet. As the initialized candidate answer is None that model cannot properly answer for that target year, the algorithm also starts with make empty list of candidate answer list A. and accumulated prompt P. Then, the algorithm checks the correct object within each time span and . If one of those 19 Preprint. Under review. span has no correct object, the algorithm passes that side of traversal. It the preparation is all done, the first step in previous span (if no previous span exists, the nearest next span) begins with selecting object ˆo by majority voting. Appending prompts in each step, the model is asked to generate or verify and refine the answer of each step, like in Figure 5. After all step is done, the last candidate answer, which is the most refined result, is being checked with the original target object on coming from target triplet. If it is matched (we also used fuzzy match in here), the category of Incorrect is updated to Chrono-correct. Algorithm 2: Chronological Prompting Algorithm Data: Correct set = {(ti, ci)}, target time t, triplet (s, r, o), Prev span , Next span Result: List of candidate answers A, Updated Category 1 Initialize accumulated prompt 2 Initialize candidate answer 3 Initialize candidate answer list 4 Tprev time before in up to span 5 Tnext time after in up to span // Find correct object in next time 6 if Tprev = then 7 Skip backward traversal and process next years only // Find correct object in previous time 8 if Tnext = then Skip forward traversal and process previous years only ˆo MajorityVote(C(tp)) 10 for tp Tprev do 11 12 PromptAugment(tp, t, s, r, P, ˆo, a, previous) 13 // Process previous years first // Get the correct object by majority voting // Generate or verify answer based on system prompt // Augment prompt by adding above anew LLMResponse(M) aext ExtractAnswer(anew) if aext = and aext = then aext Append to Update accumulated prompt with anew LLMResponse(M) aext ExtractAnswer(anew) if aext = and aext = then aext Append to Update accumulated prompt with 15 16 17 18 23 25 26 27 ˆo MajorityVote(C(tp)) 19 for tn Tnext do 20 21 PromptAugment(tn, t, s, r, P, ˆo, a, next) 22 // Process next years after previous years // Get the correct object by majority voting // Generate or verify answer based on system prompt // Augment prompt by adding below 28 if ai A, ai = then 29 Update knowledge categorization to Chrono-Correct 30 return A, Updated Category A.6 EVALUATION OF CHROKNOWPROMPT IN LEGAL DOMAIN Table 4 shows the evaluation of ChroKnowPrompt in legal domain. While our approach demonstrates significant improvements in certain domains, it shows limited or negligible gains in the legal domain. Overall score in dynamic dataset is mimic, as the highest gain is 1.9 increasing. However, the increase in static dataset is still impressive as the highest increase is comparable level as much as biomedical domains results (more than 10 in the proprietary model). Another finding is that although the increase in Table 3s result in general domain is not higher than the static figures in the legal domain, the variation in figures between models is significantly larger in the legal domain. As the format of legal dataset is the unstructured format with long context, this would be one factor of low edit quality. 20 Preprint. Under review. Table 4: Result of ChroKnowPrompt in Legal domain. The order of open-source LLMs follows the same sequence as in Table 3, starting with the latest model and progressing to the most outdated one. The numeric score represents the level of Known in chronological categorization, and the increase in parentheses reflects the ratio of Chrono-correct answers, considering total span in the left side and previous span in the right side. Models total span previous span dynamic static dynamic static total span previous span legal Model Increase Proprietary Large Language Models GPT4o-mini 3.2 (+1.9) 51.9 (+14.1) 2.6 (+1.3) 48.4 (+10.6) 8.0 Open-Source Large Language Models Phi3.5 Mini LLaMA3.1 Gemma2 Mistral v0.3 LLaMA3 Gemma SOLAR LLaMA2 0.6 (+0.3) 0.3 (+0.0) 2.9 (+0.6) 1.3 (+0.6) 1.0 (+0.3) 0.3 (+0.0) 0.6 (+0.0) 0.3 (+0.0) 14.2 (+4.5) 13.8 (+1.3) 44.6 (+2.6) 21.1 (+7.0) 18.9 (+0.6) 8.70 (+0.0) 26.8 (+1.3) 21.8 (+12.8) 0.6 (+0.3) 0.3 (+0.0) 2.6 (+0.3) 1.0 (+0.3) 1.3 (+0.6) 0.3 (+0.0) 0.6 (+0.0) 0.3 (+0.0) 11.9 (+2.3) 12.5 (+0.0) 43.9 (+1.9) 19.2 (+5.1) 18.9 (+0.6) 8.70 (+0.0) 28.4 (+2.9) 17.3 (+8.3) 2.4 0.7 1.6 3.8 0.5 0.0 0.7 6. Temporal Increase 0.4 4.9 0.3 3.5 Domain Increase 2.7 1.9 6.0 1.3 0.0 1.1 2.7 0.6 0.0 1.5 4.2 21 Preprint. Under review. Template: Generation Q. In 2020, Pedro Braillard Poccard, position held, [Object] A. member of the Argentine Chamber of Senators Q. In 2020, Ana Santos Aramburo, position held, [Object] A. Spain National Library general manager Q. In 2020, James E. McPherson, position held, [Object] A. United States Secretary of the Navy Q. In 2020, Jesus Avila de Grado, position held, [Object] A. chief scientific officer Q. In 2020, Donald Tusk, position held, [Object] : (generate from here ...) Template: MCQA In 2020, what office does Pedro Braillard Poccard hold? (a) member of the Argentine Chamber of Senators, (b) Minister of Foreign Affairs, (c) Governor of Corrientes Province, (d) Mayor of Buenos Aires (a) member of the Argentine Chamber of Senators In 2020, what office does Ana Santos Aramburo hold? (a) Minister of Culture and Sports of Spain, (b) Director of the Prado Museum, (c) Spain National Library general manager, (d) President of the Spanish Royal Academy (c) Spain National Library general manager In 2020, what office does James E. McPherson hold? (a) United States Secretary of Homeland Security, (b) United States Attorney General, (c) United States Secretary of the Navy, (d) United States Secretary of Defense (c) United States Secretary of the Navy In 2020, what office does Jesus Avila de Grado hold? (a) President of the National Research Council, (b) Minister of Health, (c) Director of the World Health Organization, (d) chief scientific officer (d) chief scientific officer In 2020, what office does Donald Tusk hold? (a) President of the European Commission, (b) President of Poland, (c) Chancellor of Germany, (d) chairperson : (generate from here ...) Table 5: Example of two templates: Generation and MCQA. Domain: general, Subject: Donald Tusk, Relation: position held (P39), year to target: 2020. 22 Preprint. Under review. Domain: General [System] You are an expert in natural language processing and logic puzzles, skilled at generating plausible yet misleading distractor options that challenge users to distinguish between correct and incorrect answers. Pay special attention to questions that involve negative phrasing, such as those containing not or which does not, to ensure that the distractors do not confuse users into overlooking the negative aspect of the question. [User] The question What office does Mitt Romney hold? can be answered with United States senator. Create three plausible incorrect distractors for this question. [Assistant] 1. Governor of Massachusetts 2. Secretary of State 3. Speaker of the House [User] The question Which sports team is Yann MVila member of? can be answered with Rubin Kazan, France national association football team, Sunderland A.F.C., France national under-21 association football team, Inter Milan, Stade Rennais F.C.. Create three plausible incorrect distractors for this question. [Assistant] 1. Paris Saint-Germain F.C. 2. Olympique Lyonnais 3. AS Monaco FC [User] The question [Q] can be answered with [O1], [O2], ... , [On]. Create three plausible incorrect distractors for this question. [Assistant] : (generate from here ...) Table 6: 2-shot prompt for generating three distractors in general domain. Preprint. Under review. Domain: Biomedical [System] You are an expert in natural language processing and logic puzzles, skilled at generating plausible yet misleading distractor options that challenge users to distinguish between correct and incorrect answers. Pay special attention to questions that involve negative phrasing, such as those containing not or which does not, to ensure that the distractors do not confuse users into overlooking the negative aspect of the question. [User] The question What is not the primary anatomic site of Rhabdomyosarcoma of the orbit? can be answered with Bones set, structure bone, Bone structure, os, Skeletal bone, Bone structure (body structure), bones structure, bone structures, Ossa, Set of bone organs, Bone organ, Skeleton system, Osseous, Bone, NOS, Bones, bones structures, Bone (organ), Bone, skeletal, Set of bones, Bones, NOS. Create three plausible incorrect distractors for this question. [Assistant] 1. Orbital connective tissue 2. Extrinsic ocular muscles 3. Orbital soft tissue (two more shots) [User] The question What negatively regulates Vitellogenesis? can be answered with downregulation of yolk production, downregulation of vitellogenesis, down regulation of vitellogenesis, inhibition of yolk production, negative regulation of vitellogenesis, down regulation of yolk production, down-regulation of yolk production, inhibition of vitellogenesis, down-regulation of vitellogenesis, negative regulation of yolk production. Create three plausible incorrect distractors for this question. [Assistant] 1. Partial left salpingectomy 2. Unilateral oophorectomy 3. Hysterectomy [User] The question [Q] can be answered with [O1], [O2], ... , [On]. Create three plausible incorrect distractors for this question. [Assistant] : (generate from here ...) Table 7: 4-shot prompt for generating three distractors in biomedical domain. Preprint. Under review. Domain: Legal [System] You are legal expert skilled in crafting challenging fill-in-the-blank questions and generating plausible yet misleading distractor options. You will receive question and answer where part of legal text has been blanked out. For the provided question, create three plausible incorrect distractors that challenge users to distinguish between correct and incorrect answers. [User] The question \"\"\" ... (beginning of text omitted) Subpart BCertification of Substantially Equivalent Agencies Substantial equivalency certification is granted if the determines that state or local agency enforces law that is substantially equivalent to the Fair Housing Act with regard to substantive rights, procedures, remedies, and the availability of judicial review. The Department has developed two-phase process of substantial equivalency certification. \"\"\" can be answered with Department. Create three plausible incorrect distractors for this question. [Assistant] 1. Secretary 2. Commission 3. Board [User] The question \"\"\" ... (beginning of text omitted) Subpart BMinimum Standards for Substantial Compliance by States Within the period defined in 383.73( \"\"\" can be answered with f) of this title, the State shall, h) of this subchapter, the State must. Create three plausible incorrect distractors for this question. : [Assistant] 1. g) of this chapter, the State will 2. e) of this section, the State is required to 3. d) of this part, the State should [User] The question \"\"\" [Q] \"\"\" can be answered with [O1], [O2], ... , [On]. Create three plausible incorrect distractors for this question. [Assistant] : (generate from here ...) Table 8: 2-shot prompt for generating three distractors in legal domain. Preprint. Under review. Domain: CommonSense [System] You are an expert in natural language processing and logic puzzles, skilled at generating plausible yet misleading distractor options that challenge users to distinguish between correct and incorrect answers. Pay special attention to questions that involve negative phrasing, such as those containing not or which does not, to ensure that the distractors do not confuse users into overlooking the negative aspect of the question. [User] The question What properties does human not have? can be answered with strong enough to lift horse, strong enough to lift truck, strong enough to lift car, strong enough to lift cow, strong enough to lift building. Create three plausible incorrect distractors for this question. [Assistant] 1. strong enough to lift motorcycle 2. strong enough to lift large suitcase 3. strong enough to lift heavy stone [User] The question What is synonym for salix tristis? can be answered with dwarf gray willow. Create three plausible incorrect distractors for this question. [Assistant] 1. green weeping willow 2. dwarf green willow 3. gray weeping willow [User] The question What does kids desire? can be answered with play games, play, find crabs in tidal pools, read games magazines, play with, play with toys, learn things. Create three plausible incorrect distractors for this question. [Assistant] 1. eat vegetables 2. do homework 3. fold laundry [User] The question What is oil and water not capable of? can be answered with mix. Create three plausible incorrect distractors for this question. [Assistant] 1. boil 2. freeze 3. emulsify [User] The question [Q] can be answered with [O1], [O2], ... , [On]. Create three plausible incorrect distractors for this question. [Assistant] : (generate from here ...) Table 9: 4-shot prompt for generating three distractors in commonsense domain. 26 Preprint. Under review. Prompt: Mathematics / Data Structure & Algorithm [System] You are an expert in mathematics and computer science, skilled at generating plausible yet misleading distractor options that challenge users to distinguish between correct and incorrect answers. Pay special attention to questions that involve negative phrasing, such as those containing not or which does not, to ensure that the distractors do not confuse users into overlooking the negative aspect of the question. [User] The question Which of the following is unrelated to Output? can be answered with Left child, Set, Depth, Modify, Post-order traversal, Dictionary, Predecessor, Array, Delete, Sparse matrix, Infix expression, Leaf node, Shortest path, Right subtree, Node, In-order traversal, Hashing, Head node, Pointer, Level-order traversal, Keyword, Pattern string, Record, Determinism, Linked list, Critical Path, Function name, Connected component, Loop statement, Robustness, Inverted index, Preorder traversal, Memory, Dequeue, Singly Linked List, Image. Create three plausible incorrect distractors for this question. [Assistant] 1. Print statement 2. Display buffer 3. Output stream [User] The question What is synonym for Ancestor? can be answered with Parent. Create three plausible incorrect distractors for this question. [Assistant] 1. Descendant 2. Sibling 3. Offspring [User] The question [Q] can be answered with [O1], [O2], ... , [On]. Create three plausible incorrect distractors for this question. [Assistant] : (generate from here ...) Table 10: 2-shot prompt for generating three distractors in mathematics, data structure, and algorithm domain."
        }
    ],
    "affiliations": [
        "AIGEN Sciences",
        "Korea University",
        "Upstage AI"
    ]
}