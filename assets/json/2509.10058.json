{
    "paper_title": "Color Me Correctly: Bridging Perceptual Color Spaces and Text Embeddings for Improved Diffusion Generation",
    "authors": [
        "Sung-Lin Tsai",
        "Bo-Lun Huang",
        "Yu Ting Shen",
        "Cheng Yu Yeo",
        "Chiang Tseng",
        "Bo-Kai Ruan",
        "Wen-Sheng Lien",
        "Hong-Han Shuai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Accurate color alignment in text-to-image (T2I) generation is critical for applications such as fashion, product visualization, and interior design, yet current diffusion models struggle with nuanced and compound color terms (e.g., Tiffany blue, lime green, hot pink), often producing images that are misaligned with human intent. Existing approaches rely on cross-attention manipulation, reference images, or fine-tuning but fail to systematically resolve ambiguous color descriptions. To precisely render colors under prompt ambiguity, we propose a training-free framework that enhances color fidelity by leveraging a large language model (LLM) to disambiguate color-related prompts and guiding color blending operations directly in the text embedding space. Our method first employs a large language model (LLM) to resolve ambiguous color terms in the text prompt, and then refines the text embeddings based on the spatial relationships of the resulting color terms in the CIELAB color space. Unlike prior methods, our approach improves color accuracy without requiring additional training or external reference images. Experimental results demonstrate that our framework improves color alignment without compromising image quality, bridging the gap between text semantics and visual generation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 8 5 0 0 1 . 9 0 5 2 : r Color Me Correctly: Bridging Perceptual Color Spaces and Text Embeddings for Improved Diffusion Generation Sung-Lin Tsai National Yang Ming Chiao Tung University Hsinchu, Taiwan tsai412504004.ee12@nycu.edu.tw Cheng-Yu Yeo National Yang Ming Chiao Tung University Hsinchu, Taiwan boyyeo123.ee12@nycu.edu.tw Bo-Lun Huang National Yang Ming Chiao Tung University Hsinchu, Taiwan kevin503.ee12@nycu.edu.tw Chiang Tseng National Yang Ming Chiao Tung University Hsinchu, Taiwan chiang.ee11@nycu.edu.tw Yu-Ting Shen National Yang Ming Chiao Tung University Hsinchu, Taiwan yuting89830.cs11@nycu.edu.tw Bo-Kai Ruan National Yang Ming Chiao Tung University Hsinchu, Taiwan bkruan.ee11@nycu.edu.tw Wen-Sheng Lien National Yang Ming Chiao Tung University Hsinchu, Taiwan vincentlien.ii13@nycu.edu.tw Hong-Han Shuai National Yang Ming Chiao Tung University Hsinchu, Taiwan hhshuai@nycu.edu.tw Figure 1: Examples of prompt-induced ambiguity. Top: baseline T2I diffusion (SynGen) outputs misinterpret color terms. Bottom: our disambiguation-guided method resolves these issues, yielding more accurate, semantically aligned results. Contribute equally to this work. Corresponding author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. MM 25, Dublin, Ireland. Abstract Accurate color alignment in text-to-image (T2I) generation is critical for applications such as fashion, product visualization, and interior design, yet current diffusion models struggle with nuanced and compound color terms (e.g., Tiffany blue, baby pink), often producing images that are misaligned with human intent. Existing approaches rely on cross-attention manipulation, reference images, 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-2035-2/2025/10 https://doi.org/10.1145/3746027.3755369 MM 25, October 2731, 2025, Dublin, Ireland. Sung-Lin Tsai et al. or fine-tuning but fail to systematically resolve ambiguous color descriptions. To precisely render colors under prompt ambiguity, we propose training-free framework that enhances color fidelity by leveraging large language model (LLM) to disambiguate colorrelated prompts and guiding color blending operations directly in the text embedding space. Our method first employs large language model (LLM) to resolve ambiguous color terms in the text prompt, and then refines the text embeddings based on the spatial relationships of the resulting color terms in the CIELab color space. Unlike prior methods, our approach improves color accuracy without requiring additional training or external reference images. Experimental results demonstrate that our framework improves color alignment without compromising image quality, bridging the gap between text semantics and visual generation. All supplementary materials are available at https://Sung-Lin.github.io/TintBench/. CCS Concepts Information systems Multimedia content creation. Keywords color disambiguation; diffusion model; training-free ACM Reference Format: Sung-Lin Tsai, Bo-Lun Huang, Yu-Ting Shen, Cheng-Yu Yeo, Chiang Tseng, Bo-Kai Ruan, Wen-Sheng Lien, and Hong-Han Shuai. 2025. Color Me Correctly: Bridging Perceptual Color Spaces and Text Embeddings for Improved Diffusion Generation. In Proceedings of the 33rd ACM International Conference on Multimedia (MM 25), October 2731, 2025, Dublin, Ireland. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3746027."
        },
        {
            "title": "1 Introduction\nColor plays a crucial role in visual perception and aesthetics, signifi-\ncantly impacting domains such as fashion [10], product design [12],\nin-house decoration [23], and digital art [18]. Accurate color align-\nment is essential for applications where color fidelity determines\nthe outcome, such as e-commerce visualization, where customers\nexpect generated product previews to match real-world colors pre-\ncisely. Similarly, in virtual interior design, users may specify com-\nplex color schemes for furniture or walls, and any misinterpretation\ncan lead to unrealistic renderings. However, current text-to-image\n(T2I) diffusion models often struggle with accurately rendering\nhues, particularly when interpreting compound color names such\nas lime green, which can lead to imprecise color reproduction (e.g.,\ngenerating plain green) or object-color lexical confusion (e.g., gen-\nerating limes).",
            "content": "To better address this issue, several approaches have been proposed to address color alignment in T2I generation. Early works such as Attend-and-Excite [5] and Divide & Bind [13] explore cross-attention manipulation to enforce more faithful text-image correspondence, including color attributes. More recent methods introduce training-free solutions for color control. For example, ColorEdit [26] applies reference-based color adjustments using cross-attention feature alignment, while Color-Style Disentanglement [1] leverages CIELab space feature separation to isolate color properties from luminance and style. Fine-tuning-based methods such as ColorPeel [3] train diffusion models to internalize new color embeddings, enabling precise color reproduction for specific shades. Meanwhile, LLM-based guidance frameworks like LLMgrounded Diffusion [14] improve compositional reasoning in T2I synthesis but do not explicitly address color fidelity. While these approaches make significant strides, they often rely on reference images [1, 26], training-intensive pipelines [3], or indirect color manipulations [5, 13, 14] that fail to resolve ambiguous or underspecified color descriptions systematically. Specifically, major challenge in addressing color ambiguity lies in interpreting complex color expressions and compound descriptors within natural language prompts. Many such expressionse.g., cerulean blue, dusty rose, or warm taupelack precise semantic grounding and may be interpreted differently depending on context or model internalization. While large diffusion models are trained on vast web data, they often struggle to render accurate colors when prompts include fine-grained or less conventional color names. This difficulty stems not from limited data, but from the semantic variability and ambiguity inherent in human color language, which is rarely standardized. As result, text-to-image models often produce outputs that diverge from user intent when color terms are complex or linguistically nuanced. As illustrated in Fig. 1, text-to-image diffusion models frequently produce incorrect generations when the input prompt contains ambiguous or compound color terms. These failures typically arise from the models misinterpretation of modifiers or the holistic blending of complex expressions. For example, in the second column, the modifier dark leads the model to apply globally darker tone, diverging from the intended object-specific coloration. In the third column, the term jungle green is misunderstood, prompting the model to insert jungle-like visual elements into the background instead of applying the correct hue. This motivates the need for more explicit disambiguation strategies that can bridge the gap between human color semantics and learned visual representations. In this paper, we propose novel approach to enhance color fidelity in T2I diffusion models via Semantic Color Disambiguation with LLMs and Retrieval-Based Embedding Refinement for Color Representation. Our method consists of two stages: First, an LLM refines ambiguous color descriptions by translating them into explicit, unambiguous terms for clearer intent interpretation. Second, we introduce retrieval-based embedding refinement that interpolates between nearby basic color embeddings to yield more precise target color representation. By mapping these interpolated terms to the CIELab space, we use surrogate color templates for numerical interpolation. This enables smooth, controllable color blending and embeddings that closely match the intended shade. Our approach integrates semantic understanding with visual accuracy, ensuring high-fidelity color rendering while preserving textual input. Unlike prior methods requiring reference images or tuning cross-attention, ours directly improves color awareness in training-free manneryielding significantly better color consistency and realism. Our main contributions are summarized as follows. We identify critical challenges in achieving accurate color alignment in T2I synthesis, particularly with novel color terms. new benchmark is established to systematically assess color fidelity across various T2I models, setting standard for future evaluations. Color Me Correctly: Bridging Perceptual Color Spaces and Text Embeddings for Improved Diffusion Generation MM 25, October 2731, 2025, Dublin, Ireland. The method introduces two key modules: Semantic Color Disambiguation with LLMs to disambiguate color terms, enhancing semantic clarity; and Retrieval-Based Embedding Refinement for Color Representation, which applies semantic arithmetic of word embeddings based on precise surrogate color matching in the CIELab space. This training-free approach ensures efficient and scalable improvements in color fidelity for T2I synthesis. Extensive experiments demonstrate the superior performance of the proposed method. The results showcase significant improvements in color accuracy and consistency, validating the effectiveness of the approach in practical T2I generation tasks."
        },
        {
            "title": "2.2 LLM-Enhanced Generation and Fine-Tuning\nRecent work has also integrated large language models (LLMs)\nto parse and refine prompts. LLM-grounded Diffusion [14] and\nRPG [25] use LLMs to handle complex instructions, produce struc-\ntured scene layouts, and plan multi-entity compositions. Though\nthese frameworks enhance compositional consistency, they do not\nexplicitly address subtle color nuances. Conversely, ColorPeel [3]\nproposes a fine-tuning strategy that learns a new prompt embed-\nding for each specific color token, yielding accurate color rendering\nbut at the cost of extra training. Our solution remains training-free\nyet leverages an LLM to disambiguate compound or ambiguous\ncolor terms, then directly blends their CIELab space representations\nwithin the diffusion process. By fusing LLM-based disambiguation\nstrategies with precise color interpolation, we focus specifically on\ncolor accuracy, offering a lightweight alternative to training-heavy\nmethods while expanding T2I models’ capability to handle intricate\nshades.",
            "content": "Figure 2: Failure cases of T2I diffusion models when processing prompts containing ambiguous color terms. Top: the term rose red is misinterpreted, causing the model to generate multiple rose flowers instead of the intended color. Bottom: when additional descriptive details are added to the prompt (e.g.middle-aged, walking), the model incorrectly renders green shirt as yellow."
        },
        {
            "title": "3 Tint Benchmark\nTo estimate whether different T2I generation models can accurately\ninterpret color descriptions, especially as users naturally employ ex-\npressive, context-dependent language, it is crucial to build a dataset\nthat reflects real-world usage. Existing prompt datasets lack cover-\nage of complex color expressions, focusing mainly on basic color\nterms like red or blue, and relying on rigid syntactic templates. This\nlimits their utility in evaluating fine-grained color understanding.\nFor instance, while CC-500 prompt dataset [7] explicitly defines\nattribute binding for color, its prompts follow fixed structures such\nas \"a {adj} {noun} and a {adj} {noun}\" (e.g., “a blue backpack and\na red bench.”), ailing to capture the richness of natural language.\nAs analyzed in Fig. 2, enhancing prompts with more detailed and\nhuman-like descriptions often leads to generation failures in cur-\nrent T2I models. This highlights the need for a dataset that better\nreflects the expressive diversity of real-world color usage. To ad-\ndress these limitations, we introduce TintBench, a benchmark that\ncombines a curated taxonomy of compound color names, includ-\ning blended, modified, object-based, signature, and abstract types,\nwith prompts that resemble natural image captions. This design\nallows us to evaluate model performance in more realistic settings,\nwhere color expressions appear within diverse and contextually",
            "content": "MM 25, October 2731, 2025, Dublin, Ireland. Sung-Lin Tsai et al. Table 1: Summary of the TintBench dataset. We report the number of prompts per category and provide representative examples after compound color substitution. Category Example single-color \"A woman with ruby red bags rides her bike over bridge.\" multi-color \"A man with light blue shirt and Barbie pink shorts is walking off of soccer field.\" Total 500 grounded descriptions. In the following sections, we describe how the compound color taxonomy is constructed and how the prompts are generated accordingly."
        },
        {
            "title": "3.1 Compound Color Names\nColor perception varies significantly between individuals and is\nhighly influenced by context. As a result, people use tens of thou-\nsands of phrases to describe colors. Among these, compound color\nnames are especially important for expressing subtle shades and\ntones. A compound color typically consists of a basic color term-a\nfoundational color category that can be modified with descriptive\nadjectives like light, deep, etc. To capture this diversity in color lan-\nguage, which is often overlooked in existing datasets, we curated an\nextensive collection of compound color names as a foundation for\nconstructing TintBench. These compound color names provide the\nfine-grained semantics needed to rigorously test a model’s capacity\nfor color understanding in realistic T2I scenarios.",
            "content": "Following prior work [2, 17], we begin with eleven core basic colors: black, blue, brown, gray, green, orange, pink, purple, red, white, and yellow. Building upon these, we searched the corresponding compound color names from web-based repositories such as Wikipedia and specialized color naming databases1, and collected diverse set of compound colors. Each compound color is linked to standardized color representation (i.e., RGB code), enabling precise integration into prompt augmentation and evaluation. The constructed compound colors can be categorized into the following five types: Blended Color: Created by combining two basic color terms to indicate mixed hue, such as red purple and yellow green. Modified Color: Formed by modifying basic color term with lightness-related adjectives, such as dark brown and light blue. Object Color: Constructed by prefixing basic color term with the name of an object that represents the color, such as olive green and salmon pink. Signature Color: color associated with specific organization, institution, or geographical region, serving as an identifying hue, such as Duke blue and Caribbean green. Abstract Color: color name where the prefixing adjective originates from abstract concepts, cultural references, or human-assigned labels, rather than physical attributes, such as Baker-Miller pink and cyber yellow. By incorporating compound color names from all five categories, TintBench introduces challenging prompts that better reflect the 1For example, https://colornames.org/. Figure 3: Comparison of pairwise distance matrices and Spearman correlations between text embeddings and color spaces. Overall correlation (𝜌) is computed across eleven basic color terms, with group-wise coefficients (𝜌𝑤, 𝜌𝑛, 𝜌𝑐 ) for warm (orange), neutral (gray), and cool (purple) colors. CIELab achieves the highest alignment overall and within all groups. complexity of real-world user inputs. The full list of compound color names and codes is available in the Appendix."
        },
        {
            "title": "3.2 TintBench Construction\nEquipped with the collected compound colors, we aim to construct\nthe prompt benchmarks in a natural way. Therefore, we selected\nthe Flickr30k dataset [27] as our starting point due to its relatively\nrich and varied descriptions. We first selected the prompts with\ncolor terms, leading to 30.21% of the captions. Please note that these\nonly contain basic color terms, and none sufficiently capture the\ndiversity of compound color names. To introduce the compound\ncolors, we first divided the captions into two groups: single-color and\nmulti-color, based on the number of color terms present. We then\napplied k-means clustering independently to each group, forming\n20 clusters per group. From each cluster, we sampled five captions,\nresulting in 100 prompts for the single-color group and 100 for the\nmulti-color group, for a total of 200 prompts.",
            "content": "Afterward, we augmented the selected prompts by replacing basic color terms with compound color names, randomly sampled from our five predefined categories. This process generated 500 single-color and 500 multi-color prompts, resulting in total of 1,000 augmented examples. The augmentation enhances the granularity of color representation in the text prompts and enables more robust benchmarking of T2I models in interpreting descriptive, real-world color language. Tab. 1 summarizes the composition of TintBench and provides representative examples."
        },
        {
            "title": "4 Method\nCurrent methods often fail to resolve the ambiguity of diverse color\nterms and struggle to ensure precise color rendering, even when\nthe intended semantics are correctly inferred. While fine-tuning\nwith annotated color datasets is a potential solution, it is costly\nand requires expert input (e.g., via Photoshop). To address this, we\npropose a training-free framework for color blending in diffusion\nmodels via LLM-guided disambiguation, as illustrated in Fig. 4.",
            "content": "Given text prompt with fine-grained or compound color expressions, our pipeline uses large language model to rewrite ambiguous terms into clarified descriptions grounded in basic colors (Sec. 4.1). Using CIELab codes of both the target and basic colors, Color Me Correctly: Bridging Perceptual Color Spaces and Text Embeddings for Improved Diffusion Generation MM 25, October 2731, 2025, Dublin, Ireland. Figure 4: Our pipeline begins by using large language model (GPT-4o) to resolve color ambiguity, producing structured color analysis list. In the Embedding Refinement stage, the target color is interpolated with nearby basic colors within its Color Group to generate precise embedding, which guides generation via cross-attention binding during denoising. we compute color-space offset that captures their perceptual difference. This offset guides embedding refinement via interpolation in the text embedding space (Sec. 4.2), analogous to semantic vector arithmetic (e.g., king man + woman queen). The refined embeddings are then passed to the diffusion model, enabling more accurate and semantically faithful color generation."
        },
        {
            "title": "4.1 Semantic Color Disambiguation with LLMs\nText prompts often contain diverse compound color expressions,\nwhich can lead to the generation of unintended objects or visually\ninconsistent content. Recent T2I methods have incorporated POS-\nbased analyses to improve attribute binding in diffusion models [7,\n21]. However, as shown in Sec. 3, these approaches remain limited in\nhandling semantically ambiguous or fine-grained color terms, often\nfalling short of user expectations. Motivated by the recent progress\nof large language models (LLMs) in semantic understanding, we\npropose leveraging their strong reasoning capabilities to explicitly\naddress the ambiguity in complex color descriptions.",
            "content": "As illustrated in the top-left of Fig. 4, our method employs an LLM (GPT-4o) to perform semantic analysis on text prompts containing intricate color expressions. Given an input prompt 𝑝original, the LLM first analyzes each color term 𝑐. If an ambiguous term 𝑐ambiguous is identifiedi.e., one likely to mislead the diffusion modelit is explicitly labeled. The LLM then selects the basic color term 𝑐basic that best represents the intended meaning and rewrites the prompt into disambiguated version 𝑝disambiguated to resolve semantic confusion. In addition, the LLM provides reference color code 𝛾𝑐 (e.g., in RGB), informed by the scene context and intended interpretation of 𝑐. This process mitigates the risk of misinterpretation by aligning textual color semantics with perceptual expectations. The resulting color code directly informs the refinement of color-related embeddings, as detailed in Sec. 4.2. Further details on prompt setup are provided in the Appendix."
        },
        {
            "title": "Color Representation",
            "content": "Text embeddings encode semantic relationships between words in continuous vector spaces, enabling models to capture subtle linguistic similarities. Since the introduction of vector-based representations such as Word2Vec [16] and GloVe [19], numerous studies have shown that semantically related concepts tend to occupy nearby regions in the embedding space. For example, embeddings of color terms like scarlet, crimson, and rubyall denoting shades of redtypically cluster together more closely than with unrelated terms such as blue or green. This compositional structure of text embeddings offers promising foundation for modeling color semantics, particularly for interpolating between new color descriptions. Motivated by these observations, we hypothesize that the spatial arrangement of color terms in learned text embedding spaces reflects their perceptual relationships in human color space. Specifically, we expect that terms representing similar hues (e.g., various shades of red) are embedded near each other, while those corresponding to perceptually distinct hues (e.g., red vs. blue) are more widely separated. To validate this hypothesis, we examine the spatial correlation between eleven basic color terms in the human color perception space and their corresponding representations in the text embedding space. Specifically, we first compute pairwise distances between color terms across several commonly used color spaces, including RGB, CIELab, HSV, YCbCr, YUV, and CIE1931. To account for the characteristics of non-linear color spaces, we adopt CIEDE2000 color difference formula (Δ𝐸00) [24] in the CIELab space. Unlike the simpler Euclidean formulation in CIE1931, Δ𝐸00 incorporates corrections for lightness, chroma, and hue differences and includes rotation term to account for perceptual interactions between chroma and hue in the blue region. The formula is defined as: MM 25, October 2731, 2025, Dublin, Ireland. Sung-Lin Tsai et al. Table 2: User study results on TintBench. We report the average win rate for each evaluation metric. Bold values indicate win rate exceeding 50%. Method SD [22] AE [5] Conform [15] DivideBind [13] Rich [8] InitNO [9] SDG [1] SynGen [21] SDXL [20] SDXL SynGen [21] SDXL Rich [8] Single Multiple Prompt Ambiguous Color Overall Prompt Ambiguous Color Overall 95.84% 91.66% 83.35% 75.00% 77.78% 77.09% 87.70% 81.25% 69.83% 81.48% 88.10% 60.44% 72.20% 62.50% 66.67% 69.44% 79.17% 79.17% 72.91% 69.44% 74.99% 84.50% 79.17% 94.47% 75.00% 69.44% 86.11% 68.75% 85.42% 83.34% 74.99% 84.26% 65.00% 93.75% 94.47% 79.20% 80.56% 83.33% 79.16% 89.58% 91.67% 68.06% 87.96% 89.28% 86.11% 87.50% 63.89% 74.90% 75.00% 86.07% 79.17% 80.57% 68.75% 79.16% 77.78% 66.67% 54.17% 75.00% 68.06% 75.00% 74.97% 79.17% 91.57% 83.15% 79.16% 83.33% 86.11% 91.67% 80.55% 68.54% 68.75% 86.90% 68.75% 69.32% 67.66% 69.47% 76.39% 94.44% 87.50% 66.66% 83.34% 83.34% 91.53% 83.33% 77.67% 91.62% 87.50% 88.90% Δ𝐸00 = (cid:20) (cid:16) Δ𝐿 𝑘𝐿𝑆𝐿 (cid:17) 2 + (cid:16) Δ𝐶 𝑘𝐶 𝑆𝐶 (cid:17) 2 + (cid:16) Δ𝐻 𝑘𝐻 𝑆𝐻 (cid:17) 2 + 𝑅𝑇 (cid:17) (cid:16) Δ𝐶 𝑘𝐶 𝑆𝐶 (cid:16) Δ𝐻 𝑘𝐻 𝑆𝐻 (cid:17) (cid:21) 1/2 , (1) where Δ𝐿, Δ𝐶 , and Δ𝐻 represent the differences in lightness, chroma, and hue, respectively; 𝑆𝐿, 𝑆𝐶 , and 𝑆𝐻 are the weighting functions; 𝑘𝐿, 𝑘𝐶 , and 𝑘𝐻 are typically set to 1; and 𝑅𝑇 is rotation term that accounts for the interaction between chroma and hue. For HSV, we isolate and project the hue component before computing distances. For all other linear color spaces, Euclidean distance is directly applied. We then calculate Spearmans rank correlation coefficient (𝜌) between each color spaces distance matrix and the corresponding pairwise distances in the text embedding space. As shown in Fig. 3, all color spaces exhibit only weak correlations with the text embedding space, suggesting that while linguistic and perceptual representations may share some structure, the alignment is imperfect and varies across encoding schemes. Interestingly, we observe that the text embeddings of the color terms naturally form clusters based on hue categories. Based on this observation, we group the 11 basic color terms into three semantic categories: warm colors (including red, orange, pink, and yellow), cool colors (blue, green, and purple), and neutral colors (black, white, gray, and brown). We then compute Spearman correlations for each group individually. Results show that among all tested color spaces, CIELab consistently exhibits the highest positive correlation with the text embedding space across all three color groups, with an average correlation coefficient of 0.924. The correlation coefficients for the warm, neutral, and cool groups are denoted as 𝜌𝑤, 𝜌𝑛, and 𝜌𝑐 , respectively, and are visualized in Fig. 3. We attribute this outcome to findings in color science, which suggest that human vision is particularly sensitive to variations in lightness. As perceptually uniform space, CIELab more closely reflects how humans perceive color differences in images. This result also aligns with findings from ColorPeel [3], which showed that projecting diffusion latents into CIELab enables more precise control over generated colors. Figure 5: Interpolated text embeddings between orange car. and yellow car. As the blending ratio changes, the resulting hues transition smoothly from orange to yellow, depending on the dominant color. Consequently, we adopt CIELab as the default color space for all subsequent operations. To further demonstrate that the text embeddings can be manipulated to achieve more accurate representations of target colors, we perform color blending within embedding space based on the relationships among basic color terms in perceptual color space. Specifically, we apply proportional blending to the text embeddings of color terms, following the behavior of color interpolation in the CIELab space. For example, the Lab code (66, 43, 68) corresponds to darker shade of orange, while (92, -21, 94) represents highly saturated yellow. Averaging these two values yields color code that lies between them, producing yellow-orange hue that visually resembles perceptual blend of both original colors. Following this principle, we interpolate between the text embeddings of orange and yellow in varying proportions, controlled by blending factor 𝛼 [0, 1]. Specifically, we compute the interpolated embedding using the formula: 𝛼 𝑒yellow + (1 𝛼) 𝑒orange, where 𝑒yellow and 𝑒orange denote the text embeddings of the tokens orange and yellow, respectively. The resulting vector is then used to replace the color token embedding in the prompt. As shown in Fig. 5, when the blending ratio 𝛼 shifts, the cars color also shifts toward either orange or yellow, depending on the dominant component. To enable perceptually grounded color blending, we first classify the predicted color into predefined hue group based on the color code generated by the LLM during the Semantic Color Disambiguation stage. As demonstrated in our earlier analysis, hue Color Me Correctly: Bridging Perceptual Color Spaces and Text Embeddings for Improved Diffusion Generation MM 25, October 2731, 2025, Dublin, Ireland. Figure 6: Qualitative comparison between our method and previous work using different prompts in SD v1.4. The first two rows show results for single-color prompts, while the last two rows demonstrate the multi-color prompt setting. groups exhibit stronger correlation between perceptual color distances and text embedding similarities. Restricting subsequent operations to colors within the same hue group ensures higher semantic relevance in the blending process. Within the identified hue group, we then retrieve the top-𝑘 nearest basic color terms by computing perceptual distances in the CIELab space using the Δ𝐸00 metric. This metric closely aligns with human visual perception and provides more reliable measure than Euclidean distance in RGB or other non-uniform color spaces. Prior work [17] has shown that diffusion models respond more consistently to basic color terms. Motivated by this, we treat the embeddings of the selected basic color terms as directional anchors, using them to guide the offset computation for the target color term in the text embedding space. This strategy ensures that the blended embedding is grounded in semantically and perceptually salient color representations. To compute the blending weights between the selected basic color embeddings, we move beyond simple linear interpolation and instead use Gaussian-based softmax formulation. This accounts for the non-linear structure of the CIELab space and yields smooth, perceptually meaningful blending. The target color embedding 𝑒target and weight 𝛼𝑖 for each basic color 𝑖 is defined as: 𝑒𝑡𝑎𝑟𝑔𝑒𝑡 = 𝑘 𝑖 𝛼𝑖𝑒𝑖, with 𝛼𝑖 = softmax (cid:18) 𝑑 2 𝑖 2𝜎 2 (cid:19) , (2) where 𝑘 is the number of color terms, 𝑒𝑖 is the embedding for the basic color 𝑖, 𝑑𝑖 is the perceptual distance (Δ𝐸00) between the target color and the 𝑖-th basic color, and 𝜎 is temperature-like scaling factor that controls the sharpness of the weight distribution. Through this retrieval mechanism, we can generate text embeddings that accurately capture the semantics of the blended color representation. This ensures that the resulting embedding effectively conveys the intended color meaning. To improve the binding between color terms and their corresponding visual entities, we incorporate guidance loss during the denoising step. This component is inspired by the positive loss formulation in Cross-Attention-based Guidance used in SynGen [21]. Specifically, the Color-Binding loss Lbinding is applied after embedding refinement and serves as soft constraint that guides the attention maps toward more semantically consistent color-object alignment: L𝑖 binding = 1 2 DKL (𝐴𝑖 color 𝐴𝑖 entity) + 1 DKL (𝐴𝑖 entity 𝐴𝑖 color), (3) where 𝐴𝑖 color and 𝐴𝑖 entity denote the cross-attention maps corresponding to the 𝑖thcolor term and the 𝑖th entity term, respectively. Each attention map is normalized such that its elements sum to 1. The symmetric Kullback-Leibler divergence Lbinding encourages alignment between these two distributions, guiding the model to associate the correct color with the correct visual region. After computing the binding loss Lbinding, we update the output latents 𝑥𝑡 during the denoising step 𝑡 by the Color-Binding Step: 𝑥𝑡 𝑥𝑡 𝛼 𝑥 𝑐 𝐶 L𝑐 binding , (4) where 𝛼 is scalar that controls the binding scale, and 𝐶 represents the set of basic color terms identified in the input prompt. MM 25, October 2731, 2025, Dublin, Ireland. Sung-Lin Tsai et al."
        },
        {
            "title": "5.3 Qualitative Result\nWe present a qualitative comparison between our method and other\napproaches in Fig. 6 and Fig. 7. In Fig. 6, we show that the original\nStable Diffusion 1.4 (SD1.4) model is easily misled by ambiguous\ncolor terms in the prompt. For example, in row 4, the word \"DUKE\"\nappears on the shirt of a young boy—an artifact caused by the\nmodel interpreting the compound color name \"Duke blue\" as re-\nferring to the university rather than the intended color. Although\nprevious works on attribute binding help mitigate this issue to\nsome extent, they still struggle to generate accurate compound\ncolors as defined in our Tint Benchmark. In contrast, our method\naddresses both the ambiguity in prompts and successfully gener-\nates the correct color output according to the specified compound",
            "content": "Figure 7: Qualitative comparison between our method and previous work using different prompts in SDXL. The first two rows show results for single-color prompts, while the last two rows demonstrate the multi-color prompt setting. color. Similarly, Fig. 7 demonstrates that this issue also occurs in Stable Diffusion XL (SDXL), and again, our method effectively resolves it. These results highlight the importance of semantic color disambiguation when interpreting compound color names. By integrating perceptual grounding and LLM-driven understanding, our approach ensures that visually grounded color meanings are preserved during generations."
        },
        {
            "title": "6 Conclusion and Future Work\nIn this work, we addressed the long-overlooked challenge of fine-\ngrained color understanding in text-to-image generation. We intro-\nduced TintBench, a comprehensive benchmark constructed by aug-\nmenting real-world prompts with diverse and nuanced compound\ncolor expressions. Our dataset enables a more rigorous evaluation\nof how well generative models capture human-like interpretations\nof color semantics. To bridge the gap between textual color de-\nscriptions and perceptual color representations, we proposed a\ntraining-free pipeline that leverages LLMs for semantic color dis-\nambiguation. Additionally, we introduced a perceptually weighted\ninterpolation mechanism grounded in the Δ𝐸00 metric, enabling\nsmooth and semantically meaningful blending of basic color em-\nbeddings. Our user studies and visualizations demonstrate that the\nproposed approach offers superior color fidelity and effectively re-\nsolves color ambiguities in prompt interpretation. We hope that\nTintBench and our accompanying framework will facilitate future\nresearch in controllable generation and semantic grounding, partic-\nularly in applications that demand high-fidelity coloring. Looking\nahead, we plan to scale our dataset to include a wider range of input\nmodalities, such as raw color codes, image regions, and free-form\ntext descriptions. We also aim to develop a foundation model for\ncolor grounding that can flexibly interpret and generate accurate\ncolor representations across modalities. This would pave the way\ntoward more generalizable and interactive multimodal generation\nsystems with fine-grained color control.",
            "content": "Color Me Correctly: Bridging Perceptual Color Spaces and Text Embeddings for Improved Diffusion Generation MM 25, October 2731, 2025, Dublin, Ireland. Acknowledgments This work is partially supported by the National Science and Technology Council, Taiwan, under Grant: NSTC-112-2221-E-A49-059MY3 and NSTC-112-2221-E-A49-094-MY3. References [1] Aishwarya Agarwal, Srikrishna Karanam, and Balaji Vasan Srinivasan. 2024. Training-free Color-Style Disentanglement for Constrained Text-to-Image Synthesis. arXiv preprint arXiv:2409.02429 (2024). [2] Brent Berlin and Paul Kay. 1991. Basic color terms: Their universality and evolution. Univ of California Press. [3] Muhammad Atif Butt, Kai Wang, Javier Vazquez-Corral, and Joost van de Weijer. 2024. ColorPeel: Color prompt learning with diffusion models via color and shape disentanglement. In European Conference on Computer Vision. [4] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. 2023. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In Proceedings of the IEEE/CVF international conference on computer vision. [5] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. 2023. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. ACM transactions on Graphics (TOG) (2023). [6] Minghao Chen, Iro Laina, and Andrea Vedaldi. 2024. Training-free layout control with cross-attention guidance. In Proceedings of the IEEE/CVF winter conference on applications of computer vision. [7] Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Reddy Akula, Pradyumna Narayana, Sugato Basu, Xin Eric Wang, and William Yang Wang. 2023. Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis. In The Eleventh International Conference on Learning Representations. [8] Songwei Ge, Taesung Park, Jun-Yan Zhu, and Jia-Bin Huang. 2023. Expressive textto-image generation with rich text. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 75457556. [9] Xiefan Guo, Jinlin Liu, Miaomiao Cui, Jiankai Li, Hongyu Yang, and Di Huang. 2024. Initno: Boosting text-to-image diffusion models via initial noise optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 93809389. [10] Yunpeng Han, Lisai Zhang, Qingcai Chen, Zhijian Chen, Zhonghua Li, Jianxin Yang, and Zhao Cao. 2023. Fashionsap: Symbols and attributes prompt for finegrained fashion vision-language pre-training. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1502815038. [11] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-or. 2023. Prompt-to-Prompt Image Editing with Cross-Attention Control. In The Eleventh International Conference on Learning Representations. [12] Yihan Hou, Xingchen Zeng, Yusong Wang, Manling Yang, Xiaojiao Chen, and Wei Zeng. 2025. GenColor: Generative Color-Concept Association in Visual Design. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems. 119. [13] Yumeng Li, Margret Keuper, Dan Zhang, and Anna Khoreva. 2023. Divide & Bind Your Attention for Improved Generative Semantic Nursing. In BMVC. [14] Long Lian, Boyi Li, Adam Yala, and Trevor Darrell. 2024. LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models. Transactions on Machine Learning Research (2024). [15] Tuna Han Salih Meral, Enis Simsar, Federico Tombari, and Pinar Yanardag. 2024. Conform: Contrast is all you need for high-fidelity text-to-image diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 90059014. [16] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781 (2013). [17] Nathan Moroney. 2024. Color Terms and Stable Diffusion. In Color and Imaging Conference. [18] Muragul Muratbekova and Pakizar Shamoi. 2024. Color-emotion associations in art: Fuzzy approach. IEEE Access 12 (2024), 3793737956. [19] Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). [20] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. 2023. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952 (2023). [21] Royi Rassin, Eran Hirsch, Daniel Glickman, Shauli Ravfogel, Yoav Goldberg, and Gal Chechik. 2023. Linguistic Binding in Diffusion Models: Enhancing Attribute Correspondence through Attention Map Alignment. In Thirty-seventh Conference on Neural Information Processing Systems. [22] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1068410695. [23] Pakizar Shamoi, Muragul Muratbekova, Assylzhan Izbassar, Atsushi Inoue, and Hiroharu Kawanaka. 2023. Towards universal understanding of color harmony: Fuzzy approach. In Fuzzy Systems and Data Mining IX. IOS Press, 2028. [24] Gaurav Sharma, Wencheng Wu, and Edul Dalal. 2005. The CIEDE2000 colordifference formula: Implementation notes, supplementary test data, and mathematical observations. Color Research & Application: Endorsed by Inter-Society Color Council, The Colour Group (Great Britain), Canadian Society for Color, Color Science Association of Japan, Dutch Society for the Study of Color, The Swedish Colour Centre Foundation, Colour Society of Australia, Centre Français de la Couleur (2005). [25] Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and Bin Cui. 2024. Mastering text-to-image diffusion: Recaptioning, planning, and generating with multimodal llms. In Forty-first International Conference on Machine Learning. [26] Xingxi Yin, Zhi Li, Jingfeng Zhang, Chenglin Li, and Yin Zhang. 2024. ColorEdit: Training-free Image-Guided Color Editing with Diffusion Model. arXiv preprint arXiv:2411.10232 (2024). [27] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. 2014. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the association for computational linguistics (2014)."
        }
    ],
    "affiliations": [
        "National Yang Ming Chiao Tung University Hsinchu, Taiwan"
    ]
}