{
    "paper_title": "Inference-Time Scaling for Flow Models via Stochastic Generation and Rollover Budget Forcing",
    "authors": [
        "Jaihoon Kim",
        "Taehoon Yoon",
        "Jisung Hwang",
        "Minhyuk Sung"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose an inference-time scaling approach for pretrained flow models. Recently, inference-time scaling has gained significant attention in LLMs and diffusion models, improving sample quality or better aligning outputs with user preferences by leveraging additional computation. For diffusion models, particle sampling has allowed more efficient scaling due to the stochasticity at intermediate denoising steps. On the contrary, while flow models have gained popularity as an alternative to diffusion models--offering faster generation and high-quality outputs in state-of-the-art image and video generative models--efficient inference-time scaling methods used for diffusion models cannot be directly applied due to their deterministic generative process. To enable efficient inference-time scaling for flow models, we propose three key ideas: 1) SDE-based generation, enabling particle sampling in flow models, 2) Interpolant conversion, broadening the search space and enhancing sample diversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of computational resources across timesteps to maximize budget utilization. Our experiments show that SDE-based generation, particularly variance-preserving (VP) interpolant-based generation, improves the performance of particle sampling methods for inference-time scaling in flow models. Additionally, we demonstrate that RBF with VP-SDE achieves the best performance, outperforming all previous inference-time scaling approaches."
        },
        {
            "title": "Start",
            "content": "Inference-Time Scaling for Flow Models via Stochastic Generation and Rollover Budget Forcing Jaihoon Kim Taehoon Yoon Jisung Hwang Minhyuk Sung KAIST {jh27kim,taehoon,4011hjs,mhsung}@kaist.ac.kr 5 2 0 M 5 2 ] . [ 1 5 8 3 9 1 . 3 0 5 2 : r Figure 1. Diverse applications of our inference-time scaling method. Our inference-time scaling method extends the capabilities of pretrained flow model [25] to generate images that more precisely align with user preferences. More computation during inference improves alignment, reducing Residual Sum of Squares (RSS) over time (top row). Our flow-based method outperforms diffusion models, even with five times fewer number of function evaluations (NFEs) (top-right). For compositional text-to-image generation applications (logical, comparison, spatial relation), we use the reward from VQAScore [28] to ensure precise alignment with the input text, where the description is particularly challenging for typical text-to-image generative models to satisfy (see the results on the left side of each case). We use the object detection score [31] for the \"counting\" application and the aesthetic score [44] for the \"aesthetic\" application. For concept erasure, the reward is the number of removed concepts computed using VLM [3] queries. The red box denotes the results of our method."
        },
        {
            "title": "Abstract",
            "content": "We propose an inference-time scaling approach for pretrained flow models. Recently, inference-time scaling has *Equal contribution. gained significant attention in LLMs and diffusion models, improving sample quality or better aligning outputs with user preferences by leveraging additional computation. For diffusion models, particle sampling has allowed more efficient scaling due to the stochasticity at intermediate denoising steps. On the contrary, while flow models have gained popu1 larity as an alternative to diffusion modelsoffering faster generation and high-quality outputs in state-of-the-art image and video generative modelsefficient inference-time scaling methods used for diffusion models cannot be directly applied due to their deterministic generative process. To enable efficient inference-time scaling for flow models, we propose three key ideas: 1) SDE-based generation, enabling particle sampling in flow models, 2) Interpolant conversion, broadening the search space and enhancing sample diversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of computational resources across timesteps to maximize budget utilization. Our experiments show that SDE-based generation, particularly variance-preserving (VP) interpolantbased generation, improves the performance of particle sampling methods for inference-time scaling in flow models. Additionally, we demonstrate that RBF with VP-SDE achieves the best performance, outperforming all previous inferencetime scaling approaches. Project page: https://flowinference-time-scaling.github.io/. 1. Introduction Over the past years, scaling laws of AI models have mainly focused on increasing model size and training data. However, recent advancements have shifted attention toward inference-time scaling [49, 51], leveraging computational resources during inference to enhance model performance. OpenAI o1 [38] and DeepSeek R1 [11] exemplify this approach, demonstrating consistent output improvements with increased inference computation. Recent research in LLMs [37] attempting to replicate such improvements has introduced test-time budget forcing, achieving high efficiency with limited token sampling during inference. For diffusion models [46, 48], which are widely used for generation tasks, research on inference-time scaling has been growing in the context of reward-based sampling [21, 27, 45]. Given reward function that measures alignment with user preferences [23] or output quality [28, 44], the goal is to find the sample from the learned data distribution that best aligns with the reward through repeated sampling. Fig. 1 showcases diverse applications of inference-time scaling, enabling the generation of faithful images that accurately align with complex user descriptions involving objects quantities, logical relationships, and conceptual attributes. Notably, naïve generation from text-to-image models [25, 43] often fails to fully meet user specifications, highlighting the effectiveness of inference-time scaling. Our goal in this work is to extend the inference-time scaling capabilities of diffusion models to flow models. Flow models [29] power state-of-the-art image [14, 25] and video generation [7, 60], achieving high-quality synthesis with few inference steps, enabled by trajectory stratification techniques during training [32]. Beyond just speed, recent pretrained flow models, equipped with enhanced text-image embeddings [41] and advanced architectures [14], significantly outperform previous pretrained diffusion models in both image and video generation quality. Despite their advantages in generating high-quality results more efficiently than diffusion models, flow models have an inherent limitation in the context of inference-time scaling. Due to their ODE-based deterministic generative process, they cannot directly incorporate particle sampling at intermediate steps, key mechanism for effective inference-time scaling in diffusion models. Building on the formulation of stochastic interpolant framework [1], we adopt an SDEbased sampling method for flow models at inference-time, enabling particle sampling for reward alignment. Furthermore, we observe that converting an ODE to its corresponding SDE in the generative process of flow model does not provide sufficient diversity in particle sampling to effectively seek high-reward samples. To expand the exploration space, we consider not only stochasticity but also the choice of the interpolant itself. While typical flow models use linear interpolant, diffusion models commonly adopt Variance-Preserving (VP) interpolant [17, 48]. Inspired by this, for the first time, we incorporate the VP interpolant into the generative process of flow models and demonstrate its effectiveness in increasing sample diversity, ultimately enhancing the likelihood of discovering high-reward samples. We emphasize that while we propose converting the generative process of pretrained flow model to align with that of diffusion modelsi.e., VP-SDE-based generationinference-time scaling with flow models offers significant advantages over diffusion models. Flow models, particularly those with rectification fine-tuning [32, 33], produce much clearer expected outputs at intermediate steps, enabling more precise future reward estimation and, in turn, more effective particle sampling. We additionally explore strategy for tight budget enforcement in terms of the number of function evaluations (NFEs) of the velocity prediction network. Previous particlesampling-based inference-time scaling approaches for diffusion models [27, 45] allocate the NFEs budget uniformly across timesteps in the generative process, which we empirically found to be ineffective in practice. To optimize budget utilization, we propose Rollover Budget Forcing, method that adaptively reallocates NFEs across timesteps. Specifically, we perform denoising step upon identifying new particle with higher expected future reward and allocate the remaining NFEs to subsequent timesteps. Experimentally, we demonstrate that our inference-time SDE conversion and VP interpolant conversion enable particle sampling in flow models, leading to consistent improvements in reward alignment across two challenging tasks: compositional text-to-image generation and quantity-aware image generation. Additionally, our Rollover Budget Forcing (RBF) provides further performance gains, outperform2 ing all previous particle sampling approaches. We also demonstrate that for differentiable rewards, such as aesthetic image generation, integrating RBF with gradient-based method [8] creates synergistic effect, leading to further performance improvements. In summary, we introduce an inference-time scaling method for flow models, analyzing three key factors and proposing the best options for each: ODE vs. SDE: We introduce an SDE-based generative process for flow models to enable particle sampling. Interpolant: We demonstrate that replacing the linear interpolant of flow models with Variance Preserving (VP) interpolant expands the search space, facilitating the discovery of higher-reward samples. NFEs Allocation: We propose Rollover Budget Forcing that adaptively allocates NFEs across timesteps to ensure efficient utilization of the available compute budget. 2. Related Work 2.1. Reward Alignment in Diffusion Models In the literature of diffusion models, reward alignment approaches can be broadly categorized into fine-tuning-based methods [5, 9, 39, 55, 57, 58] and inference-time-scalingbased methods [6, 12, 27, 45, 56]. While fine-tuning diffusion models enables the generation of samples aligned with user preferences, it requires fine-tuning for each task, potentially limiting scalability. In contrast, inference-time scaling approaches offer significant advantage as they can be applied to any reward without requiring additional finetuning. Moreover, they can also be applied to fine-tuned models to further enhance alignment with the reward. Since our proposed approach is an inference-time scaling method, we focus our review on related literature in this domain. Furthermore, for differentiable rewards, gradient-based methods [4, 8, 15, 16, 54, 59] have been extensively studied. We note that inference-time scaling can be integrated with gradient-based approaches to achieve synergistic performance improvements. 2.2. Particle Sampling with Diffusion Models The simplest iterative sampling method that can be applied to any generative model is Best-of-N (BoN) [4951], which generates batches of samples and selects the one with the highest reward. For diffusion models, however, incorporating particle sampling during the denoising process has been shown to be far more effective than naïve BoN [27, 45]. This idea has been further developed through various approaches that sample particles at each step. For instance, SVDD [27] proposed simply selecting the particle with the highest reward at every step. CoDe [45] extends this idea by selecting the highest-reward particle only at specific intervals. On the other hand, methods based on Sequential Monte Carlo (SMC) [6, 12, 21, 56] employ probabilistic selection approach, in which particles are sampled from multinomial distribution according to their importance weights. Despite the success of particle sampling approaches for diffusion models, they have not been applicable to flow models due to the absence of stochasticity in their generative process. In this work, we present the first inference-time scaling method for flow models based on particle sampling by introducing stochasticity into the generative process and further increasing sampling diversity through trajectory modification. 2.3. Inference-Time Scaling with Flow Models To our knowledge, SoP [36], concurrent work to ours, is the only inference-time scaling method proposed for flow models, which applies forward kernel to sample particles from the deterministic sampling process of flow models. However, SoP does not explore the possibility of modifying the reverse kernel, which could enable more diverse particlesampling-based methods [21, 27, 45]. To the best of our knowledge, we are the first to investigate the application of particle sampling to flow models through the lens of the reverse kernel. 3. Problem Definition and Background 3.1. Inference-Time Reward Alignment Given pretrained flow model that maps the source distribution, standard Gaussian distribution p1, into the data distribution p0, our objective is to generate high-reward samples x0 Rd from the pretrained flow model without additional traininga task known as inference-time reward alignment. We denote the given reward function as : Rd R, which measures text alignment or user preference for generated sample. Following previous works [24, 52, 53], our objective can be formulated as finding the following target distribution: 0 = arg max Ex0q [r(x0)] (cid:124) (cid:123)(cid:122) (cid:125) Reward , β DKL [q(x0)p0(x0)] (cid:125) (cid:123)(cid:122) KL Regularization (cid:124) (1) which maximizes the expected reward while the KL divergence term prevents 0(x0) from deviating too far from p0(x0), with its strength controlled by the hyperparameter β. As shown in previous work [40], the target distribution 0 can be computed as: 0(x0) ="
        },
        {
            "title": "1\nZ",
            "content": "p0(x0) exp (cid:18) r(x0) β (cid:19) , (2) where is normalization constant. We present details in Appendix A.1. However, sampling from the target distribution is non-trivial. notable approach for sampling from the target distribution is particle sampling, which maintains set of candidate samplesreferred to as particlesand iteratively propagates high-reward samples while discarding lower-reward ones. When combined with the denoising process of diffusion models, particle sampling can improve the efficiency of limited 3 computational resources in inference-time scaling. In the next section, we review particle sampling methods used in diffusion models and explore insights for adapting them to flow models. 3.2. Particle Sampling Using Diffusion Model pretrained diffusion model generates data by drawing an initial sample from the standard Gaussian distribution and iteratively sampling from the learned conditional distribution pθ(xttxt). Building on this, previous works [26, 53] have shown that data from the target distribution in Eq. 2 can be generated by performing the same denoising process while replacing the conditional distribution pθ(xttxt) with the optimal policy: θ(xttxt) = pθ(xttxt) exp (cid:82) pθ(xttxt) exp (cid:16) v(xtt) β (cid:16) v(xtt) β (cid:17) (cid:17) , (3) dxtt where the details are presented in Appendix A.2. We denote v() : Rd as the optimal value function that estimates the expected future reward of the generated samples at current timestep. Following previous works [4, 8, 21, 27], we approximate the value function using the posterior mean computed via Tweedies formula [42], given by v(xt) r(x0t), where x0t := Ex0pθ(x0xt) [x0]. Since directly sampling from the optimal policy distribution in Eq. 3 is nontrivial, one can first approximate the distribution using importance sampling while taking pθ(xttxt) as the proposal distribution: Figure 2. Comparison of Linear-SDE and VP-SDE. Starting from the same initial noise latent, we generate 50 samples using Linear-ODE, Linear-SDE, and VP-SDE. identical. This restricts the applicability of particle sampling methods in flow models. To this end, we propose an inference-time approach that introduces stochastic sampling into the generative process of flow models to enable particle sampling. We first transform the deterministic sampling process of flow models into stochastic process (Sec. 4.2). We further expand the search space by modifying the sampling trajectory of flow models to align with that of diffusion models (Sec. 4.3). Additionally, previous particle sampling methods in diffusion models allocated fixed computational budget (i.e., uniform number of particles) across all denoising timesteps, potentially limiting exploration. We explore sampling with the rollover strategy, which adaptively allocates compute across timesteps during the sampling process (Sec. 5). (4) 4. SDE-Based Generation Using Flow Models θ(xttxt) tt (cid:88) (cid:80)K w(i) j=1 w(j) i=1 pθ(xttxt), δx(i) tt i=1 tt {x(i) tt}K (cid:16) exp tt)/β where is v(x(i) the number of particles, w(i) (cid:17) tt = is the weight, and δx(i) is Dirac distribution. Li et al. [27] proposed an approximate sampling method for the optimal policy by selecting the sample with the largest weight from Eq. 4. tt Notably, key factor in seeking high-reward samples using particle sampling is defining the proposal distribution to sufficiently cover the distribution of high-reward samples. Consider scenario where high-reward samples reside in low density region of the original data distribution, which is common when generating complex or highly specific samples that deviate from the mode of the pretrained model distribution. In this case, the proposal distribution must have large variance to effectively explore these low density regions. This highlights the importance of the stochasticity of the proposal distribution, which has been instrumental in the successful adoption of particle sampling in diffusion models. In contrast, flow models [29] employ deterministic sampling process, where all particles xtt drawn from xt are 4 In this section, we review flow and diffusion models within the unified stochastic interpolant framework (Sec. 4.1) and introduce our inference-time approaches for efficient particle sampling in flow models (Sec. 4.2 and 4.3). 4.1. Background: Stochastic Interpolant Framework At the core of both diffusion and flow models is the construction of probability paths {pt}0t1, where xt pt serves as bridge between x1 p1 and x0 p0: xt = αtx0 + σtx1, (5) where αt and σt are smooth functions satisfying α0 = σ1 = 1, α1 = σ0 = 0, and αt < 0, σt > 0; we denote the dot as time derivative. This formulation provides flexible choice of interpolant (αt, σt) which determines the sampling trajectory. 4.2. Inference-Time SDE Conversion Flow models [29, 32] learn the velocity field ut : Rd Rd, which enables sampling of x0 by solving the Probability Flow-ODE [48] backward in time: dxt = ut(xt)dt. (6) The deterministic process in Eq. 6 accelerates the sampling process enabling few-step generation of high-fidelity samples. However, as discussed in Sec. 3.2, the deterministic nature of this sampling process limits the applicability of particle sampling in flow models. To address this, we transform the deterministic sampling process into stochastic process. Song et al. [48] proposed the reverse-time SDE that shares the same marginal probability densities as the deterministic process in Eq. 6: dxt = ft(xt)dt + gtdw, ft(xt) = ut(xt) g2 2 log pt(xt), (7) (8) where ft(xt) and gt represent the drift and diffusion coefficient, respectively, and is the standard Wiener process. Ma et al. [35] have shown that gt, which determines the level of stochasticity of the proposal distribution, can be freely chosen within the stochastic interpolant framework [1, 20] (details in Appendix B). Note that in the case where gt = 0 the process reduces to deterministic sampling in Eq. 6. Using the velocity ut(xt) predicted by pretrained flow model, the score function log pt(xt) appearing in the drift coefficient ft(xt) can be computed as: log pt(xt) = 1 σt αtut(xt) αtxt αtσt αt σt . (9) This enables the conversion of the deterministic sampling to stochastic sampling, which we refer to as inference-time SDE conversion. Given the drift coefficient term ft(xt) and diffusion coefficient gt, the proposal distribution in the discrete-time domain is derived as follows: pθ(xttxt) = (xt ft(xt)t, g2 I). (10) Since flow models utilize the linear interpolant (αt = 1 t, σt = t), we refer to the generative processes of the flow models using Eq. 6 and Eq. 7 as Linear-ODE and LinearSDE. In Fig. 2 (left), we visualize the sampling trajectories of Linear-ODE and Linear-SDE. The samples generated using Linear-ODE are identical and collapse to single point, restricting exploration. In contrast, Linear-SDE introduces sample variance, allowing for broader exploration and increasing the likelihood of discovering high-reward samples. However, in practice, we empirically observe that the search space of Linear-SDE remains constrained. In Fig. 3 (ab), we visualize images sampled from Linear-ODE and Linear-SDE using FLUX [25], respectively. As discussed previously, the particles drawn from the proposal distribution of Linear-ODE are identical. While Linear-SDE introduces variations across different particles, we find that (a) Linear-ODE (b) Linear-SDE (c) VP-SDE Figure 3. Sample variance test using FLUX [25] under linear and Variance Preserving (VP) interpolant. All samples share the same initial latent. Prompt: steaming cup of coffee. the sample variance remains insufficient for effectively exploring low-density regions. In the next section, we introduce inference-time interpolant conversion, which further increases the search space. 4.3. Inference-Time Interpolant Conversion To address the limitation of Linear-SDE, we draw inspiration from the effective use of particle sampling in diffusion models, where we identified key difference: the interpolant. While the forward process in diffusion models follows the Variance Preserving (VP) interpolant (αt = 1 exp (cid:82) exp 1 0 βsds), where βs is predefined variance schedule, flow models instead follow linear interpolant. To bridge this gap, we introduce inferencetime interpolant conversion, which transforms the linear interpolant into VP interpolant. (cid:82) 0 βsds, σt = (cid:113) 2 As shown in the previous works [20, 30], we note that given velocity model ut based on an interpolant (αt, σt) (e.g., linear), one can transform the vector field and generate sample based on new interpolant (αs, σs) (e.g., VP) at inference-time. The two paths xs = αsx0 + σsx1 and xt = αtx0 + σtx1 are connected through scale-time transformation: xs = csxts ts = ρ1(ρ(s)) cs = σs/σts, (11) where ρ(t) = αt σt ratio of the original and the new interpolant, respectively. and ρ(s) = αs σs define the signal-to-noise The velocity for the new interpolant is given as: us(xs) = cs cs σts σs σs σts σ2 ts xs + cs tsuts ts ts = , (cid:19) (cid:18) xs cs (cid:0)σs αs αs σs σ2 ts σ2 (σts αts αts σts ) (cid:1) (12) . cs = Plugging the transformed velocity into the SDE in Eq. 7 after computing the score using Eq. 9 gives our inferencetime interpolant conversion. Since the new trajectory follows the VP interpolant, we refer to this as VP-SDE. We visualize VP-SDE sampling in Fig. 2 (right). At inference-time, we query the velocity of the new interpolant from the original interpolant (purple arrow). 5 In Fig. 3 (c), we visualize the sample variance under VPSDE using FLUX [25]. As shown in the figure, VP-SDE produces significantly more diverse samples compared to Linear-SDE, which exhibits smaller variations across particles. This property of VP-SDE effectively expands the search space, improving particle sampling in flow models. Note that this takes the opposite approach of previous works on diffusion models, which aimed to convert the SDE into an ODE [34, 47, 48] and transform trajectories into straighter paths [20, 30] for faster generation. Importantly, while we modify the generative process of flow models to align with that of diffusion models, inference-time scaling with flow models still provides distinct advantages. The rectified trajectories of flow models [25, 32, 33] allow for much clearer posterior mean via Tweedies formula [42], leading to more precise future reward estimation and, in turn, more effective particle filtering. Furthermore, when generating samples using the SDE in Eq. 7, we enhance exploration by taking smaller time intervals during the early stages of the generative process, where the variance level is high, then gradually increasing the interval to accelerate generationenabled by the few-step generation ability of flow models (details in Appendix C). 5. Rollover Budget Forcing In this section, we propose new budget-forcing particle sampling strategy to maximize the use of limited compute in inference-time scaling. To the best of our knowledge, previous inference-time scaling approaches based on particle sampling for diffusion models [27, 45] use fixed number of particles throughout all denoising steps. However, our analysis shows that this uniform allocation leads to inefficiency, where the NFEs required at each denoising step to obtain sample xtt with higher reward than the current sample xt significantly varies across different runs. We present the analysis results in Appendix C. This motivates us to adopt rollover strategy that adaptively allocates NFEs across timesteps. First, given the total NFEs budget, the NFEs quota is allocated uniformly across timesteps. At each timestep, if particle xtt achieves higher reward than the current sample xt within the NFEs quota, we immediately proceed to the next timestep, rolling over the remaining NFEs. Otherwise, we select the particle with the highest expected future reward from the current set of particles as done by Li et al. [27]. We present the pseudocode of our method, RBF, in Appendix E. In the next section, we demonstrate the effectiveness of RBF, along with SDE conversion and interpolant conversion. 6. Applications In this section, we present the experimental results of particle sampling methods for inference-time reward alignment. In Appendix, we present i) aesthetic image generation, ii) implementation details of the search algorithms, iii) additional L.O. L.S. mouse pad has two pencils on it, the shorter pencil is green and the longer one is not V.S. Eight chairs Figure 4. Qualitative results of inference-time SDE and interpolant conversion. Employing SDE and VP interpolant conversion effectively scales model performance. Each column presents results from Linear-ODE (L.O.), Linear-SDE (L.S.), and VP-SDE (V.S.). qualitative results, and iv) scaling comparison of Best-of-N (BoN) and RBF. 6.1. Experiment Setup Tasks. In this section, we present the results for the following applications: compositional text-to-image generation and quantity-aware image generation, where the rewards are non-differentiable. For the differentiable reward case, we consider aesthetic image generation (Appendix D.1). In compositional text-to-image generation, we use all 121 text prompts from GenAI-Bench [19] that contain three or more advanced compositional elements. For quantity-aware image generation, we use 100 randomly sampled prompts from T2I-CompBench++ [18] numeracy category. For all applications, we use FLUX [25] as the pretrained flow model. We fix the total number of function evaluations (NFEs) to 500 and set the number of denoising steps to 10, which allocates 50 NFEs per denoising step. For comparison with diffusion models, we use Stable Diffusion 2 [43]. For diffusion models, we additionally report case where the number of denoising steps is set to 50 (totaling 2, 500 NFEs), allocating five times the compute budget compared to the flow model. As reference, we also include the results of the base pretrained models without inference-time scaling. Baselines. We evaluate inference-time search algorithms discussed in Sec. 2, including Best-of-N (BoN), Search over Paths (SoP) [36], SMC [21], CoDe [45], and SVDD [27]. We categorize BoN and SoP as Linear-ODE-based methods, as their generative processes follow the deterministic process in Eq. 6. For SMC, we adopt DAS [21]; however, when the reward is non-differentiable, we use the reverse transition kernel of the pretrained model as the proposal distribution. Table 1. Quantitative results of compositional image generation. denotes the given reward used in inference-time scaling. For particle-based sampling methods, the relative improvement in each cell is computed with respect to the Linear-ODE case. The best result in the given and held-out reward is highlighted by bold, and the runner-up is underlined. L.O., L.S., and V.S. indicate Linear-ODE, Linear-SDE, and VP-SDE, respectively. * denotes results with 50 denoising steps. Metric SD2 Diffusion Model SD2 SD2 SVDD [27] SD2 FLUX BoN SoP [36] SMC [21] Flow Model CoDe [45] SVDD [27] RBF (Ours) VQAScore [28] Inst. BLIP [10] (held-out) Aesthetic Score [44] V.S. V.S. L.O. L.O. 0.671 0.647 0. 0.886 0.726 0.879 0.844 0.841 L.S. 0.862 +2.50% V.S. 0.877 +4.28% L.O. 0. L.S. 0.887 +12.56% V.S. 0.914 +15.99% L.O. 0.788 L.S. 0.893 +13.32% V.S. 0.915 +16.12% L.O. 0.788 L.S. 0.900 +14.21% V.S. 0.925 +17.39% 0.741 0. 0.790 0.799 0.775 0.820 0.799 0.815 0.811 0.49% 0.836 +2.58% 0. 0.815 +3.69% 0.839 +6.74% 0.789 0.813 +3.04% 0.847 +7.35% 0. 0.813 +3.04% 0.843 +6.84% 5.107 5.115 5.181 5. 5.338 5.162 5.254 5.233 5.016 5.156 5.190 5.023 5. 5.200 5.052 5.249 5.200 5.072 5. BoN SoP [36] SMC [21] CoDe [45] SVDD [27] RBF (Ours) Three mugs are placed side by side; the two closest to the faucet each contain toothbrush, while the one furthest away is empty. Five ants are carrying biscuits, and an ant that is not carrying biscuits is standing on green leaf directing them. Figure 5. Qualitative results of compositional text-to-image generation. At inference-time, we guide the generation process using VQAScore [28] as the given reward, which evaluates the image-text alignment. 6.2. Compositional Text-to-Image Generation Evaluation Metrics. In this work, we refer to the reward used for inference-time scaling as the given reward. Here, the given reward is VQAScore, measured with CLIPFlanT5 [28], which evaluates text-image alignment. For the held-out reward, which is not used during inference, we evaluate the score using different model, InstructBLIP [10]. Additionally, we evaluate aesthetic score [44] to assess the quality of the generated images. Inference-Time SDE and Interpolant Conversion. The quantitative and qualitative results of compositional textto-image generation are presented in Tab. 1 and Fig. 5, respectively. As discussed in Sec. 4.2, the deterministic sampling process of flow models limits the efficiency of particle sampling. The results in Tab. 1 support this, showing that Linear-SDE consistently improves the given reward over the Linear-ODE case for all particle sampling methods. However, the limited search space of Linear-SDE, as discussed in Sec. 4.3, leads to suboptimal particle sampling performance, falling behind Linear-ODE-based sampling methods, BoN and SoP [36], in the held-out reward. Through inference-time interpolant conversion, VP-SDE further improves performance across all particle sampling methods. Notably, particle sampling methods generate high-reward samples without significantly compromising image quality, as reflected in the aesthetic score, which remains comparable to the base FLUX model [25]. Lastly, we note that the quality of the pretrained Stable Diffusion 2 (SD2) [43] falls behind that of FLUX [25], even with 5 the compute budget (SD2), highlighting the importance of adopting particle sampling through flow models. Qualitatively, SDE conversion and interpolant conversion shown in Fig. 4 bring consistent performance improvements. Note that these results are obtained by using SVDD [27]; however, similar trend is observed across all other particle sampling methods (see Appendix G.1). Rollover Budget Forcing. As discussed in Sec. 5, instead of fixing the number of particles throughout the denoising process, we explore adaptive budget allocation through RBF. Quantitatively, we demonstrate that budget forcing provides additional performance improvements, outperforming all 7 Table 2. Quantitative results of quantity-aware image generation. denotes the given reward used in inference-time scaling. For particle-based sampling methods, the relative improvement in each cell is computed with respect to the Linear-ODE case. The best result in the given and held-out reward is highlighted by bold, and the runner-up is underlined. L.O., L.S., and V.S. indicate Linear-ODE, Linear-SDE, and VP-SDE, respectively. * denotes results with 50 denoising steps. Metric SD2 Diffusion Model SD2 SD2 SVDD [27] SD2 FLUX BoN SoP [36] V.S. 22.200 18.590 V.S. 5.820 5.140 L.O. L.O. 11.430 1.760 3.460 4.870 0.020 0.050 0.470 0.470 0.130 0.580 0.470 0. 0.523 0.532 0.617 0.648 0.648 0.756 0.740 0.731 SMC [21] L.S. 1.140 +76.59% 0.630 +18.87% 0.768 +5.06% V.S. 1.070 +78.03% 0.620 +16.98% 0.756 +3.42% Flow Model CoDe [45] SVDD [27] RBF (Ours) L.S. L.O. 11.740 1.400 +88.07% 0.650 +622.22% 0.772 +21.96% 0.090 0.633 V.S. 1.090 +90.72% 0.650 +622.22% 0.778 +22.91% L.O. L.S. 11.740 1.140 +90.29% 0.660 +633.33% 0.761 +20.22% 0. 0.633 V.S. 0.840 +92.84% 0.750 +733.33% 0.765 +20.85% L.O. L.S. 11.740 0.890 +92.42% 0.700 +677.78% 0.761 +20.22% 0.090 0.633 V.S. 0.540 +95.40% 0.800 +788.89% 0.769 +21.48% 5.060 5.223 4.983 5.081 5.182 5.420 5.676 5.464 5. 5.506 5.179 5.471 5.571 5.179 5. 5.576 5.179 5.343 5.581 RSS [31] Accuracy [31] VQAScore [28] (held-out) Aesthetic Score [44] BoN SoP [36] SMC [21] CoDe [45] SVDD [27] RBF (Ours) Seven balloons, four bears and four swans. Six horses and six deers and four balloons. Figure 6. Qualitative results of quantity-aware image generation. At inference-time, we guide generation using the negation of RSS (Residual Sum of Squares) as the given reward, which measures the discrepancy between detected and target object counts. other particle sampling methods in the given reward. Qualitatively, Fig. 5 shows that our method, RBF efficiently searches for high-reward samples that best align with the given text prompt (see Appendix G.2 for additional qualitative results). 6.3. Quantity-Aware Image Generation Evaluation Metrics. Here, the given reward is the negation of the Residual Sum of Squares (RSS) between the target counts and the detected object counts, computed using GroundingDINO [31] and SAM [22] (details in Appendix F). Additionally, we report object count accuracy, which evaluates whether all object quantities are correctly shown in the image. For the held-out reward, we report VQAScore measured with CLIP-FlanT5 [28]. As in the previous application, we evaluate the quality of the generated images using the aesthetic score [44]. Results. The quantitative and qualitative results of quantity-aware image generation are presented in Tab. 2 and Fig. 6, respectively. The trend in Tab. 2 align with those in Sec. 6.2, demonstrating that SDE conversion and interpolant conversion synergistically enhance the identification of high-reward samples. Notably, particle sampling methods with Linear-SDE already outperform Linear-ODE-based methods, while interpolant conversion further enhances accuracy, achieving 4 5 improvement over the base FLUX model [25]. Notably, our RBF achieves the highest accuracy, outperforming all other particle-based sampling methods. Qualitatively, Fig. 6 shows that RBF effectively identifies high-reward samples that precisely match the specified object categories and quantities (additional qualitative results are presented in Appendix G.2). 7. Conclusion We introduced novel inference-time scaling method for flow models with three key contributions: (1) ODE-to-SDE conversion for particle sampling, (2) Linear-to-VP interpolant conversion for enhanced diversity and search efficiency, and (3) Rollover Budget Forcing (RBF) for adaptive compute allocation. We demonstrated the effectiveness of VP-SDE-based generation in applying off-the-shelf particle sampling to flow models and showed that our RBF combined with VP-SDE generation outperforms previous methods."
        },
        {
            "title": "References",
            "content": "[1] Michael S. Albergo, Nicholas M. Boffi, and Eric VandenEijnden. Stochastic interpolants: unifying framework for flows and diffusions. arXiv, 2023. 2, 5, 13 [2] Brian D.O. Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Applications, 1982. 14 [3] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. 1 [4] Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Universal guidance for diffusion models. In CVPRW, 2023. 3, 4 [5] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv, 2024. 3, 16 [6] Gabriel Cardoso, Yazid Janati El Idrissi, Sylvain Le Corff, and Eric Moulines. Monte carlo guided diffusion for bayesian linear inverse problems. In ICLR, 2024. 3 [7] Shoufa Chen, Chongjian Ge, Yuqi Zhang, Yida Zhang, Fengda Zhu, Hao Yang, Hongxiang Hao, Hui Wu, Zhichao Lai, Yifei Hu, Ting-Che Lin, Shilong Zhang, Fu Li, Chuan Li, Xing Wang, Yanghua Peng, Peize Sun, Ping Luo, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Xiaobing Liu. Goku: Flow based video generative foundation models. arXiv preprint arXiv:2502.04896, 2025. [8] Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong Chul Ye. Diffusion posterior sampling for general noisy inverse problems. In ICLR, 2023. 3, 4, 15, 16 [9] Kevin Clark, Paul Vicol, Kevin Swersky, and Fleet David J. Directly fine-tuning diffusion models on differentiable rewards. In ICLR, 2024. 3 [10] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose visionlanguage models with instruction tuning. In NeurIPS, 2023. 7, 18 [11] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. 2 [12] Zehao Dou and Yang Song. Diffusion posterior sampling for linear inverse problem solving: filtering perspective. In ICLR, 2024. [13] Arnaud Doucet, Nando De Freitas, Neil James Gordon, et al. Sequential Monte Carlo methods in practice. Springer, 2001. 17 [14] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis, 2024. 2024. 2 [15] Luca Eyring, Shyamgopal Karthik, Karsten Roth, Alexey Dosovitskiy, and Zeynep Akata. Reno: Enhancing one-step text-to-image models through reward-based noise optimization. In NeurIPS, 2024. 3 [16] Xiefan Guo, Jinlin Liu, Miaomiao Cui, Jiankai Li, Hongyu Yang, and Di Huang. InitNO: Boosting text-to-image diffusion models via initial noise optimization. In CVPR, 2024. 3 [17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 2 [18] Kaiyi Huang, Chengqi Duan, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2I-CompBench++: An Enhanced and Comprehensive Benchmark for Compositional Text-to-Image Generation . IEEE Transactions on Pattern Analysis Machine Intelligence, 2025. 6, 16 [19] Dongfu Jiang, Max Ku, Tianle Li, Yuansheng Ni, Shizhuo Sun, Rongqi Fan, and Wenhu Chen. Genai arena: An open evaluation platform for generative models. arXiv, 2024. 6 [20] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. 2022. 5, 6 [21] Sunwoo Kim, Minkyu Kim, and Dongmin Park. Testtime alignment of diffusion models without reward overoptimization. In ICLR, 2025. 2, 3, 4, 6, 7, 8, 13, 17, 20, 22, 23 [22] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross Girshick. Segment anything. In ICCV, 2023. 8, [23] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. In NIPS, 2023. 2 [24] Tomasz Korbak, Hady Elsahar, Germán Kruszewski, and Marc Dymetmant. On reinforcement learning and distribution matching for fine-tuning language models with no catastrophic forgetting. In NeurIPS, 2022. 3 [25] Black Forest Labs. FLUX. https://github.com/ black-forest-labs/flux, 2024. 1, 2, 5, 6, 7, 8, 15 [26] Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review. arXiv, 2018. 4 [27] Xiner Li, Yulai Zhao, Chenyu Wang, Gabriele Scalia, Gokcen Eraslan, Surag Nair, Tommaso Biancalani, Aviv Regev, Sergey Levine, and Masatoshi Uehara. Derivative-free guidance in continuous and discrete diffusion models with soft value-based decoding. arXiv, 2024. 2, 3, 4, 6, 7, 8, 13, 15, 16, 18, 21, 22, 23 [28] Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, and Deva Ramanan. Evaluating text-to-visual generation with image-to-text generation. arXiv, 2024. 1, 2, 7, 8, 16, 18, [29] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. In ICLR, 2023. 2, 4 9 [30] Yaron Lipman, Marton Havasi, Peter Holderrieth, Neta Shaul, Matt Le, Brian Karrer, Ricky TQ Chen, David Lopez-Paz, Heli Ben-Hamu, and Itai Gat. Flow matching guide and code. arXiv preprint arXiv:2412.06264, 2024. 5, 6 [31] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In ECCV, 2024. 1, 8, 16, 19 [32] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In ICLR, 2023. 2, 4, 6 [33] Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, and Instaflow: One step is enough for high-quality qiang liu. diffusion-based text-to-image generation. In ICLR, 2024. 2, 6 [34] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. In NIPS, 2022. [35] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In ECCV, 2024. 5, 13 [36] Nanye Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, YuChuan Su, Mingda Zhang, Xuan Yang, Yandong Li, Tommi Jaakkola, Xuhui Jia, and Saining Xie. Inference-time scaling for diffusion models beyond scaling denoising steps. arXiv, 2025. 3, 6, 7, 8, 17, 22, 23 [37] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. 2 [38] OpenAI. https : Learning to Reason with LLMs. / / openai . com / index / learning - to - reason - with-llms/, 2024. 2 [39] Mihir Prabhudesai, Anirudh Goyal, Deepak Pathak, and Katerina Fragkiadaki. Aligning text-to-image diffusion models with reward backpropagation. arXiv, 2023. 3 [40] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In NIPS, 2023. [41] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of Machine Learning Research, 2020. 2 [42] Herbert E. Robbins. An Empirical Bayes Approach to Statistics. Springer, 1992. 4, 6 [43] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 2, 6, 7 [45] Anuj Singh, Sayak Mukherjee, Ahmad Beirami, and Hadi Jamali-Rad. Code: Blockwise control for denoising diffusion models. arXiv, 2025. 2, 3, 6, 7, 8, 15, 18, 20, 22, 23 [46] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. 2015. [47] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. ICLR, 2021. 6 [48] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In ICLR, 2021. 2, 5, 6 [49] Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize from human feedback. In NeurIPS, 2020. 2, 3, 17 [50] Luming Tang, Nataniel Ruiz, Chu Qinghao, Yuanzhen Li, Aleksander Holynski, David Jacobs, Bharath Hariharan, Yael Pritch, Neal Wadhwa, Kfir Aberman, and Michael Rubinstein. Realfill: Reference-driven generation for authentic image completion. ACM TOG, 2024. 17 [51] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, MarieAnne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. arXiv, 2023. 2, [52] Masatoshi Uehara, Yulai Zhao, Kevin Black, Ehsan Hajiramezanali, Gabriele Scalia, Nathaniel Lee Diamant, Alex Tseng, Tommaso Biancalani, and Sergey Levine. Fine-tuning of continuous-time diffusion models as entropy-regularized control. arXiv, 2024. 3, 13 [53] Masatoshi Uehara, Yulai Zhao, Ehsan Hajiramezanali, Gabriele Scalia, Gökcen Eraslan, Avantika Lal, Sergey Levine, and Tommaso Biancalani. Bridging model-based optimization and generative modeling via conservative fineIn NeurIPS, 2024. 3, 4, 13, tuning of diffusion models. 16 [54] Bram Wallace, Akash Gokul, Stefano Ermon, and Nikhil Naik. End-to-end diffusion latent optimization improves classifier guidance. In ICCV, 2023. 3 [44] C. Schuhmann. Laion aesthetics. https://laion.ai/ blog/laion-aesthetics, 2022. 1, 2, 7, 8, 15, 16 [55] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In CVPR, 2024. 3 [56] Luhuan Wu, Brian Trippe, Christian Naesseth, David Blei, and John Cunningham. Practical and asymptotically exact conditional sampling in diffusion models. In NeurIPS, 2023. 3 [57] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: learning and evaluating human preferences for text-to-image generation. In NeurIPS, 2023. 3, 15, 16 [58] Kai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen, Qimai Li, Weihan Shen, Xiaolong Zhu, and Xiu Li. Using human feedback to fine-tune diffusion models without any reward model. In CVPR, 2024. 3 [59] Jiwen Yu, Yinhuai Wang, Chen Zhao, Bernard Ghanem, and Jian Zhang. Freedom: Training-free energy-guided conditional diffusion model. In ICCV, 2023. 3 [60] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all, 2024."
        },
        {
            "title": "Appendix",
            "content": "A. Proofs A.1. Derivation of the Target Distribution From Eq. 1, we obtain the target distribution of the pretrained model p0: 0, which maximizes the reward while maintaining proximity to the distribution 0(x0) = arg max Ex0q [r(x0)] βDKL [q(x0)p0(x0)] , r(x0) β log (cid:20) (cid:20) log = arg max Ex0q = arg min Ex0q (cid:90) = arg min q(x0) log q(x0) p0(x0) q(x0) p0(x0) (cid:21) q(x0) p0(x0) (cid:21) 1 β r(x0) dx0 (cid:90) 1 β q(x0)r(x0)dx0. This can be solved via calculus of variation where the functional is given as follows: [q(x0)] := (cid:90) (cid:18) q(x0) log q(x0) p0(x0) 1 β (cid:19) r(x0) dx0. Substituting q(x0, ϵ) := q(x0) + ϵη(x0) gives: [q(x0, ϵ)] = (cid:90) (cid:18) q(x0, ϵ) log q(x0, ϵ) p0(x0) 1 β (cid:19) r(x0) dx0, where η(x0) is an arbitrary smooth function, and ϵ is scalar parameter. Introducing Lagrange multiplier λ to constraint (cid:82) q(x0)dx0 = 1 gives: [q(x0, ϵ)] = (cid:90) (cid:18) q(x0, ϵ) log (cid:90) := {q; x0}dx0 q(x0, ϵ) p0(x0) 1 β (cid:19) r(x0) + λq(x0, ϵ)dx0 Then the problem boils down to finding function q(x0, ϵ) satisfying: ϵ (cid:12) (cid:12) (cid:12) (cid:12)ϵ= = 0 This can be solved using the Euler-Lagrange equation: q dx0 q = 0, where is derivative of with respect to x0 and tilde notation is dropped since the condition is to be satisfied at ϵ = 0. Note that does not appear in , so the Euler-Lagrange equation simplifies to: q = (cid:18) (cid:18) q(x0) log q(x0) p0(x0) 1 β (cid:19) (cid:19) r(x0) + λq(x0) = 0 = log q(x0) p0(x0) 1 β r(x0) + 1 + λ = 0. Solving Eq. 13 gives the target distribution 0, which minimizes the objective function in Eq. 1: 0(x0) = p0(x0) exp (cid:18) r(x0) β (cid:19) 1 λ 12 (13) (14) Lastly, the Lagrangian multiplier λ is obtained from the normalization constraint, exp(λ) = (cid:82) p0(x0) exp (cid:16) r(x0) (cid:17) β 1 dx0. Plugging this into Eq. 14 gives the target distribution presented in Eq. 2: 0(x0) = p0(x0) exp (cid:82) p0(x0) exp (cid:17) (cid:16) r(x0) β (cid:16) r(x0) β (cid:17) , dx0 (15) A.2. Derivation of the Optimal Policy Here, we provide the derivations of the optimal policy given in Eq. 3 for completeness, which is proposed in previous works [52, 53]. To sample from the target distribution defined in Eq. 15, previous studies utilize an optimal policy θ(xttxt). The optimal value function v(xt) is defined as the expected future reward at current timestep t: v(xt) = β log Ex0pθ(x0xt) (cid:20) exp (cid:18) r(x0) β (cid:19)(cid:21) The optimal policy is the policy that maximizes the objective function: θ(xttxt) = arg max Eq [v(xtt)] βDKL [q(xttxt)pθ(xttxt)] (16) (17) = pθ(xttxt) exp (cid:16) 1 (cid:82) pθ(xttxt) exp β v(xtt) (cid:17) β v(xtt) (cid:16) 1 (cid:17) . dxtt For completeness, we present the theorem. Theorem 1. (Theorem 1 of Uehara et al. [53]). The induced distribution of the optimal policy in Eq. 17 is the target distribution in Eq. 15. 0(x0) = (cid:90) (cid:40) p1(x1) 1 (cid:89) s=T θ(x 1 (cid:41) T ) dx 1 :1. However, computing the optimal value function in Eq. 16 is non-trivial. Hence, we follow the previous works [21, 27] and approximate it using the posterior mean x0t := Ex0pθ(x0xt) [x0]: v(xt) = β log (cid:18)(cid:90) (cid:18) exp (cid:18) r(x0) β (cid:18) r(x0t) β (cid:19)(cid:19) (cid:19) (cid:19) pθ(x0xt)dx0 = r(x0t). (18) B. Choice of Diffusion Coefficient β log exp Ma et al. [35] have shown that the diffusion coefficient can be chosen freely within the stochastic interpolant framework [1]. We present detailed derivations for completeness. Here, we use interchangeably to denote the standard Wiener process for both forward and reverse time flows. Proposition 1. For linear stochastic process xt = αtx0 + σtx1 and the Probability-Flow ODE dxt = ut(xt)dt that yields the marginal density pt(xt), the following forward and reverse SDEs with an arbitrary diffusion coefficient gt 0 share the same marginal density: Forward SDE: dxt = Reverse SDE: dxt = (cid:20) ut(xt) + (cid:20) ut(xt) g2 2 g2 2 (cid:21) log pt(xt) dt + gtdw (cid:21) log pt(xt) dt + gtdw. (19) (20) 13 Proof. When velocity field ut generates probability density path pt, it satisfies the continuity equation: pt(xt) = (pt(xt)ut(xt)) . Similarly, for the SDE dxt = ft(xt)dt + gtdw, the Fokker-Planck equation describes the time evolution of pt: pt(xt) = (pt(xt)ft(xt)) + 1 2 2 pt(xt) g2 (21) (22) where 2 denotes the Laplace operator. To find an SDE that yields the same marginal probability density as the ODE, we equate the probability density functions in Eq. 22 and Eq. 21, resulting in the following equation: (pt(xt)ft(xt)) + 1 2 2pt(xt) = (pt(xt)ut(xt)) g2 (pt(xt)(ft(xt) ut(xt))) = 1 2 2pt(xt) g2 (23) This implies that any SDE with drift coefficient ft(xt) and diffusion coefficient gt that satisfies Eq. 23 will generate pt. One particular choice is to set pt(xt)(ft(xt) ut(xt)) proportional to pt(xt), i.e., pt(xt)(ft(xt) ut(xt)) = Atpt(xt). Then Eq. 23 can be rewritten as: At2pt(xt) = 1 2pt(xt), g2 which leads to the relation At = 1 2 g2 . Similarly, the drift coefficient is given by: ft(xt) = ut(xt) + = ut(xt) + g2 pt(xt) pt(xt) g2 log pt(xt) 1 2 1 2 Thus, family of SDEs that generate pt takes the following form: (cid:20) ut(xt) + dxt = (cid:21) g2 log pt(xt) 1 2 dt + gtdw, which is the forward SDE presented in Eq. 19. Similarly, the reverse SDE in Eq. 20 can be derived by applying the time reversal formula, following Anderson [2]. In the experiment, for all SDEs, we use gt = t2, with its norm scaled by factor of 3, which we found to work well for all inference-time search algorithms. C. Adaptive Time Scheduling and Rollover Strategy In this section, we provide details of adaptive time scheduling and NFE analysis result which inspired rollover strategy. Adaptive Time Scheduling. As discussed in Sec. 4.3, to maximize the exploration space in VP-SDE sampling, we design the time scheduler to take smaller steps during the initial phasewhen variance is highand gradually increase the step size in later stages. Specifically, we define the time scheduler as tnew = (cid:112)1 (1 t)2. While this approach can be problematic when the number of steps is too lowresulting in excessively large discretization steps in later iterationswe find that using reasonable number of steps (e.g., 10) works well in practice, benefiting from the few-step generation capability of flow models. This setup effectively balances broad exploration space with fast inference time, highlighting one of the key advantages of flow models over diffusion models. 14 Figure 7. Analysis of number of function evaluations (NFEs) across timesteps. The NFEs required to achieve higher reward for each timestep. The plot illustrates the 1 sigma variation band. The blue-dotted line represents the uniform allocation of compute (NFEs) across timesteps. We observe that the NFEs required to identify higher-reward sample may exceed the uniformly allocated budget (blue dotted line). FLUX [25] DPS [8] SVDD [27] + DPS [8] RBF (Ours) + DPS [8] Bird Bat Table 3. Quantitative results of aesthetic image generation. denotes the given reward used in inference time. The relative improvement in each cell is computed with respect to the base model. The best result in each row is highlighted in bold. Metric FLUX [25] DPS [8] SVDD [27] +DPS [8] RBF (Ours) +DPS [8] Aesthetic Score [44] 5.795 6.438 +11.10% 6.887 +18.85% ImageReward [57] (held-out) 0.991 0.605 38.97% 1.077 +8.61% 7.170 +23.73% 1.152 +16.20% Figure 8. Qualitative results of aesthetic image generation. At inferencetime, we guide generate using the aesthetic score [44] as the given reward, which assesses visual appeal. NFE Analysis. As discussed in Sec. 5, we analyze the number of function evaluations (NFEs) required to obtain sample with higher reward than the current one. In Fig. 7, we visualize the variance band of the required NFEs across timesteps, with the blue-dotted line representing the uniform allocation used in previous particle sampling methods [27, 45]. Notably, uniform compute allocation may constrain exploration and fail to identify high-reward samples, as evidenced by crossings within the variance band. This observation motivates the use of rollover strategy to optimize compute utilization efficiently. As demonstrated in Sec. 6, our experiments confirm that RBF provides additional improvements over previous particle-samplingbased methods [27, 45]. D. Additional Results D.1. Aesthetic Image Generation In this section, we demonstrate that inference-time scaling can also be applied to gradient-based methods, such as DPS [8], for differentiable rewards. Specifically, we consider aesthetic image generation and show that RBF leads to synergistic performance improvements. We first derive the formulation of the proposal distribution for differentiable rewards and then present qualitative and quantitative results. 15 Table 4. Quantitative results of quantity-aware image generation in NFE scaling expriment. We use the same 100 prompts from T2I-CompBench [18] as in the quantity-aware image generation task. denotes the given reward used in the inference-time. NFEs RSS [31] Accuracy VQAScore [28] (held-out) Aesthetic Score [44] 50 100 300 500 1000 ) 50 100 300 500 ( 4.360 3.280 2.190 1.760 1.340 3.250 1.860 0.690 0.540 0.290 0.400 0.510 0.570 0.580 0.590 0.410 0.590 0.720 0.800 0.880 0.758 0.750 0.755 0.756 0. 0.756 0.760 0.779 0.769 0.777 5.408 5.522 5.463 5.420 5.466 5.560 5.627 5.503 5.581 5.526 Figure 9. Scaling behavior comparison of BoN and RBF. We plot the known reward (RSS) [31] against accuracy for different numbers of function evaluations: {50, 100, 300, 500, 1, 000}. Note that the horizontal axis is displayed on logarithmic scale. D.1.1. Gradient-Based Guidance Uehara et al. [53] have shown that the marginal distribution p (xt) exp (cid:19) (cid:18) v(xt) β (xt) is computed as follows: (cid:18) r(x0t) β pt(xt), (cid:19) pt(xt) exp where the approximation follows from Eq. 18. When the reward is differentiable (e.g., aesthetic score [44]), one can simulate samples from (xt) by computing its score function: log (xt) = log (cid:20) exp (cid:18) r(x0t) β (cid:19) (cid:21) pt(xt) = 1 β r(x0t) (cid:125) (cid:123)(cid:122) (cid:124) Guidance + log pt(xt) (cid:125) (cid:123)(cid:122) (cid:124) Pretrained Score . (24) For differentiable rewards, we incorporate the gradient-based guidance defined in Eq.24 into the SDE sampling process described in Eq.7. Notably, this approach is orthogonal to inference-time scaling, and RBF can be additionally utilized to further enhance performance. In the next section, we experimentally demonstrate that RBF can be effectively integrated with gradient-based guidance. D.1.2. Results The aesthetic image generation task aims to sample images that best capture human preferences, such as visual appeal. We use 45 animal prompts from previous work, DDPO [5]. The aesthetic score [44] serves as the given reward, while ImageReward [57] is used as the held-out reward. We present quantitative and qualitative results of aesthetic image generation in Tab.3 and Fig.8. Notably, RBF, implemented with DPS [8], achieves significant improvements on both the given and held-out rewards, even surpassing SVDD [27]. Qualitatively, RBF effectively adapts the pretrained flow model to better align with human preferences, particularly in terms of visual appeal. D.2. Scaling Behavior Comparison As discussed in Sec. 4, expanding the exploration space and applying budget forcing significantly enhance the efficiency of RBF, leading to superior performance improvements over BoN. Here, we compare the scaling behavior of BoN, representative Linear-ODE-based method, with RBF across different numbers of function evaluations (NFEs). 16 We present qualitative and quantitative results of scaling behavior in quantity-aware image generation in Fig.9 and Tab. 4, respectively. Our results indicate that allocating more compute leads to performance improvements for both BoN and RBF. However, the accuracy of BoN plateaus after 300 NFEs, whereas RBF continues to scale and achieves an accuracy of 0.88. Notably, RBF shows similar trend in the held-out reward, outperforming BoN and demonstrating its efficiency. Figure 10. Schematics of inference-time search algorithms. Linear-ODE-based methods, BoN and SoP use deterministic sampling process, whereas particle-sampling-based methods follow stochastic process. Note that RBF adaptively allocates NFEs across denoising timesteps. E. Search Algorithms In this section, we introduce the inference-time search algorithms discussed in Sec.2 along with their implementation details. An illustrative figure of the algorithms is provided in Fig.10. Here, we define the batch size (N ) as the number of initial latent samples and the particle size (K) as the number of samples drawn from the proposal distribution pθ(xttxt) at each denoising step. Best-of-N (BoN) [49, 50] is form of rejection sampling. Given generated samples {x(i) sample with the highest reward. 0 }N i=1, BoN selects the x0 = arg max {x(i) 0 }N i=1 r(x(i) 0 ). As presented in Sec. 6, we fixed the total compute budget to 500 NFEs and the number of denoising steps to 10, which sets the batch size of BoN to = 50. Search over Paths (SoP) [36] begins by sampling initial noises and running the ODE solver up to predefined timestep t0. Then the following two operations iterate until reaching = 0: 1. Applying the forward kernel: For each sample in the batch at time t, particles are sampled using the forward kernel, which propagates them from to + . 2. Solving the ODE: The resulting particles are then evolved from + to + b by solving the ODE. The top candidates with the highest rewards are selected. We followed the original implementations [36] for and b. We used = 2 and = 5. Sequential Monte Carlo (SMC) [13, 21] extends the idea of importance sampling to time-sequential setting by maintaining samples and updating their importance weights over time: w(i) tt = (cid:16) exp 0tt)/β r(x(i) (cid:16) (cid:17) (cid:17) . exp r(x(i) 0t)/β 17 At each step, indices {a(i) }N i=1 are first sampled from multinomial distribution based on the normalized importance weights: {a(i) }N i=1 Multinomial (cid:40) N, w(i) j=1 w(j) (cid:80)N (cid:41)N . i=1 These ancestor indices a(i) are then used to replicate high-weight particles and discard low-weight ones, yielding the resampled i=1. Lastly, one-step denoised samples are obtained from {x(a(i) set {x(a(i) ) ) }N }N i=1: t tt pθ(xttx(a(i) x(i) ) ). We used = 50 for all applications. Controlled Denoising (CoDe) [45] extends BoN by incorporating an interleaved selection step after every denoising steps. xtLt = arg max tLt}K {x(i) i=1 r(x(i) 0tLt) We used = 2, = 25, and = 2 for all applications. SVDD [27] approximates the optimal policy in Eq. 3 by leveraging weighted particles: θ(xttxt) i=1 tt (cid:80)K (cid:88) w(i) j=1 w(j) i=1 pθ(xttxt) (cid:16) tt = exp v(x(i) tt)/β tt {x(i) tt}K w(i) δx(i) tt (cid:17) . (25) At each timestep, the approximate optimal policy in Eq. 25 is sampled by first drawing an index att from categorical distribution: (cid:40) att Categorical tt w(i) j=1 w(j) tt (cid:80)K (cid:41)K i=1 (26) This index is then used to select the sample from {x(i) . In practice, SVDD uses β = 0, replacing sampling from the categorical distribution with direct arg max operation, i.e., selecting the particle with the largest importance weight. Following the original implementation [27], we used = 2 and = 25 for all applications. i=1, i.e., xtt x(att) tt}K tt Rollover Budget Forcing (RBF) adaptively allocates compute across denoising timesteps. At each timestep, when particle with higher reward than the previous one is discovered, it immediately takes denoising step, and the remaining NFEs are rolled over to the next timestep, ensuring efficient utilization of the available compute. To maintain consistency with SVDD [27], we set = 2, with the compute initially allocated uniformly across all timesteps. We present the pseudocode for sampling from the stochastic proposal distribution with interpolant conversion in Alg. 1. Specifically, the pseudocode for RBF with SDE conversion and interpolant conversion is provided in Alg. 2. Here, we denote {S(i)}M i=1 as sequence of timesteps in descending order, where S(1) = 1 and S(M ) = 0, and is the total number of denoising steps. F. Implementation Details F.1. Compositional Text-to-Image Generation In the compositional text-to-image generation task, we use the VQAScore metric as the reward, which evaluates image-text alignment using visual question-answering (VQA) model (CLIP-FlanT5 [28] and InstructBLIP [10]). Specifically, VQAScore measures the probability that given attribute or object is present in the generated image. To compute the reward, we scale the probability value by setting β = 0.1 in Eq. 3. 18 Algorithm 1: stoch_denoise: 1-step stochastic denoising Inputs: original velocity field u, original interpolant (α, σ), new interpolant (α, σ), diffusion coefficient g, current sample xs, current timestep s, denoising step size Outputs: Stochastically denoised sample xss 1 ts ρ1(ρ(s)) 2 us cs cs xs + cs tsuts cs σs/σts (cid:16) xs cs (cid:17) 3 ss 1 σs 4 fs = us g2 αs us αs xs αs σs αs σs 2 ss 5 (0, I) 6 xss xs fss + gs // Eq. 12 // Eq. // Eq. 8 Algorithm 2: Rollover Budget Forcing (RBF) Inputs: Number of denoising steps , timesteps {S(i)}M i=1, NFE quota {Q(i)}M i=1 Outputs: Aligned sample x0 r(x01) 1 x1 (0, I) 2 for {1, . . . , } do 3 4 5 6 7 9 10 11 12 S(i) S(i) S(i+1) for {1, . . . , q} do Q(i) x(j) ss stoch_denoise (xs, s, s) if < r(x(j) 0ss) then Q(i+1) Q(i+1) + Q(i) r(x(j) 0ss) xss x(j) ss // Alg. 1 // Sec. 5 break if = then arg maxk{1,...,q} r(x(k) xss x(k) ss 0ss) F.2. Quantity-Aware Image Generation In quantity-aware image generation, text prompts specify objects along with their respective quantities. To generate images that accurately match the specified object counts, we use the negation of the Residual Sum of Squares (RSS) as the given reward. Here, RSS is computed to measure the discrepancy between the detected object count ˆCi and the target object count Ci specified in the given text prompt: RSS = (cid:88) (cid:16) i=1 Ci ˆCi (cid:17) , where is the total number of object categories in the prompt. We additionally report accuracy, which is defined as 1 when RSS = 0 and 0 otherwise. For the held-out reward, we report VQAScore measured with CLIP-FlanT5 [28] model. Object Detection Implementation Details. To compute the given reward, RSS, it is necessary to detect the number of objects per category, ˆCi. Here, we leverage the state-of-the-art object detection model, GroundingDINO [31] and the object segmentation model SAM [22], which is specifically used to filter out duplicate detections. We observe that naïvely using the detection model [31] to compute RSS leads to poor detection accuracy due to two key issues: inner-class duplication and cross-class duplication. Inner-class duplication occurs when multiple detections are assigned to the same object within category, leading to overcounting. This often happens when an object is detected both individually and as part of larger group. Cross-class duplication arises when an object is assigned to multiple categories due to shared characteristics (e.g., toy airplane being classified as both toy and an airplane), making it difficult to assign it to single category. To address inner-class duplication, we refine the object bounding boxes detected by GroundingDINO [31] using SAM [22] and filter out overlapping detections. Smaller bounding boxes are prioritized, and larger ones that significantly overlap with existing detections are discarded. This ensures that each object is counted only once within its category. To resolve cross-class duplication, we assign each object to the category with the highest GroundingDINO [31] confidence score. This ensures that objects are classified into single category, preventing double counting across multiple classes. More qualitative results are presented in the following pages. 19 G. Additional Qualitative Results G.1. Comparisons of Inference-Time SDE Conversion and Interpolant Conversion L.O. L.S. Four balloons, one cup, four desks, two dogs and four microwaves. V.S. L.O. L.S. Four candles, two balloons, one dog, two tomatoes and three helicopters. V.S. ] 1 2 [ y n ] 1 2 [ S t Q ] 1 2 [ n i m ] 5 4 [ o i u ] 5 4 [ C t Q ] 5 4 [ o i o C Four rabbits, three apples, two mice and four televisions. Seven pigs snorted and played in the mud. Two frogs in tracksuits, competing in high jump. The frog in blue tracksuit jumps higher than the frog not in blue tracksuit. Three small, non-blue boxes on large blue box. L.O. L.S. Seven helmets V.S. L.O. L.S. Four couches, three candles, two fish, one frog and three plates. V.S. Seven lamps. Seven desks. In collection of hats, each one is plain, but one is adorned with feathers. Five origami cranes hang from the ceiling, only one of which is red, and the others are all white. Figure 11. Qualitative comparisons of inference-time SDE conversion and interpolant conversion. Each column represents Linear-ODE (L.O.), Linear-SDE (L.S.), and VP-SDE (V.S.). L.O. L.S. Four balloons, one cup, four desks, two dogs and four microwaves. V.S. L.O. L.S. Five hamburgers sizzled on the grill. V.S. Two men, four vases, four chickens and four ships. Six bicycles. Two people and two bicycles in the street, the bicycle with the larger wheels belongs to the taller person. There are two cups on the table, the cup without coffee is on the left of the other filled with coffee. L.O. L.S. Two giraffes, three eggs, two breads, three microwaves and four strawberries. V.S. L.O. L.S. Four pears, four desks, three paddles and two rabbits. V.S. One egg, three camels, four cars and four pillows. Seven women. Three sailboats on the water, each with sails of different color. Two birds are chasing each other in the air, with the one flying higher having long tail and the other bird having short tail. ] 7 2 ["
        },
        {
            "title": "D\nD\nV\nS",
            "content": "y n ] 7 2 ["
        },
        {
            "title": "D\nD\nV\nS",
            "content": "y n ] 7 2 ["
        },
        {
            "title": "D\nD\nV\nS",
            "content": "n i m ) O ( y n ) O ( R i u ) O ( n i m Figure 12. Qualitative comparisons of inference-time SDE conversion and interpolant conversion. Each column represents Linear-ODE (L.O.), Linear-SDE (L.S.), and VP-SDE (V.S.). G.2. Comparisons of Inference-Time Scaling"
        },
        {
            "title": "BoN",
            "content": "SoP [36] SMC [21] CoDe [45] SVDD [27] RBF (Ours) In room, all the chairs are occupied except one. In pack of wolves, each one howls at the moon, but one remains silent. An open biscuit tin contains three biscuits, one without sultanas is square-shaped and the other two are round-shaped. rose that is not fully bloomed is higher than rose that is already in bloom. There are two colors of pots in the flower garden; all green pots have tulips in them and all yellow pots have no flowers in them. Two dragons fly towards the castle, dragon with backpack and no hat on the left of the dragon without backpack and with hat. Figure 13. Additional qualitative results of compositional text-to-image generation task."
        },
        {
            "title": "BoN",
            "content": "SoP [36] SMC [21] CoDe [45] SVDD [27] RBF (Ours) Eight apples, three bicycles and five rabbits. Six helicopters buzzed over eight pillows. Five swans and seven ducks swam in the pond. Four drums, seven tomatoes, and five candles. Three chickens, four birds, and eight pears. Six airplanes flying over desert with seven camels walking below. Figure 14. Additional qualitative results of quantity-aware image generation task."
        }
    ],
    "affiliations": [
        "KAIST"
    ]
}