{
    "paper_title": "Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs",
    "authors": [
        "Huanyu Zhang",
        "Wenshan Wu",
        "Chengzu Li",
        "Ning Shang",
        "Yan Xia",
        "Yangyu Huang",
        "Yifan Zhang",
        "Li Dong",
        "Zhang Zhang",
        "Liang Wang",
        "Tieniu Tan",
        "Furu Wei"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While Multimodal Large Language Models (MLLMs) excel at visual understanding, they often struggle in complex scenarios that require visual planning and imagination. Inspired by how humans use sketching as a form of visual thinking to develop and communicate ideas, we introduce Latent Sketchpad, a framework that equips MLLMs with an internal visual scratchpad. The internal visual representations of MLLMs have traditionally been confined to perceptual understanding. We repurpose them to support generative visual thought without compromising reasoning ability. Building on frontier MLLMs, our approach integrates visual generation directly into their native autoregressive reasoning process. It allows the model to interleave textual reasoning with the generation of visual latents. These latents guide the internal thought process and can be translated into sketch images for interpretability. To realize this, we introduce two components: a Context-Aware Vision Head autoregressively produces visual representations, and a pretrained Sketch Decoder renders these into human-interpretable images. We evaluate the framework on our new dataset MazePlanning. Experiments across various MLLMs show that Latent Sketchpad delivers comparable or even superior reasoning performance to their backbone. It further generalizes across distinct frontier MLLMs, including Gemma3 and Qwen2.5-VL. By extending model's textual reasoning to visual thinking, our framework opens new opportunities for richer human-computer interaction and broader applications. More details and resources are available on our project page: https://latent-sketchpad.github.io/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 4 1 5 4 2 . 0 1 5 2 : r Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs Huanyu Zhang1,2,3, Wenshan Wu1, Chengzu Li4 Ning Shang1 Yan Xia1 Yangyu Huang1 Yifan Zhang2,3 Li Dong1 Zhang Zhang2,3 Liang Wang2,3 Tieniu Tan3,5 Furu Wei1 https://latent-sketchpad.github.io/ 4Cambridge 3CASIA 2UCAS 1MSR 5NJU"
        },
        {
            "title": "Abstract",
            "content": "While Multimodal Large Language Models (MLLMs) excel at visual understanding, they often struggle in complex scenarios that require visual planning and imagination. Inspired by how humans use sketching as form of visual thinking to develop and communicate ideas, we introduce Latent Sketchpad, framework that equips MLLMs with an internal visual scratchpad. The internal visual representations of MLLMs have traditionally been confined to perceptual understanding. We repurpose them to support generative visual thought without compromising reasoning ability. Building on frontier MLLMs, our approach integrates visual generation directly into their native autoregressive reasoning process. It allows the model to interleave textual reasoning with the generation of visual latents. These latents guide the internal thought process and can be translated into sketch images for interpretability. To realize this, we introduce two components: Context-Aware Vision Head autoregressively produces visual representations, and pretrained Sketch Decoder renders these into human-interpretable images. We evaluate the framework on our new dataset MAZEPLANNING. Experiments across various MLLMs show that Latent Sketchpad delivers comparable or even superior reasoning performance to their backbone. It further generalizes across distinct frontier MLLMs, including Gemma3 and Qwen2.5-VL. By extending models textual reasoning to visual thinking, our framework opens new opportunities for richer humancomputer interaction and broader applications. Figure 1: (a) Latent Sketchpad extends frontier MLLMs (e.g. Gemma3 and Qwen2.5-VL) to interleave text and visual latents generation, incorporating visual thoughts into reasoning. (b) The framework enables interleaved generation by equipping the pretrained MLLM with Vision Head to generate visual latents autoregressively. separately pretrained Sketch Decoder visualizes these latents into interpretable sketches. Equal Contributions. Work done during internship at Microsoft Research. Preprint."
        },
        {
            "title": "Introduction",
            "content": "Multimodal Large Language Models (MLLMs) extend pretrained LLMs with sophisticated vision encoders [1, 2], demonstrating remarkable success on wide range of understanding tasks (e.g. VQA) [3, 4]. Furthermore, reasoning techniques such as Chain-of-Thought (CoT) [5] have enabled models to tackle complex challenges by generating step-by-step textual reasoning traces [6]. However, current MLLMs still face difficulties in more advanced multimodal reasoning scenarios, especially those requiring precise spatial reasoning and dynamic visual grounding [7, 8, 9, 10]. Humans naturally overcome such challenges by leveraging internal visual sketches alongside language, using mental imagery to simulate scenarios, test alternatives, and refine plans [11, 12]. This interplay between verbal and visual thinking is crucial for effective reasoning, as visual imagination provides complementary structure and clarity that language alone fails to convey [13]. Motivated by this, recent research has explored equipping MLLMs with visual thinking to enhance reasoning [14]. One common strategy for enhancing multimodal reasoning is to interface with external visual tools, such as object detectors [15, 16] or executable code generators [17, 18]. However, these approaches are constrained by predefined tool capabilities and dependence on external environments. Recent efforts such as MVoT [19] have explored synthesizing intermediate visual outputs to aid reasoning. To validate its effectiveness, MVoT employs unified generative architectures capable of producing both text and images. But these models [20, 21, 22] are fundamentally oriented toward pixel-level rendering. Their training objectives prioritize image realism over visual abstractions most conductive for reasoning. In parallel, frontier pretrained MLLMs like Qwen2.5-VL and Gemma3 [1, 2] excel at perceptual understanding through large-scale visionlanguage pretraining. However, they lack the native ability to generate visual content as part of their reasoning process. Critically, leveraging their pretrained visual features to actively produce visual thought for enhancing reasoning also remains largely unexplored. This gap prompts the question: Can the pretrained visual features of powerful MLLMs be repurposed as generative sketchpad to enable more complex multimodal reasoning? To address the limitations of existing approaches, we propose Latent Sketchpad, simple yet effective framework that extends pretrained MLLMs to integrate visual thoughts into their reasoning process, as illustrated in Figure 1(a). Inspired by human mental sketching for complex reasoning, Latent Sketchpad enables the model to generate continuous visual latents within its reasoning trajectory. Rather than decoding into images, these latents remain in the latent representation space during reasoning. Furthermore, our approach seamlessly integrates visual reasoning into the MLLMs autoregressive generation loop, without compromising its multimodal understanding capabilities. Specifically, as illustrated in Figure 1(b), we introduce Context-Aware Vision Head, which is responsible for generating visual latents at each reasoning step. It is conditioned not only on the current hidden state but also on the previous visual representations. This design allows the model to maintain visual coherence and refine its internal visual representation based on both interand intra-image contextual cues. To make these visual representations human-interpretable, we further propose standalone Sketch Decoder, pretrained to render visual latents into sketch-style images. This enables inspection of the models evolving reasoning trajectory, offering interpretable insight into the models internal visual thought process. Together, these components endow the MLLM with the ability to generate visual latents during reasoning and to render them into explicit, human-interpretable images. To evaluate the effectiveness of our framework, we construct MAZEPLANNING dataset featuring complex, interleaved multimodal reasoning trajectories. Experimental results demonstrate that Latent Sketchpad preserves the reasoning strength of pretrained MLLMs while augmenting it with interpretable visual traces. Moreover, Latent Sketchpad exhibits broad applicability, enabling models such as Gemma3 and Qwen2.5-VL to reason beyond text through internal visual generation. The main contributions of this paper include: We propose Latent Sketchpad, framework that equips pretrained MLLMs with Vision Head to interleave the autoregressive generation of visual latents and text, thereby enhancing their ability to perform complex multimodal reasoning beyond language-only deliberation. We introduce pretrained Sketch Decoder that faithfully visualize the pretrained visual features into images for transparent inspection of internal reasoning steps, and is broadly compatible with diverse pretrained vision encoders like CLIP and SigLIP. 2 Figure 2: Architecture of the Context-Aware Vision Head and Sketch Decoder. The Vision Head transforms hidden states from the MLLM backbone into visual latents. The Sketch Decoder operates independently, converting these latents into sketch-style images for visualization and interpretability. We validate the effectiveness of Latent Sketchpad through comprehensive evaluations and analysis. The results show that our approach yields interpretable visual traces while retaining plug-and-play modularity and broad applicability across diverse pretrained MLLMs."
        },
        {
            "title": "2 Latent Sketchpad",
            "content": "To solve complex problems, humans often go beyond language, creating internal mental sketches to organize thoughts and visualize solutions. Inspired by this dual-modality process, we propose Latent Sketchpad, framework that enables MLLMs to think visually by repurposing pretrained visual features to generate continuous visual latents alongside text. By integrating linguistic and visual representations, Latent Sketchpad enhances reasoning with greater expressiveness and interpretability. 2.1 Overview In the connector-based MLLM, pretrained vision encoder encodes an input image X0 into sequence of latent visual tokens: lX0 = G(X0) Rnvdv , where nv denotes the number of visual tokens and dv is the dimensionality of each token. connector module, as illustrated in Figure 1, projects these visual latents into the LLMs embedding space: hX0 = C(lX0) Rnvdh, where dh denotes the dimensionality of LLMs embedding. The resulting visual embeddings hv are then concatenated with text embeddings ht, forming multimodal input sequence. Our framework, as depicted in Figure 1, builds upon frontier MLLMs by introducing two new components: Context-Aware Vision Head: This vision head is integrated into the backbone. By leveraging previous visual features in the context, it generates context-aware visual latents from the internal hidden states of the backbone, reflecting the models evolving mental images. Pretrained Sketch Decoder: The decoder operates independently of the MLLM and serves as visualizer. By aligning the feature space of pretrained vision encoder with the latent space of pretrained VAE, it can translate the generated visual latents into sketch-style images. With the Vision Head, the model can interleave textual and visual latent generation during the autoregressive generation of multimodal reasoning traces. Meanwhile, the Sketch Decoder serves as visualization module, converting these internal latents into sketches. Together, our Latent Sketchpad supports interpretable and flexible multimodal reasoning. 2.2 Context-Aware Vision Head To interleave visual and textual reasoning within the the autoregressive generation, we introduce Context-Aware Vision Head. While the hidden state of the MLLM backbone provides prior 3 context information, fine-grained visual details may become attenuated during long-range multimodal reasoning. To address this, the Vision Head explicitly perform visual generation by leveraging both: 1) Global Context: the latents of all preceding images, serving as long-range visual memory. 2) Local Context: the partial latents already produced within the current image, capturing short-term visual continuity. Through the Vision Head, the resulting context-enriched visual latents can be projected into the language embedding space for continued autoregressive generation. Besides, they can also be decoded by our pretrained Sketch Decoder to produce interpretable sketch images. Auto-regressive Visual Latent Generation. The visual generation process begins with special <start_of_image> token, indicating the start of new image. Following this signal, the model enters an auto-regressive loop to generate the visual latents lXk for image Xk, one token at time. When generating the tth image token, as illustrated in Figure 2 (a), the Vision Head first collects hidden states from global context {h(last) i=1. Then all these hidden states are projected into visual latent space as {l j=0 and local context {h(last) }k1 }k1 j=0 and {l Xk,i}t i=0, respectively. Xk,i}t Xj Xj Xk , Xk1 = [l , . . . , Xk,t = [l Xk,0:t1, ] denote the global context latents, and Llocal Let Lglobal Xk,t] X1 represent the local context latents. Here Xk,0:t1 are the visual latents from previous steps within Xk and Xk,t is the current latent at t. To incorporate contextual knowledge into current latent generation, the Vision Head performs causal cross-attention on Llocal , as illustrated in Figure 2 (a). Xk Specifically, each token in the local context attends only to tokens preceding it across the entire sequence, thereby retrieving relevant visual cues from previously generated segments. This causal structure ensures that visual latents are generated in an autoregressive manner, with each image token conditioned on prior context. Subsequently, causal self-attention is applied over the current images local context latents Llocal , ensuring coherence within the current image. Xk The resulting context-enriched latent, Xk,t, is then projected back into the language embedding space to auto-regressively predict the next token. This process iterates until fixed number nv of visual tokens are generated, forming the complete latent sequence i=0 . The visual Xk generation concludes with the <end_of_image> token, after which text generation continues. Xk,i}nv1 and Lglobal = {l Xk Loss. To supervise the Vision Head, we apply latent-level regression loss between the predicted context-enriched latent and the target latent lXk , which is obtained from pretrained visual features Xk of the vision encoder. The loss can be instantiated using various similarity or distance measures (e.g., cosine similarity or L1 distance): Lreg = D(l Xk , lXk ), (1) where D(, ) denotes generic latent regression criterion. Training. The Vision Head is trained from scratch using the regression loss Lreg, while keeping all parameters of the MLLM frozen. This training scheme isolates the learning of visual latent generation from the backbone, thereby preserving the original reasoning capacity of the MLLM. 2.3 Pretrained Sketch Decoder To support transparent and interpretable multimodal reasoning, we introduce pretrained Sketch Decoder that converts pretrained visual features into human-interpretable sketches. Latent-to-Pixel Projection. The Sketch Decoder is designed as standalone visualization module, capable of decoding visual features obtained from pretrained ViT based vision encoder. As illustrated in Figure 2(b), the core component of the Sketch Decoder is learnable alignment network (AlignerNet [23]), which is implemented as Transformer-based architecture comprising an encoder and decoder. It projects the visual latents into the latent space of pretrained VAE. Specifically, since ViT features and VAE latent representations reside in distinct semantic spaces, the AlignerNet serves as mapping function, transforming the visual tokens into latent vectors. For example, sequence of visual latents lXk is projected by the AlignerNet into VAE-compliant latent codes ˆz. These transformed codes are subsequently fed into frozen VAE decoder (e.g., from SDXL-VAE [24]) to generate the corresponding pixel-space image Xk. 4 Loss. Given training image and its foreground mask {0, 1}HW , we first obtain target latent posterior q(z x) from the frozen VAE encoder EVAE. Meanwhile, the vision encoder extracts visual tokens, which are processed by AlignerNet to predict the parameters (µ, σ) of Gaussian distribution q(ˆz) = (µ, σ2). The latent ˆz is then decoded by the frozen VAE decoder DVAE to produce reconstruction ˆx = DVAE(ˆz). Together, these losses ensure alignment at both pixel and latent levels: = Lrec + Llatent + Lemb, (2) where: Lrec = Focal(ˆx, x, m) is focal reconstruction loss designed to put extra emphasis on foreground pixels where mij = 1; Llatent = NLLN (µ, σ; z) is the negative log-likelihood loss [25] that encourages the predicted latent distribution to approximate the ground-truth posterior; Lemb = 1 2 is mse loss between predicted and target patch embeddings. i=1 ei ˆei2 (cid:80)N Training. We employ the decoder of SDXL-VAE and use its encoder to provide target latent posterior during training. The transformer-based sketch decoder is trained from scratch, with both vision encoder and VAE model frozen. During pretraining, we use the Quick, Draw! dataset [26], which comprises 50 million sketch-style images across 345 categories."
        },
        {
            "title": "3 Experiments",
            "content": "3.1 Experimental Setups Data. To evaluate complex multimodal reasoning capabilities, we construct MAZEPLANNING dataset. It comprises 47.8K mazes of size from 35 to 55 for training, each accompanied by interleaved text-and-image reasoning sequences. Additionally, we provide test set of 500 mazes within the same size range, further divided into an easy set (< 45) and hard set (45 and 55) based on their size. Detailed dataset statistics and construction procedures are provided in the Appendix A. Models. We employ Gemma3-12B and Qwen2.5-VL-7B as our backbone, enhanced with Latent Sketchpad and fine-tuned on MAZEPLANNING to support interleaved text-image generation. Both models are evaluated under two reasoning modes: text-only CoT and multimodal CoT. To enable this, we adopt unified fine-tuning scheme that equips single model to operate in both modes. Visual generation is supported by our Vision Head, which is trained with the backbone frozen, making it plug-and-play without compromising the pretrained reasoning capacity of MLLMs. We also evaluate several proprietary models including GPT-4o, o1, o4-mini, and o3-pro. We further include GPT-4o + Latent Sketchpad setting, in which the Vision Head is trained solely on Qwen2.5-VL, keeping all backbone parameters frozen. Full implementation details are provided in the Appendix B. Evaluation Metrics. We extract the model-predicted action sequences by pattern matching the content enclosed between the <actions> and </actions> tags. We employ two complementary evaluation metrics: (1) Success Rate (SR) measures the proportion of test cases in which the model generates complete and correct action sequence. (2) Progress Rate (PR) quantifies the ratio of consecutively correct actions, reflecting how far the model progresses before making its first mistake. 3.2 Experimental Results We evaluate Latent Sketchpad on two representative MLLMs, Gemma3 and Qwen2.5-VL, and provide the results together with proprietary models in Table 1. Each model equipped with Latent Sketchpad is compared against its own backbone under consistent training protocol. The complete results across diverse training configurations and maze sizes are provided in Appendix C.4. Proprietary models struggles with complex and dynamic multimodal reasoning tasks. As shown in Table 1, the results show that even strong proprietary models (e.g., o4-mini, o3-pro) achieve less than 20% success rate on our MAZEPLANNING. In addition, their progress rates remain below 50%, underscoring the difficulty proprietary models face in complex and dynamic multimodal reasoning. These failures primarily stem from the models inability to track evolving spatial states (detailed in Appendix C.2.1), underscoring the limitations of these models in complex reasoning tasks. Notably, when GPT-4o is equipped with our Latent Sketchpad, the generated visual traces provide o3-pro refers to the version with access to external tools. 5 Table 1: Experimental results on MAZEPLANNING. o3-pro (tool) refers to the version with access to external tools. The Latent Sketchpad integrated with GPT-4o is trained with all Qwen2.5-VL weights frozen. The absolute improvement of models equipped with Latent Sketchpad (+LS) are highlighted in blue . (cid:213) and denote text-only output and interleaved text-image output. Model Output o1 o4-mini Proprietary o3-pro (tool) GPT-4o T Fine-tuned + LS (ours) (cid:213) , Gemma + LS (ours) (cid:213) , Qwen2.5-VL + LS (ours) (cid:213) , Success Rate(%) Progress Rate(%) Easy 21.00 28.33 24.33 11. +5.67 85.67 +2.67 65.67 +0.33 Hard Average 6.50 6.50 9.50 5.00 +1. 46.50 +1.50 33.00 +0.50 15.20 19. 18.40 8.60 +3.80 70.00 +2.20 52. +0.40 Easy 40.72 49.88 46.03 32. Hard 27.95 32.61 35.08 28.12 +10. +6.61 95.22 +0.86 88.32 +0.35 76. +0.05 70.91 +0.44 Average 35.61 42. 41.65 30.71 +9.06 87.57 +0.53 81. +0.39 complementary spatial cues that effectively guide its reasoning, yielding significant improvements in both success and progress rates. In particular, it achieves performance comparable to dedicated reasoning models and even surpasses o1 on progress rate. Latent Sketchpad demonstrates promising plug-and-play capability. key advantage of Latent Sketchpad lies in its modular architecture: the Vision Head can be trained independently and attached to MLLMs without altering their parameters. This preserves the backbones original reasoning ability while seamlessly augmenting it with visual generation. Empirical results show that Latent Sketchpad can be attached to MLLMs without noticeable degradation in reasoning performance, while simultaneously enabling the generation of visual traces that support multimodal reasoning. Besides, when integrated with GPT-4o, Latent Sketchpad effectively lifts the intrinsic limitation of text-only reasoning. Despite training only the Vision Head, it substantially enhances GPT-4os reasoning: the generated visual traces provide complementary spatial cues and yield significant performance gains. These results underscore Latent Sketchpad promising plug-and-play capability. Latent Sketchpad exhibits broad applicability across different MLLMs. Our experiments demonstrate that Latent Sketchpad seamlessly adapts to diverse pretrained backbones, including Gemma3 and Qwen2.5-VL. Despite their architectural differences, Latent Sketchpad consistently enables these models to externalize internal visual features as explicit reasoning traces, thereby enhancing interpretability and extending their multimodal reasoning capacity. This highlights Latent Sketchpad as generally applicable enhancement for diverse MLLMs."
        },
        {
            "title": "4 Discussion and Analysis",
            "content": "4.1 Generalization and Compatibility of the Pretrained Sketch Decoder To assess the generalization ability of our pretrained Sketch Decoder, we evaluate its zero-shot reconstruction performance on unseen samples from the MAZEPLANNING test set. As shown in Figure 3 (a), the decoder achieves consistently high SSIM (Structural Similarity) scores across three representative vision encoders (OpenCLIP, Qwen2.5-VL, and Gemma3), demonstrating strong generalization. Notably, these encoders differ significantly in pretraining schemes: Qwen2.5-VLs encoder employs window attention and is trained from scratch, while Gemma3 adopts SigLIPinitialized encoder, highlighting our Sketch Decoder compatibility with diverse ViT-based vision encoders. In addition, qualitative examples (Figure 3 (b)) further present the decoders ability to reconstruct sketches with high structural fidelity. Additional examples are provided in Appendix C.5. 4.2 Visualization Quality in Downstream Reasoning Task Qualitative Analysis. Figure 4 illustrates examples of visual thoughts generated by Latent Sketchpadenhanced Gemma3 and Qwen2.5-VL on in-distribution test set. As shown in the figure, while the 6 Figure 3: Illustration of generalization and compatibility of the pretrained Sketch Decoder. (a) Quantitative reconstruction results (SSIM) across different vision encoders (OpenCLIP, Qwen2.5-VL and Gemma3) on unseen samples from MAZEPLANNING. (b) Qualitative examples of reconstructed sketches from visual latents produced by each encoder. Figure 4: Qualitative analysis illustrating visualizations from Latent Sketchpad-enhanced Gemma3 and Qwen2.5-VL on in-distribution mazes. visualizations rendered via our Sketch Decoder may appear lower in perceptual quality, such as the arrows or digits, they exhibit great structural stability. This can be attributed to the Context-Aware Vision Head, which allows semantic context to dynamically guide the visual trajectory and enforce structural consistency throughout the planning process. More examples are provided in Appendix C.6. Quantitative Analysis. To evaluate the quality of generated visual traces, we introduce two metrics: Layout Consistency Rate (LCR): whether the generated images preserve the spatial configuration of the maze, including the start point, end point, and wall placements Visual Success Rate (VSR): Assesses whether valid path from the start to the goal is successfully drawn within the correct maze layout. Table 2: Quantitative results of visualization quality on MAZEPLANNING. First and Last refer to the first and final visualizations within complete reasoning sequence, respectively. As summarized in Table 2, our Latent Sketchpad consistently performs well across different MLLMs. We highlight two key findings from these results: (1) Latent Sketchpad preserves visual contextual consistency. Across both models, Latent Sketchpad achieves notably high LCR, reflecting its stronger ability to maintain spatial structure throughout reasoning steps. This contextual stability enables MLLMs to plan valid paths, as evidenced by the correlation between layout consistency and VSR. (2) Latent Sketchpad shows potential to support reasoning through visual generation. For Gemma3 equipped with Latent Sketchpad, the VSR reaches 75.6%, substantially higher than the baseline SR of 70%. Therefore, as illustrated in Table 1, its performance is enhanced by the generated visual traces (70% to 72.2%). consistent trend is also observed on Qwen2.5-VL, further confirming the ability of Latent Sketchpad to facilitate reasoning through visual generation. Layout Consistency Rate (%) Overall Last First 99.34 99.20 99.40 98.77 98.60 99.80 Visual Success Rate (%) 75.60 66.60 Gemma3+LS Qwen2.5-VL+LS 7 Figure 5: Visualizations from Latent Sketchpad on Gemma3 and Qwen2.5-VL in the OOD test set. Table 3: Performance on the OOD test set of MAZEPLANNING. Table 4: Ablation results across different components. Qwen2.5-VL Gemma3 Gemma3+LS SR (%) 5.50 8.00 10. PR (%) 32.16 38.76 39.39 Gemma3 w/o adaptation Gemma3+LS - w/o augmentation - w/ cosine Lreg SR (%) 9.40 72.20 54.20 71.40 PR (%) VSR (%) 33.04 88.10 77.47 87.65 - 75.60 68.20 73. 4.3 Further Analysis Out-of-Distribution Generalization. To further assess the generalization ability of Latent Sketchpad, we construct an OOD test set consisting of 200 mazes of size 66. Although fine-tuned Gemma3 and Qwen2.5-VL achieve strong performance on the in-distribution test set, their results drop sharply on the OOD set, as shown in Table 3. When equipped with Latent Sketchpad, Gemma3 shows improved robustness: it generates correct visual thoughts that yield performance gains  (Table 3)  , with examples illustrated in Figure 5 and failure cases in Figure 11. However, Qwen2.5-VL fine-tuned with our limited data does not yet exhibit clear generalization with Latent Sketchpad. This is mainly due to Qwen2.5-VL constructs visual tokens by concatenating four encoded features before projection, in contrast to Gemma3, which pools them directly. This design produces higher-dimensional input and demands substantially more data for generalization. Performance Across Maze Sizes As maze size increases, the evaluated models exhibit notable decline in performance. As shown in Figure 6, this trend holds consistently across both proprietary models and Gemma3 equipped with our Latent Sketchpad. While our method maintains higher success rate than the baselines across all maze scales, the increased spatial complexity in larger mazes presents greater challenge for accurate planning. 4.4 Ablations . Figure 6: Performance Variation with Maze Size We conduct series of ablation studies to investigate the effects of modality alignment, data augmentation strategy, and different choices of regression loss on model performance. Effect of Connector Adaptation. We investigate the impact of connector adaptation on model performance by analyzing whether the visual representations are updated during training. Taking Gemma3 as an example, freezing the connector severely impairs spatial understanding. The model often confuses directions such as left and right, leading to notable performance degradation as shown in the first row of Table 4. We also observe similar trends on Qwen2.5-VL. These findings highlight the critical role of connector adaptation during downstream task fine-tuning. 8 Data Augmentation Improves Visual Accuracy and Task Performance. To increase robustness, we introduce an augmentation strategy on the intermediate visual thoughts in the input of each training sample (detailed in Appendix B.4). The images are repeatedly reconstructed through our Sketch Decoder before being encoded, generating semantically equivalent but pixel-level perturbed views. This augmentation strategy preserves spatial semantics while injecting appearance variability, encouraging the model to focus on spatial structures. As shown in Table 4, the proposed augmentation improves the accuracy of visual thoughts and leads to higher task success rates. Choice of Regression Loss. We compare L1 loss and cosine similarity as regression objectives for training the Vision Head. Empirically, we find that L1 loss consistently outperforms cosine similarity across all evaluation metrics. This suggests that directly minimizing element-wise distance in latent space better preserves the spatial and semantic fidelity in Latent Sketchpad."
        },
        {
            "title": "5 Related Work",
            "content": "Multimodal Reasoning. Recent studies have enhanced multimodal reasoning with visual inputs through Chain-of-Thought (CoT) prompting [5] or the use of external tools such as cropping and zooming [15, 27, 18, 28], enabling more fine-grained visual perception during the reasoning process. Beyond tool-assisted approaches, methods like MVoT [19] and Visual Planning [29] generate visual thoughts natively for step-by-step reasoning, which demonstrate the feasibility and benefits of incorporating visual information as an additional modality for reasoning, complementing textual cues. While these methods reason across modalities in generative manner, they typically rely on unified auto-regressive models trained for multimodal generation, often operating over discrete token sequences [30, 22]. However, the potential to leverage the internal visual representation of pretrained MLLMs to generate visual thoughts directly remains largely underexplored. To address this gap, we propose Latent Sketchpad, lightweight framework enabling pretrained MLLMs to generate visual latents, integrating visual thinking directly into its native autoregressive loop. Latent Reasoning. Reasoning in large language models is often guided by explicit Chain-ofThought (CoT) prompting, where verbalizing intermediate steps improves final accuracy [5]. While effective, this approach is fundamentally constrained by the expressiveness of natural language. To overcome this, recent work on latent reasoning performs multi-step inference directly within the models continuous hidden states, forgoing explicit token generation [31]. These methods, developed primarily for text, typically use architectural modifications for recurrent computation [32, 33] or training strategies that induce implicit reasoning steps [34, 35]. In multimodal scenarios, latent representation also helps to alleviate the modality gap by avoiding discretizing the image into visual tokens, with most previous work focusing on multimodal generation [36] instead of reasoning. Yang et al. [37] introduce latent visual tokens to enable multimodal reasoning, but their approach is still limited to generating one single image as the answer image during the reasoning process. In contrast, our Latent Sketchpad enables pretrained MLLMs to actively generate and utilize visual latents interleaved with textual rationales as internal reasoning steps. Unified Multimodal Generation. Following recent advances in multimodal reasoning with textual outputs [38, 2, 1], unified models capable of multimodal generation have begun to emerge [39, 22, 40, 41, 42, 43]. These models extend output modalities beyond text to include images [22, 41, 44] and more [45, 46], typically through combination of autoregressive modeling and diffusion-based image decoders. Rather than training unified multimodal model from scratch, MetaMorph [21] introduces VPiT, which equips pretrained LLMs with the ability to both understand visual inputs and generate mixture of discrete text and continuous visual tokens. However, instead of reasoning, MetaMorph emphasizes image generation with surface-level semantics, which overlooks the intrinsic visual transitions within interleaved multimodal reasoning traces. In this work, we bridge that gap with context-aware vision head by enabling an MLLM that already understands visual inputs to generate coherent multimodal reasoning traces without requiring extensive pretraining."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduce Latent Sketchpad, simple yet effective framework that equips pretrained MLLMs with the ability to generate visual features as internal visual thoughts within their autoregressive 9 reasoning loop. Inspired by the role of mental sketching in human cognition, Latent Sketchpad introduce Context-Aware Vision Head to enable MLLMs to generate internal visual representations for enhanced reasoning, without relying on external tools. Additionally, separately pretrained Sketch Decoder can be employed to translate these latent representations into interpretable sketches, facilitating human understanding and interaction. Extensive experiments show that Latent Sketchpad extends the reasoning capabilities of frontier MLLMs, enriching them with interpretable visual traces. Moreover, it shows broad applicability across diverse backbones, highlighting its potential as general and plug-and-play enhancement. Our findings highlight the potential of integrating visual imagination directly into pretrained MLLMs, opening new avenues for more interpretable and capable multimodal systems."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [2] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. [3] YiFan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li, Kun Wang, Qingsong Wen, Zhang Zhang, et al. Mme-realworld: Could your multimodal llm challenge high-resolution real-world scenarios that are difficult for humans? In The Thirteenth International Conference on Learning Representations, 2025. [4] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models, 2024. URL https://arxiv. org/abs/2306.13394, 2(8), 2024. [5] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [6] Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, et al. From system 1 to system 2: survey of reasoning large language models. arXiv preprint arXiv:2502.17419, 2025. [7] Huanyu Zhang, Chengzu Li, Wenshan Wu, Shaoguang Mao, Yifan Zhang, Haochen Tian, Ivan Vulic, Zhang Zhang, Liang Wang, Tieniu Tan, et al. Scaling and beyond: Advancing spatial reasoning in mllms requires new recipes. arXiv preprint arXiv:2504.15037, 2025. [8] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking In in space: How multimodal large language models see, remember, and recall spaces. Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1063210643, 2025. [9] Chengzu Li, Caiqi Zhang, Han Zhou, Nigel Collier, Anna Korhonen, and Ivan Vulic. Topviewrs: Vision-language models as top-view spatial reasoners. arXiv preprint arXiv:2406.02537, 2024. [10] Chengzu Li, Wenshan Wu, Huanyu Zhang, Qingtao Li, Zeyu Gao, Yan Xia, José HernándezOrallo, Ivan Vulic, and Furu Wei. 11plus-bench: Demystifying multimodal llm spatial reasoning with cognitive-inspired analysis. arXiv preprint arXiv:2508.20068, 2025. [11] Raymond Bruyer and Jean-Christophe Scailquin. The visuospatial sketchpad for mental images: Testing the multicomponent model of working memory. Acta Psychologica, 98(1):1736, 1998. [12] David Pearson. Imagery and the visuo-spatial sketchpad. Working memory in perspective, pages 5379, 2002. [13] Allan Paivio. Dual coding theory: Retrospect and current status. Canadian Journal of Psychology/Revue canadienne de psychologie, 45(3):255, 1991. 10 [14] Zhaochen Su, Peng Xia, Hangyu Guo, Zhenhua Liu, Yan Ma, Xiaoye Qu, Jiaqi Liu, Yanshu Li, Kaide Zeng, Zhengyuan Yang, et al. Thinking with images for multimodal reasoning: Foundations, methods, and future frontiers. arXiv preprint arXiv:2506.23918, 2025. [15] Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing\" thinking with images\" via reinforcement learning. arXiv preprint arXiv:2505.14362, 2025. [16] Alex Su, Haozhe Wang, Weiming Ren, Fangzhen Lin, and Wenhu Chen. Pixel reasoner: Incentivizing pixel-space reasoning with curiosity-driven reinforcement learning. arXiv preprint arXiv:2505.15966, 2025. [17] Yushi Hu, Otilia Stretcu, Chun-Ta Lu, Krishnamurthy Viswanathan, Kenji Hata, Enming Luo, Ranjay Krishna, and Ariel Fuxman. Visual program distillation: Distilling tools and programmatic reasoning into vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95909601, 2024. [18] Junfei Wu, Jian Guan, Kaituo Feng, Qiang Liu, Shu Wu, Liang Wang, Wei Wu, and Tieniu Tan. Reinforcing spatial reasoning in vision-language models with interwoven thinking and visual drawing. arXiv preprint arXiv:2506.09965, 2025. [19] Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vulic, and Furu Wei. Imagine while reasoning in space: Multimodal visualization-of-thought. In The Forty-Second International Conference on Machine Learning, 2025. [20] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. [21] Shengbang Tong, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, and Zhuang Liu. Metamorph: Multimodal understanding and generation via instruction tuning. arXiv preprint arXiv:2412.14164, 2024. [22] Ethan Chern, Jiadi Su, Yan Ma, and Pengfei Liu. Anole: An open, autoregressive, native large multimodal models for interleaved image-text generation. arXiv preprint arXiv:2407.06135, 2024. [23] Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng, Wenhu Chen, and Furu Wei. Kosmos-g: Generating images in context with multimodal large language models. In ICLR, 2024. [24] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [25] Michael Tschannen, Cian Eastwood, and Fabian Mentzer. Givt: Generative infinite-vocabulary transformers. In European Conference on Computer Vision, pages 292309. Springer, 2024. [26] Jonas Jongejan, Henry Rowley, Takashi Kawashima, Jongmin Kim, and Nick Fox-Gieg. Quick, Draw! A.I. Experiment, 2016. URL https://quickdraw.withgoogle.com/. [27] Zhaochen Su, Linjie Li, Mingyang Song, Yunzhuo Hao, Zhengyuan Yang, Jun Zhang, Guanjie Chen, Jiawei Gu, Juntao Li, Xiaoye Qu, et al. Openthinkimg: Learning to think with images via visual tool reinforcement learning. arXiv preprint arXiv:2505.08617, 2025. [28] Xingyu Fu, Minqian Liu, Zhengyuan Yang, John Corring, Yijuan Lu, Jianwei Yang, Dan Roth, Dinei Florencio, and Cha Zhang. Refocus: Visual editing as chain of thought for structured image understanding. In The Forty-Second International Conference on Machine Learning, 2025. [29] Yi Xu, Chengzu Li, Han Zhou, Xingchen Wan, Caiqi Zhang, Anna Korhonen, and Ivan Vulic. Visual planning: Lets think only with images. arXiv preprint arXiv:2505.11409, 2025. [30] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. 11 [31] Rui-Jie Zhu, Tianhao Peng, Tianhao Cheng, Xingwei Qu, Jinfa Huang, Dawei Zhu, Hao Wang, Kaiwen Xue, Xuanliang Zhang, Yong Shan, et al. survey on latent reasoning. arXiv preprint arXiv:2507.06203, 2025. [32] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. Universal transformers. arXiv preprint arXiv:1807.03819, 2018. [33] Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, and Tom Goldstein. Scaling up test-time compute with latent reasoning: recurrent depth approach. arXiv preprint arXiv:2502.05171, 2025. [34] Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. Training large language models to reason in continuous latent space. arXiv preprint arXiv:2412.06769, 2024. [35] Jihoon Tack, Jack Lanchantin, Jane Yu, Andrew Cohen, Ilia Kulikov, Janice Lan, Shibo Hao, Yuandong Tian, Jason Weston, and Xian Li. Llm pretraining with continuous concepts. arXiv preprint arXiv:2502.08524, 2025. [36] Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, Ji Hou, and Saining Xie. Transfer between modalities with metaqueries. arXiv preprint arXiv:2504.06256, 2025. [37] Zeyuan Yang, Xueyang Yu, Delin Chen, Maohao Shen, and Chuang Gan. Machine mental imagery: Empower multimodal reasoning with latent visual tokens. arXiv preprint arXiv:2506.17218, 2025. [38] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. URL https: //llava-vl.github.io/blog/2024-01-30-llava-next/. [39] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. [40] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, Song Han, and Yao Lu. VILA-u: unified foundation model integrating visual understanding and generation. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=02haSpO453. [41] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. [42] Ruichuan An, Sihan Yang, Renrui Zhang, Zijun Shen, Ming Lu, Gaole Dai, Hao Liang, Ziyu Guo, Shilin Yan, Yulin Luo, et al. Unictokens: Boosting personalized understanding and generation via unified concept tokens. arXiv preprint arXiv:2505.14671, 2025. [43] Weifeng Lin, Xinyu Wei, Ruichuan An, Peng Gao, Bocheng Zou, Yulin Luo, Siyuan Huang, Shanghang Zhang, and Hongsheng Li. Draw-and-understand: Leveraging visual prompts to enable mllms to comprehend what you want. arXiv preprint arXiv:2403.20271, 2024. [44] Ruichuan An, Sihan Yang, Ming Lu, Renrui Zhang, Kai Zeng, Yulin Luo, Jiajun Cao, Hao Liang, Ying Chen, Qi She, et al. Mc-llava: Multi-concept personalized vision-language model. arXiv preprint arXiv:2411.11706, 2024. [45] Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan, Ge Zhang, Linyang Li, Hang Yan, Jie Fu, Tao Gui, Tianxiang Sun, Yu-Gang Jiang, and Xipeng Qiu. AnyGPT: Unified multimodal LLM with discrete sequence modeling. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 96379662, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/ v1/2024.acl-long.521. URL https://aclanthology.org/2024.acl-long.521/. 12 [46] Shufan Li, Konstantinos Kallidromitis, Akash Gokul, Zichun Liao, Yusuke Kato, Kazuki Kozuka, and Aditya Grover. Omniflow: Any-to-any generation with multi-modal rectified flows. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1317813188, 2025."
        },
        {
            "title": "A MAZEPLANNING",
            "content": "To facilitate research on visual-language reasoning in complex environments, we construct maze planning dataset that supports multimodal step-wise inference. A.1 Dataset Overview The dataset comprises 47.8K unique mazes for training, each with varying grid sizes. For evaluation, we provide two distinct test sets: (1) an in-distribution (ID) test set of 500 mazes drawn from the same size distribution as the training data, and (2) an out-of-distribution (OOD) test set of 200 larger mazes with fixed 66 grid configuration, designed to assess generalization to more complex scenarios. Each maze instance is annotated with multimodal trajectory that intertwines visual and textual reasoning steps. Unlike traditional grid-based formulations, we define action steps based on decision points to better reflect the natural, flexible reasoning process employed by humans. Specifically, we use the following three abstract action types: Go forward: Move straight until reaching the next decision point (e.g., an intersection or turn). Turn left: Rotate left before moving forward. Turn right: Rotate right before moving forward. To enable dynamic visual grounding during inferencei.e., determining the agents current location and verifying the correctness and plausibility of the inferred pathwe segment the reasoning process into discrete states. Each state comprises short sequence of [4, 6] actions, after which rendered image of the agents path so far is generated. The system then validates the inferred state: if the state is deemed valid and coherent, inference proceeds to the next state. To facilitate training, we decompose each mazes output label in the training set by individual states. During training, each sample is supervised to predict the reasoning process leading to the subsequent state. The complete statistics of our MAZEPLANNING dataset are provided in Table 1. Table 5: Statistics of the MAZEPLANNING dataset. Grid Size Action Length State Length Action Length of Each State 3 6.78 1.91 5.04 35 7.75 2.25 5.12 44 7.92 2.31 5.16 4 8.98 2.67 5.24 55 10.56 3.12 5.42 Train Set Size Test Set Size 5,758 100 9,559 9,548 100 9,580 100 13,355 100 66 22.96 - - 0 A.2 Dataset Curation Figure 7: Input image and label images for the same sample in Table 6. To ensure control over maze complexity and the interpretability of the reasoning process, we synthetically curated all maze samples and their corresponding annotations. Each maze was manually constructed to guarantee single unique solution path from the start point to the goal. The layout of each maze was designed with varying grid sizes and branching patterns to create diverse decision-making scenarios, while maintaining the property of unambiguous solvability. Based on the unique ground-truth trajectory of each maze, we manually annotated the sequence of actions (e.g. go forward, turn right and turn left) at key decision points. These annotations served as 14 Table 6: Textual reasoning steps for an example of MAZEPLANNING. MAZEPLANNING Input Text: Given the maze in the input image <image>, determine valid action sequence to navigate from the starting point (green arrow) to the endpoint (red circle). The black lines represent walls, and the white areas are traversable paths. Each action in the sequence must be one of the following: \"go forward\": Move straight until reaching the next turn or intersection. \"turn left\": Rotate left before moving forward. \"turn right\": Rotate right before moving forward. During the reasoning process, clearly mark each confirmed action using the format <actions>confirmed action</actions>. Label Text: Now, lets reason through the next 9 steps. At the mazes starting point, left turn corner presents itself, marking the initial curve in the path. Continuing along, right turn corner is encountered, leading to another turn in the corridor. Subsequently, another right turn corner directs the path further along the maze. Finally, left turn corner appears, guiding the way deeper into the labyrinth. Taking into account the visible layout of the maze, the next steps should be to move forward into the maze, then turn left and proceed forward, followed by right turn and advance, another right turn and move forward, and finally left turn to continue further into the maze. The actions of this part are <actions>go forward, turn left, go forward, turn right, go forward, turn right, go forward, turn left, go forward</actions> <image> Lets continue. Now, lets reason through the next 4 steps. The path begins with right turn corner, seamlessly transitioning into new section of the maze. Continuing through this segment leads to left turn corner, indicating another change in direction. Considering the structure of this maze section, the appropriate movement sequence is to first turn right and proceed forward, then make left turn and continue moving forward, exploring deeper into the maze. The actions of this part are <actions>turn right, go forward, turn left, go forward</actions> <image> Lets keep going. Now, lets reason through the next 2 steps. The path reaches the 1st junction, where the left path leads directly to the exit. Considering the structure of this maze section, the appropriate movement sequence is to turn left and proceed forward to reach the exit immediately. The actions of this part are <actions>turn left, go forward</actions> <image> The inference process has concluded. 15 Table 7: Hyper-parameters of fine-tuning different models with various settings. Hyper-Parameters Random Seed Epochs Learning Rate Global Batch Size LiquidT 42 13 0.0001 128 Liquid Gemma3 LS of Gemma3 Qwen2.5-VL LS of Qwen2.5-VL 42 13 0.0001 128 42 2 0.0001 128 42 5 0.0001 128 42 2 0.0001 128 42 5 0.0005 128 Table 8: Model version of proprietary models. Model Version GPT-4o 2024-11-20 o1 2024-12-17 o4-mini 2025-04-16 o3-pro 2025-06-10 the foundation for generating the multimodal reasoning sequences. To simulate natural, human-like step-by-step reasoning, we employed GPT-4o to synthesize rich textual descriptions for each sample. Given the ground-truth action sequence, GPT-4o was prompted to produce coherent reasoning narratives that align with the intended visual path, effectively integrating spatial reasoning, language generation, and task context. The resulting data instances thus comprise tightly coupled image-text sequences, designed to reflect realistic and interpretable reasoning workflows. An illustrative example of this multimodal reasoning process is provided in Table 6 and Figure 7."
        },
        {
            "title": "B Implementation Detail",
            "content": "B.1 Models The Context-Aware Vision Head consists of 2 layers of cross-attention, followed by 8 layers of self-attention. And the Sketch Decoder follows standard encoderdecoder transformer architecture, which comprises 12 encoder layers and 12 decoder layers. All the employed proprietary models are hosted on the Azure platform, with model version outlined in Table 7. We fine-tune both Qwen2.5-VL and Gemma3 on our MAZEPLANNING dataset. Additionally, we also employ discrete-token based unified MLLLM Liquid for finetuning. To support both text-only and multimodal chain-of-thought (CoT) reasoning within unified framework, we design fine-tuning scheme as follows. We fine-tune Gemma3-12B and Qwen2.5-VL-7B on single source of reasoning trajectories from the MAZEPLANNING dataset, which contain interleaved text and image states. During training, all images except the initial input are randomly masked with fixed probability (0.5). This strategy exposes the model to mixture of purely textual reasoning steps and interleaved textimage sequences, allowing single checkpoint to naturally operate in both text-only and multimodal modes at inference time. Visual generation is enabled through our Context-Aware Vision Head. This component is trained independently of the backbone. In this way, we preserve the original reasoning ability of the pretrained backbone while augmenting it with the capacity to generate visual thoughts. During Inference, we do not modify the decoding process for text-only CoT. For multimodal CoT, however, we automatically insert special token <start_of_image> during generation, which triggers the model to interleave textual and visual features. Specifically, on the MAZEPLANNING dataset, we append <start_of_image> immediately after each </actions> token, thereby enabling the model to generate the subsequent visual state. B.2 Hyper-Parameter Table 7 shows the hyper-parameters for training Liquid, Qwen2.5-VL and Gemma3. All models were trained on MI300X GPUs. Table 7 provides the details of GPU configurations and hyperparameters for various experimental settings. The backbone of Gemma3 and Qwen2.5-VL are both finetuned for https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct https://huggingface.co/google/gemma-3-12b-it https://huggingface.co/Junfeng5/Liquid_V1_7B 16 Table 9: Example of prompt template. Prompt Template Given the maze in the input image <image>, determine valid action sequence to navigate from the starting point (green arrow) to the endpoint (red circle). The black lines represent walls, and the white areas are traversable paths. Each action in the sequence must be one of the following: \"go forward\": Move straight until reaching the next turn or intersection. \"turn left\": Rotate left before moving forward. \"turn right\": Rotate right before moving forward. During the reasoning process, clearly mark each confirmed action using the format <actions>confirmed action</actions>. 2 epoch. As detailed in Appendix C.4, we have explored different training setting for the connector. Furthermore, for the training of the Latent Sketchpad or the Sketch Decoder, all loss weights were set to 1.0. All the employed proprietary models are hosted on the Azure platform, with model version outlined in Table 7. B.3 Prompting Templates Table 9 shows an example of prompting templates and responses with different system variants. B.4 Latent Reconstruction Augmentation Figure 8: Step-wise reconstruction of the input image over iterations. As described in Appendix A, the input of each training sample may include intermediate visual thoughts generated by the model. To improve the robustness of visual representations, we apply Latent Reconstruction Augmentation during training. Specifically, we repeatedly pass each input visual thought through the vision encoder and the pretrained decoder for up to rounds (k [0, 3]), reconstructing the image from its latent features in each step. This process preserves the semantic content while introducing minor perturbations in appearance, effectively encouraging the model to focus on stable spatial structures. The final reconstructed sketch is then used as the input image for training. Examples of this multi-step reconstruction are illustrated in Figure 8. B.5 Implementation details of GPT-4o + Latent Sketchpad. We first train the Latent Sketchpad on the MAZEPLANNING dataset. During training, only the parameters of the Vision Head are unfrozen, while all other components remain fixed. The optimization objective only employ the regression loss Lreg. For inference, we integrate the trained Latent Sketchpad into GPT-4o via plug-and-play interface. Specifically, we define stop sequence </actions> to capture the intermediate reasoning steps 17 generated by GPT-4o. Whenever all preceding actions inferred by GPT-4o are correct, the Latent Sketchpad is invoked to render an updated maze layout image based on the current reasoning context. This synthesized image is then fed back into GPT-4o, enabling it to continue reasoning with the visualized state representation. This iterative reasoning loop allows GPT-4o to refine its decision-making process by leveraging visual feedback in real time."
        },
        {
            "title": "C Additional Experiments and Discussion",
            "content": "C.1 Experiments on Liquid C.1.1 Impact of Modality Alignment Figure 9: Failure cases from Liquid with and without modality alignment. To investigate the impact of modality alignment during training, we compare two configurations of the Liquid model: one with the image embedding layer frozen (i.e., no alignment), and one with the image embedding layer unfrozen and trained jointly (i.e., with alignment). As shown in the left part of Figure 9, when the image embedding layer remains frozen, the model fails to establish meaningful correspondences between visual and textual modalities. This results in severely degraded outputsuninterpretable images characterized by chaotic. In contrast, enabling modality alignment by unfreezing the image embedding layer leads to significant improvement in semantic coherence. As illustrated in the right part of Figure 9, the generated images become more structured and visually plausible, often matching the intended high-level semantics of the task. C.1.2 Unstable Layout Consistency As illustrated in the right part of Figure 9, the model tends to alter the underlying maze layout itself. This structural inconsistency violates task constraints, as the generated solution no longer corresponds to the original maze, rendering the prediction incorrect despite its visual clarity. Such instability may stem either from the limited capacity of the base model or from the inherent brittleness of discrete tokenbased encoding. C.2 Error Analysis C.2.1 Proprietary Models Figure 10: Failure cases of o3-pro (tool). Despite the recent success of proprietary reasoning models in wide range of complex tasks, their performance on our MAZEPLANNING benchmark reveals notable limitations, which is presented in Table 1. As illustrated in Figure 10, even o3-pro, powerful reasoning model that supports external tool usage during inference, fails to solve certain maze navigation tasks. key failure mode we observe is the models inability to reliably localize itself during reasoning, especially in multi-step scenarios that require consistent visual tracking across states. Most models are able to correctly follow the initial steps. However, as the reasoning progresses and the agent moves deeper into the maze, these models often lose track of their spatial location, leading to compounding errors in path prediction and ultimately an incorrect final plan. These failures highlight fundamental gap in current proprietary systems: while they excel at executing external tools and producing fluent responses, they often lack internal visual thought, coherent internal representation of spatial progress and accumulated visual knowledge throughout reasoning sequence. In contrast, our proposed Latent Sketchpad explicitly maintains and updates such an internal visual memory, enabling dynamic localization and more accurate path planning. C.2.2 Latent Sketchpad Figure 11: Failure cases of Latent Sketchpad. To better understand the limitations of our proposed Latent Sketchpad framework, we conduct qualitative error analysis under both in-distribution (ID) and out-of-distribution (OOD) settings. In the ID setting, although the model performs well in most cases, we observe occasional failures where the predicted path exhibits spatial violations. As illustrated in the left part of Figure 11, the agent may generate trajectories that cut through maze walls or suddenly teleport to distant locations without following physically valid path. These discontinuities often lead to incorrect final plans, despite the individual actions appearing locally coherent. Furthermore, under the OOD setting (larger and unseen mazes), the model encounters different failure mode. For Gemma3, this manifests as gradual degradation of visual sketches, eventually causing the model to lose track of its position within the maze. In contrast, Qwen2.5-VL exhibits different limitation: due to its vision encoder producing features four times larger than those of Gemma3, our limited fine-tuning data is insufficient to ensure generalization. As result, Qwen2.5VL fails to preserve maze layouts reliably and struggles to generate valid navigation paths. These observations reveal two distinct types of failure: structural violations in familiar settings and cumulative degradation in novel environments, both of which point to potential avenues for future improvement in spatial consistency and robustness to distribution shifts. C.3 Performance of Gemma3 on MAZEPLANNING As shown in Table 10, the base Gemma3 model exhibits limited performance on MAZEPLANNING, indicating insufficient capability for complex spatial reasoning. To address this, we first fine-tune the model using text-only data to build foundational understanding. This step alone yields substantial performance improvement, confirming the effectiveness of text-only supervision in enhancing baseline reasoning abilities. It also establishes suitable backbone for directly equipping our Latent Sketchpad, enabling plug-and-play visual reasoning without requiring full model retraining. 19 Table 10: Task performance of the original Gemma3 and our fine-tuned Gemma3*. Model Gemma3 Gemma3* Standard-Size Maze ( 55) Extended-Size Maze (66) Success Rate (%) 5.80 70.00 Progress Rate (%) 24.15 87.57 Success Rate (%) 0.50 8. Progress Rate (%) 11.76 38.76 Table 11: Success Rate of different system variants on MAZEPLANNING Grid Size GPT-4o o1 o4-mini o3-pro LiquidT Liquid 3 4 6.00 31.00 42.00 32.00 55.00 91.00 3 5 8.00 16.00 25.00 21.00 49.00 72.00 4 4 2.00 16.00 18.00 20.00 43.00 75. 4 5 4.00 11.00 10.00 14.00 31.00 52.00 5 5 Overall 3.00 2.00 3.00 5.00 13.00 29.00 4.60 15.20 19.60 18.40 38.20 63.80 We do not report results on Qwen2.5-VL in this setting, as its weaker instruction-following capability prevents us from obtaining consistent and meaningful outputs. C.4 Task Performance To provide comprehensive comparison across different model configurations, we report the task performance of all system variants on mazes of varying sizes. The results of proprietary models and Liquid are presented in Table 11 (success rate) and Table 12 (progress rate). In addition, we conducted experiments under three connector tuning configurations for each model: (i) connector frozen throughout fine-tuning, (ii) connector unfrozen for one epoch, and (iii) connector unfrozen for two epochs. Our observations indicate that the two backbones exhibit distinct convergence behaviors, as illustrated in Table 13 and Table 14. When the connector remains frozen, both Qwen2.5-VL and Gemma-3 perform poorly. Allowing one epoch of connector tuning substantially improves Qwen2.5-VL, which adapts quickly, whereas Gemma3 still underperforms. In this regime, LS does not yield noticeable improvements on Gemma3 compared to Qwen2.5-VL, as the base model itself has not reached sufficiently strong level of task performance. When the connector is unfrozen for two epochs, Qwen2.5-VL achieves strong performance, leaving limited headroom for further gains. In this case, adding Latent Sketchpad results in visual success rate of 82.6, which is comparable to the text-only reasoning baseline (82.4) and thus brings little additional benefit. In contrast, Gemma3 benefits significantly from Latent Sketchpad under the same setting. With the visual success rate reaches 75.6, which is higher than its text-only baseline (70), the task performance of Latent Sketchpad enhanced Gemma3 increases to 72.2. C.5 Additional Qualitative Examples of Reconstruction As illustrated in Figure C.5, we present additional qualitative reconstruction results on unseen sketchstyle samples. These examples span variety of structural layouts and visual abstractions, and consistently demonstrate the decoders ability to recover key geometric and semantic patterns from the visual latent space. While minor degradations in fine-grained line reconstruction and color fidelity are observed, the current performance is sufficient for supporting visual reasoning within the Latent Sketchpad. Future work may further enhance visual fidelity to expand applicability in tasks requiring finer perceptual precision. C.6 Visualizations We additionally provide visualizations of the visual latents produced by the Latent Sketchpad on the MAZEPLANNING tasks, as presented in Figure 13. These examples, decoded via our pretrained Sketch Decoder, illustrate how the model leverages visual thoughts to organize spatial information and guide step-by-step decision making. The results demonstrate that even without photorealistic detail, the generated sketches capture sufficient structural cues to support accurate multimodal reasoning. 20 Table 12: Progress Rate of different system variants on MAZEPLANNING Grid Size GPT-4o o1 o4-mini o3-pro LiquidT Liquid 3 4 23.75 47.76 59.02 49.21 74.51 97.64 3 5 22.50 37.00 48.44 43.97 70.32 89.17 4 4 21.39 37.40 42.18 44.92 68.25 90. 4 5 20.10 33.46 36.97 40.60 60.98 81.19 5 5 Overall 20.78 16.14 35.61 22.44 42.97 28.25 41.65 29.56 63.54 43.63 84.67 64.44 Table 13: Success Rate of different system variants on MAZEPLANNING Gemma3 Gemma3 Gemma3+LS Gemma3 Gemma3+LS Qwen2.5-VL Qwen2.5-VL Qwen2.5-VL+LS Qwen2.5-VL Qwen2.5-VL+LS Connector Frozen 1 epoch 1 epoch 2 epoch 2 epoch Frozen 1 epoch 1 epoch 2 epoch 2 epoch 3 4 10.00 52.00 51.00 93.00 94.00 27.00 79.00 79.00 98.00 98. 3 5 17.00 30.00 30.00 85.00 86.00 17.00 64.00 63.00 96.00 94.00 4 4 12.00 21.00 24.00 79.00 85.00 18.00 54.00 56.00 95.00 94.00 4 5 5.00 23.00 21.00 59.00 61.00 12.00 45.00 45.00 79.00 81.00 5 5 Overall 3.00 8.00 7.00 34.00 35.00 2.00 21.00 22.00 44.00 43.00 9.40 26.80 26.60 70.00 72.20 15.20 52.60 53.00 82.40 82.00 Table 14: Progress Rate of different system variants on MAZEPLANNING Gemma3 Gemma3 Gemma3+LS Gemma3 Gemma3+LS Qwen2.5-VL Qwen2.5-VL Qwen2.5-VL+LS Qwen2.5-VL Qwen2.5-VL+LS Connector Frozen 1 epoch 1 epoch 2 epoch 2 epoch Frozen 1 epoch 1 epoch 2 epoch 2 epoch 3 4 34.54 65.94 65.11 98.21 98.78 53.19 92.72 92.92 99.46 99.46 3 5 44.05 55.84 54.56 95.74 95.19 48.65 87.06 86.93 98.26 97.39 4 4 37.80 51.11 51.84 91.72 94.27 42.92 85.17 86.16 98.61 98.14 4 5 27.79 48.40 47.93 84.71 85.20 39.85 78.90 78.22 93.44 93. 5 5 Overall 33.04 21.01 50.98 33.64 50.42 32.68 87.57 67.47 88.10 67.08 41.20 21.38 81.35 62.91 81.74 64.47 93.55 77.98 93.23 77.33 21 Figure 12: Additional qualitative examples of reconstructed sketches of Sketch Decoder. Figure 13: Examples of visual thoughts produced by Latent Sketchpad."
        }
    ],
    "affiliations": [
        "CASIA",
        "Cambridge",
        "MSR",
        "NJU",
        "UCAS"
    ]
}