{
    "paper_title": "MDAgent2: Large Language Model for Code Generation and Knowledge Q&A in Molecular Dynamics",
    "authors": [
        "Zhuofan Shi",
        "Hubao A",
        "Yufei Shao",
        "Dongliang Huang",
        "Hongxu An",
        "Chunxiao Xin",
        "Haiyang Shen",
        "Zhenyu Wang",
        "Yunshan Na",
        "Gang Huang",
        "Xiang Jing"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Molecular dynamics (MD) simulations are essential for understanding atomic-scale behaviors in materials science, yet writing LAMMPS scripts remains highly specialized and time-consuming tasks. Although LLMs show promise in code generation and domain-specific question answering, their performance in MD scenarios is limited by scarce domain data, the high deployment cost of state-of-the-art LLMs, and low code executability. Building upon our prior MDAgent, we present MDAgent2, the first end-to-end framework capable of performing both knowledge Q&A and code generation within the MD domain. We construct a domain-specific data-construction pipeline that yields three high-quality datasets spanning MD knowledge, question answering, and code generation. Based on these datasets, we adopt a three stage post-training strategy--continued pre-training (CPT), supervised fine-tuning (SFT), and reinforcement learning (RL)--to train two domain-adapted models, MD-Instruct and MD-Code. Furthermore, we introduce MD-GRPO, a closed-loop RL method that leverages simulation outcomes as reward signals and recycles low-reward trajectories for continual refinement. We further build MDAgent2-RUNTIME, a deployable multi-agent system that integrates code generation, execution, evaluation, and self-correction. Together with MD-EvalBench proposed in this work, the first benchmark for LAMMPS code generation and question answering, our models and system achieve performance surpassing several strong baselines.This work systematically demonstrates the adaptability and generalization capability of large language models in industrial simulation tasks, laying a methodological foundation for automatic code generation in AI for Science and industrial-scale simulations. URL: https://github.com/FredericVAN/PKU_MDAgent2"
        },
        {
            "title": "Start",
            "content": "MDAgent2: Large Language Model for Code Generation and Knowledge Q&A in Molecular Dynamics Zhuofan Shi1,2, Hubao A3, Yufei Shao4, Dongliang Huang1,2, Hongxu An1,2, Chunxiao Xin1,2, Haiyang Shen1,2, Zhenyu Wang1, Yunshan Na5, Gang Huang1,2, Xiang Jing1,2* 1Peking University. 2National Key Laboratory of Data Space Technology and System. 3The Hong Kong University of Science and Technology. 4Liaoning Technical University. 5Wenjing Future Lab (Beijing) Technology Co., Ltd. *Corresponding author(s). E-mail(s): jingxiang@pku.edu.cn; These authors contributed equally to this work. Abstract Molecular dynamics (MD) simulations are essential for understanding atomicscale behaviors in materials science, yet writing LAMMPS scripts remains highly specialized and time-consuming tasks. Although LLMs show promise in code generation and domain-specific question answering, their performance in MD scenarios is limited by scarce domain data, the high deployment cost of state-of-the-art LLMs, and low code executability. Building upon our prior MDAgent, we present MDAgent2, the first end-to-end framework capable of performing both knowledge Q&A and code generation within the MD domain. We construct domain-specific data-construction pipeline that yields three high-quality datasets spanning MD knowledge, question answering, and code generation. Based on these datasets, we adopt three stage post-training strategycontinued pre-training (CPT), supervised fine-tuning (SFT), and reinforcement learning (RL)to train two domain-adapted models, MD-Instruct and MD-Code. Furthermore, we introduce MD-GRPO, closed-loop RL method that leverages simulation outcomes as reward signals and recycles low-reward trajectories for continual refinement. We further build MDAgent2-RUNTIME, deployable multi-agent system that integrates code generation, execution, evaluation, and self-correction. Together with MD-EvalBench proposed in this work, 6 2 0 2 7 ] . [ 3 5 7 0 2 0 . 1 0 6 2 : r the first benchmark for LAMMPS code generation and question answering, our models and system achieve performance surpassing several strong baselines. This work systematically demonstrates the adaptability and generalization capability of large language models in industrial simulation tasks, laying methodological foundation for automatic code generation in AI for Science and industrial-scale simulations. URL: https://github.com/FredericVAN/PKU MDAgent"
        },
        {
            "title": "1 Introduction",
            "content": "Molecular dynamics simulation [1] has become core tool for exploring material and molecular behavior at the atomic scale [2]. With specialized simulation platforms such as LAMMPS, researchers can model wide range of physical processesfrom crystal construction to thermal conductivity analysis [3]. However, these simulations typically involve complex modeling scripts, strict physical constraints, and highly structured input formats, which demand strong domain expertise and extensive hands-on experience [4]. In addition, LAMMPS workflows often require large amount of repetitive manual work, becoming bottleneck that limits research efficiency[5]. In recent years, the rapid development of LLMs such as GPT [6], Qwen [7], and DeepSeek [8] has driven breakthrough progress in the emerging field of LLMs for Science [9]. Existing studies have shown that large language models (LLMs) are accelerating knowledge discovery and dissemination in materials science [10]. For example, Jablonka et al.[11] showcased multiple LLM applications in materials science and chemistry, while subsequent work[12] emphasized their potential in predictive chemistry. ChemLLM [13] fine-tunes LLMs for chemical materials, and MatterGen [14] employs generative modeling for inorganic material design. ChemCrow [15] leverages API tools to tackle chemistry-related problems, and HoneyComb [16] explores LLM-based agent systems for materials science, albeit without fine-tuning. Similarly, ChatMOF [17] uses AI systems to predict and generate metalorganic frameworks. MDCrow [18] uses prompt engineering and tool integration for automated MD workflows, and MDAgent [19] introduces fine-tuned large language model based molecular dynamics agent for code generation to obtain material thermodynamic parameters. Jacobs et al.[20] improved ORCA input generation via prompt engineering and lightweight synthetic fine-tuning. Dong et al.[21] fine-tuned Qwen2.5-7B to generate OpenFOAM CFD configs from natural language using 28k promptcode pairs. Zhang et al.[22] used LLMs to accelerate organic chemistry synthesis, achieving significant advances in active learningbased experimental optimization. Collectively, these studies validate the growing potential of large language models in materials science and the broader AI for Science paradigm. Although LLMs have demonstrated remarkable potential in scientific computing, adapting LLMs to highly specialized tasks such as LAMMPS-based MD simulations remains challenging. In summary, the main challenges include: Domain data scarcity and high construction difficulty: In materials science research, AI-driven approaches increasingly demand large-scale, high-quality ii datasets. However, the current materials data ecosystem still suffers from scarcity, fragmentation, inconsistent formats, limited accessibility, and uneven data quality issues that are particularly pronounced in text-to-code generation tasks. Although several databases and platforms exist, their coverage remains limited and data distribution scattered, making them insufficient to directly support domainspecific post-training of LLMs. At present, there is lack of high-quality datasets and systematic construction methodologies specifically tailored for molecular dynamics. Lack of evaluation benchmarks for LAMMPS: Although materials science benchmarks such as DiSCoMaT [23], MaScQA [24], ChemBench [25], and ChemIQ [26] have been developed, these benchmarks provide limited coverage of molecular dynamics simulations and LAMMPS code generation, making it difficult to comprehensively evaluate the capabilities of LLMs in this field. Insufficient research on code generation and lack of closed-loop optimization: Most existing studies focus on text-to-text generation, with limited exploration of text-to-code generation particularly for industrial and scientific simulation code in the materials science domain. Many existing methods remain at the one-shot generation stage, lacking closed-loop mechanisms for automated execution, evaluation, and self-correction, which hinders continuous improvement in the quality of generated code[19]. High cost and limited deployability of large-scale SOTA LLMs: Although state-of-the-art general-purpose LLMs exhibit certain capabilities in MD-related tasks, their practical use remains constrained. Proprietary commercial models (e.g., GPT-5.1) cannot be deployed locally due to closed-source restrictions, while ultra-large open-source models (e.g., Qwen3-235B) impose prohibitive inference costs. These limitations highlight the necessity of developing lightweight, domainspecialized LLMs that can be deployed efficiently on local systems while maintaining strong code generation and reasoning performance. Fig. 1 Overall workflow of the proposed MDAgent2, integrating data construction, model training, multi-agent runtime, and benchmark. iii To address these challenges, this paper proposes MDAgent2, the first framework designed for code generation and knowledge Q&A in Molecular Dynamics, as shown in Fig. 1. At the data level, we design structured data construction pipeline that produces three high-quality datasets: MD-Knowledge (for domain adaptive pretraining), MD-InstructQA (for instruction fine-tuning), and MD-CodeGen (for simulation code generation). These datasets cover diverse material systems, physical conditions, and simulation tasks, filling long-standing gap in high-quality data resources for the molecular dynamics domain. At the methodological training level, prior studies have shown that continued pre-training (CPT) and supervised fine-tuning (SFT) are key techniques for domain adaptation and task alignment of LLMs[27, 28]. Meanwhile, growing body of research on code LLMs indicates that incorporating end-to-end reinforcement learning (RL) with execution-based feedback can further improve code generation quality and reliability[29, 30]. In contrast, prior MDAgent[19] mainly relied on SFT and did not incorporate execution-feedback-driven RL into the training loop, leaving room for further improvement. Motivated by these findings, we conduct three-stage post-training pipeline: CPT, SFT, and RL. Specifically, CPT injects molecular dynamics and LAMMPS-related knowledge from large-scale unlabeled corpora, enhancing domain representations as well as the models mastery of specialized terminology and structured formats. We then perform SFT on high-quality instruction data to align the model with MD task requirements. Moreover, we introduce MD-GRPO, closed-loop RL framework built upon GRPO[31]. After generating simulation code, the system automatically executes it, evaluates the outcomes, and uses the resulting scores as reward signals to optimize the policy. In addition, we propose low-reward trajectory recycling mechanism to continually refine generation strategies, effectively improving the executability and physical correctness of the generated code. At the model level, we train two specialized models based on the Qwen3 series: MDInstruct (domain understanding model) and MD-Code (simulation code generation model), which substantially enhance the models capability to generate accurate and executable codes within materials science contexts. At the system level, we implement deployable multi-agent runtime system, MDAgent2-RUNTIME, which enables fully automated workflow from naturallanguage task descriptions to industrial-grade simulation code generation, execution, evaluation, and self-correction. At the evaluation level, we establish the first benchmark for molecular dynamics question answering and code generation, MD-EvalBench. Experimental results demonstrate that our LLM exhibits strong question-answering and code generation capabilities compared to selected baselines. Furthermore, MDAgent2-RUNTIME effectively enhances the models ability to generate correct and executable LAMMPS code. iv"
        },
        {
            "title": "2.1 Evaluation Benchmark for Molecular Dynamics:",
            "content": "MD-EvalBench To comprehensively evaluate the performance of LLMs in the MD domain, we construct an integrated benchmark suite named MD-EvalBench, consisting of three complementary datasets: MD-KnowledgeEval, LAMMPS-SyntaxEval, and LAMMPSCodeGenEval. These datasets jointly assess model capabilities from three key perspectives theoretical understanding, syntactic comprehension, and code generation. We collaborated with domain experts to design MD-KnowledgeEval and LAMMPS-SyntaxEval. Both datasets share unified question structure encompassing four types: single-choice, multiple-choice, fill-in-the-blank, and short-answer questions. MD-KnowledgeEval: Theoretical Knowledge Assessment in Molecular Dynamics. This dataset evaluates the models understanding of fundamental concepts, simulation principles, and thermodynamic systems in molecular dynamics. It contains total of 336 expert-curated questions covering topics such as interatomic potentials, integration algorithms, equilibrium conditions, and statistical ensembles. LAMMPS-SyntaxEval: Command and Syntax Understanding Assessment. This dataset focuses on the models comprehension of LAMMPS scripting including command usage, syntax rules, parameter structures, and functional modules thereby measuring its ability to interpret and reason about simulation scripts. It comprises 333 questions designed to test practical command-level proficiency. LAMMPS-CodeGenEval: Automatic Code Generation Quality Assessment. This dataset assesses the models ability to automatically generate executable LAMMPS scripts from natural language task descriptions. Each sample describes user-defined simulation objective, and the model is required to produce corresponding runnable code. Generated scripts are then evaluated through structural and functional scoring criteria to quantify generation accuracy and execution reliability."
        },
        {
            "title": "Evaluation Protocol",
            "content": "To ensure fair comparison, all experiments are repeated three times and the average results are reported. We adopt two primary metrics. Exec-Success@k measures the proportion of tasks for which at least one of the generated candidates can be successfully executed in LAMMPS. Code Human Score is subjective rating in the range [0, 10] assigned by domain experts based on readability, robustness, and physical correctness."
        },
        {
            "title": "Baselines and Compared Models",
            "content": "We compare MDAgent2 against range of representative systems and models: MDAgent [19], multi-agent framework using generateevaluaterewrite loop; Direct Prompting, which generates LAMMPS scripts via single prompt without tool integration, agent orchestration, or execution feedback. Qwen3 [7], general-purpose LLM family released by Alibaba. v"
        },
        {
            "title": "2.3 Evaluation of QA Ability",
            "content": "Table 1 Performance comparison across Overall Avg, MD-KnowledgeEval, and LAMMPS-SyntaxEval. Total Score denotes the aggregated QA performance over all question types, while scores under By Question Type correspond to different QA categories. indicates the absolute improvement over the 8B baseline (Qwen3-8B). Boldface highlights the best result in each column. MODEL Size / Access All vs 8B Single Multi Fill OpenQA Total Score By Question Type Overall Avg Qwen3-max Qwen3-32b MD-Instruct-8B Qwen-flash Qwen3-14b Qwen3-8b Large / Closed 32B / Open 8B / Open Large / Closed 14B / Open 8B / Open 82.49 77.34 74.67 73.47 72.91 70.50 +11.99 +6.84 +4.17 +2.97 +2.41 0.00 91.84 89.63 86.15 86.87 87.46 79.77 MD-KnowledgeEval Qwen3-max Qwen3-32b Qwen-flash Qwen3-14b MD-Instruct-8B Qwen3-8b Large / Closed 32B / Open Large / Closed 14B / Open 8B / Open 8B / Open 86.57 81.94 78.64 77.90 76.89 75.15 +11.42 +6.79 +3.49 +2.75 +1.74 0.00 93.38 88.08 89.40 87.42 86.10 80.13 LAMMPS-SyntaxEval Qwen3-max Qwen3-32b MD-Instruct-8B Qwen-flash Qwen3-14b Qwen3-8b Large / Closed 32B / Open 8B / Open Large / Closed 14B / Open 8B / Open 78.40 72.74 72.45 68.30 67.92 65.84 +12.56 +6.90 +6.61 +2.46 +2.08 0.00 90.30 91.18 86.20 84.33 87.50 79.41 72.83 69.81 69.67 60.84 60.93 66.28 75.29 72.94 61.18 63.53 65.88 65.88 70.37 66.67 73.46 60.49 58.33 66. 77.80 65.52 64.06 62.17 59.43 53.62 85.66 74.63 71.96 68.81 64.44 61.80 69.94 56.41 63.67 52.37 50.04 45.43 90.67 88.24 78.99 88.45 89.04 85.59 93.33 92.25 93.42 92.43 90.74 92.07 88.01 84.23 67.23 83.48 85.65 79. We evaluate the QA capability using MD-KnowledgeEval and LAMMPSSyntaxEval. The QA results in Table 1 reveal two clear findings. MD-Instruct-8B shows competitive performance despite its smaller size. With domain-specific post-training, MD-Instruct-8B attains an average score of 74.67, surpassing Qwen-Flash and Qwen3-14B, and substantially narrowing the gap to the much larger Qwen3-32B. This suggests that the model has acquired grasp of MDrelated domain knowledge, providing solid foundation for subsequent code-generation tasks. We attribute this advantage to domain-specific post-training: CPT injects MD knowledge and strengthens the models understanding of MD concepts, while SFT vi further consolidates this knowledge through supervised MD question answering and promotes faithful, well-structured responses. Qwen3-Max provides the strongest overall performance. Qwen3-Max achieves the highest average score of 82.49, demonstrating that large-scale SOTA LLMs still retain strong generalization ability in both MD theoretical knowledge and LAMMPS syntax understanding."
        },
        {
            "title": "2.4 Evaluation of Code Generation Ability",
            "content": "Fig. 2 Comparison of Code-Score-Human and Execution Success@3 across all methods. We evaluate the code generation capability using the LAMMPS-CodeGenEval benchmark. The results in Fig. 2 highlight several important observations. MDAgent2-RUNTIME significantly improves code-generation performance. Taking MD-Code-8B as an example, enabling the RUNTIME loop boosts vii ExecSucc@3 from 14.23 % to 37.95 %, and slightly improves Code-Score-Human from 9.29 to 9.32. These results highlight the value of iterative evaluation and self-correction in multi-agent system for improving the reliability and executability of generated LAMMPS scripts. Notably, the gains also rely on the task-specific tools we design for LAMMPS, which provide actionable feedback signals during the runtime loop. MDAgent2-RUNTIME outperforms the MDAgent [19] multi-agent framework. Across the three backbone models evaluated, MDAgent2 consistently achieves better results than the MDAgent framework. We attribute the improvement to two key differences: (i) MDAgent2 incorporates dedicated, LAMMPS-tailored tools; and (ii) it leverages feedback from actual LAMMPS execution, which was not available in MDAgent. Domain-specific post-training of MD-Code-8B is crucial. After posttraining, MD-Code-8B attains Code-Score-Human of 9.29 even under the Direct Prompting generation setting, indicating substantial improvement over the Qwen38B baseline. We attribute this gain to the full post-training stack: the first two stages strengthen domain knowledge and improve the models understanding of LAMMPS syntax and knowledge, while the MD-GRPO RL stage further enhances end-to-end code generation capability by optimizing directly against execution-based feedback. Consequently, domain-specific post-training substantially improves the models ability to produce accurate and executable LAMMPS scripts, providing stronger base model for agentic runtime optimization."
        },
        {
            "title": "3.1 Construction of Training Datasets",
            "content": "To support multi-task capability training and high-quality evaluation, three datasets were constructed in this study, respectively used for incremental pre-training, finetuning, and reinforcement learning. MD-Knowledge: Pretraining Corpus for Molecular Dynamics We organized team of domain experts to systematically collect thousands of high-quality molecular-dynamicsrelated papers, textbooks, technical documents, and public manuals as the primary sources for corpus construction. Following best practices established in large-scale pretraining corpora such as Lee et al.[32] and Soldaini et al.[33], we designed and implemented multi-stage data-cleaning pipeline to ensure that the resulting corpus is high-quality, low-redundancy, and minimally sensitive for large language model training: 1. Useless Text Filtering The raw text is first pre-screened to remove empty strings, corrupted content, and samples with excessively short lengths, thereby improving the baseline quality of the corpus. 2. Regex-based Content Removal Regular expressions are used to automatically identify and remove potential sensitive information such as DOI numbers, URLs, and email addresses. viii 3. Approximate Deduplication via MinHash and LSH The MinHash technique is used to represent textual data as approximate sets, and combined with Locality Sensitive Hashing (LSH) to perform large-scale efficient text deduplication. This effectively reduces semantically similar but textually varied redundant data. 4. Semantic Deduplication via Embedding Similarity Each text sample is encoded using the pretrained sentence embedding model to obtain its semantic representation. Cosine similarity between samples is then computed to further identify and remove semantically duplicate or highly similar content, achieving fine-grained deduplication. 5. High-Quality Sample Filtering via LLM Evaluation The deepseek-chat model is employed as an automatic quality evaluator. By designing appropriate prompts to guide the models assessment, the quality of each text is evaluated across multiple dimensionsincluding linguistic clarity, logical coherence, and information densityto select high-quality samples suitable for model training. MD-InstructQA: Domain-Specific InstructionAnswer Dataset for Molecular Dynamics Based on the original corpus resources collected above, we further constructed domain knowledge question answering dataset MD-InstructQA for the instruction finetuning stage through set of automated semantic extraction and question-answer generation processes. The dataset contains two parts: one with reasoning and one without reasoning. First, we convert original PDF documents into Markdown through multi-stage workflow encompassing filtering, metadata extraction, layout analysis, structured content extraction, specialized recognition of elements such as formulas and tables, reading order sorting, and new format generation. This enables seamless utilization of the content in downstream tasks like LLM fine-tuning. We then employed structure-sensitive hybrid chunking process to flexibly segment the converted Markdown documents into semantically coherent text chunks. This process, which takes titles, paragraphs, and catalogs into account, balances automation with manual control, ensuring the resulting chunks are compatible with LLM context windows while preserving semantic integrity. Next, we generate semantic domain label tree based on the semantic chunks, and use deliberately tailored prompts to generate questions that accurately reflect the key information in the label treethereby ensuring diversity in question styles. Furthermore, to generate answers that are faithful to the source textpreserving key factual details and semantic meaningwe employed knowledge-enhanced prompting with selectable chain-of-thought process. This approach prevents hallucination and ensures answers remain semantically aligned with the source content. Lastly, we paired the generated questions and answers into QA pairs and formatted them into the Alpaca schema to streamline the LLM fine-tuning workflow. MD-CodeGen: Code Generation Dataset for Molecular Dynamics Simulations This dataset consists of paired data samples in the form of (task description, LAMMPS code). On one hand, domain experts manually collected and constructed high-quality ix examples. On the other hand, we designed an automated task modeling approach to generate large-scale data by combining multiple key elements such as material system, research objective, and simulation conditions. Natural-language task descriptions are created through structured templates and then manually reviewed for correctness before being used as inputs. Subsequently, based on the proposed MDAGENT2-RUNTIME with SOTA LLMs produce physically meaningful LAMMPS scripts. Finally, simulation experts with extensive materials science experience carefully review and refine each generated sample, yielding high-quality synthetic dataset."
        },
        {
            "title": "3.2 Post-Training for MD-LLM",
            "content": "To enhance the models understanding and reasoning capabilities in molecular dynamics, we conduct three-stage post-training process on the Qwen3-8B backbone using our constructed datasets: MD-Knowledge, MD-InstructQA, and MD-CodeGen. The resulting model, denoted as MD-Instruct-8B and MD-Code-8B."
        },
        {
            "title": "3.2.1 CPT and SFT",
            "content": "Continual Pretraining (CPT). In this stage, we perform incremental domainadaptive pretraining by mixing the MD-Knowledge corpus with general-domain data. This process enhances the models representation of materials terminology, simulation workflows, and structural concepts, while preserving its general linguistic competence, thereby providing solid semantic foundation for subsequent instruction tuning. Supervised Fine-Tuning (SFT). Next, we construct an instruction-aligned training set by mixing MD-InstructQA with general instruction data. The model is fine-tuned under supervised objective to align its responses with domain reasoning patterns and improve factual accuracy. Additionally, curated subset of MD-CodeGen samples is introduced as cold-start seed to expose the model to taskcode mappings at an early stage. two these After stages, we language model, MD-Instruct-8B, capable of comprehending and answering molecular dynamicsrelated questions with strong domain fidelity, laying the groundwork for reinforcement learning in the subsequent MD-GRPO stage. obtain domain-aware"
        },
        {
            "title": "3.2.2 MD-GRPO",
            "content": "GRPO [31], widely adopted algorithm for reinforcement learning in LLMs [34], has already demonstrated substantial value, with domain-specific variants such as Med-R1 [35] and QoQ-Med [36] achieving strong performance in vertical applications. Building upon GRPO, we develop reinforcement learning framework named MD-GRPO to train LLMs to generate executable and goal-achieving LAMMPS scripts from natural-language task descriptions. The overall process consists of three stages: first, the model generates an initial code candidate based on the input task description; second, the script is executed through scheduler to perform the molecular dynamics simulation and produce the corresponding results; finally, an evaluator model assesses the quality of the simulation outputs and returns reward signal. This reward serves Fig. 3 Overview of the MD-GRPO training framework. The policy LLM generates candidate codes through the rollout module, which are evaluated by reward model against reference LLM. Group computation aggregates rewards and updates the policy via KL-regularized optimization. Low-scoring tasks are rewritten for robustness. as the feedback signal for policy optimization under the GRPO framework, forming closed-loop learning process that continuously improves both task completion and code validity. To further enhance the models self-correction capability, MD-GRPO introduces failure-feedback-tracking memory mechanism into each training iteration. Whenever the generated code fails to execute or receives low evaluation score, the system records the underlying failure cause and reconstructs the task context accordingly. The modified context is then used as an additional training sample in the next iteration, guiding the model to avoid the previously encountered mistakes. In this way, the framework extends traditional single-turn RL into multi-turn, trajectory-aware RL process that more faithfully simulates real runtime behavior, where multiple rounds of code refinement are often required. Consequently, the model not only learns how to produce valid simulation code, but also gradually understands why earlier attempts failed, thereby achieving an automated, reward-driven self-improvement loop."
        },
        {
            "title": "Total Reward",
            "content": "where: Rtotal = λ1Rformat + λ2Rcorrect, (1) Rformat {0, 1}: Format reward, which equals 1 only when the generated output satisfies the required formatting constraints; otherwise, it is 0. ˆRcorrect [0, 1]: Correctness reward, representing the normalized multidimensional score that evaluates the quality of the generated LAMMPS script. λ1 =1 and λ2 =5 by default."
        },
        {
            "title": "Format Reward",
            "content": "To ensure the model adheres to structured reasoning-and-answering protocol, we design binary format reward Rformat {0, 1} that imposes both syntactic and xi semantic constraints on the output. Specifically, the model is expected to first produce reasoning trace enclosed within <think> tags, followed by final answer enclosed in <answer> tags. Moreover, the content inside the <answer> block must be valid JSON object that can be parsed without error, and must contain exactly the required set of fields as specified by the taskno missing fields, and no extraneous ones. Rformat = 1, 0, if <think> and <answer> appear in order, and <answer> contains valid and structurally correct JSON otherwise This reward encourages the model to output not only interpretable reasoning steps, but also task-specific structured results that can be programmatically consumed and verified. During training, we implement the format check via regular expressions combined with strict JSON schema validation."
        },
        {
            "title": "Correct Reward",
            "content": "Based on the evaluation dimensions proposed in MDAgent [19], we invited domain experts to further refine and expand the framework into eight dimensions that comprehensively capture the essential aspects of lammps script quality assessment(see Appendices for details). Building on these dimensions, we design hybrid reward that integrates both additive bonus signals and penalty signals, enabling balanced evaluation of LAMMPS script quality across all key factors. Rraw = (cid:88) k=1 w+ Bk (cid:88) m=1 mPm, and we map it to [0, 1] via clip-and-rescale operation: Rcorrect = scale(clip(Rraw, Rmin, Rmax)) . where (2) (3) Bk {0, 1} is bonus indicator that equals 1 if the k-th key module is completed; Pm {0, 1} is penalty indicator that equals 1 if the m-th error type is detected. w+ are expert-assigned weights reflecting the relative importance of each and component. Rmin and Rmax denote the minimum and maximum possible values of Rraw."
        },
        {
            "title": "3.3 Runtime Multi-Agent Framework",
            "content": "After the model training stage, we construct the runtime multi-agent system MDAgent2-RUNTIME, built upon the previous version, MDAgent [19], as illustrated in xii Fig. 4 Overall architecture of the MDAgent2-RUNTIME multi-agent system. Fig. 4. Once user provides natural language task description, the system automatically completes code generation, simulation execution, and result evaluation without requiring any manual intervention. Additionally, to empower the agents we have developed suite of MD tools for agent use, as detailed in the Appendix A. visualization tool is provided to generate thermodynamic curves such as temperature, energy, and pressure versus time, as well as to convert atomic trajectory dump files into GIF animations for intuitive observation. An automated quality evaluation tool is implemented to identify the simulation type and to conduct multi-dimensional assessments based on energy stability, temperature control, pressure consistency, and numerical robustness. We further implement LAMMPS syntax verification tool that rapidly determines whether script is executable by performing an actual launch with timeout mechanism, returning results immediately without waiting for the full simulation to complete. In addition, based on our experimental findings, LLMs show limited capability in selecting appropriate potential functions. Therefore, we provide set of potentialrelated tools. In addition to listing all available local potential files and retrieving information about specific potential files, we also implement potential file management tool that automatically scans the generated LAMMPS scripts to detect missing local potential files and supplements them as needed to ensure smooth execution. Furthermore, when the potential function provided by the model is incorrect, we provide an automatic recommendation tool that suggests the Top-K most similar potential functions. The entire system is organized into three functional nodes: Code Generator, Code Runner, and Result Evaluator. In Code Generator, the Writer LLM drafts the initial LAMMPS code, which then undergoes two levels of verification by tools calling. First, the Syntax tools checks for syntax correctness, identifies potential issues, and provides feedback for revision. Next, the potential-file tools can be invoked, if the LLM has specified valid LAMMPS potential file, the tool ensures that the corresponding parameter file is available locally; xiii if the specified potential file does not actually exist, the tool automatically recommends the Top-K most similar LAMMPS potential files. Based on the feedback from these two checks, the Writer LLM internally revises and regenerates its code iteratively, continuing this loop until either convergence or predefined iteration limit is reached. The Code Runner executes the generated LAMMPS code safely within sandboxed Docker container by calling tools, ensuring environment isolation and reproducibility. Execution results are stored in designated directories, accompanied by an automatically generated summary of results. The Result Evaluator then analyzes both the input code and the simulation outputs based on the multi-dimensional evaluation criteria proposed in this work, generating structured score with detailed reward and penalty components. These feedback signals are returned to the Code Generator to guide further refinement; if the final score falls below predefined threshold, the system automatically initiates new iteration of code regeneration. Meanwhile, MDAgent2-RUNTIME supports human-in-the-loop interaction. Users may pause the automated workflow at any stage and provide natural-language feedback or directives. For example, before execution, users can modify the generated code, adjust simulation parameters, or inject domain-specific instructions. This flexible integration of automation and human expertise enables closed-loop optimization process that continuously improves code quality and simulation reliability."
        },
        {
            "title": "4 Discussion",
            "content": "In this work, we presented MDAgent2, an end-to-end large language model framework for molecular dynamics knowledge question answering and simulation code generation. By systematically addressing domain data scarcity, the lack of executable feedback, and insufficient evaluation standards in existing approaches, MDAgent2 advances MD code generation from one-shot text-to-code paradigm toward closed-loop workflow. Specifically, we constructed high-quality domain-specific data pipeline spanning MD knowledge, instruction reasoning, and LAMMPS code generation, and trained two lightweight yet effective domain-adapted models, MD-Instruct and MD-Code. Building upon these models, we introduced MD-GRPO, reinforcement learning framework that directly leverages simulation execution outcomes as reward signals, enabling continuous refinement toward executable and physically meaningful MD scripts. Furthermore, we implemented MDAgent2-RUNTIME, deployable multiagent system that integrates generation, execution, evaluation, and self-correction into an automated simulation pipeline. Extensive experiments on the proposed MD-EvalBench benchmark demonstrate that MDAgent2 and the MD-series LLMs can effectively address molecular dynamics question answering and simulation code generation tasks."
        },
        {
            "title": "Limitation and Outlook",
            "content": "Currently, the dataset covers simulation tasks including the computation of thermodynamic properties of materials, fluid dynamics simulations, and mechanical property xiv simulations of materials; however, we plan to further expand the range of supported task types in future work. Future work will explore the integration of multimodal LLMs into the MDAgent framework. The outputs of LAMMPS simulationssuch as visualized thermodynamic curves, atomic trajectories, and structural evolution in .gif or .png formatsprovide rich visual information that can be incorporated as additional evaluation inputs to improve interpretability and assessment granularity. Furthermore, the proposed framework demonstrates strong generality and scalability. Therefore, we aim to extend its application to other scientific fields in future work. Its modular design facilitates seamless adaptation not only within molecular dynamics but also across other industrial and materials science simulation domains, paving the way toward universal AI-driven framework for scientific code generation and autonomous experimentation. Acknowledgements. We acknowledge the support of the National Supercomputing Center in Tianjin for providing high-performance computing resources."
        },
        {
            "title": "Data and Code Availability",
            "content": "The data and code will be made publicly available upon acceptance at https://github. com/FredericVAN/PKU MDAgent"
        },
        {
            "title": "References",
            "content": "[1] Plimpton, S. Fast parallel algorithms for short-range molecular dynamics. Journal of computational physics 117, 119 (1995). [2] Hollingsworth, S. A. & Dror, R. O. Molecular dynamics simulation for all. Neuron 99, 11291143 (2018). [3] Frenkel, D. & Smit, B. Understanding molecular simulation: from algorithms to applications (Elsevier, 2023). [4] Xu, W., Tao, Y., Xu, H. & Wen, J. Theoretical trends in the dynamics simulations of molecular machines across multiple scales. Physical Chemistry Chemical Physics 26, 48284839 (2024). [5] Noe, F., Tkatchenko, A., Muller, K.-R. & Clementi, C. Machine learning for molecular simulation. Annual Review of Physical Chemistry 71, 361 390 (2020). URL https://www.annualreviews.org/content/journals/10.1146/ annurev-physchem-042018-052331. [6] Achiam, J. et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023). [7] Yang, A. et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388 (2025). xv [8] Liu, A. et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412. (2024). [9] Zhang, Y., Khan, S. A. & el al. Exploring the role of large language models in the scientific method: from hypothesis to discovery. npj Artificial Intelligence 1, 14 (2025). URL https://doi.org/10.1038/s44387-025-00019-5. [10] Miret, S. & Krishnan, N. A. Enabling large language models for real-world materials discovery. Nature Machine Intelligence 18 (2025). [11] Jablonka, K. et al. 14 examples of how llms can transform materials science and chemistry: reflection on large language model hackathon electronic supplementary information (esi) available. see. Digital Discovery 2, 12331250 (2023). [12] Jablonka, K. M., Schwaller, P., Ortega-Guerrero, A. & Smit, B. Leveraging large language models for predictive chemistry. Nature Machine Intelligence 6, 161169 (2024). [13] Zhang, D. et al. Chemllm: chemical large language model (2024). URL https: //arxiv.org/abs/2402.06852. arXiv:2402.06852. [14] Zeni, C. et al. Mattergen: generative model for inorganic materials design (2024). URL https://arxiv.org/abs/2312.03687. arXiv:2312.03687. [15] M. Bran, A. et al. Augmenting large language models with chemistry tools. Nature Machine Intelligence 6, 525535 (2024). URL https://doi.org/10.1038/ s42256-024-00832-8. [16] Zhang, H., Song, Y., Hou, Z., Miret, S. & Liu, B. Honeycomb: flexible llmbased agent system for materials science (2024). URL https://arxiv.org/abs/ 2409.00135. arXiv:2409.00135. [17] Kang, Y. & Kim, J. ChatMOF: an artificial intelligence system for predicting and generating metal-organic frameworks using large language models. Nature Communications 15, 4705 (2024). URL https://doi.org/10.1038/s41467-024-48998-4. [18] Campbell, Q., Cox, S., Medina, J., Watterson, B. & White, A. D. Mdcrow: Automating molecular dynamics workflows with large language models. arXiv preprint arXiv:2502.09565 (2025). [19] Shi, Z. et al. fine-tuned large language model based molecular dynamics agent for code generation to obtain material thermodynamic parameters. Scientific Reports 15, 10295 (2025). [20] Jacobs, P. F. & Pollice, R. Developing large language models for quantum chemistry simulation input generation. Digital Discovery 4, 762775 (2025). xvi [21] Dong, Z., Lu, Z. & Yang, Y. Fine-tuning large language model for automating computational fluid dynamics simulations. Theoretical and Applied Mechanics Letters 100594 (2025). [22] Zhang, Y. et al. Large language models to accelerate organic chemistry synthesis. Nature Machine Intelligence 113 (2025). [23] Gupta, T. et al. Discomat: distantly supervised composition extraction from tables in materials science articles. arXiv preprint arXiv:2207.01079 (2022). [24] Zaki, M., Krishnan, N. A. et al. Mascqa: investigating materials science knowledge of large language models. Digital Discovery 3, 313327 (2024). [25] Walker, T., Grulke, C. M., Pozefsky, D. & Tropsha, A. Chembench: cheminformatics workbench. Bioinformatics 26, 30003001 (2010). [26] Runcie, N. T., Deane, C. M. & Imrie, F. Assessing the chemical intelligence of large language models. arXiv preprint arXiv:2505.07735 (2025). [27] Xie, Y., Aggarwal, K. & Ahmad, A. Ku, L.-W., Martins, A. & Srikumar, V. (eds) Efficient continual pre-training for building domain specific large language models. (eds Ku, L.-W., Martins, A. & Srikumar, V.) Findings of the Association for Computational Linguistics: ACL 2024, 1018410201 (Association for Computational Linguistics, Bangkok, Thailand, 2024). URL https://aclanthology.org/ 2024.findings-acl.606/. [28] Lu, W., Luu, R. K. & Buehler, M. J. Fine-tuning large language models for domain adaptation: Exploration of training strategies, scaling, model merging and synergistic capabilities. npj Computational Materials 11, 84 (2025). [29] Wang, J. et al. Enhancing code llms with reinforcement learning in code generation: survey. arXiv preprint arXiv:2412.20367 (2024). [30] Gehring, J. et al. Rlef: Grounding code llms in execution feedback with reinforcement learning. arXiv preprint arXiv:2410.02089 (2024). [31] Shao, Z. et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300 (2024). [32] Lee, K. et al. Muresan, S., Nakov, P. & Villavicencio, A. (eds) Deduplicating training data makes language models better. (eds Muresan, S., Nakov, P. & Villavicencio, A.) Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 84248445 (Association for Computational Linguistics, Dublin, Ireland, 2022). URL https://aclanthology. org/2022.acl-long.577/. xvii [33] Soldaini, L., Kinney, R. & et al. Ku, L.-W., Martins, A. & Srikumar, V. (eds) Dolma: an open corpus of three trillion tokens for language model pretraining research. (eds Ku, L.-W., Martins, A. & Srikumar, V.) Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 1572515788 (Association for Computational Linguistics, Bangkok, Thailand, 2024). URL https://aclanthology.org/2024.acl-long.840/. [34] Zhang, K. et al. survey of reinforcement learning for large reasoning models (2025). URL https://arxiv.org/abs/2509.08827. arXiv:2509.08827. [35] Lai, Y. et al. Med-r1: Reinforcement learning for generalizable medical reasoning in vision-language models. arXiv preprint arXiv:2503.13939 (2025). [36] Dai, W., Chen, P., Ekbote, C. & Liang, P. P. Qoq-med: Building multimodal clinical foundation models with domain-aware grpo training. arXiv preprint arXiv:2506.00711 (2025)."
        },
        {
            "title": "Appendix A Tools Details",
            "content": "Table A1 provides comprehensive list of the tools integrated into the MDAgent2 system. Each entry details the tools name and its specific functionality within the molecular dynamics workflow. Supplementary to the system architecture discussed in Section 3, this table details the specific tools available to the agent. xviii Tool Name Description check syntax tool check lammps potentials tool get potential file info tool list available potentials tool find similar potentials tool Verifies whether LAMMPS input script is syntactically correct and executable, using either LLM-based semantic assessment or dry-run execution test. Scans referenced interatomic potential files in the input script, checks their local availability, and attempts to download missing files from official LAMMPS sources (failure to download does not imply nonexistence). Retrieves detailed metadata and content information for specified potential file. Lists all potential files available in the local potentials directory. Given possibly misspelled potential name, searches local potential files and returns the most relevant matches. check potential real exists tool Uses an LLM-based search module or web search API to determine whether potential file truly exists. Returns False only when confirmed nonexistent. Visualizes LAMMPS outputs (e.g., including trajectory plots and diagnostic curves. log.lammps, dump.lammpstrj) visualize tool evaluate log quality by rule tool Performs rule-based and multi-dimensional evaluation of LAMMPS log lammps run tool quality, such as convergence, numerical stability, and error patterns. Executes LAMMPS simulations and stores all generated output files in designated directory. Table A1 Summary of tool functionalities in the MDAgent2 system."
        },
        {
            "title": "Appendix B Eval Details",
            "content": "To ensure rigorous and standardized assessment of the generated LAMMPS scripts, we established multi-dimensional correctness evaluation framework, as detailed in Table B2. This framework encompasses eight distinct dimensions, ranging from basic Syntax Correctness to high-level Physical Soundness. Crucially, this rubric serves dual purpose in our system. First, it provides unified guideline for human experts to conduct ground-truth evaluations. Second, these exact dimensions and deduction rules are explicitly incorporated into the prompt design for the LLM. By aligning the LLMs scoring instructions with this expert-verified rubric, we ensure that the automated MDAgent Correctness Reward remains consistent with human scientific judgment. xix Table B2 Evaluation dimensions and typical deduction examples for the MDAgent Correctness Reward. ID Evaluation Dimension Description Syntax Correctness 2 Logical Consistency Whether the script follows valid LAMMPS syntax rules and avoids misspellings or invalid commands. Whether the lattice type, simulation procedure, and physical setup are consistent with the task specification. 3 5 Core Logic Accuracy Parameter Rationality Whether physical parameters such as potential, temperature, and pressure are set appropriately. compukey Whether density, (e.g., tations energy) are performed correctly. Logical Completeness Whether 6 Code Completeness 7 Result Validity 8 Physical Soundness the script includes all essential steps such as potential setup, thermostat, and boundary conditions. Whether the script contains complete simulation workflow (e.g., run, output). Whether the script can run successfully without abnormal termination (e.g., NaN values, empty output). Whether temporal the evolution of temperature, energy, or pressure follows physical laws. Typical Example Deduction Invoking non-existent command or having misspelled keyword. The scenario requires BCC but mistakenly uses FCC. The pressure unit in fix npt does not match the systems unit style. Using volume instead of mass when calculating density. Thermostat not specified or pair style missing. Missing the run command or output directives. file shows lost log atoms or dump file is empty. temperature Target is 300 K, but results diverge to 3000 K."
        },
        {
            "title": "Appendix C Datasets Details",
            "content": "A comprehensive statistical analysis was performed on all datasets referenced in this work, as presented in Table C3 and Table C4 xx Table C3 Statistics of training and evaluation datasets. Dataset Name Number / Tokens Training Datasets MD-Knowledge (#Samples) MD-Knowledge (Tokens) MD-InstructQA (#Samples) MD-CodeGen (#Samples) Evaluation Datasets MD-KnowledgeEval (#Samples) LAMMPS-SyntaxEval (#Samples) LAMMPS-CodeGenEval (#Samples) 17,808 10,865,191 27,346 4,253 336 368 566 Table C4 Distribution of question types and difficulty levels in evaluation datasets. Dataset Single Multiple Fill-in Open-QA MD-KnowledgeEval LAMMPS-SyntaxEval Total 134 285 85 81 166 81 144 37"
        },
        {
            "title": "Easy Medium Hard",
            "content": "MD-KnowledgeEval LAMMPS-SyntaxEval"
        },
        {
            "title": "Total",
            "content": "83 61 144 166 164 87"
        },
        {
            "title": "Appendix D Case Study",
            "content": "Case Trajectory in MDAgent2-RUNTIME. Using LAMMPS to simulate the melting process of CuNi nanoparticle. User Task Step 1 - Code Generator [Write Code] lammps_code # - - -[ Omitted ] --- pair_style eam / alloy pair_coeff * * CuNi . eam Cu Ni # - - -[ Omitted ] --- xxi [Tool call Potential Tool] The potentials checking tool scans the script and detects that the referenced file CuNi.eam is not available in the local environment. compatible alloy potential file CuNi.eam.alloy is recommended based on material composition and filename similarity. [Next Action Generate] Step 2 - Code Generator [Write Code] lammps_code # - - -[ Omitted ] --- pair_style eam / alloy pair_coeff * * CuNi . eam . alloy Cu Ni # - - -[ Omitted ] --- [Tool call Potential Tool] The potentials checking tool verifies that the specified file CuNi.eam.alloy is available and compatible with the CuNi alloy system. [Tool call Syntax Tool] The syntax checking tool confirms that the revised script conforms to valid LAMMPS syntax rules and is free of structural errors. [Next Action Run] Step 3 - Code Runner [Run Lammps] { status : \" Execution completed successfully \" , error : \" None \" , runtime : \" < elapsed_time >\" , um ma ry _i nf _by _ rule : { ...[ Omitted ]... } } [Generated Artifacts] The following result files are produced during execution: log.lammps [Next Action Eval] Step 4 - Result Evaluator [Eval] { \" final_score \": 4 , \" module_detail \":[...] , #[ Omitted ] \" penalty_detail \":[...] #[ Omitted ] } [Score Assessment] The obtained score falls below the predefined acceptance threshold. The result is therefore marked as not acceptable, and further code revision is required. [Next Action Generate] xxii Step 5 - Code Generator [Write Code] lammps_code # - - -[ Omitted ] --- pair_style eam / alloy pair_coeff * * CuNi . eam . alloy Cu Ni # - - -[ Omitted ] --- [Tool call Potential Tool] The potentials checking tool verifies that the specified file CuNi.eam.alloy is available and compatible with the CuNi alloy system. [Tool call Syntax Tool] The syntax checking tool confirms that the revised script conforms to valid LAMMPS syntax rules and is free of structural errors. [Next Action Run] Step 6 - Code Runner Step 7 - Result Evaluator Step 8 - Code Generator Step 9 - Code Runner Step 10 - Result Evaluator Step 10 - END [Final Answer] lammps_code units boundary atom_style metal atomic variable lattice A0 equal 3.589 fcc $ { A0 } region create_box mybox block 0 40 0 40 0 40 2 mybox # 2 atom types # Nanoparticle ( sphere ) region create_atoms set CuNi_nano sphere 20 20 20 2 1 region CuNi_nano type 1 type / fraction 2 0.5 7777 # Atomic masses mass mass 1 63.54600000 2 58. # Cu # Ni pair_style pair_coeff eam / alloy * * CuNi . eam . alloy Cu Ni # Output initial structure write_data run nanoparticle . cif 0 thermo 1000 xxiii variable label loop 0 20 loop_j # Temperature definition variable variable variable temperature equal 900 + 10* $ { } Eatom equal temp equal etotal / atoms fix $ { temperature } 0.1 50000 1 run unfix 1 all nvt temp $ { temperature } fix dump 2 all ave / time 100 5 1000 v_T v_Eatom file data_ave$ { temperature }. txt 1 all atom 5000 fusion_$ { temperature }. atom fix run 1 all nvt temp $ { temperature } $ { temperature } 0.1 unfix undump unfix next jump 1 1 2 SELF loop_j xxiv"
        }
    ],
    "affiliations": [
        "Liaoning Technical University",
        "National Key Laboratory of Data Space Technology and System",
        "Peking University",
        "The Hong Kong University of Science and Technology",
        "Wenjing Future Lab (Beijing) Technology Co., Ltd."
    ]
}