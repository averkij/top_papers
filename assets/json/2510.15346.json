{
    "paper_title": "When to Ensemble: Identifying Token-Level Points for Stable and Fast LLM Ensembling",
    "authors": [
        "Heecheol Yun",
        "Kwangmin Ki",
        "Junghyun Lee",
        "Eunho Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Ensembling Large Language Models (LLMs) has gained attention as a promising approach to surpass the performance of individual models by leveraging their complementary strengths. In particular, aggregating models' next-token probability distributions to select the next token has been shown to be effective in various tasks. However, while successful for short-form answers, its application to long-form generation remains underexplored. In this paper, we show that using existing ensemble methods in long-form generation requires a careful choice of ensembling positions, since the standard practice of ensembling at every token often degrades performance. We identify two key factors for determining these positions: tokenization mismatch across models and consensus in their next-token probability distributions. Based on this, we propose SAFE, (Stable And Fast LLM Ensembling), a framework that selectively ensembles by jointly considering these factors. To further improve stability, we introduce a probability sharpening strategy that consolidates probabilities spread across multiple sub-word tokens representing the same word into a single representative token. Our experiments on diverse benchmarks, including MATH500 and BBH, demonstrate that SAFE outperforms existing methods in both accuracy and efficiency, with gains achieved even when ensembling fewer than 1% of tokens."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 6 4 3 5 1 . 0 1 5 2 : r Preprint WHEN TO ENSEMBLE: IDENTIFYING TOKEN-LEVEL POINTS FOR STABLE AND FAST LLM ENSEMBLING Heecheol Yun1 Kwangmin Ki2 Junghyun Lee1 Eunho Yang1,3 1KAIST 2Korea University 3AITRICS"
        },
        {
            "title": "ABSTRACT",
            "content": "Ensembling Large Language Models (LLMs) has gained attention as promising approach to surpass the performance of individual models by leveraging their complementary strengths. In particular, aggregating models next-token probability distributions to select the next token has been shown to be effective in various tasks. However, while successful for short-form answers, its application to long-form generation remains underexplored. In this paper, we show that using existing ensemble methods in long-form generation requires careful choice of ensembling positions, since the standard practice of ensembling at every token often degrades performance. We identify two key factors for determining these positions: tokenization mismatch across models and consensus in their next-token probability distributions. Based on this, we propose SAFE, (Stable And Fast LLM Ensembling), framework that selectively ensembles by jointly considering these factors. To further improve stability, we introduce probability sharpening strategy that consolidates probabilities spread across multiple sub-word tokens representing the same word into single representative token. Our experiments on diverse benchmarks, including MATH500 and BBH, demonstrate that SAFE outperforms existing methods in both accuracy and efficiency, with gains achieved even when ensembling fewer than 1% of tokens."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recently, Large Language Models (LLMs) have achieved remarkable performance across diverse domains, including mathematics (Yang et al., 2024b), coding (Guo et al., 2024) and reasoning (Yang et al., 2025; OpenAI, 2024). Despite this progress, each LLM possesses unique strengths shaped by its training recipe, and no single model dominates across all domains. As result, combining the complementary strengths of multiple models at inference time has emerged as promising way to surpass the performance of any individual model (Wang et al., 2025a; Yao et al., 2025; Chen et al., 2025a). Compared to training new model that jointly integrates all such capabilities, these collaborative approaches provide more practical and efficient pathway to superior performance. Among various collaboration methods, probability-level ensemble, which aggregates the next-token probability distributions of multiple LLMs to select the most confident token, has emerged as one of the most effective ways (Yao et al., 2025; Yu et al., 2024; Huang et al., 2024). It enables collaboration across diverse model architectures and effectively leverages the knowledge of multiple models embedded in their probability distributions. Consequently, it has outperformed individual models, particularly when directly answering multiple-choice or short-answer questions without reasoning. natural question then arises: are probability-level ensemble methods equally effective for longform generation? We find that, in long-form generation, the effectiveness of ensembling critically depends on deciding when to ensemble. Our analysis reveals that accuracy and efficiency improve when ensembling occurs at appropriate token positions, guided by two key factors: tokenization mismatch across models and their consensus in next-token probability distributions. The first factor, tokenization mismatch across models, is crucial for stability, especially in longform generation where such mismatches occur more frequently. mismatch arises when an ensemble selects token that conflicts with the tokenization scheme of participating model. We refer to these tokens as OOV-like tokens because while not truly out-of-vocabulary (OOV), they force 1 Preprint Figure 1: Illustration of the OOV-like token problem. When So is fed into LLM2, which tokenizes Sofia as single token, the probability distribution of the next token becomes corrupted. Figure 2: (Left) Failure cases of existing ensemble methods in long sequence generation. (Right) Feeding OOV-like tokens into model often causes it to produce wrong tokens. model to predict the next token in an out-of-distribution state, often resulting in the generation of erroneous tokens, as illustrated in Figure 1. Suppose the ensemble process first generates the token So when constructing the word Sofia. However, since LLM2 tokenizes Sofia as single token, So acts as an OOV-like token for LLM2. Conditioning on this unnatural prefix corrupts LLM2s next-token probability distribution, leading to an erroneous output (i.e., A). Such errors accumulate in long-sequence generation, degrading output quality. For example, in the case of unnatural token repetition shown in Figure 2, an initial error in generating the word Sofia propagates, causing the model to repeatedly output corrupted tokens like on the subsequent generation. Consequently, existing ensemble method (Yao et al., 2025) that performs ensembling at every token, suffers substantial performance degradation in Chain-of-Thought (CoT) reasoning (Wei et al., 2022), as shown in Table 1. Therefore, accounting for tokenization mismatch across models is essential to prevent the introduction of OOV-like tokens and ensure stable ensembling under CoT. The second factor, consensus in models next-token probability distributions, plays key role in enhancing efficiency. Existing ensemble methods become increasingly inefficient when generating long sequences, as the number of costly ensemble operations grows with sequence length. This expense arises primarily from vocabulary alignment required to construct the ensemble distribution, defined as the average of models next-token probability distributions (Yao et al., 2025). However, when individual models next-token distributions exhibit sufficient consensus, the most confident token can be identified without explicitly constructing the ensemble distribution. Leveraging this property, we can determine the most confident token directly from the models next-token probability distributions, thereby improving efficiency by skipping costly alignment operations. To this end, we propose SAFE (Stable And Fast LLM Ensembling), which identifies the opportune moments for ensembling in long-sequence generation by considering the two key factors above. SAFE adopts speculative strategy in which one model, the drafter, generates lookahead sequence of tokens, while the remaining models, the verifiers, identify token-level ensemble points within that sequence. Similar to speculative decoding (Chen et al., 2023), this role separation reduces computational cost by limiting autoregressive generation to the drafter, whereas conventional ensemble methods require every model to do so. SAFE then iterates three-step cycle: GenerateVerifyEnsemble. Preprint Method Qwen2.5-7B Internlm3-8B MMLU-redux ARC-C MATH No CoT 68.86 67.52 CoT 74.88 76.89 No CoT 87.37 88. CoT 88.74 90.27 CoT 72.4 74.8 UniTE UniTE + Ours 69.36 (+0.50) 69.36 (+0.50) 73.39 (-3.50) 77.92 (+1.03) 88.40 (-0.17) 88.40 (-0.17) 87.97 (-2.30) 90.78 (+0.51) 59.6 (-15.2) 77.6 (+2.8) Table 1: Performance of the baseline ensemble method (UniTE) degrades under CoT prompting. In contrast, it matches or outperforms individual models when directly answering multiple-choice questions, since tokenizer mismatches do not arise. All models are instruction-tuned. (Generate) First, the drafter produces lookahead sequence of tokens. (Verify) Next, the verifiers examine drafters tokens in single forward pass to determine whether ensembling at each token is both stable and necessary. Ensembling is triggered among the drafters tokens only when the following two conditions are satisfied: (i) OOV-like token is not introduced and (ii) the verifiers exhibit insufficient agreement on the token. (Ensemble) Finally, ensembling is applied only at the tokens validated in the Verify step, replacing them with the ensembled tokens. At these points, if the ensemble distribution is overly smooth, we further apply probability sharpening strategy that concentrates the probability mass onto the most plausible token for precise token selection. Our method offers the following key advantages when generating long sequences via ensemble. Efficiency: SAFE significantly reduces computational cost in two ways. First, its speculative strategy restricts costly autoregressive generation to single drafter. Second, its selective ensembling reduces the number of expensive ensemble operations. Therefore, SAFE can achieve inference speed comparable to individual models, even on long sequences. Stability: SAFE prevents OOV-like tokens from being fed into models, thereby improving the stability of ensembling. Our experiments show that SAFE outperforms existing ensembling methods by an average of 5.72% in CoT settings. Plug-and-Play: SAFE can be seamlessly integrated with existing ensemble methods by simply adding the generate-verify logic. SAFE consistently improves recent ensembling approaches across diverse model combinations."
        },
        {
            "title": "2 RELATED WORK",
            "content": "LLM ensemble methods can be broadly categorized according to whether ensembling occurs after inference or during inference (Chen et al., 2025b). Research in both directions has progressed in parallel, each of which is detailed below. 2.1 ENSEMBLE AFTER INFERENCE These approaches aggregate the responses generated by individual LLMs to obtain better final answer. Early work focused on methods in which multiple models engaged in iterative discussions to converge to single response (Du et al., 2023; Chen et al., 2024a; Liang et al., 2024). More recently, attention has shifted toward methods that move away from debate-style interactions and instead stack LLMs either in cascade or parallel structure. Cascade structure FrugalGPT (Chen et al., 2024b) arranges models in cascade ordered by cost, invoking the next model only when the previous one produces an unreliable response, thereby reducing cost while preserving performance. Gupta et al. (2024) takes finer-grained approach by deciding whether to call the next model based on token-level uncertainty rather than full responses. Similarly, AutoMix (Aggarwal et al., 2024) employs self-verification to determine whether an additional model should be invoked. Parallel structure In contrast to cascading, parallel ensembling runs multiple models independently and then selects the best response among them. MORE (Si et al., 2023) trains classifier to select the optimal response by considering model expertise, confidence, and agreement across responses. Preprint LLM-Blender (Jiang et al., 2023) employs pairwise ranker to score responses and then fuses the top-k candidates into single answer. Hybrid structure Recent work has also explored combining the advantages of cascade and parallel structures. MoA (Wang et al., 2025a) proposed framework that iteratively feeds responses from multiple models into an aggregator LLM, which consolidates those responses into single response. Self-MoA (Li et al., 2025) showed that, in certain cases, using single best-performing model within this framework outperforms using multiple distinct models. Under the same framework, SymbolicMoE (Chen et al., 2025a) introduced an adaptive routing strategy that selects models according to the query. Nevertheless, such frameworks require numerous LLM calls, and the aggregator LLM often underperforms majority voting (Wang et al., 2025b), making consolidating multiple responses into persistent challenge. 2.2 ENSEMBLE DURING INFERENCE In this setting, ensembling occurs during response generation, most commonly at the token level. Co-LLM (Shen et al., 2024) adopts routing method which dynamically selects which model to use for generating each token. CoSD (Wang et al., 2025c) improves efficiency by introducing lightweight router and integrating speculative decoding. These approaches primarily target models with identical tokenizers and rely on routing rather than aggregating probability across models. To better exploit the collective intelligence of multiple models, another line of work explores probability-level ensemble methods. These methods average the next-token probability distributions of different models to select the most confident token. Since probability distributions are defined over heterogeneous vocabularies, prior work has focused on constructing the ensemble distribution by aligning different vocabularies across models. GaC (Yu et al., 2024) integrates probabilities by taking the union of all model vocabularies and then mapping each models vocabulary to this union. DEEPEN (Huang et al., 2024) projects each models vocabulary into shared embedding space, merges distributions there, and maps them back to the individual vocabulary spaces. UniTE (Yao et al., 2025) demonstrates that aligning only the top-k tokens from each model is effective both in performance and efficiency. While these methods achieve strong performance in directly generating answer tokens by selecting the most confident token, they face challenges in long-sequence generation that involves reasoning. In such cases, an increase in OOV-like tokens destabilizes the ensemble, and the high computational cost of repeated vocabulary alignment makes such approaches inefficient. Therefore, we aim to simultaneously improve the stability and efficiency of probabilitylevel ensembling by introducing verification algorithm that determines when to ensemble."
        },
        {
            "title": "3 SAFE: TOWARD STABLE AND FAST LLM ENSEMBLING",
            "content": "We aim to solve the problem of instability and inefficiency that arises when ensembling next-token probability distributions across LLMs with heterogeneous tokenizers, particularly in long-sequence generation. To this end, we propose SAFE, an algorithm that preemptively determines optimal points for ensembling by jointly considering tokenization mismatch and consensus in next-token probability distributions. Notably, SAFE can be seamlessly integrated with existing ensemble methods. Given LLMs, our method begins by dividing the models into two roles: drafter Mdraft, which generates lookahead sequence of tokens, and verifiers Mver, which identify the ensemble points among the drafters tokens. We select the best-performing model as Mdraft, while the remaining models serve as Mver. We then iterate the Generate (Section 3.1)-Verify (Section 3.2)-Ensemble (Section 3.3) cycle. In each iteration, Mdraft first generates sequence of tokens, which Mver then examine to find token that requires ensembling. At such points, ensembling is performed to replace the token with the most confident token from the averaged distributions of all models, after which Mdraft resumes generation from the ensembled token. Figure 3 shows the overview of SAFE. 3.1 GENERATE The drafter Mdraft generates predefined number of tokens (ti, . . . , ti+n1). Producing sequence rather than single token allows SAFE to account for the different tokenization schemes of the various models. For example, consider the word Incorrect. Suppose Mdraft generates it as 4 Preprint Figure 3: Overview of SAFE. The drafter generates sequence of tokens, which the verifiers retokenize it using their own tokenization schemes and identify where ensembling is required. At this position, ensembling is performed with our probability sharpening strategy. three tokens (Inc, orr, ect), while other models generate it as single token (Incorrect). If Mdraft were to generate only the first token Inc, it would fail to capture the tokenization schemes of the other models. Therefore, at this stage, Mdraft produces sequence of tokens to ensure compatibility with diverse tokenization schemes. The choice of is discussed in Section 4.4. 3.2 VERIFY In this step, the verifiers Mver collaboratively examine the drafters tokens (ti, . . . , ti+n1) to identify which tokens require ensembling. Ensembling is triggered at the earliest token tj that satisfies two checks: (i) OOV-like token verification, requiring that the immediately preceding token tj1 is not an OOV-like token, and (ii) ensemble distribution verification, requiring that tj is not the most confident token in the ensemble distribution Pens. This selective process addresses instability by preventing the introduction of OOV-like tokens and mitigates inefficiency by reducing the number of ensemble operations, thereby lowering the cost of constructing ensemble distributions. Importantly, this entire verification process is efficient, as the drafters tokens are processed by the verifiers in single forward pass rather than autoregressively. (i) OOV-like Token Verification This check requires that the preceding token is not an OOV-like token. First, each verifier model LLMv Mver tokenizes the drafters sequence t<i+n into its own tokenization, tv <i+nv . Then, we define tj as an OOV-like token for LLMv if the tokenization boundary of t<j+1 does not match any boundary in LLMvs tokenization. This condition is formally stated in Equation (1): tj is OOV-like in LLMv [0, + nv], Decode(t<j+1) = Decode(tv <x+1), (1) where Decode() means merging tokens back into text. If tj is identified as OOV-like by any verifier model, ensembling is not triggered at the subsequent token tj+1. For example, in the word Incorrect, tokens such as Inc or orr are OOV-like, but ect is not, because tokenization boundary up to ect is consistent with another models tokenization, Incorrect. Therefore, ensembling is skipped at orr and ect, but can be triggered at the token following ect. (ii) Ensemble Distribution Verification For tokens that pass OOV-like token verification, our method further checks whether the token is the most probable prediction in the ensemble distribution. Since explicitly constructing the ensemble distribution is costly, we instead verify whether token tj is the most probable token by examining each models own distribution. Specifically, given LLMvs tokenization tv <vj of the drafters tokens t<j, the drafters token tj is regarded as the most confident and ensembling is therefore skipped, if either of the following holds: 1. (Unanimous consensus among verifiers) = arg maxt Pv(t tv If tv vj the probability distribution of LLMv. <vj ) for all LLMv Mver, we skip ensembling, where Pv is 5 Preprint 2. (Average probability above one half) (cid:80) If 1 MverMdraft LLMvMverMdraft Pv(tv vj tv <vj ) > 1 2 , we skip ensembling at tj. Intuitively, the first condition checks whether tv vj , the token corresponding to tj under LLMvs tokenization, is the most probable token for all verifiers, while the second checks whether its average probability across all models is greater than 0.5. Adopting these criteria does not compromise accuracy compared to using the exact ensemble distribution, which is proved in Section C. if tj1 = OOV-like token and tj passes Ensemble Distribution Verification then <vj )}vMverMdraft) 3. Ensemble 1. Generate 2. Verify ti, , ti+n1 Mdraft(p, t<i) tv <i+nv TOKENIZEv(t<i+n), for = + 1 do Algorithm 1 SAFE: Generate-Verify-Ensemble algorithm Require: Mdraft, Mver, p: prompt, n: drafters sequence length 1: 1 2: t0 BOS token 3: while not End-of-Sentence do 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: end for 14: 15: end while Pens AVERAGEDIST({Pv( p, tv tj arg maxt Pens(t p, t<j) + 1 + end if else Mver Mdraft 3.3 ENSEMBLE: SHARPENING ENSEMBLE DISTRIBUTION In the Ensemble step, any token that passes both verifications in the Verify step is replaced with the most probable token from Pens, which is constructed as the average of all models prabibility distributions using existing ensemble methods. However, different tokenization schemes across models can scatter probability mass for the same word across multiple sub-word tokens, resulting in an overly smooth ensemble distribution (i.e., max Pens < 0.5) that hinders confident token selection. In such cases, we apply probability sharpening to consolidates the diffused probability by reallocating the probability mass from variant sub-word tokens to their common prefix token. To prevent inflating probabilities of low-quality tokens, reallocation is applied only to the drafters tokens with initial probability greater than 0.1. The entire sharpening process is defined as follows: Pens(tj) Pens(tj) + (cid:88) Pens(ti), where Pens(tj) > 0.1. ti : ti.startswith(tj ) After selecting the most confident token from the (potentially sharpened) ensemble distribution, the drafter resumes generation from this token."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "This section evaluates SAFE across various benchmarks and model combinations. We begin by outlining the implementation details, including our KV caching strategy to improve efficiency. We then describe the experimental setup, followed by an analysis of the results. 4.1 IMPLEMENTATION KV Cache Implementation KV caching is essential for efficient generation in LLMs. However, unlike standard generation settings where previously generated tokens remain fixed, ensemble generation may replace tokens during the ensembling process, leading to inconsistencies between the cache and the actual input sequence. Consequently, prior approaches have typically avoided implementing KV cache management, leaving it as future work. In contrast, our method updates each 6 Preprint models KV cache at the end of every ensemble step to align with the ensembled output, and uses this updated cache in the next step, thereby ensuring cache consistency. We apply our KV cache management to all baselines in our experiments. Please refer to Section for details. Hardware and Hyperparameters We configure our method as follows. Following the approach of UniTE (Yao et al., 2025) for selecting primary model, we select the model with the best average performance as Mdraft. The drafter generates tokens in chunks of 5, and all models use greedy decoding with maximum output length of 2048. For ensembling, each model is loaded onto separate GPU, with all experiments conducted on RTX 3090 GPUs with FP16 precision and FlashAttention2 (Dao, 2023) enabled. 4.2 EXPERIMENTAL SETTINGS Models Building on prior findings (Yao et al., 2025) that ensembling models with comparable performance yields stronger gains, we select three widely used LLMs with similar capability but heterogeneous tokenization schemes: Internlm3-8B-Instruct (Cai et al., 2024), Qwen2.5-7B-Instruct (Qwen et al., 2025), and EXAONE3.5-7.8B-Instruct (An et al., 2024). Figure 4 presents the tokenization similarity between each model pair on Oxford 5000 words (Oxford, 2018), which consists of commonly used English words. As illustrated, only small portion of the words are tokenized identically across models, with agreement rates ranging from 40% to 60%. To also evaluate ensembling on models with nearly identical tokenizations, we include two widely used LLMs of similar capability: Qwen2-7B-Instruct (Yang et al., 2024a) and Llama-3.1-8BInstruct (Dubey et al., 2024). For these two models, more than 99% of Oxford 5000 words are tokenized identically. Finally, 32B-scale models are used in Section F. Figure 4: Tokenization agreement rates between each model pair on Oxford 5000 words. Benchmarks To evaluate performance across diverse domains, we use five benchmarks. For general knowledge, we adopt MMLU-redux (Gema et al., 2025), refined subset of MMLU (Hendrycks et al., 2021) that covers 30 subjects with human-annotated corrections. For mathematical reasoning, we use MATH500 (Lightman et al., 2024) and GSM8K (Cobbe et al., 2021). For general reasoning, we employ ARC-Challenge (Clark et al., 2018) and BBH (Suzgun et al., 2023). All benchmarks are evaluated under zero-shot CoT setting, except for BBH, which uses 3-shot CoT. For BBH, we choose 15 subjects where the models exhibit comparable performance. Further details, including prompt templates and selected BBH subjects, are provided in Section A. Baselines We apply our method to two recent state-of-the-art (SOTA) probability-level ensemble methods: GaC (Yu et al., 2024) and UniTE (Yao et al., 2025). GaC performs ensembling only when the main LLMs next-token probability falls below 0.5. In contrast, UniTE represents the SOTA among methods that ensemble at every generation step. 4.3 MAIN ANALYSIS Table 2 shows results when ensembling models with substantially different tokenization schemes. We also report E/T, the percentage of tokens that undergo ensembling over the entire sequence. SAFE improves performance with less ensembling. Overall, SAFE generally outperforms individual models, making existing ensemble methods practical even under CoT. As shown in Table 2, the baseline UniTE struggles significantly under CoT setting, consistently underperforming individual models across all experiments. This is because it ensembles at every generation step, which increases the frequency of OOV-like tokens and consequently corrupts probability distributions. In contrast, applying SAFE enables UniTE to achieve the best performance in many cases (9/15) while reducing the ensemble frequency (E/T) to fewer than 20% of tokens. This highlights the importance of determining when to ensemble, particularly when generating long sequences with models that use heterogeneous tokenizers. GaC, on the other hand, is more robust, since it performs ensembling only when the main LLMs probability falls below 0.5, yielding an unintended but beneficial effect of preventing the introduction of OOV-like tokens. Nevertheless, SAFE further improves GaCs performance while reducing E/T. 7 Preprint Method MMLU-redux MATH500 GSM8K BBH ARC-C Accuracy E/T Accuracy E/T Accuracy E/T Accuracy E/T Accuracy E/T Internlm3-8B Qwen2.5-7B EXAONE3.5-7.8B 76.89 74.88 73. GaC GaC + SAFE UniTE UniTE + SAFE 77.00 (+0.11) 77.11 (+0.22) 73.39 (-3.5) 77.81 (+0.92) GaC GaC + SAFE UniTE UniTE + SAFE 76.01 (+1.13) 76.79 (+1.91) 53.75 (-21.13) 76.54 (+1.66) GaC GaC + SAFE UniTE UniTE + SAFE 76.36 (-0.53) 77.21 (+0.32) 72.51 (-4.38) 76.08 (-0.81) - - - 8.43 5.23 100 12.59 13.42 7.52 100 17.24 8.71 5. 100 15.84 74.8 72.4 72.8 - - - 90.14 91.81 90.45 - - - 82.26 79.15 78. Two-model ensembling (Internlm3 + Qwen2.5) 74.2 (-0.6) 76.0 (+1.2) 59.6 (-15.2) 77.4 (+2.6) 1.04 0.71 100 3.82 91.28 (-0.53) 91.36 (-0.45) 75.06 (-16.75) 92.04 (+0.23) 0.82 0.67 100 5.16 82.34 (+0.08) 82.34 (+0.08) 79.58 (-2.68) 82.97 (+0.71) Two-model ensembling (Qwen2.5+ EXAONE3.5) 75.4 (+2.6) 76.4 (+3.6) 43.4 (-29.4) 76.4 (+3.6) 2.31 1.09 100 4.69 92.65 (+0.84) 92.57 (+0.76) 77.03 (-14.78) 92.72 (+0.91) 2.60 1.26 100 5.60 79.61 (+0.46) 79.66 (+0.51) 67.45 (-11.70) 81.69 (+2.54) Two-model ensembling (Internlm3 + EXAONE3.5) 75.8 (+1.0) 77.2 (+2.4) 73.6 (-1.2) 77.0 (+2.2) 1.14 0.84 100 4.72 90.75 (+0.30) 90.67 (+0.22) 89.31 (-1.14) 90.75 (+0.30) 0.88 0. 100 5.55 81.57 (-0.69) 81.54 (-0.72) 78.04 (-4.22) 81.37 (-0.89) Three-model ensembling (Internlm3 + Qwen2.5 + EXAONE3.5) - - - 5.69 3. 100 10.35 8.15 4.51 100 14.03 6.32 4.38 100 13.75 90.27 88.74 90. 90.61 (+0.34) 91.13 (+0.86) 87.97 (-2.30) 90.78 (+0.51) 90.27 (-0.17) 90.78 (+0.34) 72.61 (-17.83) 90.78 (+0.34) 90.78 (+0.34) 91.72 (+1.28) 88.23 (-2.21) 90.27 (-0.17) - - - 10.22 6.22 100 14.47 14.66 8.31 100 19.24 10.07 6. 100 17.89 UniTE UniTE + SAFE 73.92 (-2.97) 77.60 (+0.71) 100 16.18 76.0 (+1.2) 79.0 (+4.2) 100 4. 91.28 (-0.53) 92.04 (+0.23) 100 5.14 77.47 (-4.79) 82.77 (+0.51) 100 12.74 87.20 (-3.24) 91.55 (+1.11) 100 18. Table 2: Ensembling results of models with substantially different tokenizations. E/T (%) represents the percentage of ensembling when generating responses, which is computed by # Ensemble (%). Num- # Token bers in parentheses denote the performance gap relative to the best-performing individual model. Figure 5: Latency comparison on MATH500. Our method shows similar latency compared to individual models, even when generating long sequences. w/ KV indicates that our KV caching strategy is applied. Note that the time-axis scale in (c) differs from (a) and (b). One interesting finding is that much less ensembling is required in math datasets. When SAFE is applied with UniTE, ensembling is triggered for only 4.85% of tokens on average in math datasets, whereas it rises to 15.24% in general-domain datasets, which is nearly three times higher. We attribute this to the nature of math responses, which often contain equations or structured expressions with limited variation, leading to higher agreement among verifier models. In contrast, responses in general-domain datasets allow for greater linguistic variability, which reduces agreement across models and thus requires more frequent ensembling. Increasing the number of models is not always optimal. As shown in Table 2, ensembling three models does not consistently outperform two-model ensembling, whereas ensembling the top-2 best performing models typically yields the strongest results. This suggests that when model rankings are known, restricting ensembling to the top-2 models is both effective and efficient. On the other hand, when rankings are unknown, ensembling multiple comparable models provides stable, though not necessarily optimal, performance. SAFE can be as fast as individual models. key challenge in LLM ensembling is achieving inference speed comparable to running single model. As shown in Figure 5, SAFE closely matches the latency of individual models when generating hundreds of tokens, regardless of the underlying ensemble method. Moreover, under the same computational resources, it significantly improves efficiency over existing ensemble methods when generating long responses. This efficiency stems from three properties of our approach. First, only Mdraft is responsible for autoregressive generation. SecPreprint Method MMLU-redux MATH500 GSM8K Qwen2-7B Llama3.1-8B 69.25 68. 49.8 47.6 85.90 82.56 Two-model ensembling (Qwen2 + Llama3.1) GaC GaC + SAFE 69.50 (+0.25) 69.99 (+0.74) 52.4 (+2.6) 59.4 (+9.6) 85.37 (-0.53) 86.66 (+0.76) UniTE 68.90 (-0.35) UniTE + SAFE 69.71 (+0.46) 54.0 (+4.2) 55.6 (+5.8) 79.98 (-5.92) 84.08 (-1.82) Method Internlm3-8B Qwen2.5-7B MMLU-redux MATH500 GSM8K 76.89 74.88 74.8 72.4 Two-model ensembling (Internlm3 + Qwen2.5) GaC GaC + SAFE (w/o sharpen.) GaC + SAFE UniTE UniTE + SAFE (w/o sharpen.) UniTE + SAFE 77.00 77.11 77.11 73.39 77.53 77.81 74.2 75.2 76.0 59.6 76.6 77.4 90.14 91.81 91.28 91.36 91. 75.06 91.66 92.04 Table 3: Ensembling results of models with similar tokenization. Table 4: Ablation on our probability sharpening strategy. Method MMLU-redux GSM8K ARC-C Internlm3-8B Qwen2.5-7B 76.89 74.88 90.14 91.81 90.27 88.74 Two-model ensembling (Internlm3 + Qwen2.5) UniTE + SAFE3 UniTE + SAFE5 UniTE + SAFE8 77.67 77.81 78. 91.66 92.04 92.04 90.19 90.78 90.78 Table 5: Ablation on drafter sequence length. SAFEn denotes generation of n-token sequences. ond, SAFE substantially reduces the number of ensembling. Third, our KV caching strategy further improves efficiency, as illustrated in Figure 5(c). We provide more comparisons in Section E. SAFE further improves performance when ensembling models with similar tokenization. We further evaluate SAFE on models with highly similar tokenizations, where more than 99% of Oxford 5000 words are tokenized identically. As shown in Table 3, the performance drop of existing methods is less severe than in Table 2. This is because highly aligned tokenization schemes greatly reduce the occurrence of OOV-like tokens, leading to more stable ensembling. Nevertheless, applying SAFE to existing methods consistently improves performance, yielding over 9% gain on MATH500 compared to the best-performing individual model. 4.4 ABLATION STUDY We conduct ablation studies on our probability sharpening strategy and the drafters sequence length. As shown in Table 4, incorporating probability sharpening consistently improves performance across benchmarks and ensemble methods. This result indicates that our probability sharpening strategy effectively selects more precise token when the ensemble distribution becomes overly smooth. Table 5 presents the ablation on drafter sequence length. Generating short sequences may fail to capture differences in tokenization across models, causing slight performance drops. Conversely, generating overly long sequences does not harm accuracy but may reduce efficiency, as shown in Figure 10. This is because longer sequences force the drafter to regenerate tokens more often from the ensembled token. length of 5 provides the best balance between accuracy and efficiency."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this paper, we examined probability-level ensemble methods for long-form generation and showed that deciding when to ensemble is critical for both accuracy and efficiency. To this end, we proposed SAFE, generate-verify-ensemble framework that triggers ensembling only when safe and necessary, guided by tokenization mismatch and consensus in models next-token probability distributions. SAFE further improved stability via probability sharpening to mitigate smooth ensemble distribution, and our KV cache implementation enabled much faster ensembling in long-form generation. Experiments demonstrated that SAFE outperforms existing methods with only few ensemble operations across widely used 7B-scale model combinations. We believe SAFE offers practical step toward making LLM ensembling both robust and deployable in real-world applications. 9 Preprint"
        },
        {
            "title": "REFERENCES",
            "content": "Pranjal Aggarwal, Aman Madaan, Ankit Anand, Srividya Pranavi Potharaju, Swaroop Mishra, Pei Zhou, Aditya Gupta, Dheeraj Rajagopal, Karthik Kappaganthu, Yiming Yang, Shyam Upadhyay, Manaal Faruqui, and Mausam . Automix: Automatically mixing language models. In The Thirtyeighth Annual Conference on Neural Information Processing Systems, 2024. URL https:// openreview.net/forum?id=e6WrwIvgzX. Soyoung An, Kyunghoon Bae, Eunbi Choi, Kibong Choi, Stanley Jungkyu Choi, Seokhee Hong, Junwon Hwang, Hyojin Jeon, Gerrard Jeongwon Jo, Hyunjik Jo, et al. Exaone 3.5: Series of large language models for real-world use cases. arXiv e-prints, pp. arXiv2412, 2024. Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia Guo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang, Tao Jiang, Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiaxing Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yining Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kaiwen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun Lv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang Ning, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang, Yunfan Shao, Demin Song, Zifan Song, Zhihao Sui, Peng Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang, Jiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi Wang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong Xiong, Chao Xu, Ruiliang Xu, Hang Yan, Yirong Yan, Xiaogui Yang, Haochen Ye, Huaiyuan Ying, Jia Yu, Jing Yu, Yuhang Zang, Chuyu Zhang, Li Zhang, Pan Zhang, Peng Zhang, Ruijie Zhang, Shuo Zhang, Songyang Zhang, Wenjian Zhang, Wenwei Zhang, Xingcheng Zhang, Xinyue Zhang, Hui Zhao, Qian Zhao, Xiaomeng Zhao, Fengzhe Zhou, Zaida Zhou, Jingming Zhuo, Yicheng Zou, Xipeng Qiu, Yu Qiao, and Dahua Lin. Internlm2 technical report, 2024. Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023. Justin Chen, Swarnadeep Saha, and Mohit Bansal. ReConcile: Round-table conference improves In Lun-Wei Ku, Andre Martins, and Vivek reasoning via consensus among diverse LLMs. Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 70667085, Bangkok, Thailand, August 2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.381. URL https: //aclanthology.org/2024.acl-long.381/. Justin Chih-Yao Chen, Sukwon Yun, Elias Stengel-Eskin, Tianlong Chen, and Mohit Bansal. Symbolic mixture-of-experts: Adaptive skill-based routing for heterogeneous reasoning. arXiv preprint arXiv:2503.05641, 2025a. Lingjiao Chen, Matei Zaharia, and James Zou. Frugalgpt: How to use large language models while reducing cost and improving performance. Transactions on Machine Learning Research, 2024b. Zhijun Chen, Jingzheng Li, Pengpeng Chen, Zhuoran Li, Kai Sun, Yuankai Luo, Qianren Mao, Dingqi Yang, Hailong Sun, and Philip Yu. Harnessing multiple large language models: survey on llm ensemble. arXiv preprint arXiv:2502.18036, 2025b. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the AI2 reasoning challenge. CoRR, abs/1803.05457, 2018. URL http://arxiv.org/abs/1803.05457. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021. URL https://arxiv.org/abs/2110.14168. Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning, 2023. URL https://arxiv.org/abs/2307.08691. 10 Preprint Yilun Du, Shuang Li, Antonio Torralba, Joshua Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. In Forty-first International Conference on Machine Learning, 2023. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pp. arXiv2407, 2024. Aryo Pradipta Gema, Joshua Ong Jun Leang, Giwon Hong, Alessio Devoto, Alberto Carlo Maria Mancino, Rohit Saxena, Xuanli He, Yu Zhao, Xiaotang Du, Mohammad Reza Ghasemi Madani, Claire Barale, Robert McHardy, Joshua Harris, Jean Kaddour, Emile Van Krieken, and Pasquale Minervini. Are we done with MMLU? In Luis Chiruzzo, Alan Ritter, and Lu Wang (eds.), Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 50695096, Albuquerque, New Mexico, April 2025. Association for Computational LinISBN 979-8-89176-189-6. doi: 10.18653/v1/2025.naacl-long.262. URL https: guistics. //aclanthology.org/2025.naacl-long.262/. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, YK Li, et al. Deepseek-coder: When the large language model meets programming the rise of code intelligence. arXiv preprint arXiv:2401.14196, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Neha Gupta, Hari Narasimhan, Ankit Singh Rawat, Wittawat Jitkrittum, Aditya Menon, and Sanjiv Kumar. Language model cascades: Token-level uncertainty and beyond. In International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id= KgaBScZ4VI. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021. Yichong Huang, Xiaocheng Feng, Baohang Li, Yang Xiang, Hui Wang, Ting Liu, and Bing Qin. Ensemble learning for heterogeneous large language models with deep parallel collaboration. Advances in Neural Information Processing Systems, 37:119838119860, 2024. Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. LLM-blender: Ensembling large language models with pairwise ranking and generative fusion. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1416514178, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.792. URL https: //aclanthology.org/2023.acl-long.792/. Wenzhe Li, Yong Lin, Mengzhou Xia, and Chi Jin. Rethinking mixture-of-agents: Is mixing different large language models beneficial? arXiv preprint arXiv:2502.00674, 2025. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, and Zhaopeng Tu. Encouraging divergent thinking in large language models through In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Promulti-agent debate. ceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 1788917904, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.992. URL https://aclanthology.org/2024. emnlp-main.992/. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=v8L0pN6EOi. 11 Preprint OpenAI. Gpt-o4 mini. introducing-o3-and-o4-mini/, 2024. https://openai.com/index/ Oxford. The oxford 5000. https://www.oxfordlearnersdictionaries.com/ wordlists/oxford3000-5000, 2018. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. Zejiang Shen, Hunter Lang, Bailin Wang, Yoon Kim, and David Sontag. Learning to decode collaboratively with multiple language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1297412990, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.701. URL https: //aclanthology.org/2024.acl-long.701/. Chenglei Si, Weijia Shi, Chen Zhao, Luke Zettlemoyer, and Jordan Boyd-Graber. Getting MoRE out of mixture of language model reasoning experts. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 82348249, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. findings-emnlp.552. URL https://aclanthology.org/2023.findings-emnlp. 552/. Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, and Jason Wei. Challenging BIG-bench tasks and whether chain-of-thought can solve them. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, pp. 1300313051, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.824. URL https://aclanthology.org/2023. findings-acl.824/. Junlin Wang, Jue WANG, Ben Athiwaratkun, Ce Zhang, and James Zou. Mixture-of-agents In The Thirteenth International Conference enhances large language model capabilities. on Learning Representations, 2025a. URL https://openreview.net/forum?id= h0ZfDIrj7T. Junlin Wang, Shang Zhu, Jon Saad-Falcon, Ben Athiwaratkun, Qingyang Wu, Jue Wang, Shuaiwen Leon Song, Ce Zhang, Bhuwan Dhingra, and James Zou. Think deep, think fast: Investigating efficiency of verifier-free inference-time-scaling methods. arXiv preprint arXiv:2504.14047, 2025b. Ziyao Wang, Muneeza Azmat, Ang Li, Raya Horesh, and Mikhail Yurochkin. Speculate, then collaborate: Fusing knowledge of language models during decoding. In Forty-second International Conference on Machine Learning, 2025c. URL https://openreview.net/forum?id= XCBYIfu9Fs. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Preprint Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report, 2024a. URL https://arxiv.org/abs/2407.10671. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024b. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, arXiv preprint Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv:2505.09388, 2025. Yuxuan Yao, Han Wu, Mingyang LIU, Sichun Luo, Xiongwei Han, Jie Liu, Zhijiang Guo, and Linqi Song. Determine-then-ensemble: Necessity of top-k union for large language model ensembling. In The Thirteenth International Conference on Learning Representations, 2025. URL https: //openreview.net/forum?id=FDnZFpHmU4. Yao-Ching Yu, Chun Chih Kuo, Ye Ziqi, Chang Yucheng, and Yueh-Se Li. Breaking the ceiling of the LLM community by treating token generation as classification for ensembling. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 18261839, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-emnlp.99. URL https://aclanthology.org/2024.findings-emnlp.99/. 13 Preprint"
        },
        {
            "title": "A EXPERIMENTAL DETAILS",
            "content": "In this section, we provide additional experimental details. We first present the exact prompts and BBH subjects used in our experiments, followed by an explanation of baseline selection. Prompts For multiple-choice questions, we follow the template in Figure 6. For math datasets, we use the template in Figure 7. Figure 6: Prompt template used for multiple-choice questions. Figure 7: Prompt template used for math problems. BBH subjects In our main experiments  (Table 2)  , we filter BBH subjects to include only subjects where models exhibit comparable performance. This choice is based on the observation of UniTE (Yao et al., 2025) that ensemble is meaningful when the base models exhibit similar performance levels. The chosen 15 BBH subjects are: boolean expressions, causal judgement, date understanding, disambiguation qa, formal fallacies, logical deduction three objects, movie recommendation, navigate, penguins in table, reasoning about colored objects, ruin names, salient translation error detection, snarks, temporal sequences, and tracking shuffled objects three objects. Baseline selection As our purpose is to enhance the stability and efficiency of probability-level ensemble methods, we focus on recent, state-of-the-art probability-level ensemble methods as baselines. Specifically, we consider two representative methods that differ in when ensembling is performed. The first is GaC, which ensembles only when the main LLMs probability falls below 0.5. To the best of our knowledge, this is the only work that does not ensemble at every token generation. The second is UniTE, which achieves state-of-the-art performance among methods that ensemble at every generation step. By applying SAFE to both methods, we demonstrate its versatility and its effectiveness in determining when to ensemble."
        },
        {
            "title": "B LIMITATIONS AND FUTURE WORK",
            "content": "SAFE does not always guarantee superior performance compared to the best-performing individual model. As shown in Table 2, in few cases SAFE performs slightly worse than the best-performing individual models. We believe this is because the most confident token in the ensemble distribution is not necessarily the optimal choice, as poorly performing models can distort the ensemble distribution by elevating an incorrect token as the most confident. Nevertheless, SAFE generally outperforms individual models by ensuring stable ensemble distribution when applied to ensembles of models with similar capability. Additionally, our experiments are limited to non-reasoning models. Extending SAFE to reasoning models (Yang et al., 2025; Guo et al., 2025) would be promising direction, as reasoning models have recently gained significant attention. 14 Preprint"
        },
        {
            "title": "C CORRECTNESS OF ENSEMBLE DISTRIBUTION VERIFICATION",
            "content": "Theorem 1 guarantees that applying the ensemble distribution verification criteria does not compromise accuracy. Theorem 1. Let t<j denote the drafters prefix. For each verifier model LLMv Mver, let tv vj be its next token, aligned such that tv <vj is the LLMvs tokenization of t<j. Let Pv be the nexttoken probability distribution of LLMv. Suppose (i) all verifier models unanimously agree on the next token tv <j) for all v, or (ii) their average probability in tv vj vj exceeds 1 2 : = arg maxt Pv(t tv , where tv vj 1 Mver Mdraft (cid:88) vMverMdraft Pv(tv vj tv <vj ) > 1 2 . Then tj is the token selected by the ensemble, i.e., tj = arg maxt Pens(t t<j), where Pens is the ensemble distribution. Proof. Define the ensemble distribution as average of all models distributions which is aligned with Mdrafts tokenization Pens(tj t<j) 1 Mver + 1 (cid:88) (cid:16) Pv tv vj tv <vj (cid:17) , vMverMdraft (i) Unanimous consensus across verifiers. tv <vj ), then for any token u, If for all Mver we have tv vj = arg maxt Pv(tj Pv(tv vj tv <vj ) Pv(u tv <vj ) for all v. Since tv vj is aligned with tj for all v, averaging over preserves the inequality: Pens(tj t<j) = 1 Mver + 1 (cid:88) Pv(tv vj tv <vj ) 1 Mver + 1 (cid:88) Pv(u tv <vj ) = Pens(u t<j). Hence, tj = arg maxt Pens(t t<j). (ii) Average probability above one half. Assume Pens(tj t<j) = 1 Mver + 1 (cid:88) Pv(tv vj tv <vj ) > 1 2 . vMverMdraft Because Pens( t<j) is probability distribution, (cid:80) Pens(t t<j) = 1. Thus, no other token = tj can have Pens(u t<j) Pens(tj t<j), since two distinct tokens cannot both exceed 1/2. Therefore, tj = arg maxt Pens(t t<j). In either case (i) or (ii), the ensemble selects tj, proving the claim. Figure 8: Our KV cache management. The cache is pruned to ensure alignment with the ensembled output. Preprint"
        },
        {
            "title": "D DETAILS OF OUR KV CACHE MANAGEMENT",
            "content": "While KV caching is essential for generating long responses, it introduces challenge in ensemble settings because the token selected by the ensemble may differ from the tokens generated by individual models. As illustrated in Figure 8, the drafter initially generates The answer is Incorrect, but after ensembling the output becomes The answer is Correct. In this case, the drafters KV cache, which contains states for the discarded token Incorrect, must be updated. To resolve this inconsistency, we prune each participating models KV cache by fixed buffer at the end of every ensemble step. This pruning ensures that each models KV cache is consistent with the actual input sequences before producing the next token."
        },
        {
            "title": "E LATENCY COMPARISON",
            "content": "We present additional latency comparisons on general-domain datasets, where the proportion of ensembled tokens is higher than in math datasets. Figure 9 shows the latency of SAFE on MMLUredux dataset. As illustrated, SAFE substantially reduces latency compared to existing ensemble methods and achieves speeds comparable to individual models when generating hundreds of tokens, even in general-domain tasks. These results highlight the practical applicability of SAFE, demonstrating that it enables efficient ensembling across diverse domains. Figure 9: Latency comparison on MMLU-redux. SAFE significantly improves efficiency on generaldomain tasks. Note that the time-axis scale in (c) differs from (a) and (b). Figure 10: Latency comparison depending on the drafters sequence length, where denotes the drafters sequence length in SAFEn. Generating longer sequence reduces efficiency. We use MMLU-redux for the comparison."
        },
        {
            "title": "F ENSEMBLING LARGER MODELS",
            "content": "Table 6 presents the results for ensembling 32B-scale models: Qwen2.5-32B-Instruct (Qwen et al., 2025) and EXAONE-3.5-32B-Instruct (An et al., 2024), evaluated on MMLU-redux and MATH500. For MMLU-redux, we report two variants: MMLU-redux and the full MMLU-redux. MMLUredux includes only 21 subjects, excluding 9 subjects where Qwen2.5-32B-Instruct largely outperforms EXAONE-3.5-32B-Instruct by more than 10%, making ensembling less meaningful. As shown in the table, applying SAFE to existing ensemble methods consistently outperforms the baselines, demonstrating its effectiveness on larger-scale models. The 9 subjects excluded in MMLU16 Preprint redux are: college chemistry, college mathematics, college physics, formal logic, electrical engineering, high school chemistry, professional accounting, clinical knowledge, and econometrics. Method MMLU-redux MMLU-redux MATH Accuracy Accuracy Accuracy Qwen2.5-32B EXAONE3.5-32B 85.06 82.34 84.54 79. Two-model ensembling (Qwen2.5 + EXAONE3.5) GaC GaC + SAFE 84.70 85.11 82.73 83.79 80.8 77.2 80.4 81. Table 6: Results of ensembling 32B-scale models."
        },
        {
            "title": "G QUALITATIVE RESULTS",
            "content": "In this section, we present qualitative results of SAFE. Tables 7 to 10 compare responses from existing ensemble methods with those obtained when SAFE is applied. As shown in Table 7, SAFE can successfully yield the correct solution even when individual models fail to produce the correct answer or reasoning. Tables 7 and 10 further illustrate how existing ensemble methods suffer from repeated generation of erroneous tokens due to the introduction of OOV-like tokens. Finally, Tables 8 and 9 highlight cases where existing ensemble methods generate incorrect reasoning that leads to wrong answers."
        },
        {
            "title": "H LLM USAGE",
            "content": "We used the LLM solely for the purpose of polishing the writing. 17 Preprint Question Sofia and Tess will each randomly choose one of the 10 integers from 1 to 10. What is the probability that neither integer chosen will be the square of the other? (A) 0.64 (B) 0.72 (C) 0.81 (D) 0.95 Answer (D) 0.95 Internlm38B-Instruct Qwen2.57B-Instruct UniTE UniTE SAFE + To determine the probability that neither integer chosen by Sofia and Tess will be the square of the other, we can follow these steps: 1. Identify the squares of the integers from 1 to 10: The squares of the integers from 1 to 10 are: [12 = 1, 22 = 4, 32 = 9] So, the squares are 1, 4, and 9. 2. Determine the total number of possible outcomes: Each of Sofia and Tess can choose any of the 10 integers, so the total number of possible outcomes is: [10 10 = 100] 3. Count the number of outcomes where one integer is the square of the other: We need to count the pairs ((a, b)) where (a) is the square of (b) or (b) is the square of (a). The pairs are: [(1, 1), (1, 4), (1, 9), (4, 1), (4, 4), (4, 9), (9, 1), (9, 4), (9, 9)] There are 9 such pairs. 4. Calculate the number of outcomes where neither integer is the square of the other: [100 9 = 91] 5. Calculate the probability: [ 91 100 = 0.91] However, none of the provided answer choices match 0.91. ... The answer is C. To solve this problem, we need to determine the probability that neither of the two chosen integers is the square of the other. First, lets identify the squares of integers from 1 to 10: (12 = 1), (22 = 4), (32 = 9) So, the only pairs of numbers in the range from 1 to 10 where one number is the square of the other are:(1, 1), (4, 2), (4, 4), (9, 3), (9, 9) There are 5 such pairs. Since each person can independently choose any of the 10 integers, there are total of (10 10 = 100) possible outcomes when both Sofia and Tess make their choices. Next, we calculate the number of outcomes where at least one of the chosen integers is the square of the other. This includes the 5 pairs listed above and their reverses (since if (4, 2) is pair, so is (2, 4)). However, since (1, 1), (4, 4), and (9, 9) are symmetric, they are already counted once. Therefore, we have:(1, 1) - 1 way, (4, 2) and (2, 4) - 2 ways, (4, 4) - 1 way, (9, 3) and (3, 9) - 2 ways, (9, 9) - 1 way. This gives us total of (1 + 2 + 1 + 2 + 1 = 7) pairs where one number is the square of the other. Thus, the number of outcomes where neither integer is the square of the other is (100 7 = 93). The probability that neither integer chosen will be the square of the other is therefore: [ 93 100 = 0.93] However, this does not match any of the provided options exactly. ... Given the options, the closest match to our calculated probability of 0.93 is 0.95. The answer is (D). To determine the probability that neither integer chosen by Sof Aa and Tess will be the square of the other, we need to follow these steps: 1. Identify the squares of the integers from 1 to 10: The squares of the integers from 1 to 10 are: [12 = 1, 22 = 4, 32 = 9] So, the squares are 1, 4, and 9. 2. Determine the total number of possible outcomes: Each of Sof A A A... To determine the probability that neither integer chosen by Sofia and Tess will be the square of the other, we can follow these steps: 1. Identify the squares of the integers from 1 to 10: The squares of the integers from 1 to 10 are: [12 = 1, 22 = 4, 32 = 9] So, the squares are 1, 4, and 9. 2. Determine the total number of possible outcomes: Each of Sofia and Tess can choose any of the 10 integers. Therefore, the total number of possible outcomes is: [10 10 = 100] 3. Count the number of outcomes where one integer is the square of the other: We need to count the pairs ((a, b)) where (a) is the square of (b) or (b) is the square of (a). The pairs are: [(1, 1), (4, 2), (2, 4), (9, 3), (3, 9)] There are 5 such pairs. 4. Calculate the number of outcomes where neither integer is the square of the other: [100 5 = 95] 5. Calculate the probability: [ 95 100 = 0.95] Therefore, the probability that neither integer chosen will be the square of the other is D. Table 7: Response comparisons of UniTE and UniTE + SAFE. Some verbose outputs are omitted due to space constraints. 18 Preprint Question Terry eats 2 yogurts day. They are currently on sale at 4 yogurts for $5.00. How much does he spend on yogurt over 30 days? Answer 75 Qwen2.57B-Instruct EXAONE3.5-7.8BInstruct UniTE UniTE SAFE + Lets break down the problem step by step to find out how much Terry spends on yogurt over 30 days. 1. Determine the number of yogurts Terry eats in 30 days: Terry eats 2 yogurts per day. Over 30 days, he will eat (2 30 = 60) yogurts. 2. Calculate how many sets of 4 yogurts are in 60 yogurts: Since the yogurts are sold in sets of 4 for $5.00, we need to find out how many such sets are in 60 yogurts. The number of sets is ( 60 4 = 15). 3. Calculate the total cost: Each set of 4 yogurts costs $5.00. For 15 sets, the total cost is (15 5 = 75) dollars. Therefore, the amount Terry spends on yogurt over 30 days is 75 To determine how much Terry spends on yogurt over 30 days, lets break down the problem step by step: 1. Calculate Daily Yogurt Consumption: - Terry eats 2 yogurts per day. 2. Determine Total Yogurt Consumption Over 30 Days: Over 30 days, Terry will consume (2 yogurts/day 30 days = 60 yogurts). 3. Understand the Sale Price: The yogurts are sold at rate of 4 yogurts for $5.00. 4. Calculate the Cost Per Yogurt: The cost per yogurt is ( 5.00 dollars 4 yogurts = 1.25 dollars/yogurt). 5. Calculate Total Cost for 60 Yogurts: If each yogurt costs $1.25, then for 60 yogurts, the total cost will be (60 yogurts 1.25 dollars/yogurt = 75 dollars). Therefore, the total amount Terry spends on yogurt over 30 days is 75 To determine how much Terry spends on yogurt over 30 days, we can break down the problem into several steps: 1. Calculate the daily yogurt consumption in terms of the sale unit: Terry eats 2 yogurts per day. The sale is 4 yogurts for $5.00. Therefore, Terry consumes ( 2 4 = 0.5) of sale unit per day. 2. Determine the daily cost based on the sale price: Since 4 yogurts cost $5.00, 1 yogurt costs ( 5.00 4 = 1.25). - Therefore, 0.5 of sale unit (which is equivalent to 2 yogurts) costs (0.5 1.25 = 0.625). So, Terry spends $0.625 per day on yogurt. 3. Calculate the total cost over 30 days: Terry spends $0.625 per day. Over 30 days, the total cost is (0.625 30 = 18.75). Therefore, the total amount Terry spends on yogurt over 30 days is 18.75 To determine how much Terry spends on yogurt over 30 days, we can break down the problem into several steps: 1. Calculate the daily yogurt consumption cost: Terry eats 2 yogurts per day. The sale price is 4 yogurts for $5.00. To find the cost per yogurt, we divide the total cost by the number of yogurts: Cost per yogurt = 5.00 4 = 1.25 dollars per yogurt. Since Terry eats 2 yogurts per day, the daily cost is: Daily cost = 2 1.25 = 2.50 dollars 2. Calculate the total cost over 30 days: Terry spends $2.50 per day on yogurt. Over 30 days, the total cost is: Total cost = 2.50 30 = 75.00 dollars Therefore, the total amount Terry spends on yogurt over 30 days is 75. Table 8: Response comparisons of UniTE and UniTE + SAFE. 19 Preprint Question Answer Internlm38B-Instruct Qwen2.57B-Instruct GaC GaC SAFE + Compute 992 + 99 + 1 in your head. 9901 To compute 992 + 99 + 1 in my head, can use algebraic manipulation to simplify the expression. Lets start by rewriting the expression in more convenient form. First, lets denote = 99. Then the expression becomes: x2 + + 1. Notice that this expression can be rewritten using the identity for the square of binomial: x2 + + 1 = (x + 1)2 x. Substituting = 99 back into the expression, we get: 992 + 99 + 1 = (99 + 1)2 99. Simplifying inside the parentheses, we have: 99 + 1 = 100. So the expression becomes: 1002 99. Now, we know that 1002 = 10000, so the expression simplifies to: 10000 99 = 9901. Therefore, the value of 992 + 99 + 1 is 9901 To compute 992 + 99 + 1 in my head, will break it down into simpler steps. First, lets calculate 992: [992 = (1001)2 = 100221001+12 = 10000200+1 = 9801.] Next, we add 99 to the result: 9801 + 99 = 9900. Finally, we add 1 to the result: 9900 + 1 = 9901. So, the final answer is 9901 To compute 992 + 99 + 1 in my head, can use algebraic manipulation to simplify the expression. Lets start by rewriting the expression in more convenient form. First, lets denote = 99. Then the expression becomes: x2 + + 1. Now, we need to compute x2 + + 1 for = 99. Notice that x2 + + 1 can be rewritten using the identity for the sum of cubes: x2 + + 1 = (x + 1)2 x. However, simpler approach is to recognize that x2 + + 1 can be expressed as: x2 + + 1 = (x+1)2 x+x = (x+1)2. So, we have: 992 +99+1 = (99+1)2 = 1002. Now, we just need to compute 1002: 1002 = 10000. Therefore, the value of 992 +99+1 is 10000 To compute 992 + 99 + 1 in my head, can use algebraic manipulation to simplify the expression. Lets start by rewriting the expression in more convenient form. First, lets denote = 99. Then the expression becomes: x2 + + 1. Now, we can use the identity for the square of binomial to rewrite x2 + + 1. Notice that: x2 + + 1 = (x + 1)2 x. Substituting = 99 back into the expression, we get:992 + 99 + 1 = (99 + 1)2 99. Simplifying inside the parentheses, we have: 99 + 1 = 100. So the expression becomes: 1002 99. Next, we compute 1002: 1002 = 10000. Now, we subtract 99 from 10000: 10000 99 = 9901. Therefore, the value of 992 + 99 + 1 is Table 9: Response comparisons of GaC and GaC + SAFE. 20 Preprint Question At 25C, the maximum amount of PbI2 that can be dissolved in 1.00 of pure water is 1.0 mmol. Assuming complete dissociation, the solubility product, Ksp, for lead iodide at 25C is (A) 1.0 103, (B) 1.0 106, (C) 1.0 109, (D) 4.0 109 Answer (D) Qwen2.57B-Instruct EXAONE3.5-7.8BInstruct GaC GaC SAFE + To solve this problem, we need to understand the relationship between the solubility of compound and its solubility product constant Ksp. 1. Dissociation of Lead Iodide PbI2: PbI2(s) Pb2+(aq) + 2I(aq) 2. Solubility in Water: Given that 1.0 mmol (or 1.0 103mol) of PbI2 dissolves in 1.00 of pure water, we can determine the concentrations of the ions: The concentration of Pb2+ is 1.0 103M. The concentration of is 2 1.0 103 = 2.0 103M. 3. Solubility Product Constant Ksp: The expression for the solubility product constant Ksp for PbI2 is: Ksp = [Pb2+][I]2 Substituting the concentrations: Ksp = (1.0 103)(2.0 103)2 = (1.0 103)(4.0 106) = 4.0 109 Therefore, the solubility product, Ksp, for lead iodide at 25C is 4.0 109. The answer is D. 1.0 1.0 = 1.0 mmol/L = 1.0 103 M, [I] = 21.0 mmol To determine the solubility product constant Ksp for lead iodide PbI2, we need to follow these steps: 1. Understand the Dissociation Reaction: Lead iodide dissociates in water as follows: PbI2 Pb2+ + 2I 2. Determine the Concentrations: Given that the maximum amount of PbI2 that can dissolve in 1.00 of water is 1.0 mmol, we can find the molar concentrations of Pb2+ and ions produced. Since 1 mmol of PbI2 dissociates into 1 mmol of Pb2+ and 2 mmol of I: [Pb2+] = 1.0 mmol = 2.0 mmol/L = 2.0 103 3. Write the Expression for Ksp: The solubility product constant expression for PbI2 is: Ksp = [Pb2+] [I]2 4. Substitute the Concentrations into the Ksp Expression: Ksp = (1.0 103) (2.0 103)2 = (1.0 103) (4.0 106) = 4.0 109 Therefore, the solubility product constant Ksp for lead iodide PbI2 at 25C is 4.0 109. The answer is (D). To solve this problem, we need to understand the relationship between the solubility product constant Ksp and the solubility of compound. 1. Identifing the Dissociation Equation: Lead(II) iodide PbI2 dissociates in water as follows: PbI2(s) Pb2+(aq) + 2I(aq) 2. Expressing Solubility in Terms of Concentrations: Let be the molar solubility of PbI2. This means that when PbI2 dissolves, it produces moles of Pb A2 a g per liter of solution and 2s moles of Iaaaaa... To solve this problem, we need to understand the relationship between the solubility product constant Ksp and the solubility of compound. For lead iodide PbI2, the dissociation in water can be represented as: PbI2 Pb2+ + 2I Given that the maximum amount of PbI2 that can be dissolved in 1.00 of pure water is 1.0 mmol (which is 1.0 103mol, we can determine the concentrations of the ions in solution. 1. Determine the concentration of Pb2+ ions: Since 1.0 mmol of PbI2 dissociates into 1.0 mmol of Pb2+ ions, the concentration of Pb2+ ions is: [Pb2+] = 1.0 103M 2. Determine the concentration of ions: Each mole of PbI2 dissociates into 2 moles of ions. Therefore, the concentration of ions is: [I] = 2 1.0 103M = 2.0 103M 3. Calculate the solubility product constant Ksp: The expression for Ksp for PbI2 is: Ksp = [Pb2+][I]2. Substituting the concentrations we found: Ksp = (1.0 103)(2.0 103)2 = (1.0 103)(4.0 106) = 4.0 109 Therefore, the solubility product, Ksp, for lead iodide at 25C is 4.0 109. The answer is D. Table 10: Response comparisons of GaC and GaC + SAFE. Some verbose outputs are omitted due to space constraints."
        }
    ],
    "affiliations": [
        "AITRICS",
        "KAIST",
        "Korea University"
    ]
}