{
    "paper_title": "SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially?",
    "authors": [
        "Jianzhu Yao",
        "Kevin Wang",
        "Ryan Hsieh",
        "Haisu Zhou",
        "Tianqing Zou",
        "Zerui Cheng",
        "Zhangyang Wang",
        "Pramod Viswanath"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reasoning and strategic behavior in \\emph{social interactions} is a hallmark of intelligence. This form of reasoning is significantly more sophisticated than isolated planning or reasoning tasks in static settings (e.g., math problem solving). In this paper, we present \\textit{Strategic Planning, Interaction, and Negotiation} (\\textbf{SPIN-Bench}), a new multi-domain evaluation designed to measure the intelligence of \\emph{strategic planning} and \\emph{social reasoning}. While many existing benchmarks focus on narrow planning or single-agent reasoning, SPIN-Bench combines classical PDDL tasks, competitive board games, cooperative card games, and multi-agent negotiation scenarios in one unified framework. The framework includes both a benchmark as well as an arena to simulate and evaluate the variety of social settings to test reasoning and strategic behavior of AI agents. We formulate the benchmark SPIN-Bench by systematically varying action spaces, state complexity, and the number of interacting agents to simulate a variety of social settings where success depends on not only methodical and step-wise decision making, but also \\emph{conceptual inference} of other (adversarial or cooperative) participants. Our experiments reveal that while contemporary LLMs handle \\emph{basic fact retrieval} and \\emph{short-range planning} reasonably well, they encounter significant performance bottlenecks in tasks requiring \\emph{deep multi-hop reasoning} over large state spaces and \\emph{socially adept} coordination under uncertainty. We envision SPIN-Bench as a catalyst for future research on robust multi-agent planning, social reasoning, and human--AI teaming."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 9 4 3 2 1 . 3 0 5 2 : r SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? Jianzhu Yao1, Kevin Wang2, Ryan Hsieh2, Haisu Zhou2, Tianqing Zou2, Zerui Cheng1, Zhangyang Wang2, Pramod Viswanath1 1Department of Electrical and Computer Engineering, Princeton University, New Jersey, US 2Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, US {jy0246, pramodv}@princeton.edu {kevinwang.1839,atlaswang}@utexas.edu * Equal contribution. Website: https://spinbench.github.io Abstract Reasoning and strategic behavior in social interactions is hallmark of intelligence. This form of reasoning is significantly more sophisticated than isolated planning or reasoning tasks in static settings (e.g., math problem solving). In this paper, we present Strategic Planning, Interaction, and Negotiation (SPIN-Bench), new multi-domain evaluation designed to measure the intelligence of strategic planning and social reasoning. While many existing benchmarks focus on narrow planning or single-agent reasoning, SPIN-Bench combines classical PDDL tasks, competitive board games, cooperative card games, and multi-agent negotiation scenarios in one unified framework. The framework includes both benchmark as well as an arena to simulate and evaluate the variety of social settings to test reasoning and strategic behavior of AI agents. We formulate the benchmark SPIN-Bench by systematically varying action spaces, state complexity, and the number of interacting agents to simulate variety of social settings where success depends on not only methodical and step-wise decision making, but also conceptual inference of other (adversarial or cooperative) participants. Our experiments reveal that while contemporary LLMs handle basic fact retrieval and short-range planning reasonably well, they encounter significant performance bottlenecks in tasks requiring deep multihop reasoning over large state spaces and socially adept coordination under uncertainty. We envision SPIN-Bench as catalyst for future research on robust multi-agent planning, social reasoning, and humanAI teaming. 1. Introduction Large Language Models (LLMs) have recently demonstrated remarkable proficiency in generating coherent 1 text, contextual understanding, and variety of agentic tasks (Schick et al., 2023; DataBassGit, 2023). This progress has opened numerous avenues for deploying LLMbased agents in real-world applications such as digital assistance, complex decision-making support, and collaborative humanAI systems (Liu et al., 2023; Mialon et al., 2024). Yet, many tasks in these domains demand more than simple question-answering or short-range inference: they hinge on strategic planning, where large action/state spaces and multistep goal formulations require sophisticated long-horizon thinking (Sawada et al., 2024; Duan et al., 2024b). Beyond step-wise decision-making, modern AI systems increasingly need social intelligence: the ability to negotiate, cooperate, and reason about other agents hidden goals and beliefs (Hou et al., 2024; Cross et al., 2024). Environments such as Diplomacy (, FAIR), Avalon (Light et al., 2023), Werewolf (Xu et al., 2023), and various negotiation-based games (Liang et al., 2023; Abdelnabi et al., 2023; Hua et al., 2024) require not just logical strategies but also alliance formation, perspective-taking, and the capacity to handle incomplete or uncertain information. Taken together, these abilities can be seen as both methodical, step-wise planning (executing sequences of actions in possibly large search space) and conceptual inference (reasoning about hidden states, other players intentions, and partial observability). Successful multi-agent collaboration or competition arises when an AI system integrates both these aspects, adapting its plan based on the changing behavior of other agents. However, few existing benchmarks provide comprehensive, unified setting to rigorously test how LLMs balance these two intertwined requirementsstrategic long-horizon planning and multi-agent social reasoning. Traditional AI planning benchmarks, such as those based on PDDL (Aeronautiques et al., 1998), usually emphasize step-wise problem solving for single agents under fully observable conditions. By contrast, conversation-focused benchmarks often evaluate an LLMs linguistic coherence or role-play acumen (Li et al., 2023; Chen et al., 2023b) without imposing genuinely SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? complex strategic demands. 2. Related Work To address these gaps, we introduce Strategic Planning, Interaction, and Negotiation (SPIN-Bench), new evaluation framework specifically designed to capture both: 1. Strategic planning across single-agent formal tasks (PDDL) and highly interactive board/card games with large action and state spaces. 2. Social intelligence in multi-agent scenarios requiring negotiation, cooperation, or competition, as exemplified by Hanabi and Diplomacy. Crucially, SPIN-Bench integrates rich array of tasks that systematically scale up the complexity of the environment, thus exposing where and how LLMs planning or social reasoning falls short. We evaluate broad spectrum of popular open-source and state-of-the-art closed-source models, revealing significant performance bottlenecks as tasks become more intricate and agent interactions intensify. Our key findings are twofold. First, although models such as o1 exhibit competitive performance in certain long-horizon planning scenarios, they still falter in deep multi-hop reasoning once the action and state spaces expand substantially. Second, despite their advanced language generation capabilities, most current LLMs underperform in cooperative and negotiation tasks, suggesting deficiency in social intelligence under complex strategic conditions. Intriguingly, we observe that large-scale social interaction can negatively affect the chain-of-thought coherence in otherwise strong planning models like o1an interplay that has not been scrutinized in simpler benchmarks. In summary, our work makes the following contributions: 1. Unified Evaluation: We introduce single framework that spans formal planning (PDDL), board and card games, and negotiation-based scenarios, bridging previously siloed areas of research. 2. Systematic Strategic Reasoning Analysis: We expose fundamental limitations in how LLMs handle increasing action/state complexity, thus identifying future directions for improving long-horizon planning. 3. Social Intelligence Assessment: By testing multiagent negotiation, cooperation, and alliance-building, we highlight critical gaps between LLM performance and human baselines in strategic social contexts. Altogether, SPIN-Bench underscores both the real-world promise and current insufficiencies of LLMs in long-horizon, multi-agent tasks. We hope that our benchmark will motivate deeper research into strategic planning and social reasoning, paving the way for more capable humanAI partnerships and progress toward broader AI goals, including aspects of artificial general intelligence (AGI). 2.1. Evaluation of LLM in Planning LLMs have shown both promise and notable limitations in tasks demanding methodical, step-wise planning. Early work highlighted fundamental shortcomings in LLMs reasoning abilities, particularly on problem formulations grounded in PDDL (Hao et al., 2023; Valmeekam et al., 2023; 2024a; Wang et al., 2024). Yet, improvements in test-time scaling and prompt engineering have yielded models that can now achieve over 95% accuracy on discrete, place-based PDDL benchmarks (Valmeekam et al., 2024b; Wang et al., 2024). While these developments indicate an upward trajectory for LLM-based planners, current assessments tend to focus on relatively narrow or routine domains (e.g., classical planning formulations or daily schedule generation) (Zheng et al., 2024; Xie et al., 2024). Such specialized settings fail to expose many of the deeper challenges involved in long-horizon and multi-agent scenarios. To advance LLMs beyond these simpler planning tests, there is need for more holistic benchmarks that illuminate how models handle large state and action spaces, manage partial or uncertain information, and adapt their strategies to nontrivial dynamics. As we argue in this paper, such north star benchmark should not only evaluate how LLMs perform step-wise or fact-based reasoning, but also capture the interplay of conceptual inference (e.g., hidden intentions, shifting goals) and social intelligence (e.g., negotiation, alliance formation). These dimensions are especially vital if LLMs are to fulfill roles in complex decision-support systems and humanAI teaming. 2.2. LLMs for Gaming Gameswhether board, card, or digitaloffer an appealing testbed for LLMs because they often demand multi-step reasoning, strategic planning, and, in many instances, multiagent interaction. Recent work on single-game evaluations has explored domains such as Minecraft (Gong et al., 2024), Avalon (Light et al., 2023), and Werewolf (Xu et al., 2023). While these studies provide insights into how LLMs perform under specific game mechanics, each games unique rules and constraints limit how much we can generalize about broader AI competencies. Efforts to diversify gaming benchmarks have led to platforms that incorporate multiple tasks and game types (Zhang et al., 2024; Duan et al., 2024b;a; Costarelli et al., 2024), as well as multi-agent frameworks emphasizing coordination or competition (Huang et al., 2024; Cross et al., 2024; Agashe et al., 2024). Although these works have substantially enhanced our understanding of LLMs strategic reasoning, many still focus on narrow slices of game-playfor example, analyzing deception or coordination in isolation, using 2 SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? Figure 1: Overview of the Strategic Planning, Interaction, and Negotiation (SPIN-Bench) framework, highlighting its two core components: (1) the Game Agent, which encompasses the LLMs and their adaptive prompting, and (2) the Environment and Evaluation subsystem, which manage game logic, track interactions, and quantify performance. coarse-grained outcome metrics, or constraining interaction to simplified environments. Some critical dimensionslike open-ended negotiation or the evolving interplay between cooperation and conflictremain yet underexplored. Other lines of research delve deeper into social intelligence in game-based contexts, investigating theory of mind (Cross et al., 2024; Hou et al., 2024), game-theoretic decisionmaking (Chen et al., 2023a; Akata et al., 2023; Abdelnabi et al., 2023), and dialogue-driven challenges (Qiao et al., 2023; Chen et al., 2024; Duan et al., 2024b; Abdulhai et al., 2023). However, many existing setups still employ relatively constrained strategies or limited forms of social dynamics, risking an incomplete view of how LLMs behave under truly complex, multi-turn interactions. In contrast, our SPIN-Bench provides unified setting that encompasses formal PDDL analysis, multi-agent cooperation and competition, open-ended dialogues, and direct comparisons against optimal solvers and human baselines. By weaving strategic planning and social intelligence assessments into single testbed, we aim to uncover richer insights into the interplay between step-wise action selection and conceptual inference in extended-horizon, multi-agent gaming environments. 3. SPIN-Bench: Evaluating Game-Planning"
        },
        {
            "title": "Competency in LLMs",
            "content": "We now present SPIN-Bench, comprehensive benchmark designed to evaluate how well LLMs integrate methodical, step-wise planning with conceptual social reasoning across diverse range of tasks. As depicted in Figure 1, SPIN-Bench encompasses four major componentsPDDL Tasks, Competitive Games, Cooperative Games, and Strategic Gameseach reflecting increasingly intricate environments and forms of multi-agent interaction. In this section, we first formalize the problem settings underlying these tasks (Section 3.1), then introduce the specific game environments (Section 3.2) and describe our benchmark construction and evaluation metrics (Sections 3.33.4). 3.1. Problem Definition: Three Settings for Action"
        },
        {
            "title": "Sequence Generation",
            "content": "To capture the full spectrum of planning and social intelligence required in real-world scenarios, SPIN-Bench is organized around three progressively complex frameworks for automated action selection: 1. Classical Planning (Single-Agent, Deterministic). classical planning problem is typically defined by the tuple S, sinit, SG, A, , where: 3 SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? is the set of all possible states of the environment, 3.2. Game Taxonomy and Environments sinit is the known initial state, SG denotes the goal region or goal states, is the set of actions the agent can take, : is deterministic transition function. valid plan is finite sequence of actions π = a1, a2, . . . , an that transforms sinit into some goal state sg SG. By design, this setting involves one decisionmaker in fully observable, deterministic environment, thereby providing the most controlled context for probing an LLMs fundamental step-wise planning capabilities. 2. Multi-Agent Games (Cooperative or Competitive). Multi-agent games generalize single-agent planning to multiple decision-makers {1, 2, . . . , n}, each potentially with distinct or overlapping goals. Formally, such game can be written as = S, sinit, {SGi}n i=1, {Ai}n i=1, , where each agent has its own goal set SGi and action space Ai. The transition function depends on the joint action of all agents. Games can be: Competitive: if SGi SGj = for = j, as in zerosum formulations like Chess, requiring adversarial reasoning or minimax strategies. Cooperative: if all SGi coincide, thus encouraging agents to engage in joint planning under shared objective (e.g., ). 3. Strategic Games (Mixed Cooperation, Competition, and Negotiation). Strategic games extend multi-agent scenarios by introducing dynamic alliances, partial cooperation, and negotiation. Here, the environment state may include past communication logs or partially hidden objectives, allowing agents to form or break alliances, re-evaluate shared goals, and even negotiate mid-game. This richer structure captures broader spectrum of realistic multiagent behavior that blends both cooperation and competition within the same sessionDiplomacy (, FAIR) being prominent example. By exploring these three categories in tandem, SPIN-Bench systematically illuminates how LLMs grow from simpler single-agent planners into more complex negotiators and alliance builders, testing both mechanical planning skills and social inference in dynamic, multi-turn environments. 4 Figure 1 overviews how these three settings map onto specific SPIN-Bench tasks. In total, we integrate PDDL-based planning, competitive board games, cooperative card games, and negotiation-focused strategic games: PDDL. Within SPIN-Bench, classical planning tasks offer controlled environment to rigorously test core reasoning skills. We employ three subcategories: (i) factual retrieval in domains like elevator, grid, and floortile with progressively expanding state spaces; (ii) spatial reasoning in modified floortile scenario that challenges models to track agent location from relative instructions; (iii) full classical planning tasks spread across 21 domains (1,280 tasks), each involving sequential decisions, resource management, and constraint handling. Appendix A.1 provides domain-specific details. By starting with simpler tasks and escalating to complex multi-step planning, SPIN-Bench pinpoints whether an LLMs failures arise from inadequate state tracking, partial-order reasoning, or chain-of-thought breakdowns. Competitive Games. To evaluate adversarial reasoning, we include three widely studied turn-based board games of escalating complexity: Tic-tac-toe, Connect Four, and Chess. Each underscores key facet of competitive strategy: from short-range lookahead and forcing moves to deeper multi-step tactics. These games also enable direct comparisons against established AI solvers (Minimax (von Neumann, 1928) or Stockfish (Developers, 2023)) and against human-level baselines, providing clear gauge of how far LLMs are from optimal or near-optimal play. Cooperative Game (Hanabi). For multi-agent collaboration under imperfect information, SPIN-Bench leverages the cooperative card game Hanabi. Here, each player observes others cards but not their own, and must communicate implicitly to play cards in the correct order. This format demands not just straightforward planning but also trust-building, inference about hidden states, and coordinated actions essential for more advanced cooperative planning. Strategic Game (Diplomacy). Finally, SPIN-Bench incorporates Diplomacy, complex multi-player board game in which negotiation, alliance formation, and strategic betrayal are integral. Players exchange messages to coordinate or mislead, while simultaneously issuing movement orders to capture territories. Because negotiation is central, LLMs are tested on both long-horizon planning and social intelligencemanaging alliances, inferring hidden intentions, and adapting their strategies in response to evolving alliances. This setting is especially suited for evaluating how SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? chain-of-thought reasoning coexists with dynamic social interaction under partial cooperation and partial competition. 3.3. Benchmark Construction Classical Planning Data Generation. SPIN-BenchClassical is built by adapting established PDDL competition benchmarks (Seipp et al., 2022) and designing additional domains to target specific LLM capabilities (factual retrieval, spatial updates, etc.). Problem instances are generated via systematic randomization pipeline and validated with Fast Downward (Helmert, 2006) or SMTPlan (Cashmore et al., 2016). The VAL system (Planning, 2019) ensures solution feasibility. This tiered approach captures how LLMs transition from simpler factual tasks to multi-step planning under growing constraints. LLM Game Agent Interface. To accommodate broad range of tasks (from single-agent PDDL to multi-agent diplomacy), we implement flexible interface that feeds LLMs the current state description, relevant history, and list of legal actions or moves. In dialogue-based games (like Diplomacy), we also provide any public or private messages. Models are allowed up to ten retries when proposing illegal moves; exceeding this limit results in an automatic loss. This setup strikes balance between strict rule compliance and capturing potential misjudgments or misunderstandings that might arise in open-ended text generation. 3.4. Evaluation Metrics Rule-Based Metrics. For classical planning tasks, we measure both accuracy and N-Step Look Ahead: Step Look Ahead = (cid:80)N i=1(Ci Si) , where is the total number of tasks, Ci indicates correctness (0 or 1), and Si is the required number of steps for task i. This weighted measure highlights performance gaps on more complex problems. In competitive games, we compare LLM moves against solver-generated top-k actions and track head-to-head outcomes against the solver. For Hanabi, we use the final score to assess cooperative efficiency. Meanwhile, Diplomacy performance is evaluated through factual consistency, order-level correctness, and final game outcomes (see Appendix A.4 for details). Leaderboard-Based Comparison. We maintain internal Elo ratings for competitive games, updating models rating after each match to reflect its performance relative to an opponents rating. This helps us gauge the skill gap among different LLMs and compare them against human baselines (Appendix B.1). LLM-Assisted Negotiation Metrics. In negotiationheavy settings like Diplomacy, we use six fine-grained, LLM-assisted metrics: (1) alignment of messages with stated negotiation strategies, (2) proposal acceptance rate, (3) mutual vs. one-sided benefit of proposed deals, (4) peace vs. conflict inclination, (5) evidence of perspective-taking, and (6) conditional negotiation. An advanced reference model (e.g., o1) is prompted to annotate chat logs along these dimensions, enabling deeper look into each LLMs social intelligence, strategic coherence, and ability to balance persuasion with deception. Details of these annotations appear in Appendix A.4.2. Taken together, SPIN-Benchs unified approach and multidimensional metrics offer rich lens through which to measure how LLMs manage long-horizon strategic planning, multi-agent cooperation, and social negotiation in realistic, evolving scenarios. 4. Experiments on SPIN-Bench In this section, we provide comprehensive evaluation of how well Large Language Models (LLMs) handle the range of SPIN-Bench tasks introduced in Section 3. Specifically, we investigate whether current LLMs: 1. Manage core planning competencies, such as spatial reasoning, factual retrieval, and constraint following. 2. Cope with rising action complexity and larger multiagent interactions, comparing their performance to both optimal solvers and human baselines. 3. Demonstrate social intelligence in negotiation and cooperative settings. In what follows, we first outline the overall experimental setup (Section 4.1) and glance over the model performance (Section 4.2). Next, we detail results on core planning skills (Section 4.3), examine the impact of action complexity and agent scaling (Section 4.4), evaluate social intelligence and negotiation behaviors (Section 4.5), and conclude with summary of key insights (Section 4.6). 4.1. Outline and Setup Models and Configurations. We evaluate suite of popular LLMs that encompasses both closed-source The closedcommercial and open-source systems. source models include GPT-4o (Hurst et al., 2024), GPT-4o mini (OpenAI, 2024a), o1 (Jaech et al., 2024), o1-preview (OpenAI, 2024c), o1-mini (OpenAI, 2024b), o3-mini (OpenAI, 2025), GPT-4-turbo, GPT-3.5-turbo, Claude 3.5 Sonnet (Anthropic, 2024a), and Claude 3.5 Haiku (Anthropic, 2024b). 5 SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? Model Plan Acc N-Step TTTWR C4WR CHWR C4T3 CHT3 2P 3P 4P 5P Classical Planning Competitive Games Collaborative: Hanabi o1 o1-mini o3-mini GPT-4o GPT-4-turbo Claude 3.5 Sonnet Claude 3.5 haiku DeepSeek R1 Llama-3.3-70b 58.59 13.20 51.25 8.75 5.62 20.55 4.22 44.30 5.78 16.09 1.95 13.04 0.60 0.13 4.44 0.30 10.71 0. 30.0 50.0 80.0 100.0 40.0 40.0 50.0 90.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 83.1 87.0 74.2 84.1 83.8 78.9 69.6 78.9 79.5 45.9 36.5 52.8 32.2 38.7 49.5 35.9 47.8 25.4 14.8 7.4 7.6 4.8 5.6 9.4 4.0 16. 14.2 16.4 10.2 6.8 8.0 8.8 4.6 6.6 6.0 5.2 8.4 8.2 2.8 2.4 13.0 6.0 N/A N/A N/A N/A 14.8 11.4 8.8 4.8 5.0 7.4 2.8 11.3 Table 1: Merged results for Classical Planning, Competitive Games, and Collaborative Game (Hanabi). Plan Acc is the Planning Accuracy. N-Step denotes Steps Look-Ahead. TTT, C4, CH are three competitive games: Tic Tac Toe, Connect Four and Chess. WR subscript shows the solvers win rate (%) against each LLM(for chess, their opponent is stockfish-20). T3 subscript shows the top-3 moves percentages(%) among all games against the solver. Hanabi columns report average score for player counts 25. N/A in Hanabi indicates the model cant play that game even with prompt. 4 Agents (w/o vs. w. Negotiation) Negotiation Metrics: Social Intelligence Model o1 GPT-4o GPT-4-turbo DeepSeek-R1 o1-preview Claude 3.5 Haiku BS pass pass fail fail pass fail Move Attack SS SO SC CR AlignR AcceptR Mut:1S P:C Persp. Cond. 0.70/0.85 0.55/0.62 0.47/0.66 N/A N/A N/A 0.44/0.83 0.33/0.37 0.14/0.36 N/A N/A N/A 0.45/0.46 0.30/0.29 0.23/0.30 N/A N/A N/A 0.00/0.00 0.25/0.40 0.00/0.43 N/A N/A N/A 17/10 15/17 7/8 N/A N/A N/A 37/18 28/29 14/19 N/A N/A N/A 0.9800 0.9545 0.9643 0.9571 0.9490 0. 0.6533 0.6238 0.5411 0.6071 0.4699 0.3571 20.50 31.50 52.00 3.38 5.00 8.75 7.05 11.00 2.49 0.77 1.60 0.84 0.333 0.310 0.416 0.322 0.400 0.225 0.196 0.038 0.077 0.406 0.173 0.083 Table 2: Diplomacy results for the 4-agent setting, showing (without negotiation / with negotiation (x/y)) success rates for Move, Attack, Support-Self (SS), and Support-Others (SO) orders, as well as the number of Supply Centers (SC) and Controlled Regions (CR) when game finished. Full version of the multi-agent experiment result is shown in Table 10. On the right side are negotiation metrics from left to right: Alignment Ratio, Acceptance Rate, Mutual:1Side, Peace:Conflict, Perspective, and Conditionality. N/A indicates that model was not tested in that setting. Basic Skill Test(BS) shows whether specific models pass or fail the single agent minimal setting of Diplomacy game. al., 2025), Our open-source models include DeepSeek-R1 (Guo Llama3.1-70b, Llama3-70b, et Llama3.2-3b, Llama3.3-70b (AI, 2024a;b;c;d), Qwen2.5-72b (Team, 2024), and Mistral-7b (Jiang et al., 2023). Depending on the task, we select reprefor sentative subsets to manage computational costs; instance, classical planning experiments focus on nine well-known models (o1, o1-mini, o3-mini, GPT-4o, GPT-4-turbo, Claude 3.5 Haiku, Deepseek R1 and Llama3.3-70b). Claude 3.5 Sonnet, Evaluation Protocol. We adopt multiple metrics suitable for each domain. In classical planning tasks, we use accuracy, N-Step Look Ahead, and plan validation through VAL (Planning, 2019). Competitive board games are tested via LLM vs. solver and LLM vs. LLM matches, Top-3 action percentages and we maintain internal Elo scores for leaderboards (Appendix B.1). In cooperative and strategic settings, we measure final scores, success rates for different action types, and advanced LLM-assisted negotiation metrics when negotiations are enabled. Details of experimental settings, including prompt templates and domain rules, appear in Appendix A. 4.2. Overall Model Performance Across Tasks We begin with high-level view of each models performance across SPIN-Benchs diverse tasks. As summarized in Tables 1, 2, and further detailed in Appendix B.1, the strongest closed-source models (e.g., o1, o1-mini, o3-mini) generally lead in classical planning, board-game play, and cooperative scenarios. Among open-source models, DeepSeek-R1 and higher-parameter Llama3 variants tend to outperform smaller-scale systems, although their margins behind the top-tier commercial LLMs remain significant. In classical planning tasks, o1 and o3-mini consistently demonstrate robust state tracking and high N-step lookahead scores. Yet, even o1 struggles once the action/state space grows large or resource constraints become intricate (Figure 5). For competitive games such as Tic Tac Toe, SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? Figure 2: Spatial reasoning accuracy of LLMs. The average score is reported in Table 3. most LLMs achieve passable, if not optimal, results when matched against solvers, whereas performance in Chess drops dramatically. In cooperative games (Hanabi), models like o1 and DeepSeek R1 lead but still trail behind even first-quartile human scores (Figure 6). Finally, in strategic negotiation settings (Diplomacy), certain models exhibit solid basic competence but experience performance drops once multi-party negotiations and hidden alliances come into play. Overall, although some LLMs excel at short-horizon or less interactive tasks, they face major hurdles in extended, highbranching strategic planning and socially complex negotiations. Below, we delve deeper into the specific competencies and bottlenecks that underlie these results. 4.3. Core Planning Competencies: Spatial Reasoning, Factual Retrieval, and Constraint Following We first examine why LLMs might falter on long-horizon planning by isolating two crucial facets: (i) spatial reasoning and (ii) factual retrieval. In addition, we perform an error analysis that highlights common failure modes. 4.3.1. SPATIAL REASONING Spatial Tracking in PDDL Domain. To investigate whether LLMs planning deficits stem from weaker spatial understanding, we designed tasks requiring each model to track positions across sequences of relative movements. In these tasks, the LLM is given an initial spatial coordinate (e.g., (2,2)) along with series of movements(up, down, left, right), and it tb must produce the final coordinate. We evaluated all models on 90 scenes, each with varying number of steps, to assess their spatial reasoning capabilities. Figure 2 plots the accuracy of each model against the length of the movement trajectory. Notably, o1-mini and GPT-4o exhibit declining performance as the number of steps increases, whereas o1 sustains perfect accuracy (100%) up to 29 steps. 7 Figure 3: Evaluation of Large Language Model (LLM) performance in retrieving specific states from full-information trajectories. Each dot indicates the average accuracy for an individual task setting, computed over trajectories with lengths ranging from 1 to 50. The overall mean performance across all task settings is summarized in Table 3. Spatial Orders in Diplomacy. We further examine spatial understanding in more strategic setting, Diplomacy, where orders like Move and Attack require consistent tracking of boards and adjacency relations. Table 2 shows that o1 surpasses other models by 20%30% on success rate of basic orders. However, all models performance drops markedly for complex moves requiring multi-step or multi-agent logic (e.g., Support-Self and Support-Others), suggesting that deeper spatial reasoning plus alliance considerations create new failure modes. Model Factual Accuracy Spatial Accuracy o1 o1-mini GPT-4o Claude 3.5 Sonnet DeepSeek-R1 94.29% 56.33% 41.68% - - 100% 84.93% 43.93% 76.76% 97.38% Table 3: Error breakdown in the classical planning task. Refer to Figure 3 for an analysis of factual accuracy and to Figure 2 for spatial reasoning. Model BC (%) GS (%) Other (%) o1 o1-mini GPT-4o Claude 3.5 Sonnet 17.97 68.91 81.56 28.59 17.89 5.94 3.59 44.77 5.55 11.95 6.09 6.09 Table 4: Error breakdown across models, categorized by type. Percentages represent each categorys proportion of the total 1,280 problems. SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? (3) Others encompasses parsing errors, format inconsistencies, and miscellaneous validation issues. Our analysis reveals that models such as o1-mini and GPT-4o experience high rates of execution failures (68.91% and 81.56%, respectively), correlating with their performance in explicit factual retrieval tasks. In contrast, Claude 3.5 Sonnet demonstrates fewer execution failures but significantly higher rate of unmet goals, indicating effective factual retrieval but difficulties in reasoning toward goal completion. The detailed quantitative breakdown is provided in Table 4. Error Analysis in Board Games To evaluate the propensity of various models to generate illegal moves during gameplay, and its ability to follow game constraint, we conduct analysis on three competitive games. Experimental settings and detailed results are shown in Appendix B.1. The result analysis underscores clear correlation between the game complexity and the prevalence of illegal moves. Simpler games present minimal challenges, but in complex games(Chess), models exhibit higher tendency to generate illegal moves to break the game constraint. This suggests that more sophisticated strategic planning and rule comprehension are required to excel in complex games. 4.4. Impact of Action Complexity and Multi-Agent"
        },
        {
            "title": "Scale on Planning Performance",
            "content": "We now shift to analyze how the breadth of possible actions and the number of participating agents influence performance. 4.4.1. COMPLEX OR EXPANSIVE ACTION SPACES PDDL Domains. We further examine the results of 10 domains in classical planning tasks by analyzing two key measures: the average number of legal actions available at each step and the total stateaction space, defined as the complete set of possible stateaction pairs given an initial state and action space.To assess their impact on model performance, we compute the correlation between accuracy and these two measures. Surprisingly, as shown in Figure 5, accuracy exhibits stronger correlation with the total stateaction space rather than the average number of legal actions. This implies that cognitively, the model carries the burden of large future branching factors, even if only few choices are valid at each step. Competitive Board Games. Turning to our solver vs. LLM matchups (Table 1 for main results, Appendix 8 for all results), solvers always win or draw the game. Tic-tac-toe reveals that advanced LLMs (e.g., o1, GPT-4-turbo, Claude 3.5 Sonnet) can achieve draws some of the time, but typically still lose or draw to the Figure 4: Heatmap displaying the F1 scores across evaluation categories: Unit, Influence, Adjacent, Attackable, and Attack Analysis, in the Diplomacy game. 4.3.2. FACTUAL RETRIEVAL Factual Retrieval in Classical Planning. Here, we investigate whether LLMs can reliably retrieve key facts from planning trajectory. For each task, we provide the model with the initial state (sini), the goal state (SG), and trajectories that explicitly detail the actions along with their corresponding state transitions. We control the number of mutable properties; for example, in task involving moving balls, the ball location is considered mutable, whereas ball color might also be propertybut if no action can alter this property, we do not count it as mutable. We then prompt the model to report the state at specified step. Figure 3 illustrates how retrieval accuracy varies with trajectory length. Notably, o1 performs most consistently, confirming that it reads multi-step expansions more accurately than either GPT-4o or o1-mini."
        },
        {
            "title": "Factual",
            "content": "Checks."
        },
        {
            "title": "Knowledge",
            "content": "Diplomacy In Diplomacy, we design and categorize several factual queries into one-hop (e.g., Which locations does Russia control?) vs. multi-hop (e.g., Which territories can France attack, and how many units of support are needed at least?) to further check models factual retrieval in highly strategic environment. As summarized in Figure 4, nearly all LLMs do well on basic location or adjacency checks but degrade by large margin on Attackable and Attack Analysis, which demand deeper, multi-hop inference. Again, o1 and o1-preview lead, but still exhibit significant drops compared to simpler tasks. 4.3.3. ERROR ANALYSIS Error Analysis in Classical Planning In classical planning tasks, the model is provided with the sinit, sG, and is required to generate valid trajectory from the initial to the goal state. We categorized the errors into three distinct types: (1) Breaking Constraints (BC) includes execution failures, type mismatches, and structural issues, high BC reflecting difficulty in implicitly retrieving necessary preconditions; (2) Goal Not Satisfied (GS) refers to plans that satisfy all constraints yet fail to achieve the goal state; and 8 SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? Diplomacy with More Agents. Diplomacy also allows variable numbers of participating powers. Detailed results of more multi-agent settings are shown in Appendix B.2.2. As the agent count grows (beyond 2-3 test seats for LLMs), we observe decreasing order accuracy, fewer successful attacks, and minimal supply-center gains. Ultimately, LLMs lose traction in highly interactive scenarios, underscoring how partial observability and shifting alliances further intensify the multi-agent complexity. 4.5. Social Intelligence and Negotiation Performance While the preceding sections highlight LLMs challenges in purely strategic or planning-centric domains, complex multi-agent settings further demand social intelligencethe ability to navigate cooperative, adversarial, and negotiated alliances. In this part of our evaluation, we compare LLM performance to human baselines and introduce explicit negotiation phases to assess whether social interactions degrade or enhance model behavior. Overall, we find that most current LLMs under-perform in both cooperative and negotiation-driven tasks, revealing critical gaps in their ability to manage dynamic, multi-agent contexts. In some cases (e.g., o1), large-scale social interaction even appears to negatively affect chain-of-thought coherence, underscoring an interplay between deep planning and social complexity that has not been thoroughly investigated in simpler benchmarks. 4.5.1. COOPERATIVE SCENARIOS VS. HUMAN"
        },
        {
            "title": "BASELINES IN HANABI",
            "content": "We collected 54,977 human-played Hanabi games from BoardGameGeek, spanning 2to 5-player settings. Figure 6 plots the human score distribution, highlighting quartiles (Q1Q4) around typical range of 1525 points. While some LLMs do show patterns of declining performance with more agents (cf. Table 1), none approach even the first quartile of human scores. This underscores the significant gap in cooperative planning under hidden-information constraintsdespite Hanabis narrower branching factor relative to some competitive games. The results suggest pervasive deficiency in social intelligence when LLMs must coordinate multiple incomplete information channels and track teammates evolving knowledge states. 4.5.2. NEGOTIATION IN DIPLOMACY Overall Impact on Strategic Play. Negotiation phases in Diplomacywhich theoretically enable players to form or dissolve alliancesoften produce counterintuitive effects on strong planners like o1. As shown in Table 2 and Table 10, o1s all kinds of order numbers drop when negotiation is introduced, whereas other models sometimes stay the same or even improve slightly. And final game Figure 5: Relationship between accuracy and action space constraints for o1. The left plot depicts accuracy as function of the average number of legal actions, while the right examines accuracy concerning the state-action space size. perfect solver. In Connect Four and Chess, the gap widens: our solver and Stockfish-level engines maintain 100% win rate across all tested LLMs. We also conduct analysis on the top action percentages in Appendix(Figure 7). The distribution shows that while LLMs sometimes pick optimal moves in Connect Four, their accuracy drops drastically in Chess, underscoring how deeper tactics and branching expansions are beyond current LLMs capacity. Claude 3.5 Haiku and o3-mini fare somewhat better, yet remain far from human-expert or solver-level play. Diplomacy Basic Skills. Diplomacy features an even larger branching factor. As baseline no negotiation test, we pit one LLM against six neutral powers. There are only 3 out of 9 models pass the basic skill test: Table 2 shows that only o1-preview, o1 and GPT-4o can take 18 supply centers in the first 20 years. Other models time out after 20 years, reflecting the significant difficulty of long-horizon planning in such vast environment. Details of each model and humans performance on this basic skill test is shown in Appendix B.2.1. 4.4.2. SCALING NUMBER OF AGENTS Hanabi with More Agents. We vary the number of players from 2 to 5 in Hanabi and track final scores. Table 1 indicates that o1 remains the top performer, but its average score still declines from 16.4 (2-player) to 14.2 (5-player). This dip suggests that coordinating beliefs and actions among more agents still strains strong LLMs capacity to maintain consistent strategies over multiple incomplete information channels. However, models such as o1-mini and DeepSeek-R1 display erratic performance. Their high variance and relatively low average scores suggest insufficient strategic reasoning capabilities for effective Hanabi play. 9 SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? betrayals or emergent alliances, models often cling to simplistic patterns of communication, or in o1s case, become overwhelmed by large volumes of social exchange. These observations reinforce that contemporary LLMs exhibit only nascent social intelligence in scenarios where partial cooperation and deception can dramatically alter long-horizon outcomes. Figure 6: Human Score Distribution with different number of players on Hanabi in 54977 games. 4.6. Key Insights and Limitations scores Supply Centers and Controlled Regions for o1 also dropped by large margin when enabling the negotiation, compared to other models. This result suggests that intense social interaction can disrupt planning coherence in otherwise capable LLMs, pointing to tension between extended chain-of-thought reasoning and the cognitive overhead of real-time alliance-building, deception, or persuasion. Negotiation Message Analysis. To understand each models negotiation style in greater depth, we prompted o1 to annotate 1,991 messages across six-agent Diplomacy settings using six LLM-assisted metrics  (Table 2)  . We highlight several patterns: (Details of each metric is shown in Appendix A.4.2.) 1. High Strategy Alignment: All models display alignment ratios above 0.90, implying that their negotiation messages largely match stated objectives. 2. Persuasiveness: o1 exhibits the highest acceptance rate ( 0.65), whereas Claude 3.5 Haiku is less effective ( 0.36). 3. Mutual vs. One-Sided Benefits: Models mostly propose mutually beneficial deals; GPT-4-turbo in particular tends not to push overtly one-sided offers. 4. Peace vs. Conflict Messaging: DeepSeek-R1 and Claude 3.5 Haiku demonstrate relatively low peace ratio (0.77 and 0.84), showing their tendency to provoke conflicts, whereas o1 and GPT-4o strongly prefers peaceful approaches (7.05 and 11.00), showing conservatism in potentially adversarial negotiations. 5. Perspective-Taking and Conditional Plans: Although moderate across the board, GPT-4-turbo leads in referencing other agents intentions ( 0.42), while DeepSeek-R1 uses conditional if-then tactics most frequently ( 0.406). Despite these varied negotiation styles, most LLMso1 includedlack flexibility and adaptivity akin to skilled human negotiators. Rather than shifting strategies in response to 10 Synthesizing our findings across all SPIN-Bench tasks, we note the following themes: LLMs handle simpler planning but falter with high branching factors. Although many models perform decently in small-scale puzzles or games like Tic-tac-toe, performance crumbles in Chess, complex PDDL domains, or Diplomacy. Multi-hop reasoning and partial information remain core bottlenecks. Even advanced models like o1 see large drops in multi-step spatial or factual inference, especially under partial observability. Social interactions can impede strategic coherence. Negotiation phases degrade o1s performance, revealing trade-off between pure logical planning and the overhead of dynamic alliance-making through social interaction, which can disrupt chain-of-thought reasoning. LLMs underperform significantly vs. humans in cooperative tasks. Hanabi experiments confirm considerable gap: no model meets even the first-quartile human score distribution. These results suggest that although LLMs have advanced in short-horizon planning and fact retrieval, major challenges persist in long-horizon reasoning, complex multi-agent collaboration, and strategic negotiation. Addressing these limitations likely demands novel training paradigms, specialized planning modules, or more sophisticated representations of theory-of-mind and hidden information. 5. Conclusion In this paper, we introduced SPIN-Bench, comprehensive benchmark designed to assess strategic planning and social intelligence in Large Language Models (LLMs) across multiple game environments of escalating complexity. Our experiments encompassed formal PDDL-based planning, competitive board games, cooperative incomplete-information scenarios, and negotiation-intensive strategic settings. By systematically varying action and state-space size, interSPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? action modalities, and the number of agents, SPIN-Bench exposes critical limitations in todays LLMs. Despite recent improvementsparticularly in short-range planning and factual recallour findings reveal that most LLMs struggle with long-horizon tasks involving large branching factors and intricate multi-agent coordination. We also observe that negotiation and social interplay often degrade an LLMs chain-of-thought coherence, suggesting tension between pure strategic reasoning and the cognitive overhead of dynamic alliances and potential deception. Overall, our work highlights pressing gaps in both deep, multi-hop reasoning and robust social interaction, pointing toward the need for novel architectural innovations, integrated planning modules, and more advanced training methodologies. We hope SPIN-Bench will serve as foundation and catalyst for continued progress in developing LLMs that are more strategically competent, socially adept, and ultimately closer to the vision of AGI. Limitation. primary limitation of this work lies in the use of pre-defined prompt templates. Additionally, our benchmarkthough broadcannot fully capture the complete spectrum of real-world strategic and social interactions. Consequently, the generalizability of our findings may be constrained. Future research could address these limitations by exploring more adaptive or automated promptengineering strategies and by extending the scope of SPINBench to include an even wider array of tasks. 6. Impact Statements Our work rigorously evaluated LLMs abilities in longhorizon decision-making, multi-agent coordination, and social reasoning. In doing so, SPIN-Bench provides structured platform for both diagnosing current limitations and guiding the development of more advanced AI reasoning tools. Potential applications of this research span autonomous decision-making, game-theoretic analysis, and humanAI collaboration. However, these same capabilities raise important ethical concerns, particularly around fairness, manipulation, and unintended biases when highly capable systems engage in competitive or high-stakes settings. Ensuring that such systems remain transparent, accountable, and aligned with human values is crucial aspect of responsible AI development. By identifying specific gaps in strategic and social competencies, this research not only advances the technical frontier but also highlights the need for frameworks that mitigate misuse and foster ethically grounded innovation."
        },
        {
            "title": "References",
            "content": "Abdelnabi, S., Gomaa, A., Sivaprasad, S., Schonherr, L., and Fritz, M. Llm-deliberation: Evaluating llms with interactive multi-agent negotiation games. 2023. Abdulhai, M., White, I., Snell, C., Sun, C., Hong, J., Zhai, Y., Xu, K., and Levine, S. Lmrl gym: Benchmarks for multi-turn reinforcement learning with language models. arXiv preprint arXiv:2311.18232, 2023. Aeronautiques, C., Howe, A., Knoblock, C., McDermott, I. D., Ram, A., Veloso, M., Weld, D., Sri, D. W., Barrett, A., Christianson, D., et al. Pddl the planning domain definition language. Technical Report, Tech. Rep., 1998. Agashe, S., Fan, Y., Reyna, A., and Wang, X. Llmcoordination: evaluating and analyzing multi-agent coordination abilities in large language models. arXiv preprint arXiv:2310.03903, 2024. AI, M. Meta llama 3, 2024a. URL https://ai.meta. com/blog/meta-llama-3/. Accessed: 2024-0130. AI, M. Meta llama 3.1, 2024b. URL https://ai.m eta.com/blog/meta-llama-3-1/. Accessed: 2024-01-30. AI, M. Llama 3.2: Connect 2024 vision and edge mobile devices, 2024c. URL https://ai.meta.com/bl og/llama-3-2-connect-2024-vision-edg e-mobile-devices/. Accessed: 2024-01-30. AI, M. Llama 3.3 70b instruct, 2024d. URL https: //huggingface.co/meta-llama/Llama-3.3 -70B-Instruct. Accessed: 2024-01-30. Akata, E., Schulz, L., Coda-Forno, J., Oh, S. J., Bethge, M., and Schulz, E. Playing repeated games with large language models. arXiv preprint arXiv:2305.16867, 2023. Anthropic. Claude 3.5 sonnet, 2024a. URL https://ww w.anthropic.com/news/claude-3-5-sonne t. Accessed: 2024-01-30. Anthropic. Claude haiku, 2024b. URL https://ww w.anthropic.com/claude/haiku. Accessed: 2024-01-30. Cashmore, M., Fox, M., Long, D., and Magazzeni, D. compilation of the full pddl+ language into smt. In Proceedings of the international conference on automated planning and scheduling, volume 26, pp. 7987, 2016. Chen, J., Yuan, S., Ye, R., Majumder, B. P., and Richardson, K. Put your money where your mouth is: Evaluating strategic planning and execution of llm agents in an auction arena. arXiv preprint arXiv:2310.05746, 2023a. Chen, J., Hu, X., Liu, S., Huang, S., Tu, W.-W., He, Z., and Wen, L. Llmarena: Assessing capabilities of large language models in dynamic multi-agent environments. arXiv preprint arXiv:2402.16499, 2024. 11 SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? Chen, J. C.-Y., Saha, S., and Bansal, M. Reconcile: Round-table conference improves reasoning via consensus among diverse llms. arXiv preprint arXiv:2309.13007, 2023b. Costarelli, A., Allen, M., Hauksson, R., Sodunke, G., Hariharan, S., Cheng, C., Li, W., Clymer, J., and Yadav, A. Gamebench: Evaluating strategic reasoning abilities of llm agents. arXiv preprint arXiv:2406.06613, 2024. Cross, L., Xiang, V., Bhatia, A., Yamins, D. L., and Haber, N. Hypothetical minds: Scaffolding theory of mind for multi-agent tasks with large language models. arXiv preprint arXiv:2407.07086, 2024. DataBassGit. Auto-gpt: An autonomous gpt-4 experiment, 2023. URL https://github.com/DataBassG it/Auto-GPT. Accessed: 2025-01-30. Developers, T. S. Stockfish chess engine. https://st ockfishchess.org/, 2023. Accessed: 2025-01-31. Duan, J., Wang, S., Diffenderfer, J., Sun, L., Chen, T., Kailkhura, B., and Xu, K. Reta: Recursively thinking ahead to improve the strategic reasoning of large language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 22322246, 2024a. Duan, J., Zhang, R., Diffenderfer, J., Kailkhura, B., Sun, L., Stengel-Eskin, E., Bansal, M., Chen, T., and Xu, K. Gtbench: Uncovering the strategic reasoning limitations of llms via game-theoretic evaluations. arXiv preprint arXiv:2402.12348, 2024b. (FAIR), M. F. A. R. D. T., Bakhtin, A., Brown, N., Dinan, E., Farina, G., Flaherty, C., Fried, D., Goff, A., Gray, J., Hu, H., et al. Human-level play in the game of diplomacy by combining language models with strategic reasoning. Science, 378(6624):10671074, 2022. Gong, R., Huang, Q., Ma, X., Noda, Y., Durante, Z., Zheng, Z., Terzopoulos, D., Fei-Fei, L., Gao, J., and Vo, H. Mindagent: Emergent gaming interaction. In Findings of the Association for Computational Linguistics: NAACL 2024, pp. 31543183, 2024. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Hao, S., Gu, Y., Ma, H., Hong, J. J., Wang, Z., Wang, D. Z., and Hu, Z. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992, 2023. Helmert, M. The fast downward planning system. Journal of Artificial Intelligence Research, 26:191246, 2006. Hou, G., Zhang, W., Shen, Y., Tan, Z., Shen, S., and Lu, W. Entering real social world! benchmarking the theory of mind and socialization capabilities of llms from firstperson perspective. arXiv preprint arXiv:2410.06195, 2024. Hua, W., Liu, O., Li, L., Amayuelas, A., Chen, J., Jiang, L., Jin, M., Fan, L., Sun, F., Wang, W., et al. Gametheoretic llm: Agent workflow for negotiation games. arXiv preprint arXiv:2411.05990, 2024. Huang, J.-t., Li, E. J., Lam, M. H., Liang, T., Wang, W., Yuan, Y., Jiao, W., Wang, X., Tu, Z., and Lyu, M. R. How far are we on the decision-making of llms? evaluating llms gaming ability in multi-agent environments. arXiv preprint arXiv:2403.11807, 2024. Hurst, A., Lerer, A., Goucher, A. P., Perelman, A., Ramesh, A., Clark, A., Ostrow, A., Welihinda, A., Hayes, A., Radford, A., et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Jaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Li, G., Hammoud, H., Itani, H., Khizbullin, D., and Ghanem, B. Camel: Communicative agents for mind exploration of large language model society. Advances in Neural Information Processing Systems, 36:5199152008, 2023. Liang, T., He, Z., Jiao, W., Wang, X., Wang, Y., Wang, R., Yang, Y., Shi, S., and Tu, Z. Encouraging divergent thinking in large language models through multi-agent debate. arXiv preprint arXiv:2305.19118, 2023. Light, J., Cai, M., Shen, S., and Hu, Z. Avalonbench: Evaluating llms playing the game of avalon. In NeurIPS 2023 Foundation Models for Decision Making Workshop, 2023. Liu, X., Yu, H., Zhang, H., Xu, Y., Lei, X., Lai, H., Gu, Y., Ding, H., Men, K., Yang, K., et al. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688, 2023. Mialon, G., Fourrier, C., Wolf, T., LeCun, Y., and Scialom, T. GAIA: benchmark for general AI assistants. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/f orum?id=fibxvahvs3. 12 SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? and reasoning about change. Advances in Neural Information Processing Systems, 36, 2024a. Valmeekam, K., Stechly, K., and Kambhampati, S. Llms still cant plan; can lrms? preliminary evaluation of openais o1 on planbench. arXiv preprint arXiv:2409.13373, 2024b. von Neumann, J. Zur theorie der gesellschaftsspiele. Mathematische Annalen, 100(1):295320, 1928. Wang, K., Li, J., Bhatt, N. P., Xi, Y., Liu, Q., Topcu, U., and Wang, Z. On the planning abilities of openais o1 models: Feasibility, optimality, and generalizability. arXiv preprint arXiv:2409.19924, 2024. Xie, J., Zhang, K., Chen, J., Zhu, T., Lou, R., Tian, Y., Xiao, Y., and Su, Y. Travelplanner: benchmark for real-world planning with language agents. arXiv preprint arXiv:2402.01622, 2024. Xu, Y., Wang, S., Li, P., Luo, F., Wang, X., Liu, W., and Liu, Y. Exploring large language models for communication games: An empirical study on werewolf. arXiv preprint arXiv:2309.04658, 2023. Zhang, Y., Mao, S., Ge, T., Wang, X., de Wynter, A., Xia, Y., Wu, W., Song, T., Lan, M., and Wei, F. Llm as mastermind: survey of strategic reasoning with large language models. arXiv preprint arXiv:2404.01230, 2024. Zheng, H. S., Mishra, S., Zhang, H., Chen, X., Chen, M., Nova, A., Hou, L., Cheng, H.-T., Le, Q. V., Chi, E. H., et al. Natural plan: Benchmarking llms on natural language planning. arXiv preprint arXiv:2406.04520, 2024. OpenAI. Gpt-4o mini: Advancing cost-efficient intelligence, 2024a. URL https://openai.com/index/gpt -4o-mini-advancing-cost-efficient-int elligence/. Accessed: 2024-01-30. OpenAI. Openai o1 mini: Advancing cost-efficient reasoning, 2024b. URL https://openai.com/index /openai-o1-mini-advancing-cost-effic ient-reasoning/. Accessed: 2024-01-30. OpenAI. Introducing openai o1 preview, 2024c. URL https://openai.com/index/introducing -openai-o1-preview/. Accessed: 2024-01-30. OpenAI. Openai o3-mini: Pushing the frontier of costeffective reasoning, January 2025. URL https://op enai.com/index/openai-o3-mini/. Accessed: 23 February 2025. Planning, K. Val: Validation and checking for pddl models. GitHub repository, 2019. Available at https://gith ub.com/KCL-Planning/VAL. Pons, P. Connect4, 2023. URL https://github.com /PascalPons/connect4. Accessed: 2025-01-29. Qiao, D., Wu, C., Liang, Y., Li, J., and Duan, N. Gameeval: Evaluating llms on conversational games. arXiv preprint arXiv:2308.10032, 2023. Sawada, T., Paleka, D., Havrilla, A., Tadepalli, P., Vidas, P., Kranias, A. P., Nay, J. J., Gupta, K., and Komatsuzaki, A. ARB: Advanced reasoning benchmark for large language models, 2024. URL https://openreview.net/f orum?id=gsZAtAdzkY. Schick, T., Dwivedi-Yu, J., Dess`ı, R., Raileanu, R., Lomeli, M., Hambro, E., Zettlemoyer, L., Cancedda, N., and Scialom, T. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36:6853968551, 2023. Seipp, J., Torralba, A., and Hoffmann, J. PDDL generators. https://doi.org/10.5281/zenodo.63821 73, 2022. Team, Q. Qwen2.5: party of foundation models, September 2024. URL https://qwenlm.github.io/b log/qwen2.5/. Valmeekam, K., Marquez, M., Sreedharan, S., and Kambhampati, S. On the planning abilities of large language models-a critical investigation. Advances in Neural Information Processing Systems, 36:7599376005, 2023. Valmeekam, K., Marquez, M., Olmo, A., Sreedharan, S., and Kambhampati, S. Planbench: An extensible benchmark for evaluating large language models on planning SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? Within this supplementary material, we elaborate on the following points: Appendix A: Experiment Settings for Each Game Appendix B: Supplementary Experiments Appendix C: Prompt Design A. Experiment Settings A.1. Classification and description of PDDL Domains Unlike existing LLM-based planning benchmarks, which typically focus on discrete-state PDDL problems, SPIN-Bench extends coverage to include numerical PDDL tasks with optimization goals. In these numeric domains, the objective is no longer simply to reach discrete state but to achieve specified numeric threshold or maximize/minimize particular quantity. For example, in the markettrader domain, the goal is to accrue at least $100. LLMs perform worse on these numeric tasks than on standard discrete-state PDDL tasks, likely due to the combinatorial explosion and broader goal-state space inherent to numeric domains. selection of the PDDL tasks is provided in the supplementary materials. Spatial Domains drone: Focuses on controlling drone across map, planning flight paths and possibly collecting items or information. floortile: Involves moving an agent over grid of tiles to mark or paint them, ensuring proper coverage or pattern completion. grid: Classic grid-navigation domain where an agent moves between cells to reach designated locations or achieve tasks. depots (also resource management): Combines transportation and hoist operations to move crates between trucks and warehouses, requiring both navigation and efficient resource allocation. logistics: Centers on delivering packages between locations using trucks and airplanes, emphasizing route planning and scheduling. rovers (also resource management): Involves planetary rovers navigating terrain, collecting samples, and managing limited resources like fuel or battery power. sokoban: Puzzle-based domain where an agent pushes crates in warehouse grid, aiming to position them in specific target cells. termes: Models autonomous robots constructing structures in grid by navigating, carrying blocks, and cooperating on building tasks. Sequential Domains assembly: Entails sequentially combining parts to form final product, highlighting the order of assembly steps. blocksworld: Iconic stacking puzzle where blocks must be ordered and stacked under tight move constraints. briefcaseworld: Tasks an agent with moving objects via briefcase between locations, planning the order of loading/unloading. Multi Agent coordination: Focuses on collaboration among multiple gripper agents, they need to cooperate to achieve the goal. cooperate sequential gripper (also resource management): Multiple agents with limited grippers must coordinate object pickups, balancing the sequence of actions with shared resources. elevator (also spatial): Handles transporting passengers or goods through building floors, requiring efficient elevator movements and scheduling. barman: Bartender scenario where limited glasses and ingredients must be managed to prepare ordered drinks. Resource Management Domains counters: Involves manipulating numerical counters under constraints to reach target values or states. 14 SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? markettrader: financial trading scenario requiring buying, selling, and resource (capital) management to achieve profit goals. satellite: Space-based operations where satellites manage instruments, energy, and data storage to fulfill observation tasks. freecell: Card puzzle variant with limited free cells, each used as temporary resource to reshuffle the card configuration. rovers (also spatial): Rovers must navigate and handle tasks while managing finite consumables such as fuel or batteries. settlersnumeric: Inspired by resource collection and expansion strategies, requiring numeric resource management to meet specified targets. sugar: Involves transforming sugar among various states (solid, dissolved, etc.), requiring careful handling of processes and resource constraints. A.2. Two-player Competitive Games Game Introduction Tic Tac Toe is classic and simple two-player game played on 3x3 grid. Each player takes turns marking space on the grid, with one player using and the other O. The objective is to be the first to align three of your marks horizontally, vertically, or diagonally. Despite its straightforward rules, the game challenges players to think strategically, anticipate their opponents moves, and block potential winning combinations. Tic Tac Toe is widely popular because it is easy to learn, quick to play, and requires minimal setup. So in our paper, we use Tic Tac Toe as the simplest setting testing LLMs ability to understand basic rules and strategy. Connect Four is two-player strategy game played on vertical grid with six rows and seven columns. Players take turns dropping colored discs into the grid, with the goal of being the first to align four of their discs horizontally, vertically, or diagonally. The game combines elements of planning and foresight, as players must both create winning opportunities and block their opponents moves. Connect Four is known for its simple rules and engaging game-play. It is little harder than Tic Tac Toe, so we include it to evaluate LLMs capacity for intermediate-level strategic reasoning. Chess is timeless two-player strategy game played on an 8x8 checkered board. Each player controls an army of 16 pieces, including king, queen, rooks, bishops, knights, and pawns, each with unique movements and abilities. The objective is to checkmate the opponents king, putting it in position where it cannot escape capture. Chess challenges players to think several moves ahead, employing tactics, strategy, and creativity. Revered as game of intellect, it offers endless possibilities and rich complexities, making it an ideal test for LLMs advanced planning and decision-making skills."
        },
        {
            "title": "Game Setting",
            "content": "We have two settings for the two player competitive games, one is LLM-vs-Solver, and the other is LLM-vs-LLM. For the LLM-vs-Solver setting, we make LLMs play against the most powerful game engines. For Tic Tac Toe, we implemented solver using the Minimax algorithm to determine optimal moves by recursively evaluating all possible game states. It first checks if the game has ended in win or draw. If not, it explores all legal moves, simulating each outcome by alternating between maximizing Xs score and minimizing Os score. The best move is randomly chosen from the moves with highest values. This approach guarantees perfect play, and the solvers winning rate can be used as strong benchmark for evaluating LLMs. For Connect Four, we use the Connect 4 solver(Pons, 2023) implementation. For Chess, we use the well-known Stockfish engine, with different skill levels: 0, 5, 10, 15, 20 to compete with LLMs. Although chess engine Stockfish has external Elo ratings anchored to the CCLR Blitz Elos, since all LLMs are losing every game with even the lowest level of Stockfish, we didnt report the external elo rating for chess. For the LLM-vs-LLM setting, we investigate different LLMs performance in competitive game setting. For each game, we tested 14 models in total, and every two models forming pair, while introducing 91 game pairs. For each pair, we conduct 10 repetitive games for Tic Tac Toe and Connect Four, and 4 repetitive games for Chess. Players switch their positions after half of the pair competition. To provide baseline on how average or expert humans perform against LLMs, especially in simpler games like Tic Tac Toe and Connect Four, we also include human performance in the leaderboard, with multiple game rounds between different LLMs and human to update the Elo rating. 15 SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? In the leaderboard, we report internal Elo ratings (relative Elo between members of the population) for Tic Tac Toe, Connect Four and Chess. Elo rating is well-established method widely used in chess and other competitive games, updated incrementally based on match outcomes and opponents ratings. This metric effectively captures the skill gap between different LLMs playing these games, providing quantitative measure of their relative performance. For the step-wise evaluation metrics, we use the scores given by the solver and Stockfish, among all the trajectories in LLM-vs-Solver setting. For Tic Tac Toe and Connect Four, use the solver to generate scores for each move of LLM at each state. For Chess, we use Stockfish level 20 as the rating machine for step-wise analysis, on each step in the game between LLMs and Stockfish level 0. We use centipawns as measurement of the advantage of chess board. centipawn is equal to 1/100 of pawn. These values are essentials in computer chess to evaluate positions. For each pair of the game, we conduct 4 repetitive experiments. A.3. Multi-player Cooperative Game - Hanabi Game Introduction Hanabi is cooperative card game where players work together to create series of fireworks by playing cards in ascending numerical order starting from 1. Each player holds their cards facing outward so that all players can see everyone elses cards but not their own. The objective is to play cards in sequence (1 through 5) for each color without making mistakes. There are 5 different colors and each color has cards numbered 1 to 5. The game begins with 8 available information tokens and 3 life/fuse tokens. Play proceeds around the table; each turn, player must take one of the following actions: 1. Give Information: The player points out the cards of either given number or given color in the hand of another player (examples: This card is your only red card, These two cards are your only 3s). The information given must be complete and correct. Giving information consumes one information token. 2. Discard Card: The player chooses card from their hand and adds it to the discard pile, then draws card to replace it. The discarded card is out of the game and can no longer be played. Discarding card replenishes one information token. 3. Play Card: The player chooses card from their hand and attempts to add it to the cards already played. This is successful if the card is 1 in color that has not yet been played, or if it is the next number sequentially in color that has been played. Otherwise life/fuse token is consumed and the misplayed card is discarded. Successfully playing 5 of any color replenishes one information token. Whether the play was successful or not, the player draws replacement card. The game ends immediately when either all life/fuse tokens are used up, resulting in game loss, or all 5s have been played successfully, leading to game win. Otherwise, play continues until the deck runs out, and for one full round after that. At the end of the game, the values of the highest cards in each color are summed, resulting in total score out of possible 25 points. Experiment Setting In our Hanabi experiment, we tested 9 LLMs in total: o1, o1-mini, o3-mini, GPT-4o, GPT-4-turbo, Claude 3.5 Sonnet, Claude 3.5 Haiku, DeepSeek R1, Llama-3.3-70b. However, Llama-3.3-70b failed to play this game, so we only reported the above 8 models results. For each model, we have four settings: allowing the player number to range from 2 to 5. For example, model o1 with player number of 3 means there are 3 separate o1 models playing the game. And we use the final score when the game ends as the score of each game setting. A.4. Strategic Game - Diplomacy A.4.1. GAME INTRODUCTION (Note: The following game introduction is excerpted from the Diplomacy (game) Wikipedia Page) Diplomacy is 7-player turn based game, where players can use negotiation and strategy to control the most supply centers on the map. The players can move their units to different locations on the map, and can support other players units to help them succeed. The game is played on map of Europe, divided into territories and sea zones. The players can issue orders to their units to move, support, hold, or convoy. The game ends when one player controls 18 supply centers. 34 of the land provinces are supply centers. Possession of these supply centers allows the powers who control them to raise and maintain 16 SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? armies and fleets. As they are also central part of the games victory conditions, they are the focus of much of the games activity. Each player is given three (save for Russia, which has four) home supply centers. These spaces are the starting point for their owning powers initial forces. The players can then build new units at these home supply centers as they capture further supply centers. New units can only be built on powers home supply centers. If power loses all of its home supply centers it may continue to play; however, it may not build new units until it has recaptured at least one of its home supply centers. In Diplomacy, there are two types of units: Armies and Fleets. An army can travel in land spaces and coastal land spaces, and fleet can travel in sea spaces and coastal land spaces. All units in Diplomacy move only one space at time and only one unit may occupy any space at any time. The exception to this rule comes in the form of successful convoy, where convoyed army may travel multiple spaces depending on the length of the chain created by the convoying fleets. convoyed army must embark from coastal land province and land at coastal land province. Diplomacy proceeds by seasons, beginning in the year 1901, with each year divided into two main seasons: the Spring and Fall (Autumn) moves. Each season is further divided into negotiation and movement phases, followed by retreat or disband adjustments and an end-of-the-year Winter phase of new builds or removals following the Fall adjustments. Negotiation Phase In the negotiation phase, players discuss tactics and strategy, form alliances, and share intelligence or spread disinformation. Negotiations may be made public or kept private. Players are not bound to anything they say or promise, and no agreements are enforceable. Communication and trust are highly important; players must forge alliances with others and observe their actions to evaluate their trustworthiness. At the same time, they must convince others of their own trustworthiness while making plans to turn against their allies when least expected. well-timed betrayal can be just as profitable as an enduring, reliable alliance. Movement Phase After the negotiation period, players write secret orders for each unit; these orders are revealed and executed simultaneously. unit can move from its location to an adjacent space, support an adjacent unit to hold an area in the event of an attack, support another unit to attack space into which it could move itself, or hold defensively. In addition, fleets may transport armies from one coast space to another when in chain called convoy. Only one unit may occupy each region. If multiple units are ordered to move to the same region, only the unit with the most support moves there. If two or more units have the same highest support, standoff occurs and no units ordered to that region move. unit ordered to give support that is attacked has those orders canceled and is forced to hold, except in the case that support is being given to unit invading the region from which the attack originated (in which case the unit that had been ordered to give support must retreat from, rather than hold, its position). Certain spaces on the board have two coasts and here player must specify which one they want their fleet to occupy. fleet can only move to coasts and oceans that border the coast that it is on. For example, fleet occupying the southern coast of Bulgaria cannot move into Romania or the Black Sea, but fleet on the east coast could. End-of-year After each Fall move, newly acquired supply centers become owned by the occupying player, and each powers supply center total is recalculated; players with fewer supply centers than units on the board must disband units, while players with more supply centers than units on the board are entitled to build units in their open (unoccupied) Home centers (supply centers controlled at the start of the game). Players who have lost all of their Home centers may not build new units, while players controlling no supply centers are eliminated from the game. If player controls 18 or more (being more than half) of the 34 supply centers at the end of year, they are the winner. Players who remain may also agree to draw around half of all games will end in draw. A.4.2. EVALUATION METRICS Basic Skill Evaluation Diplomacy is highly intricate strategy game, so to effectively assess the basic proficiency of LLMs in playing it, we designed this basic skill(BS) evaluation. In this experiment, single LLM is assigned the role of France, while the remaining six major powers - England, Germany, Italy, Austria, Russia, and Turkey are controlled by neutral agents. These neutral powers adopt passive stance, not doing movement or building, while only disbanding their units when necessary. We also disable the negotiation part in each movement phase, to check whether LLM has the ability to play Diplomacy. 17 SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? Multi-agent Experiment Settings To evaluate the performance of large language models (LLMs) in the Diplomacy game, we conducted multi-agent experiments with varying numbers of agents controlling the games powers. The Diplomacy version we used supports 1 to 7 players; however, our experiments focused on settings with 2 to 5 agents. In each setting, powers were distributed approximately equally among the agents, ensuring no neutral powers were present. For every configuration, two experiments were performed: one including the negotiation phase and another where the negotiation phase was skipped. Details of the agent assignment in Table 10 are shown as follows: Table 5: Agent Assignments for Multi-Agent Experiment Settings"
        },
        {
            "title": "Number of Agents",
            "content": "2 3 4 5 Agent GPT-4o Tested Agent GPT-4-turbo Tested Agent GPT-4o GPT-4-turbo Claude 3.5 Haiku GPT-4o Tested Agent GPT-4o Claude 3.5 Haiku o1-preview GPT-4-turbo Tested Agent Powers Controlled Austria, England, France, Germany Italy, Russia, Turkey Germany, Italy Russia, Turkey Austria, England, France Austria, England France, Germany Italy, Turkey Russia Austria, England Italy, Turkey France Russia Germany Table 5 details the distribution of powers among agents for each experimental setting. The Tested Agent varies across experiments and includes models such as GPT-4o, GPT-4-turbo and o1. This variability allows us to assess the performance and strategic capabilities of different LLMs within the same game environment. This structured distribution ensures balanced evaluation across different agent configurations, allowing us to comprehensively assess the strategic and negotiation capabilities of various LLMs. By varying the Tested Agent across different models, we can compare performance metrics and understand the strengths and limitations of each model. Evaluation Metrics model in each setting. In Table 2 and Table 10, we have two result-oriented metrics and four action-wise metrics for each The two result-oriented metrics are the number of supply centers and the number of controlled regions(influence locations) at the end of game for each player. These two metrics provide comprehensive assessment of the models strategic planning capability for Diplomacy. Action-wise Metrics The action-wise metrics are computed in complete game. 1. Move Orders: Move orders are the orders the agent issue to move its units to other locations. We have three metrics for the move orders issued by the agent: Successful move count, Total move count, and Successful move rate. This metric evaluates models ability to move its unit to other locations, demonstrating the models understanding of the basic knowledge to play Diplomacy. 2. Attack Orders: Attack orders are the orders the agent issue to move its units to other location occupied by other powers units. We have three metrics for the attack orders issued by the agent: Successful attack count, Total attack count, and Successful attack rate. This metric evaluates models ability to attack other powers and take more supply centers, showing the models aggression and high-level planning capability. 3. Support-Self Orders: Support-Self orders are the orders the agent issue to make its unit to support its own units attacking other powers unit. We have three metrics for the support-self orders issued by the agent: Successful 18 SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? support-self count, Total support-self count, and Successful support-self rate. This metric evaluates models multi-step reasoning ability and the planning-action consistency. It is the key to successful attack order. 4. Support-Others Orders: Support-Other orders are the orders the agent issue to make its unit to support others units attacking another powers unit. Supporting others means helping others, which is highly strategic action in such competitive game setting. We have three metrics for the support-others orders issued by the agent: Successful support-others count, Total support-others count, and Successful support-others rate. This metric evaluates models social intelligence, theory of mind, and its strategic reasoning on building breakable but profitable relationships with its opponents. Negotiation Metrics As shown in Table 2, we designed six LLM-assisted metrics for each model. The metrics are only applied when the negotiation is enabled. The definition of each metric is as follows: (1) Reasoning and Negotiation Alignment: During the negotiation, together with models messages, we prompted the model to generate Negotiation Strategy showing its strategy for each negotiation round, outlining its goals or intended approach. We then examine each outgoing message (e.g., RUSSIA GERMANY) to see if it aligns with that declared strategy. If so, we label it 1; otherwise 0. high ratio of 1s to total messages indicates that the agents communicated proposals match its own declared plan. For example, if the agent states will avoid any moves against Germany in its strategy, but later messages propose attacking Germany, we label that mismatch as 0. (2) Proposal Acceptance Rate: We define proposal as direct request to the recipient, for example: Lets demilitarize Silesia, Support me into Munich, or Agree to truce. proposal is accepted if the recipients subsequent message indicates compliance (e.g., the opponent confirms or does not contest it). We compute the ratio: Acceptance Rate ="
        },
        {
            "title": "Number of Accepted Proposals\nTotal Proposals",
            "content": "As an illustration, if RUSSIA says, Please hold in Munich; do not move south, and the opponents response shows the agreement and final order does indeed Hold in Munich, we mark that proposal as accepted. high proposal acceptance rate indicates the model has strong persuasive capability. (3) Mutual Benefit or Exploitative Proposal Nature: We label each proposal as mutual benefit: both sides clearly benefit from the proposal or one sided: mostly the proposer benefits. For example, Lets swap Munich and Berlin so we both gain positions is often mutual benefit, whereas Support me into Belgium and Ill maybe repay you might be more one-sided if theres no guarantee or advantage for the recipient. This metric demonstrates models strategic thinking and personality in the social interaction. (4) Peace or Conflict Messaging: For each negotiation round, we mark an agents messages to specific power as either: promote peace: urging DMZs or non-aggression, provoke conflict: pushing for an attack or hostility, neutral: no clear push for peace or conflict (e.g., purely factual statements). For instance, will help you attack Austria now is conflict-provoking message, whereas Lets sign pact of nonaggression is peace-promoting one. Then we compute the ratio of peace to conflict. (5) Perspective Taking or Empathy: Here we label whether the message demonstrates an explicit acknowledgment of the other partys viewpoint or needs. We mark 1 if the agent references or empathizes with the recipients perspective (e.g., understand youre concerned about Italy in the Mediterranean), or 0 if it does not. High values of perspective taking can show advanced theory of mind capabilities. In the metric, we compute the ratio of the perspective taking messages to the total number of messages. (6) Conditional Plans: We check each negotiation round for if-then or conditional statements, like If you move to Tyrolia, Ill support you into Vienna, or Ill only demilitarize if you do as well. We label each round as 1 if there are such conditional statements, otherwise 0. This can show more sophisticated strategic depth, and advanced negotiation social skills of LLMs. 19 SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? For the game configuration, we set up 6 models (o1, o1-preview, Claude 3.5 Haiku, GPT-4o, GPT-4-turbo and DeepSeek R1) playing 7 powers in total, and let them compete with each other for maximum of 20 years. After the game ends, we collect and clean all the negotiation messages throughout the game and use LLM-as-a-judge to generate the annotation for above metrics. For each negotiation phase in complete game trajectory (20 years), we provide the annotation prompt to o1 model, gathering all the results from its analysis, and compute the statistics. An example of one negotiation phase and o1 models annotation is shown in Appendix C.2. B. Supplementary Experiments B.1. Two-player Competitive Games Model Tic Tac Toe Connect Four Chess Human o1-preview o1-mini Claude 3.5 sonnet GPT-4o Claude 3.5 haiku Qwen2.5:72b Llama3.3 Llama3.1:70b GPT-4o-mini GPT-4-turbo Llama3:70b-instruct Mistral:7b Llama3.2:3b GPT-3.5-turbo 1415 1263 1205 1138 1025 983 944 944 936 912 857 856 855 836 829 1692 1377 1083 942 980 907 877 878 829 925 1007 993 800 821 890 - 1395 1247 1196 1255 1041 1150 1146 1164 842 1265 1155 243 421 480 Table 6: Internal Elo Ratings of Various LLMs (Initial Elo: 1000) Model Leaderboard Table 6 presents the internal Elo ratings of various LLMs across three competitive games: Tic Tac Toe, Connect Four, and Chess. It is important to note that these ratings are internal metrics and do not directly reflect performance among human players. Among evaluated models, o1-preview and o1-mini lead in this category, demonstrating strongest strategic planning capabilities over the remaining models, further indicating the advantages of test time scaling in strategic games. Additionally, Claude 3.5 Sonnet, GPT-4o, and GPT-4-turbo maintain relatively high Elo ratings, indicating superior ability to handle complex strategies compared to their counterparts. In contrast, models such as Mixtral, Llama3.2 perform significantly worse in Chess reflecting the increased demands that games with larger action and state spaces impose on models planning and reasoning capabilities. However, for Tic Tac Toe and Connect Four, human outperforms all LLMs by substantial margin. This result underscores the current limitations of LLMs in strategic reasoning, highlighting the significant gap between AI and human-level planning capabilities. Complete results of the top moves distribution Figure 7 presents the complete frequency distribution of top move selections for various models in Connect Four (top) and Chess (bottom). Through this figure, Claude 3.5 Sonnet, o1-preview and DeepSeek R1 are strong Chess players compared to other models, while still have low top-1 move accuracy compared to basic level Stockfish engine. Llama3:70b model even has half of its actions outside the top 10 moves in Chess. Illegal Move Analysis To evaluate the propensity of various models to generate illegal moves during gameplay, and its ability to follow game constraint, an analysis was conducted across three distinct games: Tic Tac Toe, Connect Four, and Chess. The evaluation employs two primary metrics: Illegal Move Lost Rate (IML) (%): The ratio of games lost due to illegal moves to the total number of rounds played. Illegal Moves per Total Turns (IMT) (%): The ratio of illegal moves to the total number of turns taken by the model. 20 SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? Figure 7: Frequency distribution of top move indices for different models in Connect Four (top) and Chess (bottom). The x-axis represents the rank of top moves selected by the models, while the y-axis shows the frequency. Results are averaged across all analyzed games against solver (Connect Four) and Stockfish level 0 (Chess). game is considered lost by model if it consecutively generates ten illegal moves within single turn. To ensure that the model is aware of what constitutes legal move, we provide the complete list of available legal moves at each of the three game turns. The following tables present the results of this analysis for each game: Table 7: Illegal Move Metrics Across Tic Tac Toe, Connect Four, and Chess Model Tic Tac Toe IMT (%) Connect Four Chess IML (%) IMT (%) IML (%) IMT (%) Mistral Llama3.2:3b GPT-3.5-turbo GPT-4o-mini Qwen2.5:72b Claude-3-5-haiku GPT-4o Llama3.1:70b Llama3:70b-instruct GPT-4-turbo Llama3.3 o1-mini Claude-3-5-sonnet o1-preview 0.385 0.471 0.081 0.128 0.049 0.223 0.011 0.091 0.406 0.088 0.002 0.020 0.002 0.023 0.008 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.200 0.078 0.039 0.110 0.018 0.114 0.003 0.041 0.264 0.024 0.001 0.000 0.000 0.000 0.962 0.904 0.857 0.389 0.069 0.053 0.053 0.015 0.000 0.000 0.000 0.000 0.000 0.000 0.632 0.503 0.371 0.172 0.265 0.017 0.027 0.081 0.045 0.024 0.016 0.001 0.001 0.000 Table 7 provides comprehensive overview of the illegal move metrics observed across the three games. The analysis reveals distinct patterns in the performance of various models with respect to their tendency to generate illegal moves, measured by three key metrics. Notably, the IML column is omitted for Tic Tac Toe, as all models exhibit 0% rate in this metric. Both Tic Tac Toe and Connect Four demonstrate relatively lower probabilities of IMT and IML, likely due to their simplicity and limited move options. Nearly no models lose the game due to 10 times of consecutive illegal moves. However, certain models display higher IMT values for Tic Tac Toe compared to Connect Four. We hypothesize 21 SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? that this discrepancy arises from the dimensionality of the action space: Tic Tac Toe employs two-dimensional action space, whereas Connect Four operates in one-dimensional space. This suggests that LLMs struggle more as the dimensionality of the action space increases. For Chess, which features two-dimensional action space coupled with significantly more complex game constraints, larger number of models tend to lose due to illegal moves, and IMT values are generally higher. This underscores the observation that as the action space expands and gameplay constraints become more intricate, models are increasingly prone to generating illegal moves. Furthermore, high IMT combined with low IML indicates that model is adept at self-correcting after an illegal move, thereby avoiding loss. Overall, Mixtral, GPT-3.5-turbo and Llama3.2:3b demonstrate weak constraint following abilities, while o1-preview and o1-mini behave perfectly in Connect Four and Chess, outperforming all the other models. The analysis underscores clear correlation between the complexity of the game and the prevalence of illegal moves among different models. Simpler games like Tic Tac Toe present minimal challenges, resulting in perfect compliance with game rules across all models in terms of losing games. However, the additional metric IMT reveals varying degrees of rule adherence during gameplay, highlighting nuances in model behavior beyond just game outcomes. As the complexity of the game increases, evidenced by Connect Four and Chess, models exhibit higher tendency to generate illegal moves, both in terms of losing games and during move generation. This suggests that more sophisticated strategic planning and rule comprehension are required to excel in complex games. Future research should focus on refining model training processes to better handle rule-based constraints and strategic complexities, particularly in highly intricate games like Chess. Incorporating feedback mechanisms that penalize illegal moves during training could further enhance models adherence to game rules. Additionally, exploring hybrid models that combine planning and execution phases may yield improvements in rule compliance and strategic depth. LLM Tic Tac Toe (%) Connect Four (%) Chess (%) o1-preview o1 GPT-4-turbo Claude 3.5 Sonnet o1-mini Claude 3.5 Haiku Llama3:70b GPT-4o-mini Llama3.1:70b o3-mini Qwen2.5:72b Llama3.2:3b GPT-3.5-turbo DeepSeek-R1 Llama3.3:70b GPT-4o 10.0 30.0 40.0 40.0 50.0 50.0 80.0 80.0 80.0 80.0 90.0 90.0 90.0 90.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 Table 8: The Complete Result of win rates of our solvers playing against LLMs in Tic Tac Toe, Connect Four, and Chess. Solvers win or draw all the time, without losing any single match. B.2. Diplomacy B.2.1. BASIC SKILL EVALUATION Table 9 demonstrates the results in basic skill Evaluation, corresponding to the BS column in Table 2. Surprisingly, only o1-preview, o1 and GPT-4o can finish the game in the minimal Diplomacy setting. Other models cant even complete the game with extremely loose constraints, with DeepSeek-R1 obtaining 16 supply centers, GPT-4-turbo obtaining 14 supply centers falling behind. Some strong models like o1-mini even cant attack other powers units to take more supply centers. Such evidence shows Diplomacy acts as strong strategic planning environment for current large language models. Thus, we choose o1, GPT-4o, and GPT-4-turbo as the independent variables in multi-agent experiments, and choose o1-preview, Claude 3.5 Haiku as other powers agent as described in Table 5. 22 SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? Agent Years to Win Supply Centers o1-preview human o1 GPT-4o DeepSeek-R1 GPT-4-turbo o3-mini Claude 3.5 haiku Claude 3.5 sonnet o1-mini GPT-4o-mini 7 8 10 16 - - - - - - - 20 20 18 19 16 14 13 11 9 8 6 Table 9: Performance of LLMs and human in the single-player setting of Diplomacy. - means that game was forcibly terminated after 20 years in the game, indicating that model can not win the game in that time. B.2.2. MULTI-AGENT EXPERIMENT Move 2 agents: playing with GPT-4o Attack Support-Self Support-Others SC CR S/T S/T S/T S/T o1 GPT-4o GPT-4-turbo 90/72 204/128 135/118 105/84 371/231 239/233 0.86/0.86 0.55/0.55 0.56/0.51 12/12 25/24 13/9 20/16 70/57 32/ 0.60/0.75 0.36/0.42 0.41/0.22 21/21 48/84 35/29 46/43 209/397 152/133 0.46/0.49 0.23/0.21 0.23/0.22 3 agents: playing with GPT-4-turbo and GPT-4o o1 GPT-4o GPT-4-turbo 51/43 137/137 57/65 69/51 249/246 165/117 0.74/0.84 0.55/0.56 0.35/0.56 13/5 10/18 5/10 23/5 57/63 28/24 0.57/1.00 0.18/0.29 0.18/0. 19/18 44/56 26/40 42/37 191/244 95/154 0.45/0.49 0.23/0.23 0.27/0.26 4 agents:X playing with GPT-4o, Claude 3.5 Haiku, GPT-4-turbo o1 GPT-4o GPT-4-turbo 112/64 108/102 53/ 161/75 195/165 113/77 0.70/0.85 0.55/0.62 0.47/0.66 28/10 25/24 4/9 64/12 75/65 28/25 0.44/0.83 0.33/0.37 0.14/0.36 40/18 36/57 23/ 88/39 120/205 102/87 0.45/0.46 0.30/0.28 0.23/0.30 5 agents:X playing with GPT-4o, Claude 3.5 Haiku, GPT-4-turbo, o1-preview o1 GPT-4o GPT-4-turbo 87/48 17/6 26/18 116/57 47/10 61/ 0.75/0.84 0.36/0.60 0.43/0.64 21/9 5/1 9/5 34/12 27/4 35/12 0.62/0.75 0.19/0.25 0.26/0.42 27/22 4/1 15/8 62/47 14/7 77/ 0.44/0.47 0.29/0.14 0.19/0.26 0/0 0/0 1/1 0/1 1/1 0/0 0/0 1/2 0/3 0/1 0/0 2/0 0/1 3/9 4/ 0/1 5/8 1/2 0/1 4/5 4/7 0/1 0/1 5/3 0.00/0.00 0.00/0.00 0.25/0.33 0.00/1.00 0.20/0.13 0.00/0.00 0.00/0.00 0.25/0.40 0.00/0. 27/15 22/18 16/15 20/10 15/18 6/13 17/10 15/17 7/8 0.00/1.00 0.00/0.00 0.40/0.00 15/4 1/6 0/1 54/31 50/36 32/ 37/21 30/31 13/21 37/18 28/29 14/19 36/8 3/8 3/3 Table 10: Performance indicators for three models (o1, GPT-4o, GPT-4-turbo) across various agent settings in Diplomacy (2/3/4/5 agents), presented as without negotiation / with negotiation (x/y). Each row shows: (Move Order) success number / total number / success rate, (Attack Order) success number / total number / success rate, (Support-Self Order) success number / total number / success rate, (Support-Others Order) success number / total number / success rate, then the number of supply centers(SC), and controlled regions(CR) at the end of game. 23 SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? C. Prompt Design C.1. Cooperative Game: Hanabi We provide an example of the prompt for LLM playing Hanabi as follows: (in 3 agents setting) Below is the current detailed state information. There are 3 players in the game. Game State: There are 3 life tokens and 8 info tokens remaining. The fireworks progress: stack is at 0, stack is at 0, stack is at 0, stack is at 0, stack is at 0. Your hand contains the following cards: Card 1: - Hidden info: XX. This represents what you cannot see about this card. It means you have no direct knowledge about the cards identity from your perspective. - Known info: XX. No hints about this cards color or rank have been given yet. - Possible identities: RYGWB12345. This list represents the set of all cards that could possibly be in this position, given the hints received and the remaining cards in the deck. Card 2: - Hidden info: XX. This represents what you cannot see about this card. It means you have no direct knowledge about the cards identity from your perspective. - Known info: XX. No hints about this cards color or rank have been given yet. - Possible identities: RYGWB12345. This list represents the set of all cards that could possibly be in this position, given the hints received and the remaining cards in the deck. Card 3: - Hidden info: XX. This represents what you cannot see about this card. It means you have no direct knowledge about the cards identity from your perspective. - Known info: XX. No hints about this cards color or rank have been given yet. - Possible identities: RYGWB12345. This list represents the set of all cards that could possibly be in this position, given the hints received and the remaining cards in the deck. Card 4: - Hidden info: XX. This represents what you cannot see about this card. It means you have no direct knowledge about the cards identity from your perspective. - Known info: XX. No hints about this cards color or rank have been given yet. - Possible identities: RYGWB12345. This list represents the set of all cards that could possibly be in this position, given the hints received and the remaining cards in the deck. Card 5: - Hidden info: XX. This represents what you cannot see about this card. It means you have no direct knowledge about the cards identity from your perspective. - Known info: XX. No hints about this cards color or rank have been given yet. - Possible identities: RYGWB12345. This list represents the set of all cards that could possibly be in this position, given the hints received and the remaining cards in the deck. From your perspective, you can see the other players hands clearly. Heres what you observe: Player +1s hand: - card: You can see the card: B3, This player has no specific hints about the card identity, This player knows the card could be one of the following: RYGWB12345. - card: You can see the card: W4, This player has no specific hints about the card identity, This player knows the card could be one of the following: RYGWB12345. - card: You can see the card: B1, This player has no specific hints about the card identity, This player knows the card could be one of the following: RYGWB12345. - card: You can see the card: Y5, This player has no specific hints about the card 24 SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? identity, This player knows the card could be one of the following: RYGWB12345. - card: You can see the card: R4, This player has no specific hints about the card identity, This player knows the card could be one of the following: RYGWB12345. Player +2s hand: - card: You can see the card: G2, This player has no specific hints about the card identity, This player knows the card could be one of the following: RYGWB12345. - card: You can see the card: R2, This player has no specific hints about the card identity, This player knows the card could be one of the following: RYGWB12345. - card: You can see the card: Y4, This player has no specific hints about the card identity, This player knows the card could be one of the following: RYGWB12345. - card: You can see the card: G5, This player has no specific hints about the card identity, This player knows the card could be one of the following: RYGWB12345. - card: You can see the card: Y3, This player has no specific hints about the card identity, This player knows the card could be one of the following: RYGWB12345. There are 35 cards remaining in the deck. The discard pile is currently empty. Please think step by step based on the current state # Think step by step ## Evaluate Playable Cards in Hand Look at each card in your hand. Cross-reference with the current game state to see if any card can be immediately played to complete or extend firework stack. Consider hints you have received about each card (color/rank information) to determine if it might be safe to play. If card can be played without risk, prioritize playing it to score point. ## Consider Teammates Hands and Hint Opportunities Analyze the visible cards in your teammates hands. Identify if any of their cards can now be played based on the current firework stacks or previous hints. If you notice teammate holds card that can be played but they may not realize it, think about what hints you could give them. Use hints to communicate critical information, such as color or rank, to help them make the right play. Choose the hint that maximizes the chance for correct play while considering the limited hint tokens. ## Assess Discard Options to Gain Info Tokens Look for cards in your hand that are least likely to be playable or helpful in the near future. Consider the remaining deck composition and cards already played/discarded to predict the value of each card. Discard card that you believe to be least useful to gain an Info token, especially if no immediate playable or hint options are available. Ensure that discarding this card wont permanently remove critical card needed to complete any firework stack. Now its your turn. You can choose from the following legal actions: The legal actions are provided in mapping of action identifiers to their descriptions: {5: (Play 0), 6: (Play 1), 7: (Play 2), 8: (Play 3), 9: (Play 4), 10: (Reveal player +1 color R), 11: (Reveal player +1 color Y), 13: (Reveal player +1 color W), 14: (Reveal player +1 color B), 15: (Reveal player +2 color R), 16: ( Reveal player +2 color Y), 17: (Reveal player +2 color G), 20: (Reveal player +1 rank 1), 22: (Reveal player +1 rank 3), 23: (Reveal player +1 rank 4), 24: ( Reveal player +1 rank 5), 26: (Reveal player +2 rank 2), 27: (Reveal player +2 rank 3), 28: (Reveal player +2 rank 4), 29: (Reveal player +2 rank 5)} 25 SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? (Reveal player +N color C): Give hint about color to the player who is positions ahead of you. (Reveal player +N rank R): Give hint about rank to the player who is positions ahead. (Play X): Play the card in position from your hand. (Discard X): Discard the card in position from your hand. Based on the annotated state and the list of legal actions, decide on the most appropriate move to make. Consider factors like current tokens, firework progress, and information available in hands. Then, output one of the legal action descriptions as your chosen action. Your output should be in this format: {\"reason\": string, \"action\": int} And the action should be one of the legal actions provided above. You can only use json valid characters. When you write json, all the elements (including all the keys and values) should be enclosed in double quotes!!! To win, you need to play the cards in the correct sequence and maximize the total score of the fireworks. Good luck! To evaluate the performance of LLM agents in playing Hanabi in multi-player setting, we designed comprehensive prompt that guides the model through the games decision-making process. The prompt begins by providing detailed description of the current game state, including life tokens, information tokens, fireworks progress, current players cards(with hidden information, known information, and possible identities), and the visible hands of other players. The natural language prompt ensures that the LLM has all necessary data to make decisions. Then, the prompt employs step-by-step reasoning framework to emulate the strategy thought process required in Hanabi: evaluating playable cards, considering teammates hands and hint opportunities, and assessing discard options to gain more information tokens. We explicitly encourages the LLM agent to engage in logical and strategic thinking, mirroring the deliberative processes human players use during gameplay. At last, we provide all the possible legal actions available to the player, normally including playing, revealing colors or ranks, and discarding specific cards. The required output format for the model is JSON object containing reason for the chosen action and the corresponding action index. An example of models response: { } \"reason\": \"Card 5 in your hand is known to be B2, which is exactly the next playable card for the blue stack (currently at B1). Playing it will advance the blue firework without risk.\", \"action\": \"9\" C.2. Strategic Game: Diplomacy Prompt Design: State Description The following is the state description prompt, which includes all the public information from the Diplomacy board that players need to be aware of. The current state of the game is as follows: Note: No notes are provided for this state. Phase Name: The current game phase is W1901A. Units: Stores the location of the units currently controlled by each player on the map: AUSTRIA: VIE, TRI, GAL ENGLAND: ENG, WAL, BEL FRANCE: SPA, PIC, POR GERMANY: BER, DEN, TYR ITALY: VEN, ION, PIE RUSSIA: UKR, RUM, FIN, SWE SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? TURKEY: BLA, CON, GRE Retreats: If unit is defeated but not destroyed and it needs to retreat to neighboring empty province. The units that need to retreat are as follows: AUSTRIA: No retreats needed. ENGLAND: No retreats needed. FRANCE: No retreats needed. GERMANY: No retreats needed. ITALY: No retreats needed. RUSSIA: No retreats needed. TURKEY: No retreats needed. Supply Centers: The supply centers controlled by each player are: AUSTRIA: BUD, TRI, VIE ENGLAND: EDI, LON, LVP, BEL FRANCE: BRE, MAR, PAR, POR, SPA GERMANY: BER, KIE, MUN, DEN ITALY: NAP, ROM, VEN RUSSIA: MOS, SEV, STP, WAR, RUM, SWE TURKEY: ANK, CON, SMY, GRE Home Centers: Each players initial or home supply centers are: AUSTRIA: BUD, TRI, VIE ENGLAND: EDI, LON, LVP FRANCE: BRE, MAR, PAR GERMANY: BER, KIE, MUN ITALY: NAP, ROM, VEN RUSSIA: MOS, SEV, STP, WAR TURKEY: ANK, CON, SMY Influence: The regions influenced or controlled by each player are: AUSTRIA: BUD, VIE, TRI, GAL ENGLAND: EDI, LON, LVP, NTH, ENG, WAL, BEL FRANCE: BRE, MAR, PAR, MAO, SPA, PIC, POR GERMANY: KIE, BER, MUN, DEN, TYR ITALY: NAP, ROM, VEN, ION, TUS, PIE RUSSIA: WAR, MOS, SEV, UKR, STP, RUM, BOT, FIN, SWE TURKEY: ANK, SMY, BUL, BLA, CON, GRE Civil Disorder: Status of players under civil disorder (1 for yes, 0 for no): AUSTRIA: No civil disorder ENGLAND: No civil disorder FRANCE: No civil disorder GERMANY: No civil disorder ITALY: No civil disorder RUSSIA: No civil disorder TURKEY: No civil disorder Builds: Each players allowable builds or disbands are: AUSTRIA: 0 builds allowed. Homes available for builds: No specific build locations ENGLAND: 1 builds allowed. Homes available for builds: EDI, LON, LVP FRANCE: 2 builds allowed. Homes available for builds: BRE, MAR, PAR GERMANY: 1 builds allowed. Homes available for builds: KIE, MUN ITALY: 0 builds allowed. Homes available for builds: No specific build locations RUSSIA: 2 builds allowed. Homes available for builds: MOS, SEV, STP, WAR TURKEY: 1 builds allowed. Homes available for builds: ANK, SMY There are no neutral powers in this game. There is several important information in the state description: current game phase, every players units, retreat information, supply centers, home centers, influence, builds, and neutral power information. All the information is critical for agents to make decisions in each phase. Prompt Design: Negotiation Phase In the negotiation phase, the agent is provided with the current game state, and the following negotiation prompt: It is now round 2 of the negotiation phase. There are 3 total negotiation rounds. After these, you must decide your actions. 27 SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? You are playing as [RUSSIA, TURKEY]. The other powers are: AUSTRIA, ENGLAND, FRANCE, GERMANY, ITALY. The last rounds messages are: FRANCE to all players: Attention all powers, suggest mutual ceasefire this turn to stabilize our regions and focus on strategic planning. GERMANY to RUSSIA: Discuss possible strategies against Austria; would appreciate insight on your intentions towards the Balkans. ITALY to TURKEY: Exploring potential cooperation in the Balkans. What are your views on Austria this year? Now you must analyze the negotiation phase step by step, then provide your final messages in JSON object. ## Think step by step 1. **Recap & Trust Analysis:** 1.1. Recap each message from the last round, identifying who said what. 1.2. Assess their intentions and whether they might be truthful or deceptive. 1.3. Discuss how much you trust each powers statements based on their track record or alignment with your interests. 2. **Current State and Strategic Analysis:** 2.1. Summarize your current strategic position (units, supply centers, alliances, conflicts). 2.2. Summarize your opponents positions (who seems strong, who seems weak, who might be desperate). 2.3. Identify any immediate threats or opportunities for alliances, betrayals, or beneficial deals. 3. **Goal Setting:** 3.1. Reiterate your ultimate objective (gain more supply centers, dominate the map). 3.2. Decide how to approach each power: ally, remain neutral, or plan to attack soon. 4. **Negotiation Strategy:** 4.1. Determine which powers you want to communicate with this round and why. 4.2. Decide whether to propose alliances, coordinate attacks, request/demand support, or spread disinformation. 4.3. Consider carefully any promises you might make (remember they are not binding). 5. **Message Drafting:** 5.1. Outline the content of each message to each recipient. 5.2. Make sure your messages are concrete: specify regions, proposed moves, or conditions for cooperation. 5.3. Keep in mind the possibility of someone sharing your messages (lack of enforceability). 6. **Review & Finalize:** 6.1. Verify if your negotiation plan is consistent with your overall strategy. 6.2. Finalize the messages you will send out. After you finish your step-by-step reasoning, provide the result as JSON object with the following format: { \"phase\": \"negotiation_phase1\", \"trust_analysis\": [ { \"power\": \"ENGLAND\", \"trust_level\": \"low/medium/high\", \"analysis\": \"explanation of why\" }, ... ], 28 SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? \"negotiation_strategy\": \"In-depth explanation of how youre approaching each power ( alliance, deception, etc.)\", \"messages\": { \"FRANCE\": { \"recipients\": [\"GERMANY\", \"GLOBAL\"], \"messages\": [ \"Hello Germany, Id like your support in Burgundy.\", \"Greetings everyone, propose mutual ceasefire this turn.\" ] }, \"TURKEY\": { \"recipients\": [\"RUSSIA\"], \"messages\": [ \"I propose we coordinate against Austria in the Black Sea.\" ] }, ... } } ### Explanation of Fields: - **phase:** Always set this to \"negotiation_phase1\" for this round of negotiation phase . - **trust_analysis:** - list of dictionaries analyzing how much you trust each other power. - For each entry, note the power, your trust level, and brief explanation of why. - **negotiation_strategy:** - Describe your overarching plan for dealing with the other powers, including proposals, alliances, or sabotage. - **messages**: dictionary where each key is one of the powers that you control. You dont need to send message to neutral countries or yourself. - For each key (e.g. \"FRANCE\"), the value is another dictionary: - \"recipients\": - list of strings representing who you are sending messages to. - Valid recipients: AUSTRIA, ENGLAND, FRANCE, GERMANY, ITALY - You can also include \"GLOBAL\" to broadcast message to all powers simultaneously. - \"messages\": list of strings, each corresponding to message directed to the recipients in the same order. - If you list two recipients, the first message is intended for the first recipient, the second message for the second recipient, and so on. Use only standard double quotes in your JSON. Do not include Python-style triple-quoted strings or comments inside the JSON output. There are no neutral powers in this game. Happy negotiating - analyze carefully and craft your messages with purpose! To facilitate LLMs in handling the negotiation phase of Diplomacy, we meticulously designed negotiation prompt that guides the model through structured reasoning process. This design ensures that the LLM systematically analyzes the game state, assesses the intentions of other powers, and formulates strategic messages. The negotiation prompt is divided into six key stages: Recap & Trust Analysis, Current State and Strategic Analysis, Goal Setting, Negotiation Strategy, Message Drafting, and Review & Finalize. This framework ensures that the LLM thoroughly analyzes previous communications, assesses the trustworthiness of other powers, evaluates the current strategic landscape, sets clear objectives, formulates coherent negotiation strategy, drafts precise messages, and verifies the alignment of its actions with overall goals. The final output from the LLM is structured in standardized JSON format. It includes all the messages intended for transmission, specifies the corresponding agent power to send each message, and incorporates the meta information outlined in the prompt. The model is also asked to generate its negotiation strategy in its output. That strategy is for evaluating the 29 SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? negotiation consistency of the model. An example of models response: { \"phase\": \"negotiation_phase1\", \"trust_analysis\": [ { }, { }, { }, { }, { } \"power\": \"AUSTRIA\", \"trust_level\": \"low\", \"analysis\": \"They border our regions in the Balkans and could quickly become hostile if we weaken our defenses, so we remain cautious.\" \"power\": \"ENGLAND\", \"trust_level\": \"medium\", \"analysis\": \"They are likely preoccupied with Western Europe and Scandinavia, but could pose threat if they expand eastward.\" \"power\": \"FRANCE\", \"trust_level\": \"medium\", \"analysis\": \"They propose ceasefire, which may indicate focus on their western or southern fronts. We suspect they are genuine for now but remain watchful.\" \"power\": \"GERMANY\", \"trust_level\": \"medium\", \"analysis\": \"They express interest in peaceful arrangement with Russia, which might be sincere, though we must be alert if their western front stabilizes.\" \"power\": \"ITALY\", \"trust_level\": \"low\", \"analysis\": \"Italian fleets in the Mediterranean can quickly disrupt Turkeys plans; unclear whether they want conflict or just safe expansion.\" ], \"negotiation_strategy\": \"We wish to maintain and strengthen the Russia-Turkey axis. Russia may accept Germanys overture for peace in the north if it frees us to focus south. Turkey will keep an eye on Austria while seeking to avoid direct clash with Italy for now. We are open to Frances ceasefire proposal but will remain vigilant in case they shift focus to the east.\", \"messages\": { \"RUSSIA\": { \"recipients\": [\"GERMANY\", \"FRANCE\"], \"messages\": [ \"Greetings Germany. welcome discussions of peaceful arrangement. Lets consider demilitarized zone around the Baltic to ensure mutual nonaggression, while we each focus on other potential threats.\", \"France, thank you for offering ceasefire. have no immediate conflict with you, so Im open to refraining from aggressive moves toward the west as long as we remain mutually respectful of each others territories.\" ] }, \"TURKEY\": { \"recipients\": [\"RUSSIA\", \"ITALY\"], \"messages\": [ \"Russia, mentioned targeting Germany, but we should be flexible. If Germany grants you security in the north, focusing on Austria might be more profitable for both of us. Lets coordinate to see which path yields the greatest gain.\", \"Hello Italy. Id like to keep the Eastern Med stable. If you refrain from moving fleets aggressively into the Aegean or Eastern waters, Ill avoid sending forces westward. Lets keep lines of communication open.\" 30 SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? ] } } } Prompt Design: Movement Phase In the movement phase, the agent is provided with the current game state, and the following movement prompt: Now its your turn to issue orders, and you are playing as [RUSSIA, TURKEY]. The current state of the game is as follows: [GAME STATE] The adjacent regions of your orderable regions are as follows: <adjacent_regions> [ADJACENT INFORMATION] </adjacent_regions> First please think step by step given my instructions, do some self verification and revise on your orders: ## Think step by step 0. You should recap all the information in the previous negotiation phase, remember the agreements, promises, and threats made by each power. Analyze the outcomes of the negotiation phase and how they influence your next actions on the board. 1. Please analyze the current state of the game and your position. Which regions are you controlling? How many supply centers do you have? Where are your units? Please make detailed state analysis about you and other powers. 2. The goal of the game is to take control as many supply centers as possible. Analyze your current state, and plan how to take more supply centers later. For example, among the regions which are not your location, which location do you want to attack? You should think if you will have enough support to win each attack. You should also check all your adjcent regions and make sure you can move to them, based on < adjacent_regions> information. 3. For each of your units, analyze the possible orders. will give you all the possible orders enclosed in <possible_orders> for each of your unit. Please consider and tell me your reasoning for each the following points: 3.1. Whether you should move into other regions to expand or take more supply centers? 3.2. Whether you should move into other powers unit to attack? 3.3. If you dont have enough unit to attack some region, do you want to move your other unit to the adjacent region to prepare for the attack in the next round? 3.4. When you choose your attack target, dont set it as your controlled region. Iterate over your planned attack targets, and tell me whether that target is among your controlled region. If you find target as your attack target, please make sure that target is other powers location, not your own location. Moving into your unit is not considered an attack! 3.5. unit may not move into province held by another unit unless it has support, and the attacking unit must have more support than the defending unit if the attack is to be successful. If you want to attack other powers unit, do you have other units support this move? 3.6. unit can only move to its adjacent region. Please iterate over your units adjacent regions, and make sure that your move target is among its adjacent regions. 3.7. The support unit can only support the adjacent region, and the attack target should also be its adjacent region. Please iterate over your units adjacent regions, and make sure that your support target is among its adjacent regions. If the unit cant support attacking the target because its too far, you should choose another supporter or move it closer and plan to attack later. 3.8. If you want to support unit attacking some region, you should make sure is actually attacking the target region. If not, it is invalid. 4. Make the order decision based on your analysis. Check your decision about whether you set wrong attack target, move into your controlled region is not an attack!!! 31 SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? 5. For each intended order, please revise and verify your move target and the reason against the definitions, game rules, and your goal. For example, 5.1. Moving your unit into your own region is not considered an attack. You should move into other powers unit location. 5.2. You should take control more and more supply centers 5.3. Ensure that your attacks are supported sufficiently to overcome any defenses. 5.4. Iterate over your planned attack targets, and tell me whether that target is among your controlled region. If so, you should start from the beginning and think again. 5.5. IIterate over your planned attack targets, and compute how many other powers units are in that target location. Next, determine how many of your own units and how much support you need to succeed in the attack. You cannot attack with fewer supports than the defending unit. Verify that the total of your supports plus one is greater than the defending units strength. If this condition is not met, return to the beginning and think again. If your orders cant pass the verification, please start from the beginning and think again by the above steps. Once you are confident, please finalize your plan, and give me JSON object in the following format: { \"phase\": \"current_phase\", \"step_by_step_reasoning\": \"Your step by step reasoning here\", \"reason\": \"Your strategic reasoning here\", \"my_location\": [\"PAR\", ...], \"my_unit\": [\"BER\", ...], \"adjacent\": [{\"PAR\": [\"BUR\", \"GAS\"]}, ...], \"other_power_location\": [\"MUN\", ...], \"move_to_our_region_mask\": [0, 1, ...], \"attackable\": [\"MUN\", ...], \"attack_analysis\": [{\"MUN\": 2}, ...], \"support_given\": [{\"supporter\": \"BUR\", \"supported\": \"PAR\", \"target\": \"PIC\"}, ...], \"attack_mask\": { \"FRANCE\": [1, 0, ...], ... } \"orders\": { \"FRANCE\": [\"F BRE - MID\", \"A PAR - BUR\", ...], ... } } ### Instructions for the JSON object: 1. **Reason:** - Provide your strategic reasoning summary in the \"reason\" field. 2. **Additional Fields:** - **step_by_step_reasoning:** - Provide your above step by step reasoning details in the \"step_by_step_reasoning\" field. - **my_location:** - list of strings. - Each element corresponds to location where you have the influence or control ( Please refer to the game state). - Example: [\"MUN\", ...] - **my_unit:** - list of strings. - Each element corresponds to location where you have unit. - Example: [\"BER\", ...] 32 SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? - **adjacent:** - list of dictionary. - Each element is dictionary with your location as the key and list of all the adjacent locations as the value. Please refer to <adjacent_regions> - Example: [{\"MUN\": [\"TYR\", \"BOH\"]}, ...] - **other_power_location:** - list of strings. - Each element corresponds to location that is other powers location. - Example: [\"MUN\", ...] - **move_to_our_region_mask:** - list of integers (0 or 1). - Each element corresponds to an order in the \"orders\" list. - 1 indicates that the order involves moving to your influenced or controlled region. (Please refer to the game state) - 0 indicates that the order does not involve moving to your influenced or controlled region. - Example: [0, 1, 0] - **attackable:** - list of locations. - Each location is region that is other powers location and you can move into to attack in this turn. - You should NOTICE, you usually made mistakes here! The locations in my_location and my_unit should not be included in this list. You can only choose from adjacent locations. - Example: [\"MUN\", ...] - **attack_analysis:** - list of dictionaries. - Each dictionary contains key: the location string you want to attack in this round . value: the number of units anyone needs to win the attack. unit may not move into province held by another unit unless it has support. As units may be supported either in attacking province or in holding province, the attacking unit must have more support than the defending unit if the attack is to be successful. If the attack is not successful, the attacking unit does not move anywhere. already gave you other powers unit locations above. Only an estimate is needed here. Because you dont know whether other powers will support or not. - Example: [{\"MUN\": 2}, {\"RUH\": 1} ...] - **support_given:** - list of dictionaries. - Each dictionary contains the supported units location and the location of the unit being supported. And the target location. (based on the order you want to issue) - Example: [{\"supporter\": \"BOH\", \"supported\": \"MUN\", \"target\": \"TYR\"}, ...] - **attack_mask:** - dictionary of lists of integers (0 or 1). The same length as the \"orders\" list. - Each element corresponds to an order in the \"orders\" list. If the target is in my_location, it is not considered an attack. Attack is moving into other power unit location. - 1 indicates that the order is an attack. - 0 indicates that the order is not an attack. - Example: {\"FRANCE\": [1, 0, ...], ...} 3. **Orders:** - The \"orders\" field should be dictionary where the key is the power name and the value is list of strings chosen from the possible orders. - For each location, you can only issue one order. - The number of orders should match the number of locations you can issue orders for . 33 SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? - Each string represents an order for unit. - If possible_orders is empty for power, just leave the \"orders\" field empty for that power. 4. **JSON Formatting Guidelines:** - Use standard straight double quotes (\"). - Do not include special characters like , , etc. - Do not add comments inside the JSON output. ### **Possible Orders:** All the possible orders that you can issue are as follows (key is the location, value is the list of possible orders at that location): <possible_orders> [POSSIBLE ORDERS] </possible_orders> ### **Your Objective:** Your ultimate goal is absolute domination, seize every supply center and crush your opposition to win! Build and expand relentlessly, taking control of regions by launching bold, decisive attacks, control as many supply center as you can. Strike hard, strike fast, and let nothing stand in your way. Victory belongs to the bold, go conquer it! Now please think step by step in your response, and provide the JSON object with your strategic reasoning and orders. Field Explanation: 1. Adjacent Information: While the agent can inherently determine which locations are proximate to its own units, we provide all adjacent locations for ease of access. Adjacent information is crucial as it enables the agent to decide potential movement destinations or identify units to support during maneuvers. Combined with the opponents unit information from the game state, the agent can anticipate possible collaborations or conflicts. This data is structured as list, where each element is dictionary formatted as {MOS: [LVN, SEV, STP, UKR, WAR]}. Here, the key represents the location, and the values are the adjacent locations to that key. 2. Possible Orders: Diplomacy features an extensive action space with numerous legal orders available in each phase. To reduce the complexity for the agent in adhering to constraints, we supply all legal orders for each of the agents units. These orders are presented as list, with each element being dictionary in the following format: {TURKEY: [{SMY: [F SMY - SYR, SMY CON, ...]}, ...]}. In this structure, the key denotes the power being controlled by that agent, and the value is list of orderable units along with their corresponding possible orders. Component Explanation: The prompt comprises three primary components: the current state description, the chain of thought prompt, and the factual knowledge checker. 1. State Description: Detailed information is provided in the sections above. 2. Chain of Thought Prompt: Diplomacy is complex strategic game that requires multi-step reasoning and strategic planning. Through our experiments, we determined the necessity of crafting explicit and detailed prompts to facilitate the LLMs thought process. Consequently, we meticulously designed chain of thought templates tailored for both the negotiation and movement phases of the game. 3. Factual Knowledge Checker: To evaluate the agents factual understanding, we prompt the model to generate factual information based on the provided state description. This includes details such as controlled units, influence locations, adjacent locations, other powers positions, attackable locations, attack analyses, and support statistics. Each field is thoroughly explained within the prompt. Although the state description already includes controlled units, locations, and adjacent locations, the factual knowledge checker serves to verify the agents accurate comprehension of the game state. SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? The final output from the LLM is structured in standardized JSON format, which includes fields described in the factual knowledge checker and the final issued orders. We conducted an ablation study to assess the necessity of the Chain of Thought (CoT) prompt and the Factual Knowledge Checker (FC) in our Diplomacy game setup. Using the GPT-4o model playing as France, with the other powers set as neutral (negotiation disabled), we established baseline where both CoT and FC were included in the prompt during the movement phase. Two comparison experiments were performed: (1) removing CoT, while keeping the FC in the prompt (2) removing both CoT and FC. The game was run for maximum of 20 years with token threshold of 10,000,000. The winning condition was controlling more than 18 supply centers. After the simulations, we compared the number of supply centers acquired by the agent: Baseline (CoT + FC): GPT-4o won in 16 years by taking over 19 supply centers. Without CoT (only FC): The agent did not win, securing 17 supply centers in 20 years. Without CoT and FC: The agent also failed to win, capturing only 15 supply centers in 20 years. These results demonstrate that both the Chain of Thought and the Factual Knowledge Checker are essential for the models effective strategic decision-making and overall performance in player the game Diplomacy. An example of the agents response: { \"phase\": \"S1903M\", \"step_by_step_reasoning\": \"1) Considering the Juggernaut alliance, Russia and Turkey aim to press Austria. 2) Russia plans to seize key Austrian territories around Galicia and Budapest. 3) Turkey advances into the Balkans (Serbia, Bulgaria) to expand and pressure Austria. 4) In the north, Russia holds or shifts position minimally to respect the English DMZ. 5) This coordinated push preserves the Russo -Turkish pact.\", \"reason\": \"We focus on Austria as our primary target, using concentrated force around Galicia and Budapest (Russia) and occupying Serbia/Bulgaria (Turkey). Our northern units hold or reposition without provoking England or Germany, continuing the agreed DMZ.\", \"my_location\": [ \"WAR\", \"MOS\", ... ], \"my_unit\": [ \"MOS\", ... ], \"adjacent\": [ { \"MOS\": [\"LVN\", \"SEV\", \"STP\", \"UKR\", \"WAR\"] }, ... ], \"other_power_location\": [ \"VIE\", \"TRI\", ... ], \"move_to_our_region_mask\": { \"RUSSIA\": [0, 0, 0, 0, 0, 0, 0], \"TURKEY\": [0, 1, 0, 0] }, \"attackable\": [\"BUD\", \"GAL\"], \"attack_analysis\": [ { }, { \"BUD\": 1 \"GAL\": 2 SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? } ], \"support_given\": [ { } \"supporter\": \"UKR\", \"supported\": \"WAR\", \"target\": \"GAL\" ], \"attack_mask\": { \"RUSSIA\": [0, 0, 1, 0, 1, 0, 0], \"TURKEY\": [0, 0, 0, 0] }, \"orders\": { \"RUSSIA\": [ \"A MOS H\", \"A NWY - FIN\", \"F RUM - BUD\", \"F SWE H\", \"A WAR - GAL\", \"F STP/NC H\", \"A UKR WAR - GAL\" ], \"TURKEY\": [ \"F BLA H\", \"A CON - BUL\", \"A GRE - SER\", \"F SMY - AEG\" ] } } Prompt Design: Retreat Phase and Winter Adjustment In the retreat phase and the winter adjustment phase, the agent is provided with the current game state, and the following prompt: Retreat Phase: Now its the retreat phase, and you are playing [AUSTRIA, ENGLAND, FRANCE]. It is now your turn to play. The current state of the game is as follows: [GAME STATE] This is the retreat phase. You must retreat your units that have been dislodged. If you do not retreat unit, it will be disbanded. Please respond with JSON object like: { \"reason\": \"Your strategic reasoning here\", \"orders\": { \"FRANCE\": [\"A MAR SPA\", ...], ... } } Where \"orders\" is dictionary where the key is the power name and the value is list of each locations decision. For each location, you can only issue one order. The length of the list of each power should match the number of locations for which you can issue orders. The string should be able to be parsed to json str. So dont use special characters like n, t, etc. Generate the text with standard straight double quotes. Do not add SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? comments inside the output! The possible orders you can issue are as follows: {AUSTRIA: {TRI: [A TRI ALB, TRI D]}, ENGLAND: {}, FRANCE: {}} An example of the agents response: { \"reason\": \"The Austrian army in Trieste has been dislodged and must retreat. The only available retreat option is to move the army to Albania, which keeps the unit on the board and in position to continue influencing nearby regions.\", \"orders\": { \"AUSTRIA\": [\"A TRI ALB\"], \"ENGLAND\": [], \"FRANCE\": [] } } Winter Adjustment: Now its winter adjustment phase, and you are playing [RUSSIA, TURKEY]. It is now your turn to play. The current state of the game is as follows: [GAME STATE] This is the build/disband phase. You can build new units in your home centers, or disband units if you have too many. Remember, you need to control enough supply centers to win. Please respond with JSON object like: { \"reason\": \"Your strategic reasoning here\", \"orders\": { \"FRANCE\": [\"F BRE B\", \"A PAR B\", ...], ... } } Where \"orders\" is dictionary where the key is the power name and the value is list of each locations decision. For each location, you can only issue one order. The length of the list of each power should match the number of locations for which you can issue orders. The string should be able to be parsed to json str. So dont use special characters like n, t, etc. Generate the text with standard straight double quotes. Do not add comments inside the output! The possible orders you can issue are as follows: {RUSSIA: {MOS: [WAIVE, MOS B], SEV: [WAIVE, SEV B, SEV B]}, TURKEY: {}} Remember, you need to reach the configured number of supply centers (build more and more supply centers) to win. An example of the agents response: { \"reason\": \"Russia has one free build and needs to strengthen the land front against potential threats in the west or south. Building an army in Moscow offers greater strategic flexibility than fleet in Sevastopol. Turkey has no builds this phase.\", \"orders\": { \"RUSSIA\": [\"A MOS B\"], \"TURKEY\": [] } } SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? Prompt Design: LLM-assisted Negotiation Metrics Input Negotiation and Order Data: \"neg_message\": { \"negotiation_round0\": [ { \"phase\": \"negotiation_phase0\", \"negotiation_strategy\": \"Italy aims to seek cooperation with France or at least ensure neutrality, while Turkey will explore potential coordination with Russia against Austria in the Black Sea region.\", \"messages\": { \"ITALY\": { \"recipients\": [ \"FRANCE\" ], \"messages\": [ \"Hello France, propose we focus on peaceful expansion and cooperation in the Mediterranean.\" ] }, \"TURKEY\": { \"recipients\": [ \"RUSSIA\" ], \"messages\": [ \"Greetings Russia, lets discuss potential cooperation in the Black Sea to limit Austrian movements.\" ] } } }, { \"phase\": \"negotiation_phase0\", \"negotiation_strategy\": \"Establish strong FRANCE-GERMANY alliance, coordinate initial moves to expand territory, create buffer against other powers, and prevent early conflicts between us.\", \"messages\": { \"FRANCE\": { \"recipients\": [ \"GERMANY\", \"GLOBAL\" ], \"messages\": [ \"Germany, lets coordinate our initial moves. propose supporting each others expansion while avoiding direct conflict between us.\", \"To all powers: We seek peaceful initial expansion and are open to reasonable diplomatic arrangements.\" ] }, \"GERMANY\": { \"recipients\": [ \"FRANCE\" ], \"messages\": [ \"Excellent proposal. suggest we coordinate moves around Burgundy and potential expansion routes. Mutual support will be key to our success .\" ] } } }, { \"phase\": \"negotiation_phase0\", \"negotiation_strategy\": \"We aim to neutralize threats in the north (Germany, England) and southeast (Turkey) through careful negotiation. Propose DMZ or bounce with both Austria and Turkey to avoid early conflicts. Maintain 38 SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? neutrality with Germany while clarifying intentions in the Baltic. Possibly cultivate Italy and France as distant allies against more immediate threats .\", \"messages\": { \"RUSSIA\": { \"recipients\": [ \"AUSTRIA\", \"TURKEY\", \"GERMANY\" ], \"messages\": [ \"Greetings Austria, Id like to propose either DMZ or planned bounce in Galicia to avoid mistrust. Are you open to maintaining peace on our border this spring?\", \"Hello Turkey, Id like to suggest DMZ in the Black Sea so we can focus elsewhere. Lets not move fleets there and build some trust early on .\", \"Hello Germany, would like to keep the northern border calm. Perhaps we can discuss plan around Sweden to avoid direct confrontation. Let me know your thoughts.\" ] } } }, { \"phase\": \"negotiation_phase0\", \"negotiation_strategy\": \"Austria will focus on stabilizing its borders by securing alliances with Italy and Russia, potentially opening southern front against Turkey. England will focus on securing the North Sea and coordinating with Germany for movement against France, while diplomatically engaging Russia for actions in Scandinavia.\", \"messages\": { \"AUSTRIA\": { \"recipients\": [ \"ITALY\", \"RUSSIA\" ], \"messages\": [ \"Proposition for solid alliance to secure our borders and coordinate against mutual threats in the Balkans and Turkey.\", \"Interest in maintaining peace and cooperation in the Balkans, proposing joint move against Turkish territories.\" ] }, \"ENGLAND\": { \"recipients\": [ \"GERMANY\", \"RUSSIA\" ], \"messages\": [ \"Suggesting tactical partnership to mutually secure the North Sea and manage the French threat.\", \"Proposal for collaboration in Scandinavia. Offers support for Russian expansion provided English interests are respected.\" ] } } } ], \"negotiation_round1\": [ { \"phase\": \"negotiation_phase1\", \"negotiation_strategy\": \"Italy considers Austrias alliance proposal, emphasizing cooperation against mutual threats. Turkey aims to secure the DMZ agreement with Russia to stabilize the Black Sea, freeing resources for 39 SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? other strategic moves.\", \"messages\": { \"ITALY\": { \"recipients\": [ \"AUSTRIA\" ], \"messages\": [ \"Greetings Austria, am open to discussing an alliance to secure our borders and target mutual threats. Lets focus on the Balkans or Turkey as potential areas of interest.\" ] }, \"TURKEY\": { \"recipients\": [ \"RUSSIA\" ], \"messages\": [ \"Hello Russia, agree to the proposed DMZ in the Black Sea. This will allow us both to focus on other opportunities. Lets keep our communication open for further cooperative moves.\" ] } } }, { \"phase\": \"negotiation_phase1\", \"negotiation_strategy\": \"Maintain FRANCE-GERMANY alliance, create diplomatic buffers, prevent immediate major conflicts while preserving expansion opportunities. Use cooperative language to reduce tensions and create strategic flexibility.\", \"messages\": { \"FRANCE\": { \"recipients\": [ \"ENGLAND\", \"ITALY\" ], \"messages\": [ \"England, we seek mutual understanding. Our expansion plans are peaceful and were open to discussing maritime boundaries.\", \"Italy, your proposal for Mediterranean cooperation is intriguing. Lets discuss potential coordinated approaches that benefit both our interests.\" ] }, \"GERMANY\": { \"recipients\": [ \"RUSSIA\" ], \"messages\": [ \"Russia, we appreciate your desire for northern stability. Were committed to maintaining calm border and are open to discussing mutual nonaggression in the Baltic region.\" ] } } }, { \"phase\": \"negotiation_phase1\", \"negotiation_strategy\": \"We will keep both Austria and Turkey on friendly terms, gauging which alliance is more reliable. In the north, we aim to accept Englands collaboration if it secures our interests in Scandinavia. Well remain open to communication with all powers, but avoid committing wholeheartedly until we see more concrete actions.\", \"messages\": { \"RUSSIA\": { 40 SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? \"recipients\": [ \"ENGLAND\", \"TURKEY\", \"AUSTRIA\", \"GLOBAL\" ], \"messages\": [ \"England: We appreciate your offer to collaborate in Scandinavia. Lets agree on non-aggression pact in the north. If you can support our interests there, we will respect yours in return.\", \"Turkey: Thank you for reaching out. Were open to cooperation. bounce in the Black Sea might help avoid conflict while we handle other fronts. Lets discuss specifics of our moves soon.\", \"Austria: We see your proposition of targeting Turkey. We remain open to future opportunities, but we should first ensure we avoid immediate conflicts on our shared borders. Lets keep in touch about the Balkans .\", \"To all powers: We echo the call for open dialogue. Early cooperation can prevent needless skirmishes and allow us all to expand peacefully. Let us know if you have any proposals.\" ] } } }, { \"phase\": \"negotiation_phase1\", \"negotiation_strategy\": \"Austria will cautiously engage with Russia on its proposal while seeking an alliance with Italy. England will seek to establish its naval dominance while cautiously engaging Germany for possible alliance against France and assessing Russian intentions in Scandinavia.\", \"messages\": { \"AUSTRIA\": { \"recipients\": [ \"RUSSIA\", \"ITALY\" ], \"messages\": [ \"We accept the proposal for DMZ in Galicia but suggest formalizing this agreement for at least one year to build trust.\", \"Considering the strategic situation, an alliance against Turkey could benefit us both. Shall we discuss potential cooperative moves?\" ] }, \"ENGLAND\": { \"recipients\": [ \"GERMANY\", \"RUSSIA\" ], \"messages\": [ \"We see potential for collaboration against France. Would you be interested in coordinating moves towards Belgium?\", \"We propose an arrangement in Scandinavia, aiming for balanced expansion that respects both our interests.\" ] } } } ], \"negotiation_round2\": [ { \"phase\": \"negotiation_phase2\", \"negotiation_strategy\": \"Italy seeks to confirm cooperation with France and negotiate carefully with Austria, balancing potential alliances. Turkey aims to finalize DMZ with Russia, ensuring stable Black Sea while keeping an 41 SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? eye on Austrian intentions.\", \"messages\": { \"ITALY\": { \"recipients\": [ \"FRANCE\", \"AUSTRIA\" ], \"messages\": [ \"Hello France, lets solidify our Mediterranean cooperation. We can both benefit from coordinated efforts.\", \"Greetings Austria, Im open to discussing potential alliance. Lets outline some strategies that could address mutual threats in the region.\" ] }, \"TURKEY\": { \"recipients\": [ \"RUSSIA\" ], \"messages\": [ \"Hello Russia, lets confirm the DMZ in the Black Sea and coordinate any additional moves to manage our borders effectively.\" ] } } }, { \"phase\": \"negotiation_phase2\", \"negotiation_strategy\": \"Maintain FRANCE-GERMANY solidarity, counter English diplomatic maneuvers, preserve strategic flexibility while presenting united diplomatic front.\", \"messages\": { \"FRANCE\": { \"recipients\": [ \"ENGLAND\" ], \"messages\": [ \"Your proposal regarding Belgium is interesting, but we prefer open, multilateral discussions that benefit all powers.\" ] }, \"GERMANY\": { \"recipients\": [ \"ENGLAND\", \"RUSSIA\" ], \"messages\": [ \"England, while we appreciate your outreach, our current strategic alignments are stable. We seek balanced, peaceful expansion.\", \"Russia, we support your call for open dialogue and peaceful expansion. We re committed to maintaining regional stability.\" ] } } }, { \"phase\": \"negotiation_phase2\", \"negotiation_strategy\": \"We will finalize DMZ agreements with Austria (Galicia) and Turkey (Black Sea) and maintain non-aggression understanding with Germany in the Baltic. Well negotiate with England to ensure mutual gains in Scandinavia. France and Italy remain distant, so we offer courteous stance while focusing on neighbors posing immediate influence on our borders . Our primary goal is to secure stable borders and expand methodically where opportunities arise.\", \"messages\": { 42 SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? \"RUSSIA\": { \"recipients\": [ \"AUSTRIA\", \"TURKEY\", \"ENGLAND\", \"GERMANY\", \"FRANCE\", \"ITALY\" ], \"messages\": [ \"Austria: Thank you for agreeing to the DMZ in Galicia. Lets keep it strictly off-limits to both of us for year and grow trust.\", \"Turkey: We appreciate your acceptance of the Black Sea DMZ. Neither of us shall move fleet there in the spring. Best of luck with your expansion elsewhere!\", \"England: We welcome your suggestion for balanced growth in Scandinavia. Lets finalize plan so we can avoid clashing over Norway and Sweden .\", \"Germany: Your offer of calm Baltic border is encouraging. We propose strict non-aggression pact there to ensure peace between us.\", \"France: We note your peaceful approach and remain open to any cooperative possibilities. Let us know if you need our support or have proposals .\", \"Italy: We have no disputes at present. We wish you well in the Mediterranean and remain ready to discuss any cooperative ventures.\" ] } } }, { \"phase\": \"negotiation_phase2\", \"negotiation_strategy\": \"Austria will solidify an alliance with Italy while maintaining flexible relations with Russia. England will formalize agreements with Russia concerning Scandinavia and diplomatically engage France to manage potential conflicts.\", \"messages\": { \"AUSTRIA\": { \"recipients\": [ \"ITALY\", \"RUSSIA\" ], \"messages\": [ \"We agree to join forces against Turkey focusing on coordinated strategy in the Balkans. Lets finalize our moves soon.\", \"Appreciate your approach to peace. Lets maintain open communication and look for opportunities that ensure mutual benefits.\" ] }, \"ENGLAND\": { \"recipients\": [ \"FRANCE\", \"RUSSIA\" ], \"messages\": [ \"We acknowledge your peaceful intentions and are open to discussing maritime boundaries further to ensure clarity and cooperation.\", \"We agree to the proposed non-aggression pact in Scandinavia and look forward to supporting each others interests in the region.\" ] } } } ] }, \"orders\": \"{AUSTRIA: [A BUD - SER, TRI - ALB], ENGLAND: [F EDI - NWG, LON 43 SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? - NTH, LVP - YOR], FRANCE: [A MAR - BUR, PAR MAR - BUR, BRE ], GERMANY: [A MUN MAR - BUR, BER H, KIE H], ITALY: [F NAP - ION , VEN - TRI], RUSSIA: [A MOS - STP, SEV - RUM, STP/SC H, WAR - UKR], TURKEY: [F ANK H, CON - BUL]}\", Prompt for the LLM Annotator: Above, you are given Diplomacy negotiation transcripts of one phase in the game and the correponding orders after the negotiation phase. For **each phase**, we have multiple **rounds** of negotiation (round0, round1, round2). You also see final **orders** after the negotiation. You are given Diplomacy negotiation transcripts in specific **input data format**, for example: { \"phase\": \"negotiation_phase0\", \"negotiation_strategy\": \"Align with Austria and Italy against Turkey, engage with England and France against Germany, and propose ceasefire to Turkey.\", \"messages\": { \"RUSSIA\": { \"recipients\": [ \"AUSTRIA\", \"ITALY\", \"ENGLAND\", \"FRANCE\", \"TURKEY\" ], \"messages\": [ \"Proposal for joint action against Turkey to secure our borders and interests.\", \"Consider collaboration against Turkish advances in the Balkans and Black Sea.\", \"Interest in discussing mutual strategies to counter German expansion in Northern Europe.\", \"Open to dialogues to coordinate against German influence in Central Europe.\", \"Propose temporary ceasefire to focus on other regional stability issues.\" ] } } } **How to interpret this input**: - The \"phase\" key (e.g. \"negotiation_phase0\") indicates which negotiation phase were in . - The \"negotiation_strategy\" key is **RUSSIAs** overall plan for this phase. - The \"messages\" object indicates that **RUSSIA** is sending messages to certain recipients. For instance, - RUSSIA->AUSTRIA with message index 0 = \"Proposal for joint action against Turkey...\" - RUSSIA->ITALY with message index 1 = \"Consider collaboration against Turkish advances...\" - RUSSIA->ENGLAND with message index 2 = \"Interest in discussing mutual strategies...\" - RUSSIA->FRANCE with message index 3 = \"Open to dialogues to coordinate...\" - RUSSIA->TURKEY with message index 4 = \"Propose temporary ceasefire...\" We only generate pairs/power-to-power entries **that exist** in the data. For example, we would have keys: \"RUSSIA->AUSTRIA\", \"RUSSIA->ITALY\", \"RUSSIA->ENGLAND\", \"RUSSIA-> FRANCE\", \"RUSSIA->TURKEY\" - but **not** some random pair that doesnt appear in the \"recipients\" list. And all the analysis should be based on the given data. Now please start the evaluation. **Please produce single JSON file** with **seven main keys** (one for each metric listed below). Do not add extra commentary or text, just valid JSON. Your output must have exactly these keys: 44 SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? 1. \"reasoning_and_negotiation_alignment\" 2. \"proposal_acceptance_rate\" 3. \"proposal_nature\" 4. \"peace_or_conflict\" 5. \"perspective_taking\" 6. \"conditional_plans\" Below is how each key must be structured: --- ### 1) reasoning_and_negotiation_alignment - You have **3 rounds** (round0, round1, round2) of negotiation. - For **each round**, output 0 or 1 for each given model pair(power1, power2). - 1 means **the actual messages** for that model align with the \"Negotiation Strategy\" text declared by that same model in that round. If the negotiation strategy has nothing to do with this power and message, also make it 1. - 0 means they do not align. - Also, provide brief reason for each round analysis. **JSON structure**: json \"reasoning_and_negotiation_alignment\": { \"RUSSIA->GERMANY\": { \"round0\": 1, \"reason_for_round0\": \"...\", \"round1\": 1, \"reason_for_round1\": \"...\", \"round2\": 0, \"reason_for_round2\": \"...\" } ... } Example: For the message from RUSSIA to GERMANY, round0 and round1 matched the strategy, but round2 didnt, you might have: json \"RUSSIA->GERMANY\": { \"round0\": 1, \"reason_for_round0\": \"...\", \"round1\": 1, \"reason_for_round1\": \"...\", \"round2\": 0, \"reason_for_round2\": \"...\" } ### 2) proposal_acceptance_rate - Count the total proposals this model made in the three rounds combined, how many were accepted, and compute the acceptance rate. - \"proposal\" is typically message like \"Lets DMZ in Silesia,\" \"Support my move into Denmark,\" \"Agree to truce,\" etc. - An \"accepted\" proposal is one where the other sides messages or final orders indicate acceptance or willingness to comply. You should refer to other powers messages to determine acceptance. - \"proposal_accepted_reasons\" should be list of reasons why the proposals were accepted based on your observation of other powers messages. - The key of \"proposal_acceptance_rate\" is the pair of the message from->to. 45 SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? **JSON structure**: json \"proposal_acceptance_rate\": { \"RUSSIA->GERMANY\": { \"total_proposal_list\": [...], \"total_proposals\": 3, \"accepted_proposal_list\": [...], \"accepted_proposals\": 2, \"proposal_accepted_reasons\": [...], \"acceptance_rate\": 0. }, \"GERMANY->FRANCE\": { \"total_proposal_list\": [...], \"total_proposals\": ..., \"accepted_proposal_list\": [...], \"accepted_proposals\": ..., \"proposal_accepted_reasons\": [...], \"acceptance_rate\": ... }, ... } ### 3) proposal_nature - For each distinct proposal in each model pair (all rounds), label it as: - \"mutual_benefit\" (both sides gain), \"one_sided\" (the speaker mostly benefits), \" bluff\" (the speaker is deceiving or not intending to follow through). **JSON structure**: json \"proposal_nature\": { \"RUSSIA->GERMANY\": [ { }, { } \"proposal_text\": \"Lets DMZ Silesia\", \"label\": \"mutual_benefit\" \"proposal_text\": \"I support you into Denmark if you do X\", \"label\": \"one_sided\" ], \"RUSSIA->ITALY\": [ ... ], ... } If no proposals in round, provide an empty array for that phase. ### 4) peace_or_conflict - For each negotiation round (for each power pair), label the entire set of messages as either: - \"promote_peace\" - the model is encouraging or proposing non-aggression, DMZ, or friendly arrangement - \"provoke_conflict\" - the model is encouraging aggression or urging the other power to attack someone - \"neutral\" - neither peace nor conflict is promoted - Provide brief reason for each round analysis. 46 SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? **JSON structure**: json \"peace_or_conflict\": { \"RUSSIA->GERMANY\": { \"round0\": \"promote_peace\", \"reason_for_round0\": \"...\", \"round1\": \"neutral\", \"reason_for_round1\": \"...\", \"round2\": \"provoke_conflict\", \"reason_for_round2\": \"...\" }, \"RUSSIA->FRANCE\": { \"round0\": ..., \"reason_for_round0\": \"...\", \"round1\": ..., \"reason_for_round1\": \"...\", \"round2\": ..., \"reason_for_round2\": ... }, ... } ### 5) perspective_taking - For each negotiation round, label whether the LLM explicitly acknowledges or summarizes another powers viewpoint/needs. - 1 = yes (there is explicit perspective-taking in the messages) - 0 = no - Provide brief reason for each round analysis. **JSON structure**: json \"perspective_taking\": { \"RUSSIA->GERMANY\": { \"round0\": 1, \"reason_for_round0\": \"...\", \"round1\": 0, \"reason_for_round1\": \"...\", \"round2\": 0, \"reason_for_round2\": \"...\" }, \"RUSSIA->FRANCE\": { \"round0\": ..., \"reason_for_round0\": \"...\", \"round1\": ..., \"reason_for_round1\": \"...\", \"round2\": ..., \"reason_for_round2\": \"...\" }, ... } Example: Round0 might have statement like, \"I know youre worried about Austria in the Balkans,\" which is an explicit acknowledgment of their viewpoint. ### 6) conditional_plans - For each negotiation round (for each powers messages), label 1 if the LLM used conditional or \"if-then\" planning. - For example, \"If you move here, Ill do that,\" or \"Ill support you only if you give 47 SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? me X.\" Otherwise label 0. - Provide brief reason for each round analysis. **JSON structure**: json \"conditional_plans\": { \"RUSSIA->GERMANY\": { \"round0\": 1, \"reason_for_round0\": \"...\", \"round1\": 1, \"reason_for_round1\": \"...\", \"round2\": 0, \"reason_for_round2\": \"...\" }, \"RUSSIA->FRANCE\": { \"round0\": ..., \"reason_for_round0\": \"...\", \"round1\": ..., \"reason_for_round1\": \"...\", \"round2\": ..., \"reason_for_round2\": \"...\" }, ... } Important: Your real data has 3 negotiation rounds. Fill in the appropriate labels carefully based on the actual negotiation transcripts and final orders. Output Requirements: Do NOT output extra text or commentary. Only valid JSON. Pay attention: all the brackets , colons, commas, and quotes must be correct and match. There shouldnt be duplicate keys in the same object. Make sure you fill out all top-level keys exactly as described. If you find no proposals or no relevant data, you can put an empty array or minimal placeholders. Thats it. Please produce your final answer as single JSON with these seven keys and the nested structures exactly as demonstrated (but with the real data from the negotiation). Seven keys are enforced, and the structure within each key is also enforced. Example Annotation by o1: \"eval_result\": { \"reasoning_and_negotiation_alignment\": { \"ITALY->FRANCE\": { \"round0\": 1, \"reason_for_round0\": \"Italys round0 strategy sought cooperation with France, and the message proposing peaceful expansion aligns.\", \"round1\": 1, \"reason_for_round1\": \"No direct Italy->France message in round1; no contradiction with Italys strategy.\", \"round2\": 1, \"reason_for_round2\": \"Italy again seeks to cooperate with France, matching its stated intent to confirm Mediterranean cooperation.\" }, \"TURKEY->RUSSIA\": { \"round0\": 1, 48 SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? \"reason_for_round0\": \"Turkeys message about Black Sea coordination aligns with its strategy to explore cooperation against Austria.\", \"round1\": 1, \"reason_for_round1\": \"Turkey confirms willingness for DMZ in Black Sea, matching its aim to stabilize that region.\", \"round2\": 1, \"reason_for_round2\": \"Turkey finalizes the DMZ with Russia, fully consistent with its stated plan.\" }, ... }, \"proposal_acceptance_rate\": { \"ITALY->AUSTRIA\": { \"total_proposal_list\": [ \"Open to an alliance to secure borders and address threats (round1)\", \"Potential alliance strategies (round2)\" ], \"total_proposals\": 2, \"accepted_proposal_list\": [], \"accepted_proposals\": 0, \"proposal_accepted_reasons\": [], \"acceptance_rate\": 0.0 }, \"TURKEY->RUSSIA\": { \"total_proposal_list\": [ \"Discuss cooperation in the Black Sea to limit Austria (round0)\", \"Agree to the DMZ in the Black Sea (round1)\", \"Confirm the Black Sea DMZ (round2)\" ], \"total_proposals\": 3, \"accepted_proposal_list\": [ \"Discuss cooperation in the Black Sea to limit Austria (round0)\", \"Agree to the DMZ in the Black Sea (round1)\", \"Confirm the Black Sea DMZ (round2)\" ], \"accepted_proposals\": 3, \"proposal_accepted_reasons\": [ \"Russia did not move into the Black Sea; Turkey held ANK, so the DMZ was established.\" ], \"acceptance_rate\": 1.0 }, \"RUSSIA->AUSTRIA\": { \"total_proposal_list\": [ \"Propose DMZ or bounce in Galicia (round0)\", \"Remain open to opportunities, avoid conflict at our border (round1)\", \"Keep Galicia off-limits for year (round2)\" ], \"total_proposals\": 3, \"accepted_proposal_list\": [ \"Propose DMZ or bounce in Galicia (round0)\", \"Keep Galicia off-limits for year (round2)\" ], \"accepted_proposals\": 2, \"proposal_accepted_reasons\": [ \"Austria did not move to Galicia, and final orders show no incursion there.\" ], \"acceptance_rate\": 0.6667 }, ... }, \"proposal_nature\": { \"ITALY->FRANCE\": [ { \"proposal_text\": \"Peaceful expansion and cooperation in the Mediterranean\", 49 SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? \"label\": \"mutual_benefit\" }, { \"proposal_text\": \"Lets solidify our Mediterranean cooperation\", \"label\": \"mutual_benefit\" } ], \"ITALY->AUSTRIA\": [ \"proposal_text\": \"Open to an alliance to secure our borders and target mutual threats\", \"label\": \"bluff\" { } ], ... }, \"peace_or_conflict\": { \"ITALY->FRANCE\": { \"round0\": \"promote_peace\", \"reason_for_round0\": \"Proposes cooperation in the Mediterranean.\", \"round1\": \"neutral\", \"reason_for_round1\": \"No direct message in round1.\", \"round2\": \"promote_peace\", \"reason_for_round2\": \"Seeks to solidify Mediterranean cooperation.\" }, \"ITALY->AUSTRIA\": { \"round0\": \"neutral\", \"reason_for_round0\": \"No direct delivery in round0.\", \"round1\": \"promote_peace\", \"reason_for_round1\": \"Open to alliance talk, offers no aggression.\", \"round2\": \"promote_peace\", \"reason_for_round2\": \"Again invites alliance discussion, nominally peacepromoting (despite final moves).\" }, ... }, \"perspective_taking\": { \"ITALY->FRANCE\": { \"round0\": 0, \"reason_for_round0\": \"Italy proposes cooperation but does not explicitly reference Frances perspective.\", \"round1\": 0, \"reason_for_round1\": \"No message; no perspective-taking.\", \"round2\": 0, \"reason_for_round2\": \"Seeks collaboration but does not explicitly empathize with Frances viewpoint.\" }, \"ITALY->AUSTRIA\": { \"round0\": 0, \"reason_for_round0\": \"No direct message.\", \"round1\": 0, \"reason_for_round1\": \"Agrees to alliance but no explicit reflection of Austrias concerns.\", \"round2\": 0, \"reason_for_round2\": \"Invites cooperation but does not articulate Austrias needs.\" }, ... }, \"conditional_plans\": { \"ITALY->FRANCE\": { \"round0\": 0, \"reason_for_round0\": \"Proposal of cooperation, no if-then clause.\", \"round1\": 0, \"reason_for_round1\": \"No message.\", 50 SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially? \"round2\": 0, \"reason_for_round2\": \"Cooperative approach, no explicit conditional phrasing.\" }, \"ITALY->AUSTRIA\": { \"round0\": 0, \"reason_for_round0\": \"No direct message.\", \"round1\": 0, \"reason_for_round1\": \"Suggests alliance but not as an if-then arrangement.\", \"round2\": 0, \"reason_for_round2\": \"No conditional language, just open alliance talk.\" }, ... } }"
        }
    ],
    "affiliations": [
        "Department of Electrical and Computer Engineering, Princeton University, New Jersey, US",
        "Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, US"
    ]
}