{
    "paper_title": "WebDancer: Towards Autonomous Information Seeking Agency",
    "authors": [
        "Jialong Wu",
        "Baixuan Li",
        "Runnan Fang",
        "Wenbiao Yin",
        "Liwen Zhang",
        "Zhengwei Tao",
        "Dingchu Zhang",
        "Zekun Xi",
        "Yong Jiang",
        "Pengjun Xie",
        "Fei Huang",
        "Jingren Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Addressing intricate real-world problems necessitates in-depth information seeking and multi-step reasoning. Recent progress in agentic systems, exemplified by Deep Research, underscores the potential for autonomous multi-step research. In this work, we present a cohesive paradigm for building end-to-end agentic information seeking agents from a data-centric and training-stage perspective. Our approach consists of four key stages: (1) browsing data construction, (2) trajectories sampling, (3) supervised fine-tuning for effective cold start, and (4) reinforcement learning for enhanced generalisation. We instantiate this framework in a web agent based on the ReAct, WebDancer. Empirical evaluations on the challenging information seeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of WebDancer, achieving considerable results and highlighting the efficacy of our training paradigm. Further analysis of agent training provides valuable insights and actionable, systematic pathways for developing more capable agentic models. The codes and demo will be released in https://github.com/Alibaba-NLP/WebAgent."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 8 4 6 2 2 . 5 0 5 2 : r WebDancer: Towards Autonomous Information Seeking Agency Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang, Zekun Xi, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou"
        },
        {
            "title": "Tongyi Lab",
            "content": ", Alibaba Group Correspondence to: wujialongml@gmail.com {yinwenbiao.ywb,yongjiang.jy}@alibaba-inc.com"
        },
        {
            "title": "Abstract",
            "content": "Addressing intricate real-world problems necessitates in-depth information seeking and multi-step reasoning. Recent progress in agentic systems, exemplified by Deep Research, underscores the potential for autonomous multi-step research. In this work, we present cohesive paradigm for building end-to-end agentic information seeking agents from data-centric and training-stage perspective. Our approach consists of four key stages: (1) browsing data construction, (2) trajectories sampling, (3) supervised fine-tuning for effective cold start, and (4) reinforcement learning for enhanced generalisation. We instantiate this framework in web agent based on the ReAct, WebDancer. Empirical evaluations on the challenging information seeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of WebDancer, achieving considerable results and highlighting the efficacy of our training paradigm. Further analysis of agent training provides valuable insights and actionable, systematic pathways for developing more capable agentic models."
        },
        {
            "title": "Introduction",
            "content": "Web agents are autonomous systems that perceive their real-world web environment, make decisions, and take actions to accomplish specific and human-like tasks. Recent systems, such as ChatGPT Deep Research [1] and Grok DeepSearch [2], have demonstrated strong deep information-seeking capabilities through end-to-end reinforcement learning (RL) training. The communitys previous approaches for information seeking by agentic systems can be categorized into three types: (i) Directly leveraging prompting engineering techniques to guide Large Language Models (LLMs) or Large Reasoning Models (LRMs) [35] to execute complex tasks. (ii) Incorporating search or browser capabilities into the web agents through supervised fine-tuning (SFT) or RL [6, 5, 710]. The first training-free methods are unable to effectively leverage the reasoning capabilities enabled by the reasoning model. Although the latter methods internalize certain information-seeking capabilities through SFT or RL training, both the training and evaluation datasets are relatively simple and do not capture the real-world challenges, for instance, performance on the 2Wiki dataset has already reached over 80%. Moreover, the current SFT or RL training paradigm does not fully and efficiently exploit the potential of information-seeking behavior. Building autonomous information seeking agency involves addressing set of challenges that span web environment perception and decision-making: (1) acquiring high-quality, fine-grained browsing data that reflects diverse user intents and rich interaction contexts, (2) constructing reliable trajectories that support Equal Core Contributors. Jialong Wu and Wenbiao Yin are project leaders. Correspondence. 3The codes and demo will be released in https://github.com/Alibaba-NLP/WebAgent. Preprint. Work in Process. long-horizon reasoning and task decomposition, and (3) designing scalable and generalizable training strategies capable of endowing the web agent with robust behavior across out-of-distribution web environments, complex interaction patterns, and long-term objectives. To address these challenges, our objective is to unlock the autonomous multi-turn informationseeking agency, exploring how to build web agent like Deep Research from scratch. An agent model like Deep Research produces sequences of interleaved reasoning and action steps, where each action invokes tool to interact with the external environment autonomously. Observations from these interactions guide subsequent reasoning and actions until the task is completed. This process is optimized through end-to-end tool-augmented training. The ReAct framework [11] is the most suitable paradigm, as it tightly couples reasoning with action to facilitate effective learning and generalization in interactive settings. We aim to provide the research community with systematic guideline for building such agents from data-centric and training-stage perspective. From data-centric perspective, constructing web QA data is crucial to building web agents, regardless of whether the training paradigm is SFT or RL. Widely used QA datasets are often shallow, typically consisting of problems that can be solved with single or few-turn search. Previous works often filter the difficult QA pairs from open-sourced human-labeled datasets using prompting techniques [7]. Additionally, challenging web-based QA datasets typically only have test or validation sets, and their data size is relatively small. For example, GAIA [12] only has 466, WebWalkerQA [3] contains 680 examples, and BrowseComp [13] has 1,266, making them insufficient for effective training. Therefore, the automatic synthesis of high-quality datasets becomes crucial. [14, 15]. We synthesise the datasets in two ways: 1). By crawling web pages to construct deep queries, referred to as CRAWLQA, enabling the acquisition of web information through click actions. 2). By enhancing easy-to-hard QA pairs synthesis to incentivize the progression from weak-to-strong agency, transforming simple questions into complex ones, termed E2HQA. From training-stage perspective, prior work has explored SFT or off-policy RL, but these approaches often face generalization issues, particularly in complex, real-world search environments. Other methods adopt on-policy RL directly [6], but in multi-tool settings, early training steps tend to focus primarily on learning tool usage via instruction following. To enable more efficient and effective training, we adopt two-stage approach combining rejection sampling fine-tuning (RFT) with subsequent on-policy RL. For the trajectory sampling, we restrict the action space to two commonly effective web information-seeking tools as action: search . Building on this setup, we employ rejection sampling to generate trajectories using two prompting strategies: one with strong instruction LLMs for Short-CoT and another leveraging the LRMs for Long-CoT. These yield high-quality trajectories containing either short or long thought, respectively. In the RL stage, we adopt the Decoupled Clip and Dynamic Sampling Policy Optimization (DAPO) algorithm [16], whose dynamic sampling mechanism can effectively exploit QA pairs that remain underutilized during the SFT phase, thereby enhancing data efficiency and policy robustness. and click Our key contributions can be summarized as follows: we abstract the end-to-end web agents building pipeline into four key stages: Step I: Construct diverse and challenging deep information seeking QA pairs based on the real-world web environment (2.1); Step II: Sample high-quality trajectories from QA pairs using both LLMs and LRMs to guide the agency learning process (2.2); Step III: Perform fine-tuning to adapt the format instruction following to agentic tasks and environments (3.1); Step IV: Apply RL to optimize the agents decision-making and generalization capabilities in real-world web environments (3.2). We offer systematic, end-to-end pipeline for building long-term information-seeking web agents. Extensive experiments on two web information seeking benchmarks, GAIA and WebWalkerQA, show the effectiveness of our pipeline and WebDancer (4). We further present comprehensive analysis covering data efficiency, agentic system evaluation, and agent learning (5)."
        },
        {
            "title": "2.1 QA Pairs Construction",
            "content": "To enable longer-horizon web exploration trajectories, it is essential to curate substantial corpus of complex and diverse QA pairs that can elicit multi-step reasoning, goal decomposition, and rich 2 Figure 1: Two web data generation pipelines. ‚ù∂ For CRAWLQA, we first collect root url of knowlageable websites. Then we mimic human behavior by systematically clicking and collecting subpages accessible through sublinks on the root/... page. Using predefined rules, we leverage GPT4o to generate synthetic QA pairs based on the gathered information. ‚ù∑ For E2HQA, the initial question Q1 is iteratively evolved using the new information Ci retrieved from the entity Ei at iteration i, allowing the task to progressively scale in complexity, from simpler instances to more challenging ones. We use GPT-4o to rewrite the question until the iteration reaches n. interaction sequences. The main requirements for these QAs are: (i) diversity of question types, and (ii) increased task complexity as measured by the number of interaction steps required for resolution. In contrast to prior datasets that predominantly involve shallow queries solvable in 23 steps, our objective is to scale both the volume and the depth of multi-hop reasoning. To achieve this, we primarily develop the below datasets: CRAWLQA and E2HQA. CRAWLQA Constructing QA pairs based on information crawled from web pages represents an effective paradigm for scalable knowledge acquisition [3]. We begin by collecting the root URLs of official and knowledgeable websites spanning arxiv, github, wiki, etc. [12] To emulate human browsing behavior, we recursively navigate subpages by following accessible hyperlinks from each root site. We employ GPT-4o to synthesize QA pairs from the collected content. To ensure specificity and relevance of questions, inspired by Sen et al. [17], we prompt LLMs to generate questions of designed types (e.g., COUNT, MULTI-HOP, INTERSECTION) via in-context learning [18]. E2HQA Similar to the reverse construction strategy [13, 19], we begin from large QA pairs in SimpleQA style [20] where each answer is concise, fact-seeking entity. We first select an entity En from the question Qn, where represents the number of refinement iterations. Then, we use the LLMs to construct query based on this entity in order to search via search engine for information Cn related to En. After that, we use LLMs œÄ to restructure the obtained content into new query Rn to replace the original entity in the question. The process can be signaled as: Rn = œÄ(S(Cn)). This way, the new question Qn+1 requires solving the sub-problem we have constructed before finding the answer to the original question. Moreover, it ensures that the answer does not change during the question refinement, thereby preserving the validity of the QA pairs. By continuously searching, we can gradually rephrase an initially simple question into more complex multi-step one. Moreover, the number of steps needed to solve the problem can be controlled by adjusting the number of rephrasing times."
        },
        {
            "title": "2.2 Agent Trajectories Rejection Sampling",
            "content": "Agent Setup Our agent framework is based on ReAct [11], the most popular approach to language agents. ReAct trajectory consists of multiple Thought-Action-Observation rounds, where an LM generates free-form Thought for versatile purposes, and structured Action to interact with 3 environments (tools) and receive Observation feedback. We assume that the agent execution loop at time can be denoted as (œÑt, Œ±t, ot), where œÑ denotes Thought, Œ± signifies Action, and represents Observation. Œ± can be further expressed as (Œ±m, Œ±p), where Œ±m is the name of the action, and Œ±p is the parameters required to perform the action. Œ±m {search, visit, answer}, which corresponds action, Œ±p to the two most important agentic tools in the deep information seeking. For search action, Œ±p consists of goal and url_link. The consists of query and filter_year, while for visit observation of search action includes the Top-10 titles and snippets, whereas the observation of the visit action is the evidence and summary, generated by summarizer model Ms. The iteration terminates when the action is answer. Then the historical trajectory can be signaled as: Ht = (œÑ0, Œ±0, o0, œÑ1, ..., œÑt1, Œ±t1, ot1). (1) At time step t, the agent receives an observation ot from the web environment and generates thought œÑt taking an action Œ±t, following poliy œÄ(œÑt, Œ±tHt). The Chain-of-Thought (CoT) method has significantly enhanced the inferential capabilities of LLMs through step-by-step reasoning process [21], corresponding to the thought component in agentic systems. This process is critical for agentic execution, enabling high-level workflow planning, self-reflection, information extraction, adaptive action planning, and accurate action (tool usage). Short and Long CoT Construction Agent models internalise the CoT generation capability as an active behavioral component of the model. [22, 23] The length of CoT and the associated thinking patterns play crucial role in performance [2426] We propose two simple yet effective methods for constructing the short CoT and long CoT, respectively. For short CoTs, we directly leverage the ReAct framework to collect the trajectories using powerful model, GPT-4o. For long CoTs, we sequentially provide the LRMs, QwQ-Plus, with the historical actions and observations at each step, enabling it to decide the next action autonomously. Notably, we exclude the previous thought during further inference, as the LRM, QwQ-Plus, has not been exposed to multi-step reasoning inputs during training. However, we retain the thought at each step in the generated trajectory, as they serve as valuable supervision signals. The LRMs intermediate reasoning process, denoted as, denoted as <reasoning_content>, is recorded as the current thought of the current step. Each constructed QA instance undergoes rejection sampling up to times to ensure quality and coherence. Trajectories Filtering We adopt three-stage funnel-based trajectory filtering framework consisting of validity control, correctness verification, and quality assessment. For validity control, directly prompting LLMs to generate responses in the ReAct format under long-content conditions may result in non-compliance with instructions. In such cases, we discard these data points. For correctness verification, we only retain correct results. We follow the evaluation methodology proposed by Phan et al. [27] and Wei et al. [13] and use GPT-4o for accurate judgment. For quality assessment, we first apply rules to filter out trajectories with more than two actions, ensuring that there are no hallucinations and no severe repetitions. Subsequently, we filter the trajectories based on prompting to retain those that meet the following three criteria: Information Non-redundancy, Goal Alignment, and Logical Reasoning and Accuracy. QA pairs that are not filtered out, they do not contain valid trajectories, can be utilized during the reinforcement learning stage effectively."
        },
        {
            "title": "3 Multi-Step Multi-Tool Agent Learning",
            "content": "After obtaining high-quality trajectories in ReAct format, we seamlessly incorporate them into our agent SFT training stage. Specifically, Thought segments are closed by <think> and </think> , Action segments by <tool_call> and </tool_call> , Observation segments by <tool_response> and </tool_response> . The final Action segment corresponds to the final answer, enclosed by <answer> and </answer> . In addition, the QA data without trajectories, which those filtered during earlier stages, can be effectively leveraged during the RL phase. We first train policy model œÄŒ∏ via agent SFT for cold start, followed by agent RL for generalization. The overall training framework is illustrated in Figure 2. 4The details of training datasets and are shown in App. D. 4 Figure 2: The overview of training framework. (I) The SFT stage for cold start utilizes the reformatted ReAct datasets, where the thought includes both short and long CoT, respectively. (II) The RL stage performs rollouts with the tool calls on the QA pairs that are not utilized during the SFT stage, and optimizes the policy using the DAPO algorithm."
        },
        {
            "title": "3.1 Agent Supervised Fine Tuning",
            "content": "To capture complete agentic trajectories, we train the policy model Œ∏ via supervised fine-tuning on obtained decision-making trajectories. The cold start enhances the models capability to couple multiple reasoning and action steps, teaching it behavioral paradigm of alternating reasoning with action, while preserving its original reasoning capabilities as much as possible. Following the empirical findings of Chen et al. [28, 6], Zhang et al. [22], to avoid interference from external feedback during learning, we mask out loss contributions from observation in the agentic world modelling task, which has been shown to generally improve performance and robustness. Given the task context tc and the complete agentic execution trajectory = (x0, x1, ..., xn1, xn), where each xi {œÑ, Œ±, o}, the loss function is computed as follows: ="
        },
        {
            "title": "1\nI[xi Ã∏= o]",
            "content": "H (cid:88) i=1 (cid:80)H i=1 I[xi = o] log œÄŒ∏(xi tc, x<i) (2) Here, I[xi = o] filters out tokens corresponding to external feedback, ensuring that the loss is computed over the agents autonomous decision steps. The SFT stage offers strong initialization for the subsequent RL stage [29]."
        },
        {
            "title": "3.2 Agent Reinforcement Learning",
            "content": "The agent RL stage aims to internalize the agency capability into the reasoning model, enhancing its multi-turn, multi-tool usage capacity with outcome-based rewards. [30] Building on the SFT stage, RL employs Decoupled Clip and Dynamic Sampling Policy Optimization algorithm to refine and incentivize the policy model œÄŒ∏s ability to interleave Thought-Action-Observation sequences. DAPO Decoupled Clip and Dynamic Sampling Policy Optimization (DAPO) algorithm is an RL algorithm that optimizes policy œÄŒ∏ to produce higher-reward outputs under reward model [16, 31]. For each stage trajtory ti with partial answer ai, DAPO samples set of candidate agentic execution {yj j=1. The policy is then updated to maximize the following objective: }G JDAPO(Œ∏) = E(ti,ai) (cid:20) 1 (cid:88) j= (cid:16) min (cid:0)yj (cid:1) Aj , (cid:2)ri (cid:0)yj ri (cid:1) œµlow, 1 + œµhigh (cid:3) Aj (cid:17)(cid:21) , (3) subject to constraints on the candidate set size and filtering conditions (0 < yi < G), whereri(yj ) is the reward models score for completion yj , The dynamic sampling mechanism over-samples and 5 filters out prompts with accuracy equal to 1 and 0. It is crucial in our data-training pipeline, as the remaining QA pairs, being synthetically generatedmay contain invalid or noisy instances that could otherwise degrade policy learning. Such unreliable samples can be effectively ignored, ensuring the agent focuses on learning from high-quality signals. œµlow, œµhigh are clipping thresholds, Aj denotes an advantage-like term (e.g., derived from relative performance among the sampled completions). Instead of relying on fixed KL penalty, DAPO estimates the probability ratio between the new policy œÄŒ∏ and the old policy œÄŒ∏old as follows: (cid:0)oi qi, oi,<t (cid:1) (cid:0)oi qi, oi,<t Ri mean(cid:0){Ri}(cid:1) std(cid:0){Ri}(cid:1) (cid:1) , Aj ri(Œ∏) = œÄŒ∏ œÄŒ∏old = (4) , where Ri represents raw reward values for the sampled completions, and mean(), std() are computed over the batch to normalize the reward scores into advantage estimates. Agentic Action Rollout Within the ReAct framework, each round of agentic execution begins by generating thought, closed by <think> and <think> , followed by action name Œ±m and corresponding parameters Œ±p, enclosed by <tool_call> and </tool_call> operation, all conditioned on the iteration history H. These components are iteratively used to interact with the real-world search environment, producing an observation as feedback, bounded by <tool_response> and </tool_response> upon the <tool_response> is detected. The round of interaction spans from <think> to </tool_response> . The rollout concludes with the generation of <answer> and </answer> , following the final thought. Reward Design The reward design plays critical role during the RL training process [25]. Our reward system mainly consists of two types of rewards, scoreformat and scoreanswer. Given that format consistency has been largely addressed during the initial RFT stage, we assign small weight to the scoreformat in the overall reward. The scoreformat is binary: it is set to 1 only if the entire output strictly conforms to the required format and all tool calls in json format are valid. Considering that the QA answers are inherently non-verifiable, cannot be reliably evaluated using rule-based F1/EM metrics, despite the brevity of the responses, and that the final evaluation relies on LLM-as-Judge [32] which the judge model is Mj, we opt to employ model-based prompt evaluation as the answer reward signal [3335]. The scoreanswer is also binary, assigned as 1 only when the response is judged as correct by the LLMs. The final reward function is: R(ÀÜyi, y) = 0.1 scoreformat + 0.9 scoreanswer (5) where ÀÜyi denotes the model prediction and is the reference answer."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "We evaluate our approach on two established deep information-seeking benchmarks: GAIA and WebWalkerQA. In this work, we adopt the LLM-as-Judges paradigm to evaluate both tasks using the Pass@1 metric, following [4]. The details of the datasets and baselines are introduced in App. E.1 and App. E.2, respectively. The implementation details are shown in App. E.3. Qwen-7B and Qwen-32B are trained on Short-CoT datasets, while QwQ-32B is trained on Long-CoT datasets. Further analyses are shown in Sec. 5."
        },
        {
            "title": "4.2 Experimental Results",
            "content": "Main Results As shown in Table 1, frameworks without agentic capabilities (No Agency) perform poorly on both the GAIA and WebWalkerQA benchmarks, highlighting the necessity of active information-seeking and agentic decision-making for these tasks. The closed-source agentic system, OpenAI DR, through end-to-end RL training achieves the highest scores. Among Open-sourced frameworks, agentic approaches built on top of native strong reasoning models like QwQ-32B consistently outperform their non-agentic counterparts, demonstrating the effectiveness of leveraging reasoning-specialized models in agent construction. Importantly, under the highly extensible ReAct framework, our proposed WebDancer shows substantial gains over the vanilla ReAct baseline across different model scales. Notably, it even surpasses the performance of GPT-4o in the best-case scenario. This demonstrates that even within lightweight framework, our method significantly enhances 6 Table 1: Main results on GAIA and WebWalkerQA benchmarks. denotes our reproduced results based on the official codes for fair comparison. We discuss the reported results of baselines and concurrent works in App. C.1. denotes our RL model, and denotes our SFT model. - means we can not reproduce the results. The best results among all frameworks are in bolded. GAIA WebWalkerQA Backbone Framework Level 1 Level 2 Level 3 Avg. Easy Medium Hard Avg. Qwen-2.5-7B Qwen-2.5-32B Qwen-2.5-72B GPT-4o QwQ-32B DeepSeek-R1-671B Base Base RAG Base Base Base RAG Base No Agency 3.8 9.6 11. 13.5 15.4 15.4 36.5 26.9 12.8 20.5 12. 20.5 23.1 30.8 33.3 43.6 0.0 8.3 8. 0.0 8.3 25.0 8.3 8.3 6.8 13.6 11. 14.6 17.5 22.3 32.0 31.1 Close-Sourced Agentic Frameworks OpenAI DR 74.3 69.1 47.6 67.4 Open-sourced Agentic Frameworks Search-o1 R1-Searcher Search-o1 Search-o1 WebThinker-Base WebThinker-RL 23.1 28.2 33.3 53.8 51.2 53.8 17.3 19. 25.0 34.6 43.4 44.2 0.0 8.3 0.0 16.7 8.3 8.3 ReAct Agentic Frameworks Vanilla ReAct WebDancer Vanilla ReAct WebDancer Vanilla ReAct WebDancer Vanilla ReAct 28.2 41.0 46.1 46. 48.7 56.4 51.2 15.3 30.7 26.9 44.2 34.6 48.1 34. 0.0 0.0 0.0 8.3 16.6 25.0 8.3 17.5 20.4 28. 39.8 41.7 43.7 18.4 31.0 31.0 40.7 37.8 46.6 34.6 Qwen-2.5-7B Qwen-2.5-32B QwQ-32B Qwen-2.5-7B Qwen-2.5-32B QwQ-32B GPT-4o 1.25 3.8 23.1 9.4 6.7 7.5 36.9 5. - - - - 43.1 47.5 46.2 28.1 40.6 35.6 44. 35.6 49.4 34.6 0.8 2.5 14.3 7.1 6. 2.1 26.1 11.8 - - - - 35.0 33.2 39. 31.2 44.1 38.7 46.7 29.1 55.0 42.0 0.7 3.3 11. 3.3 4.2 4.6 33.5 11.3 - - - - 27.1 25.0 28.7 16.0 28.2 22.5 29.2 13.2 29.6 23. 0.8 3.1 15.3 6.3 5.5 4.3 31.2 10. - - - - 34.1 33.6 37.2 24.2 36.0 31.9 38. 24.1 43.2 33.8 agentic capabilities over the underlying base model, validating the strength and generality of our approach. Results on More Challenging Benchmarks We evaluate our approach on two more challenging datasets, BrowseComp (En.) [13] and BrowseComp-zh (Zh.) [19], which are designed to better reflect complex information-seeking scenarios using PASS@1/PASS@3. As shown in Table 2, WebDancer demonstrates consistently strong performance across both datasets, highlighting its robustness and effectiveness in handling difficult reasoning and information-seeking tasks. Table 2: Results on BrowseComp (En.) and BrowseComp-zh (Zh.). Framework GPT-4o QwQ-32B WebDancer Browsing En. 0.6 1.9 - Zh. 6.2 - 11. 2.8/5.0 13.5/22."
        },
        {
            "title": "5 Analysis",
            "content": "Detailed Results We conduct detailed analyses on the GAIA datasets. Given the dynamic and complex nature of agent environments, as well as the relatively small and variable test set, we further conduct fine-grained analysis of Pass@3 and Cons@3 in Figure 4. The Cons@3 metric is computed by evaluating the number of correct responses out of three independent attempts: achieving one correct answer yields score of 1/3, two correct answers yield 2/3, and three correct answers result in full score of 1. For non-reasoning models, RL leads to substantial improvements in both 7 Figure 4: Detailed evaluation results using Pass@1, Pass@3 and Cons@3 metric on GAIA benchmark. Pass@3 and Cons@3. Notably, the Pass@1 performance after RL is comparable to the Pass@3 of the SFT baseline, consistent with previous findings [36, 37] suggesting that RL can sample correct responses more efficiently. For LRMs, while no significant gains are observed in Pass@1, Pass@3, or Cons@3 after RL, there is noticeable improvement in consistency; this may be attributed to sparse reward signals caused by excessively long trajectories [38, 39]. Nevertheless, we observe notable improvement in consistency, with 30% increase in the number of problems answered correctly across all three attempts. This suggests that continued on-policy optimization yields limited benefits for LRMs in agentic tasks. Our best-performing model achieves Pass@3 score of 61.1% on GAIA and 54.6% on WebWalkerQA. High-quality trajectory data is crucial for effective SFT of agents. We propose two data construction strategies, resulting in the creation of datasets CRAWLQA and E2HQA. After applying trajectory rejection sampling to the QA data, we further perform filtering to enhance data quality. In Figure 3, we conduct ablation studies on the QwQ and evaluate the effectiveness of the constructed datasets. In long-CoT, hallucinations often arise when the model attempts to answer by simulating observations, primarily due to its exclusive reliance on internal reasoning mechanisms. [5] Final performs better than all under low-data regimes, emphasizing the value of robust filtering. Figure 3: Results on data efficiency using GAIA benchmark. Open-only refers to using only challenging QA datasets from open-source sources. SFT for cold start is essential, as the agent tasks demand strong multi-step multi-tool instructionfollowing capabilities. We empirically investigate this by comparing performance under single reinforcement learning setting using QwQ. The results show that the Pass@3 performance is significantly limited, achieving only 5% on the GAIA. For the RL phase, both Pass@3 and Cons@3 show consistent improvements as the number of training steps increases, as illustrated in Figure 5a. R. Model Pass@3 Short-Cot Cons@3 Qwen2.5-7B Qwen2.5-32B QwQ-32B Table 3: Results on CoT knowledge transfer. Inv. denotes invalid rate. R. refers to whether the model is reasoning model. The thinking pattern knowledge used by strong reasoner models is struggle transferable to those of small instruction models. As shown in Table 3, reasoning models trained on trajectories synthesized by reasoning models significantly enhance their reasoning performance [40]. For non-reasoning models, LongCoT also demonstrates good performance, but it introduces additional issues, such as higher invalid rate, often manifested as repetition, leading to exceeding the models context length, particularly in smaller-scale models. These reasoning patterns do not easily transfer to instruction-tuned models, which are generally optimized for taskfollowing behavior rather than deep reasoning. This observation aligns with the findings in [41, 42], which highlight the brittleness of cross-model reasoning knowledge transfer. 5 As such, direct transfer of reasoning capabilities from reasoner models to instruction models remains non-trivial challenge. 21.36% 13.59% 13.27% Long-Cot Cons@3 0.65% 4.20% 0.97% 21.00 30.00 39. 33.98 42.72 44.66 35.92 45.63 58.25 22.33 24.33 28.33 Pass@3 Inv. Inv. 5We also experiment with mixing short-CoTs and long-CoTs, but observe no significant performance improvements. 8 (a) Performance across training steps using the DAPO algorithm. (b) Evolution of thought length and number of actions. (c) Pass@1 and Pass@3 results on different temperatures. Figure 5: Analysis on RL algorithm, emergent agency, and agent environments using GAIA benchmark. RL enables longer reasoning processes and supports more complex agentic action. As demonstrated by the results on Qwen-32B in Figure 5b, we observe that SFT leads to more frequent action generation and extended reasoning sequences, largely due to the nature of our training data (App. E.1). RL frameworks facilitate the emergence of more sophisticated reasoning strategies by allowing models to optimize over sequences of decisions, rather than single-step outputs. This enables models to learn from delayed rewards and engage in deeper exploration of action spaces, leading to more coherent and longer reasoning trajectories. Moreover, RL encourages agentic behaviors where models autonomously decide intermediate steps, subgoals, or tools to achieve final objectives, as shown in App. F. Such capabilities are particularly useful in complex environments where straightforward task-following fails to generalize. Web agent executes in dynamic, evolving environment that inherently resists stabilization. As shown in Figure 5c, adjusting the decoding temperature had minimal impact on final performance, indicating that decoding variability alone does not account for agent instability. Instead, we attribute much of the performance fluctuation to changes in the web environment itself, highlighting the non-stationary and open-ended nature of real-world agent deployment. Unlike static datasets with fixed distributional properties, real-world environments evolve over time, requiring agents to remain robust under changing contexts and partial observability. Additionally, to further investigate potential overfitting, we conduct memorization stress test: we fine-tuned Qwen-7B model on 69 correctly sampled trajectories from the GAIA development set for 10 epochs, and subsequently evaluate its performance on the same set. Despite this, greedy decoding only achieved 37.4%, suggesting the difficulty of stabilization on the open-domained agentic tasks."
        },
        {
            "title": "6 Related Works",
            "content": "Information Seeking Agents and Benchmarks. Recent advances in information-seeking agents aim to integrate web interaction into LLMs reasoning. [43] WebThinker [4] and Search-o1 [5] use tool-augmented LLMs that actively retrieve evidence mid-inference. Some works like R1Searcher [7], ReSearch [6] and Search-R1 [8] focus on reinforcement learning to teach search behavior from outcome-based rewards. DeepResearcher [10] extends this by operating in real web environments with online RL, while SimpleDeepSearcher [9] shows that small number of distilled demonstrations can train effective agents without full RL. These works demonstrate promising capabilities but often rely on limited or simplistic data. In parallel, benchmarks like GAIA [12] and WebWalkerQA [3] test reasoning and browsing, but many are single-turn or domain-limited. BrowseComp [13] and BrowseComp-zh [19] increase task complexity, requiring multi-hop search and multilingual reasoning, yet still lack diversity and scalability. Our work addresses these gaps by proposing automatic synthesis QA datasets designed to challenge agents across domains and task types in more realistic web environments. Agents Learning. Agent learning has evolved from in-context learning towards training-based methods [4446]. Recent studies [4749] have primarily focused on leveraging SFT with curated tasksolving trajectories following the ReAct paradigm. However, empirical evidence suggests that pure SFT-based agents often exhibit limited generalization performance when confronted with adaptive operational contexts [10, 5052]. Building upon these limitations, RL-based methods [7, 10, 10, 50] have demonstrated remarkable potential in developing sophisticated search strategies through learned 9 exploration policies. Despite their theoretical advantages, practical implementations face persistent challenges in training stability and sample efficiency. WebDancer implements two-stage framework: an initial cold-start phase employing trajectory-based STF to establish fundamental agency patterns, followed by targeted RL to cultivate adaptive long-term agency capabilities."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we propose systematic framework for building end-to-end multi-step informationseeking web agents from scratch. By introducing scalable QA data synthesis methods and twostage training pipeline combining SFT and on-policy RL, our WebDancer agent achieves strong performance on GAIA and WebWalkerQA. These findings underscore the significance of our proposed training strategy and provide valuable insights into the critical aspects of agent training. Moving forward, this research offers actionable and systematic pathways for the community to advance the development of increasingly sophisticated agentic models capable of tackling complex real-world information-seeking tasks."
        },
        {
            "title": "References",
            "content": "research [1] OpenAI. deep-research-system-card.pdf."
        },
        {
            "title": "Deep",
            "content": "system card, 2025. URL https://cdn.openai.com/ [2] x.ai. Grok 3 beta the age of reasoning agents, 2025. URL https://x.ai/news/grok-3. [3] Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Deyu Zhou, Pengjun Xie, and Fei Huang. Webwalker: Benchmarking llms in web traversal, 2025. URL https://arxiv.org/abs/ 2501.07572. [4] WebThinker Team. Webthinker: Empowering large reasoning models with deep research capability, 2025. URL https://github.com/RUC-NLPIR/WebThinker. Github. [5] Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. Search-o1: Agentic search-enhanced large reasoning models. arXiv preprint arXiv:2501.05366, 2025. [6] Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Fan Yang, Zenan Zhou, Weipeng Chen, Haofen Wang, Jeff Pan, et al. Learning to reason with search for llms via reinforcement learning. arXiv preprint arXiv:2503.19470, 2025. [7] Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. R1-searcher: Incentivizing the search capability in llms via reinforcement learning. arXiv preprint arXiv:2503.05592, 2025. [8] Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. [9] Shuang Sun, Huatong Song, Yuhao Wang, Ruiyang Ren, Jinhao Jiang, Junjie Zhang, Lei Fang, Zhongyuan Wang, Wayne Xin Zhao, and Ji-Rong Wen. Simpledeepsearcher: Deep information seeking via web-powered reasoning trajectory synthesis. 2025. URL https://github.com/RUCAIBox/SimpleDeepSearcher. [10] Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. Deepresearcher: Scaling deep research via reinforcement learning in real-world environments, 2025. URL https://arxiv.org/abs/2504.03160. [11] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023. [12] Gr√©goire Mialon, Cl√©mentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: benchmark for general ai assistants. In The Twelfth International Conference on Learning Representations, 2023. [13] Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Ame lia Glaese. Browsecomp: simple yet challenging benchmark for browsing agents. 10 [14] Runnan Fang, Xiaobin Wang, Yuan Liang, Shuofei Qiao, Jialong Wu, Zekun Xi, Ningyu Zhang, Yong Jiang, Pengjun Xie, Fei Huang, et al. Synworld: Virtual scenario synthesis for agentic action knowledge refinement. arXiv preprint arXiv:2504.03561, 2025. [15] Yuxin Zuo, Kaiyan Zhang, Shang Qu, Li Sheng, Xuekai Zhu, Biqing Qi, Youbang Sun, Ganqu Cui, Ning Ding, and Bowen Zhou. Ttrl: Test-time reinforcement learning, 2025. URL https://arxiv.org/abs/2504. 16084. [16] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. [17] Priyanka Sen, Alham Fikri Aji, and Amir Saffari. Mintaka: complex, natural, and multilingual dataset for end-to-end question answering. In Nicoletta Calzolari, Chu-Ren Huang, Hansaem Kim, James Pustejovsky, Leo Wanner, Key-Sun Choi, Pum-Mo Ryu, Hsin-Hsi Chen, Lucia Donatelli, Heng Ji, Sadao Kurohashi, Patrizia Paggio, Nianwen Xue, Seokhwan Kim, Younggyun Hahm, Zhong He, Tony Kyungil Lee, Enrico Santus, Francis Bond, and Seung-Hoon Na, editors, Proceedings of the 29th International Conference on Computational Linguistics, pages 16041619, Gyeongju, Republic of Korea, October 2022. International Committee on Computational Linguistics. URL https://aclanthology.org/2022.coling-1.138/. [18] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [19] Peilin Zhou, Bruce Leon, Xiang Ying, Can Zhang, Yifan Shao, Qichen Ye, Dading Chong, Zhiling Jin, Chenxuan Xie, Meng Cao, et al. Browsecomp-zh: Benchmarking web browsing ability of large language models in chinese. arXiv preprint arXiv:2504.19314, 2025. [20] OpenAI. Introducing simpleqa, 2025. URL https://openai.com/index/introducing-simpleqa/. [21] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [22] Yuxiang Zhang, Yuqi Yang, Jiangming Shu, Xinyan Wen, and Jitao Sang. Agent models: Internalizing chain-of-action generation into reasoning models. arXiv preprint arXiv:2503.06580, 2025. [23] Xinji Mai, Haotian Xu, Xing W, Weinong Wang, Yingying Zhang, and Wenqiang Zhang. Agent rl scaling law: Agent rl with spontaneous code execution for mathematical problem solving, 2025. [24] QwQ Team. Qwq-32b: Embracing the power of reinforcement learning, 2025. URL https://qwenlm. github.io/blog/qwq-32b/. [25] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [26] Siwei Wu, Zhongyuan Peng, Xinrun Du, Tuney Zheng, Minghao Liu, Jialong Wu, Jiachen Ma, Yizhi Li, Jian Yang, Wangchunshu Zhou, Qunshu Lin, Junbo Zhao, Zhaoxiang Zhang, Wenhao Huang, Ge Zhang, Chenghua Lin, and J. H. Liu. comparative study on reasoning patterns of openais o1 model, 2024. URL https://arxiv.org/abs/2410.13639. [27] Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, et al. Humanitys last exam. arXiv preprint arXiv:2501.14249, 2025. [28] Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, and Shunyu Yao. Fireact: Toward language agent fine-tuning. arXiv preprint arXiv:2310.05915, 2023. [29] Chong Zhang, Yue Deng, Xiang Lin, Bin Wang, Dianwen Ng, Hai Ye, Xingxuan Li, Yao Xiao, Zhanfeng Mo, Qi Zhang, et al. 100 days after deepseek-r1: survey on replication studies and more directions for reasoning language models. arXiv preprint arXiv:2505.00551, 2025. [30] Komal Kumar, Tajamul Ashraf, Omkar Thawakar, Rao Muhammad Anwer, Hisham Cholakkal, Mubarak Shah, Ming-Hsuan Yang, Phillip HS Torr, Fahad Shahbaz Khan, and Salman Khan. Llm post-training: deep dive into reasoning large language models. arXiv preprint arXiv:2502.21321, 2025. [31] Mohamed Amine Ferrag, Norbert Tihanyi, and Merouane Debbah. Reasoning beyond limits: Advances and open problems for llms. arXiv preprint arXiv:2503.22732, 2025. 11 [32] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. [33] ByteDance Seed, Yufeng Yuan, Yu Yue, Mingxuan Wang, Xiaochen Zuo, Jiaze Chen, Lin Yan, Wenyuan Xu, Chi Zhang, Xin Liu, et al. Seed-thinking-v1. 5: Advancing superb reasoning models with reinforcement learning. arXiv preprint arXiv:2504.13914, 2025. [34] Wenyuan Xu, Xiaochen Zuo, Chao Xin, Yu Yue, Lin Yan, and Yonghui Wu. unified pairwise framework for rlhf: Bridging generative reward modeling and policy optimization. arXiv preprint arXiv:2504.04950, 2025. [35] Zijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, Peng Li, Yang Liu, and Yu Wu. Inferencetime scaling for generalist reward modeling. arXiv preprint arXiv:2504.02495, 2025. [36] Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model?, 2025. URL https://arxiv.org/abs/2504.13837. [37] Gokul Swamy, Sanjiban Choudhury, Wen Sun, Zhiwei Steven Wu, and J. Andrew Bagnell. All roads lead to likelihood: The value of reinforcement learning in fine-tuning, 2025. URL https://arxiv.org/abs/2503. 01067. [38] Lang Feng, Zhenghai Xue, Tingcong Liu, and Bo An. Group-in-group policy optimization for llm agent training, 2025. URL https://arxiv.org/abs/2505.10978. [39] Zhepei Wei, Wenlin Yao, Yao Liu, Weizhi Zhang, Qin Lu, Liang Qiu, Changlong Yu, Puyang Xu, Chao Zhang, Bing Yin, Hyokun Yun, and Lihong Li. Webagent-r1: Training web agents via end-to-end multi-turn reinforcement learning, 2025. URL https://arxiv.org/abs/2505.16421. [40] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen. Tora: tool-integrated reasoning agent for mathematical problem solving. ArXiv, abs/2309.17452, 2023. URL https://api.semanticscholar.org/CorpusID:263310365. [41] Yuetai Li, Xiang Yue, Zhangchen Xu, Fengqing Jiang, Luyao Niu, Bill Yuchen Lin, Bhaskar Ramasubramanian, and Radha Poovendran. Small models struggle to learn from strong reasoners. arXiv preprint arXiv:2502.12143, 2025. [42] Huifeng Yin, Yu Zhao, Minghao Wu, Xuanfan Ni, Bo Zeng, Hao Wang, Tianqi Shi, Liangying Shao, Chenyang Lyu, Longyue Wang, Weihua Luo, and Kaifu Zhang. Towards widening the distillation bottleneck for reasoning models, 2025. [43] Zekun Xi, Wenbiao Yin, Jizhan Fang, Jialong Wu, Runnan Fang, Ningyu Zhang, Jiang Yong, Pengjun Xie, Fei Huang, and Huajun Chen. Omnithink: Expanding knowledge boundaries in machine writing through thinking. arXiv preprint arXiv:2501.09751, 2025. [44] Bang Liu, Xinfeng Li, Jiayi Zhang, Jinlin Wang, Tanjin He, Sirui Hong, Hongzhang Liu, Shaokun Zhang, Kaitao Song, Kunlun Zhu, et al. Advances and challenges in foundation agents: From brain-inspired intelligence to evolutionary, collaborative, and safe systems. arXiv preprint arXiv:2504.01990, 2025. [45] Wangchunshu Zhou, Yixin Ou, Shengwei Ding, Long Li, Jialong Wu, Tiannan Wang, Jiamin Chen, Shuai Wang, Xiaohua Xu, Ningyu Zhang, Huajun Chen, and Yuchen Eleanor Jiang. Symbolic learning enables self-evolving agents. 2024. URL https://arxiv.org/abs/2406.18532. [46] Wangchunshu Zhou, Yuchen Eleanor Jiang, Long Li, Jialong Wu, Tiannan Wang, Shi Qiu, Jintian Zhang, Jing Chen, Ruipu Wu, Shuai Wang, Shiding Zhu, Jiyu Chen, Wentao Zhang, Xiangru Tang, Ningyu Zhang, Huajun Chen, Peng Cui, and Mrinmaya Sachan. Agents: An open-source framework for autonomous language agents. 2023. URL https://arxiv.org/abs/2309.07870. [47] Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou, Yuchen Jiang, Chengfei Lv, and Huajun Chen. Autoact: Automatic agent learning from scratch for qa via self-planning. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 30033021, 2024. [48] Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang. Agenttuning: Enabling generalized agent abilities for llms. In Findings of the Association for Computational Linguistics ACL 2024, pages 30533077, 2024. 12 [49] Zehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang, Jiangning Liu, Dahua Lin, Kai Chen, and Feng Zhao. Agent-flan: Designing data and methods of effective agent tuning for large language models. In Findings of the Association for Computational Linguistics ACL 2024, pages 93549366, 2024. [50] Shaokun Zhang, Yi Dong, Jieyu Zhang, Jan Kautz, Bryan Catanzaro, Andrew Tao, Qingyun Wu, Zhiding Yu, and Guilin Liu. Nemotron-research-tool-n1: Tool-using language models with reinforced reasoning. arXiv preprint arXiv:2505.00024, 2025. [51] Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-T√ºr, Gokhan Tur, and Heng Ji. Toolrl: Reward is all tool learning needs. arXiv preprint arXiv:2504.13958, 2025. [52] Yuanqing Yu, Zhefan Wang, Weizhi Ma, Zhicheng Guo, Jingtao Zhan, Shuai Wang, Chuhan Wu, Zhiqiang Guo, and Min Zhang. Steptool: step-grained reinforcement learning framework for tool learning in llms. arXiv preprint arXiv:2410.07745, 2024. [53] Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried, Gabriel Synnaeve, Rishabh Singh, and Sida I. Wang. Swe-rl: Advancing llm reasoning via reinforcement learning on open software evolution, 2025. [54] Shiyi Cao, Sumanth Hegde, Dacheng Li, Tyler Griggs, Shu Liu, Eric Tang, Jiayi Pan, Xingyao Wang, Akshay Malik, Graham Neubig, Kourosh Hakhamaneshi, Richard Liaw, Philipp Moritz, Matei Zaharia, Joseph E. Gonzalez, and Ion Stoica. Skyrl-v0: Train real-world long-horizon agents via reinforcement learning, 2025. [55] Chaoyun Zhang, Shilin He, Liqun Li, Si Qin, Yu Kang, Qingwei Lin, and Dongmei Zhang. Api agents vs. gui agents: Divergence and convergence. arXiv preprint arXiv:2503.11069, 2025. [56] Yijia Shao, Yucheng Jiang, Theodore Kanell, Peter Xu, Omar Khattab, and Monica Lam. Assisting in writing wikipedia-like articles from scratch with large language models. In NAACL-HLT, 2024. [57] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [58] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. [59] Mengkang Hu, Yuhang Zhou, Wendong Fan, Yuzhou Nie, Bowei Xia, Tao Sun, Ziyu Ye, Zhaoxuan Jin, Yingru Li, Zeyu Zhang, Yifeng Wang, Qianshuo Ye, Ping Luo, and Guohao Li. Owl: Optimized workforce learning for general multi-agent assistance in real-world task automation, 2025. URL https://github.com/ camel-ai/owl. [60] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for \"mind\" exploration of large language model society. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [61] Xinbin Liang, Jinyu Xiang, Zhaoyang Yu, Jiayi Zhang, Sirui Hong, Sheng Fan, and Xiao Tang. Openmanus: An open-source framework for building general ai agents, 2025. URL https://doi.org/10.5281/zenodo. 15186407. [62] anthropic. Meet claude, 2025. URL https://www.anthropic.com/claude. [63] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop questions via single-hop question composition, 2022. URL https://arxiv.org/abs/2108.00573. [64] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models, 2022. [65] Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories, 2022. [66] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing multi-hop qa dataset for comprehensive evaluation of reasoning steps, 2020. URL https://arxiv.org/abs/2011.01060. [67] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering, 2018. URL https://arxiv.org/abs/1809.09600. [68] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [69] OpenAI. Gpt-4 system card, 2022. URL https://cdn.openai.com/papers/gpt-4-system-card. pdf. [70] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. [71] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023."
        },
        {
            "title": "A Limitations",
            "content": "Although our proposed framework has demonstrated promising results, several limitations remain, which point to ongoing efforts and potential directions for future work. Tool Number and Type Currently, we integrate only two basic information-seeking tools. To enable more advanced and fine-grained retrieval capabilities, we plan to incorporate more sophisticated tools, such as browser modeling by abstracting browser functionalities into modular tools, and Python sandbox environment for interacting with external APIs [5355]. This allows the agent to perform more human-like and efficient interactions, paving the way not only for tackling more challenging benchmarks but also for progressing toward more general and autonomous agency. Task Generalization and Benchmarks Our current experiments focus on two short-answer information-seeking tasks. However, comprehensive web agent should also be capable of documentlevel research and generation [56]. Extending to such open-domain, long-form writing poses significant challenges in reward modeling in agentic tasks, which we are actively investigating, particularly how to design more reliable and informative reward signals for long-form generation in open-ended settings [35]. Data Utilization While we have accumulated large corpus of QA pairs and corresponding trajectories, effectively scaling learning remains challenge, particularly in the RL stage, where only small subset (e.g., 5,000 pairs) can be utilized due to computational and stability constraints of RL in agentic tasks. This underscores the need for more efficient data utilization strategies to fully exploit the richness of the collected dataset. High Rollout Cost The RL phase incurs substantial computational and time overhead, as each rollout involves multiple rounds of tool invocations and LLM completions. This high cost not only limits scalability but also slows down iterative development and experimentation. promising direction is to develop more efficient mechanisms for integrating tool calls with model completions, which can reduce rollout time and cost without sacrificing learning policy. Hybrid Thinking We consider two types of datasets characterized by short and long CoTs. Currently, our models are trained on single dataset type. In future work, we plan to develop hybrid reasoning agent model capable of dynamically controlling the reasoning length of the agent. [57] Thinking Pattern In tool invocation, hallucinations may occur. For example, when dealing with mathematical problems, one might erroneously invoke calculate tool that does not actually exist. Additionally, over-action may arise during the reasoning process, where redundant actions are performed even after the answer has been confirmed."
        },
        {
            "title": "B Broader Impacts",
            "content": "Building open-source, autonomous web agents capable of long-term information seeking has the potential to greatly benefit scientific research, education, and productivity by democratizing access to complex web-based reasoning tools. However, such systems also raise concerns, including the risk of misinformation propagation if agents rely on unreliable sources, and the possibility of misuse in automated content extraction or surveillance. We emphasize the importance of transparency, source attribution, and responsible deployment practices to mitigate potential harms."
        },
        {
            "title": "C Discussions",
            "content": "C.1 Concurrent Work Comparison with the Training-based Methods The reported results of these two methods are shown in Table 4. WebThinker also adopts an SFT followed by RL setup, but employs an offpolicy RL algorithm [58]. Furthermore, WebThinker triggers actions and observations within the <thinking_content>, whereas our approach adopts native ReAct style architecture, executing each 15 Table 4: Reported results of concurrent works, WebThinker and Simple DS. GAIA"
        },
        {
            "title": "Framework",
            "content": "Level 1 Level 2 Level 3 Avg."
        },
        {
            "title": "Hard",
            "content": "Avg. Qwen-2.5-7B"
        },
        {
            "title": "Simple DS",
            "content": "Qwen-2.5-32B"
        },
        {
            "title": "Simple DS",
            "content": "QwQ-32B Simple DS WebThinker-Base WebThinker-RL 41.0 53.8 61.5 53.8 56.4 30. 44.2 44.2 44.2 50.0 8.3 8.3 16.7 16.7 16.7 32. 44.7 47.6 44.7 48.5 - - - 47.2 58.8 - - - 41.1 44.6 - - - 39.2 40.4 - - - 41.9 46.5 action after completing its corresponding reasoning step. In contrast, Simple DeepSearcher relies solely on supervised fine-tuning over carefully curated dataset. Our approach similarly follows an SFT-then-RL paradigm, but crucially leverages on-policy RL via DAPO. Our core contribution lies in building scalable end-to-end pipeline, from data construction to algorithmic design, that supports native ReAct reasoning. This framework is compatible with both instruction LLMs and LRMs, enabling seamless integration and improved generalization. Comparison with the Prompting-based Methods Recent efforts in the community have explored building more autonomous and general-purpose agent systems, such as OWL [59, 60], and OpenManus [61], by leveraging foundation models with strong native agentic capabilities, such as Claude [62]. These systems typically rely on carefully engineered agent frameworks and prompting workflows, often involving multi-step tool usage and human-curated task structures. In contrast, we advocate for open-source models with emergent agency, crucial for democratizing agentic AI and advancing fundamental understanding of how agency can arise and scale in open systems. Our native RAct framework embraces simplicity, embodying the principle that less is more. Training native agentic models is fundamentally valuable. C.2 Post-train Agentic Models Agentic models refer to foundation models that natively support reasoning, decision-making, and multi-step tool use in interactive environments. They exhibit emergent capabilities such as planning, self-reflection, and action execution through structured prompting alone. Recent systems like DeepSearch and Deep Research illustrate how powerful foundation models can serve as agentic cores, enabling autonomous web interaction through native support for tool invocation and iterative reasoning. However, since web environments are inherently dynamic and partially observable, reinforcement learning plays crucial role in improving the agents adaptability and robustness. In this work, we aim to elicit autonomous agency in open-source models through targeted posttraining."
        },
        {
            "title": "D Training Dataset",
            "content": "We collect 40K samples of E2HQA and 60K samples of CRAWLQA. These data samples are used to generate trajectories via either QwQ or GPT-4o, followed by multi-stage filtering process to ensure quality, as described in Sec. 2.2. Table 5 separately reports the statistics for SFT data generated using Long-CoT and Short-CoT reasoning. We plan to scale this high-quality dataset further to investigate whether increasing the data volume leads to significant performance gains in future work. Table 5: Statistics of training datasets. The thinking length is the average of the tokenized length of the thoughts. CoT Type Num. Action Count Thinking Length Short Long 7,678 6, 4.56 2.31 510.03 1599.39 Filtering Criterion: Regarding the trajectory filter employed in Sec. 2.2, it is important to note that, during the quality assessment phase, we mitigate the presence of repetitive patterns by identifying and constraining the maximum occurrence of n-grams (n=10) within each trajectory to threshold of 4. The purpose of this is to prevent the model from internalizing detrimental patterns, thereby safeguarding the integrity of the inference process. 16 Open-only Datasets: We select set of widely-used QA datasets, including MuSiQue [63], Bamboogle [64], PopQA [65], 2Wiki [66], and HotpotQA [67]. To ensure question difficulty, we apply simple RAG-based filtering process to remove easy questions."
        },
        {
            "title": "E Experimental Details",
            "content": "E.1 Benchmarks GAIA is designed to evaluate general AI assistants on complex information retrieval tasks, while WebWalkerQA focuses specifically on deep web information retrieval. Our experiments use 103 questions from GAIAs text-only validation split and 680 questions from the WebWalkerQA test set. E.2 Baselines We compare WebDancer against the following frameworks: No Agency: which denotes direct use base ability of models and simply uses retrievalaugmented generation (RAG). Includes Qwen2.5-7/32/72B-Instruct [68], QwQ-32B [24], DeepSeek-R1-671B [25], GPT-4o [69]. Close-Sourced Agentic Frameworks: OpenAI Deep Research (DR) use end-to-end reinforcement learning to complete multitask research tasks. Open-Sourced Agentic Frameworks: WebThinker equips an LRM with Deep Web Explorer to autonomously search and browse web pages mid-reasoning, interleaving tool use with chain-of-thought. For fair comparison, we reproduced the results using Google Search and further replicated both the Base and RL versions of the method. Search-o1 [5] performs information-seeking by first generating search queries, retrieving web documents, and then using an LLM to answer based on the retrieved content, without optimizing the search process itself. R1-Searcher [7] trains an LLM to learn when and how to search using outcome-based reinforcement learning, without any supervised demonstrations. E.3 Implements Details Dataset Construction: The number of reject samplling = 5. The summarizer model Ms is Qwen-2.5-72B. We build our system using the widely adopted ReAct framework, implemented on top of the Qwen-Agents 6. Training and Inference: We construct the judge model Mj based on Qwen-72B-Instruct, and design the reward prompt following [27]. For RL, we implement verl [70, 71] to support the RL algorithm and rollouts. The rollout number in RL is 16. We set the inference parameters as follows: temperature = 0.6, topp = 0.95. For the LRM, we use repetition penalty of 1.1, while for the LLM, the repetition penalty is set to 1.0. In the RL, the temperature of rollout is 1.0 and topp = 1.0. We conduct all experiments using 32 nodes with 8 NVIDIA H20 (96GB). E.4 Prompts for Agent Trajectories Sampling"
        },
        {
            "title": "Prompts for ReAct",
            "content": "Answer the following questions as best you can. Use the following format: Question: the input question you must answer Thought: you should always think about what to do 6https://github.com/QwenLM/Qwen-Agent/ 17 Action: the action to take, should be one of [{tool_names}] Action Input: the input to the action, use JSON Schema with explicit parameters Observation: the result of the action ... (this Thought/Action/Action Input/Observation can be repeated many times) Thought: you should always think about what to do Action: Final Answer: the final answer to the original input question ## Execution Framework 1. Thinking phase - Mandatory components: (a). Evidence chain completeness assessment (b). Tool selection rationale 2. Action Phase - Allowed tools: Only use tools listed in {tool_descs} or can be `Final Answer`, which returns the answer and finishes the task. You may only provide the Final Answer when you can confidently confirm the answer. You must also ensure that the Final Answer is accurate and reliable. To output the Final Answer, use the following template: Final Answer: [YOUR Final Answer] 3. Observation phase - Return information from the tool: The result of the action, you can use the result to think about the next step. You have access to the following tools: {tool_descs} Begin! You are likely to use the given tools to gather information and then make the final answer. Solve the following question using interleaving thought, action, and observation steps. You may take as many steps as necessary. Question: {query} Figure 6: Prompts for ReAct using LLMs."
        },
        {
            "title": "Case Trajectory in GAIA",
            "content": "Answer the following questions as best you can. Allowed tools: Only use tools listed in {tool_descs} or can be Final Answer: . You must also ensure that the Final Answer is accurate and reliable. You have access to the following tools: {tool_descs} Begin! Output Format: Action: the action to take, should be one of [{tool_names}] Action Input: the input to the action, use JSON Schema with explicit parameters, when the action is Final Answer, do not have Action Input, directly return the answer You may take as many steps as necessary. Always use the tools to gather information before making final answer. When you want to make tool call, please output complete \"Action: \" and \"Action Input: \" to make the tool call successful and then output \"Observation: \" to make the tool call successful. 18 Question: {query}"
        },
        {
            "title": "F Case Study",
            "content": "Figure 7: Prompts for ReAct using LRMs. As shown in Figure 8, several sophisticated thinking patterns are demonstrated as below: Step-by-step Decomposition WebDancer breaks down complex problem into smaller, manageable steps with \"First ... Then ... Finally\". Hypothesis Testing WebDancer proposes hypotheses and verifies their validity. It assumes that Nemo from Finding Nemo is the orange clownfish (Amphiprion ocellaris). It validates this assumption by consulting the USGS database for non-native occurrences. Handling Information Gaps When certain information is missing, WebDancer takes action to fill the gap actively. Iterative Reflection After failing to find the ZIP code in the USGS database, WebDancer realizes it needs to search for the specific address separately. Through second search, it successfully retrieves the ZIP code. This demonstrates the Agents flexibility and adaptability when facing uncertainty."
        },
        {
            "title": "Case Trajectory in GAIA",
            "content": "Question: Im researching species that became invasive after people who kept them as pets released them. Theres certain species of fish that was popularized as pet by being the main character of the movie Finding Nemo. According to the USGS, where was this fish found as nonnative species, before the year 2020? need the answer formatted as the five-digit zip codes of the places the species was found, separated by commas if there is more than one place. Step 1 <think> Okay, lets tackle this question step by step. The user wants to know the zip codes where the fish from Finding Nemo, which became invasive after being released by pet owners, was found as non-native species according to USGS before 2020. First, remember that the main fish in Finding Nemo is Nemo, clownfish. So the species in question is likely the orange clownfish, Amphiprion ocellaris. But should confirm that. Maybe should start by searching for clownfish invasive species USGSto see if theres any info. Wait, but the user mentioned the USGS specifically. Let me check what USGS says about ... </think> <tool_call> {\"name\": \"search\", \"arguments\": {\"query\": \"USGS Amphiprion ocellaris nonnative locations before 2020\"}} </tool_call> <tool_response> Google search for USGS Amphiprion ocellaris nonnative locations before 2020 found 9 results: 1. file](https://nas.er.usgs.gov/queries/FactSheet.aspx?speciesID=3243) Table 1. with nonindigenous occurrences, the earliest and ... 2. [clown anemonefish (Amphiprion ocellaris)] (https://nas.er.usgs.gov/viewer/omap.aspx) Citation information: U.S. Geological Survey. [2025]. Nonindigenous Aquatic Species Database. Gainesville, Florida. Accessed [5/9/2025]. Contact us if you are... ... </tool_response> (Amphiprion anemonefish ocellaris)"
        },
        {
            "title": "Species",
            "content": "ProStates [clown - 19 Step 2 <think> Okay, so need to find the absolute difference in tens of thousands between the population of chinstrap penguins from the Wikipedia page as of the end of 2018 and the population from the 2020 Nature.com article. The user also wants us to assume two penguins per breeding pair. Let me start by going through the information provided.First, looking at the search results for the Wikipedia part. The sixth result mentions the Chinstrap penguins conservation status, stating that in 2018, the IUCN estimated around 8 million specimens... </think> <tool_call> {\"name\": \"visit\", \"arguments\": {\"url\": \"https://nas.er.usgs.gov/queries/FactSheet.aspx?speciesID =3243\", \"goal\": \"Identify states with nonindigenous occurrences of Amphiprion ocellaris before 2020\"}} </tool_call> <tool_response> {\"evidence\": \"The provided webpage content includes table listing nonindigenous occurrences of Amphiprion ocellaris. The table shows that one individual was collected in Florida in July 2018 at Fred Howard Park, Pinellas Co., which is before 2... </tool_response> Step 3 <think> Okay, lets tackle this question step by step. The user is asking about the zip codes where the clownfish (Amphiprion ocellaris), made famous by Finding Nemo, was found as nonnative species in the US before 2020 according to USGS. First, did search using the provided tools. The initial search led me to the USGS Nonindigenous Aquatic Species database page for Amphiprion ocellaris. Visiting that page showed table listing occurrences. The key point here is that the only entry before 2020 is in Florida, specifically Pinellas County, collected in July 2018 at Fred Howard Park.Now, the next step is converting that location into zip code ... </think> <tool_call> {\"name\": \"visit\", \"arguments\": {\"url\": \"https://nas.er.usgs.gov/queries/collectioninfo.aspx? SpeciesID=3243\", \"goal\": \"Find specific collection locations with zip codes for Amphiprion ocellaris in the US before 2020\"}} </tool_call> <tool_response> \"evidence\": \"1468741 FL Pinellas Gulf of America, Florida, Fred Howard Park 2018 03100207 Crystal-Pithlachascotee eradicated\", \"rational\": \"The webpage provides collection record for Amphiprion ocellaris (clown anemonefish) in the United States, specifically in Pinellas County, Florida. The record indicates that specimen was collected at Fred Howard Park in 2018. However, the webpage does not provide zip codes for the collection locations.\" ... </tool_response> Step 4 <think> So now need to figure out the zip code for Fred Howard Park in Pinellas County, Florida. Since the USGS data doesnt provide zip codes, might need to look up the locations address. Let me think if theres another way using the available tools. The allowed tools are search and visit. Maybe can perform another search to find the zip code of Fred Howard Park. Wait, the users instructions say to use the tools provided. Since cant just know the zip code offhand, should use the search tool again to find the zip code of Fred Howard Park, Pinellas County, Florida. Let me try that. So next action: search for \"Fred Howard Park Pinellas County Florida zip code\". But need to structure it properly. Let me check the parameters for the search tool. The query can be anything, so can input \"What is the zip code of Fred Howard Park, Pinellas County, Florida?\" After getting that info, can then present the zip code ... 20 </think> <tool_call> {\"name\": \"search\", \"arguments\": {\"query\": \"Fred Howard Park Pinellas County Florida zip code\"}} </tool_call> <tool_response> Google search for Fred Howard Park Pinellas County Florida zip code found 9 results: 1. [Fred Howard Park - Pinellas County](https://pinellas.gov/parks/fred-howard-park/) Fred Howard Park is 155 acres of park and beach located off the Gulf of Mexico in Tarpon Springs. There is 1-mile causeway from the park to the white sandy ... 2. US](https://www.mapquest.com/us/florida/fred-howard-park-4792017) ... </tool_response>"
        },
        {
            "title": "1700 Sunset Dr, Tarpon Springs,",
            "content": "[Fred Howard Park, FL 34689, Step 5 <think> The first link is the species profile for Amphiprion ocellaris, which lists states with nonindigenous occurrences. The summary from visiting that page showed that Florida had recorded occurrence in 2018 at Fred Howard Park in Pinellas County. Next, needed to confirm the exact location details. The second visit to the collection info page provided record from 2018 in Pinellas County, Florida, specifically Fred Howard Park. However, the zip code wasnt listed there, ... , and the specific locations zip code is 34689. Since the question asks for zip codes, and this is the only location mentioned in the provided data, the answer should be 34689. need to ensure there arent other locations, but based on the search results, only Floridas Pinellas County was cited. Thus, the final answer is the zip code 34689. </think> <answer>34689 <answer> Figure 8: Case Study."
        }
    ],
    "affiliations": [
        "Alibaba Group"
    ]
}