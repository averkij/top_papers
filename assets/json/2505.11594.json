{
    "paper_title": "SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training",
    "authors": [
        "Jintao Zhang",
        "Jia Wei",
        "Pengle Zhang",
        "Xiaoming Xu",
        "Haofeng Huang",
        "Haoxu Wang",
        "Kai Jiang",
        "Jun Zhu",
        "Jianfei Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The efficiency of attention is important due to its quadratic time complexity. We enhance the efficiency of attention through two key contributions: First, we leverage the new FP4 Tensor Cores in Blackwell GPUs to accelerate attention computation. Our implementation achieves 1038 TOPS on RTX5090, which is a 5x speedup over the fastest FlashAttention on RTX5090. Experiments show that our FP4 attention can accelerate inference of various models in a plug-and-play way. Second, we pioneer low-bit attention to training tasks. Existing low-bit attention works like FlashAttention3 and SageAttention focus only on inference. However, the efficiency of training large models is also important. To explore whether low-bit attention can be effectively applied to training tasks, we design an accurate and efficient 8-bit attention for both forward and backward propagation. Experiments indicate that 8-bit attention achieves lossless performance in fine-tuning tasks but exhibits slower convergence in pretraining tasks. The code will be available at https://github.com/thu-ml/SageAttention."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 4 9 5 1 1 . 5 0 5 2 : r SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-bit Training Jintao Zhang, Jia Wei, Pengle Zhang, Xiaoming Xu, Haofeng Huang, Haoxu Wang, Kai Jiang, Jun Zhu, Jianfei Chen Tsinghua University {zhang-jt24@mails., jianfeic@, dcszj@}tsinghua.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "The efficiency of attention is important due to its quadratic time complexity. We enhance the efficiency of attention through two key contributions: First, we leverage the new FP4 Tensor Cores in Blackwell GPUs to accelerate attention computation. Our implementation achieves 1038 TOPS on RTX5090, which is 5 speedup over the fastest FlashAttention on RTX5090. Experiments show that our FP4 attention can accelerate inference of various models in plug-and-play way. Second, we pioneer low-bit attention to training tasks. Existing low-bit attention works like FlashAttention3 and SageAttention focus only on inference. However, the efficiency of training large models is also important. To explore whether low-bit attention can be effectively applied to training tasks, we design an accurate and efficient 8-bit attention for both forward and backward propagation. Experiments indicate that 8-bit attention achieves lossless performance in fine-tuning tasks but exhibits slower convergence in pretraining tasks. The code will be available at https://github.com/thu-ml/SageAttention. Figure 1: The upper left figure shows the kernel speedup on RTX5090. The other two figures show the end-to-end inference speedup of generating video using HunyuanVideo on RTX5090. Note that FlashAttention3 can only run on Hopper GPUs, so FlashAttention2 is already the fastest on RTX5090."
        },
        {
            "title": "Introduction",
            "content": "Motivation. The efficiency of attention is critical for generation models, especially given their quadratic time complexity with longer sequences [1, 2]. Quantization offers an effective way to Equal Contribution. Preprint. Under review. accelerate inference by utilizing low-bit Tensor Cores in GPUs [3]. The new FP4 Tensor Cores in Blackwell GPUs deliver significantly faster performance compared to FP16 [4]. We want to propose novel FP4 attention implementation that provides plug-and-play compatibility for inference acceleration. Beyond inference, training efficiency is equally important. However, no prior work has explored low-bit attention for training large models. To address this gap, we design trainable 8-bit attention to explore its feasibility in training tasks. To the best of our knowledge, we are the first work that designs FP4 attention for inference and the first work to explore the feasibility of low-bit attention for training large models. Challenges. There are two primary obstacles for FP4 attention and one key difficulty for 8-bit trainable attention. First, (C1) FP4 quantization suffers from severe value limitations (only 15 representable values), making both per-tensor and per-token quantization approaches inadequate for preserving model accuracy. Second, (C2) The attention map consists primarily of small values in the range [0, 1]. When directly quantized to FP4, these values force the scaling factors into an extremely narrow dynamic range. However, hardware requires the quantization factors to be in FP8 data type. This leads to significant accuracy loss when presenting these scale factors in FP8. Third, (C3) When employing 8-bit attention during training, we find that the attention map gradients are particularly vulnerable to quantization errors, resulting in accumulated errors in the input gradients. Our Method. To address (C1), we propose to use FP4 microscaling quantization for the two matrix multiplications in attention, i.e., QK and . By constraining the quantization group size to 1x16 (instead of per-tensor or per-channel), our method effectively contains outlier effects within each block while improving FP4 quantization accuracy. To overcome (C2), we propose two-level quantization method for to fully utilize the presentative range of the FP8 scaling factor, enhancing the quantization accuracy of . Specifically, this approach first normalizes each tokens range to [0, 448 6] through per-token quantization, then applies FP4 microscaling quantization for enhanced precision. To address (C3), we identify the most accuracy-sensitive matrix multiplication among the five in backpropagation and maintain its accuracy in FP16. Result. Our FP4 attention, named SageAttention3, could achieve 1038 TOPS on RTX5090, which is 5 speedup than FlashAttention. Furthermore, we demonstrate that 8-bit trainable attention, named SageBwd, could achieve lossless performance when fine-tuning base models for instructionfollowing tasks, but is not suitable for pretraining tasks. Contribution. Our work makes the following key contributions: (1) We design the first FP4 attention to accelerate inference, achieving 1000+ TOPS on RTX5090. (2) We propose the first trainable low-bit attention, enabling accelerated training with lossless fine-tuning performance, while revealing key insights for low-bit attention in training. Figure 2: Workflow of microscaling FP4 attention."
        },
        {
            "title": "2 Preliminary",
            "content": "FlashAttention. The attention computation contains two matrix multiplications and one softmax calculation: = QK , = Softmax(S), = . The Q, K, are in the shape of D, where means the sequence length and means the dimension of an attention head. P, are in the shape of D, and divides . FlashAttention divides to blocks K, to D. Then it uses online softmax to avoid the large memory Vi} , Ki} { { IO for and : Sij = QiK , Pij = OnlineSoftmax(Sij), Oij = PijVj. in the shape of Bkv in the shape of Bq Qi} { 2 Notation. For simplicity, we omit subscripts and use Q, K, V, S, P, to denote the matrix blocks in FlashAttention, while retaining full subscript notation in Algorithm 1, 2, and 3. Quantization. Quantization is used to accelerate Matmul by converting two matrices from highbit to low-bit with scale factors. Take INT8 quantization for Matmul AB as an example, where , and are in FP16 data type. A/sA , where ˆA, ˆB are in INT8 and the others are in FP32. Then, sB = max( ˆA ˆB sB, which can be accelerated by the INT8 Tensor Core. The granularity of AB quantization is determined by the dimensions reduced by the max operation. For example, in pertoken quantization, the max is computed along each row of matrix. In per-block quantization, the max is computed on block of matrix, which in our paper means FlashAttention block. It can be formulated: sA = max( )/127, ˆB = sA )/127, ˆA = B/sB A"
        },
        {
            "title": "3 FP4 Attention for Inference Acceleration",
            "content": "This section presents our microscaling FP4 attention through three key components: (1) the fundamental workflow for applying microscaling FP4 quantization to attention in Section 3.1, (2) the two-level quantization approach for the attention map in Section 3.2, and (3) critical hardware implementation optimization in Section 3.3. P. (b) and Figure 3: Analysis of the benefit of two-level quantization. (a) shows the distribution of (c) show the distribution of sP using direct quantization and two-level quantization. (d) and (e) show the error of sP and using direct quantization and two-level quantization. (cid:101) 3.1 Microscaling FP4 Attention (cid:101) FP4 microscaling quantization. Given matrix with scale factor matrix sX in FP8 data type. Specifically, is partitioned into Xij where each 1 ([ ˆX, sX = ϕ(X)]) and dequantization (X = ϕ1( ˆX, sX )) can be formulated as follows. RN d, we quantize it to ˆX in FP4 data type R1n blocks, block corresponds to one scale factor sij. The FP4 microscaling quantization Quantization ϕ: sij = max( ˆXij ij = sij Dequantization ϕ1: )/6, ˆXij = Xij/sij (1) (2) means FP4 rounding. Where the FP4 microscaling quantization Matmul. Consider matrix multiplication AB, where and are in FP16 precision. The speed of the Matmul is about 200 TOPS on RTX5090. In contrast, the speed of the FP4 microscaling Matmul is about 1600 TOPS, which is an 8x speedup. The FP4 microscaling Matmul instruction (FP4MM) takes four inputs, i.e., ˆA, sA, ˆB, sB, and the output equals to the Matmul result between ϕ1( ˆA, sA) and ϕ1( ˆB, sB): = FP4MM( ˆA, sA, ˆB, sB) Attention computation. We accelerate attention computation by applying FP4 microscaling quantization to both matrix multiplications: QK and PV. (3) ˆQ, sQ = ϕ(Q), ˆK, sK =ϕ(K), = FP4MM( ˆQ, sQ, ˆK, sK) =OnlineSoftmax(S) ˆP, sP = ϕ( P), ˆV, sV =ϕ(V), = FP4MM( ˆP, sP, ˆV, sV) (cid:101) (4) (cid:101) 3 P, and in our formulation correspond to FlashAttentions tiled Q, K, It is important to note that our hardware implementation builds on FlashAttention, where the matrices Q, K, , and blocks as described in Section 2. Additionally, to enhance the attention accuracy, we adopt the smoothing and in SageAttention2 [5]. The complete algorithm is presented in Algorithm 1. (cid:101) (cid:101) Data type determination. There are two choices for the FP4 data type [6]. The first one is the NVFP4, which is in E2M1 data type and its quantization block size is 1 16 and its scale factor is in E4M3 data type. The second one is the MXFP4, which is also in E2M1 data type. However, its quantization block size is 1 32 and its scale factor is in E8M0 data type. We choose NVFP4 because the accuracy of NVFP4 is much higher than that of MXFP4 in attention quantization. Empirical results: Table 1(a) shows the accuracy of MXFP4 and NVFP4 using real Q, K, across all layers of CogVideoX. Results indicate that the accuracy of NVFP4 outperforms that of MXFP4. qi = mean(Qi), (sQ, ˆQi) = ϕ(Qi qi) ; // Smoothing of SageAttention2. for in [1, Tn] do Algorithm 1: Implementation of the microscaling FP4 attention. 1: Input: Matrices Q(FP16), K(FP16), (FP16) RN d, block size Bq, Bkv. 2: Preprocessing: = mean(K) // Smoothing of SageAttention. 3: Divide to Tm = N/Bq blocks {Qi}; divide K, and to Tn = N/Bkv blocks {Ki}, {Vi} ; 4: for = 1 to Tm do 5: 6: 7: 8: 9: (sK, ˆKj) = ϕ(K Sij = FP4MM( ˆQi, sQ, ˆKj, sK) + GEMV(qi, mij = max(mi,j1, rowmax(Sij)), (cid:101)Pij = exp(Sij mij), lij = emi,j1mij + rowsum( (cid:101)Pij) ; sP1 = rowmax( (cid:101)Pij)/(448 6), (cid:101)Pij = (cid:101)Pij/sP1 , sP2 , ˆPij = ϕ( (cid:101)Pij); // two-level quantization Oij = diag(emi,j1mij )1Oi,j1 + FP4MM( ˆPij, sP2 , ˆVj, sV) sP1 (sV, ˆVj) = ϕ(Vj) ; ) ; // Smoothing Q. ) , 10: 11: 12: 13: Oi = diag(li,Tn )1Oi,Tn ; 14: end for 15: return = {Oi} end for 3.2 Two-level Scaling for (cid:101) (cid:101) Applying microscaling FP4 quantization for presents challenge to attention accuracy. For example, Fig. 12( c) shows direct quantization severely degrades output quality, producing results substantially different from full-precision outputs. Our analysis reveals that the issue occurs because microscaling NVFP4 quantization requires the scale factor to be represented in E4M3 FP8 format [7], rather than the FP32 data type typically used for scale factors. This causes accuracy loss when the scale factor is directly converted to E4M3 format. To better understand this accuracy loss, we analyze is computed using online softmax [8], the data distribution of Pij fall [0, 1]. Consequently, the scale factor (scale factor = the values in each microscaling block Pij)/6) ranges between 0 and 0.167. This narrow range leads to inefficient usage of E4M3s max( representable range, increasing accuracy loss. To reduce accuracy loss by fully utilizing E4M3s range, matrix. Specifically, we first quantize each we propose two-level quantization method for the (cid:101) row of P. The two-level quantization can be formulated as follows: 6]. Then we apply the standard FP4 quantization ϕ for the quantized and its scale factors in Fig. 3. Since to [0, 448 (cid:101) (cid:101) (cid:101) (cid:101) (cid:101) sP1 = rowmax( ˆP2 ( P)/(448 sP2 (cid:101) 6), P2 = sP2 , ˆP2 = ϕ( P/sP1 , sP1 ), = FP4MM( ˆP2, sP2 , ˆV, sV) P2) sP (cid:101) P2, and sP1 are in FP32 data type. sP2 and sV are in FP8 data type. ˆP2 and ˆV are in FP4 (cid:101) (cid:101) (cid:101) (cid:101) (5) Empirical results: As shown in Fig. 3, our two-level quantization maximizes the E4M3 range utilization for sP, thereby reducing both the numerical representation error of sP and the quantization error of P. more formal theoretical analysis is provided in the Appendix. Table 1(b) shows the accuracy of Where P, data type. (cid:101) (cid:101) (cid:101) two-level quantization against naive direct quantization, using real Q, K, from layers of CogVideoX. Results indicate that two-level quantization boosts the accuracy. 3.3 Implementation and Optimization on Hardware Permutation for K. Unlike FP16, the FP32 accumulators memory layout in FP4 MatMul [9] differs from its operand As register layout (shown in Fig. 20 and 19). Performing thread shuffles to match operand As layout would degrade kernel performance. Our solution transforms the accumulator layout  (Fig. 21)  by permuting the tiles columns. To maintain correct MatMul, we correspondingly rearrange Ks columns, which can be fused with the quantization kernel. Reuse shuffle. The in-kernel micro-scaling quantization of requires finding the max value of 16 consecutive row elements. However, as shown in Fig. 21, these 16 elements are distributed across four threads, necessitating intra-thread max reduction followed by inter-thread shuffling, significantly slowing down the kernel. We optimize this by fusing quantization with online softmax, which also computes row-wise maxima. First, we compute the max over 16 elements in and reuse it in the subsequent softmax max-reduction. This fusion reduces redundant shuffles and max operations by 50%, yielding about 10% whole kernel speedup. (cid:101) Producer warp epilogue. In conventional warp-specialized kernels, consumer warps typically handle both MatMul and store operations while producers merely load inputs, with ping-pong scheduling between consumers enabling stage overlap [10]. However, register constraints make this approach infeasible for our FP4 attention kernel. Instead, we implement ping-pong scheduling between producer warps: while one producer loads inputs for the next MatMul operation, another concurrently stores outputs to global memory, with consumer warps solely responsible for transferring MatMul results from registers to shared memory. This novel design overlaps MatMul and global memory stores within register constraints, boosting throughput. 4 INT8 Attention for Training Low-bit quantization attention works, such as FlashAttention3 and SageAttention, are only for inference. In this section, we propose an INT8 attention for training, named SageBwd, which quantizes six of seven matrix multiplications in attention to INT8, achieving no performance degradation in fine-tuning tasks. for in [1, Tn] do )}, {sV, ˆVi} = {ψ(Vi)} ; // Per-block. Algorithm 2: Foward pass of the 8-bit attention. 1: Input: FP16 matrices Q, K, RN d, and block size Bq, Bkv. 2: Divide to Tm = N/Bq blocks {Qi}; divide K, and to Tn = N/Bkv blocks {Ki}, {Vi} ; 3: Quantization: {sQ, ˆQi} = {ψ(Qi)}, {sK, ˆKi} = {ψ(K 4: for = 1 to Tm do 5: Oi RBq = (0), Li RBq = (0), mi RBkv = (0) ; 6: 7: 8: 9: 10: 11: 12: Oi = diag(li,Tn )1Oi,Tn ; Li = mi,Tn + log(li,Tn ) ; 13: 14: end for 15: return = {Oi}, = {Li} ; Sij = MM( ˆQi, ˆKj) sQ sK ; mij = max(mi,j1, rowmax(Sij)), (cid:101)Pij = exp(Sij mij), lij = emi,j1mij + rowsum( (cid:101)Pij); sP = exp(rowmax(Sij) mij)/127, ˆPij = (cid:101)Pij/sP ; // Per-token quantization. Oij = diag(emi,j1mij )1Oi,j1 + MM( ˆPij, ˆVj) sP sV end for 4.1 Forward There are two matrix multiplications in the forward pass of attention: = QK, = PV Per-token quantization for P. Following SageAttention [11], we apply smoothing and per-block INT8 quantization for the QK. However, for the PV, static per-block INT8 quantization with (6) (cid:101) 5 static scale factor of 1/127 for is inaccurate [11]. Fortunately, we find applying per-token INT8 quantization for PV and per-block INT8 quantization for can enhance the attention accuracy. Furthermore, we eliminate the need for explicit max operations on by reusing both global and local maximum values from the online softmax computation (Line 9 in Algorithm 2). The algorithm for the forward is shown in Algorithm 2. (cid:101) (cid:101) Given our extensive use of INT8 per-block quantization in trainable attention, we formalize the process as follows. For each FlashAttention block X, the quantization process sX, ˆX = ψ(X) can be formulated as: sX = max( )/127, ˆX = X/sX (7) for in [1, Tm] do Algorithm 3: Backward pass of the 8-bit attention. 1: Input: {sQ, ˆQi}, {sK, ˆKi}, {sV, ˆVi}, O, {Li} from the forward, dO RN d, and block size Bq, Bkv ; 2: = rowsum(dO O), divide to Tm = N/Bq blocks {Di}; 3: for = 1 to Tn do 4: 5: 6: 7: 8: 9: 10: Sij = MM( ˆQi, ˆKj) sQ sK ; Pij = exp(Sij Li) ; sP, ˆPij = ψ(Pij), dVj dVj + MM( ˆP dPij = MM(dO, dSij = Pij (dPij Di) ; dQi dQi + MM( ˆdSij, ˆKj) sdS sK ; dKj dKj + MM( ˆdS ij, ˆQi) sdS sQ ; sdO, ˆdOi = ψ(dOi) ; // INT8 per-block quantization. ij, ˆdOi) sP sdO ; sdS, ˆdSij = ψ(dSij) ; // INT8 per-block quantization. ) ; // Keep in FP16. end for 11: 12: 13: end for 14: return dQ, dK, dV ; 4.2 Backward There are five matrix multiplications in the backward pass of attention: = QK, dV = PdO, dP = dOV, dQ = dSK, dK = dSQ (8) (cid:101) We observe that whether applying quantizing to dOV has significant impact on the accuracy of the gradient of Q, K. This is because the accuracy of dOV directly determines the accuracy of dP and dS (see computational dependencies in Algorithm 3). The accuracy loss in dS will continuously accumulate errors into dQ and dK during the recurrent process along the sequence length in FlashAttentions backward pass, meaning longer sequences lead to greater error accumulation. Therefore, we maintain dOV in FP16 while accelerating the other four matrix multiplications using INT8 per-block quantization. The algorithm for the forward is shown in Algorithm 3. Empirical results: Table 1 (c) shows the accuracy of the dQ with and without quantization of dOV. We find that the accuracy of dQ is significantly improved when keeping dOV in FP16. Table 1: Accuracy ablation using different quantization strategies. (a) Different FP4 choices (b) Different scale strategies for (cid:101)P (c) Different data types for dOV Type CosSim L1 RMSE Method CosSim L1 RMSE Method CosSim L1 RMSE MXFP4 98.37% 0.294 NVFP4 99.52% 0.077 0.994 0.201 Direct 93.32% 0.193 1.103 Two-level 99.52% 0.077 0.201 INT8 FP16 97.47% 0.171 2.440 99.77% 0.039 0.692 6 Figure 4: Speed comparison between SageAttention3 and Baselines (RTX5090, headim=128). Figure 5: Speed comparison between SageAttention3 and Baselines (RTX5090, headim=64). Figure 6: Speed comparison between SageBwd and Baselines (RTX4090, headim=128). Figure 7: Speed comparison between SageBwd and Baselines (RTX4090, headim=64). Table 2: End-to-end metrics comparison on various models. Model CogvideoX Hunyuan Video Mochi Attention Full-Precision (16bit) SageAttention2 (8bit) SageAttention3 (4bit) Full-Precision (16bit) SageAttention2 (8bit) SageAttention3 (4bit) Full-Precision (16bit) SageAttention2 (8bit) SageAttention3 (4bit) CLIPSIM 0.1865 0.1880 0.1881 0.1838 0.1836 0.1866 0.1828 0.1819 0.1800 CLIP-T 0.9968 0.9969 0.9969 0.9993 0.9993 0.9993 0.9990 0.9990 0. VQA-a 70.476 69.414 69.860 68.998 69.497 70.552 61.9840 61.0093 61.863 VQA-t 69.875 70.750 70.364 78.891 77.019 75.440 61.0000 60.3732 59.429 FScore 4.780 4.534 4.035 1.4793 1.4741 1.232 1.8042 1.7539 1.649 Model Attention Flux Stable-Di ffusion3.5 Full-Precision (16bit) SageAttention2 (8bit) SageAttention3 (4bit) Full-Precision (16bit) SageAttention2 (8bit) SageAttention3 (4bit) FID 162.812 163.107 162.121 166.421 164.986 166.102 sFID 146.980 146.213 142.839 146.379 148.557 145. CLIP 31.409 31.436 31.450 31.93 32.01 32.01 IR 0.91 0.90 0.94 0.93 0.93 0.92 7 (a) (c) Figure 8: Pretraining and Finetuing loss curves of BF16 and 8-bit atttention. (d) (b) (e) Table 3: 8-bit attention finetune results on Qwen2.5 and Llama3.2 models. Model Method GSM8K(Acc) DROP(F1) MMLU(Acc) HELLASWAG(Acc) Qwen2.5 (1.5B) Qwen2.5 (3B) Llama3.2 (1B) BF16 SageBwd BF SageBwd BF16 SageBwd 0.521 0.520 0. 0.607 0.259 0.268 0.733 0.734 0. 0.782 0.641 0.637 0.569 0.574 0. 0.653 0.464 0.458 0.905 0.911 0. 0.943 0.828 0."
        },
        {
            "title": "5 Experiment",
            "content": "Main results. SageAttention3 is faster than FlashAttention and xformers by 5 on RTX5090, and maintains end-to-end metrics across various models. Furthermore, SageBwd is faster than FlashAttention and xformers by 1.67 on RTX4090, and achieves no measurable and 3 degradation in fine-tuning tasks. and 11 5.1 Setup Models and attentions. We validate the effectiveness of SageAttention3 and SageBwd across diverse set of representative models from language, image, and video generation. Specifically, we conduct experiments on: Qwen2.5 [12] and Llama3.2 [13] for text2text, CogvideoX [14], HunyuanVideo [15], and Mochi [16] for text2video, Flux [17], and Stable-Diffusion3.5 [18] for text2image. We compare our method with FlashAttention2 [19], xformers [20], SageAttention [11], and SageAttention2 [5]. Please note that FlashAttention3 can only run on Hopper GPUs, so FlashAttention2 is already the fastest version for RTX5090 and RTX4090. Datasets, metrics, and hyperparameters. For the details about the datasets, metrics, and hyperparameters we used, please refer to Appendix A.3. Implementation. We implement SageAttention3 using CUTLASS [21] and CUDA, and implement SageBwd using OpenAI Triton [22]. Figure 9: Visible examples of video generation on HunyuanVideo (left) and image generation on Stable-Diffusion3.5 (right). Table 4: End-to-end speedup performance using SageAttention3 and SageBwd. (a) Inference latency using SageAttention3. (b) One iteration training latency using SageBwd. Model Original Sage Sage2 Sage3 Model Original SageBwd CogvideoX (2B) HunyuanVideo 64 489 55 257 46 240 27 164 Llama (8K) Llama (16K) 2.1 6.0 1.9 5.2 5.2 Efficiency and Effectiveness Kernel Speed. Fig. 4 and 5 show the kernel speed of SageAttention3 and baselines on RTX5090. We can see that SageAttention3 achieves 45 speedup over xformers. Fig. 6 and 7 show the forward+backward speed of SageBwd and baselines on RTX4090. It shows that SageBwd achieves 1.67 speedup at most than FlashAttention2 and higher speedup than FlashAttention2 implemented in Triton and xformers. speedup over FlashAttention2 and 811 End-to-end metrics loss of SageAttention3. In Table 2, we compare the end-to-end quality metrics on various models using SageAttention3 and other attention methods. The results demonstrate that SageAttention3 almost incurs almost no end-to-end quality loss across these models. End-to-end metrics loss of SageBwd. To evaluate the effectiveness of SageBwd on training tasks, we conduct two experiments. First, we fine-tune the base models of Qwen2.5 (3B) and Llama3.2 (1B) on GSM8K [23], DROP [24], MMLU [25], and HELLASWAG [26] datasets. Fig. 8 (b-e) shows the fine-tuning loss results, indicating that SageBwd perfectly aligns with BF16. Moreover, our evaluation of the fine-tuned models answer quality across multiple test datasets  (Table 3)  demonstrates that SageBwd achieves the same performance as BF16. Second, we conduct pre-training tasks on FineWebEdu [27] using Llama (400M) [28] model. Fig. 8 (a) shows the loss curve, indicating that while SageBwd can achieve loss convergence, its convergence speed is relatively slow. This limitation restricts its applicability in pretraining tasks. Visible example. Fig. 9 visualizes some comparative examples of video generation on HunyuanVideo and image generation on Stable-diffsion3.5 using SageAttention3. The results demonstrate that SageAttention3 maintains full generation quality. Additional visible examples are provided in Fig. 10, 11, 13, and 14 in the Appendix. End-to-end speedup. Table 4(a) and 4(b) summarize end-to-end inference and training latency improvements. The results show thatSageAttention3 (Table 4(a)) achieved about 3 (HunyuanVideo) and 2.4 Furthermore, SageBwd (Table 4(b)) accelerates the training of Llama (1B) by about 1.15 8K/16K token micro-batches on RTX4090. (CogVideoX) end-to-end inference generation speedups on RTX5090. using"
        },
        {
            "title": "6 Related Work",
            "content": "Recent works that utilize hardware features to accelerate attention computation methods mainly include the following: FlashAttention [29] introduces tiling to reduce the GPU memory I/O between global memory and on-chip SRAM, achieving significant speedup. FlashAttention2 [19] improves the parallelism and warp partition strategies. FlashAttention3 [30] exclusively optimizes the kernel speed on the Hopper GPUs. xformers [20] accelerates attention using dedicated CUDA kernels. SageAttention [11] and SageAttention2 [5] accelerate attention using quantization and some novel outlier smoothing techniques. RingAttention [31] extends FlashAttention to multi-GPU/Node environments. In these works, although FlashAttention3 proposes version of FP8 attention, it has failed to be applied to video generation models in plug-and-play way [5]. Moreover, the FP8 attention in FlashAttention3 does not support the backward pass, limiting its applicability to training tasks. Additionally, numerous efficient attention variants have emerged, including linear attention [32, 33, 34, 35, 36, 37] and sparse attention [38, 39, 40, 41, 42, 43, 2, 44, 45, 46, 47, 48]. Although these works represent promising research directions, they are orthogonal to our work."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we make two key contributions. Firstly, we design SageAttention3, the first microscaling FP4 attention for inference acceleration, achieving 1038 TOPS on RTX5090, which is 5 speedup than the fastest FlashAttention on RTX5090. Experiments show that SageAttention3 could accelerate various models with no end-to-end quality metrics degradation. Secondly, we introduce the first trainable 8-bit attention (SageBwd) for training acceleration and explore its feasibility in training tasks. We find that the 8-bit attention could achieve lossless performance in fine-tuning tasks, but currently has some limitations in pertaining tasks. Future Work. First, while SageBwd demonstrates faster performance than FP16 implementation, we observe noticeable gap between its current speed and theoretical upper bounds. This gap may be caused by suboptimal Triton kernel implementations, which we plan to further optimize. Second, and more importantly, investigating the application of low-bit attention in pretraining tasks presents promising research direction worthy of exploration."
        },
        {
            "title": "References",
            "content": "[1] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. [2] Huiqiang Jiang, YUCHENG LI, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir H. Abdi, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. MInference 1.0: Accelerating pre-filling for long-context LLMs via dynamic sparse attention. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [3] Jianfei Chen, Yu Gai, Zhewei Yao, Michael Mahoney, and Joseph Gonzalez. statistical framework for low-bitwidth training of deep neural networks. Advances in neural information processing systems, 33:883894, 2020. [4] NVIDIA. Nvidia rtx blackwell gpu architecture. https://images.nvidia.com/aem-dam/ Solutions/geforce/blackwell/nvidia-rtx-blackwell-gpu-architecture.pdf. [5] Jintao Zhang, Haofeng Huang, Pengle Zhang, Jia Wei, Jun Zhu, and Jianfei Chen. Sageattention2: Efficient attention with thorough outlier smoothing and per-thread int4 quantization. In International Conference on Machine Learning (ICML), 2025. [6] Bita Darvish Rouhani, Ritchie Zhao, Ankit More, Mathew Hall, Alireza Khodamoradi, Summer Deng, Dhruv Choudhary, Marius Cornea, Eric Dellinger, Kristof Denolf, et al. Microscaling data formats for deep learning. arXiv preprint arXiv:2310.10537, 2023. [7] Paulius Micikevicius, Dusan Stosic, Neil Burgess, Marius Cornea, Pradeep Dubey, Richard Grisenthwaite, Sangwon Ha, Alexander Heinecke, Patrick Judd, John Kamalu, et al. Fp8 formats for deep learning. arXiv preprint arXiv:2209.05433, 2022. [8] Maxim Milakov and Natalia Gimelshein. Online normalizer calculation for softmax. arXiv preprint arXiv:1805.02867, 2018. [9] NVIDIA. Parallel Thread Execution ISA Version 8.7. https://docs.nvidia.com/cuda/ pdf/ptx_isa_8.4.pdf, 2025. Accessed: 2025-05-16. [10] NVIDIA. Efficient gemm in cuda. https://docs.nvidia.com/cutlass/media/docs/ cpp/efficient_gemm.html, 2025. Accessed: 2025-05-16. [11] Jintao Zhang, Jia Wei, Pengle Zhang, Jianfei Chen, and Jun Zhu. Sageattention: Accurate 8-bit attention for plug-and-play inference acceleration. In The International Conference on Learning Representations, 2025. [12] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [13] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [14] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. In The Thirteenth International Conference on Learning Representations, 2025. [15] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Junkun Yuan, Kai Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng, Yang Li, Yanxin Long, Yi Chen, Yutao Cui, Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Zunnan Xu, Yangyu Tao, Qinglin Lu, Songtao Liu, Daquan Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, and Caesar Zhong. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 11 [16] Genmo Team. Mochi 1. https://github.com/genmoai/models, 2024. [17] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2023. [18] Stability AI. Introducing stable diffusion 3.5. https://stability.ai/news/ introducing-stable-diffusion-3-5, 2023. [19] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. In The Twelfth International Conference on Learning Representations, 2024. [20] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, Daniel Haziza, Luca Wehrstedt, Jeremy Reizenstein, and Grigory Sizov. xformers: modular and hackable transformer modelling library. https://github.com/facebookresearch/xformers, 2022. [21] NVIDIA. CUTLASS: CUDA Templates for Linear Algebra Subroutines and Solvers. GitHub repository, 2023. [22] Philippe Tillet, H. T. Kung, and David Cox. Triton: an intermediate language and compiler for tiled neural network computations. MAPL 2019, page 1019, New York, NY, USA, 2019. Association for Computing Machinery. [23] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [24] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. DROP: reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proc. of NAACL, 2019. [25] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021. [26] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019. [27] Anton Lozhkov, Loubna Ben Allal, Leandro von Werra, and Thomas Wolf. Fineweb-edu: the finest collection of educational content, 2024. [28] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [29] Tri Dao, Daniel Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with IO-awareness. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. [30] Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. In The Flashattention-3: Fast and accurate attention with asynchrony and low-precision. Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [31] Hao Liu, Matei Zaharia, and Pieter Abbeel. Ringattention with blockwise transformers for near-infinite context. In The Twelfth International Conference on Learning Representations, 2024. [32] Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. 12 [33] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention with performers. In International Conference on Learning Representations, 2021. [34] Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang, Jiashi Feng, and In Proceedings of the Shuicheng Yan. Metaformer is actually what you need for vision. IEEE/CVF conference on computer vision and pattern recognition, pages 1081910829, 2022. [35] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pages 51565165. PMLR, 2020. [36] Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, and Yiran Zhong. Lightning attention-2: free lunch for handling unlimited sequence lengths in large language models. arXiv preprint arXiv:2401.04658, 2024. [37] Songlin Yang, Jan Kautz, and Ali Hatamizadeh. Gated delta networks: Improving mamba2 with delta rule. arXiv preprint arXiv:2412.06464, 2024. [38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1001210022, 2021. [39] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Twins: Revisiting the design of spatial attention in vision transformers. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021. [40] Kunchang Li, Yali Wang, Gao Peng, Guanglu Song, Yu Liu, Hongsheng Li, and Yu Qiao. Uniformer: Unified transformer for efficient spatial-temporal representation learning. In International Conference on Learning Representations, 2022. [41] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2024. [42] Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, and Maosong Sun. Infllm: Training-free long-context extrapolation for llms with an efficient context memory. In First Workshop on Long-Context Foundation Models@ ICML 2024, 2024. [43] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models. In The International Conference on Learning Representations, 2024. [44] Shashanka Venkataramanan, Amir Ghodrati, Yuki Asano, Fatih Porikli, and Amir Habibian. Skip-attention: Improving vision transformers by paying less attention. In The Twelfth International Conference on Learning Representations, 2024. [45] Yizhao Gao, Zhichen Zeng, Dayou Du, Shijie Cao, Hayden Kwok-Hay So, Ting Cao, Fan Yang, and Mao Yang. Seerattention: Learning intrinsic sparse attention in your llms. arXiv preprint arXiv:2410.13276, 2024. [46] Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan Zhang, Boju Chen, Tianqi Wu, Hongyi Wang, Zixiao Huang, Shiyao Li, Shengen Yan, Guohao Dai, Huazhong Yang, and Yu Wang. Moa: Mixture of sparse attention for automatic large language model compression. arXiv preprint arXiv:2406.14909, 2024. [47] Haocheng Xi, Shuo Yang, Yilong Zhao, Chenfeng Xu, Muyang Li, Xiuyu Li, Yujun Lin, Han Cai, Jintao Zhang, Dacheng Li, et al. Sparse videogen: Accelerating video diffusion transformers with spatial-temporal sparsity. arXiv preprint arXiv:2502.01776, 2025. 13 [48] Jintao Zhang, Chendong Xiang, Haofeng Huang, Jia Wei, Haocheng Xi, Jun Zhu, and Jianfei Chen. Spargeattn: Accurate sparse attention accelerating any model inference. In International Conference on Machine Learning (ICML), 2025. [49] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. [50] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. [51] Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying Shan. Evalcrafter: Benchmarking and evaluating large video generation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2213922149, 2024. [52] Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou, Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi Lin. Exploring video quality assessment on user generated contents from aesthetic and technical perspectives. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2014420154, 2023. [53] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. [54] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. [55] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 75147528, 2021. [56] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Visible Comparison Examples Figure 10: Visible examples of image generation on Stable-Diffusion3.5. Figure 11: Visible examples of image generation on Flux. (a) Full-Precision (b) Two-level quantization (c) Direct quantization Figure 12: Visual comparison of different scale strategies for from CogVideoX. 15 (cid:101) Figure 13: Visible examples of video generation on CogVideoX. Figure 14: Visible examples of video generation on HunyuanVideo. 16 Fig. 10 and Fig. 11 show additional visual comparison examples of image generation tasks. Fig. 13 and Fig. 14 show more visual comparison examples of video generation tasks. A.2 Additional Kernel Speed Comparison Fig. 15 and Fig. 16 show the forward kernel speed of SageBwd. Fig. 17 and Fig. 18 show the backward kernel speed of SageBwd. SageBwd achieved 2x speed up than FlashAttention in the forward propagation. SageBwd achieved 1.21.6x speed up than FlashAttention in the backward propagation. Figure 15: Forward speed comparison between SageBwd and Baselines (RTX4090, headim=128). Figure 16: Forward speed comparison between SageBwd and Baselines (RTX4090, headim=64). Figure 17: Backward speed comparison between SageBwd and Baselines (RTX4090, headim=128). Figure 18: Backward speed comparison between SageBwd and Baselines (RTX4090, headim=64). 17 A.3 Datasets, Metrics, and Hyperparameters Datasets. Text-to-video models are evaluated using the open-sora [49] prompt sets. Text-to-image models are assessed on COCO annotations [50]. Language models are evaluated on GSM8K [23], DROP [24], MMLU [25], and HELLASWAG [26] datasets. End-to-end metrics. For text-to-text models, we use Accuracy (Acc.) and F1-Score (F1). For text-tovideo models, we evaluate the quality of generated videos on five metrics: CLIPSIM and CLIP-Temp (CLIP-T) [51] to measure the text-video alignment; (VQA-a) and (VQA-t) to assess the video aesthetic and technical quality, respectively; and Flow-score (FScore) for temporal consistency [52]. For text-to-image models, generated images are evaluated in three aspects: FID [53] and sFID [54] for fidelity evaluation, Clipscore (CLIP) [55] for text-image alignment, and ImageReward (IR) [56] for human preference. Accuracy metrics. We use three metrics to assess the accuracy of quantized attention output compared to attention output in full-precision O: First, we flatten and into vectors in the shape O2, Relative L1 distance: of 1 L1 = n. Then, Cosine similarity: CosSim = OO/ O)2. O2 (O / , Root mean square error: RM SE = (cid:112)(cid:80) (cid:80) (1/n) (cid:112)(cid:80) (cid:80) (cid:80) Hyperparameters. For pretraining tasks, we use 400M model with hidden size of 1024, 20 layers, an intermediate size of 3072, and 16 attention heads. The training uses learning rate of 1e-3 with linear decay over 1000 warmup steps, and each step processes 2M tokens. For finetuning tasks, we train for 700 steps using learning rate of 3e-5 with linear decay and 100 warmup steps with batch size of 32 on GSM8K dataset and 128 on MMLU, DROP, and HELLASWAG datasets. (cid:80) (cid:112) Figure 19: FP4 operand register layout - rows 0 and 8, thread 0-3, entries 0-15. Figure 20: FP32 accumulator register layout - rows 0 and 8, thread 0-3, entries 0-15. Figure 21: Permuted FP32 accumulator register layout - rows 0 and 8, thread 0-3, entries 0-15. A.4 Additional Experiments Table 510 show Qwen2.5 (1.5B), Qwen2.5 (3B), and Llama3.2 (3B) fine-tuning results on four datasets with five different random seeds. The average and standard deviation show SageBwd is highly consistent with BF16 across various random seeds. 18 Table 5: Comparison of SageBwd and BF16 performance on GSM8K and DROP across different seeds on Qwen2.5 (1.5B). Seed 42 233 1234 5678 1 Avg Std GSM8K DROP SageBwd BF SageBwd BF16 0.5133 0.5027 0.4973 0.5201 0.5049 0.5077 0.0090 0.5125 0.5042 0.4973 0.5208 0.5057 0.5081 0. 0.7316 0.7269 0.7329 0.7340 0.7278 0.7307 0.0032 0.7364 0.7295 0.7342 0.7332 0.7404 0.7348 0.0040 Table 6: Comparison of SageBwd and BF16 performance on MMLU and HellaSwag across different seeds on Qwen2.5 (1.5B). Seed 42 233 1234 5678 1 Avg Std MMLU HellaSwag SageBwd BF SageBwd BF16 0.5814 0.5746 0.5805 0.5736 0.5830 0.5786 0.0043 0.5873 0.5785 0.5836 0.5693 0.5823 0.5802 0. 0.9089 0.9082 0.9025 0.9112 0.9058 0.9073 0.0033 0.9065 0.9049 0.9047 0.9053 0.9075 0.9058 0.0012 Table 7: Comparison of SageBwd and BF16 performance on GSM8K and DROP across different seeds on Qwen2.5 (3B). Seed 42 233 1234 5678 1 Avg Std GSM8K DROP SageBwd BF SageBwd BF16 0.5982 0.5997 0.6156 0.6065 0.6171 0.6074 0.0001 0.6232 0.5974 0.6103 0.6012 0.6073 0.6079 0. 0.7800 0.7786 0.7786 0.7816 0.7813 0.7800 0.0000 0.7812 0.7812 0.7824 0.7853 0.7832 0.7827 0.0000 Table 8: Comparison of SageBwd and BF16 performance on MMLU and HellaSwag across different seeds on Qwen2.5 (3B). Seed 42 233 1234 5678 1 Avg Std MMLU HellaSwag SageBwd BF SageBwd BF16 0.9419 0.9405 0.9414 0.9430 0.9446 0.9423 0.0000 0.9402 0.9402 0.9429 0.9440 0.9434 0.9421 0. 0.6434 0.6431 0.6492 0.6531 0.6510 0.6480 0.0000 0.6425 0.6437 0.6492 0.6400 0.6454 0.6442 0.0000 19 Table 9: Comparison of SageBwd and BF16 performance on GSM8K and DROP across different seeds on Llama3.2 (1B). Seed 42 233 1234 5678 1 Avg Std GSM8K DROP SageBwd BF16 SageBwd BF16 0.2722 0.2661 0.2616 0.2684 0.2646 0.2666 0.0000 0.2547 0.2570 0.2873 0.2585 0. 0.2582 0.0003 0.6367 0.6456 0.6439 0.6372 0.6393 0.6405 0.0000 0.6447 0.6424 0.6352 0.6409 0.6441 0.6414 0.0000 Table 10: Comparison of SageBwd and BF16 performance on MMLU and HellaSwag across different seeds on Llama3.2 (3B). Seed 42 233 1234 5678 1 Avg Std MMLU HellaSwag SageBwd BF16 SageBwd BF16 0.4665 0.4646 0.4702 0.4580 0.4666 0.4652 0.0000 0.4705 0.4560 0.4757 0.4639 0. 0.4670 0.0000 0.8230 0.8327 0.8202 0.8232 0.8218 0.8242 0.0000 0.8319 0.8256 0.8243 0.8276 0.8236 0.8266 0."
        }
    ],
    "affiliations": [
        "Tsinghua University"
    ]
}