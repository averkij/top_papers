{
    "paper_title": "Chirpy3D: Continuous Part Latents for Creative 3D Bird Generation",
    "authors": [
        "Kam Woh Ng",
        "Jing Yang",
        "Jia Wei Sii",
        "Jiankang Deng",
        "Chee Seng Chan",
        "Yi-Zhe Song",
        "Tao Xiang",
        "Xiatian Zhu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper, we push the boundaries of fine-grained 3D generation into truly creative territory. Current methods either lack intricate details or simply mimic existing objects -- we enable both. By lifting 2D fine-grained understanding into 3D through multi-view diffusion and modeling part latents as continuous distributions, we unlock the ability to generate entirely new, yet plausible parts through interpolation and sampling. A self-supervised feature consistency loss further ensures stable generation of these unseen parts. The result is the first system capable of creating novel 3D objects with species-specific details that transcend existing examples. While we demonstrate our approach on birds, the underlying framework extends beyond things that can chirp! Code will be released at https://github.com/kamwoh/chirpy3d."
        },
        {
            "title": "Start",
            "content": "Chirpy3D: Continuous Part Latents for Creative 3D Bird Generation Jing Yang2 Jiankang Deng4 Chee Seng Chan3 Yi-Zhe Song1 Tao Xiang1 Xiatian Zhu1 Kam Woh Ng1 Jia Wei Sii3 5 2 0 2 J 7 ] . [ 1 4 4 1 4 0 . 1 0 5 2 : r 1University of Surrey 2University of Cambridge 3Universiti Malaya 4Imperial College London Figure 1. Generated chirpy 3D birds composed of diverse parts. Our Chirpy3D learns part latent space from raw 2D images and can generate high-quality creative 3D birds by exploring the part latent space. (Top) Existing species. (Bottom) Novel species."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction In this paper, we push the boundaries of fine-grained 3D generation into truly creative territory. Current methods either lack intricate details or simply mimic existing objects we enable both. By lifting 2D fine-grained understanding into 3D through multi-view diffusion and modeling part latents as continuous distributions, we unlock the ability to generate entirely new, yet plausible parts through interpolation and sampling. self-supervised feature consistency loss further ensures stable generation of these unseen parts. The result is the first system capable of creating novel 3D objects with species-specific details that transcend existing examples. While we demonstrate our approach on birds, the underlying framework extends beyond things that can chirp! Code will be released at https: //github.com/kamwoh/chirpy3d. This paper tackles the problem of fine-grained creative 3D asset generation. We differ significantly from prior art on what we call fine-grained, and the extent of being creative. Current methods [24, 52] either generate coarse 3D assets that lack intricate details, or are less creative, merely mimicking existing objects [2, 19, 66]. Instead, we push both boundaries simultaneously maintaining intricate details while enabling true creativity in generating entirely new objects. For the first time, the result is the ability to create novel birds in 3D that do not exist in the real world, as shown in Fig. 1 (feel free to name them!). stark gap exists between the maturity of fine-grained understanding in 2D and 3D domains. While 2D methods [1, 39] routinely handle intricate species-specific details, e.g., differentiating subtle variations between warbler species, 3D generation remains largely confined to reconfiguring rigid objects like chairs and tables [20, 29, 61]. We 1 therefore make it our first objective to narrow this gap by lifting fine-grained objects from 2D to 3D, demonstrating the first successful generation of species-specific 3D birds as shown in Fig. 1. Beyond lifting fine-grained objects into 3D, we enable unprecedented creative control through part-level manipulation. While recent advances in 2D generation focus on swapping or recombining existing parts [39], we enable creativity at the part level itself generating entirely new, yet plausible parts through interpolation that maintains intricate details crucial for fine-grained domains. This dual innovation in both fine-grained detail and creative capability enables the generation of truly novel 3D objects that are both intricate and unprecedented. Our key technical contribution is conceptually straightforward yet practically significant: we lift 2D fine-grained understanding into 3D space through multi-view diffusion. We discover that fine-tuning MVDream [52] with 2D fine-grained images alone preserves its multi-view consistency prior, enabling direct NeRF [37] optimization through Score Distillation Sampling (SDS) loss [44]. To further condition part-level creative control, we optimize the model with multiple objectives: diffusion loss and attention loss for part-wise disentanglement. For efficient training, we incorporate LoRA layers [21] into cross-attention layers of the U-Net of MVdream. Extending the extent of being creative poses significantly more technical challenges. Our first key innovation is learning part latent space, modeled as Gaussian distribution, capturing continuous variance among training species rather than being limited to discrete choices like prior work (e.g., PartCraft [39]). This enables both interpolation between species to create hybrids and random sampling to generate entirely new species. Our second innovation tackles the fundamental challenge of generating unseen parts through novel self-supervised approach. Our key insight is that pre-trained diffusion models have an implicit ability to reason about correspondence between two images due to the weak supervision of the natural language during pre-training [25, 30, 56, 62, 67, 68]. We therefore introduce self-supervised loss enforcing cross-attention feature map similarity during denoising which explicitly enforces the correspondence of unseen parts. We find this loss reduces visual artifacts during denoising. Last but not least, while we demonstrate our approach primarily through bird generation challenging fine-grained domain with rich part variations the technical innovations we introduce are domain-agnostic and applicable to any fine-grained generation task. Our contributions are two-fold: (i) We present the first method to lift fine-grained objects from 2D to 3D while maintaining intricate details, achieved through fine-tuning MVDream with 2D images while preserving multi-view consistency; (ii) We introduce novel framework for creative 3D generation through two key technical innovations: modeling part latents as continuous distributions to enable interpolation and sampling of new parts, and selfsupervised feature consistency loss that ensures stable generation of unseen parts. Extensive experiments demonstrate our methods ability to generate high-quality 3D objects with unprecedented fine-grained details and creative freedom. 2. Related work Fine-Grained Generation. Research in fine-grained image generation often centers on 2D domains for recognition [5, 10, 38, 59, 69] and object generation [6, 9, 32, 33, 54, 64]. In contrast, 3D modeling has mostly focused on simpler categories (e.g., chairs, cars) [14, 16, 17, 23, 42, 53, 65]. Fine-grained objects, such as birds and dogs, present challenges due to their intricate texture details, and acquiring them as 3D data is often costly. Our work addresses this gap by lifting 2D fine-grained categories to 3D, leveraging MVDreams multi-view consistency and SDS loss to achieve high-quality 3D outputs from 2D image data. Diffusion Models for Novel View Synthesis. 2D diffusion models face challenges in maintaining consistent views across perspectives, known as the Janus problem. Methods [24, 52] such as MVDream generate multi-views given text prompt, while Zero123 and related works [28, 34, 35, 51, 60, 63] infer new views from single images. Our model adopts MVDreams text-to-multi-view approach to generate compositional objects with embedded part knowledge, using SDS loss [44] to create coherent 3D models without explicit 3D ground truth. Part-Aware Object Generation. Part-aware generation In 2D, Break-Amethods differ for 2D and 3D outputs. Scene [1] and PartCraft [39] rely on attention-based losses and token compositions for part-aware synthesis. In 3D, generation is often limited to categories with explicit 3D part annotations, such as ShapeNet [4, 20, 29, 42, 61]. Our approach circumvents the need for annotated 3D data by using MVDreams multi-view priors to achieve part-aware generation in continuous latent space, allowing flexible interpolation and sampling to generate new objects with multi-views. Novel Object Creation. Creativity in machine learning encompasses diverse methods, with GANs [12, 15, 40, 50] and VAEs [7, 8] combining components for novel object generation. Concept Decomposition [57], which recombines fine-grained elements with diffusion models, and ConceptLab [48] and TP2O [31], which mix concepts or embeddings to discover new ideas. Our method establishes partspecific latent space within each category, enabling novel object generation by sampling fine-grained parts for creative and diverse outputs. 2 Figure 2. Overall architecture of our Chirpy3D. (Top) During training, we fine-tune text-to-multi-view diffusion model (e.g., MVDream) with only 2D images of birds. We aim to learn the underlying part information by modeling continuous part-aware latent space. This is achieved by learning set of species embeddings e, project them into part latents through learnable , decode into word embeddings through learnable and insert into text prompt. We train the diffusion model with diffusion loss (Eq. 5) and multiple loss objectives Lreg (Eq. 2) to model part latents as Gaussian distribution, Lattn (Eq. 6) for part disentanglement, and our proposed Lcl (Eq. 4) to enhance visual coherency. and are trainable modules. For efficient training, we added LoRA layers into cross-attention layers of the U-Net. (Bottom) During inference, we can first preview multi-view images by selecting desired part latents as condition before turning them into 3D representations (e.g., NeRF) through SDS loss LSDS. 3. Methodology 3.1. Continuous Part-Aware Latent Space To achieve fine-grained 3D generation, we aim to bridge the gap between the detailed understanding achieved in 2D domains and the current limitations in 3D asset creation. By leveraging the multi-view consistency of pre-trained multi-view diffusion model MVDream, this allows us to effectively lift fine-grained 2D images into the 3D space, setting the stage for both detailed and creative 3D object generation. Our solution is to design continuous part-aware latent space that enables flexible, generative, and interpolative representation of object parts. By leveraging this latent space, our Chirpy3D empowers users to create novel objects interpolating between and combining unseen part compositions, vastly expanding generative creativity. In the following section, we outline our approach to constructing this latent space, ensuring visual coherency in unseen part generation, and optimizing the model for diverse applications in 3D object synthesis. To enable generative creativity and to unearth unseen parts, we build continuous part-aware latent space that models each object as composition of interchangeable parts, unlike conventional methods [39] which encode object parts as discrete tokens limiting generative variation within parts. Given collection of images within broad category (e.g., birds), containing fine-grained subcategories representing individual species (e.g., CUB-200-2011 [58] dataset), our first step is to fine-tune multi-view diffusion model (i.e., MVDream [52]) to generate these birds with part-control and within multi-view context. Specifically, each species in the dataset is represented by learnable species-level embeddings {ec RDe}C c=1, capturing its unique characteristics. For each image xc in the dataset, which belongs to species c, we divide it into distinct parts and obtain its part segmentation maps {Sm}M m=1 through unsupervised part discovery [39]. Each part is treated as an independent generative unit and is reprem RDt. To generate these sented by word embedding tc 3 (cid:80) lc (cid:80)(lc = 1 and σ2 where µm = 1 µm)2. The sampled latent (cid:101)lm is then input to to produce textual embedding (cid:101)tm. However, without direct image supervision for these novel concepts (cid:101)tm, the fine-tuned MVDream tends to generate unnatural or distorted images during part latents sampling. We hypothesize that this occurs because the diffusion model has not encountered these specific latents during training, making it challenging to guide the denoising process accurately. Figure 3. As we do not have images of unseen part latents, we use real natural images as our proxy. We extract cross-attention feature maps of two noised latents, then minimize the discrepancy between the two feature maps. This will encourage the model to compute similar feature maps for any given part latents, which indirectly stabilizes the denoising process for unseen latents. part-specific embeddings, we project the species-level embedding ec through learnable function , obtaining partm RDl }M specific latent vectors {lc m=1, and De Dl. denote the dimension of the vector where {e, t, l}. By learnable decoder g, these part latents are further that condition the difmapped into textual embeddings tc fusion model to focus on each part: 1, ..., lc lc = (ec), 1, ..., tc tc = g([lc m, PEm]), (1) where PEm RDl is learnable positional embedding to distinguish between parts, [, ] means concatenation operation, is simple one layer MLP and is two-layer MLP. Further, we assume the prior distribution of the part latent to be zero-mean multivariate-Gaussian with spherical covariance σ2I. Therefore, we apply regularization loss on the part latents to stabilize the generative space, ensuring smooth interpolation between parts: Lreg = Ec,m l2(cid:105) (cid:104) . 1 σ2 (2) In practice, we set σ2 = 1 to simplify the loss. Remark. Instead of learning individual part latent separately, we use an encoder to project species embedding into its part latents, in this way the part latents are entangled with the species embedding, so that they know the roles of each other. We find that this enables faster and more stable training empirically. 3.2. Visual Coherency for Novel Generation Novel parts are generated by sampling from latent space of l. To generate an unseen part m, we sample its latent via: (cid:101)lm (µm, σ2 m), (3) 4 To address this, we expose the model to those sampled latents during training. When provided with textual embedding tm, the U-Net in diffusion model generates similar feature maps regardless of sampled noise [30, 62, 67]. This is because it was already trained using different sampled noises on tm to denoise, implicitly aligning the feature maps to be similar. We implement this by extracting cross-attention feature maps from multiple layers, which directly influence image content. We introduce feature consistency loss Lcl, that minimizes the L2-distance between feature maps at different noise levels: Lcl = Et,ϵ,L (cid:88) i,j Fϵi Fϵj 2 . (4) Here, i, represent different random noises at the same timestep t, and represents various layers in the U-Net. See Figure 3 for illustration. Incorporating this self-supervised regularization during training enhances the feature consistency of generated outputs across noise levels and viewpoints, which is crucial for achieving coherent representations of unseen concepts. 3.3. Optimization To enforce part-wise disentanglement during generation, we adopt entropy-based attention loss proposed in [39]. This loss directs tc to its corresponding spatial location in crossattention maps during the fine-tuning of MVDream. Our objectives are defined as follows: Ldiff = Ex,y,t,ϵ Lattn = Ez,t,m (cid:2)ϵ ϵθ(zt; y, t)2 (cid:2) Sm log ˆAm (cid:3), (cid:3) , (5) (6) where ˆAm,i,j = Am,i,j (cid:80)M k=1 Ak,i,j , Am = 1 L (cid:88) Al,m, 1,...,tc where Ldiff is the diffusion loss, with zt as the noised latent representation of at timestep and as the text prompt, structured as tc bird. Lattn is the cross-entropy loss between normalized cross-attention map of part and its binary segmentation mask Sm predicted using the part segmentation module from [39]. Here, Al,m denotes the cross-attention map selected from layer-l indicating correlation between part and noisy latent zt. Am represents (see Figure 10), making this method effective across both NeRF [37, 44] and 3DGS [26, 55] representations. 4. Experiment Datasets. We run experiments on fine-grained bird dataset: CUB-200-2011 [58] which contains 5,994 training images and 5,997 testing images, we use only training images as our training set. There are 200 species in the dataset and approximate 30 images per species. Implementation. We add LoRA [21] into cross-attention layers in MVDream and optimize LoRA parameters with single-view images only. Attention loss is applied on cross attention maps with sizes of 8 8 and 16 16. We train the model for 100 epochs, on batch size of 4 and learning rate of 0.0001, on single RTX 2080ti GPU. For 3D object generation, we use threestudio [18] as our framework and single RTX 3090 GPU. In our setup, De = 768, Dl = 42, Dt = 1024, is simple one-layer MLP and is two-layer MLP. Number of parts = 5 following PartCraft [39] (we exclude learning background part/token as we hope to maintain the grey background in multi-view generation). Part segmentation maps are obtained using the detector provided in PartCraft3. Baselines. We compare three models: (a) Textual Inversion, an adaptation of [13], learns set of part textual embeddings (i.e., learnable word embeddings where each embedding tc (b) PartCraft, which incorporates non-linear projector to further enhance word (c) Chirpy3D (ours), which modembeddings from (a). els the part distribution within latent space and generates textual embeddings through decoder. For fair comparison, all models are built on MVDream with rank-4 LoRA layers [21], using attention loss Lattn (Eq. 6) for part disentanglement. RDt). 4.1. Multi-view Subject Generation It is important to determine whether diffusion model has learned to generate the subject accurately. Hence, we evaluate how well it can generate the multi-view images by calculating the average pairwise cosine similarity between the CLIP [47]/DINO [3] embeddings of generated multiview images (treating them as 4 different images) and real species-specific images, following the approach in [49]. Figure 5 shows that PartCraft and our Chirpy3D effectively reconstruct the subject, while textual inversion cannot reconstruct well. The subject fidelity metrics in Table 1 show that PartCraft and Chirpy3D are comparable in terms of subject fidelity, establishing fair comparison for the next experiment. We also test FID and FIDCLIP between training and generated images, finding that our Chirpy3D outperforms PartCraft by around 4% in terms of FIDCLIP. 2We also ablated with Dl = 16, 32, 64 in supplementary material. 3https://github.com/kamwoh/partcraft Figure 4. (a) Seen part selection generation. Unseen part synthesis via (b) novel sampling and (c) interpolation. the averaged attention map for part across selected layers, while ˆAm,i,j is the normalized cross-attention map for part across parts. To reduce overfitting while fine-tuning MVDream on 2D images, we incorporate LoRA [21] layers into the crossattention layers, adjusting only these additional parameters to efficient fine-tuning. The final loss function is: = Ldiff + λattnLattn + λclLcl + λregLreg, (7) where λattn = 0.01, λcl = 0.001, and λreg = 0.0001. 3.4. Inference and Interpolation of Novel Parts Part-aware Multi-view Image Generation With to flexible approaches Chirpy3D, users have access for generating objects, leading to potentially endless possibilities through their own choices. We offer three primary methods for creating unique hybrids: (i) Seen part selection generation selects existing parts to create new instances with text prompt such as t 1, bird, (ii) Unseen part where parts from different species . synthesis via novel sampling achieves new hybrids in the latent space with text prompt such as (cid:101)t1, ..., (cid:101)tM bird, where sampling is done through Eq. 3. (iii) Unseen part synthesis via interpolation achieves new hybrids through interpolation where unique hybrid is created by blending part latents from different species: ..., (cid:101)tm = g([a lc1 + (1 a) lc2 m, PEm]) (8) where (cid:101)tm is the decoded textual embedding of interpolated part latents, and is the interpolation scale between species c1 and species c2. See Figure 4 for illustration. 3D Generation With Multi-view Score Distillation Our method, Chirpy3D, enables 3D object generation through SDS loss [44]1, allowing for: (i) generation of 3D objects for species, (ii) new 3D hybrids from seen parts, (iii) synthesis of 3D objects with unseen parts. Unlike general text-to-3d generation, where terms like bird encompass diverse forms, each token in Chirpy3D is fine-grained, producing highly similar objects for various noise inputs. (see generated images in Figure 11(b)). As result, we can apply SDS with lower guidance scale to prevent oversaturation 1We refer readers to the original paper for more details. 5 (a) Textual Inversion (b) PartCraft (c) Chirpy3D (Ours) Figure 5. Subject generation of 2 different species -blue jay, white pelican. Method DINO CLIP FID FIDCLIP Textual Inversion PartCraft Chirpy3D (Ours) 0.357 0.365 0.380 0.594 0.597 0.619 43.86 43.25 43.41 30.89 28.98 27."
        },
        {
            "title": "Method",
            "content": "Textual Inversion PartCraft Chirpy3D (Ours) 4.93 4.07 4.81 eH 138.2 58.6 123.2 Table 1. Subject fidelity metrics. Method EMR CoSim Textual Inversion PartCraft Chirpy3D (Ours) 0.210 0.291 0.295 0.691 0.722 0.724 Table 2. Part composition metrics. Part Composition. To evaluate the models ability to disentangle and combine parts, we conduct part composition by replacing one part in target image with the corresponding part from source image. Table 2 presents quantitative comparison based on two metrics proposed by [39]: Exact Matching Rate (EMR) and cosine similarity score (CoSim). Visual comparisons are shown in Figure 6. Among the methods, Textual Inversion shows the weakest performance, while PartCraft and our Chirpy3D perform comparably. Unlike Textual Inversion which learns directly within the textual embedding space, both PartCraft and our Chirpy3D use shared projector to map part representations into textual embeddings. This enhancement highlights the importance of incorporating interactions between multiple parts for improved part disentanglement and optimization. Linear Interpolation. Interpolation can be highly useful for editing. We explore the latent space by interpolating latent codes to generate each object progressively from left to right, as shown in Figure 7. Our observations are as follows: (i) textual inversion performs poorly in this setting. Since each learned word embedding is independent, interpolating between embeddings creates an unknown embedding, resulting in unnatural and glitchy images. (ii) PartCraft can handle interpolation without glitches, likely due to the embedding projector which may have learned robust manifold of word embeddings. However, it displays an abrupt switching effect, where sudden, significant change occurs at certain interpolation step. (iii) Chirpy3D is able to faithfully generate smooth and meaningful transition from one object to another, greatly benefiting from regularization loss Table 3. Entropy and the effective number of classes eH of top-5 retrieved classes using generated images from random part latents. Higher values indicate greater diversity. Chirpy3D (Ours) With Lcl Without Lcl Preference 82.5% 17.5% Table 4. User study to verify effectiveness of Lcl. Lreg, promoting smoothness. Random Sampling. To assess creativity of competitors, we sample from the part latent space, compose them to generate new objects, then compute entropy H, and effective number of classes eH , to compare generations against training samples to measure diversity and class distribution. Both of which capture how broadly or narrowly generated objects resemble classes within the training set. We start by extracting feature embedding of each generated multi-view image with DINO [3], then performing retrieval task to search similar training images for each view. We retrieve the top-5 species and calculate the frequency of each retrieved species. Next, we compute the entropy of these frequencies to quantify the diversity across the classes, where higher entropy indicates broader diversity. The effective number of classes can be simply calculated through eH , which reflects the diversity, in terms of how many classes would appear if they were uniformly represented. As shown in Table 3, our Chirpy3D model achieves higher effective class count than PartCraft, indicating greater diversity in generating novel species. Although Textual Inversion shows higher entropy, we attribute this to its lower-quality samples, as seen in Table 1. Qualitative results in Figures 8 and 9 support these findings: Textual Inversion often produces artifacts, PartCraft displays limited class diversity (primarily brownish birds), while Chirpy3D generates well-clustered DINO features and significantly broader range of diverse outputs. 6 Figure 6. Visual comparison of part composition. A, B, C, D, E, represent cardinal, wilson warbler, least auklet, california gull, horned lark, and song sparrow respectively. Red circles indicate changed parts. All generated (including sources & targets) by the same seed. (a) Textual Inversion (b) PartCraft (c) Chirpy3D (Ours) Figure 7. Linear interpolation of all part latents between two different species blue jay and cardinal. Only one view is shown. Our Chirpy3D achieves much smoother interpolation, unlike PartCraft exhibits an abrupt switch phenomenon after certain step (red box). (a) Textual Inversion (b) PartCraft (c) Chirpy3D (Ours) Figure 8. t-SNE embeddings of DINO features of generated images. Blue represents images of subject reconstruction; Orange represents images of novel generation. 4.2. 3D Generation with Multi-view SDS We show NeRF4 generation of subject generation, novel generation (by random sampling) and part composition in Figure 10. Note that all generations are optimized with standard guidance scale (i.e., 7.5). Due to low guidance scale, PartCraft and Textual Inversion couldnt optimize high-quality 3D objects for novel generation and part composition (e.g., smooth texture and small generation)5. Our Chirpy3D with Lcl can obtain much higher quality 3D object compared to without it, proving the importance of improving the visual coherency. 4.3. Further Analysis Effect of Lcl. It is worth noting that we do not have any training examples for unseen random samples, making it challenging for the model learn how to generate unseen samples given unseen embeddings. Figure 11 shows the comparison between our model with our proposed selfsupervised regularization loss Lcl, our model without it, and PartCraft. We observe that Lcl greatly enhances the visual coherency of different generation. Additionally, at4see supplementary material for 3DGS-based generation and image-to5Higher guidance scale can yield better 3D objects (e.g., larger out3D methods [63]. puts), but may introduce oversaturation effects. 7 (a) Textual Inversion (b) PartCraft (c) Chirpy3D (Ours) Figure 9. Generated images with random sampled latents/embeddings. Textual Inversion often produces images with artifacts due to the direct interpolation of word embeddings. PartCraft can generate images with fewer artifacts but lacks consistency. In contrast, our Chirpy3D generates novel images with greater diversity. (a) Ours without Lcl (b) Ours with Lcl Figure 11. All images are generated with the same camera pose but with different seeds on unseen latent. (a) Without our feature consistency loss Lcl, the generated images lack consistency (e.g., less artifact, and inconsistent visual feature) compared to (b). Figure 10. NeRF rendering of learned 3D objects. tributes generated by PartCraft lack consistency across different seeds (e.g., the wings patterns vary). We also conducted user study to verify that Lcl improves visual coherency, we generate 100 images (different seeds) of 10 existing species and 10 random samplings for models with- /without Lcl, asking 20 users to vote their preference. In Table 4, 82.5% prefer images generated by the model with Lcl. This verifies that it indeed improves visual coherency. 5. Discussion and Conclusion In this paper, we introduce Chirpy3D, framework for finegrained 3D generation that lifts 2D understanding into 3D through multi-view diffusion, modeling part latents as conFigure 12. hybrid (middle) between siberian husky (left) and papillon (right), trained with Stanford Dogs [27]. tinuous distributions and regularizing feature consistency to stabilize the generation of unseen parts. This approach enables the creation of novel 3D objects with unprecedented fine-grained details and creative freedom. While we demonstrate on birds, this approach can also apply to other objects6 such as dogs (see Figure 12). However, the models generalizability is currently limited by constraints in the base model, particularly in controlling lighting and object poses. Another limitation is that the learned part latent codes are not fully disentangled as each code comprises both structural and texture information. Future work could focus on both to further broaden Chirpy3Ds versatility in generative 3D modeling. 6See more at supplementary material."
        },
        {
            "title": "References",
            "content": "[1] Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel Cohen-Or, and Dani Lischinski. Break-a-scene: Extracting multiple concepts from single image. In SIGGRAPH Asia, 2023. 1, 2 [2] Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, and Kwan-Yee Wong. Dreamavatar: Text-and-shape guided 3d human avatar generation via diffusion models. In CVPR, 2024. 1 [3] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021. 5, 6 [4] Angel Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015. 2 [5] Dongliang Chang, Yifeng Ding, Jiyang Xie, Ayan Kumar Bhunia, Xiaoxu Li, Zhanyu Ma, Ming Wu, Jun Guo, and Yi-Zhe Song. The devil is in the channels: Mutual-channel loss for fine-grained image classification. TIP, 2020. 2 [6] Tianyi Chen, Yunfei Zhang, Xiaoyang Huo, Si Wu, Yong Xu, and Hau San Wong. Sphericgan: Semisupervised hyper-spherical generative adversarial netIn CVPR, works for fine-grained image synthesis. 2022. 2 [7] Celia Cintas, Payel Das, Brian Quanz, Girmaw Abebe Tadesse, Skyler Speakman, and Pin-Yu Chen. Towards creativity characterization of generative models via group-based subset scanning. In IJCAI, 2022. 2 [8] Payel Das, Brian Quanz, Pin-Yu Chen, Jae-wook Ahn, and Dhruv Shah. Toward neuro-inspired creative decoder. In IJCAI, 2020. [9] Rui Ding, Kehua Guo, Xiangyuan Zhu, Zheng Wu, and Liwei Wang. Comgan: unsupervised disentanglement and segmentation via image composition. 2022. 2 [10] Ruoyi Du, Dongliang Chang, Ayan Kumar Bhunia, Jiyang Xie, Zhanyu Ma, Yi-Zhe Song, and Jun Guo. Fine-grained visual classification via progressive multi-granularity training of jigsaw patches. In ECCV, 2020. 2 [11] Ruoyi Du, Dongliang Chang, Timothy Hospedales, Demofusion: Yi-Zhe Song, and Zhanyu Ma. Democratising high-resolution image generation with no $$$. In CVPR, 2024. [12] Ahmed Elgammal, Bingchen Liu, Mohamed Elhoseiny, and Marian Mazzone. Can: Creative adversarial networks generating art by learning about styles and deviating from style norms. In ICCC, 2017. 2 [13] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. 5 [14] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja Fidler. Get3d: generative model of high quality 3d textured shapes learned from images. In NeurIPS, 2022. [15] Songwei Ge, Vedanuj Goswami, C. Lawrence Zitnick, and Devi Parikh. Creative sketch generation. In ICLR, 2021. 2 [16] Kyle Genova, Forrester Cole, Daniel Vlasic, Aaron Sarna, William Freeman, and Thomas Funkhouser. Learning shape templates with structured implicit functions. In ICCV, 2019. 2 [17] Kyle Genova, Forrester Cole, Avneesh Sud, Aaron Sarna, and Thomas Funkhouser. Local deep implicit functions for 3d shape. In CVPR, 2020. 2 [18] Yuan-Chen Guo, Ying-Tian Liu, Ruizhi Shao, Christian Laforte, Vikram Voleti, Guan Luo, Chia-Hao Chen, Zi-Xin Zou, Chen Wang, Yan-Pei Cao, and Song-Hai Zhang. threestudio: unified framework for 3d content generation. https://github. com/threestudio-project/threestudio, 2023. 5, 12 [19] Xiao Han, Yukang Cao, Kai Han, Xiatian Zhu, Jiankang Deng, Yi-Zhe Song, Tao Xiang, and KwanYee Wong. Headsculpt: Crafting 3d head avatars with text. In Advances in Neural Information Processing Systems, 2024. 1 [20] Amir Hertz, Or Perel, Raja Giryes, Olga SorkineHornung, and Daniel Cohen-Or. Spaghetti: Editing implicit shapes through part aware generation. TOG, 2022. 1, [21] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In ICLR, 2022. 2, 5, 12 [22] Yukun Huang, Jianan Wang, Yukai Shi, Xianbiao Qi, Zheng-Jun Zha, and Lei Zhang. Dreamtime: An improved optimization strategy for text-to-3d content creation. arXiv preprint arXiv:2306.12422, 2023. 12 [23] Ka-Hei Hui, Ruihui Li, Jingyu Hu, and Chi-Wing Fu. Neural template: Topology-aware reconstruction and disentangled generation of 3d meshes. In CVPR, 2022. 2 [24] Yash Kant, Aliaksandr Siarohin, Ziyi Wu, Michael Vasilkovsky, Guocheng Qian, Jian Ren, Riza Alp Guler, Bernard Ghanem, Sergey Tulyakov, and Igor Gilitschenski. Spad: Spatially aware multi-view diffusers. In CVPR, 2024. 1, 2 9 [25] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In CVPR, 2024. 2 Thomas [26] Bernhard Kerbl, Georgios Kopanas, Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM ToG, 2023. [27] Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Fei-Fei Li. Novel dataset for finegrained image categorization: Stanford dogs. In CVPRW, 2011. 8, 13 [28] Xin Kong, Shikun Liu, Xiaoyang Lyu, Marwan Taher, Xiaojuan Qi, and Andrew Davison. Eschernet: generative model for scalable view synthesis. In CVPR, 2024. 2 [29] Juil Koo, Seungwoo Yoo, Minh Hieu Nguyen, and Minhyuk Sung. Salad: Part-level latent diffusion for 3d shape generation and manipulation. In ICCV, 2023. 1, 2 [30] Alexander Li, Mihir Prabhudesai, Shivam Duggal, Ellis Brown, and Deepak Pathak. Your diffusion model is secretly zero-shot classifier. In ICCV, 2023. 2, 4 [31] Jun Li, Zedong Zhang, and Jian Yang. Tp2o: Creative text pair-to-object generation using balance swapsampling. In ECCV, 2024. 2 [32] Yuheng Li, Krishna Kumar Singh, Utkarsh Ojha, and Yong Jae Lee. Mixnmatch: Multifactor disentanglement and encoding for conditional image generation. In CVPR, 2020. [33] Yuheng Li, Krishna Kumar Singh, Yang Xue, and Yong Jae Lee. Partgan: Weakly-supervised part decomposition for image generation and segmentation. In BMVC, 2021. 2 [34] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. In Zero-1-to-3: Zero-shot one image to 3d object. ICCV, 2023. 2 [35] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d using cross-domain diffusion. In CVPR, 2024. 2 [36] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2018. 12 [37] Mildenhall, PP Srinivasan, Tancik, JT Barron, Ramamoorthi, and Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020. 2, [38] Kam Woh Ng, Xiatian Zhu, Yi-Zhe Song, and Tao Xiang. Concepthash: Interpretable fine-grained hashing via concept discovery. In CVPRW, 2024. 2 [39] Kam Woh Ng, Xiatian Zhu, Yi-Zhe Song, and Tao Xiang. Partcraft: Crafting creative objects by parts. In ECCV, 2024. 1, 2, 3, 4, 5, 6, 13 [40] Amin Heyrani Nobari, Muhammad Fathy Rashad, and Faez Ahmed. Creativegan: Editing generative adversarial networks for creative design synthesis. arXiv preprint arXiv:2103.06242, 2021. 2 [41] OpenAI. Chatgpt (november 2024 version). https: //openai.com, 2024. Large language model. 13 [42] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In CVPR, 2019. [43] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. 13 [44] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In ICLR, 2022. 2, 5 [45] Zhiyu Qu, Tao Xiang, and Yi-Zhe Song. Sketchdreamer: Interactive text-augmented creative sketch ideation. In BMVC, 2023. [46] Zhiyu Qu, Lan Yang, Honggang Zhang, Tao Xiang, Kaiyue Pang, and Yi-Zhe Song. Wired perspectives: Multi-view wire art embraces generative ai. In CVPR, 2024. [47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 5 [48] Elad Richardson, Kfir Goldberg, Yuval Alaluf, and Daniel Cohen-Or. Conceptlab: Creative generation using diffusion prior constraints. arXiv preprint arXiv:2308.02669, 2023. [49] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In CVPR, 2023. 5 [50] Othman Sbai, Mohamed Elhoseiny, Antoine Bordes, Yann LeCun, and Camille Couprie. Design: Design In ECCVW, inspiration from generative networks. 2019. 2 [51] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su. Zero123++: single image to consistent multi-view diffusion base model, 2023. 2, 12 [52] Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Ke10 [65] Jing Yang, Kyle Fogarty, Fangcheng Zhong, and Cengiz Oztireli. Sym3d: Learning symmetric triplanes arXiv preprint for better 3d-awareness of gans. arXiv:2406.06432, 2024. [66] Taoran Yi, Jiemin Fang, Junjie Wang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Qi Tian, and Xinggang Wang. Gaussiandreamer: Fast generation from text to 3d gaussians by bridging 2d and 3d diffusion models. In CVPR, 2024. 1 [67] Junyi Zhang, Charles Herrmann, Junhwa Hur, Luisa Polania Cabrera, Varun Jampani, Deqing Sun, and Ming-Hsuan Yang. tale of two features: Stable diffusion complements dino for zero-shot semantic correspondence. In NeurIPS, 2024. 2, 4 [68] Wenliang Zhao, Yongming Rao, Zuyan Liu, Benlin Liu, Jie Zhou, and Jiwen Lu. Unleashing textto-image diffusion models for visual perception. In ICCV, 2023. 2 [69] Heliang Zheng, Jianlong Fu, Zheng-Jun Zha, and Jiebo Luo. Looking for the devil in the details: Learning trilinear attention sampling network for finegrained image recognition. In CVPR, 2019. 2 jie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. In ICLR, 2023. 1, 2, 3, 12, 14 [53] Yawar Siddiqui, Antonio Alliegro, Alexey Artemov, Tatiana Tommasi, Daniele Sirigatti, Vladislav Rosov, Angela Dai, and Matthias Nießner. Meshgpt: Generating triangle meshes with decoder-only transformers. In CVPR, 2024. 2 [54] Krishna Kumar Singh, Utkarsh Ojha, and Yong Jae Lee. Finegan: Unsupervised hierarchical disentanglement for fine-grained object generation and discovery. In CVPR, pages 64906499, 2019. [55] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian In ICLR, splatting for efficient 3d content creation. 2024. 5, 12 [56] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath Hariharan. Emergent correspondence from image diffusion. 2023. 2 [57] Yael Vinker, Andrey Voynov, Daniel Cohen-Or, and Ariel Shamir. Concept decomposition for visual exploration and inspiration. In SIGGRAPH Asia, 2023. 2 [58] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds200-2011 dataset. 2011. 3, 5 [59] Xiu-Shen Wei, Yi-Zhe Song, Oisin Mac Aodha, Jianxin Wu, Yuxin Peng, Jinhui Tang, Jian Yang, and Serge Belongie. Fine-grained image analysis with deep learning: survey. TPAMI, 2021. 2 [60] Kailu Wu, Fangfu Liu, Zhihan Cai, Runjie Yan, Hanyang Wang, Yating Hu, Yueqi Duan, and Kaisheng Ma. Unique3d: High-quality and efficient 3d mesh generation from single image. arXiv preprint arXiv:2405.20343, 2024. 2 [61] Zhijie Wu, Xiang Wang, Di Lin, Dani Lischinski, Daniel Cohen-Or, and Hui Huang. Sagnet: Structureaware generative network for 3d-shape modeling. TOG, 2019. 1, [62] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello. Openvocabulary panoptic segmentation with text-to-image diffusion models. In CVPR, 2023. 2, 4 [63] Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying Shan. Instantmesh: Efficient 3d mesh generation from single image with sparsearXiv preprint view large reconstruction models. arXiv:2404.07191, 2024. 2, 7, 12 [64] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Fine-grained text to image generation with attentional generative adversarial networks. In CVPR, 2018. 2 11 A. Implementation Details B.2. 3DGS recipe We conducted training on single GeForce RTX 2080ti GPU with batch size of 4 over 100 epochs, taking around 8GB VRAM. AdamW [36] optimizer was employed with constant learning rate of 0.0001 and weight decay of 0.01. Only random horizontal flip augmentation is used. 256 256 image resolution is applied, following the trained resolution in MVDream [52]. We adopted the LoRA design [21] from diffusers library7, in which the low-rank adapters were added to the QKV and out components of all cross-attention modules. Regarding the attention loss Lattn (see Eq. 6), we selected cross-attention maps with feature map size of 8 8 and 16 16 (given input resolution of 256 256). For 3D generation, we run with single RTX 3090. The SDS loss LSDS we used is MVDreams [52] x0reconstruction loss with CFG rescale of 0.3. This implementation is from MVDream-threestudio 8. B. 3D Generation B.1. NeRF recipe For NeRF-based 3D generation, we use threestudio [18] as our framework and MVDream-threestudio as plugin. We implemented custom prompt processor to handle the input prompt, as we replace the word embeddings using Eq. 1. After tokenization and before passing the word embeddings, we substitute the placeholders word embedding with our computed word embedding, t. An example prompt with placeholders is formatted as [part1] ... [partM ] bird. We use all default settings, except for setting the number of samples per ray to 256 to prevent out-of-memory errors and disabling background color augmentation. The guidance scale is set to 7.5 by default. Different timesteps affect the SDS optimization process [22]. Lower timesteps (less noise) emphasize detailed textures, while higher timesteps (more noise) focus on coarse structure. During SDS loss optimization, the timestep is randomly sampled within specified minimum and maximum range. We set the minimum timestep range to decay from 0.98 to 0.02 over 3,000 steps and the maximum timestep range to decay from 0.98 to 0.3 over 8,000 steps. We observe that bird structures begin forming around 1,000 steps. Therefore, we quickly decay the minimum timesteps to 0.02 to allow the model to focus on texture details during the early stages of training. 7https://github.com/huggingface/diffusers/blob/ main/examples/text_to_image/train_text_to_image_ lora.py 8https : / / github . com / bytedance / MVDream - threestudio For 3DGS-based generation, we use DreamGaussians [55] official implementation, with the training strategy used in NeRF recipe. Figure 13. Optimization-based 3D generation with NeRF or 3DGS. Figure 13 shows that our Chirpy3D can be used in both NeRF and 3DGS-based 3D generation. B.3. InstantMesh Figure 14. Image-to-3D using front view and side view of generated object. We also present additional results in Figure 14, where generated images are processed through InstantMesh [63] to directly obtain 3D objects. While it provides fast inference, it occasionally produces incorrect conversions as it relies on Zero123Plus [51] to predict six views. This limitation arises because Zero123Plus may not have encountered the specific object (or similar objects) during training, leading to inaccurate view predictions. Fine-tuning an image-tomulti-view model typically requires 3D ground-truth data, which is why we focus on text-to-multi-view generation for 12 ical covariance matrix σ2I, the probability density function is expressed as: p(x) = 1 2πσ2 (cid:18) exp µ2 2σ2 (cid:19) . (9) Since we assume zero-mean Gaussian distribution, i.e., bmmu = 0, the expression simplifies to: p(x) = 1 2πσ2 (cid:18) exp (cid:19) . x2 2σ2 Taking the logarithm of p(x) yields: log p(x) = log (cid:18) 1 2πσ2 (cid:19) (cid:18) + (cid:19) x2 2σ2 (10) (11) Maximizing log p(x) is therefore equivalent to minimizing the term: Lreg = x2 2σ2 (12) (a) Mixing chow with golden retriever, pomeranian with pug. (b) Hamster-cat, Hamster-horse, Elephant-horse Figure 15. (a) Dog generation. (b) Animal generation. In practice, we set σ2 = 1 for simplicity, leading to the regularization loss defined in Eq. (2) of the main paper. fine-grained 3D generation. However, with the 3D objects obtained through our method, it may be possible to finetune an image-to-multi-view model without relying on 3D ground-truth dataan avenue worth exploring in future research. C. Other domains In this section, we demonstrate that our method can be applied to other domains, such as dog and animal generation. In Figure 15 (a), we generate images of existing dog breeds and their hybrids using model trained on Stanford Dogs [27]. Remarkably, our method produces smooth and plausible hybrids between existing species, showcasing its versatility. For animal generation, we present proof-of-concept experiment. Using ChatGPT [41], we generated list of 30 common animal names. We then used SDXL [43] to generate 50 images per animal, resulting in total of 1,500 training images. Following the procedure outlined in PartCraft [39], we segmented two simple partshead and body. In Figure 15 (b), we showcase creative part compositions, highlighting the potential of our method for novel and imaginative animal generation. D. Derivation of regularization loss We use the symbol to represent the part latent for clarity. Assuming multivariate Gaussian distribution with spher13 E. Multi-view generation on common token and fine-grained token (a) bird, 3d asset. (b) cardinal, 3d asset. Figure 16. Multi-view generation with text prompt through MVDream [52]. The guidance scale is 7.5. Each row is different seed. (a) The generation varies for different seeds for the token bird. (b) The generation with fine-grained token cardinal. As highly similar objects are generated for each seed, we can use lower guidance scale for SDS loss and enable 3D generation without oversaturated effect. F. Multi-view generation on existing species (a) Textual Inversion (b) PartCraft (c) Chirpy3D (Ours) (d) Figure 17. Multi-view generation on existing species, trained with respective methods (a, b, c). (d) One of the training images of the species. Not only our Chirpy3D (c) can reconstruct well in multi-view perspective comparing to Textual Inversion (a) and PartCraft (b), but our generated images are also consistent in terms of orientation and cleaner background. 15 G. Multi-view generation on novel species (random sampling) (a) Textual Inversion (b) PartCraft (c) Chirpy3D (Ours) Figure 18. Multi-view generation on novel species (random sampling), trained with respective methods. All were generated with the same seed but with different sampled part latents. (a) Trained with Textual Inversion, the generated images are often incomprehensible, indicating that direct sampling from word embedding space is insufficient to generate novel species. (b) PartCraft has non-linear projector to project word embeddings, while able to generate comprehensible objects, but lacking diversity since it is not trained to have continuous distribution of part latents. (c) Our Chirpy3D not only can generate images of diverse species, also stable in terms of bird pose. 16 H. Multi-view generation on novel species (interpolation) (a) Textual Inversion (b) PartCraft (c) Chirpy3D (Ours) Figure 19. Multi-view generation on novel species (interpolation) trained with respective methods. All images were generated using the same seed, but with different interpolated part latents. (a) Trained with Textual Inversion, the generated images are often incomprehensible, consistent with previous visualizations. (b) PartCraft exhibits an abrupt switching effect, with the object remaining unchanged before and after switching, as it is not designed to support continuous distribution of part latents. (c) Our Chirpy3D method successfully generates smooth interpolated samples. 17 I. Qualitative comparisons of visual coherency before and after applying feature consistency loss (a) (b) Figure 20. Each row is different seed with random sampled part latents (a) before applying Lcl and (b) after applying. We can see that, although the part latents are unseen during training, applying the loss can increase visual coherency and less artifacts. J. Further Analysis Figure 21. Inverse part latents used to reconstruct the image with 1,000 learning steps. Inversion experiment. Once trained, we can perform code inversion on input images. Figure 21 shows the results of an experiment where part latents were inverted given an input image. Textual Inversion can reconstruct the input but struggles with texture quality. PartCraft achieves reconstruction but exhibits artifacts, possibly due to the ambiguity of input samples (e.g., camouflage birds). In contrast, our method successfully reconstructs the input with accurate size and significantly improved texture quality. images, it successfully transfers to multi-view image generation, leveraging its learned prior. The cross-attention map highlights strong correlation between the image and the tokens of the corresponding parts, demonstrating clear disentanglementeach part avoids attending to incorrect spatial locations, thanks to the part attention loss. Furthermore, when parts of the image are occluded (e.g., the tail is occluded when facing forward), the model attends to the background rather than incorrectly focusing on other parts. We display the attention map in Figure 22. Although we only fine-tune MVDream on 2D images, it can be transferred to multi-view image generation thanks to the prior. The cross-attention map shows the strong correlation between the image and the parts token and shows strong disentanglement each part doesnt attend to incorrect spatial location thanks to the part attention loss. When the parts in the image are occluded (e.g., facing front, tail will be occluded), the model will attend to the background but not other parts. 1 - IoU Textual Inversion 0.744 PartCraft Chirpy3D (Ours) 0.954 0.957 Table 5. Overlapping score between the cross-attention map of all parts. Part disentanglement test. In Table 5, we present the overlap score, defined as 1 IoU (intersection-over-union), calculated between the cross-attention maps of all parts (see an example in Fig. 22). The scores are averaged over 1,000 samples. These results demonstrate that the parts are effectively disentangled during generation using our Chirpy3D, enabling part-aware generation. In contrast, Textual Inversion struggles with part-aware generation, possibly because the learned word embeddings for each part are independent and lack mutual awareness, making it difficult to generate cohesive whole object from individual parts. Figure 22. Cross-attention map of multi-view images. The crossattention map is averaged over all layers with size 16 16. Effect of Lattn. We display the attention map in Figure 22. Although we fine-tune MVDream exclusively on 2D We display the part latent space in Figure 23. In this laFigure 23. Visualizing part latent space via t-SNE embeddings. 19 tent space, we can traverse to sample desired species/generations. For instance, we can perform interpolation between two species, randomly sample, and also perform part composition, all can be done within this part latent space. K. Ablation Study (a) 0.1 (b) 0.01 (c) 0.001 Algorithm 1 Code snippet for diversity evaluation Figure 24. Comparison of generated images with different scales, λcl, for Lcl. # db_feats: DINO feats of training images (Nx768) # query_feats: DINO feats of generated images (Qx768) # db_labels: Training ground-truth labels (N) Lcl. We did not set λcl higher than 0.001, as higher values degrade generation quality (e.g., failure to generate objects correctly) due to the model overfitting to minimize it. See Figure 24. def retrieve(query_features, db_features, db_labels): # both features assume normalized cossim = query_features @ db_features.t() # (Q, N) indices = torch.argsort(cossim, dim=-1) top5 = indices[:, :5] idx = torch.arange(len(db_features)).to(db_labels. device) return ( db_labels[top5.reshape(-1)].reshape(top5.size(0) Dl 4 4.81 eH 123.2 16 4.68 108.1 32 4.67 106.7 64 4.33 76.3 , top5.size(1)) ) def histogram_entropy(hist): # Normalize the histogram to obtain probabilities prob = hist / hist.sum() Table 6. Entropy and the effective number of classes eH of top-5 retrieved classes using generated images from random part latents. Higher values indicate greater diversity. # Compute the entropy entropy = -torch.sum(prob * torch.log(prob)) return entropy Latent dimension, Dl. In the main paper, we use Dl = 4 as it has the highest diversity for random sampling compared to others as shown in Table 6. For the evaluation algorithm, please refer to Algorithm 1. retrieve_labels = retrieve( query_feats, db_feats, db_labels ) hist1 = torch.zeros(200) for in range(200): hist1[i] = (retrieve_labels == i).sum() entropy_hist1 = histogram_entropy(hist1) print(f\"Entropy of {n}:\", entropy_hist1.item()) print(\"Num classes\", torch.exp(entropy_hist1).item())"
        }
    ],
    "affiliations": [
        "Imperial College London",
        "Universiti Malaya",
        "University of Cambridge",
        "University of Surrey"
    ]
}