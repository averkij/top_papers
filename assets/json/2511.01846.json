{
    "paper_title": "Towards Robust Mathematical Reasoning",
    "authors": [
        "Thang Luong",
        "Dawsen Hwang",
        "Hoang H. Nguyen",
        "Golnaz Ghiasi",
        "Yuri Chervonyi",
        "Insuk Seo",
        "Junsu Kim",
        "Garrett Bingham",
        "Jonathan Lee",
        "Swaroop Mishra",
        "Alex Zhai",
        "Clara Huiyi Hu",
        "Henryk Michalewski",
        "Jimin Kim",
        "Jeonghyun Ahn",
        "Junhwi Bae",
        "Xingyou Song",
        "Trieu H. Trinh",
        "Quoc V. Le",
        "Junehyuk Jung"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Finding the right north-star metrics is highly critical for advancing the mathematical reasoning capabilities of foundation models, especially given that existing evaluations are either too easy or only focus on getting correct short answers. To address these issues, we present IMO-Bench, a suite of advanced reasoning benchmarks, vetted by a panel of top specialists and that specifically targets the level of the International Mathematical Olympiad (IMO), the most prestigious venue for young mathematicians. IMO-AnswerBench first tests models on 400 diverse Olympiad problems with verifiable short answers. IMO-Proof Bench is the next-level evaluation for proof-writing capabilities, which includes both basic and advanced IMO level problems as well as detailed grading guidelines to facilitate automatic grading. These benchmarks played a crucial role in our historic achievement of the gold-level performance at IMO 2025 with Gemini Deep Think (Luong and Lockhart, 2025). Our model achieved 80.0% on IMO-AnswerBench and 65.7% on the advanced IMO-Proof Bench, surpassing the best non-Gemini models by large margins of 6.9% and 42.4% respectively. We also showed that autograders built with Gemini reasoning correlate well with human evaluations and construct IMO-GradingBench, with 1000 human gradings on proofs, to enable further progress in automatic evaluation of long-form answers. We hope that IMO-Bench will help the community towards advancing robust mathematical reasoning and release it at https://imobench.github.io/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 6 4 8 1 0 . 1 1 5 2 : r 2025-11-"
        },
        {
            "title": "Towards Robust Mathematical Reasoning",
            "content": "Thang Luong, Dawsen Hwang*, Hoang H. Nguyen*, Golnaz Ghiasi*, Yuri Chervonyi*, Insuk Seo*, Junsu Kim*, Garrett Bingham, Jonathan Lee, Swaroop Mishra, Alex Zhai, Clara Huiyi Hu, Henryk Michalewski, Jimin Kim, Jeonghyun Ahn, Junhwi Bae, Xingyou Song, Trieu H. Trinh, Quoc V. Le, Junehyuk Jung Corresponding authors, *Core and equal contributors, Work previously conducted under Google DeepMind Finding the right north-star metrics is highly critical for advancing the mathematical reasoning capabilities of foundation models, especially given that existing evaluations are either too easy or only focus on getting correct short answers. To address these issues, we present IMO-Bench, suite of advanced reasoning benchmarks, vetted by panel of top specialists and that specifically targets the level of the International Mathematical Olympiad (IMO), the most prestigious venue for young mathematicians. IMO-AnswerBench first tests models on 400 diverse Olympiad problems with verifiable short answers. IMO-Proof Bench is the next-level evaluation for proof-writing capabilities, which includes both basic and advanced IMO level problems as well as detailed grading guidelines to facilitate automatic grading. These benchmarks played crucial role in our historic achievement of the gold-level performance at IMO 2025 with Gemini Deep Think (Luong and Lockhart, 2025). Our model achieved 80.0% on IMO-AnswerBench and 65.7% on the advanced IMO-Proof Bench, surpassing the best non-Gemini models by large margins of 6.9% and 42.4% respectively. We also showed that autograders built with Gemini reasoning correlate well with human evaluations and construct IMO-GradingBench, with 1000 human gradings on proofs, to enable further progress in automatic evaluation of long-form answers. We hope that IMO-Bench will help the community towards advancing robust mathematical reasoning and release it at https://imobench.github.io. 1. Introduction The field of artificial intelligence, particularly large language or foundation models, has demonstrated remarkable progress in mathematical reasoning capabilities. Many popular benchmarks such as GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021), and the recently popular AIME have approached saturation, limiting their usefulness in differentiating model performances. The problems in these datasets often rely on limited set of techniques and do not always require the deep, multi-step reasoning needed to truly evaluate AI mathematical reasoning. Indeed, relying on final answer matching, even in recent benchmarks such as FrontierMath (Glazer et al., 2024) and Humanitys Last Exam (Phan et al., 2025), is not entirely reliable. It could lead to AI systems that are good at guessing answers but do not exhibit robust reasoning. To address these shortcomings, we propose IMO-Bench, suite of benchmarks that focus on robust reasoning at the level of the International Mathematical Olympiad (IMO), the worlds most celebrated arena for young mathematicians. The IMO is selected due to its notoriously difficult Figure 1 IMO-Proof Bench, benchmark in IMOBench, for measuring proof-writing capabilities. We demonstrated high correlations between human and automatic evaluations on variety of public models, including our IMO-gold model. See 3 and 5.3 for more details. problems, which require not only rigorous multistep reasoning but also high degree of novelty, going beyond the simple application of known Corresponding author(s): thangluong@google.com, junehuyk@google.com External affiliations: Georgia Institute of Technology (Hoang Nguyen), Seoul National University (Insuk Seo, Junsu Kim, Jimin Kim), Microsoft (Swaroop Mishra), Massachusetts Institute of Technology (Jeonghyun Ahn, Junhwi Bae), Brown University (Junehyuk Jung). 2025 Google DeepMind. All rights reserved Towards Robust Mathematical Reasoning formulas. Such characteristics make IMO an excellent testbed for assessing reasoning capability. IMO-Bench covers three different tasks as summarized in Table 1 and all problems were vetted by panel of IMO medalists1 and mathematicians. Benchmark Size Task IMO-AnswerBench IMO-Proof Bench IMO-GradingBench 1000 Grade proof 400 Get the right answer 60 Write rigorous proof Table 1 Benchmarks in the IMO-Bench suite. The first benchmark, IMO-AnswerBench, consists of 400 problems with verifiable answers carefully chosen from past Olympiad competitions and then altered by experts to avoid memorization. Problems were chosen from variety of topics whose solutions require different problem solving techniques to ensure diverse representation of topics, ideas, and domain knowledge as illustrated in Figure 2. The second benchmark, IMO-Proof Bench, consists of 60 problems of varying difficulty levels, similar to those found at the IMO. While some problems have short answers, all require models to generate complete proofs. The benchmark is divided into two subsets, basic and advanced, each with 30 problems. While the basic set covers difficulty levels from pre-IMO up to IMO-Medium, problems in the advanced set are up to IMO-hard level and consist of 5 complete IMO sets, 3 of which are novel. We designed this benchmark to steer the communitys focus from final answers to proofs, enabling more rigorous assessment of AI reasoning processes. To ensure consistent evaluation, we include detailed grading schemes suitable for both human experts and automated systems. Figure 1 provides an early look into the potential of automatic graders for proofs. These two benchmarks played crucial role in the development of our Gemini Deep Think, leading to the historic achievement of the goldlevel performance at IMO 2025 (Luong and Lockhart, 2025). Our IMO-gold model achieved an accuracy of 80.0% on IMO-AnswerBench by automatic evaluation, surpassing the best non-Gemini 1Together, they won 10 gold and 5 silver IMO medals. model and the best open-weight model by large margin of 6.9% and 19.2% respectively. The advanced IMO-Proof Bench is much more challenging. Our IMO-gold model scored 65.7%, whereas the best non-Gemini and the best open-weight models performed poorly with only 23.3% and 7.1% accuracy according to human evaluations. Furthermore, we demonstrate that automated graders for both answers and proofs, built upon Gemini 2.5 Pro, achieve high correlation with expert human evaluations. Last but not least, we introduce IMOGradingBench, benchmark of 1000 solutions to problems in the advanced IMO-Proof Bench, together with grades from human experts. This resource is designed to foster progress in the automatic evaluation of long-form answers. We release IMO-Bench to the community and hope that it will spur further research towards advancing robust mathematical reasoning. 2. IMO-AnswerBench 2.1. Problem Selection 400 math problems were handpicked from various national, regional, and international Olympiad contests, spanning across four categories (Algebra, Combinatorics, Geometry, Number Theory). For each category, the benchmark contains 100 problems across four levels of difficulty: pre-IMO (middle school or pre-Math Olympiad problems), IMO-Easy (equivalent to Problem 1 or Problem 4 at the IMO), IMO-Medium (equivalent to Problem 2 or Problem 5 at the IMO) and IMO-Hard (equivalent to Problem 3 or Problem 6 at the IMO or post-Math Olympiad problems). The difficulty breakdown for each category is listed in Table 2. Category Pre-IMO IMO-Easy IMO-Medium IMO-Hard Algebra Combinatorics Geometry Number Theory 11 4 13 2 46 19 44 20 32 31 32 31 11 46 11 47 Table 2 Difficulty statistics for IMO-AnswerBench. Problems with short answers were chosen so the correctness of models output can be quickly and reliably determined. Given the proof-heavy nature of many math Olympiad problems, we 2 Towards Robust Mathematical Reasoning Figure 2 Topic distribution by category in IMO-AnswerBench. Number Theory and Combinatorics have the most topics which reflect the broad knowledge required to solve these problems while Geometry is mostly skewed towards angle and sidelength computation problems due to the nature of the short answer benchmark. perform an additional reformulation step for certain examples. This adjustment ensures that each problem yields clear and nontrivial short answer, thereby reducing ambiguity during solving and verification and confirming that models utilize nontrivial reasoning. See further details in A.4. 2.2. Problem Robustification To avoid data memorization, an additional step of problem modification is done via paraphrasing, changing the name of objects in the problem (such as changing point names for geometry problems), reformulating, modifying numerical values and/or adding distractors to the problem. This process is done either manually or automatically using language models. We highlight some examples in Table 8 and detail below. One example is an algebra problem from Austria Math Olympiad 2017. The problem is modified by making the substitution 洧논 = 洧녩 + 洧녪 洧녫, 洧녽 = 洧녪 + 洧녫 洧녩, and 洧녾 = 洧녫 + 洧녩 洧녪 for positive real numbers 洧논, 洧녽, 洧녾 with 洧녩, 洧녪, and 洧녫 being the lengths of the sides of some triangle to obtain the modified problem in the Robustified column. This modification uses the knowledge that 洧녩, 洧녪, and 洧녫 are lengths of triangle if and only if they satisfy the triangle inequalities 洧녩 + 洧녪 > 洧녫, 洧녩 + 洧녫 > 洧녪, and 洧녪 + 洧녫 > 洧녩. Another example is combinatorics problem from USA TST 2005. From the original statement, the problem is modified using several techniques such as modifying numerical values (by assigning specific value to the variable 洧녵 so that it is harder to guess the pattern), adding distractors (by introducing function or variables that are 3 Towards Robust Mathematical Reasoning not relevant to the problem), and adding layer of challenge that could confuse the models. 3. Going Beyond Short Answers with IMO-Proof Bench Experts also reformulated original problems into equivalent ones with completely different expressions. One such example is the Czech-Slovak Math Olympiad 2017 problem. We obtained robustified problem by transforming the governing equation and changing the objective from finding all possible values of 洧녲 to finding all even integers 洧녬 such that the number of solutions is even. 2.3. Answer autograder Even for the problems with short answers, automatic answer verification presents few substantial challenges. The difficulty arises from two main issues: (1) ensuring that model outputs adhere to parsable format and (2) evaluating semantically equivalent but syntactically different expressions.2 To circumvent this issue, benchmarks such as FrontierMath (Glazer et al., 2024) select problems with only numerical answers or mathematical objects that can be expressed as SymPy objects. However, this approach narrows the scope of evaluable problems and reduces robustness of the benchmark to minor formatting or syntax errors. To address these limitations, we use large language models as automated verifiers for model answers on IMO-AnswerBench. We name this approach, AnswerAutoGrader, which is built by prompting the public Gemini 2.5 Pro model to extract final answers from generated solutions and assess their correctness against ground truths (See A.5 for the full prompt). This method allows much more flexibility in acceptable answer formats and improves the overall robustness of our benchmark. As we demonstrate in Section 5.1, AnswerAutoGraders performance is nearly identical to that of human evaluators, validating its use for future public usage and also for reporting the results in this work. 2For example, given the ground truth answer \"(, 4) (4, )\", the answer \"all real numbers except -4\" should also be graded as correct. While the final answer accuracy provided by IMOAnswerBench offers valuable metric for measuring mathematical abilities, it is insufficient for comprehensive assessment of mathematical reasoning. final answer can be correct while the full solution contains flawed reasoning. Furthermore, many IMO-level competition problems do not come with final short answer. Even in cases where short answer exists, guessing the correct short answer is often significantly easier than rigorously deriving the solution. IMO-Proof Bench is designed to evaluate the ability of AI models to construct comprehensive and valid mathematical arguments. This benchmark consists of 60 proof-based problems, curated to mirror the kinds of problems found in the IMO. While some problems may have concise numerical answers, models are only given credit if they produce correct and relevant reasoning steps. This benchmark is essential for assessing an AIs underlying reasoning process, its ability to apply mathematical principles, and its capacity to formulate coherent and logical arguments. 3.1. Benchmark setup The benchmark is divided into two subsets: basic set covering pre-IMO to IMO-Medium difficulty levels, and an advanced set featuring novel, highly challenging problems simulating complete IMO examinations, up to IMO-Hard level. The basic problem set primarily consists of rephrased versions of existing problems. Since standard IMO problems may be too challenging for most of current models, the basic set is designed to assess models in their early stages of development. Sufficiently strong performance on the basic set would justify progression to the advanced set. The advanced problem set features 30 problems in the style and difficulty of the IMO. The collection includes 18 novel problems crafted by IMO medalists, alongside 12 problems from recent top-tier competitions: 6 robustified from IMO 2024 and 6 directly from USAMO 2025. Table 10 provides examples of such robustified problems. Towards Robust Mathematical Reasoning IMO-Proof Bench uses an evaluation framework designed for both simplicity and precision. We provide primary grading guideline with four ratings (Correct, Almost, Partial, Incorrect) as detailed in Table 3. While this rubric offers clear and consistent baseline, we do not restrict our expert evaluators to these four values. To allow for more nuanced assessments, human experts are empowered to use their own judgments to assign any integer score from 0 to 7 for each problem. Category IMO Points Solution quality Correct Almost Partial Incorrect 7 6 1 Fully correct, rigorous, and complete Almost correct, minor errors Mostly incorrect, some relevant results Completely incorrect or irrelevant. Table 3 Our simplified IMO ratings. 3.2. Proof Autograder While human expert evaluation remains the gold standard for mathematical proofs, its cost and time intensity limit scalable research. To address this, we built ProofAutoGrader, an automatic grader for IMO-Proof Bench. The autograder leverages Gemini 2.5 Pro, providing it with prompt containing the problem statement, the candidate solution, reference solution, and specific grading guidelines (see Appendix B.5). Automatic evaluation for informal proofs is highly intricate task, and current systems are not yet perfect substitute for human experts-a key distinction from AnswerAutoGrader, whose purpose is primarily format matching. For this reason, all primary results in this paper are based on expert human evaluation to ensure all results are absolutely correct. Nevertheless, as we demonstrate in Section 5.3, we prove our autograder can be reasonable proxy, establishing it as reasonable tool for the community to assess future models on IMO-Proof Bench. developing reliable automated grading systems and improving general mathematical reasoning. As part of our IMO effort (Luong and Lockhart, 2025), we have benchmarked extensively many internal models on the advanced set of IMO-Proof Bench using human evaluations. These human gradings led to the creation of IMOGradingBench with 1000 examples, each containing problem statement, proposed solution, and its human-assigned grade (on 07 scale). To reduce noise from fine-grained scoring, we frame the evaluation as four-way classification by mapping the given IMO points to the labels (Correct, Almost, Partial, Incorrect) as detailed in Table 3. To ensure robust evaluation, the dataset has been balanced with roughly equal number of examples per category. Figure 3 illustrates that when problems are grouped by their IMO difficulties, clear trend emerges. The proportion of correct and almost solutions decreases as the intended difficulty moves from IMO-easy to IMO-hard, while the proportion of incorrect and partial solutions increases. This confirms that the grading distribution of IMO-GradingBench aligns with its assigned difficulty levels. See further discussions in Section C.1. Figure 3 Grade distribution for solutions in IMO-GradingBench by difficulty levels (IMO-Hard, IMO-Medium, IMO-Easy). 4. IMO-GradingBench 5. Results While IMO-Proof Bench evaluates proof-writing abilities, it is equally important to assess models in terms of their ability to evaluate the correctness of given solutions. This capability is crucial for We evaluate IMO-Bench on wide variety of publicly available models: Claude Opus 4 (20250514), Claude Sonnet 4 (Anthropic, 2025), DeepSeek V3 (DeepSeek, 2025b), DeepSeek 5 Towards Robust Mathematical Reasoning Model Query date Algebra Combinatorics Geometry Number Theory Overall Claude Opus 4 Claude Sonnet 4 DeepSeek V3 Kimi-K2-Instruct Qwen3-235B DeepSeek R1 o3 GPT-5 o4-mini (high reasoning) Gemini 2.5 Pro Gemini 2.5 Deep Think Grok 4 Gemini Deep Think (IMO Gold) 2025-08-04 2025-08-06 2025-09-17 2025-09-17 2025-08-20 2025-09-17 2025-08-04 2025-09-17 2025-08-04 2025-08-04 2025-08-20 2025-08-06 2025-0919.4% 20.6% 39.0% 45.6% 57.6% 65.0% 62.8% 69.9% 71.3% 73.4% 78.0% 75.5% 85.0% 20.0% 17.8% 26.0% 31.1% 37.5% 40.0% 43.0% 46.4% 46.6% 48.0% 49.0% 55.9% 69.0% 23.3% 26.0% 35.0% 49.3% 57.6% 73.0% 70.6% 74.8% 78.4% 74.3% 83.0% 80.1% 88.0% 26.6% 27.6% 48.0% 56.9% 62.3% 65.0% 68.0% 71.2% 75.3% 77.1% 77.0% 80.9% 78.0% 22.3% 23.0% 37.0% 45.8% 53.8% 60.8% 61.1% 65.6% 67.9% 68.2% 71.8% 73.1% 80.0% Table 4 Model accuracy on IMO-AnswerBench. Results are averaged over 8 runs, except for Gemini 2.5 Deep Think and Gemini Deep Think (IMO Gold) (single run). An evaluation of Grok 4 (heavy) on 2025-08-13 using multiple paid accounts was aborted due to significant instability (only 117/400 responses were received despite multiple, hour-long attempts), and thus its results are not reported."
        },
        {
            "title": "AnswerAutoGrader",
            "content": ""
        },
        {
            "title": "Human\nGrade",
            "content": "0 274 (99.6%) 1 8 (1.5%) 1 (0.4%) 517 (98.5%) Table 5 AnswerAutoGrader predictions against human grades for IMO-AnswerBench. The solutions were generated by Gemini 2.5 Pro and o3. R1 (DeepSeek, 2025a), Kimi-K2-Instruct (Moonshot AI, 2025), Qwen3-235B (A22B-Instruct2507tput) (Qwen Team, 2025), o3 (2025-0416), o4-mini (high reasoning) (OpenAI, 2025b), GPT-5 (2025-08-07) (OpenAI, 2025a), Gemini 2.5 Pro (Google DeepMind, 2025), Gemini 2.5 Deep Think (Deep Think team, 2025), Gemini Deep Think (IMO Gold) (Luong and Lockhart, 2025), Gemini 2.5 Pro with (Huang & Yang, 2025) (Huang and Yang, 2025), Grok 4 (0709) (xAI, 2025). Since Gemini 2.5 Pro with (Huang & Yang, 2025) is an agentic framework rather than single model call, Appendix B.3 contains further implementation details. responses and assesses their semantic equivalence to the ground truths. Our Gemini Deep Think (IMO Gold) model achieved an overall accuracy of 80.0%, surpassing the best non-Gemini model (Grok 4) by 6.9% and the best open-weight model (DeepSeek R1) by 19.2%. Latest models such as Kimi-K2-Instruct and GPT-5 are still struggling with overall accuracy of only 45.8% and 65.6% respectively. Across the four categories of Algebra, Combinatorics, Geometry, and Number Theory, models generally perform the worst in Combinatorics, potentially highlighting difficulties with advanced abstract reasoning. We also analyze the performances of models on the original problems, before robustification, summarized in Table 9. As anticipated, we find robustification leads to consistent drop in performance across all models. Lastly, we validate the reliability of AnswerAutoGrader by comparing it with expert human labels. As reported in Table 5, the autograder shows nearly perfect performance, achieving overall accuracy of 98.9% on the positive (correct) class. 5.1. IMO-AnswerBench with AnswerAuto5.2. IMO-Proof Bench with Expert Evaluations"
        },
        {
            "title": "Grader",
            "content": "Results for IMO-AnswerBench are summarized in Table 4. Accuracy was determined by AnswerAutoGrader, which extracts final answers from model Model outputs on IMO-Proof Bench were graded by human experts according to the guidelines described in Section B.2. Table 6 presents the results of this evaluation. Performance on the basic 6 Towards Robust Mathematical Reasoning Model Query date IMO-Proof Bench Advanced IMO-Proof Bench Breakdown Basic Advanced Novel IMO 2024 USAMO 2025 Number of Problems 30 30 18 Claude Opus 4 DeepSeek V3 Kimi-K2-Instruct Claude Sonnet 4 DeepSeek R1 Qwen3-235B o4-mini (high reasoning) Grok 4 o3 Gemini 2.5 Pro GPT-5 Grok 4 (heavy) Gemini 2.5 Pro with (Huang & Yang, 2025) Gemini 2.5 Deep Think Gemini Deep Think (IMO Gold) 11.9% 2025-08-04 18.6% 2025-09-16 19.5% 2025-08-21 27.1%1 2025-09-17 29.0% 2025-09-16 33.3% 2025-08-21 37.6% 2025-08-04 46.7% 2025-08-20 54.8% 2025-08-04 55.2% 2025-08-04 59.0% 2025-09-18 NA 2025-07-12 69.5% 2025-07-14 2025-08-20 83.8% 2025-08-02 89.0% 2.9% 4.3% 7.1% 4.8%1 3.8% 5.2% 11.4% 18.6% 20.5% 17.6% 20.0% 23.3%3 24.8% 37.6% 65.7% 0.0% 6.3% 4.0% 6.4%1 6.4% 7.1% 8.7% 17.5% 15.1% 15.9% 15.9% 11.1%3 17.5% 31.7% 61.1% 2.4% 2.4% 2.4% 2.4% 0.0% 0.0% 7.1% 16.7% 4.8% 7.1% 33.3% 7.1% 19.1% 40.5% 76.2% 11.9% 0.0% 21.4% 2.4% 0.0% 4.8% 23.8% 23.8% 52.4% 33.3% 19.0% 76.2% 52.4% 52.4% 69.0% Table 6 Expert evaluation results on the Basic and Advanced subsets of IMO-Proof Bench. Scores are presented as percentage of the total possible points for the problems in each respective subset, with each problem graded from 07 (as described in Section B.2). The Advanced IMO-Proof Bench is further broken down by problem source. Robustified IMO 2024 problem set, see Section 3. An attempt to query Grok 4 (heavy) on 2025-08-13 was unsuccessful due to model instability (only 5 of 30 problems responded with 3 attempts). 洧녲Scores indicate that there were 洧녲 problems that were treated as incorrect (a score of 0) because of query failures (for at least 3 times). IMO-Proof Bench varies significantly; while most models score below 60%, Gemini Deep Think (IMO Gold) achieves high score of 89.0%. The performances of other frontier models such as Qwen3-235B (33.3%) and GPT-5 (59.0%) show there is still considerable room for improvements. The advanced IMO-Proof Bench proves to be more significant challenge that all non-Gemini models score below 25%. Our Gemini Deep Think (IMO Gold) model achieved score of 65.7%, surpassing the best non-Gemini model (Grok 4 (heavy)) by large margin of 42.4%. This represents substantial leap in capability, but its distance from perfect score indicates that even the strongest models have room for growth in sophisticated mathematical reasoning. breakdown of the advanced IMO-Proof Bench reveals significant performance disparity across problem types, suggesting potential overfitting in certain models. This trend is most evident with Grok 4 (heavy), which scores 76.2% on USAMO 2025 but only 11.1% on novel problems. Other models, including o3 (52.4% vs. 15.1%) and Gemini 2.5 Pro with (Huang & Yang, 2025) (52.4% vs. 17.5%), exhibit similar, pronounced gap. In contrast, Gemini Deep Think (IMO Gold) scored 69.0% on the USAMO and 61.1% on the novel sets, indicating it has more general capabilities (Deep Think team, 2025) without overfitting to particular dataset. The low performances of latest frontier models such as GPT-5 and Grok 4 (heavy) on the advanced IMO-Proof Bench underscore the difficulty of advanced mathematical reasoning and highlight the importance of rigorous examination the full details of model outputs for complete understanding of their mathematical abilities. 5.3. Autograder for IMO-Proof Bench To assess the feasibility of using automatic graders for proofs, we apply ProofAutoGrader to the 14 public models  (Table 6)  , which were previously graded by human experts on IMO-Proof Bench. Figure 1 shows that the average grades from ProofAutoGrader highly correlate with human grades, yielding high Pearson correlation coefficients of 0.96 and 0.93 on both basic and advanced problems respectively. 7 Towards Robust Mathematical Reasoning Figure 4 Correlation between ProofAutoGrader and human experts on the advanced IMOProof Bench, evaluated over 170 internal models on our IMO-gold journey. In addition, we also visualized, in Figure 4, the performance of ProofAutoGrader on 170 internal systems, developed as part of our IMO effort (Luong and Lockhart, 2025). On this larger pool, our automatic grader achieved lower, but still reasonable Pearson correlation coefficient of 0.87. To better understand the grading agreement, we visualize, in Figure 5, the confusion matrix of all human and automatic gradings on the 14 public models (for total of 840 model solutions). We observed that most common misclassifications happened between the Incorrect and Partial classes. Overall, ProofAutoGrader shows reasonable performance, exhibiting high correlation with human experts, and also shows potential in identifying nuances that might be overlooked by human graders. On the other hand, detailed analysis with persolution breakdowns further reveals that ProofAutoGrader occasionally still has weaknesses such as failures to identify high-level logical errors or being overly punitive for unconventional yet correct solutions. Specific examples are highlighted in appendix B.6. Therefore, while we hope that ProofAutoGrader can serve as valuable tool for the community to evaluate models on IMO-Proof Bench, we recommend that it augments human verification to guarantee the accuracy of individual grading results. 5.4. IMO-GradingBench The IMO-GradingBench measures the ability of models in assessing the quality of proof when provided with only problem statements and model-generated solutions, without any reference solutions or specific grading guidelines. We measure model performances under two metrics: 1. Accuracy human gradings on 7-point scale are first converted to 4 categories (Correct, Almost, Partial, Incorrect) corresponding to 4 buckets (7, 6-4, 3-1, 0). The categorized human gradings are then compared with modelpredicted categories. 2. Mean Absolute Error (MAE) modelpredicted categories are converted from (Correct, Almost, Partial, Incorrect) to IMO scores (7, 6, 1, 0) according to Table 3. We then compare with human grading ground truths on 7-point scale. Figure 5 Confusion matrix for ProofAutoGrader vs. human expert grades, over 840 solutions generated by 14 public models (See Table 6). Results for IMO-GradingBench are summarized in Table 7. In terms of accuracies, o3 achieved the highest performance of 54.0%. The low ac8 Towards Robust Mathematical Reasoning Model Accuracy MAE 6. Related Work Gemini 2.5 Pro o4-mini (high reasoning) Gemini 2.5 Deep Think o3 Gemini Deep Think (IMO Gold) 44.3% 47.3% 52.5% 54.0% 50.2% 30.2% 25.2% 20.5% 20.2% 18.4% Table 7 IMO-GradingBench results in accuracy (higher is better) and MAE (lower is better). curacies highlight the fact that this benchmark is quite challenging in predicting precise categories. The MAE accounts for the fact that different categories are closer semantically, e.g., Correct vs. Almost and Partial vs. Incorrect. On this metric, Gemini Deep Think (IMO Gold) achieved the best MAE score of 18.4%, indicating that there is still significant room for improvement3. Comparison with ProofAutoGrader Model performances on IMO-GradingBench are notably worse than what might be expected from the strong performance of ProofAutoGrader, in terms of Pearson correlation coefficients as reported in Section 5.3. This discrepancy stems from two key methodological distinctions. 1. First, ProofAutoGrader performance was measured on scores aggregated over 30 problems, which smooths out noise from individual grading variations, unlike the per-instance evaluation of IMO-GradingBench. 2. Second, the IMO-GradingBench evaluation provides models with minimal contextonly the problem and the proposed solution; whereas for ProofAutoGrader on IMOProof Bench, we additionally provide both reference solutions and grading guidelines."
        },
        {
            "title": "These",
            "content": "distinctions explain why IMOGradingBench with per-instance, minimal-context evaluation is challenging benchmark; whereas aggregated assessments by ProofAutoGrader on IMO-Proof Bench can still yield robust model rankings. 3Because of our simplified gradings (7, 6, 1, 0), the best possible grader will achieve golden MAE of 3.9% on IMOGradingBench, instead of 0%. In recent years, harder reasoning math benchmarks have been proposed as performance on existing benchmarks becomes saturated. For example, Olympiad Bench (He et al., 2024) and Omni-MATH (Gao et al., 2024) contain questions at the Olympiad level across diverse domains, while Humanitys Last Exam (HLE) (Phan et al., 2025) evaluates knowledge across many domains. Other benchmarks include Brainteaser (Han et al., 2025), which consists of long-form brainteaser puzzles, and Frontier Math (Glazer et al., 2024), which contains hard math questions and hidden evaluation set. MiniF2F (Zheng et al., 2021) provides benchmark for evaluating formal proofs around Olympiad-level difficulty. Reward Bench (Lambert et al., 2024) provides benchmark to evaluate reward models. HARDMath (Fan et al., 2024) presents challenging math benchmark containing applied mathematics problems that require analytical approximation techniques. The AlphaGeometry papers (Chervonyi et al., 2025; Trinh et al., 2024) provide benchmarks of 80 IMO and IMO Shortlist Euclidean geometry problems from 2000 to 2024, written in domain-specific language. In contrast, IMO-Bench provides suite for evaluating advanced mathematical reasoning with short answer matching and rigorous proof evaluation in natural language across wide variety of Math Olympiad areas. As performance on math benchmarks continues to improve, robustness benchmarks have been introduced to evaluate potential overfitting and obtain better estimates of models true reasoning capabilities. These benchmarks have shown that simply perturbing benchmark questions is enough to significantly hurt performance compared to the original problems. SVAMP (Patel et al., 2021) generated perturbed benchmark for word math problems, whereas Lila (Mishra et al., 2022) contained perturbations across diverse range of reasoning questions. The functional variant of the MATH benchmark (Srivastava et al., 2024) demonstrated large performance drops across models when varying existing problems. Putnam-AXIOM (Gulati et al., 2024) similarly shows that perturbing Putnam 9 Towards Robust Mathematical Reasoning questions causes significant drop in model performance. MATH-Perturb (Huang et al., 2025) also adds simple perturbations to math questions (Hendrycks et al., 2021), and shows model performance drops, raising concerns about memorization. Lightman et al. (2024) propose an alternative strategy to improve model robustness by supervising the reasoning process from start to finish, rather than solely on the final outcome. This approach led to improved performance on the MATH dataset. IMO-Bench contributes to robust mathematical reasoning with already modified questions in IMO-AnswerBench, rigorous proof requirements in IMO-Proof Bench, and the task of proof grading in IMO-GradingBench. 7. Conclusion This paper introduced IMO-Bench, comprehensive suite of benchmarks for robust evaluation of mathematical reasoning capabilities, including IMO-AnswerBench for short answer matching, IMO-Proof Bench for full proof correctness, and IMO-GradingBench for proof verification. The three benchmarks demonstrated that frontier models struggle on IMO-Bench problems and that getting the short answers right does not necessarily equate to correct mathematical reasoning for most models. Furthermore, we have developed and validated automated graders for both answers and proofs. Our AnswerAutoGrader achieves near-human accuracy (98.9%) , while ProofAutoGrader shows strong correlation (0.93-0.96 %) with expert human scores. These tools along with IMOGradingBench provide scalable and reliable method for the community to evaluate future models, even as human expertise remains the gold standard for high-stakes evaluation. By releasing IMO-Bench4 to the research community, we aim to shift the communitys focus from mere answer-getting to the development of deep, verifiable, and robust reasoning processes. We hope this suite will serve as valuable tool to measure and drive progress toward more advanced and reliable artificial intelligence."
        },
        {
            "title": "Acknowledgments",
            "content": "Special thanks to Miroslav Ol코치k, Seongbin Jeon, Donghyun Kim, Jiwon Kang, Chu-Lan Kao, Sara Javanmardi, and Mahan Malihi for help with IMOBench. In addition, we would like to thank Orhan Firat, Tania Bedrax-Weiss, and Ed Chi for reviewing the work and Koray Kavukcuoglu for guidance on the release of IMO-Bench. Last but not least, we thank all our collaborators in the IMO 2025 effort5 for trusting IMO-Bench as north-star metrics along the way."
        },
        {
            "title": "References",
            "content": "Anthropic. Introducing claude 4. https: //www.anthropic.com/news/claude-4, May 2025. Guidance for problem captains: how to write an olympiad rubric. E. Chen. Or: https://web.evanchen.cc/static/ usemo/captain-guidance-usemo.pdf, December 2023. Y. Chervonyi, T. H. Trinh, M. Ol코치k, X. Yang, H. Nguyen, M. Menegali, J. Jung, V. Verma, Q. V. Le, and T. Luong. Gold-medalist performance in solving olympiad geometry with alphageometry2, 2025. URL https://arxiv. org/abs/2502.03544. K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems, 2021. Deep Think team. Try deep think in the gemini https://blog.google/products/ app. gemini/gemini-2-5-deep-think/, 2025. DeepSeek. Deepseek-r1-0528 release. https://api-docs.deepseek.com/ news/news250528, May 2025a. DeepSeek. Deepseek-v3-0324 release. https://api-docs.deepseek.com/ news/news250325, March 2025b. 4https://imobench.github.io 5https://goo.gle/imo-gold 10 Towards Robust Mathematical Reasoning J. Fan, S. Martinson, E. Y. Wang, K. Hausknecht, J. Brenner, D. Liu, N. Peng, C. Wang, and M. P. Brenner. Hardmath: benchmark dataset for challenging problems in applied mathematics. arXiv preprint arXiv:2410.09988, 2024. B. Gao, F. Song, Z. Yang, Z. Cai, Y. Miao, Q. Dong, L. Li, C. Ma, L. Chen, R. Xu, et al. Omni-math: universal olympiad level mathematic benchmark for large language models. arXiv preprint arXiv:2410.07985, 2024. E. Glazer, E. Erdil, T. Besiroglu, D. Chicharro, E. Chen, A. Gunning, C. F. Olsson, J.-S. Denain, A. Ho, E. d. O. Santos, et al. FrontierMath: benchmark for evaluating advanced mathematical reasoning in ai. arXiv preprint arXiv:2411.04872, 2024. Google DeepMind. Gemini 2.5 pro. https:// deepmind.google/models/gemini/pro/, 2025. A. Gulati, B. Miranda, E. Chen, E. Xia, K. Fronsdal, B. de Moraes Dumont, and S. Koyejo. Putnamaxiom: functional and static benchmark for measuring higher level mathematical reasoning. In The 4th Workshop on Mathematical Reasoning and AI at NeurIPS24, 2024. S. Han, S. Xia, G. Zhang, H. Dai, C. Liu, L. Chen, H. H. Nguyen, H. Mei, J. Mao, and R. T. McCoy. Creativity or brute force? using brainteasers as window into the problem-solving abilities of large language models, 2025. URL https: //arxiv.org/abs/2505.10844. C. He, R. Luo, Y. Bai, S. Hu, Z. L. Thai, J. Shen, J. Hu, X. Han, Y. Huang, Y. Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset, 2021. K. Huang, J. Guo, Z. Li, X. Ji, J. Ge, W. Li, Y. Guo, T. Cai, H. Yuan, R. Wang, et al. Math-perturb: Benchmarking llms math reasoning abilities against hard perturbations. arXiv:2502.06453, 2025. arXiv preprint Y. Huang and L. F. Yang. Gemini 2.5 pro capable of winning gold at imo 2025, 2025. URL https://arxiv.org/abs/2507.15855. N. Lambert, V. Pyatkin, J. Morrison, L. Miranda, B. Y. Lin, K. Chandu, N. Dziri, S. Kumar, T. Zick, Y. Choi, et al. Rewardbench: Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787, 2024. H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. SchulLets I. Sutskever, and K. Cobbe. man, In The Twelfth Interverify step by step. national Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=v8L0pN6EOi. T. Luong and E. Lockhart. Advanced version of gemini with deep think officially achieves gold-medal standard at the international mathematical olympiad. https://goo. gle/imo-gold, July 2025. S. Mishra, M. Finlayson, P. Lu, L. Tang, S. Welleck, C. Baral, T. Rajpurohit, O. Tafjord, A. Sabharwal, P. Clark, et al. Lila: unified benchmark for mathematical reasoning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 58075832, 2022. Moonshot AI. Kimi k2: Open agentic intelligence. https://moonshotai.github.io/ Kimi-K2/, July 2025. OpenAI. Introducing gpt-5. https://openai. com/index/introducing-gpt-5/, August 2025a. OpenAI. mini. introducing-o3-and-o4-mini/, 2025b. Introducing openai o3 and o4https://openai.com/index/ April A. Patel, S. Bhattamishra, and N. Goyal. Are nlp models really able to solve simple math word In Proceedings of the 2021 Conproblems? ference of the North American Chapter of the 11 Towards Robust Mathematical Reasoning Association for Computational Linguistics: Human Language Technologies, pages 20802094, 2021. L. Phan, A. Gatti, Z. Han, N. Li, J. Hu, H. Zhang, S. Shi, M. Choi, A. Agrawal, A. Chopra, et al. Humanitys last exam. arXiv preprint arXiv:2501.14249, 2025. Qwen Team. Qwen3: Think deeper, act faster. https://qwenlm.github.io/blog/ qwen3/, April 2025. S. Srivastava, A. PV, S. Menon, A. Sukumar, A. Philipose, S. Prince, S. Thomas, et al. Functional benchmarks for robust evaluation of reasoning performance, and the reasoning gap. arXiv preprint arXiv:2402.19450, 2024. T. H. Trinh, Y. Wu, Q. V. Le, H. He, and T. Luong. Solving olympiad geometry without human demonstrations. Nature, 625(7995):476 482, Jan. 2024. xAI. Grok 4. https://x.ai/news/grok-4, July 2025. K. Zheng, J. M. Han, and S. Polu. Minif2f: crosssystem benchmark for formal olympiad-level mathematics. arXiv preprint arXiv:2109.00110, 2021. 12 Towards Robust Mathematical Reasoning"
        },
        {
            "title": "Limitations",
            "content": "Our work has two primary limitations: evaluation cost and the risk of data contamination. Evaluation Cost. While our automatic grader, ProofAutoGrader, correlates strongly with human scores, it is not perfect substitute and can introduce noise. Consequently, definitive assessments still require verification by human experts, who are both costly and difficult to source. Future Data Contamination. The second limitation is the risk of long-term data contamination. As IMO-Bench is publicly released, its problems and solutions will likely be scraped and absorbed into future training datasets. This threatens the integrity of the benchmark, as models may achieve high scores by memorizing answers rather than demonstrating genuine reasoning. Preventing this form of benchmark decay remains significant, field-wide challenge. A. IMO-AnswerBench A.1. Examples We show examples of IMO-AnswerBench in Table 8. A.2. Subject Distribution and Robustification Examples of IMO-AnswerBench At the IMO, the problems are typically classified into four main categories: Algebra, Combinatorics, Geometry and Number Theory. Therefore, we also structure our IMO-AnswerBench in accordance to these four categories as well, where each category has exactly 100 problems. Algebra is one of the core competencies for Math Olympiad students and appears at all levels of competitions. Distinct from previous benchmarks (Hendrycks et al., 2021), IMO-Bench puts more emphasis on Math Olympiad topics, including inequalities, polynomials (including polynomial equations and factorization), functional equations, sequence problems and advanced topics such as Algebraic Number Theory. Combinatorics problems, despite requiring seemingly basic insights, are notoriously challenging. Successfully solving them serves as strong indicator of models reasoning capabilities. The combinatorics set of this benchmark contains problems covering Graph Theory, Enumerative Combinatorics (combinatorial counting problems), Extremal Combinatorics, Existence Combinatorics (problems asking the existence of certain combinatorial objects), Additive Combinatorics, Set Combinatorics, Tiling, Combinatorial Geometry, Operations (problems involving operations, often requiring finding invariant or monovariant properties), and Game Theory. Geometry problems at the IMO are well-known for their visual elegance. While there are several existing geometry benchmarks (Hendrycks et al., 2021), they do not cover Math Olympiad level problems. To address this discrepancy, IMO-Bench contains geometry problems with short answers spanning subcategories such as angle and sidelength computation, locus problems, and proof-based geometry problems, as well as unconventional categories such as 3D geometry and combinatorial geometry. Additionally, we would like to note that most Math Olympiad level geometry problems are proof-based, and so designing Math Olympiad level short-answer benchmark for geometry is highly non-trivial. Number Theory problems typically consist of problems involving objects and properties derived from integers and arithmetic functions, spanning various topics such as Diophantine equations, divisibility problems, polynomials, sequence problems, functional equation problems on the set 13 Towards Robust Mathematical Reasoning Subj. Source Original Robustified Austria MO 2017 Determine the maximum 洧 of 洧논 + 洧녽 + 洧녾 where 洧논, 洧녽 and 洧녾 are positive real numbers with Let 洧녩, 洧녪, 洧녫 be lengths of the sides of some triangle of positive area, satisfying 16洧논 洧녽洧녾 = (洧논 + 洧녽)2(洧논 + 洧녾)2. 洧녩2洧녪2 = 2(洧녩 + 洧녪 洧녫)(洧녪 + 洧녫 洧녩)(洧녫 + 洧녩 洧녪). Find the maximum value for 洧녩 + 洧녪 + 洧녫. USA TST 2005 Let 洧녵 be an integer greater than 1. For positive integer 洧녴, let 洧녡洧녴 = {1, 2, . . . , 洧녴洧녵}. Suppose that there exists 2洧녵-element set 洧녢 such that (a) each element of 洧녢 is an 洧녴-element subset of 洧녡洧녴; (b) each pair of elements of 洧녢 shares at most one common element; and (c) each element of 洧녡洧녴 is contained in exactly two elements of 洧녢. Determine the maximum possible value of 洧녴 in terms of 洧녵. For positive integer 洧녴, let 洧녡洧녴 = {1, 2, . . . , 25洧녴}. Supset 洧녢 such pose that there exists (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) 50-element (cid:123)(cid:122) (cid:125) (cid:124) Modify numerical value (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) that: 1. Each element of 洧녢 is an 洧녴-element subset of 洧녡洧녴; 2. Each pair of elements of 洧녢 shares at most one common element; 3. Each element of 洧녡洧녴 is contained in exactly two elements of 洧녢. Let 洧녞 be set of 50 random integers. Suppose we define function 洧녭 (洧논) = 洧논2 + 2洧논 + 1 . Determine the maximum (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:125) (cid:124) (cid:123)(cid:122) Add distractors possible value of 洧녴. USA TST 2024 CzechSlovak Math Olympiad 2017 Let 洧냢洧냣洧냤 be triangle with incenter 洧냪. Let segment 洧냢洧냪 intersect the incircle of triangle 洧냢洧냣洧냤 at point 洧냥. Suppose that line 洧냣洧냥 is perpendicular to line 洧냢洧냤. Let 洧녞 be point such that 洧냣洧녞 洧냢 = 洧녞 洧냢洧냪 = 90. Point 洧녟 lies on segment 洧냣洧냥 such that the circumcircle of triangle 洧냢洧냣洧녟 is tangent to line 洧냣洧냪. Point 洧녦 lies on line 洧녞洧녟 such that 洧냪 洧냢洧녦 = 洧녦 洧냢洧냤. Prove that 洧냢洧녦 洧녞 = 45. Let 洧녲 0 be an integer and suppose that there the number of ordered pairs (洧논, 洧녽) of integers satisfying 洧녲 = 洧논2 洧논 洧녽 + 2洧녽2 洧논 + 洧녽 Let 洧녦洧녧 洧녨 be triangle with incenter 洧냫. Let segment 洧녦 洧냫 meets the incircle of triangle 洧녦洧녧 洧녨 at point 洧. Suppose that the angle created by line 洧녧 洧 and line 洧녦 洧녨 is 90. Let 洧녠 be point such that 洧녧 洧녠洧녦 = 洧녠洧녦 洧냫 = 90. Point 洧녡 lies on segment 洧녧 洧 such that the circumcircle of triangle 洧녦洧녧 洧녡 is tangent to line 洧녧 洧냫. Point 洧녢 lies on line 洧녠洧녡 such that 洧냫洧녦洧녢 = 洧녢 洧녦 洧녨. Let 洧 be the value of 洧녦洧녢 洧녠 in terms of degree, . 洧 compute 3 (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:125) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:123)(cid:122) compute instead prove (cid:124) Find all even integers 洧녬 such that the number of ordered integer pairs (洧논, 洧녽) satisfying (洧논 + 2洧녽 洧녬)2 = 洧논 洧녽 (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:125) (cid:123)(cid:122) (cid:124) substitute 洧논 洧논 + 洧녽, 洧녽 洧녲 洧녽, 洧녬 2洧녲 is odd. Find all possible values of 洧녲. is even. Table 8 Examples in the IMO-AnswerBench, per category (Algebra, Combinatorics, Geometry, Number Theory). of integer, existence problems, problems involving arithmetic functions (such as divisor functions, fractional functions), set problems, number theoretic game problems and straategies such as modular analysis, divisor analysis and base representation problems. These problems serve as good representation of Math Olympiad problems at various levels and across different national, regional and international contests, as well as the topics covered in these contests. strong model performance would suggest high competence level as well as good knowledge coverage since certain problems can only be solved with particular problem solving strategy, without which the model would struggle to provide rigorous with the correct answer. 14 Towards Robust Mathematical Reasoning A.3. Effects of robustification To examine the effect of robustification for IMO-AnswerBench, we also evaluate on the original, unmodified problems and present the results in Table 9. The models perform significantly better on the original problems, where the gap could be as high as 11.2% for o4-mini (high reasoning). This indicates that our robustification effort does create significant challenge for the models. Model Grok 4 (Original) Grok 4 (Robustified) Gemini 2.5 Pro (Original) Gemini 2.5 Pro (Robustified) o4-mini (high reasoning) (Original) o4-mini (high reasoning) (Robustified) o3(Original) o3 (Robustified) Claude Sonnet 4 (Original) Claude Sonnet 4 (Robustified) Algebra Combinatorics Geometry Number Theory Overall accuracy 78.8% 75.5% 77.8% 73.4% 75.1% 71.2% 67.4% 62.8% 28.2% 20.6% 61.8% 55.9% 53.0% 48.0% 52.9% 46.6% 46.8% 43.0% 15.5% 17.8% 81.4% 80.1% 77.4% 74.2% 82.5% 78.4% 74.1% 70.6% 27.6% 26.0% 78.3% 80.9% 78.8% 77.1% 75.1% 75.3% 67.5% 68.0% 27.6% 27.6% 75.0% (+1.9) 73.1% 71.7% (+3.5) 68.2% 71.4% (+3.5) 67.9% 63.9% (+2.8) 61.1% 24.8% (+1.8) 23.0% Table 9 Comparison between IMO-AnswerBench results (Robustified) and results for IMO-AnswerBench before robustification (Original). Results are averaged over 8 samples. A.4. Towards Consistent Problem Statements and Answer Evaluation Another common issue with language models solving complex Math Olympiad problems is that these models often misinterpret the statement of such problems, or the problem formulation leads the models to produce unintended outputs. Thus, we employ several additional strategies on top of robustification to ensure that the models can interpret the problems properly as follows. Instead of asking for series of numbers satisfying certain conditions (which is hard to verify), we instead reformulate the problem so that its answer is unique number that is the sum or some other non-trivial function of many inputs. Simplifying the answer as much as possible to avoid confusion. Being more specific with the problem statement to excuse possible issues with special characters, such as angle degrees in geometry problems. Avoiding questions with binary answers (yes/no), such as existence questions (which are extremely common in Math Olympiad contests), as they can be guessed without solving the problem or proving the result rigorously. Instead, we will reformulate the problem in such way that it would produce non-trivial answer. A.4.1. Ensuring unique non-trivial answer Example 1 In this example, instead of asking the model to characterize all such numbers 洧녴, we ask the model to compute certain expression, which results in 1012, value that the model is unlikely to guess by mere chance. Original problem: \"For positive integer 洧녴, let 洧녩1, 洧녩2, . . . , 洧녩洧녴+1 satisfy 3洧녰 < 洧녩洧녰 < 3洧녰+1 for each 洧녰. Find the maximum and minimum possible values of (cid:214) 1洧논洧녴+1 洧녽洧논 洧녩洧논 洧녩洧녽 1 洧녩洧논 洧녩洧녽 . (1) 15 Towards Robust Mathematical Reasoning \" Original answer: maximum of 0 and minimum of 0 if 洧녴 is odd, and maximum of 1 and minimum of 1 if 洧녴 is even. Modified problem: For positive integer 洧녴, let 洧녩1, 洧녩2, . . . , 洧녩洧녴+1 satisfy 3洧녰 < 洧녩洧녰 < 3洧녰+1 for each 洧녰. Let 洧냢洧녴 = (cid:214) 1洧논洧녴+ 洧녽洧논 洧녩洧논 洧녩洧녽 1 洧녩洧논 洧녩洧녽 . (2) Find (cid:205)2025 洧녰=1 洧냢 洧녴 Modified answer: 1012 Example 2 In this example, instead of asking the model to characterize all solution tuples, which can be hard to evaluate in the natural language form, we ask the models to compute the sum of the elements. Original problem: Let 洧녩1, 洧녩2, . . . , 洧녩2025 be positive integers such that for each positive integer 洧녴, 洧녱洧녩洧녵 洧녱 (cid:170) (cid:174) (cid:172) is an integer. Find all possible (洧녩1, 洧녩2, . . . , 洧녩2025). (cid:169) (cid:173) (cid:171) 1(cid:170) (cid:174) (cid:172) 2025 (cid:169) (cid:173) (cid:171) 洧녱=1 1 洧녵+1 Original answer: (洧녩1, . . . , 洧녩2025) = (1, 洧녲, . . . , 洧녲) with 洧녲 = 2 + 3 + + 2025 = 2051324 Modified problem: Let 洧녩1, 洧녩2, . . . , 洧녩2025 be positive integers such that for each positive integer 洧녴, 1 洧녵+1 2025 (cid:169) (cid:173) (cid:171) 洧녱=1 (cid:169) (cid:173) (cid:171) 洧녱洧녩洧녵 洧녱 (cid:170) (cid:174) (cid:172) 1(cid:170) (cid:174) (cid:172) is an integer. Find all possible values of 洧녩1 + 洧녩2 + + 洧녩2025. Modified answer: 4151879777 Example 3 In this example, instead of asking the model to characterize all such numbers 洧녴, we ask the models to count the number of such numbers in certain range, which results in 1009, value that the model is unlikely to guess by mere chance. Original problem: Find all positive integers 洧녴 2 that satisfy the following condition: For any 洧녴 distinct positive integers (洧녵1, . . . , 洧녵洧녴), at least one of the following two conditions holds: 洧녵1 + . . . + 洧녵洧녴 is multiple of 洧녴, or there exists permutation (洧녲1, . . . , 洧녲洧녴) such that 洧녲1 + 2洧녲2 + . . . + 洧녴洧녲洧녴 is multiple of 洧녴. Original answer: All powers of 2 and all odd numbers Modified problem: Find the number of all positive integers 2 洧녴 2000 that satisfy the following condition: For any 洧녴 distinct positive integers (洧녵1, . . . , 洧녵洧녴), at least one of the following two conditions holds: 洧녵1 + . . . + 洧녵洧녴 is multiple of 洧녴, or there exists permutation (洧녲1, . . . , 洧녲洧녴) such that 洧녲1 + 2洧녲2 + . . . + 洧녴洧녲洧녴 is multiple of 洧녴. 16 Towards Robust Mathematical Reasoning Modified answer: 1009 A.4.2. Answer simplification Example In the example below the original answer mixes notations and adds potentially confusing quantifier, so we simplify it. Original Problem: Let 洧녞 be function from the set of integers to itself such that for all integers , 洧녴, 洧녞2+洧녴2 ( + 洧녴 1) = 洧녴洧녞(洧녴 1) + 뀛롐( 1) + ( + 洧녴 1). Find all possible functions 洧녞. Original answer: 洧녞 1 or 洧녞(洧논) = 洧논 + 1 for all 洧논 . Modified/simplified answer: 洧녞(洧논) = 1, 洧녞(洧논) = 洧논 + 1 A.4.3. Handling geometric quantities Example For geometry, if the model is asked to find an angle, we make sure to include in degrees in the problem statement. This prevents problems mixing radians and degrees and model misinterpretations of special characters marking degrees such as 틙o or 틙 { circ}. A.4.4. Reformulating questions with binary answers Example Below is an interesting example where the problem is very difficult but the answer is binary yes/no, which can be guessed without solving the problem. Therefore, instead of asking the model to show existence, we ask the model to find the smallest positive integer to satisfy such property, which retains the difficulty level while preventing the model from guessing the answer. Original problem: Is there positive integer 洧녵 such that (洧녩+洧녪) (洧녪+洧녫) (洧녫+洧녩)+洧녩+洧녪+洧녫 = 洧녵 for infinitely many 洧녩洧녪洧녫 integer triples (洧녩, 洧녪, 洧녫)? Original answer: It exists Modified problem: Find the smallest positive integer 洧녵 such that there exists infinitely many triple (洧녩, 洧녪, 洧녫) of distinct positive integers such that (洧녩+洧녪) (洧녪+洧녫) (洧녫+洧녩)+洧녩+洧녪+洧녫 = 洧녵. 4洧녩洧녪洧녫 Modified answer: 3 A.5. Query prompt for AnswerAutoGrader The following prompt was used to query the AnswerAutoGrader for IMO-AnswerBench"
        },
        {
            "title": "Deterministic Mathematical Autograder",
            "content": "# System Role: You are precise, automated grading system. Your sole function is to determine if the final answer provided in the Model Solution is mathematically equivalent to the Golden Answer. the reasoning or steps, only the final result."
        },
        {
            "title": "You must NOT grade",
            "content": "# 1. Grading Guidelines (Equivalence Rules) Equivalence is mandatory for correct grade. verify if the answers represent the exact same mathematical value or expression, even if the format differs. * **Algebraic Equivalence:** e.g., n(n+1)/2 is equivalent to You must verify the algebra. n^2/2 + n/2."
        },
        {
            "title": "You must rigorously",
            "content": "17 Towards Robust Mathematical Reasoning * **Numerical Equivalence:** e.g., 1/2 is equivalent to 0.5; sqrt(2)/2 is equivalent to 1/sqrt(2). * **Set/List Equivalence:** Unless specified as an ordered tuple/vector, the order of elements does not matter (e.g., {1, 2} is equivalent to {2, 1}). * **Partial Credit:** No partial credit is allowed."
        },
        {
            "title": "If the answer",
            "content": "is incomplete or partially incorrect, it is incorrect. * **No Answers:** If no clear, unambiguous final answer can be extracted, the solution must be graded as incorrect. # 3. Output Protocol (Strict Compliance Required) You must execute the task using two-part structure. follow this structure will result in task failure."
        },
        {
            "title": "Failure to",
            "content": "Analysis (Chain-of-Thought)** **Part 1: You MUST perform your analysis within <thinking></thinking> tags. Make your thinking concise. This section details your reasoning process and must follow these steps sequentially: 1. **Golden Answer:** State the Golden Answer. 2. **Extracted Model Answer:** State the extracted answer based on If none found, state \"No clear final the Extraction Protocol. answer found.\" 3. **Equivalence Analysis:** Compare the two answers using the Grading Guidelines. Detail the steps taken to verify mathematical equivalence (e.g., simplification, algebraic manipulation). before concluding they are different."
        },
        {
            "title": "You must actively try to prove they are the same",
            "content": "4. **Conclusion:** State the final determination (\"Correct\" or \"Incorrect\"). Final Grade** **Part 2: Immediately following the closing </thinking> tag, output **ONLY** the final grade. * If Correct: * If Incorrect: boxed{Incorrect} boxed{Correct} **CRITICAL CONSTRAINT: Do not add any text, explanations, or formatting outside the <thinking> tags or the final boxed{} output.** Output exmaple: <thinking> 1. **Golden Answer:** (, 4) (4, ) 2. **Extracted Model Answer:** (the empty set) 3. **Equivalence Analysis:** The Golden Answer is non-empty set of real numbers. Model Answer is the empty set. equivalent. Golden Answer contains an infinite number of elements. The empty set contains no elements, while the"
        },
        {
            "title": "The",
            "content": "18 Towards Robust Mathematical Reasoning 4. **Conclusion:** Incorrect </thinking> boxed{Incorrect} # 4. Input Data Here is the problem, model solution, and golden answer to grade: Problem: Model Solution: Golden Answer: {Problem_Statement} {Model_Solution} {Golden_Answer} B. IMO-Proof Bench B.1. Examples We show robustified examples of IMO-Proof Bench in Table 10. B.2. Proof Evaluation Guidelines for IMO-Proof Bench In proof-based problem, the desired conclusion usually is either already given (\"Prove that ...\") or easy to guess (\"Determine with proof whether ...\"). Evaluating solution consists of verifying that each logical step leading to the conclusion is valid. However, grading informal6 proofs contains inherently subjective elements, such as deciding whether particular claim is justified in sufficient detail. Thus, unlike for short answers, which are either correct or incorrect, it is more appropriate to evaluate proofs on higher resolution scale, where subjective elements matter less. Additionally, solution may make partial progress by proving some but not all of the steps of full solution. It is important to capture this during evaluation. Traditionally, proof-based Math Olympiad competitions, such as the IMO, score solutions on 7-point scale. For each problem, grading rubric outlines how many points are to be awarded for certain partial results. The great majority of solutions receive polarizing score: either 5-7 points for being essentially correct, or 0-2 points if the problem remains unsolved, generally dictated by specific criteria in the rubric. Although problems often admit multiple solutions, it is rare for solution to be so novel that it falls completely outside of the rubric (which usually covers the 1-2 most common solution approaches). Thus, despite some elements of subjectivity as mentioned above, scores are typically quite consistent across graders. For further insight into how Math Olympiad grading works, refer to Chen (2023). B.3. Details of Gemini 2.5 Pro with (Huang & Yang, 2025) We use the exact agentic framework proposed in (Huang and Yang, 2025), which has been open sourced at https://github.com/lyang36/IMO25 and also contains exact hyperparameters in its binary flags. We used the same thinking budget (32K tokens) per model call as mentioned in the paper. Given an initial solution, single pipeline consists of repeated iterations (at most 30) of selfverification and bug-fixing on it. Specifically, if the current solution passes self-verification fixed number (5) of times, then the solution is returned, but if at any time self-verification does not 6i.e. written in natural language, as opposed to formal language such as LEAN. 19 Towards Robust Mathematical Reasoning pass, then the model is instructed to observe any mistakes (bugs) and fix them, and restart the self-verification process. Note that if consecutive number (10) of verifications fail, then the pipeline exits without solution. This entire pipeline will be run in parallel multiple times (100) as well, until there is at least one solution returned from any run. Theoretically the model could fail to find any solution after all parallel runs, which occurred for two IMO-Proof Bench (Advanced) problems. B.4. Common Model Mistakes Here we list some common mistakes the models were making according to the graders. B.4.1. Polynomial Assumption PB-Basic-004: We want to find all strictly increasing functions   such that: 1. 洧녮 is surjective. 2. 洧녮(洧녮(洧논)) = 洧녮(洧논) + 20洧논, 洧논 . common mistake that language models typically make while solving problems is assuming strong assumptions on the problem without legitimate or substantial justification for the assumptions. An instance of this behavior is the functional equation problem PB-Basic-004 as shown in Table 11. The proof of o3 on this problem begins by considering the linear cases. \"We wish to find all strictly increasing and surjective functions 洧녮 :   satisfying 洧녮(洧녮(洧논)) = 洧녮(洧논) + 20洧논 for all 洧논 . natural first step is to check if linear function works. Suppose 洧녮(洧논) = 洧녩洧논 + 洧녪 ....\" After figuring out 洧녩 = 5 and 洧녪 = 0, the model claims that it found the unique solution, even though the only cases it checked were when 洧녮 is linear. \".... Thus, the unique solution is 洧녮(洧논) = 5洧논.\" While the final answer is indeed correct, which typical short answer benchmark would consider correct, the proof is not rigorous and would get little to no points in proof-based competition such as the IMO. B.4.2. Final Answer Guessing PB-Basic-005: Let 洧녞 be polynomial with real coefficients whose leading coefficient is 1. Suppose that for all nonzero real numbers 洧논, we have 洧녞(洧논) + 洧녞(1/洧논) = 洧녞 (洧논+1/洧논 )+洧녞 (洧논1/洧논 ) . Determine all possibilities for 洧녞. In addition, there are the examples where models try to guess the final answer by inspecting the cases when the variables are small. They do not try to actually prove why the guessed answer is correct. In the example problem PB-Basic-005, the model does case work with degree 洧녵 = 2 and 20 Towards Robust Mathematical Reasoning degree 洧녵 = 4 and guesses the answer is 洧녞(洧논) = 洧논2 and 洧녞(洧논) = 洧논4 + 洧녩洧논2 + 洧녪 without showing these are correct answers (in fact, the correct answer should have been 洧녞(洧논) = 洧녩(洧논4 + 6) + 洧녪洧논2) nor that these are all the answers. That being said, the models often can get lot of correct answers by simply guessing rather than carrying out elaborate derivations to arrive at the correct answer. For more information, we refer the readers to the full example in Table 11. B.4.3. Commonly Missed Easy Problems Among many problems that models were not able to solve, we present here the following two pre-IMO difficulty problems from Proof Bench-basic. PB-Basic-008, (Modified) All-Russia MO 2002: Let 洧녩, 洧녪, 洧녫 be positive reals such that 洧녩 + 洧녪 + 洧녫 = 1, prove that 3(洧녩洧녪 + 洧녪洧녫 + 洧녫洧녩). 洧녫 3 洧녩 + 洧녪 + This problem is standard symmetric homogeneous inequality in three variables, whose equality condition is 洧녩 = 洧녪 = 洧녫. This is one of the easiest type of inequalities one could encounter in high school level math competition. However, not single model we tested got even partial score on this one. PB-Basic-016, (Modified) USAMO 1994 Problem 2: 101 stones are arranged in circle, numbered 1 through 101 in order (so that stone 1 is next to stone 101). Each stone is painted either red, white, or blue. Initially, stone 101 is the only blue stone. Among the remaining stones, even-numbered stones are red, and odd-numbered stones are white. We perform sequence of modifications to the coloring, where in each step, we may choose stone and repaint it different color, as long as we ensure that no two adjacent stones are ever the same color. Prove that it is not possible to eventually reach state where again stone 101 is the only blue stone, but among the remaining stones, all even-numbered stones are white and all odd-numbered stones are red. This is standard, easy combinatorics problem that uses the invariance principle. One is required to test various quantities related to the problem, and find an invariant quantity to solve the problem. In this case, the difference between the number of blue followed by white and the number of white followed by blue is invariant. However, no model found any invariant that is related to the problem. B.5. Query prompt for ProofAutoGrader The following prompt was used to query ProofAutoGrader for IMO-Proof Bench You are an expert grader for the International Mathematics Olympiad (IMO). Your task is to evaluate proposed solution strictly and rigorously. Keep in mind the standards at the IMO are extremely high: only arguments that are logically sound, complete, and precise should be rewarded. ### General Scoring Rubric Scores are assigned on 0-7 scale. The general guidelines are: * **7 Points (Correct):** The solution is complete, correct, and fully rigorous. If the submission contains incorrect attempts or lines of reasoning but ultimately presents complete and correct 21 Towards Robust Mathematical Reasoning solution, it should still be awarded full points; the presence of earlier, discarded work does not detract from the final correct proof. * **6 Points (Almost Correct):** The solution is almost correct with sound core argument, but contains minor errors in calculation or small gaps in logic. Missing proofs for major components, unjustified claims, or sketchy arguments are **not** eligible for 6 points. * **1 Point (Partial Progress):** The solution demonstrates substantial progress explicitly mentioned in the grading guidelines. without making substantive headway, or proving partial results not mentioned in the grading guidelines are generally **not** eligible for this score. Initial observations, reformulating the problem * **0 Points (Incorrect):** The solution doesnt make substantial progress that is key step in the full solution or is fundamentally flawed. or lacking rigor also fall in this category."
        },
        {
            "title": "All partial progress without key results",
            "content": "### Input Data and Interpretation You are provided with the following: 1. **Problem Statement:** The IMO problem. 2. **Ground Truth Solution:** reference solution."
        },
        {
            "title": "Assume this",
            "content": "solution is correct. It demonstrates one valid approach. 3. **Specific Grading Guidelines:** Criteria for awarding credit for this specific problem. the General Scoring Rubric, especially for partial credit."
        },
        {
            "title": "These guidelines take precedence over",
            "content": "4. **Proposed Solution:** The student submission. ### Evaluation Process You must follow this structured process: 1. **Analyze References:** Meticulously read and understand the problem and Ground Truth Solution check the Specific Grading Guidelines. the criteria for partial credit."
        },
        {
            "title": "Identify the key steps for a complete solution and",
            "content": "2. **Step-by-Step Verification:** Verify the logical validity and rigor of every step. **Make sure you fully understand every piece of logic errors. behind each step of the proposed solution, you must be careful for solutions that pretend to be correct.** Identify all flaws, gaps, assumptions, and 3. **Assess Progress:** Determine the extent of non-trivial progress made. 4. **Score Determination:** Compare the findings against the Specific Grading Guidelines and the General Rubric to determine the final score. ### Output Requirements You must provide your final score in the format <points>N out of 7</points>. Ensure the <points> block is used **only once**, as 22 Towards Robust Mathematical Reasoning your answer will be parsed based on the first <points> </points> block that appears in your whole response. **PROBLEM STATEMENT** {problem_statement} **GROUND-TRUTH SOLUTION** {solution} **SPECIFIC GRADING GUIDELINES** {guidelines} **PROPOSED SOLUTION** {student_answer} Present your detailed thought process and formal justification based on the scoring rubric and grading guidelines, and finally present your final score in the format below. [Select one of the following options] <points>7 out of 7</points> <points>6 out of 7</points> <points>1 out of 7</points> <points>0 out of 7</points> B.6. Limitations of ProofAutoGrader Despite high correlation with human grades, ProofAutoGrader still has several systematic errors and limitations, including general tendency to overestimate scores, occasional failure to identify highlevel logical errors, and being prone to be overly punitive for minor formatting issues or unconventional yet correct solutions. We demontrate specific examples of these behavior in Table 12. In PB-Basic 002, the model solution makes logical error by asserting 2(4 4 洧논 洧녽洧녾洧노) 洧논 洧녽洧녾洧노 directly from 洧논 + 洧녽 + 洧녾 + 洧노 4 4 洧논 洧녽洧녾洧노 and 2(洧논 + 洧녽 + 洧녾 + 洧노) 洧논 洧녽洧녾洧노. This comes from an incorrect assumption that if 洧냢 洧냣 and 洧냢 洧냤, then 洧냣 洧냤. Such \"specious\" errors, while seemingly plausible and easy to overlook without deep understanding of the problem, are critical and can invalidate an entire solution. ProofAutoGrader often fails to identify such deceptive logical inconsistencies. In PB-Basic 027, the model produces novel solution entirely different from the established ground truth and grading guidelines. The solution was largely correct, but its Key Lemma omits critical condition that the segment 洧녞洧녟 must have fixed slope. While the lemma is false as stated, supplying this condition makes its proof an immediate consequence of homothety. Since the rest of the solution is complete, the human grader awarded it 6 out of 7 points. However, because the lemma is technically incorrect, ProofAutoGrader marks the entire solution as wrong. This case demonstrates that ProofAutoGrader struggles to identify partial progress in solutions not anticipated by the grading guidelines, leading to overly punitive assessments for minor issues. C. IMO-GradingBench C.1. Grade distribution for IMO-GradingBench This section presents the human-assigned grade distribution for the IMO-GradingBench benchmark. As shown in Figure 6, the aggregate count of correct versus incorrect grades across the entire dataset 23 Towards Robust Mathematical Reasoning Figure 6 Grade distribution across examples in IMO-GradingBench is balanced. However, the distribution of grades (correct, almost, partial, incorrect) is not uniform on perproblem basis. This variance is expected as it reflects the natural distribution of scores that proofevaluation models will encounter in grading solutions, as problems inherently differ in difficulty. C.2. Query Prompt This section details the prompts used for the three evaluation settings in IMO-GradingBench. common definition of the scoring criteria is used across all settings, inserted into the prompts as indicated by {SCORING_CRITERIA}. C.3. Grader Prompt The following prompt was used for the vanilla setting: Towards Robust Mathematical Reasoning Carefully analyze the given problem statement and the proposed solution, and then write out your analysis regarding the correctness of the proposed solution. After the analysis, you must provide score based on the following criteria: incorrect: The solution is completely incorrect or irrelevant. partial: The solution is partially correct but has significant errors or omissions. almost: The solution is almost correct but contains minor errors or inaccuracies. correct: The solution is fully correct and complete. The very last part of your response must be only one of the following words: incorrect, partial, almost, or correct. Problem:{problem} Solution:{solution} C.4. Label extraction prompt The following prompt was used to extract the label from model response for IMO-GradingBench. Note that in the majority of cases, the last word of the model (grader) response is one of incorrect, partial, almost, or correct. As result, we first use python to extract the model grades. We only use prompting to extract the model grades when the last word in the model response is empty or is some different words. ## Instructions for Extracting Final Scores **Objective:** Given an response of an evaluation prompt, extract the final score presented within the response and format it specifically. **Process:** 1. **Analyze the response:** Scan the response to identify the final score provided by the evaluator. 2. **Extract and format the final answer:** Present the extracted score on new line, preceded exactly by \"Final answer: \". **Formatting Rules:** * **Evaluation Categories:** The expected output must be one of the following categories: incorrect, or not found. correct, partial, almost, * **Score Identification:** The extraction is based on identifying the keyword used by the evaluator to summarize their conclusion. The criteria associated with these keywords are: * **incorrect:** The evaluator concluded that the solution is completely incorrect or irrelevant. * **partial:** The evaluator concluded that the solution is partially correct but has significant errors or omissions. * **almost:** The evaluator concluded that the solution is almost correct but contains minor errors or inaccuracies. * **correct:** The evaluator concluded that the solution is fully correct and complete. 25 Towards Robust Mathematical Reasoning * **not_found:** The evaluation response does not clearly contain one of the four explicit scores listed above. * **Extraction:** Determine the provided score from the response and extract the category (correct, partial, almost, or incorrect). the text, the output must be not_found."
        },
        {
            "title": "If a score cannot be reliably identified within",
            "content": "**Note:** No additional markings or explanations are needed beyond \"Final answer: \" and the extracted answer. Below is the response: {Model Response} 26 Towards Robust Mathematical Reasoning Source Original Robustified IMO 24 P1 Determine all real numbers 洧띺 such that, for every positive integer 洧녵, the integer 洧띺 + 2洧띺 + + 洧녵洧띺 For real number 洧, let 洧냢(洧) denote the fractional part of 2洧 in its decimal representation. For real number 洧 and positive integer 洧녵, define 洧냣(洧녵, 洧) as is multiple of 洧녵. (Note that 洧녾 denotes the greatest integer less than or equal to 洧녾. For example, 洧랢 = 4 and 2 = 2.9 = 2.) 洧냣(洧녵, 洧) = 洧녵 洧녲=1 洧냢(洧녲洧). Find all positive real numbers 洧 such that 洧녵(洧녵 + 1)洧 洧냣(洧녵, 洧) is multiple of 洧녵 for all positive integers 洧녵. IMO 24 P2 Determine all pairs (洧녩, 洧녪) of positive integers for which there exist positive integers 洧녮 and 洧녜 such that For given positive integers 洧논 and 洧녽, we define sequence (洧녩洧녵)洧녵1 where 洧녩洧녵 is equal to IMO 24 P5 (cid:32) gcd 洧논洧녵 + 洧녽, ( 洧녽 洧논) (cid:33) (cid:33) 洧녽洧녰洧논洧녵洧녰1 1 (cid:32)洧녵1 洧녰= for all 洧녵 . Find all pairs (洧논, 洧녽) of positive integers such that the limit of the sequence (洧녩洧녵) exists. On table of size 3002 3001, stone is placed on the leftmost cell of the first row. James and Peter play game on this table. Peter selects 3000 cells, under the rule that he must choose one from each row except the first and last rows (i.e., the 1st and 3002th row), and there must be at most one selected cell in each column. James knows this rule too, but he doesnt know which cells Peter selected. The goal of James is to move the stone to the last row, avoiding the cells selected by Peter. The stone can only move to adjacent cells on the table. If the stone enters cell selected by Peter, James receives penalty of 1 point, and the stone returns to its initial position (i.e., the leftmost cell). Find the smallest positive integer 洧녵 such that there exists method for James to achieve his goal before receiving penalty of 洧녵 points. gcd(洧녩洧녵 + 洧녪, 洧녪洧녵 + 洧녩) = 洧녮 holds for all integers 洧녵 洧녜. (Note that gcd(洧논, 洧녽) denotes the greatest common divisor of integers 洧논 and 洧녽.) Turbo the snail plays game on board with 2024 rows and 2023 columns. There are hidden monsters in 2022 of the cells. Initially, Turbo does not know where any of the monsters are, but he knows that there is exactly one monster in each row except the first row and the last row, and that each column contains at most one monster. Turbo makes series of attempts to go from the first row to the last row. On each attempt, he chooses to start on any cell in the first row, then repeatedly moves to an adjacent cell sharing common side. (He is allowed to return to previously visited cell.) If he reaches cell with monster, his attempt ends and he is transported back to the first row to start new attempt. The monsters do not move, and Turbo remembers whether or not each cell he has visited contains monster. If he reaches any cell in the last row, his attempt ends and the game is over. Determine the minimum value of 洧녵 for which Turbo has strategy that guarantees reaching the last row on the 洧녵-th attempt or earlier, regardless of the locations of the monsters. Table 10 Examples of robustified problems, based on the IMO 2024 competition, for IMO-Proof Bench. 27 Towards Robust Mathematical Reasoning Problem Model Solution PB-Basic-004: We want to find all strictly increasing functions   such that: 1. 洧녮 is surjective. 2. 洧녮(洧녮(洧논)) = 洧녮(洧논) + 20洧논, 洧논 . PB-Basic-005: Let 洧녞 be polynomial with real coefficients whose leading coefficient is 1. Suppose that for all nonzero real numbers 洧논, we have 洧녞(洧논) + 洧녞(1/洧논) = 洧녞 (洧논+1/洧논 )+洧녞 (洧논1/洧논 ) . Determine all possibilities for 洧녞. 2 First, let us restate the problem clearly: We want to find all strictly increasing functions 洧녮 :   such that: 1) 洧녮 is surjective, 2) 洧녮(洧녮(洧논)) = 洧녮(洧논) + 20洧논, for all 洧논 . 1. Guess and check for linearity common approach for such functional equations is to hypothesize that might be linear (i.e., of the form 洧녮(洧논) = 洧녩洧논 + 洧녪). Let us try this: Assume 洧녮(洧논) = 洧녩洧논 + 洧녪, where 洧녩, 洧녪 . Compute 洧녮(洧녮(洧논)): 洧녮(洧녮(洧논)) = 洧녮(洧녩洧논 + 洧녪) = 洧녩(洧녩洧논 + 洧녪) + 洧녪 = 洧녩2洧논 + 洧녩洧녪 + 洧녪. According to the functional equation, we need: 洧녮(洧녮(洧논)) = 洧녮(洧논) + 20洧논. Hence, 洧녩2洧논 + 洧녩洧녪 + 洧녪 = (洧녩洧논 + 洧녪) + 20洧논 = 洧녩洧논 + 20洧논 + 洧녪. Matching coefficients: Coefficient of x: 洧녩2 = 洧녩 + 20. Constant term: 洧녩洧녪 + 洧녪 = 洧녪. From the constant term 洧녩洧녪 + 洧녪 = 洧녪, we can factor out 洧녪: 洧녪(洧녩 + 1) = 洧녪 either 洧녪 = 0 or 洧녩 + 1 = 1. If 洧녩 + 1 = 1, then 洧녩 = 0, but that would force 洧녮(洧논) = 洧녪, constant function, which cannot be strictly increasing nor surjective over all . Hence, that is not valid solution. Therefore, we must have = 0. Now the equation 洧녩2 = 洧녩 + 20 remains. Rewrite it as 洧녩2 洧녩 20 = 0. Factor or use the quadratic formula: (洧녩 5)(洧녩 + 4) = 0. This gives 洧녩 = 5 or 洧녩 = 4. 2. Strictly increasing and surjective requirement Since must be strictly increasing: If = -4, then would be strictly decreasing, which is not allowed. Hence must be positive, namely = 5. So the only candidate under our linear assumption is 洧녮(洧논) = 5洧논. Check surjectivity: The map 洧논 5洧논 is surjective, since for any 洧녽 , we can choose 洧논 = 洧녽/5 to get 5洧논 = 洧녽. It is also strictly increasing because 5 > 0. 3. Verification in the original equation Finally, verify 洧녮(洧논) = 5洧논 satisfies 洧녮(洧녮(洧논)) = 洧녮(洧논) + 20洧논: 洧녮(洧녮(洧논)) = 洧녮(5洧논) = 5(5洧논) = 25洧논. 洧녮(洧논) + 20洧논 = 5洧논 + 20洧논 = 25洧논. These match, so 洧녮(洧논) = 5洧논 indeed satisfies both the functional equation and the given conditions. Conclusion Therefore, the only strictly increasing surjective function 洧녮 :   satisfying 洧녮(洧녮(洧논)) = 洧녮(洧논) + 20洧논 is 洧녮(洧논) = 5洧논. We consider polynomials of degree 2 and 4. For degree 2, we found 洧녞(洧논) = 洧논2 is solution. For degree 4, we found 洧녞(洧논) = 洧논4 + 洧녪洧논2 + 洧녫洧논 + 6. We found 洧녫 = 0. So 洧녞(洧논) = 洧논4 + 洧녪洧논2 + 6 is solution for any real number 洧녪. We verified that 洧녞(洧논) = 洧논2 and 洧녞(洧논) = 洧논4 + 洧녪洧논2 + 6 satisfy the given condition. We showed that polynomials of odd degree are not solutions, and polynomials of degree 0 and 1 are not solutions. We need to show that there are no other solutions. Assume degree of 洧녞 is 洧녵. If 洧녵 is odd, we showed no solution. So 洧녵 must be even. Let 洧녵 = 2洧녲. We have shown that for 洧녵 = 2, 洧녞(洧논) = 洧논2 and for 洧녵 = 4, 洧녞(洧논) = 洧논4 + 洧녪洧논2 + 6. Final Answer: The final answer is 洧녞(洧논) = 洧논2 or 洧녞(洧논) = 洧논4 + 洧녪洧논2 + 6 for some real number 洧녪. Table 11 Examples of common mistakes that the models made for IMO-Proof Bench. Towards Robust Mathematical Reasoning Problem Model Solution and Grade PB-Basic 002: Show that 洧논2 + 洧녽2 + 洧녾2 + 洧노2 洧논 洧녽洧녾洧노 for any positive real numbers 洧논, 洧녽, 洧녾, 洧노 that satisfy 2(洧논 + 洧녽 + 洧녾 + 洧노) 洧논 洧녽洧녾洧노. Model Solution: Of course. This is great problem that elegantly combines given condition with target inequality. The key is to use the given condition to constrain the value of the product 洧논 洧녽洧녾洧노. Here is the step-by-step proof. **Step 1: Use the given condition and the AM-GM inequality to find bound for the product 洧논 洧녽洧녾洧노.** We are given two pieces of information for positive real numbers 洧논, 洧녽, 洧녾, 洧노: 1. The condition: 2(洧논 + 洧녽 + 洧녾 + 洧노) 洧논 洧녽洧녾洧노 2. The inequality to prove: 洧논2 + 洧녽2 + 洧녾2 + 洧노2 洧논 洧녽洧녾洧노 Lets start with the Arithmetic Mean-Geometric Mean (AM-GM) inequality for the four numbers 洧논, 洧녽, 洧녾, 洧노: 洧논 + 洧녽 + 洧녾 + 洧노 4 洧논 洧녽洧녾洧노 Multiplying both sides by 4, we get: 洧논 + 洧녽 + 洧녾 + 洧노 4 4 洧논 洧녽洧녾洧노 Now, we can substitute this result into the given condition 2(洧논 + 洧녽 + 洧녾 + 洧노) 洧논 洧녽洧녾洧노: 2(4 4 8 洧논 洧녽洧녾洧노) 洧논 洧녽洧녾洧노 洧논 洧녽洧녾洧노 洧논 洧녽洧녾洧노 (. . .) Human Grade: 0 out of 7 Autograder Verdict: ### Step-by-Step Verification 1. **Step 1: Bounding the product xyzt** * The student starts with the given condition: 2(洧논 + 洧녽 + 洧녾 + 洧노) 洧논 洧녽洧녾洧노. * They correctly apply the AM-GM inequality to 洧논, 洧녽, 洧녾, 洧노: 洧논 + 洧녽 + 洧녾 + 洧노 4 4 洧논 洧녽洧녾洧노) 洧논 洧녽洧녾洧노, which simplifies to 8 4 洧논 洧녽洧녾洧노. * Substituting this into the given condition yields 2(4 洧논 洧녽洧녾洧노 洧논 洧녽洧녾洧노. (. . .) The solution is complete, correct, and fully rigorous. It meets all the criteria for perfect score according to the General Scoring Rubric. It does not contain any errors, minor or otherwise. The method is different from the provided ground truth solution (its direct proof rather than proof by contradiction), but it is equally valid and elegant. <points>7 out of 7</points> Model Solution: Let 洧냩 be the orthocenter of triangle 洧냢洧냣洧냤. Let 洧냩洧녩 and 洧냩洧녪 be the reflections of 洧냩 about 洧냢洧냤 and 洧냣洧냤, respectively; then 洧냩洧녩, 洧냩洧녪 lie on the circumcircle ( 洧냢洧냣洧냤). Note that reflecting the line 洧냤 洧냩 about 洧냢洧냤 gives the line 洧냤 洧냩洧녩, hence 洧냦1 lies on 洧냤 洧냩洧녩; similarly 洧냦2 lies on 洧냤 洧냩洧녪. Key lemma (fixed tangent direction): Let 洧녳 = 洧냢洧냤, 洧1 = 洧냤 洧냩洧녩, 洧2 = 洧냤 洧냩洧녪. For any circle 洧랪 through 洧냤 intersecting 洧녳, 洧1, 洧2 again at 洧녦, 洧녞, 洧녟 respectively, the angle between the tangent to 洧랪 at 洧녦 and 洧녦洧냤 is constant (independent of the chosen 洧녞 洧1 and 洧녟 洧2). Indeed, in the cyclic quadrilateral 洧녦 洧녞洧냤洧녟 we have: 洧녦洧냤洧녞 = (洧녳, 洧1) and 洧녦洧냤洧녟 = (洧녳, 洧2), both constants; 洧녦 洧녞洧냤 = 洧녦 洧녟洧냤 (equal angles subtending chord 洧녦洧냤); hence 2洧녦 洧녞洧냤 + 洧녦洧냤洧녞 + 洧녦洧냤洧녟 = 180, so 洧녦 洧녞洧냤 = 90 ((洧녳, 洧1) + (洧녳, 洧2))/2 is constant; by the tangentchord theorem, this 洧녦 洧녞洧냤 equals the angle between the tangent at 洧녦 PB-Basic 027: Given an acute triangle 洧냢洧냣洧냤, let 洧냥 and 洧냦 be the feet of the altitudes from 洧냣 to 洧냢洧냤 and 洧냤 to 洧냢洧냣, respectively. Let 洧냦1 and 洧냦2 be the reflections of 洧냦 with respect to 洧냢洧냤 and 洧냣洧냤, respectively. If 洧녦 (not equal to 洧냤) is an intersection point of the circumcircle of 洧냤洧냦1洧냦2 and 洧냢洧냤, and 洧녝 is the circumcenter of 洧냤洧냦1洧냦2, prove that 洧녦洧녝 is perpendicular to 洧냥洧냦. and 洧녦洧냤. (. . .) Human Grade: 6 out of 7 Autograder Verdict: (. . .) ### Assessment and Score Determination The solutions overall structure is elegant, but it hinges entirely on \"Key Lemma\" which is false and whose proof is invalid. The argument is fundamentally flawed. <points>0 out of 7</points> Table 12 Examples of failure cases of ProofAutoGrader."
        }
    ],
    "affiliations": [
        "Google DeepMind"
    ]
}