{
    "paper_title": "How Much Do LLMs Hallucinate across Languages? On Multilingual Estimation of LLM Hallucination in the Wild",
    "authors": [
        "Saad Obaid ul Islam",
        "Anne Lauscher",
        "Goran Glavaš"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In the age of misinformation, hallucination -- the tendency of Large Language Models (LLMs) to generate non-factual or unfaithful responses -- represents the main risk for their global utility. Despite LLMs becoming increasingly multilingual, the vast majority of research on detecting and quantifying LLM hallucination are (a) English-centric and (b) focus on machine translation (MT) and summarization, tasks that are less common ``in the wild'' than open information seeking. In contrast, we aim to quantify the extent of LLM hallucination across languages in knowledge-intensive long-form question answering. To this end, we train a multilingual hallucination detection model and conduct a large-scale study across 30 languages and 6 open-source LLM families. We start from an English hallucination detection dataset and rely on MT to generate (noisy) training data in other languages. We also manually annotate gold data for five high-resource languages; we then demonstrate, for these languages, that the estimates of hallucination rates are similar between silver (LLM-generated) and gold test sets, validating the use of silver data for estimating hallucination rates for other languages. For the final rates estimation, we build a knowledge-intensive QA dataset for 30 languages with LLM-generated prompts and Wikipedia articles as references. We find that, while LLMs generate longer responses with more hallucinated tokens for higher-resource languages, there is no correlation between length-normalized hallucination rates of languages and their digital representation. Further, we find that smaller LLMs exhibit larger hallucination rates than larger models."
        },
        {
            "title": "Start",
            "content": "How Much Do LLMs Hallucinate across Languages? On Multilingual Estimation of LLM Hallucination in the Wild Saad Obaid ul Islam1 Anne Lauscher2 Goran Glavaš1 1WüNLP, CAIDAS, University of Würzburg {saad.obaid-ul-islam,goran.glavas}@uni-wuerzburg.de 2Data Science Group, University of Hamburg anne.lauscher@uni-hamburg.de"
        },
        {
            "title": "Abstract",
            "content": "In the age of misinformation, hallucination the tendency of Large Language Models (LLMs) to generate non-factual or unfaithful responsesrepresents the main risk for their global utility. Despite LLMs becoming increasingly multilingual, the vast majority of research on detecting and quantifying LLM hallucination are (a) English-centric and (b) focus on machine translation (MT) and summarization, tasks that are less common in the wild than open information seeking. In contrast, we aim to quantify the extent of LLM hallucination across languages in knowledge-intensive long-form question answering. To this end, we train multilingual hallucination detection model and conduct large-scale study across 30 languages and 6 open-source LLM families. We start from an English hallucination detection dataset and rely on MT to generate (noisy) training data in other languages. We also manually annotate gold data for five highresource languages; we then demonstrate, for these languages, that the estimates of hallucination rates are similar between silver (LLMgenerated) and gold test sets, validating the use of silver data for estimating hallucination rates for other languages. For the final rates estimation, we build knowledge-intensive QA dataset for 30 languages with LLM-generated prompts and Wikipedia articles as references. We find that, while LLMs generate longer responses with more hallucinated tokens for higher-resource languages, there is no correlation between length-normalized hallucination rates of languages and their digital representation. Further, we find that smaller LLMs exhibit larger hallucination rates than larger models."
        },
        {
            "title": "Introduction",
            "content": "Generalizing seamlessly to (seemingly) arbitrary language understanding, reasoning, and generation tasks, Large Language Models (LLMs) (Kojima et al., 2022; Dubey et al., 2024; Aryabumi et al., 2024; Yang et al., 2024) have arguably become the first ubiquitously adopted language technology, with application ranging from search engines (Xiong et al., 2024), interactive agents (Teubner et al., 2023) and knowledge retrieval (Yu et al., 2023) to various content generation tasks (Liu et al., 2022b). Their utility, however, is hindered by their tendency to hallucinate (Maynez et al., 2020; Zhou et al., 2021; Ji et al., 2023; Zhang et al., 2023), that is, produce information that is either (i) inaccurate or factually incorrect with respect to the objective state of the world (e.g., in open-ended question answering) or (ii) unfaithful with respect to some reference (e.g., in summarization). Consequently, large body of work on tackling LLM hallucination has emerged, with efforts falling into the three main areas: (1) detection, i.e., identification of the hallucinated content; (2) evaluation, primarily focusing on measures for quantifying the extent and severity of hallucinations ; and (3) mitigation, focusing on mitigating hallucinative tendencies of LLMs (Ji et al., 2023). While significant progress has been made in English (Maynez et al., 2020; Liu et al., 2022a; Obaid ul Islam et al., 2023; Chen et al., 2023; Lin et al., 2022; Kasai et al., 2024; Mishra et al., 2024; Vu et al., 2023), hallucination evaluation efforts targeting other languages have been much sparser (Clark et al., 2023; Guerreiro et al., 2023; Herrlein et al., 2024; Shafayat et al., 2024). Moreover, these efforts have primarily targeted high-resource languages (Qiu et al., 2023; Shafayat et al., 2024) with benchmarks limited to reference-based tasks text summarization (Clark et al., 2023; Aharoni et al., 2022) and machine translation (Dale et al., 2023; Guerreiro et al., 2023). While highly relevant, these tasks are arguably less representative of LLM usage in the wild (Trippas et al., 2024), where asking knowledge-intensive questions (and expecting free-form answers) is more prominent. In this work, we address the above gaps in mul5 2 0 2 0 ] . [ 2 9 6 7 2 1 . 2 0 5 2 : r Figure 1: Illustration of our approach for estimating hallucination rates in the wild. Hallucination Detection and Model Evaluation (left side): (1) We automatically translate the English FAVA (Mishra et al., 2024) dataset to 30 languages and train our multilingual hallucination detection (HD) model on this (noisy) multilingual training data; (2) We synthesize silver multilingual hallucination evaluation dataset by prompting state-of-the-art LLM (GPT-4) to introduce hallucinations in its answers to knowledge-seeking questions; for subset of five high-resource languages, we additionally collect gold (i.e., human) hallucination annotations; we dub this 30-language evaluation benchmark MFAVA. We use MFAVA to estimate HD models per-language performances (precision and recall). Hallucination Rate Estimation in the Wild (right side): (3) We estimate the hallucination rates for all 30 languages and six different LLM families from the number of detections of the HD model and its performance. tilingual hallucination detection and evaluation research with the ultimate goal of estimating the in the wild hallucination rates of LLMs across languages. Multilingual estimation of such hallucination rates is challenging due to the scarcity of multilingual hallucination benchmarks covering open-ended knowledge-seeking tasks that are representative of real-world LLM usage: unlike in reference-based generation tasks like summarization and machine translation, LLMs often generate long-form responses to open-ended questions, requiring more comprehensive evaluation approaches (Wei et al., 2024b). Concretely, we present largescale study that estimates hallucination rates for 30 languages (both high(er)- and low(er)-resource languages). Our main contributions are as follows: (1) We train multilingual hallucination detection model; to this end, we resort to the translatetrain approach (Artetxe et al., 2023; Ebing and Glavaš, 2024), i.e., we machine-translate an English hallucination-labeled training dataset to all other languages; (2) We create (i.e., gold) hallucination evaluation datasets with span-level finegrained human annotations for five high-resource languages and generate synthetic (i.e., silver) hallucination evaluation datasets for 25 additional languages; (3) We propose protocol for estimating in the wild hallucination rates of LLMs in massively multilingual setting; (4) We report hallucination rate estimates for six open-source LLM familes and 30 languages, finding that hallucination rates decrease with model size and increase with model language support. To the best of our knowledge, this work represents the first attempt to estimate hallucination rates of LLMs in the wild: (i) across many languages and on (ii) knowledge-intensive question answering, representative of real-world LLM usage. Figure 1 provides an overview of our whole framework1."
        },
        {
            "title": "2 Background and Related Work",
            "content": "We provide brief overview of the body of related work on (1) hallucination detection models and (2) benchmarks for evaluating LLM hallucination. Hallucination Detection. Coarsely, LLM hallucinations fall into two categories. Intrinsic hallucination are content contradicts some reference information source. The reference may be explicitly given to the LLM as part of the task (e.g., the text to be summarized in summarization or source language text in machine translation) or it may implicit (e.g., general world knowledge in question answering). In contrast, extrinsic hallucination refers to content that does not contradict the reference but is unnecessary or superfluous with respect to the task (e.g., additional facts in fact-based question answering) (Ji et al., 2023). Recent work introduced finer-grained taxonomies for both categories. For example, Mishra et al. (2024) distinguish between several types of intrinsic hallucinations (e.g., entity-based hallucinations or relation-based hallucinations). In similar vein, extrinsic hallucinations are split into subtypes such as invented, 1We release our datasets and work on: https://github. com/WorldHellow/mHallucinations-LLM subjective, and unverifiable content. Unsurprisingly, most hallucination detection (and classifications) models are based on neural languages models. These are are either pre-trained encoder LM (Zhou et al., 2021; Liu et al., 2022b), discriminatively fine-tuned to classify texts as containing hallucinations or not or LLMs prompted (zero-shot or with in-context examples) to detect hallucinations (Manakul et al., 2023; Yang et al., 2023) or fine-tuned to generate hallucinated spans (Mishra et al., 2024). In this work, we cast hallucination detection as span-detection task, formulated discriminatively, with classifier on top of an encoder-based LM. However, instead of resorting to small pretrained encoder LMs, we bidirectionally (i.e., discriminatively) fine-tune larger generative LLM, following recent advances in converting decoder LMs into encoders (Li et al., 2023b; Dukic and Šnajder, 2024; BehnamGhader et al., 2024; Schmidt et al., 2024). Hallucination Benchmarks. Hallucination detection models as well as evaluation datasets have largely focused on English vary in the granularity from document-level (Yang et al., 2023) of annotations/predictions, over passageand sentencelevel annotations (Zhou et al., 2021; Manakul et al., 2023), to fine-grained tokenor span-level annotations (Liu et al., 2022a; Mishra et al., 2024). Notable examples include SelfCheckGPT (Manakul et al., 2023), HaluEval (Li et al., 2023a), and ScreenEval (Lattimer et al., 2023), which measure hallucination detection rates in summarization and single-fact question answering. Multilingual benchmarks for evaluating hallucination detection models remain sparse and focus on reference-based tasks like machine translation (Dale et al., 2023) and summarization (Qiu et al., 2023) which poorly represent the LLM usage in the wild. Faithfulness in reference-based tasks is complemented by truthfulness (i.e., factuality) in question answering. Most benchmarks, e.g., TruthFulQA (Lin et al., 2022), RealtimeQA (Kasai et al., 2024), FreshQA (Vu et al., 2023), and SimpleQA (Wei et al., 2024a) here are English-centric and cover only questions that require simple single-factoid answer. LongFact (Wei et al., 2024b), Factscore (Min et al., 2023) and mFactScore (Kim et al., 2024) do test LLMs truthfulness in generating long and free-form answers. However, LongFact is an English-only benchmark, whereas Factscore and mFactscore, albeit multilingual, cover very speFigure 2: 1) Inter-annotator agreement (IAA) for hallucination span detection (Binary; blue bars) and classification (Category; orange bars) for five high-resource languages; 2) Hallucination span and class agreement between human labels and GPT-4 generated hallucinations (Silver-Gold; agreement on spans only: red bars; agreement on spans and hallucination type: green bars). cific domain of biographic questions."
        },
        {
            "title": "3 Hallucination Detection",
            "content": "We first describe how we obtained multilingual hallucination detection datasets (3.1) and then report on training and evaluation of multilingual hallucination detection model (3.2)."
        },
        {
            "title": "3.1 MFAVA Benchmark",
            "content": "Evaluation Datasets. We start from the English FAVA (Mishra et al., 2024) dataset and its respective set of fine-grained hallucination types. FAVAs evaluation portions were created by (1) eliciting information-seeking prompts (i.e., questions) from various sources,2 (2) generating responses with three LLMs and (3) having human annotators label hallucinated spans. We follow similar protocol to create evaluation datasets for 30 languages.3 We start from 300 information-seeking prompts, 150 from evaluation portion of FAVA and 150 from the Natural Questions dataset (Kwiatkowski et al., 2019). We then ask GPT-4 (Achiam et al., 2023) to (1) first create answer passages in target language and then to (2) explicitly introduce the hallucinations of the fine-grained FAVA types into the answer. We refer to these synthetically labeled hallucination evaluation datasets, comparable across the 30 target languages, as MFAVA-Silver. For five linguistically diverse high-resource languagesArabic, Chinese, German, Russian, and Turkishwe also collect human hallucination annotations. To this end, we provide to the 2See the original paper for more details. 3A.1 Figure 7 lists the mFAVA languages. Very Unlikely Unlikely Neutral Likely Very Likely ENT REL INV CON UNV SUB Total 21.8% 24.7% 13.0% 25.3% 15.2% Table 1: Annotator ratings for probability of augmented text fooling the reader for the 5 gold languages. annotators the reference Wikipedia page, and the (hallucination-enriched) generation from MFAVASilver (of course, without the GPT-4s hallucination annotations). We source the annotations via Prolific, recruiting 5 annotators per language: all five annotators first annotated the same 50 instances, after which they were given non-overlapping sets of 50 more instances. We provide more details on the annotation process (and costs) in the A.2. We measure the inter-annotator agreement (IAA) in terms of pairwise-averaged Cohens kappa on token-level class decisions, both with (IAA Category) and without (IAA Binary) considering the fine-grained hallucination types. As shown in Figure 2, we observe satisfactory to good IAA for all five languages. Regarding the 50 instances labeled by all annotators, we ultimately take the annotations of the annotator that has the highest IAA with hallucination annotatios of GPT-4 from MFAVA-Silver. We denote the final human-labeled evaluation datasets for the five high-resource languages with MFAVA-Gold. Figure 2 also shows the overall IAA between human annotations from MFAVA-Gold and GPT-4s synthetic annotations from MFAVA-Silver (SilverGold): interestingly, we observe that human annotators on average agree more with GPT-4 than with one another. Because we synthesize the hallucinated content with GPT-4 (the annotators, of course, did not know that nor which part of the generation was meant to be hallucination according to GPT-4), there is risk that these hallucinations may not be realistic in the sense that they can fool human reader. Because of this, we asked our annotators to additionally indicate (on 5-degree Likert scale from very unlikely to very likely) the likelihood of hallucination fooling human reader for each span that they labeled. Table 1 reveals that more than half of the labeled hallucinations were judged as convincing (i.e., not unlikely to fool human). The silver test set statistics for all 30 languages are shown in Figure 6. Gold annotations statistics are shown in Table 2. RU AR ZH DE TR 184 144 264 546 65 10 18 25 27 188 171 259 311 288 287 123 282 324 244 211 150 265 333 161 Total 1, 145 1,217 1,260 1,120 153 69 139 238 149 1,088 667 1,227 1,777 1,018 5,777 Table 2: Hallucinated span counts in the gold dataset across languages. ENT (Entity), REL (Relation), INV (Invented), CON (Contradictory), UNV (Unverifiable), SUB (Subjective). ated in the same way as the test portion, just without the human annotation step. We automatically translate the training portion of the FAVA dataset using NLLB (Costa-jussà et al., 2022) to our 30 target languages. After translation, we project the span-level annotations to token-level labels using the simple Inside-Out (I-O) scheme (Ramshaw and Marcus, 1995)4. Like our evaluation benchmark MFAVA, we prepare training data for two tasks: (1) detecting hallucinated spans, regardless of hallucination type (Binary task: tokens are classified as either part of hallucinated span or not) and (2) detection and hallucination type classification (Category task: 7-way classification, tokens classified into one of 6 FAVA hallucination types or as not part of hallucinated span)."
        },
        {
            "title": "3.2 Multilingual Hallucination Detection",
            "content": "Models. Using the translations of ca. 30K FAVA training instances in our 30 target languages, we train the following models: (1) MONO denotes monolingual models trained on data of one language (and evaluated for the same language on the respective MFAVA portion), i.e., we train 30 MONO models, one for each of our target languages; (2) MULTI refers to single multilingual model trained on concatenated training data of all 30 languages. We train the Mono models and the Multi model for both tasks, Binary and Category. We follow the recent body of work that successfully converts generative decoder LLMs into encoders for discriminative tasks (Li et al., 2023b; Dukic and Šnajder, 2024; BehnamGhader et al., 2024; Schmidt et al., 2024) and fine-tune Llama-3-8Bbase (Dubey et al., 2024) by removing future-token masking, i.e., allowing for bidirectional contextualization (Bidirect). For comparison, for the Multi model, we also fine-tune the decoder as-is, using Training Dataset. The FAVA training set, consisting of ca. 30K instances, is fully synthetically cre4In preliminary experiments, we also tested the B-I-O scheme, but I-O led to better span detection performance."
        },
        {
            "title": "Silver Gold",
            "content": "Binary MONO MULTI"
        },
        {
            "title": "Bidirect\nBidirect",
            "content": "MULTI"
        },
        {
            "title": "Causal",
            "content": "MONO Category MULTI"
        },
        {
            "title": "Bidirect\nBidirect",
            "content": "MULTI"
        },
        {
            "title": "Causal",
            "content": "78.0 89.5 81.8 53.4 73.2 68.7 58.0 65.0 59. 38.3 45.0 43.4 62.4 69.7 76.3 35.2 46.5 56. 55.1 58.7 62.2 22.6 30.1 34.1 75.3 82.5 75. 14.6 66.1 51.8 54.4 61.6 60.0 7.3 37.2 29. 78.9 89.1 75.8 63.3 72.3 62.6 60.7 65.5 55. 36.2 41.5 37.9 78.5 86.4 75.7 49.1 72.9 58. 66.7 72.5 67.3 30.3 51.8 42.4 Table 3: Token-level F1 performance of multilingual (MULTI) and monolingual (MONO) hallucination detection models for five high-resource languages with both Silver and Gold evaluation data in MFAVA. Performance reported for hallucination detection alone (Binary) and hallucination detection and type classification (Category). Models fine-tuned without (Bidirect) or with (Causal) future token masking. Bold: best result in each column. the default causal token masking (i.e., unidirectional contextualization; Causal). Training. In all cases, we freeze the original model parameters and train QLora adapters (Dettmers et al., 2024), with three runs (random seeds) for each experiment, reporting mean performance. The input to the models is the reference Wikipedia article, prepended to the LLM-generated answer to the user question/prompt, with the cross-entropy loss computed exclusively over the tokens of the LLM-generated answer. We provide further training details in A.3 for training details. Results. Table 3 summarizes the hallucination detection performance for five high-resource languages for which we have both LLM-synthesized Silver data and human-annotated Gold portions in our MFAVA benchmark. We first observe that, expectedly, just detecting hallucinated spans (Binary task) is much easier than additionally correctly recognizing the type of hallucination (Category task). Although category labels offer finer-grained insight into the nature of LLM hallucination, we deem the models performance on fine-grained hallucination type classificationespecially on Gold, humanlabeled portions of MFAVAinsufficient for reliably estimating type-specific hallucination rates in the wild (see 4.2). These results are in line with IAA from Figure 2, with consistently larger IAA for hallucination detection (Binary) then for type classification (Category). This renders finegrained hallucination type classification difficult for both humans and models and warrants broader research effort on hallucination type taxonomies as well as better hallucination type detection models. We leave this for future work. Models performance on the detection-only (Binary) tasks is much better across the board, but the results are much better on the Silver portions (hallucinations generated by GPT-4) of MFAVA than on the Gold (human-labeled hallucination spans). This is expected, because the hallucinated spans in our training data have also been generated by GPT-4this means that the human-annotated Gold mFAVA portions introduce much more of distribution shift w.r.t. training data than the corresponding Silver portions. At this point it is important to (re- )emphasize that we are not really interested in the absolute performance of the detection models, but rather using these detection performance estimates to produce reliable hallucination rate estimates for LLMs in the wild (4.2). We next observe that the 30-language multilingual model (MULTI) is consistently better than language-specific monolingual models (MONO), with gaps being particularly wide in the Category task (e.g., +30 F1 points for Arabic on the Gold MFAVA portion). Albeit smaller, the differences are also substantial in the Binary hallucination detection (e.g., +7 F1 points for Arabic and German, on respective Gold MFAVA portions). Finally, bidirectional contextualization in fine-tuning (Bidirect) seems to be generally more effective than finetuning with future-token masking (Causal), with Chinese performance as the only exception. This is in line with findings from other token-classification tasks (Li et al., 2023b; Dukic and Šnajder, 2024)."
        },
        {
            "title": "4 Estimating Hallucination in the Wild",
            "content": "We next propose protocol for estimating hallucination rates of LLMs (for wide range of languages) in the wild, based (1) on the number of hallucinated tokens detected by hallucination detection (HD) model in the wild and (2) estimates of HD models performance (precision and recall). Figure 3: Comparison of hallucination rate estimates HRest,l (mean std over five LLM runs) for Arabic (AR), Chinese (ZH), German (DE), Russian (RU), and Turkish (TR) for 3 LLMs based on the estimates of Pl and Rl of the MULTI (Bidirect) model on (1) MFAVA-Silver (top row) and (2) MFAVA-Gold (bottom row). The two sets of estimates are highly correlated (r = 0.83, = 1.26e 04)."
        },
        {
            "title": "Hallucination Rates Estimates",
            "content": "Estimating Hallucination Rates in the Wild. Let Pl and Rl be the estimates of token-level precision and recall of HD model for some language and let Hdet,l be the number of hallucination tokens that the HD model detected (i.e., predicted) on some corpus Cl of LLM generations in language l, which serves as an approximation of the LLM outputs in the wild. We then posit that the estimate of the true hallucination rate of the LLM in the wild for language l, HRest,l, is given as follows: (1) HRest,l = 100(%) Pl Hdet,l Rl Nl where Nl is the total number of tokens in Cl, i.e., the total number of tokens generated by the LLM across answers to all user prompts. Intuitively, multiplying the number of models detections Hdet,l with its estimated precision Pl discounts Hdet,l by the number of tokens falsely detected as hallucinated by the modelwhile we do not know exactly which token predictions are false positives, the expected rate of false positives is, by definition, exactly captured by Pl. Analogously, dividing Hdet,l with Rl accounts for the tokens that are hallucinated, but will (falsely) not be detected by the modeland Rl is exactly the estimate of the rate of such false negatives. We divide the estimate of the absolute number of truly hallucinated tokens (i.e., Pl Hdet,l/Rl) with Nl, making HRest,l relative measure, that is, rate (i.e., proportion) of all generated tokens that are hallucinated (multiplied by 100 and expressed as %). We provide more detailed explanation/justification of Eq. (1) in A.4. Estimation Dataset. We next create corpora Cl (one corpus for each of our 30 target languages) of free-text LLM answers to knowledge-intensive queries, as approximations of the LLM usage in the wild. We start by randomly selecting articles from the language-specific Wikipedia, to serve as ground truth reference text. To ensure quality of reference text, we choose only from Wikipedia articles that are at least 2,000 characters long and have the collaborative Wikipedia depth (Alshahrani et al., 2023) of at least 5.5 We then prompt GPT-4 to generate two knowledge-intensive queries for each selected article, ensuring that the information required to answer to the query is fully contained in the article text (see Table 6 in the A.5 for the exact prompt). As sanity check, we manually checked for 50 synthesized queries and five languages from Table 3by translating the query and reference article to Englishwhether the answers to queries are indeed contained in the article, establishing that this is indeed so in 98% of cases. Our final dataset for multilingual hallucination rate estimation consists of 25,685 Wikipedia articles (spanning over 15,940 unique Wikipedia categories) and 51,133 queries. Table 9 in provides per-language statistics. We provide details on constructing the datasets in A.5. Finally, we collected responses to all queries from total of 11 instruction-tuned open-source LLMs from 6 families (ranging in parameter count from 2 to 9 billion): Llama-3.x (Dubey et al., 2024), Aya-23 (Aryabumi et al., 2024), Euro-LLM (Martins et al., 2024), Gemma-2 (Team, 2024) Qwen-2.5 (Yang et al., 2024), and Mistral v3 (Jiang et al., 2023). We divided the queries into five subsets: for each subset the LLMs generated responses with different random seed (see Table 8 for details on the generation configurations). Estimates from MFAVA-Silver Performance. On the one hand, creating Gold datasets for hallucination detection evaluation is prohibitively expensive (see A.2)this is why we obtained such annotations for only five of 30 MFAVA languages. On the other hand, the estimates of HD models performance are much higher on MFAVA-Silver (see 5The depth indicates the number of collaborative edits and correlates with the quality/factuality of the content. Figure 4: Mean estimates of in-the-wild hallucination rates ( std) for 30 languages and 11 LLMs. Each mean score is an average of 15 HRest,l estimates, (3 different HD model instances applied to 5 different LLM responses). Average rates increase from top to bottom (over languages) and from left to right (over LLMs). Table 3), with GPT-4-labeled hallucinations: this, at first glance, questions the validity of estimating in the wild hallucination rates based on Pl and Rl estimated on Silver data, for the 25 languages for which we do not have MFAVA-Gold portions. Recall, however, that we do not care about HD models absolute Pl and Rl, but whether the Pl and Rl estimates can produce reliable hallucination rate estimates HRest,l. Looking at Eq. 1, HRest,l depends on the ratio Pl/Rl and not absolute values of Pl and Rl. We thus next test, for the five languages with both Silver and Gold portions in MFAVA, whether the HRest,l estimates based on the Silver Pl and Rl (roughly) match those based on Gold Pl and Rl. Figure 3 shows HRest,l estimates, computed from the performance of our MULTI (Bidirect) model on Silver and Gold portions, respectively, and number of its hallucination detections Hdet,l on outputs of three LLMs: Llama-3-8B, Qwen-2.5-7B, and Aya-8B. We observe very strong Pearson correlation (r = 0.83, = 1.26e 04) between the Gold-based and Silver-based HRest,l estimates, which, we argue, justifies the usage of Silver MFAVA datasets for estimating HRest,l for the 25 languages without the Gold MFAVA portions."
        },
        {
            "title": "4.2 Final Estimates",
            "content": "Figure 4 shows our in-the-wild hallucination rate estimates HRest,l for all 30 MFAVA languages and 11 LLMs. The average rate across all languages varies between 7% and 12%, with both Gemma models offering the lowest rates. Smaller Qwen-2.5 (a) (b) (c) Figure 5: 5a Larger models hallucinate significantly less than smaller ones. Bars are labeled with p-values from t-test. 5b Correlation between hallucination rates (averaged over all 30 languages) and the officially declared number of supported languages. 5c On average, as response length increases, so do the absolute hallucinations Hdetected,l. (3B) model hallucinates the most and, interestingly, significantly more than its larger counterpart (9B). More Parameters, Less Hallucination? For each LLM, we have 15 estimates (3 HD model instances 5 generations by the LLM) of HR (averaged across all languages): we apply the Students ttest to determine if the differences between models (smaller and larger) significantly differ. Figure 5a summarizes the results. The difference between the two EuroLLM variants is not significant; larger Gemma model hallucinates more (significantly), but the HR are low for both variants; for Llama and Qwen, the smaller models hallucinate significantly more. Finally, we aggregate the estimates across all small models (1.7-3B) and all large models (7-9B) and see that, overall (column Overall in Figure 5a), smaller LLMs hallucinate significantly more (p = 0.01). This agrees with Wei et al. (2024b) who report larger models to be more truthful in long-form answer generation. More Languages, More Hallucination? Figure 5b compares LLMs hallucination rates against their declared number of supported languages. We see that LLMs that support more languages tend to hallucinate more (e.g., EuroLLM supports 35 languages, whereas Gemma is declared to support English only)the correlation is strong and significant (r = 0.88, = 0.049). Say Less, Hallucinate Less? Intuitively, one would expect LLMs hallucination rates to be larger for languages in which they are less competent (i.e., seen the least in pretraining and instructiontuning). Surprisingly, however, we do not find this to be the case for any of the models. E.g., we observe the lowest hallucination rate for Sindhi (5.83% of tokens are hallucinated), language with merely 18,000 Wikipedia articles and largest hallucination rate for Hebrew (16.81%). Across all 30 languages, however, we find no correlation between the hallucination rates and measures of language resourceness: (i) proportion of language-specific data in Common Crawl and (ii) number of articles in the language-specific Wikipedia. As illustrated in Figure 5c, we do observe that LLMs generate longer responses for languages in which they are more competentthis entails larger number of hallucinated tokens for longer responses, but not (necessarily) larger (per-token) hallucination rate (recall that we account for the response length in Eq. 1). Indeed, we observe no correlation whatsoever between the response length and hallucination rates across languages (r = 0.05). This suggests that trade-off between the answer length and the amount (not rate!) of hallucinations is largely language-independent property of LLMs."
        },
        {
            "title": "5 Conclusion",
            "content": "We presented the first effort towards understanding how much multilingual LLMs hallucinate in the wild. To this end, we proposed novel framework for hallucination rate estimation, which adjusts the number of detected hallucinations based on the detectors performance resulting in more reliable rate estimates. We trained series of multilingual detection models, and measured their precision and recall scores on our newly created MFAVA datasets across 30 languages. To estimate hallucinations, we build novel synthetic open-domain knowledge-intensive QA dataset for which we collected answers from eleven open-source LLMs. Our findings indicate that smaller models and models that cover more languages hallucinate significantly more, and that model response-length does not correlate with hallucination rate."
        },
        {
            "title": "References",
            "content": "We acknowledge the following limitations of our work: (1) we adopted the common translate-train approach and thus used MT to translate the original FAVA to our 30 target languages. While one may argue that we thus add some noise to the training process resulting in unreliable detectors, recall that we are not opting for the highest possible detection performances, but rather interested in obtaining reliable performance estimates. (2) We only have gold annotations for 5 languages. Here, one might argue that, thus, our performance estimates might be unreliable. This is why in 4.1, we compare estimates obtained on MFAVA-Silver with ones obtained on MFAVA-Gold and show that silver annotations can serve as reliable proxy. (3) For our hallucination evaluation, we only manually check subset of the Arabic, Chinese, German, Russian, and Turkish queries to ensure that the answers to the synthetic prompts are present in the Wikipedia references. The high rate of 98% we observed makes us confident that the potential error we introduce via such non-grounded questions for other languages is negligible. We still acknowledge, however, that the Wikipedia articles we use might be limited in terms of the knowledge they cover (Kim et al., 2024), this is why we carefully filter via minimum length and collaborative Wikipedia depth towards higherquality articles with high coverage. (5) Finally, we deliberately limited the scope of this work to assessing factual correctness and we do not cover factual coverage. We decided to do so as quantifying hallucinations in long-form generation is already difficult for English Xu et al. (2023); Min et al. (2023); Wei et al. (2024b) and more so in non-english languages (Kim et al., 2024), and currently, resources for assessing multilingual factual coverage are still lacking."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was supported by the Alcatel-Lucent Stiftung and Deutsches Stiftungszentrum through the grant Equitably Fair and Trustworthy Language Technology (EQUIFAIR, Grant Nr. T0067/43110/23). The authors gratefully acknowledge the computing time granted by the John von Neumann Institute for Computing (NIC) and provided on the supercomputer JURECA (Jülich Supercomputing Centre, 2021) at Jülich Supercomputing Centre (JSC). Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Roee Aharoni, Shashi Narayan, Joshua Maynez, Jonathan Herzig, Elizabeth Clark, and Mirella Lapata. 2022. mface: Multilingual summarization with factual consistency evaluation. arXiv preprint arXiv:2212.10622. Saied Alshahrani, Norah Alshahrani, and Jeanna Matthews. 2023. DEPTH+: An enhanced depth metric for Wikipedia corpora quality. In Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023), pages 175189, Toronto, Canada. Association for Computational Linguistics. Mikel Artetxe, Vedanuj Goswami, Shruti Bhosale, Angela Fan, and Luke Zettlemoyer. 2023. Revisiting machine translation for cross-lingual classification. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 64896499. Viraat Aryabumi, John Dang, Dwarak Talupuru, Saurabh Dash, David Cairuz, Hangyu Lin, Bharat Venkitesh, Madeline Smith, Kelly Marchisio, Sebastian Ruder, et al. 2024. Aya 23: Open weight releases to further multilingual progress. arXiv preprint arXiv:2405.15032. Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados, and Siva Reddy. 2024. LLM2vec: Large language models are secretly powerful text encoders. In First Conference on Language Modeling. Liang Chen, Yang Deng, Yatao Bian, Zeyu Qin, Bingzhe Wu, Tat-Seng Chua, and Kam-Fai Wong. 2023. Beyond factuality: comprehensive evaluation of large language models as knowledge generators. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 6325 6341. Elizabeth Clark, Shruti Rijhwani, Sebastian Gehrmann, Joshua Maynez, Roee Aharoni, Vitaly Nikolaev, Thibault Sellam, Aditya Siddhant, Dipanjan Das, and Ankur Parikh. 2023. Seahorse: multilingual, multifaceted dataset for summarization evaluation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 93979413. Marta Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et al. 2022. No language left behind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672. David Dale, Elena Voita, Janice Lam, Prangthip Hansanti, Christophe Ropers, Elahe Kalbassi, Cynthia Gao, Loic Barrault, and Marta R. Costa-jussà. 2023. Halomi: manually annotated benchmark for multilingual hallucination and omission detection in machine translation. In The 2023 Conference on Empirical Methods in Natural Language Processing. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2024. Qlora: Efficient finetuning of quantized llms. Advances in Neural Information Processing Systems, 36. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. David Dukic and Jan Šnajder. 2024. Looking right is sometimes right: Investigating the capabilities of decoder-only llms for sequence labeling. In Annual Meeting of the Association for Computational Linguistics. Benedikt Ebing and Goran Glavaš. 2024. To translate or not to translate: systematic investigation of translation-based cross-lingual transfer to lowIn Proceedings of the 2024 resource languages. Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 53255344. Jungo Kasai, Keisuke Sakaguchi, Ronan Le Bras, Akari Asai, Xinyan Yu, Dragomir Radev, Noah Smith, Yejin Choi, Kentaro Inui, et al. 2024. Realtime qa: whats the answer right now? Advances in Neural Information Processing Systems, 36. Vu Trong Kim, Michael Krumdick, Varshini Reddy, Franck Dernoncourt, and Viet Dac Lai. 2024. An analysis of multilingual FActScore. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 43094333, Miami, Florida, USA. Association for Computational Linguistics. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in Neural Information Processing Systems, 35:22199 22213. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452 466. Barrett Lattimer, Patrick Chen, Xinyuan Zhang, and Yi Yang. 2023. Fast and accurate factual inconsistency detection over long documents. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 16911703. Nuno Guerreiro, Duarte Alves, Jonas Waldendorf, Barry Haddow, Alexandra Birch, Pierre Colombo, and André FT Martins. 2023. Hallucinations in large multilingual translation models. Transactions of the Association for Computational Linguistics, 11:1500 1517. Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023a. Halueval: largescale hallucination evaluation benchmark for large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 64496464. Janek Herrlein, Chia-Chien Hung, and Goran Glavaš. 2024. Anhalten: Cross-lingual transfer for german token-level reference-free hallucination detection. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop), pages 92100. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):138. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. Jülich Supercomputing Centre. 2021. JURECA: Data Centric and Booster Modules implementing the Modular Supercomputing Architecture at Jülich Supercomputing Centre. Journal of large-scale research facilities, 7(A182). Zongxi Li, Xianming Li, Yuzhang Liu, Haoran Xie, Jing Li, Fu-lee Wang, Qing Li, and Xiaoqin Zhong. 2023b. Label supervised llama finetuning. arXiv preprint arXiv:2310.01208. Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. Truthfulqa: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, pages 32143252. Tianyu Liu, Yizhe Zhang, Chris Brockett, Yi Mao, Zhifang Sui, Weizhu Chen, and Bill Dolan. 2022a. token-level reference-free hallucination detection benchmark for free-form text generation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, pages 67236737. Zihan Liu, Mostofa Patwary, Ryan Prenger, Shrimai Prabhumoye, Wei Ping, Mohammad Shoeybi, and Bryan Catanzaro. 2022b. Multi-stage prompting for knowledgeable dialogue generation. In Findings of the Association for Computational Linguistics: ACL 2022, pages 13171337. Potsawee Manakul, Adian Liusie, and Mark Gales. 2023. SelfCheckGPT: Zero-resource black-box hallucination detection for generative large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 90049017. Pedro Henrique Martins, Patrick Fernandes, João Alves, Nuno Guerreiro, Ricardo Rei, Duarte Alves, José Pombal, Amin Farajian, Manuel Faysse, Mateusz Klimaszewski, et al. 2024. Eurollm: Multilingual language models for europe. arXiv preprint arXiv:2409.16235. Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factuality in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 19061919. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. Factscore: Fine-grained atomic evaluation of factual precision in long form text generation. arXiv preprint arXiv:2305.14251. Sheikh Shafayat, Eunsu Kim, Juhyun Oh, and Alice Oh. 2024. Multi-FAct: Assessing factuality of multilingual LLMs using FActscore. In First Conference on Language Modeling. Gemma Team. 2024. Gemma. Timm Teubner, Christoph Flath, Christof Weinhardt, Wil Aalst, and Oliver Hinz. 2023. Welcome to the era of chatgpt et al.: The prospects of large language models. Business & Information Systems Engineering. Johanne Trippas, Sara Fahad Dawood Al Lawati, Joel Mackenzie, and Luke Gallagher. 2024. What do users really ask large language models? an initial log analysis of google bard interactions in the wild. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 27032707. Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, et al. 2023. Freshllms: Refreshing large language models with search engine augmentation. arXiv preprint arXiv:2310.03214. Abhika Mishra, Akari Asai, Vidhisha Balachandran, Yizhong Wang, Graham Neubig, Yulia Tsvetkov, and Hannaneh Hajishirzi. 2024. Fine-grained hallucination detection and editing for language models. In First Conference on Language Modeling. Jason Wei, Karina Nguyen, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, and William Fedus. 2024a. Measuring short-form factuality in large language models. arXiv preprint arXiv:2411.04368. Sebastian Nordhoff and Harald Hammarström. 2012. Glottolog/langdoc:increasing the visibility of grey In Proceedliterature for low-density languages. ings of the Eighth International Conference on Language Resources and Evaluation (LREC12), pages 32893294, Istanbul, Turkey. European Language Resources Association (ELRA). Saad Obaid ul Islam, Iza Škrjanec, Ondrej Dusek, and Vera Demberg. 2023. Tackling hallucinations in neural chart summarization. In Proceedings of the 16th International Natural Language Generation Conference, pages 414423. Yifu Qiu, Yftah Ziser, Anna Korhonen, Edoardo Ponti, and Shay Cohen. 2023. Detecting and mitigating hallucinations in multilingual summarisation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 8914 8932. Lance Ramshaw and Mitch Marcus. 1995. Text chunking using transformation-based learning. In Third Workshop on Very Large Corpora. Fabian David Schmidt, Philipp Borchert, Ivan Vulic, and Goran Glavaš. 2024. Self-distillation for model stacking unlocks cross-lingual NLU in 200+ languages. In Findings of the Association for Computational Linguistics: EMNLP 2024, Miami, Florida, USA. Association for Computational Linguistics. Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, et al. 2024b. Long-form factuality in large language models. arXiv preprint arXiv:2403.18802. Wolf. 2019. Huggingfaces transformers: State-ofthe-art natural language processing. arXiv preprint arXiv:1910.03771. Haoyi Xiong, Jiang Bian, Yuchen Li, Xuhong Li, Mengnan Du, Shuaiqiang Wang, Dawei Yin, and Sumi Helal. 2024. When search engine services meet large language models: visions and challenges. IEEE Transactions on Services Computing. Fangyuan Xu, Yixiao Song, Mohit Iyyer, and Eunsol Choi. 2023. critical evaluation of evaluations for long-form question answering. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics, pages 32253245. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. 2024. Qwen2 technical report. arXiv preprint arXiv:2407.10671. Shiping Yang, Renliang Sun, and Xiaojun Wan. 2023. new benchmark and reverse validation method for passage-level hallucination detection. arXiv preprint arXiv:2310.06498. Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. 2023. Generate rather than retrieve: Large language models are In The Eleventh Interstrong context generators. national Conference on Learning Representations. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. 2023. Sirens song in the ai ocean: survey on hallucination in large language models. arXiv preprint arXiv:2309.01219. Chunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab, Francisco Guzmán, Luke Zettlemoyer, and Marjan Ghazvininejad. 2021. Detecting hallucinated content in conditional neural sequence generation. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 13931404."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Choice of languages Initially, we wanted to cover all 14 language families based on Glottolog 5.0 (Nordhoff and Hammarström, 2012)), however, as we progressed through the languages, we found that even the best closed-source LLMs like GPT-4 and Gemini are bad at generating text in low-resource languages (e.g. Amharic, Aymara, Hausa and Tamil) and we could not employ LLMs to generate and annotate silver hallucination detection dataset in these languages. See Table 7 for 30 languages. A.2 Annotation Process Cost of Silver Annotations The total cost for generating silver data for 30 languages using GPT-4 was $2,310 with $77 per language. Distribution of categories across 30 languages is provided in Table 4 and and per language label distribution is provided in Figure 6."
        },
        {
            "title": "Count",
            "content": "11143 9036 5649 4024 5670 Table 4: Distribution of categories across 30 languages in silver set. Gold Annotations: The annotators were sourced through prolific platform. Each annotator was screened on 10 samples and if they met the threshold of 40% agreement with the silver annotation, they were invited to participate in the full study. It is worth noting that as the hallucination annotation task for longform QA is very cognitively demanding, it took us long time to find annotators who could do the task correctly with high-effort. Most of the time, annotators who passed the screening test, decided to leave the study because of the high effort requirement of the task even though our study was paying above minimum wage (14 $/hr) for the full study. The total cost of the gold annotations was (including platform and annotation fees) $4581 where each annotator was paid 14 $/hr. All the annotators were at least bachelors level and bilingual because in addition to understanding their own language (e.g. Arabic) they also needed to understand the task instructions and Wikipedia content (in English). The task instructions are given in Figure 8. Each annotator was asked if they consent to storing their prolific IDs during manual and automatic assessment stage. Following the assessment, their prolific IDs were deleted. We will release MFAVA data under an open scientific licensing. A.3 Training Details All the classifiers were trained utilizing the BiLLM (Li et al., 2023b) and transformers (Wolf, 2019) library. The models were trained with three seeds (42, 47, 49) on 4xH100 until convergence. Seeds are set for torch.manual_seed() and random.seed(). The exact hyper-parameters are given in the Table 5. Total GPU hours: 1134."
        },
        {
            "title": "Value",
            "content": "Translate Train-Val Split Seeds Quantization Model GPUs LoRA LoRA α LoRA Dropout LoRA Target Modules Epochs Input Length Learning Rate Weight Decay Batch Size Gradient Accumulation 70:30 [42, 47, 49] 4-bit BF16 Llama-3-8B (base) 4 H100 32 32 0.05 All 2 (until convergence) 4096 1 104 0.01 8 8 Table 5: Training Details A.4 Adjusting for Pl and Rl The hallucination rate HRest,l for given language , is defined as the ratio of hallucinated tokens detected by the model (Hdetected,l) to the total number of generated tokens (Nl): HRl = Hdet,l Nl . (2) To refine this rate, we adjust for the detection models precision (Pl) and recall (Rl). Precision is defined as: Pl = TP TP + FP (3) where (TP l) FP denote true and false positives respectively. Rearranging this equation gives the number of true positives: TP = Pl HRdet,l Recall is defined as: Rl = TP TP + FN (4) (5) where FN denotes false negatives. The total number of corrected hallucinations (HRest,l) can thus be expressed as: Given the following reference in language <language name>: two knowledge-intensive Generate queries in <language name>. Ensure the questions are concise but knowledgeintensive. The questions should require thorough reading of the reference text to answer. Separate the questions with newline. Table 6: Prompt for generating knowledge-intensive queries. A.6 Response Collection for Hallucination"
        },
        {
            "title": "Evaluation Dataset",
            "content": "We collect LLM responses on 5 seeds: 42, 43, 44, 47, 49 for 6 LLM model6. The generation configurations that we used are provided in the generation_config.json in model repositories on huggingface. Seeds are set for torch.manual_seed() and random.seed(). HRest,l = TP + FN ="
        },
        {
            "title": "TP l\nRl",
            "content": "(6) A.7 Manual Analysis Substituting Equations 4 in 6, we derive the Hest,l as: Hest,l = Pl Hdet,l Rl (7) A.5 Hallucination Evaluation Dataset To construct the hallucination evaluation dataset, we aimed to scrap 1000 articles per language with more than 2000 characters. However, problem with non-English languages (especially moderatelow resource) is that 2000 character articles can be scarce. Furthermore, sometimes Wikipedia has articles tagged as Unreferenced, Failed Verification, or Under Construction which flag the article as unfinished or not factually verified. Such tags are very prominent in languages other than English and we do not include such articles in our dataset. We use Wikipedia article summary (text before the first heading) as references and prompt gpt4 to generate 2 knowledge-intensive queries per article. Sometimes, it generated only one query even though we explicitly state to generate two queries. We did not prompt the GPT-4 again to generate the second query due to budget constraints. The total cost to generate prompts for 31 languages is $192. Per language statistics can be found in Table 9. We will release hallucination evaluation data under an open scientific licensing. To further check the quality and informativeness (See A.8 for definitions of informativeness) of the responses, we manually analyze 60 responses from Aya-23-8B for German and Arabic, two languages for which we have gold annotations. Overall, 10% of the responses had repetitive words and sentences and 5% of the responses were dont know responses. For the remainder of the samples, the responses were fluent and long and were relevant to the input prompt. A."
        },
        {
            "title": "Informativeness",
            "content": "Currently, there is no agreed-upon definition of informativeness. Lin et al. (2022) considers response to be informative if it is potentially relevant to the question and Wei et al. (2024b) considers response to be informative if it has certain number of supporting facts from the reference text. 6We comply with licensing agreement for each of the LLMs we use."
        },
        {
            "title": "Script",
            "content": "Test-Set Arabic Chinese German Russian Turkish Basque Cantonese Catalan Czech Esperanto Finnish French Hebrew Hindi Hungarian Indonesian Italian Japanese Korean Latin Lithuanian Malay Polish Portuguese Romanian Serbian Sindhi Spanish Urdu Vietnamese Afro-Asiatic (Semitic) Sino-Tibetan (Sinitic) Indo-European (Germanic) Indo-European (Slavic) Turkic (Common Turkic) Language Isolate Sino-Tibetan (Sinitic) Indo-European (Romance) Indo-European (Slavic) Constructed Uralic (Finnic) Indo-European (Romance) Afro-Asiatic (Semitic) Indo-Aryan Uralic (Ugric) Austronesian (Malayo-Polynesian) Indo-European (Romance) Japonic Koreanic Indo-European (Italic) Indo-European (Slavic) Austronesian (Malayo-Polynesian) Indo-European (Slavic) Indo-European (Romance) Indo-European (Romance) Indo-European (Slavic) Indo-Aryan Indo-European (Romance) Indo-Aryan Austroasiatic (Vietic) Arabic Chinese (Han) Latin Cyrillic Latin Latin Chinese (Han) Latin Latin Latin Latin Latin Hebrew Devanagari Latin Latin Latin Kanji Hangul Latin Latin Latin Latin Latin Latin Cyrillic Arabic Latin Arabic Latin Gold Gold Gold Gold Gold Silver Silver Silver Silver Silver Silver Silver Silver Silver Silver Silver Silver Silver Silver Silver Silver Silver Silver Silver Silver Silver Silver Silver Silver Silver Table 7: Classification of languages by language family (based on Glottolog 5.0), script, and test-set status. Gold test sets are available for 5 languages, while the rest have silver test sets. Figure 6: Distribution of 6 labels across 30 languages in MFAVA-SILVER dataset."
        },
        {
            "title": "Model",
            "content": "max_new_tokens temperature top_p top_k repetition_penalty do_sample Llama-3.x Aya Qwen-2.5 Mistral Gemma-2 EuroLLM 1024 1024 1024 1024 1024 1024 0.6 0.7 0.9 0.3 0.9 20 50 1.05 True True True True True True Table 8: Huggingface MODEL.GENERATE() parameters for each model family. indicate default is used. Generation configurations are provided in models respective HuggingFace (Wolf, 2019) repositories"
        },
        {
            "title": "Language\nArabic\nBasque\nCantonese\nCatalan\nChinese\nCzech\nEsperanto\nFrench\nFinnish\nGerman\nHebrew\nHindi\nHungarian\nIndonesian\nItalian\nJapanese\nKorean\nLatin\nLithuanian\nMalay\nPolish\nPortuguese\nRomanian\nRussian\nSpanish\nSerbian\nSindhi\nTurkish\nUrdu\nVietnamese\nTotal",
            "content": "Unique Categories Total Articles Total Queries 1907 1872 793 1976 1939 1975 1912 1973 1972 1967 1991 367 1964 1913 1974 1991 1488 916 1888 1556 1998 1909 1618 1996 1952 1587 1029 1650 1749 1311 51,133 959 938 401 989 977 988 956 987 995 984 999 186 992 958 988 999 747 465 946 778 1000 955 811 999 977 798 519 856 878 660 25,685 537 486 261 359 712 720 608 332 549 797 660 153 745 457 678 667 539 334 711 442 889 390 351 462 938 386 224 660 567 326 15,940 Table 9: Per language statistics for hallucination evaluation dataset."
        },
        {
            "title": "Language",
            "content": "Precision (%) Recall (%) F1 Score (%)"
        },
        {
            "title": "GOLD",
            "content": "Arabic (Gold) Chinese (Gold) German (Gold) Turkish (Gold) Russian (Gold) Average 73.98 70.73 58.19 79.67 63.18 69."
        },
        {
            "title": "Arabic\nChinese\nGerman\nTurkish\nRussian\nBasque\nCantonese\nCatalan\nCzech\nEsperanto\nFrench\nFinnish\nHebrew\nHindi\nHungarian\nIndonesian\nItalian\nKorean\nJapanese\nLithuanian\nMalay\nPortuguese\nSerbian\nSindhi\nSpanish\nVietnamese\nUrdu\nAverage",
            "content": "93.28 80.33 91.64 89.58 93.05 87.22 78.49 94.70 93.99 94.28 91.58 86.67 82.75 68.01 92.35 92.12 93.76 86.39 77.06 90.48 86.15 95.80 86.16 82.00 95.86 89.35 88.82 88.22 53.40 53.93 74.06 66.95 68.46 63.36 74.81 66.28 87.77 83.92 86.04 74.46 49.40 87.46 84.75 86.53 89.37 84.26 32.97 68.48 74.29 85.75 87.26 79.11 61.03 75.39 68.96 86.77 76.75 69.38 85.34 84.57 72.32 76.42 61.63 58.79 65.05 72.57 65.53 64.71 82.59 69.77 89.50 86.43 89.15 79.80 56.12 90.85 89.00 90.05 90.31 85.15 44.19 66.77 81.93 88.72 90.28 82.31 67.15 81.81 75.73 90.94 79.91 74.36 90.14 86.71 79.39 80.71 Table 10: Precision, Recall, and F1 scores for all languages, including GOLD scores for five languages. (a) Hallucinations vs response length correlation of smaller models. (b) Hallucinations vs response length correlation of bigger models. (c) Hallucinations vs response length correlation of bigger models. Figure 7: Per model correlations between hallucinations and response length. Figure 8: Annotation Instructions."
        }
    ],
    "affiliations": [
        "Data Science Group, University of Hamburg",
        "WüNLP, CAIDAS, University of Würzburg"
    ]
}