{
    "paper_title": "Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow",
    "authors": [
        "Haocheng Xi",
        "Charlie Ruan",
        "Peiyuan Liao",
        "Yujun Lin",
        "Han Cai",
        "Yilong Zhao",
        "Shuo Yang",
        "Kurt Keutzer",
        "Song Han",
        "Ligeng Zhu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning (RL) is essential for enhancing the complex reasoning capabilities of large language models (LLMs). However, existing RL training pipelines are computationally inefficient and resource-intensive, with the rollout phase accounting for over 70% of total training time. Quantized RL training, particularly using FP8 precision, offers a promising approach to mitigating this bottleneck. A commonly adopted strategy applies FP8 precision during rollout while retaining BF16 precision for training. In this work, we present the first comprehensive study of FP8 RL training and demonstrate that the widely used BF16-training + FP8-rollout strategy suffers from severe training instability and catastrophic accuracy collapse under long-horizon rollouts and challenging tasks. Our analysis shows that these failures stem from the off-policy nature of the approach, which introduces substantial numerical mismatch between training and inference. Motivated by these observations, we propose Jet-RL, an FP8 RL training framework that enables robust and stable RL optimization. The key idea is to adopt a unified FP8 precision flow for both training and rollout, thereby minimizing numerical discrepancies and eliminating the need for inefficient inter-step calibration. Extensive experiments validate the effectiveness of Jet-RL: our method achieves up to 33% speedup in the rollout phase, up to 41% speedup in the training phase, and a 16% end-to-end speedup over BF16 training, while maintaining stable convergence across all settings and incurring negligible accuracy degradation."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 0 2 ] . [ 1 3 4 2 4 1 . 1 0 6 2 : r 2026-1-21 Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow Haocheng Xi1,3 Charlie Ruan3 Peiyuan Liao4 Yujun Lin1 Han Cai1 Yilong Zhao3 Shuo Yang3 Kurt Keutzer3 Song Han1,2 Ligeng Zhu1, 1NVIDIA 2MIT 3UC Berkeley 4Stanford University Reinforcement learning (RL) is essential for enhancing the complex reasoning capabilities of large language models (LLMs). However, RL training pipelines are computationally inefficient and resource-intensive, with the rollout phase consuming over 70% of the total training time. Quantized RL training, particularly with FP8, offers promising solution. For example, common approach is to employ FP8 precision during rollout to alleviate this bottleneck while retaining BF16 precision during training. In this work, we present the first comprehensive study of FP8 RL training and show that the commonly adopted BF16-train + FP8-rollout strategy suffers from severe training instability and catastrophic accuracy collapse under long-rollout generation and challenging tasks. Our analysis reveals that these issues arise from the off-policy nature of the approach, which introduces significant numerical mismatch between training and inference. Motivated by these findings, we propose Jet-RL, FP8 RL training framework that enables robust and stable RL training. The key idea is to adopt an unified FP8 precision flow for both training and rollout, minimizing numerical discrepancies and avoiding the need for inefficient inter-step calibration. Extensive experiments demonstrate the effectiveness of Jet-RL. Our method achieves up to 33% rollout phase speedup, up to 41% training phase speedup, and 16% end-to-end speedup over BF16 training while maintaining robust convergence across all settings and exhibiting negligible accuracy degradation. We will release our code and pre-trained models when less anoynomous. Figure 1 Overview of RL training different between JetRL and other methods. JetRL proposes unified precision flow for FP8 RL training accommodate both performance and throughput . Figure 2 Rollout Generation dominates the RL training latency. When the rollout length is larger than 8k, rollout will take > 75% of the total latency, making it the primary bottleneck. 1. Introduction The emergence of frontier reasoning large language models (LLMs) [1, 2] has demonstrated unprecedented capabilities in solving highly complex, multi-step problems [3, 4], setting new state-of-the-art across domains ranging from advanced mathematics to scientific discovery. key driver of this success is the Chain-ofThought (CoT) [5, 6] paradigm, which enables models to generate extended, coherent reasoning traces before producing final answers. This mechanism allows LLMs to conduct detailed analyses, explore alternative solution paths, and execute structured logical reasoning. Reinforcement Learning (RL) [7, 8] has emerged as the pivotal training paradigm for unlocking robust CoT generation, propelling models from simple question answering toward goal-directed reasoning. However, RL training remains notoriously resourceintensive, with the actor rollout phase becoming the primary training bottleneck [9]. Effective reasoning often demands generating long token sequences (e.g., >6,000 tokens) to adequately explore the solution space. Owing to the inherently autoregressive nature of LLMs, this rollout stage incurs substantial training time, frequently accounting for more than 70% of the 2026 NVIDIA. All rights reserved. Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow Figure 3 Naive BF16 Train + FP8 Rollout fails when rollout context length increases. We observe that although the BF16-Train-FP8-Rollout method might exhibit similar performance compared with BF16 training, its performance quickly degrades as we extend the rollout length to more than 8k. We use the Qwen3-8B-Base model for this experiment, and evaluate and train on MATH. total end-to-end training pipeline (Figure 2). This extreme imbalance significantly hampers overall training efficiency, making rollout acceleration an urgent priority [10, 11]. To improve efficiency, FP8 quantization offers highly promising solution. It has been widely adopted in LLM inference, demonstrating significant efficiency gains [12, 13, 14, 15]. natural approach to accelerate RL training is to apply FP8 quantization during the rollout phase, which constitutes the main efficiency bottleneck, and retain BF16 precision for the training phase, which is expected to deliver better training stability and accuracy. This strategy BF16train-FP8-rollout has been widely adopted in modern RL frameworks such as VeRL, SLIME, Nemo-RL, and OpenRLHF [16, 17, 18, 19]. However, we find that this strategy exhibits critical limitations in two circumstances: (i) Long-Rollout Generation. First, we observe that this training strategy suffers from significant accuracy degradation and can even experience catastrophic collapse when applied to long sequences. For example, Figure 3 shows the training curves of the Qwen3-8B-Base model across 4K, 8K, and 16K token generations. Compared with BF16 training, its accuracy rapidly drops once the rollout length exceeds 8K tokens, ultimately resulting in significant performance drop at the 16K setting. Small numerical discrepancies will gradually accumulate during long chain-of-thought reasoning. The mismatch between training and rollout precision are negligible for short sequences but progressively increases with the rollout length. These accumulated errors amplify the effects of off-policy training, causing the generation trajectory to diverge and making RL training unstable. (ii) Challenging Tasks. Second, we observe that the claimed success of recent BF16-train-FP8-rollout is not always reliable. As shown in Figure 4, the strategy shows no degradation when the model already possesses strong task priors from pretraining. However, when applied to harder reasoning tasks or trained with weaker base models, the training curve quickly diverges between BF16 and FP8 rollouts. These observations suggest that effective quantized rollout must be robust and adaptable across diverse training settings. To address these limitations and establish FP8 rollout as reliable acceleration strategy, we propose Jet-RL. Our core contribution is enforcing truly on-policy FP8 training paradigm that stabilizes RL training. We design our framework to use an identical quantization precision flow for both training and inference, eliminating policy mismatch and removing the need for inter-step calibration. Our approach adopts mixed per-group and per-block quantization scheme [20, 14] and leverages state-ofthe-art FP8 GEMM kernels [21] to achieve acceleration for end-to-end RL training. We conduct comprehensive experiments across diverse models, datasets, and rollout configurations to validate Jet-RL. Our results demonstrate that Jet-RL successfully stabilizes training, minimizes divergence between training and rollouts, and substantially narrows the performance gap between BF16 and FP8 RL. While BF16-train-FP8-rollout methods typically incur more than 5% performance degradation compared to BF16 baselines, our approach reduces this to 1%. Meanwhile, Jet-RL achieves up to 1.33 rollout phase speedup for 32B model, up to 1.41 training phase speedup for 8B model, and 1.16 endto-end speedup for 8B experiments. These findings confirm that our method provides robust solution for efficient low-precision RL training, enabling sig2 Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow nificant acceleration without sacrificing performance. We summarize our contributions below: calculate the gradient of weights, and DGrad in the backward pass to calculate the gradient of activations. Formally, they can be expressed as: We identify that the commonly used BF16-trainFP8-rollout paradigm leads to training instability and accuracy collapse under long-rollout generation and challenging tasks. We propose Jet-RL, an on-policy FP8 RL framework by enforcing an unified trainingrollout precision flow for training and rollout. This approach resolves policy mismatch and ensures stable optimization. Jet-RL achieves substantial rollout and end-toend speedups while maintaining convergence and accuracy close to BF16 baselines across various models and tasks. 2. Background 2.1. Quantization Basis Quantization maps high-precision tensor to lower-precision tensor to speed up computation and reduce memory footprint. We consider quantizing tensor ğ‘‹ into target data format whose maximum representable value is Î”max. The quantization process can be defined as: ^ğ‘‹, ğ‘†ğ‘‹ = ğ‘„(ğ‘‹), where ğ‘„() is the quantizer ^ğ‘‹ = ğ‘‹ ğ‘†ğ‘‹ , ğ‘†ğ‘‹ = max(ğ‘‹) Î”max . Here, ^ğ‘‹ is the low-precision tensor obtained after quantization, and ğ‘†ğ‘‹ is the scaling factor. In this paper, we focus on FP8 quantization using the E4M3 format [22, 23], whose maximum representable value is Î”max = 448. 2.2. Quantization of Linear Layer When quantizing large language models, linear layers are the primary operators of interest, as they are compute-bound. Following previous works on quantized training [24], the input and output of linear layer are denoted as: ğ‘Œ Rğ‘ ğ·, ğ‘‹ Rğ‘ ğ¶, ğ‘Š Rğ·ğ¶, where ğ‘Œ is the output, ğ‘‹ is the activation, and ğ‘Š is the weight. ğ‘ is the number of tokens, ğ· is the number of output channels, and ğ¶ is the number of input channels. Their corresponding gradients are denoted as ğ‘Œ , ğ‘‹ , and ğ‘Š , each with the same shape. Each linear layer has three GEMMs: FProp in the forward pass, WGrad in the backward pass to Forward pass (FProp) ğ‘Œ = ğ‘‹ ğ‘Š , where Rğ‘ ğ· = Rğ‘ ğ¶ Rğ¶ğ·. Backward pass Gradient of weights (WGrad) ğ‘Š = ğ‘Œ ğ‘‹, where Rğ·ğ¶ = Rğ·ğ‘ Rğ‘ ğ¶. Backward pass Gradient of input (DGrad) ğ‘‹ = ğ‘Œ ğ‘Š , where Rğ‘ ğ¶ = Rğ‘ ğ· Rğ·ğ¶. Quantizing these three GEMMs for FP8 computation has specific matrix layout requirements, as current FP8 TensorCore hardware expects the first operand to be stored in row-wise (R) manner and the second operand in column-wise (C) manner. The layout are summarized in Table 1. Table 1 Layout requirements for FP8 GEMMs in linear layer."
        },
        {
            "title": "Expression",
            "content": "ğ‘‹ ğ‘Š ğ‘Œ Row Row Col Col - ğ‘‹ ğ‘Š - ğ‘Œ ğ‘‹ Col Row ğ‘Œ ğ‘Š 2.3. Workload of Reinforcement Learning standard Reinforcement Learning (RL) training pipeline, such as one based on Proximal Policy Optimization (PPO) [25], typically consists of four distinct models. The Actor Model is the primary Large Language Model (LLM) we aim to train. The Reference Model, often an initial copy of the actor, is used to calculate the KullbackLeibler (KL) divergence, which regularizes the actors updates and ensures training stability. The Reward Model provides the scalar reward signal, typically learned to reflect human preferences or task objectives. Finally, the Critic Model estimates the value (or quality) of the generated responses, predicting expected future rewards. Each RL training step can be broken into three distinct phases, where each phase is composite of three fundamental LLM workloads: decode, prefill, and training. First, in the Rollout phase, the Actor model performs an autoregressive decode process to generate one or more responses for given prompt. Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow We use the term rollout for this generation stage to differentiate it from simple inference. Second, in the Evaluation phase, the generated responses are fed into the other models. The Reference, Reward, and Critic models each perform single forward pass, which constitutes prefill workload, to compute their respective outputs. Third, in the Update phase, these metrics are gathered (often processed via Generalized Advantage Estimation (GAE) [26]), and the Actor model executes training step, consisting of one forward and one backward pass, to update its weights. These phases have distinct computational profiles, leading to common implementation strategy that employs two different types of systems. The Rollout phase is typically managed by optimized inference engines like vLLM [27] or SGLang [28], as they are highly optimized for autoregressive workloads. Conversely, the Evaluation and Update phases are handled by training frameworks such as FSDP [29], MegatronLM [30], or DeepSpeed [31], which provide greater flexibility in parallelism. To maintain the RL training on-policy , the weights of updated actor must be transferred from the training framework to the inference engine after each step. Despite optimizations from inference engines, the rollout stage remains critical performance bottleneck. As we show in 3.1, the rollout phase latency scales with the length of the generated response. Consequently, this stage gradually dominates the end-toend training latency, making it the most expensive part of the entire RL training pipeline. 3. Motivation 3.1. Rollout is the Bottleneck in RL Training In Figure 2, we profile each component in RL training and experiment with the Qwen3-8B-Base [32] model on GSM8K [33] and MATH [34]. To understand how training time scales with rollout length, we vary the maximum rollout length from 1K to 16K and measure how the latency proportion of each training component changes. We observe that when the rollout length exceeds 8K, rollout generation alone accounts for over 70% of the total training time. This trend is consistent with observations reported in previous studies [9], highlighting the critical need to accelerate rollout. FP8 quantization is promising solution to speed up rollout for two reasons. First, it can offer an ideal 2 speedup over BF16. Second, wide range of studies shows that FP8 inference does not degrade performance on downstream tasks. Moreover, it can be easily integrated into existing RL training pipelines. Existing studies have proposed simply casting the BF16 weights to FP8 while keeping BF16 for the training phase. In the rest of this paper, we refer to this simple strategy as BF16-train-FP8-rollout and discuss the limitations of such design. 3.2. BF16-Train-FP8-Rollout with Calibration is Slow While most Post-Training Quantization (PTQ) methods [35, 36, 37] are designed for offline deployment, RL training requires frequent weight synchronization between the training actors and the rollout actors. The expensive and data-dependent calibration can take tens of minutes even for small 8B LLMs, thus becoming unaffordable and conflicting with the purpose of acceleration if repeated at every synchronization step. Some frameworks such as SLIME [38] and NeMo-RL [39] propose to directly cast the BF16 weights to FP8 without calibration and claim the accuracy is not affected. However, we find this conclusion is in fact very fragile. 3.3. BF16-Train-FP8-Rollout without Calibration is Unstable The quality of RL training highly relies on the onpolicy assumption, which dictates that the agent learns from data and experiences collected while following its current policyin LLM training, this requires that rollout and training should produce the same logits when given the same prompts. RL training without this consistency may lead to divergence in extreme cases [40, 41, 42]. Directly quantizing the rollout actor to FP8 while updating full-precision actor in BF16 clearly breaks this consistency and introduces significant policy mismatch [43, 44, 45]. Though in some cases and some studies like Truncated Importance Sampling (TIS), BF16-train-FP8rollout does not result in much performance degradation, we find that this conclusion is very fragile and depends on specific dataset and model settings. Specifically, we observe that BF16-train-FP8-rollout tends to fail more in two scenarios: long-rollout generation and challenging tasks. Failure in long-rollout generation. We observe that the performance of BF16-train-FP8-rollout is strongly correlated with rollout length. We train and evaluate Qwen2.5-7B [46] on MATH using GRPO, varying the generation length from 4K to 16K. We then compare the performance of BF16 training and BF16-train-FP8-rollout. As shown in Figure 3, although BF16-train-FP8rollout performs on par with BF16 training when the 4 Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow trajectory. This leads to growing mismatch between the FP8 rollout and BF16 training, resulting in unstable optimization and degraded performance. In essence, BF16-train-FP8-rollout is only robust for easier tasks but faces convergence issues when scaling to harder tasks. Considering that the goal of RL is to help models acquire abilities they dont yet have, BF16-train-FP8-rollout is unlikely to suffice, as its instability on harder tasks hinders effective learning and scalability. These observations motivate key question: How can we mitigate the traininginference discrepancy in BF16-train-FP8-rollout to achieve competitive and stable performance across varying settings? 4. Jet-RL: Enabling on-policy FP"
        },
        {
            "title": "RL Training",
            "content": "We find that the root cause of BF16-train-FP8rollout training failure is the inconsistency of the precision flow between training and rollout, making the RL training effectively off-policy. Given the critical importance of maintaining on-policy consistency in RL [47], addressing this issue is essential. To mitigate this issue, we propose enforcing an unified FP8 precision flow between training and rollout, ensuring that the responses in rollout are consistent with training, making it an on-policy process. Formally, we model the propagation of quantization precision across the model as directed graph ğ’¢ = (ğ’±, â„°), as illustrated in Figure 5. node ğ‘£ğ‘– ğ’± represents either an operator or weight within the model, which we refer to as operator nodes or weight nodes. directed edge (ğ‘£, ğ‘£) â„° is connected when the output of ğ‘£ serves as the input of ğ‘£ to describe tensor propagation between two connected operators. An edge indicates the precision of the transferred tensor and the quantization granularity of the tensor (if quantized). In training, we have the graph ğ’¢train, which can be . and ğ’¢bwd further separated into two subgraphs ğ’¢fwd train train , edges represent activations, while in Within ğ’¢fwd train , edges represent gradients. They share the ğ’¢bwd train same nodes and topology since they have the same suite of operators, but the edge direction is reversed. These two graphs are connected by edges representing activations saved for the backward pass. For the inference engine, we have the graph ğ’¢infer, when using In the FP8 rollout scenario, the which also has the same topology as ğ’¢fwd train BF16 inference. weight nodes and their edges are in FP8 precision. For BF16-train-FP8-rollout, all weights and activations in the training graph are in BF16. How5 Figure 4 We conduct training using Qwen3-8B (reasoning) and Qwen3-8B-Base on MATH. When the model is trained on task that is easy for itself, BF16-Train-FP8Rollout is less likely to degrade. When the task is hard for the model we trained on, degradation is likely to happen. rollout length is small (<4K), the FP8 rollout quickly diverges from BF16 training when rollout length scales to 8K. When we further increase the rollout length to 16K, FP8 rollouts accuracy collapses after only 20 steps of training. We hypothesize that this degradation arises from the accumulation of differences between the rollout and training distributions at each decoding step. For shorter generation lengths, these discrepancies remain relatively minor and may even be partially beneficial for training. However, as the rollout length increases, the cumulative divergence between the rollout and training distributions becomes substantial. This intensifies the off-policy issue in reinforcement learning under FP8 rollouts, leading to significant instability and degraded training performance. Failure in challenging tasks. We observe that BF16-train-FP8-rollout tends to fail when the underlying model lacks strong capability on the target task. For instance, as shown in Figure 4, when training Qwen3-8B on GSM8K, the BF16-train-FP8-rollout training curve closely matches the BF16 training curve and even converges faster. However, after switching to the Qwen3-8B-Base model, which has not undergone instruction tuning and needs to learn both reasoning patterns and question-solving ability, BF16-train-FP8rollout soon falls behind the BF16 baseline and leads to inferior performance. In this setting, the evaluation accuracy diverges from the BF16 baseline after only 20 training steps. We hypothesize that BF16-train-FP8-rollout performs well when the model is strong and the task is relatively simple. In such cases, the model exhibits high confidence in its responses, rendering it less sensitive to minor numerical perturbations brought by FP8 quantization. Conversely, as task difficulty increases and the models confidence decreases, quantizationinduced errors can substantially distort the rollout Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow Figure 5 Overall FP8 precision flow of Jet-RL. The inference graph is subgraph of the training graph. ever, in its inference graph ğ’¢infer, all edges that feed into linear layer are quantized into FP8. This makes BF16-train-FP8-rollout have two distinct forward quantization graphs: ğ’¢fwd train and ğ’¢infer. This leads to mismatch between training and rollout, making the forward process deviate from what the actor will actually generate during rollout. Therefore, the RL training is off-policy, unstable, and unlikely to achieve satisfactory results. backward computation. Consider an edge (ğ‘£, ğ‘£) in the forward pass, where the tensor is in FP8 precision and ğ‘£ is GEMM operator. The ğ‘£ operator can only access the FP8 tensor, since the quantization is usually fused with the previous step. Therefore, we elect to store the activations for the backward pass also in FP8 precision. This strategy has been shown to maintain training stability in prior work on pretraining and supervised fine-tuning [14]. 4.1. Unified FP8 Precision Flow between"
        },
        {
            "title": "Training and Rollout",
            "content": "In Jet-RL, we propose solving this problem by forc- . All other ating ğ’¢infer to be subgraph of ğ’¢fwd train tributes (precision and granularity) of the edges are kept the same. The only difference is that ğ’¢fwd has train higher-precision master copy to stabilize training, since the master weight of ğ’¢train needs to be stored in BF16. We ensure that the forward pass of training and inference frameworks shares consistent quantization behavior, thereby mitigating the mismatch in precision propagation observed in previous approaches. This is demonstrated in Figure 5. For ğ’¢bwd train , we first focus on the activations saved for We retain the gradients transported between operators during the backward pass in BF16 precision to preserve model accuracy. Although quantizing them can further reduce communication overhead, they often introduce gradient underflow or quantization noise that degrades convergence. The GEMMs in the backward pass (DGrad and WGrad) are also quantized into FP8 precision for acceleration. We elaborate on the details in the following section. 4.2. Granularity of GEMM Quantization We quantize all GEMM operators in training and inference for the linear layers to accelerate computation. As shown in Figure 6, the FProp operator in the forward pass and the WGrad and DGrad operators in 6 Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow Figure 6 Our quantization scheme for linear layer. The FProp GEMM and the WGrad GEMM uses an (1 128) (128 128) FP8 Matmul kernel, while the DGrad GEMM uses an (1 128) (128 1) FP8 matmul kernel. the backward pass all take FP8 tensors as input and output BF16 tensors. In this section, we discuss the quantization scheme adopted for these FP8 GEMMs. Per-tensor quantization of FP8 has been shown to be unstable in training large language models. Therefore, we adopt finer-grained quantization granularity when quantizing the activations, weights, and gradients into FP8 precision. Specifically, we quantize the weights using 128 128 per-block quantization and quantize the activations and gradients using 1 128 per-group quantization. We now discuss the strategy for each GEMM below. These two operators both require the quantized gradient, but one requires it to be 1 128 quantized while the other requires it to be 128 1 quantized, so we fuse these quantization processes. Since the weights quantization scheme is symmetric along the channel and row axis, its value does not change in the backward pass, so we only need to perform transpose in the backward pass. For the activations, in the forward pass they are quantized as 1 128, but in the backward pass, they need to be quantized as 128 1. This discrepancy forces us to quantize them again in the backward pass. This is even beneficial for quantized training. FProp Operator For the FProp operator, the input activation is quantized with 1 128 per-group quantization, while the weight is quantized with 128128 per-block quantization. As described in 2.2, the hardware kernel needs to be row-wise columnwise, so both the activation and weight are stored in row-wise layout. The quantization of activation can be fused with its previous operator to reduce overhead, while the quantization of weight needs to be done explicitly during training. For weight quantization in inference, we quantize the weight during the parameter update stage, which incurs negligible overhead and is fully compatible with tensor parallelism. DGrad Operator and WGrad Operator For the DGrad operator, its workload is equivalent to the FProp operator, which is 1 128 quantized matrix multiplies 128 128 quantized matrix. Thus, it can directly reuse the same kernel configuration. For the WGrad operator, following DeepSeek-V3, we quantize the first matrix in 1 128 while quantizing the second one in 128 1. This finer-grained design helps stabilize training. 4.3. Implementation We use vLLM [27] to serve as the inference engine and VeRL [16] as our RL training framework. For the quantized GEMMs, we reference the kernels from DeepGEMM [21]. We use Triton [48] to implement the quantization, transpose, and fused activation or RMSNorm kernels. 5. Evaluation 5.1. Evaluation Setup Models. We evaluate Jet-RL on several widely used open-source models, including Llama3.1-8B, Qwen2.5-7B, and Qwen3-8B-Base. For Llama3.1-8B and Qwen2.5-7B, we set the rollout length to 8K. For Qwen3-8B-Base, we train on both 8K and 16K rollout lengths. Datasets and RL Training Setting. We conduct experiments under two dataset settings. We first train on mixture of GSM8K and MATH, where we set 7 Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow Model Training Precision GSM8k MATH 500 GPQA SuperGPQA Average Llama3.1-8B Rollout Length = 8k Dataset = GSM8k + MATH Initial 0.8 3.3 17.3 12.4 8. BF16 BF16-Train-FP8-Rollout Jet-RL 49.0 20.6 -28.4 47.2 -1.8 12.1 5.4 -6.7 11.3 -0.8 15.7 11.2 -4.5 22.3 +6.6 15.9 14.7 -1.2 19.9 +4.0 23.2 13.0 -10.2 25.2 +2.0 Qwen2.5-7B Rollout Length = 8k Dataset = GSM8k + MATH Initial BF16 BF16-Train-FP8-Rollout Jet-RL 17. 91.8 46.5 71.0 28.9 36.0 25. 28.6 Did not converge 29.6 56.9 89.9 -1.9 69.5 -1.5 35.5 -0.5 28.5 -0.1 55.9 -1.0 Qwen3-8B-Base Rollout Length = 8k Dataset = GSM8k + MATH Initial 63.4 69.7 46.2 31.8 BF16 BF16-Train-FP8-Rollout Jet-RL 92.9 92.1 -0.8 93.1 +1.0 83.3 77.0 -6.3 81.3 -2.0 43.7 44.2 +0.5 42.1 -1.6 35.1 30.3 -4.8 35.2 +0.1 52.6 63.8 60.9 -2.9 62.7 -1.1 Table 2 Performance comparison of Jet-RL and baselines when rollout length is set to 8k. In all settings of models, Jet-RL greatly reduces the degradation introduced by BF16-train-FP8-rollout and achieves close performance to BF16 RL training. Green (red) micro-numbers indicate absolute gain (drop) vs. BF16 within each model block; blank means the baseline did not converge. the rollout generation number to 4. GSM8K contains 8,500 grade school math word problems, and MATH contains 12,500 complex math competition problems. We then train on the DeepMATH dataset [49], where we set the rollout generation number to 16. DeepMATH contains 103K math problems designed with high difficulty. Hyperparameters. For all experiments, we set the learning rate to 106 and the batch size to 256. The KL loss coefficient is set to 103. The evaluation of checkpoints is conducted every 5 steps during RL training. We conduct the experiments on NVIDIA H100 GPUs. Evaluation Metrics. We evaluate the final checkpoints on 5 downstream benchmarks: GSM8K [33], MATH500 [34], AMC, GPQA [50], and SuperGPQA [51], and their average score. We evaluate GSM8K and MATH500 on their test splits. For AMC, GPQA, and SuperGPQA, we follow the preprocessing strategy of Guru-92k [11]. 5.2. Accuracy Evaluation We first analyze the results when using an 8K rollout length. As shown in Table 2, which evaluates models using an 8K rollout length on the GSM8K + MATH dataset, the BF16-train-FP8-rollout method exhibits significant instability. Most notably, it fails to converge entirely on the Qwen2.5-7B model. On models where it does converge, it incurs substantial performance degradation compared to the BF16 baseline. For instance, on Llama3.1-8B, the average score drops by 9.8%, and on Qwen3-8B-Base, it drops by 2.9%. In sharp contrast, our Jet-RL method proves It not only converges in all robust and effective. scenarios but also greatly reduces the gap to BF16 training. On Llama3.1-8B, Jet-RL even outperforms the BF16 baseline by 2.0%. On Qwen2.5-7B, the performance degradation is merely 1.0% (56.9% vs 55.9%), and on Qwen3-8B-Base it degrades by only 1.1% (63.8% vs 62.7%). We analyze the performance under more challenging configurations, including 16K rollout length and the DeepMATH dataset, as presented in Table 3. This further underscores the instability of the BF16-train-FP8-rollout method, particularly under more challenging configurations such as 16K rollout length or different training dataset (DeepMATH). The BF16-train-FP8-rollout method fails to converge on the Qwen3-8B-Base model with 16K rollout. On the Qwen3-8B-Base (DeepMATH) experiment, it suffers severe performance degrada8 Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow Model Training Precision GSM8k MATH 500 GPQA SuperGPQA Average Qwen2.5-7B Rollout Length = 16k Dataset = GSM8k + MATH Before Tuning 13.5 45.3 28.4 26.0 BF16 BF16-Train-FP8-Rollout Jet-RL 91.5 90.7 -0.8 89.3 -2.2 72.8 68.1 -4.7 69.2 -3.6 28.4 28.9 +0.5 36.5 +8.1 42.2 26.9 -15.3 27.8 -14.4 Qwen3-8B-Base Rollout Length = 16k Dataset = GSM8k + MATH Before Tuning BF16 BF16-Train-FP8-Rollout Jet-RL 63.7 94.7 69. 81.7 92.1 -2.6 76.7 -5.0 45.7 30.7 33.5 46.2 Did not converge 43.3 -2.9 33.2 -0.3 Qwen3-8B-Base Rollout Length = 16k Dataset = DeepMATH 28.3 58.7 53.7 -5.0 55.7 -3.0 52.5 64.0 61.3 -2.7 Before Tuning 63.4 69. 46.2 31.8 52.8 BF16 BF16-Train-FP8-Rollout Jet-RL * * * 83.4 57.8 -25.6 80.2 -3.2 44.2 42.6 -1.6 47.2 +3.0 36.1 32.6 -3.5 33.8 -2.3 54.6 44.3 -10.3 53.7 -0.9 Table 3 Performance comparison of Jet-RL and baseline when rollout length is 16k, or trained on DeepMATH. In all settings of models and dataset, Jet-RL greatly reduce the degradation introduced by BF16-trainFP8-rollout and achieve close performance to BF16 RL training. tion of 10.3% compared to the BF16 baseline. On the Qwen2.5-7B (16K) model, it also shows notable degradation of 5.0%. In contrast, Jet-RL successfully resolves these issues. It converges on the Qwen38B model, closing the gap to just 2.7%. On the Qwen3-8B-Base (DeepMATH) experiment, Jet-RL reduce the gap to 0.9% (54.6% vs 53.7%). On the Qwen2.5-7B model, the degradation is also reduced from 5.0% to 3.0%. The results demonstrate that Jet-RL framework achieves more stable convergence and better performance compared to BF16-train-FP8rollout, and Jet-RL is closely aligned with the BF16 baselines. Model & TP 4K 8K 16K 8B (TP=1) 14B (TP=1) 14B (TP=2) 32B (TP=2) 32B (TP=4) 1.12 1.10 1.12 1.26 1.28 1.29 1.08 1.12 1.10 1.29 1.33 1.30 1.08 1.07 1.10 Table 4 Rollout speedup in FP8 over BF16 in tokens/s at different output lengths. Used 512 prompts with max concurrent request 128, and 512 input length. Measured on H100s. 5.3. Efficiency Evaluation We quantify rollout efficiency gains from FP8 with an offline generation benchmark in vLLM [27], reporting throughput speedup from FP8. We vary the model size from 8B to 32B, and test under multiple tensor parallel settings. As summarized in Table 4, FP8 achieves consistent acceleration, with speedups ranging from 1.07 to 1.33 over BF16. We observe two trends in the speedup ratio. First, the benefit of FP8 quantization increases with model size. Larger models, such as the 32B configuration, achieve the most substantial speedups up to 1.33, as it is more compute-intensive. This allows the optimized FP8 tensor-core kernels (e.g., DeepGEMM) to more effectively accelerate computation. In smaller models (e.g., 8B), memory access overhead constitutes larger fraction of latency, limiting the overall benefit of FP8 inference. Second, higher degrees of tensor parallelism (TP) decrease the observed speedup. When the model is distributed across more GPUs, the communication overhead becomes more pronounced. For example, the 32B model with TP=4 shows only 1.1Ã– improvement compared to 1.3Ã– at TP=2. This trend highlights that FP8 rollout can be accelerated under lower tensor parallel degree. For end-to-end RL training on Qwen3-8B with an Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow 8K rollout length, FP8 quantization delivers consistent acceleration across multiple computation stages. Specifically, FP8 achieves 1.54 speedup in the actor update phase and 1.80 speedup in the reference model inference, which together contribute to an overall 1.41 improvement in the training phase throughput. Combined with the speedup from rollout, this results in an end-to-end step-time speedup of 1.16 for 8B model training. We expect the speedup to be much more significant for larger model sizes. full scaling study on 1432B models is left to future work given resource constraints. 6. Related Works 6.1. Low-Precision Training and Inference for"
        },
        {
            "title": "LLMs",
            "content": "The exponential growth in the parameter counts of Large Language Models has rendered full-precision training and inference prohibitively expensive, creating significant barriers related to memory, computation, and energy consumption [52, 53, 54]. This fundamental challenge has catalyzed the development of low-precision techniques, which reduce the numerical precision of model weights, activations, and gradients to lower-precision formats to leverage hardware accelerators like NVIDIAs Tensor Cores that offer significantly higher throughput for low-precision arithmetic. Post-Training Quantization (PTQ), which compresses pre-trained model in \"one-shot\" fashion without fine-tuning, has become the dominant approach for deploying LLMs. SmoothQuant [37] addresses the activation outlier problem by migrating the quantization difficulty from activations to weights. GPTQ compresses 175-billion-parameter models down to 3 or 4 bits with minimal impact on perplexity. AWQ protects salient weights by applying per-channel scaling factor in an activation-aware, reconstruction-free approach. On the other hand, Fully Quantized Training (FQT) accelerates the training stage of LLMs by performing computations in low precision. SwitchBack and Jetfire [24]propose advanced INT8 training for transformers through an INT8 precision flow to optimize memory access and per-block quantization method to maintain accuracy. NVIDIA Transformer Engine [55] accelerates transformer model training using FP8. Building on this, frameworks like COAT extend FP8 quantization beyond just linear layers to include optimizer states and activations. Concurrently, methods Due to format issue we do not report GSM8k results when training on DeepMATH. like QLoRA have focused on memory efficient finetuning using 4-bit quantization of frozen pre-trained model with Low-Rank Adaptation (LoRA). 6.2. Reinforcement Learning for Large Language Models Reinforcement Learning (RL) has played central role in the evolution of large language models (LLMs), progressing from alignment to reasoning. Early work on alignment employed Reinforcement Learning from Human Feedback (RLHF), where reward model trained on human preference data guided policy optimization via algorithms like Proximal Policy Optimization (PPO). Subsequent methods, such as Direct Preference Optimization (DPO), removed the explicit reward model, enabling simpler and more stable preference-based fine-tuning. Recent efforts have shifted toward reasoningoriented RL, where the correctness of models output can be automatically verified. This shift has spurred the development of new RL algorithms tailored for reasoning tasks. For instance, the DeepSeek-R1 model was developed using GRPO (Group Relative Policy Optimization), critic-free RL algorithm that estimates advantages by comparing responses reward to the average reward of group of responses generated from the same prompt. GSPO was introduced to enhance training stability by performing optimization at the sequence level. DAPO introduces several techniques, including \"Clip-Higher\" to avoid entropy collapse and \"Dynamic Sampling\" to filter out uninformative training samples for better stability and efficiency. 6.3. Efficient Reasoning with Large Language"
        },
        {
            "title": "Models",
            "content": "While RLVR is powerful paradigm for training reasoning models, it is computationally intensive due to the need to generate long autoregressive rollouts and the on-policy nature of RL, which can lead to low hardware utilization. This has motivated research into both system-level and algorithmic optimizations to make reasoning more efficient. From systems perspective, async RL frameworks such as AReaL have been proposed to break the synchronous dependency between rollout and training. These asynchronous systems allow rollout workers to continuously generate new data while training workers update the model, thereby improving GPU utilization and significantly increasing training throughput. ReaLHF focuses on improving the parallelism strategy of RL training. By automating the parallelization search period, ReaLHF improves GPU utilization and 10 Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow reduces GPU idle time. From an algorithmic perspective, key challenge is the phenomenon of \"overthinking,\" where models generate excessively long and redundant reasoning trajectories. NoThinking [56] propose to prune the reasoning trajectory and teaches models to self-regulate their reasoning process by identifying redundant steps and generating training signals that encourage earlier termination. Finally, co-design offers another angle to tackle these efficiency challenges. The QeRL framework [57] addresses the high cost of RL rollouts by combining NVFP4 quantization with Low-Rank Adaptation (LoRA). This synergy drastically reduces the memory footprint and accelerates the generation phase, enabling efficient RL training of 32B model on single GPU. Truncated Importance Sampling (TIS) [47] proposes to mitigate the off-policy issue by adding the importance ratio to the model update and truncating it if the inference probability is small. 7. Conclusion In this work, we addressed the critical performance bottleneck of the rollout phase in RL training. We demonstrated that the naive BF16-train + FP8rollout strategy is fundamentally flawed, as it introduces training-rollout mismatch that leads to training instability and catastrophic performance collapse. To solve this, we proposed Jet-RL, framework that enables robust on-policy FP8 RL training by adopting an identical FP8 precision flow for both training forward pass and inference rollout stage. Our comprehensive evaluations show that Jet-RL robustly converges across all models and benchmarks settings. Our method also maintains competitive performance close to the BF16 RL baseline, usually less than 1% degradation. By delivering up to 1.33 rollout phase speedup, up to 1.41 training phase speedup, and an 1.16 end-to-end speedup without sacrificing model accuracy, Jet-RL establishes reliable and efficient path forward for applying FP8 computation to accelerate large-scale RL training. 11 Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow"
        },
        {
            "title": "References",
            "content": "[1] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. 1 [2] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 1 [3] Francois Chollet, Mike Knoop, Gregory Kamradt, and Bryan Landers. Arc prize 2024: Technical report. arXiv preprint arXiv:2412.04604, 2024. 1 [4] Francois Chollet, Mike Knoop, Gregory Kamradt, Bryan Landers, and Henry Pinkard. Arc-agi-2: new challenge for frontier ai reasoning systems, 2025. 1 [5] Denny Zhou and Xuezhi Wang. Chain-of-thought reasoning without prompting, 2024. [6] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, F. Xia, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. ArXiv, abs/2201.11903, 2022. 1 [7] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, P. Welinder, P. Christiano, J. Leike, and Ryan J. Lowe. Training language models to follow instructions with human feedback. ArXiv, abs/2203.02155, 2022. 1 [8] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova Dassarma, Dawn Drain, Stanislav Fort, Deep Ganguli, T. Henighan, Nicholas Joseph, Saurav Kadavath, John Kernion, Tom Conerly, S. El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom B. Brown, Jack Clark, Sam McCandlish, C. Olah, Benjamin Mann, and Jared Kaplan. Training helpful and harmless assistant with reinforcement learning from human feedback. ArXiv, abs/2204.05862, 2022. 1 [9] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. Dapo: An open-source llm reinforcement learning system at scale, 2025. 1, 4 [10] Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. Searchr1: Training llms to reason and leverage search engines with reinforcement learning. 2025. 2 [11] Zhoujun Cheng, Shibo Hao, Tianyang Liu, Fan Zhou, Yutao Xie, Feng Yao, Yuexin Bian, Yonghao Zhuang, Nilabjo Dey, Yuheng Zha, Yi Gu, Kun Zhou, Yuqi Wang, Yuan Li, Richard Fan, Jianshu She, Chengqian Gao, Abulhair Saparov, Haonan Li, Taylor W. Killian, Mikhail Yurochkin, Zhengzhong Liu, Eric P. Xing, and Zhiting Hu. Revisiting reinforcement learning for llm reasoning from cross-domain perspective, 2025. 2, [12] Maxim Fishman, Brian Chmiel, Ron Banner, and Daniel Soudry. Scaling fp8 training to trilliontoken llms, 2025. 2 [13] Houwen Peng, Kan Wu, Yixuan Wei, Guoshuai Zhao, Yuxiang Yang, Ze Liu, Yifan Xiong, Ziyue Yang, Bolin Ni, Jingcheng Hu, Ruihang Li, Miaosen Zhang, Chen Li, Jia Ning, Ruizhe Wang, Zheng Zhang, Shuguang Liu, Joe Chau, Han Hu, and Peng Cheng. Fp8-lm: Training fp8 large language models, 2023. 2 [14] Haocheng Xi, Han Cai, Ligeng Zhu, Yao Lu, Kurt Keutzer, Jianfei Chen, and Song Han. Coat: Compressing optimizer states and activation for memory-efficient fp8 training. arXiv preprint arXiv:2410.19313, 2024. 2, 6 [15] Jiwoo Kim, Joonhyung Lee, Gunho Park, Byeongwook Kim, Se Jung Kwon, Dongsoo Lee, and Youngjoo Lee. An inquiry into datacenter tco for llm inference with fp8, 2025. 2 [16] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. 2, 7 Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow [17] Zilin Zhu, Chengxing Xie, Xin Lv, and slime Contributors. slime: An llm post-training framework for rl scaling. https://github.com/THUDM/ slime, 2025. GitHub repository. Corresponding author: Xin Lv. 2 [18] Nemo rl: scalable and efficient post-training library. https://github.com/NVIDIA-NeMo/RL, 2025. GitHub repository. 2 [19] Jian Hu, Xibin Wu, Zilin Zhu, Xianyu, Weixun Wang, Dehao Zhang, and Yu Cao. Openrlhf: An easy-to-use, scalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143, 2024. 2 [20] Xin Cheng, Xiaodong Liu, Yanping Huang, Zhengyan Zhang, Peng Zhang, Jiashi Li, Xinyu Yang, Damai Dai, Hui Li, Yao Zhao, Yu Wu, Chengqi Deng, Liang Zhao, H. Zhang, Kexin Huang, Junlong Li, Yang Zhang, Lei Xu, Zhen Zhang, Meng Li, Kai Hu, DeepSeek-AI, Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Huazuo Gao, Shirong Ma, Wangding Zeng, Xiao Bi, Zihui Gu, Hanwei Xu, Kai Dong, Liyue Zhang, Yishi Piao, Zhibin Gou, Zhenda Xie, Zhewen Hao, Bingxuan Wang, Junxiao Song, Zhen Huang, Deli Chen, Xin Xie, Kang Guan, Yuxiang You, Aixin Liu, Qiushi Du, Wenjun Gao, Qinyu Chen, Yaohui Wang, Chenggang Zhao, Chong Ruan, Fuli Luo, Wenfeng Liang, Yaohui Li, Yuxuan Liu, Xin Liu, Shiyu Wang, Jiawei Wang, Ziyang Song, Ying Tang, Yuheng Zou, Guanting Chen, Shanhuang Chen, Honghui Ding, Zhe Fu, Kaige Gao, Ruiqi Ge, Jianzhong Guo, Guangbo Hao, Ying He, Panpan Huang, Erhang Li, Guowei Li, Yao Li, Fangyun Lin, Wen Liu, Yiyuan Liu, Shanghao Lu, Xiaotao Nie, Tian Pei, Junjie Qiu, Hui Qu, Zehui Ren, Zhangli Sha, Xuecheng Su, Yaofeng Sun, Minghui Tang, Ziwei Xie, Yiliang Xiong, Yanhong Xu, Shuiping Yu, Xingkai Yu, Haowei Zhang, Lecong Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Shangyan Zhou, Shunfeng Zhou, Huajian Xin, Yi Yu, Yuyang Zhou, Yi Zheng, Lean Wang, Yifan Shi, Xiaohan Wang, Wanjia Zhao, Han Bao, Wei An, Yongqiang Guo, Xiaowen Sun, Yixuan Tan, Shengfeng Ye, Yukun Zha, Xinyi Zhou, Zijun Liu, Bing Xue, Xiaokang Zhang, T. Wang, Mingming Li, Jian Liang, Jin Chen, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Chenyu Zhang, Yuchen Zhu, Yue Gong, Zhuoshu Li, Zhipeng Xu, Runji Wang, Haocheng Wang, Shuang Zhou, Ruoyu Zhang, Jingyang Yuan, Yisong Wang, Xiaoxiang Wang, Jingchang Chen, Xinyuan Li, Zhigang Yan, Kuai Yu, Zhongyu Zhang, Tianyu Sun, Yuting Yan, Yunfan Xiong, Yuxiang Luo, Ruisong Zhang, X.Q. Li, Zhicheng Ma, Bei Feng, Dongjie Ji, J.L. Cai, Jiaqi Ni, Leyi Xia, Miaojun Wang, Ning Tian, R.J. Chen, R.L. Jin, Ruizhe Pan, Ruyi Chen, S.S. Li, Shaoqing Wu, W.L. Xiao, Xiangyue Jin, Xianzu Wang, Xiaojin Shen, Xiaosha Chen, Xinnan Song, Y.K. Li, Y.X. Wei, Y.X. Zhu, Yuduan Wang, Yunxian Ma, Z.Z. Ren, Zilin Li, Ziyi Gao, Zhean Xu, Bochao Wu, Chengda Lu, Fucong Dai, Litong Wang, Qiancheng Wang, Shuting Pan, Tao Yun, Wenqin Yu, Xinxia Shan, Xuheng Lin, Y.Q. Wang, Yuan Ou, Yujia He, Z.F. Wu, Zijia Zhu, and et al. (133 additional authors not shown). Deepseek-v3 technical report, 2025. [21] DeepSeek-AI. Deepgemm: Clean and efficient fp8 gemm kernels with fine-grained scaling, October 2025. GitHub repository. 2, 7 [22] Paulius Micikevicius, Dusan Stosic, Neil Burgess, Marius Cornea, Pradeep Dubey, Richard Grisenthwaite, Sangwon Ha, Alexander Heinecke, Patrick Judd, John Kamalu, et al. Fp8 formats for deep learning. arXiv preprint arXiv:2209.05433, 2022. 3 [23] Paulius Micikevicius, Stuart Oberman, Pradeep Dubey, Marius Cornea, Andres Rodriguez, Ian Bratt, Richard Grisenthwaite, Norm Jouppi, Chiachen Chou, Amber Huffman, et al. Ocp 8-bit floating point specification (ofp8). Open Compute Project, 2023. 3 [24] Haocheng Xi, Yuxiang Chen, Kang Zhao, Kai Jun Teh, Jianfei Chen, and Jun Zhu. Jetfire: Efficient and accurate transformer pretraining with int8 data flow and per-block quantization. arXiv preprint arXiv:2403.12422, 2024. 3, 10 [25] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. 3 [26] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation, 2018. [27] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Haotong Zhang, and Ion Stoica. Efficient Memory Management for Large Language Model Serving with PagedAttention. 2023. 4, 7, 9 [28] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. 13 Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow Gonzalez, Clark W. Barrett, and Ying Sheng. Sglang: Efficient execution of structured language model programs. 2023. 4 [29] Yanli Zhao, A. Gu, R. Varma, Liangchen Luo, Chien chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can Balioglu, Bernard Nguyen, Geeta Chauhan, Y. Hao, and Shen Li. Pytorch fsdp: Experiences on scaling fully sharded data parallel. Proc. VLDB Endow., 16:38483860, 2023. [30] M. Shoeybi, M. Patwary, Raul Puri, P. LeGresley, J. Casper, and Bryan Catanzaro. Megatronlm: lanTraining multi-billion parameter guage models using model parallelism. ArXiv, abs/1909.08053, 2019. 4 [31] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pages 35053506, 2020. 4 [32] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. 4 [33] K. Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. ArXiv, abs/2110.14168, 2021. 4, 8 [34] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021. 4, 8 [35] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activationaware weight quantization for on-device llm compression and acceleration. GetMobile Mob. Comput. Commun., 28:1217, 2023. [36] Elias Frantar, Saleh Ashkboos, T. Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. ArXiv, abs/2210.17323, 2022. 4 [37] Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. ArXiv, abs/2211.10438, 2022. 4, 10 Qwen3-4b documentation. [38] THUDM. slime //github.com/THUDM/slime/blob/ 08118ecfa0570e838bf1299cf8ac9bacce8765ec/ docs/en/examples/qwen3-4B.md# bf16-training-with-fp8-inference/, 2024. Accessed: 2025-10-30. example https: [39] NVIDIA. Fp8 accuracy nemohttps:// documentation. rl github.com/NVIDIA-NeMo/RL/blob/ bc24887c72a6e1b2699a228bc87c588546dfe6b7/ docs/fp8.md#accuracy/, 2024. Accessed: 202510-30. [40] Horace He and Thinking Machines Lab. Defeating nondeterminism in llm inference. Thinking Machines Lab: Connectionism, 2025. https://thinkingmachines.ai/blog/defeatingnondeterminism-in-llm-inference/. 4 [41] Jiayi Yuan, Hao Li, Xinheng Ding, Wenya Xie, Yu-Jhe Li, Wentian Zhao, Kun Wan, Jing Shi, Xia Hu, and Zirui Liu. Understanding and mitigating numerical sources of nondeterminism in llm inference, 2025. 4 [42] The SGLang Team. inference training. Towards determinisin sglang and reproducible https://lmsys.org/blog/ tic rl 2025-09-22-sglang-deterministic/, September 2025. LMSYS Org Blog. 4 [43] Feng Yao, Liyuan Liu, Dinghuai Zhang, Chengyu Dong, Jingbo Shang, and Jianfeng Gao. Your efficient rl framework secretly brings you off-policy rl training, August 2025. [44] Haizhong Zheng, Jiawei Zhao, and Bedi Chen. Prosperity before collapse: How far can off-policy rl reach with stale data on llms?, 2025. 4 [45] Edoardo Cetin, Philip J. Ball, Steve Roberts, and Oya Celiktutan. Stabilizing off-policy deep reinforcement learning from pixels, 2022. 4 [46] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang 14 Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. [47] Liyuan Liu, Feng Yao, Dinghuai Zhang, Chengyu Dong, Jingbo Shang, and Jianfeng Gao. Flashrl: 8bit rollouts, full power rl, August 2025. 5, 11 [48] Philippe Tillet, Hsiang-Tsung Kung, and David Cox. Triton: an intermediate language and compiler for tiled neural network computations. In Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, pages 1019, 2019. 7 [49] Zhiwei He, Tian Liang, Jiahao Xu, Qiuzhi Liu, Xingyu Chen, Yue Wang, Linfeng Song, Dian Yu, Zhenwen Liang, Wenxuan Wang, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. Deepmath-103k: large-scale, challenging, decontaminated, and verifiable mathematical dataset for advancing reasoning, 2025. 8 [50] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: graduate-level google-proof qa benchmark, 2023. 8 [51] Team, Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, King Zhu, Minghao Liu, Yiming Liang, Xiaolong Jin, Zhenlin Wei, Chujie Zheng, Kaixin Deng, Shawn Gavin, Shian Jia, Sichao Jiang, Yiyan Liao, Rui Li, Qinrui Li, Sirun Li, Yizhi Li, Yunwen Li, David Ma, Yuansheng Ni, Haoran Que, Qiyao Wang, Zhoufutu Wen, Siwei Wu, Tyshawn Hsing, Ming Xu, Zhenzhu Yang, Zekun Moore Wang, Junting Zhou, Yuelin Bai, Xingyuan Bu, Chenglin Cai, Liang Chen, Yifan Chen, Chengtuo Cheng, Tianhao Cheng, Keyi Ding, Siming Huang, Yun Huang, Yaoru Li, Yizhe Li, Zhaoqun Li, Tianhao Liang, Chengdong Lin, Hongquan Lin, Yinghao Ma, Tianyang Pang, Zhongyuan Peng, Zifan Peng, Qige Qi, Shi Qiu, Xingwei Qu, Shanghaoran Quan, Yizhou Tan, Zili Wang, Chenqing Wang, Hao Wang, Yiya Wang, Yubo Wang, Jiajun Xu, Kexin Yang, Ruibin Yuan, Yuanhao Yue, Tianyang Zhan, Chun Zhang, Jinyang Zhang, Xiyue Zhang, Xingjian Zhang, Yue Zhang, Yongchi Zhao, Xiangyu Zheng, Chenghua Zhong, Yang Gao, Zhoujun Li, Dayiheng Liu, Qian Liu, Tianyu Liu, Shiwen Ni, Junran Peng, Yujia Qin, Wenbo Su, Guoyin Wang, Shi Wang, Jian Yang, Min Yang, Meng Cao, Xiang Yue, Zhaoxiang Zhang, Wangchunshu Zhou, Jiaheng Liu, Qunshu Lin, Wenhao Huang, and Ge Zhang. Supergpqa: Scaling llm evaluation across 285 graduate disciplines, 2025. [52] Yuxian Gu, Qinghao Hu, Shang Yang, Haocheng Xi, Junyu Chen, Song Han, and Han Cai. Jet-nemotron: Efficient language model with post neural architecture search. arXiv preprint arXiv:2508.15884, 2025. 10 [53] Xin Dong, Yonggan Fu, Shizhe Diao, Wonmin Byeon, Zijia Chen, Ameya Sunil Mahabaleshwarkar, Shih-Yang Liu, Matthijs Van Keirsbilck, Min-Hung Chen, Yoshi Suhara, et al. Hymba: hybrid-head architecture for small language models. arXiv preprint arXiv:2411.13676, 2024. 10 [54] Aaron Blakeman, Aarti Basant, Abhinav Khattar, Adithya Renduchintala, Akhiad Bercovich, Aleksander Ficek, Alexis Bjorlin, Ali Taghibakhshi, Amala Sanjay Deshmukh, Ameya Sunil Mahabaleshwarkar, et al. Nemotron-h: family of accurate and efficient hybrid mamba-transformer models. arXiv preprint arXiv:2504.03624, 2025. 10 accelerating [55] NVIDIA. for nvidia gpus. TransformerEngine/, 2025. accessed October 30, 2025. 10 Transformer engine: library on transformer models https://github.com/NVIDIA/ Version v2.8, [56] Wenjie Ma, Jingxuan He, Charlie Snell, Tyler Griggs, Sewon Min, and Matei Zaharia. Reasoning models can be effective without thinking, 2025. 11 [57] Wei Huang, Yi Ge, Shuai Yang, Yicheng Xiao, Huizi Mao, Yujun Lin, Hanrong Ye, Sifei Liu, Ka Chun Cheung, Hongxu Yin, Yao Lu, Xiaojuan Qi, Song Han, and Yukang Chen. Qerl: Beyond efficiency quantization-enhanced reinforcement learning for llms, 2025."
        }
    ],
    "affiliations": [
        "MIT",
        "NVIDIA",
        "Stanford University",
        "UC Berkeley"
    ]
}