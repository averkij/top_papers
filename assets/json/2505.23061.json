{
    "paper_title": "DINGO: Constrained Inference for Diffusion LLMs",
    "authors": [
        "Tarun Suresh",
        "Debangshu Banerjee",
        "Shubham Ugare",
        "Sasa Misailovic",
        "Gagandeep Singh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion LLMs have emerged as a promising alternative to conventional autoregressive LLMs, offering significant potential for improved runtime efficiency. However, existing diffusion models lack the ability to provably enforce user-specified formal constraints, such as regular expressions, which makes them unreliable for tasks that require structured outputs, such as fixed-schema JSON generation. Unlike autoregressive models that generate tokens sequentially, diffusion LLMs predict a block of tokens in parallel. This parallelism makes traditional constrained decoding algorithms, which are designed for sequential token prediction, ineffective at preserving the true output distribution. To address this limitation, we propose DINGO, a dynamic programming-based constrained decoding strategy that is both efficient and provably distribution-preserving. DINGO enables sampling of output strings with the highest probability under the model's predicted distribution, while strictly satisfying any user-specified regular expression. On standard symbolic math and JSON generation benchmarks, DINGO achieves up to a 68 percentage point improvement over unconstrained inference"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 1 6 0 3 2 . 5 0 5 2 : r DINGO: Constrained Inference for Diffusion LLMs Tarun Suresh, Debangshu Banerjee, Shubham Ugare, Sasa Misailovic, Gagandeep Singh Department of Computer Science, University of Illinois Urbana-Champaign"
        },
        {
            "title": "Abstract",
            "content": "Diffusion LLMs have emerged as promising alternative to conventional autoregressive LLMs, offering substantial potential for improving runtime efficiency. However, existing diffusion models fail to provably enforce user-specified formal constraints, such as regular expressions, which makes them unreliable for tasks that require structured outputs, such as fixed-schema JSON generation. Unlike autoregressive models, which generate tokens sequentially, diffusion LLMs predict block of tokens in parallel. This parallelism makes traditional constrained decoding algorithms, designed to enforce constraints with sequential token prediction, ineffective at preserving the true output distribution. To address this limitation, we propose DINGO, dynamic programming-based constrained decoding strategy that is both efficient and provably distribution-preserving. DINGO enables sampling of output strings with the highest probability under the models predicted distribution while strictly adhering to any user-specified regular expression. On standard symbolic math and JSON generation benchmarks, DINGO achieves up to 68% points of improvement over unconstrained inference."
        },
        {
            "title": "Introduction",
            "content": "Autoregressive LLMs demonstrate impressive performance across wide range of tasks, including logical reasoning [Pan et al., 2023], theorem proving [Yang et al., 2023], and code generation [et. al., 2021]. However, because they generate one token at time, they can be slow when producing long responses. Recent work has explored using diffusion models to accelerate token generation by predicting blocks of tokens in parallel. For tasks such as logical reasoning, where the LLM output is fed into symbolic solvers like Z3 [Fedoseev et al., 2024], syntactic correctness of the output is essential. Prior works [Poesia et al., 2022, Ugare et al., 2024a, Loula et al., 2025] have shown that LLMs frequently make syntactic and semantic errors, often generating structurally invalid outputs that cause downstream tasks to fail due to unparsable input. To mitigate this issue, constrained decoding has emerged as promising approach that provably ensures structural correctness by projecting the LLM output onto set of valid strings, typically defined by regular grammar or, more generally, context-free grammar (CFG). However, existing constrained decoding techniques are designed specifically for autoregressive LLMs and rely on their step-by-step generation process to prune invalid tokens that cannot lead to structurally valid outputs. At each generation step, the decoder selects the highest-probability token from the set of valid options, based on the LLMs output distribution. In contrast, diffusion LLMs predict blocks of tokens in parallel without sequential dependencies, making existing constrained decoding algorithms incompatible. Furthermore, greedy token selection in autoregressive models maximizes the probability locally at each step but can be suboptimal over an entire sequence, potentially leading to structurally valid yet lower-quality outputs that fail to maximize the overall probability of valid strings. Lew et al. [2023], Park et al. [2024b] have reported this distortion in output distribution for autoregressive LLMs under constrained decoding. Therefore, Equal contributing authors ordered randomly Preprint. Under review. any constrained decoding algorithm for diffusion LLMs should also ensure that enforcing formal constraints does not come at the cost of distorting the true output distribution. Key Challenges: Diffusion LLMs generate block of tokens starting from fully masked string composed of special mask tokens , and iteratively unmask one or more tokens at each step until producing fully unmasked output. Each unmasking step (referred to as diffusion step) can unmask tokens at arbitrary positions in the block, with no left-to-right sequential dependency across steps. As result, designing constrained decoding for diffusion LLMs requires addressing the following: RQ1: Efficiently detecting invalid tokens and restricting token choices at each diffusion step to ensure the final unmasked string is always structurally correct. RQ2: Ensuring the generated token block maximizes the probability under the output distribution. Contributions: We present the first constrained decoding algorithm for diffusion LLMs, making the following contributions: We introduce DINGO, the first constrained decoding algorithm for diffusion LLMs that supports any user-specified regular expression. DINGO provably ensures that the output string is always valid prefix of some string in the target regular language. DINGO uses dynamic programming to ensure that the output string achieves the maximum probability among all valid strings over the output block with respect to the true output distribution. This approach guarantees scalability while maintaining optimality (e.g., maximizing the probability), in contrast to existing methods such as [Park et al., 2024b], which rely on repeated resampling. Resampling-based methods are computationally expensive and unsuitable for practical deployment. Extensive experiments on multiple open-source diffusion LLMs and benchmarks show that DINGO significantly outperforms standard unconstrained decoding, achieving up to 68% improvement on challenging tasks such as the GSM-symbolic benchmark for symbolic reasoning [Mirzadeh et al., 2024] and JSON generation benchmark [NousResearch, 2024]. Roadmap: We provide the necessary background in Section 2, formalize constrained decoding for diffusion LLMs in Section 3, describe the DINGO algorithm along with its correctness and optimality proofs in Section 4, and present experimental results in Section 5."
        },
        {
            "title": "2 Background",
            "content": "Notation: : In the rest of the paper, we use small case letters for constants, bold small case letters (xxx) for strings, capital letters for functions, for string concatenation, xxx to denote the length of xxx. Diffusion LLM: The diffusion LLM Lm,n : n processes finite strings ppp over finite alphabet including the special mask symbol and produces the output string ooo n. Typically ooo = ppp rrr with length represents the entire output string of where ppp is the input prompt, rrr is the response, and + rrr = n. can compute the response rrr over single block Austin et al. [2021], Ye et al. [2025], Nie et al. [2025] in pure diffusion setup or over multiple blocks i.e. r1r1r1 r2r2r2 rkrkrk in semi-autoregressive setup where different blocks are computed sequentially from left to right [Han et al., 2023, Arriola et al., 2025]. At high level, to compute block of tokens of size d, pads the prompt ppp with fully masked suffix, resulting in ppp d, where denotes sequence of special mask tokens . The model then iteratively unmasks subset of these tokens at each step, ultimately producing fully unmasked output string ooo. Each such step is referred to as diffusion step, and typically applies diffusion steps to compute ooo. The number of steps is usually fixed, predetermined constant satisfying < d, which enables greater scalability compared to their autoregressive counterparts. Definition 2.1 (Diffusion step). diffusion step fn : n applies single unmasking step to masked (or, partially masked) string of length to compute new masked (or, possibly unmasked) string of the same length. The first argument represents the input string appended with the output block while the second argument dictates the number of masked tokens in the output string. Each diffusion step fn consists of two components: transformer step Nn : RV , which predicts the token probability distribution at each output position, and mask prediction step + 2 + + RV Mn : RV , which determines which token positions to remask. Typically, for each position, the mask prediction step identifies the token with the highest probability and compares these maximum probabilities across positions. Mn then greedily remasks positions with relatively lower max-probability scores [Nie et al., 2025] and produces the modified token distribution. Further details about Nn and Mn are in Appendix A. Formally, the diffusion step is defined as fn(xxxi1, i) = Dm,n(Mn(Nn(xxxi1), i)) where Dm,n : RV n is the decoder. We now use the diffusion step to formally define the diffusion LLM + for generating strings of length in either single-block or multi-block setting. Definition 2.2 (Single block diffusion LLM). diffusion LLM that outputs block of tokens given an input ppp using diffusion steps is function Lm,n : n, where = + d, and the output is ooo = ppp rrr = Lm,n(ppp). Let fn : n denote single diffusion step, and let Pm,n : n be the padding function. Then the output is computed as ooo = Lm,n(ppp) = xxxT , where: xxx0 = Pm,n(ppp) = ppp and xxxi = fn(xxxi1, i) for 1 . Definition 2.3 (Semi Autoregressive diffusion LLM). In the semi-autoregressive setup, given an input ppp m, the output ooo m+dk is generated over blocks, where each block is computed via call to the single block diffusion model. The output of the i-th diffusion model call is xxxi = Lmi,ni(xxxi1), with xxx0 = ppp and the final output ooo = xxxk. The input and output lengths for each block are defined as mi = + (i 1) and ni = + for all 1 k. DFA and regular expression: We provide necessary definitions regarding regular expression. Definition 2.4. (DFA) DFA DR = (Q, Σ, δ, q0, ) for regular expression is finite-state machine that deterministically processes input strings to decide membership in the language L(R) Σ defined by R. It consists of states Q, start state q0, set of accepting states , and transition rules δ : Σ and the input alphabet Σ. Definition 2.5 (extended transition function). The extended transition function δ : Σ maps an input (www, q) to the resulting state qr, obtained by sequentially applying δ to each character ci in www = c1 cwww, starting from state q. Definition 2.6 (Live DFA states). Given DFA (Q, Σ, δ, q0, ), let Ql represent the set of live states such that Ql iff Σ s.t. δ(www, q) ."
        },
        {
            "title": "3 Optimal Constrained Decoding",
            "content": "We formalize the correctness and optimality of constrained decoding for any diffusion LLM with respect to user-defined regular expression R. Given R, let L(R) Σ (V ) denote the set of all finite strings that satisfy the expression R. Correctness: valid constrained decoding algorithm must ensure that the output string always remains valid prefix of some string in L(R), effectively eliminating any output that cannot be extended into valid completions. By treating the output string as prefix rather than fully completed string, we can accommodate the semi-autoregressive setup, where blocks of tokens are appended to the right of the current output. This approach avoids prematurely rejecting strings that may lead to valid completions in subsequent blocks and also aligns with the notion of correctness adopted in existing constrained decoding algorithms for the autoregressive LLM [Ugare et al., 2024b, Banerjee et al., 2025]. We denote the set of all valid prefixes of L(R) as LP (R). Each diffusion step fn produces string over the vocabulary , which may include one or more special mask tokens . These tokens act as placeholders for actual (non-mask) tokens that will be filled in during future diffusion steps. To account for these future substitutions, we define masked (or partially masked) string as valid if there exists replacement for all mask tokens such that the resulting fully unmasked string is valid prefix of some string in L(R). To formalize this notion, we first define the substitution set, which represents the set of fully unmasked strings obtained by replacing all mask tokens in masked or partially masked string. We then use substitution sets to define the correctness of the constrained decoder. Definition 3.1 (Substitution Set). Given masked (or, partially masked) string xxx n, the substitution set S(xxx) (V {})n is the set of all fully unmasked strings obtained by replacing each occurrence of in xxx with token from {}. For unmasked strings with no , S(xxx) = {xxx} 3 Definition 3.2 (Correctness of Constrained decoder). Any deterministic decoder Dm,n,R : RV n is valid constrained decoder if, for all N, input prompt ppp and for any output + distribution Dn provided as probability vectors each of size , there exists an unmasked string xxx in the substitution set S(Dm,n,R(Dn)) of the decoded output such that actual response ppp rrr = xxx is valid prefix i.e., rrr LP (R). 2 Optimality: Given distribution Dn and regular expression R, the set of decodings that are valid prefixes for (as defined in Definition 3.2) may not be unique. An optimal constrained decoder selects, among all valid strings, the string that maximizes the probability under Dn. The output distribution Dn is represented as vectors vvv1, . . . , vvvn, each of size , where the i-th vector vvvi captures the token distribution at position i. For any masked position j, vvvj assigns probability 1 to the mask token and 0 to all other tokens. Assuming the input prompt has length m, the token distribution of the actual response is given by vvvm+1, . . . , vvvn. For any output string rrr = tm+1 tn, let (rrr vvvm+1 . . . vvvn) denote the probability of the string rrr under the output distribution. Then, the optimal constrained decoding can be formalized as follows: rrr = arg max rrr (rrr vvvm+1 . . . vvvn) s.t. xxx .(xxx S(rrr)) (xxx LP (R)) (1) Since the token distributions vvvm+1, . . . , vvvn are independent across positions, the probability of the string rrr can be written as (rrr vvvm+1 . . . vvvn) = (cid:81)n i=m+1 vvvi[ti] where vvvi[ti] denotes the probability assigned to token ti by the vector vvvi. Using this, we can rewrite the optimization problem from Eq. 1 as follows: rrr = arg max rrr=tm+1tn (cid:89) i=m+"
        },
        {
            "title": "4 DINGO Algorithm",
            "content": "vvvi[ti] s.t. xxx .(xxx S(rrr)) (xxx LP (R)) (2) The search space for Eq. 2 is exponential d, where = denotes the block length, making naive enumeration-based methods impractical. To efficiently retrieve the optimal output string rrr from Eq. 2, DINGO leverages dynamic programming. Given regular expression R, it first modifies the transition function to handle the mask symbol , which is then utilized during inference. 4.1 Precomputation For user-provided and the corresponding DFA DR = (Q, Σ, δR, q0, ) (referred to as characterlevel DFA) with Σ (V ), we first construct the token-level DFA Dt = (Q, (V ), δt, q0, ) recognizing L(R) over strings generated by L. single token ttt (V ) can span across multiple characters in Σ i.e. ttt = c1 cl where ci Σ. To construct the token-level transition function δt : (V ) Q, we process each token ttt (V ) and state by executing the character-level DFA DR on the sequence of constituent characters c1 cl, starting from state q, and record the resulting state qr. We then define the token-level transition as δt(q, ttt) = qr. To handle the special mask token , we define the transition function δ : 2Q. For each state Q, δ(q) returns the set of states Qr that are reachable via single token transition using δt. Formally, δ(q) = {q = δt(q, ttt); ttt (V )}. Since δ may return multiple states, it resembles the transition function of non-deterministic finite automaton (NFA). The precomputation step combines δt and δ to define δ : 2Q, which is used in the dynamic programming step. Using the token-level DFA Dt, we also construct the set of live states Ql (Definition 2.6). δ(q, ttt) = (cid:26){δt(q, t)} δ(q) if (V ), if = . 4.2 DINGO Dynamic Programming Before going into details, we present two key observations that lead to the decoding algorithm. 2More precisely, if there exists at least one rrr that is valid prefix (i.e., rrr LP (R)), then constrained decoding is always capable of retrieving one of them. 4 Observation 1: Determining whether fully unmasked string rrr = t1 td (V ) is valid prefix is equivalent to checking whether the resulting state qr, obtained by applying δ to the sequence t1 td starting from q0, is live. Similarly, for partially (or fully) masked string rrr, applying δ to t1 td yields set of resulting states Qr. In this case, rrr is valid prefix if and only if any state Qr is live (Definition 3.2). Observation 2: For optimality, it is sufficient to track the maximum probability path from the start state q0 to each resulting state in Qr. Once these paths are computed, we select the one with the highest probability that leads to live state. The corresponding string is the optimal string rrr (or one of the optimal strings in case of multiple optimal solutions) for the optimization problem in Eq. 2. Based on these observations, the main challenge is to efficiently maintain the maximum probability path to each reachable state in Qr. We address this using dynamic programming (DP) approach, similar to traditional graph-based DP algorithms such as [Forney, 1973]. DP states: For each token position 1 in the block, the DP maintains: a) [i, q], which records the maximum probability with which state can be reached from the start state q0 via transitions on some token sequence with length i; and b) r[q, i], which stores the last transition, i.e., the previous state and the corresponding token, that led to the maximum probability stored in [i, q]. If state is unreachable, then [i, q] = 0. Formally, given the probability vectors vvv1, . . . , vvvi, [i, q] is defined as follows where δ is extended transition function (Definition 2.5). [i, q] = max t1...ti (cid:89) j=1 vvvj[tj] s.t. = δ (tm+1 tn, q0) DP state update: Given the states at token position i, we describe the computation for position + 1. Initially, [i, q] = 0 for all = q0, and [i, q0] = 1 (lines 1 3 in Algo. 1). To compute [i + 1, q] for each Q, we consider all tokens (including the mask token ) that can transition to from some previous state at step i. Among all such transitions, we select the one with the highest probability and add it to the maximum probability path reaching at step i. The value r[i + 1, q] stores the previous state and token that lead to the maximum probability path to at step + 1 (lines 12 15 in Algo. 1). Formally, vvvi+1(t) s.t. δ(q, t) (cid:40)max Vi+1(q, q) = tV 0 if q, are not connected [i + 1, q] = max qQ [i, q] Vi+1(q, q) Path construction: We consider all reachable states at the end of the block with [d, q] > 0. Among the live states ql Ql satisfying this condition, we select the state qmax with the highest value of [d, ql]. We then use to iteratively reconstruct the token sequence backward that forms the maximum probability path starting from qmax and ending at q0 (lines 20 22 in Algo. 1). Semi-autoregressive setup: In semi-autoregressive setup, we may not start from DFA start state q0 since one or more blocks of tokens rrr1 rrrl may have been generated in left the current block. Provide the string rrr1 rrrl ends at live state ql, we can apply dynamic programming approach with the intializtion [0, ql] = 1 and [0, q] = 0 for all state = ql. Details are in Appendix D. 4.3 Correctness of DINGO Proposition 4.1. [Correctness] Given any regular expression R, input prompt ppp m, block length d, output distribution Dm+d = vvv1 . . . vvvm+d, if LP (R) (V )d = {} and rrr vvvm+1 . . . vvvm+d be the decoded string, then xxx .(xxx S(rrr)) (xxx LP (R)) holds. Proof sketch: DINGO ensures that if state is reachable in tokens, then [i, q] > 0 for all 1 d. Since LP (R) (V )d = {}, there exists state ql Ql that is reachable in steps. Therefore, [d, qmax] > 0 (see line 16 in Alg.1). Consequently, there exists sequence xxx S(rrr) such that δ(xxx, q0) = qmax Ql, implying that xxx LP (R). Formal proof is in AppendixB. Proposition 4.2. [Optimality] Given any regular expression R, input prompt ppp m, block length d, output distribution Dm+d = vvv1 . . . vvvm+d, if LP (R) (V )d = {} and rrr vvvm+1 . . . vvvm+d be the decoded string, then for any valid string rrr satisfying xxx .(xxx S(rrr)) (xxx LP (R)), (rrr vvvm+1 . . . vvvn) (rrr vvvm+1 . . . vvvn). Proof Sketch: Formal proof is in Appendix B. Initialization of the DP maximum token probability transtion (q q) at position token for the maximum probability transition (q q) for do for (q Q) do δ(q, t) Vi(q, q), Ti(q, q) MaxTransition(vvvi, t, q, q) Algorithm 1 DINGO DP Require: q0, block length d, probability vectors vvv1, . . . vvvd for the current block, Ql, Q, δ. 1: [0, q] 0 for all (q Q) (q = q0) 2: [0, q0] 1 3: r[0, q] (None, None) for all (q Q) 4: Vi {} for all {1, . . . , d} 5: Ti {} for all {1, . . . , d} 6: for {1, . . . , d} do 7: 8: 9: 10: 11: for {1, . . . , d} do 12: 13: 14: 15: 16: qmax arg maxqQl 17: if [d, qmax] = 0 then return None, qmax 18: 19: rrr {}, qcurr qmax 20: for {d, . . . , 1} do qcurr, r[i, qcurr] 21: rrr rrr 22: 23: return reverse(rrr), qmax if [i, q] < [i 1, q] Vi(q, q) then [i, q] [i 1, q] Vi(q, q) r[i, q] (q, Ti(q, q)) for (q Q) (q Q) do [d, q] DP computation loop Update maximum probability path to Update the parents accordingly No valid prefixes Decoding the optimal string rrr 4.4 DINGO algorithm Algorithm 1 presents DINGO steps. The two main loops dominating its computational complexity involve calculating transition costs and performing the DP updates respectively. First, for each of the time steps, the algorithm computes the optimal single-token transition costs Vi(qs, qt) between all source states qs and target states qt Q. This is achieved by iterating through each source state qs, each token , and then for each state qt reached from qs via (i.e., qt δ(qs, t)), updating the cost Vi(qs, qt) with vvvi[t] if it is better. The complexity for this part is O(d (Q2 + (cid:80) δ(qs, t) represents the total number of transitions, Ntrans = O(Q + N), where is the maximum number of states reachable via the token. Thus, this part takes O(d (Q2 + V )). tV δ(qs, t))). The sum (cid:80) qsQ (cid:80) (cid:80) qs Second, the core dynamic programming update calculates [i, q] for each diffusion step and state q. This involves iterating over diffusion steps, current states q, and for each q, considering all possible previous states q. This leads to complexity of O(d Q2). Combining these dominant parts, the total complexity is O(d (Q2 + V ) + Q2), which simplifies to O(d (Q2 + V )). This can be expressed as O(d (Q + ))."
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we evaluate DINGO on math reasoning task (GSM-Symbolic Mirzadeh et al. [2024]) and schema-based text-to-JSON task (JSONModeEval [NousResearch, 2024]) and demonstrate significant improvement over baselines. In both tasks, we use the LLaDA-8B-Base (LLaDA-8BB) Nie et al. [2025], LLaDA-8B-Instruct (LLaDA-8B-I) Nie et al. [2025], Dream-v0-Base-7B (Dream-B-7B) Ye et al. [2025], and Dream-v0-Instruct-7B (Dream-I-7B) Ye et al. [2025] models. Experimental Setup. We run experiments on 48-core Intel Xeon Silver 4214R CPU with 2 Nvidia RTX A5000 GPUs. DINGO is implemented using PyTorch Paszke et al. [2019] and the HuggingFace transformers library Wolf et al. [2020]. The token-level DFA is implemented in Rust using highly efficient regex-DFA library to minimize overhead during DFA construction and LLM inference. We report the mean number of DFA states and transitions as well as the offline pre-computation time in Appendix E. 6 Baselines. We compare DINGO against unconstrained diffusion LLM generation. Furthermore, to highlight the benefit of optimal constrained decoding with DINGO, we implement constrained decoding strategy Greedy Constrained that mirrors existing autoregressive constrained generation methods Willard and Louf [2023], Ugare et al. [2024b]. Greedy Constrained iterates over the diffusion block and at each position computes binary mask {0, 1}V based on the DFA, specifying valid tokens (m = 1) and excluded tokens (m = 0). Decoding is then performed on the masked probability distribution vivivi, where denotes element-wise multiplication. Since in some cases, Unconstrained outperforms Greedy Constrained, we also report Best of Greedy + Unconstrained , which takes the better result of the two approaches for each problem in the dataset. Math Reasoning: We evaluate DINGO on GSM-Symbolic Mirzadeh et al. [2024] dataset, which consists of reasoning-based math world problems where numerical values and names are replaced by symbolic variables. Diffusion LLMs are tasked with generating correct symbolic expression solutions to those word problems. We evaluate correctness by using the Z3 solver [De Moura and Bjørner, 2008] to check if the final expressions from the LLM generations are functionally equivalent to the ground truth expressions. We set the generation length to 128, number of blocks to 8, and total diffusion steps to 64 and prompt the LLMs with 4-shot examples from GSM-Symbolic [Mirzadeh et al., 2024] (the prompts can be found in Appendix F.1). We initialize DINGO and Greedy Constrained with regex (shown in Appendix F.2) that permits math expressions wrapped in and and natural language text outside these expressions for reasoning as done in CRANE Banerjee et al. [2025]. Table 1 compares the performance of DINGO with the baseline methods. The Accuracy (%) column reports the percentage of functionally correct LLM-generated expressions, Parse (%) indicates the percentage of syntactically valid responses (i.e., expressions without invalid operations), and Time provides the average time in seconds taken to generate completion. As displayed in the table, DINGO significantly improves functional correctness over the baselines. For instance, for LLaDA-8B-I, DINGO outperforms unconstrained generation by 13 percentage points and Greedy Constrained generation by 5 percentage points. Furthermore, DINGO achieves 100% syntactic accuracy across all models evaluated. On the other hand, unconstrained and Greedy Constrained generation make many syntactic errors, especially for non-instruct tuned models. For these cases, generation with Greedy Constrained results in responses that are syntactically valid prefixes but not syntactically valid by themselves. We present case studies in Appendix F.3. Importantly, DINGO is extremely efficient, introducing marginal overhead compared to unconstrained generation. Table 1: Comparison of constrained and unconstrained generation methods on GSM-Symbolic. Model Method Acc. (%) Parse (%) Time (s) Unconstrained Greedy Constrained LLaDA-8B-B Best of Greedy + Unconstrained DINGO Unconstrained Greedy Constrained Best of Greedy + Unconstrained DINGO Unconstrained Greedy Constrained Best of Greedy + Unconstrained DINGO Unconstrained Greedy Constrained Best of Greedy + Unconstrained DINGO LLaDA-8B-I Dream-B-7B Dream-I-7B 25 30 30 31 19 27 27 32 17 21 21 23 32 34 34 54 75 75 100 35 98 98 100 33 41 41 100 61 93 93 100 9.06 9.31 9.08 9.22 23.78 23.97 23.8 23. 16.02 16.13 16.04 16.19 23.89 24.01 23.9 23.91 JSON Generation: We further evaluate DINGO on text-to-JSON generation task JSON-ModeEval, which conists of zero-shot problems specifying JSON schema and request to generate 7 JSON object that contains specified contents. Generating JSON that adheres to specified schema is extremely important for applications like tool use and function calling Ugare et al. [2024b], Willard and Louf [2023]. We evaluate the correctness of JSON generated by an LLM by first evaluating whether the JSON string can be parsed and converted to valid JSON object. We further evaluate whether the generated JSON is valid against the schema specified in the prompt. We set the generation length to 128, number of blocks to 1, and the total diffusion steps to 64. For the constrained generation methods, we convert each problems JSON schema into its corresponding regular expression and guide the diffusion LLM to generate output conforming to that regex. Table 2 presents the results of our experiment. The Parse (%) column reports the percentage of syntactically valid LLM generations while the Accuracy (%) column reports the percentage of generations that are both syntactically valid and valid against their respective schemas. Notably, DINGO achieves 100% schema validation and syntactic accuracy, while baseline methods struggle in many cases to generate valid JSON. We attribute this to the fact that Greedy Constrained may distort the distribution through its greedy approximation and can only generate valid prefix, not fulll parsable generation Park et al. [2024a]. Table 2: Comparison of constrained and unconstrained generation methods for JSON Schema. Model Method Acc. (%) Parse (%) Time (s) Unconstrained Greedy Constrained LLaDA-8B-B Best of Greedy + Unconstrained DINGO Unconstrained Greedy Constrained Best of Greedy + Unconstrained DINGO Unconstrained Greedy Constrained Best of Greedy + Unconstrained DINGO Unconstrained Greedy Constrained Best of Greedy + Unconstrained DINGO LLaDA-8B-I Dream-B-7B Dream-I-7B 57 80 88 100 87 78 99 100 15 23 32 100 85 30 91 100 59 80 90 91 79 99 100 18 23 35 100 87 30 93 100 6.37 6.47 6.41 6.43 6.7 6.81 6.73 6.78 5.31 5.41 5.34 5. 6.4 6.51 6.43 6.55 Ablation Study on The Number of Diffusion Blocks: We analyze the performance of DINGO on GSM-Symbolic using different numbers of diffusion blocks. We run generation with response length of 128, using 64 total diffusion steps, and each of 1, 2, and 8 blocks. As shown in Figure 1, DINGO performs well across all block settings, outperforming baselines in both functional and syntactic correctness. Further ablations on the number of diffusion blocks are presented in Appendix I. (a) LLaDA-8B-I (b) Dream-I-7B Figure 1: Ablation Study on The Number of Diffusion Blocks For GSM-Symbolic"
        },
        {
            "title": "6 Related Works",
            "content": "To the best of our knowledge, our work is the first to provide provable guarantees on constrained adherence for inference in diffusion language models. We next discuss the broader set of related works on diffusion language models and constrained language model decoding. Diffusion Language Models: Diffusion Language Models Austin et al. [2021] have emerged as promising alternative to traditional autoregressive architectures Radford et al. [2019], offering advantages in parallel processing and controllability while addressing limitations in sequential generation. Recent advances in semi-autoregressive diffusion models Han et al. [2023], Nie et al. [2025], Ye et al. [2025], Arriola et al. [2025] have significantly narrowed the performance gap with autoregressive counterparts. SSD-LM [Han et al., 2023] introduced semi-autoregressive approach that performs diffusion over the natural vocabulary space, enabling flexible output length and improved controllability by iteratively generating blocks of text while facilitating local bidirectional context updates. More recently, several breakthrough models have advanced the field: LLaDA (Large Language Diffusion with mAsking) achieved competitive performance with SOTA open-source autoregressive models of similar size like LLaMA3-8B through forward data masking process and reverse process, parameterized by vanilla Transformer to predict masked tokens [Nie et al., 2025]. BD3-LMs (Block Discrete Denoising Diffusion Language Models)Arriola et al. [2025] introduced novel approach that interpolates between discrete denoising diffusion and autoregressive models while supporting flexible-length generation and improving inference efficiency with KV caching. Most recently, Dream-7BYe et al. [2025] emerged as strong open diffusion large language model that matches state-of-the-art autoregressive (AR) language models of similar size. Constrained Decoding with Autoregressive LLMs: Constrained decoding has shown promising results in augmenting autoregressive language models. Researchers have developed efficient techniques for ensuring syntactic correctness in regular [Deutsch et al., 2019, Willard and Louf, 2023, Kuchnik et al., 2023] or context-free [Koo et al., 2024, Ugare et al., 2024a, Dong et al., 2024, Banerjee et al., 2025] languages. Other works have focused on semantically constrained decoding through Monte Carlo sampling [Lew et al., 2023, Loula et al., 2025] or backtracking [Poesia et al., 2022, Ugare et al., 2025]. Lew et al. [2023], Park et al. [2024a] demonstrated that all these approaches that perform greedy constrained approximation for inference can distort the sampling distribution. DINGO addresses this challenge by performing optimal constrained sampling on blocks of tokens in diffusion language model, which partially mitigates distribution distortion issues. Concurrent to our work, Cardei et al. [2025] performs constrained sampling from diffusion language models by minimizing loss function defined using surrogate model used for scoring constraints. However, their proposed method does not guarantee convergence to the constraint and necessitates differentiable surrogate model. In contrast, our work focuses on providing provable guarantees for constraint satisfaction during inference without the need of an additional surrogate model. Limitations DINGO is optimal for per-block generation, making it ideal for pure diffusion settings. However, this optimality may not hold in semi-autoregressive setups involving multiple blocks. Currently, our approach is limited to regular language constraints, while programming languages often belong to context-free or context-sensitive classes. As result, our method cannot directly enforce these more expressive constraints, which have been addressed in prior work on autoregressive constrained generation. Nonetheless, we believe the core dynamic programming framework behind DINGO can be extended to support richer language classes in future work. Moreover, important constraints like toxicity mitigation fall outside formal language classes, highlighting directions for further research."
        },
        {
            "title": "7 Conclusion",
            "content": "We presented DINGO, novel dynamic programming approach that enables diffusion LLMs to generate outputs that strictly adhere to regular language constraints while preserving the models underlying distribution. Our method overcomes the limitations of traditional constrained decoding algorithms that fail with parallel token prediction. Our experimental results on symbolic math and JSON generation tasks demonstrate significant improvements over unconstrained inference, demonstrates that DINGO is an effective solution for structured output generation with diffusion models. Our work bridges an important gap in making diffusion LLMs reliable for applications requiring formal guarantees."
        },
        {
            "title": "References",
            "content": "Marianne Arriola, Subham Sekhar Sahoo, Aaron Gokaslan, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Justin Chiu, and Volodymyr Kuleshov. Block diffusion: Interpolating between autoregressive and diffusion language models. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=tyEyYT267x. Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=h7-XixPCAL. Debangshu Banerjee, Tarun Suresh, Shubham Ugare, Sasa Misailovic, and Gagandeep Singh. CRANE: Reasoning with constrained LLM generation. arXiv preprint arXiv:2502.09061, 2025. URL https://arxiv.org/pdf/2502.09061. Michael Cardei, Jacob Christopher, Thomas Hartvigsen, Brian R. Bartoldson, Bhavya Kailkhura, and Ferdinando Fioretto. Constrained language generation with discrete diffusion models, 2025. URL https://arxiv.org/abs/2503.09790. Leonardo De Moura and Nikolaj Bjørner. Z3: an efficient smt solver. In Proceedings of the Theory and Practice of Software, 14th International Conference on Tools and Algorithms for the Construction and Analysis of Systems, TACAS08/ETAPS08, page 337340, Berlin, Heidelberg, 2008. Springer-Verlag. ISBN 3540787992. Daniel Deutsch, Shyam Upadhyay, and Dan Roth. general-purpose algorithm for constrained sequential inference. In Proceedings of the Conference on Computational Natural Language Learning, 2019. URL https://aclanthology.org/K19-1045/. Yixin Dong, Charlie Ruan, Yaxing Cai, Ruihang Lai, Ziyi Xu, Yilong Zhao, and Tianqi Chen. XGrammar: Flexible and efficient structured generation engine for large language models. arXiv preprint arXiv:2411.15100, 2024. URL https://arxiv.org/pdf/2411.15100. Chen et. al. Evaluating large language models trained on code, 2021. URL https://arxiv.org/ abs/2107.03374. Timofey Fedoseev, Dimitar Iliev Dimitrov, Timon Gehr, and Martin Vechev. LLM training data synthesis for more effective problem solving using satisfiability modulo theories. In The 4th Workshop on Mathematical Reasoning and AI at NeurIPS24, 2024. URL https://openreview. net/forum?id=hR4Hskr4GX. G. D. Forney. The viterbi algorithm. Proc. of the IEEE, 61:268 278, March 1973. Xiaochuang Han, Sachin Kumar, and Yulia Tsvetkov. Ssd-lm: Semi-autoregressive simplex-based diffusion language model for text generation and modular control, 2023. URL https://arxiv. org/abs/2210.17432. Terry Koo, Frederick Liu, and Luheng He. Automata-based constraints for language model decoding. In Conference on Language Modeling, 2024. URL https://openreview.net/forum?id= BDBdblmyzY. Michael Kuchnik, Virginia Smith, guage models with RELM. 2023. 93c7d9da61ccb2a60ac047e92787c3ef-Paper-mlsys2023.pdf. Validating large lanProceedings of Machine Learning and Systems, 5, URL https://proceedings.mlsys.org/paper_files/paper/2023/file/ and George Amvrosiadis. Alexander Lew, Tan Zhi-Xuan, Gabriel Grand, and Vikash Mansinghka. Sequential Monte Carlo steering of large language models using probabilistic programs. In ICML 2023 Workshop: Sampling and Optimization in Discrete Space, 2023. URL https://openreview.net/pdf?id= Ul2K0qXxXy. 10 João Loula, Benjamin LeBrun, Li Du, Ben Lipkin, Clemente Pasti, Gabriel Grand, Tianyu Liu, Yahya Emara, Marjorie Freedman, Jason Eisner, Ryan Cotterell, Vikash Mansinghka, Alex Lew, Tim Vieira, and Tim ODonnell. Syntactic and semantic control of large language models via sequential Monte Carlo. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/pdf?id=xoXn62FzD0. Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models, 2024. URL https://arxiv.org/abs/2410.05229. Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models, 2025. URL https://arxiv. org/abs/2502.09992. NousResearch. json-mode-eval, 2024. URL https://huggingface.co/datasets/ NousResearch/json-mode-eval. Liangming Pan, Alon Albalak, Xinyi Wang, and William Yang Wang. Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning, 2023. URL https: //arxiv.org/abs/2305.12295. Kanghee Park, Jiayu Wang, Taylor Berg-Kirkpatrick, Nadia Polikarpova, and Loris DAntoni. Grammar-aligned decoding. Advances in Neural Information Processing Systems, 37:24547 24568, 2024a. URL https://proceedings.neurips.cc/paper_files/paper/2024/ file/2bdc2267c3d7d01523e2e17ac0a754f3-Paper-Conference.pdf. Kanghee Park, Jiayu Wang, Taylor Berg-Kirkpatrick, Nadia Polikarpova, and Loris DAntoni. Grammar-aligned decoding. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024b. URL https://openreview.net/forum?id=5G7ve8E1Lu. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32. 2019. Gabriel Poesia, Alex Polozov, Vu Le, Ashish Tiwari, Gustavo Soares, Christopher Meek, and Sumit Gulwani. Synchromesh: Reliable code generation from pre-trained language models. In International Conference on Learning Representations, 2022. URL https://openreview.net/ forum?id=KmtVD97J43e. Alec Radford, Sutskever. URL unsupervised_multitask_learners.pdf. Accessed: 2024-11-15. and Ilya OpenAI, 2019. https://cdn.openai.com/better-language-models/language_models_are_ Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Language models are unsupervised multitask learners. Shubham Ugare, Tarun Suresh, Hangoo Kang, Sasa Misailovic, and Gagandeep Singh. SynCode: Improving LLM code generation with grammar augmentation. arXiv preprint arXiv:2403.01632, 2024a. URL https://arxiv.org/pdf/2403.01632. Shubham Ugare, Tarun Suresh, Hangoo Kang, Sasa Misailovic, and Gagandeep Singh. Syncode: Llm generation with grammar augmentation, 2024b. URL https://arxiv.org/abs/2403.01632. Shubham Ugare, Rohan Gumaste, Tarun Suresh, Gagandeep Singh, and Sasa Misailovic. IterGen: Iterative structured LLM generation. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/pdf?id=ac93gRzxxV. Brandon Willard and Rémi Louf. Efficient guided generation for large language models. arXiv preprint arXiv:2307.09702, 2023. URL https://arxiv.org/pdf/2307.09702. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, 11 Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural language processing. In Conference on Empirical Methods in Natural Language Processing: System Demonstrations, 2020. URL https://aclanthology.org/2020.emnlp-demos.6. Kaiyu Yang, Aidan M. Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan Prenger, and Anima Anandkumar. Leandojo: Theorem proving with retrieval-augmented language models, 2023. URL https://arxiv.org/abs/2306.15626. Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Dream 7b, 2025. URL https://hkunlp.github.io/blog/2025/dream."
        },
        {
            "title": "NeurIPS Paper Checklist",
            "content": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: You should answer [Yes] , [No] , or [NA] . [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available. Please provide short (12 sentence) justification right after your answer (even for NA). The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to question, in the justification please point to the section(s) where related material for the question can be found. IMPORTANT, please: Delete this instruction block, but keep the section heading NeurIPS Paper Checklist\", Keep the checklist subsection headings, questions/answers and guidelines below. Do not modify the questions and only use the provided macros for your answers. 1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the papers contributions and scope? Answer: [Yes] Justification: Yes, the abstract and introductions claims are validated through the proposed approach and experimental results. Guidelines: The answer NA means that the abstract and introduction do not include the claims made in the paper. The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. No or NA answer to this question will not be perceived well by the reviewers. The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] 13 Justification: Limitations are discussed in the Related Works section. Guidelines: The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. The authors are encouraged to create separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on few datasets or with few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach. For example, facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, worse outcome might be that reviewers discover limitations that arent acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. Theory assumptions and proofs Question: For each theoretical result, does the paper provide the full set of assumptions and complete (and correct) proof? Answer: [Yes] Justification: Yes, refer to Section 4. Guidelines: The answer NA means that the paper does not include theoretical results. All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. All assumptions should be clearly stated or referenced in the statement of any theorems. The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide short proof sketch to provide intuition. Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental result reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: All experiment details are in Section 5. Guidelines: The answer NA means that the paper does not include experiments. 14 If the paper includes experiments, No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. If the contribution is dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is novel architecture, describing the architecture fully might suffice, or if the contribution is specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to hosted model (e.g., in the case of large language model), releasing of model checkpoint, or other means that are appropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is new model (e.g., large language model), then there should either be way to access this model for reproducing the results or way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Code is provided. Guidelines: The answer NA means that paper does not include experiments requiring code. Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for new open-source benchmark). The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only subset of experiments are reproducible, they should state which ones are omitted from the script and why. At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). 15 Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental setting/details Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All experiment details are mentioned in Section 5. Guidelines: The answer NA means that the paper does not include experiments. The experimental setting should be presented in the core of the paper to level of detail that is necessary to appreciate the results and make sense of them. The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment statistical significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: All experiments used greedy decoding, which is deterministic. The results will not change between runs. Guidelines: The answer NA means that the paper does not include experiments. The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). The method for calculating the error bars should be explained (closed form formula, call to library function, bootstrap, etc.) The assumptions made should be given (e.g., Normally distributed errors). It should be clear whether the error bar is the standard deviation or the standard error of the mean. It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report 2-sigma error bar than state that they have 96% CI, if the hypothesis of Normality of errors is not verified. For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments compute resources Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Yes, the details are provided in Section 5. Guidelines: The answer NA means that the paper does not include experiments. The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. 16 The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didnt make it into the paper). 9. Code of ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The paper conforms to NeurIPS Code of Ethics Guidelines: The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. If the authors answer No, they should explain the special circumstances that require deviation from the Code of Ethics. The authors should make sure to preserve anonymity (e.g., if there is special consideration due to laws or regulations in their jurisdiction). 10. Broader impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The paper discusses the impact of the work in the Introduction and validate through the experiments. Guidelines: The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how system learns from feedback over time, improving the efficiency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: No such risks. Guidelines: 17 The answer NA means that the paper poses no such risks. Released models that have high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All citations for assets used are given. Guidelines: The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include URL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from particular source (e.g., website), the copyright and terms of service of that source should be provided. If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of dataset. For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. If this information is not available online, the authors are encouraged to reach out to the assets creators. 13. New assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The details about the algorithm and limitations are described in the paper Guidelines: The answer NA means that the paper does not release new assets. Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. The paper should discuss whether and how consent was obtained from people whose asset is used. At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and research with human subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] 18 Justification: No crowdsourcing nor research with human subjects. Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional review board (IRB) approvals or equivalent for research with human subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: No human subject involved. Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 16. Declaration of LLM usage Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research? Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required. Answer: [No] Justification: LLM is not used for important, original, or non-standard component of the research. Guidelines: The answer NA means that the core method development in this research does not involve LLMs as any important, original, or non-standard components. Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM) for what should or should not be described."
        },
        {
            "title": "A Transformer and Remasking Step",
            "content": "Algorithm 2 Diffusion Step Require: Transformer Nn, full output length n, prompt ppp, input prompt length m, block length d, current diffusion step i, total diffusion steps , vocabulary . 1: xxx ppp 2: v1v1v1 . . . vnvnvn Nn(xxx) 3: lll RemaskPositions(vvvm+1, . . . , vvvm+d, i, ) 4: for lll do vjvjvj 000 5: vjvjvj[] 1 Set probability of all tokens except to 0. 6: 7: rrr Dm,n(v1v1v1 . . . vnvnvn) Decoding that outputs response with first tokens are input prompt ppp 8: return rrr Pad the input prompt where = + + output distribution at position Decides which positions to remask vivivi RV We describe the two key components of single diffusion step: a) Transformer step: Computes the output distribution over all tokens in the vocabulary (line 2 Algo. 2). b) Remasking step: Based on the output from the transformer step, it greedily decides which token positions to mask. The remasking step can be viewed as updating the output distribution such that, at the masked positions, the mask token is assigned probability 1, while all other tokens receive probability 0 (lines 3 6 Algo. 2). Popular greedy remasking strategies include (line 3 Algo. 2): (i) Random: Masks tokens at randomly selected positions Nie et al. [2025]. (ii) Top token probability: Masks positions where the top-predicted token has the lowest probability Nie et al. [2025]. (iii) Entropy-based: Computes the entropy of the output distribution at each position and masks the positions with the highest entropy Ye et al. [2025]. The number of token positions to remask at the i-th step typically depends on the total number of diffusion steps and the block length d. At step 0, all positions are masked, and the number of masked tokens decreases linearly to 0 over steps. Thus, at the i-th step, the number of masked tokens is given by (cid:107) . (cid:106) d(T i) T"
        },
        {
            "title": "B Proofs",
            "content": "Proposition 4.1. [Correctness] Given any regular expression R, input prompt ppp m, block length d, output distribution Dm+d = vvv1 . . . vvvm+d, if LP (R) (V )d = {} and rrr vvvm+1 . . . vvvm+d be the decoded string, then xxx .(xxx S(rrr)) (xxx LP (R)) holds. Proof. We assume that xxx LP (R) (V )d (P (xxxvvvm+1 . . . vvvm+d) 0) then the decoded string rrr satisfy the soundness property (see Definition 3.2). In other words, if there is at least one fully unmasked valid prefix with non-zero probability then DINGO retrieves valid string. We show this by induction on the position of tokens. Before moving to the proof, we first define extended transition function δ when δ : 2Q outputs set of states instead of single state due to mask token . In this case, for any string www , δ(www, q0) represents the state of reachable states starting from q0. This can be defined as δ({}, q0) = {q0} and δ(t1 tm+1, q0) = qδ(t1tm,q0)δ(q, tm+1). 1. Let 0 d, and let t1 . . . ti denote any token sequence with positive probability j=1 vvvm+j[tj] > 0. Let δ(t1 ti, q0). Then, [i, q] > 0. We prove this using mass (cid:81)i induction on i. (a) Base case = 0: For empty strings only start state q0 is reachable. DINGO initializes [0, q0] = 1 > 0 and for all = q0, [0, q] = 0. (lines 1 3 in Algo. 1). (b) Inductive Step: At position + 1, let t1 . . . ti+1 i+1 s.t. (cid:81)i+1 j=1 vvvm+j[tj] > 0. Let δ(t1 ti, q0) and δ(q, ti+1). By the inductive hypothesis, for all such 20 [i, q] > 0. Recall, (cid:40)max Vi+1(q, q) = vvvm+i+1(t) s.t. δ(q, t) tV 0 if q, are not connected [i + 1, q] = max qQ [i, q] Vi+1(q, q) Thus, Vi+1(q, q) vvvm+i+1(ti+1) > 0 which implies [i, q] Vi+1(q, q) > 0. Therefore, [i + 1, q] = maxqQ [i, q] Vi+1(q, q) > 0. 2. Since LP (R) (V )d = {} by assumption, there exists some yyy LP (R) (V )d. (yyy, q0) Ql. From the induction above, [d, ql] > 0. [d, q]. Thus, by the definition of arg max, By the Definition 2.6, ql = δ From line 16 in Algo. 1, qmax = arg maxqQl [d, qmax] [d, ql] > 0. 3. In lines 20-22 in Algo. 1), DINGO reconstructs d-length sequence = t1 . . . td such that qmax δ(r, q0). For any tj r, if tj = , choose any token τj (V ) satisfying δt(qj1, τj) = qj where qj = δ (t1 . . . tj, q0). By definition of δ, τj exists. Substituting every in this manner yields, by Definition 3.1, xxx = x1x1x1 . . . xdxdxd (V )d. xxx S(rrr). δ (xxx, q0) = qmax. From above, [d, qmax] > 0. 4. Since qmax Ql, by Definition 2.6, Σ s.t. δ(www, qmax) . Equivalently, xxx L(R), hence xxx LP (R). Proposition 4.2. [Optimality] Given any regular expression R, input prompt ppp m, block length d, output distribution Dm+d = vvv1 . . . vvvm+d, if LP (R) (V )d = {} and rrr vvvm+1 . . . vvvm+d be the decoded string, then for any valid string rrr satisfying xxx .(xxx S(rrr)) (xxx LP (R)), (rrr vvvm+1 . . . vvvn) (rrr vvvm+1 . . . vvvn). Proof. (cid:81)d j=1 vvvm+j[rjrjrj 1. First, we show that (rrr vvvm+1 . . . vvvn) = [d, qmax], or equivalently and 0 d. We prove by induction on that if DINGOs backtracking (lines 19 23 in Algo 1) has brought us to state at position i, then [i, q] = (cid:81)i ] = [d, qmax]. Let rrr = r1r1r . . . rdrdrd ]. j=1 vvvm+j[rjrjrj (a) Base case = 0: [0, q0] = 1 = (cid:81)0 (b) Inductive Step: At position i, let q, ririri j=1 vvvm+j[rjrjrj ]. 14 15 in Algo 1, [i, q] = [i 1, q] vvvm+i(ririri [i 1, q] = (cid:81)i1 (cid:81)i ]. Thus, [i, q] = (cid:81)i1 j=1 vvvm+j[rjrjrj j=1 vvvm+j[rjrjrj ]. = r[i, q] (line 21 in Algo 1). From lines ). By the inductive hypothesis, ) = j=1 vvvm+j[rjrjrj ] vvvm+i(ririri Let qd δ(r1r1r1 (cid:81)d j=1 vvvm+j[rjrjrj ] = (rrr vvvm+1 . . . vvvn). . . . rdrdrd , q0). Since qd = qmax (line 19 in Algo 1), [d, qmax] = 2. We show that for every valid string rrr = r1r1r1 j=1 vvvm+j[rjrjrj j=1 vvvm+j[rjrjrj LP (R)), (cid:81)d show that (cid:81)i ] [d, qmax]. Let 0 and δ(r1r1r1 ] [d, q] using induction on i. satisfying xxx .(xxx S(rrr)) (xxx , q0). We . . . ririri . . . rdrdrd (a) Base case = 0: [0, q0] = 1 = (cid:81)0 ]. j=1 vvvm+j[rjrjrj ririri (b) Inductive Step: At position + 1, let δ(r1r1r1 ] [i, q]. Recall, the inductive hypothesis, (cid:81)i j=1 vvvm+j[rjrjrj , q0) and δ(q, ri+1ri+1ri+1 ). By Vi+1(q, q) = (cid:40)max vvvm+i+1(t) s.t. δ(q, t) tV 0 if q, are not connected [i + 1, q] = max qQ [i, q] Vi+1(q, q) Thus, vvvm+i+1(ri+1ri+1ri+1 vvvm+i+1(ri+1ri+1ri+1 ) Vi+1(q, q). Hence, (cid:81)i+1 ) [i, q] Vi+1(q, q) [i + 1, q]. j=1 vvvm+j[rjrjrj ] = (cid:81)i j=1 vvvm+j[rjrjrj ] 21 . . . rdrdrd Let qd δ(r1r1r1 line 16 in Algo. 1, qmax = arg maxqQl [d, qd] [d, qmax]. From the inductive hypothesis above, (cid:81)d [d, qd] [d, qmax]. , q0). Since xxx .(xxx S(rrr)) (xxx LP (R)), qd Ql. From [d, q]. Thus, by the definition of arg max, ] j=1 vvvm+j[rjrjrj 3. Hence, (rrr vvvm+1 . . . vvvn) = (cid:81)d j=1 vvvm+j[rjrjrj ] [d, qmax] = (cid:81)d j=1 vvvm+j[rjrjrj ] = (rrr vvvm+1 . . . vvvn)."
        },
        {
            "title": "C Time complexity analysis of parallelized DINGO DP",
            "content": "Initialization of the DP maximum token probability transtion (q q) at position token for the maximum probability transition (q q) The computation along all can be parallelized for do # Parallelize for each {1, . . . d} for (q Q) do δ(q, t) Vi(q, q), Ti(q, q) MaxTransition(vvvi, t, q, q) Algorithm 3 DINGO DP Require: q0, block length d, probability vectors vvv1, . . . vvvd for the current block, Ql, Q, δ. 1: [0, q] 0 for all (q Q) (q = q0) 2: [0, q0] 1 3: r[0, q] (None, None) for all (q Q) 4: Vi {} for all {1, . . . , d} 5: Ti {} for all {1, . . . , d} 6: for {1, . . . , d} do 7: 8: 9: 10: 11: 12: for {1, . . . , d} do 13: 14: 15: 16: 17: qmax arg maxqQl 18: if [d, qmax] = 0 then return None, qmax 19: 20: rrr {}, qcurr qmax 21: for {d, . . . , 1} do qcurr, r[i, qcurr] 22: rrr rrr 23: 24: return reverse(rrr), qmax if [i, q] < [i 1, q] Vi(q, q) then [i, q] [i 1, q] Vi(q, q) r[i, q] (q, Ti(q, q)) for (q Q) (q Q) do [d, q] DP computation loop Update maximum probability path to Update the parents accordingly No valid prefixes Decoding the optimal string rrr The parallelism step at line 6 in Algo. 3 can be efficiently implemented using popular frameworks like PyTorch. With parallelism, the computational depth (i.e., the minimum number of sequential steps) reduces to O(max(Q2, V ) + Q2 d). For regular expressions, where the number of states is small constant, the computational depth becomes O(V + d), which is linear in both the vocabulary size and the block length d. Semi-Autoregressive In the semi-autoregressive setup, given an input ppp m, the output ooo m+dk is generated over blocks, where each block is computed via call to the single block diffusion model. The output of the i-th diffusion model call is xxxi = Lmi,ni(xxxi1), with xxx0 = ppp and the final output ooo = xxxk. The input and output lengths for each block are defined as mi = + (i 1) and ni = + for all 1 k. 22 Algorithm 4 Semi-Autoregressive diffusion LLM Generation Require: diffusion LLM L, prompt ppp, answer length n, block length d, diffusion steps , vocabulary , number of blocks k. 1: xxx ppp 2: rrr {} 3: for {1, . . . , k} do 4: 5: 6: 7: Return rrr xxx ririri Diffusion(xxx, + (i 1) d, d, T, ) rrr rrr ririri xxx xxx ririri Initialize xxx with input prompt ppp Intialize the output string ririri is i-th output block Compute the input prompt for the next block Algorithm 5 Semi-Autoregressive Constrained diffusion LLM Generation Require: diffusion LLM L, prompt ppp, answer length n, block length d, diffusion steps , vocabulary , number of blocks k, regular expression R. Pre-compute the dfa start state, live states and δ Initialize xxx with input prompt ppp Intialize the output string Intialize the current dfa state the response is at xxx ririri, qnext Diffusion(xxx, + (i 1) d, d, T, , Ql, δ, qcurr) if qnext Ql then return None No valid completion Compute the input prompt for the next block Update current DFA state for next block 1: q0, Ql, δ PreProcess(R) 2: xxx ppp 3: rrr {} 4: qcurr q0 5: for {1, . . . , k} do 6: 7: 8: 9: 10: 11: 12: Return rrr rrr rrr ririri xxx xxx ririri qcurr qnext In the semi-autoregressive setting, after each block, we ensure that the output generated so far ends in live state from Ql; otherwise, we return the None string (line 7, Algo. 5). Additionally, we maintain variable qcurr to track the current DFA state at the end of each block. This state is then used as the starting state for the dynamic programming step in the constrained generation of the next block."
        },
        {
            "title": "E Token Transitions Statistics",
            "content": "Table 3: Token Transitions Pre-Computation Statistics"
        },
        {
            "title": "Model Family",
            "content": "V GSM-Symbolic Time(s) #States Time(s) JSON-Mode LLaDA-8B Dream-7B 126349 32.09 37.01 40 40 13.22 11.87 #States 169.31 169.31 In Table 3, we report the precomputation time and the number of states in the DFA for both tasks. For JSON generation, different regular expressions are used for different schemas; therefore, we report the mean precomputation time and mean number of states. The maximum number of states and precomputation times across all questions are 455 and 17.7 (Dream) 21.3 (LLaDA) seconds, respectively. GSM-Symbolic F.1 GSM-Symbolic Prompt You are an expert in solving grade school math tasks . You will be presented with grade - school math word problem with symbolic variables and be asked to solve it . Before answering you should reason about the problem ( using the < reasoning > field in the response described below ) . Intermediate symbolic expressions generated during reasoning should be wrapped in << > >. Only output the symbolic expression wrapped in << >> that answers the question . The expression must use numbers as well as the variables defined in the question . You are only allowed to use the following operations : + , -, / , // , % , * , and **. You will always respond in the format described below : Let think step by step . < reasoning > The final answer is << symbolic expression > > There are { } trees in the { }. { } workers will plant trees in the { } today . After they are done , there will be { tf } trees . How many trees did the { } workers plant today ? Let think step by step . Initially , there are { } trees . After planting , there are { tf } trees . The number of trees planted is << tf - > >. The final answer is << tf - > >. If there are { } cars in the parking lot and { nc } more cars arrive , how many cars are in the parking lot ? Let think step by step . Initially , there are { } cars . { nc } more cars arrive , so the total becomes <<c + nc > >. The final answer is <<c + nc > >. { p1 } had { ch1 } { o1 } and { p2 } had { ch2 } { o1 }. If they ate { } { o1 } , how many pieces do they have left in total ? Let think step by step . Initially , { p1 } had { ch1 } { o1 } , and { p2 } had { ch2 } { o1 } , making total of << ch1 + ch2 > >. After eating { } { o1 } , the remaining total is << ch1 + ch2 - > >. The final answer is << ch1 + ch2 - > >. { p1 } had { l1 } { o1 }. { p1 } gave { } { o1 } to { p2 }. How many { o1 } does { p1 } have left ? Let think step by step . { p1 } started with { l1 } { o1 }. After giving { } { o1 } to { p2 } , { p1 } has << l1 - > > { o1 } left . The final answer is << l1 - > >. { question } Listing 1: Prompt template for the GSM-Symbolic task Mirzadeh et al. [2024]. 24 F.2 GSM-Symbolic Regex ( ?: (?:(?:(?:(?:[ -;=? - ]+) ) *(?: < <(?:(?: ) ) ?( ?:( ?:( ?:( ?:( ?:[ - ]) (?:[0 -9]{1 ,3}) ((?:(?:(?:(?:[ - ]) (?:[0 -9]{1 ,3}) ((?:(?:(?:(?:[ - ]) (?:[0 -9]{1 ,3}) ((?:(?:(?:(?:[ - ]) (?:[0 -9]{1 ,3}) ) ) (?:(?:(?:(?: ) ) ?(?:(?:+ -///%***) ) (?:(?: ) ) ?(?:(?:(?:[ - ]) (?:[0 -9]{1 ,3}) ) ) ) ) *) ) ) ) (?:(?:(?:(?: ) ) ?(?:(?:+ -///%***) ) (?:(?: ) ) ?(?:(?:(?:[ - ]) (?:[0 -9]{1 ,3}) ((?:(?:(?:(?:[ - ]) (?:[0 -9]{1 ,3}) ) ) (?:(?:(?:(?: ) ) ?(?:(?:+ -///%***) ) (?:(?: ) ) ?(?:(?:(?:[ - ]) (?:[0 -9]{1 ,3}) ) ) ) ) *) ) ) ) ) ) *) ) ) ) (?:(?:(?:(?: ) ) ?(?:(?:+ -///%***) ) (?:(?: ) ) ?(?:(?:(?:[ - ]) (?:[0 -9]{1 ,3}) ((?:(?:(?:(?:[ - ]) (?:[0 -9]{1 ,3}) ((?:(?:(?:(?:[ - ]) (?:[0 -9]{1 ,3}) ) ) (?:(?:(?:(?: ) ) ?(?:(?:+ -///%***) ) (?:(?: ) ) ?(?:(?:(?:[ - ]) (?:[0 -9]{1 ,3}) ) ) ) ) *) ) ) ) (?:(?:(?:(?: ) ) ?(?:(?:+ -///%***) ) (?:(?: ) ) ?(?:(?:(?:[ - ]) (?:[0 -9]{1 ,3}) ((?:(?:(?:(?:[ - ]) (?:[0 -9]{1 ,3}) ) ) (?:(?:(?:(?: ) ) ?(?:(?:+ -///%***) ) (?:(?: ) ) ?(?:(?:(?:[ - ]) (?:[0 -9]{1 ,3}) ) ) ) ) *) ) ) ) ) ) *) ) ) ) ) ) *) ) ) ) (?:(?:(?:(?: ) ) ?(?:(?:+ -///%***) ) (?:(?: ) ) ?(?:(?:(?:[ - ]) (?:[0 -9]{1 ,3}) ((?:(?:(?:(?:[ - ]) (?:[0 -9]{1 ,3}) ((?:(?:(?:(?:[ - ]) (?:[0 -9]{1 ,3}) ((?:(?:(?:(?:[ - ]) (?:[0 -9]{1 ,3}) ) ) (?:(?:(?:(?: ) ) ?(?:(?:+ -///%***) ) (?:(?: ) ) ?(?:(?:(?:[ - ]) (?:[0 -9]{1 ,3}) ) ) ) ) *) ) ) ) (?:(?:(?:(?: ) ) ?(?:(?:+ -///%***) ) (?:(?: ) ) ?(?:(?:(?:[ - ]) (?:[0 -9]{1 ,3}) ((?:(?:(?:(?:[ - ]) (?:[0 -9]{1 ,3}) ) ) (?:(?:(?:(?: ) ) ?(?:(?:+ -///%***) ) (?:(?: ) ) ?(?:(?:(?:[ - ]) (?:[0 -9]{1 ,3}) ) ) ) ) *) ) ) ) ) ) *) ) ) ) (?:(?:(?:(?: ) ) ?(?:(?:+ -///%***) ) (?:(?: ) ) ?(?:(?:(?:[ - ]) (?:[0 -9]{1 ,3}) ((?:(?:(?:(?:[ - ]) (?:[0 -9]{1 ,3}) ((?:(?:(?:(?:[ - ]) (?:[0 -9]{1 ,3}) ) ) (?:(?:(?:(?: ) ) ?(?:(?:+ -///%***) ) (?:(?: ) ) ?(?:(?:(?:[ - ]) (?:[0 -9]{1 ,3}) ) ) ) ) *) ) ) ) (?:(?:(?:(?: ) ) ?(?:(?:+ -///%***) ) (?:(?: ) ) ?(?:(?:(?:[ - ]) (?:[0 -9]{1 ,3}) ((?:(?:(?:(?:[ - ]) (?:[0 -9]{1 ,3}) ) ) (?:(?:(?:(?: ) ) ?(?:(?:+ -///%***) ) (?:(?: ) ) ?(?:(?:(?:[ - ]) (?:[0 -9]{1 ,3}) ) ) ) ) *) ) ) ) ) ) *) ) ) ) ) ) *) ) ) ) ) ) *) ) (?:(?: ) ) ? > >) ) ) +(?:(?:.) ) ?) Listing 2: GSM-Symbolic Regex F.3 GSM-Symbolic Case Studies Case Study 1: Figure 2: An example from the GSM-symbolic dataset (variables in blue), where unconstrained generation produces syntactically incorrect output, and greedy constrained generation yields syntactically valid but incorrect answer. In contrast, DINGO generates the correct answer. Case Study 2: Figure 3: An example from the GSM-symbolic dataset (variables in blue), where unconstrained generation produces syntactically incorrect output, and greedy constrained generation yields syntactically valid but incorrect answer. In contrast, DINGO generates the correct answer. JSON-Mode G.1 JSON-Mode Example Prompt You are helpful assistant that answers in JSON . Here the json schema you must adhere to : < schema > { title : PromotionalCampaign , type : object , properties : { campaignID : { title : Campaign ID , type : string } , productID : { title : Product ID , type : string } , startDate : { title : Start Date , type : string , format : date } , endDate : { title : End Date , type : string , format : date } , discountDetails : { title : Discount Details , type : string }} , required : [ campaignID , productID , startDate , endDate ]} </ schema > organizing promotional campaign for our new eco - friendly laundry detergent , which is part of our household products line . The campaign will start on June 1 , 2023 , and end on June 30 , 2023. We re planning to offer 15% discount on all purchases during this period . The campaign ID is CAMP123456 , the product ID is PROD7891011 , and the discount details are 15% off on all purchases . Only output the JSON object , no other text or comments . Listing 3: Example JSON Prompt from the JSON-Mode-Eval task NousResearch [2024]. The prompt includes system message that specifies schema and user message that explicitly instructs the model to output JSON object following that schema with certain parameters. G.2 JSON-Mode Example Regex {[ ]?\" campaignID \"[ ]?:[ ]?\"([^\" x00 - x1F x7F - x9F ][\"]) *\"[ ]? ,[ ]?\" productID \"[ ]?:[ ]?\"([^\" x00 - x1F x7F - x9F ][\"]) *\"[ ]? ,[ ]?\" startDate \"[ ]?:[ ]?\"(?: {4}) -(?:0[1 -9]1[0 -2]) -(?:0[1 -9][1 -2][0 -9]3[0 -1]) \"[ ]? ,[ ]?\" endDate \"[ ]?:[ ]?\"(?: {4}) -(?:0[1 -9]1[0 -2]) -(?:0[1 -9][1 -2][0 -9]3[0 -1]) \"([ ]? ,[ ]?\" discountDetails \"[ ]?:[ ]?\"([^\" x00 - x1F x7F - x9F ][\"]) *\") ?[ ]?} Listing 4: Regex for the JSON Schema in Appendix G. 26 G.3 JSON-Mode Case Studies Figure 4: An example from JSON generation, where unconstrained generation produces syntactically incorrect output, and greedy constrained generation yields valid but incomplete prefix. In contrast, DINGO generates syntactically correct answer. Figure 5: An example from JSON generation, where unconstrained generation produces syntactically incorrect output, and greedy constrained generation yields valid but incomplete prefix. In contrast, DINGO generates syntactically correct answer."
        },
        {
            "title": "H Ablation Study on Number of Blocks for Diffusion LLM Generation",
            "content": "(GSM-Symbolic) We run generation with response length of 128, using 64 total diffusion steps, and each of 1, 2, and 8 blocks. Table 4 presents the result. Table 4: Ablation Study on The Number of Diffusion Blocks for GSM-Symbolic Model #Blocks Method Acc. (%) Parse (%) Time (s) 1 LLaDA-8B-I 2 8 Dream-I-7B 2 8 Unconstrained Greedy Constrained Best of Greedy + Unconstrained DINGO Unconstrained Greedy Constrained Best of Greedy + Unconstrained DINGO Unconstrained Greedy Constrained Best of Greedy + Unconstrained DINGO Unconstrained Greedy Constrained Best of Greedy + Unconstrained DINGO Unconstrained Greedy Constrained Best of Greedy + Unconstrained DINGO Unconstrained Greedy Constrained Best of Greedy + Unconstrained DINGO 20 26 26 29 22 30 30 32 19 27 27 28 32 32 34 30 33 33 34 32 34 34 36 54 94 94 100 54 96 96 100 35 98 98 100 69 90 90 100 55 87 87 100 61 93 93 100 23.66 23.7 23.66 23.73 23.63 23.81 23.65 23.93 23.78 23.97 23.8 23.92 23.56 23.64 23.65 23.67 23.62 23.71 23.62 23.65 23.89 24.01 23.89 23."
        },
        {
            "title": "I Ablation Study on Number of Blocks for Diffusion LLM Generation",
            "content": "(JSON-Mode) We run generation with response length of 128, using 64 total diffusion steps, and each of 1, 2, and 8 blocks. Table 5 presents the result. Table 5: Ablation Study on The Number of Diffusion Blocks for JSON-Mode. Model #Blocks Method Acc. (%) Parse (%) Time (s) 1 LLaDA-8B-I 2 8 Dream-I-7B 2 8 Unconstrained Greedy Constrained Best of Greedy + Unconstrained DINGO Unconstrained Greedy Constrained Best of Greedy + Unconstrained DINGO Unconstrained Greedy Constrained Best of Greedy + Unconstrained DINGO Unconstrained Greedy Constrained Best of Greedy + Unconstrained DINGO Unconstrained Greedy Constrained Best of Greedy + Unconstrained DINGO Unconstrained Greedy Constrained Best of Greedy + Unconstrained DINGO 87 78 99 100 84 92 99 100 84 98 100 85 30 91 100 79 37 86 100 70 52 86 100 91 79 99 100 92 94 99 100 89 98 100 100 87 30 93 100 82 39 88 100 74 52 89 100 6.7 6.81 6.73 6.78 6.72 6.83 6.73 6.86 6.73 6.87 6.75 6.85 6.4 6.51 6.43 6.55 6.47 6.68 6.5 6.63 6.44 6.65 6.46 6."
        },
        {
            "title": "J Ablation Study on Number of Steps for Diffusion LLM Generation",
            "content": "(GSM-Symbolic) We run generation with response length of 128, 1 block, and each of 16, 32, 64, and 128 total diffusion steps. Table 6 presents the result. Table 6: Ablation Study on The Number of Diffusion Steps for GSM-Symbolic with Dream-I-7B #Steps Method Acc. (%) Parse (%) Time (s) 16 32 64 128 Unconstrained Greedy Constrained Best of Greedy + Unconstrained DINGO Unconstrained Greedy Constrained Best of Greedy + Unconstrained DINGO Unconstrained Greedy Constrained Best of Greedy + Unconstrained DINGO Unconstrained Greedy Constrained Best of Greedy + Unconstrained DINGO 6 13 13 18 18 25 25 28 28 32 32 34 31 30 31 20 78 78 100 48 87 87 100 69 90 90 100 74 89 90 100 5.99 6.18 5.99 6.09 11.96 12.06 11.96 12. 23.56 23.64 23.65 23.67 47.83 47.88 47.83 47."
        },
        {
            "title": "K Ablation Study on Number of Steps for Diffusion LLM Generation",
            "content": "(JSON-Mode) We run generation with response length of 128, 1 block, and each of 16, 32, 64, and 128 total diffusion steps. Table 7 presents the result. Table 7: Ablation Study on The Number of Diffusion Steps for JSON-Mode with Dream-I-7B #Steps Method Acc. (%) Parse (%) Time (s) 16 32 64 128 Unconstrained Greedy Constrained Best of Greedy + Unconstrained DINGO Unconstrained Greedy Constrained Best of Greedy + Unconstrained DINGO Unconstrained Greedy Constrained Best of Greedy + Unconstrained DINGO Unconstrained Greedy Constrained Best of Greedy + Unconstrained DINGO 54 32 68 100 67 35 78 100 85 30 91 100 85 46 95 59 32 71 100 71 35 82 100 87 30 93 100 87 46 97 100 1.51 1.62 1.52 1.6 3.23 3.35 3.24 3. 6.4 6.51 6.43 6.55 13.42 13.53 13.43 13."
        }
    ],
    "affiliations": [
        "Department of Computer Science, University of Illinois Urbana-Champaign"
    ]
}