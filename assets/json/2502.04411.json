{
    "paper_title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
    "authors": [
        "Kunfeng Lai",
        "Zhenheng Tang",
        "Xinglin Pan",
        "Peijie Dong",
        "Xiang Liu",
        "Haolan Chen",
        "Li Shen",
        "Bo Li",
        "Xiaowen Chu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Model merging aggregates Large Language Models (LLMs) finetuned on different tasks into a stronger one. However, parameter conflicts between models leads to performance degradation in averaging. While model routing addresses this issue by selecting individual models during inference, it imposes excessive storage and compute costs, and fails to leverage the common knowledge from different models. In this work, we observe that different layers exhibit varying levels of parameter conflicts. Building on this insight, we average layers with minimal parameter conflicts and use a novel task-level expert routing for layers with significant conflicts. To further reduce storage costs, inspired by task arithmetic sparsity, we decouple multiple fine-tuned experts into a dense expert and several sparse experts. Considering the out-of-distribution samples, we select and merge appropriate experts based on the task uncertainty of the input data. We conduct extensive experiments on both LLaMA and Qwen with varying parameter scales, and evaluate on real-world reasoning tasks. Results demonstrate that our method consistently achieves significant performance improvements while requiring less system cost compared to existing methods."
        },
        {
            "title": "Start",
            "content": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing Kunfeng Lai * 1 Zhenheng Tang * 2 Xinglin Pan 1 Peijie Dong 1 Xiang Liu 1 Haolan Chen 3 Li Shen 4 Bo Li 2 Xiaowen Chu 1 2 5 2 0 2 1 1 ] . [ 2 1 1 4 4 0 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Model merging aggregates Large Language Models (LLMs) finetuned on different tasks into stronger one. However, parameter conflicts between models leads to performance degradation in averaging. While model routing addresses this issue by selecting individual models during inference, it imposes excessive storage and compute costs, and fails to leverage the common knowledge from different models. In this work, we observe that different layers exhibit varying levels of parameter conflicts. Building on this insight, we average layers with minimal parameter conflicts and use novel task-level expert routing for layers with significant conflicts. To further reduce storage costs, inspired by task arithmetic sparsity, we decouple multiple fine-tuned experts into dense expert and several sparse experts. Considering the out-of-distribution samples, we select and merge appropriate experts based on the task uncertainty of the input data. We conduct extensive experiments on both LLaMA and Qwen with varying parameter scales, and evaluate on real-world reasoning tasks. Results demonstrate that our method consistently achieves significant performance improvements while requiring less system cost compared to existing methods. 1. Introduction Finetuning Large Language Models (LLMs) enables them to adapt to downstream applications including sentiment analysis (Sun et al., 2023), text summarization (Fang et al., 2024), mathematical reasoning (Ruis et al., 2024), code writing (Jiang et al., 2024b), roleplay chatting (Chen et al., 2025) *Equal contribution 1The Hong Kong University of Science and Technology (Guangzhou) 2The Hong Kong University of Science and Technology 3Platform and Content Group, Tencent 4Sun Yatsen University. Correspondence to: Xiaowen Chu <xwchu@hkustgz.edu.cn>. so on. Open-source platforms such as Huggingface (Wolf et al., 2019) and torchvision (Marcel & Rodriguez, 2010) facilitate access to diverse array of highly trained expert models with varying capabilities. Considering the computational resources are scarce and implementing green computing (Samsi et al., 2023; You et al., 2022; Stojkovic et al., 2024; Bai et al., 2024), the community is increasingly interested in how to merge these models to create superior LLM that retains the strengths of finetuned ones without retraining (Yang et al., 2024b; Lu et al., 2024b; Du et al., 2024; Yadav et al., 2023b). Figure 1: Knowledge conflict across finetuned LLMs and math and code dataset. Deeper color means larger parameter conflicts. And it is difficult for the linear averaged model to achieve low loss of both tasks. One predominant merging strategy is model averaging (Yang et al., 2024b; Matena & Raffel, 2022; Thennal et al., 2024; Yu et al., 2024b), which computes weighted averages of parameters to synthesize collective knowledge (Matena & Raffel, 2022; Yadav et al., 2023b). However, model averaging faces challenges from parameter conflicts arising from diverse finetuning tasks, leading to performance degradation as shown in Figure 1. Another direction is model routing (Lu et al., 2024b; Muqeeth et al., 2024; Yang et al., 2024c; Du et al., 2024; Lu et al., 2024a; He et al., 2024a; Wei et al., 2024a; Chen et al., 2024), which aggregates models and performs model selection during inference. This method avoids parameter conflicts but incurs significant computing and storage (system) costs due to maintaining all finetuned models. This motivates us to rethink the following questions: How to better merge common and unique knowledge from various finetuned models while simultaneously avoiding parameter conflicts and minimizing system costs? 1 Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing To answer this question, we firstly quantify the conflicts between finetuned LLMs. We employ sign consistency between different task arithmetics that are difference between the finetuned LLM and the original LLM to measure the conflicts. We find that the front and last layers tend to exhibit the highest levels of conflict, suggesting that these layers are particularly sensitive to averaging. In contrast, the central layers demonstrate comparatively lower levels of conflict, indicating that they retain more common knowledge. Then, we introduce Mediator as an adaptive model merging framework to enhance LLM merging with little storage and computation costs. Inspired by the varying degrees of layer-wise parameter conflicts, we propose adaptive merging that averages layers with lower conflict levels, thereby capturing the common knowledge (Yadav et al., 2023b; He et al., 2024b) shared among LLMs while minimizing conflicts (Yadav et al., 2023b). Concurrently, layers with significant conflicts are regarded as experts to be routed during inference, preserving unique task-specific knowledge without dilution (Yadav et al., 2023b; He et al., 2024b). While direct compression of finetuned LLMs results in significant information loss (Dong et al.; Sun et al., 2024), we leverage both layer-wise model merging and the high sparsity of task arithmetics (Yadav et al., 2023b) to decompose models into base and task-specific components (Ilharco et al., 2023; He et al., 2024b; Yang et al., 2024c; Tang et al., 2024b). By integrating these two techniques, our approach reduces storage from 50% to 7% with minimal accuracy loss while preserving layer-specific knowledge. Observing that LLMs are finetuned on the complete sentences of their downstream tasks instead of the splited sub-sequences, to better preserve task-specific knowledge and improve overall model performance, we propose tasklevel expert routing instead of token-level routing (Lepikhin et al., 2020; Sukhbaatar et al., 2024a; Zhou et al., 2022; Jiang et al., 2024a). With these designs, our merged LLM achieves high efficiency with minimal performance degradation (0.06% 0.3%). Our evaluations show that we can effectively run model comparable to 7B 4 LLM ensemble on single RTX 4090 GPU, making highperformance LLM more accessible in resource-constrained environments(Appendix G.3). Considering the out-of-distribution (OOD) samples, we select and merge appropriate experts based on the task uncertainty of the input data. Thus, the unified model can select appropriate experts based on the characteristics of the input data. For example, the input may incorporate both codes and mathematical reasoning, then the unified model can select the experts that are trained on both codes and mathematical reasoning. We finetune pretrained LLMs with Chain-of-thoughts (Wei et al., 2022) enhanced datasets, showing that the model merge of Mediator can successfully preserve the reasoning ability (Zelikman et al., 2022; Kojima et al., 2022; Guo et al., 2025). As far as we know, we are the first to conduct cutting-edge LLM merging based on the finetuning with CoT enhanced downstream tasks. Our main contributions can be summarized as follows: We investigate and demonstrate that different layers of fine-tuned models exhibit varying levels of parameter conflicts (Section 3). Then we propose Mediator, an adaptive layer-wise model merging approach to average layers with minimal conflicts and use task-level expert routing for layers with significant conflicts (Section 4.1). We propose method in Mediator to decouple fine-tuned experts into one dense expert and several sparse experts (Section 4.2), achieving high compression ratio while maintaining accuracy. Our approach enables dynamic expert selection based on task uncertainty (Section 4.3), effectively handling OOD data. We conduct experiments based on the modern LLMs including LLaMA and Qwen with CoT enhanced finetuning and the real-world cutting-edge LLM evaluation tasks. Results show that our method achieves significant performance improvements and less system cost compared to existing methods. (Section 5). 2. Preliminary and Related Works 2.1. Language Modeling and LLM Finetuning Task Data Distribution. Given set of different downstream tasks , based on the sampling task τ , the pretraining document (data sample) is sequence x1:T of tokens with the maximum length generated from distribution pτ = p(x1:T τ ) = p(o1, . . . , oT τ ) (Xie et al., 2022; Wies et al., 2023; Hahn & Goyal, 2023; Li et al., 2024b). And we define the pretraining data is sampled from p(xT ) = (cid:82) τ p(o1, . . . , oT τ )p(τ )dτ . Each token is sampled from vocabulary O. And both (T and belong to large task family Ω, i.e. , Ω. Language Modeling. Current LLMs (Brown et al., 2020; Touvron et al., 2023; Xie et al., 2022) usually utilize the next word prediction as the language modelling, which predicts the next token xt given the previous tokens x1:t1 for all = 1, . . . , . Formally, LLM parameterized by θ is distribution fθ(xtx1:t1). And it is pretrained on huge corpus sampled from the pretraining distribution p(xT ) (Xie et al., 2022). Finetuning LLM. Normally, for each downstream task τ , finetuning LLM is to minimize the cross-entropy 2 Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing loss function as below: 3. Understanding Conflict between LLMs LCE(θ, τ ) = (cid:88) t=1 E[pτ (xtx1:t1) log fθ(xtx1:t1)]. After finetuning, the model parameters θ are updated to θτ . 2.2. Model Merging Given finetuned task-specific LLMs {θ1, θ2, . . . , θnτ } finetuned on task set , where nτ = , model merge aims to find unified model parameterized by ϕ that can achieve the low loss on all tasks as following min ϕ LCE(ϕ, ) = 1 nτ (cid:88) τ LCE(ϕ, τ ). (1) Different from training ϕ that can be optimized towards any direction, model merging aims to exploit combining {θ, θ1, θ2, . . . , θnτ } to obtain the ϕ. The current model merging methods include following two categories. Model Averaging. Averaging parameters to fuse the knowledge from different finetuned models is straightforward. Mathematically, averaged model is ϕ = (cid:80) τ wτ θτ , in which wτ is the averaging weight and (cid:80) τ wτ = 1. Considering different model parameters have different importance on downstream tasks (Kirkpatrick et al., 2017; Sun et al., 2024; Dong et al.), assigning larger weights to more important parameters. Current methods usually utilize Taylor expansion (Lee et al., 2019; Matena & Raffel, 2022) to measure the importance of the parameters. However, the knowledge conflicts still exist for parameters that have high importance simultaneously. Model Routing. To completely avoid the conflicts, another way is to collect models together and select the most relevant models for each task. This approach typically employs selection mechanism to activate the most relevant model based on the input task τ (Yang et al., 2024c) or sequence x1:t at t-th token like the Mixture of Experts (MoE) (Tang et al., 2024c). Current methods propose different routing and re-training mechanisms to improve the performance (He et al., 2024a; Wei et al., 2024a; Sukhbaatar et al., 2024c). However, these methods fail to consider merging the parameters to find the common knowledge that can be shared across different tasks, and cause large memory and computational costs. Due to the limited space, we left detailed discussions about related works in Appendix A). Out-of-distribution Data. In real-world deployment, the test data may come from other distributions instead of the p(xτ )τ . To this end, we need to consider how to handle OOD data within merging LLMs. In this work, we mainly consider two OOD cases and tackle them in Section 4. Table 1: Accuracy of finetuning Llama 3.2 3B. Model GSM8K Math TriviaQA H.Eval Code All tasks θ (Pretrained) θ1 (Math SFT) θ2 (QA SFT) θ3 (Coding SFT) ϕAVG ϕSEL 27.52 46.47 32.75 33.13 42.61 46.47 57.71 54.59 61.45 57.71 60.99 61.45 22.56 25.00 28.05 40.85 31.30 40. 35.93 42.02 40.75 43.90 44.97 49.59 Preliminary Experiments. We finetune Llama-3.2-3B on three datasets and evaluation with according tasks (details in Section 5). Table 1 shows the performance of the merged model and the individual finetuned models. We use the (θ, τ ) to represent the performance of the θ on τ . Comparing Performance on All Tasks. We write PORI = (θ, ) as the performance of the original model θ on all tasks , the PAVG = (ϕAVG, ) as the performance of the averaged model ϕAVG on all tasks . We write the PSEL = 1 τ (ϕSEL, τ ) as the performance of the nτ selection based model ϕSEL on all tasks . The results show that the (cid:80) PORI < PAVG < PSEL, (2) which means that finetuning and averaging can successfully increase the model performance. However, simply averaged model cannot recover the task performance of the corresponding finetuned models. Comparing Performance on Individual Models. Table 1 shows that while the model θτ has the best performance on its according task τ , its performance on other tasks τ is lower than ϕAVG. This indicates that the averaged model can still benefit from merging knowledge from different finetuned models. Thus, better merging strategy should be able to average the parameters that have less parameter conflict to find the common knowledge that can be shared across different tasks and avoid the parameter conflict that degrades the performance of the finetuned model. Figure 2: Parameter conflict distribution across different layers of finetuned models (Qwen 2.5 7B). Definition 3.1 (Task Arithmetic). task arithmetic on task τ is the parameter difference between the finetuned LLM θτ and the pre-trained LLM θ, i.e., τ = θτ θ. 3 Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing Figure 3: The framework of Mediator. Denoising Parameters. Due to the stochastic optimization process in finetuning, some elements in τ are noisy and do not influence the performance (Yadav et al., 2023b; He et al., 2024b). Thus, before measuring the parameter conflicts (Yadav et al., 2023b; He et al., 2024b), we firstly denoise the parameters by removing the elements in τ that have have small magnitude. To this end, we model the update directions of different elements as the Gaussian distribution NUPD(µUPD, σ2 UPD), where µUPD is the mean of the update direction and σ2 UPD is the variance. Then, we denoise the parameters by removing the elements within range (µUPD σUPD, µUPD + σUPD) and obtain the new parameter arithmetic ˆθτ = θ + ˆτ (details in Appendix D.1). Measuring Parameter Conflict. Following (Yadav et al., 2023b), we measure the parameter conflict between different finetuned models using sign-based approach. For layer L, given parameters wl τj from two different models, we consider them conflicting if they have opposite signs, i.e., sgn(wl j) = 1. We define the conflict ratio of layer as the proportion of conflicting parameters in that layer: τi and wl θl θl iwl (cid:88) dl = ( i,j I(sgn(wl iwl j) = 1))/θl, (3) where θl is the total number of parameters in layer l, and I() is the indicator function. Figure 2 shows that the parameter conflict is higher in the front and last layers, and lower in the central layers. This phenomenon indicates that central layers share more common knowledge, while the front and last layers capture more task-specific knowledge. 4. The Design of Mediator The high-level idea of Mediator is to hybridly combine averaging and routing to preserve the downstream knowledge and avoid the parameter conflict (Sectino 4.1) according to parameter conflict distribution across different layers. We decompose the downstream models as the base model and 4 experts of task arithmetics, which brings into the opportunity to sparsify the task arithmetics to prune out the noisy parameters to reduce the memory costs (Section 4.2). Lastly, considering the OOD inputs, we propose Bayesian expert routing (Section 4.3) to better combine knowledge from different experts. 4.1. Adaptive Layer-wise Model Averaging and Routing Inspired by the empirical observation in Figure 2, Mediator averages layers with less parameter conflicts, and route layers with more parameter conflicts. As shown in Figure 3, Mediator calculates the conflicts dl across different layers. Then, Mediator models the layer-wise conflicts as Gaussian distribution dl (µ, σ). Then, for each layer index l, Mediator average layer parameters if the conflict dl is less than the µ + σ, otherwise routing this layer. We denote the averaged layer parameters as ϕl AVG and the routing layer parameters as ϕl UP. Algorithm 1 shows this detailed process. The average operation can be any averaging operation, such as the unified average, importance based (Matena & Raffel, 2022), or subspace based (Yadav et al., 2023b). In our experiments, we mainly use the denoised parameters ˆθτ defined in Section 3 to conduct averaging like TIES (Yadav et al., 2023b) (details of averaging operations in Appendix D.3). Note that all attention layers are averaged, because they are found to save non-specific domain knowledge (Sukhbaatar et al., 2024a). 4.2. Expert Decomposition The routing layer occupies nτ Ml memory, where nτ = and Ml is the memory of each layer in original model. Large nτ significanly increases the memory cost of routing layers, thus leading to weak scalability. Thus, we consider compressing the routing layers to reduce the memory cost. However, previous LLM pruning or quantization (Dong et al.; Sun et al., 2024) on (cid:8)θl(cid:9) cannot achieve high compression ratio on θτ . Different from directly compressing the Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing Algorithm 1 Adaptive Merging and Routing with Sparsified Expert Decomposition in Mediator Input: Different finetuned models θ1, θ2, . . . , θnτ . Output: The merged layers Φ. 1: Calculate the conflict distribution {dl}l=1, ,L; 2: Estimate µ, σ based on {dl}l=1, ,L; 3: for layer = 1, , do if dl < µ + σ then 4: ϕl 1, θl AVG = M(θl 5: else 6: 7: 8: 9: 2, . . . , θl nτ τ θl τ ; ); = θl θl; τ ); 2, . . . , ˆl nτ (cid:111) ; , ˆl 10: (cid:80) θl = 1/nτ τ θl, l τ = θl ˆl τ = Denoise(l (cid:110)θl, ˆl 1, ˆl ϕl UP = end if Insert ϕl 11: 12: 13: end for 14: Return Φ = {ΦAVG, ΦUP}. AVG or ϕl UP into ΦAVG and ΦUP; (a) Qwen 1.5 4B (b) Qwen 2.5 7B Figure 4: Comparing magnitudes of task arithmetic and pretrained model parameters. finetuned model, we find that the task arithmetic τ shows significantly higher sparsity due to its noisy updates (Yadav et al., 2023b). As shown in Figure 4, the empirical magnitudes of the task arithmetic are significantly smaller than the model parameters. This indicates that the τ can be sparsified with higher degree to reduce the memory cost. To this end, we can decompose finetuned LLM θτ into its base model θ and the task-specific expert τ (task arithmetic in Definition 3.1). Each τ will be sparsified as ˆτ by removing the elements that can be seen as noise (Section 3). Thus, the memory cost is reduced from nτ Ml to nτ Ml c, where is the compression ratio. When inferencing, we can use the θ + ˆτ to approximately recover the finetuned model θτ . However, the finetuning may cause forgetting problem (Zhu et al., 2024). Thus, to preserve pretraining knowledge, we make calibration on task arithmetic as τ = θτ θ and saves the pretraining arithmetic as = θ θ, where θ = 1/nτ (cid:80) τ θτ . 4.3. Expert Routing In the token-level expert routing like pretrained MoE (Jiang et al., 2024a; Sukhbaatar et al., 2024a), there are routers for all layers trained to select the task-specific experts for each token. The token-level routing implies that the router is designed for selecting experts at different token index. In other words, for same sequence x1:T sampled from p(x1:T τ ), the token-level router may select different experts for different tokens {1, , }. Algorithm 2 Uncertainty based Expert Routing in Mediator Input: The question prompt x1:t, router πκ, ΦUP, k, β. Output: Reconstructed routing layers ΦUP,x. 1: = Indexes of TopK({πκ(τ x)} , k); 2: Calculate and cache {h(τ x)} (Equation 4); 3: for ϕl UP ΦUP do UP,x = θl + (cid:80) ϕl 4: Insert ϕl h(τ x) ˆl UP,x into ΦUP,x; τ k nτ ; 5: 6: end for 7: Return ΦUP,x. Task-level Routing. However, different from pretrained MoE where one sequence x1:T might be splited into different subsequences and feed-forwarded to different experts, the finetuned LLM θτ is trained on the complete sequence x1:T pτ . Therefore, subsequence x1:t x1:T might be OOD to the another model θˆτ . Therefore, the most suitable experts for each data sample might be the experts that are (cid:9) trained on the task τ , i.e. (cid:8)θl lL for each subsequence {x1:t}t=1, ,T in x1:T pτ . We also provide theoretical understanding to understand the differences between task-level and token-level routing following the Bayesian inference interpretation for the incontext learning (Xie et al., 2022) in Appendix B). Moreover, we also empirically show in Section 5 that the trained token-level routers (Sukhbaatar et al., 2024a) also frequently select the task-specific experts for each data sample. τ Out-of-distribution Data (OOD to but in-distribution to pretraining data p(xT )): The finetuned model θτ may not be able for processing some test-time questions xτ p(xT ) that sampled from other distributions instead of p(xτ ) for any τ . Considering the the modern LLM θ is pretrained on the huge corpus (Brown et al., 2020), the θτ might keep the original knowledge in xτ . Out-of-distribution Data II (OOD but close to and Ω): Considering that τ is continuous (Xie et al., 2022), while the test data xOOD pˆτ (xˆτ ) (ˆτ / , ) has different distribution from pτ for any τ , , the ˆτ might be close to or linear combination of multiple τ1, τ2, . . . , τnτ and τ . Any finetuned model θτ may not be able to process some test-time questions xOOD. better way to conduct 5 Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing inference on xOOD is to combine multiple experts {τ }τ and together. Thus, we propose the Algorithm 2 and the following design. Uncertainty-based Expert Selection. We propose the uncertainty-based expert selection to combine the experts from different tasks into new expert for the test data x. Thus, we need to estimate the posterior distribution p( x) for the test data x. We introduce small classifier κ use cross-entropy loss to learn the likelihood πκ(τ x) for the training dataset including all data pairs {(x, τ )x pτ }τ {τ } (details of constructing this dataset and learning πκ(τ x) is in Appendix D.4). For the training data pτ , we have known that its best expert is τ , thus having = τ . For test data x, we introduce the temperature scaling to soft the logits hκ to obtain the prediction uncertainty πκ(τ x), like OOD detection (Guo) and model distillation (Hinton et al., 2015) as follows: h(τ x) = eπκ(τ x)/β τ eπκ(τ x)/β . (cid:80) (4) Then, the final obtained expert is θl + (cid:80) h(τ x) ˆl nτ as shown in Algorithm 2. Then the whole feed-forward process is as same as the original model. We further provide system optimization in the real-world serving as follows. τ k 4.4. System-level Optimization We compare the inference times of Mediator with those of existing LLMs. The incremental time overhead primarily arises from expert routing and the loading of expert parameters. Fortunately, by using task-level routing, we only run small classifier κ and load the expert parameters once per sample. To accelerate inference, we explore two methods for optimizing the loading of expert parameters. For sparse expert parameters, we store them in the CPU and prefetch the parameters of the upcoming layer while simultaneously performing computations in the current layer. This approach masks the long loading times associated with expert uploads. Additionally, we accelerate the integration of sparse experts (i.e., Algorithm 2) into the dense backbone by using CUDAs atomicAdd for parallel processing. For non-sparse expert parameters, we store them on disk and utilize ZipNN (Hershcovitch et al., 2024) to accelerate the loading of parameters from disk to CPU. Our results show that the optimized inference time of Mediator remains between 0.2s and 0.4s longer than that of existing LLMs per sample (details in Appendix E). 5. Experiments Models and Evaluation Tasks. We conduct comprehensive experiments on cutting-edge LLMs including Qwen-1.54B, Qwen-2.5-7B (Yang et al., 2024a), LLaMA-3.1-3B, and LLaMA-3.2-8B (Dubey et al., 2024). We select different evaluation tasks to effectively demonstrate model capability in resolving parameter conflicts during model merging, including GSM8K of mathematical questionanswering (Cobbe et al., 2021), TriviaQA (Joshi et al., 2017) of large-scale Wikipedia-based question answering dataset, HumanEval (Chen et al., 2021) of Python programming tasks, WinoGrande (Sakaguchi et al., 2019) of logical reasoning, MMLU (Hendrycks et al., 2021) of vertical domain knowledge (as OOD to the finetuned models). Finetuning Settings. The finetuning datasets are constructed by augmenting some publicly datasets (task related but without overlap) with GPT-4o (Gilardi et al., 2023) and Chain-of-Thoughts (Wei et al., 2022). For each finetuning process, we use at least 180K training samples to ensure sufficient performance improvement on the corresponding task, which helps validate the effectiveness of our experiments (Details of constructing finetuning datasets in Appendix and hyperparameters in Appendix C). To the best of our knowledge, this is the first LLM merging study with CoT enhanced finetuning and evaluated with generative tasks. Table 2: Comparing performance of different model merging methods on Llama 3.1 3B. Alg./Tasks GSM. TrA. Wino. H.Eval MMLU AVG. Base Math QA Code All data TIES PCB Twin BTX Mediator 27.52 46.47 32.75 33.13 44. 42.61 46.02 39.04 45.19 46.47 57.71 54.59 61.45 57.71 47.74 60.99 60.39 52.45 62.05 61.02 69.69 69.06 69.69 68.59 69.21 71.11 71.27 69.27 71.87 72.03 22.56 25.00 28.05 40.85 34. 31.30 29.88 29.94 28.05 40.42 54.08 52.73 54.17 53.09 53.75 54.32 54.21 53.91 54.44 54.91 46.31 49.57 49.22 50.67 49.92 51.27 52.35 48.11 52.33 54.97 Table 3: Performance of different models and algorithms on Llama-3.2 8B. Alg./Tasks GSM. TrA. Wino. H.Eval MMLU AVG. Base Math QA Code All data TIES PCB Twin BTX Mediator 56.33 77.18 69.60 61.41 70.89 76.04 76.04 76.80 76.72 76. 72.39 73.99 74.14 73.94 69.77 76.78 76.89 72.71 73.99 76.70 73.64 74.98 75.45 74.59 75.06 74.19 74.35 74.49 75.22 75.69 27.44 20.12 31.71 62.80 48.17 53.05 53.66 59.14 60.98 62. 67.99 62.10 62.21 62.73 62.94 62.36 62.42 64.43 65.68 67.87 59.56 61.67 62.43 67.09 65.37 68.48 68.67 69.51 70.52 71.80 Baselines. We compare pretrained, finetuned models, and the state-of-the-art static and dynamic merging methods with Mediator. The static merging methods include TIES (Yadav et al., 2023a) and PCB-merging (Du et al., 2024) achieve the best performance in weighted average Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing method and do not require calibration data, and also partly consider OOD evaluation tasks. The dynamic merging methods include BTX (Sukhbaatar et al., 2024c) with token-level routing and the twin-merge (Lu et al., 2024a) with task-level routing and SVD decomposition (Details of hyperparameters and optimization of these baselines in Appendix C). 5.1. Main Results Fine-grained Comparison on All Tasks. Table 2 and 3 show the fine-grained performance on each tasks and their overall averaged one of different methods and algorithms. In most of time, the finetuned LLM can achieve the best performance across all single and merged models on its specialized domain, like Math finetuned models on GSM8K and Code finetuned models on HumanEval. While the merged LLMs can generally outperform single models on the averaged performance, their specialized domain performance is weaker. However, Mediator can catch up the domain performance of the specialized models, and almost always outperform other merged models. Also, the overall performance on all tasks of Mediator is consistently better than other methods. Expert routing methods includes BTX and Mediator are likely to further improve performance. This aligns with findings from TIES (Yadav et al., 2023a) and Twin-merging (Lu et al., 2024b). As model scale increases, the improvement margins of all merging algorithms decrease, which may be attributed to the enhanced comprehensive capabilities of individual finetuned models (or experts). We provide more fine-grained results about QWEN-1.5 and 2.5 in Table 25 and 27 in Appendix G. Table 4: Overall Performance on all tasks of all methods. Llama-3B Model/Algo. Llama-8B Qwen-7B Qwen-4B base model all data sft TIES Twin PCB BTX Mediator 48.87 49.04 49.50 48.80 49.93 49.94 51.40 (2.9%) 46.31 49.92 51.27 48.11 52.35 52.33 54.97(5.0%) 67.63 64.59 68.62 68.91 65.86 69.61 71.00 (2.0%) 59.56 65.37 68.48 69.51 68.67 70.52 71.80 (1.8%) time of them is significantly high. In contrast, for task-level routing approaches like Twin-merging and Mediator, taking the lowest time. Table 5: Post Training Time (Hours). Model/Algo LlaMA-3B LlaMA-8B Qwen-4B Qwen-7B Twin PCB BTX Mediator 1.33 3.42 7.55 1.35 1.87 5.75 12.52 2.03 1.60 5.73 8.83 1.57 1.80 5.80 12.18 1.78 Inference Time. Table 6 shows the inference time of different methods. As the token-level routing methods need to load and compute the layer-wise routers for each token, the inference time of them is significantly higher (more than 2x) than our method. We have detailed our inference acceleration process in Section 4.4 and Appendix E. Table 6: Inference time per sample (seconds). Model/Algo Base model Twin BTX Mediator LlaMA-3B LlaMA-8B Qwen-4B Qwen-7B 28 layers 32 layers 40 layers 32 layers 1.452 1.725 3.237 1. 3.600 4.151 8.68 4.053 3.112 3.792 7.082 3.674 3.057 3.648 7.153 3.489 Memory Cost. Table 7 shows the memory costs of different methods. Our method significantly reduces the memory costs compared to saving all finetuned models because there is only one router for all experts, and the sparsified experts saving, and the layer-wise merging strategy (Details of formally comparing memory costs of merging methods in Appendix A). Table 7: Comparing memory costs. Model/Algo LLaMA-3B LLaMA-8B Qwen-4B Qwen-7B Base model Twin-merging BTX (MOE) Mediator 9G 10G 37G 10G 33G 35G 80G 35G 11G 13G 40G 13G 31G 32G 78G 33G Overall Comparison. As shown in Table 4, the advantages of PCB over TIES become less pronounced at larger model scales, and even shows performance degradation on Qwen7B, which demonstrates PCBs instability. Dynamic routing approaches include BTX and Mediator show stable performance improvements. Our method demonstrates consistent improvements across different models. Post-Training Time After Merging. As many model merging methods like Twin, PCB and BTX require post-training, it is critical to compare the extra training time. Table 5 shows the post-training time of different methods. PCB merging require weight exploration thus leads to higher time. The BTX with token-level routing needs to completely train the layer-wise routers for each token, thus, the post-training 5.2. Ablation Studies Scalability of Finetuned Models. To verify the scalability of Mediator, we finetune another 4 LLMs according to the following 4 extra evaluation tasks including: (1) Instruction Following with IFEval. (Zhou et al., 2023) which assess models ability to accurately interpret and execute natural language instructions; (2-4) Medicine, College Economics and Law from CEval. (Huang et al., 2023b) which assess knowledge and capabilities across various academic and professional domains. We utilize four accordingly domain datasets for finetuning including Magpie (Xu et al., 2024), IndustryInstruction (Ind), DISC-Med (Bao et al., 2023), DISC-Law (Yue et al., 2023a) without overlap with IFEval and CEval (Details in Appendix F). 7 Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing Table 8: Performance of scaling up finetuned models. Alg./Tasks GSM. TrA. Wino. H.Eval MMLU I.Eval C.Eco C.Med C.law AVG. Base Math QA code Instruct. Economy medicine Law All Data TIES PCB Twin BTX Mediator 47.16 50.95 45.56 43.29 47.54 45.56 39.12 40.64 43.75 47.38 47.38 47.91 48.44 50.64 44.54 46.95 48.02 46.39 40.96 46.24 44.50 46.64 46. 47.19 47.19 44.78 46.94 48.04 56.75 54.62 57.93 54.14 55.09 57.93 56.67 56.59 56.43 55.80 55.80 57.54 57.85 57.93 41.46 26.83 39.02 43.29 37.80 28.86 1.83 0.61 40.85 36.59 36.59 40.85 42.68 44.51 54.45 53.54 52.32 54.82 54.88 54.21 54.63 54.61 54. 55.38 55.45 53.01 54.93 55.12 30.70 34.05 31.65 31.65 38.37 32.13 28.30 27.94 35.37 34.17 34.29 37.53 36.93 38.50 49.09 45.45 43.64 43.64 52.73 56.36 50.09 41.82 49.09 40.00 41.82 53.32 54.40 56.01 55.10 55.10 59.18 59.18 59.18 55.10 61.22 57.14 55. 57.14 57.14 59.56 60.36 61.17 41.67 45.83 45.83 45.83 50.00 45.83 41.66 58.33 41.67 50.00 50.80 50.00 58.33 58.33 46.77 45.92 47.02 46.91 48.51 46.78 42.00 47.15 46.90 47.07 47.38 49.38 51.18 52.25 Table 8 demonstrate several key findings: 1) Static merging methods like PCB and TIES show diminished performance improvements after task expansion, performing even worse than instruction-following finetuned models in overall scores. The similar performance between PCB merging and TIES aligns with findings from the TIES paper, which noted degraded model capabilities when merging more than three tasks; 2) Dynamic merging approaches like BTX, Twin and Mediator maintain relatively stable performance after task expansion; 3) Mediator consistently outperforms BTX by margin of 2.09% and achieves the best scores across all individual tasks, showing its good scalability. Table 9: Performance under Different Temperature β. 2.0 1.0 Temperature 1.25 1.75 1.50 0.1 0. Mediator 50.31 50.35 50.64 51.02 51. 50.84 50.92 Table 10: Model Performance w/o layer-wise merging. Model scale with averaging w/o averaging perf. gap Qwen-1.5 Qwen-2. 51.40 71.00 51.43 71.29 -0.06% -0.27% Different Temperatures. In the routing process, the hyperparameter temperature β is key factor. Table 9 shows performance change of Mediator with different temperatures. Results show that only around 2.2% score variation, the β = 1.5 achieves the highest performance, β = 0.1 almost equals to the Top-1 routing, results in the lowest performance. Layer-wise Merging. Layers with low conflicts are averaged thus reducing nτ memory occupation. Table 10 averaging or not on Qwen-1.5 with 4 and 8 experts show almost no performance loss of the parameter averaging. Given the significant compression ratios achieved (3.5x for 4 experts, 7x for 8 experts), this minor performance trade-off is acceptable. Compression Ratios of Experts. For each sparsified expert, within each routing layer, we compare different compression ratios in Table 11. Results show that the optimal performance is obtained when 14% parameters are left. This indicates that parameters with smaller magnitudes from task arithmetic are likely noise, which aligns with experiments 8 Table 11: Performance with Different Compression Ratio. Compression Ratio 10% 12% 14% 16% 18% 20% Mediator 96.6% 97.9% 100% 97.2% 97.2% 96.6% from (Yadav et al., 2023a). Comparison between Task-level routing and Token-level routing. Fig. 7 in Appendix demonstrates the expert selection probabilities of the BTX model across different tasks (MMLU, math, coding, and QA). The results indicate that 1) bottom and upper layers show obvious task preference; 2) middle layers suggesting some shared logical processing across tasks in these layers, which explains why averaging middle layers that have less conflicts in Mediator results in small performance loss. 6. Limitations Parameter Conflict Theory. Our empirical analysis in Section 3 provides initial insights, but theoretical foundation for parameter conflicts remains unexplored. Memory Efficiency. Despite reducing memory use versus storing all finetuned models, Mediators storage demands remain high. Scaling Challenges. While scaling to 8 models is feasible, expanding to hundreds or thousands (e.g., for personalization (Chan et al., 2024)) poses deployment challenges. Loading Time. Loading experts per sample remains system bottleneck. Although optimized (Section 4.4), faster loading requires further research. 7. Conclusion We propose Mediator, framework for merging LLMs that addresses parameter conflicts through adaptive layer-wise strategies, which average low-conflict layers and routing high-conflict ones via task-specific experts. This preserves individual model strengths while integrating shared knowledge, improving performance and reducing system costs. By decomposing experts into dense core and sparse components, Mediator minimizes storage without sacrificing efficacy. Dynamic expert selection via task uncertainty enhances adaptability across diverse inputs. Experiments on LLaMA and Qwen demonstrate significant performance gains over existing methods, with CoT enhanced datasets further enhancing reasoning capabilities. Mediator advances efficient LLM merging methods, balancing resource constraints with practical versatility. Future work should explore theoretical foundations of parameter conflicts, large-scale deployment optimizations, and faster expert loading mechanisms. Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing"
        },
        {
            "title": "Impact Statements",
            "content": "Societal Impacts. Our approach demonstrates significant effectiveness by enabling the deployment of merging 7B 4 LLMs with only 24GB VRAM. Compared to ensemble learning with these models, our method not only maintains better accuracy but also requires significantly less computational resources and demonstrates superior performance. This breakthrough in resource efficiency makes advanced language models more accessible and cost-effective. Ethical Concerns. We declare no conflicts of interest that could inappropriately influence our work. All experiments were conducted using publicly available resources. Our study does not involve human subjects, data collection from individuals, or experiments on protected groups. The models and basic datasets used in this work are publicly available and widely used in the research community. We have made efforts to ensure our experimental design and reporting of results are fair, unbiased, and do not misrepresent the capabilities or limitations of the methods presented. Reproducibility. For openness of LLM research, we declare our code and the CoT enhanced crafted finetuning datsets will be made available to ensure reproducibility. We will provide detailed documents of code implemnetation. And we have provided the details of all hyper-parameters of implementing Mediator and optimizing baselines. Potential Applications. The technology may have significant potential across specialized vertical domains. Considering that many vertical domains, personalized LLM agents (Li et al., 2024e), LLM applications like roleplay chatting (Chan et al., 2024; Yu et al., 2024a) and professional domain-specific writing (Gómez-Rodríguez & Williams, 2023), an LLM service provider may need to simultaneously deploy different finetuned LLMs. Our technology enables efficient and effective serving multiple popular LLM applications, and merging knowledge from different LLMs together."
        },
        {
            "title": "References",
            "content": "Baai/industryinstruction ecoURL https://huggingface.co/ finance nomics. datasets/BAAI/IndustryInstruction_ Finance-Economics. - - Al-Shedivat, M., Gillenwater, J., Xing, E., and Rostamizadeh, A. Federated learning via posterior averaging: new perspective and practical algorithms. International Conference on Learning Representations, 2020. Albuquerque, I., Monteiro, J., Darvishi, M., Falk, T. H., and 9 Mitliagkas, I. Adversarial target-invariant representation learning for domain generalization. 2020. Alizadeh, M., Kubli, M., Samei, Z., Dehghani, S., Bermeo, J. D., Korobeynikova, M., and Gilardi, F. Open-source large language models outperform crowd workers and approach chatgpt in text-annotation tasks. arXiv preprint arXiv:2307.02179, 101, 2023. Anonymous. Drop-upcycling: Training sparse mixture of In Submitted to experts with partial re-initialization. The Thirteenth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=gx1wHnf5Vp. under review. Arjovsky, M., Bottou, L., Gulrajani, I., and LopezInvariant risk minimization. arXiv preprint Paz, D. arXiv:1907.02893, 2019. Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., and Sutton, C. Program synthesis with large language models, 2021. URL https://arxiv.org/abs/2108. 07732. Babakniya, S., Elkordy, A. R., Ezzeldin, Y. H., Liu, Q., Song, K.-B., El-Khamy, M., and Avestimehr, S. Slora: Federated parameter efficient fine-tuning of language models. 08 2023. URL https://arxiv.org/pdf/ 2308.06522.pdf. Bai, G., Chai, Z., Ling, C., Wang, S., Lu, J., Zhang, N., Shi, T., Yu, Z., Zhu, M., Zhang, Y., et al. Beyond efficiency: systematic survey of resource-efficient large language models. arXiv preprint arXiv:2401.00625, 2024. Bao, Z., Chen, W., Xiao, S., Ren, K., Wu, J., Zhong, C., Peng, J., Huang, X., and Wei, Z. Disc-MedLLM: Bridging General Large Language Models and Real-World Medical Consultation. arXiv, abs/2308.14346, 2023. Belilovsky, E., Eickenberg, M., and Oyallon, E. Greedy layerwise learning can scale to imagenet. In International conference on machine learning, pp. 583593. PMLR, 2019. Belilovsky, E., Eickenberg, M., and Oyallon, E. Decoupled greedy learning of cnns. In International Conference on Machine Learning, pp. 736745. PMLR, 2020. Bengio, Y., Lamblin, P., Popovici, D., and Larochelle, H. Greedy layer-wise training of deep networks. Advances in neural information processing systems, 19, 2006. Bengio, Y., Courville, A., and Vincent, P. Representation learning: review and new perspectives. IEEE TPAMI, 35(8):17981828, 2013. Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing Blundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra, D. Weight uncertainty in neural networks. arXiv.org, 2015. Bordes, A., Chopra, S., and Weston, J. Question answering with subgraph embeddings, 2014. URL https: //arxiv.org/abs/1406.3676. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. NeurIPS, 33:18771901, 2020. Bühlmann, P. Invariance, causality and robustness. arXiv preprint arXiv:1812.08233, 2018. Chan, X., Wang, X., Yu, D., Mi, H., and Yu, D. Scaling synthetic data creation with 1,000,000,000 personas. arXiv preprint arXiv:2406.20094, 2024. Chee, J., Cai, Y., Kuleshov, V., and Sa, C. D. QuIP: 2-bit quantization of large language models with guarantees. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview. net/forum?id=xrk9g5vcXR. Chen, J., Hu, Q., Zhong, F., Zhuang, Y., and Xu, M. Upcycling noise for federated unlearning. 12 2024. URL https://arxiv.org/pdf/2412.05529.pdf. Chen, L., Li, S., Yan, J., Wang, H., Gunaratna, K., Yadav, V., Tang, Z., Srinivasan, V., Zhou, T., Huang, H., et al. Alpagasus: Training better alpaca with fewer data. arXiv preprint arXiv:2307.08701, 2023. Chen, M., Tworek, J., Jun, H., Yuan, Q., and etc. Evaluating large language models trained on code, 2021. URL https://arxiv.org/abs/2107.03374. Chen, N., Wang, Y., Deng, Y., and Li, J. The oscars of ai theater: survey on role-playing with language models, 2025. URL https://arxiv.org/abs/2407. 11484. Chen, Z., Gao, Q., Bosselut, A., Sabharwal, A., and Richardson, K. Disco: Distilling counterfactuals with large language models. arXiv preprint arXiv:2212.10534, 2022. Chi, Z., Dong, L., Huang, S., Dai, D., Ma, S., Patra, B., Singhal, S., Bajaj, P., Song, X., Mao, X.-L., Huang, H., and Wei, F. On the representation collapse of sparse mixture of experts. 04 2022. URL https://arxiv. org/pdf/2204.09179.pdf. Cho, H., Park, C., Kang, J., Yoo, K. M., Kim, T., and Lee, S.-g. Enhancing out-of-distribution detection in natural language understanding via implicit layer ensemble. In EMNLP, 2022. Choudhary, T., Mishra, V., Goswami, A., and Sarangapani, J. comprehensive survey on model compression and acceleration. Artificial Intelligence Review, 53:51135155, 2020. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems, 2021. URL https://arxiv. org/abs/2110.14168. Coignion, T., Quinton, C., and Rouvoy, R. performance In Proceedstudy of llm-generated code on leetcode. ings of the 28th International Conference on Evaluation and Assessment in Software Engineering, EASE 2024, pp. 7989. ACM, June 2024. doi: 10.1145/3661167.3661221. URL http://dx.doi.org/10.1145/3661167. 3661221. Creager, E., Jacobsen, J.-H., and Zemel, R. Environment inference for invariant learning. In ICML, pp. 21892200. PMLR, 2021. Crisostomi, D., Fumero, M., Baieri, D., Bernard, F., and Rodolà, E. Cycle-consistent multi-model merging. 05 2024. URL https://arxiv.org/pdf/2405. 17897.pdf. Daheim, N., Möllenhoff, T., Ponti, E., Gurevych, I., and Khan, M. E. Model merging by uncertainty-based gradient matching. In ICLR, 2024. Dai, D., Deng, C., Zhao, C., Xu, R. X., Gao, H., Chen, D., Li, J., Zeng, W., Yu, X., Wu, Y., Xie, Z., Li, Y. K., Huang, P., Luo, F., Ruan, C., Sui, Z., and Liang, W. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models. 01 2024. URL https://arxiv.org/pdf/2401.06066.pdf. Davari, M. and Belilovsky, E. Model breadcrumbs: Scaling multi-task model merging with sparse masks. arXiv preprint arXiv:2312.06795, 2023. Deep, P. T., Bhardwaj, R., and Poria, S. Della-merging: Reducing interference in model merging through magnitudebased sampling. arXiv preprint arXiv:2406.11617, 2024. Dettmers, T., Svirschevski, R. A., Egiazarian, V., Kuznedelev, D., Frantar, E., Ashkboos, S., Borzunov, A., Hoefler, T., and Alistarh, D. SpQR: sparse-quantized representation for near-lossless LLM weight compression. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=Q1u25ahSuy. Dixit, T., Paranjape, B., Hajishirzi, H., and Zettlemoyer, L. Core: retrieve-then-edit framework for counterfactual data generation. arXiv preprint arXiv:2210.04873, 2022. 10 Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing Dong, P., Li, L., Tang, Z., Liu, X., Pan, X., Wang, Q., and Chu, X. Pruner-zero: Evolving symbolic pruning metric from scratch for large language models. In Forty-first International Conference on Machine Learning. Dong, P., Li, L., Zhong, Y., Du, D., Fan, R., Chen, Y., Tang, Z., Wang, Q., Xue, W., Guo, Y., and Chu, X. Stbllm: Breaking the 1-bit barrier with structured binary llms, 2024. URL https://arxiv.org/abs/ 2408.01803. Dong, Q., Li, L., Dai, D., Zheng, C., Wu, Z., Chang, B., Sun, X., Xu, J., and Sui, Z. survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022. Du, G., Lee, J., Li, J., Jiang, R., Guo, Y., Yu, S., Liu, H., Goh, S. K., Tang, H.-K., He, D., and Zhang, M. Parameter competition balancing for model merging. NIIPS, 10 2024. URL https://arxiv.org/pdf/2410. 02396.pdf. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Duchi, J. and Namkoong, H. Learning models with uniform performance via distributionally robust optimization. arXiv preprint arXiv:1810.08750, 2018. Dunlap, L., Umino, A., Zhang, H., Yang, J., Gonzalez, J. E., and Darrell, T. Diversify your vision datasets with automatic diffusion-based augmentation. Advances in neural information processing systems, 36:7902479034, 2023. Eldan, R. and Li, Y. Tinystories: How small can language models be and still speak coherent english? arXiv preprint arXiv:2305.07759, 2023. Fang, J., Liu, C.-T., Kim, J., Bhedaru, Y., Liu, E., Singh, N., Lipka, N., Mathur, P., Ahmed, N. K., Dernoncourt, F., Rossi, R. A., and Deilamsalehy, H. Multi-llm text summarization, 2024. URL https://arxiv.org/ abs/2412.15487. Fei, G. and Liu, B. Breaking the closed world assumption in text classification. In NAACL, 2016. Frantar, E. and Alistarh, D. Optimal brain compression: framework for accurate post-training quantization and pruning. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. Frantar, E. and Alistarh, D. Sparsegpt: Massive language models can be accurately pruned in one-shot. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp. 1032310337. PMLR, 2023. URL https://proceedings.mlr.press/ v202/frantar23a.html. Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. OPTQ: Accurate quantization for generative pre-trained In The Eleventh International Confertransformers. ence on Learning Representations, 2023. URL https: //openreview.net/forum?id=tcbBPnfwxS. Fu, Y., Bailis, P., Stoica, I., and Zhang, H. Break the sequential dependency of llm inference using lookahead decoding. In Forty-first International Conference on Machine Learning. Gilardi, F., Alizadeh, M., and Kubli, M. Chatgpt outperforms crowd workers for text-annotation tasks. Proceedings of the National Academy of Sciences, 120(30): e2305016120, 2023. Gomez, A. N., Key, O., Perlin, K., Gou, S., Frosst, N., Dean, J., and Gal, Y. Interlocking backpropagation: Improving depthwise model-parallelism. The Journal of Machine Learning Research, 23(1):77147741, 2022. Gong, R., Li, W., Chen, Y., and Gool, L. V. Dlow: Domain In CVPR, pp. flow for adaptation and generalization. 24772486, 2019. Goodfellow, I. J., Shlens, J., and Szegedy, C. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. Guha, N., Talwalkar, A., and Smith, V. One-shot federated learning. arXiv preprint arXiv:1902.11175, 2019. Gunasekar, S., Zhang, Y., Aneja, J., Mendes, C. C. T., Del Giorno, A., Gopi, S., Javaheripi, M., Kauffmann, P., de Rosa, G., Saarikivi, O., et al. Textbooks are all you need. arXiv preprint arXiv:2306.11644, 2023. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Gómez-Rodríguez, C. and Williams, P. confederacy of models: comprehensive evaluation of llms on creative writing, 2023. URL https://arxiv.org/abs/ 2310.08433. Hahn, M. and Goyal, N. theory of emergent incontext learning as implicit structure induction. arxiv, arXiv:2303.07971, 2023. URL https://arxiv. org/abs/2303.07971. 11 Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing Han, S., Pool, J., Tran, J., and Dally, W. Learning both weights and connections for efficient neural network. In Cortes, C., Lawrence, N., Lee, D., Sugiyama, M., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015. He, C., Li, S., Soltanolkotabi, M., and Avestimehr, S. Pipetransformer: Automated elastic pipelining for distributed training of large-scale models. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 41504159. PMLR, 1824 Jul 2021. He, E., Khattar, A., Prenger, R., Korthikanti, V., Yan, Z., Liu, T., Fan, S., Aithal, A., Shoeybi, M., and Catanzaro, B. Upcycling large language models into mixture of experts. 10 2024a. URL https://arxiv.org/pdf/2410. 07524.pdf. He, Y. and Xiao, L. Structured pruning for deep convolutional neural networks: survey. TPAMI, 2023. He, Y., Hu, Y., Lin, Y., Zhang, T., and Zhao, H. Localizeand-stitch: Efficient model merging via sparse task arithmetic. arXiv preprint arXiv:2408.13656, 2024b. He, Y., Hu, Y., Lin, Y., Zhang, T., and Zhao, H. Localizeand-stitch: Efficient model merging via sparse task Transactions on Machine Learning Rearithmetic. ISSN 2835-8856. URL https:// search, 2025. openreview.net/forum?id=9CWU8Oi86d. Hendrycks, D. and Gimpel, K. baseline for detecting misclassified and out-of-distribution examples in neural networks. International Conference on Learning Representations, 2016. Hendrycks, D., Mazeika, M., and Dietterich, T. Deep anomaly detection with outlier exposure. In ICLR, 2019. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding, 2021. URL https: //arxiv.org/abs/2009.03300. Hershcovitch, M., Wood, A., Choshen, L., Girmonsky, G., Leibovitz, R., Ennmouri, I., Malka, M., Chin, P., Sundararaman, S., and Harnik, D. Zipnn: Lossless compression for ai models. arXiv preprint arXiv:2411.05239, 2024. Hinton, G. E., Osindero, S., and Teh, Y.-W. fast learning algorithm for deep belief nets. Neural computation, 18 (7):15271554, 2006. Hinton, G. E., Vinyals, O., and Dean, J. Distilling the knowledge in neural network. arXiv.org, 2015. Huang, C., Liu, Q., Lin, B. Y., Pang, T., Du, C., and Lin, M. Lorahub: Efficient cross-task generalization via dynamic lora composition. 07 2023a. URL https://arxiv. org/pdf/2307.13269.pdf. Huang, C., Ye, P., Chen, T., He, T., Yue, X., and Ouyang, W. Emr-merging: Tuning-free high-performance model merging. arXiv preprint arXiv:2405.17461, 2024. Huang, Y., Bai, Y., Zhu, Z., Zhang, J., Zhang, J., Su, T., Liu, J., Lv, C., Zhang, Y., Lei, J., Fu, Y., Sun, M., and He, J. C-eval: multi-level multi-discipline chinese evaluation suite for foundation models, 2023b. URL https://arxiv.org/abs/2305.08322. Ilharco, G., Ribeiro, M. T., Wortsman, M., Gururangan, S., Schmidt, L., Hajishirzi, H., and Farhadi, A. Editing models with task arithmetic. 12 2022. URL https: //arxiv.org/pdf/2212.04089.pdf. Ilharco, G., Ribeiro, M. T., Wortsman, M., Schmidt, L., Hajishirzi, H., and Farhadi, A. Editing models with task arithmetic. In ICLR, 2023. Izmailov, P., Wilson, A., Podoprikhin, D., Vetrov, D., and Garipov, T. Averaging weights leads to wider optima and better generalization. In UAI, pp. 876885, 2018. Jacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E. Adaptive mixtures of local experts. Neural computation,, 3(1):7987, 1991. Jhunjhunwala, D., Wang, S., and Joshi, G. Towards theoretical and practical understanding of one-shot federated learning with fisher information. In Federated Learning and Analytics in Practice: Algorithms, Systems, Applications, and Opportunities, 2023. Jhunjhunwala, D., Jali, N., Joshi, G., and Wang, S. Erasure coded neural network inference via fisher averaging. In ISIT, pp. 1318. IEEE, 2024a. Jhunjhunwala, D., Wang, S., and Joshi, G. Fedfisher: Leveraging fisher information for one-shot federated learning. In AISTATS, pp. 16121620. PMLR, 2024b. Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., de las Casas, D., Hanna, E. B., Bressand, F., Lengyel, G., Bour, G., Lample, G., Lavaud, L. R., Saulnier, L., Lachaux, M.-A., Stock, P., Subramanian, S., Yang, S., Antoniak, S., Scao, T. L., Gervet, T., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. Mixtral of experts. 01 2024a. URL https: //arxiv.org/pdf/2401.04088.pdf. 12 Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing Jiang, J., Wang, F., Shen, J., Kim, S., and Kim, S. survey on large language models for code generation, 2024b. URL https://arxiv.org/abs/2406.00515. Jin, X., Ren, X., Preotiuc-Pietro, D., and Cheng, P. Dataless knowledge fusion by merging weights of language models. ICLR, 12 2023. URL https://arxiv.org/ pdf/2212.09849.pdf. Jordan, M. I. and Jacobs, R. A. Hierarchical mixtures of experts and the em algorithm. Neural computation, 6(2): 181 214, 1994. Joshi, M., Choi, E., Weld, D. S., and Zettlemoyer, L. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension, 2017. URL https: //arxiv.org/abs/1705.03551. Kamath, A., Jia, R., and Liang, P. Selective question answering under domain shift. In ACL, 2020. Kang, J., Karlinsky, L., Luo, H., Wang, Z., Hansen, J., Glass, J., Cox, D., Panda, R., Feris, R., and Ritter, A. Self-moe: Towards compositional large language models with selfspecialized experts. arXiv preprint arXiv:2406.12034, 2024. Kendall, A. and Gal, Y. What uncertainties do we need in bayesian deep learning for computer vision? In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. Kim, B.-K., Kim, G., Kim, T.-H., Castells, T., Choi, S., Shin, J., and Song, H.-K. Shortened llama: simple depth pruning for large language models. ICLR Workshop on Mathematical and Empirical Understanding of Foundation Models (ME-FoMo), 2024. Kim, J.-K. and Kim, Y.-B. Joint learning of domain classification and out-of-domain detection with dynamic class weighting for satisficing false acceptance rates. arXiv preprint arXiv:1807.00072, 2018. Kim, S., Hooper, C., Gholami, A., Dong, Z., Li, X., Shen, S., Mahoney, M. W., and Keutzer, K. Squeezellm: Denseand-sparse quantization. CoRR, abs/2306.07629, 2023. doi: 10.48550/ARXIV.2306.07629. URL https:// doi.org/10.48550/arXiv.2306.07629. Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):35213526, 2017. Kleijn and der Vaart, V. The bernstein-von-mises theorem under misspecification. Electronic Journal of Statistics, 6:354381, 2012. URL https://api. semanticscholar.org/CorpusID:85548207. Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35: 2219922213, 2022. Kong, F., Zhang, R., and Wang, Z. Activated parameter locating via causal intervention for model merging. arXiv preprint arXiv:2408.09485, 2024. Kulkarni, M. and Karande, S. Layer-wise training of deep networks using kernel similarity. arXiv preprint arXiv:1703.07115, 2017. Lakshminarayanan, B., Pritzel, A., and Blundell, C. Simple and scalable predictive uncertainty estimation using deep ensembles. In Neural Information Processing Systems, 2016. Larson, S., Mahendran, A., Peper, J. J., Clarke, C., Lee, A., Hill, P., Kummerfeld, J. K., Leach, K., Laurenzano, M. A., Tang, L., et al. An evaluation dataset for intent classification and out-of-scope prediction. In EMNLPIJCNLP, 2019. Lee, C., Jin, J., Kim, T., Kim, H., and Park, E. OWQ: outlier-aware weight quantization for efficient fine-tuning and inference of large language models. In Wooldridge, M. J., Dy, J. G., and Natarajan, S. (eds.), Thirty-Eighth AAAI Conference on Artificial Intelligence, pp. 13355 13364. AAAI Press, 2024. doi: 10.1609/AAAI.V38I12. 29237. URL https://doi.org/10.1609/aaai. v38i12.29237. Lee, K., Lee, H., Lee, K., and Shin, J. Training confidencecalibrated classifiers for detecting out-of-distribution samples. In ICLR, 2018. Lee, N., Ajanthan, T., and Torr, P. SNIP: SINGLE-SHOT NETWORK PRUNING BASED ON CONNECTION SENSITIVITY. In International Conference on Learning Representations, 2019. URL https://openreview. net/forum?id=B1VZqjAcYX. Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N., and Chen, Z. Gshard: Scaling giant models with conditional computation and automatic sharding. 06 2020. URL https://arxiv. org/pdf/2006.16668.pdf. Lewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V., Slone, A., Anil, C., Schlag, I., Gutman-Solo, T., et al. Solving quantitative reasoning problems with language models. Advances in 13 Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing Neural Information Processing Systems, 35:38433857, 2022. Li, C., Wang, W., Hu, J., Wei, Y., Zheng, N., Hu, H., Zhang, Z., and Peng, H. Common 7b language models already possess strong math capabilities. arXiv preprint arXiv:2403.04706, 2024a. Li, H., Pan, S. J., Wang, S., and Kot, A. C. Domain generalization with adversarial feature learning. In CVPR, pp. 54005409, 2018. Li, J., Hou, Y., Sachan, M., and Cotterell, R. What do language models learn in context? the structured task hypothesis. arXiv preprint arXiv:2406.04216, 2024b. Li, M., Gururangan, S., Dettmers, T., Lewis, M., Althoff, T., Smith, N. A., and Zettlemoyer, L. Branch-train-merge: Embarrassingly parallel training of expert language models. arXiv preprint arXiv:2208.03306, 2022. Li, P., Yin, L., and Liu, S. Mix-ln: Unleashing the power of deeper layers by combining pre-ln and post-ln. 12 2024c. URL https://arxiv.org/pdf/2412. 13795.pdf. Li, P., Zhang, Z., Yadav, P., Sung, Y.-L., Cheng, Y., Bansal, M., and Chen, T. Merge, then compress: Demystify efficient smoe with hints from its routing policy. ICLR, 2024d. Li, X., Yu, P., Zhou, C., Schick, T., Zettlemoyer, L., Levy, O., Weston, J., and Lewis, M. Self-alignment with instruction backtranslation. arXiv preprint arXiv:2308.06259, 2023a. Li, Y., Bubeck, S., Eldan, R., Del Giorno, A., Gunasekar, S., and Lee, Y. T. Textbooks are all you need ii: phi1.5 technical report. arXiv preprint arXiv:2309.05463, 2023b. Li, Y., Wen, H., Wang, W., Li, X., Yuan, Y., Liu, G., Liu, J., Xu, W., Wang, X., Sun, Y., Kong, R., Wang, Y., Geng, H., Luan, J., Jin, X., Ye, Z., Xiong, G., Zhang, F., Li, X., Xu, M., Li, Z., Li, P., Liu, Y., Zhang, Y.-Q., and Liu, Y. Personal llm agents: Insights and survey about the capability, efficiency and security, 2024e. URL https: //arxiv.org/abs/2401.05459. Liang, S., Li, Y., and Srikant, R. Enhancing the reliability of out-of-distribution image detection in neural networks. International Conference on Learning Representations, 2017. Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., and Han, S. AWQ: activation-aware weight quantization for LLM compression and acceleration. CoRR, abs/2306.00978, 2023. doi: 10.48550/arXiv.2306.00978. URL https: //doi.org/10.48550/arXiv.2306.00978. Liu, D., Wang, Z., Wang, B., Chen, W., Li, C., Tu, Z., Chu, D., Li, B., and Sui, D. Checkpoint merging via bayesian optimization in llm pretraining. arXiv preprint arXiv:2403.19390, 2024a. Liu, H., Zhang, Y., Luo, Y., and Yao, A. C. Augmenting Math Word Problems via Iterative Question Composing. In ICLR 2024 Workshop on Navigating and Addressing Data Problems for Foundation Models, 2024b. Liu, L., Jiang, X., Zheng, F., Chen, H., Qi, G.-J., Huang, H., and Shao, L. bayesian federated learning framework with online laplace approximation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021. Liu, W., Wang, X., Owens, J. D., and Li, Y. Energy-based out-of-distribution detection. Neural Information Processing Systems, 2020. Liu, X., Liu, L., Ye, F., Shen, Y., Li, X., Jiang, L., and Li, J. Fedlpa: One-shot federated learning with layer-wise posterior aggregation. Neural Information Processing Systems, 2024c. Locatello, F., Bauer, S., Lucic, M., Raetsch, G., Gelly, S., Schölkopf, B., and Bachem, O. Challenging common assumptions in the unsupervised learning of disentangled representations. In ICML, pp. 41144124. PMLR, 2019. Lu, Z., Fan, C., Wei, W., Qu, X., Chen, D., and Cheng, Y. Twin-merging: Dynamic integration of modular expertise in model merging. NIPS, 06 2024a. URL https:// arxiv.org/pdf/2406.15479v2.pdf. Lu, Z., Fan, C., Wei, W., Qu, X., Chen, D., and Cheng, Y. Twin-merging: Dynamic integration of modular expertise in model merging. arXiv preprint arXiv:2406.15479, 2024b. Luo, Z., Xu, C., Zhao, P., Sun, Q., Geng, X., Hu, W., Tao, C., Ma, J., Lin, Q., and Jiang, D. Wizardcoder: Empowering Code Large Language Models with Evol-Instruct. In International Conference on Learning Representations (ICLR), 2024. Ma, X., Fang, G., and Wang, X. LLM-pruner: On the structural pruning of large language models. In Thirtyseventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/ forum?id=J8Ajf9WfXP. Maddox, W. J., Izmailov, P., Garipov, T., Vetrov, D. P., and Wilson, A. G. simple baseline for bayesian uncertainty in deep learning. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alché-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing Mahabadi, R. K., Belinkov, Y., and Henderson, J. Variational information bottleneck for effective low-resource fine-tuning. arXiv preprint arXiv:2106.05469, 2021. Marcel, S. and Rodriguez, Y. Torchvision the machinevision package of torch. In ACM MM, pp. 14851488, 2010. Marczak, D., Twardowski, B., Trzcinski, T., and Cygert, S. Magmax: Leveraging model merging for seamless continual learning. In ECCV, 2024. Matena, M. S. and Raffel, C. A. Merging models with fisher-weighted averaging. NeurIPS, 35:1770317716, 2022. McMahan, B., Moore, E., Ramage, D., Hampson, S., and Arcas, B. A. Communication-efficient learning of deep networks from decentralized data. In AISTATS, pp. 1273 1282. PMLR, 2017. Mitra, A., Khanpour, H., Rosset, C., and Awadallah, A. Orca-math: Unlocking the potential of slms in grade school math, 2024. URL https://arxiv.org/ abs/2402.14830. Molchanov, P., Mallya, A., Tyree, S., Frosio, I., and Kautz, Importance estimation for neural network pruning. J. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 1620, 2019, pp. 1126411272. Computer Vision Foundation / IEEE, 2019. doi: 10.1109/CVPR.2019.01152. Mostafa, H., Ramesh, V., and Cauwenberghs, G. Deep supervised learning using local errors. Frontiers in neuroscience, 12:608, 2018. Mou, Y., Wang, P., He, K., Wu, Y., Wang, J., Wu, W., and Xu, W. Uninl: Aligning representation learning with scoring function for ood detection via unified neighborhood learning. In EMNLP, 2022. Muandet, K., Balduzzi, D., and Schölkopf, B. Domain generalization via invariant feature representation. In ICML, 2013. Pan, R., Liu, X., Diao, S., Pi, R., Zhang, J., Han, C., and Zhang, T. Lisa: Layerwise importance sampling for memory-efficient large language model fine-tuning. 03 2024. URL https://arxiv.org/pdf/2403. 17919.pdf. Park, G., park, B., Kim, M., Lee, S., Kim, J., Kwon, B., Kwon, S. J., Kim, B., Lee, Y., and Lee, D. LUT-GEMM: Quantized matrix multiplication based on LUTs for efficient inference in large-scale generative language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=gLARhFLE0F. Pires, B. Á. and Szepesvári, C. Multiclass classification calibration functions. arXiv preprint arXiv:1609.06385, 2016. Qu, Z., Li, X., Duan, R., Liu, Y., Tang, B., and Lu, Z. Generalized federated learning via sharpness aware minimization. In International Conference on Machine Learning, pp. 1825018280. PMLR, 2022. Raghu, M., Gilmer, J., Yosinski, J., and Sohl-Dickstein, J. Svcca: Singular vector canonical correlation analysis for deep learning dynamics and interpretability, 2017. URL https://arxiv.org/abs/1706.05806. Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. Squad: 100,000+ questions for machine comprehension of text, 2016. URL https://arxiv.org/abs/ 1606.05250. Rastegari, M., Ordonez, V., Redmon, J., and Farhadi, A. Xnor-net: Imagenet classification using binary convolutional neural networks. In Leibe, B., Matas, J., Sebe, N., and Welling, M. (eds.), Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV, volume 9908 of Lecture Notes in Computer Science, pp. 525542. Springer, 2016. doi: 10.1007/ 978-3-319-46493-0_32. URL https://doi.org/ 10.1007/978-3-319-46493-0_32. Robins, A. Catastrophic forgetting, rehearsal and pseudoreMuqeeth, M., Liu, H., and Raffel, C. Soft merging of experts hearsal. Connection Science, 1995. with adaptive routing. TMLR, 2024. Nøkland, A. and Eidnes, L. H. Training neural networks with local error signals. In International conference on machine learning, pp. 48394850. PMLR, 2019. Ovadia, Y., Fertig, E., Ren, J. J., Nado, Z., Sculley, D., Nowozin, S., Dillon, J. V., Lakshminarayanan, B., and Snoek, J. Can you trust your models uncertainty? evaluating predictive uncertainty under dataset shift. Neural Information Processing Systems, 2019. Ruis, L., Mozes, M., Bae, J., Kamalakara, S. R., Talupuru, D., Locatelli, A., Kirk, R., Rocktäschel, T., Grefenstette, E., and Bartolo, M. Procedural knowledge in pretraining drives reasoning in large language models, 2024. URL https://arxiv.org/abs/2411.12580. Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y. Winogrande: An adversarial winograd schema challenge at scale, 2019. URL https://arxiv.org/abs/ 1907.10641. 15 Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing Samsi, S., Zhao, D., McDonald, J., Li, B., Michaleas, A., Jones, M., Bergeron, W., Kepner, J., Tiwari, D., and Gadepally, V. From words to watts: Benchmarking the energy costs of large language model inference, 2023. Shao, H., Liu, B., and Qian, Y. One-shot sensitivity-aware mixed sparsity pruning for large language models. In ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 1129611300, 2024. Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., and Dean, J. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. 01 2017. URL https://arxiv.org/pdf/1701. 06538.pdf. Shen, L., Tang, Z., Wu, L., Zhang, Y., Chu, X., Qin, T., and Han, B. Hot pluggable federated learning. In International Workshop on Federated Foundation Models in Conjunction with NeurIPS 2024, 2024. URL https: //openreview.net/forum?id=FazIrAXoM6. Shoemake, K. Animating rotation with quaternion curves. In Proceedings of the 12th annual conference on Computer graphics and interactive techniques, pp. 245254, 1985. Sicilia, A., Zhao, X., and Hwang, S. J. Domain adversarial neural networks for domain generalization: When it works and how to improve. arXiv preprint arXiv:2102.03924, 2021. Siddiqui, S. A., Krueger, D., LeCun, Y., and Deny, S. Blockwise self-supervised learning at scale. arXiv preprint arXiv:2302.01647, 2023. Singh, S. P. and Jaggi, M. Model fusion via optimal transport. NeurIPS, 33:2204522055, 2020. Steinwart, I. How to compare different loss functions and their risks. Constructive Approximation, 26:225287, 2007. URL https://api.semanticscholar. org/CorpusID:16660598. Stojkovic, J., Choukse, E., Zhang, C., Goiri, I., and Torrellas, J. Towards greener llms: Bringing energy-efficiency to the forefront of llm inference, 2024. URL https: //arxiv.org/abs/2403.20306. Sukhbaatar, S., Golovneva, O., Sharma, V., Xu, H., Lin, X. V., Rozière, B., Kahn, J., Li, D., tau Yih, W., Weston, J., and Li, X. Branch-train-mix: Mixing expert llms into mixture-of-experts llm. 03 2024a. URL https: //arxiv.org/pdf/2403.07816.pdf. Sukhbaatar, S., Golovneva, O., Sharma, V., Xu, H., Lin, X. V., Rozière, B., Kahn, J., Li, D., Yih, W.-t., Weston, J., et al. Branch-train-mix: Mixing expert llms into mixture-of-experts llm. arXiv preprint arXiv:2403.07816, 2024b. Sukhbaatar, S., Golovneva, O., Sharma, V., Xu, H., Lin, X. V., Roziere, B., Kahn, J., Li, S.-W., tau Yih, W., Weston, J. E., and Li, X. Branch-train-mix: Mixing expert In First ConLLMs into mixture-of-experts LLM. ference on Language Modeling, 2024c. URL https: //openreview.net/forum?id=nqLAuMOF6n. Sun, M., Liu, Z., Bair, A., and Kolter, J. Z. simple and effective pruning approach for large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=PxoFut3dWW. Sun, X., Li, X., Zhang, S., Wang, S., Wu, F., Li, J., Zhang, T., and Wang, G. Sentiment analysis through llm negotiations, 2023. URL https://arxiv.org/abs/ 2311.01876. Tang, A., Shen, L., Luo, Y., Ding, L., Hu, H., Du, B., and Tao, D. Concrete subspace learning based interference elimination for multi-task model fusion. arXiv preprint arXiv:2312.06173, 2023. Tang, A., Shen, L., Luo, Y., Liu, S., Hu, H., and Du, B. Towards efficient pareto set approximation via mixarXiv preprint ture of experts based model fusion. arXiv:2406.09770, 2024a. Tang, A., Shen, L., Luo, Y., Yin, N., Zhang, L., and Tao, D. Merging multi-task models via weight-ensembling ICML, 02 2024b. URL https: mixture of experts. //arxiv.org/pdf/2402.00433.pdf. Tang, A., Shen, L., Luo, Y., Yin, N., Zhang, L., and Tao, D. Merging multi-task models via weight-ensembling mixture of experts. ICML, 2024c. Tang, Z., Shi, S., Chu, X., Wang, W., and Li, B. Communication-efficient distributed deep learning: comprehensive survey. arXiv preprint arXiv:2003.06307, 2020. Tang, Z., Zhang, Y., Shi, S., He, X., Han, B., and Chu, X. Virtual homogeneity learning: Defending against data heterogeneity in federated learning. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 2111121132. PMLR, 1723 Jul 2022. Tang, Z., Zhang, Y., Dong, P., ming Cheung, Y., Zhou, A. C., Han, B., and Chu, X. Fusefl: One-shot federated learning through the lens of causality with progressive model fusion. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024d. URL https: //openreview.net/forum?id=E7fZOoiEKl. 16 Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing Tang, Z., Zhang, Y., Shi, S., Tian, X., Liu, T., Han, B., and Chu, X. Fedimpro: Measuring and improving client update in federated learning. In The Twelfth International Conference on Learning Representations, 2024e. URL https://openreview.net/forum? id=giU9fYGTND. Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B. Stanford alpaca: An instruction-following llama model, March 2023a. URL https://github.com/tatsu-lab/ stanford_alpaca. Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B. Stanford alpaca: An instruction-following llama https://github.com/tatsu-lab/ model. stanford_alpaca, 2023b. Thennal, D., Nathan, G., and Suchithra, M. Fisher mask nodes for language model merging. In LREC-COLING, pp. 73497355, 2024. Tishby, N. and Zaslavsky, N. Deep learning and the information bottleneck principle. In 2015 ieee information theory workshop (itw), pp. 15. IEEE, 2015. Tishby, N., Pereira, F. C., and Bialek, W. The information bottleneck method. arXiv preprint physics/0004057, 2000. Törnberg, P. Chatgpt-4 outperforms experts and crowd workers in annotating political twitter messages with zeroshot learning. arXiv preprint arXiv:2304.06588, 2023. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023. Utans, J. Weight averaging for neural networks and local resampling schemes. In AAAI Workshop, pp. 133138. Citeseer, 1996. Wang, H., Yurochkin, M., Sun, Y., Papailiopoulos, D., and Khazaeni, Y. Federated learning with matched averaging. In ICLR, 2020a. Wang, H., Yurochkin, M., Sun, Y., Papailiopoulos, D., and Khazaeni, Y. Federated learning with matched averaging. In International Conference on Learning Representations, 2020b. Wang, K., Dimitriadis, N., Ortiz-Jimenez, G., Fleuret, F., and Frossard, P. Localizing task information for improved model merging and compression. ICML, 2024. Wang, Y., Ni, Z., Song, S., Yang, L., and Huang, G. Revisiting locally supervised learning: an alternative to end-toend training. In International Conference on Learning Representations, 2020d. Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. Self-instruct: Aligning language models with self-generated instructions. In The 61st Annual Meeting Of The Association For Computational Linguistics, 2023. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Wei, T., Zhu, B., Zhao, L., Cheng, C., Li, B., Lü, W., Cheng, P., Zhang, J., Zhang, X., Zeng, L., Wang, X., Ma, Y., Hu, R., Yan, S., Fang, H., and Zhou, Y. Skywork-moe: deep dive into training techniques for mixture-of-experts language models. 06 2024a. URL https://arxiv. org/pdf/2406.06563.pdf. Wei, Y., Wang, Z., Liu, J., Ding, Y., and ZHANG, L. Magicoder: Empowering Code Generation with OSS-Instruct. In Forty-first International Conference on Machine Learning, 2024b. Wen, W., Wu, C., Wang, Y., Chen, Y., and Li, H. Learning structured sparsity in deep neural networks. In Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016. Wies, N., Levine, Y., and Shashua, A. The learnability of incontext learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. Wilson, A. and Izmailov, P. Bayesian deep learning and probabilistic perspective of generalization. Neural Information Processing Systems, 2020. Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., et al. Huggingfaces transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019. Wang, J., Liu, Q., Liang, H., Joshi, G., and Poor, H. V. Tackling the objective inconsistency problem in heterogeneous federated optimization. NeurIPS, 33:76117623, 2020c. Wu, M., Waheed, A., Zhang, C., Abdul-Mageed, M., and Aji, A. F. Lamini-lm: diverse herd of distilled models from large-scale instructions. arXiv preprint arXiv:2304.14402, 2023. 17 Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing Xie, S. M., Raghunathan, A., Liang, P., and Ma, T. An explanation of in-context learning as implicit Bayesian inference. In International Conference on Learning Representations, 2022. Yu, L., Yu, B., Yu, H., Huang, F., and Li, Y. Extend model merging from fine-tuned to pre-trained large language models via weight disentanglement. arXiv preprint arXiv:2408.03092, 2024a. Xin, H., Guo, D., Shao, Z., Ren, Z., Zhu, Q., Liu, B., Ruan, C., Li, W., and Liang, X. Deepseek-Prover: Advancing Theorem Proving in LLMs through Large-Scale Synthetic Data. arXiv, abs/2405.14333, 2024. Xiong, Y., Ren, M., and Urtasun, R. Loco: Local contrastive representation learning. Advances in neural information processing systems, 33:1114211153, 2020. Yu, L., Yu, B., Yu, H., Huang, F., and Li, Y. Language models are super mario: Absorbing abilities from homologous models as free lunch. ICML, 2024b. Yu, L., Yu, B., Yu, H., Huang, F., and Li, Y. Language models are super mario: Absorbing abilities from homologous models as free lunch. ICML, 11 2024c. URL https://arxiv.org/pdf/2311.03099.pdf. Xu, Z., Jiang, F., Niu, L., Deng, Y., Poovendran, R., Choi, Y., and Lin, B. Y. Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing. arXiv preprint arXiv:2406.08464, 2024. Yuan, L., Cui, G., Wang, H., Ding, N., Wang, X., Deng, J., Shan, B., Chen, H., Xie, R., Lin, Y., et al. Advancing llm reasoning generalists with preference trees. arXiv preprint arXiv:2404.02078, 2024. Yadav, P., Tam, D., Choshen, L., Raffel, C., and Bansal, M. Resolving interference when merging models. NeurIPS, 2023a. Yadav, P., Tam, D., Choshen, L., Raffel, C., and Bansal, M. Ties-merging: Resolving interference when merging models. 06 2023b. URL https://arxiv.org/ pdf/2306.01708.pdf. Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024a. Yang, E., Shen, L., Guo, G., Wang, X., Cao, X., Zhang, J., and Tao, D. Model merging in llms, mllms, and beyond: Methods, theories, applications and opportunities. arXiv preprint arXiv:2408.07666, 2024b. Yang, E., Wang, Z., Shen, L., Liu, S., Guo, G., Wang, X., and Tao, D. Adamerging: Adaptive model merging for ICLR, 10 2024c. URL https: multi-task learning. //arxiv.org/pdf/2310.02575.pdf. Yang, M., Liu, F., Chen, Z., Shen, X., Hao, J., and Wang, J. Causalvae: disentangled representation learning via neural structural causal models. In CVPR, pp. 95939602, 2021. Yin, P., Deng, B., Chen, E., Vasilescu, B., and Neubig, G. Learning to mine aligned code and natural language pairs from stack overflow. In International Conference on Mining Software Repositories, MSR, pp. 476486. ACM, 2018. doi: https://doi.org/10.1145/3196398.3196408. Yue, S., Chen, W., Wang, S., Li, B., Shen, C., Liu, S., Zhou, Y., Xiao, Y., Yun, S., Huang, X., and Wei, Z. DiscLawLLM: Fine-tuning Large Language Models for Intelligent Legal Services. arXiv, abs/2309.11325, 2023a. Yue, X., Qu, X., Zhang, G., Fu, Y., Huang, W., Sun, H., Su, Y., and Chen, W. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653, 2023b. Yurochkin, M., Agarwal, M., Ghosh, S., Greenewald, K., Hoang, N., and Khazaeni, Y. Bayesian nonparametIn Chaudric federated learning of neural networks. huri, K. and Salakhutdinov, R. (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 72527261. PMLR, 0915 Jun 2019a. URL https://proceedings.mlr.press/v97/ yurochkin19a.html. Yurochkin, M., Agarwal, M., Ghosh, S., Greenewald, K., Hoang, N., and Khazaeni, Y. Bayesian nonparametric federated learning of neural networks. In ICML, pp. 7252 7261. PMLR, 2019b. Zelikman, E., Wu, Y., Mu, J., and Goodman, N. STaR: Bootstrapping Reasoning With Reasoning. Advances in Neural Information Processing Systems, 35:15476 15488, 2022. Zeng, W., Xu, C., Zhao, Y., Lou, J.-G., and Chen, W. Automatic instruction evolving for large language models. arXiv preprint arXiv:2406.00770, 2024. You, J., Chung, J.-W., and Chowdhury, M. Zeus: Understanding and optimizing gpu energy consumption of dnn training, 2022. URL https://arxiv.org/abs/ 2208.06102. Zeng, Z., He, K., Yan, Y., Liu, Z., Wu, Y., Xu, H., Jiang, H., and Xu, W. Modeling discriminative representations for out-of-domain detection with supervised contrastive learning. In ACL, 2021a. 18 Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing Zeng, Z., Xu, H., He, K., Yan, Y., Liu, S., Liu, Z., and Xu, W. Adversarial generative distance-based classifier for robust out-of-domain detection. In ICASSP, 2021b. Zhang, Y., Zhao, L., Lin, M., Yunyun, S., Yao, Y., Han, X., Tanner, J., Liu, S., and Ji, R. Dynamic sparse no training: Training-free fine-tuning for sparse LLMs. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=1ndDmZdT4g. Zhao, Y., Huang, J., Hu, J., Wang, X., Mao, Y., Zhang, D., Jiang, Z., Wu, Z., Ai, B., Wang, A., Zhou, W., and Chen, Y. Swift:a scalable lightweight infrastructure for fine-tuning, 2024. URL https://arxiv.org/abs/ 2408.05517. Zhou, J., Lu, T., Mishra, S., Brahma, S., Basu, S., Luan, Y., Zhou, D., and Hou, L. Instruction-following evaluation for large language models, 2023. URL https: //arxiv.org/abs/2311.07911. Zhou, W., Liu, F., and Chen, M. Contrastive out-ofIn distribution detection for pretrained transformers. EMNLP, 2021. Zhou, Y., Lei, T., Liu, H., Du, N., Huang, Y., Zhao, V., Dai, A., Chen, Z., Le, Q., and Laudon, J. Mixture-of-experts with expert choice routing. 02 2022. URL https: //arxiv.org/pdf/2202.09368.pdf. Zhou, Y., Song, L., Wang, B., and Chen, W. Metagpt: Merging large language models using model exclusive task arithmetic. arXiv preprint arXiv:2406.11385, 2024. Zhu, D., Sun, Z., Li, Z., Shen, T., Yan, K., Ding, S., Kuang, K., and Wu, C. Model tailor: Mitigating catastrophic forgetting in multi-modal large language models. ICML, 2024. Zhu, Y., Zhang, P., Haq, E.-U., Hui, P., and Tyson, G. Can chatgpt reproduce human-generated labels? study of social computing tasks. arXiv preprint arXiv:2304.10145, 2023. 19 Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing"
        },
        {
            "title": "Appendix",
            "content": "A More Related Works A.1 Averaging-based Model Merging . A.2 Routing-based Model Merging. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Layer-wise Training Dynamics and Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.4 Bayesian Deep Learning . . . . . . A.5 OOD Detection and Generalization . A.6 Model Compression . A.7 Data Synthesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Theoretical Understanding"
        },
        {
            "title": "C Detailed Experiment Settigns",
            "content": "C.1 Detailed Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Hyperparameters of Finetuning and Implementing Baselines . . . . . . . . . . . . . . . . . . . . . . . . The variations and details of different parts of Mediator D.1 Measuring Parameter conflicts . D.2 Adaptive Merging . . . D.3 Averaging Operations . . . . . D.4 Details of Expert Routing . . . . . . . . . . System Optimization E.1 Cuda Kernal Merging . . . . . . E.2 Serving with Batched requests . Finetuning Data Generation F.1 Task-related Training Datasets . F.2 CoT based Data Augmentation . F.3 CoT based Data Augmentation . More Experiment Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.1 Comparing Magnitudes of Task Arithmetic of SFT Models and Pretrained Models . . . . . . . . . . . . . G.2 Detailed Evaluation of Model Merging Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.3 System Performance Analysis on NVIDIA RTX 4090 . . . . . . . . . . . . . . . . . . . . . . . . . . . . Real-world Case Studies 21 22 24 25 25 26 27 30 30 32 34 34 34 35 35 36 36 36 39 40 42 42 44 45 20 Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing A. More Related Works We introduce more related works about model merging and routing in this section. Current common methods include: (1) Averaging based merging. This direction combines multiple models into single model while preserving their capabilities with minimal or no additional training; (2) Routing based merging. It considers to route inputs to specialized expert modules like mixture-of-experts (MoE) approaches but completely different. Table 12 provides an overview of comparing different model merging methods and our framework. Given nτ different finetuned models, averaging based methods do not completely address the parameter conflicts, thus having higher parameter conficts than routing based merging. The weighted averaging requires calibration data to compute the importance metrics. The token-level routing requires routing for each layer, thus having totally nL routers. Our framework Mediator exploits layer-wise characteristics to both reduce parameter conflict and improve common knowledge fusion. And Mediator utilizes compression to further reduce the memory costs. To the best of our knowledge, the most of previous model merging works focus on experiments on traditional CV and NLP tasks (Matena & Raffel, 2022), while Mediator conducts experiments on modern LLMs and real-world experiments. Table 13 provides the system performance comparison. Because token-level routing like MoE requires to route each token towards different (possibly) experts, its inference cost is significantly large as more than nL times than task-level routing, where is the sequence length. Besides, the token-level routing requires more than nL times routers in memory costs. With the layer-wise adaptive averaging and Routing, Mediator significantly reduce the memory costs of from Mθ nτ to Mθ (cavg + croute nτ c) 1. The experimental memory reduction and the system performance comparisons are shown in the Section 5.1. We also provide system optimization to accelerate the inference during the deployment of Mediator in Section 4.4 and Appendix F. And the hyper-parameters cavg and croute are adaptively decided by the parameter conflict estimation. Table 12: Demystifying different merging methods. The nτ represents the number of finetuning tasks, nL the number of layers in the model. Method Type Parameter Conlict Level Merging Common Knowledge Require Calibration Data Routing Type Considering Layer-wise Characteristics Considering Compression Considering OOD Samples Experimental Scanario Basic Averaging Weighted Averaging Subspace Averaging Token-Level Routing Task-Level Routing Mediator High Middle Middle Low Low Low NA NA NA Token-level Task-level Task-level Traditional CV, NLP Traditional CV, NLP Traditional CV, NLP Traditional CV, NLP Traditional CV, NLP Generative LLMs Table 13: Demystifying different merging methods in system performance costs. Considering the memory costs of the base model and one router are Mθ and Mh, each layer occupies the same memory Ml, compression ratio c, the ratio of selected layers for averaging is cavg, for routing is croute, Pθ and BPθ are the forward time and backward time of the model. Ph and BPh are the forward time and backward time of the router. Method Type Requiring # of routers Costs of Trainig Routers Memory Costs After Merging Inference Cost Basic Averaging Weighted Averaging Subspace Averaging Token-Level Routing Task-Level Routing Mediator NA NA NA nL nτ nτ nτ NA NA NA High Middle Low Mθ Mθ Mθ Mθ nτ + Mh nL Mθ nτ + Mh Mθ (cavg + croute nτ c) + Mh Pθ Pθ Pθ Pθ + Ph nL nτ Pθ + Ph nτ Pθ + Ph nτ Besideds, we also review some highly related works include following directions that are closely related to our framework. Insights from these directions have provided valuable guidance for our framework. 1. Layer-wise training dynamics and optimization. This direction discusses the layer-wise training dynamics to help shed some light on the paramter conflicts and the layer-wise adaptivity. 2. Bayesian deep learning. This direction reviews some works of the Bayesian deep learning, discussing the uncertainty and Bayesian model averaging. 1Normally, each transformer layer occupies the same memory. 21 Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing 3. OOD Detection & Generalization. This direction reviews some works of the OOD Detection and Generalization, shedding light on deployment of the model merging on the out-of-distribution data. 4. Model compression. This direction shortly review some works about the model compression, in which many methods can be directly applied into our framework to further reduce the memory costs. Note that in our paper we propose general framework instead of new model compression method. Different model compression methods can be combined into our framework. 5. Data Synthesis. This direction reviews some works about how to generate new synthetic data to improve the model merging performance. In our framework, we exploit the CoT to generate new synthetic data to improve the finetuning performance on downstream tasks, which is real-world downstream task instead of traditional model fine-tuning using the in-domain training and testing data. A.1. Averaging-based Model Merging Model merging, also known as model fusion, combines the parameters of multiple separate models with different capabilities to create universal model. In this paper, we temporarily focus on models that have the same architecture but different parameters that are finetuned on different downstream tasks. Basic Averaging. The traditional approach to merge different trained or finetuned models is to evenly average the parameters of different models (Utans, 1996; Shoemake, 1985). This process does not require access to the original training data and allows for enhanced performance without the need for expensive computation. However, the performance of these simply weight averaging is generally unsatisfactory. Some related directions of model averaging also include Federated learning (FL) (McMahan et al., 2017; Tang et al., 2020). In FL, the model averaging is performed on the server side to reduce the communication costs after the local training. Many methods have been proposed to stable and smooth the model averaging process (Wang et al., 2020c; Jhunjhunwala et al., 2024b; Yurochkin et al., 2019b; Singh & Jaggi, 2020; Wang et al., 2020a; Tang et al., 2024e; 2022) to enhance the averaging performance. Different from the multi-rounds FL, the model merging is performed in single round, which is more similar to the one-shot FL (Guha et al., 2019; Tang et al., 2024d). Weighted Averaging. Rethinking the cause of the poor performance of the basic averaging method, many works propose to use the weighted averaging method to improve the merging performance. Intuitively, different model parameters have different importance on downstream tasks. Such heterogeneity of the parameter importance motivates other research directions including model sparsification (Sun et al., 2024; Dong et al.; 2024; Tang et al., 2020), continual learning (Robins, 1995; Kirkpatrick et al., 2017; Zhu et al., 2024; Marczak et al., 2024) and FL (Jhunjhunwala et al., 2024b; Yurochkin et al., 2019b; Singh & Jaggi, 2020). Thus, to avoid the important parameters being overwhelmed by the unimportant parameters, during averaging, we can assign large weights to those important parameters. To this end, the importance measurement is crucial. Many works propose to use fisrt or second orders of Taylor expansion to measure the importance of the parameters (Lee et al., 2019; Jhunjhunwala et al., 2023; Qu et al., 2022). Some works employ local linearization and task vectors to measure the importance of the parameters (Zhou et al., 2024). The fisher information also kind of importance measurement (Matena & Raffel, 2022; Jhunjhunwala et al., 2024a; Thennal et al., 2024; Jhunjhunwala et al., 2024b; Thennal et al., 2024; Daheim et al., 2024). While these importance measurement methods can improve the merging performance than the basic averaging method, they still face some typical challenges. 1. Require Calibration Dataset. The importance measurement is based on the calibration dataset. In the LLM era, the pretrained dataset is significantly large, it is difficult to collect the complete pretrained dataset and measure the importance of the parameters on it. 2. Computation Costs. Because that the importance measurement is based on the calibration dataset, the computation costs is almost similar to conduct the complete forward process of the different models. In traditional small models, such computaton cost is acceptable. However, in the LLM era, the model size is significantly large, such computation cost is unbearable. 3. Unaddressed Parameter Conflicts. While methods in these importance based weighted averaging methods can improve the merging performance, they still face the parameter conflicts between different models. Because of the highly Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing non-convex structure of the LLMs, it is difficult to find optimal merging method based on averaging the parameters of different models. Subspace Averaging. Considering that the neural networks are over-parameterized, removing most of the parameters from the model barely affects its accuracy (He & Xiao, 2023; Choudhary et al., 2020). Besides, during the training or finetuning, some parameters might be optimized towards random direction which has small impact on the model performance (Yadav et al., 2023a). Thus, works propose to firstly process different models in subspace manner. Then, the parameter conflicts can be mitigated by the subspace averaging methods (Deep et al., 2024; He et al., 2024b). DARE (Drop and Rescale) (Yu et al., 2024c;b) introduces parameter pruning and rescaling strategy that significantly reduces the number of parameters in SFT models while preserving their performance, thereby serving as an effective preprocessing step for model merging. Similarly, Model Breadcrumbs (Davari & Belilovsky, 2023) enhances sparsification by eliminating both low-magnitude parameters and outlier parameters with exceptionally high weights, thereby reducing noise and improving the generalization of hyperparameters during model merging. TALL-masks (Wang et al., 2024) creates task-specific mask matrices based on predefined thresholds tailored to individual models, while Model Tailor (Zhu et al., 2024) further refines this approach by masking parameters according to their sensitivity to loss changes and deviations from pre-trained values. APL (Kong et al., 2024) advances parameter importance estimation through causal interventions, providing robust metric for selective parameter retention. EMR-Merging (Huang et al., 2024) departs from traditional model merging by maintaining shared model across multiple tasks alongside sparse task-specific models, where each shared parameter is determined by the maximum value among corresponding parameters from all models. Concrete (Tang et al., 2023) further innovates by framing mask construction and model merging as learnable bi-level optimization problem, with the outer level optimizing the mask matrix and the inner level performing model merging and optimization utilizing unlabeled test samples. Task Arithmetic (Ilharco et al., 2022) exploits parameter-space arithmetic operations, treating model parameters as vectors and employing addition and subtraction to synthesize new model capabilities. However, many of these approaches, including DARE and Task Arithmetic, heavily rely on hyperparameters for parameter fusion, which can negatively impact the performance of model merging. Additionally, as highlighted in studies such as TIES (Yadav et al., 2023b) and Crisostomi (Crisostomi et al., 2024), model merging often encounters parameter conflicts that degrade performance when integrating multiple models. Addressing these challenges, TIES (Trim, Elect, and Disjoint Merge) (Yadav et al., 2023b) implements comprehensive approach by trimming parameters based on magnitude, selecting relevant weights, and disjointly merging weights using outcomes from task arithmetic operations. This methodology mitigates parameter conflicts and enhances the overall performance of the merged model, positioning TIES as robust solution in the domain of model merging. A.2. Routing-based Model Merging. Average-based methods primarily aim to enhance the averaging process of client models. However, the inherently non-linear architecture of deep neural networks complicates the derivation of globally comparable model through simple averaging. The basic, weighted-based, and subspace-based merging methods are static merging techniques. This implies that the merged model remains consistent across all samples or tasks. Given the variability among input samples and tasks, the models performance can fluctuate when processing diverse inputs. To this end, certain studies advocate for the dynamic merging of models (or subsets of layers) tailored to specific samples or tasks (Li et al., 2024d; Muqeeth et al., 2024; Tang et al., 2024c; Lu et al., 2024b; Kang et al., 2024; Tang et al., 2024d; Shen et al., 2024) during the inference phase. For each input instance, SMEAR (Muqeeth et al., 2024) initially computes weighted average of the parameters from each expert by leveraging the distribution of router inputs to the expert modules. This approach maintains computational cost comparable to that of single expert. Similarly, Twin-Merging (Lu et al., 2024b) adaptively integrates task-shared and task-specific knowledge based on routing mechanisms during inference. In the same vein, Weight-Ensembling MoE (Tang et al., 2024c) introduces dynamic merging Transformer architecture. This method identifies that the parameters of the linear layer in the fine-tuned model undergo more significant changes compared to the nonlinear layers, which adversely affects merging performance. Consequently, Weight-Ensembling MoE employs standard weighted average for all modules except the linear layer, which is dynamically weighted and merged based on the routing network (utilizing sample features as input and merging coefficients as output) during inference. PWE MoE (Tang et al., 2024a) extends Weight-Ensembling 23 Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing MoE to multi-objective optimization framework, incorporating the preference vector as an input for routing. AdaMerging (Yang et al., 2024c) adaptively learns merging coefficients in task-aware or layer-wise manner, offering an automated and unsupervised approach to task arithmetic. While this method significantly enhances performance, it incurs high computational costs. PCB Merge (Du et al., 2024) introduces parameter importance detection mechanism that accounts for parameter conflicts and employs heuristic algorithms to explore model fusion parameters, thereby achieving superior results. TwinMerge (Lu et al., 2024a) utilizes LoRA or SVD techniques in conjunction with supervised training for parameter fusion, resulting in improved performance. Nevertheless, these methods encounter inherent limitations. Both AdaMerging and PCB Merge utilize static fusion approaches, which can lead to performance degradation when the actual sample distribution varies during runtime. Meanwhile, TwinMerge performs parameter fusion at the task level; however, the application of LoRA and SVD matrix decomposition markedly reduces model accuracy and introduces substantial online computational overhead. Besides, the code implementation of the TwinMerge actually exploits the LoRA finetuning to replace SVD decomposition. Using SVD decomposition in compressing model parameters leads to disturbed LLMs and significantly degraded model performance. Mixture-of-Experts (MoE) (Jacobs et al., 1991; Jordan & Jacobs, 1994) is foundational model concatenation and routing strategy comprising multiple expert networks and router that dynamically selects relevant experts based on the input. This methodology has been extensively adopted in large language models, offering significant reductions in computational costs while preserving model performance. Recent studies, particularly sparse gated MoE (Shazeer et al., 2017) in transformerbased large language models (Lepikhin et al., 2020), have concentrated on maintaining load balancing among experts during training (Zhou et al., 2022; Jiang et al., 2024a), reducing training costs (Dai et al., 2024), and mitigating performance degradation due to uncoordinated expert training (Chi et al., 2022). Upcycling Methods have been developed to alleviate the high computational demands of training MoE models from scratch by initializing experts from existing dense models. These methods encompass copying existing dense models as experts (He et al., 2024a; Wei et al., 2024a), introducing noise to the MLP layers of dense models to create experts (Noise upcycling) (Chen et al., 2024), and drop upcycling (Anonymous, 2024), which combines parameter dropout with expert copying during training to enhance model robustness, reduce overfitting, and improve performance. Branch-Train-Merge (BTM) (Li et al., 2022) and Branch-Train-Mix (BTX) (Sukhbaatar et al., 2024c) are methodologies aimed at further optimizing model training efficiency. These approaches employ different SFT-trained dense models derived from the same base LLM as MoE experts. The experts are interconnected via router without necessitating additional training, while non-expert components are amalgamated through model merging techniques such as parameter averaging. Only the router undergoes training, thereby substantially reducing overall training costs. Although these methods achieve lower training expenses and marginally outperform traditional model merging approaches, our research indicates that token-level routing can partially degrade model performance. Additionally, maintaining all experts in GPU memory leads to significant parameter redundancy and escalates inference costs, which motivates our ongoing research endeavors. However, the token-level routing methods are not suitable for model merging. We have provided detailed discussions in the main text Section 4.1 and Appendix B. The token-level routing methods after merging normally require re-training based on all training datasets to obtian better token-level router, which significantly increases the computational costs, which is discussed in the main text Section 4.4 and Appendix E. LoRA based Routing. Routing samples to different LoRA experts is promising direction to dynamically route the input to different LoRA experts. This direction includes the LoraHub (Huang et al., 2023a) and sLora (Babakniya et al., 2023), which explore serving multiple LoRA adapters through techniques like unified paging and tensor parallelism. However, these methods do not consider the better dynamic expert merging method to further improve the model merging performance. In real-world applications, the input distribution is dynamic and the input samples are diverse, which motivates our ongoing research endeavors. Besides, their reliance on LoRA matrix decomposition significantly degrades model serving performance. Additionally, they do not consider model compression opportunities or the potential to average similar layers between models, which could further optimize storage and computation costs while maintaining model capabilities. A.3. Layer-wise Training Dynamics and Optimization Layer-wise training was initially explored to achieve effective initialization (Hinton et al., 2006; Bengio et al., 2006). From the perspective of the information propagation (Tishby et al., 2000; Mahabadi et al., 2021; Tishby & Zaslavsky, 2015), the fundamental issue with layer-wise training is that each layer is unable to access information from the layers that precede it. 24 Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing Some works (Xiong et al., 2020) proposed method that permits backpropagation within local block, allowing information from subsequent layers to progressively influence earlier layers by training them sequentially. Furthermore, (Gomez et al., 2022) builds upon the concept of overlapping local updates, introducing learning strategy that harmonizes the high parallelism characteristic of layer-wise training with the superior predictive accuracy associated with end-to-end (E2E) learning. Besides, classification-based loss functions are employed at each layer (Mostafa et al., 2018; Belilovsky et al., 2019; 2020), whereas similarity-based loss functions are utilized in other scenarios (Kulkarni & Karande, 2017; Nøkland & Eidnes, 2019; Siddiqui et al., 2023). Additionally, (Wang et al., 2020d) incorporates reconstruction error term into the local objective function, drawing from an information-theoretic perspective. Some works find that different layers have different convergence rates during the whole training process (Raghu et al., 2017). This property can be used to freeze front layers and only train the later layers, thus reducing the training costs. The PipeTransformer (He et al., 2021) utlizes this property to reduce the training costs of transformer models. LISA (Pan et al., 2024) discovered that the weight norm distributions across layers in LoRA and full parameter fine-tuning are skewed, indicating varying layer importance in large-scale LLM training. Based on this observation, LISA applies importance sampling to different layers in LLMs, randomly freezing most intermediate layers during optimization. It periodically samples Transformer layers from the model, randomly selecting layers for fine-tuning while keeping others frozen. The initial word/position embeddings (wte/wpe) and final language modeling head (lm_head) are consistently fine-tuned. This aligns with our observations regarding layer merging. Layer-wise model training and merging approaches have also provided inspiration for our research direction. (Li et al., 2024c) discovered that in the field of large language models, the effectiveness of deeper layers gradually diminishes, with many studies showing that deeper layers can be pruned without significantly affecting model performance - phenomenon often viewed as an opportunity for model compression. To address this, they proposed novel normalization technique called Mix-LN, which combines pre-LN and post-LN within the same model. Specifically, Mix-LN applies post-LN to earlier layers and pre-LN to deeper layers, ensuring more uniform gradients across all layers. Different from these methods that focus on improving the layer-wise training and optimization, we focus on improving merging LLMs inspired from the layer-wise training dynamics. A.4. Bayesian Deep Learning Bayesian Neural Networks. Considering the uncertainty of the model parameters, sampling bias in the training datasets, predictive uncertainty to domain shift (also referred to as out-of-distribution examples) (Lakshminarayanan et al., 2016; Blundell et al., 2015; Hendrycks & Gimpel, 2016), Bayesian Neural Networks (BNNs) view the model parameters as random variable. Then, optimizing the model parameters is equivalent to optimizing the posterior distribution of the model parameters conditioned on the training datasets) (Blundell et al., 2015). However, the training costs of BNNs are significantly higher than the non-Bayesian neural networks (Lakshminarayanan et al., 2016). proper scoring creterion for training non-Bayesian NN (Lakshminarayanan et al., 2016), model ensemble (Guo) and adversarial training (Goodfellow et al., 2014) are found to be good way to improve the robustness of neural networks as an alternative to BNNs. Bayesian Model Averaging (BMA). Except for the static importance measurement mentioned in previous section, Bayesian model averaging is another promising direction to improve the model merging performance based on the Bayesian inference. The deep model ensemble and Stochastic Weight Averaging (Izmailov et al., 2018; Maddox et al., 2019) are actually compelling approach to BMA (Wilson & Izmailov, 2020). The Bayesian marginalization can particularly improve the accuracy and calibration of modern deep neural networks (Wilson & Izmailov, 2020). However, the previous works in BNN and BMA consider the model parameters trained with the same datasets. How to merge models trained with different datasets is new open problem which also emerges in FL (Tang et al., 2024d; Liu et al., 2024c; 2021; Al-Shedivat et al., 2020; Yurochkin et al., 2019a; Wang et al., 2020b) and merging LLM models in pretraining (Liu et al., 2024a). A.5. OOD Detection and Generalization The input test samples in the real-world deployment are usually diverse and the distribution of the input test samples is dynamic. Normally, these samples are not shown in the training datasets, and their distribution might be different from the training distribution, which is call out-of-distribution (OOD) data. It is important to detect the OOD data (OOD Detection) (Liu et al., 2020; Hendrycks & Gimpel, 2016) and improve the model generalization on the OOD data 25 Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing (OOD Generalization) (Ovadia et al., 2019; Kendall & Gal, 2017; Lakshminarayanan et al., 2016). When confronted with distributional shifts, models optimized purely based on average training errors lead to poor performance (Duchi & Namkoong, 2018; Arjovsky et al., 2019; Creager et al., 2021). OOD Generalization. Some methods seek to find better invariant representations in neural networks (Bengio et al., 2013; Locatello et al., 2019), which means the representations are invariant to the distribution shift. From the causal perspective, the invariant representations are the representations that are invariant to the causal factors (Yang et al., 2021). Causal learning methods aim to learn the underlying causal structure of the data and to predict the outcome variable based on the identified causal variables. By correctly identifying the cause-effect relationships, these methods are expected to perform well even when the data distribution changes, as the underlying causal structure is often assumed to remain invariant across different environments or domains (Bühlmann, 2018). The invariant learning is to learn an invariant representation or model across environments leveraging contextual information such as domain labels (Muandet et al., 2013; Arjovsky et al., 2019; Albuquerque et al., 2020), where methods can be mainly divided into invariant risk minimization (Arjovsky et al., 2019) and domain-irrelevant representation learning (Li et al., 2018; Gong et al., 2019; Sicilia et al., 2021). OOD Detection. Some methods assume access to extensive OOD data alongside in-distribution (ID) data during training, formulating OOD detection as discriminative classification task by allocating special label for OOD samples (Fei & Liu, 2016; Larson et al., 2019; Kamath et al., 2020; Kim & Kim, 2018). Another approach optimizes outlier exposure regularization terms on OOD samples to refine the representations and OOD scores, such as the generalized outlier exposure (OE) loss introduced by (Hendrycks et al., 2019), which pushes the predicted distribution of OOD samples toward uniformity (Hendrycks et al., 2019; Lee et al., 2018), and entropy regularization objectives employed by (Zeng et al., 2021b) to enforce high entropy predictions for OOD samples. Additionally, leveraging contrastive learning techniques (Zeng et al., 2021a; Zhou et al., 2021; Cho et al., 2022; Mou et al., 2022) to increase inter-class discrepancies and enhance discriminative features for ID and OOD samples has been demonstrated to improve OOD detection performance. Previous works have found that the softmax outputs from models can be used as measurement of the uncertainty of model predictions (Guo; Hinton et al., 2015). And the early work in model distillation utilizes the softmax outputs as kind of soft labels to guide the model training (Hinton et al., 2015). Some works propose to scale the logits with the temperature scaling (Liang et al., 2017), thus the ID and OOD samples are more distinguishable based on the scaled softmax scores. Our work proposes dynamically merging task arithmetics from the Bayesian perspective to improve the OOD generalization. Inspired by the temperature scaling and the uncertainty measurement, we propose to scale the logits with the temperature scaling and to use the softmax outputs as an adjustment factor to estimating the likelihood of the task arithmetics conditioned on the input. A.6. Model Compression Pruning. Unstructured pruning (Frantar & Alistarh, 2023; Sun et al., 2024; Shao et al., 2024; Zhang et al., 2024; Dong et al.; Tang et al., 2020) effectively maintains LLM performance without requiring retraining, but leads to irregular structures that necessitate specialized optimizations for inference. SparseGPT (Frantar & Alistarh, 2023) offers novel one-shot pruning strategy by framing it as sparse regression problem, achieving over 50% sparsity with minimal perplexity increase. Wanda (Sun et al., 2024) reduces weight update costs by pruning low-magnitude weights scaled by input activations, while SAMSP (Shao et al., 2024) adjusts sparsity based on weight sensitivity using the Hessian matrix. DSnoT (Zhang et al., 2024) iteratively prunes and grows weights to minimize reconstruction error in sparse models. Structured pruning is hardware-agnostic, facilitating accelerated inference but may degrade performance due to the removal of critical components, often necessitating fine-tuning. Loss-based Pruning (Molchanov et al., 2019) measures the impact of unit removal on loss. LLM-Pruner (Ma et al., 2023) uses gradient information to identify dependent structures for optimal pruning. In contrast, Shortened LLaMA (Kim et al., 2024) focuses on depth pruning of Transformer blocks based on loss derivatives, employing LoRA to quickly recover performance post-pruning. Magnitude-based Pruning (Han et al., 2015) assesses pruning unit importance based on their magnitudes, pruning those below set threshold. Regularization-based Pruning (Wen et al., 2016) incorporates regularization terms to induce sparsity. Different from these pruning methods which focus on the weight pruning, our method is inspired from the sparse property of the task arithmetics to reduce the expert memory occupation (He et al., 2025; Tang et al., 2020). We sparsity the task arithmetics based on denoising and the magnitudes in our work. Note that our framework is general framework, any other sparsity method can be combined with our framework. 26 Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing Quantization. Weight-only quantization is the most conventional and widespread method. For example, LUT-GEMM (Park et al., 2024) uses binary-coding quantization (BCQ) (Rastegari et al., 2016) format, which factorizes the parameters of LLMs into binary parameters and set of scaling factors, to accelerate quantized matrix multiplications in weight-only quantization. GPTQ (Frantar et al., 2023) proposes layer-wise quantization method based on Optimal Brain Quantization (OBQ) (Frantar & Alistarh, 2022), which updates weights with inverse Hessian information, and quantizes LLMs into 3/4-bit. QuIP (Chee et al., 2023) optimally adjusts weights by utilizing the LDL decomposition of the Hessian matrix derived from vectors drawn uniformly at random from calibration set, and multiplies weight and Hessian matrices with Kronecker product of random orthogonal matrices to ensure incoherence between weight and Hessian matrices. Combining these two steps, QuIP successfully quantizes LLMs into 2-bits with minimal performance loss. To further minimize quantization errors in the weight-only quantization of LLMs, lots of works identify sensitive weights, which have an important effect on LLMs quantization performance, and store these sensitive weights in high precision. For example, AWQ (Lin et al., 2023) stores the top 1% of weights that have the most significant impact on LLM performance in high-precision, and integrates per-channel scaling method to identify optimal scaling factors. Here, \"channel\" denotes individual dimensions or feature maps within the model. Similar with AWQ, OWQ (Lee et al., 2024) store weights sensitive to activation outliers in high-precision, and quantizes other non-sensitive weights. Different from OWQ, SpQR (Dettmers et al., 2024) employs the L2 error between the original and quantized predictions as weight sensitivity metric. Furthermore, SqueezeLLM (Kim et al., 2023) introduces weights clusters algorithm based on sensitivity, using k-means centroids as quantized weight values, to identify sensitive weights. A.7. Data Synthesis Data Labeling. The data labeling process utilizes the advanced language comprehension capabilities of large language models (LLMs) to annotate extensive unlabeled datasets, proving particularly beneficial in areas like cross-lingual processing and multimodal learning (Zhu et al., 2023; Gilardi et al., 2023; Alizadeh et al., 2023). Automating this process enhances data preparation efficiency. Recent studies have investigated the zero-shot potential of models like GPT-4 for annotating political discourse on platforms like Twitter (Törnberg, 2023). Some works consider constructing preference tree (Zeng et al., 2024; Yuan et al., 2024) from LLM responses to refine incorrect responses based on feedback from models like GPT-4, creating more diverse and robust preference data. Data Reformation. Data reformation aims to transform existing datasets into diverse variations to improve data augmentation (Dixit et al., 2022; Dunlap et al., 2023). This enriches the training set with varied examples, enhancing model robustness and generalization. Novel approaches leveraging LLMs have emerged, such as Disco by Chen et al. (Chen et al., 2022), which generates large-scale, high-quality counterfactual datasets. prominent method in this area is in-context learning (Dong et al., 2022), where examples embedded in prompts guide LLMs to generate responses that reflect the provided patterns. Early works, such as Self-Instruct (Wang et al., 2023) and Unnatural Instructions (?), utilized task pools with hand-crafted seed examples. In contrast, LaMini-LM (Wu et al., 2023) built on this foundation by leveraging extensive data from Wikipedia to generate wider range of instructions. Auto Evol-Instruct (Zeng et al., 2024), originally designed to evolve instructions, automates the optimization of evolution rules through an Optimizer LLM that iteratively refines these rules based on evolving feedback data. Furthermore, Instruction Backtranslation (Li et al., 2023a) enhances instruction-following capabilities by creating instruction-response pairs from unannotated data, thus minimizing the need for manual annotation. This ongoing refinement of data reformation is essential for enhancing performance across various tasks. Generation from LLMs. Model generation utilizes powerful modelssuch as ChatGPT, StableVicuna, and GPT-4to create datasets that enhance the performance of weaker models. Techniques include generating concise narratives through templates (Eldan & Li, 2023) and assessing dataset quality with LLMs. Research by Phi-1 and its subsequent studies (Gunasekar et al., 2023; Li et al., 2023b) indicates that even small volume of high-quality data can effectively train models via generated textbooks and exercises using GPT-3.5. Additionally, performance has been improved by developing instructional datasets and fine-tuning models to enhance dataset quality (?Taori et al., 2023a; Chen et al., 2023). Domain model generation concentrates on the use of specialized models to produce domain-specific data. For example, domain generation can provide instructional materials for specific programming tasks in coding (Wei et al., 2024b; Luo et al., 2024). In mathematics, initiatives like Minerva (Lewkowycz et al., 2022) and DeepSeekMath (Xin et al., 2024) focus on generating accurate solutions. Synthetic Multi-step Reasoning. To enhance reasoning in LLMs, additional reasoning steps are incorporated into data synthesis. The MMIQC framework (Liu et al., 2024b) iteratively creates synthetic question-response pairs by expanding 27 Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing problems and integrating reasoning steps while preserving logical structure. complementary strategy involves generating chain-of-thought (CoT) answers based on questions (Li et al., 2024a). Building on question-CoT pairs through SelfInstruct, MathInstruct (Yue et al., 2023b) introduces the Program-of-Thought (PoT) rationale to streamline mathematical problem-solving. In this work, we utilize the stronger LLM to generate CoT based domain training data to enhance the reasoning performance of the downstream tasks. As far as we know, this work is the first to explore whether the model merging influences the CoT based reasoning performance. B. Theoretical Understanding In this section, we provide the theoretical interpretation from the perspective from the In-context learning (ICL) to further understand why routing finetuned models with task-level router instead of token-level ones might be better. Note that here the different finetuned models have been trained on individual tasks and never see other tasks. We re-write the preliminary in Section 2 here for convenience of reading. Task Data Distribution. Given set of different downstream tasks , based on the sampling task τ , the pretraining document (data sample) is sequence o1:T of tokens with the maximum length generated from distribution pτ = p(x1:T τ ) = p(o1, . . . , oT τ ) (Xie et al., 2022; Wies et al., 2023; Hahn & Goyal, 2023; Li et al., 2024b). sampled from p(oT ) = Pretraining Data Distribution. (cid:82) τ p(o1, . . . , oT τ )p(τ )dτ . Each token is sampled from vocabulary O. p(τ ) is prior distribution about τ . And both (T and belong to large task family Ω, i.e. , Ω. And we define the pretraining data is Language Modeling. Current LLMs (Brown et al., 2020; Touvron et al., 2023; Xie et al., 2022) usually utilize the next word prediction as the language modelling, which predicts the next token ot given the previous tokens o1:t1 for all = 1, . . . , . Formally, LLM parameterized by θ is distribution fθ(oto1:t1). And it is pretrained on huge corpus sampled from the pretraining distribution p(o1:T ) (Xie et al., 2022). Finetuning LLM. Normally, for each downstream task τ , finetuning LLM is to minimize the cross-entropy loss function as below: LCE(θ, τ ) = (cid:88) t=1 E[pτ (xtx1:t1) log fθ(xtx1:t1)]. After finetuning, the model parameters θ are updated to θτ for each task τ . Prompt distribution in Pretraining & Finetuing. Following (Xie et al., 2022), prompt is composed of an input token sequence o1:T followed by an output token y. Then, the i-th training example 2 that can appear in any place in the whole prompt o1:T is defined as Oi consisting of an input si = Oi [1 : 1] (the first 1 tokens) followed by the output yi = Oi [k] at the end, where the length is fixed for simplicity. The i-th training example is independently generated as follows: 1) Generate start hidden state hstart from prompt , τ ). The test input start distribution pprompt; 2) Given hstart xtest = sn+1 is sampled similarly. Between each example, special delimiter token odelim reset the transition between examples (Xie et al., 2022). Then, the prompt consists of sequence of training examples (Sn) followed by the example xtest: , generate the example sequence Oi = [si, yi] from p(Oihstart i [Sn, xtest] = [s1, y1, odelim, s2, y2, odelim, . . . , sn, yn, odelim, xtest] pprompt. (5) Different from (Xie et al., 2022), here we distinguish the pretraining tasks (concepts) and the finetuning tasks (concepts) = {τ1, τ2, ..., τnτ }, from which the prompts might be sampled. We mainly consider τ . In-context learning setups and Assumptions. We follow other settings and assumptions in (Xie et al., 2022). With the greedy decoding (Fu et al.), sampling the next token from the language modeling fθ(oto1:t1) becomes the predictor as = arg maxot fθ(oto1:t1). For simplicity, following (Xie et al., 2022), we consider that the finetuned LLMs have 2Here, training example in prompts means happens during the prompt learning, instead of the pretraining or the finetuning. 28 Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing been aligned with its pretraining and finetuning data distribution, i.e. pT τ = p(o1:T τ ) for any task τ . For convenience, we write pAτ = pT τ which means that the is augmented with τ . Thus, for [Sn, xtest], the in-context learning predictor can be written as (xtest) := arg maxy pAτ (ySn, xtest), which θτ outputs the most likely prediction over the pretraining distribution conditioned on the prompt distribution. Its expected 0-1 error with examples is L0-1(f θτ ) = Extest,ytestpprompt[1[f θτ (xtest) = ytest]]. We define pi τ (o) := p(O[i] = oO[1 : 1], τ ) of the i-th token with previous tokens and the analogous distribution pi prompt := pprompt(O[i] = oO[1 : 1]) under the prompt distribution. Following (Xie et al., 2022), there is distinguishability condition formalizes when in-context learning occurs giving the downstream task τ . The distinguishability condition is dependent on KL divergence between the previous two distributions and the error terms ϵτ resulting from the distribution mismatch between the prompt and the pertaining distributions for each example. Letting pi τ (o) and pi Condition B.1 (distinguishability (Xie et al., 2022)). The τ is distinguishable if for all τ Ω, τ = τ , prompt correspond to the task τ and and τ . (cid:88) KLi(τ τ ) > ϵτ , (6) where the KLi(τ τ ) := EO[1:i1]pprompt[KL(pi Lemma B.2. KL(pprompt(ytestxtest))p(ytestxtest, τ ) is bounded for all τ and that τ minimizes the multi-class logistic risk as, (Xie et al., 2022) let denotes the set of τ which does not satisfy Condition B.1. We assume that τ )]. i=1 promptpi If then LCE(τ ) = Extestpprompt[pprompt(ytestxtest) log p(ytestxtest, τ )]. Extestpprompt[KL(pprompt(ytestxtest)p(ytestxtest, τ ))] ϵτ , τ B, lim L01(f θτ ) inf L01(f ) + (cid:18) (cid:19) , (ϵτ ) sup τ (7) (8) (9) where g(ν) = 1 2 2016) for the multiclass logistic loss for ν [0, 1]. (cid:0)(1 ν) log(1 ν) + (1 + ν) log(1 + ν)(cid:1) is the calibration function (Steinwart, 2007; Pires & Szepesvári, Following (Kleijn & der Vaart, 2012; Xie et al., 2022), the task parameter τ is assumed to have the continuity, where the KL divergence is assumed to haver the 2nd-order Taylor expansion. Then, we have the following theorem and proof. Theorem B.3. KL divergences have 2nd-order Taylor expansion around τ : (Xie et al., 2022) Let the set of τ which does not satisfy Equation 6 in Condition B.1 to be B. Assume that > 1, KLi(τ τ ) = 1 2 (τ τ )Ij,τ (τ τ ) + O(τ τ 3) (10) maxj λmax(Ij,τ ) where Ij,τ is the Fisher information matrix of the j-th token distribution with respect to τ . Let γτ = min jλmin(Ij,τ ) where λmax, λmin return the largest and smallest eigenvalues. Then for 2 and as , the 0-1 risk of the in-context learning predictor θτ is bounded as lim L0-1(f θτ ) inf L0-1(f ) + g1 (cid:18) (cid:18) γτ supτ B(ϵθ start + ϵθ delim) (cid:19)(cid:19) Proof. (Xie et al., 2022) By the continuity assumption, we have for any τ in that (cid:88) j=2 KLi(τ τ ) 1 2 1 2 (cid:88) (τ τ )Ij,τ (τ τ ) + (k 1)O(τ τ 3) j=2 (k 1)λmin(Ij,τ )τ τ 2 = τ τ 2 ϵθ start + ϵθ 1 2 (k 1)(minj λmin(Ij,τ )) delim . 29 (11) (12) (13) (14) Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing Using the above term to bound the last KL term (k-th token), we have: KLk(τ τ ) = 1 2 1 2 (ϵθ (max start + ϵθ (τ τ )Ik,τ (τ τ ) + O(τ τ 3) λmax(Ij,τ ))τ τ 2 + O(τ τ 2) delim)(maxj λmax(Ij,τ ) + O(1)) (k 1) minj λmin(Ij,τ ) . (15) (16) (17) Rearranging above equation, and with the defintion that KLk(τ τ ) = Extestpprompt [KL(pprompt(ytestxtest)p(ytestxtest, τ ))], we have Extestpprompt[KL(pprompt(ytestxtest)p(ytestxtest, τ ))] (ϵθ start + ϵθ delim)(maxj λmax(Ij,τ ) + O(1)) (k 1) minj λmin(Ij,τ ) (18) Combining Equation 18 with Equation 8 into Lemma B.2 completes the proof. Task-level Routing. Observing the Equation 7 in Lemma B.2, the LCE(τ ) is the optimal risk over τ Ω. The τ which does not satisfy Condition B.1 means that the τ should be close to τ enough. Thus, we can have L01(f ) θτ converges with as in Lemma B.2. The task-level routing means to route τ to the finetuned LLM that has been trained on p(o1:T τ ). Thus, the task-level routing can satisfy the requirement of τ B. Token-level Routing. The core motivation of using token-level routing is that different tokens prefer different routers. Here, inspired by the distinguishability condition B.1, we can interpret the token-level router which dynamically finds the expert model for i-th token that satistifies: (cid:88) arg min KLi(τ τi ). (19) However, there is distribution shift between the τ and different τi . Revisiting the prompt sequence sampled as [Sn, xtest] = , τ ). If [s1, y1, odelim, s2, y2, odelim, . . . , sn, yn, odelim, xtest] pprompt, each pair Oi = [si, yi] is sampled from p(Oihstart the τi is choosed as different from τ , the distribution shift implies that the KLi cannot be minimized. Out-of-distribution Cases. While the above intuition illustrates that the task-level routing might be more suitable for the in-distribution test data xtest pprompt, we illustrate that two cases of new prompt sampling might need need combination of different LLM experts. OOD task. Considering that the τ is different from all τ , there might be needs to process different tokens with different experts following equation 19. Compositional task. Considering that Oi = [si, yi] might be sampled from p(Oihstart , τi), and each τi is different from others, the Equation 19 may helps to find the suitable experts. However, the theoretical analysis of how Equation 19 benefits ICL is difficult and we left it as the future work, which might also be beneficial to analyse the MoE models (Dai et al., 2024). Currently, we utilize the uncertainty-based model task-level routing and merging to address the OOD problem. C. Detailed Experiment Settigns C.1. Detailed Experimental Setup Hardware. All experiments were conducted on an A800 GPU with 80GB VRAM, Intel Xeon 6348 CPU, and 100GB RAM. Models and Datasets. We conduct comprehensive experiments on two cutting-edge large language model families: Qwen and LLaMA. Table 14 shows the number of parameters, memory occupation and release data of these models. These models represent the latest advancements in language model development. To the best of our knowledge, this is the first model merging study focusing primarily on generative tasks, finetuning with CoT based data and cutting-edge LLM tasks. 30 Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing Table 14: Backbone Models Overview"
        },
        {
            "title": "Number of Parameters Release Date",
            "content": "Memory Occupation (GB)"
        },
        {
            "title": "4 Billion\nQwen-1.5-4B\n7 Billion\nQwen-2.5-7B\nLLaMA-3.1-3B 3 Billion\nLLaMA-3.2-8B 8 Billion",
            "content": "February 2024 September 2024 April 2024 September 2024 15.26 26.00 11.31 30.52 Generative and Reasoning Tasks in Evaluation. In designing our evaluation tasks, we strategically selected orthogonal benchmarks to effectively demonstrate our methods capability in resolving parameter conflicts during model merging. Our task selection follows these principles: (1) The mathematical reasoning and code generation tasks represent fundamentally different parameter spaces. Specifically, mathematical computation requires numerical reasoning parameters, while code generation relies on syntax and programming logic parameters, allowing us to evaluate how well our merging approach handles potentially conflicting parameter updates. (2) Knowledge-based QA (TriviaQA) and concept understanding tasks (MMLU) evaluate distinct knowledge representations. TriviaQA focusing on factual retrieval parameters and MMLU covering broader conceptual understanding parameters across domains. This helps assess our methods ability to preserve different types of knowledge without interference. (3) The logical reasoning task (WinoGrande) may prefer to yet another independent parameter space focused on abstract reasoning, providing insights into how well our merging technique maintains reasoning capabilities while optimizing for other tasks. Based on above principle, we utilize the following cutting-edge LLm evaluation tasks about math reasoning, code generation, common sense QA, common sense logical reasoning, multi-domain knowledge. Mathematical Reasoning: We evaluate mathematical question-answering capabilities using the GSM8K dataset (Cobbe et al., 2021), which contains 8,500 high-quality elementary school math word problems (about 7,500 training, about 1,000 test) designed to evaluate mathematical reasoning capabilities. The problems feature diverse language styles and formats while avoiding templated designs. They use basic arithmetic operations with natural language solutions. Knowledge-based QA: We utilize TriviaQA (Joshi et al., 2017), large-scale Wikipedia-based question answering dataset, where models are required to generate direct answers without multiple-choice options. It contains complex questions requiring cross-sentence inference, with significant syntactic and lexical variations between questions and answer sentences. The dataset provides challenging evaluation scenarios that better approximate human-like question answering. Code Generation: The HumanEval (Chen et al., 2021) consists of human-written programming tasks where models must complete missing Python code snippets based on provided inputs. The problems simulate real-world programming challenges requiring context understanding, reasoning, and multi-step operations across varying difficulty levels and abstraction layers. Logical Reasoning: WinoGrande (Sakaguchi et al., 2019) is large-scale commonsense reasoning dataset of approximately 2800 questions developed by University of Washington researchers. Questions are presented as fill-in-the-blank tasks with two options and correct answers, with dataset bias reduced through the AfLite algorithm. The benchmark evaluates models commonsense reasoning abilities in understanding and generating relevant text. Multi-domain Knowledge: We employ MMLU (Hendrycks et al., 2021) to assess knowledge retention across diverse 57 subjects ranging from basic mathematics to US history, computer science, law, and ethics. Using multiple-choice questions of varying difficulty levels. Notably, we exploit the generation-based approach for multiple-choice evaluation, analyzing knowledge preservation across base models, fine-tuned variants, and merged models. The generation-based evaluation is better to measure the generative abilities of LLMs than choice-based evaluation. 31 Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing In the experiments of evaluating the scalability of Mediator, we also finetune another 4 LLMs according to the following 4 extra evaluation tasks. IFEval. (Zhou et al., 2023) comprehensive benchmark dataset designed to evaluate instruction-following capabilities of language models. It contains carefully curated instruction-response pairs across diverse task categories including text generation, analysis, and reasoning. The dataset aims to assess models ability to accurately interpret and execute natural language instructions while maintaining coherence and relevance in responses. The evaluation spans multiple dimensions including instruction comprehension, output quality, and adherence to specified constraints. CEval. (Huang et al., 2023b) comprehensive Chinese evaluation suite designed to assess language models knowledge and capabilities across various academic and professional domains. It consists of multiple-choice questions drawn from professional qualification exams and academic tests in China. For our evaluation, we specifically focus on three key subjects: (1) Medicine: testing clinical knowledge, diagnosis, and treatment principles from medical licensing exams; (2) College Economics: evaluating understanding of micro/macroeconomics concepts, market principles, and economic theories; (3) Law: assessing comprehension of Chinese legal principles, regulations, and judicial procedures. These subjects were chosen to evaluate models domain-specific expertise in technically demanding professional fields. Finetuning Settings. We adopt the ms-swift (Zhao et al., 2024) to finetune the given pretrained LLM. The finetuning datasets are constructed by augmenting some publicly datasets (task related but without overlap) with GPT-4o (Gilardi et al., 2023) and Chain-of-Thoughts (Wei et al., 2022). For each finetuning process, we use at least 180K training samples to ensure sufficient performance improvement on the corresponding task, which helps validate the effectiveness of our experiments. We provide the details of how we construct the finetuning datasets in Section F. Baselines. Following the summary of the related works in Section A, we compare methods in following four categories: Pretrained model. The pretrained models are directly downloaded from its open-source repository. These models are pretrained on the large corpus and have included enormous knowledge about the evaluation tasks. Finetuned Models: We finetune the pretrained models on datasets that we construct for each domain. Then, each finetuned model is evaluated on all tasks. The results help to show wheter finetuning on task enhance or decrease model performance on task B. Static merging methods. These methods use fixed weights to merge multiple finetuned models. The advanced static merging methods like Fisher merging (Matena & Raffel, 2022) and RegMean (Jin et al., 2023) require extra dataset and forward process to estimate some information like gradients, hessian, features to estimate parameter importance, which causes significant computational costs. Furthermore, considering that LLMs need to be deployed on various tasks, the utilized dataset actually cannot reflect the real-world data distribution. Therefore, these methods are shown empirically to perform worse than some calibration-less methods (Du et al., 2024). Recently, TIES (Yadav et al., 2023a) and PCB-merging (Du et al., 2024) achieve the best performance in weighted average method and do not require calibration data. Thus, we choose it for comparison. Dynamic Advanced Methods: We compare with state-of-the-art dynamic merging techniques that adapt model fusion parameters based on the input data. For example, Branch-train-mix dynamically routes different tokens to corresponding experts for generation through token-level routing. Similarly, the twin-merge (Lu et al., 2024a) computes merging weights through task-level routing mechanisms and dynamically fuses SVD-decomposed task vectors into the pretrained model in real-time. C.2. Hyperparameters of Finetuning and Implementing Baselines Hyperparameters for Single-task Finetuning. For single-task finetuning, we utilize set of hyperparameters that remain consistent across all models and tasks. The learning rate is set at 1.2e-5, applying cosine decay schedule. The batch size varies, with one sequence per batch for both the 7B and 8B models, while the 3B and 4B models use two sequences per batch considering the GPU memory limitation. The maximum sequence length is confined to 4096 tokens for both math and QA tasks and extends to 7000 tokens for coding tasks. The training consists of two epochs, and we employ the AdamW optimizer with parameters β1 = 0.9, β2 = 0.999, and ϵ = 1e 8. Additionally, warmup steps constitute 5% of the total steps. 32 Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing For all model merging baselines, the finetuned LLMs are the same. And all finetuned LLMs have shown that they can successfully improve the performance of the pretrained model on various tasks. The following is the details of how we tune and implement baseline methods. Hyperparameters for PCB-merging. We follow the original paper of PCB merging and have searched its hyperparameters. The weight clipping ratio is established at 0.1, which means weights with magnitudes in the bottom 10% are clipped to zero, following recommendations from the original paper concerning LLM generalization tasks. For model merging exploration, we perform 200 random exploration steps. The initial weights for random exploration are set to (0.4, 0.4, 0.4, 0.4) for the 3B, 4B, 7B, and 8B models with four experts, while for all models with eight experts, they are set to repeated value of 0.2 across eight instances. The validation batch size is configured to handle 8 samples per task, and we implement early stopping with patience of 10 steps without improvement. The weight clipping ratio and exploration parameters are uniform across all model sizes and tasks to facilitate fair comparison. Its worth noting that for the 7B and 8B models, the validation batch size is reduced to 4 due to memory limitations. Optimizing PCB-merging. To enhance the computational speed of PCB-merging, several optimizations were introduced based on the original framework, which do not influence its task performance. Instead of merging entire models simultaneously, we adopt layer-wise model merging strategy. This layer-by-layer merging approach has multiple benefits: it decreases memory overhead during the merging process, facilitates parallel processing of different layers, and allows for the assignment of layer-specific merging weights. Moreover, we implemented asynchronous model input/output operations which enable overlapping of I/O with computational processes. This adjustment is instrumental in reducing the total merging time by as much as 40%, enabling the seamless streaming of large models. These optimizations have significantly boosted both the efficiency and effectiveness of PCB-merging, particularly the layer-wise method, which has lowered peak memory usage by approximately 60% while maintaining or enhancing final model performance. And other hyper-parameters and settings are completely followed as the original paper. Hyperparameters for Twin-merging finetuning. For Twin-merging, we leverage LoRA finetuning in lieu of SVD to attain greater precision following the original paper. The rank is set as 32, and both the alpha and dropout parameters are also set at 32 and 0.1, respectively. The target modules involved in this finetuning process include the query and value matrices within the attention layers. And we also have conducted grid search for the hyper-parameters. Each task involves training over two epochs, with batch size set at 16; this batch size is reduced to 8 for the 7B and 8B models. The learning rate is specified at 1.5e-4, utilizing cosine decay schedule, and the optimizer employed is AdamW. Hyperparameters for Branch-train Mix (BTX). For the training of the BTX router, we follow the original implementation of it within ms-swift and its original paper to implement it. The relevant hyperparameters for this setup include training duration of 2 epochs, with batch size of 2; this is adjusted to 1 for the 7B and 8B models. The learning rate is established at 1.5e-6, utilizing linear decay schedule, alongside the AdamW optimizer, which is configured with weight decay of 0.001. The routers architecture consists of an input dimension derived from 2 layers of an FFN, with hidden dimension of 256, an output dimension corresponding to the number of experts, and dropout rate of 0.1. Warmup steps account for 5% of the total steps, and evaluations are conducted at every 1000 steps. To ensure balanced representation, the router is trained on dataset that equally samples from all tasks, employing early stopping with patience of 2 epochs based on validation accuracy. Hyperparameters for Mediator. For Mediator training, we utilize the same single-task finetuned experts as delineated in Appendix C.2. The task-level router is constructed from the first 9 layers of the pretrained LLM (with gradients stopped) and includes 2 additional FFN layers. The router is trained on balanced dataset with equal samples from each task domain to ensure unbiased task routing. We evaluate the router performance every 1000 steps and use early stopping with patience of 2 epochs based on validation accuracy. The training process for the router involves sampling 2000 examples from each task domain, specifically in mathematics, coding, question answering, law, economics, instruction following, and medicine. The specific hyperparameters applicable to router training encompass duration of 2 epochs, and batch size of 256, which is decreased to 128 for the 7B and 8B models. The learning rate is set to 3e-4, accompanied by cosine decay schedule, and the optimizer remains as AdamW. The warmup ratio is defined as 10% of the total steps. The routers architecture features frozen backbone comprising the initial 9 layers from the pretrained LLM, along with 2 trainable FFN layers. These layers have hidden dimension of 1280, with the output dimension reflecting the number of experts, and dropout rate fixed at 0.05. For router based selection, we use the temperature parameter β in Equation 4 as 1.5 to convert the prediction rates into concrete merging parameters for each expert, which achieves the best experimental results. This temperature scaling helps Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing balance between being decisive in expert selection while maintaining some degree of smoothness in the merging weights. temperature of 1.5 empirically provides the optimal trade-off, where lower temperatures lead to more concentrated weights but potentially miss useful signals from secondary experts, while higher temperatures result in overly diffuse weights that dont sufficiently leverage expert specialization. D. The variations and details of different parts of Mediator In this section, we provide the detailed variations and implementation details of different parts of Mediator. Some definitions and operations that appear in the main text may be re-defined in this section for better clarity of reading. D.1. Measuring Parameter conflicts Task Arithmetics. We define the task arithmetics as the parameter difference between the finetuned LLM θτ based on task τ and the pre-trained LLM θ, i.e., τ = θτ θ. Such task arithmetics can represent the update on the finetuned LLM θτ based on task τ . Given pretrained LLM θ, one can recover the finetuned LLM θτ = θ + τ . Denoising Parameters. Because the finetuing directions on different tasks are various and stochastic, there exist some elements in τ that do not influence the performance on task τ . Before measuring the parameter conflicts (Yadav et al., 2023b; He et al., 2024b), we firstly denoise the parameters by removing the elements in τ that do not influence the performance on task τ . We also model the update directions of different elements as the Gaussian distribution NUPD(µUPD, σ2 UPD), where µUPD is the mean of the update direction and σ2 UPD is the variance. Based on the estimated µUPD and σUPD, we can regard the elements within range (µUPD σUPD, µUPD +σUPD) as the elements that do not influence the performance on task τ . Thus, we can denoise the parameters by removing the elements within range (µUPD σUPD, µUPD + σUPD) (set as 0) and obtain the new parameter arithmetic ˆθτ = θ + ˆτ . In the deployment, these elements are saved with their indexes and values for realistic sparsification thus saving memory. D.2. Adaptive Merging Inspired by the empirical observation in Figure 2 in Section 3, we propose to leverage the parameter conflict distribution across different finetuned LLMs to adaptively merge the finetuned models. Practically, before merging, Mediator automatically calculates the conflicts dl across different finetuned LLMs. Then, Mediator models the conflicts as Gaussian distribution (µ, σ). Then, for each layer index l, Mediator average layer parameters if the conflict dl is less than the µ + σ, otherwise, Mediator Routing this layer. We denote the averaged layer parameters as ϕl AVG and the Routing layer parameters as ϕl UP. Algorithm 1 shows this detailed process. D.3. Averaging Operations Naive Average Operation. The naive average operation MAVG is defined as: MAVG(θ1, θ2, . . . , θT ) = 1 T (cid:88) τ =1 θτ , (20) which regards all finetuned LLMs equally and utilizes the same weight for each finetuned LLM. Such simple average operation is easy to implement, without fabricated procedures, thus having low computational overhead. However, different parameters may have different sensitivities to the final merged model, which may lead to suboptimal performance. Taylor Expansion. The Taylor expansion is powerful tool for approximating function around specific point, and it is widely used in various fields, including model compression (Lee et al., 2019) and previous works on model merging (Jhunjhunwala et al., 2023; Qu et al., 2022). We can utilize the Taylor expansion to measure the sensitivity of each parameter that influences the model performance on the downstream task τ as follows: LCE(θ + δθ, τ ) = Lτ (θ) + Lτ (θ) θ θδθ + O(δ2 θ ). (21) The first-order derivative LCE(θ,τ ) θ measures the sensitivity of the loss function Lτ to the parameter θ. Thus, we can see that 34 Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing utilizing the same averaging operation for all parameters may not be the optimal choice for merging different finetuned LLMs, as it does not take into account the different contributions of each finetuned LLM to the final merged model. Parameter-level Importance based Model Merging. To this end, one can utilize the first-order derivative or higher-order derivative to measure the sensitivity of the loss function Lτ to the parameter θ, based on which, the parameter-level importance can be measured as wτ = Lτ (θ) θ. Then, the parameter-level importance can be used as the averaging weight θ for each finetuned LLM like the following: MFO-Taylor(θ1, θ2, . . . , θT ) = (cid:88) τ =1 wτ θτ . (22) Preprocessing Parameters. Considering that the finetuing directions on different tasks are various and stochastic, some elements in θτ that are optimized stochastically and may not influence the performance on task τ . Thus, before averaging, we can denoise the parameters by removing the elements in θτ that do not influence the performance on task τ . Like the preprocessing the task arithmetics and the denoising, we recover the finetuned LLM ˆθτ = θ + ˆτ by removing the elements in τ that do not influence the performance on task τ . Then, the averaged models can be obtained by the following equation: Mde-noise(θ1, θ2, . . . , θT ) = (cid:88) τ =1 wτ ˆθτ . (23) D.4. Details of Expert Routing For an input x1:t sampled from the training dataset pτ , the intuitive routing mechanism is to directly use the finetuned LLM θτ that is trained on the training dataset pτ to generate the output xt+1:T . However, the real-world deployment is usually different from the training distribution, which may lead to suboptimal performance. Especially for an LLM deployment scenario, the input distribution is various. Modeling the likelihood πκ(τ x). We build task-level deep neural network as the router. In designing the router structure, we carefully balance model accuracy with additional memory requirements. While LLMs inherently demonstrate excellent classification capabilities, we need an efficient solution that wouldnt significantly impact performance. After extensive experimentation, we opt to utilize the embeddings from the first 9 layers of the base LLM combined with 2 FFN layers as our router architecture. This design choice eliminates the need for separate complex router structure while maintaining high classification accuracy with minimal memory overhead and fast execution speed. The router leverages the rich semantic understanding already present in the base models lower layers, making it both resource-efficient and effective for expert selection. Constructing training datasets for learning πκ(τ x). For training the router, we randomly sample 2000 examples from each domain rather than using the entire finetuned dataset for efficiency. For each task, we only extract the question part (other than the question and answer pairs) to better simulate real-world deployment scenarios. We do not explicitly construct training dataset for the others category to consider the OOD category and the sample will be regarded as prefering the original pretrained model. Instead, during inference, if the predicted probabilities for math, coding, and QA tasks are all below 0.5, the input is classified as others and processed this question by the base model. The training dataset can be represented as {(x, τ )x pτ }τ {math,coding,QA}. E. System Optimization The inference latency and memory consumption are critically important for the real-world LLM applications. Thus, we consider to optimize the inference latency and memory consumption of Mediator. The overall latency of Mediator is mainly affected by the routing, loading experts between CPUs and GPUs (if required offloading), inference of the models itself. Routing Latency. We run expert routing only once per sample because we use task-level routing. The classifier κ consists of two FFN layers, and its input is the hidden state of the first through ninth layers of the LLM. The total execution time of the classifier κ is between 0.2s and 0.4s. 35 Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing Loading Experts. After obtaining πκ(τ x), we compute h(τ x) according to Equation 4. We also load expert parameters only once. To optimize this process, we explore two methods. For sparse expert parameters, we store all of them in the CPU and prefetch the parameters for the next layer while performing computations in the current layer. For non-sparse expert parameters, we store them on disk and use ZipNN (Hershcovitch et al., 2024) to accelerate loading from disk to CPU. Inference Timeline. We present the optimized inference timeline of Mediator, as shown in Figure 5. The additional time incurred by Mediator is fixed and relatively small(approximately 0.2s to 0.4s). This portion of the time overhead will decrease as the model size increases or the decoding length becomes longer. Figure 5: The inference timeline of Mediator, assuming that the number of layers is three. E.1. Cuda Kernal Merging We accelerate the integration or disintegration of sparse experts into the dense backbone by using CUDAs atomicAdd, which enables parallel merging of multiple experts while maintaining accuracy. Through this approach, we can split the weights into individual elements, allowing each element to be processed in parallel. However, we have observed that parallel merging alone is sufficient to mask the associated costs. E.2. Serving with Batched requests The traditional LLM serving usually accepts different requests asynchronously. Then, different requests are allocated to different batches with predifined batch size and feed into the model. batch-style inference usually is faster than the single-request inference, because the computation matrix is more dense and become GPU friendly. However, the Mediator and many routing based merging works (Sukhbaatar et al., 2024b; Lu et al., 2024a) require to select different experts for different requests. Thus, batch of various requests may lead to various experts being selected, which would disturb the regularity of the computation matrix. To implement the batch-style serving, we implement following two new system optimization schemes to improve Mediator. Clustering Serving: Since each task arithmetic expert has been compressed to small capacity, we can merge task arithmetics with different parameter fusions into several merged experts. When multiple tasks begin serving, we select the merged experts with the closest overall distance. While this batch inference approach may introduce some errors, the key research focus lies in how to effectively cluster and construct merged experts; Batch arithmetic inference: This is our lossless solution for batch inference. Similarly, due to the small size of compressed task arithmetics, we propose the following approach: Let Θo be the parameters of the original large model, ta1, ta2, ...tan be the weighted Task arithmetics for tasks 1,2,3...n respectively, and x1, x2, ....xn be the input parameters for different tasks. We decompose the ideal case (Θo + taj)(xj) into Θo(xj) + taj(xj) to achieve efficient batch inference. F. Finetuning Data Generation F.1. Task-related Training Datasets Following benchmark datasets are used for evaluating model performance across different domains. The datasets used for finetuning are introduced Math Training Data for GSM8K. For mathematical reasoning tasks, we constructed our training dataset by combining several high-quality math-focused datasets: 36 Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing GSM8K Socratic Training Set: subset of GSM8K training data augmented with Socratic-style step-by-step reasoning, which helps models develop systematic problem-solving approaches. Note that this dataset does not have overlapped question answer pairs same with the GSM8K. Orca-Math (Mitra et al., 2024): comprehensive mathematics dataset that covers various mathematical concepts and problem types, designed to enhance mathematical reasoning capabilities. Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? Answer: Natalia sold 48 2 = 24 clips in May. Natalia sold 48 + 24 = 72 clips altogether in April and May. #### 72 Example 1 Example Question:Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn? Rationale: One rotates Rubiks Cube in 0.2 second, only needing to know how many steps it takes to solve one. Someone solve Rubiks Cube requires minimum of 50 steps. It takes 4 seconds to solve the Rubiks Cube at the fastest. Answer: Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn? #### 10 Table 15: Examples of GSM8K Socratic Training Set. The red color represents the CoT sequences. Example 1 Question:A number divided by 10 is 6. Yoongi got the result by subtracting 15 from certain number. What is the result he got? Answer: Lets call the certain number \"x\". According to the information given: number divided by 10 is 6: / 10 = 6 Yoongi got the result by subtracting 15 from x: Result = - 15 First, we need to find the value of x. We can do this by solving the first equation: / 10 = 6 = 6 * 10 = 60 Now that we know is 60, we can find the result Yoongi got by subtracting 15 from x: Result = - 15 Result = 60 - 15 Result = 45. So, the result Yoongi got is 45. Question: For the natural number A, the quotient of divided by 9 is 6 and the remainder is 5. What is the value of A? Answer: To find the value of A, we can use the formula for division with remainder: = (quotient divisor) + remainder In this case, the quotient is 6, the divisor is 9, and the remainder is 5. Plugging these values into the formula, we get: = (6 9) + 5 = 54 + 5 = 59. Therefore, the value of is 59. Example Table 16: Examples of orca-math Training Set. The red color represents the CoT sequences. By combining these datasets, we created rich and diverse training corpus that exposes models to different mathematical reasoning patterns, problem-solving strategies, and difficulty levels. Furthermore, with the help of CoT based answers generated by the GPT-4o, this comprehensive approach helps ensure robust mathematical reasoning capabilities across various scenarios. We assembled approximately 200,000 training samples. Example 1 Question: Who was the first person to reach the South Pole? Answer: Norwegian explorer Roald Amundsen led the first expedition to successfully reach the South Pole on December 14, 1911. His team beat British explorer Robert Falcon Scotts expedition by about month. Amundsens careful planning and use of dog sleds proved crucial to his success in reaching the pole first. Roald Amundsen was the first person to reach the South Pole. Question: What is the capital city of Australia? Answer: While many people think Sydney or Melbourne is Australias capital, Canberra is actually the capital city. It was purpose-built as compromise between these two rival cities and became the capital in 1927. The city is home to Parliament House and many important government institutions. The capital city of Australia is Canberra. Example Table 17: Examples of TriviaQA Training Set. The red color represents the CoT sequences. QA Training Data for TriviaQA and WinoGrande. For question answering tasks, we constructed our training dataset by combining and filtering several QA datasets: TriviaQA-Wikipedia subsets: dataset derived from Wikipedia articles containing trivia questions and answers, which helps train models on factual knowledge and reading comprehension. Alpaca QA Pairs(Taori et al., 2023b): We filtered the Alpaca dataset to extract QA pairs, specifically excluding math and programming related questions to maintain domain focus. This dataset provides diverse general knowledge questions and answers for training conversational capabilities. Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing Question: What is the primary purpose of the United Nations Security Council? Answer: The United Nations Security Councils primary purpose is to maintain international peace and security. It has the power to establish peacekeeping operations, implement international sanctions, and authorize military action. The Security Council is the only UN body that can issue binding resolutions to member states. The Security Council is responsible for maintaining international peace and security. Example 1 Question: How does photosynthesis work in plants? Answer: Photosynthesis is the process by which plants convert light energy into chemical energy. Plants use sunlight, water, and carbon dioxide to produce glucose and oxygen. The process takes place in the chloroplasts, specifically using the green pigment chlorophyll. The glucose provides energy for the plant, while the oxygen is released into the atmosphere as byproduct. Plants use sunlight to convert carbon dioxide and water into glucose and oxygen. Example 2 Table 18: Examples of Alpaca QA Training Set. The red color represents the CoT sequences. Example 1 Question: What company did Warren Buffett start his career at? Answer: Warren Buffett began his career at Buffett-Falk & Co., an investment firm in Omaha, Nebraska. He worked there from 1951 to 1954 after graduating from Columbia Business School. This was before he went on to form Buffett Partnership Ltd. in 1956 and later acquire Berkshire Hathaway in 1965. Warren Buffett started his career at Buffett-Falk & Co. Question: What is the longest river in Europe? Answer: The Volga River is the longest river in Europe, flowing entirely within Russia. It stretches for approximately 3,530 kilometers (2,193 miles) and drains into the Caspian Sea. The river has played crucial role in Russian history, culture, and economic development. The Volga River is the longest river in Europe. Example 2 Table 19: Examples of WebQuestions Training Set. The red color represents the CoT sequences. WebQuestions (Bordes et al., 2014): dataset of natural language questions paired with answers, derived from web queries. SQuAD v1 & v2(Rajpurkar et al., 2016): The Stanford Question Answering Dataset, both versions 1 and 2, from which we sampled question-answer pairs where explicit answers were provided. This combined dataset provides diverse question answer pairs across different domains and formats while maintaining high quality through careful filtering and sampling. The exclusion of math and programming questions helps maintain clear task boundaries for our specialized models. Through careful combination and sampling of these datasets, we assembled approximately 200,000 training samples. Code Training Data for HumanEval. For programming-related tasks, we constructed our training dataset by combining several programming-focused datasets: MBPP (Mostly Basic Python Programming)(Austin et al., 2021): dataset containing Python programming problems ranging from basic to intermediate difficulty levels. Magicoder-Evol-Instruct(Wei et al., 2024b): dataset of 110K high-quality programming instructions and solutions from the ise-uiuc project, covering diverse programming tasks and patterns. LeetCode dataset (Coignion et al., 2024): comprehensive collection of coding problems with varying difficulty levels, commonly used for programming practice and assessment. ConalaMineded (Yin et al., 2018): curated subset of the CoNaLa dataset containing high-quality Python programming snippets with natural language annotations, which helps models understand implementation details of specific functions and how to implement micro-level functionality, thereby improving overall chain-of-thought capabilities. This comprehensive dataset covers various programming concepts, difficulty levels, and coding patterns, enabling robust evaluation of models programming capabilities. In the scalability experiments, we utilize the following 4 datasets to finetune extra 4 models according to another 4 evaluation tasks. Note that these datasets have no overlap with the evaluation tasks. 38 Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing Question: Who wrote the play \"Romeo and Juliet\"? Answer: William Shakespeare Question: What is the capital city of Japan? Answer: Tokyo Example 1 Example 2 Table 20: Examples of SQuAD Training Set. Example 1 Question:Write python function to find the first repeated character in given string. Answer: def first_repeated_char(str1): for index,c in enumerate(str1): if str1[:index+1].count(c) > 1: return return \"None\" Question: Write function to reverse words in given string.. Answer: def reverse_words(s): return .join(reversed(s.split())) Example 2 Table 21: Examples of MBPP Training Set. Instruction Following. For instruction following tasks, we utilize the Magpie dataset (Xu et al., 2024), which contains 200K high-quality English instruction-following samples. The dataset covers diverse instruction types including writing, analysis, and problem-solving. We evaluate the models instruction following capabilities on IFEval, comprehensive benchmark containing 1,000 carefully curated instructions across multiple categories like reasoning, writing, and task completion. Economics: We use the IndustryInstruction (Ind) dataset for training, which contains instruction-response pairs focused on finance and economics concepts, analysis, and problem-solving. The model is evaluated on CEval economics benchmark, which tests understanding of economic principles, market analysis, and financial concepts. Medicine: We utilize the DISC-Med (Bao et al., 2023) Chinese medical dataset for training, which covers various aspects of medical knowledge including diagnosis, treatment, and healthcare concepts. Evaluation is performed on CEval physician tasks that assess medical domain knowledge and reasoning. Law: Training data comes from the DISC-Law Chinese legal dataset (Yue et al., 2023a), containing legal concepts, case analysis, and regulatory knowledge. The models legal capabilities are evaluated using CEval law tasks, which test understanding of legal principles and reasoning. F.2. CoT based Data Augmentation High-quality task-related training datasets are crucial for evaluating model merging algorithms effectively. When pretrained model achieves strong performance through single-task fine-tuning, it creates greater headroom for different model merging approaches to demonstrate their capabilities and differentiate themselves. The quality of task-specific datasets thus becomes key prerequisite for meaningful experimental comparisons. Therefore, we carefully curated high-quality training datasets for each specialized domain to ensure our experimental results meaningfully reflect the relative strengths of different merging strategies. The following sections detail the specific datasets used for each task domain. To enhance model performance through single-task fine-tuning, we constructed three Chain-of-Thought (CoT) datasets, as CoT has been shown to significantly improve model capabilities: 39 Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing Example 1 Question: Please amend the subsequent Python script so that it includes while loop rather than the existing for loop, which iterates through the items of an integer list. The script currently has bug where it attempts to print an object that is outside the bounds of the list. Fix this error and modify the script to use while instead of for loop. Ensure your script correctly handles empty lists. python # Establish an integer list arr = [1, 2, 3, 4] # Determine the length of the list = len(arr) # Traverse the list and output each individual element for in range(n+1): print(arr[i]) Answer: python # Establish an integer list arr = [1, 2, 3, 4] # Determine the length of the list = len(arr) # Initialize index at 0 = 0 # Traverse the list and output each individual element while < n: print(arr[i]) += 1 In the given code, it tries to access arr[n] which is out of bounds as python list indexing starts from 0 and ends at n-1 for list of length n. Also, used while loop as per your request. By initializing i=0 before the loop and += 1 within the loop, we can iterate over the list. This iteration continues until is less than n, the length of the list. The while loop will not proceed if the list is empty so it correctly handles empty lists. Example Question: Create function to search for word in an array. The word should be searched in the lowercase version of the array and it should return the index location of its first occurrence. word = \"test\" words_array = [\"Hello\", \"World\", \"Test\", \"Test\", \"me\"] Answer: Heres how you can create the function in Python: python def search_word(word, words_array): word = word.lower() words_array = [w.lower() for in words_array] if word in words_array: return words_array.index(word) else: return \"Word not found\" word = \"test\" words_array = [\"Hello\", \"World\", \"Test\", \"Test\", \"me\"] print(search_word(word, words_array)) This script turns both the search term and the words in the array to lower-case to ensure the search is case-insensitive. It checks if the word is within the array. If it is, it returns the index of the first occurrence of the word. If not, it returns Word not found message. Table 22: Examples of Magicoder-Evol-Instruct Training Set. The red color represents the CoT sequences. F.3. CoT based Data Augmentation High-quality task-related training datasets are crucial for evaluating model merging algorithms effectively. When pretrained model achieves strong performance through single-task fine-tuning, it creates greater headroom for different model merging approaches to demonstrate their capabilities and differentiate themselves. The quality of task-specific datasets thus becomes key prerequisite for meaningful experimental comparisons. Therefore, we carefully curated high-quality training datasets for each specialized domain to ensure our experimental results meaningfully reflect the relative strengths of different merging strategies. The following sections detail the specific datasets used for each task domain. To enhance model performance through single-task fine-tuning, we constructed three Chain-of-Thought (CoT) datasets, as CoT has been shown to significantly improve model capabilities: Math CoT. We utilized GSM8K Socratic and Orca-Math datasets for our mathematical Chain-of-Thought training. The GSM8K Socratic dataset, containing approximately 7,500 samples, fully exhibits step-by-step CoT reasoning characteristics. While Orca-Math does not consistently maintain CoT patterns across all examples, the combined datasets provide about 30% of samples with clear CoT reasoning, which we used for training. This mixed dataset approach helps balance between high-quality CoT examples and broader mathematical coverage. 40 Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing Example 1 Question: Given an array of integers nums and an integer target, write function that returns indices of the two numbers in the array that add up to the target. You may assume that each input would have exactly one solution, and you may not use the same element twice. nums = [2,7,11,15], target = 9 Answer: too large, neglect the code This solution uses hash map approach for O(n) time complexity. As we iterate through the array, for each number, we check if its complement (target - current number) exists in our hash map. If found, weve found our pair and return their indices. If not, we add the current number and its index to the hash map. Example 2 Question: Given the head of linked list, reverse the nodes of the list at time, and return the modified list. If the number of nodes is not multiple of then left-out nodes, in the end, should remain as it is. Input: head = [1,2,3,4,5], = 2 Answer: too large, neglect the code This solution first counts nodes ahead to ensure we have enough nodes to reverse. If we have nodes, we perform the reversal using three pointers (prev, curr, next) to carefully adjust the links. We then recursively process the rest of the list. If we dont have nodes, we leave them as is. The solution maintains the original order of remaining nodes when the list length isnt multiple of k. Table 23: Examples from LeetCode Training Set. The red color represents the CoT sequences. Question: converting integer to list in python Answer: [int(x) for in str(num)] Example 1 Example 2 Question: How to convert Date string to DateTime object? Answer: datetime.datetime.strptime(s, %Y-%m-%dT%H:%M:%SZ) Table 24: Examples from ConalaMineded Dataset. Code CoT. For coding training data, we combined multiple data sources. The MBPP dataset provides non-CoT examples, while Magicoder-Evol-Instruct and LeetCode datasets mostly contain CoT programming examples. Additionally, although ConalaMineded is CoT dataset, it uniquely explains the meaning of each small programming snippet, which helps models better understand micro-level programming components. We sampled and integrated examples from all these sources to create comprehensive training set. QA CoT. For question answering tasks, we integrated multiple data sources with varying levels of content. We used Claude 3.5 Sonnet to augment TriviaQA-Wikipedia and WebQuestions datasets with CoT reasoning by prompting it to \"expand the QA pair with necessary background knowledge for CoT training data\". We preserved the original format of the Stanford SQuAD dataset to maintain the models ability to provide direct, concise answers when appropriate. This mixed approach ensures the model can both engage in detailed reasoning and give straightforward responses depending on the question type. Table 15, 16, 17, 18, 19, 20, 21, 22, 23 and 24 show examples of the final constructed datasets for finetuning. 41 Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing G. More Experiment Results G.1. Comparing Magnitudes of Task Arithmetic of SFT Models and Pretrained Models Figure 6 shows more comparisons on the maginitudes of task arithmetic of SFT Models and Pretrained Models. Results reveal that Task Arithmetic consistently exhibits high concentration of parameters around zero (>76%) across all model architectures. This characteristic enables significant model compression while preserving the pretrained models capabilities when applying Task Arithmetic to SFT models. (a) LLaMA 3.2 3B (b) LLaMA 3.1 8B Figure 6: Parameter Distribution Comparison: Task Arithmetic of the SFT models vs Pretrained Models. G.2. Detailed Evaluation of Model Merging Algorithms We conduct experiments on four large language models: Qwen 1.5 4B, Qwen 2.5 7B, LLaMA 3.2 3B, and LLaMA 3.1 8B. The detailed results are shown in the tables below. 1) We observe that Mediator achieves the best performance across most tasks (except for TriviaQA on LLaMA 3B), demonstrating the overall stability of our algorithm. 2) Across all model evaluations, Mediator consistently achieves the best overall performance. Specifically, for Qwen 1.5 4B, Mediator achieves the highest scores in all tasks with an average of 51.40%. On LLaMA 3.2 3B, it obtains the best performance in GSM8K (46.47%), Winogrande (72.03%), HumanEval (40.42%), and MMLU (54.91%), leading to the highest average score of 54.97%. For Qwen 2.5 7B, Mediator matches or exceeds the best performance across all tasks, resulting in superior average of 71.00%. Similarly on LLaMA 3.2 8B, it achieves the highest scores in most tasks and the best overall average of 71.80%. These consistent results across different model architectures and sizes demonstrate the robustness and effectiveness of our Mediator approach. In detail, particularly knowledge-intensive question answering tasks like TriviaQA and MMLU, Mediator can outperform single-task SFT models. Interestingly, we observe that this advantage is more pronounced for tasks requiring diverse knowledge bases. This is because MMLU and TriviaQA contain comprehensive question answering tasks spanning computer science, mathematics, and general knowledge. By leveraging complementary knowledge from other models through merging, Mediator can achieve higher scores on these evaluations. Ablation study of token level routing. The two figures (Fig. 7 and Fig. 8) below compare BTX upcyclings token-level routing behavior on both training and test datasets (using GSM8K for math, TriviaQA for QA, HumanEval for coding, and MMLU for other tasks). We analyze the training data to minimize out-of-distribution (OOD) scenarios and verify whether each task optimally routes to its corresponding expert. Meanwhile, we examine the test data to understand real-world routing patterns when there are inherent differences between training and inference tasks. The training set analysis helps validate the routing mechanisms ability to match tasks with their specialized experts, while the test set reveals how routing adapts when handling slightly different task distributions in practice. Fig. 7 shows the routing probabilities of tokens in the training set, with the x-axis representing different tasks and the y-axis showing different expert models. The intensity of the colors in Fig. 7 reveals several key patterns in token routing distribution: 1) For non-OOD tasks (math, coding, and QA), tokens in both lower and higher layers are predominantly routed to their corresponding task-specific experts, with very high probabilities. This strongly indicates that specialized experts are indeed optimal for handling their designated tasks; 2) For these non-OOD tasks, while their corresponding experts still Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing Table 25: Comparing performance of different model merging methods on Qwen 1.5 4B Alg./Tasks GSM8K TrA. Winogrande HumanEval MMLU All Tasks Average base Math Code QA all-sft TIES PCB-merging Twin-merging BTX Mediator 47.16 51.00 43.29 45.56 48.52 47.76 47.83 47.99 48.44 50.94 44.54 46.95 46.39 48.02 47.73 46.59 47.60 44.63 46.94 48.20 56.75 54.62 54.14 57.93 55.88 54.14 56.75 57.54 57.77 57. 41.46 26.83 43.29 39.02 39.14 44.51 43.90 40.85 42.68 45.12 54.45 53.54 54.82 52.32 53.93 54.58 54.58 52.98 53.88 54.87 48.87 46.79 48.39 48.57 49.04 49.5 49.93 48.80 49.94 51. Table 26: Comparing performance of different model merging methods on Llama 3.1 3B Alg./Tasks GSM8K TrA. Winogrande HumanEval MMLU All Tasks Average Base Math QA Code all data sft TIES PCB-Merging Twin-Merging BTX Mediator 27.52 46.47 32.75 33.13 44. 42.61 46.02 39.04 45.19 46.47 57.71 54.59 61.45 57.71 47.74 60.99 60.39 52.45 62.05 61.02 69.69 69.06 69.69 68.59 69.21 71.11 71.27 69.27 71.87 72.03 22.56 25.00 28.05 40.85 34. 31.30 29.88 29.94 28.05 40.42 54.08 52.73 54.17 53.09 53.75 54.32 54.21 53.91 54.44 54.91 46.31 49.57 49.22 50.67 49.92 51.27 52.35 48.11 52.33 54.97 Table 27: Comparing performance of different finetuned models and algorithms (Backbone: Qwen 2.5 7B) Alg./Tasks GSM8K TrA. Winogrande HumanEval MMLU All Tasks Average Base Math Code QA All data sft TIES PCB-merging Twin-merging BTX Mediator 83.41 85.14 52.31 84.62 64.90 84.76 73.46 83.46 84.46 85. 51.67 51.67 49.47 55.58 52.98 54.46 53.90 54.64 55.89 56.06 67.68 65.75 64.64 62.83 69.30 66.46 69.53 66.37 67.72 69.30 67.68 61.59 71.95 43.29 65.85 65.85 60.98 69.51 67.68 71. 67.70 67.27 72.30 71.51 69.66 71.55 71.41 70.56 72.30 72.56 67.63 66.27 62.13 63.57 64.59 68.62 65.86 68.91 69.61 71.00 43 Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing Table 28: Comparing performance of different finetuned models and algorithms (Backbone: Llama 3.2 8B) Alg./Tasks GSM8K TrA. Winogrande HumanEval MMLU All Tasks Average"
        },
        {
            "title": "Base\nMath\nCode\nQA\nAll data sft",
            "content": "TIES PCB-merging Twin-merging BTX Mediator 56.33 77.18 61.41 69.60 70.89 76.04 76.04 76.80 76.72 76.95 72.39 73.99 73.94 74.14 69.77 76.78 76.89 72.71 73.99 76.70 73.64 74.98 74.59 74.51 75. 74.19 74.35 74.49 75.22 75.69 27.44 20.12 62.80 31.71 48.17 53.05 53.66 59.14 60.98 62.80 67.99 62.10 62.73 62.21 62.94 62.36 62.42 64.43 65.68 67.87 59.56 61.67 67.09 62.43 65. 68.48 68.67 69.51 70.52 71.80 maintain dominance in middle layers, the routing probabilities are more evenly distributed. This observation helps explain why model averaging in middle layers results in relatively minimal performance degradation; 3) For OOD tasks like MMLU, we observe more uniform distribution of token routing across experts, with QA experts becoming dominant in the final layers, likely because MMLU contains numerous knowledge-based question-answering tasks. Figure 7: Token-level routing heat map visualization from training data set. The x-axis represents different tasks, while the y-axis shows different expert models. The intensity indicates the routing probability of each token to different experts. Fig. 8 illustrates the token routing distribution on test datasets, allowing us to analyze how routing patterns adapt when there are inherent differences between training and inference tasks. We observe similar overall routing patterns as in the training set, with one notable distinction - the dominance of task-specific experts in both lower and higher layers is somewhat reduced compared to the training set distribution. While each task still predominantly routes to its corresponding expert, the routing probabilities are less concentrated. This empirical observation helps explain why we need to use πκ(τ x) to further relax the discrepancy between the estimated distribution and the true distribution when handling real-world tasks that may differ from the training distribution. G.3. System Performance Analysis on NVIDIA RTX 4090 Compared to A800 GPU results, running on consumer-grade RTX 4090 shows notably slower inference speeds, likely due to: 1) Limited VRAM capacity (24GB vs 80GB) 2) Lower memory bandwidth 3) Reduced BF16 FLOPS performance However, the system remains functional for practical deployment. Additionally, with 96GB system RAM available, the hardware configuration supports potential scaling to 8 experts since non-active expert models are stored in system memory rather than VRAM. 44 Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing Figure 8: Token-level routing heat map visualization from test data set. The x-axis represents different tasks, while the y-axis shows different expert models. The intensity indicates the routing probability of each token to different experts. Table 29: System performance of Mediator (Qwen 2.5 7B 4) on NVIDIA RTX 4090 Metric Average Inference Time GPU Memory Usage System Memory Usage Value 3.571 23.97 21.7 Unit seconds GB GB Notes Per 200 samples Peak usage For expert storage H. Real-world Case Studies Real-world Case Studies Analysis. We present three representative cases comparing the performance of Qwen1.5 4B base model, task-specific SFT model, and our Mediator across different domains: GSM8K Mathematics Task: In Tab. 30, all three models demonstrate chain-of-thought (COT) reasoning capabilities and successfully arrive at correct answers. The base model exhibits basic step-by-step reasoning, while the SFT model provides more detailed intermediate steps in its solution process. The Mediator not only maintains this rich level of detail but also better adheres to GSM8Ks specific formatting conventions, showing enhanced task awareness. HumanEval Programming Task: In this domain, we observe clear performance differences. Tab. 32 shows the base model fails to generate correct solutions and suffers from repetitive output patterns. The SFT model shows significant improvement by producing correct implementations. he Mediator further enhances the output quality by not only providing correct solutions but also including concise explanations of the problem-solving approach, demonstrating more comprehensive understanding of programming tasks. Question Answering Task: In Tab. 31, while the base model provides correct answers, it exhibits hallucination by incorrectly treating questions as multiple-choice format. Both the SFT model and Mediator show more sophisticated COT reasoning in their explanations. Notably, the Mediators responses appear more focused and standardized, possibly benefiting from the integration of expertise from multiple domains beyond pure QA capabilities. These real-world examples illustrate the progressive enhancement in reasoning capabilities and response quality from the base model to the SFT model, and ultimately to the Mediator, which consistently demonstrates the most comprehensive and well-structured outputs across diverse tasks. 45 Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing Example: Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? base: 5) Natalia sold 48 / 2 = 24 clips in May. In April, Natalia sold 48 + 24 = 72 clips. Therefore, the answer is 72. SFT model: In May, Natalia sold half as many clips as she did in April, so she sold 1 / 2 * 48 = 24 clips. In total, Natalia sold 48 + 24 = 72 clips in April and May. Therefore, the answer is 72. Mediator: How many clips did Natalia sell in May? ** In May, Natalia sold 48/2=48/2=2424 clips. How many clips did Natalia sell altogether in April and May? ** Natalia sold 48+24=48+24=7272 clips altogether in April and May. #### 72 Table 30: Example of responses to GSM8K test cases by Qwen 1.5 4B base model, SFT model, and Mediator. The red color represents the thinking process. Example: Question: Which William wrote the novel Lord Of The Flies? base: A. William Golding B. Philip Roth C. Ian McEwan D. Virginia Woolf. Answer: (Note: Answer is in Chinese) SFT model: William Golding wrote the novel Lord Of The Flies. The main character in the novel, Jack, embodies many of the traits that Golding identified as features of the emerging post-British Empire \"yuppie\" attitude: materialism, superficiality, \"superiority\" over others of \"lesser status\", and lack of personal responsibility.The novel is often seen as metaphor for the rise of authoritarianism during World WAR II by some. Mediator: William Golding. William Golding was British novelist, essayist, and poet best known for his novel Lord of the Flies, first published in 1954. The story centers on group of young boys stranded on an uninhabited island and their disastrous attempt to govern themselves. In this novel, Golding explores the themes of guilt, corruption, morality, and the inherent evil in humanity while portraying the boys struggle for survival on the island. Table 31: Example of Trivia QA by Qwen 1.5 4B base model, SFT model, and Mediator. The red color represents the thinking process. 46 Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing Example: Question: from typing import List def has_close_elements(numbers: List[float], threshold: float) -> bool: Check if in given list of numbers, are any two numbers closer to each other than given threshold. > has_close_elements([1.0, 2.0, 3.0], 0.5) False > has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3) True base: def generate_greatest_palindrome(number: int) -> str: Generate the greatest palindrome number which is less than given number. > generate_greatest_palindrome(1990) 9999 > generate_greatest_palindrome(9999) 9999 > generate_greatest_palindrome(10000) 9999 > generate_greatest_palindrome(100000) 999999 SFT model: def has_close_elements(numbers: List[float], threshold: float) -> bool: res = False for in range(len(numbers) - 1): # check ith and jth elements are close to each other = + 1 # calculate distance diff = abs(numbers[i] - numbers[j]) if diff + threshold <= abs(numbers[i + 1]) and diff - threshold >= abs(numbers[i + 1]): res = True break return res Mediator: from typing import List def has_close_elements(numbers: List[float], threshold: float) -> bool: # Remove duplicate numbers unique_numbers = set(numbers) # Sort the unique numbers sorted_numbers = sorted(unique_numbers) # Iterate over the sorted numbers for in range(1, len(sorted_numbers)): # Compare the current number with the next number if sorted_numbers[i] - sorted_numbers[i-1] < threshold: return True return False Table 32: Example of HumanEval by Qwen 1.5 4B base model, SFT model, and Mediator."
        }
    ],
    "affiliations": [
        "Platform and Content Group, Tencent",
        "Sun Yatsen University",
        "The Hong Kong University of Science and Technology",
        "The Hong Kong University of Science and Technology (Guangzhou)"
    ]
}