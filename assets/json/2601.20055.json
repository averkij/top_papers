{
    "paper_title": "VERGE: Formal Refinement and Guidance Engine for Verifiable LLM Reasoning",
    "authors": [
        "Vikash Singh",
        "Darion Cassel",
        "Nathaniel Weir",
        "Nick Feng",
        "Sam Bayless"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite the syntactic fluency of Large Language Models (LLMs), ensuring their logical correctness in high-stakes domains remains a fundamental challenge. We present a neurosymbolic framework that combines LLMs with SMT solvers to produce verification-guided answers through iterative refinement. Our approach decomposes LLM outputs into atomic claims, autoformalizes them into first-order logic, and verifies their logical consistency using automated theorem proving. We introduce three key innovations: (1) multi-model consensus via formal semantic equivalence checking to ensure logic-level alignment between candidates, eliminating the syntactic bias of surface-form metrics, (2) semantic routing that directs different claim types to appropriate verification strategies: symbolic solvers for logical claims and LLM ensembles for commonsense reasoning, and (3) precise logical error localization via Minimal Correction Subsets (MCS), which pinpoint the exact subset of claims to revise, transforming binary failure signals into actionable feedback. Our framework classifies claims by their logical status and aggregates multiple verification signals into a unified score with variance-based penalty. The system iteratively refines answers using structured feedback until acceptance criteria are met or convergence is achieved. This hybrid approach delivers formal guarantees where possible and consensus verification elsewhere, advancing trustworthy AI. With the GPT-OSS-120B model, VERGE demonstrates an average performance uplift of 18.7% at convergence across a set of reasoning benchmarks compared to single-pass approaches."
        },
        {
            "title": "Start",
            "content": "VERGE: Formal Refinement and Guidance Engine for Verifiable LLM Reasoning Vikash Singh1* Darion Cassel2, Nathaniel Weir2, Nick Feng2, Sam Bayless2 1Case Western Reserve University 2Amazon Web Services 6 2 0 2 7 ] . [ 1 5 5 0 0 2 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Despite the syntactic fluency of Large Language Models (LLMs), ensuring their logical correctness in high-stakes domains remains fundamental challenge. We present neurosymbolic framework that combines LLMs with SMT solvers to produce verificationguided answers through iterative refinement. Our approach decomposes LLM outputs into atomic claims, autoformalizes them into firstorder logic, and verifies their logical consistency using automated theorem proving. We (1) multiintroduce three key innovations: model consensus via formal semantic equivalence checking to ensure logic-level alignment between candidates, eliminating the syntactic bias of surface-form metrics, (2) semantic routing that directs different claim types to appropriate verification strategies: symbolic solvers for logical claims and LLM ensembles for commonsense reasoning, and (3) precise logical error localization via Minimal Correction Subsets (MCS), which pinpoint the exact subset of claims to revise, transforming binary failure signals into actionable feedback. Our framework classifies claims by their logical status and aggregates multiple verification signals into unified score with variance-based penalty. The system iteratively refines answers using structured feedback until acceptance criteria are met or convergence is achieved. This hybrid approach delivers formal guarantees where possible and consensus verification elsewhere, advancing trustworthy AI. With the GPT-OSS120B model, VERGE demonstrates an average performance uplift of 18.7% at convergence across set of reasoning benchmarks compared to single-pass approaches."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse reasoning tasks, from mathematical problem-solving *Work done during internship at Amazon Web Services 1 Figure 1: VERGE correcting LLM Hallucinations via Formal Verification. The solver detects an unsupported claim and guides the LLM to consistent answer through MCS-based feedback. (Lewkowycz et al., 2022; Hendrycks et al., 2021) to code generation (Chen et al., 2021; Austin et al., 2021) and logical inference (Tafjord et al., 2021). Despite these advances, ensuring the correctness of LLM-generated answers remains critical barrier in high-stakes domains such as legal policy compliances, healthcare and finance etc. While recent models achieve impressive benchmark performance (OpenAI, 2025; Anthropic, 2025; Meta AI, 2025), they rely on statistical likelihood maximization (Ouyang et al., 2022) rather than logical deduction. Consequently, they operate without mechanisms for provable correctness, making them prone to hallucinations and internal contradictions (Weng et al., 2023). Current verification strategies such as selfconsistency (Wang et al., 2022), process supervision (Lightman et al., 2023), and self-refinement (Madaan et al., 2023; Shinn et al., 2023) provide heuristic rather than formal guarantees. Even multiagent debate frameworks (Du et al., 2023; Liang et al., 2023) merely achieve consensus, which does not imply correctness. To achieve verifiable reasoning, neuro-symbolic methods (Ganguly et al., 2025, 2024; Feng et al., 2025; Pan et al., 2023; Callewaert et al., 2025; Olausson et al., 2023; Garcez et al., 2019; Mao et al., 2019) and semantic parsing (He et al., 2025; McGinness and Baumgartner, 2024; Zettlemoyer and Collins, 2005; Dong and Lapata, 2016) have attempted to bridge natural language with formal logic. However, these approaches face fundamental semantic gap: natural language is inherently ambiguous, and rigid formalization often fails on open-domain claims (Church, 1936; Turing, 1936). We present VERGE, framework that mitigates this gap by combining LLMs with Satisfiability Modulo Theories (SMT) solvers (Barrett et al., 2009; De Moura and Bjørner, 2008) to produce verification-guided answers with formal guarantees for logical/mathematical claims through iterative refinement, as illustrated in Figure 1. Unlike standard feedback loops, VERGE leverages the SMT solvers ability to extract unsatisfied assertions (Zhang and Malik, 2003; Nadel et al., 2014). This allows us to compute Minimal Correction Subsets (MCS) (Belov et al., 2012; Marques-Silva et al., 2013), identifying minimal set of modifications to the atomic claims sufficient to restore consistency and move from probabilistic self-correction to provable self-consistency correction. The Expressivity Trade-off and Pragmatic Verification. key insight of our work is that enforcing formal verification on all claims is fundamentally misaligned with the ambiguity of natural language. Rather than attempting to bridge this gap universally, theoretically intractable goal, we adopt pragmatic stance: Apply formal verification where the semantic gap is narrow (mathematical/logical claims), and fall back to consensusbased verification where it is wide (commonsense/- vague claims). VERGE introduces verification cascade with semantic routing to implement this strategy. This hybrid approach provides formal guarantees for verifiable subset of claims while maintaining the systems ability to handle broad, real-world reasoning tasks. Our work introduces: 1. High-Fidelity Consensus via SMT: We bridge the semantic gap by enforcing semantic equivaϕb) among candidate formalizalence (ϕa tions. Unlike syntactic metrics (e.g., BLEU, Jaccard) which fail on variable renaming or structural permutation, we utilize the solver to prove that different candidate formulas yield identical truth tables, ensuring robust consensus. 2. Actionable Feedback via MCS: We adapt greedy MCS computation (Marques-Silva et al., 2013; Morgado et al., 2013; Bacchus and Katsirelos, 2015) to provide polynomial-time, specific feedback (e.g., \"claim C2 does not hold\") rather than generic error signals. 3. Flexible Neuro-symbolic Integration: semantic routing framework that balances the precision of SMT solvers with the flexibility of LLMs, avoiding the pitfalls of forcing undecidable language into decidable theories."
        },
        {
            "title": "2 Related Work",
            "content": "Probabilistic vs. Formal Verification. Standard LLM reasoning strategies rely on probabilistic confidence. Methods like self-consistency (Wang et al., 2022) and process supervision (Lightman et al., 2023; Uesato et al., 2022) aggregate samples or train verifiers on human labels, but cannot guarantee logical soundness. Self-refinement approaches (Madaan et al., 2023; Shinn et al., 2023; Welleck et al., 2022) use the model to critique itself, often failing due to the faithfulness gap where reasoning does not match output (Lyu et al., 2023; Huang et al., 2024). Multi-agent debate (Du et al., 2023; Liang et al., 2023) achieves consensus, not truth, recent work shows self-correction can even degrade performance (Huang et al., 2024; Kamoi et al., 2024). In contrast, VERGE uses SMT solvers (Barrett et al., 2009; De Moura and Bjørner, 2008) to provide mathematically proven feedback. Unlike tool-augmented LLMs (Schick et al., 2023; Gou et al., 2024) that use tools for execution (e.g., calculators), we use tools for consistency checking, computing MCS. (Zhang and Malik, 2003; Nadel et al., 2014) to identify exactly which premises contradict the generated answer. 2 Neuro-Symbolic Integration and the Semantic Gap. Traditional semantic parsing (Dong and Lapata, 2016; Berant et al., 2013; Zettlemoyer and Collins, 2005) maps language to executable logical forms but requires expensive supervision. Recent work extends this to theorem proving (Polu and Sutskever, 2020; Polu et al., 2022; Jiang et al., 2022; Azerbayev et al., 2023) and augmenting LLMs with symbolic solvers (Pan et al., 2023; Olausson et al., 2023; Callewaert et al., 2025), but these require fully formalizable domains. Prior neuro-symbolic integration (Mao et al., 2019; Garcez et al., 2019; Kautz, 2022) and grammarbased approaches (Ganguly et al., 2025, 2024) struggle with the semantic gap (Church, 1936; Turing, 1936) the mismatch between ambiguous natural language and rigid formal systems. VERGE targets open-domain natural language where full formalization is often impossible. We introduce semantic routing, rather than forcing vague or commonsense claims into rigid first-order logic, we route them to consensus-based soft verifier. This treats the semantic gap as an inherent property of language requiring hybrid verification. Automated Reasoning for Repair. Our feedback mechanism adapts MCS computation (Marques-Silva et al., 2013; Liffiton and Malik, 2008) from constraint programming to NLP. MCS identifies the minimal set of constraints to delete to restore satisfiability. Recent work applies MCS to constraint relaxation (Bacchus and Katsirelos, 2015) and automated debugging (Morgado et al., 2013). We innovate by translating MCS output into natural language feedback, guiding the LLM to rewrite specific atomic claims. This prioritizes interpretability and convergence speed (O(m SAT) greedy approximation) over theoretical optimality. To our knowledge, VERGE is the first to apply MCS-based feedback to guide iterative refinement in LLM reasoning, converting abstract unsat cores into actionable guidance (see Appendix B)."
        },
        {
            "title": "3 Methodology",
            "content": "} Problem Formulation. Given context = of premise statements and query p1, . . . , pm { comq, we aim to produce verified answer with maximal posed of atomic claims is refined ververification coverage. Here, sion of the candidate answer obtained through verification. We formalize this as maximizing [0, 1] subject to the verification score ( c1, . . . , cn { } ) (1) Consistwo constraints for each claim ci: φci) = true, and (2) Entailment: tency: SAT(φC SAT(φC φci) = false, where φ denotes the logical formalization function that maps natural language statements to SMT constraints. The consistency constraint verifies that each claim is compatible with the context, while the entailment constraint ensures that each claim is logical consequence of the context. These constraints provide formal equivalence guarantees where logic permits, while falling back to semantic consistency (soft verification, see 3.4) otherwise. Pipeline. The pipeline  (Fig. 2)  executes iteratively: (1) entity extraction, (2) generation, (3) decomposition, (4) formalization & verification, and (5) refinement. 3.1 Entity Extraction and Generation We first extract entities , q) (e.g., Felix, Monday, Process A) to serve as typed constants in SMT. At iteration t, we generate answer (t) via language model : = Extract( (t) = ( , q, (t1), (t1)) (1) (t1) is structured feedback (see 3.5). where The initial iteration (t = 1) utilizes zero-shot prompting (e.g., A0 = ( , q, )). , 3.2 Claim Decomposition and Classification into atomic claims We decompose . } To ensure our system is honest about what it can and cannot formally prove, we classify each claim ci into semantic type τi c1, . . . , cn via : { τi = arg max τ {τM,τL,τT,τP,τC,τV} PM (τ ci) (2) where types correspond to Mathematical, Logical, Temporal, Probabilistic, Commonsense, and Vague claims. These categories align with standard distinctions in semantic parsing to differentiate verifiable facts from subjective or probabilistic statements. This classification is crucial for minimizing \"false formalization\", the error of forcing ambiguous natural language into rigid logic. Vague claims are identified by the model as containing subjective predicates (e.g., \"likely\", \"possibly\") that lack binary truth values, preventing brittle SMT assertions. 3.3 SMT Formalization with Consensus For claims classified as logical or mathematical, we target the QF_UF (Quantifier-Free Uninterpreted Functions) and QF_LIA (Quantifier-Free ; Generation produces and refines answers Figure 2: Overview of VERGE: The pipeline is structured into five distinct stages: Setup prepares the context (t) iteratively; Formalization classifies claim types τi and and entities generates SMT formulas φci; Verification routes claims to SMT, Soft, or Hybrid verifiers based on semantic type; (t) for the and Decision computes the aggregate score next iteration. ) to either accept the answer or generate feedback ( F Linear Integer Arithmetic) fragments within the SMT-LIB2 standard. Given the context and query q, we first extract entities as set of typed constants. For example, from the statement All humans above age 18 are adults, we extract entities age of type and adult of type AGE_GROUP, which are declared as uninterpreted constants of the corresponding sorts. We then use generate can- ) for each claim ci didate formulas ϕ = (ci, over the vocabulary in . To mitigate the stochastic nature of autoformalization, we generate = 3 and check for candidate formulas consensus. Instead of relying on brittle syntactic overlap, we compute consensus based on semantic equivalence (see Appendix C.2 for the formal definition). Two formulas ϕa and ϕb are deemed equivalent if and only if their bidirectional entailment is valid, which we verify by querying the SMT solver: ϕ1, . . . , ϕK { } (ϕa UNSAT (ΣE Equiv(ϕa, ϕb) ϕb)) (3) where ΣE represents the declarations of constants and uninterpreted functions derived from the entity extraction phase. This procedure constructs semantic equivalence graph where edges represent proven logical identity. K/2 ). Adsemantic equivalence clique is ditionally, we employ Round-Trip Translation (SMT Natural Language) as semantic sanity check to ensure alignment with the source text. If consensus fails or the confidence (derived from clique size and round-trip similarity) is low, we trigger self-correction step φnew = (φ, . . . ), constrained such that φnew = φ. This ensures the system strictly strengthens (and never weakens) the constraints during refinement. The strengthening constraint is verified by checking the unsatisfiability of φnew φ. To ensure decidability, we perform this check over the finite domain of entities extracted in the setup phase, effectively reducing the check to propositional logic or quantifier-free first-order logic(QF-FOL). 3.4 Verification Cascade We employ hybrid strategy that routes claims to the most rigorous verifier available for their type, prioritizing formal guarantees where applicable. Semantic Routing (Flexibility Mechanism). We define routing function τi: SMT-Verify Soft-Verify Hybrid τi = τM, τL, τT τC, τV, τP if τi } if τi } else (or SMT error) { { (4) formalization is accepted only if majority consensus is reached (i.e., the size of the largest While SMT provides provable correctness, it cannot natively handle vague predicates (e.g., is simi4 lar to) without excessive and fragile axioms. By routing these to Soft-Verify, VERGE preserves logical rigor where possible while preventing the system from falsely treating probabilistic or ambiguous reasoning as logical certainty. SMT-Based Verification. For logic-amenable claims, we check satisfiability using the Z3 solver. We assign statuses based on rigorous logical tests: φci) = False. Contradictory (σC): SAT(φC The claim violates the context. Entailed (σE): SAT(φC φci) = False. The claim is proven true (proof by contradiction). Possible (σP): Consistent (SAT(φC φci) = True) but not entailed. The context allows the claim but does not force it. Unknown (σU): Solver timeout or execution error. φci S, SAT(φC Minimal Correction Sets (MCS). For contradictions (σC), we compute the MCS to generate precise feedback. Let φci be set of clauses, the subset of φci is minimal correction set (MCS) if S) = true φci SAT(φC Intuitively, MCS is small subset of clauses to remove to restore satisfibility. This provides actionable guidance to address contradictions (e.g., \"remove constraint X\") rather than generic error messages. See Appendix for details about MCS computation. S) = false vote claims Soft Verification. For unsuitable for SMT, we use an ensemble of LLM judges. We compute confidence weighted majority confi- (using dence of the judge) by defining verdicts Supported, Plausible, Unsupported, Uncertain Constraint: To penalize the lack of formal guarantees, soft-verified claims are capped at lower maximum score contribution than SMT-verified claims (see 3.5). self-reported . } { Hybrid Verification. The Hybrid strategy acts as robustness fallback. Claims routed to SMT that fail due to non-logical errors (e.g., syntax errors, undeclared variables, or timeouts) are automatically re-routed to Soft Verification. This prevents the pipeline from stalling on \"Unknown\" (σU ) statuses due to correctable formalization issues, allowing the system to degrade gracefully from formal proof to probabilistic consensus. 3.5 Score Aggregation We compute an aggregated verification score for the entire answer by combining verification results from both soft and hard verification across be an answer to query all atomic claims. Let given context is decomposed into atomic claims c1, . . . , cn and verified against and to produce verification results σ1, . . . , σn, respecC tively. The verification score S(σi) for each result is defined as: . Suppose C S(σi) = 1.0 0.9 0.7 0.0 if Entailed(σi) if Supported(σi) if Possible(σi) if Contradictory(σi) or else (5) These weights reflect verification rigor: formally entailed claims receive the maximum score (1.0), while soft-verified supported claims receive 0.9, ensuring the system favors provable logic over semantic consistency. The final aggregated score ) ( integrates variance-based penalty (Eq. 6) to discourage gaming the system, where model might generate claims that are individually confident but mutually contradictory under joint verification: ) = ( (cid:18) max 0.5, 1.0 (cid:19) σS + 0.01 (6) where is the mean of the verification scores and σS is their standard deS(σ1), . . . , S(σn) { viation. } ) ( (t) containing: Iterative Refinement. At each iteration t, we (1) Unsat generate feedback Cores & MCS for contradictions, pinpointing exact logical conflicts; (2) Joint Conflicts for mutually incompatible claims; and (3) Formalization Alerts for low-confidence mappings. The process 0.75 and JointSAT=True repeats until (where JointSAT is the boolean result of the joint satisfiability check), or until convergence ( < 0.01). For Joint Consistency, soft-verified claims are treated as atomic boolean variables (bi) within the SMT solver. To capture interactions between soft and hard claims, the context formalization φC includes bridging axioms generated by the LLM (e.g., assertions linking vague predicates like small to numerical bounds). This allows the solver to detect if mathematically proven claim (τM ) contradicts commonsense claim (τC) (e.g., Mathematical Claim > 10 vs Commonsense Claim is small single-digit number which implies < 10), ensuring holistic consistency even without full formalization of the commonsense component. If claims are not jointly consistent, we compute the MCS over the claims as refinement feedback, signaling minimal patch to restore joint consistency. Algorithm 1 (Appendix) details the complete refinement procedure."
        },
        {
            "title": "4 Results",
            "content": "4.1 Benchmarking Neuro-Symbolic Reasoning Table 1 presents systematic evaluation of VERGE against state-of-the-art inference-time compute (prompting) strategies. To ensure comparable computational budgets, we configure SelfConsistency (SC) with = 3 samples and SelfRefinement (SR) with = 3 iterations with selfcritique. Results for established neuro-symbolic baselines (Proof of Thought, LINC, Logic-LM) were computed using their officially released codebase. Consistent with these parameters, VERGE operates with maximum budget of Tmax = 3 iterations. This setting balances accuracy with computational cost, whereas baselines like CoT represent single-pass performance (for convergence analysis up to Tmax = 10, see Figure 3). Consequently, the improvements shown represent the specific value of iterative verification-guided refinement. To distinguish these gains from those achieved by iteration alone, we refer to our ablation study  (Table 2)  . By comparing the full system against the w/o MCS and w/o Routing variants (both of which also operate iteratively), we isolate the substantial performance contributions of VERGEs verification and feedback mechanisms. General Performance Trends and Robustness. As evidenced in Table 1, VERGE consistently outperforms standard prompting and existing neurosymbolic baselines on 5 out of the 6 evaluated benchmarks. The method demonstrates robust scaling across model sizes, notably on the Humanities Last Exam (HLE), where it improves GPT-OSS120B performance from 14.2% (CoT) to 30.5%. This contrasts with traditional neuro-symbolic baselines (DSB, LogicLM), which suffer from translation bottleneck, where invalid SMT specifications cause the solver to fail silently or reject valid reasoning. VERGE overcomes this via its Verification Cascade, which utilizes Minimal Correction Sets (MCS) to isolate specific formalization errors. By iteratively refining the context based on solver feedback (Unsat Cores) rather than discarding the entire proof, VERGE successfully salvages logical entailments that probabilistic baselines miss (see Appendix F). notable outlier is the ProofWriter dataset, where Proof of Thought (PoT) retains dominance (98.4% vs. 89.9%). This performance gap highlights fundamental methodological distinction between monolithic execution and modular verification. PoT approaches reasoning as program synthesis, converting the full context into single executable artifact to derive the answer in one pass. This is ideal for the rigid, deductive structure of synthetic datasets like ProofWriter. In contrast, VERGE treats reasoning as semantic entailment task, employing routing mechanism to decompose and verifying individual atomic claims. On purely synthetic tasks, this general-purpose machineryspecifically the overhead of claim decomposition and semantic routingintroduces unnecessary complexity compared to PoTs direct solver execution. However, it is precisely this modular flexibility that allows VERGE to generalize to semantically complex domains like Law (AR-LSAT), where monolithic translation to executable logic often fails due to linguistic ambiguity. 4.2 Ablation study: The Role of MCS, Routing, and Feedback systems Impact of Architectural Components. Table 2 isolates the contributions of VERGEs key components using GPT-OSS-120B. The full pipeline consistently outperforms all ablated variants, confirming that both Minimal Correction Sets (MCS) and Semantic Routing are integral to maximizing VERGEs performance. We observe that MCS is particularly critical for strict constraint satisfaction tasks. On AR-LSAT, removing MCS causes sharp drop from 91.7% to 83.0%. This indicates that while the solver can detect contradictions, the model struggles to resolve complex scheduling conflicts without the precise, minimal deletion guidance provided by the MCS. Conversely, Semantic Routing proves advantageous for open-ended and commonsense reasoning. Removing the routing mechanism, thereby forcing all claims into formal logic, substantially impacts HLE and BBEH, with HLE scores halving from 30.5% to 15.2%. This lends support to the Formalization Barrier hypothesis (discussed in 4.4): Table 1: Comprehensive Performance Analysis. Results are averaged over 5 independent runs, with error bars (subscript) indicating standard deviation. We compare VERGE against standard prompting (CoT, SC, SR) and neuro-symbolic baselines (DSB, LogicLM, LINC, PoT) across three different backbone models: GPT-OSS-20B, GPT-OSS-120B, and Claude 3.7 Sonnet. VERGE consistently outperforms baselines across diverse domains and model sizes, except on ProofWriter where the specialized PoT method remains dominant. indicates that the baseline does not support the dataset. Dataset Model CoT SC SR DSB LogicLM LINC PoT w/o MCS w/o Rt Full Prompting Neuro-Symbolic Baselines VERGE (Ours) FOLIO ProofWriter ZebraLogic AR-LSAT BBEH HLE GPT-20B 40.4 52.2 34.0 53.7 GPT-120B 32.0 35.5 42.9 14.0 Sonnet-3.7 70.4 72.4 62.1 44.5 GPT-20B 74.6 71.4 56.8 38.8 GPT-120B 52.4 65.6 67.2 31.4 Sonnet-3.7 71.6 86.6 74.0 42.8 GPT-20B 67.6 72.2 46.2 44.8 GPT-120B 84.0 77.8 72.2 28.2 Sonnet-3.7 52.0 51.0 58.8 48. GPT-20B 81.7 89.1 63.9 59.6 GPT-120B 87.8 88.7 84.8 32.2 Sonnet-3.7 61.7 58.7 73.5 67.8 GPT-20B 34.4 38.4 28.0 10.0 GPT-120B 38.4 41.8 37.4 20.2 Sonnet-3.7 33.0 29.4 37.8 24.0 9.6 15.0 8.6 GPT-20B 1.4 GPT-120B 14.2 14.0 12.8 6.4 0.6 Sonnet-3.7 5.8 6.8 5. 29.9 27.5 71.6 35.8 32.0 64.7 - - - 21.7 19.9 31.2 - - - - - - 18.6 48.8 47.5 54.2 65.2 58.1 42.8 94.0 76.4 98.4 71.1 98.2 - - - - - - - - - - - - - - - - - - - - - - - - 73.9 80.7 86.7 82.7 88.7 90. 80.9 88.9 58.0 82.6 83.0 88.2 42.2 54.1 40.4 12.2 21.0 16.7 86.7 81.6 83.0 85.8 84.6 88. 83.6 90.9 62.8 86.8 87.4 87.7 43.7 50.2 43.5 13.7 15.2 14.7 89.21.1 84.70.9 87.90.7 85.21.3 89.90.8 93.00. 87.31.0 91.00.6 64.81.4 89.50.8 91.70.5 88.60.9 49.91.5 58.91.2 45.91.4 19.91.1 30.51.6 17.20.9 Table 2: Ablation Study on Component Contributions (GPT-OSS-120B). We isolate the impact of Minimal Correction Subsets (MCS), Semantic Routing (Rt), and the SMT Solver itself. Full represents the complete VERGE pipeline. w/o Routing forces SMT verification for all claims. Soft-Only removes the SMT solver entirely, relying solely on LLM consensus. Unsat-Core-Only and MSF vary the granularity of the feedback. Architectural Components Feedback Granularity Dataset Full w/o MCS w/o Routing Soft-Only Unsat-Core Only MSF 84.7 FOLIO ProofWriter 89.9 ZebraLogic 91.0 91.7 AR-LSAT 58.9 BBEH 30.5 HLE 80.7 88.7 88.9 83.0 54.1 21.0 81.6 84.6 90.9 87.4 50.2 15. 80.8 75.8 70.2 84.6 41.7 13.2 79.3 82.4 85.3 89.4 42.7 23.2 76.5 80.1 72.5 89.2 40.1 19.8 Avg. Drop -6.8% -8.3% -22.8% -10.9% -15.3% attempting to formalize vague or fuzzy predicates leads to brittle systems that reject valid reasoning (e.g., solver might reject valid commonsense claim due to missing axiom). Routing allows VERGE to fallback to soft verification when necessary, benefit most pronounced in domains with 7 high ambiguity like HLE. Feedback Granularity. The right side of Table 2 demonstrates the value of high-resolution feedback. There is clear hierarchy of performance correlated with feedback specificity. Unsat-Core Only feedback, which identifies conflicting constraints but does not prescribe fix, lags behind the full model by 10.9% on average. Minimal Solver Feedback (MSF) performs worst, with 15.3% average drop. This confirms that simply telling an LLM you are wrong (UNSAT) (binary feedback) is insufficient for complex reasoning; the model requires the actionable, structural guidance that VERGE provides. To address the converse hypothesis whether formal verification is necessary at all we evaluated Soft-Only variant where all claims are routed to the LLM consensus mechanism, bypassing the SMT solver entirely. As shown in Table 2, this results in the most significant performance degradation across the board (average drop: -22.8%). The impact is most severe on strict constraint sat70.2%) isfaction tasks like ZebraLogic (91.0% and ProofWriter (89.9% 75.8%), confirming that while soft verification is useful for ambiguity, it lacks the precision required to resolve complex logical dependencies. This effectively validates the necessity of the hybrid approach: VERGE requires SMT for precision and Semantic Routing for flexibility. Semantic Router Reliability. To validate our routing mechanism, we evaluated the classifier on stress-test dataset (also see Appendix C.3) of = 54 claims, including adversarial edge cases such as idioms (e.g., gave 110%) and vague quantifiers. The router achieved an overall accuracy of 94%, with strong discrimination between categories (SMT: F1=0.93; Soft: F1=0.95). Crucially, the error patterns reveal safe failure mode: only 1 out of 32 soft claims was misrouted to SMT; recoverable error that triggers autoformalization fallback. The high recall for Soft claims (0.97) and precision for SMT claims (0.95) confirm the system effectively shields the solver from ambiguity while preserving logical rigor. 4.3 Efficiency and Convergence Fig. 3 (in Appendix) reveals striking divergence in refinement dynamics. VERGE achieves monotonic improvement across all six datasets (Kendalls τ = 1.0, < 0.001), with average convergence at iteration 6.2 (σ = 1.3). In contrast, probabilistic self-refinement exhibits what we term the correlation cliff phenomenon: performance systematically degrades in 85.2% of trials (χ2 = 26.7, < 0.001), characterized by strong negative correlation between iteration and accuracy (τ = 0.84 average, < 0.001). This could be due to self-refinement (without formal verification) introducing hallucinations where the model \"corrects\" valid reasoning into invalid states. This convergence analysis provides the strongest statistical evidence for VERGEs value: while final accuracy gains are modest (average +9.3 points), the observed consistent monotonic improvement has high practical value in production systems where reliability matters more than peak performance. 4.4 Model Scalability and The Formalization Barrier We observe that effectiveness is contingent on the models ability to produce syntactically valid SMT code. We term this threshold the Formalization Barrier. Our tested model under 20B parameters (GPTOSS-20B) struggles, achieving only 30% syntax validity. In this regime, the solver acts merely as spell-checker. However, models in the 120B+ and frontier regime (GPT-OSS-120B, Sonnet) cross the barrier (>90% validity). Here, solver feedback shifts from generic error messages to semantic logical contradictions, enabling VERGEs MCS mechanism to perform actual reasoning repairs. This suggests that neuro-symbolic verification is capability that emerges with scale, and VERGE is uniquely positioned to leverage the next generation of highly capable reasoning models."
        },
        {
            "title": "5 Conclusions",
            "content": "We present VERGE, neuro-symbolic framework combining LLMs with SMT solvers for verified reasoning via iterative refinement. Our approach introduces three key innovations: (1) high-fidelity formalization via multi-model consensus, (2) semantic routing that balances symbolic solvers with soft verification, and (3) actionable feedback via Minimal Correction Subsets (MCS) for precise error localization. Evaluation shows VERGE excels in formal reasoning and achieves convergence across all datasets, contrasting with the degradation often observed in probabilistic self-refinement. Our analysis identifies Formalization Barrier at the 70B+ parameter scale where semantic verifica8 tion becomes viable. By bridging neural generation with symbolic reasoning, VERGE provides practical step toward trustworthy AI with provable correctness where logic permits."
        },
        {
            "title": "6 Limitations",
            "content": "Computational Overhead. VERGE incurs significantly higher latency than single-pass generation. Each iteration requires claim decomposition, multiple formalization attempts (K = 3), consensus computation, SMT solver calls, and feedback generation. For problems with > 20 atomic claims, the pipeline requires 15-30 seconds per iteration compared to > 2 seconds for standard Chainof-Thought prompting. While our greedy MCS approximation reduces complexity from exponential O(2n) to linear O(n SAT), the multiplicative overhead remains substantial. This latencyaccuracy trade-off limits deployment in interactive applications requiring sub-second response times (e.g., conversational AI, real-time decision support). Formalization Barrier and Access Inequality. Our analysis reveals models under 20B parameters achieve only 30% formalization validity, restricting the solver to syntax checking rather than semantic verification. This creates capability threshold where only organizations with access to frontier models ( 70B parameters) can leverage VERGEs full potential. Additionally, restricting to decidable logics (QF-UF, LIA) sacrifices expressivenessclaims requiring universal quantification, non-linear arithmetic, or recursive definitions cannot be formally verified and must fall back to soft verification. This undermines the frameworks promise of provability for complex mathematical or algorithmic reasoning."
        },
        {
            "title": "7 Ethical Considerations",
            "content": "Logical Correctness is not Ethical Correctness. VERGE verifies internal consistency and logical entailment, not moral soundness or factual truth. The system could formally prove harmful reasoningdiscriminatory policies that satisfy legal constraints, exploit chains in cybersecurity, or conspiracy theories with internally consistent logic but false premises. SMT solvers are fundamentally value-neutral tools. Deploying VERGE in highstakes domains (legal, medical, military decisionmaking) requires additional ethical oversight layers: premise provenance tracking, factuality verification 9 orthogonal to logic, and human expert review for consequential decisions. Risk of Overconfidence and Misplaced Trust. Labeling outputs as \"verified\" may induce false confidence in users unfamiliar with the distinction between formal and soft verification. Our scoring system assigns high scores (0.9) to soft-verified commonsense claims that lack mathematical guarantees. More critically, if autoformalization misrepresents claims semantics producing syntactically valid but semantically incorrect SMT code the solver verifies the wrong statement, creating \"verified hallucinations.\" Users may over-rely on verification badges without understanding rigor gradations. Clear interface design distinguishing \"formally proven\" (σE) from \"consensus-supported\" (νS) claims is essential but insufficient if users lack technical literacy."
        },
        {
            "title": "References",
            "content": "Anthropic. 2025. Introducing Claude Opus 4.5. Accessed: 2025-12-19. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and 1 others. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732. Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf, Edward Ayers, Dragomir Radev, and Jeremy Avigad. 2023. ProofNet: Autoformalizing and formally proving undergraduate-level mathematics. arXiv preprint arXiv:2302.12433. Fahiem Bacchus and George Katsirelos. 2015. Finding In Internaa collection of MUSes incrementally. tional Conference on Integration of Constraint Programming, Artificial Intelligence, and Operations Research, pages 3544. Springer. Clark Barrett, Roberto Sebastiani, Sanjit A. Seshia, and Cesare Tinelli. 2009. Satisfiability modulo theories. In Handbook of Satisfiability, volume 185, pages 825885. IOS Press. Anton Belov, Mikoláš Janota, Inês Lynce, and Joao Marques-Silva. 2012. On computing minimal correction subsets. In International Joint Conference on Artificial Intelligence, pages 615622. Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 15331544. Benjamin Callewaert, Simon Vandevelde, and Joost Vennekens. 2025. Verus-lm: versatile framework for combining llms with symbolic reasoning. arXiv preprint arXiv:2501.14540. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, and 1 others. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. Alonzo Church. 1936. An unsolvable problem of elementary number theory. American Journal of Mathematics, 58(2):345363. Leonardo De Moura and Nikolaj Bjørner. 2008. Z3: An efficient SMT solver. In International Conference on Tools and Algorithms for the Construction and Analysis of Systems, pages 337340. Springer. Li Dong and Mirella Lapata. 2016. Language to logical form with neural attention. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, volume 1, pages 3343. Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch. 2023. Improving factuality and reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325. Yu Feng, Nathaniel Weir, Kaj Bostrom, Sam Bayless, Darion Cassel, Sapana Chaudhary, Benjamin KieslReiter, and Huzefa Rangwala. 2025. Vericot: Neurosymbolic chain-of-thought validation via logical consistency checks. arXiv preprint arXiv:2511.04662. Debargha Ganguly, Srinivasan Iyengar, Vipin Chaudhary, and Shivkumar Kalyanaraman. 2024. PROOF OF THOUGHT : Neurosymbolic program synthesis allows robust and interpretable reasoning. In The First Workshop on System-2 Reasoning at Scale, NeurIPS24. Debargha Ganguly, Vikash Singh, Sreehari Sankar, Biyao Zhang, Xuecen Zhang, Srinivasan Iyengar, Xiaotian Han, Amit Sharma, Shivkumar Kalyanaraman, and Vipin Chaudhary. 2025. Grammars of formal uncertainty: When to trust LLMs in automated reasoning tasks. In The Thirty-ninth Annual Conference on Neural Information Processing Systems. Artur dAvila Garcez, Marco Gori, Luis C. Lamb, Luciano Serafini, Michael Spranger, and Son N. Tran. 2019. Neural-symbolic computing: An effective methodology for principled integration of machine learning and reasoning. Journal of Applied Logics, 6(4):611632. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. 2024. CRITIC: Large language models can self-correct with tool-interactive critiquing. In International Conference on Learning Representations (ICLR). Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Wenfei Zhou, James Coady, David Peng, Yujie Qiao, Luke Benson, and 1 others. 2024. Folio: Natural language reasoning with first-order logic. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 2201722031. Mingqian He, Yongliang Shen, Wenqi Zhang, Qiuying Peng, Jun Wang, and Weiming Lu. 2025. Star-sql: Self-taught reasoner for text-to-sql. arXiv preprint arXiv:2502.13550. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the MATH dataset. Advances in Neural Information Processing Systems, 34:6901 6914. Jie Huang, Xinyun Gu, Lisha Shen, Weijia Shi, Qizhe Yuan, Jiawei Wang, Xianjun Zhao, Keze Zhou, Linjun Zhang, Jianlong Yu, and 1 others. 2024. Large language models cannot self-correct reasoning yet. arXiv preprint arXiv:2310.01798. Albert Jiang, Sean Welleck, Jin Peng Zhou, Wenda Li, Jiacheng Liu, Mateja Jamnik, Timothee Lacroix, Yuhuai Wu, and Guillaume Lample. 2022. Draft, sketch, and prove: Guiding formal theorem provers with informal proofs. arXiv preprint arXiv:2210.12283. Ryo Kamoi, Olga Golovneva, Esin Durmus, Asli Celikyilmaz, and Yejin Cao. 2024. Can LLMs critique and correct their own outputs? arXiv preprint arXiv:2406.01297. Henry Kautz. 2022. The third AI summer: AAAI Robert S. Engelmore memorial lecture. AI Magazine, 43(1):105125. Mehran Kazemi, Bahare Fatemi, Hritik Bansal, John Palowitch, Chrysovalantis Anastasiou, Sanket Vaibhav Mehta, Lalit Jain, Virginia Aglietti, Disha Jindal, Yuanzhu Peter Chen, and 1 others. 2025. BigIn Proceedings of the 63rd Anbench extra hard. nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 26473 26501. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, and 1 others. 2022. Solving quantitative reasoning problems with language models. In Advances in Neural Information Processing Systems, volume 35, pages 38433857. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. 2023. Encouraging divergent thinking in large language models through multi-agent debate. arXiv preprint arXiv:2305.19118. Mark Liffiton and Ammar Malik. 2008. Algorithms for computing minimal unsatisfiable subsets of constraints. Journal of Automated Reasoning, 40(1):1 33. 10 Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Lets verify step by step. arXiv preprint arXiv:2305.20050. Bill Yuchen Lin, Ronan Le Bras, Kyle Richardson, Ashish Sabharwal, Radha Poovendran, Peter Clark, and Yejin Choi. 2025. Zebralogic: On the scaling limits of llms for logical reasoning. arXiv preprint arXiv:2502.01100. Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Faithful chain-ofChris Callison-Burch. 2023. thought reasoning. arXiv preprint arXiv:2301.13379. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, and 1 others. 2023. Self-refine: Iterative refinement with self-feedback. In Advances in Neural Information Processing Systems, volume 36. Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B. Tenenbaum, and Jiajun Wu. 2019. The neurosymbolic concept learner: Interpreting scenes, words, and sentences from natural supervision. In International Conference on Learning Representations. Joao Marques-Silva, Federico Heras, Mikoláš Janota, Alessandro Previti, and Anton Belov. 2013. On computing minimal correction subsets. In International Joint Conference on Artificial Intelligence, pages 615 622. Lachlan McGinness and Peter Baumgartner. 2024. Automated theorem provers help improve large arXiv preprint language model reasoning. arXiv:2408.03492. Meta AI. 2025. The llama 4 herd: The beginning of new era of natively multimodal intelligence. Technical report, Meta. Technical Report. Antonio Morgado, Federico Heras, Mark Liffiton, Jordi Planes, and Joao Marques-Silva. 2013. Iterative and core-guided MaxSAT solving: survey and assessment. Constraints, 18(4):478534. Alexander Nadel, Vadim Ryvchin, and Ofer Strichman. 2014. Efficient MUS extraction with resolution. In Formal Methods in Computer-Aided Design, pages 197200. IEEE. Theo Olausson, Alex Gu, Ben Lipkin, Cedegao Zhang, Armando Solar-Lezama, Joshua Tenenbaum, and Roger Levy. 2023. Linc: neurosymbolic approach for logical reasoning by combining language models with first-order logic provers. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 51535176. OpenAI. 2025. Introducing OpenAI o3 and o4-mini. Accessed: 2025-12-19. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, and 1 others. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, volume 35, pages 2773027744. Liangming Pan, Alon Albalak, Xinyi Wang, and Logic-lm: EmWilliam Yang Wang. 2023. powering large language models with symbolic solvers for faithful logical reasoning. Preprint, arXiv:2305.12295. Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, and 1 others. 2025. Humanitys last exam. arXiv preprint arXiv:2501.14249. Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, and Ilya Sutskever. 2022. Formal mathematics statement curriculum learning. In International Conference on Learning Representations. Stanislas Polu and Ilya Sutskever. 2020. Generative language modeling for automated theorem proving. arXiv preprint arXiv:2009.03393. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. In Advances in Neural Information Processing Systems, volume 36. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning. In Advances in Neural Information Processing Systems, volume 36. Oyvind Tafjord, Bhavana Dalvi, and Peter Clark. 2021. ProofWriter: Generating implications, proofs, and abductive statements over natural language. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 36213634. Alan M. Turing. 1936. On computable numbers, with an application to the Entscheidungsproblem. Proceedings of the London Mathematical Society, s242(1):230265. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. 2022. Solving math word problems with process-and outcomebased feedback. arXiv preprint arXiv:2211.14275. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. In International Conference on Learning Representations. 11 Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. 2022. Generating sequences by learning to selfcorrect. In International Conference on Learning Representations. Jie Weng, Oumaima Kitouni, Fanghua Shi, Neel Gupta, Preetum Nakkiran, Boaz Barak, Sham Chaudhuri, and Shunyu Yao. 2023. Can large language models self-verify? arXiv preprint arXiv:2310.11638. Luke S. Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In Conference on Uncertainty in Artificial Intelligence, pages 658666. Lintao Zhang and Sharad Malik. 2003. Extracting small unsatisfiable cores from unsatisfiable boolean formula. In International Conference on Theory and Applications of Satisfiability Testing, volume 2003. Wanjun Zhong, Siyuan Wang, Duyu Tang, Zenan Xu, Daya Guo, Jiahai Wang, Jian Yin, Ming Zhou, and Nan Duan. 2021. Ar-lsat: Investigating analytical reasoning of text. arXiv preprint arXiv:2104.06598. VERGE: Algorithm / Pseudo code"
        },
        {
            "title": "Computation",
            "content": "To provide actionable feedback when generated answer is contradictory, VERGE must identify Minimal Correction Subset (MCS): minimal subset of atomic claims that, if removed, restores consistency. Formally, finding an MCS is equivalent to finding the complement of Maximal Satisfiable Subset (MSS). While exact algorithms ensure the theoretically smallest removal set, they require exploring the combinatorial search space of all subsets, leading to exponential time complexity (O(2n)). For an LLM inference pipeline where latency is critical, this approach is computationally intractable. The Greedy Approximation. To address this, we employ linear-scan approximation (c.f. Algorithm 1; (Marques-Silva et al., 2013)). The algorithm iterates through atomic claims c1, . . . , cn sequentially. For each claim, it checks if adding it to the current set of consistent claims preserves satisfiability (SAT). If SAT(Context ci) holds, ci is kept; otherwise, it is marked for removal. Current_Set This reduces the complexity from exponential SAT)). As shown in Table 3, the to linear (O(n 12 computational disparity becomes prohibitive even for moderate claim counts (n = 20). The trade-off is that the greedy approach is orderdependent and may not find the global maximum subset (e.g., it might recommend deleting 3 claims when deleting 2 would suffice). However, in the context of Iterative Refinement, actionability prioritizes optimality. Getting \"valid enough\" correction signal in seconds is practically superior to waiting hours for mathematically perfect one. The greedy MCS provides specific, consistent sub-context that guides the LLM effectively, even if it is sub-optimal. Stability & Order Dependence: Greedy MCS is order-dependent and to maximize the retention of high-quality reasoning, we sort claims by their individual verification confidence (descending) before the greedy pass. This prioritizes keeping Entailed or High-Confidence Soft claims and suggests modifying the weakest links first. In preliminary tests, this sorting reduced feedback variance significantly compared to random ordering."
        },
        {
            "title": "C Semantic Claim Type Definitions",
            "content": "To ensure the Semantic Router directs claims to the appropriate verification strategy (as described in Section 2.2 and Eq. 2), we formally define the six semantic categories (τ ) used by VERGE in Table (4). The routing decision is binary based on these types: Hard Verification (SMT) is applied to deterministically provable claims (τM , τL, τT ), while Soft Verification (Consensus) is applied to claims requiring world knowledge or subjective interpretation (τC, τV , τP ). C.1 Technical Implementation Details To ensure reproducibility and formal rigor, we define the specific mechanisms used for claim decomposition, autoformalization, and consensus scoring. C.1.1 Atomic Decomposition Strategy We define an Atomic Claim ci as the minimal semantic unit that carries truth value independent of other claims. Given generated answer , we employ zero-shot decomposition funcA tion fdecomp : ( . To pre- } vent context loss, we enforce that every ci must be self-contained (i.e., resolving pronouns like \"he\" to specific entities such as \"Felix\"). This is implemented via structure-enforcing prompt that c1, . . . , cn { ) , Algorithm 1 Iterative Verification and Refinement (VERGE) ) {Boolean var for Soft claims} , τi, φci ) return A(t) ckV φck ) Update best score: best_score (t), best_answer A(t) end if if (t) τacc and JointSAT then Generate answer: A(t) LLM(C, q, (t1)) Extract claims: {c(t) } Decompose(A(t)) Classify claims: τi SemanticRouter(c(t) ) if IsFormal(τi) then φci , αi FormalizeConsensus(c(t) ) else φci BooleanAbstract(c(t) ), αi SoftConf(c(t) Verify individual claims: si RouteAndVerify(c(t) Identify valid subset: = {ci si is not Contradictory} Verify joint consistency: JointSAT, Core Solver(φC (cid:86) Compute score: (t) AggScore({si}, JointSAT, {αi}) if (t) > best_score then 1: Initialize A(0) null, (0) 0 2: Extract entities: EntityExtraction(C, q) 3: Formalize context: φC AutoFormalize(C, E) 4: if SAT(φC) then φC RefineContext(φC) {Handle Translation Bottleneck} 5: best_answer null, best_score 0 6: for = 1 to Tmax do 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: 33: 34: 35: 36: 37: 38: 39: 40: 41: 42: 43: end for 44: return best_answer end if {Feedback Generation} Findiv GetIndividualErrors({ci} ) if JointSAT then {Sorted Greedy MCS on Valid Claims} Sort by confidence α descending MSS , MCS for ck do end if if Convergence detected (S < ϵ) then end for Fjoint FormatFeedback(MCS, Core) if Solver(φC MSS φck ) is SAT then end if (t) Findiv Fjoint MCS MCS {ck} {Conflict found} Fjoint IdentifyWeakClaims(V ) MSS MSS {ck} return best_answer end if else else outputs JSON list of claims, preventing the hallucination of non-existent dependencies. C.1.2 Formalization into SMT-LIB2 For claims classified as τLogic or τM ath, we target the QF_UF (Quantifier-Free Uninterpreted Functions) and LIA (Linear Integer Arithmetic) logics within the SMT-LIB2 standard. The translation φi operates under strict syntactic process constraints: : ci 1. Type Declaration: All entities extracted in the Setup phase are declared as uninterpreted constants of generic sort Object or specific sorts (e.g., Person, Number) where applicable. 13 2. Predicate Mapping: Relations are mapped to boolean functions. For example, Felix eats food maps to (assert (Eats Felix Food)). 3. Quantifier Handling: While SMT solvers support quantifiers ( ), they often lead to , undecidability. Where possible, we instantiate universals over the finite set of extracted to maintain decidability. entities C.2 Formal Definition of Semantic Equivalence In Section 3.3, we introduce Semantic Equivalence Checking to compute consensus among canFeature Optimality Complexity Space MCS Greedy MCS (Ours) Guaranteed Minimal O(2n SAT) Exponential Approximation SAT) O(n Linear Estimated SAT Calls required: 1,024 = 10 atoms 1,048,576 = 20 atoms 1 Billion = 30 atoms 10 20 30 Table 3: Computational comparison between exact and greedy MCS strategies. For real-time LLM reasoning tasks (where often exceeds 20), the exact approach is infeasible, whereas the greedy approach scales linearly. didate formalizations. Here, we precisely define the logical framework and the equivalence condition. value in every possible interpretation with the signature: consistent C.2.1 Logical Framework VERGE operates within the framework of ManySorted First-Order Logic (specifically, the SMTLIB2 standard). However, to ensure decidability and efficiency, we restrict the formalization to specific fragments: QF_UF (Quantifier-Free Uninterpreted Functions): Used for abstract relationships and categorical claims. ϕa Σ ϕb = ΣE , ϕa = (cid:75) (cid:74) ϕb (cid:74) (cid:75) (7) C.2.3 Verification Implementation In practice, we verify this condition using the SMT solver (Z3) by checking the unsatisfiability of the negated biconditional. We construct query Q: = Declare(ΣE ) (ϕa ϕb) (8) QF_LIA (Quantifier-Free Linear Integer Arithmetic): Used for numerical constraints and temporal sequencing. If SOLVE(Q) returns UNSAT, it implies there is no model where ϕa and ϕb differ; thus, they are logically equivalent. Finite Domain Quantification: Where universal quantifiers ( ) are unavoidable in natural language, we instantiate them over the (see 3.1), finite set of extracted entities effectively reducing them to conjunctions in Propositional Logic. be the problem context and C.2.2 Equivalence Definition Let be the set of extracted entities. We define Signature ΣE = ( ) consisting of: , , : set of sorts (types) derived from the context (e.g., Student, Day). : set of function symbols (e.g., age: Person Int). : set of predicate symbols (e.g., gives_report: Student Day Bool). Two candidate formulas ϕa and ϕb generated by the LLM are defined as Semantically Equivalent modulo ΣE if and only if they share the same truth 14 This approach is robust to: 1. Syntactic Permutation: (A B) (B A). 2. Variable Renaming: y.P (y) (handled via canonicalization or finite instantiation). x.P (x) 3. Tautological Variance: (P Q). Q) ( Unlike string matching or embedding similarity, this provides mathematically rigorous guarantee that the consensus candidates represent the exact same logical constraint. C.3 Semantic Router Stress-Test Dataset To rigorously evaluate the Semantic Router (Section 4.2), we constructed diverse evaluation set of = 54 atomic claims designed to probe the decision boundary between formalizable logic and natural language ambiguity. The dataset was composed of three distinct subsets: Type Definition Example Router Group A: SMT-Amenable Claims (Hard Verification) τM τL τT Mathematical: Claims involving arithmetic operations, algebraic constraints, numerical comparisons, or unit conversions. Logical: Claims involving formal entailment, set theory inclusion, boolean logic, or syllogistic structure. Temporal: Claims involving linear sequencing, specific timestamps, duration, or precedence constraints. \"x is prime number greater than 5 and less than 20.\" SMT \"All entities in set must also belong to set B.\" SMT \"The event occurred 3 days after the signing.\" SMT Group B: LLM-Consensus Claims (Soft Verification) τC τV τP Commonsense: Claims relying on general world knowledge, causality, or physical properties not strictly definable by axioms. Vague: Claims involving subjective predicates, qualitative descriptors, or attributes lacking boolean truth value. Probabilistic: Claims explicitly stating uncertainty, likelihood, or future predictions without deterministic data. \"Glass dropped on concrete.\" typically shatters when Soft \"The painting is considered beautiful by most critics.\" Soft \"It is likely to rain tomorrow given the clouds.\" Soft Table 4: Definitions of Semantic Claim Types (τ ) used in the Routing Module. Claims in Group are autoformalized into SMT-LIB2 logic; Claims in Group are verified via multi-model consensus. 1. Logic & Math (Standard): 22 claims sampled from FOLIO and AR-LSAT containing explicit logical operators, arithmetic constraints, and temporal sequences (e.g., The meeting is at 2 PM, is greater than 70). 2. Commonsense & Vague (Standard): 20 claims involving subjective predicates, probability, or world knowledge not strictly definable in SMT (e.g., It is likely to rain, The painting is beautiful). 3. Adversarial Edge Cases: 12 manually crafted claims designed to trick keywordbased classifiers. These include: Numeric Idioms: Phrases containing numbers that are not mathematical (e.g., He gave 110% effort, She was on cloud nine). Logical Homonyms: Words like follows or implies used rhetorically rather than deductively (e.g., It follows that he was angry). Perturbed Contexts: Claims derived from logic puzzles where constraints were inverted to test stability (e.g., swapping banned for permitted to verify routing consistency remains robust under contradiction). Ground truth labels (SMT-Amenable vs. SoftVerification) were manually assigned by two authors with high inter-annotator agreement (κ > 0.9). Table 5 provides examples of these challenging edge cases."
        },
        {
            "title": "Faithfulness",
            "content": "A known failure mode of reasoning models is the Faithfulness Gap, where the model ignores provided context in favor of its parametric memory (e.g., refusing to accept counterfactual premise like Cats are not mammals). To evaluate VERGEs robustness to such logical perturbations, we conducted an adversarial probe using paired samples where the logical constraints were inverted or perturbed (e.g., changing banned to permitted, or swapping temporal order). As shown in Table 6, VERGE demonstrated 100% robustness across the tested categories. Notably, the system maintained high verification scores (avg. 0.91) on the adversarial samples. This indicates that VERGE successfully overcame the prior bias of the LLM; rather than hallucinating the standard answer or failing verification, the 15 Category Example Claim Correct Route The team gave 110% effort. Numeric Idiom The score was 110 points. Numeric (Real) Logical Homonym It follows that he felt sad. Logical (Real) Vague Quantifier Exact Quantifier It follows that B. Many people attended. More than 50 people attended. Soft SMT Soft SMT Soft SMT Table 5: Examples from the Semantic Router Stress-Test Dataset, highlighting adversarial pairs used to test the routers discrimination capabilities. Perturbation Type Original Perturbed Status Adaptation Logic Inversion Numeric Threshold Sequence Swap Mutually Exclusive Safe Safe Safe Safe Correct Flip Correct Flip Correct Flip Correct Flip Table 6: Adversarial Robustness results. Safe indicates the system either rejected the invalid premise or, in these cases, successfully adapted its reasoning to the counterfactual context (faithfulness), achieving high verification scores (> 0.9) on the perturbed inputs. MCS-guided feedback loop forced the model to align its generated answer with the perturbed context. This confirms that our formal verification constraint effectively enforces context-faithfulness over parametric memory. Reproducibility & Implementation"
        },
        {
            "title": "Details",
            "content": "To ensure the replicability of VERGE, we provide detailed specifications of our prompt engineering, hyperparameters, and computational infrastructure. E.1 Hyperparameters and Configuration We utilize all models in Table 1 (via AWS Bedrock) as the backbone LLM for all generation and refinement steps due to its strong reasoning capabilities. We use Z3 (version 4.12.2) as the SMT solver. Table 7 details the specific configuration used across all experiments. E.2 Prompt Templates We employ modular prompting strategy. The system prompts for the key components of the pipeline are provided below. E.2.1 Claim Decomposition & Classification This prompt breaks the raw generation into atomic units and routes them to the appropriate verifier. Parameter LLM Generation Temperature (Tgen) Top-P Max Output Tokens Thinking Budget Value 1.0 0.99 10,000 4,000 VERGE Pipeline Max Iterations (Tmax) Consensus Samples (K) Consensus Threshold Soft-Verify Judges (N ) Acceptance Score (τacc) Convergence ϵ Judge Model 3 3 0.70 5 0.75 0.01 Sonnet-3.7 Table 7: Hyperparameters for the VERGE pipeline. These values were held constant across all datasets (FOLIO, ZebraLogic, etc.) to demonstrate robustness. System Prompt: Decomposition Analyze the You are an expert logician. into provided atomic, each claim, assign Semantic Type based on the following definitions: Answer verifiable decompose claims. it For and MATHEMATICAL: Involves arithmetic, algebra, or numerical constraints. LOGICAL: Involves first-order logic, set theory, or strict deduction. COMMONSENSE: Involves general world knowledge or vague predicates. Output format: JSON list of objects {text\": string,type\": string}. 16 E.2.2 Autoformalization (Natural Language SMT-LIB2) This prompt translates text into code. Note the instruction (set-logic ALL), which allows the solver to handle mixed integer arithmetic and uninterpreted functions dynamically. System Prompt: Formalization Translate the following natural language context and claims into SMT-LIB2 format for the Z3 solver. Constraints: 1. Declare sorts Object) explicitly. all (e.g., Person, 2. Use (set-logic ALL) to support mixed arithmetic and logic. 3. Do not include (check-sat) commands; 4. If simply assert the facts. statement uninterpreted functions. is ambiguous, use Input Context: [CONTEXT] Input Claim: [CLAIM] Output: <smt> ... code ... </smt> Variance-Based Scoring. The final confidence score ) is penalized if high-confidence claims contradict each other. We define the score as: ( ( ) = µclaims (cid:18) max 0.5, 1.0 (cid:19) σclaims µclaims + ϵ (9) where µclaims is the weighted average verification status (Entailed=1.0, Soft-Pass=0.9, Possible=0.7, Fail=0.0) and σclaims is the standard deviation. This penalizes answers that contain mix of True and False claims, encouraging internal consistency. E.4 Compute Infrastructure All experiments were conducted on standard workstation (64GB RAM, 16-core CPU). The Z3 solver component handles both verification (avg. < 200ms) and equivalence checking (capped at 2.0s timeout per pair). The LLM inference was performed using the AWS Bedrock API. E.2.3 Feedback Injection (Refinement Step) E.5 Benchmark Descriptions When the SMT solver returns UNSAT, we calculate the Minimal Correction Set (MCS) and feed it back to the model using this template. System Prompt: Refinement answer previous Your logical contradictions verified by formal solver. Please refine your reasoning. Verification Report: contained Status: UNSATISFIABLE Conflict Logic: [Z3_UNSAT_CORE] Actionable Feedback: The set of claims {C1, C3} cannot be true simultaneously. Specifically, [MCS_DESCRIPTION]. Instruction: Rewrite your answer to resolve this contradiction. Do not weaken the premises; correct your derivation. E.3 Consensus and Scoring Metrics Formalization Consensus. To ensure the SMT code faithfully represents the natural language, we generate = 3 candidate formalizations for every claim. We compute the Semantic Equivalence between candidates by querying the solver for the unsatisfiability of their negation (see Eq. 3). We construct an equivalence graph where edges represent proven logical identity. If clique of size (majority consensus) is found, the representative formalization is accepted. Otherwise, the claim is flagged as Ambiguous and routed to Soft Verification. K/2 We used test split of each dataset mentioned below: FOLIO (Han et al., 2024): First-order logic reasoning requiring entailment verification. Metric: Accuracy on predicting \"Entailed/Contradictory/Unknown.\" ProofWriter (Tafjord et al., 2021): Synthetic deductive reasoning. Metric: Proof step accuracy. ZebraLogic (Lin et al., 2025): Constraint satisfaction puzzles (Einsteins riddle variants). Metric: Exact answer match. AR-LSAT (Zhong et al., 2021): Analytical reasoning from LSAT exams. Metric: Multiple-choice accuracy. BBEH (Kazemi et al., 2025): Big-Bench Extra Hard reasoning subset. Metric: Task-specific accuracy (varies by sub-task). HLE (Phan et al., 2025): Humanitys Last Examdiverse reasoning including ethics, physics, literature (1,200 questions). Metric: Multiplechoice accuracy. All reported numbers are task accuracy against gold labels, not internal verification scores. S( ) is used only to decide whether to accept or refine during iteration. 17 Figure 3: The Correlation Cliff in Iterative Refinement. Accuracy progression over 10 iterations (GPTOSS-120B). VERGE exhibits perfect monotonic improvement (Kendalls τ = 1.0, < 0.001 across all datasets). Probabilistic Self-Refinement shows system0.84 average, < 0.001), atic degradation (τ = stagnating below the CoT baseline in 85% of trials (χ2 = 26.7, < 0.001). Shaded regions show 95% confidence bands."
        },
        {
            "title": "F Detailed Verification Trajectories",
            "content": "Case Study 1: AR-LSAT (Temporal Scheduling) Problem Context & Question Context: Of eight studentsGeorge, Helen, Irving, Kyle, Lenore, Nina, Olivia, and Robertin seminar, exactly six will give individual oral reports during three consecutive daysMonday, Tuesday, and Wednesday. Exactly two reports will be given each dayone in the morning and one in the afternoonaccording to the following conditions: Tuesday is the only day on which George can give report. Neither Olivia nor Robert can give an afternoon report. If Nina gives report, then on the next day Helen and Irving must both give reports... Question: If Kyle and Lenore do not give reports, then the morning reports on Monday, Tuesday, and Wednesday, respectively, could be given by: Options: A) Helen, George, Nina B) Irving, Robert, Helen C) Nina, Helen, Olivia D) Olivia, Robert, Irving E) Robert, George, Helen Iteration 1 (Failed) Generated Answer Answer: E) Robert, George, and Helen Extracted Claims & Verification Status Idx Claim Text Type Status Score The selected answer is Option LOGICAL CONTRADICTORY 1 2 3 4 Robert gives the Monday Morning report George gives the Tuesday Morning report Helen gives the Wednesday Morning report Olivia gives an afternoon report (Implicit) TEMPORAL POSSIBLE TEMPORAL POSSIBLE TEMPORAL POSSIBLE 0.00 0.70 0.70 0. LOGICAL CONTRADICTORY 0.00 Conf 0.95 0. 0.95 0.92 1.00 Detailed Failure Analysis Claim 0 Failure: Constraint Violation Issue: Option assigns slots (Robert, George, Helen) to mornings. Since Kyle and Lenore are out, the remaining slots (Afternoons) must be filled by Nina, Irving, Olivia. This forces Olivia into an afternoon slot. ; Context Constraint : Neither Olivia nor Robert can give afternoon reports ( assert ( not ( exists (( Day )) ( gives_report Olivia Afternoon )))) ; Claim 0 implies : ( assert ( gives_report Robert Monday Morning )) ( assert ( gives_report George Tuesday Morning )) ( assert ( gives_report Helen Wednesday Morning )) ; This leaves Olivia with only Afternoon slots available -> CONTRADICTION Z3 Result: unsat Unsat Core: [context_axiom_olivia, claim_0 part 0] Minimal Correction Subset (MCS): Remove (gives_report Robert Monday Morning) OR Remove Option E. Metrics (Iteration 1) Overall Score: 0. Joint Cons: 0.0% Entailment: 0.42 Verified: False VERGE Feedback: \"Constraint Violation: Olivia forced into Afternoon. MCS: Change Option or Roberts slot.\" 19 Iteration 2 (Success) Refined Answer Answer: D) Olivia, Robert, and Irving Reasoning: The previous attempt failed because it forced Olivia into an afternoon slot. To satisfy the condition \"Neither Olivia nor Robert can give an afternoon report,\" both must be assigned to morning slots. Option places Olivia (Mon AM), Robert (Tue AM), and Irving (Wed AM). This leaves valid afternoon slots for the others. Extracted Claims & Verification Status Type Claim Text Idx Status Score 0 2 3 4 The selected answer is Option LOGICAL ENTAILED Olivia gives the Monday Morning report TEMPORAL POSSIBLE Robert gives the Tuesday Morning report TEMPORAL POSSIBLE Irving gives the Wednesday Morning report TEMPORAL POSSIBLE 1.00 0.95 0. 0.95 Constraint on Olivia/Robert is satisfied LOGICAL ENTAILED 1.00 Verification Success Joint Consistency Verified ( assert ( gives_report Olivia Monday Morning )) ( assert ( gives_report Robert Tuesday Morning )) ( assert ( gives_report Irving Wednesday Morning )) Z3 Result: sat. Entailment Check: The conjunction of these assignments is consistent with all global constraints. Conf 0.95 0.99 0. 0.99 1.00 Final Metrics Overall Score: 0.965 Joint Cons: 100% Entailment: 0. Verified: TRUE 20 Case Study 2: FOLIO (First-Order Logic) Problem Context & Question Context: 1. All guests at the party drink wine or beer. (x.Guest(x) => ine(x) Beer(x)) 2. If guest drinks wine, they eat cheese. (x.Guest(x) ine(x) => Cheese(x)) 3. No one who eats cheese likes fish. (x.Cheese(x) => LikesF ish(x)) 4. Alan is guest. (Guest(Alan)) 5. Alan likes fish. (LikesF ish(Alan)) Question: Does Alan drink beer? Iteration 1 (Hallucination) Generated Answer No, Alan drinks wine. Since Alan is guest, he drinks wine or beer. Guests usually drink wine with meals, and there is no rule against it. Detailed Failure Analysis Claim 0: \"Alan drinks wine\" is Contradictory SMT Formalization: ( assert ( Drinks Alan Wine )) Constraint Propagation: 1. (Drinks Alan Wine) => (Eats Alan Cheese) [Context Axiom 2] 2. (Eats Alan Cheese) => (Not (Likes Alan Fish)) [Context Axiom 3] 3. Context Axiom 5 asserts: (Likes Alan Fish) 4. Contradiction: (Likes Alan Fish) AND (Not (Likes Alan Fish)) Z3 Result: unsat. Unsat Core: [Drinks_Alan_Wine, Axiom_2, Axiom_3, Axiom_5] Metrics (Iteration 1) Score: 0. Joint Cons: 0.0% Verified: False VERGE Feedback: \"Claim Alan drinks wine causes contradiction with Alan likes fish. MCS: Delete claim.\" Iteration 2 (Success) Refined Answer Yes, Alan drinks beer. Reasoning: We know Alan is guest, so he drinks Wine or Beer. If he drank Wine, he would eat Cheese. If he ate Cheese, he would not like Fish. However, we know Alan DOES like Fish. Therefore, he cannot drink Wine. Since he must drink Wine or Beer, and Wine is impossible, he must drink Beer. Extracted Claims & Verification Status Type Claim Text Idx Status 0 1 Alan drinks beer LOGICAL ENTAILED Alan does not drink wine LOGICAL ENTAILED 21 Score 1. 1.00 Conf 0.98 0.98 Verification Success Entailment Proof ; To prove \" Alan drinks Beer \" is entailed , we assert its negation and check for UNSAT ( push ) ( assert ( not ( Drinks Alan Beer ) )) ( check-sat ) ; Returns UNSAT -> Proves Entailment ( pop ) Final Metrics Score: 1.000 Joint Cons: 100% Entailment: 1. Verified: TRUE 22 Case Study 3: ZebraLogic (Spatial Relations) Problem Context & Question Context: Three houses (Red, Blue, Green) are in row, numbered 1 (left) to 3 (right). The Blue house is immediately to the right of the Red house. The Green house is somewhere to the left of the Blue house. Question: What is the order of the houses from left to right? Iteration 1 (Inconsistent) Generated Answer The order is Red, Blue, Green. Reasoning: Red is at 1. Blue is right of Red, so Blue is 2. Green is left of Blue, so Green is 3... wait. Extracted Claims & Verification Status Idx Claim Text Red is at position 1 Blue is at position 2 0 1 Type LOGICAL LOGICAL Status POSSIBLE POSSIBLE Score 0.70 0.70 0.00 Conf 0. 0.90 0.90 Green is at position 3 LOGICAL CONTRADICTORY Detailed Failure Analysis Joint Inconsistency Issue: Claim 2 (Green=3) contradicts Claim 1 (Blue=2) given the constraint \"Green is left of Blue\". ( declare-fun pos ( House ) Int ) ( assert (= ( pos Blue ) (+ ( pos Red ) 1) )) ( assert (< ( pos Green ) ( pos Blue ))) ; Blue right of Red ; Green left of Blue ; Claims : ( assert (= ( pos Blue ) 2) ) ( assert (= ( pos Green ) 3) ) ; Check : 3 < 2 ? -> FALSE . Z3 Result: unsat. Unsat Core: [Green_Left_Blue_Constraint, Claim_1, Claim_2] Metrics (Iteration 1) Score: 0.466 Joint Cons: 0.0% Verified: False VERGE Feedback: \"Constraint violation: Green must be < Blue. Current: 3 < 2 (False).\" Iteration 2 (Success) Refined Answer The order is Green, Red, Blue. Reasoning: 1. Blue is right of Red -> = + 1. 2. Green is left of Blue -> < B. Possible values: If R=1, B=2. Then < 2 implies G=1. But houses are distinct, so cannot be 1. Therefore, Red cannot be 1. Try R=2, B=3. Then < 3 implies G=1 or G=2. Since R=2, must be 1. Solution: Green(1), Red(2), Blue(3). Extracted Claims & Verification Status Type Claim Text Idx Status Score 1 2 Green is at position 1 Red is at position 2 Blue is at position 3 LOGICAL ENTAILED LOGICAL ENTAILED LOGICAL ENTAILED 1. 1.00 1.00 23 Conf 0.95 0. 0.95 Verification Success Entailment Verified ( assert (= ( pos Green ) 1) ) ( assert (= ( pos Red ) 2) ) ( assert (= ( pos Blue ) 3) ) Z3 Result: sat. The solution uniquely satisfies all spatial constraints. Final Metrics Score: 1.000 Joint Cons: 100% Entailment: 1.00 Verified: TRUE"
        }
    ],
    "affiliations": [
        "Amazon Web Services",
        "Case Western Reserve University"
    ]
}