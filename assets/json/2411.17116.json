{
    "paper_title": "Star Attention: Efficient LLM Inference over Long Sequences",
    "authors": [
        "Shantanu Acharya",
        "Fei Jia",
        "Boris Ginsburg"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Inference with Transformer-based Large Language Models (LLMs) on long sequences is both costly and slow due to the quadratic complexity of the self-attention mechanism. We introduce Star Attention, a two-phase block-sparse approximation that improves computational efficiency by sharding attention across multiple hosts while minimizing communication overhead. In the first phase, the context is processed using blockwise-local attention across hosts, in parallel. In the second phase, query and response tokens attend to all prior cached tokens through sequence-global attention. Star Attention integrates seamlessly with most Transformer-based LLMs trained with global attention, reducing memory requirements and inference time by up to 11x while preserving 95-100% of accuracy."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 6 2 ] . [ 1 6 1 1 7 1 . 1 1 4 2 : r Star Attention: Efficient LLM Inference over Long Sequences STAR ATTENTION: EFFICIENT LLM INFERENCE OVER LONG SEQUENCES Shantanu Acharya, Fei Jia, Boris Ginsburg NVIDIA {shantanua,fjia}@nvidia.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Inference with Transformer-based Large Language Models (LLMs) on long sequences is both costly and slow due to the quadratic complexity of the selfattention mechanism. We introduce Star Attention, two-phase block-sparse approximation that improves computational efficiency by sharding attention across multiple hosts while minimizing communication overhead. In the first phase, the context is processed using blockwise-local attention across hosts, in parallel. In the second phase, query and response tokens attend to all prior cached tokens through sequence-global attention. Star Attention integrates seamlessly with most Transformer-based LLMs trained with global attention, reducing memory requirements and inference time by up to 11x while preserving 95-100% of accuracy."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent Large Language Models (LLMs) can support contexts up to millions of tokens in length (Gemini-Team, 2024; Anthropic, 2024; Meta-AI, 2024), unlocking applications such as repositorylevel code analysis, multi-document summarization, and large corpus retrieval. However, processing such long sequences with LLMs requires substantial computational and memory resources due to the quadratic complexity of the self-attention mechanism. To address these challenges, various techniques have been proposed to reduce memory usage and increase inference speed. For example, Flash Attention introduces an efficient GPU block-wise implementation of the global attention, achieving significant reductions in memory overhead and runtime (Dao et al., 2022; Dao, 2024). Ring Attention further extends this idea by distributing the computation of self-attention and feed-forward modules across multiple devices, cleverly overlapping communication with shard-local attention computations to enhance scalability (Liu et al., 2024a; Beltagy et al., 2020). More broadly, distributed strategies such as tensor, pipeline, sequence, and data parallelism have been proposed to divide compute effectively across multiple machines (Shoeybi et al., 2019; Huang et al., 2019; Li et al., 2023; Meta-AI, 2021). Several prior works have shown that the attention matrix can be approximated with sparse attention mechanisms reducing the algorithmic complexity from quadratic to linear or log-linear. Child et al. (2019) significantly reduces the complexity of attention by leveraging sparse factorizations and (Choromanski et al., 2021) approximates attention using kernel-based methods. (Beltagy et al., 2020) employs sliding window attention and global tokens for efficient long-sequence processing while Xiao et al. (2024) adapts it for real-time long-sequence generation utilizing attention sinks. Complementing these approaches, memory-efficient techniques have also emerged. Key-value (KV) cache compression (Dai et al., 2019; Ge et al., 2024; Munkhdalai et al., 2024; Sun et al., 2024; Liu et al., 2024b) and low-rank approximations (Srebro & Jaakkola, 2003) trade precision for reduced memory usage. We introduce Star Attention, novel algorithm for efficient LLM long-context inference 1. This method is based on the observation that LLM inference usually has two stages: (1) prompt encoding, where the model processes input and stores KV vectors in the cache and (2) token generation, where model attends to the KV cache and autoregressively generates new tokens while updating the 1Code is available at: https://github.com/NVIDIA/Star-Attention Star Attention: Efficient LLM Inference over Long Sequences cache with the new KV vectors. In many long-context tasks, the input consists of long context followed by short query and short answer. The information needed for answering the query is often localized within small parts of the context, meaning context tokens need only attend to nearby tokens, while query tokens need to attend to all prior tokens. Based on this observation, Star Attention utilizes two-phase approach shown in Figure 1: 1. Context Encoding: The context is divided into contiguous blocks and distributed across context hosts, with each host also receiving copy of the first block (an anchor block). Hosts compute self-attention only for their assigned blocks, without communicating with each other, reducing attention complexity from quadratic to linear with respect to context length. This distributed processing is similar to Ring Attention (Liu et al., 2024a) but without the ring communication during context encoding (Figure 1a). 2. Query Encoding and Token Generation: The query is replicated across all hosts where it initially attends to the KV cache on each host. Global attention is then computed by aggregating the results at designated query host by efficiently communicating single vector and scalar per token from each context host. Only the query host updates its KV cache during this stage (Figure 1b). Star Attention enables the context length to scale linearly with the number of hosts by distributing the context processing across multiple hosts. Star Attention is compatible with most Transformerbased LLMs trained with global attention, operating seamlessly out-of-the-box without additional model fine-tuning. We evaluate Star Attention for Llama3.1-8B and Llama3.1-70B (Meta-AI, 2024) on several long-context benchmarks. Star Attention achieves up to 11 times faster inference while maintaining 95-100% of the baseline accuracy. Furthermore, Star Attention can be combined with other LLM optimization methods like Flash Attention or KV cache compression, allowing for additional speedup enhancements during inference. (a) Phase 1: Local Context Encoding with Anchor Blocks (b) Phase 2: Query Encoding and Output Generation with Global Attention Figure 1: Star Attention inference flow. All devices in the system are grouped into hosts where one of the hosts is labeled as the query host. The input sequence is processed in two phases. Phase 1 - context encoding. The context portion of the input is partitioned into smaller blocks and distributed across hosts. All blocks, except the first, are prefixed with the initial block, called the anchor block. Each host processes its assigned block and stores the non-anchor portion of the KV cache. Phase 2 - query encoding and token generation. The input query is broadcast to all the hosts, where in each host, it first attends to the local KV cache computed during phase one. Then the query host computes global attention by aggregating the softmax normalization statistics from all the hosts. This process is repeated for each generated token. 2 Star Attention: Efficient LLM Inference over Long Sequences c1 c2 c3 c4 c5 Figure 2: Block sparsity pattern for sequence partitioned into 5 context blocks ci and query block q. Each context block attends only to itself and the anchor block whereas the query attends to the entire input. (a) Global Attention (b) Blockwise Context Encoding (c) Blockwise Context Encoding with Anchor Blocks Figure 3: Attention distribution along the sequence length for context encoded with different strategies in phase 1 of Star Attention. (a) Global attention shows spike at the start, corresponding to the attention sink. (b) Star Attention without anchor blocks shows several attention sinks present at the beginning of each block. (c) Star Attention with anchor blocks shifts sinks to anchor tokens, resulting in an attention distribution approximating global attention. In the plot, the input sequence (4K tokens) is divided into 512-token chunks."
        },
        {
            "title": "2 STAR ATTENTION ALGORITHM",
            "content": "Star Attention operates in two phases: (1) Context Encoding, where the long context is divided into contiguous blocks and is processed with local blockwise attention, and (2) Query Encoding and Token Generation, where the query is processed, and answer tokens are generated using global attention. Below, we detail each phase of the algorithm. 2.1 PHASE 1: CONTEXT ENCODING Given an input sequence comprising context followed by query q, the context is divided into contiguous blocks: = [c1, c2, . . . , cn], where each block ci contains tokens. We introduce an anchor block mechanism, in which, each blockexcept the firstis prefixed with the first block c1 of the sequence, referred to as the anchor block. This concatenation forms an augmented context c: = [c1, (c1 c2), (c1 c3), . . . , (c1 cn)] where each augmented block contains 2b tokens: tokens from the anchor block c1 followed by tokens from the current block ci (Figure 2). The positional indices of c1 are preserved, ensuring that its tokens retain their original position indices [0, 1, . . . , b1]. The augmented blocks are distributed across compute hosts, where each host computes attention over the 2b tokens from its assigned block and generates the corresponding key-value (KV) vectors. While KVs for the anchor block c1 are discarded, the KVs for the current block ci are retained in the cache. We observe that, without anchor blocksi.e., applying blockwise attention only to the original context cthe model fails to generate correct outputs. We conjecture this failure is due to the incorrect approximation to the attention patterns observed during phase 2 (Figure 3b), where multiple attention spikes, known as attention sinks (Xiao et al., 2024), are distributed across the sequence. These spikes occur because each block is processed independently, creating an attention sink at the start of each block. As result, the model struggles to effectively focus on relevant parts of the context. To address this issue, we prefix the blocks with the anchor block c1, shifting the attention sinks to the 3 Star Attention: Efficient LLM Inference over Long Sequences anchor tokens. By discarding the KVs of the anchor tokens the intermediate attention sinks are removed ensuring the attention distribution of block-local attention (Figure 3c) closely approximates global attention (Figure 3a) while maintaining the computational efficiency of blockwise processing. 2.2 PHASE 2: QUERY ENCODING AND TOKEN GENERATION In phase 2, global attention is employed to encode the query and generate output tokens by using distributed softmax algorithm that eliminates the need to transfer KV cache between hosts (Figure 1b). designated query-host hq coordinates this computation. The query is broadcast to all hosts and transformed into the sequence Rlqd, where lq is the query length, and is the attention head dimension. Each host computes the local attention output Ah for the query using its local key-value pairs Kh, Vh Rlkd, where lk is the sequence length of the KV cache. The local attention is computed as: Ah = exp (cid:17) (cid:16) QK (cid:16) QK h,k (cid:80)lk k=1 exp Vh (cid:17) (1) In addition to Ah, each host also stores the sum of the exponents sh from the the local softmax operation (the denominator from Equation 1): lk(cid:88) (cid:33) sh = exp k=1 (cid:32) QK h,k The query-host hq gathers the local attention Ah and the sums of exponents sh from all hosts: The global softmax denominator, sglobal, is then computed as the sum of all local exponents: = [A1, A2, . . . , AH ] = [s1, s2, . . . , sH ] The query-host uses sglobal to aggregate the local attentions to compute the global attention: sglobal = (cid:88) h= sh (cid:88) Aglobal = sh sglobal Ah h=1 This method ensures that the global attention scores are normalized correctly across all hosts. It requires the communication of only single scalar sh (the local sum of exponents) and vector Ah (the local attention) per token. In practice, the log-sum-exp method from online softmax (Milakov & Gimelshein, 2018) can be used to maintain the numerical stability during global attention aggregation. This distributed approach enables efficient computation by minimal data transfers between hosts. Output generation and cache update. After computing the global attention output, the query-host hq generates the next token and its KV cache is updated with the key and value vectors of the new token. This process is repeated for each generated token. This two-phase mechanismlocal context encoding with anchor blocks in Phase 1 followed by global query encoding with token generation in Phase 2gives significant improvements in inference speed, while keeping the accuracy close to the global attention."
        },
        {
            "title": "3 EXPERIMENTS",
            "content": "We evaluate Star Attention on several Llama-based models with sequence lengths ranging from 16K to 1M tokens on RULER (Hsieh et al., 2024) and BABILong (Kuratov et al., 2024) benchmarks. We begin by comparing accuracy and the speed achieved by Star Attention versus baseline - Ring attention. Further, we investigate the impact of varying block sizes on accuracy, illustrating the trade-off between accuracy and speedup. Finally, we conduct detailed analysis of challenging and favorable cases for Star Attention by examining distinct RULER task categories. 4 (2) (3) (4) Star Attention: Efficient LLM Inference over Long Sequences Model Seq. Len. Block Size Ring-Attn Acc.(%) (K) (K) Star-Attn Acc. Speedup Llama-3-8B-Instruct, 1048K Gradient.ai (2024) Llama-3.1-70B-Instruct, 128K Meta-AI (2024) 16 32 64 128 16 32 64 4 8 16 32 4 8 16 86.12 82.52 79.05 77.39 95.09 94.61 88. +2.47% +1.54% +1.28% +1.23% -2.85% -2.70% -1.63% 1.1x 1.2x 1.8x 2.7x 1.7x 2.0x 4.7x Table 1: Star Attention vs Ring Attention (baseline) accuracy and relative inference speed-up. The for Star Attention shows the relative accuracy improvement (+) or degradation (-). We set block size to one-quarter of the sequence length. Star Attention achieves significant speedup over Ring Attention while maintaining the accuracy. For larger models, the speedup of Star Attention is even more pronounced. 3.1 SETUP Models. We benchmark the base and instruct variants of the Llama-3.1 8B model which support context lengths up to 128K tokens (Meta-AI, 2024). In addition, we evaluate two Gradient.ai models that extend Llama-3-8Bs context up to 1M tokens Gradient.ai (2024). To access the scalability of our method, we also evaluate the Llama-3.1-70B-Instruct model. We observe that large LMs achieve even greater speedups with Star Attention on long context tasks. Baseline. We compare Star Attention with Ring Attention (Liu et al., 2024a). Ring Attention computes global block-wise attention by having each host communicate its respective KV cache in ring pattern across all the hosts . More details regarding our baseline selection in Appendix C.1. Configuration. We implement Star Attention using HuggingFace Transformers library (Wolf et al., 2020). All experiments are done on A100 GPUs with bfloat16 precision. More details on the experiment configuration are in Appendix C. Evaluation Benchmarks. We use the RULER benchmark for evaluation. It consists of 13 tasks categorized into 4 domains: Needle-in-a-Haystack (Retrieval), Multi-Hop Tracing, Aggregation, and Question Answering (Hsieh et al., 2024). Additionally, we report results on the BABILong benchmark, which encompass tasks where multiple supporting facts encoded in the context are required to generate accurate answers (Kuratov et al., 2024). Further details on the benchmarks and specific tasks can be found in Appendix B. 3.2 RESULTS Table 1 provides relative speedup and accuracy achieved by Star Attention versus Global (Ring) Attention from 16K to 128K tokens on the RULER benchmark. In each setting, the context block size and anchor block size are set to one-quarter of the total sequence length. Star Attention achieves similar accuracy to full global attention, with relative accuracy degradation limited to 0-3% while also giving upto 5x inference speedup. This demonstrates that Star Attention effectively preserves the models ability to retrieve relevant information, even with significantly reduced context window. In case of larger models, such as Llama-3.1-70B Instruct, we find that these models achieves even greater speedups at any given sequence length while maintaining similar levels of accuracy degradation. We discuss Star Attentions strengths and limitations based on RULER subtasks in Section 3.4. Full RULER scores for all models can be found in Appendix E. Extending this analysis to other benchmarks and models, we evaluate Star Attention on the BABILong benchmark as well using Llama-3.1-8B-Instruct, Llama-3.1-8B-Base, and the gradientaiLlama-3-8B-Instruct-262K model. We have similar observation here that Star Attention achieves similar accuracy to full global attention, with accuracy degradation limited to 0-3% across all tasks up to 128K tokens, as shown in Figure 4. 5 Star Attention: Efficient LLM Inference over Long Sequences Figure 4: Accuracy (%) of Star Attention on RULER and BABILong evaluated on sequence lengths of 16K, 32K, 64K, and 128K. In all experiments, the block size and anchor block size are set to one-quarter of the total sequence length. Results using the Llama-3-8B-Instruct-262k, Llama-3.18B-Instruct and Llama-3.1-8B-Base models demonstrate that Star Attention retains 95-100% of the accuracy of global attention, and in some cases, even outperform it. However, we observe several anomalies with the Llama-3.1 8B base model on the BABILong benchmark. There is significant improvement at the 16K sequence length, but severe drop at 128K. These fluctuations likely stem from the benchmarks format-specific requirements for generating answers, which pose challenges for base models since they are not optimized for instruction-following tasks. As result, scores for base models, especially at longer sequence lengths, may be less reliable. More details provided in Appendix D. 3.3 TRADE-OFF BETWEEN ACCURACY AND SPEED Figure 5, illustrates the effect of varying block sizes during context encoding with sequence length of 128K tokens. Larger block sizes correlate to higher accuracy with Star Attention. This trend is consistent across all sequence lengths in our experiments. From our experiments, we observe that setting the block size to approximately one-quarter of the total sequence length strikes an optimal trade-off between accuracy and speed. However, for sequence lengths exceeding 128K, as shown in Table 2, we fix the block size at 32K tokens to prioritize speedup, allowing for some acceptable accuracy degradation. Similarly, for larger models such as the Llama-3.1-70B-Instruct, we limit the block size to 16K tokens. The choice of block size is dependent on the user on how much accuracy can be traded for improved speed. As the block size increases, Star Attentions performance approaches that of full global attention, providing users with flexibility in balancing computational efficiency with accuracy. Additional details regarding the experimental setup are provided in Appendix C.2. 3.4 IN-DEPTH ANALYSIS ON RULER TASK CATEGORIES In this section we investigate the strengths and limitations of Star Attention, using different categories of tasks within RULER. The benchmark has five primary categories: Single-NIAH, MultiNIAH, Multi-Hop Tracing, Aggregation, and Question Answering (QA). Figure 6 presents categorical results of RULER for the Llama-3.1-8B-Instruct model on sequence length of 32K. The trend is consistent across all sequence lengths (16K, 32K, 64K, and 128K), as detailed in Appendix (Figure 8). Notably, Star Attention achieves scores nearly identical to global attention in SingleNIAH tasks. However, in more complex tasks such as Multi-NIAH and QA, it shows slight decline in performance, with reductions ranging from 1.6% to 4.9% in Multi-NIAH and 0.9% to 6.8% in QA tasks. Despite these challenges, Star Attention consistently retains overall 95-100% accuracy of global attention. Star Attention: Efficient LLM Inference over Long Sequences Figure 5: Effect of block size on the accuracy of Star Attention on the RULER benchmark with block sizes ranging from 4K to 32K tokens for Llama-3.1-8B instruct model at sequence length of 128K. In each setting, the anchor block size matches the context block size. The results indicate that larger context block sizes are positively correlated with improved accuracy. Model Seq. Len. Block Size Ring-Attn Acc. (%) (K) (K) Star-Attn Acc. Speedup Llama3-8B-Instruct, 1048K Gradient.ai (2024) Llama-3.1-70B-Instruct, 128K Meta-AI (2024) 128 256 512 1024 64 128 32 32 32 32 16 77.39 74.44 69.30 63.70 88.54 65.29 +1.23% -1.04% -9.71% -8.36% -1.63% -11.44% 2.7x 10.8x 16.2x 16.9x 4.7x 8.7x Table 2: Accuracy versus speed trade-off for Star Attention compared to Ring (Global) Attention on RULER. The for star attention shows the relative accuracy degradation and the relative speedup compared to global attention. When the block size remains fixed and the as sequence length increases, Star Attention achieves exponential speedup over Ring (Global) Attention at the cost of slightly more accuracy degradation. It is upto the user to decided how much accuracy they want to trade-off for speed by setting the block size. Tasks such as Multi-Hop Tracing and Aggregation necessitate an in-depth comprehension of context. Multi-Hop Tracing poses significant challenge for Star Attention, as it requires the model to propagate information across multiple hops within the sequence, demanding effective inter-block communication. However, Star Attention lacks inter-block communication, relying solely on global attention between the query and segregated KV caches within each block. Due to this, the performance degradation is considerable compared to global attention. Aggregation tasks, encompassing Common and Frequent Words Extraction, assess models ability to aggregate relevant information within long-range contexts. Star Attention yields significant performance improvements across all sequence lengths. This enhancement stems from Star Attentions chunk-based context division, enabling local attention within each chunk to strengthen summarization capabilities. Effective chunk-wise summarization in Phase 1 facilitates global attentions information aggregation in Phase 2."
        },
        {
            "title": "4 ABLATION STUDY",
            "content": "The ablation experiments focus on the Needle-in-a-Haystack (NIAH) task, which tests models ability to answer queries based on small, relevant piece of information (needle) embedded within large context (haystack). To increase the tasks complexity, we explore three variations from the RULER benchmark (Hsieh et al., 2024): Single-NIAH, Multi-key NIAH, and Multi-query NIAH. 7 Star Attention: Efficient LLM Inference over Long Sequences Figure 6: Accuracy (%) of Star Attention using the Llama-3.1-8B-Instruct model on the 5 categories of tasks in RULER on sequence lengths of 32K. In all experiments, the block size and anchor block size are set to one-quarter of the total sequence length. For the NIAH and QA tasks, Star Attention retains upto 95-100% accuracy of the baseline. The Multi-Hop Tracing task becomes quite challenging since it requires inter-block communication. In aggregation tasks, Star Attention show significant improvement as distributed local attention helps the model in such summarization tasks. The trend is consistent to other sequence lengths as shown in Appendix (Figure 8) 4.1 POSITION AND CONTENT OF ANCHOR BLOCK In this section, we explore the role of anchor blocks during Phase 1 that enables Star Attention to approximate global attention behavior. As outlined in Section 2.1, anchor blocks are crucial in managing the attention spikes generated at the start of each context block, helping Star Attention approximate global attention (see Table 3 ) Drawing from the hypotheses on sink tokens in Xiao et al. (2024), we consider two potential explanations for the effectiveness of anchor blocks: (1) the model may develop bias toward the absolute position of the anchor block, or (2) the semantic content of the anchor block is essential for maintaining performance. To better understand how anchor blocks enable Star Attention to approximate global attention distribution, we test both the hypotheses. We conduct experiments on the Llama-3.1-8B-Instruct model, varying both the position and content of the anchor block. We evaluate two configurations: block size of 16K for sequences of length 64K, and block size of 32K for sequences of length 128K, in both the cases, with anchor block size matching the context block size. Position of anchor block: Here, we fix the content of the anchor block to the first context block and vary its position IDs. We test three scenarios : (1) the position IDs are randomly sampled from the range [0, starting position of the current block] (e.g., for block starting at position 32K, position IDs are sampled from [0, 32K] ); (2) the position IDs are derived from the previous block (e.g., for block of size 16K starting at position 32K, position IDs are sampled from [16K, 32K] ); (3) the position IDs are fixed to the first block (our proposed approach). As shown in Table 3, varying the position of the anchor block has minimal impact on accuracy. Content of anchor block: We fix the position IDs of the anchor block to that of the first block but vary its content. We explore several configurations (as shown in Table 3): (i) single repeated token (e.g., , the, or .); (ii) random tokens; (iii) shuffling the tokens of the first block; and (iv) using the original first block content (the proposed approach). Our results show that the content of the anchor block significantly impacts performance, with the original first block content yielding the best results. This outcome suggests that since global attention is performed during Phase 2, it is important for the local context blocks to attend to anchor blocks whose content reflects what the model would see during global attention. Previous block as anchor block: To examine the roles of both position and content, we experiment with using the previous block as the anchor block. For example, for block of size 16K starting at position 32K, the anchor block would be the block with position IDs from 16K to 32K. This configuration has lower accuracy comparing to using the first block as the anchor  (Table 3)  . In summary, we found that while the positional placement of the anchor block is not important , its content is critical for optimal performance. 8 Star Attention: Efficient LLM Inference over Long Sequences Experiments Global attention No anchor block Content set to first-block, position IDs are: randomly sampled from [0, current block) same as previous block same as first block Position IDs set to first-block, content is: constant token (ex: or the or . ) random tokens shuffled first block tokens first block tokens Previous-block used as anchor 64K 99.50 60.11 96.79 97.35 97.61 0.00 90.55 92.96 97. 94.20 RULER-NIAH (%) 64k 128k 128k - 98. - -39.59% 73.75 -25.12% -2.72% -2.16% -1.90% -100.00% -8.99% -6.57% -1.90% -5.33% 97.16 96.80 97.54 0 82.63 90.76 94.94 96.13 -1.35% -1.71% -0.96% -100.00% -10.15% -3.26% -0.96% -2.40% Table 3: Experiments on analyzing the impact of varying the position and content of the anchor block with the LLaMA-3.1-8B-Instruct model, with block size of 16K for 64K sequence length, and 32K for 128K sequence lengths. In each setting, the size of the anchor block matches the context block size. The for star attention shows the relative accuracy degradation compared to global attention. The experiments are categorized into 4 groups: (i) absence of anchor block; (ii) varying the position IDs; (iii) varying the content; (iv) varying both the position and the content. Results indicate that while the anchor blocks position is not critical, its content is essential for optimal performance. Figure 7: Effect of anchor block size on the accuracy of Star Attention on the RULER-NIAH benchmark with the Llama-3.1 8B Instruct model. In these experiments, the context block size is fixed to 32K for sequence length 128K, respectively. Results indicate that larger anchor block sizes lead to improved accuracy. The observed trend holds for all sequence lengths in our experiments. 4.2 SIZE OF ANCHOR BLOCK As discussed in Section 3.3, larger block sizes improve the accuracy of Star Attention. In this section, we analyze the impact of varying anchor block size while maintaining fixed block size of 32K for sequence length of 128K. As illustrated in Figure 7, increasing the anchor block size enhances model accuracy, with the best performance observed when the anchor block size equals the context block size. Although Figure 3b demonstrates that attention spikes predominantly occur in the first few tokens, reducing the number of tokens in the anchor block leads to substantial drop in performance. This suggests that larger anchor block is critical for maintaining model accuracy, despite attention spikes being concentrated at the beginning of the sequence. This observation implies that the anchor blocks effectiveness is not solely due to its role in managing attention sinks but may involve other underlying factors. These findings remain consistent across both base and instruct models, as well as for all sequence lengths. Further investigation into why the anchor block size must be equivalent to the context block size is left for future work. 9 Star Attention: Efficient LLM Inference over Long Sequences"
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this paper, we introduced Star Attention, novel block-sparse attention mechanism designed to enable efficient inference on long sequences in transformer-based LLMs. The method operates in two phases: (1) context tokens are processed using blockwise-local attention, with the context segmented into blocks where each block is prefixed with an anchor block; and (2) then the query and response tokens attend to all prior cached tokens through sequence-global attention. Star Attention delivers up to 11x speedup over Ring Attention while maintaining 95-100% accuracy, significantly enhancing both memory efficiency and inference speed. Scaling Star Attention to longer sequences (up to 1M) and larger models, we observe even greater speedups while preserving similar levels of accuracy. Despite these advances, several open questions remain. The role and optimal size of anchor blocks relative to context blocks require further exploration. Additionally, while Star Attention performs effectively with block sizes set to one-quarter of the sequence length, accuracy degrades when using smaller blocks on longer sequences. Future work will focus on refining the anchor block mechanism and improving performance on more complex long-context tasks to enhance the scalability and robustness of Star Attention."
        },
        {
            "title": "ACKNOWLEDGEMENTS",
            "content": "We thank Santiago Akle, Vahid Noroozi, Somshubra Majumdar, Jocelyn Huang, Zhiyuan Jerry Lin and NVIDIA Long Context team for helpful discussion and feedback."
        },
        {
            "title": "REFERENCES",
            "content": "Anthropic. The Claude 3 model family: Opus, Sonnet, Haiku. https://www-cdn. anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_ Card_Claude_3.pdf, 2024. Iz Beltagy, Matthew Peters, and Arman Cohan. Longformer: The long-document Transformer. arXiv:2004.05150, 2020. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv:1904.10509, 2019. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. ICLR, 2021. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc V. Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond fixed-length context. ACL, 2019. Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. ICLR, 2024. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. FlashAttention: Fast and memoryefficient exact attention with IO-awareness. NeurIPS, 2022. Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. ICLR, 2024. Gemini-Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv:2403.05530, 2024. Gradient.ai. RULER vs. Gradients 1M context length Llama-3-70B. https://gradient.ai/ blog/ruler-vs-gradient-s-1m-context-length-llama-3-70b, 2024. Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg. Ruler: Whats the real context size of your long-context language models? COLM, 2024. Star Attention: Efficient LLM Inference over Long Sequences Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc Le, Yonghui Wu, et al. GPipe: Efficient training of giant neural networks using pipeline parallelism. NeurIPS, 2019. Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Ivan Rodkin, Dmitry Sorokin, Artyom Sorokin, and Mikhail Burtsev. BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-aHaystack. arXiv:2406.10149, 2024. Shenggui Li, Fuzhao Xue, Chaitanya Baranwal, Yongbin Li, and Yang You. Sequence parallelism: Long sequence training from system perspective. ACL, 2023. Hao Liu, Matei Zaharia, and Pieter Abbeel. Ringattention with blockwise transformers for nearinfinite context. ICLR, 2024a. Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi ICML, Chen, and Xia Hu. KIVI: tuning-free asymmetric 2bit quantization for KV cache. 2024b. Meta-AI. Fully sharded data parallel: faster AI training with fewer GPUs. engineering.fb.com/2021/07/15/open-source/fsdp/, 2021. https:// Meta-AI. Introducing Llama 3.1: Our most capable models to date. https://ai.meta.com/ blog/meta-llama-3-1/, 2024. Maxim Milakov and Natalia Gimelshein. Online normalizer calculation for softmax. arXiv:1805.02867, 2018. Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal. Leave no context behind: Efficient infinite context Transformers with infini-attention. arXiv:2404.07143, 2024. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-LM: Training multi-billion parameter language models using model parallelism. arXiv:1909.08053, 2019. Nathan Srebro and Tommi Jaakkola. Weighted low-rank approximations. ICML, 2003. Yutao Sun, Li Dong, Yi Zhu, Shaohan Huang, Wenhui Wang, Shuming Ma, Quanlu Zhang, Jianyong Wang, and Furu Wei. You only cache once: Decoder-decoder architectures for language models. arXiv:2405.05254, 2024. Thomas Wolf, Lysandre Debut, et al. Transformers: State-of-the-art natural language processing. EMNLP: System Demonstrations, 2020. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. ICML, 2024. 11 Star Attention: Efficient LLM Inference over Long Sequences STAR ATTENTION PSEUDO-CODE Algorithm 1 Star Attention - Phase 1: Context Encoding Require: Context c, Block size 1: length(c) 2: Split into = L/b blocks, such that = [c1, c2, . . . , cn] 3: for = 2 to do (c1, ci) 4: 5: end for 6: for each host concurrently do Initialize an empty list kv 7: 8: end for 9: Distribute augmented blocks [c 10: for each host concurrently do for each assigned block 11: 12: 13: 14: 15: 16: 17: end for Compute attention over 2b tokens in Generate KV cache for Discard KV cache for anchor block c1 Append remaining KV cache (for ci) to kv n] across all hosts 2, . . . , end for 1, do Each block has upto tokens Each block ci is prefixed with anchor block c1 Parallel processing on each host for each transformer layer do for each host concurrently do Algorithm 2 Star Attention - Phase 2: Query Encoding and Token Generation Require: Query tokens q, number of output tokens no, KV cache kvh of each host from Phase 1 1: Designate one host as the query-host hq 2: Broadcast query tokens to all hosts 3: Initialize input tokens 4: Initialize output tokens [] 5: for = 1 to no do 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: end for Gather all Ah and sh from hosts: = [s1, s2, . . . , sH ], = [A1, A2, . . . , AH ] Initialize sglobal s1, Aglobal A1 for = 2 to do end if Compute local attention scores Ah for query using the local KV cache kvh Compute local log-sum-exp sh (logarithm of the softmax denominator) Compute query, key, and value vectors (Q, K, ) using input tokens if = hq then Generate no output tokens Process through all transformer layers Execute parallel computations on each host Update global log-sum-exp sglobal using online softmax: Append the new and vectors to kvhq Aggregate attention scores across hosts If this is the query-host sglobal sglobal + log (1 + exp(sh sglobal)) 19: Update global attention scores: Aglobal exp(sh sglobal) Aglobal + exp(Ah sglobal) Ah end for Generate the next output token from the last transformer layer Append the new output token to output tokens Set input tokens [new output token] end for 20: 21: 22: 23: 24: 25: end for 26: return output tokens 12 Star Attention: Efficient LLM Inference over Long Sequences"
        },
        {
            "title": "B EVALUATION BENCHMARKS",
            "content": "We evaluate Star Attention on the RULER benchmark which comprises 13 tasks covering domains such as Needle-in-a-Haystack (Retrieval), Multi-Hop Tracing, Aggregation, and Question Answering. Each task comprises 500 samples. For the ablations, we choose four Needle-In-A-Haystack (NIAH) tasks where Paul Graham essays serve as the distractor text (haystack): Single 2, Single 3, MultiKey 1, and MultiQuery. In these tasks, key-value pair is concealed within long context, and the model must identify the value corresponding to the key based on the provided input query. Table 4 presents the configurations of all the tasks in RULER. Category NIAH (Retrieval) Task Name Single 1 Single 2 Single 3 MultiKey 1 MultiKey 2 MultiKey 3 MultiValue MultiQuery Haystack Type Keys Type # Values Type noise book book book line kv book book numbers words 1 numbers words 1 uuids words 1 words numbers 4 words numbers uuids uuids 1 words 4 words numbers numbers # 1 1 1 1 1 1 4 1 # Outputs 1 1 1 1 1 1 1 Multi-Hop Tracing Aggregation Variable Tracking Common Words Extraction Frequent Words Extraction Question Answering QA 1 (squad) QA 2 (hotpotqa) Table 4: Configuration of RULER tasks Additionally, we also evaluate our method on the BABILong benchmark. In BABILong, we choose 5 tasks (shown in Table 5), each containing 1000 samples. These tasks are generated by simulating set of characters and objects engaged in various movements and interactions across multiple locations. Each interaction is represented by factual statement, and the objective is to answer questions based on the facts derived from the current simulation. Task Name # Facts per task qa1 qa2 qa3 qa4 qa5 single supporting fact two supporting facts three supporting facts two arg relations three arg relations 2 - 10 2 - 68 4 - 32 2 2 - 126 Table 5: Configuration of tasks in BABILong"
        },
        {
            "title": "C EXPERIMENT DETAILS",
            "content": "C.1 BASELINE COMPARISON Our implementation utilizes the HuggingFace Transformers library (Wolf et al., 2020), which currently lacks support for multi-node inference. As result, when performing inference with the Llama-3.1 8B model using standard causal autoregressive generation on sequences exceeding 64K tokens with bfloat16 precision across 8 A100 GPUs, we encounter out-of-memory (OOM) errors. Given these limitations, we adopt Ring Attention as practical and relevant baseline for evaluating Star Attentions performance on sequences up to 1 million tokens in length. 13 Star Attention: Efficient LLM Inference over Long Sequences Table 6 presents the time per sample for vanilla autoregressive generation, Ring Attention, and Star Attention across sequence lengths ranging from 16K to 128K. The results indicate that both Ring and Star Attention can process sequences up to 128K tokens on 8 A100 GPUs, whereas vanilla autoregressive inference encounters OOM issues beyond 64K tokens. For sequence lengths below 32K, vanilla inference is faster than the distributed attention mechanisms, primarily due to the GPU communication overhead incurred in the distributed setups. However, in long context scenarios i.e. on sequence lengths exceeding 32K tokens, Star Attention begins to demonstrate clear performance advantages. As demonstrated in Table 1, the speedup achieved by Star Attention increases significantly with longer sequence lengths. Seq. Length (K) Time Per Sample (s) Star Vanilla Ring 16 32 64 128 7 10 18 OOM 10 12 22 53 9 10 12 20 Table 6: Time per sample (seconds) for Llama3.1-8B-Instruct model with vanilla (global) inference, ring (global) and star attention, using 8 A100 GPUs. Vanilla autoregressive generation encounters out-of-memory (OOM) at 128K sequence length. It performs best in short context scenarios (i.e. sequences upto 32K tokens) but in long context scenarios, star attention demonstrates significant speedup. C.2 HARDWARE FOR INFERENCE SPEED In table 1, we use A100 GPUs to run the inference speedup tests. Table 7 describes the number of GPUs and the number of parallel workers used to obtain these numbers for Ring Attention vs Star Attention. Model Size Seq. Length # GPUs # Workers 8B 70B 16K - 128K 256K - 512K 1M 16K - 32K 64K 128K 8 16 32 8 16 4 8 16 4 4 8 Table 7: Resources used for the speedup experiments C.3 PROMPT TEMPLATES Prompt structure for base models: 1 {context}{query}{answer prefix} Prompt structure for Llama-3 and Llama-3.1 Instruct models: 1 <begin of text><start header id>system<end header id> 2 3 You are helpful assistant.<eot id><start header id>user<end header id> 4 5 {context}{query}<eot id><start header id>assistant<end header id> 6 7 {answer prefix} 14 Star Attention: Efficient LLM Inference over Long Sequences The portion in blue is processed during Phase 1 for blockwise context encoding, while the remaining text in gray is processed in Phase 2 for query encoding and token generation. The {context} and {query}{answer prefix} denote the context and the query portion of the input prompt, respectively. The {answer prefix} is only relevant for the RULER benchmark."
        },
        {
            "title": "D ACCURACY OF STAR ATTENTION",
            "content": "Table 8 shows the exact accuracy scores of star attention vs global attention across the RULER and the BABILong benchmark from Figure 4. Model Seq. Block length size Global RULER (%) Star Global BABILONG (%) Star GradientAI Llama-3-8B -Instruct-262k Meta Llama-3.1-8B -Instruct Meta Llama-3.1-8B -Base 16K 32K 64K 128K 16K 32K 64K 128K 16K 32K 64K 128K 4K 8K 16K 32K 4K 8K 16K 32K 4K 8K 16K 32K 88.92 85.25 83.17 79.25 99.78 99.66 98.72 92.54 77.18 74.76 70.01 64.68 89.48 85.74 82.30 77.79 91.27 88.70 83.37 74.41 78.64 76.91 69.09 69. +0.63% +0.58% -1.05% -1.83% -1.02% +1.34% -1.67% -2.49% +1.9% +2.88% -1.32% +7.58% 43.60 40.00 40.40 30.80 59.60 54.60 49.20 40.00 22.00 22.60 26.80 31. 43.40 39.40 39.00 33.20 59.80 54.00 46.60 38.60 25.20 24.00 27.20 26.40 -0.46% -1.50% -3.47% +7.79% +0.34% -1.10% -5.28% -3.50% +14.55% +6.19% +1.49% -14.84% Table 8: Accuracy (%) of star attention on RULER and BABILONG evaluated on sequence lengths of 16K, 32K, 64K, and 128K. In all experiments, the block size and anchor block size are set to one-quarter of the total sequence length. Results using the Llama-3-8B-Instruct-262k, Llama-3.18B-Instruct and Llama-3.1-8B-Base models demonstrate that star attention retains 95-100% of the accuracy of global attention, and in some cases, even outperform it."
        },
        {
            "title": "E ACCURACY ON ALL RULER TASKS",
            "content": "This section contains the accuracy of all the models we evaluated across all 13 tasks in RULER. Block Size (K) Global Attn. 4 8 16 32 Seq. Len. (K) 16 32 64 16 32 64 128 Llama-3.1-8B-Instruct Retrieval (NIAH) Single 1 Single 2 Single MultiKey 1 MultiKey 2 MultiKey 3 MultiValue MultiQuery 100 100 100 100 100 100 100 100 100 100 99.6 99.4 100 100 100 100 100 100 99.8 100 100 100 99.6 99.8 99.8 99.4 97. 98 99.2 99.2 96.4 100 99.8 99.2 87.2 98.8 99.4 98 84.8 99 99.6 96.8 66.8 99 98.2 90 59 99.9 99 95.15 91. 91.1 94 85.35 82.7 99.5 99.05 99.2 97.8 98.25 98.3 97.9 96.55 Table 9: Accuracy of Llama-3.1-8B-Instruct on retrieval tasks in RULER with Global Attention and Star Attention 15 Star Attention: Efficient LLM Inference over Long Sequences Block Size (K) Seq. Len. (K) Global Attn. 4 8 16 32 16 32 64 128 16 32 64 Llama-3.1-8B-Instruct Multi-Hop Aggregation Question Answering VT 99.56 99.2 95.44 61. 91.96 92.68 92.32 62.8 CWE FWE QA 1 QA 2 75 14.7 1.96 0.04 85.72 45.66 5.78 0. 88.87 93.93 85.13 72.33 89.73 95.27 86.47 75.87 80.8 78.8 78.8 76 80.2 78.6 78.4 68 56.4 54 51.2 41.6 54.4 51.8 50.4 41. Table 10: Accuracy of Llama-3.1-8B-Instruct on multi-hop, aggregation, and question answering tasks in RULER with Global Attention and Star Attention Block Size (K) Global Attn. 4 8 16 32 Seq. Len. (K) 16 32 64 16 32 64 128 Llama-3.1-8B-Base Retrieval (NIAH) Single 1 Single 2 Single MultiKey 1 MultiKey 2 MultiKey 3 MultiValue MultiQuery 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 98 100 100 100 98.2 99.2 99 98.8 93. 97.4 96.2 96.6 88.8 100 99.4 86.2 53.6 99.2 98.2 90.6 67 99.4 99.4 95.4 64 99 99.2 85.6 47.6 99.45 99.55 96.8 80. 98.4 98.55 94.9 72.75 99.85 99.4 97.55 85.3 99.15 98.7 96.15 77.55 Table 11: Accuracy of Llama-3.1-8B-Base on retrieval tasks in RULER with Global Attention and Star Attention Block Size (K) Seq. Len. (K) Global Attn. 4 8 16 32 16 32 64 128 16 32 64 128 Llama-3.1-8B-Base Multi-Hop Aggregation Question Answering VT 99.92 99.28 96.8 71.68 97.24 97.2 94.44 81.6 CWE FWE QA 1 QA 2 65.66 23.56 2.04 0.64 86.46 58.72 8.86 2.98 17.4 28.27 13.73 30.53 20.67 30.47 11.2 81. 11 13.8 14.2 51.2 11.6 11.8 10.6 48.2 11.4 10.2 8.6 11.2 13.2 10.8 9.2 38.6 Table 12: Accuracy of Llama-3.1-8B-Base on multi-hop, aggregation, and question answering tasks in RULER with Global Attention and Star Attention Star Attention: Efficient LLM Inference over Long Sequences Block Size (K) Global Attn. 4 8 16 32 Seq. Len. (K) 16 32 64 16 32 64 128 Llama-3.1-70B-Instruct Retrieval (NIAH) Single 1 Single 2 Single MultiKey 1 MultiKey 2 MultiKey 3 MultiValue MultiQuery 100 100 100 97. 100 100 100 98.2 100 100 100 99.2 100 100 100 100 100 100 100 99.4 100 100 100 99.4 97.8 99.6 99.8 97.4 98.2 98 80 99.8 99 96 26 99.4 96.4 93.2 19.2 98.6 99 97.6 27.4 99.2 95 95.4 16.4 99 99.1 95.65 92. 80.9 87.85 86.25 61.65 99.65 100 99.95 92.45 97.2 97.4 96.45 70.8 Table 13: Accuracy of Llama-3.1-70B-Instruct on retrieval tasks in RULER with Global Attention and Star Attention Block Size (K) Seq. Len. (K) Global Attn. 4 8 16 32 16 32 64 128 16 32 64 128 Llama-3.1-70B-Instruct Multi-Hop Aggregation Question Answering VT 100 100 100 50.08 87.32 90.08 91.52 41.4 CWE FWE QA 1 QA 2 99.3 94.22 39.7 2.98 99.52 94.5 49.54 2.7 97 98.87 93.73 77 97.13 99.2 94.93 80. 82.6 80.4 74.6 58.4 82.2 80.2 73.4 50.8 62.4 59.8 54 33.6 60.6 58 53.6 31 Table 14: Accuracy of Llama-3.1-70B-Instruct on multi-hop, aggregation, and question answering tasks in RULER with Global Attention and Star Attention Block Size (K) Seq. Len. (K) Global Attn. 4 8 16 32 32 16 32 64 128 256 16 32 64 128 256 GradientAI Llama-3-8B-Instruct-262K Retrieval (NIAH) Single 1 Single 2 Single 3 MultiKey 1 MultiKey MultiKey 3 MultiValue MultiQuery 100 100 100 100 100 100 100 100 100 100 100 100 100 97.8 98.4 100 100 100 99.6 99.8 100 100 98.8 99.4 96.6 100 100 100 98.4 99.6 99.8 98.4 98.8 96.4 99.6 99.2 99.4 98.4 91.4 100 100 99.4 99.8 89. 99.4 99.6 99.4 97.8 53 96 95 91.4 79.8 25.6 97 96 90 66.8 23 95.35 96.2 97.75 94.65 87.3 89.2 91.6 91.45 89.3 75 99.85 99.75 99.6 98.05 93. 99.75 99.7 99.3 96.8 81.05 Table 15: Accuracy of GradientAI Llama-3-8B-Instruct-262K on retrieval tasks in RULER with Global Attention and Star Attention 17 Star Attention: Efficient LLM Inference over Long Sequences GradientAI Llama-3-8B-Instruct-262K Block Size (K) Seq. Len. (K) Global Attn. 4 8 16 32 32 16 32 64 128 256 16 32 64 128 256 Multi-Hop Aggregation Question Answering VT 95.36 93.88 92.28 77.88 52.8 90.64 92.16 88.6 81.12 72.64 CWE FWE QA 1 QA 2 42.1 4.5 0.22 0.36 1.8 67.32 19.2 0.4 0.3 1.8 91.07 90.53 82.73 73.27 77.93 90.53 89.6 84.13 75.4 81. 80.2 74 69.8 65.6 67 77.2 73.8 69.6 61.6 61.6 56.6 54.6 49.6 45.4 37 57.6 53.8 47.6 43.8 33 Table 16: Accuracy of GradientAI Llama-3-8B-Instruct-262K on multi-hop, aggregation, and question answering tasks in RULER with Global Attention and Star Attention Block Size (K) Global Attn. 4 8 16 32 32 32 32 Seq. Len. (K) 16 32 64 128 256 512 1024 16 32 64 128 256 512 1024 GradientAI Llama-3-8B-Instruct-1048K Retrieval (NIAH) Single 1 Single 2 Single 3 MultiKey 1 MultiKey MultiKey 3 MultiValue MultiQuery 100 100 100 100 100 100 99.4 100 100 100 100 100 99.8 99.6 99.2 100 100 98.2 100 99.8 99. 98 99.8 100 100 99.8 95.8 97.2 100 100 100 99.8 100 100 100 96.8 100 100 100 100 97.6 100 99 99.4 99 99.8 98.4 95.6 92.6 98.6 98.8 99.4 99.4 95.4 85.8 84.2 99.6 99.2 99 98.8 97 88.4 67. 99 99.4 99 98.4 90.4 64.2 27 90.2 69.8 51.4 42.8 22.4 9.4 1.4 94.4 87 66.8 62.8 53.8 19.4 1 96.1 96.3 96 98.2 96.1 89.25 82 90.3 91 92.3 92.25 76.5 57.2 55.15 99.25 98.45 98.75 97.75 97.15 92.55 88. 98.1 97.6 97.95 96.8 88.6 63.8 60.3 Table 17: Accuracy of GradientAI Llama-3-8B-Instruct-1048K on retrieval tasks in RULER with Global Attention and Star Attention 18 Star Attention: Efficient LLM Inference over Long Sequences GradientAI Llama-3-8B-Instruct-1048K Block Size (K) Seq. Len. (K) Global Attn. 4 8 16 32 32 32 32 16 32 64 128 256 512 1024 16 32 64 128 256 512 1024 Multi-Hop Aggregation Question Answering VT 93.32 91.96 81.6 76.68 63 34.8 24.28 90 90.08 81.96 79.32 66.4 50.76 63.68 CWE FWE QA 1 QA 2 22.52 0.54 0.32 0.22 0.22 0.86 3.5 62.64 11.38 0.38 0.22 0.22 0.24 6.9 88.73 87.27 79.8 76.67 78.27 85.6 72.53 86.53 85.2 82 78.33 81.53 85 79. 76.8 75.4 75.4 68.4 71.4 66.8 66.2 77.4 75.2 74.4 64.6 67.4 63 57.8 54.8 54.4 46.4 48.8 43.8 37.8 30.2 55.4 53.8 46.6 46.4 37.6 30.8 26.4 Table 18: Accuracy of GradientAI Llama-3-8B-Instruct-1048K on multi-hop, aggregation, and question answering tasks in RULER with Global Attention and Star Attention Figure 8: Accuracy (%) of star attention using the Llama-3.1-8B-Instruct model on the 5 categories of tasks in RULER on sequence lengths of 16K, 32K, 64K, and 128K. In all experiments, the block size and anchor block size are set to one-quarter of the total sequence length. For the NIAH and QA tasks, Star Attention retains upto 95-100% accuracy of the baseline. The Multi-Hop Tracing task is notably challenging because it requires inter-block communication, which leads to expected performance degradation. Interestingly, Star Attention performs better with sequence lengths of 128k on this task, but this may be due to noise given the suboptimal baseline. In aggregation tasks, Star Attention show significant improvement as distributed local attention helps the model in such summarization tasks."
        }
    ],
    "affiliations": [
        "NVIDIA"
    ]
}