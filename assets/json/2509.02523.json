{
    "paper_title": "Flavors of Moonshine: Tiny Specialized ASR Models for Edge Devices",
    "authors": [
        "Evan King",
        "Adam Sabra",
        "Manjunath Kudlur",
        "James Wang",
        "Pete Warden"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present the Flavors of Moonshine, a suite of tiny automatic speech recognition (ASR) models specialized for a range of underrepresented languages. Prevailing wisdom suggests that multilingual ASR models outperform monolingual counterparts by exploiting cross-lingual phonetic similarities. We challenge this assumption, showing that for sufficiently small models (27M parameters), training monolingual systems on a carefully balanced mix of high-quality human-labeled, pseudo-labeled, and synthetic data yields substantially superior performance. On average, our models achieve error rates 48% lower than the comparably sized Whisper Tiny model, outperform the 9x larger Whisper Small model, and in most cases match or outperform the 28x larger Whisper Medium model. These results advance the state of the art for models of this size, enabling accurate on-device ASR for languages that previously had limited support. We release Arabic, Chinese, Japanese, Korean, Ukrainian, and Vietnamese Moonshine models under a permissive open-source license."
        },
        {
            "title": "Start",
            "content": "Flavors of Moonshine: Tiny Specialized ASR Models for Edge Devices Evan King * Adam Sabra * Manjunath Kudlur * James Wang Pete Warden Moonshine AI 5 2 0 2 2 ] . [ 1 3 2 5 2 0 . 9 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We present the Flavors of Moonshine, suite of tiny automatic speech recognition (ASR) models specialized for range of underrepresented languages. Prevailing wisdom suggests that multilingual ASR models outperform monolingual counterparts by exploiting cross-lingual phonetic similarities. We challenge this assumption, showing that for sufficiently small models (27M parameters), training monolingual systems on carefully balanced mix of high-quality human-labeled, pseudo-labeled, and synthetic data yields substantially superior performance. On average, our models achieve error rates 48% lower than the comparably sized Whisper Tiny model, outperform the 9x larger Whisper Small model, and in most cases match or outperform the 28x larger Whisper Medium model. These results advance the state of the art for models of this size, enabling accurate on-device ASR for languages that previously had limited support. We release Arabic, Chinese, Japanese, Korean, Ukrainian, and Vietnamese Moonshine models under permissive open-source license. 1. Introduction Automatic Speech Recognition (ASR) has seen growing interest in recent years, driven by new opportunities for speech-driven human-machine interaction (Namvarpour & Razi, 2025). Low latency ASR models that are small enough to deploy on-device can empower new generation of voicedriven applications, from real-time translation devices to intelligent conversational interfaces in automobiles (Rege et al., 2024) and smart appliances (King et al., 2025). While resource-intensive ASR models like Whisper Large (Radford et al., 2023) and NVIDIA Canary (Puvvada et al., 2024), or multimodal models like Phi-4 (Abouelenin et al., 2025) can be deployed in the cloud, they incur substantial infrastructure costs, rely on internet connectivity, and raise user privacy concerns. These limitations have spurred the development of ASR models that are compact enough to run efficiently on the edge (Jeffries et al., 2024). To date, most lightweight ASR research has focused on English or on multilingual models that underperform on non-English languages. Whisper Tiny is multilingual model that is small enough to run on-device, and provides good example: while it achieves strong 12% error rate on English, the model is less-than-stellar for Vietnamese ASR, where it has 60% error rate (Radford et al., 2023). In other words, while Whisper Tiny technically supports Vietnamese, its usability in real-world on-device applications is limited. This performance gap motivates the development of tiny ASR models that better support non-English languages. Prior work suggests that multilingual models can leverage cross-lingual similarities, allowing knowledge from highresource languages to transfer and improve recognition in lower-resource ones (Cho et al., 2018; Toshniwal et al., 2018; Ardila et al., 2019; Pratap et al., 2020). However, training models that are lightweight enough for edge devices requires reducing the number of learnable parameters, meaning that convergence on truly high-performance multilingual model is challenging. In this paper, we focus our efforts on training lightweight (27M parameter) monolingual models for edge devices, exploiting efficiencies in training data that we achieve from combination of open human-labeled datasets, high-quality pseudo-labels, and synthetic utterances. We choose six languagesArabic, Chinese, Japanese, Korean, Ukrainian, and Vietnamese with varying levels of training data availability. Our models achieve word error and character error rates (WER/CER) that are on-average 48% lower than the comparably sized Whisper Tiny model, and are in most cases on-par or better than the 28x larger Whisper Medium model. Additionally, by exploiting the architectural benefits of the Moonshine model architecture, our models run between 5x-15x faster than Whisper in on-device applications. Equal contribution. <pete@moonshine.ai>. Correspondence to: Pete Warden In summary, this paper introduces the following: Flavors of Moonshine: Tiny Specialized ASR Models for Edge Devices 2 We present new Moonshine ASR models for Arabic, Chinese, Japanese, Korean, Ukrainian, and Vietnamese. We show that the models achieve an average of 48% lower WER/CER than Whisper Tiny, making them significantly better-suited to edge ASR applications. We show that in all cases, the models outperform the 9x larger Whisper Small model. We also show that in most cases, the models are on par or 5-10% better than Whisper Medium, which is 28x larger. We release the models under permissive open-source license. The remainder of this paper is structured as follows. Section 2 describes our approach to training data collection, preprocessing, and model training. Section 3 summarizes the results of our evaluations. Section 4 concludes the paper. Appendices A, B, and include information about public datasets, evaluation procedure, and detailed results. Language Open Internal Synthetic Total Arabic Chinese Japanese Korean Ukrainian Vietnamese 4.6 50.9 36.9 27.6 1.7 8.4 10.0 19.0 17.0 44.4 12.9 85.8 0.9 0.0 0.0 0.0 5.1 0. 15.5 69.8 53.9 72.0 19.6 94.2 Table 1. Training data hours (in thousands) by language and source. We supplement existing publicly-released datasets with internally collected datasets and, in some cases, synthesize utterances for lower-resource languages. 2. Approach This section describes the Moonshine model architecture before detailing our data preparation and model training process. 2.1. Architecture We leverage the Moonshine model architecture (Jeffries et al., 2024), an encoder-decoder transformer that applies rotary position embeddings (RoPE) in its encoder and decoder layers (Su et al., 2024). Compared to Whisper, Moonshine is especially well-suited for edge applications because its inference costin terms of FLOPs and corresponding latencyscales with the duration of the input audio. In contrast, Whisper pads all inputs to 30 seconds, regardless of length, leading to unnecessary computation. We adopt the Moonshine Tiny variant (27M parameters), which is small enough to be deployed in resource-constrained environments. Table 2 provides comparison between the Moonshine Tiny architecture and the similarly-sized Whisper Tiny model. Parameter Moonshine Whisper Dimension Encoder layers Decoder layers Attention heads Encoder FFN activation Decoder FFN activation Parameters (M) FLOPs vs. Whisper Tiny 288 6 6 8 384 4 4 GELU SwiGLU 27.1 0.7x GELU 37.8 1.0x Table 2. Tiny model architectures and inference FLOPs 2.2. Data collection, preprocessing, & synthesis The amount of data used as input for training transformerbased ASR model is loosely predictive of model performance, with minimum of between 104 to 105 hours required for usable results (Radford et al., 2023). Some languages are considered higher-resource than othersthat is, there is more recorded audio available that can be used for training. As we are focused on range of mid to lowresource languages, the availability of data varies for each. With this in mind, we approached data collection, preprocessing, and, in some cases, synthesis, using three-stage strategy applied to each language: 1. Aggregate publicly-available ASR datasets. We leveraged ASR datasets from prior work. This allowed us to establish baseline for model performance effectively the best results we could achieve without performing raw data collection and labeling. 2. Collect and pseudo-label publicly-available data. Because existing public datasets lack the volume needed to train high-performing models, we gathered and pseudo-labeled large collection of raw audio from publicly available sources such as podcasts and radio streams. 3. Synthesize labeled data from text-only datasets. In cases where enough raw data was not widely available, we used high-quality text-to-speech models to synthesize diverse utterances from traditionally textonly datasets. Table 1 provides an overview of the sources and sizes of training datasets for each language. Appendix provides detailed breakdown of the public datasets for each language. Since we are targeting lower-resource languages, publicly available datasets are insufficient to achieve high performance. Our data collection target was to exceed the amount Flavors of Moonshine: Tiny Specialized ASR Models for Edge Devices 3 (in hours) used to train the initial Whisper models, based on the intuitions that (1) monolingual models do not benefit from transfer learning, and thus need larger datasets and (2) model performance loosely scales with dataset size. We therefore collected and labeled large amount of raw, inthe-wild speech data available on the open internet. Leveraging WhisperX (Bain et al., 2023) modified to run in custom framework for distributed data processing, we prepared around 173,000 hours of internal datasets across languages. In all cases except Chinese 1, our monolingual datasets exceeded the size of those used to train the original Whisper by an order of magnitude. In the cases of Arabic and Ukrainian, raw data sources on the open internet were insufficient to meet the threshold for dataset hours. To fill the gap, we leveraged text-only datasets for each language to create uniform distributions of utterance lengths, which we then input to high-quality, high-diversity text-to-speech models to generate fully synthetic utterances. Employing style interpolation between speaker embeddings helped us ensure diverse set of synthetic speakers (Dumoulin et al., 2016). 2.3. Training We train all models using schedule-free AdamW optimizer (Defazio et al., 2024) with learning rate of 2e5 for 8 epochs, with batch size of 32. All models were trained via DDP on 8xH100 GPUs until completion. Training took between 1 to 3 days depending on the amount of data in our language-specific corpus. 3. Evaluations Our evaluations measure the performance of Moonshine Tiny models against several multilingual Whisper variants of increasing sizeTiny, Base, Small, and Medium. We choose these Whisper variants since they are small enough to be deployed in the same on-device applications we target with Moonshine. We use beam size of 1 in evaluations with Whisper, which is functionally equivalent to the greedy decoding used by Moonshine 2. Our primary metric is error rate. Both word error rate (WER) and character error rate (CER) are commonly used in evaluating ASR systems; however, there is some subjectivity as to the best metric for certain languages. For completeness, we measure both WER and CER in every evaluation (except Japanese and Chinese, which lack word boundaries) and 1Since Whispers release, the amount of publicly-available Chinese data has grown substantially, which reduces the need for internal collection. 2In the original Whisper paper, the authors use beam size of 5. This produces slightly different results than the ones we report in this paper. report the exhaustive results in Appendix C. The summarized results in this section only report one or the other as is appropriate for the language. We rely on two multilingual test sets for evaluating every language: Common Voice 17 (Ardila et al., 2019) and Fleurs (Conneau et al., 2023). For some languages, we also evaluate using language-specific test sets. Arabic uses the test sets proposed by the Open Universal Arabic ASR Leaderboard (Wang et al., 2024), Japanese uses Reazon Speech (Fujimoto, 2016), Korean uses Zeroth-Korean (Jo & Lee, 2022), and Ukrainian uses Eurospeech (disco-eth, 2025). To ensure consistent evaluations across datasets, models, and languages, we normalize both the reference and predicted transcription prior to calculating the error rate. 3.1. Results Moonshine outperforms Whisper models between 1x and 28x its size. We first compare Moonshine Tiny with the similarly-sized Whisper Tiny model. Table 3 shows that the monolingual Moonshine Tiny models significantly outperform Whisper Tiny on all tests. Table 4 further illustrates the difference in error rate between Moonshine and progressively larger Whisper models (averaged across all evaluations for language). These results show that Moonshine outperforms the 9x larger Whisper Small model, attaining performance that is on-par or better than the 28x larger Whisper Medium model. There is an inherent tradeoff between model size and performance: more compute resources allow for larger models, which have higher upper bound of performance. We compare this tradeoff using measure of models unit accuracy, i.e., the inverse of error rate divided by model size (100 Error)/(# of Parameters). Figure 1 visualizes this tradeoff for Moonshine and Whisper, showing that Moonshine Tiny generally offers superior tradeoff between compute requirements and accuracy than Whisper models its size and larger. Performance scales loosely with dataset size. Figure 3 compares two checkpoints of Moonshine Tiny training (Baseline and Final) with increasing hours of training data. Model performance improves as the size of the training set increases, but the rate of increase is not consistent across languages. Vietnamese, for instance, has large increase in data and correspondingly large increase in performance; Korean, on the other hand, has an equally large increase in data, but the performance benefits are less significant. Performance degrades gracefully with reduced gain and increased noise. Figure 2 depicts the error rate of each model when evaluated on Fleurs with varying linear gain and signal-to-noise ratio. This allows us to assess the efFlavors of Moonshine: Tiny Specialized ASR Models for Edge Devices 4 Model whisper-tiny moonshine-tiny whisper-tiny moonshine-tiny CV17 Fleurs Arabic Chinese 66.0 36.1 88.9 36.6 Japanese Korean Ukrainian Vietnamese 100.9 18.8 37.3 14.9 67.1 26.1 96.1 18. 66.0 20.8 71.1 29.4 47.2 17.9 15.8 8.9 63.8 18.2 91.9 13. Table 3. Error rates of Moonshine Tiny and Whisper models across languages. Chinese, Japanese, and Korean results are CER; other languages are WER. Specialized Moonshine Tiny models outperform multilingual Whisper Tiny on all languages. Comparison Relative Size Arabic Chinese vs. whisper-tiny 1.4x vs. whisper-base 2.7x 9.0x vs. whisper-small vs. whisper-medium 28.5x -42.4 -36.4 -12.1 1.0 -35.7 -26.4 -14.0 -7.6 Japanese Korean Ukrainian Vietnamese -81.0 -57.0 -25.3 -12.2 -80.5 -36.9 -10.5 -2. -14.1 -6.1 -0.0 2.2 -47.1 -28.9 -6.2 3.2 Table 4. Error rate difference between Moonshine Tiny and all Whisper models; lower is better. Chinese, Japanese, and Korean results are CER; other languages are WER. Moonshine Tiny outperforms Whisper Small, which is 9x larger; in some cases, Moonshine Tiny outperforms the 28x larger Whisper Medium model. fect of input audio gain (quiet to loud) and audio noise on model performance. Performance trends lower as gain decreases and noise increases, though the models are robust to around -20 dB gain and 20 dB SNR, respectively. Interestingly, Ukrainian performance does not degrade more substantially than other languages despite being trained on relatively larger amount of clean audio from synthetic speakers. End-to-end applications can leverage methods for input gain normalization and noise suppression to provide added robustness beyond the models baseline capabilities. 4. Discussion & Conclusion This section discusses limitations and future work before concluding the paper. Low and ultra-low resource languages. We aim to further expand our scope to low and ultra-low resource languages, leveraging similar data collection and pseudo-labeling techniques outlined in the paper. More extensive use of synthesis, as well as introduction of data augmentation techniques may be necessary to fill the gaps in raw data available. Language-specific tokenizers. Moonshine currently relies on GPT-2s multilingual tokenizer, which has larger vocabulary size than necessary for individual languages. We expect that reducing the vocabulary size for each model will simplify the next-token prediction task, with potential benefits to accuracy and latency. In summary, we introduce family of 27M parameter Moonshine Tiny models that match or outperform the 28x larger Whisper Medium model across 6 languages. Our experiments show that small, monolingual, and specialized ASR models are better suited for on-device tasks in underrepresented languages. To contribute to the broader research effort around these tasks, we release the open weights to all the models discussed in the paper with permissive license."
        },
        {
            "title": "References",
            "content": "Abouelenin, A., Ashfaq, A., Atkinson, A., Awadalla, H., Bach, N., Bao, J., Benhaim, A., Cai, M., Chaudhary, V., Chen, C., et al. Phi-4-mini technical report: Compact yet powerful multimodal language models via mixtureof-loras. arXiv preprint arXiv:2503.01743, 2025. Alharbi, S., Alowisheq, A., Tuske, Z., Darwish, K., Alrajeh, A., Alrowithi, A., Tamran, A. B., Ibrahim, A., Aloraini, R., Alnajim, R., et al. Sada: Saudi audio dataset for arabic. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 1028610290. IEEE, 2024. Masrispeech-full: Alnwsany, Y. M. egyptian arabic https://huggingface.co/datasets/ NightPrince/MasriSpeech-Full. speech corpus, 2025. Large-scale URL Ardila, R., Branson, M., Davis, K., Henretty, M., Kohler, M., Meyer, J., Morais, R., Saunders, L., Tyers, F. M., and Weber, G. Common voice: massively-multilingual speech corpus. arXiv preprint arXiv:1912.06670, 2019. Bain, M., Huh, J., Han, T., and Zisserman, A. Whisperx: Time-accurate speech transcription of long-form audio. INTERSPEECH 2023, 2023. Flavors of Moonshine: Tiny Specialized ASR Models for Edge Devices 5 Figure 1. Difference in accuracy per model size between Moonshine and Whisper models. Model size places an upper bound on performance, and some models manage this tradeoff better than others. Moonshine offers superior tradeoff between performance and size than Whisper models its size and larger. Figure 2. Effect of input audio gain and signal-to-noise ratio (SNR) on Moonshine Tiny error rate. Butt, S. A. Arabic tts wav 24k dataset. https://huggingface.co/datasets/NeoBoy/arabic-ttswav-24k, 2025. Cho, J., Baskar, M. K., Li, R., Wiesner, M., Mallidi, S. H., Yalta, N., Karafiat, M., Watanabe, S., and Hori, T. Multilingual sequence-to-sequence speech recognition: architecture, transfer learning, and language modeling. In 2018 IEEE SLT Workshop, pp. 521527. IEEE, 2018. Chowdhury, S. A., Hussein, A., Abdelali, A., and Ali, A. Towards one model to rule all: Multilingual strategy for dialectal code-switching arabic asr, 2021. Conneau, A., Ma, M., Khanuja, S., Zhang, Y., Axelrod, V., Dalmia, S., Riesa, J., Rivera, C., and Bapna, A. Fleurs: Few-shot learning evaluation of universal representations of speech. In 2022 IEEE SLT Workshop. IEEE, 2023. Figure 3. Error rate vs. hours of training data. Model performance scales loosely with training data hours, dependent on data quality. Flavors of Moonshine: Tiny Specialized ASR Models for Edge Devices 6 Defazio, A., Yang, X., Mehta, H., Mishchenko, K., Khaled, A., and Cutkosky, A. The road less scheduled, 2024. disco-eth. EuroSpeech: large-scale multilingual parliamentary speech corpus. Hugging Face Datasets, 2025. URL https://huggingface.co/ datasets/disco-eth/EuroSpeech. Dumoulin, V., Shlens, J., and Kudlur, M. learned representation for artistic style. arXiv preprint arXiv:1610.07629, 2016. Fujimoto, Y. Reazonspeech: free and massive corpus for japanese asr. 2016. He, H., Shang, Z., Wang, C., Li, X., Gu, Y., Hua, H., Liu, L., Yang, C., Li, J., Shi, P., et al. Emilia: An extensive, multilingual, and diverse speech dataset for large-scale In 2024 IEEE SLT Workshop, pp. speech generation. 885890. IEEE, 2024. Jeffries, N., King, E., Kudlur, M., Nicholson, G., Wang, J., and Warden, P. Moonshine: Speech recognition for live transcription and voice commands. arXiv preprint arXiv:2410.15608, 2024. Jo, L. and Lee, W. Zeroth-Korean: Korean Open-source Speech Corpus for Speech Recognition (SLR40). Open Speech and Language Resources (OpenSLR), 2022. Available at https://openslr.org/40/. King, E., Yu, H., Vartak, S., Jacob, J., Lee, S., and Julien, C. Teaching things to think: Bootstrapping local reasoning for smart (er) devices. In 2025 IEEE International Conference on Pervasive Computing and Communications (PerCom), pp. 7888. IEEE, 2025. Kulkarni, A., Kulkarni, A., Shatnawi, S. A. M., and Aldarmaki, H. Clartts: An open-source classical arabic text-tospeech corpus. In 2023 INTERSPEECH, pp. 55115515, 2023. doi: 10.21437/Interspeech.2023-2224. Le, T.-T., Nguyen, L. T., and Nguyen, D. Q. Phowhisper: Automatic speech recognition for vietnamese. arXiv preprint arXiv:2406.02555, 2024. Liao, H., Ni, Q., Wang, Y., Lu, Y., Zhan, H., Xie, P., Zhang, Q., and Wu, Z. Nvspeech: An integrated and scalable pipeline for human-like speech modeling w/ paralinguistic vocalizations. arXiv preprint arXiv:2508.04195, 2025. Luong, H.-T. and Vu, H.-Q. non-expert Kaldi recipe for Vietnamese speech recognition system. In Proceedings of the Third International WLSI / OIAF 4 HLT 2016, pp. 51 55, Osaka, Japan, December 2016. The COLING 2016 Organizing Committee. Namvarpour, M. and Razi, A. The art of talking machines: comprehensive literature review of conversational user interfaces. In Proceedings of the 7th ACM Conference on CUI, pp. 118, 2025. Nhut, P. Q., Anh, D. P. H., and Tiep, N. V. Vietspeech: Vietnamese social voice dataset, 2024. URL https: //github.com/NhutP/VietSpeech. Obeid, O., Zalmout, N., Khalifa, S., Taji, D., Oudah, M., Alhafni, B., Inoue, G., Eryani, F., Erdmann, A., and Habash, N. CAMeL tools: An open source python toolkit for Arabic natural language processing. In Proceedings of the 12th LREC, May 2020. Pratap, V., Sriram, A., Tomasello, P., Hannun, A., Liptchinsky, V., Synnaeve, G., and Collobert, R. Massively multilingual asr: 50 languages, 1 model, 1 billion parameters. arXiv preprint arXiv:2007.03001, 2020. Puvvada, K. C., Zelasko, P., Huang, H., Hrinchuk, O., Koluguri, N. R., Dhawan, K., Majumdar, S., Rastorgueva, E., Chen, Z., Lavrukhin, V., et al. Less is more: Accurate speech recognition & translation without web-scale data. arXiv preprint arXiv:2406.19674, 2024. Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., and Sutskever, I. Robust speech recognition via largescale weak supervision. In ICML. PMLR, 2023. Rege, A., Currano, R., Sirkin, D., and Kim, E. talking with your car: Design of human-centered conversational ai in autonomous vehicles. In Proceedings of the 16th International Conference on AutoUI, pp. 338349, 2024. Smoliakov. URL Yehor/opentts-uk. opentts-uk (revision 32abc9c), 2025. https://huggingface.co/datasets/ Su, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Toshniwal, S., Sainath, T. N., Weiss, R. J., Li, B., Moreno, P., Weinstein, E., and Rao, K. Multilingual speech recognition with single end-to-end model. In 2018 IEEE ICASSP, pp. 49044908. IEEE, 2018. Wang, Y., Alhmoud, A., and Alqurishi, M. Open Universal Arabic ASR Leaderboard. arXiv preprint arXiv:2412.13788, 2024. Yang, Y., Song, Z., Zhuo, J., Cui, M., Li, J., Yang, B., Du, Y., Ma, Z., Liu, X., Wang, Z., Li, K., Fan, S., Yu, K., Zhang, W.-Q., Chen, G., and Chen, X. Gigaspeech 2: An evolving, large-scale and multi-domain asr corpus for lowresource languages with automated crawling, transcription and refinement. arXiv preprint arXiv:2406.11546, 2024. Flavors of Moonshine: Tiny Specialized ASR Models for Edge Devices 7 A. Public Datasets Language Dataset (Hugging Face) Arabic Chinese Japanese Korean Ukrainian NeoBoy/arabic-tts-wav-24k (Kulkarni et al., 2023; Butt, 2025) nadsoft/arabic-98 MAdel121/arabic-egy-cleaned Sabri12blm/Arabic-Quran-ASR-dataset MohamedRashad/SADA22 (Alharbi et al., 2024) NightPrince/MasriSpeech-Full (Alnwsany, 2025) xmodar/commonvoice-12.0-arabic-voice-converted (Ardila et al., 2019) google/fleurs (Conneau et al., 2023) Hannie0813/NVSpeech170k (Liao et al., 2025) amphion/Emilia-Dataset (He et al., 2024) reazon-research/reazonspeech (Fujimoto, 2016) kadirnar/Combined-Japanese-TTS mozilla-foundation/common voice 17 0 (Ardila et al., 2019) amphion/Emilia-Dataset (He et al., 2024) brainer/korean-telemedicine-speech imTak/korean-audio-text-economy Junhoee/STT Korean Dataset JaepaX/korean dataset Bingsu/zeroth-korean (Jo & Lee, 2022) jp1924/GyeongsangSpeech amphion/Emilia-Dataset (He et al., 2024) jp1924/KoreanUniversityLectureData jp1924/KoreaSpeech jp1924/KconfSpeech jp1924/KrespSpeech jp1924/JeollaSpeech disco-eth/EuroSpeech (disco-eth, 2025) speech-uk/voice-of-america (Smoliakov, 2025) Vietnamese NhutP/VietSpeech (Nhut et al., 2024) speechcolab/gigaspeech2 (Yang et al., 2024) Hours 12 163 72 360 647 3100 148 116 573 50300 35000 995 610 266 1120 39 267 424 52 2546 7500 4000 3630 2970 2907 2121 1287 391 1100 7324 Table 5. Publicly-available training datasets. We include citations for datasets that have an associated paper, or that have citation instructions on the repo at time of writing. B. Normalization Steps This section outlines the normalization steps for each language. To normalize Arabic, we leveraged the normalization code provided by (Chowdhury et al., 2021; Wang et al., 2024). To be specific, we removed punctuation and diacritics, and converted Eastern Arabic numerals to Western Arabic numerals. For Korean, we leveraged the normalizing steps proposed by Zeroth 3. To normalize Korean, we removed all punctuation (including brackets) and converted Western Arabic numbers to their phonetic spellings. For Japanese, we followed the 3https://github.com/goodatlas/zeroth Flavors of Moonshine: Tiny Specialized ASR Models for Edge Devices 8 neologism dictionary for MeCab 4. Specifically, we leverage the default normalizer 5. For Ukranian, Vietnamese, and Chinese, we leverage the basic text normalizer provided by Whisper. C. Complete Results WER CER Model CV17 Casablanca Fleurs SADA22 CV17 Casablanca Fleurs SADA22 whisper-medium whisper-small whisper-base whisper-tiny moonshine-tiny (Baseline) moonshine-tiny (Final) 36.09 48.02 80.35 88.89 77.54 36.55 76.2 87.22 108.33 109.32 97.8 80.62 18.79 29.09 50.15 66.01 79.16 20. 69.48 88.55 111.39 109.76 100.82 66.55 12.4 17.39 38.21 44.02 40.41 12.94 39.16 46.19 63.16 66.98 60.75 43.1 5.82 9.1 17.7 25.7 40.97 7.62 44.39 57.08 73.92 72.93 61.5 35.38 Table 6. Arabic CER Model CV17 Fleurs whisper-medium whisper-small whisper-base whisper-tiny moonshine-tiny (Baseline) moonshine-tiny (Final) 28.06 35.46 54.11 65.92 22.17 36. 52.75 58.06 64.14 71.1 52.13 29.44 Table 7. Chinese CER Model CV17 Fleurs Reazon Speech whisper-medium whisper-small whisper-base whisper-tiny moonshine-tiny (Baseline) moonshine-tiny (Final) 29.09 37.69 71.13 96.11 30.99 18.3 11.5 16.76 27.13 47.2 19.16 17.87 Table 8. Japanese 43.06 68.37 119.81 146.81 11.28 10.89 4https://github.com/neologd/mecab-ipadic-neologd 5https://github.com/ikegami-yukino/neologdn Flavors of Moonshine: Tiny Specialized ASR Models for Edge Devices 9 WER CER Model CV Fleurs Zeroth CV17 Fleurs Zeroth whisper-medium whisper-small whisper-base whisper-tiny moonshine-tiny (Baseline) moonshine-tiny (Final) 26.93 33.38 45.45 63.79 55.46 35.22 16.18 19.56 27.16 35.44 29.1 20 21.83 27.95 43.56 47.84 12.48 16. 9.38 12.32 19.42 37.27 28.89 14.94 6.99 8.47 11.95 15.83 13.52 8.9 6.66 8.82 16.42 18.66 5.72 5.72 Table 9. Korean WER CER Model CV17 Fleurs CV17 Fleurs whisper-medium whisper-small whisper-base whisper-tiny moonshine-tiny (Baseline) moonshine-tiny (Final) 23.54 30.31 59.54 100.9 149.15 18.83 13.45 22.61 46.03 91.89 97.33 13.01 12.28 15.19 35.54 72.46 115.51 10.26 6.78 11.09 22.79 61.18 82.25 6.74 Table 10. Vietnamese WER CER Model CV17 Eurospeech Fleurs CV17 Eurospeech Fleurs whisper-medium whisper-small whisper-base whisper-tiny moonshine-tiny (Baseline) moonshine-tiny (Final) 20.9 32.44 55.55 67.07 92.49 26.11 17.01 24.94 46.06 69.41 64.93 14.73 11.62 20.41 44.08 63.83 92.28 18.25 5.93 9.03 18.4 24.03 63.49 8.57 8.99 11.1 18.6 28.93 44.21 7.81 3.68 5.33 14.51 20.69 68.12 6. Table 11. Ukrainian"
        }
    ],
    "affiliations": [
        "Moonshine AI"
    ]
}