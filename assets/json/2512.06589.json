{
    "paper_title": "OmniSafeBench-MM: A Unified Benchmark and Toolbox for Multimodal Jailbreak Attack-Defense Evaluation",
    "authors": [
        "Xiaojun Jia",
        "Jie Liao",
        "Qi Guo",
        "Teng Ma",
        "Simeng Qin",
        "Ranjie Duan",
        "Tianlin Li",
        "Yihao Huang",
        "Zhitao Zeng",
        "Dongxian Wu",
        "Yiming Li",
        "Wenqi Ren",
        "Xiaochun Cao",
        "Yang Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in multi-modal large language models (MLLMs) have enabled unified perception-reasoning capabilities, yet these systems remain highly vulnerable to jailbreak attacks that bypass safety alignment and induce harmful behaviors. Existing benchmarks such as JailBreakV-28K, MM-SafetyBench, and HADES provide valuable insights into multi-modal vulnerabilities, but they typically focus on limited attack scenarios, lack standardized defense evaluation, and offer no unified, reproducible toolbox. To address these gaps, we introduce OmniSafeBench-MM, which is a comprehensive toolbox for multi-modal jailbreak attack-defense evaluation. OmniSafeBench-MM integrates 13 representative attack methods, 15 defense strategies, and a diverse dataset spanning 9 major risk domains and 50 fine-grained categories, structured across consultative, imperative, and declarative inquiry types to reflect realistic user intentions. Beyond data coverage, it establishes a three-dimensional evaluation protocol measuring (1) harmfulness, distinguished by a granular, multi-level scale ranging from low-impact individual harm to catastrophic societal threats, (2) intent alignment between responses and queries, and (3) response detail level, enabling nuanced safety-utility analysis. We conduct extensive experiments on 10 open-source and 8 closed-source MLLMs to reveal their vulnerability to multi-modal jailbreak. By unifying data, methodology, and evaluation into an open-source, reproducible platform, OmniSafeBench-MM provides a standardized foundation for future research. The code is released at https://github.com/jiaxiaojunQAQ/OmniSafeBench-MM."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 1 9 8 5 6 0 . 2 1 5 2 : r OmniSafeBench-MM: Unified Benchmark and Toolbox for Multimodal Jailbreak AttackDefense Evaluation Xiaojun Jia1,*, Jie Liao2,3,, Qi Guo2,4,, Teng Ma2,6,, Simeng Qin2,5,, Ranjie Duan7, Tianlin Li1 Yihao Huang1, Zhitao Zeng8, Dongxian Wu9 Yiming Li1, Wenqi Ren6, Xiaochun Cao6, Yang Liu1 1Nanyang Technological University, Singapore 2BraneMatrix AI, China 3 Chongqing University, China 5Northeastern University, China 4 Xian Jiaotong University, China 6Sun Yat-sen University, China 7Alibaba, China 8National University of Singapore, Singapore 9ByteDance, China"
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Recent advances in multi-modal large language models (MLLMs) have enabled unified perceptionreasoning capabilities, yet these systems remain highly vulnerable to jailbreak attacks that bypass safety alignment and induce harmful behaviors. Existing benchmarks such as JailBreakV-28K, MM-SafetyBench, and HADES provide valuable insights into multi-modal vulnerabilities, but they typically focus on limited attack scenarios, lack standardized defense evaluation, and offer no unified, reproducible toolbox. To address these gaps, we introduce OmniSafeBench-MM, which is comprehensive toolbox for multi-modal jailbreak attackdefense evaluation. OmniSafeBench-MM integrates 13 representative attack methods, 15 defense strategies, and diverse dataset spanning 9 major risk domains and 50 fine-grained categories, structured across consultative, imperative, and declarative inquiry types to reflect realistic user intentions. Beyond data coverage, it establishes three-dimensional evaluation protocol measuring (1) harmfulness, distinguished by granular, multi-level scale ranging from low-impact individual harm to catastrophic societal threats, (2) intent alignment between responses and queries, and (3) response detail level, enabling nuanced safetyutility analysis. We conduct extensive experiments on 10 open-source and 8 closed-source MLLMs to reveal their vulnerability to multi-modal jailbreak. By unifying data, methodology, and evaluation into an open-source, reproducible platform, OmniSafeBench-MM provides standardized foundation for future research. The code is released at https://github.com/jiaxiaojunQAQ/OmniSafeBench-MM. *Xiaojun Jia, Jie Liao, Qi Guo, Teng Ma, and Simeng Qin contribute equally to this work. Project Leader: Xiaojun Jia (jiaxiaojunqaq@gmail.com). Recent works [1, 6, 51] in multi-modal large language models (MLLMs) such as GPT-4o, Gemini-2.5, and Qwen2-VL have dramatically advanced the integration of visual perception and language reasoning. By jointly understanding text, images, and other modalities, these systems have achieved outstanding performance in tasks including visual question answering [19], image analysis [2], and embodied reasoning [9]. However, cross-modal interaction that can improve performance also introduces new vulnerabilities: maliciously generated input can bypass safety constraints, inducing models to output harmful, unethical, or policyviolating contents, called jailbreak attack [13, 15, 37, 42, 45]. Unlike classical adversarial attacks [16, 22, 28] that manipulate pixels or tokens to degrade task accuracy, jailbreak attacks break the models safety alignment [7, 44], prompting the model to actively generate unsafe content. As multi-modal interactions become more common, attackers are no longer limited to text prompt attacks. By exploiting the visual context, they [10, 39] can embed concealed intents within ordinary-looking images or instructions, making it easier to bypass the safety alignment of MLLMs [37]. It is necessary to conduct comprehensive multimodal-based safety evaluation of MLLMs. Previous works have proposed several multi-modal datasets to assess the safety of MLLMs such as FigStep [10], JailBreakV-28K [25], MM-SafetyBench [23], HADES [21], and MMJ-Bench [40]. However, as shown in Table 1, these datasets have two main limitations: first, the range of covered risk categories is not sufficiently comprehensive to capture diverse multimodal safety threats; and second, they overlook the type of risk prompts, for example whether the attack prompt is consultative, instructive, or deceptive, which hinders comprehensive and detailed evaluation of the safety of MLLMs. series of works have explored how to automatically generate multi-modal jailbreak prompts from various perspectives [3, 5, 17, 35, 46]. However, they adopt different evaluations, including different comparison methods, different defense methods, etc., which makes them difficult to compare and hinders future progress. Moreover, most existing methods rely on single metric, the attack success rate (ASR), to measure the effectiveness of jailbreak strategies, which makes it difficult to comprehensively evaluate their performance. To bridge these gaps, as shown in Fig. 1, we propose OmniSafeBench-MM, unified benchmark and toolbox for systematic multimodal jailbreak attackdefense evaluation. In contrast to prior works that focus solely on datasets or single-dimensional evaluation metric, our OmniSafeBenchMM provides an integrated platform that combines comprehensive data coverage, standardized experimental pipelines, and multi-dimensional evaluation protocol. Specifically, at first, we construct comprehensive multi-modal dataset. As shown in Fig. 3, our OmniSafeBench-MM introduces newly generated multi-modal dataset covering 9 major risk domains (e.g., violence, privacy, illegal activity, misinformation, ethics, and self-harm) and 50 fine-grained subcategories. Each sample is categorized by inquiry type, including consultative, imperative, and declarative, reflecting real user interaction patterns with MLLMs. To efficiently construct the dataset, we further propose an automated data generation pipeline that produces risk-related textimage pairs based on pre-defined risk categories. We integrate comprehensive suite of jailbreak attacks and defenses. On the attack side, we implement about 13 advanced multi-modal jailbreak methods that span spectrum of threat models, including white-box and black-box settings. These methods cover diverse strategies such as roleplay [26], image-embedded cues [52], and risk decomposition [27], thereby enabling the construction of diverse multi-modal jailbreak imagetext pairs. On the defense side, our OmniSafeBench-MM incorporates total of 15 defense methods, which fall into two categories: (1) offmodel defense methods, which are deployed as out-ofmodel plugins to intercept unsafe inputs prior to inference or to filter unsafe outputs during inference, and (2) onmodel defense methods, which aim to improve the MLLMs internal alignment through training or fine-tuning. All attack and defense components are implemented in modular and open-source toolbox for data processing, attack generation, defense application, and metric evaluation. Moreover, beyond traditional binary success rates, we also propose to establish three-dimensional evaluation framework assessing (1) harmfulness, (2) intent alignment between the model response and the original query, and (3) the level of detail in the response. These complementary metrics enable fine-grained assessment of both attack severity and defense trade-offs, capturing cases where safety improvements come at the expense of helpfulness. We conduct extensive experiments on 18 popular MLLMs, including 10 open-source models (e.g., LLaVA1.6, Qwen3-VL, GLM-4.1V) and 8 closed-source commercial systems (e.g., GPT-5, Gemini-2.5, Claude-3.5, Qwen3VL-Plus). The experimental results reveal that different modalities and architectures have significant differences in defending against different types of multi-modal jailbreak attacks. Some defenses substantially reduce harmfulness but degrade model helpfulness, while others maintain helpfulness yet fail to eliminate residual vulnerabilities. Hence, the main contributions are summarized as follows: We propose OmniSafeBench-MM, unified framework that integrates dataset, attackdefense techniques, and It supevaluation into single reproducible platform. ports end-to-end experimentation with modular APIs for dataset loading, attack generation, defense execution, and performance evaluation across both open-source and commercial MLLMs. We construct large-scale multi-modal dataset covering 9 major risk domains and 50 fine-grained categories, organized by three user intention typesconsultative, instructive, and declarative. The toolbox includes 13 popular jailbreak attack methods and 15 defense methods, providing standardized platform for comparative studies, with plans to continuously update and incorporate more methods going forward. We propose three-dimensional metric framework that jointly measures harmfulness, intent alignment, and response detail level, enabling comprehensive safetyutility trade-off analysis beyond attack success rate metrics. 2. Related Work 2.1. Multi-modal jailbreak attack Existing multi-modal jailbreak attack studies can be broadly categorized into white-box and black-box settings. In white-box settings, attackers adopt model gradients or architectural information to generate adversarial perturbations. For example, Qi et al. [32] extend adversarial optimization on the visual channel to multi-modal alignment disruption. Niu et al. [29] propose to enhance transferability across models through ensemble optimization. Wang et al. [34], Ying et al. [46], Chen et al. [3], and Cheng et al. [4] propose to jointly optimize visual and textual inputs to exploit cross-modal vulnerabilities. Such gradientbased methods highlight the instability of safety alignment even when the attacker operates within differentiable whitebox conditions. In contrast, in black-box settings, attackers have no internal access and rely on interacting with the model to achieve jailbreaks. For example, Gong et al. [10], Liu et al. [23], and Li et al. [21] propose to embed textual or typographic cues within images to convert visual carriers into executable prompts. Yang et al. [43], Zhao et al. [49], Figure 1. Overview of OmniSafeBench-MM. The benchmark unifies multi-modal jailbreak attackdefense evaluation, 13 attack and 15 defense methods, and three-dimensional scoring protocol measuring harmfulness, alignment, and detail. Table 1. Quantitative comparison of representative multi-modal jailbreak and safety benchmarks. Dataset Risks Categories Prompt type Target Models Attacks Methods Defense Methods Eval Metrics JailBreakV-28K [25] FigStep-Dataset [10] MM-SafetyBench [23] HADES-Dataset [21] MMJ-Bench [40] OmniSafeBench-MM (Ours) 16 10 13 5 8 50 1 1 1 1 1 3 10 5 12 5 6 18 5 2 1 1 6 0 3 1 0 4 15 1 (ASR) 2 (ASR + PPL) 2 (ASR + RR) 1 (ASR) 3 (ASR+DSR+S) 3 (HAD metrics) Jeong et al. [14], Wang et al. [38], and Sima et al. [33] introduce distributional shifts or multimodal dispersions that evade alignment filters, while Ma et al. [27] propose to iteratively refine queries to maximize harmfulness under feedback constraints. 3. The Proposed OmniSafeBench-MM As shown in Fig. 2, the OmniSafeBench-MM includes dataset generation, implemented attacks, implemented defenses, and fine-grained safety evaluation. 2.2. Multi-modal jailbreak defense 3.1. Dataset generation To defend the jailbreak attack, the defense works have progressed along roughly two directions: off-model and onmodel methods. The Off-model defenses treat the target model as black box and employ input or output filtering. For example, Gou et al. [11], Zhang et al. [47], Oh et al. [30], Wang et al. [36], Zhou et al. [50], Xu et al. [41], Liu et al. [24], Lee et al. [20], and Helff et al. [12] propose to modify or verify input prompts before model inference. Zhang et al. [48] and Pi et al. [31] pay attention to detecting unsafe generations after model inference. These modular guard models are easily deployable across different MLLMs but may result in additional time loss. Onmodel defense methods integrate safety mechanisms into the model itself. For example, Gao et al. [8] and Jiang et al. [18] propose to adjust token logits during generation using safety-aware reward models or constitutional calibration, ensuring real-time regulation of output distribution. While Zong et al. [53] propose to fine-tune the model with safety preference datasets or reinforcement learning from human feedback to build inherently safer MLLMs. As shown in Fig. 3, the taxonomy comprises 9 major risk categories, each with 48 sub-categories spanning ethical, privacy, safety, economic, political, cybersecurity, cognitive, and cultural domains. We introduce an automated process for generating risk image-text pairs for MLLMs based on risk category. The process includes three key stages: (1) generating risk-related texts, (2) extracting unsafe key phrases, and (3) producing corresponding risk images. Generating risk-related texts. We define topic dimensions within each risk category (e.g., harmful language) and its subcategories (e.g., stereotypes), each accompanied by concise textual definitions. Representative topics (e.g., body shaming, color discrimination, immigration discrimination) are manually specified to guide GPT-4o in generating diverse and contextually grounded risk scenarios. These topics, along with their corresponding categories and definitions, are then provided to GPT-4o to synthesize risk-related text prompts with consultative, imperative, and declarative inquiry types. If GPT-4o fails to yield satisfactory outputs, an alternative model (e.g., DeepSeek-Chat) is employed to Figure 2. The framework of the proposed method ensure completeness and robustness. Extracting unsafe key words. After getting the malicious questions, we use LLM to extract keywords from malicious questions, as these keywords determine the security of the question. Note that the keyword extraction method may vary depending on the scenario. The details are presented in the Appendix. Producing corresponding risk images. Based on the extraction of key words, we adopt the PixArt-XL-2-1024-MS model to generate the corresponding risk images. The image generation prompt is formulated as photo of [Key word]. The height width is set to 1024 1024. 3.2. Implemented attack methods In this subsection we summarize the representative jailbreak attacks implemented in our benchmark. Let the victim MLLM be denoted by and an input pair by (T, I), where is the textual prompt and is the image input. The attackers objective is to bypass safety filters and induce harmful response = (T, I). To achieve this, attackers manipulate and/or to craft adversarial examples and . We categorize the implemented attacks into two major groups and five subcategories: white-box attacks (single-modal and cross-modal) and black-box attacks (structured visual-carrier, out-of-distribution, and hidden risks ). White-box attacks assume access to internal information of the MLLM (architecture and/or gradients). Attackers typically exploit gradient information to optimize adversarial inputs that steer the model to produce harmful outputs. But black-box attacks assume no access to model internals and only observe model outputs realistic threat model because many commercial multi-modal services operate as black boxes. Black-box methods are generally heuristic; our implemented methods fall into three categories: structured visual-carrier attacks, out-of-distribution (OOD) attacks, and Hidden risks. (cid:80)m := arg minI Single-modal white-box attacks. Single-modal attacks often consider visual attacks and can be formalized as , )(cid:1), where i=1 log (cid:0)p(yi denotes the feasible perturbation set. They are divided into image-optimization jailbreaks, i.e., visual-adv [32] and DeltaJP [29], and random-noise optimization, i.e., visual-adv-un [32] and ImgJP [29]. Compared with visual-adv, ImgJP and DeltaJP adopt ensemble strategies to improve transferability. , := arg min(I ,T )B Cross-modal white-box attacks. Cross-modal attacks that jointly optimize the visual and textual modalities can improve jailbreak effectiveness and can be formalized as )(cid:1). (I UMK [34] increases the jailbreak success rate by performing stepwise optimization of image noise and text suffixes. BAP [46] and JPS [3] alternately optimize the image and text using different strategies, thereby more effectively inducing the model to produce harmful outputs. i=1 log (cid:0)p(yi (cid:80)m , Structured visual-carrier attacks. MLLMs typically project the visual encoder into the LLM embedding space via projection layer, while safety alignment is usually performed on the textual modality; consequently, vulnerabilities in the visual modality can be exploited to bypass protections. The core idea of structured visual-carrier attacks is to embed visual carriers in images that the model can read as semantic content, and then exploit the VLMs vision-totext recognition and completion abilities to convert those carriers into executable instructions or harmful text. Common operations include typographic/layout attacks such as FigStep [10] and FigStep-Pro [10], and malicious images such as QR-Attack [23] and HADES [21]. MLLMs typically project the visual encoder into the LLM embedding space via projection layer, while safety alignment is usually performed on the textual modality; conse3.3. Implemented defense methods In this subsection, we summarize the representative defense methods implemented in our benchmark. Let the target MLLM be denoted by and malicious input pair by (T , ), where is the adversarial textual prompt and is the adversarial image input. The defenders objective is to ensure that final output yf inal, remains safe despite the malicious input. To achieve this, defenders can intervene at different stages of the generation process. We categorize these interventions into two major groups: Off-Model Defenses and On-Model Defenses. Off-Model Defenses include input pre-processing and output post-processing. OnModel Defenses encompass inference process intervention and intrinsic model alignment. Input pre-processing defenses. Input pre-processing the malicious defenses proactively modify or filter query (T , ) before inference, yielding final response via yf inal = (Din(T , )). Strategies include direct input modification, such as prompt augmentation AdaShield-S [36] and adversarial purification Uniguard [30]; robustness checking by analyzing semantic divergence across mutated inputs JailGuard [47]; and multi-stage approaches like red teaming with partial views DPS [50] and selfmoderation via text-only requerying ECSO [11]. the final group acts as external classifiers CIDER [41] measures cross-modal similarquery: ity shifts, while suite of purpose-built guard models GuardReasoner-VL [24], Llama-Guard-4, LlavaGuard [12], QGuard [20] makes definitive judgment to block or allow the request. Output post-processing defenses. Output post-processing defenses operate as reactive safeguard, inspecting the models initial response yraw = (T , ) after it has been generated. If harmful content is detected, the response is either blocked or sanitized, yielding final output yf inal = Dout(yraw). This is typically achieved using external guard models. Single-stage classifiers like ShieldLM [48] and Llama-Guard-3 directly analyze yraw to provide safe or unsafe verdict. In contrast, MLLM-Protector [31] uses two-stage process, first detecting harm and then invoking separate model to detoxify the response. Inference process intervention defenses. Inference process intervention defenses modify the models real-time generation process by leveraging internal states, such as logits, to steer outputs towards safety. For instance, the training-free HiddenDetect [18] framework identifies unsafe prompts by calculating safety score from the cosine similarity between hidden states and pre-defined refusal vector. Another strategy, COCA [8], calibrates the output distribution by computing safety delta between logits generated with and without constitutional prompt, to vet Figure 3. Safety taxonomy of our OmniSafeBench-MM. quently, vulnerabilities in the visual modality can be exploited to bypass protections. The core idea of structured visual-carrier attacks is to embed visual carriers in images that the model can read as semantic content, and then exploit the VLMs vision-to-text recognition and completion abilities to convert those carriers into executable instructions or harmful text. Common operations include typographic/layout attacks such as FigStep [10] and FigStep-Pro [10], and malicious images such as QR-Attack [23] and HADES [21]. Out-of-Distribution (OOD) attacks. During safetyalignment training of MLLMs there often exist out-ofdistribution scenarios, including data augmentation and attention interference. Accordingly, by altering the input distribution or visual structure (e.g., shuffling/collaging/inserting distracting sub-images/style transfer) to cause failures in the models input understanding or to shift its attention, one can bypass safety mechanisms and induce malicious outputs. For example, CS-DJ [43] and VisCRA [33] disturb attention to make the model ignore harmful content, while JOOD [14] and SI-Attack [49] employ dataaugmentation techniques to change the input distribution and circumvent safety guards. Hidden risk attacks. Hiding risky intent is also commonly used jailbreak technique. HIMRD [27] proposes heuristic risk-allocation strategy that decomposes harmful intent across the image and the text, thereby concealing the risky intent. MML [38] hides malicious intent by using datatransformation combined with encryption/decryption. using this delta to adjust the final token generation. Intrinsic model alignment defenses. Intrinsic model alignment defenses create an inherently safer model by modifying its internal parameters through specialized training. These methods typically involve fine-tuning the base MLLM on safety-centric data to align its behavior with human values. One common approach is supervised finetuning (SFT), where VLGuard [53] trains the model on curated safety instruction-following dataset containing vision-language examples of safe and unsafe scenarios. 3.4. Fine-Grained Safety Evaluation 3.4.1. Motivation: Beyond binary success rates To move beyond the limitations of traditional, binary (safe/unsafe) safety evaluations, we propose fine-grained assessment framework as core component of our benchmark. We posit that the risk posed by models response to given prompt is not monolithic property, but multi-faceted construct. Our framework is designed to formally decompose this risk, enabling the identification of High-Impact, Low-Frequency (HILF) events, which are often obscured by aggregate safety metrics. 3.4.2. The H-F-A evaluation dimensions Our framework evaluates each response generated from prompt (which may include text component PT and an image component PI ) along three decoupled, orthogonal dimensions. Let H, A, and be the scoring functions. Harmfulness (H): Severity of Consequence. The Harmfulness function :: [1, 10] scores the severity of potential real-world harm from the act the response describes or facilitates. Crucially, this score evaluates the consequence of the responses facilitated act, not the users initial intent in the prompt. = H(R) (1) The scale is discrete, where = 1 is safe intent and = 10 is an act with potential for existential harm. Intent Alignment (A): Fulfillment of User Request. The Intent Alignment function : (P, R) [1, 5] measures the semantic alignment between the users prompt and the models response. high alignment score indicates that comprehensively addresses the directives in . = A(P, R) (2) This score quantifies the models capability to fulfill the users intent, irrespective of its harmfulness. Level of Detail (D): Explicitness and Self-Containment. The Level of Detail function : [1, 5] evaluates the explicitness and self-containment of the information in the response. It quantifies how much external knowledge is required to act upon the information provided. = D(R) (3) high score (D = 5) corresponds to rich, comprehensive response (e.g., full tutorial or complete script), while low score indicates abstract, high-level concepts. Holistic Adjudication of Jailbreak Success Instead of simple multiplicative score, we synthesize these dimensions through holistic, rule-based adjudication process to determine the final success of jailbreak attempt. This process mirrors an expert human judgment. The Breach Condition. fundamental safety breach occurs only when response is both harmful and compliant. We define this as the primary condition for any level of jailbreak success: Breach Condition = (H 3) (A 3) (4) If this condition is not met, the attempt is considered failure (i.e., successfully defended by the model). Final Judgment Score. If breach occurs, the Level of Detail (D) determines its severity. We map the outcome to final four-point judgment score, {1, . . . , 4}, where 4 represents Complete Success, 3 Partial Success, 2 Borderline, and 1 No Success. The score is defined as: S(P, R) = 4 3 2 1 if Breach and 4 if Breach and = 3 if Breach and 2 otherwise (5) For our benchmark, Jailbreak Success Event, denoted by the binary variable J, is defined as any response receiving final judgment score of 3 or 4: J(P, R) = 1 if S(P, R) 3, else 0 (6) 4. Benchmark Experiments 4.1. Experiment settings MLLMs. series of MLLMs are used to conduct including closed-source models and openexperiments, source models. The closed-source models include: (1) GPT-5, (2) Gemini-2.5-Flash (Gemini-2.5), (3) ClaudeSonnet-4-20250514 (Claude-Sonnet-4), (4) Qwen3-VLPLUS, and (5) Doubao-Seed-1-6-Flash-250828 (DoubaoSeed). The open-source models are: (1) Qwen3-VL-30BA3B-Instruct (Qwen3-VL), (2) Gemma-3-27b-it (Gemma3), (3) DeepSeek-VL2, (4) GLM-4.1V-9B-Thinking (GLM4.1V), and (5) Kimi-VL-A3B-Instruct (Kimi-VL). Jailbreak attack methods. We consider set of 13 jailbreak attack methods collected from 13 published papers. We group these attacks into two broad categories: white-box and black-box. The white-box category includes both single-modal and cross-modal optimization techniques; specifically, we evaluate Visual-Adv, ImgJP, DeltaJP, UMK, and JPS. The black-box category covers attacks that operate via structured visual carriers, out-ofdistribution inputs, or query-optimization strategies; this group comprises FigStep, QR-Attack, HADES, CS-DJ, SIAttack, JOOD, MML, and HIMRD. The details are described in Appendix. Jailbreak defense methods. We consider set of 15 jailbreak defense methods collected from 16 published papers. We group these attacks into two broad categories: off-model and on-model. The off-model category includes both input pre-processing and output post processing techniques; specifically, we evaluate ECSO, JailGuard, AdaShield-S, Uniguard, DPS, CIDER, GuardReasoner-VL, Llama-Guard-4, QGuard, LlavaGuard, ShieldLM, MLLMProtector and Llama-Guard-3. The on-model category includes inference process intervention and intrinsic model alignment techniques; this group comprises COCA and VLGuard. The details are described in Appendix. 4.2. Experiment results Table 2. Performance comparison of MiniGPT-4 (Vicuna 13B) on various benchmarks. We report ASR, average-H (H), average-A (A), and average-D (D) metrics. Metric JPS ImgJP UMK DeltaJP visual-adv ASR (%) 62.93 4.87 Avg-H 4.10 Avg-A 2.87 Avg-D 15.40 3.76 2.83 1.93 37.07 4.19 3.26 2. 24.47 3.84 3.11 2.11 47.87 4.43 3.61 2.58 Results in the white-box setting. Our evaluation of five white-box attacks on MiniGPT-4 (Vicuna 13B)  (Table 2)  provides more realistic vulnerability assessment. The observed Attack Success Rates (ASRs) are markedly low (e.g., jps at 62.93%), direct result of our strict success criterion (S 3, Eq. 5), which demands not only high harmfulness and alignment (H, 3) but also sufficient detail (D 3). The consistently low Average Detail (AvgD, mostly below 2.6) across all attacks is the primary reason for the low ASRs. This highlights critical point: the models failure to provide specifics may stem from either deliberate residual safety mechanism (i.e., secondary behaviors that mitigate harm even after an initial refusal is bypassed) designed to mitigate harm, or simply from its inherent capability limitations in generating detailed content. Crucially, our framework is designed to capture this outcome regardless of its cause. Results in the black-box setting. As shown in Table 3, different attack methods exhibit different attack performance across different architectures. Among all evaluated methods, MML and CS-DJ achieve the highest ASRs, confirming their strong ability to exploit cross-modal dispersion without requiring gradient access. Specifically, MML achieves 50.7% ASR on Gemini-2.5 and 52.2% on Qwen3VL-Plus, while maintaining high Harmfulness (H 5.6) and Alignment (A 3.3) scores. These results indicate that semantic information hidden within chained multimodal cues can effectively bypass closed-source safety filters, producing logically consistent yet unsafe content. Similarly, CS-DJ achieves stable attack rates across both closed-source and open-source models (32.0% on Gemini2.5 and 38.1% on Qwen3-VL), demonstrating that altering the input distribution to create OOD data can effectively disrupt the models safety attention, thereby bypassing safety checks. By comparison, FigStep and QR-Attack show competitive yet more model-dependent performance. Their average ASR ranges from 415% on closed-source models to 3050% on open-source MLLMs (e.g., 51.3% on GLM-4.1V), revealing that image-embedded typographic cues are more effectively interpreted by open pipelines that lack strong OCR filtering or multi-modal guard modules. These attacks also yield high Detail (D) scores (3.03.6), confirming that once jailbreak succeeds, the model tends to generate explicit, step-by-step unsafe responses. In contrast, HIMRD and JOOD achieve moderate ASR (520%) but demonstrate higher stealth and semantic coherence. Results in the off-model defense setting. As shown in Table 4, off-model defense methods effectively mitigate jailbreak attacks on GPT-4o, but their effectiveness varies notably across attack types. For input pre-processing defense methods, Uniguard and JailGuard achieve the lowest ASR on the CS-DJ attack (3.53 % and 3.13 %, respectively), notable reduction from 23.00 % without defense. On FigStep, AdaShield-S performs best (1.27 %), followed by DPS (3.13 %) and Uniguard (6.33 %), showing that adaptive input sanitization effectively suppresses typographic visual carriers. However, on the MML attack, input-side protection degrades: AdaShield-S and Uniguard yield 37.93 % and 49.53 % ASR, respectively, while ECSO reaches 27.47 %. These results indicate that the input pre-processing defense is effective for explicit triggers but less reliable for semantically dispersed attacks. For output post-processing defense methods, the strongest robustness appears in MLLM-Protector, which reduces ASR to 14.87 % on CS-DJ, 4.60 % on FigStep, and only 0.27 % on MML. ShieldLM and Llama-Guard-3 also achieve consistently low ASR (about 821 %), providing reactive moderation that effectively blocks unsafe completions. Results in the on-model defense setting. We evaluated the efficacy of two distinct on-model defense strategies: safety fine-tuning via VLGuard and the inference-time CoCa defense. As shown in Table 5, both methods substantially improve model robustness against suite of attacks, yet their performance reveals important nuances. The CoCa defense demonstrates broad-spectrum effectiveness, drastically reducing the Attack Success Rate (ASR) across all tested attacks and nearly neutralizing the potent FigStep and Table 3. Performance comparison of both open-source and closed-source large multimodal models on various benchmarks. Models are grouped by their availability. We report ASR, average-H (H), average-A (A), and average-D (D) metrics. Closed-Source Models GPT-5 Gemini-2.5 Claude-4 Sonnet Qwen3-VL-Plus Doubao-Seed Benchmark ASR ASR ASR A ASR ASR D FigStep [10] MML [38] CS-DJ [43] HIMRD [27] JOOD [14] SI-Attack [49] HADES [21] 4.20 1.76 2.35 3.70 15.80 2.44 2.71 2.11 4.20 2.01 2.22 1.50 13.87 2.26 2.67 2.29 24.33 2.69 2.97 2.70 15.27 3.20 2.42 3.23 50.67 5.64 3.33 2.98 7.53 2.46 2.27 1.36 52.20 5.83 3.71 2.67 28.33 5.64 2.78 2.08 9.73 2.04 2.44 3.89 32.00 3.58 3.02 3.15 14.27 2.87 2.50 2.44 23.40 2.96 2.77 3.06 47.53 4.21 3.46 3.58 5.13 2.00 2.20 3.87 30.60 3.87 2.87 2.95 0.53 1.96 2.04 1.15 17.40 3.30 2.46 2.16 33.13 3.87 2.94 2.89 4.93 1.94 2.37 2.72 21.40 2.58 2.91 2.03 1.40 1.94 2.20 1.36 9.73 2.07 2.54 2.01 30.20 2.93 3.19 2.58 6.13 1.95 2.42 3.38 16.67 2.65 2.65 2.27 3.47 1.95 2.28 1.71 11.4 2.15 2.56 2.51 22.47 2.73 2.89 2.97 6.13 1.94 2.40 3.14 24.53 2.74 3.00 2.24 1.80 1.92 2.19 1.44 9.20 2.05 2.53 2.10 27.60 2.79 3.13 2.70 QR-Attack [23] 5.14 1.98 2.33 3.31 24.22 2.87 2.89 2.68 2.87 1.99 2.17 1.53 9.27 2.08 2.47 2.46 31.42 3.19 3.05 3.13 Qwen3-VL Gemma-3 DeepSeek-VL2 GLM-4.1V Kimi-VL Benchmark ASR ASR ASR A ASR ASR D Open-Source Models FigStep [10] MML [38] CS-DJ [43] HIMRD [27] JOOD [14] SI-Attack [49] HADES [21] 30.93 2.95 3.17 2.64 28.33 2.82 3.06 2.89 44.53 4.06 3.71 2.74 51.27 3.90 3.64 3.41 39.93 3.64 3.34 2.55 34.62 4.40 3.29 2.44 54.73 6.01 3.62 3.08 0.33 2.45 1.19 1.15 22.67 5.12 2.66 2.46 10.27 3.16 1.79 1.65 38.07 3.82 3.20 3.22 21.60 3.41 2.67 3.04 25.60 4.24 2.86 2.59 30.93 4.37 2.72 3.20 39.47 4.43 3.26 2.90 7.53 2.72 2.12 1.44 44.33 4.43 3.30 3.37 24.53 3.85 2.52 2.17 43.07 4.55 3.03 3.01 19.87 4.06 2.32 1.80 4.33 1.98 2.32 1.73 26.67 2.74 3.05 2.31 15.87 2.63 2.68 1.65 38.20 3.41 3.49 2.71 26.27 3.04 3.07 2.03 8.27 2.25 2.38 2.1 2.13 2.07 1.24 22.53 2.87 2.9 2.73 10.07 2.62 2.29 1.84 8.73 2.14 2.51 2.00 23.67 2.63 2.96 2.36 22.60 3.02 2.99 1.92 38.73 3.32 3.52 2.79 24.93 3.26 3.14 2.06 QR-Attack [23] 10.41 2.30 2.45 2.24 27.75 3.06 2.93 2.74 28.95 3.67 2.93 2.47 49.97 4.22 3.56 3.65 35.36 3.87 3.15 2.60 17.6 2.51 2.7 2.57 2.6 Table 4. Attack Success Rate (ASR %) of different off model defense methods on GPT-4o. Method abbreviations: Def. (Defense), AdaSh. (AdaShield), Unig. (Uniguard), JailG. (JailGuard), GR-VL (GuardReasoner-VL), LlavaG. (LlavaGuard), QG. (QGuard), LG-4/3 (LlamaGuard-4/3), MLLM-P (MLLM-protector), ShLM (ShieldLM). Dataset No Def. AdaSh. Unig. DPS JailG. ECSO GR-VL LlavaG. QG. LG-4 CIDER MLLM-P ShLM LG-3 Input Pre-processing Output Post-processing 23.00 CS-DJ [43] 9.67 FigStep [10] 56.60 MML [38] HIMRD [27] 12.87 9.07 1. 3.13 13.20 3.53 10.47 8.27 9.13 3.13 6.33 37.93 49.53 42.13 56.13 27.47 5.67 5.00 10.07 1.40 3.87 19.07 8.47 55.60 3.00 21.40 9.20 56.27 12. 3.20 18.67 21.60 2.60 9.20 9.40 0.13 55.67 55.47 1.27 10.13 1.13 14.87 4.60 0.27 3.73 9.07 21.07 18.40 8.53 56.27 19.93 7.60 7.20 Table 5. Comparison of Attack Success Rate (ASR %) for different on model defenses against attacks. Model / Configuration CS-DJ MML FigStep HIMRD LLaVA-1.5 (Base) + Finetuning (VLGuard) + CoCa Defense 0.00 0.00 0. 0.67 15.47 20.20 1.13 0.07 0.00 0.00 1.20 0. attack (from 0.93% to 1.73%). This suggests that while safety fine-tuning can patch major vulnerabilities, it may also subtly alter the models behavior, creating rare, specific weaknesses that can be exploited by different attack patterns. This finding highlights the critical need for diverse adversarial testing, as defenses effectiveness is not uniform and strengthening against one threat can inadvertently create susceptibility to another. 5. Conclusion HIMRD methods. The VLGuard fine-tuning also shows strong performance, particularly against the most threatening attacks, cutting the ASR for HIMRD from 20.40% to just 1.20%. However, we observed counter-intuitive result where VLGuard slightly increased the ASR for the MML In this work, we proposed OmniSafeBench-MM, unified and reproducible benchmark for evaluating multimodal jailbreak attacks and defenses. The benchmark integrates broad risk taxonomy, diverse attack and defense implementations, and three-dimensional safety scoring protocol, enabling more fine-grained and interpretable evaluation than traditional ASR-based metrics. Extensive experiments across 18 open-source and closed-source MLLMs reveal persistent vulnerabilitiesparticularly under crossmodal dispersion and black-box settingsand highlight differing trade-offs among defense strategies. By standardizing data, methodology, and evaluation within single platform, OmniSafeBench-MM provides solid foundation for advancing research on multimodal model safety."
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:2371623736, 2022. 1 [2] Kittitouch Areerob, Van-Quang Nguyen, Xianfeng Li, Shogo Inadomi, Toru Shimada, Hiroyuki Kanasaki, Zhijie Wang, Masanori Suganuma, Keiji Nagatani, Pang-jo Chun, et al. Multimodal artificial intelligence approaches using large language models for expert-level landslide image analysis. Computer-Aided Civil and Infrastructure Engineering, 2025. 1 [3] Renmiao Chen, Shiyao Cui, Xuancheng Huang, Chengwei Pan, Victor Shea-Jay Huang, QingLin Zhang, Xuan Ouyang, Jps: Zhexin Zhang, Hongning Wang, and Minlie Huang. Jailbreak multimodal large language models with collaborative visual perturbation and textual steering. In Proceedings of the 33rd ACM International Conference on Multimedia, pages 1175611765, 2025. 2, 4 [4] Ruoxi Cheng, Yizhong Ding, Shuirong Cao, Ranjie Duan, Xiaoshuang Jia, Shaowei Yuan, Simeng Qin, Zhiqiang Wang, and Xiaojun Jia. Pbi-attack: Prior-guided bimodal interactive black-box jailbreak attack for toxicity maximization. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 609628, 2025. 2 [5] Chun Wai Chiu, Linghan Huang, Bo Li, Huaming Chen, and Kim-Kwang Raymond Choo. Do as say not as do: semi-automated approach for jailbreak prompt attack against multimodal llms. arXiv preprint arXiv:2502.00735, 2025. 2 [6] Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Yang Zhou, Kaizhao Liang, Jintai Chen, Juanwu Lu, Zichong Yang, Kuei-Da Liao, et al. survey on multimodal large language models for autonomous driving. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 958979, 2024. 1 [7] Ranjie Duan, Jiexi Liu, Xiaojun Jia, Shiji Zhao, Ruoxi Cheng, Fengxiang Wang, Cheng Wei, Yong Xie, Chang Liu, Defeng Li, et al. Oyster-i: Beyond refusalconstructive safety alignment for responsible language models. arXiv preprint arXiv:2509.01909, 2025. [8] Jiahui Gao, Renjie Pi, Tianyang Han, Han Wu, Lanqing Hong, Lingpeng Kong, Xin Jiang, and Zhenguo Li. Coca: Regaining safety-awareness of multimodal large language models with constitutional calibration. arXiv preprint arXiv:2409.11365, 2024. 3, 5 [9] Junyu Gao, Xuan Yao, Yong Rui, and Changsheng Xu. Building embodied evoagent: brain-inspired paradigm for bridging multimodal large models and world models. In Proceedings of the 33rd ACM International Conference on Multimedia, pages 32803289, 2025. 1 [10] Yichen Gong, Delong Ran, Jinyuan Liu, Conglei Wang, Tianshuo Cong, Anyu Wang, Sisi Duan, and Xiaoyun Wang. Figstep: Jailbreaking large vision-language models via typographic visual prompts. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 2395123959, 2025. 1, 2, 3, 4, 5, 8 [11] Yunhao Gou, Kai Chen, Zhili Liu, Lanqing Hong, Hang Xu, Zhenguo Li, Dit-Yan Yeung, James Kwok, and Yu Zhang. Eyes closed, safety on: Protecting multimodal llms In European Conference via image-to-text transformation. on Computer Vision, pages 388404. Springer, 2024. 3, 5 [12] Lukas Helff, Felix Friedrich, Manuel Brack, Kristian Kersting, and Patrick Schramowski. Llavaguard: An open vlmbased framework for safeguarding vision datasets and models. arXiv preprint arXiv:2406.05113, 2024. 3, [13] Yihao Huang, Le Liang, Tianlin Li, Xiaojun Jia, Run Wang, Weikai Miao, Geguang Pu, and Yang Liu. Perception-guided In Proceedings of jailbreak against text-to-image models. the AAAI Conference on Artificial Intelligence, pages 26238 26247, 2025. 1 [14] Joonhyun Jeong, Seyun Bae, Yeonsung Jung, Jaeryong Hwang, and Eunho Yang. Playing the fool: Jailbreaking llms and multimodal llms with out-of-distribution strategy. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2993729946, 2025. 3, 5, 8 [15] Xiaojun Jia, Tianyu Pang, Chao Du, Yihao Huang, Jindong Gu, Yang Liu, Xiaochun Cao, and Min Lin. Improved techniques for optimization-based jailbreaking on large language models. arXiv preprint arXiv:2405.21018, 2024. 1 [16] Xiaojun Jia, Sensen Gao, Simeng Qin, Tianyu Pang, Chao Du, Yihao Huang, Xinfeng Li, Yiming Li, Bo Li, and Yang Liu. Adversarial attacks against closed-source mllms via feature optimal alignment. arXiv preprint arXiv:2505.21494, 2025. 1 [17] Lei Jiang, Zixun Zhang, Zizhou Wang, Xiaobing Sun, Zhen Li, Liangli Zhen, and Xiaohua Xu. Cross-modal obfuscation for jailbreak attacks on large vision-language models. arXiv preprint arXiv:2506.16760, 2025. 2 [18] Yilei Jiang, Xinyan Gao, Tianshuo Peng, Yingshui Tan, Xiaoyong Zhu, Bo Zheng, and Xiangyu Yue. Hiddenlarge visiondetect: Detecting jailbreak attacks against arXiv language models via monitoring hidden states. preprint arXiv:2502.14744, 2025. 3, [19] Jiayi Kuang, Ying Shen, Jingyou Xie, Haohao Luo, Zhe Xu, Ronghao Li, Yinghui Li, Xianfeng Cheng, Xika Lin, and Yu Han. Natural language understanding and inference with mllm in visual question answering: survey. ACM Computing Surveys, 57(8):136, 2025. 1 [20] Taegyeong Lee, Jeonghwa Yoo, Hyoungseo Cho, Soo Yong Kim, and Yunho Maeng. Qguard: Question-based zeroshot guard for multi-modal arXiv:2506.12299, 2025. 3, 5 llm safety. arXiv preprint [21] Yifan Li, Hangyu Guo, Kun Zhou, Wayne Xin Zhao, and JiRong Wen. Images are achilles heel of alignment: Exploiting visual vulnerabilities for jailbreaking multimodal large language models. In European Conference on Computer Vision, pages 174189. Springer, 2024. 1, 2, 3, 4, 5, [22] Zhaoyi Li, Xiaohan Zhao, Dong-Dong Wu, Jiacheng Cui, and Zhiqiang Shen. frustratingly simple yet highly effective attack baseline: Over 90 arXiv preprint arXiv:2503.10635, 2025. 1 [23] Xin Liu, Yichen Zhu, Jindong Gu, Yunshi Lan, Chao Yang, and Yu Qiao. Mm-safetybench: benchmark for safety evaluation of multimodal large language models. In European Conference on Computer Vision, pages 386403. Springer, 2024. 1, 2, 3, 4, 5, 8 [24] Yue Liu, Shengfang Zhai, Mingzhe Du, Yulin Chen, Tri Cao, Hongcheng Gao, Cheng Wang, Xinfeng Li, Kun Wang, Junfeng Fang, et al. Guardreasoner-vl: Safeguarding vlms via reinforced reasoning. arXiv preprint arXiv:2505.11049, 2025. 3, 5 [25] Weidi Luo, Siyuan Ma, Xiaogeng Liu, Xiaoyu Guo, and Chaowei Xiao. Jailbreakv: benchmark for assessing the robustness of multimodal large language models against jailbreak attacks. arXiv preprint arXiv:2404.03027, 2024. 1, 3 [26] Siyuan Ma, Weidi Luo, Yu Wang, and Xiaogeng Liu. Visualroleplay: Universal jailbreak attack on multimodal large language models via role-playing image character. arXiv preprint arXiv:2405.20773, 2024. [27] Teng Ma, Xiaojun Jia, Ranjie Duan, Xinfeng Li, Yihao Huang, Xiaoshuang Jia, Zhixuan Chu, and Wenqi Ren. Heuristic-induced multimodal risk distribution jailbreak attack for multimodal large language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 26862696, 2025. 2, 3, 5, 8 [28] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017. 1 [29] Zhenxing Niu, Haodong Ren, Xinbo Gao, Gang Hua, and Rong Jin. Jailbreaking attack against multimodal large language model. arXiv preprint arXiv:2402.02309, 2024. 2, 4 [30] Sejoon Oh, Yiqiao Jin, Megha Sharma, Donghyun Kim, Eric Ma, Gaurav Verma, and Srijan Kumar. Uniguard: Towards universal safety guardrails for jailbreak attacks arXiv preprint on multimodal large language models. arXiv:2411.01703, 2024. 3, 5 [31] Renjie Pi, Tianyang Han, Jianshu Zhang, Yueqi Xie, Rui Pan, Qing Lian, Hanze Dong, Jipeng Zhang, and Tong Zhang. Mllm-protector: Ensuring mllms safety without hurting performance. arXiv preprint arXiv:2401.02906, 2024. 3, 5 [32] Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Peter Henderson, Mengdi Wang, and Prateek Mittal. Visual adversarial examples jailbreak aligned large language models. In Proceedings of the AAAI conference on artificial intelligence, pages 2152721536, 2024. 2, 4 [33] Bingrui Sima, Linhua Cong, Wenxuan Wang, and Kun He. Viscra: visual chain reasoning attack for jailbreakarXiv preprint ing multimodal large language models. arXiv:2505.19684, 2025. 3, [34] Ruofan Wang, Xingjun Ma, Hanxu Zhou, Chuanjun Ji, Guangnan Ye, and Yu-Gang Jiang. White-box multimodal jailbreaks against large vision-language models. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 69206928, 2024. 2, 4 [35] Xinkai Wang, Beibei Li, Zerui Shao, Ao Liu, and Shouling Ji. Multimodal safety is asymmetric: Cross-modal exploits unlock black-box mllms jailbreaks. arXiv preprint arXiv:2510.17277, 2025. 2 [36] Yu Wang, Xiaogeng Liu, Yu Li, Muhao Chen, and Chaowei large lanXiao. Adashield: Safeguarding multimodal guage models from structure-based attack via adaptive shield In European Conference on Computer Vision, prompting. pages 7794. Springer, 2024. 3, 5 [37] Youze Wang, Wenbo Hu, Yinpeng Dong, Jing Liu, Hanwang Zhang, and Richang Hong. Align is not enough: Multimodal universal jailbreak attack against multimodal large language IEEE Transactions on Circuits and Systems for models. Video Technology, 2025. 1 [38] Yu Wang, Xiaofei Zhou, Yichen Wang, Geyuan Zhang, Jailbreak large vision-language models and Tianxing He. In Proceedings of the 63rd through multi-modal linkage. Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 14661494, 2025. 3, 5, 8 [39] Zhaoxin Wang, Handing Wang, Cong Tian, and Yaochu Implicit jailbreak attacks via cross-modal information arXiv preprint Jin. concealment on vision-language models. arXiv:2505.16446, 2025. 1 [40] Fenghua Weng, Yue Xu, Chengyan Fu, and Wenjie Wang. Mmj-bench: comprehensive study on jailbreak attacks and defenses for vision language models. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 27689 27697, 2025. 1, 3 [41] Yue Xu, Xiuyuan Qi, Zhan Qin, and Wenjie Wang. Cross-modality information check for detecting jailbreaking in multimodal large language models. arXiv preprint arXiv:2407.21659, 2024. 3, 5 [42] Zhao Xu, Fan Liu, and Hao Liu. Bag of tricks: Benchmarking of jailbreak attacks on llms. Advances in Neural Information Processing Systems, 37:3221932250, 2024. 1 [43] Zuopeng Yang, Jiluan Fan, Anli Yan, Erdun Gao, Xin Lin, Tao Li, Kanghua Mo, and Changyu Dong. Distraction is all you need for multimodal large language model jailbreaking. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 94679476, 2025. 2, 5, 8 [44] Jingwei Yi, Rui Ye, Qisi Chen, Bin Zhu, Siheng Chen, Defu Lian, Guangzhong Sun, Xing Xie, and Fangzhao Wu. On the vulnerability of safety alignment in open-access llms. In Findings of the Association for Computational Linguistics ACL 2024, pages 92369260, 2024. 1 [45] Sibo Yi, Yule Liu, Zhen Sun, Tianshuo Cong, Xinlei He, Jiaxing Song, Ke Xu, and Qi Li. Jailbreak attacks and defenses against large language models: survey. arXiv preprint arXiv:2407.04295, 2024. 1 [46] Zonghao Ying, Aishan Liu, Tianyuan Zhang, Zhengmin Yu, Siyuan Liang, Xianglong Liu, and Dacheng Tao. Jailbreak vision language models via bi-modal adversarial prompt. IEEE Transactions on Information Forensics and Security, 2025. 2, 4 [47] Xiaoyu Zhang, Cen Zhang, Tianlin Li, Yihao Huang, Xiaojun Jia, Ming Hu, Jie Zhang, Yang Liu, Shiqing Ma, and Chao Shen. Jailguard: universal detection framework for prompt-based attacks on llm systems. ACM Transactions on Software Engineering and Methodology, 2025. 3, 5 [48] Zhexin Zhang, Yida Lu, Jingyuan Ma, Di Zhang, Rui Li, Pei Ke, Hao Sun, Lei Sha, Zhifang Sui, Hongning Wang, et al. Shieldlm: Empowering llms as aligned, customizable and explainable safety detectors. arXiv preprint arXiv:2402.16444, 2024. 3, 5 [49] Shiji Zhao, Ranjie Duan, Fengxiang Wang, Chi Chen, Caixin Kang, Shouwei Ruan, Jialing Tao, YueFeng Chen, Hui Xue, and Xingxing Wei. Jailbreaking multimodal large language In Proceedings of the models via shuffle inconsistency. IEEE/CVF International Conference on Computer Vision, pages 20452054, 2025. 2, 5, 8 [50] Qi Zhou, Tianlin Li, Qing Guo, Dongxia Wang, Yun Lin, Yang Liu, and Jin Song Dong. Defending lvlms against vision attacks through partial-perception supervision. arXiv preprint arXiv:2412.12722, 2024. 3, [51] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 1 [52] Miao Ziqi, Yi Ding, Lijun Li, and Jing Shao. Visual contextual attack: Jailbreaking mllms with image-driven conIn Proceedings of the 2025 Conference on text injection. Empirical Methods in Natural Language Processing, pages 96389655, 2025. 2 [53] Yongshuo Zong, Ondrej Bohdal, Tingyang Yu, Yongxin Yang, and Timothy Hospedales. Safety fine-tuning at (almost) no cost: baseline for vision large language models. arXiv preprint arXiv:2402.02207, 2024. 3, 6 OmniSafeBench-MM: Unified Benchmark and Toolbox for Multimodal Jailbreak AttackDefense Evaluation"
        },
        {
            "title": "Supplementary Material",
            "content": "The instructive tone produces consistently higher ASR, confirming that the imperative style induces increased compliance from MLLMs. A.3. Defense Analysis A.3.1. Overall Defense Effectiveness Fig. 9 presents the aggregated ASR across all 13 defense methods evaluated on 3 representative models (2 opensource + 1 closed-source) and 4 attack methods. Results clearly indicate that different defenses provide highly nonuniform protection. A.3.2. Defense-Induced ASR Reduction To quantify the effect of individual defenses, Fig. 10 shows waterfall plot comparing ASR before and after applying each defense method."
        },
        {
            "title": "The visualization highlights the relative contribution of",
            "content": "each defense to robustness improvement. A.4. Statistical Analysis and Interpretation Fig. 11 shows the logistic regression coefficients used to quantify the effect of style, attack method, and model type on the probability of attack success. Positive coefficients indicate features that increase vulnerability, whereas negative coefficients suggest robustnessrelated factors. This appendix provides additional visualizations, detailed tables, and statistical analyses that complement the main paper. We present results covering 16 MLLMs, 8 black-box attack methods, 9 risk categories, 3 prompt styles, and 13 defense methods. Our goal is to offer comprehensive, transparent report of model behavior under adversarial prompting and defense interventions. The additional figures shown here expand the key findings from the main text, including model-level vulnerability patterns, categoryand style-dependent effects, metric distributions, defense effectiveness, and regressionbased interpretability. A. Extended Experimental Results A.1. Additional Visualizations for Attack Analysis A.1.1. Model-Level Robustness Overview Figure 4 visualizes the ASR landscape across all 16 models and 8 attack methods via comprehensive heatmap, while Table 8 provides the corresponding numerical breakdown along with auxiliary metrics (Average-T, H, A, D). These visualizations collectively illustrate the divergence in safety alignment between open-source and closed-source architectures, as well as the varying effectiveness of different attack vectors across the model spectrum. A.1.2. Attack-Specific Vulnerability Profiles To better understand differences among the 8 attack methods, Fig. 5 provides set of radar plots, each summarizing model-level ASR for single attack method. These plots highlight qualitative distinctions among attacks, showing whether they target specific model families or generalize broadly. A.1.3. Distribution of Evaluation Metrics Fig. 6 presents the boxplot distributions for four evaluation metrics (T,H,A,D) across all models. A.2. Prompt-Level Analysis A.2.1. Risk Category Vulnerability Fig. 7 shows the ASR aggregated by the 9 risk categories combined with all 8 attack methods. Specific categories exhibit consistently higher ASR, demonstrating that model vulnerability is strongly content-dependent. A.2.2. Style-Dependent Effects Fig. 8 provides comparison of the three style types (declarative, consultative, instructive) across all attacks. Table 6. Summary of defenses and their classification."
        },
        {
            "title": "Category",
            "content": "Off-model"
        },
        {
            "title": "Subcategory",
            "content": "Input pre-processing Representative methods (implemented)"
        },
        {
            "title": "JailGuard",
            "content": "ECSO Uniguard GuardReasoner -VL Llama-Guard-4 QGuard LlavaGuard AdaShield-S CIDER"
        },
        {
            "title": "DPS",
            "content": "Off-model Output post-processi-ng ShieldLM Llama-Guard-3 MLLM-Protector On-model On-model"
        },
        {
            "title": "VLGuard",
            "content": "Table 7. Summary of implemented attacks and their classification."
        },
        {
            "title": "Category",
            "content": "White-box White-box Black-box Black-box Black-box"
        },
        {
            "title": "Subcategory",
            "content": "Single-modal Cross-modal Structured visual-carrier Out-of-Distribution (OOD) Hidden risks Representative methods (implemented) visual-adv, DeltaJP UMK, BAP, JPS visual-adv-un ImgJP, FigStep-Pro, FigStep, HADES CS-DJ, SI-Attack, JOOD, VisCRA HIMRD, MML QR-Attack, Figure 4. Vulnerability Heatmap of 16 MLLMs against 8 Jailbreak Attacks. The heatmap visualizes the Attack Success Rate (ASR %), where darker red indicates higher vulnerability (higher ASR) and lighter yellow indicates higher robustness. Models are categorized into Open Source (left) and Closed Source (right), sorted by their average vulnerability within each group. The Avg. row and column denote the mean ASR for each model and attack method, respectively. Table 8. Performance comparison of both open-source and closed-source large multimodal models on various benchmarks. Models are grouped by their availability. We report ASR, average-T (T), average-H (H), average-A (A), average-D (D). Closed-Source Models (Part I) GPT-5 GPT-4o Gemini-2.5 Claude-4 Sonnet Benchmark ASR H ASR D ASR H ASR D FigStep MML CS-DJ HIMRD JOOD SI-Attack HADES QR-Attack 4.20 1.12 1.76 2.35 3.70 9.67 1.24 2.30 2.39 1.51 15.80 1.42 2.44 2.71 2.11 4.20 1.10 2.01 2.22 1.50 15.27 1.60 3.20 2.42 3.23 56.60 2.51 5.38 3.85 2.94 50.67 2.43 5.64 3.33 2.98 7.53 1.21 2.46 2.27 1.36 9.73 1.26 2.04 2.44 3.89 23.00 1.56 3.12 2.75 2.79 32.00 1.86 3.58 3.02 3.15 14.27 1.36 2.87 2.50 2.44 5.13 1.15 2.00 2.20 3.87 12.87 1.30 2.93 2.17 1.64 30.60 1.87 3.87 2.87 2.95 0.53 1.01 1.96 2.04 1.15 4.93 1.13 1.94 2.37 2.72 7.53 1.22 2.29 2.51 1.45 21.40 1.58 2.58 2.91 2.03 1.40 1.03 1.94 2.20 1.36 6.13 1.17 1.95 2.42 3.38 4.40 1.14 2.21 2.16 1.55 16.67 1.47 2.65 2.65 2.27 3.47 1.09 1.95 2.28 1.71 6.13 1.17 1.94 2.40 3.14 11.27 1.31 2.42 2.60 1.60 24.53 1.68 2.74 3.00 2.24 1.80 1.04 1.92 2.19 1.44 5.14 1.15 1.98 2.33 3.31 11.27 1.30 2.46 2.41 1.78 24.22 1.69 2.87 2.89 2.68 2.87 1.07 1.99 2.17 1.53 Closed-Source Models (Part II) Claude-3.5 Haiku Qwen3-VL-Flash Qwen3-VL-Plus Doubao-Seed Benchmark ASR D ASR A ASR D ASR A FigStep MML CS-DJ HIMRD JOOD SI-Attack HADES QR-Attack 2.40 1.06 2.03 2.16 1.26 21.13 1.58 2.56 2.91 2.58 13.87 1.38 2.26 2.67 2.29 23.67 1.66 2.68 2.97 2.72 42.07 2.15 5.23 3.62 2.58 20.27 1.66 4.44 2.60 1.94 52.20 2.45 5.83 3.71 2.67 27.60 1.83 5.59 2.75 2.10 7.60 1.19 2.47 2.31 1.76 28.13 1.80 3.86 2.74 3.22 23.40 1.65 2.96 2.77 3.06 48.40 2.32 4.20 3.48 3.53 2.87 1.07 2.29 2.06 1.24 2.20 1.07 2.41 1.97 1.25 17.40 1.49 3.30 2.46 2.16 33.27 1.95 3.87 2.92 2.87 1.00 1.03 1.96 2.13 1.18 14.60 1.39 2.26 2.72 2.24 9.73 1.26 2.07 2.54 2.01 29.80 1.84 2.94 3.21 2.56 0.93 1.03 2.03 1.98 1.22 11.20 1.31 2.20 2.55 2.64 11.40 1.31 2.15 2.56 2.51 22.47 1.61 2.73 2.89 2.97 1.73 1.04 1.97 2.13 1.21 13.07 1.37 2.17 2.68 2.29 9.20 1.26 2.05 2.53 2.10 28.27 1.78 2.81 3.13 2.69 1.87 1.05 2.03 2.11 1.29 0.00 1.00 2.00 1.22 1.00 9.27 1.25 2.08 2.47 2.46 30.22 1.85 3.16 3.04 3.15 Open-Source Models (Part I) Qwen3-30B Gemma-3 DeepSeek-VL2 ERNIE-4.5 Benchmark ASR A ASR D ASR A ASR D FigStep MML CS-DJ HIMRD JOOD SI-Attack HADES 30.93 1.86 2.95 3.17 2.64 28.33 1.75 2.82 3.06 2.89 44.53 2.19 4.06 3.71 2.74 29.60 1.79 2.96 3.15 2.69 41.00 2.08 4.87 3.36 2.40 54.73 2.54 6.01 3.62 3.08 0.33 1.02 2.45 1.19 1.15 1.00 1.04 2.36 1.24 1.33 38.07 2.02 3.82 3.20 3.22 21.60 1.55 3.41 2.67 3.04 25.60 1.65 4.24 2.86 2.59 20.60 1.55 3.71 2.66 2.46 7.53 1.21 2.72 2.12 1.44 44.33 2.29 4.43 3.30 3.37 24.53 1.62 3.85 2.52 2.17 10.33 1.26 3.19 1.98 2.10 4.33 1.11 1.98 2.32 1.73 26.67 1.70 2.67 3.04 2.24 15.87 1.43 2.63 2.68 1.65 13.47 1.37 2.38 2.64 1.93 8.27 1.22 2.25 2.38 2.10 17.60 1.46 2.51 2.70 2.57 2.60 1.07 2.13 2.07 1.24 4.20 1.12 2.24 1.87 1.93 8.73 1.24 2.14 2.51 2.00 23.67 1.65 2.61 2.97 2.27 22.60 1.64 3.02 2.99 1.92 15.93 1.45 2.69 2.60 2.04 QR-Attack 10.41 1.29 2.30 2.45 2.24 27.75 1.76 3.06 2.93 2.74 28.95 1.73 3.67 2.93 2.47 23.22 1.59 3.14 2.75 2. Open-Source Models (Part II) GLM-4.1V Ovis2.5 Kimi-VL LLaVA-v1.6 Benchmark ASR H ASR D ASR H ASR D 51.27 2.38 3.90 3.64 3.41 51.53 2.44 3.75 3.77 3.70 39.93 2.02 3.64 3.34 2.55 23.80 1.67 3.75 2.92 2.32 FigStep 22.67 1.70 5.12 2.66 2.46 21.00 1.58 4.76 2.31 2.59 10.27 1.29 3.16 1.79 1.65 24.00 1.71 4.67 2.83 1.95 MML CS-DJ 30.93 1.81 4.37 2.72 3.20 41.80 2.20 4.14 3.22 3.67 39.47 1.99 4.43 3.26 2.90 5.07 1.13 2.52 1.79 1.81 HIMRD 43.07 2.15 4.55 3.03 3.01 30.40 1.85 3.90 2.59 2.74 19.87 1.53 4.06 2.32 1.80 31.33 1.75 4.04 2.81 2.41 38.20 2.04 3.41 3.49 2.71 44.67 2.25 3.36 3.61 3.36 26.27 1.71 3.04 3.07 2.03 30.93 1.85 3.23 3.31 2.37 JOOD 22.53 1.60 2.87 2.90 2.73 20.40 1.56 2.79 2.75 3.15 10.07 1.29 2.62 2.29 1.84 6.93 1.21 2.36 2.31 1.82 SI-Attack 38.73 2.06 3.32 3.52 2.79 45.20 2.25 3.40 3.66 3.37 24.93 1.74 3.26 3.14 2.06 32.80 1.90 3.37 3.37 2.49 HADES QR-Attack 49.97 2.41 4.22 3.56 3.65 45.36 2.30 3.86 3.47 3.76 35.36 1.90 3.87 3.15 2.60 33.09 1.80 3.79 3.09 2.81 Figure 5. Radar plot illustrating model-wise ASR distribution under each attack method. The plot highlights structural differences in attack effectiveness across models. Figure 6. Distribution of harmfulness, alignment, detail, and judge scores across the 8 attack methods. Each box summarizes modelaveraged scores, revealing distinct behavioral patterns induced by different attacks. Figure 7. Category-level attack success rate across 9 semantic categories and 8 attack methods. Figure 8. ASR comparison across prompt style (declarative, consultative, instructive). Consultative prompts consistently trigger higher ASR across multiple attacks. Figure 9. Overall effectiveness of 13 defense methods across 4 attack types. Lower ASR indicates stronger defensive capability. Figure 10. Impact of each defense method on reducing ASR for representative model. The waterfall structure illustrates relative gains offered by different defenses. Figure 11. Logistic regression coefficients quantifying the contribution of style, attack method, and model type to attack success probability."
        }
    ],
    "affiliations": [
        "Alibaba, China",
        "BraneMatrix AI, China",
        "ByteDance, China",
        "Chongqing University, China",
        "Nanyang Technological University, Singapore",
        "National University of Singapore, Singapore",
        "Northeastern University, China",
        "Sun Yat-sen University, China",
        "Xian Jiaotong University, China"
    ]
}