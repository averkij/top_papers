{
    "paper_title": "VisText-Mosquito: A Multimodal Dataset and Benchmark for AI-Based Mosquito Breeding Site Detection and Reasoning",
    "authors": [
        "Md. Adnanul Islam",
        "Md. Faiyaz Abdullah Sayeedi",
        "Md. Asaduzzaman Shuvo",
        "Muhammad Ziaur Rahman",
        "Shahanur Rahman Bappy",
        "Raiyan Rahman",
        "Swakkhar Shatabda"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Mosquito-borne diseases pose a major global health risk, requiring early detection and proactive control of breeding sites to prevent outbreaks. In this paper, we present VisText-Mosquito, a multimodal dataset that integrates visual and textual data to support automated detection, segmentation, and reasoning for mosquito breeding site analysis. The dataset includes 1,828 annotated images for object detection, 142 images for water surface segmentation, and natural language reasoning texts linked to each image. The YOLOv9s model achieves the highest precision of 0.92926 and mAP@50 of 0.92891 for object detection, while YOLOv11n-Seg reaches a segmentation precision of 0.91587 and mAP@50 of 0.79795. For reasoning generation, our fine-tuned BLIP model achieves a final loss of 0.0028, with a BLEU score of 54.7, BERTScore of 0.91, and ROUGE-L of 0.87. This dataset and model framework emphasize the theme \"Prevention is Better than Cure\", showcasing how AI-based detection can proactively address mosquito-borne disease risks. The dataset and implementation code are publicly available at GitHub: https://github.com/adnanul-islam-jisun/VisText-Mosquito"
        },
        {
            "title": "Start",
            "content": "VisText-Mosquito: Multimodal Dataset and Benchmark for AI-Based Mosquito Breeding Site Detection and Reasoning Md. Adnanul Islam1*, Md. Faiyaz Abdullah Sayeedi1*, Md. Asaduzzaman Shuvo1, Muhammad Ziaur Rahman1, Shahanur Rahman Bappy1, Raiyan Rahman2, Swakkhar Shatabda3 1United International University, Bangladesh 2University of Portsmouth, United Kingdom 3BRAC University, Bangladesh {mislam221096, msayeedi212049, ashuvo221104, mrahman202004, sbappy211002}@bscse.uiu.ac.bd, raiyan@cse.uiu.ac.bd, swakkhar.shatabda@bracu.ac.bd AbstractMosquito-borne diseases pose major global health risk, requiring early detection and proactive control of breeding sites to prevent outbreaks. In this paper, we present VisTextMosquito, multimodal dataset that integrates visual and textual data to support automated detection, segmentation, and reasoning for mosquito breeding site analysis. The dataset includes 1,828 annotated images for object detection, 142 images for water surface segmentation, and natural language reasoning texts linked to each image. The YOLOv9s model achieves the highest precision of 0.92926 and mAP@50 of 0.92891 for object detection, while YOLOv11n-Seg reaches segmentation precision of 0.91587 and mAP@50 of 0.79795. For reasoning generation, our finetuned BLIP model achieves final loss of 0.0028, with BLEU score of 54.7, BERTScore of 0.91, and ROUGE-L of 0.87. This dataset and model framework emphasize the theme Prevention is Better than Cure, showcasing how AI-based detection can proactively address mosquito-borne disease risks. The dataset and implementation code are publicly available at GitHub: https://github.com/adnanul-islam-jisun/VisText-Mosquito Index TermsMosquito Breeding Site, Object Detection, Segmentation, Natural Language Generation, Multimodal, Public Health I. INTRODUCTION Mosquito-borne diseases continue to be leading cause of illness and death worldwide, particularly affecting lowand middle-income countries. According to the World Health Organization (WHO), approximately 700 million people are affected by mosquito-borne illnesses every year, resulting in over one million deaths [1]. Diseases such as Malaria, Dengue Fever, Zika Virus, and Chikungunya are transmitted by mosquito vectors that breed primarily in stagnant water. The economic burden associated with these diseases is also substantial; for instance, malaria alone is estimated to cost African economies over $12 billion annually due to healthcare expenses and lost productivity [2]. Despite ongoing efforts to combat these diseases, the incidence of dengue has increased 30-fold in the past 50 years, with nearly 390 million cases reported annually [3]. The *These authors contributed equally to this work. rapid pace of urbanization and poor water management in many regions further exacerbates the problem by creating ideal conditions for mosquito breeding. Traditional mosquito control methods, including manual inspection and elimination of breeding sites, are labor-intensive, time-consuming, and often infeasible in large or inaccessible areas. These challenges underscore the urgent need for scalable, technology-driven approaches to detect and manage mosquito habitats more efficiently. Although artificial intelligence and computer vision have made significant strides, the task of accurately detecting mosquito breeding places and analyzing water surfaces remains challenging. Existing models often underperform due to lack of specialized datasets capable of handling both detection and segmentation tasks. Moreover, most datasets are unimodal, lacking contextual explanations that could aid in human interpretation or decision-making [4]. This absence of interpretability limits the utility of AI models in realworld public health scenarios, where both performance and explainability are critical. To address these gaps, we introduce VisText-Mosquito, multimodal dataset specifically designed to support three interrelated tasks: (i) detection of mosquito breeding sites, (ii) segmentation of water surfaces within these sites, and (iii) generation of natural language reasoning that describes the visual content and justifies AI predictions. The dataset comprises three main parts: Breeding"
        },
        {
            "title": "Place",
            "content": "Detection: 1,828 images with 3,752 annotations across five classes: coconut_exocarp, vase, tire, drain_inlet, and bottle."
        },
        {
            "title": "Includes",
            "content": "Water Surface Segmentation: Contains 142 images with 253 annotations for two classes: vase_with_water and tire_with_water. Textual Reasoning: Each image is also paired with human-authored natural language explanation, enabling models to learn visual-linguistic associations and produce interpretable reasoning. 5 2 0 2 7 1 ] . [ 1 9 2 6 4 1 . 6 0 5 2 : r Fig. 1. Overview of the VisText-Mosquito pipeline, which involves several key steps, beginning with the collection of image and textual data, annotation, preprocessing, and model training. It supports object detection, water surface segmentation, and reasoning generation for robust mosquito breeding site analysis. This paper is structured as follows: Section II reviews existing related works. Section III describes the methodology employed in our study. Section IV presents the result analysis, discussing the performance of our models across all modalities. Finally, Section concludes the paper with summary of contributions and future research directions. II. EXISTING WORKS Mosquito-borne diseases remain significant global health threat, necessitating innovative AI-driven approaches for early detection and control. Machine learning models and highquality datasets play pivotal role in identifying mosquito breeding sites, enabling proactive interventions. Several datasets facilitate this research, including MosquitoFusion [5], M. Mehra et al. [6], and Chathura et al. [7], which capture diverse breeding environments and enhance model adaptability. Object detection models like YOLOv8 have demonstrated effectiveness, achieving an mAP@50 of 57.1% on the MosquitoFusion dataset. CNN-based segmentation models have also been used to identify stagnant water surfaces in drone and satellite imagery with high precision [8]. Transformer-based approaches such as DETR and SAM further improve feature representation and segmentation performance in mosquito habitat analysis tasks [9]. Meanwhile, geospatial AI techniques leveraging satellite and UAV imagery have been successfully applied to GIS-based mosquito risk mapping, enabling detection and classification of urban breeding hotspots with spatial accuracy [10]. Additionally, multimodal fusion approaches that incorporate weather, topographical, and epidemiological data have enhanced vector prediction models for disease outbreak mapping and control [11]. Despite these advancements, notable gaps remain. Many existing datasets are limited to controlled environments and focus on single-class detection, which reduces their generalizability to complex, real-world scenarios [12]. Most critically, and to the best of our knowledge, no prior work has introduced multimodal dataset that integrates both visual (image-based detection and segmentation) and textual (natural language reasoning) components for mosquito breeding site analysis. This represents significant gap in the field, as combining vision and language could improve interpretability, model trust, and real-world applicability of AI-driven mosquito surveillance systems. III. METHODOLOGY In this section, we discuss the detailed methodology of our proposed solution shown in Figure 1. A. Data Collection The data collection process is designed to ensure diversity, accuracy, and real-world relevance in capturing mosquito breeding sites and water surfaces. High-quality images are collected from various regions across Bangladesh, covering diverse breeding habitats under both daylight (8 AM5 PM) and nighttime conditions to enhance dataset variability. To improve model generalization, multiple images are taken from different angles and distances (13 meters), ensuring detailed visual representation. Both natural and artificial breeding sites are documented, though challenges such as unpredictable weather and difficult terrain occasionally impacted data collection. Ethical considerations are prioritized by obtaining permission from local authorities and property owners. The process remains non-invasive, avoiding harm to natural habitats or disruptions to local communities. Anonymization techniques are applied to protect sensitive location details. The initial dataset comprises 1,828 images with 3,752 annotations for breeding place detection and 142 images with 253 annotations for water surface segmentation. This ensures diverse, comprehensive foundation for training models on Fig. 2. Fully tagged and labeled images mosquito habitat detection and surface segmentation. Additionally, text modality is included in the dataset to enable multimodal analysis. This dataset contains 3,762 instances, each associated with an image and annotated with three text fields: (a) Question: binary question asking whether the image shows mosquito breeding site. (b) Response: Yes or No answer (3,748 Yes and 14 No responses). (c) Reasoning: short free-text explanation justifying the response. The average length of the reasoning statements is approximately 36 words. This textual annotation provides semantic context and interpretability, significantly enhancing the datasets capacity for explainable AI. B. Data Preprocessing The data preprocessing involves annotation, transformations, and augmentation to enhance the dataset. All images are manually annotated using the Roboflow [13] platform, ensuring precise labeling of mosquito breeding sites and water surfaces. Figure 2 shows examples of the annotated images. The following preprocessing steps are applied to each image: (a) Auto-Orient: Images are auto-oriented to correct any device orientation inconsistencies. (b) Resize: All images are resized to 640x640 pixels for uniform input shape. (c) Auto-Adjust Contrast: The contrast is automatically adjusted to enhance visual clarity. To improve model robustness, augmentation techniques are applied: (a) Flip: Horizontal flips doubled the dataset size by varying object orientations. (b) Rotation: Random rotations introduce alignment variations. (c) Brightness Adjustment: Image brightness is varied to simulate real-world lighting conditions. As result of these the total number of images in the dataset augmentations, increases to 4,425 for the detection part and 331 for the segmentation part. This augmentation strategy significantly enhances the datasets variability, ensuring that the models trained on this dataset would be more robust and capable of generalizing well to unseen data. For the text modality, the binary responses (Yes or No) and the accompanying reasoning statements were initially generated using the Gemini-2.5-Flash model. To ensure high-quality annotations, all generated responses are subsequently curated and validated by human annotators. This semi-automated annotation workflow allows efficient dataset expansion while preserving semantic integrity and contextual accuracy. C. Distribution Analysis and Folder Structure The breeding place detection subset of the dataset comprises total of 1,828 images with 3,752 annotations distributed across five classes. The class-wise distribution indicates that the Coconut-Exocarp class has the highest number of instances with 923 annotations, followed closely by the Vase class with 911 annotations. The Tire class contains 780 annotations, while the Drain-Inlet and Bottle classes have 585 and 553 annotations, respectively. For the segmentation part of the dataset, there are 142 images with total of 253 annotations across two classes: vase_with_water and tire_with_water. The vase_with_water class has significantly higher number of annotations, with 181 instances, compared to the tire_with_water class, which contains 72 annotations. Table summarizes the class-wise annotation distribution in our dataset. TABLE ANNOTATION DISTRIBUTION IN VISTEXT-MOSQUITO DATASET Category Breeding Place Detection Water Surface Segmentation Class coconut_exocarp vase tire drain-inlet bottle vase_with_water tire_with_water Annotations 923 911 780 585 553 181 In addition to visual data, the dataset contains textual annotations in the form of reasoning responses that describe the rationale behind each detection or segmentation. Analysis of the reasoning texts reveals that the average length is approximately 230 characters, with most entries ranging between 175 and 280 characters. The text lengths follow roughly normal distribution, indicating consistency in the annotation style. Most frequently occurring terms in the reasoning responses include phrases such as stagnant water, mosquitoes, coconut shell, and potential breeding site, reflecting common descriptors and domain-specific language used during the annotation process. The organization of the dataset is designed to optimize data management and accessibility for both object detection and segmentation tasks. The dataset is divided into three main directories: Train, Valid, and Test. Each of these directories contains two sub-folders: images: This folder houses the visual data in the form of images collected from various mosquito breeding sites. labels: This folder contains the corresponding annotation files for each image. The annotations detail the positions and classes of objects or segments identified in the images, serving as guide for training the machine learning models. In addition to the visual components, This dual-folder structure is consistently maintained across the Train, Valid, and Test directories to streamline the datasets usability. Organizing the data in this manner facilitates the training and validation processes by clearly distinguishing between the images and their respective labels. the dataset also includes textual reasoning component, which provides natural language justifications for each image annotation. These reasoning texts are stored in separate CSV file that contains filename column. This column acts as key to link each reasoning entry directly to its corresponding image file in the dataset. D. Experimental Setup All the images in the dataset are manually reviewed to ensure that no individually identifiable information was included or embedded in the dataset. This careful review process is implemented to maintain privacy standards and ensure the datasets suitability for training deep learning models. For the breeding place detection part, object detection models are trained using pre-trained versions of the YOLOv5s, YOLOv8n, and YOLOv9s models. For the segmentation part, the YOLOv8x-Seg and YOLOv11n-Seg models are employed due to their advanced capabilities in pixel-level segmentation tasks, which are critical for accurately identifying water surfaces in potential mosquito breeding sites. For the textual reasoning component, we utilize the BLIP (Bootstrapped Language Image Pretraining) [14] model to generate natural language descriptions that explain the visual is vision-language content of each annotated image. It model (VLM) that integrates image and text understanding and is pretrained on large-scale image-caption datasets using contrastive and generative objectives. In our method, we finetune the BLIP model on our curated set of reasoning texts aligned with the annotated images. During training, the model learned to associate specific visual patterns, such as objects like tires or vases containing water, with semantically rich textual descriptions that reflect potential mosquito breeding risks."
        },
        {
            "title": "The dataset",
            "content": "is randomly split into three subsets: 70% training images, 20% validation images, and 10% test images. This split is done to ensure comprehensive evaluation of the models performance, providing ample data for both training and validation while reserving portion for unbiased testing. The training process is conducted on Windows 11 (Version 23H2) machine equipped with the following hardware: Nvidia RTX 3070Ti GPU with 8GB of video memory and AMD Ryzen 5800X processor. The training is run for total of 100 epochs with the input image size set to 640 pixels, and standard hyperparameters are utilized throughout the training sessions to ensure consistent and reproducible results. IV. RESULT ANALYSIS In this section, we present comprehensive performance analysis of the models used for three core tasks: object detection, water surface segmentation, and multimodal reasoning. Evaluation metrics include Precision, Recall, and Mean Average Precision at 50% Intersection over Union (mAP@50), commonly adopted for assessing object detection and segmentation, alongside BLEU, BERTScore, ROUGE-L, and final loss for text generation evaluation. A. Object Detection Performance To detect potential mosquito breeding containers, we train and evaluate three object detection models, YOLOv5s, YOLOv8n, and YOLOv9s, on 1,828 annotated images across five object classes. The detailed performance is summarized in Table II. TABLE II OBJECT DETECTION MODEL PERFORMANCE Model YOLOv5s YOLOv8n YOLOv9s Precision 0.91514 0.89028 0.92926 Recall 0.87595 0.87314 0.86011 mAP@50 0.92400 0.90817 0.92891 The YOLOv9s model achieves the highest precision and mAP@50, demonstrating its superior ability to accurately localize and classify breeding-related objects. In contrast, YOLOv5s offers the most balanced performance, maintaining high recall and demonstrating stable predictions across all classes, making it suitable for applications where minimizing false negatives is essential. YOLOv8n performed slightly lower in all metrics, suggesting that the architectural advancements in YOLOv9s provide meaningful edge in complex real-world imagery. B. Water Surface Segmentation Performance The segmentation task is focused on identifying water surfaces in objects like vases and tires, critical indicators of mosquito breeding potential. We evaluate two advanced models, YOLOv8x-Seg and YOLOv11n-Seg, on 142 images annotated for vase_with_water and tire_with_water. Table III presents their performance: TABLE III SEGMENTATION MODEL PERFORMANCE Model YOLOv8x-Seg YOLOv11n-Seg Precision 0.89372 0.91587 Recall 0.73074 0. mAP@50 0.79345 0.79795 YOLOv11n-Seg consistently outperform YOLOv8x-Seg across all three metrics. The improved recall indicates that YOLOv11n-Seg was more effective in correctly segmenting water surfaces without missing positive cases, which is essential in public health applications. The marginal gains in mAP@50 suggest greater consistency in its pixel-level predictions, which is especially important in distinguishing water patches under occlusion, varying illumination, or cluttered backgrounds. C. Multimodal Reasoning Performance In the textual reasoning task, we fine-tune the BLIP model to generate natural language justifications for each detection. is trained on image-reasoning pairs from the This model dataset to map visual patterns to semantically aligned textual outputs. After training, the model achieves final loss of 0.0028, indicating successful convergence. The quality of generated reasoning is quantitatively evaluated using BLEU, BERTScore, and ROUGE-L metrics, as shown in Table IV. TABLE IV MULTIMODAL REASONING PERFORMANCE (BLIP MODEL) Metric Loss (Final) BLEU BERTScore ROUGE-L Score 0.0028 54.7 0.91 0.87 The high BERTScore (0.91) and ROUGE-L (0.87) indicate that the generated reasoning texts closely matched the semantic and structural properties of the ground truth. The BLEU score of 54.7 confirms strong n-gram overlap, which is valuable in capturing factual consistency. Qualitative reviews of sample outputs showed that BLIP could correctly contextualize key visual cues, such as stagnant water, container types, and environmental clutter, into meaningful, human-readable explanations, enhancing model transparency. The combined results from detection, segmentation, and reasoning tasks validate the robustness and applicability of the VisText-Mosquito dataset and the supporting models. The YOLOv9s and YOLOv11n-Seg models exhibited stateof-the-art performance in their respective tasks, while the BLIP model added interpretability by providing contextual descriptions aligned with public health goals. This multimodal pipeline offers powerful toolset for scalable mosquito surveillance and control systems, bridging the gap between detection accuracy and decision-making transparency. V. CONCLUSION In conclusion, the VisText-Mosquito dataset provides comprehensive multimodal resource for advancing AI-driven mosquito breeding site detection, water surface segmentation, and natural language reasoning. YOLOv9s achieve the highest precision and mAP@50 for breeding place detection, while YOLOv11n-Seg delivers superior segmentation performance. Additionally, the fine-tuned BLIP model effectively generated human-like reasoning, demonstrating the feasibility of integrating visual and textual modalities in public health. These findings emphasize the value of multimodal learning in enabling early and accurate identification of mosquito breeding sites. In the future, we plan to expand the dataset across more ecological settings and object categories, improve the diversity and quality of reasoning annotations, and explore promptbased large language models for generating region-specific intervention strategies."
        },
        {
            "title": "REFERENCES",
            "content": "[1] A. I. Qureshi, Chapter 2 - mosquito-borne diseases, in Zika Virus Disease, A. I. Qureshi, Ed. Academic Press, 2018, pp. 2745. [Online]. Available: https://www.sciencedirect.com/science/article/pii/ B9780128123652000032 [2] L. Braack, A. P. Gouveia de Almeida, A. J. Cornel, R. Swanepoel, and C. De Jager, Mosquito-borne arboviruses of african origin: review of key viruses and vectors, Parasites & vectors, vol. 11, pp. 126, 2018. [3] S.-L. Fong, K.-T. Wong, and C.-T. Tan, Dengue virus infection and neurological manifestations: an update, Brain, vol. 147, no. 3, pp. 830 838, 2024. [4] C. Laranjeira, D. Andrade, and J. A. dos Santos, Yolov7 for mosquito breeding grounds detection and tracking, in 2023 IEEE International Conference on Image Processing Challenges and Workshops (ICIPCW), 2023, pp. 15. [5] M. F. A. Sayeedi, F. Hafiz, and M. A. Rahman, Mosquitofusion: multiclass dataset for real-time detection of mosquitoes, swarms, and breeding sites using deep learning, arXiv preprint arXiv:2404.01501, 2024. [6] M. Mehra, A. Bagri, X. Jiang, and J. Ortiz, Image analysis for identifying mosquito breeding grounds, in 2016 IEEE International conference on sensing, communication and networking (SECON workshops). IEEE, 2016, pp. 16. [7] C. Suduwella, A. Amarasinghe, L. Niroshan, C. Elvitigala, K. De Zoysa, and C. Keppetiyagama, Identifying mosquito breeding sites via drone images, in proceedings of the 3rd workshop on micro aerial vehicle networks, systems, and applications, 2017, pp. 2730. [8] M. Mehra, A. Bagri, X. Jiang, and J. Ortiz, Image analysis for identifying mosquito breeding grounds, in 2016 IEEE International conference on sensing, communication and networking (SECON workshops). IEEE, 2016, pp. 16. [9] P. Mylvaganam and M. B. Dissanayake, Detection of mosquito breeding areas using semantic segmentation, in Proceedings of the 3rd International Women in Engineering Symposium 2022, 2022, pp. 3436. [10] D. T. Bravo, G. A. Lima, W. A. L. Alves, V. P. Colombo, L. Djogbenou, S. V. D. Pamboukian, C. C. Quaresma, and S. A. de Araujo, Automatic detection of potential mosquito breeding sites from aerial images acquired by unmanned aerial vehicles, Computers, Environment and Urban Systems, vol. 90, p. 101692, 2021. [11] S. P. Beeman, Synthesis of multimodal ecological model for scalable, high-resolution arboviral risk prediction in florida, Ph.D. dissertation, University of South Florida, 2021. [12] M. A. Islam, M. F. Abdullah Sayeedi, J. F. Deepti, S. R. Bappy, S. S. Islam, and F. Hafiz, Mosquitominer: light weight rover for detecting and eliminating mosquito breeding sites, in 2024 IEEE Region 10 Symposium (TENSYMP), 2024, pp. 16. [13] B. Dwyer, J. Nelson, J. Solawetz et al., Roboflow (version 1.0)[software], 2022. [14] J. Li, D. Li, C. Xiong, and S. Hoi, Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation, in International conference on machine learning. PMLR, 2022, pp. 12 88812 900."
        }
    ],
    "affiliations": [
        "BRAC University, Bangladesh",
        "United International University, Bangladesh",
        "University of Portsmouth, United Kingdom"
    ]
}