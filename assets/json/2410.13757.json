{
    "paper_title": "MobA: A Two-Level Agent System for Efficient Mobile Task Automation",
    "authors": [
        "Zichen Zhu",
        "Hao Tang",
        "Yansi Li",
        "Kunyao Lan",
        "Yixuan Jiang",
        "Hao Zhou",
        "Yixiao Wang",
        "Situo Zhang",
        "Liangtai Sun",
        "Lu Chen",
        "Kai Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Current mobile assistants are limited by dependence on system APIs or struggle with complex user instructions and diverse interfaces due to restricted comprehension and decision-making abilities. To address these challenges, we propose MobA, a novel Mobile phone Agent powered by multimodal large language models that enhances comprehension and planning capabilities through a sophisticated two-level agent architecture. The high-level Global Agent (GA) is responsible for understanding user commands, tracking history memories, and planning tasks. The low-level Local Agent (LA) predicts detailed actions in the form of function calls, guided by sub-tasks and memory from the GA. Integrating a Reflection Module allows for efficient task completion and enables the system to handle previously unseen complex tasks. MobA demonstrates significant improvements in task execution efficiency and completion rate in real-life evaluations, underscoring the potential of MLLM-empowered mobile assistants."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 7 1 ]"
        },
        {
            "title": "A\nM",
            "content": ". [ 1 7 5 7 3 1 . 0 1 4 2 : r MobA: Two-Level Agent System for Efficient Mobile Task Automation ZICHEN ZHU, HAO TANG, YANSI LI, KUNYAO LAN, YIXUAN JIANG, HAO ZHOU, YIXIAO WANG, SITUO ZHANG, LIANGTAI SUN, LU CHEN, and KAI YU, Shanghai Jiao Tong University, China Fig. 1. The Illustration of System Overview of MobA. (1) The User give command and receive the response from MobA if further instructions are required or the task is completed. (2) The Global Agent (GA) acts as the users interpreter, which comprehends the users command and decomposes the task into several easier and clearer sub-tasks through the Plan Module. (3) The Local Agent (LA) acts as the controller of the screen, decides and executes the action with the Action Module. (4) The Reflection Module verifies whether the sub-task is completed by previous execution, and it also reflects whether the sub-goal can be completed after planned and before executed by LA. (5) The Memory Module provides and updates in-context information, such as the relevant historical experience of previous commands and knowledge about the user and installed applications. (6) The optional Expert is human that initializes several basic memories for the warm startup. Current smart assistants on mobile phones are often limited by dependence on APIs of system and third-party applications. At the same time, model-based screen agents struggle with diverse interfaces and complex commands, due to restricted understanding and decision-making abilities. To address these challenges, we propose MobA, novel Mobile-phone Agent empowered by multimodal large language models that enhances comprehension and planning capabilities through sophisticated two-level agent architecture. The high-level Global Agent (GA) interprets user commands, manages history, and plans tasks, while the low-level Local Agent (LA) executes precise actions as function calls based on sub-tasks and memories from GA. By incorporating double-reflection mechanism, MobA efficiently handles tasks, even those previously unseen. MobA demonstrates significant improvements in task execution efficiency and completion rate in real-life evaluations, underscoring the potential of MLLM-empowered mobile assistants. Project Leader. Core contributors. Corresponing authors. Authors address: Zichen Zhu, JamesZhutheThird@sjtu.edu.cn; Hao Tang, tangh55@sjtu.edu.cn; Yansi Li, yansi_li@sjtu.edu.cn; Kunyao Lan, lankunyao@ sjtu.edu.cn; Yixuan Jiang, 1270645409@sjtu.edu.cn; Hao Zhou, wewea00@sjtu.edu.cn; Yixiao Wang, sjtuwangyx@sjtu.edu.cn; Situo Zhang, situozhang@ sjtu.edu.cn; Liangtai Sun, slt19990817@sjtu.edu.cn; Lu Chen, chenlusz@sjtu.edu.cn; Kai Yu, kai.yu@sjtu.edu.cn, Shanghai Jiao Tong University, Minhang District, Shanghai, China. 1 2 Zhu et al. CCS Concepts: Computing methodologies Mobile agents; Multi-agent systems; Planning for deterministic actions; Humancentered computing Interaction techniques; Usability testing. Additional Key Words and Phrases: Agent, Multimodal Large Language Model, Graphic User Interface, Mobile Device Assistant 1 INTRODUCTION Multimodal large language models (MLLMs) have experienced rapid development in recent years, driven by their training on vast amounts of multimodal data. These models [4, 5, 7, 12, 24, 25, 32, 41, 43, 6062, 69] are not only highly aligned with human preferences in understanding and generating content but also demonstrate emergent abilities such as Chain-of-Thought (CoT) reasoning [51] and In-Context Learning (ICL) [3]. Consequently, MLLMs have been widely adopted in various tasks, including structured rich-content understanding and reasoning [49, 50], multimodal recognition and content creation [6, 10, 26, 33, 53], and numerous industry applications [17, 35, 37]. The rise of MLLMs has also revolutionized the development of agent systems. Compared to traditional dialogue or control systems [19, 21], MLLM-based agents possess significantly enhanced capabilities for environment observation, task decomposition, and action decision-making. These agents are becoming increasingly prevalent across embodied robots and virtual environment manipulations. Among these, mobile phone assistants represent typical and widely used type of agent system. However, unlike agents operating in virtual or artificially constructed test environments, mobile assistants must handle operations in highly dynamic and unpredictable real-world Graphic User Interface (GUI) environments, interpreting complex user instructions, which presents several unique challenges. Firstly, mobile interfaces are often cluttered with information that varies across different operating systems, applications, and even with time. Moreover, these interfaces frequently contain task-irrelevant distractions such as pop-up notifications or ads. Secondly, similar tasks may require different execution methods (e.g., setting an alarm might involve scrolling to select or directly entering numbers). Thirdly, user instructions can be ambiguous or colloquial (e.g., \"Do need to bring an umbrella tomorrow?\" rather than \"Check the rain probability of tomorrow in the weather application.\"), making it harder for the assistant to interpret the users intent accurately. Traditional mobile assistants, which rely on slot-filling techniques and API calls, often fail to handle these complex scenarios effectively. End-to-end pre-trained models [40, 63, 70], while capable in some pre-trained cases, cannot adapt and learn from new environments dynamically. Both methodologies struggle to handle complex scenarios and lack versatility across diverse applications and tasks. In contrast, MLLMs, with strong capabilities in understanding, reasoning, decision-making, and self-reflection, present promising direction for serving as effective mobile assistants. Recent research efforts have introduced several MLLM-based agent systems that utilize the CoT capabilities of MLLMs to plan tasks, simulate actions such as screen clicks, and determine next steps based on execution results [27, 31, 46, 47, 64, 65]. These systems can operate on physical mobile devices and have shown promising results on everyday tasks, demonstrating the powerful reasoning ability and robust generalization of MLLMs. However, several limitations [31, 56] exist in these systems: (1) Agents often successfully complete the overall task but overlook or incorrectly execute finer details (e.g., setting 7:30 PM silent alarm but forgetting to mute the ringtone). (2) Agents may miss executing necessary but unstated actions based on common sense (e.g., forgetting to click \"Save\" after setting an alarm). (3) Agents may reflect that task has not been successfully executed but continue to repeat the same actions afterward, ultimately getting stuck in loop, instead of considering alternative approaches to resolve the issue. (4) Agents may not be able to extract and utilize information (e.g., checking the weather for trip requires first retrieving the destination and dates from the calendar, which the agent may forget or even skip it). MobA: Two-Level Agent System for Efficient Mobile Task Automation 3 To address these challenges, we propose MobA, an MLLM-empowered mobile phone agent system designed to enhance task execution capabilities in complex and dynamic environments. Our approach introduces several elements: Two-Level Agent Structure: Inspired by the human brains working mechanism, we adopt two-level agent architecture, with the \"cerebrum\" handling abstraction, planning, and reasoning, and the \"cerebellum\" focusing on execution. Our architecture consists of Global Agent and Local Agent. The Global Agent is responsible for high-level task planning and decomposition, while the Local Agent concentrates on selecting target actions for each sub-task. This division of responsibilities facilitates better task management, reduces the complexity of decision-making processes, and enhances overall system efficiency. Task Planning and Execution Pipeline: We develop robust task planning pipeline that includes task decomposition, feasibility assessment, action generation, and result validation. This pipeline leverages both the Global and Local Agents to break down complex tasks into manageable sub-tasks, generate appropriate actions, and verify the success of each step. The use of double-reflection mechanism not only allows the system to correct errors quickly but also helps prevent ineffective operations before execution. Multi-aspect Memory: We incorporate Memory Module that learns from historical experiences to enhance the agents adaptability and reduce redundancy. This module stores task execution traces, user preferences, application information and observation, and contextual information, which can be retrieved to inform future decisions. By learning from past successes and failures, the agent can correct and optimize its behavior over time, improving its performance in handling complex tasks. We conduct thorough evaluation on MobBench, test set with 50 real-life tasks across variety of applications and difficulty levels. MobA achieves the highest milestone score rate of 66.2%, surpassing the second-highest baseline agent system by over 17%. This sets new standard for mobile assistants by effectively integrating multimodal capabilities, task planning, reflection, and memory mechanisms. 2 RELATED WORK 2.1 LLM Agents The development of intelligent agents has been significantly influenced by the advancements in large language models (LLMs) and multimodal large language models (MLLMs). LLM-based agents leverage the autonomy, reactivity, proactiveness, and social ability of these models to perceive external environments and make decisions [55]. Emerging abilities, such as chain-of-thought (CoT) reasoning [48, 51, 68] and in-context learning (ICL) [3, 30]. Recent studies have explored LLM-based approaches for reflection [28, 39, 57, 59], planning [13, 36, 42], and memory mechanisms [15, 23, 29, 66, 67]. At the same time, the agents that utilize M/LLMs to interact with the environments are quickly developed. These agents possess significantly enhanced capabilities for environment observation, task decomposition, and action decisionmaking, which enable M/LLMs to solve complex tasks across social simulations [1, 14, 15, 34], embodied robots [54], software development [35, 37] and virtual assistants [45]. 2.2 GUI Agents 2.2.1 Traditional GUI Agents. Controlling graphical user interface (GUI) screens based on user commands is complex task that involves both GUI understanding and command interpretation. Early approaches to GUI agents focused on embedding and modular systems. For example, several agents [19, 21] combined natural language and programming 4 Zhu et al. demonstrations, allowing users to define tasks via descriptions and demonstrations. This method relied on text and image matching for script-based control of the interface. Traditional GUI agents were largely limited by their reliance on pre-defined rules and manual programming. These agents were effective within controlled environments but struggled with dynamic, real-world GUI contexts due to their lack of flexibility and adaptability. They required specific scripts or rules for each task, making them less robust when handling the diverse and evolving nature of real-world applications. 2.2.2 Advancements with Multimodal Pretrain Models. The advent of multimodal pretraining models [2, 8, 11, 18, 20, 22, 44] for GUI understanding marked significant shift in the development of GUI agents. Pretrained agents [40, 63, 70] integrated multimodal information, such as dialogue history, screenshots, and action history, through pretraining. Unlike earlier methods that relied on rigid scripts, these end-to-end models adopted more human-like approach to interacting with interfaces, enhancing their efficiency in information retrieval and task execution by mapping visual observations and text commands directly into actions. 2.2.3 MLLM-Empowered GUI Agents. The integration of MLLMs in GUI agents has introduced new opportunities to further enhance their capabilities. With the rise of larger scale models, GUI agents [16, 64, 65] began to leverage advanced reasoning and decision-making processes. These models utilized structural information provided in the view hierarchy (VH) to annotate and locate UI elements, guiding sequence of atomic actions to achieve specific goals. VH-only agents [52] depend on the structural information to reason and make decisions, which greatly lowers the cost of inference making it suitable for deployment on the device. Image-only agents [9, 46, 47, 58], which employs optical character recognition (OCR), CLIP [38] module, and object detection methods to identify operation targets. This image-only approach is particularly effective when the view hierarchy is inaccessible or noisy, but it may also encounter challenges, e.g. opening target application by clicking when names are hidden, or logos vary across screens. 3 THE MOBA SYSTEM Fig. 2. The Illustration of System Workflow of MobA. These dashed lines indicate how task is completed by decomposed into multiple sub-tasks. The system overview of MobA is shown in Figure 1, and detailed workflow is shown in Figure 2. The User gives the command and receives response from MobA if further instructions are required or the task is completed. MobA comprises several functional modules and the core part is dual agent structure. The Global Agent (GA) acts as the users interpreter, which comprehends the users command, retrieves relevant experience from the Memory Module, and resolves the task into several easier and clearer sub-tasks through the Plan Module. The Local Agent MobA: Two-Level Agent System for Efficient Mobile Task Automation (LA) acts as the controller of the screen, which receives the decomposed sub-tasks and memories from GA, decides the target action based on the current screen information and history memories, and executes the action with the Action Module. We also add Reflection Module to verify whether sub-goal can be completed before being executed by LA, and to confirm whether it is truly completed afterward. It is called the Plan Module when more fine-grained task planning is needed. The overall process is also outlined in algorithm 1. This framework allows for efficient task decomposition, action generation, and goal validation, which are critical for managing complex, dynamic environments. Input: Global Agent 洧냨洧냢, Local Agent 洧洧냢, Controller 洧냤洧노洧洧녳 Output: Completion of task 洧노洧녩洧멇롐_洧녫洧녶洧녴洧녷洧녳洧뉧롐뫯롐 false; while not 洧노洧녩洧멇롐_洧녫洧녶洧녴洧녷洧녳洧뉧롐뫯롐 do 洧녫洧녹洧 _洧노洧녩洧멇롐 洧냨洧냢.洧녮洧뉧롐뫯롏윓롐뮗롐洧냨洧녶洧녩洧녳 (); // Validate complete possibility of current goal and generate target action 洧녫洧녩洧녵_洧녬洧녶, 洧녫洧녹洧 _洧녩洧녫洧노洧녰洧녶洧녵 洧洧냢.洧녩洧녫洧노 (洧녫洧녹洧 _洧노洧녩洧멇롐) // Validate goal status and generate reflection if 洧녫洧녩洧녵_洧녬洧녶 then 洧냤洧노洧洧녳 .洧뉧롐봻롐뉧롐넗롐뮗롐뫯롐 (洧녫洧녹洧 _洧녩洧녫洧노洧녰洧녶洧녵); 洧녫洧녹洧 _洧노洧녩洧멇롐_洧녫洧녶洧녴洧녷洧녳洧뉧롐뫯롐 洧냨洧냢.洧洧 洧녭 洧녳洧뉧롐넗롐 () end // Decompose the current task into sub-tasks if not 洧녫洧녩洧녵_洧녬洧녶 or not 洧녫洧녹洧 _洧노洧녩洧멇롐_洧녫洧녶洧녴洧녷洧녳洧뉧롐뫯롐 then 洧냨洧냢.洧녷洧녳洧녩洧녵() end 洧냨洧냢.洧녹洧녷洧녬洧녩洧노洧뉧롐洧뉧롐뛿롐럻롐洧녽 (); 洧노洧녩洧멇롐_洧녫洧녶洧녴洧녷洧녳洧뉧롐뫯롐 洧냨洧냢.洧녮洧뉧롐뫯롐洧노洧녩洧노洧녹洧 (); end Algorithm 1: The Logic of Task Execution Process of MobA 3.1 Global Agent 3.1.1 Plan Module. The Plan Module is the core component of the Global Agent, responsible for decomposing the current problem into sequence of sub-tasks until it can be handled one by one with single action execution by the Local Agent. Formally, our goal is to decompose the long-term task 洧노 into sequence of sub-tasks 洧녡洧녢洧노 = [洧노1, . . . , 洧노洧녲 ], each of which can be completed by the Action Module in single step. Each task node 洧노洧녰 is initialized based on the goal description. The inputs include the task 洧노, the environment 洧, the retrieved memory 洧洧녞洧노 , and the set of executable actions A. Given the model 洧洧洧洧洧녷 , the set of sub-tasks can be determined as follows: 洧녡洧녢洧노 = 洧洧洧洧洧녷 (洧노, 洧, 洧洧녞洧노 , A) (1) Zhu et al. The retrieved memory 洧洧녞洧노 contains routes from the Route History that describe similar tasks, consisting of successfully executed Action Nodes. The route provides pathway for handling similar tasks and offers guidance on task decomposition. Furthermore, these nodes reflect the Action Modules problem-solving capabilities, allowing the model to decompose tasks with an appropriate level of granularity. Details of the retrieval methods and memory structure are provided in 3.3. The trace of each sub-task will be added to the Task History, forming tree-structured memory. Compared to most works that generate the next action based on iteration, the task decomposition method improves memory efficiency, helps LA understand task progress, optimizes collaboration between the agents, and reduces action API calls. Detailed explanations are provided in the case studies in 5. 3.1.2 Reflection Module. In the Reflection Module, the objective is to assess the tasks execution status 洧멇롐 based on the task 洧노, previous action 洧녩洧노 , environment 洧, the environment after executing the action 洧, and the retrieved memory 洧洧녤洧노 . The model used at this stage is 洧洧洧洧洧 , formulated as: 洧멇롐 = 洧洧洧洧洧 (洧노, 洧녩洧노 , 洧, 洧, 洧洧녤洧노 ) (2) If the task execution fails, 洧洧洧洧洧 also generates Execution Reflection on the current task execution. The retrieved memory includes the LAs observations and thoughts from this previous attempt, aiding 洧洧洧洧洧녷 in analyzing the reasons for the failure. The completion status and reflections are stored in the Action Node of the Task Memory, which can be retrieved later by querying the Action Node. Additionally, for tasks deemed too complex to be executed directly within the Action Module, the Thought generated by the Action Module is also stored in Memory as Plan Reflection. This reflection process is combined with the target action selection process, as they are highly related and for efficiency consideration. 3.2 Local Agent 3.2.1 Action Module. The Action Module is designed for two functions, first, it detects whether this task can be completed with one action, and later it provides the target action and invokes the controller to execute the specific action if it is feasible, otherwise, the Global Agent is called upon to further decompose the task. Formally, in the Action Module, we need to generate the CanComplete flag 洧녭洧노 {洧녢 , 洧냧 } and its corresponding action 洧녩洧노 based on the current task 洧노, environment 洧노, retrieved memory in the Action module 洧洧냢洧노 , and the provided set of executable actions A. If no such command exists, the model will return negative flag indicating that the task is too complex and requires further decomposition. Let the model used at the current stage be 洧洧洧洧洧녩, then the generation of the instruction can be expressed as: 洧녭洧노 , 洧녩洧노 = 洧洧洧洧洧녩 (洧노, 洧, 洧洧냢洧노 , A) (3) At this stage, various historical information can be incorporated as prompts. Relation-based retrieval elements like the last failed node help minimize repeated errors, while the last successful node and the parent node enhance task comprehension. Content-based retrieval from Task Memory, User Memory, and App Memory provides the agent with insights from past similar tasks. Function 洧洧洧洧洧녩 also requires generating Observation, Thought, and Message if the task includes user requests that need to be answered in natural language. This information is incorporated into the App Memory and Task Memory to enhance subsequent decision-making processes of MobA. MobA: Two-Level Agent System for Efficient Mobile Task Automation 7 Additionally, if the target action is to open specific application, another round of the target application selection process will be conducted with external knowledge of installed applications from App Memory. 3.2.2 Action List. We present the set of executable actions used in experiments in Table 1. The actions are categorized into single, combination, and system actions. We prompt LA to prioritize using combination and system actions, to improve efficiency in cases like searching items with the search bar or avoiding clicking the icon in the launcher when opening an application. All actions are designed in the format of function calls, which can accommodate additional operations or APIs. Action Type Usage Description Click single Click(element_index: int) Click by Coordinate single Click_by_Coordinate(x: double, y: double) Double Click single Double_Click(element_index: int) Long Press single Long_Press(element_index: int) Scroll single Scroll(element_index: int, direction: str, distance: str or int) Swipe single Swipe(direction: str, distance: str) Type Back single Type(text: str) single Back() Box Input combination Box_Input(element_index: int, text: str) Open App system Open_App(description: Optional[str]) Close App system Close_App(package_name: Optional[str]) Error system Failed() Finish system Finish() This function clicks the center of the UI element with the specified element index. This function simulates click at the specified and coordinates on the screen. This function double clicks the center of the UI element with the specified element index. This function long-presses the center of the UI element with the specified element index. This function swipes from the center of the UI element with the specified element index. This function swipes from the center of the screen. This function inputs text on the current input box. This function presses the back key to return to the previous screen or status. This function clicks the input box, inputs given text, and confirms it. This function locates and opens an app with short description. This function closes the specified app by its package name. This function indicates that the task cannot be completed. This function indicates that the task is completed. Table 1. Available Actions and Descriptions 3.3 Memory Module Zhu et al. We categorize Memory into three types based on the historical data generated by agents during task execution: (1) Task Memory, generated from the history of task execution. (2) User Memory, derived from the history of interactions with users. (3) App Memory, learned from the history observations and thoughts of interfaces. All kinds of memories can be initialized by human expert for warm start up. Integrating these memory types into the execution of each module helps enhance the efficiency and success rate of task execution. Task Memory captures detailed information about each task, including its execution status, environmental context, and associated sub-tasks. The design of the memory is informed by the ICE Strategy[36], organizing tasks and sub-tasks in hierarchical tree format. Task Memory includes the following data types: Task Node: Tasks are stored in tree structure. In the 洧녰-th round of the agents execution, Task Node 洧녢洧녰 is created with the goal 洧노洧녰 , where 洧노1 is the users initial requirement. If the node executes command, it becomes an Action Node 洧냢洧녰 . In cases where 洧노洧녰 cannot be achieved, child nodes are generated for the next iteration. Otherwise, the node does not split further and becomes leaf node. Route: The execution path of task. Once the task of the root node is completed, for each non-leaf node 洧노洧녰 , route 洧洧녰 = [洧냢洧녰,1, 洧냢洧녰,2, . . . , 洧냢洧녰,洧녵] is generated, where 洧냢洧녰,洧논 represents the 洧논-th successfully executed Action Node among all sub-tasks of 洧노洧녰 . Route History: The routes of all sub-tasks in each completed task are stored in the Route History. Action History: Success History stores the collection of all successfully executed Action Nodes. Failure History stores the collection of all unsuccessfully executed Action Nodes with reflections. Miscellaneous: Includes Reflection History on failed nodes, Thought History during task execution, and Response History to requests (if the goal involves providing answers). User Memory stores historical interactions between the user and MobA. It helps MobA to better grasp the users needs and infer implicit information that may not be explicitly stated in the current task. App Memory maintains information about each application. It includes both an overall description of functions and detailed observations of previously visited pages and explorations on these pages. This component aids MobA in comprehending similar pages by leveraging past experiences as well as locating target applications. The retrieval of memory information includes both relation-based and content-based methods. Relation-based retrieval encompasses relationally close information. Specifically, the last Action Node and the task description of the parent node will be retrieved in the Action Module, which helps the agent to better understand the current task and its stage. Content-based retrieval employs cosine similarity of keys, selecting memories with high similarity. For example, the Task History is retrieved based on the cosine similarity of the task descriptions in the Plan Module, providing the agent with experiences from similar historical tasks. The two approaches can also be combined through weighted retrieval by setting relation distance and content distance, along with their corresponding weights. Finally, the results are ranked based on the aggregated outcome. 3.4 GUI Interface We also build an installable application to provide user-friendly experience. An ASR module will transcript the users voice commands into text, and TTS module will read out the response from MobA. With the wireless ADB, the user is not required to type the command in the computer terminal directly nor connect to the computer through USB cable. MobA: Two-Level Agent System for Efficient Mobile Task Automation 9 4 EXPERIMENTS To get thorough comparison between MobA and other GUI agents about the capability to handle complex user instructions and execute GUI interactions on mobile devices, we conduct the evaluation on real-life scenario test set naming MobBench. 4.1 The MobBench Test set The MobBench comprises diverse test set of 50 tasks designed to evaluate the performance of MobA in real-world mobile application scenarios. The test set includes 10 applications widely used in China, each with four tasks of varying difficulty: Easy, Medium, Hard, and Indirect Comprehension, totaling 40 tasks. Easy, Medium, and Hard tasks are categorized by the complexity and steps required to complete. Indirect Comprehension is designed for common cases where the user gives vague instruction without detailing which application or specific steps are required. The agent is expected to decide target application and find an effective approach. Additionally, there are 10 Cross-Application tasks, which involve interacting with two applications and are more close to Hard level in difficulty. These tasks focus on evaluating the ability of information extraction and retrieval, as well as the awareness of sub-goal completion and application switching. Compared with several similar task sets mentioned in other papers [16, 46, 47, 64, 65], which only get score when it finishes the task completely, we assign several milestone scores for sub-tasks in MobBench. This allows for more precise process assessment, in the cases where the task is partially finished. We also include detailed preparation instruction for tasks when more justice and stable start is needed. Three human operators manually performed these tasks on three different mobile phones, recording their execution steps. The actions considered as step include interactions such as clicks, swipes, opening or closing an app, dismissing non-skippable ads, typing text, or replying to the user. The average number of steps is considered as the human expert baseline. Table 2 provides five examples of the tasks included in MobBench. 4.2 Metrics Three metrics are designed to better compare the capability of GUI agents thoroughly. Milestone Score (MS): Scoring milestones are assigned to several sub-tasks, evenly distributed during the task completion process. Since each task contains 1 to 6 milestones, the agent will get score as it reaches each milestone. We sum up all milestone scores of 50 tasks as the primary metric. Complete Rate (CR): If the agent gets all milestone scores in one task, it is considered as task complete. This is the most common and straightforward metric for GUI agent evaluation. Execution Efficiency (EE): We record the effective number of steps for each task and the corresponding milestone scores, that is, the total number of steps executed at the time of getting the last effective milestone score, and calculate the average number of steps required to obtain each effective milestone score. The lower this number, the more efficient the execution; the higher it is, the more it includes ineffective actions. The average milestone scores and execution steps for each task type are summarized in Table 3. 10 Zhu et al. Table 2. Several example tasks in MobBench. The content is translated from Chinese. The complete table can be found in Appendix A. Type Application Task Preparation Scoring Milestones Steps Easy McDonalds Switch the language of the McDonalds app to English. Switch to Chinese. 1. Task completion. 6.7 Medium (China Railway) Check the schedule for train G104 from Shanghai to Beijing tomorrow, and find out what time it is expected to arrive in Nanjing. - 1. Enter the timetable screen, 2. Correct train number, 3. Task completion. Hard Douban Search for the movie \"The Shawshank Redemption\" on Douban, mark it as \"watched\", rate it five stars, and leave positive review. Remove the previous mark, rating, and review of this movie. 1. Correct movie, 2. Correct mark, 3. Correct rating, 4. Positive review. Indirect BiliBili If Im out of mobile data, what videos can still watch on the phone? Download several videos in advance. 1. Open BiliBili, 2. Check downloads. Cross-APP JD.com, WeChat Share the product link of the most recent JD.com order with WeChat friend, and write recommendation message. There is an existing order. 1. Enter the order list, 2. Correct order, 3. Suitable message, 4. Task completion. 11.7 9.7 3.3 10.3 Table 3. Average scores and expert execution steps for different task types of MobBench. Task Type # Tasks Avg. MS Avg. Steps per Task Avg. Steps per MS (EE) Easy Medium Hard Indirect Cross-App Overall 10 10 10 10 10 1.0 2.2 4.1 2.8 3.1 2.7 4.3 7.3 15.2 9.4 10.8 9.4 4.30 3.32 3.71 3.36 3.48 3. 4.3 Setups To provide comprehensive evaluation, MobA is compared against several baselines that illustrate spectrum of capabilities, from basic manual operations to several sophisticated agent-based automation. Human Baseline as mentioned in 4.1 are considered as the optimal solution for each task. GPT-4o + Human Baseline utilizes an iterative process where the GPT model [32] provides guidance for manual task execution using screenshots. Each task involves presenting task and screenshot to the model, which then suggests the target actions. Human operators interpret the suggestions and execute these actions on the device, continuing until the task is completed or stopping criterion is met. AppAgent [65] uses both view hierarchy and screenshot for planning and choosing target actions. All interactive elements are marked with bounding boxes and unique index for better grounding performance. Mobile Agent (v2) [46, 47] uses only visual information from screenshots when making decisions. Target elements are selected with the guidance of OCR and CLIP [38] modules. MobA is evaluated under several settings by disabling the Memory Module or/and Plan Module to assess its performance and the impact of these two modules. We disable the Plan Module by replacing the Global Agent with MobA: Two-Level Agent System for Efficient Mobile Task Automation 11 plain agent, and no sub-tasks are provided to the action and Reflection Module. We disable the Memory Module by removing all in-context examples and historical experience information (including observations, thoughts, previous actions, and their execution status), focusing on assessing the core capability in real-time zero-shot task execution without relying on historical data. All experiments are conducted with gpt-4o-2024-05-13 model version to ensure consistent model capabilities across tests. The primary evaluation metric is the first attempt complete rate, directly measuring the effectiveness of each system in completing tasks on the first try without retries. 4.4 Results and Analysis The overall experiment results are as listed in Table 4. And for more detailed results categorized by task type please refer to Figure 3. Table 4. Overall Performance on MobBench. Model Human GPT-4o + Human AppAgent[65] MobileAgent (v2)[46] MobA w/o Memory and Plan MobA w/o Plan MobA w/o Memory MobA CR 50/50 49/50 6/50 17/ 13/50 15/50 22/50 28/50 MS EE 133 130 (97.7%) 3.56 3.82 (107.2%) 35 (28.6%) 63 (48.9%) 52 (39.1%) 65 (48.9%) 72 (54.1%) 88 (66.2%) 4.43 (124.4%) 4.84 (136.0%) 4.42 (124.2%) 4.17 (117.1%) 3.81 (106.9%) 3.44 (96.7%) Fig. 3. Performance on MobBench Categorized by Task Type. While the performance of all models is relatively similar on simpler tasks, MobA demonstrates superior results in more challenging tasks, outperforming other models except for Human and GPT-4o + Human. This suggests that MobA is more efficient in handling complex cases. Additionally, the incorporation of both the Memory Module and Plan Module enhances performance, highlighting their respective contributions to the systems overall capability. 12 Zhu et al. The upper part of Table 4 shows the performance of four baselines. Due to the complexity of mobile interfaces and the technical limitations encountered during task execution, the overall task completion rates (Complete Rate, CR) are relatively low for all agents. Consequently, the Milestone Score (MS) serves as finer metric to more accurately reflect the performance of each agent by considering partial task completion. While there are notable differences in Milestone Scores among the baseline models, the gap in Execution Efficiency (EE) is less significant. This is because most agents can smoothly complete simpler sub-goals, whereas for more complex sub-goals, the agents either complete them or fail entirely, resulting in closer performance regarding execution efficiency. 4.4.1 Human is more adaptive and robust to screen interactions. While the human baseline is considered the optimal solution for each task, the GPT-4o + Human method achieves performance very close to that of human operators on all metrics. In the evaluation of GPT-4o + Human, the agent only provides textual task descriptions and an initial screenshot, and the GPT-4o generates detailed step-by-step instructions, which are then executed manually by human operator. The eye-catching performance of GPT-4o + Human can be attributed to several factors: (1) relatively lenient standard in task execution, allowing human operators to interpret GPT-4os general instructions flexibly; (2) human operators automatically completing tasks such as OCR, target detection, and localization, ensuring more precise actions; (3) GPT-4o provides global plan, avoiding redundant or missed steps; (4) technical issues (e.g., inability to retrieve XML files or missing information in the files) do not affect task completion. 4.4.2 Performance Comparison. The performance of MobileAgent is notably higher than that of AppAgent. This improvement is mainly due to the inclusion of both Memory and Reflection modules in MobileAgent, which enhance reasoning capacity and utilize more computational resources, such as tokens. Additionally, MobileAgent keeps record of all historical actions, allowing it to learn from the entire sequence of operations, whereas AppAgent can only track the most recent action. Furthermore, MobileAgent relies on OCR and CLIP modules for target localization, offering greater flexibility and avoiding the technical limitations that AppAgent faces when dependent on XML files. By adopting twice-reflection strategy, the ineffective execution steps are slightly reduced, where the sub-tasks that are not able to be completed with single action are decomposed finer before executed. This gives clearer guidance for the Local Agent to decide the target actions. 4.4.3 Ablation Study. The lower part of Table 4 presents the results of the ablation study, where we experimented with four different configurations by selectively enabling or disabling the Memory and Plan modules. The results indicate that incorporating both Memory and Plan modules significantly enhances the agents overall performance. The Plan module alone shows much stronger effect than the Memory module alone, validating one of the core contributions of this paperthe effectiveness of task decomposition planning. By decomposing tasks into manageable sub-tasks, MobA can perform global planning, avoid redundant actions, and minimize overlooked details, effectively managing its historical actions (since in tree-structured task, previously completed sub-tasks are inherently tracked). Unlike MobileAgent, which focuses solely on the next specific action, MobA first determines the next abstract task and then plans the specific execution steps, closely mirroring human reasoning patterns and providing more structured approach. When the Memory module is introduced, MobAs performance further improves, particularly in cross-application tasks (see Figure 3 (b)). This enhancement is due to the Memory modules ability to retain crucial information over longer periods, such as \"the day am traveling to Shenzhen\", allowing it to reference previous screens key MobA: Two-Level Agent System for Efficient Mobile Task Automation 13 content. In contrast, without the Memory module, the agent is limited to short-term memory of only the current and the immediately preceding steps, resulting in less effective task execution. 4.4.4 Take-home Messages. Combining MLLM-driven planning with human flexibility and adaptive decision-making and target-locating can bridge gaps in current MLLM capabilities, leading to near-optimal performance in complex, dynamic tasks. Integrating comprehensive planning, memory, and reflection capabilities in task-oriented agents significantly enhances their ability to handle complex tasks by optimizing decision-making processes and learning from past actions. Effective task decomposition and planning strategies are crucial for improving agent performance, as they enable structured execution and minimize redundant actions. Cooperating long-term memory capabilities further strengthens the agents ability to manage complex, crossapplication tasks by retaining critical contextual information. Combining reflections before and after action execution helps reduce ineffective moves. 5 CASE STUDY In this section, we present two case studies to showcase several key features of MobA. 5.1 Case Study MobA demonstrates exceptional proficiency in handling tasks that involve intricate details and multiple sequential steps. This case study focuses on representative scenario involving clock application, as shown in Figure 4. The objective is to Create 6:30 PM alarm with the title Work and use vibration alerts from each Monday to Thursday. The process begins with MobA (1) assessing the task feasibility, i.e. whether it can be completed in single step, as indicated by the CanComplete status. If the task cannot be accomplished in one step, MobA (2) decomposes it into smaller, manageable sub-tasks in the Plan Module, as listed in Subgoals. This systematic decomposition ensures that each part of the original command is executed with precision. Following this, MobA (3) carries out the first action to solve its first sub-task and (4) reviews and reflects the task completion status indicated by Subgoal_Status and Goal_Status. Once sub-task is completed, as the output Subgoal_Status is set to True, indicating successful sub-task progress, and MobA will move to the following sub-task if the sub-task sequence is not empty. For sub-task 2 MobA further divides it into two distinct steps of adjusting the hour number and minute number separately. These two new sub-tasks are also added to the sub-task sequence. For most agents, this way of thinking might be too cumbersome for they may just adopt simpler approach: adjusting only the hour number to 18 while ignoring the minute. By breaking down the task, we adopt Chain-of-Thought (CoT)-like approach that explicitly structures the reasoning process of MobA. For instance, sub-task 2.2 can be further decomposed into several times of clicking if required. In sub-task 4, MobA mis-comprehended the description and selected only Monday and Thursday. During the reflection process, it detects that sub-task 4 has not been completed. Therefore the Plan Module will be invoked to add two more sub-tasks select Tuesday and select Wednesday. In conclusion, the task decomposition approach invoked by the assessment before action execution and reflection after ensures the accurate execution of series of detailed steps. The tree-structured task decomposition strategy not 14 Zhu et al. Fig. 4. The First Example Case of MobA. MobA decompose the task into several sub-tasks and solve them in sequence. Please note that several unimportant stages during the execution of sub-task are omitted for clarity. The key features for each part are as follows. Task: Decompose task. Sub-task 1: Execute sub-task and reflect. Sub-task 2: Re-split sub-task invoked by feasibility assessment. Sub-task 3: Execute sub-task. Sub-task 4: Re-split sub-task invoked by result reflection. only allows for sequential and orderly completion of the task but also minimizes the risk of overlooking critical details and ineffective attempts. 5.2 Case Study Figure 5 demonstrates how the reflection and Memory Modules support task execution in MobA. The command is Help me check when will reach the travel destination tomorrow. MobA can accurately interpret user intent from this command and give decomposed sub-tasks. For sub-task 1, MobA retrieves relevant details from User Memory (e.g., frequent use of Google Calendar), extracts key information (train schedule and destination), and stores it in Task Memory. Leveraging the information stored in App Memory, MobA is able to locate the next application (China Railway), due to its high relevance to train travel. When encountering failures, MobA uses historical experiences to reflect and adapt. During sub-task 3, when MobA initially failed to input the train number using the Box_Input function, it reflects on its previous operations and employs character-by-character input method, successfully completing the task. MobA: Two-Level Agent System for Efficient Mobile Task Automation 15 Fig. 5. The Second Example Case of MobA. Please note that several unimportant stages during the execution of sub-task are omitted for clarity. The key features for each part are as follows. Task: MobA supports cross-application tasks and can interpret indirect commands. Sub-task 1: Memories are retrieved to select target applications and updated to track the trace. Sub-task 3: MobA will reflect and try other approaches if the attempt is failed. Sub-task 9 and sub-task 13: Memories are used to choose correct actions. Additionally, memory retrieval is crucial for handling contextual tasks. In sub-tasks 9 and 13, although the user doesnt explicitly specify the travel date or destination in the task request. MobA can rely on previously stored Task Memory data to provide an accurate response. In conclusion, by integrating reflection, memory retrieval, and seamless cross-app interaction, the agent efficiently handles complex tasks, enhancing user experience and broadening its potential applications. 6 LIMITATIONS AND FUTURE WORK Task Decomposition and Failure Handling. The effectiveness of MobA largely depends on accurate initial task decomposition. Incorrect decomposition, such as creating sub-tasks that cannot be completed, leads directly to task failure. Currently, while we invoke task decomposition upon failure, there is no mechanism for automatically correcting the initial task plan. If sub-task cannot be further decomposed or completed, it may lead to cyclical failure. Moreover, sub-tasks often depend on each other; for example, subsequent actions on screen can only be executed after completing information retrieval on screen A. If the initial plan wrongly orders the tasks as [B, A], the tasks will not be completed successfully. Although rare in practical tests, such decomposition errors pose significant challenges. Dependence on View-Hierarchy Information. MobA relies view hierarchy (VH) information from XML files to understand mobile interfaces and locate target elements, which assumes the accuracy of these files. However, discrepancies Zhu et al. between XML content and actual screen elements can occur, such as unclickable items being mislabeled as clickable. While MobA prefers relying on VH over visual reasoning, technical limitations (e.g. the page has changing banners or videos) sometimes prevent retrieving XML files, forcing reliance on visual information like OCR and object detection. To address this, integrating fine-tuned side model with robust OCR and target localization capabilities as local agent could enhance flexibility in task execution. Execution Model Efficiency. The plan-assess-action-reflection execution pipeline, while thorough, often necessitates frequent calls to MLLMs, increasing both time costs and token consumption. To optimize this process, employing finely-tuned side models for specific actions or utilizing an experience replay mechanism to reuse successful execution trajectories could reduce redundant computations and improve overall efficiency. To overcome these limitations, future work could focus on: Developing adaptive task decomposition algorithms that can dynamically adjust plans based on real-time feedback and task progress. Enhancing the robustness of the system against XML inaccuracies by integrating more advanced visual reasoning capabilities. Reducing dependence on large models by incorporating more efficient local processing units or lightweight models that can handle specific sub-tasks. Exploring machine learning techniques such as reinforcement learning to enable MobA to learn from past successes and failures inherently and improve over time. These enhancements will aim to make MobA more reliable, efficient, and capable of handling broader range of tasks with higher complexity and variability. 7 CONCLUSION This paper presented MobA, an innovative Mobile phone Assistant system empowered by MLLMs. Utilizing twolevel agent structure, comprising Global Agent and Local Agent, MobA effectively understands user commands, plans tasks, and executes actions. The combination of Memory and Plan Modules enhances its ability to learn from previous interactions, improving efficiency and accuracy. Our evaluations demonstrated that MobA surpasses existing mobile assistants in handling complex tasks, leveraging multi-level memory, task decomposition, and action-validation mechanisms. These features enable precise task execution even with intricate or indirect commands. Future work will focus on improving the performance on image-only scenarios where the view hierarchy is unattainable, deploying an end-side model on mobile phones for faster response and secured privacy. We hope MobA illustrates the potential of MLLMs-empowered mobile assistants and provides valuable insights for future works. MobA: Two-Level Agent System for Efficient Mobile Task Automation 17 REFERENCES [1] Gati Aher, Rosa I. Arriaga, and Adam Tauman Kalai. 2023. Using large language models to simulate multiple humans and replicate human subject studies. In Proceedings of the 40th International Conference on Machine Learning (Honolulu, Hawaii, USA) (ICML23). JMLR.org, Article 17, 35 pages. [2] Chongyang Bai, Xiaoxue Zang, Ying Xu, Srinivas Sunkara, Abhinav Rastogi, Jindong Chen, and Blaise Aguera Arcas. 2021. UIBert: Learning Generic Multimodal Representations for UI Understanding. arXiv:2107.13731 [cs.CV] https://arxiv.org/abs/2107. [3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 18771901. https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. 2023. MiniGPT-v2: large language model as unified interface for vision-language multi-task learning. arXiv:2310.09478 [cs.CV] https://arxiv.org/abs/2310.09478 [4] [5] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. 2024. InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks. arXiv:2312.14238 [cs.CV] https://arxiv.org/abs/2312.14238 [6] Zhanpeng Chen, Chengjin Xu, Yiyan Qi, and Jian Guo. 2024. MLLM Is Strong Reranker: Advancing Multimodal Retrieval-augmented Generation via Knowledge-enhanced Reranking and Noise-injected Training. arXiv:2407.21439 [cs.AI] https://arxiv.org/abs/2407.21439 [7] Wenliang Dai, Junnan Li, DONGXU LI, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. 2023. InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning. In Advances in Neural Information Processing Systems, A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (Eds.), Vol. 36. Curran Associates, Inc., 4925049267. https://proceedings.neurips.cc/ paper_files/paper/2023/file/9a6a435e75419a836fe47ab6793623e6-Paper-Conference.pdf Jingwen Fu, Xiaoyi Zhang, Yuwang Wang, Wenjun Zeng, and Nanning Zheng. 2024. Understanding mobile GUI: From pixel-words to screen-sentences. Neurocomputing 601 (2024), 128200. [8] [9] Difei Gao, Lei Ji, Zechen Bai, Mingyu Ouyang, Peiran Li, Dongxing Mao, Qinchen Wu, Weichen Zhang, Peiyi Wang, Xiangwu Guo, et al. 2024. AssistGUI: Task-Oriented PC Graphical User Interface Automation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1328913298. [10] Yuying Ge, Sijie Zhao, Chen Li, Yixiao Ge, and Ying Shan. 2024. SEED-Data-Edit Technical Report: Hybrid Dataset for Instructional Image Editing. arXiv:2405.04007 [cs.CV] https://arxiv.org/abs/2405.04007 [11] Zecheng He, Srinivas Sunkara, Xiaoxue Zang, Ying Xu, Lijuan Liu, Nevan Wichers, Gabriel Schubiner, Ruby Lee, and Jindong Chen. 2021. Actionbert: Leveraging user actions for semantic understanding of user interfaces. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35. 59315938. Jinyi Hu, Yuan Yao, Chongyi Wang, SHAN WANG, Yinxu Pan, Qianyu Chen, Tianyu Yu, Hanghao Wu, Yue Zhao, Haoye Zhang, Xu Han, Yankai Lin, Jiao Xue, dahai li, Zhiyuan Liu, and Maosong Sun. 2024. Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=Kuh5qgCGCp [12] [13] Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and Enhong Chen. 2024. Understanding the planning of LLM agents: survey. arXiv:2402.02716 [cs.AI] https://arxiv.org/abs/2402.02716 [14] Eunkyung Jo, Daniel A. Epstein, Hyunhoon Jung, and Young-Ho Kim. 2023. Understanding the Benefits and Challenges of Deploying Conversational AI Leveraging Large Language Models for Public Health Intervention. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (Hamburg, Germany) (CHI 23). Association for Computing Machinery, New York, NY, USA, Article 18, 16 pages. https://doi.org/10.1145/ 3544548.3581503 [15] Kunyao Lan, Bingui Jin, Zichen Zhu, Siyuan Chen, Shu Zhang, Kenny Q. Zhu, and Mengyue Wu. 2024. Depression Diagnosis Dialogue Simulation: Self-improving Psychiatrist with Tertiary Memory. arXiv:2409.15084 [cs.CL] https://arxiv.org/abs/2409.15084 [16] Sunjae Lee, Junyoung Choi, Jungjae Lee, Munim Hasan Wasi, Hojun Choi, Steven Y. Ko, Sangeun Oh, and Insik Shin. 2024. Explore, Select, Derive, and Recall: Augmenting LLM with Human-like Memory for Mobile Task Automation. arXiv:2312.03003 [cs.HC] https://arxiv.org/abs/2312.03003 [17] Unggi Lee, Minji Jeon, Yunseo Lee, Gyuri Byun, Yoorim Son, Jaeyoon Shin, Hongkyu Ko, and Hyeoncheol Kim. 2024. LLaVA-Docent: Instruction Tuning with Multimodal Large Language Model to Support Art Appreciation Education. arXiv:2402.06264 [cs.AI] https://arxiv.org/abs/2402.06264 [18] Gang Li and Yang Li. 2023. Spotlight: Mobile UI Understanding using Vision-Language Models with Focus. In The Eleventh International Conference on Learning Representations. [19] Toby Jia-Jun Li, Amos Azaria, and Brad A. Myers. 2017. SUGILITE: Creating Multimodal Smartphone Automation by Demonstration. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (Denver, Colorado, USA) (CHI 17). Association for Computing Machinery, New York, NY, USA, 60386049. https://doi.org/10.1145/3025453.3025483 18 Zhu et al. [20] Toby Jia-Jun Li, Lindsay Popowski, Tom Mitchell, and Brad Myers. 2021. Screen2vec: Semantic embedding of gui screens and gui components. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 115. [21] Toby Jia-Jun Li, Marissa Radensky, Justin Jia, Kirielle Singarajah, Tom Mitchell, and Brad Myers. 2019. Pumice: multi-modal agent that learns concepts and conditionals from natural language and demonstrations. In Proceedings of the 32nd annual ACM symposium on user interface software and technology. 577589. [22] Yang Li, Gang Li, Xin Zhou, Mostafa Dehghani, and Alexey Gritsenko. 2021. VUT: Versatile UI Transformer for Multi-Modal Multi-Task User Interface Modeling. arXiv:2112.05692 [cs.CV] https://arxiv.org/abs/2112.05692 [23] Yang Li, Yangyang Yu, Haohang Li, Zhi Chen, and Khaldoun Khashanah. 2023. TradingGPT: Multi-Agent System with Layered Memory and Distinct Characters for Enhanced Financial Trading Performance. arXiv:2309.03736 [q-fin.PM] https://arxiv.org/abs/2309.03736 [24] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2024. Improved Baselines with Visual Instruction Tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2629626306. [25] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual Instruction Tuning. In Advances in Neural Information Processing Systems, A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (Eds.), Vol. 36. Curran Associates, Inc., 3489234916. https: //proceedings.neurips.cc/paper_files/paper/2023/file/6dcf277ea32ce3288914faf369fe6de0-Paper-Conference.pdf [26] Xinyu Liu, Yingqing He, Lanqing Guo, Xiang Li, Bu Jin, Peng Li, Yan Li, Chi-Min Chan, Qifeng Chen, Wei Xue, Wenhan Luo, Qifeng Liu, and arXiv:2409.02919 [cs.CV] https: Yike Guo. 2024. HiPrompt: Tuning-free Higher-Resolution Generation with Hierarchical MLLM Prompts. //arxiv.org/abs/2409.02919 [27] Xinbei Ma, Zhuosheng Zhang, and Hai Zhao. 2024. CoCo-Agent: Comprehensive Cognitive MLLM Agent for Smartphone GUI Automation. In Findings of the Association for Computational Linguistics ACL 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand and virtual meeting, 90979110. https://aclanthology.org/2024.findings-acl.539 [28] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-Refine: Iterative Refinement with Self-Feedback. arXiv:2303.17651 [cs.CL] [29] Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, and Yuwei Fang. 2024. Evaluating Very Long-Term Conversational Memory of LLM Agents. arXiv:2402.17753 [cs.CL] https://arxiv.org/abs/2402.17753 [30] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, Abu Dhabi, United Arab Emirates, 1104811064. https://doi.org/10.18653/v1/2022.emnlp-main.759 [31] Songqin Nong, Jiali Zhu, Rui Wu, Jiongchao Jin, Shuo Shan, Xiutian Huang, and Wenhao Xu. 2024. MobileFlow: Multimodal LLM For Mobile GUI Agent. arXiv:2407.04346 [cs.CV] https://arxiv.org/abs/2407.04346 [32] OpenAI. 2023. Gpt-4v(ision) System Card. https://openai.com/research/gpt-4v-system-card. [33] Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng, Wenhu Chen, and Furu Wei. 2024. Kosmos-G: Generating Images in Context with Multimodal Large Language Models. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=he6mX9LTyE Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. 2023. Generative Agents: Interactive Simulacra of Human Behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology (San Francisco, CA, USA) (UIST 23). Association for Computing Machinery, New York, NY, USA, Article 2, 22 pages. https://doi.org/10.1145/3586183. [34] [35] Chen Qian, Yufan Dang, Jiahao Li, Wei Liu, Zihao Xie, YiFei Wang, Weize Chen, Cheng Yang, Xin Cong, Xiaoyin Che, Zhiyuan Liu, and Maosong Sun. 2024. Experiential Co-Learning of Software-Developing Agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 56285640. https://aclanthology.org/2024.acl-long.305 [36] Cheng Qian, Shihao Liang, Yujia Qin, Yining Ye, Xin Cong, Yankai Lin, Yesai Wu, Zhiyuan Liu, and Maosong Sun. 2024. Investigate-ConsolidateExploit: General Strategy for Inter-Task Agent Self-Evolution. arXiv:2401.13996 [cs.CL] https://arxiv.org/abs/2401.13996 [37] Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2024. ChatDev: Communicative Agents for Software Development. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 1517415186. https://aclanthology.org/2024.acl-long.810 [38] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models From Natural Language Supervision. In Proceedings of the 38th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 139), Marina Meila and Tong Zhang (Eds.). PMLR, 87488763. https://proceedings.mlr.press/v139/radford21a.html [39] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: language agents with verbal reinforcement learning. In Advances in Neural Information Processing Systems, A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (Eds.), Vol. 36. Curran Associates, Inc., 86348652. https://proceedings.neurips.cc/paper_files/paper/2023/file/1b44b878bb782e6954cd888628510e90-PaperConference.pdf MobA: Two-Level Agent System for Efficient Mobile Task Automation 19 [40] Liangtai Sun, Xingyu Chen, Lu Chen, Tianle Dai, Zichen Zhu, and Kai Yu. 2022. META-GUI: Towards Multi-modal Conversational Agents on Mobile GUI. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, Abu Dhabi, United Arab Emirates, 66996712. https://doi.org/10.18653/v1/2022.emnlpmain.449 [41] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. 2024. Generative Multimodal Models are In-Context Learners. arXiv:2312.13286 [cs.CV] https://arxiv.org/abs/2312.13286 [42] Simeng Sun, Yang Liu, Shuohang Wang, Dan Iter, Chenguang Zhu, and Mohit Iyyer. 2024. PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), Yvette Graham and Matthew Purver (Eds.). Association for Computational Linguistics, St. Julians, Malta, 469486. https://aclanthology.org/2024.eacl-long.29 [43] Gemini Team. 2024. Gemini: Family of Highly Capable Multimodal Models. arXiv:2312.11805 [cs.CL] https://arxiv.org/abs/2312.11805 [44] Bryan Wang, Gang Li, Xin Zhou, Zhourong Chen, Tovi Grossman, and Yang Li. 2021. Screen2Words: Automatic Mobile UI Summarization with Multimodal Learning. In The 34th Annual ACM Symposium on User Interface Software and Technology (Virtual Event, USA) (UIST 21). Association for Computing Machinery, New York, NY, USA, 498510. https://doi.org/10.1145/3472749. [45] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023. Voyager: An [46] [47] Open-Ended Embodied Agent with Large Language Models. arXiv:2305.16291 [cs.AI] https://arxiv.org/abs/2305.16291 Junyang Wang, Haiyang Xu, Haitao Jia, Xi Zhang, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. 2024. Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration. (2024). arXiv:2406.01014 [cs.CL] https://arxiv.org/abs/2406.01014 Junyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. 2024. Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception. arXiv:2401.16158 [cs.CL] https://arxiv.org/abs/2401.16158 [48] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. SelfConsistency Improves Chain of Thought Reasoning in Language Models. In The Eleventh International Conference on Learning Representations. https://openreview.net/forum?id=1PL1NIMMrw [49] Yiqi Wang, Wentao Chen, Xiaotian Han, Xudong Lin, Haiteng Zhao, Yongfei Liu, Bohan Zhai, Jianbo Yuan, Quanzeng You, and Hongxia Yang. 2024. Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): Comprehensive Survey on Emerging Trends in Multimodal Reasoning. arXiv:2401.06805 [cs.CL] https://arxiv.org/abs/2401. [50] Yuqing Wang and Yun Zhao. 2023. Gemini in Reasoning: Unveiling Commonsense in Multimodal Large Language Models. arXiv:2312.17661 [cs.CL] [51] https://arxiv.org/abs/2312.17661 Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. In Advances in Neural Information Processing Systems, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates, Inc., 2482424837. https://proceedings.neurips.cc/paper_files/paper/2022/file/ 9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf [52] Hao Wen, Yuanchun Li, Guohong Liu, Shanhui Zhao, Tao Yu, Toby Jia-Jun Li, Shiqi Jiang, Yunhao Liu, Yaqin Zhang, and Yunxin Liu. 2024. AutoDroid: LLM-powered Task Automation in Android. In Proceedings of the 30th Annual International Conference on Mobile Computing and Networking (Washington D.C., DC, USA) (ACM MobiCom 24). Association for Computing Machinery, New York, NY, USA, 543557. https: //doi.org/10.1145/3636534.3649379 Jiannan Wu, Muyan Zhong, Sen Xing, Zeqiang Lai, Zhaoyang Liu, Wenhai Wang, Zhe Chen, Xizhou Zhu, Lewei Lu, Tong Lu, Ping Luo, Yu Qiao, and Jifeng Dai. 2024. VisionLLM v2: An End-to-End Generalist Multimodal Large Language Model for Hundreds of Vision-Language Tasks. arXiv:2406.08394 [cs.CV] https://arxiv.org/abs/2406.08394 [53] [54] Zhenyu Wu, Ziwei Wang, Xiuwei Xu, Jiwen Lu, and Haibin Yan. 2023. Embodied Task Planning with Large Language Models. arXiv:2307.01848 [cs.CV] https://arxiv.org/abs/2307.01848 [55] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang, and Tao Gui. 2023. The Rise and Potential of Large Language Model Based Agents: Survey. arXiv:2309.07864 [cs.AI] https://arxiv.org/abs/2309.07864 [56] Mingzhe Xing, Rongkai Zhang, Hui Xue, Qi Chen, Fan Yang, and Zhen Xiao. 2024. Understanding the Weakness of Large Language Model Agents within Complex Android Environment. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (Barcelona, Spain) (KDD 24). Association for Computing Machinery, New York, NY, USA, 60616072. https://doi.org/10.1145/3637528.3671650 [57] Hongshen Xu, Zichen Zhu, Situo Zhang, Da Ma, Shuai Fan, Lu Chen, and Kai Yu. 2024. Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions Using RL from Knowledge Feedback. arXiv:2403.18349 [cs.CL] https://arxiv.org/abs/2403.18349 [58] An Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin, Linjie Li, Jianfeng Wang, Jianwei Yang, Yiwu Zhong, Julian McAuley, Jianfeng Gao, Zicheng Liu, and Lijuan Wang. 2023. GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone GUI Navigation. arXiv:2311.07562 [cs.CV] https://arxiv.org/abs/2311. [59] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. ReAct: Synergizing Reasoning and Acting in Language Models. In The Eleventh International Conference on Learning Representations. https://openreview.net/forum?id=WE_vluYUL-X 20 Zhu et al. [60] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, Qianyu Chen, Huarong Zhou, Zhensheng Zou, Haoye Zhang, Shengding Hu, Zhi Zheng, Jie Zhou, Jie Cai, Xu Han, Guoyang Zeng, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2024. MiniCPM-V: GPT-4V Level MLLM on Your Phone. arXiv:2408.01800 [cs.CV] https://arxiv.org/abs/2408.01800 [61] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. 2024. mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality. arXiv:2304.14178 [cs.CL] https://arxiv.org/abs/2304. [62] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. 2023. mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration. arXiv:2311.04257 [cs.CL] https://arxiv.org/abs/2311.04257 [63] Zhuosheng Zhan and Aston Zhang. 2023. You Only Look at Screens: Multimodal Chain-of-Action Agents. arXiv preprint arXiv:2309.11436 (2023). [64] Chaoyun Zhang, Liqun Li, Shilin He, Xu Zhang, Bo Qiao, Si Qin, Minghua Ma, Yu Kang, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, and Qi Zhang. 2024. UFO: UI-Focused Agent for Windows OS Interaction. arXiv:2402.07939 [cs.HC] https://arxiv.org/abs/2402.07939 [65] Chi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. 2023. AppAgent: Multimodal Agents as Smartphone Users. arXiv:2312.13771 [cs.CV] https://arxiv.org/abs/2312.13771 [66] Shaokun Zhang, Jieyu Zhang, Jiale Liu, Linxin Song, Chi Wang, Ranjay Krishna, and Qingyun Wu. 2024. Training Language Model Agents without Modifying Language Models. ICML24 (2024). [67] Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and Ji-Rong Wen. 2024. Survey on the Memory Mechanism of Large Language Model based Agents. arXiv:2404.13501 [cs.AI] https://arxiv.org/abs/2404.13501 [68] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2023. Automatic Chain of Thought Prompting in Large Language Models. In The Eleventh International Conference on Learning Representations. https://openreview.net/forum?id=5NTt8GFjUHkr [69] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2024. MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id= 1tZbq88f [70] Zichen Zhu, Liangtai Sun, Jingkai Yang, Yifan Peng, Weilin Zou, Ziyuan Li, Wutao Li, Lu Chen, Yingzi Ma, Danyang Zhang, et al. 2023. CAM-GUI: Conversational Assistant on Mobile GUI. In National Conference on Man-Machine Speech Communication. Springer, 302315. Easy Easy Easy Easy Easy Easy Easy Easy Easy MobA: Two-Level Agent System for Efficient Mobile Task Automation 21 MOBBENCH We provide the complete details of 50 tasks as shown in Table 5. Please note that we give commands to agents in Chinese and all the applications are set to Chinese. Table 5. Complete list of tasks in MobBench. The content is translated from Chinese. Type Application Task Preparation Scoring Milestones 12306 (China Railway) Go to the My page in the 12306 app, and switch the app from standard mode to senior mode under common functions. Switch to standard mode. 1. Task completion. Bilibili Search Advanced Mathematics in Bilibili and play the first video. Camera Take photo using the cameras night mode. - - Clock Turn off the earliest opened alarm. Set and turn on several alarms. Douban Check the Douban rating of the movie Artificial Intelligence. Easy Google Calendar Create new task on Google Calendar titled Thesis Writing. 1. Task completion. 2. Skip the ad if applicable. 1. Task completion. 1. Task completion. 1. Task completion. 1. Task completion. World JD.com Help me check how much discount my World membership has. Logging in. 1. Task completion. Search for an assigned store on JD and add the first product to the cart. - 1. Task completion. McDonalds Switch the language of the McDonalds app to English. Weather Check tomorrows weather. Switch to Chinese. 1. Task completion. Medium 12306 (China Railway) Check the schedule for train G104 from Shanghai to Beijing tomorrow, and find out what time it is expected to arrive in Nanjing. Medium Bilibili From the \"Hot\" page of Bilibili, go to the leaderboard, play the top-ranked video, follow the account, and add the video to the favorites. Cancel following this account Medium Camera Set the timer selfie with the longest delay. Switch to the front camera. 1. Task completion. 1. Enter the timetable query interface. 2. Train number correct. 3. Task completed. 1. Enter the leaderboard. 2. Play the video. 3. Follow the account. 4. Add to favorites. 1. Set delay. 2. Switch to the front camera. Medium Clock Add Hong Kong in World Clock. There is no Hong Kong in world time. 1. Enter the addition interface. 2. Task completion. Medium Douban Search \"Douban Movie Top 250\" and like the top comment of the second-ranked movie. Medium Google Calendar Create new all-day event on Google Calendar for tomorrow titled Company Annual Meeting. Medium World Medium JD.com On the World app, search for the starting price of the nearest Hanting Hotel to me. Find the most recent product awaiting review on JD app, give it full five-star rating, write review, and add two photos. - - - 1. Enter the list. 2. Task completed. 1. Correct date. 2. Correct time. 3. Task completion. 1. Distance priority sorting. 2. Task completion. There are orders that have not been evaluated. 1. Enter review interface. 2. Task completion. Medium McDonalds Invoice the most recent McDonalds order. There is an order and no invoice has been issued. Medium Weather Check the sunset time tonight. - 1. Enter order interface. 2. Select the first order to invoice. 3. Task completion. 1. Task completion. - - - - 22 Zhu et al. Table 5. Complete list of tasks in MobBench. (Cont.) Type Application Task Preparation Scoring Milestones Hard 12306 (China Railway) Buy high-speed rail ticket for tomorrow from Shanghai to Beijing, with departure time between 9 am and 11 am. Departure and arrival stations differ from those on the homepage, and direct tickets must be available. 1. Correct date. 2. Correct station. 3. Correct train selection. 4. Add passenger. 5. Complete task. Hard BiliBili Search in Bilibili and follow the assigned account, play the newest video and write friendly comment. Unfollow the account, and disable the auto-play next video setting. Hard Clock Create an alarm at 10 oclock titled Work with vibration reminders on every Monday to Thursday. - Hard Camera Switch the camera to video mode, turn on the fill light, set the video quality to FHD 30FPS, and stop recording after more than twenty seconds. Switch to non-video mode. Hard Douban Search for the movie The Shawshank Redemption on Douban, mark it as watched, rate it five stars, and leave positive review. Remove the previous mark, rating, and review of this movie. Hard Google Calendar Hard World Hard JD.com Hard McDonalds Create recurring event on Google Calendar titled Computer Vision Course scheduled for every Wednesday from 6 PM to 8 PM, starting this week and repeating eight times. On the World app, book hotel in Chengdu for todays check-in and the day after tomorrows check-out, near Tianfu Square. Choose the second cheapest hotel listed when sorted by price, and use WeChat to pay. Search for assigned store on JD.com, find and buy the cheapest item in the store, choose the light-colored style, and buy two. Order 1+1 Mix & Match for immediate in-store pickup, choose McChicken and Mini Chocolate Sundae, add to cart, place your order, and choose Alipay to pay. - - - - Hard Weather Add Chengdu in the weather app and tell me the temperature range for tomorrow. Ensure that Chengdu is not listed in the city list. 1. Enter the account homepage. 2. Follow account. 3. Play video. 4. Write comment. 1. Time correct. 2. Title correct. 3. Period repetition is correct. 4. Turn off the ringtone and turn on the vibration. 5. Task completed. 1. Turn on the fill light. 2. Switch video quality. 3. Complete 20s recording. 1. Enter the movie details interface. 2. Successfully marked. 3. Successfully rated. 4. Leave review. 1. Correct time title. 2. Correct period repetition. 3. Task completed. 1. Date correct. 2. City location is correct. 3. Price correct. 4. Enter order interface. 5. Submit order. 6. Payment method correct. 1. Enter the store interface. 2. Correct product selection. 3. Task completed. 1. Find the set meal. 2. Select the set meal. 3. Settle the order. 4. Payment method correct. 5. Task completed. 1. Enter the city list. 2. Successfully added. 3. Task completed. MobA: Two-Level Agent System for Efficient Mobile Task Automation 23 Table 5. Complete list of tasks in MobBench. (Cont.) Type Application Task Preparation Scoring Milestones Indirect 12306 (China Railway) Book high-speed train ticket for the day after tomorrow, returning to Beijing before 6:00 PM. - 1. Correct train. 2. Correct date selection. 2. Add passenger details. 3. Task completion. Indirect Bilibili Im out of mobile data, what videos can still watch on the phone? Caching multiple videos in advance. 1. Open Bilibili. 2. Watch saved videos. Indirect Camera Delete the last photo taken recently. Indirect Clock Remind me in two hours. Indirect Douban Help me check which is the most popular movie among the upcoming releases in theaters. Indirect Google Calendar Record that have Natural Language Processing class from 10 AM to 12 PM tomorrow at campus. Indirect World Tell me about the nearest available hotel nearby and book it for me. Indirect JD.com Ask customer service if there is any discount for the top item in the shopping cart. Indirect McDonalds Order McSpicy Chicken Filet Burger Combo for dine-in. Indirect Weather Do need to bring an umbrella tomorrow? - - - - - - - - 1. Delete photo. 2. Task completion. 1. Enter the timer interface. 2. Task completion. 1. Open Douban. 2. Task completion. 1. Enter the calendar creation interface. 2. select the correct time. 3. select the correct location correctly. 4. Task completion. 1. Select current location. 2. Confirm the selection. 3. Submit the booking. 4. Task completion. 1. Select the correct item. 2. Enter customer service page. 3. Task completion. 1. Find the combo. 2. Checkout. 3. Task completion. 1. Task completion. Zhu et al. Table 5. Complete list of tasks in MobBench. (Cont.) Type Application Task Preparation Scoring Milestones Cross-APP 12306 (China Railway), Google Calendar Help me add my latest train journey to the schedule, titled {Departure Station}{Train Number}{Arrival Station}, and set exact time range. Cross-APP BiliBili, Douban Cross-APP Camera, JD.com Play the movie ranked third in the \"Douban Movie Top 250\" on Bilibili. Open the camera and tell me what item is in front of the lens, search for this type of product on JD.com, and select similar item to add to the shopping cart. - - Point the rear camera at common object Cross-APP Camera, WeChat Take photo with telephoto lens and share it on WeChat Moments, write few words with the note This is an automatically posted Moments by MobA in the end. Log in to WeChat in advance, the camera must face bright area, and click to keep the interface unchanged. 1. Enter ticket list. 2. Correct title. 3. Correct time. 1. Correct movie. 2. Successful play. 1. Correct answer. 2. Successfully search. 3. Add to shopping cart. 1. Take photo. 2. Enter share menu from gallery. 3. Correct caption. 4. Successfully send to Moments. Cross-APP Google Calendar, Clock What time is my meeting tomorrow? Set an alarm for two hours before the meeting to remind me. Cross-APP Google Calendar, Weather Check my calendar for the schedule of my trip to Shenzhen and tell me the weather forecast for that day. Set meeting schedule that starts at 10 a.m tomorrow morning. 1. Correct answer. 2.Successful alarm addition. Create new all day schedule for the day after tomorrow in the calendar, named Shenzhen Tour. 1. Enter Google Calendar. 2. Correct date retrieval. 3. Enter weather. 4. Successful weather query. Cross-APP World, BiliBili Check the address of my most recent order in World application and search for travel guides for that area on Bilibili. There is an order (which can be in unpaid or canceled payment status). 1. Retrieve order. 2. Enter Bilibili. 3. Successful video play. Cross-APP JD.com, WeChat Share the product link of the most recent JD.com order with WeChat friend, and write recommendation message. There is an order (which can be in unpaid or canceled payment status). Cross-APP McDonalds, Weather Order dine-in Spicy McWings at McDonalds. If the current weather is not suitable for going out, order delivery instead. Cross-APP Weather, Clock If it rains tomorrow, set an alarm for 10 a.m. to wake me up; otherwise, set the alarm for 8 a.m. - - 1. Enter order interface. 2. Select order. 3. Suitable recommendation message. 4. Successful sharing. 1. Check weather. 2. Select meal. 3. Checkout. 4. Correct dining option. 1. Successfully judge tomorrows weather. 2. Successfully set the alarm. VIEW HIERARCHY PROCESSING Given that (1) large models still exhibit limitations in processing visual information and (2) certain elements of the mobile phone interface cannot be obtained through visual means alone, the view hierarchy (VH) plays crucial role in enabling agents to effectively interpret the mobile interface. However, the XML files representing mobile interfaces contain substantial amount of redundant information. This redundancy increases token counts and complicates the agents task of identifying key UI elements. To address this issue, we developed an algorithm designed to filter UI elements. The algorithm consists of four steps: (1) parsing UI elements from the XML file, (2) filtering user-interactable UI elements based on their attributes, and adding them in ascending order of size, unless they exhibit significant overlap with previously added elements, (3) for UI elements containing text, merging the text content with interactive elements if the text is largely contained within those elements, thus enriching the interactive element with explanatory information, and (4) assigning an index to MobA: Two-Level Agent System for Efficient Mobile Task Automation 25 Input: xml file of the current screen Output: the annotated screen // First pass: Filter the small elements and all useless attributes 洧뉧롐뙗롐뉧롐뛿롐뉧롐洧노洧 (洧멇롐럻롐洧노 (洧녭 洧녰洧녳洧노洧뉧롐 (洧뉧롐뙗롐뉧롐뛿롐뉧롐洧노洧), 洧녲洧뉧롐 = 洧녩洧洧뉧롐); 洧멇롐뉧롐뙗롐뉧롐넗롐뫯롐뉧롐_洧뉧롐뙗롐뉧롐뛿롐뉧롐洧노洧 ; // Second pass: select elements whose overlapping area with former ones is small foreach 洧뉧롐뙗롐뉧롐뛿롐뉧롐洧노 in 洧뉧롐뙗롐뉧롐뛿롐뉧롐洧노洧 do if 洧뉧롐뙗롐뉧롐뛿롐뉧롐洧노 is interactive then 洧녰洧_洧녺洧녩洧녳洧녰洧녬 true; foreach 洧멇롐뉧롐뙗롐뉧롐넗롐뫯롐뉧롐_洧뉧롐뙗롐뉧롐뛿롐뉧롐洧노 in 洧멇롐뉧롐뙗롐뉧롐넗롐뫯롐뉧롐_洧뉧롐뙗롐뉧롐뛿롐뉧롐洧노洧 do if 洧녶洧녺洧뉧롐洧녳洧녩洧녷洧녷洧녰洧녵洧녮_洧녩洧洧뉧롐 is large then 洧녰洧_洧녺洧녩洧녳洧녰洧녬 false; end end if 洧녰洧_洧녺洧녩洧녳洧녰洧녬 is true then 洧멇롐뉧롐뙗롐뉧롐넗롐뫯롐뉧롐_洧뉧롐뙗롐뉧롐뛿롐뉧롐洧노洧 洧멇롐뉧롐뙗롐뉧롐넗롐뫯롐뉧롐_洧뉧롐뙗롐뉧롐뛿롐뉧롐洧노洧 + 洧뉧롐뙗롐뉧롐뛿롐뉧롐洧노; end end end // Third pass: Add the texts and merge the information of text into interactive elements foreach 洧뉧롐뙗롐뉧롐뛿롐뉧롐洧노 in 洧뉧롐뙗롐뉧롐뛿롐뉧롐洧노洧 do foreach 洧멇롐뉧롐뙗롐뉧롐넗롐뫯롐뉧롐_洧뉧롐뙗롐뉧롐뛿롐뉧롐洧노 in 洧멇롐뉧롐뙗롐뉧롐넗롐뫯롐뉧롐_洧뉧롐뙗롐뉧롐뛿롐뉧롐洧노洧 do if 洧뉧롐뙗롐뉧롐뛿롐뉧롐洧노 is contained in 洧멇롐뉧롐뙗롐뉧롐넗롐뫯롐뉧롐_洧뉧롐뙗롐뉧롐뛿롐뉧롐洧노 then merge(洧뉧롐뙗롐뉧롐뛿롐뉧롐洧노, 洧멇롐뉧롐뙗롐뉧롐넗롐뫯롐뉧롐_洧뉧롐뙗롐뉧롐뛿롐뉧롐洧노); end end end // Final pass: Sort the elements from left to right, top to bottom Sort(洧뉧롐뙗롐뉧롐뛿롐뉧롐洧노洧, key=(y,x)); Plot all the interactive elements with their index; Algorithm 2: The Logic of View-Hierarchy Process Algorithm each UI element according to its central coordinates, from left to right and top to bottom, while plain text elements are assigned an index of -1. This ensures that the index ordering aligns more closely with the users natural visual scanning behavior. In summary, the core of our algorithm is the preservation of key interactive elements and their associated textual information, while minimizing occlusion in the image. For example, in the case of the \"plane ticket\" element demonstrated in Figure 6, the UI element itself does not contain text, and the text information associated with the plane ticket is non-clickable. By merging the two, the agent can infer that clicking the UI element corresponds to selecting the plane ticket. 26 Zhu et al. However, limitations remain in this approach. There are cases where all elements in the XML file are marked as \"clickable=false\", despite the presence of interactive elements in practice. Additionally, technical limitations sometimes prevent the XML file from accurately reflecting the current state of the interface. Fig. 6. An Example Diagram of View-Hierarchy Processing. From left to right are the original image, unprocessed image and processed image. The underlined parts are the properties that are retained after the merge. ETHICS AND BROADER IMPACT STATEMENT Crowd-sourcing. Data collection and performance evaluation are done by the authors and no third-party crowdsourcers have participated. Data Privacy. The content involving user privacy has been processed as necessary for disclosure in the paper and supplementary files. Intended Use. MobA system is designed to help users efficiently use mobile applications to perform complex operations and difficult commands. Users send their commands and further instructions with either voice or text to MobA, which in turn comprehends, predicts, and executes the target operation. The system can subsequently be deployed on smart devices including GUIs such as mobile phones, tablets, computers, and cars to significantly enhance the capability of smart assistants. Potential Misuse. To control the GUI with user commands requires MobA to get high level of operational freedom and access to control, and will most likely involve large amount of user privacy information. Considering that decisions are often made with online APIs on remote servers, remote hijacking control or service providers failure to follow user privacy protocols may lead to user privacy leakage, data loss, or even damage to safety. MobA: Two-Level Agent System for Efficient Mobile Task Automation 27 Environmental Impact. There is large portion of elderly and people with disabilities around the world. We hope that MobA can improve their accessibility to the fast-evolving electronic devices, lower the threshold of using complex applications, reduce their fragmentation from the modern information society, and finally promote social equality."
        }
    ],
    "affiliations": [
        "Shanghai Jiao Tong University, China"
    ]
}