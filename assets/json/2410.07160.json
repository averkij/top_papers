{
    "paper_title": "TextToon: Real-Time Text Toonify Head Avatar from Single Video",
    "authors": [
        "Luchuan Song",
        "Lele Chen",
        "Celong Liu",
        "Pinxin Liu",
        "Chenliang Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose TextToon, a method to generate a drivable toonified avatar. Given a short monocular video sequence and a written instruction about the avatar style, our model can generate a high-fidelity toonified avatar that can be driven in real-time by another video with arbitrary identities. Existing related works heavily rely on multi-view modeling to recover geometry via texture embeddings, presented in a static manner, leading to control limitations. The multi-view video input also makes it difficult to deploy these models in real-world applications. To address these issues, we adopt a conditional embedding Tri-plane to learn realistic and stylized facial representations in a Gaussian deformation field. Additionally, we expand the stylization capabilities of 3D Gaussian Splatting by introducing an adaptive pixel-translation neural network and leveraging patch-aware contrastive learning to achieve high-quality images. To push our work into consumer applications, we develop a real-time system that can operate at 48 FPS on a GPU machine and 15-18 FPS on a mobile machine. Extensive experiments demonstrate the efficacy of our approach in generating textual avatars over existing methods in terms of quality and real-time animation. Please refer to our project page for more details: https://songluchuan.github.io/TextToon/."
        },
        {
            "title": "Start",
            "content": "TextToon: Real-Time Text Toonify Head Avatar from Single Video LUCHUAN SONG, University of Rochester, USA LELE CHEN, University of Rochester, USA CELONG LIU, Bytedance, USA PINXI LIU, University of Rochester, USA CHENLIANG XU, University of Rochester, USA 4 2 0 2 3 2 ] . [ 1 0 6 1 7 0 . 0 1 4 2 : r Fig. 1. Our method takes short monocular video as input (top) and animates the toonified appearance with synchronized expressions and movements using human-friendly text descriptions, e.g., \"Turn him into an American Comic style\" (middle). Moreover, our system achieves real-time animation (bottom), operating at 25 FPS (generation inference is about 48 FPS) on an NVIDIA RTX 4090 machine and 15 FPS on an Apple MacBook (M1 chip). All toonified faces (middle and bottom) are generated from the same pre-trained appearance model. Top block: Natural faceTee Noir (CC BY). Middle and bottom: Natural faceTrevor Noah (CC BY). We propose TextToon, method to generate drivable toonified avatar. Given short monocular video sequence and written instruction about Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SA Conference Papers 24, December 36, 2024, Tokyo, Japan 2024 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-1131-2/24/12. https://doi.org/10.1145/3680528.3687632 the avatar style, our model can generate high-fidelity toonified avatar that can be driven in real-time by another video with arbitrary identities. Existing related works heavily rely on multi-view modeling to recover geometry via texture embeddings, presented in static manner, leading to control limitations. The multi-view video input also makes it difficult to deploy these models in real-world applications. To address these issues, we adopt conditional embedding Tri-plane to learn realistic and stylized facial representations in Gaussian deformation field. Additionally, we expand the stylization capabilities of 3D Gaussian Splatting by introducing an adaptive pixel-translation neural network and leveraging patch-aware contrastive SA Conference Papers 24, December 36, 2024, Tokyo, Japan Luchuan Song, Lele Chen, Celong Liu, Pinxi Liu, and Chenliang Xu learning to achieve high-quality images. To push our work into consumer applications, we develop real-time system that can operate at 48 FPS on GPU machine and 15-18 FPS on mobile machine. Extensive experiments demonstrate the efficacy of our approach in generating textual avatars over existing methods in terms of quality and real-time animation. Please refer to our project page for more details: https://songluchuan.github.io/TextToon/. CCS Concepts: Computing methodologies Animation; Motion processing. ACM Reference Format: Luchuan Song, Lele Chen, Celong Liu, Pinxi Liu, and Chenliang Xu. 2024. TextToon: Real-Time Text Toonify Head Avatar from Single Video. In SIGGRAPH Asia 2024 Conference Papers (SA Conference Papers 24), December 36, 2024, Tokyo, Japan. ACM, New York, NY, USA, 11 pages. https: //doi.org/10.1145/3680528."
        },
        {
            "title": "INTRODUCTION",
            "content": "Generating high-quality toonified avatar videos is long-studied topic in computer vision and computer graphics, with applications in daily life, social media, movies, and video game character creation. Recently, generative models have shown great promise in creating stylized portrait videos. For instance, VToonify [Yang et al. 2022] uses learnable StyleGAN [Karras et al. 2020, 2019] to translate the specific portrait videos into different style via pre-collection images. AvatarStudio [Pan et al. 2023] edits dynamic facial appearances by score distillation sampling [Shi et al. 2023] from multi-view camera views. However, existing methods face challenges such as re-animation issues, low quality, and inference inefficiency, limiting their use in consumer applications. An ideal toonification avatar generation system should meet the following principles: Text Toonification: Text is widely used for providing instructions in everyday activities, requiring minimal expert knowledge. Consequently, user-friendly system can generate the corresponding appearance based on user-input instructions, without pre-collection data or annotations. Generalizability to the Driven Signal: The toonified avatar generated should have the capability to be controlled by arbitrary identites with varying head motions and expressions. Rapid Adaptation and Real-time Animation: The model should swiftly adjust to the desired style following user-input instructions within few minutes. Once adapted, real-time animation should be achievable on consumer devices. To achieve the above principles, we propose an effective method named TextToon for head avatar toonification. Inspired by Triplane based neural volumetric representation, we use normalized orthographic rendering as the conditional Tri-plane inputs for 3D Gaussian points properties. The 3D Gaussian points, which control dynamic head expressions in the canonical space, are influenced not only by the linear deformations from 3DMM coefficients but also by the learnable Tri-plane features. However, since 3DMM is linear model with limited facial expression capacity, we utilize the Tri-plane neural network structure to adaptively learn the mapping between the condition rendering and the corresponding stylized appearance in canonical space. Specifically, the normalized orthographic rendering map and 3DMM mesh deformed by expression 2 provide coarse representation of Gaussian point clouds. The finegrained topology structure results from further displacement of the coarse points positions by the learned Tri-plane features. For head motion, the estimated rotation and translation matrix are multiplied by each Gaussian points property to achieve full movement. Although the conditional Tri-plane-based 3D Gaussian Splatting significantly improves the accuracy of stylized facial expression representation, the toonification approaches still suffers from poor visual quality. For example, the inconsistencies in the diffusion-based Text-to-Image (T2I) editor [Aneja et al. 2023; MÃ¼ller et al. 2022; Shen et al. 2024; Wu et al. 2024; Zhang et al. 2023b] result in unclear and over-smoothed rendering outcomes [Shao et al. 2023]. To address this issue, we employ patch-aware contrastive learning manner in the fine-tuning phrase. Specifically, the corresponding patches of the edited image and the target image are mapped to language embeddings with CLIP model [Radford et al. 2021], and contrastive constraints are imposed on the embeddings, which avoids the blur issue caused by direct pixel supervision. To accelerate the execution, we adopt two-stage training strategy. At first, the framework is warmed-up on realism appearances by input single-view video. Then, we fine-tune the framework by the edited images from T2I module. The fine-tuning phrase usually takes five minutes. Benefit from the high inference speed of 3DGS, our pipeline could be easily accelerated to 48 FPS on single GPU machine and 15-18 FPS on mobile machine (e.g. Apple MacBook M1 chip). Overall, our main contributions are as follows: (1) To the best of our knowledge, we present the first method for text-driven head avatar editing from monocular video. It leverages the state-of-the-art in 3D Gaussian Splatting and Text-to-Image (T2I) model to achieve high-quality editing of dynamic avatar. (2) We propose conditional Tri-plane based Gaussian deformation field, which employs the canonical embedding to handle the nonlinear facial motion issue for toonification characters and significantly improve the animation accuracy. (3) We design real-time system capable of achieving inference speeds of over 48 FPS and pipeline about 25 FPS, with the 3DMM tracking algorithm being the efficiency bottleneck. Additionally, it is able to complete stylistic fine-tuning of special prompt in minutes."
        },
        {
            "title": "2 RELATED WORKS\n2.1 Parametrical Facial Model",
            "content": "The parametric facial model [Bao et al. 2021; Blanz and Vetter 2023; Brunton et al. 2014; Chai et al. 2022; Grassal et al. 2022; Hu et al. 2017; Li et al. 2017; Neumann et al. 2013; Song et al. 2023, 2021; Tewari et al. 2018; Tran et al. 2019; Tran and Liu 2018; Wang et al. 2022; Yang et al. 2023b; Zielonka et al. 2022] serves as an explicit facialization approach for reconstructing 3D face from RGB images. The facial shape ğ‘† is articulated as: ğ‘† = ğ‘† (ğ›¼, ğ›½) = ğ‘† + ğµğ‘–ğ‘‘ğ›¼ + ğµğ‘’ğ‘¥ğ‘ ğ›½, (1) where ğ‘† R3ğ¸ denotes the average shape, with ğ¸ representing the total number of vertices. ğµğ‘–ğ‘‘ and ğµğ‘’ğ‘¥ğ‘ are the PCA bases for identity and expression, respectively, and ğ›¼, ğ›½ are the corresponding parameters. The model facilitates spatial positioning adjustments through an Euler rotation matrix R33 and translation vector TextToon: Real-Time Text Toonify Head Avatar from Single Video SA Conference Papers 24, December 36, 2024, Tokyo, Japan Fig. 2. The overview of our methods. It takes monocular video as input and tracks per frame, initializing the Gaussian point clouds using the tracked geometry from the first frame. We leverage the rigid transformation matrix (R, Tğ‘¥,ğ‘¦,ğ‘§ ) and learnable lazy factor ğ‘¤ (in Sec. 3.3) to transfer points from the canonical space to the observation space. The proposed conditional Tri-plane Gaussian Deformation Field Dğ‘ uses the normalized render map ğ‘šğ‘¡ , expression ğ›½ğ‘¡ and vertex position Sğ‘¡ to predict the Gaussian properties deformation on each Gaussian points. Both the pre-training and fine-tuning phases share the same structure but target realistic appearance and T2I synthesized appearance, respectively. The details of conditional Tri-plane Gaussian Deformation Field and Text2Image editing are shown in III) and IV) respectively. Natural faceLizhen Wang et al. (CC BY). R3. The spatial coordinates ğ‘†ğ‘¥,ğ‘¦,ğ‘§ of the facial model is: ğ‘†ğ‘¥,ğ‘¦,ğ‘§ = ğ‘† (ğ›¼, ğ›½) + Tğ‘¥,ğ‘¦,ğ‘§, (2) enabling the projection of the 3D facial model onto 2D plane."
        },
        {
            "title": "2.2 Text-to-Image Generation",
            "content": "The approaches for text-driven image (T2I) synthesis with diffusion models [Dhariwal and Nichol 2021; Ho et al. 2020; Radford et al. 2021], they work on the static appearance editing in terms of content and style. Due to the powerful ability of diffusion model in style generation, the T2I is able to produce wide range of visual with text prompts or other modalities. For example, Latent Diffusion Models (LDM) [Rombach et al. 2022] synthesis high-resolution images through text, controlnet [Zhang et al. 2023b] generates corresponding images under given control conditional maps and text prompts and InstructPix2Pix [MÃ¼ller et al. 2022] focuses on the edit the given images with prompts."
        },
        {
            "title": "2.3 Text-to-Video Generation",
            "content": "The Text-to-Video (T2V) is further step toward T2I, it pay more attention to dynamic video generation rather that static images. Some state-of-the-art works [Clark and Jaini 2024; Ho et al. 2022; Hong et al. 2022; Mei and Patel 2023] have achieved the continuous visual effect and aligned with the input prompts. These methods learn visual features corresponding to text embeddings from extremely large amounts of videos, and synthesize frames with optical flow as supervision."
        },
        {
            "title": "2.4 Video Transfer via StyleGAN",
            "content": "Recently, the StyleGANs [Karras et al. 2020, 2019] show the outstanding performances in terms of video stylization. Some methods [Liu et al. 2022; Richardson et al. 2021] decode the aligned appearance of the corresponding style by editing the facial attributes latent codes. To achieve unaligned controllability, Stitch-Time [Tzaban et al. 2022] proposes an align-unalign-paste process to edit the in-the-wild videos. Furthermore, VToonify [Yang et al. 2022] and StyleGANEX [Yang et al. 2023a] incorporates the random geometric transformations for fine-tuning on the unaligned face images while improving the quality."
        },
        {
            "title": "3 METHOD",
            "content": "The pipeline of our proposed method is illustrated in Figure 2. Given single portrait video as input, we preprocess the video data using 3DMM estimation to generate normalized orthographic renderings ğ‘šğ‘¡ , expression parameters ğ›½ğ‘¡ , and corresponding vertex geometry for each frame. Our method then applies the conditional Tri-plane Gaussian deformation field to edit the appearance in the canonical space and control expressions. The training phase is divided into two 3 SA Conference Papers 24, December 36, 2024, Tokyo, Japan Luchuan Song, Lele Chen, Celong Liu, Pinxi Liu, and Chenliang Xu steps: (1) photo-realistic appearance pre-training and (2) text-driven appearance fine-tuning. Both steps share the same inputs but differ in objective functions. The (1) lies in the supervision of realistic appearance, and (2) is focused in the semi-supervised adaptation of style images."
        },
        {
            "title": "3.1 Data Acquisition",
            "content": "We apply 3DMM tracking to generate the corresponding parameters (euler roatation R33, translation vector R13, facial identity ğ›¼ R30 and expression ğ›½ R52) for monocular inputs. The pose and identity removed geometry R17033 is set with = I, = 1 and ğ›¼ = 0 in Equ. 2. For effectiveness, we directly solve for the analytical solutions of the 3DMM parameters with Gauss-Newton optimization on CPU, as in the face2face [Thies et al. 2016]. Specifically, we compute the Jacobian related to the residual of detected facial landmarks and projected landmarks, the preconditioned conjugate gradient (PCG) method is used for updating the parameters solutions ğ‘‹ , as: ğ‘‡ ğ‘‡ JÎ”ğ‘ƒ = (3) the Î”ğ‘ƒ is the updated scale of each step, we perform four-step PCG in each Gauss-Newton optimization. In this way, we achieve the 3DMM estimation speed of 25 30 FPS on CPU, freeing up the GPU (if available) for backend processing. r,"
        },
        {
            "title": "3.2 Tri-plane Gaussian Deformation Field",
            "content": "The Tri-plane [Chan et al. 2022; Shao et al. 2023; Song et al. 2024; Sun et al. 2022] has made significant improvement in 3D representation. It directly represents the density and color in NeRF through the stored three-plane features, which avoids the heavy rely on implicit features in connected MLP layers. Inspired by the color and density decoded by Tri-plane, we propose the conditional Tri-plane Gaussian Deformation Field to decode the Gaussian properties. Considering that our setting is person-specific neural rendering, the inputs are conditional embeddings instead of sampled noise. As shown in Figure 2, it takes the normalized orthographic rendering maps ğ‘šğ‘¡ and expression ğ›½ğ‘¡ as input with vertex position Sğ‘¡ as prior to obtain the three plane features Fğ‘‹ğ‘Œ ğ‘ = {Fğ‘‹ğ‘Œ , Fğ‘‹ ğ‘ , Fğ‘Œ ğ‘ }. We build Gaussian deformation decoder for the properties in 3DGS. Specifically, the points in Gaussian field can be represented by the offset on position vector ğ‘¥ğ‘¦ğ‘§ Rğ‘ 3, quaternion vector ğ‘ Rğ‘ 4 and scaling vector ğ‘  Rğ‘ 3 (please note the scale ğ‘  is distinguished from geometry S), where the ğ‘ is the number of Gaussian points. Apart from that, each point has additional parameters: spherical harmonics (SH) ğ‘ â„ Rğ‘ 3 and opacity ğ›¾ Rğ‘ 1. The head canonical appearance Gaussian deformation field Dğ¶ can be formulated as: {ğ‘¥ğ‘¦ğ‘§, ğ‘, ğ‘ , ğ‘ â„, ğ›¾ } = Dğ¶ (ğ‘šğ‘¡ , ğ›½ğ‘¡ , Sğ‘¡ ). (4) Considering that the expression motion of toonified face and the realism face are different, it is hard to directly fine-tune {ğ‘¥ğ‘¦ğ‘§, ğ‘, ğ‘ } to achieve the toonified expression control. Therefore, we do not completely adopt the learned ğ‘¥ğ‘¦ğ‘§ as the fine-grained face motion as previous methods [Qian et al. 2023; Xiang et al. 2023; Xu et al. 2024]. The vertex from coarse facial geometry is set as prior Gaussian points for each frame. Specifically, we sample the vertices coordinate Fig. 3. visualization of the adaptively selected points via ğ‘¤. After introducing the lazy factor ğ‘¤, soft boundary forms between the head and shoulders. Otherwise (w/o), they are mixed together and difficult to distinguish. It avoids mis-segmentation issues (indicated by the blue arrow). Natural faceTee Noir (CC BY). from each Sğ‘¡ and concatenate them with the learnable Gaussian points to jointly guide the rendering."
        },
        {
            "title": "3.3 Photo-Realistic Appearance Pre-training",
            "content": "3.3.1 Non-rigid Motion Decoupling. In the 3DGS for dynamic scene, the rigid transformation matrix (R and T) is applied to the point clouds from the canonical to the observation space. To avoid the artifacts from the non-rigid motion of the shoulder, SplattingAvatar [Shao et al. 2024] and FlashAvatar [Xiang et al. 2023] remove the shoulder and only retain the face region (with neck). AD-NeRF [Guo et al. 2021] leverages the pre-trained segmentation model to parse the face and train two NeRFs to represent different areas, which heavily relies on the accuracy of the segmentation model. To address this problem, our approach arises from two assumptions: I) The head and shoulder within the same structure, which cannot be modeled independently. II) The shoulder is not completely fixed but exhibits the lazy\" movement with much lower amplitude and frequency than head motion. For assumption I), we initialize the shoulder from cuboid structure point clouds, and optimize its properties in Gaussian splatting together with the face part. In assumption II), we introduce learnable lazy\" factor ğ‘¤ Rğ‘ 4 to control the shoulder movements. Specifically, the following rigid transformation is: ğ‘¥ğ‘¦ğ‘§ + ğ‘¥ğ‘¦ğ‘§, Rğ‘ ğ‘ ğ‘, the {ğ‘¥ğ‘¦ğ‘§, ğ‘} is the position and quaternion vector in observation space, Rğ‘ is the quaternion corresponding to R. Then, we apply the lazy vector ğ‘¤ for non-rigid adjustments to Eq. 5: (5) ğ‘¥ğ‘¦ğ‘§ ğ‘¤ + ğ‘¥ğ‘¦ğ‘§, Rğ‘ ğ‘ ğ‘¤ ğ‘. (6) ğ‘¥ğ‘¦ğ‘§1 ğ‘¤1 + ğ‘¥ğ‘¦ğ‘§2 ğ‘¤2 + ğ‘¥ğ‘¦ğ‘§ The Eq. 6 can be divided into head and shoulder parts: 1 + ğ‘¥ğ‘¦ğ‘§ 2 1 + ğ‘ Rğ‘ ğ‘1 ğ‘¤1 + Rğ‘ ğ‘2 ğ‘¤2 ğ‘ , 2 ğ‘¥ğ‘¦ğ‘§1, ğ‘¥ğ‘¦ğ‘§ , ğ‘¤1 are for face region and ğ‘¥ğ‘¦ğ‘§2, ğ‘¥ğ‘¦ğ‘§ , ğ‘¤2 are 1 2 for shoulder region, ğ‘¤ is the concatenation of ğ‘¤1 and ğ‘¤2. Then, the head moves rigidly and the shoulder moves lazily, so we have: , ğ‘2, ğ‘ 2 , ğ‘1, ğ‘ 1 (7) , ğ‘¤1 Iğ‘, ğ‘¤2 (R1)ğ‘ . (8) 4 TextToon: Real-Time Text Toonify Head Avatar from Single Video SA Conference Papers 24, December 36, 2024, Tokyo, Japan The ğ‘¤ is learned according to the approximation in Eq. 8, and Iğ‘ is the quaternion corresponding to the unit rotation matrix. As shown in Figure 3, the lazy factor ğ‘¤ is broadcast to each point in the 3D Gaussian to achieve non-rigid coupling of the head and shoulder parts with soft boundary. 3.3.2 Pre-training Objectives. We warm-up the Tri-plane Gaussian deformation field and image-to-image (I2I) module on the realism appearance. The objective function is divided into three parts, coarse loss, refine-grained loss, and regularization of ğ‘¤. The coarse loss Lğ‘…ğºğµ is euclidean distance supervision on pixel color of generated images (ğ¼ğ‘Ÿ and ğ¼ ) and ground-truth (ğ¼ğ‘”ğ‘¡ ): Lğ‘…ğºğµ = ğ¼ğ‘Ÿ ğ¼ğ‘”ğ‘¡ 1 + ğ¼ ğ¼ğ‘”ğ‘¡ 1. (9) The refine-grained loss Lğ¿ğ‘ƒğ¼ ğ‘ƒğ‘† is defined by the LPIPS [Johnson et al. 2016] between output images (ğ¼ ) from I2I module and groundtruth: Lğ¿ğ‘ƒğ¼ ğ‘ƒğ‘† = LPIPS(ğ¼, ğ¼ğ‘”ğ‘¡ ). At last, the regularization of ğ‘¤ is: ğ‘¤ ğ‘¤ğ‘‡ ğ‘ Iğ‘ 2, (10) (11) where ğ‘¤ğ‘ Rğ‘ 4 is vector and equal to [Iğ‘, Rğ‘], where ğ‘¤ = [ğ‘¤1, ğ‘¤2] is set by the face and shoulder point cloud. The total objective function is: Lğ‘…ğºğµ + Lğ¿ğ‘ƒğ¼ ğ‘ƒğ‘† + ğœ† ğ‘¤ ğ‘¤ğ‘‡ ğ‘ Iğ‘ 2, (12) the ğœ† is hyperparameter and it is equal to 1ğ‘’ 3 in our work."
        },
        {
            "title": "3.4 Text-Driven Appearance Fine-tuning",
            "content": "Text-driven appearance is fine-tuned with the images edited by T2I model [Brooks et al. 2023] instead of the realistic appearance from video frames, as shown in Figure 2 IV). To achieve high-quality toonify editing and address inconsistency issues by T2I, the objective function focuses on feature distance rather than pixel distance. Inspired by NeRF-Art [Wang et al. 2023a], we employ patchaware contrastive loss. Specifically, we randomly select three patches of size 224 224 from the edited image and encode each patch into the language feature space using the CLIP model [Radford et al. 2021]. In each iteration, positive samples for contrastive learning are constructed from the edited image patches ğœ‹ (Pğ‘— ), where ğœ‹ is the pre-trained CLIP model for textural feature embedding and ğ‘— is patch from the edited images. Negative samples are built from predefined negative prompts ğœ‹ (neg), where neg is randomly selected from set of negative prompts such as {Animal, Low quality, Blurry, Low res, Long neck, ...etc\"}. During optimization, we aim to shorten the distance between ğ‘— and the positive samples while pushing them away from the negative descriptions. The patch-aware contrastive learning objective function is: Lğ¶ğ‘‚ğ‘ = log[ PI exp(ğœ‹ (P ğ‘— ) ğœ‹ (Pğ‘— )) exp(ğœ‹ (P ğ‘— ) ğœ‹ (Pğ‘— )) + exp(ğœ‹ (Pğ‘— ) ğœ‹ (neg)) ], (13) where the Pğ‘— is the corresponding rendered image patches from Gaussian splatting. From Eq. 13, we avoid measuring the pixel distances and refer to text feature distances. Taking over the objective functions in Sec. 3.3, the objective functions for fine-tuning are: Lğ¿ğ‘ƒğ¼ ğ‘ƒğ‘† + ğœ†1 Lğ¶ğ‘‚ğ‘ + ğœ†2 ğ‘¤ ğ‘¤ğ‘‡ ğ‘ Iğ‘ 2, (14) where the ğœ†1 and ğœ†2 are set to 1ğ‘’ 3 in our work. The overall objects remain similar to Eq. 12, the difference is that we apply Lğ¶ğ‘‚ğ‘ instead of Lğ‘…ğºğµ. The objective is to accentuate the high-frequency features of the toonified appearance, such as teeth and hair, while avoiding issues of over-smoothing."
        },
        {
            "title": "Implementation Details",
            "content": "We conduct experiments with 8 subjects monocular videos from the public datasets PointAvatar [Zheng et al. 2023], StyleAvatar [Wang et al. 2023b], and InstantAvatar [Zielonka et al. 2023]. Apart from that, we include two videos from self-recordings and YouTube to evaluate performance in the wild. All videos are presented at 25 FPS with resolution of 512512. We take an average length of 1000-6000 frames for training (about 80%) while the test dataset includes frames with novel expressions and poses (about 20%). For each frame, we apply the RobustVideoMatting [Lin et al. 2022] to remove the background. The number of Gaussian points is set to 1ğ‘’4, and the number of prior vertex points is 1703. During pre-training and fine-tuning phases, the learning rate for Gaussian properties {ğ‘¥ğ‘¦ğ‘§, ğ‘, ğ‘ , ğ›¾, ğ‘ â„} are {2ğ‘’ 5, 1ğ‘’ 3, 5ğ‘’ 3, 5ğ‘’ 2, 2.5ğ‘’ 3}. The learning rate for ğ‘¤, I2I module, Tri-plane generator and encoder are 1ğ‘’ 3, 1ğ‘’ 2, 1ğ‘’ 3 and 1ğ‘’ 3 respectively. We employ the InstructPix2Pix [Brooks et al. 2023] as T2I module with 20 denoise steps to edit the frames. It takes about 40k iterations for photo-realistic appearance pre-training, and 200 500 iterations for text-driven appearance fine-tuning. We employ the MLPs with dimensions [32+14] for the decorder of Tri-Plane, these account for 11-dimensions for xyz (Gaussian points position), Rotation, Scale and Opacity, 3-dimensions for Colors, respectively. And other components are shown and discussed in Section 5 and Figure 14."
        },
        {
            "title": "4.2 Baseline",
            "content": "Given the challenges associated with text-driven avatar stylization, including video continuity, monocular-limited input, and synchronized re-animation movements, few methods exactly handle this task. Therefore, we select several related works for comparison, as, VToonify [Yang et al. 2022]: It is StyleGAN-based for precollection and annotation style images transfer to unaligned faces. We take the in-the-wild portrait movement to generate the corresponding toonified images. StyleGANEX [Yang et al. 2023a]: The StyleGANEX is also based on unalignment StyleGAN, which brings more flicker suppression and improved video consistency than VToonify. FRESCO [Yang et al. 2024]: The setting is similar to StyleGANEX and VToonify. But the difference is that it does not need to the pre-collection style images but adopts prompts and stable diffusion to edit the video style with optical flows. 5 SA Conference Papers 24, December 36, 2024, Tokyo, Japan Luchuan Song, Lele Chen, Celong Liu, Pinxi Liu, and Chenliang Xu Fig. 5. Visualization of cross-identity driven results. The drive actor is captured in the wild (Obama White House Daily), and the toonified avatar from different identity is synchronized by facial expressions and poses. Fig. 4. The perceptual evaluation of our method and baselines. For the sake of fairness and to avoid cherry-picked results, we adopt Pixar/cartoon stylization models provided by StyleGANEX and VToonify, and align them with our prompts. The style strength of VToonify is set to 0.7. Natural faceLizhen Wang et al. (CC BY), and Wojciech Zielonka et al. (CC BY). Methods BRISQUE CLIP-D STD (102) ( 0) ( 1) FPS (w/o Track) VToonify StyleGANEX FRESCO TextToon 0.62 0.54 0.58 0.49 Quantitative Results 0.17 0.10 0.21 0.25 0.171 0.195 0.227 0. 14 8.4 0.5 48 IP TP MS VQ ( 5) User Study 4.5 2.9 3.5 4.3 3.1 4.3 2.9 3.8 3.9 3.2 3.8 4.1 4.2 3.5 3.0 4.7 We also acknowledge other related methods, such as AvatarStudio [Pan et al. 2023]. However, due to its multi-camera setup (differing from the single view) and the lack of sufficient implementation details, we do not include it in the detailed comparison."
        },
        {
            "title": "4.3 Numerical Evaluations",
            "content": "Due to the absence of ground-truth data, it is challenging to evaluate text-driven visual edits numerically. At here, we take two criteria for numerical evaluations, one is from quantitative measurement, the other is from human assessment. 4.3.1 Quantitative Metrics. It is based on three aspects. (1) Blind / Referenceless Image Spatial Quality Evaluator (BRISQUE): The BRISQUE [Mittal et al. 2011] is non-reference image quality assessment method, and is used to directly assess the quality of edited images. (2) Text-Image Consistency (CLIP-D) [Haque et al. 2023]: The CLIP-D is the embedding between text and image to calculate cosine similarity for each pair via CLIP. higher CLIP-D score indicates closer match between the generated appearance and the text. (3) Standard Deviation for video stylization (STD): We measure the CLIP features standard deviation of each frames across the video, which represents the stability of style in the video. (4) Frame Per Second without 3DMM Tracking (FPS w/o Track): We evaluate model inference speed on single NVIDIA RTX4090 GPU. Table 1. (1) Left: Quantitative evaluations with the baseline methods. The / indicates lower/higher values of better performance. The best results are in bold, and the second-best are underlined. (2) Right: The 5-point Likert scale for user study (closer to 5 indicates better performance). 4.3.2 User Study. We sample 10 different identities each one with 8 styles (80 videos for self-reenactment and 50 videos for crossreenactment), and invite 32 attendees from the Amazon Mechanical Turk (MTurk) to join the study. The Mean Opinion Scores (MOS) rating protocol is adopted for evaluation and the attendees are required to rate the generated videos from four aspects: (1) IP (Identity Preservation): Do you agree the identity is well-preserved in the stylized video? , (2) TP (Text Preservation): Do you agree the text description is well-presented in the stylized video?, (3) MS (Motion Synchronization): Do you agree the head motion in stylized video is synchronized with source one? and (4) VQ (Video Quality): Do you agree the overall video quality is good, e.g. frame quality, temporal consistency etc? There is 5-point Likert scale for each term, range from 1-5 corresponds to strongly disagree, disagree, dont know, agree and strongly agree respectively (the closer to 5 the better)."
        },
        {
            "title": "4.4 Comparison with Baseline Methods",
            "content": "The comparison results are shown in Table 1 and Figure 4 respectively. From Table 1, our method achieves almost the best or second best results in user study and quantitative evaluations. The VToonify [Yang et al. 2022] is still powerful baseline, which leads 6 TextToon: Real-Time Text Toonify Head Avatar from Single Video SA Conference Papers 24, December 36, 2024, Tokyo, Japan Fig. 6. The relationship of pre-trained and fine-tuned appearance. We list the different correspondences, better pre-trained model results in more detailed fine-tuned appearance (the pre-training iterations are not constant, but the fine-tuning iterations are consistent). The error maps show pixel Euclidean distance in RGB (color in [0, 255]). lower mean error (white number) indicates better pre-trained appearance. Please refer to the arrows for facial details. Natural faceYao Feng et al. (CC BY). the way in terms of style preservation (TP) and FPS. As the meantime, our method achieves the most consistent style across one video from STD. The perception comparisons are shown in Figure 4, our approach achieves better performance than the excessive style of VToonify or the source identity preservation of StyleGANEX. It is worth noting that stylization assessment is based on personal perception awareness. We acknowledge the superiority of baseline methods in different aspects, but our method is also better than them in terms of user-friendly text input, fast style fine-tuning, inference speed and re-animation. Additionally, we present the results of re-animation are shown in Figure 5, where the actor is in the top-left in the Figure 5 and the rest are driven text-toonification appearances."
        },
        {
            "title": "4.5 Ablation Study",
            "content": "We focus on the following aspects for ablation studies. 4.5.1 Relationship between pre-training and fine-tuning. We enumerated the fine-tuned results under the pre-trained models under different epochs (the fine-tuning steps are set 500 iterations). As shown in Figure 6, larger pre-train epoch corresponds to more detailed realistic appearance (mean error 2.74 for epoch 30), and the fine-tuned stylization appearance also has more details. It can Fig. 7. The visualization of three ablation studies (from top to bottom) results from multi-view, the multi-view results are only learned from monocular inputs. (1) Top: The fine-tuned appearance by our method with different prompts. (2) Middle: The visualization of stylization intensity, where intensity = 0 represents the source appearance. (3) Bottom: The style trajectory over fine-tuning time, with Time = 0 min representing the source appearance. Please refer to the arrow for details. At the meantime, we present the multi-view visualization results at here for perception assessment, the multi-view appearance are learned from the monocular inputs. Natural faceYufeng Zheng et al. (CC BY), and Wojciech Zielonka et al. (CC BY). be concluded that fine-tuning will not contribute additional finegrained gains to the appearance, and an excellent pre-trained model is fundamental for effective stylization. 4.5.2 Adaptation to prompts. In the previous mention, the format of the used prompt is similar to \"Turn ... into ...\". To demonstrate robustness to prompts, we show the results by simple prompts, such as \"Kawaii\", as shown in the Figure 7 (first row). We present each style from multiple view, which are learned from single-view input. Moreover, our method is adaptable to different prompts and can even handle mixed styles like \"Kid + Marble\". Stylization intensity. The control over stylization intensity 4.5.3 is shown in the second row in Figure 7. We take the ratio of the text control strength and the image guidance strength in the T2I (the larger the ratio, the data provided by T2I is closer to the text description style) as the stylization intensity. The intensity is 0 representing the facial appearance without stylization. We find that as intensity increases, the appearance is closer to textually descriptive (e.g., the face color becomes whiter) while maintaining the same expressions as the pre-trained one. 7 SA Conference Papers 24, December 36, 2024, Tokyo, Japan Luchuan Song, Lele Chen, Celong Liu, Pinxi Liu, and Chenliang Xu Fig. 8. We visualize the conditional Tri-plane Gaussian deformation field Dğ‘ using cross-identity driven approach. Utilizing Dğ‘ (w/) helps manage more complex mouth shapes compared to not using Dğ‘ (w/o). This improvement occurs throughout both the pre-training and fine-tuning phases. We highlight the mouth details for better evaluation. Natural faceWojciech Zielonka et al. (CC BY). Fig. 9. The visualization of the ablation study Lğ¶ğ‘‚ğ‘ via cross-identity driven manner. After introducing contrastive learning loss (w/ Lğ¶ğ‘‚ğ‘ ), we achieve significant improvement in the detail quality of the toonify appearance, as indicated by the arrows (e.g. facial wrinkles etc.). Natural faceWojciech Zielonka et al. (CC BY). Fine-tuning time. The style trajectory over fine-tuning time 4.5.4 are shown in Figure 7 (3ğ‘Ÿğ‘‘ row). We initialize from the pre-trained model (Time = 0 min). After more than 2 minutes, the change in appearance is minor. Generally, the fine-tuning time is set to 5 minutes. 4.5.5 Ablation study on Dğ‘ . The conditional Tri-plane Gaussian deformation field is used throughout the pre-training and fine-tuning phase. Its main role is to reduce reliance on large number of MLPs and control facial expressions with conditional inputs. We replace Dğ‘ with the connected MLP layers and repeat the pipeline. As shown in Figure 8, w/o Dğ‘ is to replace Dğ‘ by MLPs. It can be observed that the introduction of Dğ‘ improves the accuracy of expression for pre-trained realism appearance. This enhancement also positively 8 Fig. 10. The visualize of the ablation study on ğ‘¤. (a) is for the hard-parsed motion on head/shoulder, (b) for is the same motion on head/shoulder and (c) for is the adaptively learned ğ‘¤. We zoom-in the neck-shoulder transition area to highlight the details. There are artifacts around the neck in (a) due to inconsistent head/shoulder movement, and the shoulder in (b) is displaced synchronously with the head. The influence occurs throughout both the pre-training and fine-tuning phases. Natural faceYufeng Zheng et al. (CC BY). affects the stylized appearance. Additionally, we present quantitative measurements to verify this perceptual assessment. Specifically, we leverage the average ğ¿1 distance on facial key points to evaluate the results corresponding to w/ and w/o Dğ‘ on the pre-trained appearance. Since keypoints cannot be accurately detected on stylized faces, we exclude the fine-tuned appearance from this discussion. Please refer to [Duan et al. 2023] for key point distance details. The keypoint distance for w/ and w/o Dğ‘ are 4.87 and 5.90 respectively, indicating that Dğ‘ leads to more accurate facial movements. 4.5.6 Ablation study on Lğ¶ğ‘‚ğ‘ . The Lğ¶ğ‘‚ğ‘ is used to improve the quality of stylized images through patch-aware contrastive learning during fine-tuning, preventing over-smoothing. We show the fine-tuned appearance with (w/) and without (w/o) it in Figure 9. The results without Lğ¶ğ‘‚ğ‘ are smoother and lack the facial details. Additionally, we provide quantitative evaluations: the BRISQUE score with Lğ¶ğ‘‚ğ‘ is 52.5, while the score is 67.1 for w/o. 4.5.7 Ablation study on factor ğ‘¤. We introduce the \"lazy\" factor ğ‘¤ in Section 3.3. The primary assumption behind ğ‘¤ is to eliminate the need for separate processing of the shoulder (also referred to as the torso) and the head, as was required in previous works such as ADNeRF [Guo et al. 2021]. This adaptive parsing method significantly enhances the robustness of rigid upper-body movement modeling. As illustrated in Figure 10, we employ different configurations of ğ‘¤ to achieve the transition of rigid movement from canonical space to camera space. (a): fixed unit rotation matrix ğ¼ is assigned to ğ‘¤ TextToon: Real-Time Text Toonify Head Avatar from Single Video SA Conference Papers 24, December 36, 2024, Tokyo, Japan Fig. 11. The visualize of comparison with Next3D, the PTI (StyleGAN inversion) is used for feature embedding. We find that the results suffer from facial identity drift. Furthermore, the two styles associated with Next3D (Pixar and Cartoon) are not distinct. We apply the prompts Cartoon\" and Pixar\" for Next3D, the corresponding prompts for our method are Turn him into cartoon style\" and Turn him into Pixar style\". Natural faceLizhen Wang et al. (CC BY). in Eq. 7 and estimated head rotation matrix for ğ‘¤1, the head and shoulder motion are hard parsed. (b): The head and shoulders share the same estimated rotation matrix. (c): ğ‘¤ is adaptively learned. When ğ‘¤ is hard-parsed, the head and shoulder movements are uncoordinated, leading to separation as seen in Figure 10(a). Conversely, in Figure 10(b), the shoulders move in sync with the head, which is not accurate, as the shoulders typically remain still or exhibit only minimal movement. Our method is able to improve stabilize shoulder motion. While shoulder movement can be managed using other techniques, such as Linear Blend Skinning (LBS) around the neck, our approach is specifically designed for the real-time head tracking, as detailed in Section 3.1. It is trade-off between real-time capable and render quality."
        },
        {
            "title": "4.6 Discussion with EG3D Methods",
            "content": "Some methods related to EG3D [Chan et al. 2022] also demonstrate their ability in stylization, such as the DeformToon3D [Zhang et al. 2023a] and Next3D [Sun et al. 2023]. But they requires extra facial alignment and depend on StyleGAN [Karras et al. 2019] inversion for specific identities. And DeformToon3D is built based on StyleSDF [Or-El et al. 2022]. The StyleSDF [Or-El et al. 2022] does not have the inversion approach so far. In addition, we provide some comparison results with Next3D by PTI [Roich et al. 2021] inversion in Figure 11. We find the Next3D suffer from the shift of identity by StyleGAN inversion and facial expression inconsistent."
        },
        {
            "title": "4.7 Real-Time Application\nBenefit from the 3D Gaussian Splatting, the lazy factor ğ‘¤ (avoid the\nseparate parsing of head and shoulder) and the CPU optimizations\nfor matrix operations in 3DMM-estimation, we present the real-time\napplication in Figure 12 and the project website. It can achieve the\nphoto-realistic and text-based toonification appearance animation\nfrom the camera capture. The system is about 25 FPS on the daily\nGPU machine and 18 FPS on Apple MacBook M1 Chips (with quality",
            "content": "Fig. 12. The visualize of the real-time application. We use the camera capture appearance to animate the toonified appearance, the prompt Turn her into the Joker\" is applied for fine-tuning the realistic appearance. The reference source portrait is shown in bottom right corner. Natural faceYufeng Zheng et al. (CC BY). Fig. 13. The visualization of the problematic cases. We present the limitations regarding unseen poses and expressions. Artifacts for unseen poses are around the neck and head, while for unseen expressions, blurriness appears near the cheeks. Natural faceWojciech Zielonka et al. (CC BY). compression). Moreover, for the pure inference settings, the speed can be improved to about 48 FPS. It is worth mentioning that the efficiency of the 3D head tracking algorithm is the bottleneck of our real-time system."
        },
        {
            "title": "5 DISCUSSION AND CONCLUSION\n5.1 Limitations",
            "content": "There are few limitations to our methods. First, our stylization relies on the Text2Image modules ability. Achieving fine-grained control such as \"writing Siggraph on his/her left cheek\" could be challenging. Second, the pre-training phase is limited by the diversity and quality of the training dataset. Similar to other neural 9 SA Conference Papers 24, December 36, 2024, Tokyo, Japan Luchuan Song, Lele Chen, Celong Liu, Pinxi Liu, and Chenliang Xu enables the learned toonify head avatar to be re-animated by other in-the-wild camera captures in real-time. We expect that our method and the principles of real-time system design will inspire further advancements in related approaches."
        },
        {
            "title": "REFERENCES",
            "content": "Shivangi Aneja, Justus Thies, Angela Dai, and Matthias NieÃŸner. 2023. Clipface: Text-guided editing of textured 3d morphable models. In ACM SIGGRAPH 2023 Conference Proceedings. 111. Linchao Bao, Xiangkai Lin, Yajing Chen, Haoxian Zhang, Sheng Wang, Xuefei Zhe, Di Kang, Haozhi Huang, Xinwei Jiang, Jue Wang, et al. 2021. High-fidelity 3d digital human head creation from rgb-d selfies. ACM Transactions on Graphics (TOG) 41, 1 (2021), 121. Volker Blanz and Thomas Vetter. 2023. morphable model for the synthesis of 3D faces. In Seminal Graphics Papers: Pushing the Boundaries, Volume 2. 157164. Tim Brooks, Aleksander Holynski, and Alexei Efros. 2023. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1839218402. Alan Brunton, Timo Bolkart, and Stefanie Wuhrer. 2014. Multilinear wavelets: statistical shape space for human faces. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13. Springer, 297312. Zenghao Chai, Haoxian Zhang, Jing Ren, Di Kang, Zhengzhuo Xu, Xuefei Zhe, Chun Yuan, and Linchao Bao. 2022. Realy: Rethinking the evaluation of 3d face reconstruction. In European Conference on Computer Vision. Springer, 7492. Eric Chan, Connor Lin, Matthew Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, et al. 2022. Efficient geometry-aware 3d generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1612316133. Kevin Clark and Priyank Jaini. 2024. Text-to-Image Diffusion Models are Zero Shot Classifiers. Advances in Neural Information Processing Systems 36 (2024). Prafulla Dhariwal and Alexander Nichol. 2021. Diffusion models beat gans on image synthesis. Advances in neural information processing systems 34 (2021), 8780 8794. Hao-Bin Duan, Miao Wang, Jin-Chuan Shi, Xu-Chuan Chen, and Yan-Pei Cao. 2023. Bakedavatar: Baking neural fields for real-time head avatar synthesis. ACM Transactions on Graphics (TOG) 42, 6 (2023), 117. Philip-William Grassal, Malte Prinzler, Titus Leistner, Carsten Rother, Matthias NieÃŸner, and Justus Thies. 2022. Neural head avatars from monocular RGB videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1865318664. Yudong Guo, Keyu Chen, Sen Liang, Yong-Jin Liu, Hujun Bao, and Juyong Zhang. 2021. Ad-nerf: Audio driven neural radiance fields for talking head synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 5784 5794. Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander Holynski, and Angjoo Kanazawa. 2023. Instruct-nerf2nerf: Editing 3d scenes with instructions. arXiv preprint arXiv:2303.12789 (2023). Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. Advances in neural information processing systems 33 (2020), 68406851. Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. 2022. Video diffusion models. Advances in Neural Information Processing Systems 35 (2022), 86338646. Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. 2022. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868 (2022). Liwen Hu, Shunsuke Saito, Lingyu Wei, Koki Nagano, Jaewoo Seo, Jens Fursund, Iman Sadeghi, Carrie Sun, Yen-Chun Chen, and Hao Li. 2017. Avatar digitization from single image for real-time rendering. ACM Transactions on Graphics (ToG) 36, 6 (2017), 114. Justin Johnson, Alexandre Alahi, and Li Fei-Fei. 2016. Perceptual losses for real-time style transfer and super-resolution. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14. Springer, 694711. Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. 2020. Training generative adversarial networks with limited data. Advances in neural information processing systems 33 (2020), 1210412114. Tero Karras, Samuli Laine, and Timo Aila. 2019. style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 44014410. Tianye Li, Timo Bolkart, Michael. J. Black, Hao Li, and Javier Romero. 2017. Learning model of facial shape and expression from 4D scans. ACM Transactions on Graphics, (Proc. SIGGRAPH Asia) 36, 6 (2017), 194:1194:17. https://doi.org/10.1145/3130800. 3130813 Fig. 14. The visualize of components details. The conditional facial embedding is concatenation of encoded orthogonal rendering and the corresponding 3DMM expression coefficients. The size of out rendered map for I2I input is 32 512 512 and the output size is 3 512 512. The size of input normalized render is 3 128 128, and the output of the conditional encoder (embedding) is 1 256. rendering methods (NeRF, 3D Gaussian splatting, and GAN-based methods), our method suffers when generating image frames with out-of-distribution head poses and expressions, as shown in Figure 13 (the drawbacks show in realistic appearance generation and affects toonification, and tend to produce similar artifacts). Third, the input renderings are derived from 3DMM tracking algorithm, which is not capable of accurately describing detailed expressions, such as precise gaze control or pouting mouth. Finally, our method cannot achieve eye closure in some styles, because the data provided by the Text2Image module does not include eye-closing motions."
        },
        {
            "title": "5.2 Technology Abuse Claim",
            "content": "There is risk of misuse of our method, such as the utilization of racially racism or insulting prompts for the specific portrait synthesis and DeepFakes. We strongly oppose applying our work for such purposes as well as support the development of adversarial generative methods (or called DeepFake detection [Song et al. 2022a,b]). And in our paper, we further discuss the drawbacks of stylization synthesis technically, the readers could have better understanding of this field and know the limitations."
        },
        {
            "title": "5.3 Technical Details",
            "content": "The details of the components in our pipeline are shown in Figure 14. To reduce FLOPs during inference and training, the networks are composed of some light-weight convolution (Conv33) and MLP layers. The overall weight of the model is about 49Mb with each components."
        },
        {
            "title": "5.4 Conclusion",
            "content": "We present TextToon, real-time text toonification head avatar generation method from monocular videos. It takes conditional Tri-plane Gaussian deformation field to learn the Gaussian properties. Meanwhile, we focus on pipeline efficiency and consumer applications, leading to the development of real-time system, that 10 TextToon: Real-Time Text Toonify Head Avatar from Single Video SA Conference Papers 24, December 36, 2024, Tokyo, Japan pattern recognition. 2099121002. Ayush Tewari, Michael ZollhÃ¶fer, Pablo Garrido, Florian Bernard, Hyeongwoo Kim, Patrick PÃ©rez, and Christian Theobalt. 2018. Self-supervised multi-level face model learning for monocular reconstruction at over 250 hz. In Proceedings of the IEEE conference on computer vision and pattern recognition. 25492559. Justus Thies, Michael Zollhofer, Marc Stamminger, Christian Theobalt, and Matthias NieÃŸner. 2016. Face2face: Real-time face capture and reenactment of rgb videos. In Proceedings of the IEEE conference on computer vision and pattern recognition. 23872395. Luan Tran, Feng Liu, and Xiaoming Liu. 2019. Towards high-fidelity nonlinear 3D face morphable model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 11261135. Luan Tran and Xiaoming Liu. 2018. Nonlinear 3d face morphable model. In Proceedings of the IEEE conference on computer vision and pattern recognition. 73467355. Rotem Tzaban, Ron Mokady, Rinon Gal, Amit Bermano, and Daniel Cohen-Or. 2022. Stitch it in time: Gan-based facial editing of real videos. In SIGGRAPH Asia 2022 Conference Papers. 19. Can Wang, Ruixiang Jiang, Menglei Chai, Mingming He, Dongdong Chen, and Jing Liao. 2023a. Nerf-art: Text-driven neural radiance fields stylization. IEEE Transactions on Visualization and Computer Graphics (2023). Lizhen Wang, Zhiyuan Chen, Tao Yu, Chenguang Ma, Liang Li, and Yebin Liu. 2022. Faceverse: fine-grained and detail-controllable 3d face morphable model from hybrid dataset. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2033320342. Lizhen Wang, Xiaochen Zhao, Jingxiang Sun, Yuxiang Zhang, Hongwen Zhang, Tao Yu, and Yebin Liu. 2023b. StyleAvatar: Real-time Photo-realistic Portrait Avatar from Single Video. arXiv preprint arXiv:2305.00942 (2023). Yunjie Wu, Yapeng Meng, Zhipeng Hu, Lincheng Li, Haoqian Wu, Kun Zhou, Weiwei Xu, and Xin Yu. 2024. Text-Guided 3D Face Synthesis-From Generation to Editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 12601269. Jun Xiang, Xuan Gao, Yudong Guo, and Juyong Zhang. 2023. FlashAvatar: High-Fidelity Digital Avatar Rendering at 300FPS. arXiv preprint arXiv:2312.02214 (2023). Yuelang Xu, Benwang Chen, Zhe Li, Hongwen Zhang, Lizhen Wang, Zerong Zheng, and Yebin Liu. 2024. Gaussian Head Avatar: Ultra High-fidelity Head Avatar via Dynamic Gaussians. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Kai Yang, Hong Shang, Tianyang Shi, Xinghan Chen, Jingkai Zhou, Zhongqian Sun, and Wei Yang. 2023b. ASM: Adaptive Skinning Model for High-Quality 3D Face Modeling. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 2070820717. Shuai Yang, Liming Jiang, Ziwei Liu, and Chen Change Loy. 2022. Vtoonify: Controllable high-resolution portrait video style transfer. ACM Transactions on Graphics (TOG) 41, 6 (2022), 115. Shuai Yang, Liming Jiang, Ziwei Liu, and Chen Change Loy. 2023a. Styleganex: Styleganbased manipulation beyond cropped aligned faces. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 2100021010. Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change Loy. 2024. FRESCO: SpatialarXiv preprint Temporal Correspondence for Zero-Shot Video Translation. arXiv:2403.12962 (2024). Junzhe Zhang, Yushi Lan, Shuai Yang, Fangzhou Hong, Quan Wang, Chai Kiat Yeo, Ziwei Liu, and Chen Change Loy. 2023a. Deformtoon3d: Deformable neural radiance fields for 3d toonification. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 91449154. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023b. Adding Conditional Control to Text-to-Image Diffusion Models. Yufeng Zheng, Wang Yifan, Gordon Wetzstein, Michael Black, and Otmar Hilliges. 2023. Pointavatar: Deformable point-based head avatars from videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 21057 21067. Wojciech Zielonka, Timo Bolkart, and Justus Thies. 2022. Towards metrical reconstruction of human faces. In European Conference on Computer Vision. Springer, 250269. Wojciech Zielonka, Timo Bolkart, and Justus Thies. 2023. Instant volumetric head avatars. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 45744584. Shanchuan Lin, Linjie Yang, Imran Saleemi, and Soumyadip Sengupta. 2022. Robust highresolution video matting with temporal guidance. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 238247. Feng-Lin Liu, Shu-Yu Chen, Yu-Kun Lai, Chunpeng Li, Yue-Ren Jiang, Hongbo Fu, and Lin Gao. 2022. Deepfacevideoediting: Sketch-based deep editing of face videos. ACM Transactions on Graphics (TOG) 41, 4 (2022), 116. Kangfu Mei and Vishal Patel. 2023. Vidm: Video implicit diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 37. 91179125. Anish Mittal, Anush Moorthy, and Alan Bovik. 2011. Blind/referenceless image spatial quality evaluator. In 2011 conference record of the forty fifth asilomar conference on signals, systems and computers (ASILOMAR). IEEE, 723727. Thomas MÃ¼ller, Alex Evans, Christoph Schied, and Alexander Keller. 2022. Instant neural graphics primitives with multiresolution hash encoding. arXiv preprint arXiv:2201.05989 (2022). Thomas Neumann, Kiran Varanasi, Stephan Wenger, Markus Wacker, Marcus Magnor, and Christian Theobalt. 2013. Sparse localized deformation components. ACM Transactions on Graphics (TOG) 32, 6 (2013), 110. Roy Or-El, Xuan Luo, Mengyi Shan, Eli Shechtman, Jeong Joon Park, and Ira Kemelmacher-Shlizerman. 2022. Stylesdf: High-resolution 3d-consistent image and geometry generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1350313513. Mohit Mendiratta Pan, Mohamed Elgharib, Kartik Teotia, Ayush Tewari, Vladislav Golyanik, Adam Kortylewski, Christian Theobalt, et al. 2023. Avatarstudio: Textdriven editing of 3d dynamic human head avatars. arXiv preprint arXiv:2306.00547 (2023). Shenhan Qian, Tobias Kirschstein, Liam Schoneveld, Davide Davoli, Simon Giebenhain, and Matthias NieÃŸner. 2023. GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians. arXiv preprint arXiv:2312.02069 (2023). Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning. PMLR, 87488763. Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and Daniel Cohen-Or. 2021. Encoding in style: stylegan encoder for image-toimage translation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 22872296. Daniel Roich, Ron Mokady, Amit Bermano, and Daniel Cohen-Or. 2021. Pivotal Tuning for Latent-based Editing of Real Images. ACM Trans. Graph. (2021). Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 10684 10695. Ruizhi Shao, Jingxiang Sun, Cheng Peng, Zerong Zheng, Boyao Zhou, Hongwen Zhang, and Yebin Liu. 2023. Control4d: Dynamic portrait editing by learning 4d gan from 2d diffusion-based editor. arXiv preprint arXiv:2305.20082 (2023). Zhijing Shao, Zhaolong Wang, Zhuang Li, Duotun Wang, Xiangru Lin, Yu Zhang, Mingming Fan, and Zeyu Wang. 2024. Splattingavatar: Realistic real-time human avatars with mesh-embedded gaussian splatting. arXiv preprint arXiv:2403.05087 (2024). Xiaolong Shen, Jianxin Ma, Chang Zhou, and Zongxin Yang. 2024. Controllable 3d face generation with conditional style code diffusion. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 48114819. Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. 2023. Mvdream: Multi-view diffusion for 3d generation. arXiv preprint arXiv:2308.16512 (2023). Luchuan Song, Zheng Fang, Xiaodan Li, Xiaoyi Dong, Zhenchao Jin, Yuefeng Chen, and Siwei Lyu. 2022a. Adaptive face forgery detection in cross domain. In European Conference on Computer Vision. Springer, 467484. Luchuan Song, Xiaodan Li, Zheng Fang, Zhenchao Jin, YueFeng Chen, and Chenliang Xu. 2022b. Face forgery detection via symmetric transformer. In Proceedings of the 30th ACM International Conference on Multimedia. 41024111. Luchuan Song, Pinxin Liu, Lele Chen, Guojun Yin, and Chenliang Xu. 2024. Tri2arXiv:2401.09386 [cs.CV] plane: Thinking Head Avatar via Feature Pyramid. https://arxiv.org/abs/2401.09386 Luchuan Song, Guojun Yin, Zhenchao Jin, Xiaoyi Dong, and Chenliang Xu. 2023. Emotional listener portrait: Realistic listener motion simulation in conversation. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV). IEEE, 20782 20792. Luchuan Song, Guojun Yin, Bin Liu, Yuhui Zhang, and Nenghai Yu. 2021. Fsft-net: face transfer video generation with few-shot views. In 2021 IEEE International Conference on Image Processing (ICIP). IEEE, 35823586. Jingxiang Sun, Xuan Wang, Yichun Shi, Lizhen Wang, Jue Wang, and Yebin Liu. 2022. Ide3d: Interactive disentangled editing for high-resolution 3d-aware portrait synthesis. ACM Transactions on Graphics (ToG) 41, 6 (2022), 110. Jingxiang Sun, Xuan Wang, Lizhen Wang, Xiaoyu Li, Yong Zhang, Hongwen Zhang, and Yebin Liu. 2023. Next3d: Generative neural texture rasterization for 3d-aware head avatars. In Proceedings of the IEEE/CVF conference on computer vision and"
        }
    ],
    "affiliations": [
        "Bytedance, USA",
        "University of Rochester, USA"
    ]
}