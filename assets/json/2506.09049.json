{
    "paper_title": "VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning",
    "authors": [
        "Li Kang",
        "Xiufeng Song",
        "Heng Zhou",
        "Yiran Qin",
        "Jie Yang",
        "Xiaohong Liu",
        "Philip Torr",
        "Lei Bai",
        "Zhenfei Yin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Coordinating multiple embodied agents in dynamic environments remains a core challenge in artificial intelligence, requiring both perception-driven reasoning and scalable cooperation strategies. While recent works have leveraged large language models (LLMs) for multi-agent planning, a few have begun to explore vision-language models (VLMs) for visual reasoning. However, these VLM-based approaches remain limited in their support for diverse embodiment types. In this work, we introduce VIKI-Bench, the first hierarchical benchmark tailored for embodied multi-agent cooperation, featuring three structured levels: agent activation, task planning, and trajectory perception. VIKI-Bench includes diverse robot embodiments, multi-view visual observations, and structured supervision signals to evaluate reasoning grounded in visual inputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a two-stage framework that fine-tunes a pretrained vision-language model (VLM) using Chain-of-Thought annotated demonstrations, followed by reinforcement learning under multi-level reward signals. Our extensive experiments show that VIKI-R significantly outperforms baselines method across all task levels. Furthermore, we show that reinforcement learning enables the emergence of compositional cooperation patterns among heterogeneous agents. Together, VIKI-Bench and VIKI-R offer a unified testbed and method for advancing multi-agent, visual-driven cooperation in embodied AI systems."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 9 4 0 9 0 . 6 0 5 2 : r VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning Li Kang1,2, Xiufeng Song1,2, Heng Zhou2,3, Yiran Qin2,5, Jie Yang5, Xiaohong Liu1, Philip Torr4, Lei Bai2, Zhenfei Yin4 1Shanghai Jiao Tong University 2Shanghai Artificial Intelligence Laboratory 3University of Science and Technology of China 4University of Oxford 5The Chinese University of Hong Kong, Shenzhen {faceong02, sparklexfantasy, hengzzzhou}@gmail.com Equal contribution Corresponding author https://faceong.github.io/VIKI-R/ Figure 1: Embodied multi-agent cooperation involves two key aspects: (1) cross-embodiment collaboration, where different embodiments are required for different tasks (e.g., washing requires humanoid, while only wheeled robots can fetch from high cabinets); and (2) efficient coordination, where agents work in parallel (e.g., multiple arms passing apples while humanoid washes them) to improve overall efficiency. To support such fine-grained teamwork, we propose VIKI-Bench , which structures the process into three levels of visual reasoning: Level 1 agent activation, Level 2 task planning, and Level 3 trajectory perception, aiming to realize an embodied multi-agent system."
        },
        {
            "title": "Abstract",
            "content": "Coordinating multiple embodied agents in dynamic environments remains core challenge in artificial intelligence, requiring both perception-driven reasoning and scalable cooperation strategies. While recent works have leveraged large language models (LLMs) for multi-agent planning, few have begun to explore vision-language models (VLMs) for visual reasoning. However, these VLM-based approaches remain limited in their support for diverse embodiment types. In this work, we introduce VIKI-Bench , the first hierarchical benchmark tailored for embodied multi-agent cooperation, featuring three structured levels: agent activation, task planning, and trajectory perception. VIKI-Bench includes diverse robot embodiments, multi-view visual observations, and structured supervision signals to evaluate reasoning grounded in visual inputs. To demonstrate the utility of VIKIBench , we propose VIKI-R , two-stage framework that fine-tunes pretrained vision-language model (VLM) using Chain-of-Thought annotated demonstrations, followed by reinforcement learning under multi-level reward signals. Our extensive experiments show that VIKI-R significantly outperforms baselines method across all task levels. Furthermore, we show that reinforcement learning enables the emergence of compositional cooperation patterns among heterogeneous agents. Together, VIKI-Bench and VIKI-R offer unified testbed and method for advancing multi-agent, visual-driven cooperation in embodied AI systems."
        },
        {
            "title": "Introduction",
            "content": "In the science-fiction film I, Robot [45], the super-computer VIKI orchestrates thousands of NS-5 robots, illustrating the extraordinary coordination capabilities of heterogeneous robotic agents. This fictional depiction highlights fundamental challenge in artificial intelligence: enabling multiple embodied agents to collaborate in dynamic, real-world environments. As illustrated in Fig. 1, addressing this challenge is critical for advancing multi-agent systems capable of achieving effective, large-scale coordination: (1) Real-world tasks often necessitate specialized embodimentsfor instance, reaching high cabinets may call for robot with extended reach, while delicate tasks demand manipulators with fine-grained control. (2) Cooperative behaviors substantially enhance task efficiency through parallelization and mutual assistance. Recent advances have demonstrated the potential of large language models (LLMs) in enabling multi-agent planning [6, 8, 52]. While these LLM-based approaches have made significant progress in high-level coordination, only few works have explored the use of vision-language models (VLMs) for perception-driven reasoning [24, 43, 53]. However, existing VLM-based methods remain limited by the lack of embodiment diversity. As result, the ability to reason about visual observations in heterogeneous multi-agent settings remains an underexplored challenge. To address these gaps, we introduce VIKI-Bench , comprehensive benchmark for evaluating collaborative capabilities in embodied multi-agent systems. As illustrated in Fig. 1, VIKI-Bench is designed around three levels of task: Agent Activation, Task Planning, and Trajectory Perception. Each task provides multi-view visual input and incorporates diverse set of heterogeneous robots. Moreover, VIKI-Bench provides multi-dimensional evaluation framework that assesses execution feasibility, task completion and planning efficiency. To the best of our knowledge, VIKI-Bench is the first comprehensive benchmark specifically designed to evaluate the reasoning capabilities of VLMs in hierarchical embodied multi-agent cooperation. To advance reasoning capabilities in the multi-agent system, we introduce VIKI-R , VLM-based framework that fosters reasoning abilities in multi-agent cooperation. Inspired by [12, 25, 39], our approach first grounds pretrained VLM in task understanding through Chain-of-Thought annotations, then optimizes it via Reinforcement Learning, leveraging hierachical supervision in VIKI-Bench . Extensive experimental results demonstrate that VIKI-R significantly outperforms baseline methods across all three task levels, highlighting the effectiveness of the proposed approach. In summary, the main contributions of this paper are as follows: We introduce VIKI-Bench , the first hierarchical benchmark for embodied multi-agent cooperation, which consists of three structured task levels: agent activation, high-level task planning, and low-level trajectory perception. The benchmark features heterogeneous robot types, multi-view visual inputs, and structured supervision signals to enable comprehensive evaluation. We propose VIKI-R , two-stage learning framework that enhances visual reasoning capabilities in embodied multi-agent systems by using hierarchical reward signals to learn structured reasoning across diverse tasks, enabling generalizable cooperation in complex environments. Extensive experimental results demonstrates the effectiveness of VIKI-R in VIKI-Bench . Our analysis highlights the importance of hierarchical supervision and reveals how reinforcement learning facilitates the emergence of compositional collaboration patterns in embodied environments. 2 Table 1: Comparison to similar embodied benchmarks. We compare VIKI-Bench to embodied AI benchmarks, focusing on natural language and multi-agent collaboration tasks. [Keys: Views: EGO (Ego-centric view), GL (Global view). H.E.: Coordination among Heterogeneous Embodiments. ] H.E. Tasks Num"
        },
        {
            "title": "Views",
            "content": "Overcooked [7] RoCo [27] WAH [31] Co-ELA [52] FurnMove [17] PARTNR [8] RoboCasa [28] VIKI-Bench (Ours) 2D 3D 3D 3D 3D 3D 3D 3D - - - - EGO - EGO EGO, GL 4 6 1,211 44 30 100,000 100 23,"
        },
        {
            "title": "2 Related Work",
            "content": "Embodied Multi-Agent Cooperation Real-world embodied tasks often require cooperation among multiple agents. Existing studies [2, 13, 35, 51, 59, 34, 57, 61, 60, 32] have explored this problem in various application domains. Research focuses on multi-agent task allocation [22, 29, 42] and joint decision-making [41, 52]. significant body of recent work [6, 14, 20, 28, 36, 55, 62] leverages large language models (LLMs) to handle high-level reasoning and planning. Some recent works leverage video generation models [50, 49, 48, 47, 33, 5] to construct multi-agent world models [53], achieving promising results on specific tasks. However, these approaches lack visual grounding, limiting their ability to reason about spatial constraints and perceptual affordances. While few recent efforts [43, 56, 58] incorporate vision-language models (VLMs) to obtain more grounded understanding of the environment, research on heterogeneous multi-agent cooperation remains sparseparticularly in settings requiring fine-grained visual reasoning and embodied perception. In contrast, our work incorporates both agent heterogeneity and visual reasoning to support complex, perception-driven collaboration. Visual Reasoning Visual reasoning requires vision-language models (VLMs) to interpret and reason over visual observations to perform complex tasks. It has been applied in areas such as geometric problem-solving [11, 38, 54], robotic [15, 18] and scientific research [26, 19]. Previous work has explored enhancing visual reasoning in VLMs through multi-stage supervision. For example, LLaVACoT [46] applies multi-stage supervised fine-tuning (SFT) with chain-of-thought [44] prompting. With the introduction of rule-based reinforcement learning (RL) method, DeepSeek-R1 [12] demonstrates significant improvements in reasoning performance. Recent works [23, 25, 39] incorporate RL to further enhance visual reasoning capabilities. Our work shows that R1-style methods perform better in multi-agent embodied visual reasoning tasks. Embodied multi-agent benchmarks Recent research [1, 7, 52, 8, 28] has developed several embodied multi-agent benchmarks to evaluate collaborative behaviors. In 2D environments, LLMCo [1] and Overcooked [7] study coordination in game play, but the simplified 2D settings limit their abilities in physical interaction. For 3D environments, thread of work has focused on language-guided cooperative planning for embodied tasks. For instance, WAH [31] examines social intelligence in household scenarios. PARTNR [8] evaluates visual planning and reasoning under LLM-based evaluation. Other benchmarks target multi-agent manipulation. RocoBench [27] conducts object interaction tasks within tabletop environment. FurnMove [17] requires collaboration on synchronized furniture arrangement. Building upon these advances, our work introduces threelevel hierarchical visual reasoning benchmark that bridges both planning and manipulation domains, coupled with structured checker that incorporates spatial-temporal constraints into the generation pipeline to minimize infeasible plans. 3 Figure 2: Overview of VIKI-Bench . VIKI-Bench is hierarchical benchmark for evaluation on multiagent embodied cooperation, featuring visual reasoning tasks in three levels: (1) Agent Activation, where robots are selected based on the scene image and the task context; (2) Task Planning, where structured multi-agent action plan is generated, verified, and refined; and (3) Trajectory Perception, where the fine-grained motion trajectory of each agent is tracked from egocentric views. The benchmark involves diverse robot types and complex 3D environments, with multiple metrics for quantitative evaluation."
        },
        {
            "title": "3.1 Overview",
            "content": "We introduce VIKI-Bench , hierarchical benchmark for studying visual reasoning in embodied multi-agent collaboration, as illustrated in Fig. 2. VIKI-Bench covers three levels of tasks: (1) Agent Activation, which selects appropriate agents to activate by considering the task description and the scene image; (2) Task Planning, which requires generating an ordered sequence of action primitives of multiple agents; and (3) Trajectory Perception, which involves predicting the motion trajectories of all agents. Each task includes language instruction, with global visual observations provided for the first two levels, and egocentric views used for the trajectory perception level. Spanning thousands of tasks across heterogeneous robot morphologies and diverse household-to-industrial layouts, VIKI-Bench offers concise yet comprehensive benchmark for scalable multi-agent cooperation."
        },
        {
            "title": "3.2.1 Agent Activation",
            "content": "We formulate the agent activation task as visual reasoning problem, where the task allocator selects set of appropriate robots among all agents to complete the task. Each sample is formatted as an instruction-question pair, consisting of an image observation and task instruction I. The expected answer is set of selected agents = {rj}, [1, ] chosen from the visible agent pool Avisible based on embodiment reasoning and task affordance. To generate ground truth labels, we construct task-specific templates that specify which agent types are required or not required for solving the task, given the task goal and environmental context. These templates are grounded in embodiment rules and capability-based constraints (e.g., mobile agents for navigation, dual-arm agents for bimanual manipulation). To encourage interpretable reasoning, we adopt chain-of-thought format in which the model is expected to: (1) analyze the task requirements, (2) visually identify the robots present, (3) assess each robots suitability, and (4) conclude the final selection. For data generation, we employ GPT-4o [30] 4 as the task allocator gact, prompting it with the task template and the corresponding image context. The activation result is then obtained as = gact(I, O). verification module Cact is used to automatically check whether the generated labels conform to embodiment-grounded task constraints, followed by human inspection to correct failure cases and ensure overall label quality."
        },
        {
            "title": "3.2.2 Task Planning",
            "content": "We construct task planning data as question-answer pairs according to the environment and specific instructions. To describe high-level operations of agents in the environment, we design basic primitive set (e.g., move, grasp, etc.) as the atomic operations of all agents. The planning answer is designed as sequence of action descriptions = {a1, a2, ...aN }, where is the length of the sequence. Each action description is formed as ai = (ri, ti, pi, di), where ri, ti, pi, di denotes the agent, the timestep, the primitive and the destination of action ai, respectively. To generate effective planning in versatile environments, we use GPT-4o as the plan generator gplan and introduce an iterative refinement process. Given an instruction I, the corresponding observation O, and the primitive set , the generator first decomposes the instruction into set of goals G, and generates possible planning result A0, as A0 = gplan(I, O, ). Then, an Action Checker verifies the feasibility of each action based on the rules of primitives, followed by World Simulator recording the position and status of interactive entities in the environment. Subsequently, Plan Refiner checks the completion of the goals. For any failure in planning, the refiner provides detailed feedback as an additional instruction, which is concatenated with the original instruction for the generator to revise the planning result until success. This procedure is formulated as follows. Algorithm 1 Iterative Refinement Process Require: Plan Generator gplan, Instruction I0, Goals G, Observation O, Primitives Ensure: Successful Planning 1: Success alse 2: I0 3: while Success do 4: 5: 6: 7: 8: 9: 10: end while gplan(I, O, ) Act_success C(act), act Status S(A) Goal_success is_successf ul(Status, goal), goal Success Act_success Goal_success + R(Act_success, Goal_success) Action feasibility check Goal check Update feedback instruction"
        },
        {
            "title": "3.2.3 Trajectory Perception",
            "content": "We formulate trajectory perception in multi-agent environments as spatial keypoint prediction problem, where the model predicts motion trajectories from egocentric observations based on the task instruction. Unlike prior work [8, 18] that focuses solely on the observing agent, our setting requires predicting both the trajectory of the ego agent and those of other visible agents to facilitate collaboration, which are referred as the ego-trajectory and partner-trajectories, respectively. Given an egocentric RGB image and an action description ai = (ri, ti, pi, di) indicating the ongoing execution, the model predicts set of 2D trajectories = {Tk}, [1, ], where is the number of agents in the scene, and Lk = {(xj, yj)}L j=1 denotes temporally ordered spatial motion for agent rk in coordinate sequences. To construct these samples, we sample diverse egocentric observations from simulated multi-agent scenes with the corresponding task descriptions. Based on the egocentric observations and detailed instructions for each visible agent, the trajectory of each agent is manually annotated by formulating feasible motion path in the form of coordinate sequences. All data undergoes human verification to ensure temporal consistency and spatial alignment with the instruction and environment. 5 Figure 3: Framework of VIKI-R . We adopted supervised fine-tuning (SFT) and reinforcement finetuning on the VIKI dataset, incorporating format and accuracy rewards to optimize the policy model."
        },
        {
            "title": "3.3 Data Statistics",
            "content": "The VIKI benchmark comprises over 20,000 multi-agent task samples across 100 diverse scenes derived from the RoboCasa [28] based on ManiSkill3 [40], each with fine-grained object configurations and varied spatial layouts. The dataset involves 6 types of heterogeneous embodied agents (e.g., humanoids, wheeled arms, quadrupeds) interacting with over 1,000 unique asset combinations. Each scene provides both global and egocentric camera views to support perception and planning. More details are provided in Supplementary Section C."
        },
        {
            "title": "4.1 Overview",
            "content": "We introduce VIKI-R , two-stage fine-tuning framework that endows visionlanguage models with robust visual reasoning abilities, as shown in Fig. 3. In the first stage, SFT-based Warmup, the model undergoes supervised fine-tuning on high-quality Chain-of-Thought (CoT) annotations, optimizing the likelihood of both intermediate reasoning steps and final answers. This stage instructs the model to acquire domain-specific reasoning patterns. In the second stage, Reinforcement Fine-Tuning, the policy is refined using the Grouped Relative Proximal Optimization (GRPO) algorithm [37]. For each visualquestion pair, grouped candidate answers are sampled and evaluated using composite reward function based on answer format and correctness. Standardized advantages are then computed to guide policy updates under KL-divergence constraint, ensuring stable and consistent improvement."
        },
        {
            "title": "4.2 Training Objectives",
            "content": "SFT-based Warmup In the first phase, we employ Supervised Fine-Tuning (SFT) with data annotated with Chain-of-Thought (CoT) reasoning process. Each training instance is denoted as (x, q, r, a), where represents the visual input, the associated task, the intermediate reasoning steps, and the final answer. The SFT objective maximizes the joint likelihood of the reasoning and answer tokens conditioned on the input: LSFT = E(x,q,r,a)D (cid:88) t=1 log πθ (cid:0)yt x, q, y<t (cid:1), (1) where is the CoT-annotated dataset, = [r, a] is the concatenated sequence of reasoning and answer tokens, and πθ denotes the models token distribution. 6 Reinforcement Fine-Tuning Starting from πCoT, we sample group of candidate outputs {ai} per input = (x, q). Let ri be the reward of ai and r, σr its sample mean and standard deviation. We form relative advantages and update the policy by maximizing Ai = ri σr J(θ) = Es (cid:104) (cid:88) i=1 (cid:105) Ai log πθ(ai s) λ KL(cid:0)πθ (cid:13) (cid:13) πCoT (cid:1), (2) (3) where πθ denotes the learned policy, πCoT is the initial policy obtained via Chain-of-Thought prompting and λ is regularization coefficient controlling the KL penalty."
        },
        {
            "title": "4.3 Reward Design",
            "content": "To guide the model towards both structured output and task accuracy, we formulate the overall reward into format reward and task-specific accuracy reward, as: = λ1 Rformat + λ2 Racc, (4) where Rformat enforces the output format and Racc corresponds to the three subtask rewards, as defined below. λ1 and λ2 refer to the weights of both rewards, respectively. Format Reward To encourage explicit reasoning, we assign binary format reward: the model receives 1 point if it correctly encloses the intermediate reasoning steps within <think>. . . </think> and the final answer within <answer>. . . </answer>, and 0 otherwise. By enforcing these tags, we prompt the model to articulate its chain-of-thought before delivering the answer, thereby improving interpretability and guiding systematic reasoning. Agent Activation Reward We define the agent activation reward as an exact-match indicator between the predicted agent set Spred and the ground-truth set Sgt: RL1 acc = (cid:26)1, if Spred Sgt, 0, otherwise. (5) Task Planning Reward While multiple feasible plans may exist, we define the reward to favor efficient solutions. Specifically, predicted plan only receives the reward if it is feasible and its length does not exceed that of the ground-truth plan Ngt. Let (A) denote the length of the predicted action sequence A, the task planning reward is defined as: RL2 acc = (cid:26)1, if is feasible and (A) Ngt, 0, otherwise. (6) Details on how plan feasibility is checked are provided in Supplementary Section C. t=1 and G(k) = {g(k) Trajectory Perception Reward Let (k) = {p(k) }T t=1 denote the predicted and ground-truth trajectories for agent k, respectively. To evaluate trajectory prediction quality for each agent k, we compute three normalized standard geometric distance metrics between the predicted trajectory and the ground-truth trajectory: Root Mean Square Error (RMSE, denoted as ˆdRMSE), Hausdorff Distance (HD, denoted as ˆdHD)[16], and Discrete Fréchet Distance (DFD, denoted as ˆdDFD)[10]. Since smaller distances indicate better alignment between predicted and ground-truth trajectories, we transform the distance ˆd into reward-like score using the transformation = 1 ˆd. The final trajectory perception reward is defined as: }T RL acc = 1 3K (cid:88) (cid:16) k=1 RMSE + r(k) r(k) HD + r(k) DFD (cid:17) , (7) Table 2: Performance comparison across the three hierarchical task levels of VIKI-Bench. Best scores are highlighted in bold, and the second-best scores are underlined. Method VIKI-L1 VIKI-L2 VIKI-L3 ACCID ACCID ACCOOD ACCAVG RMSE HD DFD AVG Closed-Source Models GPT-4o Claude-3.7-Sonnet Gemini-2.5-Flash-preview Open-Source Models Qwen2.5-VL-72B-Instruct Qwen2.5-VL-32B-Instruct Llama-3.2-11B-Vision Qwen2.5VL-3B-Instruct Zero-Shot +Ans SFT +VIKI-R-Zero +VIKI-R Qwen2.5VL-7B-Instruct Zero-Shot +Ans SFT +VIKI-R-Zero +VIKI-R 18.40 12.40 31.40 11.31 9.50 0.40 1.95 35.29 20.40 74.10 4.26 72.20 93.59 93."
        },
        {
            "title": "5.1 Experimental Setup",
            "content": "22.56 19.44 20.00 8.40 3.60 0.50 0.22 81.06 0.00 93.61 0.44 96.89 0.17 95.22 10.02 0.57 10.51 1.20 0.00 0. 0.00 30.71 0.00 32.11 0.00 25.62 0.00 33.25 17.50 11.82 16.17 5.49 2.15 0.30 0.13 60.74 0.00 68.78 0.26 68.13 0.10 69. 100.80 283.31 453.89 81.31 88.48 192.69 96.22 74.70 80.36 75.69 81.93 65.32 67.42 64.87 115.34 323.53 519.14 94.62 99.80 223. 114.93 90.28 95.36 90.25 103.82 81.20 85.30 79.23 131.05 346.88 540.80 113.15 119.78 231.85 130.98 102.26 120.27 103.65 112.91 90.89 95.32 89. 115.73 317.91 504.61 96.36 102.69 216.04 114.04 89.08 98.66 89.86 99.55 79.14 82.68 77.82 Training Paradigms and Baselines To assess the impact of different training strategies on performance and generalization, we compare the following methods: (1)Ans-SFT: supervised fine-tuning (SFT) approach focusing solely on answer generation. (2)VIKI-R-Zero: reinforcement learning (RL) variant that applies GRPO directly, without any prior CoT activation. (3)VIKI-R: our two-phase schemefirst SFT on small CoT-annotated subset, followed by GRPO-based RL. All variants use Qwen2.5-VL-Instruct [4] as the base model in both 3B and 7B sizes to study the effect of model scale. For comprehensive comparison, we include open-source models[4, 21]and leading closed-source systems GPT-4o [30], gemini-2.5-flash-preview [9]) and claude-3.7-sonnet[3] as baselines. Detailed hyperparameters and additional setup information are provided in Supplementary Section C. Evaluation Metrics We adopted task-specific metrics to evaluate performance across the three stages of the VIKI-Bench . For agent activation (VIKI-L1), we report classification accuracy based on whether the selected agents match the ground truth. For task planning (VIKI-L2), we evaluate accuracy based on whether the predicted plan is both feasible and no longer than the ground-truth plan, reflecting correctness and execution efficiency. For trajectory perception (VIKI-L3), we evaluate the predicted trajectories using RMSE, Hausdorff Distance (HD) [16] and Discrete Fréchet Distance (DFD) [10], which measure spatial and temporal alignment with ground-truth motion paths."
        },
        {
            "title": "5.2 Overall Performance Analysis",
            "content": "Tab. 2 highlights three main observations. First, when comparing open-source and closed-source models under zero-shot evaluation (without any VIKI-Bench training), closed-source models hold clear advantage. Among closed-source systems, Gemini-2.5-Flash-preview achieves the highest agent activation accuracy, while GPT-4o excels at trajectory perception. In contrast, both Gemini and Claude exhibit almost no trajectory-prediction capability. Second, the model scale critically affects open-source VLM performance. The 72B-parameter Qwen2.5-VL matches or even surpasses some closed-source baselines on perception metrics, but reducing the model to 32B parameters incurs substantial drops in both planning accuracy and trajectory quality. This underscores the importance of model capacity for handling complex multi-agent visual reasoning. Third, our two-stage fine-tuning framework VIKI-R outperforms purely supervised Ans-SFT and VIKI-R-zero. While Ans-SFT yields strong in-domain improvements, it fails to generalize to out-of-domain scenarios. These results confirm that integrating reinforcement learning substantially enhances visual reasoning capabilities in hierarchical multi-agent cooperation."
        },
        {
            "title": "5.3 Feedback-Driven Iterative Refinement",
            "content": "We compare two planning strategies: standard sampling (up to attempts without guidance) and feedback-driven sampling (injecting feedback between attempts). Tab. 3 demonstrates the impact of feedback-driven sampling. By injecting feedback between failed attempts, GPT-4o achieves improvements of 1.9% at pass@3 and 3.6% at pass@6. Claude-3-7-Sonnet sees gains of 1.5% and 2.3% and Gemini-2.5-Flash records increases of 1.8% and 3.0%. On average, feedback-driven sampling boosts pass@3 by 1.7% and pass@6 by 3.0%, highlighting that iterative feedback effectively steers the model away from repeated mistakes and yields more reliable plans. Table 3: Task planning success rates (%) under two sampling strategies. pass@k denotes the probability of obtaining at least one valid plan within independent attempts, while pass@k_fb is measured when feedback is appended after each failed attempt."
        },
        {
            "title": "Model",
            "content": "pass@1 pass@3_fb pass@3 pass@6_fb pass@6 GPT-4o Claude-3.7-Sonnet Gemini-2.5-Flash-preview 18.4 12.4 31.4 20.6 13.9 33.4 18.7 12.4 31.6 22.3 14.8 34.7 18.7 12.5 31."
        },
        {
            "title": "5.4 Ablation Study",
            "content": "Tab. 4 demonstrates the impact of step penalty. By incorporating constraint-based penalty, VIKI-R achieves improvements by 39.7% and 88.0% in the accuracy of out-of-domain and in-domain tasks, respectively. These results underscore the effectiveness of the step penalty in generalization and execution accuracy. Besides, the steps of action length is reduced by an average of 1.92 steps, highlighting the critical role of penalizing unnecessary steps to enforce concise planning. Overall, the step penalty promotes more transferable and efficient planning strategies. Table 4: Effect of the step penalty on 1,000 challenging reasoning tasks sampled from both the out-of-domain (OOD-H) and in-domain (ID-H) splits. Steps measures the average difference between the action length of predicted plan and the ground-truth plan."
        },
        {
            "title": "Variant",
            "content": "ACCOOD-H ACCID-H Steps VIKI-R (with step penalty) VIKI-R (without step penalty) 46.8 7.1 96.0 8.0 0.05 1.97 5."
        },
        {
            "title": "Insights from Training",
            "content": "Throughout our experiments, we identified several key behaviors that illustrate both the strengths and limitations of GRPO in our hierarchical multi-agent setting. Dependence on Base Policy Quality The effectiveness of GRPO depends critically on the competence of the pretrained policy. In VIKI-L2 planning, the zero-shot model produces almost no valid plans, and VIKI-R-Zero yields negligible improvement. By contrast, in the VIKI-L1 activation and VIKIL3 perception tasks where the base policy already generates some correct responsesGRPO delivers clear performance gains. These observations indicate that reinforcement-based fine-tuning requires an initial set of correct rollouts to guide effective policy updates. Figure 4: Response length of the Qwen2.5-VL-3B/7B-Instruct model at training time. Evolution of Response Length We tracked the average token length of model outputs during VIKI-R training in Fig. 4. In the early stages, output length decreases as the model prioritizes format compliance to secure the format reward. Once format accuracy saturates, the policy shifts focus 9 toward maximizing task correctness, and output length gradually increases to include the necessary reasoning details."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper presents VIKI-Bench , hierarchical benchmark for evaluating vision-language models in embodied multi-agent collaboration. We further introduce VIKI-R , two-stage framework that combines supervised pretraining and reinforcement learning to solve multi-agent tasks across activation, planning, and perception levels. While our study focuses on simulated environments, extending this framework to real-world settings and dynamic agents remains promising future work."
        },
        {
            "title": "References",
            "content": "[1] Saaket Agashe, Yue Fan, Anthony Reyna, and Xin Eric Wang. Llm-coordination: evaluating and analyzing multi-agent coordination abilities in large language models. arXiv preprint arXiv:2310.03903, 2023. [2] Xing An, Celimuge Wu, Yangfei Lin, Min Lin, Tsutomu Yoshinaga, and Yusheng Ji. Multi-robot systems and cooperative object transport: Communications, platforms, and challenges. IEEE Open Journal of the Computer Society, 4:2336, 2023. [3] AI Anthropic. The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card, 2024. [4] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [5] Amir Bar, Gaoyue Zhou, Danny Tran, Trevor Darrell, and Yann LeCun. Navigation world models. arXiv preprint arXiv:2412.03572, 2024. [6] Xiaohe Bo, Zeyu Zhang, Quanyu Dai, Xueyang Feng, Lei Wang, Rui Li, Xu Chen, and Ji-Rong Wen. Reflective multi-agent collaboration based on large language models. Advances in Neural Information Processing Systems, 37:138595138631, 2024. [7] Micah Carroll, Rohin Shah, Mark Ho, Tom Griffiths, Sanjit Seshia, Pieter Abbeel, and Anca Dragan. On the utility of learning about humans for human-ai coordination. Advances in neural information processing systems, 32, 2019. [8] Matthew Chang, Gunjan Chhablani, Alexander Clegg, Mikael Dallaire Cote, Ruta Desai, Michal Hlavac, Vladimir Karashchuk, Jacob Krantz, Roozbeh Mottaghi, Priyam Parashar, et al. Partnr: benchmark for planning and reasoning in embodied multi-agent tasks. arXiv preprint arXiv:2411.00081, 2024. [9] Google DeepMind. Gemini 2.5. gemini-model-thinking-updates-march-2025, 2025. https://blog.google/technology/google-deepmind/ [10] Thomas Eiter and Heikki Mannila. Computing discrete fréchet distance. Technical Report CD-TR 94/64, Christian Doppler Laboratory for Expert Systems, 1994. [11] Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, et al. G-llava: Solving geometric problem with multi-modal large language model. arXiv preprint arXiv:2312.11370, 2023. [12] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [13] Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh Chawla, Olaf Wiest, and Xiangliang Zhang. Large language model based multi-agents: survey of progress and challenges. arXiv preprint arXiv:2402.01680, 2024. [14] Xudong Guo, Kaixuan Huang, Jiale Liu, Wenhui Fan, Natalia Vélez, Qingyun Wu, Huazheng Wang, Thomas Griffiths, and Mengdi Wang. Embodied llm agents learn to cooperate in organized teams. arXiv preprint arXiv:2403.12482, 2024. [15] Yingdong Hu, Fanqi Lin, Tong Zhang, Li Yi, and Yang Gao. Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning. arXiv preprint arXiv:2311.17842, 2023. 10 [16] Daniel Huttenlocher, Gregory Klanderman, and William Rucklidge. Comparing images using the hausdorff distance. IEEE Transactions on Pattern Analysis and Machine Intelligence, 15(9):850863, 1993. [17] Unnat Jain, Luca Weihs, Eric Kolve, Ali Farhadi, Svetlana Lazebnik, Aniruddha Kembhavi, and Alexander Schwing. cordial sync: Going beyond marginal policies for multi-agent embodied tasks. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part 16, pages 471490. Springer, 2020. [18] Yuheng Ji, Huajie Tan, Jiayu Shi, Xiaoshuai Hao, Yuan Zhang, Hengyuan Zhang, Pengwei Wang, Mengdi Zhao, Yao Mu, Pengju An, et al. Robobrain: unified brain model for robotic manipulation from abstract to concrete. arXiv preprint arXiv:2502.21257, 2025. [19] Yanhao Jia, Xinyi Wu, Qinglin Zhang, Yiran Qin, Luwei Xiao, and Shuai Zhao. Towards robust evaluation of stem education: Leveraging mllms in project-based learning. arXiv preprint arXiv:2505.17050, 2025. [20] Shyam Sundar Kannan, Vishnunandan LN Venkatesh, and Byung-Cheol Min. Smart-llm: Smart multiagent robot task planning using large language models. In 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 1214012147. IEEE, 2024. [21] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [22] Jiaqi Liu, Chengkai Xu, Peng Hang, Jian Sun, Mingyu Ding, Wei Zhan, and Masayoshi Tomizuka. Language-driven policy distillation for cooperative driving in multi-agent reinforcement learning. IEEE Robotics and Automation Letters, 2025. [23] Xiangyan Liu, Jinjie Ni, Zijian Wu, Chao Du, Longxu Dou, Haonan Wang, Tianyu Pang, and Michael Qizhe Shieh. Noisyrollout: Reinforcing visual reasoning with data augmentation. arXiv preprint arXiv:2504.13055, 2025. [24] Xinzhu Liu, Di Guo, Huaping Liu, and Fuchun Sun. Multi-agent embodied visual semantic navigation with scene prior knowledge. IEEE Robotics and Automation Letters, 7(2):31543161, 2022. [25] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025. [26] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. [27] Zhao Mandi, Shreeya Jain, and Shuran Song. Roco: Dialectic multi-robot collaboration with large language models. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 286299. IEEE, 2024. [28] Soroush Nasiriany, Abhiram Maddukuri, Lance Zhang, Adeet Parikh, Aaron Lo, Abhishek Joshi, Ajay Mandlekar, and Yuke Zhu. Robocasa: Large-scale simulation of everyday tasks for generalist robots. arXiv preprint arXiv:2406.02523, 2024. [29] Kazuma Obata, Tatsuya Aoki, Takato Horii, Tadahiro Taniguchi, and Takayuki Nagai. Lip-llm: Integrating linear programming and dependency graph with large language models for multi-robot task planning. IEEE Robotics and Automation Letters, 2024. [30] OpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander adry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, Alexi Christakis, Alexis Conneau, Ali Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoochian, Amin Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej Karpathy, Andrew Braunstein, Andrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrew Tulloch, Andrey Mishchenko, Angela Baek, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, Behrooz Ghorbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky, Ben Wang, Benjamin Zweig, Beth Hoover, Blake Samic, Bob McGrew, Bobby Spero, Bogo Giertler, Bowen Cheng, Brad Lightcap, Brandon Walkin, Brendan Quinn, Brian Guarraci, Brian Hsu, Bright Kellogg, Brydon Eastman, Camillo Lugaresi, Carroll Wainwright, Cary Bassin, Cary Hudson, Casey Chu, Chad Nelson, Chak Li, Chan Jun Shern, Channing Conger, Charlotte Barette, Chelsea Voss, Chen Ding, Cheng Lu, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina Kim, Christine Choi, Christine 11 McLeavey, Christopher Hesse, Claudia Fischer, Clemens Winter, Coley Czarnecki, Colin Jarvis, Colin Wei, Constantin Koumouzelis, Dane Sherburn, Daniel Kappler, Daniel Levin, Daniel Levy, David Carr, David Farhi, David Mely, David Robinson, David Sasaki, Denny Jin, Dev Valladares, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay, Edede Oiwoh, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Antonow, Eric Kramer, Eric Peterson, Eric Sigler, Eric Wallace, Eugene Brevdo, Evan Mays, Farzad Khorasani, Felipe Petroski Such, Filippo Raso, Francis Zhang, Fred von Lohmann, Freddie Sulit, Gabriel Goh, Gene Oden, Geoff Salmon, Giulio Starace, Greg Brockman, Hadi Salman, Haiming Bao, Haitang Hu, Hannah Wong, Haoyu Wang, Heather Schmidt, Heather Whitney, Heewoo Jun, Hendrik Kirchner, Henrique Ponde de Oliveira Pinto, Hongyu Ren, Huiwen Chang, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian OConnell, Ian Osband, Ian Silber, Ian Sohl, Ibrahim Okuyucu, Ikai Lan, Ilya Kostrikov, Ilya Sutskever, Ingmar Kanitscheider, Ishaan Gulrajani, Jacob Coxon, Jacob Menick, Jakub Pachocki, James Aung, James Betker, James Crooks, James Lennon, Jamie Kiros, Jan Leike, Jane Park, Jason Kwon, Jason Phang, Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jenia Varavva, Jessica Gan Lee, Jessica Shieh, Ji Lin, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joanne Jang, Joaquin Quinonero Candela, Joe Beutler, Joe Landers, Joel Parish, Johannes Heidecke, John Schulman, Jonathan Lachman, Jonathan McKay, Jonathan Uesato, Jonathan Ward, Jong Wook Kim, Joost Huizinga, Jordan Sitkin, Jos Kraaijeveld, Josh Gross, Josh Kaplan, Josh Snyder, Joshua Achiam, Joy Jiao, Joyce Lee, Juntang Zhuang, Justyn Harriman, Kai Fricke, Kai Hayashi, Karan Singhal, Katy Shi, Kavin Karthik, Kayla Wood, Kendra Rimbach, Kenny Hsu, Kenny Nguyen, Keren Gu-Lemberg, Kevin Button, Kevin Liu, Kiel Howe, Krithika Muthukumar, Kyle Luther, Lama Ahmad, Larry Kai, Lauren Itow, Lauren Workman, Leher Pathak, Leo Chen, Li Jing, Lia Guy, Liam Fedus, Liang Zhou, Lien Mamitsuka, Lilian Weng, Lindsay McCallum, Lindsey Held, Long Ouyang, Louis Feuvrier, Lu Zhang, Lukas Kondraciuk, Lukasz Kaiser, Luke Hewitt, Luke Metz, Lyric Doshi, Mada Aflak, Maddie Simens, Madelaine Boyd, Madeleine Thompson, Marat Dukhan, Mark Chen, Mark Gray, Mark Hudnall, Marvin Zhang, Marwan Aljubeh, Mateusz Litwin, Matthew Zeng, Max Johnson, Maya Shetty, Mayank Gupta, Meghan Shah, Mehmet Yatbaz, Meng Jia Yang, Mengchao Zhong, Mia Glaese, Mianna Chen, Michael Janner, Michael Lampe, Michael Petrov, Michael Wu, Michele Wang, Michelle Fradin, Michelle Pokrass, Miguel Castro, Miguel Oom Temudo de Castro, Mikhail Pavlov, Miles Brundage, Miles Wang, Minal Khan, Mira Murati, Mo Bavarian, Molly Lin, Murat Yesildal, Nacho Soto, Natalia Gimelshein, Natalie Cone, Natalie Staudacher, Natalie Summers, Natan LaFontaine, Neil Chowdhury, Nick Ryder, Nick Stathas, Nick Turley, Nik Tezak, Niko Felix, Nithanth Kudige, Nitish Keskar, Noah Deutsch, Noel Bundick, Nora Puckett, Ofir Nachum, Ola Okelola, Oleg Boiko, Oleg Murk, Oliver Jaffe, Olivia Watkins, Olivier Godement, Owen Campbell-Moore, Patrick Chao, Paul McMillan, Pavel Belov, Peng Su, Peter Bak, Peter Bakkum, Peter Deng, Peter Dolan, Peter Hoeschele, Peter Welinder, Phil Tillet, Philip Pronin, Philippe Tillet, Prafulla Dhariwal, Qiming Yuan, Rachel Dias, Rachel Lim, Rahul Arora, Rajan Troll, Randall Lin, Rapha Gontijo Lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Ricky Wang, Rob Donnelly, Rob Honsby, Rocky Smith, Rohan Sahai, Rohit Ramchandani, Romain Huet, Rory Carmichael, Rowan Zellers, Roy Chen, Ruby Chen, Ruslan Nigmatullin, Ryan Cheu, Saachi Jain, Sam Altman, Sam Schoenholz, Sam Toizer, Samuel Miserendino, Sandhini Agarwal, Sara Culver, Scott Ethersmith, Scott Gray, Sean Grove, Sean Metzger, Shamez Hermani, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shino Jomoto, Shirong Wu, Shuaiqi, Xia, Sonia Phene, Spencer Papay, Srinivas Narayanan, Steve Coffey, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tao Xu, Tarun Gogineni, Taya Christianson, Ted Sanders, Tejal Patwardhan, Thomas Cunninghman, Thomas Degry, Thomas Dimson, Thomas Raoux, Thomas Shadwell, Tianhao Zheng, Todd Underwood, Todor Markov, Toki Sherbakov, Tom Rubin, Tom Stasi, Tomer Kaftan, Tristan Heywood, Troy Peterson, Tyce Walters, Tyna Eloundou, Valerie Qi, Veit Moeller, Vinnie Monaco, Vishal Kuo, Vlad Fomenko, Wayne Chang, Weiyi Zheng, Wenda Zhou, Wesam Manassra, Will Sheu, Wojciech Zaremba, Yash Patil, Yilei Qian, Yongjik Kim, Youlong Cheng, Yu Zhang, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, and Yury Malkov. Gpt-4o system card, 2024. [31] Xavier Puig, Tianmin Shu, Shuang Li, Zilin Wang, Yuan-Hong Liao, Joshua Tenenbaum, Sanja Fidler, and Antonio Torralba. Watch-and-help: challenge for social perception and human-ai collaboration. arXiv preprint arXiv:2010.09890, 2020. [32] Yiran Qin, Li Kang, Xiufeng Song, Zhenfei Yin, Xiaohong Liu, Xihui Liu, Ruimao Zhang, and Lei Bai. Robofactory: Exploring embodied agent collaboration with compositional constraints. arXiv preprint arXiv:2503.16408, 2025. [33] Yiran Qin, Zhelun Shi, Jiwen Yu, Xijun Wang, Enshen Zhou, Lijun Li, Zhenfei Yin, Xihui Liu, Lu Sheng, Jing Shao, et al. Worldsimbench: Towards video generation models as world simulators. arXiv preprint arXiv:2410.18072, 2024. [34] Yiran Qin, Ao Sun, Yuze Hong, Benyou Wang, and Ruimao Zhang. Navigatediff: Visual predictors are zero-shot navigation assistants. arXiv preprint arXiv:2502.13894, 2025. [35] Yiran Qin, Enshen Zhou, Qichang Liu, Zhenfei Yin, Lu Sheng, Ruimao Zhang, Yu Qiao, and Jing Shao. Mp5: multi-modal open-ended embodied system in minecraft via active perception. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1630716316. IEEE, 2024. [36] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36:6853968551, 2023. [37] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [38] Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, and Roy Ka-Wei Lee. Math-llava: Bootstrapping mathematical reasoning for multimodal large language models. arXiv preprint arXiv:2406.17294, 2024. [39] Huajie Tan, Yuheng Ji, Xiaoshuai Hao, Minglan Lin, Pengwei Wang, Zhongyuan Wang, and Shanghang Zhang. Reason-rft: Reinforcement fine-tuning for visual reasoning. arXiv preprint arXiv:2503.20752, 2025. [40] Stone Tao, Fanbo Xiang, Arth Shukla, Yuzhe Qin, Xander Hinrichsen, Xiaodi Yuan, Chen Bao, Xinsong Lin, Yulin Liu, Tse-kai Chan, et al. Maniskill3: Gpu parallelized robotics simulation and rendering for generalizable embodied ai. arXiv preprint arXiv:2410.00425, 2024. [41] Weizheng Wang, Ike Obi, and Byung-Cheol Min. Multi-agent llm actor-critic framework for social robot navigation. arXiv preprint arXiv:2503.09758, 2025. [42] Yongdong Wang, Runze Xiao, Jun Younes Louhi Kasahara, Ryosuke Yajima, Keiji Nagatani, Atsushi Yamashita, and Hajime Asama. Dart-llm: Dependency-aware multi-robot task decomposition and execution using large language models. arXiv preprint arXiv:2411.09022, 2024. [43] Yujin Wang, Quanfeng Liu, Zhengxin Jiang, Tianyi Wang, Junfeng Jiao, Hongqing Chu, Bingzhao Gao, and Hong Chen. Rad: Retrieval-augmented decision-making of meta-actions with vision-language models in autonomous driving. arXiv preprint arXiv:2503.13861, 2025. [44] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [45] Wikipedia. I, robot (film). https://en.wikipedia.org/wiki/I,_Robot_(film), 2004. [46] Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, and Li Yuan. Llava-cot: Let vision language models reason step-by-step, 2024. URL https://arxiv. org/abs/2411.10440. [47] Jiwen Yu, Jianhong Bai, Yiran Qin, Quande Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Xihui Liu. Context as memory: Scene-consistent interactive long video generation with memory retrieval. arXiv preprint arXiv:2506.03141, 2025. [48] Jiwen Yu, Yiran Qin, Haoxuan Che, Quande Liu, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Hao Chen, and Xihui Liu. survey of interactive generative video. arXiv preprint arXiv:2504.21853, 2025. [49] Jiwen Yu, Yiran Qin, Haoxuan Che, Quande Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Xihui Liu. Position: Interactive generative video as next-generation game engine. arXiv preprint arXiv:2503.17359, 2025. [50] Jiwen Yu, Yiran Qin, Xintao Wang, Pengfei Wan, Di Zhang, and Xihui Liu. Gamefactory: Creating new games with generative interactive videos. arXiv preprint arXiv:2501.08325, 2025. [51] Hangtao Zhang, Chenyu Zhu, Xianlong Wang, Ziqi Zhou, Shengshan Hu, and Leo Yu Zhang. Badrobot: Jailbreaking llm-based embodied ai in the physical world. arXiv preprint arXiv:2407.20242, 2024. [52] Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua Tenenbaum, Tianmin Shu, and Chuang Gan. Building cooperative embodied agents modularly with large language models. arXiv preprint arXiv:2307.02485, 2023. [53] Hongxin Zhang, Zeyuan Wang, Qiushi Lyu, Zheyuan Zhang, Sunli Chen, Tianmin Shu, Behzad Dariush, Kwonjoon Lee, Yilun Du, and Chuang Gan. Combo: compositional world models for embodied multi-agent cooperation. arXiv preprint arXiv:2404.10775, 2024. [54] Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Ziyu Guo, Shicheng Li, Yichi Zhang, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei, et al. Mathematical visual instruction tuning with an automatic data engine. arXiv preprint arXiv:2407.08739, 2024. [55] Zirui Zhao, Wee Sun Lee, and David Hsu. Large language models as commonsense knowledge for large-scale task planning. Advances in Neural Information Processing Systems, 36:3196731987, 2023. [56] Sipeng Zheng, Jiazheng Liu, Yicheng Feng, and Zongqing Lu. Steve-eye: Equipping llm-based embodied agents with visual perception in open worlds. arXiv preprint arXiv:2310.13255, 2023. [57] Wenzhao Zheng, Weiliang Chen, Yuanhui Huang, Borui Zhang, Yueqi Duan, and Jiwen Lu. Occworld: Learning 3d occupancy world model for autonomous driving. In European Conference on Computer Vision, pages 5572. Springer, 2025. [58] Enshen Zhou, Jingkun An, Cheng Chi, Yi Han, Shanyu Rong, Chi Zhang, Pengwei Wang, Zhongyuan Wang, Tiejun Huang, Lu Sheng, and Shanghang Zhang. Roborefer: Towards spatial referring with reasoning in vision-language models for robotics. arXiv preprint arXiv:2506.04308, 2025. [59] Enshen Zhou, Yiran Qin, Zhenfei Yin, Yuzhou Huang, Ruimao Zhang, Lu Sheng, Yu Qiao, and Jing Shao. Minedreamer: Learning to follow instructions via chain-of-imagination for simulated-world control. arXiv preprint arXiv:2403.12037, 2024. [60] Enshen Zhou, Qi Su, Cheng Chi, Zhizheng Zhang, Zhongyuan Wang, Tiejun Huang, Lu Sheng, and He Wang. Code-as-monitor: Constraint-aware visual programming for reactive and proactive robotic failure detection. arXiv preprint arXiv:2412.04455, 2024. [61] Heng Zhou, Hejia Geng, Xiangyuan Xue, Zhenfei Yin, and Lei Bai. Reso: reward-driven self-organizing llm-based multi-agent system for reasoning tasks. arXiv preprint arXiv:2503.02390, 2025. [62] Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, et al. Sotopia: Interactive evaluation for social intelligence in language agents. arXiv preprint arXiv:2310.11667, 2023."
        },
        {
            "title": "A Limitations",
            "content": "Although VIKI-Bench and VIKI-R advance embodied multi-agent cooperation, several challenges remain unresolved. First, the hierarchical task levels proposed in VIKI-Bench, while useful for structuring cooperative interactions, may not fully reflect all dynamic conditions and cooperative practice in real-world scenarios. Real environments involve unforeseen obstacles, shifting objectives, and adaptive agent behaviors that are difficult to model within fixed hierarchical framework. Second, while VIKI-Rs hierarchical reward designs effectively enhance agent performance, their effectiveness relies on multilevel fine-tuning, which raises the needs for computational costs. possible solution is to devise more precious reward structure that adapt to complex environmental variations without requiring excessive tuning. Additionally, although the emergent collaborative behaviors show prominent performance in multi-level tasks, the reasoning process and interpretability remains underexplored. Such studies are crucial for ensuring trust and facilitating human-AI collaboration in real-world deployments. In conclusion, while our benchmark and framework provide strong foundation for multi-agent cooperation, they face challenges in adaptability, computational complexity and interpretability. Future work could focus on expanding task diversity, improving framework design, and enhancing model transparency. Addressing these limitations will be essential for developing more robust and scalable multi-agent systems."
        },
        {
            "title": "B Broader Impacts",
            "content": "The development of VIKI-Bench and VIKI-R has significant influence on both embodied multiagent research and real-world applications. By introducing hierarchical benchmark for embodied multi-agent cooperation, this work enables multi-dimensional evaluation and comparison of visionlanguage models (VLMs) in complex, dynamic environments. In practice, VIKI-Bench can accelerate progress in real-world robotics applications, such as warehouse automation, autonomous driving, and collaborative industrial robots, where heterogeneous agents must coordinate under visual uncertainty. The proposed VIKI-R framework demonstrates how fine-tuning VLMs with reasoning and reinforcement learning can improve multi-agent decision-making, potentially leading to more adaptable and efficient autonomous systems. However, the deployment of such systems raises important considerations, including safety, fairness in decision-making, and the potential relationships between human and robots. Future work should address these challenges while leveraging VIKI-Benchs structured supervision to ensure robustness and interpretability in real-world scenarios. Ultimately, this research paves the way for more sophisticated AI systems capable of seamless cooperation in visually rich, dynamic environments. Details of VIKI-Bench This section introduces details of VIKI-Bench, including the overview of data statistics, plan feasibility checking and experimental setup. C.1 Data Statistics The datasets in VIKI-Bench are organized into three hierarchical levels: VIKI-L1: Agent activation (10,714 samples; 8,171 training, 2,043 testing). VIKI-L2: Symbolic planning (10,714 samples; 7,196 in-domain training, 1,800 in-domain testing, and 1,218 out-of-domain testing from held-out scenes). VIKI-L3: Trajectory perception (2,309 samples; 1,767 training, 442 testing). This multi-stage structure facilitates comprehensive evaluation of high-level coordination and lowlevel motion prediction in realistic, dynamic environments. C.2 Plan Feasibility Check The VIKI-L2 task must produce plan list that successfully passes the feasibility checker described in Section 3.2.2identical to the process used during data generation."
        },
        {
            "title": "Output Constraint",
            "content": "Output Format Requirements: <answer> [ { \"step\": 1, \"actions\": {'R1': ['Move', 'banana'], 'R2': ['Move', 'apple']} }, { \"step\": 2, \"actions\": {'R1': ['Reach', 'banana'], 'R2': ['Reach', 'apple']} } # ... subsequent steps ... ] </answer> - step is the time step number (starting from 1, incrementing sequentially). - Each robot can only have ONE action per time step. - \"actions\" is dictionary that specifies the action for each robot during (cid:44) (cid:44) (cid:44) single time step. Each key (e.g., \"R1\", \"R2\") represents robot. Each value is list describing the single action that robot will perform in this step, with the following format: action_type, target_object_or_location, (optional: extra_argument) (cid:44) Action primitives and descriptions: {ACTION_DESCRIPTION} Available robot set: {robots} Robot characteristics: {available_robots} Their available operation APIs: {available_actions} C.3 Experimental Setup As summarized in Table 5, during GRPO we operate under PPO framework using the VLLM engine with the XFORMERS attention backend. Inputs are truncated beyond 4096-token prompt length and 2048-token response length. We employ total batch size of 256, subdivided into PPO minibatches of 128 and micro-batches of 10 per GPU; the actor network is optimized with learning rate of 1 106. KL-divergence regularization (coefficient 0.01, low_var_kl variant) is used, while entropy regularization and KL-based rewards are disabled. Gradient checkpointing reduces memory footprint, targeting 60% memory utilization during rollout with five rollouts per prompt, and both chunked prefill and eager execution are disabled. Finally, we train VIKI-L1 for five epochs, VIKI-L2 for fifteen epochs, and VIKI-L3 for two epochs."
        },
        {
            "title": "D Implementation Details",
            "content": "This section details our implementation and hyperparameter settings for data generation, model training, and inference. We build upon the Qwen2.5-VL-3B and Qwen2.5-VL-7B backbones, orchestrating the entire pipeline with the open-source verl framework and employing LLamaFactory for supervised fine-tuning. All experiments run on single node equipped with eight NVIDIA A800 GPUs. Three tasksVIKI-L1, VIKI-L2, and VIKI-L3with the following data splits: VIKI-L1: 10714 samples (500 cold-start SFT, 8171 training, 2043 testing);VIKI-L2 (planning): 10174 samples, of which 1218 are held out as OOD and the remaining 8956 as ID (with 500 cold-start CoT, 7196 training, 1800 ID testing);VIKI-L3 (trajectory): 2309 samples (100 cold-start CoT, 1767 training, 442 testing). 16 Table 5: Hyperparameter settings for GRPO training Parameter Engine VLLM attention backend Algorithm (adv estimator) Train batch size Max prompt length Max response length Filter overlong prompts Truncation strategy Actor learning rate Remove padding PPO mini-batch size PPO micro-batch per GPU Use KL loss KL loss coefficient KL loss type Entropy coefficient Gradient checkpointing FSDP param offload FSDP optimizer offload Rollout log-prob micro-batch Tensor model parallel size Rollout engine name GPU memory utilization Chunked prefill Enforce eager Free cache engine Rollout samples (n) Reference log-prob micro-batch Reference FSDP param offload Use KL in reward Critic warmup steps GPUs per node Number of nodes epoch VIKI-L1 $1:-vllm XFORMERS grpo 256 4096 2048 True error 1 106 True 128 10 True 0.01 low_var_kl 0 True False False 20 1 vllm 0.6 False False False 5 20 True False 0 8 1 VIKI-L2 $1:-vllm XFORMERS grpo 256 4096 2048 True error 1 106 True 128 10 True 0.01 low_var_kl 0 True False False 20 1 vllm 0.6 False False False 5 20 True False 0 8 1 15 VIKI-L3 $1:-vllm XFORMERS grpo 256 4096 2048 True error 1 106 True 128 10 True 0.01 low_var_kl 0 True False False 20 1 vllm 0.6 False False False 5 20 True False 0 8 1 2 Analysis of VIKI-R As illustrated in Figure 5, the four panels reflect two distinct axes of variation: model capacity (3B in panels ab vs. 7B in cd) and initialization strategy (RL-only in VIKI-R-ZERO vs. SFT cold-start + RL in VIKI-R). Even without SFT, the 7B R-ZERO curve (panel c) begins at 0.1 and climbs to 0.9 by step 120, compared to 0.04 to 0.27 for the 3B R-ZERO (panel a), underscoring scale effects. However, both R-ZERO variants exhibit sluggish and oscillatory learning: the base model lacks sufficient task reasoning capacity to roll out coherent action sequences, resulting in unstable gradient signals and limited policy improvement without prior SFT warm-up. Introducing CoT cold-start further boosts performance: the 3B variant (panel b) launches at 0.3 and reaches 0.85 by step 140substantially outpacing its R-ZERO counterpartwhile the 7B (panel d) jumps in at 0.65 and exceeds 0.95 by step 70. Taken together, these results show that both larger capacity and SFT initialization independently accelerate learning, and when combined, yield the fastest convergence and highest final rewards. Figure 6 juxtaposes the planning task dynamics across both model sizes and initialization strategies. On the 3B backbone, the RL-only R-ZERO variant (panel a) starts near negligible mean reward ( 0.009), briefly peaks at 0.019 around step 50, then settles to 0.011 with persistent fluctuationsindicative of learning but limited headroom. This poor performance stems from the base models limited reasoning capacity, which fails to produce coherent rollout trajectories without SFT initialization, yielding weak reward signals. In contrast, the CoT-initialized variant (panel b) launches at 0.56 (reflecting SFT warm-up) and swiftly climbs to 0.92 by step 60, eventually plateauing around 0.94 with minimal oscillation, demonstrating dramatically accelerated and stable policy improvement. For the 7B backbone, R-ZERO (panel c) begins at 0.06 and converges to 0.075 by step 20, maintaining narrow band around that value thereafter. The 7B design (panel d), however, initiates at 0.45 and reaches 0.90 by step 60, then settles around 0.92, mirroring the 3B pattern of strong SFT head start followed by rapid RL fine-tuning. These results confirm that both increased model capacity and SFT cold-start independently enhance planning performance, with their combination yielding the most pronounced gains. In the trajectory execution task (Figure 7), both initialization strategies achieve high asymptotic rewards, reflecting the models inherent ability to roll out local motion trajectories. On the 3B backbone, the RL-only R-ZERO variant (panel a) starts from modest 0.12 and climbs rapidly to 0.82 by step 20, thereafter plateauing around 0.83 with minimal volatility. The CoT-initialized variant (panel b) gains an immediate head start at 0.45 and reaches the same 0.82 level by step 15, shaving several steps off the convergence time and maintaining slightly higher plateau ( 0.84). For the 7B backbone, R-ZERO (panel c) follows similar patternrising from 0.12 to 0.80 by step 20while the variant (panel d) begins near 0.40, quickly approaches 0.75 by step 15, experiences transient dip to 0.68 around step 35, and then recovers to settle near 0.75. These results indicate that trajectory execution relies less on high-level planning and thus even RL-only training suffices for strong performance; nonetheless, SFT cold-start combined with reward shaping yields modest acceleration in early learning, with marginal improvements in final reward levels. (a) Qwen2.5VL3B-VIKI-R-ZERO (b) Qwen2.5VL3B-VIKI-R (c) Qwen2.5VL7B-VIKI-R-ZERO (d) Qwen2.5VL7B-VIKI-R Figure 5: GRPO reward mean curve about task VIKI-L1 18 (a) Qwen2.5VL3B-VIKI-R-ZERO (b) Qwen2.5VL3B-VIKI-R (c) Qwen2.5VL7B-VIKI-R-ZERO (d) Qwen2.5VL7B-VIKI-R Figure 6: GRPO reward mean curve about task VIKI-L2 (a) Qwen2.5VL3B-VIKI-R-ZERO (b) Qwen2.5VL3B-VIKI-R (c) Qwen2.5VL7B-VIKI-R-ZERO (d) Qwen2.5VL7B-VIKI-R Figure 7: GRPO reward mean curve about task VIKI-L"
        },
        {
            "title": "F Data Demonstration",
            "content": "Fig. 8 provides additional data visualizations showcasing qualitative demos of VIKI-L1 and VIKI-L2 across diverse real-world tasks."
        },
        {
            "title": "G Prompt",
            "content": "Prompt of VIKI-L1 Possible robots: {robot_set} First, identify the robots visible in the image from list of \"possible (cid:44) robots\". Among the visible robots, select the most suitable one or more to collaborate on the task. (cid:44) You FIRST think about the reasoning process as an internal monologue and then provide the final answer. (cid:44) The reasoning process MUST BE enclosed within <think> </think> tags. The (cid:44) (cid:44) (cid:44) (cid:44) final answer MUST BE enclosed within <answer>Your final answer must be provided as Python list format, for example: ['fetch', 'unitree_h1']. Include only the robot names that are suitable for the task.</answer> tags. Prompt of VIKI-L3 You are an expert in visual understanding and trajectory planning. **INPUT:** * An ego-view image showing two robotic arms working together; the arm closest to the camera represents **you**. (cid:44) * string describing the overall task. * Two strings specifying your subtask (\"you\") and your partner's subtask. **YOUR JOB:** 1. Enclose your scene analysis and task division within `<think>...</think>` tags. (cid:44) 2. Enclose your final output within `<answer>...</answer>` tags as nested (cid:44) list of **ten 2D pixel coordinates**: * Two groups of five points each: * **First group:** your trajectory * **Second group:** your partner's trajectory 3. Follow this format **exactly** (no additional text): [[ [x1, y1], [x2, y2], [x3, y3], [x4, y4], [x5, y5] ], [ [x1', y1'], [x2', y2'], [x3', y3'], [x4', y4'], [x5', y5'] ]] Prompt of VIKI-L2 (cid:44) You are plan creator. will provide you with an image of robots in scene, available robots and their action primitives, and task description. You need to create plan to complete the task. (cid:44) You must first analyze the image to fully understand the scene depicted. (cid:44) Then, analyze the task description. Finally, create plan to complete the task. (cid:44) Your reasoning must strictly adhere to the visual content of the image and the task descriptionno assumptions, hypotheses, or guesses are allowed. (cid:44) 1. Create plan to complete the task, noting: - Each robot can only perform ONE action per time step. - Multiple robots can work in parallel, but each robot is limited to one (cid:44) action at time. 2. You need to first provide your reasoning process within <think> and </think> tags. (cid:44) 3. Your final answer must be within <answer> and </answer> tags, and (cid:44) **strictly follow the JSON format specified below**. Output Format Requirements(please comply strictly, do not output any additional content): (cid:44) <answer> [ 20 {{ \"step\": 1, \"actions\": {{'R1': ['Move', 'banana'], 'R2': ['Move', 'apple']}} }}, {{ \"step\": 2, \"actions\": {{'R1': ['Reach', 'banana'], 'R2': ['Reach', 'apple']}} }} # ... subsequent steps ... ] </answer> Where: - step is the time step number (starting from 1, incrementing sequentially). - Each robot can only have ONE action per time step. - \"actions\" is dictionary that specifies the action for each robot during (cid:44) (cid:44) (cid:44) single time step. Each key (e.g., \"R1\", \"R2\") represents robot. Each value is list describing the single action that robot will perform in this step, with the following format: action_type, target_object_or_location, (optional: extra_argument) (cid:44) Action primitives and descriptions: {ACTION_DESCRIPTION} Available robot set: {robots} Robot characteristics: {available_robots} Their available operation APIs: {available_actions}"
        },
        {
            "title": "Primitives and description",
            "content": "ROBOT_DESCRIPTION = { 'stompy': 'A bipedal robot designed for dynamic walking and stomping (cid:44) tasks, featuring articulated arms. Color: Light blue body with yellow and orange accents.', (cid:44) 'fetch': 'A wheeled robot with flexible arm for object manipulation, designed for mobility and dexterity. Color: White with blue and black accents.', (cid:44) 'unitree_h1': 'A humanoid robot with arms and legs designed for (cid:44) human-like movements and tasks. Color: Black.', (cid:44) 'panda': 'A fixed robotic arm designed for precise and delicate manipulation tasks. Color: White with black accents.', (cid:44) 'anymal_c': 'A quadrupedal robot built for navigating rough terrains and (cid:44) performing complex tasks with four articulated legs. Color: Red and black with some accents.', (cid:44) 'unitree_go2': 'A compact quadrupedal robot optimized for agile movement (cid:44) and stability with four legs for efficient locomotion. Color: White.' } ACTION_DESCRIPTION = { 'Move': \"Command ['Move', 'object']: Robot moves to the specified object.\", (cid:44) 'Open': \"Command ['Open', 'object']: Open the object held by the Robot R's end effector.\", (cid:44) 'Close': \"Command ['Close', 'object']: Close the object held by the Robot R's end effector.\", (cid:44) 'Reach': \"Command ['Reach', 'object']: Robot reaches the specified object.\", (cid:44) 'Grasp': \"Command ['Grasp', 'object']: Robot R's end effector performs grasping operation on specified object.\", (cid:44) 'Place': \"Command ['Place', 'object']: Place the object held by the (cid:44) Robot R's end effector at specified location (the release point, not the object itself).\", (cid:44) 'Push': \"Command ['Push', 'object', 'R1']: Robot pushes the object to robot R1.\", (cid:44) 'Interact': \"Command ['Interact', 'object']: general interaction (cid:44) operation, flexible for representing interactions with any asset.\" } AGENT_AVAIL_ACTIONS = { 'panda': 'fetch': ['Reach', 'Grasp', 'Place', 'Open', 'Close', 'Interact'], ['Move', 'Reach', 'Grasp', 'Place', 'Open', 'Close', 'Interact'], (cid:44) 'unitree_go2':['Move', 'Push', 'Interact'], 'unitree_h1': ['Move', 'Reach', 'Grasp', 'Place', 'Open', 'Close', 'Interact'], (cid:44) 'stompy': ['Move', 'Reach', 'Grasp', 'Place', 'Open', 'Close', 'Interact'], (cid:44) 'anymal_c': ['Move', 'Push', 'Interact'], } AGENT_END_EFFECTOR_NUM = { 'panda': 1, 'fetch': 1, 'unitree_go2': 0, 'unitree_h1': 2, 'stompy': 2, 'anymal_c': 0, } 22 Figure 8: Qualitative demonstrations of VIKI-L1 and VIKI-L2 showcasing visual input, agent activation, and task planning across diverse real-world scenarios."
        }
    ],
    "affiliations": [
        "Shanghai Artificial Intelligence Laboratory",
        "Shanghai Jiao Tong University",
        "The Chinese University of Hong Kong, Shenzhen",
        "University of Oxford",
        "University of Science and Technology of China"
    ]
}