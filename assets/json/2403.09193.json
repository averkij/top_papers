{
    "paper_title": "Are Vision Language Models Texture or Shape Biased and Can We Steer Them?",
    "authors": [
        "Paul Gavrikov",
        "Jovita Lukasik",
        "Steffen Jung",
        "Robert Geirhos",
        "Bianca Lamm",
        "Muhammad Jehanzeb Mirza",
        "Margret Keuper",
        "Janis Keuper"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision language models (VLMs) have drastically changed the computer vision model landscape in only a few years, opening an exciting array of new applications from zero-shot image classification, over to image captioning, and visual question answering. Unlike pure vision models, they offer an intuitive way to access visual content through language prompting. The wide applicability of such models encourages us to ask whether they also align with human vision - specifically, how far they adopt human-induced visual biases through multimodal fusion, or whether they simply inherit biases from pure vision models. One important visual bias is the texture vs. shape bias, or the dominance of local over global information. In this paper, we study this bias in a wide range of popular VLMs. Interestingly, we find that VLMs are often more shape-biased than their vision encoders, indicating that visual biases are modulated to some extent through text in multimodal models. If text does indeed influence visual biases, this suggests that we may be able to steer visual biases not just through visual input but also through language: a hypothesis that we confirm through extensive experiments. For instance, we are able to steer shape bias from as low as 49% to as high as 72% through prompting alone. For now, the strong human bias towards shape (96%) remains out of reach for all tested VLMs."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 4 1 ] . [ 1 3 9 1 9 0 . 3 0 4 2 : r Are Vision Language Models Texture or Shape Biased and Can We Steer Them? Paul Gavrikov1 Jovita Lukasik2 Steffen Jung3,6 Robert Geirhos4 Bianca Lamm1 Muhammad Jehanzeb Mirza5 Margret Keuper6,3 2 University of Siegen 1 IMLA, Offenburg University Janis Keuper1,6 3 Max Planck Institute for Informatics, Saarland Informatics Campus 4 Google DeepMind 5 ICG, Graz University of Technology 6 University of Mannheim"
        },
        {
            "title": "Abstract",
            "content": "Vision language models (VLMs) have drastically changed the computer vision model landscape in only few years, opening an exciting array of new applications from zero-shot image classification, over to image captioning, and visual question answering. Unlike pure vision models, they offer an intuitive way to access visual content through language prompting. The wide applicability of such models encourages us to ask whether they also align with human vision specifically, how far they adopt human-induced visual biases through multimodal fusion, or whether they simply inherit biases from pure vision models. One important visual bias is the texture vs. shape bias, or the dominance of local over global information. In this paper, we study this bias in wide range of popular VLMs. Interestingly, we find that VLMs are often more shape-biased than their vision encoders, indicating that visual biases are modulated to some extent through text in multimodal models. If text does indeed influence visual biases, this suggests that we may be able to steer visual biases not just through visual input but also through language: hypothesis that we confirm through extensive experiments. For instance, we are able to steer shape bias from as low as 49% to as high as 72% through prompting alone. For now, the strong human bias towards shape (96%) remains out of reach for all tested VLMs. Figure 1: Unlike many unimodal models, vision language models (VLMs) prefer shape over texture for object recognition, but not to the same extent as humans. Further, we find that the (visual) texture/shape bias [1] can be steered through language alone, albeit not to the extent as through vision. Here we visualize the texture/shape bias of some exemplary VLMs, and highlight the steerability of InternVL-Chat 1.1 [2]."
        },
        {
            "title": "Introduction",
            "content": "As the old adage goes, all models are wrong, but some are useful. Similarly, all (machine learning) models are biased, but according to the no free lunch theorem, only some biases are useful [3, 4]. While most biases observed in language models reflect social particularities that originate in unbalanced training data, vision models have been shown to be exceptionally misaligned with human perception, referred to as the texture vs. shape bias [1]. Specifically, when recognizing objects, an objects shape often plays minor role to models while the texture is strongly preferred. Humans instead predominantly decide by the objects shape (96% shape over texture decisions). This finding has received significant attention [5, 6, 7, 8, 9, 10], because it shows that our vision models, while approaching near-human levels in accuracy on specialized tasks, poorly approximate fundamentals of human perception. As the current generation of deep learning models is increasingly multimodal [11, 12, 13, 14], it is often unclear whether these models inherit certain biases from their text encoders, vision encoders, or whether those interact and are combined through multimodal fusion. Specifically for the (vision-only) texture vs. shape bias, we are interested in how this bias is influenced by language. If language is indeed able to influence vision-only bias, this offers the possibility of aligning and generally steering biases simply and intuitively via prompts. We study of the texture vs. shape bias and its steerability in recent vision language models (VLMs). Here we use VLM to refer to text/vision-input models that output text generated by large language model (LLM). Our investigation shows that the models inherent bias towards texture is far less pronounced in VLMs than in most previously studied vision-only models. As shown in Fig. 1, most VLMs decide by shape more often than by texture, even when prompted in neutral way (while not approaching the human shape bias). Further, we can confirm that VLMs learn to understand the visual concepts of shape and texture, such that they allow steering of the prediction to some extent by simple prompt modifications. Our contribution can be summarized into the following findings: Text appears to influence visual texture/shape bias. Through large-scale study, we report that even though texture bias is vision-only bias found in common vision models, VLMs are often more shape-biased than their vision encoders, even though they decide based on texture more often than humans would do (Sec. 4). VLMs have an understanding of the visual concepts shape and texture which allows steering. Given that multimodal models do not appear to simply inherit visual biases from their vision encoders, this suggests that we may be able to steer visual biases through language. We test and confirm this hypothesis with set of experiments. For instance, shape bias can be steered as low as 49% and as high as 72% through prompting alone: given that visual biases can be influenced through text, this insight can be harnessed to steer them (Sec. 5)."
        },
        {
            "title": "2 Related Work",
            "content": "Vision Language Models. Sparked by the success of vision language pretraining [11, 15, 16, 17], where features extracted from image-text paired data are aligned in joint embedding space, recent VLMs added language modeling during training [18, 19, 20], enabling models to reason about images. Subsequently, finetuning these VLMs on instruction-following data [13, 21, 22, 23, 14], such as reinforcement learning from human feedback (RLHF) [24], enables users to prompt these models, easing their usability for humans. Resulting models are commercialized [12, 25, 26] or open-sourced [21, 23, 2], and consequently become accessible to wide range of users. Shape Bias. Humans primarily rely on shape information to recognize objects. On the other hand standard ImageNet-trained vision-models, such as convolutional neural networks (CNNs), are biased towards texture to make their classification decisions [1]. Nonetheless, shape information can still be present in layers/latent space of the model before the classifier [5, 7]. Prior research has shown that the texture bias of CNNs can be reduced in training [1, 27, 28, 5, 29, 30, 31]. But the network architecture has high influence, and vision-only ViTs [32] were shown to be more shape-biased by default [9], more human-like [33], scalable by data size [34], and can be explicitly designed to separate shape and texture in their token space [9]. Jointly embedding vision and language in these networks (but not CNNs as we will show!), through CLIP [11], further increases their shape bias in zero-shot classification [29]. Yet, these models still do not reach human levels. The only known models to achieve such levels are image-generative classifiers [31], which in some way also combine vision and language. 2 Other Biases. For the sake of completeness, we list other relevant works on uni/multimodal biases. However, we want to point out that the focus of this paper lies on the texture/shape bias. Vision biases have been widely investigated, such as single-demographic effects (race and gender) for face recognition tasks [35, 36], or high-frequency biases for classification tasks [37, 27]. Recently, [10] found another bias that separates human from model vision, by investigating the critical frequency band of object recognition. For language models, several works focus on investigating societal biases, such as gender and race [38, 39] and ways of debiasing them [39, 40, 41], or explicitly forcing them [42]. recent study also found that LLMs are biased towards high-value over likely options [43]. Another study focused on encoded moral beliefs [44]. LLMs can also pick up human traits - one study found that adding \"take deep breath\" to prompts improves performance [45]. Of course, some of the unimodal biases also apply to VLMs (e.g., [45]), but few works have also explicitly focused on biases in VLMs. For example, neurons of CLIP [11] were studied in [46], revealing that some neurons respond to the same concept regardless of its presentation, which is potential reason for the high generalizability. On the other hand, this enables attacks by rendering text on images (typographic attacks). Additionally, several works demonstrated that VLMs fail to count objects [11, 47, 48], and generally struggle in structured tasks [34]."
        },
        {
            "title": "3 Methodology",
            "content": "A cornerstone of our analysis is the measurement of texture/shape bias in (LLM-based) VLMs when performing tasks that require some level of image classification. In the following subsections, we introduce how and in which tasks exactly we measure shape bias. 3.1 Measuring Texture/Shape Bias We use the texture-shape cue-conflict classification problem (cue-conflict) [1] consisting of 1,280 samples with conflicting shape and texture cues synthetically generated via style transfer model [49] from ImageNet [50] samples (see Fig. 1 for examples). The shape and texture classes belong to 16 super-classes of ImageNet. Following [1], we have excluded 80 images from the dataset where texture and shape cues belong to the same class. We want to emphasize that from an information perspective alone, predicting either label (or both) would be correct. However, humans tend to prioritize the shape cue for predictions which is in stark contrast to most models [1]. Using the shape or texture cue label as the correct label allows us to measure the shape and texture accuracy, respectively. Based on these measurements, we measure the cue accuracy as the ratio of predictions that contain either the shape or texture label (as opposed to misclassification): Cue Accuracy = Shape Accuracy + Texture Accuracy (1) Throughout the paper, we will refer to this as the accuracy. We use the definition of shape bias [1], which is defined by the ratio of shape decisions over accurate decisions: Shape Bias = Shape Accuracy Cue Accuracy (2) While we primarily focus on measuring the shape bias in this study, the accuracy provides an important signal of the model quality and robustness, as well as for the comparability of results. 3.2 VLM Tasks We measure the texture/shape bias of VLMs in two common tasks: visual question answering (VQA) [51] where we seek to obtain zero-shot classification [11] of the object, and image captioning [52] where we look for an accurate description of the image. In both cases, we use the cue-conflict dataset in single-round answering with no shared conversation history between conversations. 3.2.1 Image Captioning. In this task, we are instructing models to generate brief descriptions (\"Describe the image. Keep your response short.\"). We specifically request the model to provide short response to 3 encourage it to single out the most crucial aspects of the image according to its judgment. Additionally, this has the benefit of faster inference. Response Extraction. As the responses are open-ended we rely on zero-shot classifications of the generated description to marginalize out the most descriptive class. To this end, we embed the generated descriptions and all (raw) class labels using ember-v1 [53] and predict the class with the smallest cosine distance (similar to zero-shot classification in CLIP [11]). However, the generated caption may refer to multiple class labels (or none). As an additional signal, we perform more granular analysis using an additional LLM (Nous-Hermes-2-Mixtral-8x7B-DPO [54]) by instructing the model to extract all mentioned classes (similar to [55]). This allows us to understand if the model detects both cues (but the embedding model enforces specific prediction) and to quantify how often the model response is too generic to detect any class. 3.2.2 VQA Classification. Following the questioning style in LLaVA [56], we ask the model \"Which option best describes the image?\" and provide an alphabetic enumeration of all class labels in the style \"A. airplane\". For simpler response extraction and confidence evaluation (see below), we end the prompt by instructing the model to answer with only the letter corresponding to the correct answer (\"Answer with the options letter from the given choices directly.\"). Compared to captioning, this is similar to the discrimination in ImageNet [50] image classifiers [57, 58, 59, 60, 32] in the sense that it only allows the model to respond with single class and does not provide an option to not answer - if models follow the instruction. Response Extraction. Despite instructing the models to only respond with an option letter, we observe multiple response styles: option letter + label (\"H. cat.\"), just the label (\"cat.\"), long explanation containing the option letter and/or label (\"The image features black and white image of cat.\"). In all cases, punctuation and capitalization may be different (\"H.\", \"H\", \"h)\"). The first two response styles are easily correctable by simple post-processing (we prioritize the option letter in case of conflicting option letter and label), and in some cases, explanations can be corrected as well if the response includes the option letter. However, we avoid heavy post-processing and consider individual answers as wrong if they are not recoverable. In most cases, the ratio of these is negligible. Confidence of VQA predictions. All answer options in our prompts correspond to single character and, thus, token. Well-behaving models, where the response consistently starts with the option letter (and nothing else) allow us to gather insights into the prediction process. For these models, the logits of each token correspond to the logits of option letters. By applying softmax function we can then analyze the confidence in each option (we map invalid tokens to separate null class). This allows us to perform additional studies on sampling behavior."
        },
        {
            "title": "4 Are VLMs Biased towards Texture or Shape?",
            "content": "In this section, we measure the shape bias of VLMs and try to understand if there are driving factors (specific models, scale, RLHF [24] etc.) behind potential differences in behavior. Beyond this quantitative analysis, we aim to better understand the internal mechanisms through insights into the decision formation and ablation of the representation of the vision encoder. 4.1 Main Results We start our experimental evaluation by measuring the shape bias in the VQA and captioning tasks, using collection of diverse vision language models reflecting the multitude of research directions. These models include connections of common pretrained CLIP encoders and LLMs [21, 61, 62, 23, 63, 64, 65, 66], mixture-of-expert-LLMs [67], optimized architectures for resource-constrained systems [68], finetuning with RLHF [69, 12], massive vision encoders [2]. Additionally, we survey commercial, closed-source models like Gemini Pro Vision 1.0 [25], GPT-4V (Preview) [12], and Qwen-VL Plus/Max [26] where access is limited to APIs and few details are known. For detailed list of models, we refer the reader to the appendix. 4 Figure 2: Most VLMs are slightly shape-biased but some models show differences when asked to describe an image compared to VQA. We measure the shape bias on the cue-conflict dataset [1]. For reference, we also provide measurements on ResNet-50 [59] from the initial shape bias study [1], zero-shot classification (CLIP ViT-L/14 [11]), and human average (over 10 subjects [1]). The results in Fig. 2 (a full table is shown in the appendix) paint fairly uniform picture. Across different models and on two different tasks, most VLMs seem to perform relatively similarly in terms of shape bias. Shape bias of VLMs is still significantly lower than in humans (96%), but overall higher than in typical image-only discriminative classifiers (e.g., 22% for an ImageNet-trained ResNet-50 [59, 1]). For most models, shape bias is higher than the shape bias of CLIP ViT-L/14 [11] (60%) which is common vision encoder used in many of our tested models. On average, we observe slightly higher shape bias for the image captioning task at some cost in accuracy (mostly due to generic captions that do not mention any objects) and vice-versa. For image description tasks, it can be significantly lower (38.2%) for some outliers but generally ranges from 54.1 - 73.2%. For VQA it ranges from 52.9 - 73.8%. In this task, we had to remove some models as they did not follow instructions. Some of these models are on the lower end of the shape bias in captioning, which raises the question of whether there might exist correlation between instruction tuning quality and shape bias (to answer this question, more samples would be needed for conclusive answer). Which models are the most shape-biased? The strongest shape bias is observed in InstructBLIP Vicuna-7B [23] for VQA, but the model generally shows lower accuracy compared to other models. more accurate model is InternVL-Chat 1.1 [2] which ranks second place for VQA but first for captioning. GPT-4V is an outlier. Given that GPT-4V often achieves SOTA performance and is considered an important baseline, it has surprisingly poor accuracy compared to most other models - mostly due to refusal to answer: 131/1280 VQA conversations, i.e., roughly 10% which is substantially higher than the refusal rate of all other models (< 1%). Note that refusal rates do not affect the shape bias measurement. GPT-4V is also the model with the largest amount of generic image captions (60.4%). We acknowledge that other prompts may have led to other results, however, the result is noteworthy, as the other VLMs mostly behave well under the same prompts. Interestingly, GPT-4V is also significantly more texture-biased than most models in both tasks. Does scale matter? LLM capacity does not seem to correlate with shape bias and unpredictably skews the shape bias by few percent in each way as can be seen in Qwen-VL, LLaVA v1.5/NeXT/RLHF, or InternVL. Similarly, the overall largest models do not have the highest shape bias. However, scale usually improves accuracy. 5 (a) (b) (c) Figure 3: VLMs provide high-confidence answers in our test that prefer one specific cue and ignore the other. Confidence only slightly differs by cue, but answers with the highest confidence exclusively belong to shape decisions. (a) Confidence distribution of shape and texture tokens for all samples, (b) Confidence distribution in correct predictions (i.e., the predicted label matches either shape or texture), (c) Evolution of shape bias if filtering for confidence. Measured on LLaVA-NeXT 7B for the VQA task. Does RLHF align shape bias? RLHF-tuned VLMs are still rare at this point and we only have three samples. On both LLaVA-RLHF [69] models we see no changes in comparison to the default LLaVA models. GPT-4V [12] (though it is unclear if vision was also RLHF trained) shows one of the lowest shape biases in our study, but we do not know how the base model ranks. Overall it is hard to derive conclusive answer, but it seems that at least RLHF does not necessarily guarantee an alignment of visual preferences. 4.2 Examining Decision Formation To better understand how decisions are formed we utilize confidence measurement as described in Sec. 3.2. Exemplarily, we choose LLaVA-NeXT 7B [62] to study the sampling of responses in VQA. The choice of this model is motivated by the fact that we can obtain access to the logits, in addition to the models ability to consistently follow the instructions as opposed to only responding with the option letter. In Fig. 3a, we visualize the confidence of the token corresponding to the shape or texture answer option. We were expecting some normal distribution that is slightly offset in favor of shape bias. However, to our surprise, we find that confidence in both options is almost binary. Analogously, when we only focus on correct answers (Fig. 3b), we observe that the model is highly confident in its responses. As the model assigns such high confidence to the cue it answers with, this suggests that information from the other cue, in fact, gets lost in processing. This is also backed by the fact that the second (significantly less confident) prediction token often (70.7%) does not belong to the conflicting cue. Only 17.7% of the top-2 pairs contain both, shape and texture. On average, we also observe that the model is slightly less confident in texture decisions (p = 0.87 vs. 0.92). This becomes even more evident when we only analyze responses above confidence threshold (Fig. 3c). Up until = 0.5, this does not significantly influence any of the measurements. Beyond this point, the ratio of shape decisions increases without decrease in texture decisions resulting in higher shape bias. Beyond = 0.9 the ratios diverge, and the model reasons exclusively on shape cues. It is not clear if these findings generalize to other models - e.g., GPT-4 was shown to be significantly less confident compared to its base model [70]. However, LLaVA is currently the building block for many successors [62, 69] and as such will strongly induce its bias into those. The next section also shows that many VLMs actually reason very similarly in our tasks hinting that our results may generalize after all. 4.3 The Role of the Vision Encoder The texture/shape bias is primarily vision bias - we ablate the vision encoder to understand how it drives the model decision. For this section, we additionally dissect inference from encoder pretraining. Do VLMs just inherit the shape bias from their encoders? Most VLMs combine (frozen) CLIP vision tower with an LLM via some projector [23, 21, 13]. This raises the question of whether these 6 VLMs simply learn to perform zero-shot classification using their encoders whenever the prompt requires some form of classification - thus, inheriting the shape bias from the encoder. To gain insights into this we measure the respective difference in shape bias (and accuracy). We see that shape bias is indeed (strongly) influenced by the encoder, but the VLM can deviate in either direction (9.5% to +7.4% in our examples in the appendix). Further, all models decrease in accuracy compared to their encoders (all of which were frozen during expansion into VLMs). This is not surprising, as VLMs are less specialized, but it shows that VLMs do not simply pass through their encoders classifications and have some form of bias control. As an additional signal, we measure the error consistency [71] for insights into the alignment: pair of models with high error consistency will share some traits in their internal decision-making process. We observe that VLMs show high but not perfect error consistency to their respective encoders (up to 79.8%). Generally, they are fairly similar to all zero-shot classifiers that we benchmarked (except CNN-based models; see below). This trend even prevails in models with different (MoE)-LLMs but the same encoder. In line with our previous results, this further emphasizes that shape bias in VLMs is influenced by the encoder, but their multimodal fusion with LLMs allows them to deviate beyond that. the combination of vision and language. Does multimodal training guarantee shape-biased encoders? Next, we aim to understand critical aspect of encoder training: It was already demonstrated that ViT-CLIP models show an increased shape bias in comparison to vision-only models independent of their architecture, training data, or method [29]. This may suggest that joint embedding alone increases shape bias. To verify this hypothesis, we measure the shape bias of ResNet-50-based CLIP [11] - and observe an opposite trend. With just 20.8% (refer to the appendix for details), this model even slightly reduced shape bias compared to an ImageNet-trained ResNet-50 (22.2%). As such, just fusing language into encoder training does not guarantee an increased shape bias and the results strongly depend on the vision architecture, as well. For the design of shape-biased VLMs it is, thus, reasonable to rely on representations of ViT-based CLIP as opposed to vision-only models."
        },
        {
            "title": "5 Steering Texture/Shape Bias",
            "content": "In the previous section, we have seen that in VLMs, visual biases are not simply inherited from the vision encoder, but the fusion with an LLM (including training) plays crucial role. We here seek to test how flexible the representation of texture/shape bias is and whether we can systematically steer it - first through visual steering or preprocessing (Sec. 5.1), and then through language or prompt engineering (Sec. 5.2). Since texture/shape bias is visual bias, we generally expect more flexibility with vision steering. However, being able to steer visual bias through language alone provides powerful way of shaping visual biases in user-specified way. 5.1 Steering in Vision Earlier work demonstrated that ImageNetmodels can still detect objects even if the image is split into patches and shuffled [72, 6, 9]. As patch size decreases, the operation is destroying more global shape information, yet retaining local texture information. While it is straightforward to assume that this also applies to VLMs and significantly reduces shape bias, we emphasize quantifying the shape bias precisely and understanding whether detection is still possible for these models, as they might be more intricate than specialized models. Oppositely, to increase the shape bias we experiment with Gaussian noise processed inputs. This is loosely inspired by applying diffusionlike noise during training and inference which Figure 5: Image preprocessing can strongly steer texture/shape bias. Left: Shuffling image patches with decreasing patch size results in strong texture bias, Right: Increasing Gaussian noise introduces strong shape bias. Figure 4: VLMs make similar errors on the cue-conflict datasets and share similarities with their vision encoders. In terms of errors, VLMs are also more similar to humans than ImageNet-trained/finetuned models. We measure the pair-wise error consistency [71] between predictions. For this analysis, an error is any answer that does not belong to the shape class (analogous to [29]). Shown responses belong to LLM-based VLMs (under the VQA task), other selected models including ImageNet models, (some) VLM encoders under ImageNet-finetuning and zero-shot classification, and ten human subjects. has been shown to drastically improve the shape bias of ImageNet-ResNets [31]. However, we only apply the noise during inference and use more simplistic approach by adding (0, σ2) noise to all channels, consecutively clamping values to [0, 1]. We show results on LLaVA-NeXT 7B in Fig. 5, also in comparison to its CLIP encoder under zero-shot classification, and finetuned on ImageNet. Adding noise results increases the VLMs shape bias up to 89.5% at σ2 = 0.5, and patch shuffling decreases shape bias (increases texture bias) to 8.4% at 28 28 patches. We see similar trends for the encoder, but the VLM overall achieves the highest shape bias. Inspired by these strong results, we repeat the experiments on the more shape-biased and larger InternVL-Chat 1.1 [2]. In this model, we can further extend the range 91.7% shape bias (σ2 = 0.3), and down to 6.1% (28 28 patches). While extreme steering of shape bias comes at cost in accuracy, all results are still well beyond random chance (min: 48.6% accuracy). In conclusion, the shape bias can be strongly steered in the vision representation (including processing by the LLM) to almost both extremesmore shape-biased and more texture-biased. 8 Figure 6: Prompts can steer the texture/shape bias to some extent. We test the same texture/shape-biased instructions on multiple models, showing that these can already shift some decisions (usually in favor of texture). For InternVL 1.1 and LLaVA-NeXT 7B we additionally test the understanding of texture/shape by using synonyms. Furthermore, we use an LLM to automatically search for specific prompts to optimize in either direction. 5.2 Steering in Language: Prompt Engineering Our previous results suggest that VLMs learn connected multimodal understanding of shape and texture. This opens the question of whether visual biases can be influenced through text processing in these models. We test this hypothesis by recording texture/shape bias as function steering them via text through prompt engineering. Bias steering through hand-crafted prompting. We start by asking VLMs to specifically identify either the shape or the texture category in given cue-conflict image. This hand-crafted biased prompting approach does indeed steer shape bias to certain degree: As shown in Fig. 6, prompting can steer visual bias (without significantly affecting accuracy). Details on these biased prompts can be found in the appendix. Neutral prompts perform often similarly to shape-biased prompts, whereas texture-biased prompts deviate more significantly. This suggests that models may be more inclined to use shape by default, but also have access to certain amount of texture information which can be accessed through biased prompting. For two models (InternVL-Chat 1.1 [2] and LLaVA-NeXT 7B [62]) we additionally replace the terms texture/shape by strong synonyms obtained from Thesaurus.com [73, 74]. Synonyms of either term can steer shape bias as well to certain degree. For texture synonyms we observe more variance, as texture is overloaded by different meanings (e.g., some synonyms like feeling, taste, or touch are unrelated to texture in vision). In contrast, shape is fairly well-defined term. While the effect of steering by language is systematically visible, language steering alone does not fundamentally change the reliance on the underlying cue. This effect does not appear to be limitation of LLM capacity: We performed an additional study on InternVL-Chat 1.2+ (34B vs. 13B) but did not obtain evidence that larger LLMs guarantee more steerability. Interestingly, our findings of steering by language show parallels to human psychophysical laboratory experiment. [1] conducted control experiments in which humans were either asked to identify the shape while ignoring texture, or conversely to identify the texture while ignoring the shape. This human prompt steering worked, but only to certain extent: When humans were tasked to ignore the shape, the human shape bias decreased from 96% (neutral instruction) only to approx. 70% shape bias (texture-biased instruction). Our models behave somewhat similarly: their visual shape bias can be steered through prompting, but it appears hard for them to completely go against their default visual bias. Bias steering through automated prompt engineering. Above we observed that hand-crafted prompts can steer visual biases, but only to limited extent. Does this indicate limit on how much language/prompting can influence biases, or merely reflect that the handcrafted prompts were chosen suboptimally? To investigate this question, we tested an automatically crafted prompt. This is achieved by employing an LLM as optimizer [45] to continuously generate new prompts in natural language targeting to maximize either shape or texture bias in feedback loop. We provide the LLM feedback about the achieved accuracy and shape bias. Additionally, we opt for greedy token sampling in the VLM to reduce noise in the feedback loop. For further details, we refer the reader to the appendix. The results are shown in Fig. 6 in gray and denoted as search. We observe that for both LLaVA-NeXT-7B and InternVL-Chat 1.1, automatically generated prompts exceed the manually crafted biased prompts in terms of their effectiveness to increase texture bias, and roughly match them when it comes to increasing shape bias. For InternVL-Chat 1.1 the delta between both extremes is 23.3%, which can only serve as lower bound and is likely improvable by better design of the LLM task (or using other optimizers). In line with hand-crafted prompts, overall accuracy does not change considerably, which is key differentiator from the vision-steering we explored in the section above. We should also note that the optimization here is done with respect to the cue-conflict test set; this is simply done as proof of concept to show that there are prompts which are able to substantially influence visual biases and not to claim SOTA shape bias. It would be interesting to test whether the generated prompts generalize to other models or datasets. Taken together, language-based steering clearly has more reduced effect compared to visual steering. That said, unlike visual steering through image modifications which substantially hurts accuracy, our prompting strategy barely affects accuracy. Additionally, the method may be more intuitive for practitioners who can access or change the prompt as desired."
        },
        {
            "title": "6 Conclusion",
            "content": "In this study, we investigated texture/shape bias in VLMs. Through the lens of this specific bias, we are able to assess whether visual biases are inherited from vision encoders or modified through text processing. We find that the latter is the case: VLMs are often more shape-biased than their vision-only backbones. Beyond that, an intriguing aspect of VLMs is that the task/response can be steered through not just vision but also through language. Our experiments have demonstrated that VLMs have learned multimodal association between the terms shape/texture, their synonyms, and their respective visual concepts to some extent. Taken together, we see that when it comes to multimodal models, the sum is greater than the parts, as visual bias (texture/shape bias) can be steered to certain degree through both manual and automated prompt engineering, alone. Similar to the active research line on prompt engineering for CLIP [75, 76, 77, 78], it remains to be seen if prompts exist that can steer texture/shape bias to the same extent as vision can. While this aligns with human behavior in psychophysical studies [1], it remains unclear if this is reflection of learned bias from human-annotated data or spurious correlation. We plan to explore this further in future studies. Lastly, despite experiments on an extensive and diverse collection of VLMs, we have seen (surprisingly) homogenous landscape in terms of texture/shape bias, steerability, and error consistency. We are curious to see how our insights apply under more radical training/architecture changes and to other visual biases. We encourage the community to provide more open-source implementations without which studies like ours would not be feasible. Limitations. Our study, like most studies, is not free of limitations. Although we utilized diverse array of VLMs, there is possibility that we overlooked significant outliers. Moreover, the applicability of our findings to future VLMs remains uncertain; however, we believe our results effectively reflect the current landscape. potential bias introduced in our study stems from the prompts we employed. Different prompts could have yielded disparate results, and certain models might have performed differently, particularly in Visual Question Answering (VQA) tasks. While we mitigated this by utilizing simple, widely used prompts (refer to the appendix for an exploration of alternatives), the possibility of better choices remains open for future investigation."
        },
        {
            "title": "References",
            "content": "[1] Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, and Wieland Brendel, Imagenet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness., in International Conference on Learning Representations, 2019. 1, 2, 3, 5, 9, 10, 25 [2] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai, Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks, 2024. 1, 2, 4, 5, 8, 9, 17, 18, 25 [3] D.H. Wolpert and W.G. Macready, No free lunch theorems for optimization, IEEE Transactions on Evolutionary Computation, vol. 1, no. 1, pp. 6782, 1997. 1 [4] David H. Wolpert, The Lack of Priori Distinctions Between Learning Algorithms, Neural Computation, vol. 8, pp. 13411390, 10 1996. [5] Katherine Hermann, Ting Chen, and Simon Kornblith, The origins and prevalence of texture bias in convolutional neural networks, in Advances in Neural Information Processing Systems (H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, eds.), vol. 33, pp. 19000 19015, Curran Associates, Inc., 2020. 2 [6] Baifeng Shi, Dinghuai Zhang, Qi Dai, Zhanxing Zhu, Yadong Mu, and Jingdong Wang, Informative dropout for robust representation learning: shape-bias perspective, in Proceedings of the 37th International Conference on Machine Learning (Hal Daumé III and Aarti Singh, eds.), vol. 119 of Proceedings of Machine Learning Research, pp. 88288839, PMLR, 1318 Jul 2020. 2, 7 [7] Md Amirul Islam, Matthew Kowal, Patrick Esser, Sen Jia, Björn Ommer, Konstantinos G. Derpanis, and Neil Bruce, Shape or texture: Understanding discriminative features in CNNs, in International Conference on Learning Representations, 2021. 2 [8] Elior Benarous, Sotiris Anagnostidis, Luca Biggio, and Thomas Hofmann, Harnessing synthetic datasets: The role of shape bias in deep neural network generalization, in NeurIPS 2023 Workshop on Synthetic Data Generation with Generative AI, 2023. 2 [9] Muhammad Muzammal Naseer, Kanchana Ranasinghe, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang, Intriguing properties of vision transformers, in Advances in Neural Information Processing Systems (M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, eds.), vol. 34, pp. 2329623308, Curran Associates, Inc., 2021. 2, 7 [10] Ajay Subramanian, Elena Sizikova, Najib Majaj, and Denis Pelli, Spatial-frequency channels, shape bias, and adversarial robustness, Advances in Neural Information Processing Systems, vol. 36, 2024. 2, [11] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever, Learning transferable visual models from natural language supervision, in Proceedings of the 38th International Conference on Machine Learning (Marina Meila and Tong Zhang, eds.), vol. 139 of Proceedings of Machine Learning Research, pp. 87488763, PMLR, 1824 Jul 2021. 2, 3, 4, 5, 7, 23, 24 [12] OpenAI, Gpt-4 technical report, 2023. 2, 4, 6, 17, 18, 25 [13] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikoł aj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karén Simonyan, Flamingo: visual language model for few-shot learning, in Advances in Neural Information Processing Systems (S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, eds.), vol. 35, pp. 2371623736, Curran Associates, Inc., 2022. 2, 6 [14] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, and Furu Wei, Language is not all 11 you need: Aligning perception with language models, in Thirty-seventh Conference on Neural Information Processing Systems, 2023. 2 [15] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig, Scaling up visual and vision-language representation learning with noisy text supervision, in International conference on machine learning, pp. 49044916, PMLR, 2021. [16] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer, Lit: Zero-shot transfer with locked-image text tuning, CVPR, 2022. 2 [17] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao, Eva-clip: Improved training techniques for clip at scale, 2023. 2, 24 [18] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi, Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation, in ICML, 2022. 2 [19] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu, Coca: Contrastive captioners are image-text foundation models, in TMLR, 2022. 2 [20] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi, Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, 2023. 2 [21] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee, Visual instruction tuning, in Thirty-seventh Conference on Neural Information Processing Systems, 2023. 2, 4, 6 [22] Gen Luo, Yiyi Zhou, Tianhe Ren, Shengxin Chen, Xiaoshuai Sun, and Rongrong Ji, Cheap and quick: Efficient vision-language instruction tuning for large language models, in Thirty-seventh Conference on Neural Information Processing Systems, 2023. 2 [23] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi, InstructBLIP: Towards general-purpose vision-language models with instruction tuning, in Thirty-seventh Conference on Neural Information Processing Systems, 2023. 2, 4, 5, 6, 17, 18, 19, 24, 25 [24] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe, Training language models to follow instructions with human feedback, in Advances in Neural Information Processing Systems (S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, eds.), vol. 35, pp. 2773027744, Curran Associates, Inc., 2022. 2, [25] Gemini Team, Gemini: family of highly capable multimodal models, 2023. 2, 4, 17, 18 [26] Qwen Team, Introducing qwen-vl. https://qwenlm.github.io/blog/qwen-vl/ [Accessed: 12 February 2024], Jan 2024. 2, 4, 17, 18 [27] Jovita Lukasik, Paul Gavrikov, Janis Keuper, and Margret Keuper, Improving native CNN robustness with filter frequency regularization, Transactions on Machine Learning Research, 2023. 2, 3 [28] Yingwei Li, Qihang Yu, Mingxing Tan, Jieru Mei, Peng Tang, Wei Shen, Alan Yuille, and cihang xie, Shape-texture debiased neural network training, in International Conference on Learning Representations, 2021. 2 [29] Robert Geirhos, Kantharaju Narayanappa, Benjamin Mitzkus, Tizian Thieringer, Matthias Bethge, Felix A. Wichmann, and Wieland Brendel, Partial success in closing the gap between human and machine vision, in Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, 2021. 2, 7, 8, 23, [30] Paul Gavrikov, Janis Keuper, and Margret Keuper, An extended study of human-like behavior under adversarial training, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pp. 23602367, June 2023. 2 [31] Priyank Jaini, Kevin Clark, and Robert Geirhos, Intriguing properties of generative classifiers, 2024. 2, 8 12 [32] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby, An image is worth 16x16 words: Transformers for image recognition at scale, in International Conference on Learning Representations, 2021. 2, 4, 25 [33] Shikhar Tuli, Ishita Dasgupta, Erin Grant, and Thomas L. Griffiths, Are convolutional neural networks or transformers more like human vision?, 2021. [34] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer, Scaling vision transformers, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1210412113, June 2022. 2, 3 [35] Joy Buolamwini and Timnit Gebru, Gender shades: Intersectional accuracy disparities in commercial gender classification, in Conference on Fairness, Accountability and Transparency, FAT 2018, 23-24 February 2018, New York, NY, USA, vol. 81 of Proceedings of Machine Learning Research, pp. 7791, PMLR, 2018. 3 [36] Inioluwa Deborah Raji and Joy Buolamwini, Actionable auditing: Investigating the impact of publicly naming biased performance results of commercial AI products, in Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, AIES 2019, Honolulu, HI, USA, January 27-28, 2019, pp. 429435, ACM, 2019. 3 [37] Haohan Wang, Xindi Wu, Zeyi Huang, and Eric P. Xing, High-frequency component helps explain the generalization of convolutional neural networks, in 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pp. 86818691, Computer Vision Foundation / IEEE, 2020. 3 [38] Soumya Barikeri, Anne Lauscher, Ivan Vulic, and Goran Glavas, Redditbias: real-world resource for bias evaluation and debiasing of conversational language models, in Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pp. 19411955, Association for Computational Linguistics, 2021. 3 [39] Anne Lauscher, Tobias Lüken, and Goran Glavas, Sustainable modular debiasing of language models, in Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November, 2021, pp. 47824797, Association for Computational Linguistics, 2021. [40] Nicholas Meade, Elinor Poole-Dayan, and Siva Reddy, An empirical survey of the effectiveness of debiasing techniques for pre-trained language models, in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pp. 18781898, Association for Computational Linguistics, 2022. 3 [41] Yue Guo, Yi Yang, and Ahmed Abbasi, Auto-debias: Debiasing masked language models with automated biased prompts, in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pp. 10121023, Association for Computational Linguistics, 2022. 3 [42] Patrick Haller, Ansar Aynetdinov, and Alan Akbik, Opiniongpt: Modelling explicit biases in instruction-tuned llms, 2023. 3 [43] Sarath Sivaprasad, Pramod Kaushik, Sahar Abdelnabi, and Mario Fritz, Exploring value biases: How llms deviate towards the ideal, 2024. [44] Nino Scherrer, Claudia Shi, Amir Feder, and David Blei, Evaluating the moral beliefs encoded in LLMs, in Thirty-seventh Conference on Neural Information Processing Systems, 2023. 3 [45] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc Le, Denny Zhou, and Xinyun Chen, Large language models as optimizers, in The Twelfth International Conference on Learning Representations, 2024. 3, 10, 21 [46] Gabriel Goh, Nick Cammarata, Chelsea Voss, Shan Carter, Michael Petrov, Ludwig Schubert, Alec Radford, and Chris Olah, Multimodal Neurons in Artificial Neural Networks, Distill, vol. 6, p. e30, Mar. 2021. 3 [47] Nan Liu, Shuang Li, Yilun Du, Josh Tenenbaum, and Antonio Torralba, Learning to compose visual relations, in Advances in Neural Information Processing Systems (M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, eds.), vol. 34, pp. 2316623178, Curran Associates, Inc., 2021. 3 [48] Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross, Winoground: Probing vision and language models for visio-linguistic compositionality, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 52385248, June 2022. 3 [49] Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge, Image style transfer using convolutional neural networks, in 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 24142423, 2016. 3 [50] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei, Imagenet: largescale hierarchical image database, in 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 248255, 2009. 3, 4 [51] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh, Vqa: Visual question answering, in Proceedings of the IEEE International Conference on Computer Vision (ICCV), December 2015. 3 [52] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan, Show and tell: neural image caption generator, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015. [53] LLMRails, llmrails/ember-v1 Hugging Face, Feb. 2024. [Online; accessed 27. Feb. 2024]. 4 [54] Teknium, theemozilla, karan4d, and huemin_art, Nous Hermes 2 Mistral 7B DPO, Feb. 2024. [Online; accessed 27. Feb. 2024]. 4, 20 [55] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas, Videogpt: Video generation using vq-vae and transformers, 2021. 4 [56] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, and Jianfeng Gao, Multimodal foundation models: From specialists to general-purpose assistants, 2023. [57] Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, Imagenet classification with deep convolutional neural networks, in Advances in Neural Information Processing Systems (F. Pereira, C.J. Burges, L. Bottou, and K.Q. Weinberger, eds.), vol. 25, Curran Associates, Inc., 2012. 4 [58] Rupesh Kumar Srivastava, Klaus Greff, and Jürgen Schmidhuber, Highway networks, 2015. 4 [59] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, Deep residual learning for image recognition, 2015. 4, 5, 25 [60] Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger, Densely connected convolutional networks, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017. 4 [61] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee, Improved baselines with visual instruction tuning, 2023. 4, 17, 18, 19, 24 [62] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee, Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. 4, 6, 9, 17, 18, 19, 24 [63] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou, Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond, 2023. 4, 17, 18 [64] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxuan Zhang, Juanzi Li, Bin Xu, Yuxiao Dong, Ming Ding, and Jie Tang, Cogagent: visual language model for gui agents, 2023. 4, 17, 18 [65] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang, Cogvlm: Visual expert for pretrained language models, 2023. 4, 17, 18 [66] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang, Generative multimodal models are in-context learners, 2023. 4, 17, 18, 14 [67] Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng Jin, Jinfa Huang, Junwu Zhang, Munan Ning, and Li Yuan, Moe-llava: Mixture of experts for large vision-language models, 2024. 4, 17, 18, 24 [68] Mikhail Kim, Vladimir Orshulevich, and Ash Vardanian, UForm by Unum Cloud, Jan. 2023. 4, 17, 18 [69] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer, and Trevor Darrell, Aligning large multimodal models with factually augmented rlhf, 2023. 4, 6, 17, 18 [70] OpenAI, GPT-4, Mar. 2024. [Online; accessed 6. Mar. 2024]. [71] Robert Geirhos, Kristof Meding, and Felix A. Wichmann, Beyond accuracy: quantifying trial-by-trial behaviour of cnns and humans by measuring error consistency, in Advances in Neural Information Processing Systems (H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, eds.), vol. 33, pp. 1389013902, Curran Associates, Inc., 2020. 7, 8 [72] Tianyuan Zhang and Zhanxing Zhu, Interpreting adversarially trained convolutional neural networks, in Proceedings of the 36th International Conference on Machine Learning (Kamalika Chaudhuri and Ruslan Salakhutdinov, eds.), vol. 97 of Proceedings of Machine Learning Research, pp. 75027511, PMLR, 0915 Jun 2019. 7 [73] Dictionary.com, texture, Feb. 2024. [Online; accessed 25. Feb. 2024]. 9, 19 [74] Dictionary.com, shape, Feb. 2024. [Online; accessed 25. Feb. 2024]. 9, 19 [75] Karsten Roth, Jae Myung Kim, A. Sophia Koepke, Oriol Vinyals, Cordelia Schmid, and Zeynep Akata, Waffling around for performance: Visual classification with random words and broad concepts, in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 1574615757, October 2023. 10 [76] Sachit Menon and Carl Vondrick, Visual classification via description from large language models, 2022. 10 [77] Sarah Pratt, Ian Covert, Rosanne Liu, and Ali Farhadi, What does platypus look like? generating customized prompts for zero-shot image classification, in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 1569115701, October 2023. 10 [78] Muhammad Jehanzeb Mirza, Leonid Karlinsky, Wei Lin, Horst Possegger, Mateusz Kozinski, Rogerio Feris, and Horst Bischof, LaFTer: Label-free tuning of zero-shot classifier using language and unlabeled image collections, in Thirty-seventh Conference on Neural Information Processing Systems, 2023. 10 [79] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu, Qwen technical report, arXiv preprint arXiv:2309.16609, 2023. 17 [80] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos Riquelme Ruiz, Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj Kumar, Sjoerd Van Steenkiste, Gamaleldin Fathy Elsayed, Aravindh Mahendran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bastings, Mark Collier, Alexey A. Gritsenko, Vighnesh Birodkar, Cristina Nader Vasconcelos, Yi Tay, Thomas Mensink, Alexander Kolesnikov, Filip Pavetic, Dustin Tran, Thomas Kipf, Mario Lucic, Xiaohua Zhai, Daniel Keysers, Jeremiah J. Harmsen, and Neil Houlsby, Scaling vision transformers to 22 billion parameters, in Proceedings of the 40th International Conference on Machine Learning (Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, eds.), vol. 202 of Proceedings of Machine Learning Research, pp. 74807512, PMLR, 2329 Jul 2023. 17 [81] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger, On calibration of modern neural networks, in Proceedings of the 34th International Conference on Machine Learning (Doina Precup and Yee Whye Teh, eds.), vol. 70 of Proceedings of Machine Learning Research, pp. 13211330, PMLR, 0611 Aug 2017. 23 [82] Quan Sun, Jinsheng Wang, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, and Xinlong Wang, Eva-clip-18b: Scaling clip to 18 billion parameters, 2024. 24 [83] Ross Wightman, Pytorch image models. https://github.com/rwightman/ pytorch-image-models, 2019. 25 [84] Rui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming Xiong, Yingbo Zhou, and Semih Yavuz, Sfr-embedded-mistral. Salesforce AI Research Blog, 2024. 25 [85] Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers, MTEB: Massive text embedding benchmark, in Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics (Andreas Vlachos and Isabelle Augenstein, eds.), (Dubrovnik, Croatia), pp. 20142037, Association for Computational Linguistics, May 2023."
        },
        {
            "title": "Appendix",
            "content": "A Overview of VLMs Detailed Results Table Prompting C.1 Main Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Exploration of Alternative Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . C.3 Biased Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . LLM-based Response Extraction Automated Prompt Search Ablation of Temperature Scaling Results on CLIP Models Results on ImageNet-trained Models Additional Thoughts on the Image Captioning Task Class-wise Texture/Shape Bias Responsibility to Human Subjects Visualization of Vision Steering 16 17 17 18 18 19 20 21 23 24 25 25"
        },
        {
            "title": "A Overview of VLMs",
            "content": "Here we provide an overview of all used models from the main paper. Qwen-VL-Chat [63] Qwen-VL Plus/Max [26] CogAgent [64] CogVLM [65] Adds vision capabilities to Qwen-7B [79]. We set repetition penalty of 1.2 for this model. AliBabass proprietary larger variants of Qwen-VL-Chat. Access only via API. special model for interaction with graphical user interfaces (GUIs) at high-resolution. Adds \"trainable visual expert module\" in LLM layers to combine vision and language. Emu2 [66] The 37B model claims \"strong multimodal in-context learning abilities\". InstructBLIP [23] Connects frozen vision encoders and LLMs through trainable Q-Former. Uses Vicuna or FLAN-T5 as LLMs. LLaVA v1.5 [61] Improvements of LLaVA with modifications on the image encoder, the projector, and task-specific data. Uses Vicuna-7/13B as LLM. LLaVA-NeXT [62] Successor of LLaVA v1.5 supporting higher resolutions through patching, and using better SFT training data for training, claiming improved reasoning, OCR, and world knowledge [62]. The 34B version switches from Vicuna-7/13B to Nous Hermes 2 Yi 34B. MoE-LLaVA v1.5 [67] Variants of LLaVA v1.5 employing 4 sparsely activated Mixture-of-Experts (MoE), and smaller LLMs (Qwen, Phi-2, StableLM). LLaVA-RLHF [69] Variants of LLaVA v1.5 aligned with Factually Augmented RLHF (FactRLHF) [69]. UForm-Gen Chat [68] small (1.5B) model for VQA and image captioning finetuned for multimodal chat. Gemini 1.0 Pro Vision [25] Googles proprietary multi-modal model based on the Gemini Pro LLM. Access only via API. InternVL Chat 1.1/1.2+ [2] An open-source effort to provide an alternative to ViT-22B [80]. V1.1 is based on 6B ViT and Vicuna-13B, V1.2+ uses Nous Hermes 2 Yi 34B as LLM including additional SFT on 10x more data. GPT-4V (Preview) [12] OpenAIs proprietary multi-modal model based on the GPT-4 LLM. Access only via API. Often considered to be the most powerful model. For our main analysis, we prompt all models at the default generation parameters (e.g., temperature) unless stated otherwise. Appendix shows why this is not an issue."
        },
        {
            "title": "B Detailed Results Table",
            "content": "Tab. 2 shows the shape bias and accuracy for the VQA and Image Captioning task (see Fig. 2 in the main paper for visualization). For the open-ended Image Captioning responses, we additionally provide evaluations through an LLM (see Sec. 3). These include the number of generated tokens (to measure how effective our Keep [...] short. instruction is), the ratio of responses where exactly one class was detected (single class ratio), and the ratio of responses that do not refer to any description (generic ratio). In Appendix I/Tab. 9, we provide ablations on Image Captioning under the removal of generic responses."
        },
        {
            "title": "C Prompting",
            "content": "This section provides an overview of all the prompts we have used in our study, including fine-grained details and ablation studies on their effectiveness. 17 Table 2: The shape bias and respective accuracy on the cue-conflict dataset for various VLMs in VQA classification or image description tasks. For the image description task, we additionally provide the average number of tokens generated by Vicunas tokenizer and the ratio of responses that only contain single class or are generic (do not mention any class) as judged by separate LLM. - indicates models that did not follow instructions on VQA and could, thus, not be evaluated. Model Gemini 1.0 Pro Vision [25] GPT-4V (Preview) [12] Qwen-VL Plus [26] Qwen-VL Max [26] Qwen-VL Chat [63] InternVL Chat 1.1 [2] InternVL Chat 1.2+ [2] LLaVA v1.5 7B [61] LLaVA v1.5 13B [61] LLaVA-RLHF 7B [69] LLaVA-RLHF 13B [69] LLaVA-NeXT 7B [62] LLaVA-NeXT 13B [62] LLaVA-NeXT 34B [62] MoE-LLaVA-StableLM [67] MoE-LLaVA-Qwen [67] MoE-LLaVA-Phi2 [67] InstructBLIP Flan-T5-xl [23] InstructBLIP Vicuna-7B [23] Emu2-Chat [66] CogAgent Chat [64] CogVLM Chat [65] UForm Gen Chat [68] VQA Image Captioning Shape Bias [%] AccuShape racy [%] Bias [%] AccuAvg. Single Class Generic racy [%] Tokens Ratio [%] Ratio [%] 64.1 47.9 64.8 62.4 - 68.3 61.1 61.4 64.1 61.7 63.4 59.2 57.2 56.0 59.1 62.9 59.6 68.2 73.8 52.9 - - - 82.33 69.75 82.92 85.50 - 89.33 90.83 80.75 80.25 68.08 80.42 82.58 83.42 73.83 80.08 59.50 82.33 79.58 72.25 75.08 - - - 63.2 53.6 67.9 69.7 38.2 73.2 61.3 61.4 62.7 63.0 62.3 64.0 63.5 66.2 63.0 63.2 61.1 67.1 67.7 59.6 67.4 57.6 38.8 68.00 52.67 65.50 68.50 67.42 75.58 82.42 76.08 75.58 71.83 73.25 65.08 65.25 57.50 73.92 75.33 75.42 81.50 80.67 65.00 60.33 66.58 64.50 18.9 44.8 21.9 151.9 27.3 16.9 15.8 12.1 28.9 47.9 38.3 20.2 48.8 93.4 24.1 13.3 34.9 116.7 94.0 13.6 40.1 35.8 30.2 63.0 37.2 59.2 52.1 59.1 74.9 80.4 73.8 65.8 65.1 64.7 55.5 52.6 36.2 67.4 69.4 67.0 57.0 60.9 63.0 49.6 53.2 59.3 32.3 60.4 36.0 41.0 33.2 19.4 11.4 19.2 23.8 24.7 27.7 39.5 40.9 59.1 21.6 20.7 18.6 22.3 28.0 34.0 47.7 40.1 33.0 C.1 Main Prompts The prompt for VQA Classification is: \"{VQA_INSTRUCTION} A. airplane B. bear C. bicycle D. bird E. boat F. bottle G. car H. cat I. chair J. clock K. dog L. elephant M. keyboard N. knife O. oven P. truck Answer with the options letter from the given choices directly.\" with default setting VQA_INSTRUCTION=\"Which option best describes the image?\". We use the following prompt for the Image Captioning task: \"Describe the image. response short.\" Keep your C.2 Exploration of Alternative Prompts In initial testing, we found that the choice of prompts affects the eventual results and has the potential to inevitably influence our study. Thus, in an effort to address this, we extensively evaluated our 18 models with multiple different prompting techniques, used in literature [61, 23] and chose the best one. Our prompt for VQA is inspired by LLaVAs prompts for multiple-choice questions 1. In an additional experiment (Tab. 3), we ablated alternative prompts on LLaVA-NeXT 7B [62]. We change or use an empty VQA_INSTRUCTION and change options to CLIP-style options (\"X. photo of {class}\"). However, we only observed minor fluctuation in accuracy and shape bias and no significant effects. Our default prompt delivers the best accuracy and is, thus, our preferred choice. Table 3: Exploration of alternative VQA prompts. Prompt \"Which option best describes the image? [...]\" (default) Default with CLIP-style options \"Describe the object in the image: [...]\" \"Describe the object in the image: [...]\" with CLIP-style options Empty instruction (just options) Shape Bias [%] Accuracy [%] 59.2 82.58 59.5 60.2 59.4 59. 81.92 81.33 80.17 81.33 Our image captioning prompt is reformulation of the VQA prompt (\"Which option best describes the image?\" \"Describe the image.\"). In the following, we ablate if the suffix (\"Keep your response short.\") may have interfered with our results. Additionally, we tested an alternative suffix that explicitly asks for more details on LLaVA-NeXT 7B [62]. The results for the former investigation in Tab. 4, show that our suffix indeed did not heavily bias the results in terms of shape bias. While adding the suffix leads to an impact in accuracy, it reduces the ratio of generic descriptions (not referring to any class) and has on average almost 4x fewer tokens resulting in significantly faster inference. Switching the suffix to \"Be precise.\" increases shape bias, but at the same time also increases the number of generated tokens and worryingly the ratio of generic responses. Overall, we find that captioning prompts are more fragile, but our chosen default prompt provides an intriguing balance. For all ablated prompts, we find that the shape bias is higher than in VQA. Table 4: Exploration of alternative Image Captioning prompts. Prompt \"Describe the image. response short.\" (default) Keep your \"Describe the image.\" \"Describe the image. Be precise.\" Shape Bias [%] Accuracy [%] Tokens Ratio [%] Generic Avg. 64.0 65. 55.5 39.5 63.6 67.3 68.25 64. 202.9 166.2 46.8 50.6 C.3 Biased Prompts Hand-crafted. For our hand-crafted biased prompts, we set VQA_INSTRUCTION =\"Identify the primary {BIASED_TERM} in the image.\", with BIASED_ TERM=\"shape\" and BIASED_TERM=\"texture\", for shape-, and texture-biased prompts, respectively. Synonyms. We have retrieved the following strong synonyms from Thesaurus [73, 74]. shape: architecture, aspect, body, configuration, contour, format, frame, model, outline, pattern, shadow, silhouette 1https://github.com/haotian-liu/LLaVA/blob/main/docs/Evaluation.md [Online; accessed 6. Mar. 2024] texture: balance, character, composition, consistency, fabric, feeling, makeup, nature, pattern, quality, sense, smoothness, structure, surface, taste, touch Figure 7: Detailed shape bias measurements under synonyms for biased VQA prompts. The prompt is formed by replacing BIASED_TERM in the biased VQA prompt with the corresponding synonym. We did not filter out any synonyms but still want to emphasize that some of them are not reasonable in the context of vision (e.g., for texture) which explains why the variance between them can be high. In Fig. 7, we also show the detailed shape bias measurements for each synonym (complementary to Fig. 6 in the main paper). We find that pattern - synonym for both terms - tends to be more correlated with texture. LLM-based Response Extraction We rely on an LLM to extract labels from the generated captions for the Image Captioning task. For every generated description we instruct Nous-Hermes-2Mixtral-8x7B-DPO [54] with the following prompt: \"Your task is to extract all objects that are described in the given message. Only answer with all letters from the given choices that apply. If none apply, reply with X. Do not explain. These are the possible objects: A. airplane B. bear C. bicycle D. bird E. boat F. bottle G. car H. cat I. chair J. clock K. dog L. elephant M. keyboard N. knife O. oven P. truck Message: {Generated Image Caption}\" 20 Then we simply split the generated string into list. We found this prompt by manually testing some examples and picking the best-performing one. For example, we experimented with other options to denote generic responses like -. However, we found that this increases hallucinations, presumably as - is often used to begin bullet points and, thus, causes the model to continue generation."
        },
        {
            "title": "E Automated Prompt Search",
            "content": "Loosely inspired by [45], we utilized an LLM to optimize prompts. we switched to Mixtral-8x7B-Instruct-v0.12, as it performed better than the Nous Hermes version in early tests. The results shown in Sec. 5.2/Fig. 6 are summary of multiple prompts that we tried in numerous multi-round conversations. We instruct the model to provide prompt in new line starting with PROMPT: that we then extract and automatically evaluate. Afterwards, we return the results to the LLM and ask it to generate the next prompt. We have experimented with multiple prompts but ultimately our approaches can be loosely divided into prompts that try to maximize or minimize shape bias without significantly affecting accuracy. Besides linguistic tweaks, we experimented with the following techniques: 1. Offering rewards: We offered tips to the LLM to encourage it to generate more and better results3. However, Mixtral seems to be fine-tuned to refuse such attempts. 2. Adding in-context examples: We added an example (in language) of what it means to be shape or texture-biased in classification. This often seemed to bias the model to generate prompts that contain the example, too. 3. Summarizing previous attempts: We encouraged the LLM to summarize previous attempts before generating the next prompt, hoping to keep the most important aspects in context. The LLM did not always follow this suggestion. 4. Returning the extracted prompt: The LLM sometimes did not start the prompt with the requested prefix or misplaced it. We mitigated this by including the extracted prompt in our responses. 5. Encouragements in response: Initially, we only returned the accuracy and shape bias but found that the LLM sometimes abruptly quits the search. Thus we included encouragements in the form of questions like What is your next prompt?. This seemed to improve conversations in terms of length, but could not entirely prevent the LLM from quitting. 6. Simple but creative prompts: When just instructed the LLM to generate prompts, we noticed that it would sometimes collapse to verbose prompts where it would attempt to rephrase terms by synonyms. Inspired by regularization terms in optimization, we ask the model to keep its prompt simple and creative to avoid minor tweaking in favor of more radical changes. In all cases, we append mock conversation (i.e., both roles are written by us) to the history containing the neutral prompt and the respective shape bias/accuracy. An example conversation is shown in Tab. 5. Optimization with LLMs is highly exciting but also very active research field, where best practices have not yet emerged. For example, we have noticed that our instruction sometimes caused the LLM to refuse to continue when it found that the search was exhausted, or caused the LLM to maximize shape bias despite the instruction to minimize it. Overall, this is not an issue for our study as we are merely interested in understanding if quantitatively more texture/shape-biased prompts exist. The prompt shown in Tab. 5 (first message) is the final iteration integrating all of the above techniques."
        },
        {
            "title": "F Ablation of Temperature Scaling",
            "content": "We are interested in determining if generation parameters can influence the behavior of shape bias. Generally, VLMs only expose few controllable parameters but all offer some form of stochastic 2https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1 [Online; accessed 6. Mar. 2024] 3https://twitter.com/voooooogel/status/1730726744314069190 [Online; accessed 6. Mar. 2024] Table 5: Example conversation for LLM-guided Prompt Search. Mock conversation. User Assistant User Assistant User Assistant Your task is to help me design an ideal image classification prompt for vision/language model to MINIMIZE the shape bias without significantly hurting accuracy. The provided photos show natural objects modified to contain shape and texture of conflicting object classes. E.g., it could be photo of an elephant (shape) with dog fur (texture). The model should classify the image as dog based on the texture of the object and ignore the shape. The models accuracy is the percentage of correctly classified images. The shape bias is the ratio of how often the model classified based on shape over texture. You can test your prompt by outputting single new line starting with PROMPT: . Do not list options - the system will provide them automatically. Try to keep the prompt as short and simple as possible but be creative. It might be reasonable to summarize insights of previous attempts and to outline your goals before responding with new prompt, but make sure that only the prompt starts with PROMPT:. In response to the prompt you will be told the accuracy and shape bias. Then you will refine the prompt and we will continue until say stop. Lets go! PROMPT: Which option best describes the image? Prompt: [...], Accuracy: 82.58 %, Shape Bias: 59.43 %. What is your next prompt? To minimize the shape bias while maintaining accuracy, lets encourage the model to focus more on textures than shapes. Heres my next prompt: PROMPT: Based on the surface characteristics and patterns, which category does the image most resemble? Prompt: [...], Accuracy: 78.25 %, Shape Bias: 50.9 %. Can you improve this? have provided various prompts and explored different approaches to minimize the shape bias while maintaining high accuracy. The current optimal prompt is: [...] Here are some additional suggestions for minimizing the shape bias: [...] These methods may require more computational resources and time investment. If you have other questions or need guidance on different topics, please dont hesitate to reach out. Wishing you the best in your continued efforts! At this point, believe that we have explored various approaches, and the current prompt is the best-performing one among those tested. If you decide to pursue any of the mentioned alternatives, am confident that you will find ways to improve the models performance and minimize the shape bias. Good luck with your continued work! As our conversation has concluded, would like to thank you for engaging in this exercise, and hope have been helpful in providing prompts and suggestions. If you have other questions or need guidance on different topics in the future, please dont hesitate to reach out. Wishing you the best in your projects and research! 22 Figure 8: Temperature scaling has no significant effect on shape bias neither under VQA (left) nor Image Captioning (right) tasks but starts to decrease accuracy at higher levels. Experiments performed on LLaVA-NeXT 7B with 3 seeds (except Temperature = 0 and Temperature = 1 of Image Captioning where we use single seed). sampling of tokens, often via temperature scaling of the token logits. Most models default to lowtemperature settings (or settle for greedy token strategy) which is more correlated with precise answers and reasonable for VQA. On the contrary, higher temperatures are correlated with more creative outputs and eventually token gibberish at extreme values. In general, temperature scaling also results in better-calibrated models [81]. Exemplarily, we study this on LLaVA-NeXT 7B for both VQA and Image Captioning. We repeat the non-greedy experiments 3 times for statistically meaningful results; however, we generally notice marginal error between runs. Our results in Fig. 8, show no correlation between temperature and shape bias. As expected, the accuracy (slightly) decreases, because less confident tokens mapping to correct predictions are now replaced by false predictions. Yet, this affects texture/shape information alike. This can easily be explained by our token sampling analysis in Sec. 4. On average, texture/shape options are fairly similarly confident and top-1 tokens have very high confidence (in the VQA setting) which the temperature scaling barely affects. On the one hand, this finding serves as important confirmation that our comparison of VLMs at default values (picked by the original authors) is reasonable as it does not interfere with the shape bias. On the other hand, this implies that users seeking more creative outputs can tune the temperature (and similarly other parameters that control stochastic token sampling) without changing the underlying reasoning paths for vision inputs."
        },
        {
            "title": "G Results on CLIP Models",
            "content": "In this section, we provide results for CLIP models that we referenced in the main paper. We provide results for different architectures under three different prompting strategies: computation of zero-shot centroids from 80 different prompts including usage of the class name [11], \"a photo of {class}.\" which is often used as default prompt (note the dot), and \"{class}\" (without dot). We will argue that the latter is more comparable to the VQA task of our VLMs - but of course, VLMs may have better representation in weights. Either way, the shape bias does not significantly deviate between the three strategies. Tab. 6 shows the obtained shape bias (and accuracy) measurements. We also noticed that the observed scaling laws in [29] do not always hold for vision encoders, despite an increase in parameters from EVA02-CLIP-E/14+ (5B) to EVA02-CLIP-8B, we actually see significant decrease in shape bias (but an improvement in accuracy). Our results also contain (rather uncommon) ResNet-based CLIP models. Note these are the only models, where the 80 prompts significantly improve accuracy. In terms of shape bias, ResNet-based CLIPs significantly underperform any ViT or ViT-based CLIP. 23 Table 6: Zero-shot classification on cue-conflict with different CLIP(-like) joint embedding models. Model Prompt Shape Bias [%] Accuracy [%] EVA01-CLIP-g/14 [17] EVA01-CLIP-g/14 [17] EVA01-CLIP-g/14 [17] 80 Prompts [11] \"a photo of {class}.\" \"{class}\" EVA02-CLIP-8B@448px [82] EVA02-CLIP-8B@448px [82] EVA02-CLIP-8B@448px [82] 80 Prompts [11] \"a photo of {class}.\" \"{class}\" EVA02-CLIP-E/14+ [17] EVA02-CLIP-E/14+ [17] EVA02-CLIP-E/14+ [17] CLIP-ViT-L/14 [11] CLIP-ViT-L/14 [11] CLIP-ViT-L/14 [11] CLIP-ViT-L/14@336px [11] CLIP-ViT-L/14@336px [11] CLIP-ViT-L/14@336px [11] CLIP-ResNet-50 [11] CLIP-ResNet-50 [11] CLIP-ResNet-50 [11] CLIP-ResNet-101 [11] CLIP-ResNet-101 [11] CLIP-ResNet-101 [11] 80 Prompts [11] \"a photo of {class}.\" \"{class}\" 80 Prompts [11] \"a photo of {class}.\" \"{class}\" 80 Prompts [11] \"a photo of {class}.\" \"{class}\" 80 Prompts [11] \"a photo of {class}.\" \"{class}\" 80 Prompts [11] \"a photo of {class}.\" \"{class}\" 66.03 66.03 66.44 58.26 57.58 56. 65.62 64.44 62.48 60.95 60.20 60.16 61.52 60.56 59.80 19.70 20.96 20.77 25.50 25.23 25.41 87.83 87.08 86. 91.83 89.00 88.33 90.67 89.75 86.17 84.08 84.17 81.17 86.83 86.42 83.75 77.83 72.75 71.83 74.83 71.00 70. Tab. 7 shows the delta between shape bias and accuracy for some selected VLMs and their vision encoders under zero-shot classification (using \"{class}\" as prompt). All selected models rely on frozen vision encoder during training. As discussed in Sec. 4.3, the VLM accuracy decreases in all cases, but shape bias can deviate in either direction. Table 7: Quantitative comparison between shape bias and accuracy of VLMs (VQA) and their respective vision encoders (CLIP zero-shot classification) on cue-conflict. VLM Vision Encoder VLM - Encoder [%] Accuracy Shape-Bias LLaVA v1.5 7B [61] LLaVA v1.5 13B [61] LLaVA-NeXT 7B [62] LLaVA-NeXT 13B [62] LLaVA-NeXT 34B [62] MoE-LLaVA v1.5 Phi2 x4 [67] MoE-LLaVA v1.5 Qwen x4 [67] MoE-LLaVA v1.5 StableLM x4 [67] InstructBLIP FLAN-T5-XL [23] InstructBLIP Vicuna-7B [23] (cid:25) CLIP ViT-L/14@336px[11] (cid:25) EVA-01-CLIP ViT-g/14@224px [17] Emu2-Chat [66] (cid:25) EVA-02-CLIP-E/14+@448px [17] 3.50 3.00 9.92 0.33 1.08 1.42 24.25 3.67 6.83 14.42 11.08 4.2 1.5 3.9 2.7 0.2 0.3 3.0 0.8 1.8 7. 9.5 Results on ImageNet-trained Models Complementary to the results on CLIP, we also provide some shape bias evaluations of ImageNettrained/finetuned models in Tab. 8. Note how ViTs are much more shape-biased than ResNets, as shown in [29], and yet, after ImageNet-finetuning, the previously well-performing CLIP (ViTL/14@336px) model drops from 59.80 % to just 32.1 % of shape bias. 24 Table 8: Classification on cue-conflict with ImageNet-trained/finetuned models. Model Shape Bias [%] Accuracy [%] ResNet-50 [59] ResNet-50 (timm) [83] ResNet-152 (timm) [83] ViT-B/16 (ImageNet-21k pretraining) [32] ImageNet-finetuned CLIP (ViT-L/14@336px) [83] 22.3 23.1 28.6 45.4 32.1 67.33 65.42 65.83 63.67 82."
        },
        {
            "title": "I Additional Thoughts on the Image Captioning Task",
            "content": "Did our choice of embedding model bias the results? While we assume that most embedding models will provide similar classification performance if the description clearly mentions one class, it is unclear how the classification is biased if the description refers to multiple classes, invalid classes, or is generic. Thus, we additionally ablate results with SFR-Embedding [84] which at the time of writing was the overall SOTA English embedding model on the Massive Text Embedding Benchmark (MTEB) [85]. While the accuracy improved by negligible amount, shape bias results were largely unaffected. Thus, we settled for the faster ember-v1 models. What happens if the description mentions multiple classes? Based on our LLM analysis, we notice that in the majority of cases (min: 79.3%, mean: 92.2%, median: 93.0%, max: 97.6%; minimum is given by InstructBLIP Flan-T5-xl [23]), descriptions do not refer to multiple labels, thus potential bias of the embedding model is negligible for our analysis. What happens if the description is generic? According to our LLM analysis, many generated descriptions do not refer to any object class (min: 11.4%, mean: 31.9%, median: 32.3%, max: 60.4%) - in stark contrast to VQA responses. However, we also notice that the embedding accuracy is above random choice in these cases. This suggests that the LLM may have missed objects and slightly overreported the ratio. In the cases where the caption is indeed generic, our choice of embedding model may have biased our study. However, the previously mentioned embedding with SFR-Embedding [84] showed similar trends. Thus, we assume that most other SOTA embedding models would behave similarly - yet, we are excited how future embedding models will embed these cases. Another option is to remove generic responses from the analysis. We observe that this typically increases shape bias (and accuracy) - naturally more notably in cases where the generic ratio was high (Tab. 9). E.g., for the extreme case of GPT-4V [12], shape bias increases by 9.3% and accuracy by 40.47% (!). This preprocessing also seems to restore scaling laws to large extent: larger models achieve higher shape bias and accuracy. One outlier to this trend is InternVL Chat v1.2+ [2]. It may be intriguing to replace the reported results in Sec. 4/Fig. 2 with the analysis on non-generic responses, but we avoid doing so, as this would a) remove significant portion of results; b) lead to poorly comparable results obtained on different subsets. Class-wise Texture/Shape Bias Following the original shape bias study [1], we include plots that show the class-specific texture/shape bias for all our models in VQA and Image Captioning  (Fig. 9)  ."
        },
        {
            "title": "K Responsibility to Human Subjects",
            "content": "We did not collect any human data ourselves. Instead, the human shape bias data is taken from [1] who collected the data in controlled psychophysical laboratory (Wichmann-lab in Tübingen, Germany). Participants provided written informed consent and were compensated above minimum wage. The data was open-sourced by the authors on github without personally identifiable information. The experiment was conducted in accordance with institutional guidelines for human subjects research. For further information on the human experiment, we refer the reader to [1]. 25 Table 9: Comparison of shape bias and accuracy for the Image Captioning tasks for all responses and only responses which an LLM did not classify as generic. Model Gemini 1.0 Pro Vision GPT-4V (Preview) Qwen-VL Plus Qwen-VL Max Qwen-VL Chat InternVL Chat 1.1 InternVL Chat 1.2+ LLaVA v1.5 7B LLaVA v1.5 13B LLaVA-RLHF 7B LLaVA-RLHF 13B LLaVA-NeXT 7B LLaVA-NeXT 13B LLaVA-NeXT 34B MoE-LLaVA-StableLM MoE-LLaVA-Qwen MoE-LLaVA-Phi2 InstructBLIP Flan-T5-xl InstructBLIP Vicuna-7B Emu2-Chat CogAgent Chat CogVLM Chat UForm Gen Chat All responses Shape Bias [%] Accuracy [%] Non-generic Shape Bias [%] Accuracy [%] 63.2 53.6 67.9 69.7 38.2 73.2 61.3 61.4 62.7 63.0 62.3 64.0 63.5 66.2 63.0 63.2 61.1 67.1 67.7 59.6 67.4 57.6 38.8 68.00 52.67 65.50 68.50 67.42 75.58 82.42 76.08 75.58 71.83 73.25 65.08 65.25 57.50 73.92 75.33 75.42 81.50 80.67 65.00 60.33 66.58 64.50 65.7 62.9 71.9 72.1 40.1 74.5 62.3 62.8 65.1 64.7 66.3 66.9 65.3 73.6 64.1 64.4 63.1 68.7 68.4 60.3 70.8 61.9 37.9 88.40 93.14 88.56 91.52 83.92 87.89 88.15 87.24 88.24 83.80 86.08 92.48 92.95 96.39 86.28 88.03 86.05 89.09 90.27 89.94 97.38 93.72 83.00 (a) VQA (b) Image Captioning Figure 9: Shape bias per object class. We show the results of all our models under the VQA (left) and Image Captioning (right) task."
        },
        {
            "title": "L Visualization of Vision Steering",
            "content": "We visualize the effects of vision steering (Sec. 5.1 in the main paper) in Fig. 10. 26 Figure 10: Steering by Vision. For one example image, we show how patch shuffling (top) increases texture bias by destroying shape information. Below we show how adding Gaussian noise increases shape bias by destroying texture information. Please note that we show more extreme values than those used in our experiments for visualization purposes."
        }
    ],
    "affiliations": [
        "Google DeepMind",
        "ICG, Graz University of Technology",
        "IMLA, Offenburg University",
        "Max Planck Institute for Informatics, Saarland Informatics Campus",
        "University of Mannheim",
        "University of Siegen"
    ]
}