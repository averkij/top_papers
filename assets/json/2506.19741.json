{
    "paper_title": "Noise Consistency Training: A Native Approach for One-Step Generator in Learning Additional Controls",
    "authors": [
        "Yihong Luo",
        "Shuchen Xue",
        "Tianyang Hu",
        "Jing Tang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The pursuit of efficient and controllable high-quality content generation remains a central challenge in artificial intelligence-generated content (AIGC). While one-step generators, enabled by diffusion distillation techniques, offer excellent generation quality and computational efficiency, adapting them to new control conditions--such as structural constraints, semantic guidelines, or external inputs--poses a significant challenge. Conventional approaches often necessitate computationally expensive modifications to the base model and subsequent diffusion distillation. This paper introduces Noise Consistency Training (NCT), a novel and lightweight approach to directly integrate new control signals into pre-trained one-step generators without requiring access to original training images or retraining the base diffusion model. NCT operates by introducing an adapter module and employs a noise consistency loss in the noise space of the generator. This loss aligns the adapted model's generation behavior across noises that are conditionally dependent to varying degrees, implicitly guiding it to adhere to the new control. Theoretically, this training objective can be understood as minimizing the distributional distance between the adapted generator and the conditional distribution induced by the new conditions. NCT is modular, data-efficient, and easily deployable, relying only on the pre-trained one-step generator and a control signal model. Extensive experiments demonstrate that NCT achieves state-of-the-art controllable generation in a single forward pass, surpassing existing multi-step and distillation-based methods in both generation quality and computational efficiency. Code is available at https://github.com/Luo-Yihong/NCT"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 1 4 7 9 1 . 6 0 5 2 : r Noise Consistency Training: Native Approach for One-Step Generator in Learning Additional Controls Yihong Luo1, Shuchen Xue2, Tianyang Hu3, Jing Tang4,1 1HKUST 2UCAS 3NUS 4HKUST(GZ)"
        },
        {
            "title": "Abstract",
            "content": "The pursuit of efficient and controllable high-quality content generation remains central challenge in artificial intelligence-generated content (AIGC). While onestep generators, enabled by diffusion distillation techniques, offer excellent generation quality and computational efficiency, adapting them to new control conditionssuch as structural constraints, semantic guidelines, or external inputsposes significant challenge. Conventional approaches often necessitate computationally expensive modifications to the base model and subsequent diffusion distillation. This paper introduces Noise Consistency Training (NCT), novel and lightweight approach to directly integrate new control signals into pre-trained one-step generators without requiring access to original training images or retraining the base diffusion model. NCT operates by introducing an adapter module and employs noise consistency loss in the noise space of the generator. This loss aligns the adapted models generation behavior across noises that are conditionally dependent to varying degrees, implicitly guiding it to adhere to the new control. Theoretically, this training objective can be understood as minimizing the distributional distance between the adapted generator and the conditional distribution induced by the new conditions. NCT is modular, data-efficient, and easily deployable, relying only on the pre-trained one-step generator and control signal model. Extensive experiments demonstrate that NCT achieves state-of-the-art controllable generation in single forward pass, surpassing existing multi-step and distillation-based methods in both generation quality and computational efficiency. Code is available at https://github.com/Luo-Yihong/NCT."
        },
        {
            "title": "Introduction",
            "content": "The pursuit of high-quality, efficient, and controllable generation has become central theme in the advancement of artificial intelligence-generated content (AIGC). The ability to create diverse and realistic content is crucial for wide range of applications, from art and entertainment to scientific visualization and data augmentation. Recent breakthroughs in diffusion models and their distillation techniques have led to the development of highly capable one-step generators [1, 2, 3, 4, 5]. These models offer compelling combination of generation quality and computational efficiency, significantly reducing the cost of content creation. Methods such as Consistency Training [6] and Inductive Moment Matching [7] have further expanded the landscape of native few-step or even one-step generative models, providing new tools and perspectives for efficient generation. However, as AIGC applications continue to evolve, new scenarios are constantly emerging that demand models to adapt to novel conditions and controls. These conditions can take many forms, encompassing structural constraints (e.g., generating an image with specific edge arrangements), semantic guidelines (e.g., creating an image that adheres to particular artistic style), and external Core contribution. Corresponding authors. Preprint. Under review. factors such as user preferences or additional sensory inputs (e.g., generating an image based on depth map). Integrating such controls effectively and efficiently is critical challenge. The conventional approach to incorporating controls into diffusion models often involves modifying the base model architecture and subsequently performing diffusion distillation to obtain one-step student model [8]. This process, while effective, can be computationally expensive and time-intensive, requiring significant resources and development time. more efficient alternative would be to extend the distillation pipeline to accommodate new controls directly, potentially bypassing the need for extensive retraining of the base diffusion model [9]. However, even extending the distillation pipeline can still be heavy undertaking, adding complexity and computational overhead. Therefore, the question of how to directly endow one-step generators with new controls in lightweight and efficient manner remains significant challenge. In this paper, we answer this question by proposing Noise Consistency Training (NCT) simple yet powerful approach that enables pre-trained one-step generator to incorporate new conditioning signals without requiring access to training images or retraining the base model. NCT achieves this by introducing an adapter module that operates in the noise space of the pre-trained generator. Specifically, we define noise-space consistency loss that aligns the generation behavior of the adapted model across different noise levels, implicitly guiding it to satisfy the new control signal. Besides, we employ boundary loss ensuring that when given condition already associated with input noise, the generation should remain the same as one-step uncontrollable generation. This can ensure the distribution of the adapter generator remains in the image domain rather than collapsing. Theoretically, we demonstrate in Section 3.2 that this training objective can be understood as matching the adapted generator to the intractable conditional induced by discriminative control model when the boundary loss is satisfied, effectively injecting the desired conditioning behavior. Our method is highly modular, data-efficient, and easy to deploy, requiring only the pre-trained onestep generator and control signal model, without the need for full-scale diffusion retraining or access to the original training data. Extensive experiments across various control scenarios demonstrate that NCT achieves state-of-the-art controllable generation in single forward pass, outperforming existing multi-step and distillation-based methods in both quality and computational efficiency."
        },
        {
            "title": "2 Preliminary",
            "content": "Diffusion Models (DMs). DMs [2, 1] operate via forward diffusion process that incrementally adds Gaussian noise to data over timesteps. This process is defined as q(xtx) (xt; αtx, σ2 I), where αt and σt are hyperparameters dictating the noise schedule. The diffused samples are obtained via xt = αtx + σtϵ, with ϵ (0, I). The diffusion network, ϵθ is trained by denoising: Ex,ϵ,tϵθ(xt, t) ϵ2 2. Once trained, generating samples from DMs typically involves iteratively solving the corresponding diffusion stochastic differential equations (SDEs) or probability flow ordinary differential equations (PF-ODEs), process that requires multiple evaluation steps. ControlNet. Among other approaches for injecting conditions [10, 11, 12, 13, 14], ControlNet [15] has emerged as prominent and effective technique for augmenting pre-trained DMs with additional conditional controls. Given pre-trained diffusion model ϵθ, ControlNet introduces an auxiliary network, parameterized by ϕ. This network is trained by minimizing conditional denoising loss L(ϕ) to inject the desired controls: L(ϕ) = Ex,ϵ,tϵ ϵθ,ϕ(xt, c)2 2. After training, ControlNet enables the integration of new controls into the pre-trained diffusion models. (1) Maximum Mean Discrepancy. Maximum Mean Discrepancy (MMD [16]) between distribution p(x), q(y) is an integral probability metric [17]: MMD2(p, q) = Ex[ψ(x)] Ey[ψ(y)]2, (2) where ψ() is kernel function. Diffusion Distillation. While significant advancements have been made in training-free acceleration methods for DMs [18, 19, 20, 21, 22], diffusion distillation remains key strategy for achieving high-quality generation in very few steps. Broadly, these distillation methods follow two primary paradigms: 1) Trajectory distillation [23, 24, 25, 26, 27, 28], which seeks to replicate the teacher 2 Figure 1: Framework description of our method. models ODE trajectories on an instance-by-instance basis. These methods can encounter difficulties with precise instance-level matching. 2) Distribution matching, often realized via score distillation [5, 3, 29, 4], which aims to align the output distributions of the student and teacher models using divergence metrics. Our work utilizes pre-trained one-step generator, which itself is product of diffusion distillation; however, the training of our proposed NCT method does not inherently require diffusion distillation. Additional Controls for One-step Diffusion. The distillation of multi-step DMs into one-step generators, particularly through score distillation, is an established research avenue [3, 5, 29]. However, the challenge of efficiently incorporating new controls into these pre-trained one-step generators is less explored. CCM [30], for example, integrates consistency training with ControlNet, demonstrating reasonable performance with four generation steps. In contrast, our work aims to surpass standard ControlNet performance in most cases using merely single step. Many successful score distillation techniques [3, 5, 31, 32] rely on initializing the one-step student model with the weights of the teacher model. SDXS [8] explored learning controlled one-step generators via score distillation, but their framework requires both the teacher model and the generated \"fake\" scores to possess ControlNet compatible with the specific condition being injected. JDM [9] minimizes tractable upper bound of the joint KL divergence, which can teach controllable student with an uncontrollable teacher. Generally, prior works are built on specific distillation techniques for adapting controls to one-step models. We argue that given an already proficient pre-trained one-step generator, performing an additional distillation for adding new controls is computationally expensive and unnecessary. However, how to develop native technique for one-step generators remains unexplored. Our work takes the first step in designing native approach for one-step generators to add new controls to one-step generators without requiring any diffusion distillation."
        },
        {
            "title": "3 Method",
            "content": "Problem Setup. Let Rm be latent variable following standard Gaussian density p(z). We have pre-trained generator fθ : Rm Rn that maps to data sample = fθ(z). The distribution of these generated samples has density pθ(x), providing high-quality approximation of the data distribution, such that pθ(x) pd(x). For any x, there is conditional probability density p(cx) specifying the likelihood of condition given x. Our goal is to directly incorporate additional control for pre-trained one-step generator with additional trainable parameters ϕ (e.g. ControlNet). More specifically, we aim to train conditional generator fθ,ϕ(z, c) that, when given latent code sampled from standard Gaussian distribution and an independently sampled condition c, produces sample such that the joint distribution of (x, c) matches p(x, c) = pθ(x)p(cx). 3 3.1 Failure modes of Naive Approaches for Adding Controls Given pre-trained diffusion model ϵθ(xt, t), the adapters for injecting new conditions can be trained by minimizing denoising loss [15, 10]. Hence, natural idea for injecting new conditions into the pre-trained one-step generator is also adapting the denoising loss for training as follows: minϕd(fθ,ϕ(z, c), x), = αT + σT ϵ, p(cx), (3) where d(, ) is distance metric. This approach can potentially inject new conditions into the one-step generator fθ, similar to existing adapter approaches for DMs. However, it fails to generate high-quality images the resulting images are blurry, which is due to the high variance of the optimized objective. Specifically, its optimal solution is achieved at fθ,ϕ(z, c) = E[xz, c], which is an average of every potential image. To reduce the variance, one may consider performing denoising loss over coupled pairs (z, x, c), where (0, I), is the condition corresponding to the generated samples = fθ(z). However, such an approach is unable to perform conditional generation given random z. This is because the model is only exposed to instances of strongly associated with (i.e., p(cfθ(z))) during its training, and never encountered random pairings of and z. High variance in denoising loss is also key factor hindering fast sampling in diffusion models. Several methods have been proposed to accelerate the sampling of diffusion models, with optimization objectives typically characterized by low variance properties [26, 24, 33]. Among these, consistency models [26, 27] stand out as promising approach instead of optimizing direct denoising loss, they optimize the distance between denoising results of highly-noisy samples and lowly-noisy samples: min α L(α) = d(gα(xtn+1), sg(gα(xtn ))), (4) where sg() denotes the stop-gradient operator and gα denotes the desired consistency models. Similar to denoising loss, consistency loss can also force networks to use conditions; thus, it can be used to train adapters to inject new conditions [30]. However, the consistency approach cannot be adapted to the one-step generator since it requires defining the loss over multiple noisy-level images, while the one-step generator only takes random noise as input. 3.2 Our Approach: Noise Consistency Training To directly inject condition to one-step generator, we propose Noise Consistency Training, which diffuses noise to decouple it from the condition and operates the consistency training in noise space. Specifically, we diffuse an initial noise (0, I) to multiple levels zt via variance-preservation diffusion as follows: zt = 1 σtz + σtϵ, (5) where ϵ (0, I). This ensures that zt also follows the standard Gaussian distribution, thus it can be transformed to the high-quality image by the pre-trained one-step generator fθ. To inject new conditions to fθ, we apply an adapter with parameter ϕ, which transforms fθ() that only takes random noise as input to fθ,ϕ(, ) that can take an additional condition as input. We sample coupled pairs (z, c) from pθ(z, c), where p(z, c) = p(z)pθ(cz), and pθ(cz) pθ(cfθ(z)). By the (z, c) pairs, we can perform Noise Consistency Loss as follows: minϕEp(z)p(cfθ(z))Eq(ztn z),q(ztn1 z)EϵN (0,I)d(fθ,ϕ(ztn , c), sg(fθ,ϕ(ztn1, c))) = Ez,cz,ϵd(fθ,ϕ(ztn, c), sg(fθ,ϕ(ztn1, c))), #Simplified Notation (cid:113) (cid:113) (6) 1 σ2 1 σ2 tn + σtn ϵ and ztn1 = where ztn = tn1z + σtn1 ϵ. The defined diffusion process can gradually diffuse the coupled pairs (z, c) to independent uncoupled pairs (zT , c). By minimizing the distance between predictions given less-coupled pairs and more-coupled pairs, we can force the network to utilize the condition. Once trained, the consistency is ensured in the noise space. It is expected that the adapter ϕ can be trained for injecting new conditions c, while keeping the high-quality generation capability in one-step. Since the optimized objective has low variance and the generator fθ can produce high-quality images, the adapter just need to learn how to adapt to the conditions c. 4 and p(ztc) (cid:82) q(ztz0)p(z0c)dz0. The forward Lemma 1. Define p(z0c) p(z0)p(cfθ(z)) diffusion process defines an interpolation for the joint distribution p(zt, c) p(ztc)p(c) between p(z0, c) = p(cfθ(z0))p(z0) and p(zT , c) = p(zT )p(c). p(c) The above Lemma 1 provides formal justification to our noise diffusion process as interpolation between the coupled pairs (z, c) to independent pairs (zT , c). The proof can be found in Appendix A. Lemma 2. We define the fθ,ϕ( proposed noise consistency loss is practical estimation of the following loss: + σtk+1ϵ, c) induced distribution to be pθ,ϕ,tk+1. The tk+1 1 σ (cid:113) L(ϕ) = 1 (cid:88) k=0 MMD2(pθ,ϕ,tk+1, pθ,ϕ,tk ), (7) under specific hyper-parameter choices (e.g., set particle samples to 1). See proof in the Appendix A. The above Lemma 2 builds the connection between our noise consistency training and conditional distribution matching. Technically speaking, using larger particle numbers can further reduce training variance. However, in practice, we found that directly using single particle achieves similar performance and is more computationally feasible. More investigations on the effect of particle numbers can be found in Appendix B. This work serves as proof of concept that we can design an approach native to one-step generator in learning new controls, we leave other exploration for further reducing variance in future work. Boundary Loss core difference between NCT and CM lies in the models behavior when reaching boundaries. Specifically, for CM, when the input reaches the boundary x0, the model only needs to degenerate into an identity mapping outputting x0, which can be easily satisfied through reparameterization to stabilize the training. NCT, however, is fundamentally different when the input reaches the boundary z, the model cannot simply degenerate into an identity mapping, but needs to map to high-quality clean images. This means this boundary is non-trivial the network needs to learn to map to corresponding images. Simply reparameterizing fθ,ϕ cannot fully stabilize the training. To satisfy this boundary condition and stabilize the training, we propose setting the clean image corresponding to as fθ(z) and implementing the following boundary loss: minϕEz,cz,ϵd(fθ,ϕ(z, c), fθ(z)), (0, I), p(cfθ(z)). (8) By minimizing this loss, we can ensure the boundary conditions hold and constrain the generators output to be close to the data distribution. Intuitively, this loss is easy to understand: when the generator receives the same noise and conditions corresponding to fθ, its generation should be invariant. With the help of this loss, we can constrain the generators output to stay near the data distribution otherwise, if we only minimize the noise consistency loss, the model might find unwanted shortcut solutions. Theorem 1. Consider parameter set ϕ that satisfies the following two conditions: 1. Boundary Condition: The parameters ϕ ensure the boundary loss is zero: 2. Consistency Condition: The parameters ϕ also satisfy: Ep(z)p(cfθ(z))[d(fθ,ϕ(z, c), fθ(z))] = 0, L(ϕ) = 1 (cid:88) k=0 MMD2(pθ,ϕ,tk+1, pθ,ϕ,tk ) = 0 Then fθ,ϕ maps independent p(z)p(c) to the target joint distribution pθ(x)p(cx). See proof in the Appendix. Theorem 1 provides theoretical insight for our optimization objective, which is an empirical version for practice. Overall Optimization We observed that the noise consistency loss is only meaningful when boundary conditions are satisfied or nearly satisfied; otherwise, the generator fθ,ϕ can easily find undesirable shortcut solutions, thus we suggest using constrained optimization form as follows: Algorithm 1 Noise Consistency Training Require: Pre-trained One-Step Generator fθ, Adapter ϕ, total iterations Ensure: Optimized adapter ϕ for injecting new condition. 1: for 1 to do 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: end for Sample noise from standard Gaussian distribution; Sample noise ϵ from standard Gaussian distribution; Sample with initialized noise from frozen generator fθ, i.e., = fθ(z). Sample condition corresponding to by p(cx). # Primal Step: ## Diffuse Noise via Variance-Preserved Diffusion ztk+1 αtk+1z + σtk+1ϵ and ztk αtk + σtk ϵ. ## Compute Noise Consistency Loss Lcon d(fθ,ϕ(ztk+1, c), sg(fθ,ϕ(ztk , c))) ## Compute Boundary loss Lbound d(fθ,ϕ(z, c), x) ## Compute Total Loss and Update Ltotal Lcon + λLbound Update ϕ using ϕLtotal # Dual Step: Update λ according to Eq. (12). Definition 1 (Noise Consistency Training). Given fixed margin ξ, the general optimization can be transformed into the following: Ez,cz,ϵLcon(ϕ) = d(fθ,ϕ(ztn, c), sg(fθ,ϕ(ztn1, c))) min ϕ s.t. Lbound(ϕ) = Ez,cz,ϵd(fθ,ϕ(z, c), fθ(z)) < ξ, (9) where z, ϵ (0, I), p(cfθ(z)), ztn = σtn+1ϵ. (cid:113) 1 σ2 tn + σtn ϵ and ztn+1 = (cid:113) 1 σ tn+1z + The constrained optimization problem presented in Definition 1 is hard to optimize directly. We therefore reformulate it as corresponding saddle-point problem: max λ min ϕ (cid:8)Lcon(ϕ) + λLbound(ϕ)(cid:9), λ 0. (10) Concrete Algorithm To efficiently optimize this saddle-point problem, we employ the primaldual algorithm tailored for the saddle-point problem, which alternates between updating the primal variables ϕ and the dual variable λ. Specifically, in the primal step, for given dual variable λ, the algorithm minimizes the corresponding empirical Lagrangian with respect to ϕ under given dual variable λ, i.e., ϕk+1 := arg min ϕ (cid:110) Lcon(ϕk) + λLbound(ϕk) (cid:111) (11) In practice, this update for ϕ is performed using stochastic gradient descent. Subsequently, in the dual step, we update the dual variable λ as follows: λt+1 := max (cid:8)λt + η (cid:0)Lcon ξ(cid:1), 0(cid:9) , (12) where η is the learning rate for the dual update. Algorithm 1 provides the pseudo-code for our primal-dual optimization of the adapter parameters ϕ. In contrast to the direct application of stochastic gradient descent in Eq. (10), the primal-dual algorithm dynamically adjusts λ. This avoids an extra hyper-parameter tuning and can provide an early-stopping condition (e.g., λ = 0). Additionally, convergence is guaranteed under sufficiently long training and an adequately small step size [34]. 6 Table 1: Comparison of machine metrics of different methods for Canny, HED, Depth and 8 Super Resolution tasks. The mark denotes our reimplementation with the same one-step generator as used in NCT. Method NFE Canny FID Consistency HED FID Consistency Depth FID Consistency 8 Super Resolution FID Consistency Avg FID Consistency ControlNet DI + ControlNet JDM NCT (Ours) 50 1 1 1 14.48 22.74 14.35 13.67 0.113 0.141 0.122 0. 19.21 28.04 16.75 14.96 0.101 0.113 0.055 0.060 15.25 22.49 16.71 16. 0.093 0.097 0.093 0.088 11.93 15.57 13.23 12.17 0.065 0.126 0.068 0. 15.22 22.21 15.26 14.31 0.093 0.119 0.085 0.078 Figure 2: Qualitative comparisons on controllable generation across different control signals against competing methods."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Controllable Generation Experimental Setup. All models are trained on an internally collected dataset. The one-step generator was initialized using weights from Stable Diffusion 1.5 [35]. Subsequently, the one-step generator was pre-trained using the Diff-Instruct [3]. The ControlNet was initialized following the procedure outlined in its original publication [15]. An Exponential Moving Average (EMA) with decay rate of 0.9999 was applied to the ControlNet parameters, denoted as ϕ. To evaluate the performance of our proposed method in one-step controllable generation, we employed four distinct conditioning signals: Canny edges [36], HED (Holistically-Nested Edge Detection) boundaries [37], depth maps, and lower-resolution images. Evaluation Metric. Image quality was assessed using the Fréchet Inception Distance (FID) [38]. Specifically, the FID score was computed by comparing images generated by the base diffusion model without controls against images generated with the incorporation of the aforementioned conditional inputs. The consistency metric for measuring controllability is quantified between the conditioning input and the condition extracted from the generated image h(x), as formulated below: Consistency = h(x) c1, (13) where h() represents the function used to extract the conditioning information (e.g., Canny edge detector, depth estimator) from generated image x, and is the target conditional input. Furthermore, to assess computational efficiency, we report the NFE required to generate single image. Quantitative Results. We conduct comprehensive evaluations, benchmarking our proposed approach against three established baseline methods: (1) the standard diffusion model with ControlNet; (2) 7 Figure 3: Visual samples of image-reference generations. The samples are generated by our NCT with 1NFE. pre-trained one-step generator integrated with the DMs ControlNet; and (3) crafted ControlNet specifically trained for one-step generator trained via JDM distillation [9]. Notably, the JDM approach necessitates an additional, computationally intensive distillation phase to incorporate control mechanisms. This step is redundant given that the one-step generator has already undergone distillation process. In contrast, our method is tailored for one-step generators, obviating the need for further distillation and thereby enhancing computational efficiency. The quantitative results, presented in Table 1, assess both image fidelity (FID) and adherence to conditional inputs across diverse control tasks. Our proposed method achieves remarkable reduction in the number of function evaluations (NFEs) from 50 to 1, while concurrently maintaining or surpassing the performance metrics of the baselines. Specifically, our approach demonstrates superior FID scores and stronger consistency measures across various conditioning tasks, signifying enhanced image quality and more precise alignment with control conditions. These findings collectively establish that our method achieves superior trade-off between computational efficiency and sample quality in controlled image generation. It delivers state-of-the-art performance with substantially reduced computational overhead and more streamlined training pipeline. Qualitative Comparison. qualitative comparison of our method against baselines is presented in Fig. 2, comparing standard ControlNet and DI+ControlNet which does not require additional distillation. Visual results reveal that while the standard DMs ControlNet can impart high-level control to one-step generators, this integration frequently results in discernible degradation of image quality. In stark contrast, our approach, which involves customized training for adding new controls to one-step generator, consistently produces images of significantly higher fidelity. These visual results substantiate the efficacy of our proposed methodology, suggesting its capability to implicitly learn the conditional distribution p(xc) through our novel noise consistency training. 4.2 Image Prompted Generation Experiment Setting. The pre-trained one-step generator remains consistent with that employed in the prior experiments. We employ the IP-Adapter [39] architecture to serve as the adapter for injecting image prompts. Following IP-Adapter, we use OpenCLIP ViT-H/14 as the image encoder.. Quantitative Comparison. Our method is quantitatively benchmarked against the original IPAdapter. Following IP-Adapter [39], we generate four images conditioned on each image prompt, for every sample in the COCO-2017-5k dataset [40]. Alignment with the image condition is assessed using two established metrics: 1) CLIP-I: The cosine similarity between the CLIP image embeddings of the generated images and the respective image prompt; 2) CLIP-T: The CLIP Score measuring 8 Method IP-Adapter Ours NFE Clip-T Clip-I 100 1 0.588 0.593 0.828 0.821 Table 2: Comparison of machine metrics of different methods regarding imageprompted generation. The mark denotes that the result is taken from the official report. Method Ours w/o noise consistency loss w/o boundary loss w/o primal-dual FID 13.67 20.56 216.93 14.13 Con. 0.110 0.165 0.113 0.117 Table 3: Ablation study on proposed components in our NCT. Figure 4: Both boundary loss and noise consistency loss are crucial to our NCT. Without Boundary loss, the models distribution collapses. Without noise consistency loss, the model ignores the injected condition. the similarity between the generated images and the captions corresponding to the image prompts. The quantitative results, summarized in Table 2, reveal that our Noise Consistency Training (NCT) method achieves performance comparable to the original IP-Adapter (which necessitates 100 NFEs) on both CLIP-I and CLIP-T metrics. Crucially, NCT attains this level of performance with only single NFE, signifying an approximate 100-fold improvement in computational efficiency. Multi-modal Prompts. Our investigations indicate that NCT can concurrently process both image and textual prompts. Fig. 3 illustrates generation outcomes achieved through the use of such multimodal inputs. As demonstrated, the integration of supplementary text prompts facilitates the generation of more diverse visual outputs. This allows for capabilities such as attribute modification and scene alteration based on textual descriptions, relative to the content of the primary image prompt. Structure Control. We observe that NCT permits the test-time compatibility of adapters designed for image prompting with those designed for controllable generation. This enables the generation of images based on image prompts while jointly incorporating additional structural or conditional controls, as shown in Fig. 3. Such test-time compatibility underscores the inherent flexibility and potential of NCT for training distinct adapters for one-step generator, which can subsequently be combined effectively during the inference stage. 4.3 Ablation Study The Effect of Noise Consistency Loss. The noise consistency loss is crucial to force adapter ϕ to learn condition c. Without the loss, it can be seen that the consistency metric degrades severely, and the generated samples do not follow the condition at all. This is because the adapter ϕ is trained on fully-coupled (z, c) pairs, allowing it find find shortcut solution that directly ignores the learnable parameters to satisfy the boundary loss. The Effect of Boundary Loss. The boundary loss can constrain the output of the generator fθ,ϕ in the image domain. Without the loss, although the generator can still learns some conditions, its generated samples entirely collapse as indicated by the FID and visual samples. The Effect of Primal-Dual. We use primal-dual since it is crafted for solving the constrained problem, while it owns theoretical guarantees and dynamically balances the noise consistency loss and boundary loss. We empirically validate its effectiveness, it can be seen that without primal-dual, the performance slightly degrades regarding both fidelity and condition alignment."
        },
        {
            "title": "5 Conclusion",
            "content": "This paper addressed the critical challenge of efficiently incorporating new controls into pre-trained one-step generative models, key bottleneck in the rapidly evolving field of AIGC. We introduced Noise Consistency Training (NCT), novel and lightweight approach that empowers existing one-step 9 generators with new conditioning capabilities without the need for retraining the base diffusion model or additional diffusion distillation or accessing the original training dataset. By operating in the noise space and leveraging carefully formulated noise-space consistency loss, NCT effectively aligns the adapted generator with the desired control signals. Our proposed NCT framework offers significant advantages in terms of modularity, data efficiency, and ease of deployment. The experimental results across diverse control scenarios robustly demonstrate that NCT achieves state-of-the-art performance in controllable, single-step generation. It surpasses existing multi-step and distillation-based methods in both the quality of the generated content and computational efficiency."
        },
        {
            "title": "References",
            "content": "[1] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [2] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 22562265. PMLR, 2015. [3] Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhihua Zhang. Diffinstruct: universal approach for transferring knowledge from pre-trained diffusion models. Advances in Neural Information Processing Systems, 36, 2023. [4] Yihong Luo, Xiaolong Chen, Xinghua Qu, Tianyang Hu, and Jing Tang. You only sample once: Taming one-step text-to-image synthesis by self-cooperative diffusion gans, 2024. [5] Tianwei Yin, Michaël Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. arXiv preprint arXiv:2311.18828, 2023. [6] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models, 2023. [7] Linqi Zhou, Stefano Ermon, and Jiaming Song. Inductive moment matching. arXiv preprint arXiv:2503.07565, 2025. [8] Xuanwu Yin Yuda Song, Zehao Sun. Sdxs: Real-time one-step latent diffusion models with image conditions. arxiv, 2024. [9] Yihong Luo, Tianyang Hu, Yifan Song, Jiacheng Sun, Zhenguo Li, and Jing Tang. Adding additional control to one-step diffusion with joint distribution matching. arXiv preprint arXiv:2503.06652, 2025. [10] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. arXiv preprint arXiv:2302.08453, 2023. [11] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. [12] Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Roni Sengupta, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Universal guidance for diffusion models. In The Twelfth International Conference on Learning Representations, 2024. [13] Jiajun Ma, Tianyang Hu, Wenjia Wang, and Jiacheng Sun. Elucidating the design space of classifier-guided diffusion generation. arXiv preprint arXiv:2310.11311, 2023. [14] Yihong Luo, Tianyang Hu, Weijian Luo, Kenji Kawaguchi, and Jing Tang. Rewardinstruct: reward-centric approach to fast photo-realistic image generation. arXiv preprint arXiv:2503.13070, 2025. [15] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. 10 [16] Arthur Gretton, Karsten Borgwardt, Malte Rasch, Bernhard Schölkopf, and Alexander Smola. kernel two-sample test. The Journal of Machine Learning Research, 13(1):723773, 2012. [17] Alfred Müller. Integral probability metrics and their generating classes of functions. Advances in applied probability, 29(2):429443, 1997. [18] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpmsolver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022. [19] Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and Jiwen Lu. Unipc: unified predictorcorrector framework for fast sampling of diffusion models. Advances in Neural Information Processing Systems, 36:4984249869, 2023. [20] Shuchen Xue, Zhaoqiang Liu, Fei Chen, Shifeng Zhang, Tianyang Hu, Enze Xie, and Zhenguo Li. Accelerating diffusion sampling with optimized time steps. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 82928301, 2024. [21] Chenyang Si, Ziqi Huang, Yuming Jiang, and Ziwei Liu. Freeu: Free lunch in diffusion u-net. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 47334743, 2024. [22] Jiajun Ma, Shuchen Xue, Tianyang Hu, Wenjia Wang, Zhaoqiang Liu, Zhenguo Li, Zhi-Ming Ma, and Kenji Kawaguchi. The surprising effectiveness of skip-tuning in diffusion sampling. arXiv preprint arXiv:2402.15170, 2024. [23] Eric Luhman and Troy Luhman. Knowledge distillation in iterative generative models for improved sampling speed. arXiv preprint arXiv:2101.02388, 2021. [24] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1429714306, June 2023. [25] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations, 2022. [26] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. 2023. [27] Yang Song and Prafulla Dhariwal. Improved techniques for training consistency models. In The Twelfth International Conference on Learning Representations, 2024. [28] Hanshu Yan, Xingchao Liu, Jiachun Pan, Jun Hao Liew, Qiang Liu, and Jiashi Feng. Perflow: Piecewise rectified flow as universal plug-and-play accelerator. arXiv preprint arXiv:2405.07510, 2024. [29] Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, Mingzhang Yin, and Hai Huang. Score identity distillation: Exponentially fast distillation of pretrained diffusion models for one-step generation. In International Conference on Machine Learning, 2024. [30] Jie Xiao, Kai Zhu, Han Zhang, Zhiheng Liu, Yujun Shen, Zhantao Yang, Ruili Feng, Yu Liu, Xueyang Fu, and Zheng-Jun Zha. CCM: Real-time controllable visual content creation using text-to-image consistency models. In Forty-first International Conference on Machine Learning, 2024. [31] Tianwei Yin, Michaël Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and William T. Freeman. Improved distribution matching distillation for fast image synthesis, 2024. [32] Yihong Luo, Tianyang Hu, Jiacheng Sun, Yujun Cai, and Jing Tang. Learning few-step diffusion models by trajectory distribution matching, 2025. [33] Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, and Qiang Liu. Instaflow: One step is enough for high-quality diffusion-based text-to-image generation. arXiv preprint arXiv:2309.06380, 2023. [34] Luiz FO Chamon, Santiago Paternain, Miguel Calvo-Fullana, and Alejandro Ribeiro. Constrained learning with non-convex losses. arXiv:2103.05134, 2021. [35] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [36] John Canny. computational approach to edge detection. PAMI, 1986. [37] Saining Xie and Zhuowen Tu. Holistically-nested edge detection. In ICCV, 2015. [38] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. [39] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. 2023. [40] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014."
        },
        {
            "title": "A Theoretical Foundation of Noise Consistency Training",
            "content": "This section establishes the theoretical foundation: it begins with definitions and the mathematical setup (A.1), then introduces key lemmas (A.2) that collectively build the necessary mathematical foundationby defining critical distribution relationships and an input interpolation pathfor formulating the conditions and proving the main theorem (A.3). Specifically, Lemma 1, Lemma 2, and Theorem 1 presented in the main paper are proved in Lemma A.3, Remark 1, and Theorem A.1 respectively in this section. A.1 Definition and Setup Latent Distribution: The latent distribution Γ is the standard Gaussian measure on (Rm, B(Rm)). The measure Γ has density γ(z) w.r.t. dz, so dΓ(z) = γ(z)dz. Γ is probability measure: (cid:82) Rm γ(z)dz = 1. Implicit Generator: fθ : Rm Rn is measurable function. Data Distribution: Pθ on (Rn, B(Rn)) is the push-forward Pθ = fθ #Γ. It has density p(x) w.r.t. dx, so dPθ(x) = pθ(x)dx. Since Γ is probability measure, Pθ is also probability measure: (cid:82) Rn pθ(x)dx = 1. Condition:. Let (C, BC, µC) be measure space for the conditions. BC is σ-algebra on C, and µC is reference measure (e.g., Lebesgue measure if = Rk (such as Canny Edge), or counting measure if is discrete) (such as class labels). For each Rn, p(x) is probability measure on (C, BC). We assume it has density p(cx) with respect to µC. Thus, for any Rn: (cid:82) p(cx)dµC(c) = 1. Combined Map: = fθ id : Rm Rn C, (z, c) = (fθ(z), c). Since fθ and id are measurable, is measurable with respect to the product σ-algebras B(Rm) BC and B(Rn) BC. Implicit Generator with Condition: fθ,ϕ : Rm Rn be measurable function. Combined Map with Condition: We define new map Tϕ : Rm Rn as Tϕ(z, c) = (fθ,ϕ(z, c), c). Marginal Condition Density: We define the marginal probability density p(c) of the condition as: p(c) = (cid:90) Rn pθ(x)p(cx)dx = (cid:90) Rm γ(z)p(cfθ(z))dz This is probability density with respect to µC, i.e., (cid:82) p(c)dµC(c) = 1. Initial Coupled Latent-Condition Distribution: Density ν(z, c) = γ(z)p(cfθ(z)) w.r.t. dzdµC(c). Independent Latent-Condition Coupling: We define the probability measure ρ on the input space (Rm C, B(Rm) BC) by its density with respect to the reference measure dz dµC(c): dρ(z, c) = γ(z)p(c)dz dµC(c) Here, γ(z) is the density of the standard Gaussian measure Γ on Rm. The measure ρ corresponds to sampling Γ independently from sampling p(c). Target Data-Condition Distribution η: Density pη(x, c) = pθ(x)p(cx) w.r.t. dxdµC(c). MMD (Maximum Mean Discrepancy): MMD2(P, Q) is metric between probability distributions and Q. For characteristic kernel, MMD2(P, Q) = 0 = Q. A.2 Lemmas Lemma A.1. Let Γ be the standard Gaussian measure on Rm with density γ(z) with respect to the Lebesgue measure dz. Let fθ : Rm Rn be measurable function, and let Pθ = fθ #Γ be the push-forward measure on Rn, assumed to have density pθ(x) with respect to the Lebesgue measure dx. Let (C, BC, µC) be measure space for conditions, and let p(cx) be conditional probability density on with respect to µC for each Rn, such that (cid:82) p(cx)dµC(c) = 1. 13 Define the measure ν on Rm by its density with respect to dzdµC(c): dν(z, c) = γ(z)p(cfθ(z))dzdµC(c) Define the map = fθ id : Rm Rn by (z, c) = (fθ(z), c). Then the push-forward measure T#ν on RnC has the density pθ(x)p(cx) with respect to dxdµC(c). That is, (fθ id)#(γ(z)p(cfθ(z))dzdµC(c)) = pθ(x)p(cx)dxdµC(c) Proof. We want to show T#ν = η. By definition of equality of measures, it suffices to show that for any bounded, measurable test function Φ : Rn R: (cid:90) RnC Φ(x, c)d(T#ν)(x, c) = (cid:90) RnC Φ(x, c)dη(x, c) We start with the left-hand side (LHS). Using the change of variables formula for push-forward measures: (cid:90) LHS = RnC (cid:90) RmC (cid:90) RmC = = = Φ(x, c)d(T#ν)(x, c) Φ(T (z, c))dν(z, c) (Change of Variables) Φ(fθ(z), c)γ(z)p(cfθ(z))dzdµC(c) (Substitute and density of ν) (cid:90) Rm γ(z) Φ(fθ(z), c)p(cfθ(z))dµC(c) dz (Fubinis Theorem) (cid:21) (cid:20)(cid:90) The application of Fubinis theorem is justified because Φ is bounded, γ(z) 0, p(cfθ(z)) 0, and ν is finite (probability) measure. Lets define an auxiliary function : Rn as: (cid:90) g(y) = Φ(y, c)p(cy)dµC(c) Since Φ is bounded (say Φ ) and (cid:82) p(cy)dµC(c) = 1, g(y) is also bounded (g(y) ). If Φ is B(Rn) BC-measurable and p(cy) defines measurable transition kernel, then is B(Rn)- measurable. Substituting into our integral expression: LHS = (cid:90) Rm g(fθ(z))γ(z)dz Now, recall the definition of the push-forward measure Pθ = f#Γ. For any bounded, measurable function : Rn R: (cid:90) (cid:90) In terms of densities: (cid:90) Rn h(x)pθ(x)dx = Applying this identity with = g: h(x)dPθ(x) = h(fθ(z))dΓ(z) Rn Rm (cid:90) Rm h(fθ(z))γ(z)dz So, (cid:90) Rm g(fθ(z))γ(z)dz = (cid:90) Rn g(x)pθ(x)dx LHS = (cid:90) Rn g(x)pθ(x)dx Now, substitute back the definition of g(x): LHS = (cid:90) (cid:20)(cid:90) Rn (cid:21) Φ(x, c)p(cx)dµC(c) pθ(x)dx 14 Applying Fubinis Theorem again (justified as before): (cid:90) LHS = RnC Φ(x, c)pθ(x)p(cx)dxdµC(c) This is precisely the integral with respect to the target measure η: (cid:90) LHS = RnC Φ(x, c)dη(x, c) Since we have shown that (cid:82) Φd(T#ν) = (cid:82) Φdη for all bounded, measurable test functions Φ, the measures must be equal: T#ν = η Lemma A.2 (Boundary Loss). Let fθ,ϕ : Rm Rn be measurable function. Let ν be the measure on Rm with density γ(z)p(cfθ(z)) w.r.t. dzdµC(c). Let be distance metric on Rn. If the boundary loss E(z,c)ν[d(fθ,ϕ(z, c), fθ(z))] = 0 then: The push-forward measure ηϕ = (Tϕ)#ν is equal to the target measure η = T#ν. This means the joint distribution pθ,ϕ(x, c) induced by fθ,ϕ is pθ(x)p(cx), i.e., ηϕ = η. Proof. The condition is E(z,c)ν[d(fθ,ϕ(z, c), fθ(z))] = 0. Since d(a, b) 0 for any a, Rn, and d(a, b) = 0 if and only if = b, the expectation of this non-negative quantity being zero implies that the integrand must be zero ν-almost everywhere. That is, This implies d(fθ,ϕ(z, c), fθ(z)) = 0 for ν-a.e. (z, c) fθ,ϕ(z, c) = fθ(z) for ν-a.e. (z, c) We want to show that ηϕ = (Tϕ)#ν is equal to η = T#ν. Recall the definitions of the maps: (z, c) = (fθ(z), c) Tϕ(z, c) = (fθ,ϕ(z, c), c) Since fθ,ϕ(z, c) = fθ(z) for ν-a.e. (z, c), it follows directly that the maps Tϕ and are equal ν-almost everywhere: Tϕ(z, c) = (fθ,ϕ(z, c), c) = (fθ(z), c) = (z, c) for ν-a.e. (z, c) If two measurable maps and Tϕ are equal ν-a.e., their push-forward measures T#ν and (Tϕ)#ν are identical. Let Ψ : Rn be any bounded, measurable test function. (cid:90) (cid:90) Ψ(x, c)d((Tϕ)#ν)(x, c) = Ψ(Tϕ(z, c))dν(z, c) RnC RmC (cid:90) RmC (cid:90) RnC = = Ψ(T (z, c))dν(z, c) (since Tϕ = ν-a.e. and Ψ is bounded) Ψ(x, c)d(T#ν)(x, c) Since this holds for all bounded measurable Ψ, we have (Tϕ)#ν = T#ν. From Lemma 1, we know T#ν = η, where η has density pθ(x)p(cx) with respect to dxdµC(c). Therefore, ηϕ = (Tϕ)#ν = η. Lemma A.3 (Interpolation of Joint Latent-Condition Distributions (Lemma 1 in main paper)). Let γ(z) be the density of the standard Gaussian measure on Rm. Let fθ : Rm Rn be measurable function. Let p(cx) be conditional probability density on (with respect to reference measure µC) for each Rn. Define the marginal condition density p(c) as: p(c) = (cid:90) Rm γ(z)p(cfθ(z))dz 15 Assume p(c) > 0 for µC-almost every in the support of interest. Define the conditional latent density pdata(z0c) as: pdata(z0c) = γ(z0)p(cfθ(z0)) p(c) Consider time-dependent process for [0, 1] where zt is generated from z0 pdata(c) by: zt = αtz0 + σtϵ, where ϵ (0, Im) independent of z0 and c. The coefficients αt, σt satisfy: α0 = 1, σ0 = 0 α1 = 0, σ1 = 1 αt is monotonically decreasing, σt is monotonically increasing. Im) be the density of zt given z0. Define the conditional density Let qt(ztz0) = (zt; αtz0, σ2 pt(zc) as: (cid:90) pt(zc) = And the joint density pt(z, c) on Rm (with respect to dzdµC(c)) as: qt(zz0)pdata(z0c)dz0 Rm Then, pt(z, c) = pt(zc)p(c) 1. At = 0, the joint density is p0(z, c) = γ(z)p(cfθ(z)). 2. At = 1, the joint density is p1(z, c) = γ(z)p(c). Proof. The joint density at time is given by pt(z, c) = pt(zc)p(c). Substituting the definition of pt(zc): pt(z, c) = p(c) (z; αtz0, σ2 Im)pdata(z0c)dz0 (cid:90) Rm Now, substitute the definition of pdata(z0c) = γ(z0)p(cfθ(z0)) p(c) : pt(z, c) = p(c) (cid:90) Rm (z; αtz0, σ2 Im) γ(z0)p(cfθ(z0)) p(c) dz0 Assuming p(c) = 0 (for µC-a.e. c), we can cancel p(c): pt(z, c) = (z; αtz0, σ2 Im)γ(z0)p(cfθ(z0))dz (cid:90) Rm At = 0, we have α0 = 1 and σ0 = 0. The Gaussian density (z; α0z0, σ2 (z; z0, 0 Im). This is interpreted as the Dirac delta function δ(z z0). So, 0Im) becomes p0(z, c) = δ(z z0)γ(z0)p(cfθ(z0))dz (cid:90) Rm = γ(z)p(cfθ(z)) This matches the first target distribution. At = 1, we have α1 = 0 and σ1 = 1. The Gaussian density (z; α1z0, σ2 (z; 0 z0, 12Im) = (z; 0, Im). By definition, (z; 0, Im) = γ(z). So, (by the sifting property of the Dirac delta) 1Im) becomes p1(z, c) = (cid:90) Rm = γ(z) γ(z)γ(z0)p(cfθ(z0))dz0 (cid:90) Rm γ(z0)p(cfθ(z0))dz0 The integral (cid:82) Rm γ(z0)p(cfθ(z0))dz0 is, by definition, p(c). Therefore, p1(z, c) = γ(z)p(c) This matches the second target distribution. Thus, the process defines an interpolation for the joint density pt(z, c) between p0(z, c) = γ(z)p(cfθ(z)) and p1(z, c) = γ(z)p(c). 16 A.3 Main Theorem and Proof Definition 1 (Interpolation Distribution Sequence (from Lemma 3)). sequence of time points 0 = t0 < t1 < < tN = 1. For each tk, we have latent-condition distribution νtk (density ptk (z, c)) such that νt0 = ν0 and νtN = ρ. Theorem A.1. Assume the distributions η, ν0, ρ and the interpolation sequence {νtk }N k=0 as defined above. Let fθ : Rm Rn be pre-trained generator, and fθ,ϕ : Rm Rn be conditional generator with single set of trainable parameters ϕ. The map Tϕ is defined as Tϕ(z, c) = (fθ,ϕ(z, c), c). Consider the following two conditions: 1. Boundary Condition: The parameters ϕ ensure the boundary loss is zero: E(z,c)νt [d(fθ,ϕ(z, c), fθ(z))] = 0 where d(, ) is distance metric on Rn. By the Boundary Loss Lemma, this implies (Tϕ)#νt0 = η. 2. Consistency Condition: The parameters ϕ also satisfy: Ltotal(ϕ) = 1 (cid:88) k= MMD2((Tϕ)#νtk+1, (Tϕ)#νtk ) = 0 If such parameter set ϕ exists and satisfies both conditions above, then fθ,ϕ (when its input is distributed according to ρ) generates the target data-condition distribution η: (Tϕ)#ρ = η That is, if (z, c) ρ (i.e., γ() and independently p()), then (fθ,ϕ(z, c), c) η (i.e., its density is pθ(x)p(cx)). Proof. Let ϕ be parameter set that satisfies the two conditions stated in the theorem. The first condition is E(z,c)νt0 [d(fθ,ϕ(z, c), fθ(z))] = 0. Recall that νt0 is the distribution with density γ(z)p(cfθ(z)). According to the Boundary Loss Lemma (Lemma 2), this zero loss implies that the push-forward measure (Tϕ)#νt0 is equal to the target distribution η. So, (Tϕ)#νt0 = η. The second condition is (cid:80)N 1 k=0 MMD2((Tϕ)#νtk+1, (Tϕ)#νtk ) = 0. Since MMD2(P, Q) 0 for any probability distributions P, Q, for the sum of non-negative terms to be zero, each individual term in the sum must be zero. Therefore, for each {0, 1, . . . , 1}: MMD2((Tϕ)#νtk+1, (Tϕ)#νtk ) = 0 Assuming MMD is based on characteristic kernel, MMD2(P, Q) = 0 if and only if = Q. Thus, for each {0, 1, . . . , 1}: (Tϕ)#νtk+1 = (Tϕ)#νtk The result from step 2 implies chain of equalities for the push-forward measures generated by Tϕ from the sequence of input distributions νtk : (Tϕ)#νtN = (Tϕ)#νtN 1 = (Tϕ)#νtN 2 ... = (Tϕ)#νt1 = (Tϕ)#νt0 So, we have (Tϕ)#νtN = (Tϕ)#νt0. From the first condition, we established that (Tϕ)#νt0 = η. Substituting this into the equality chain: (Tϕ)#νtN = η 17 From Lemma A.3, we know that νtN (which corresponds to pt(z, c) at = tN = 1) is the independent latent-condition distribution ρ. The density of ρ is pρ(z, c) = γ(z)p(c). Substituting νtN = ρ: (Tϕ)#ρ = η This is the desired conclusion. If (z, c) is sampled from ρ (meaning γ() independently of p()) and then transformed by Tϕ (i.e., forming (fθ,ϕ(z, c), c)), the resulting distribution is the target data-condition distribution η (which has density pθ(x)p(cx)). Remark 1 (Lemma 2 in main paper). Specifically, when we take = 1 (particle number) in MMD loss, and take we have some specific kernel choice: k(x, y) = y2, although it is not proper positive definite kernel required by MMD, we find it works well in practice k(x, y) = (cid:112)x y2 + c2 is conditionally positive definite kernel."
        },
        {
            "title": "Then the summed MMD Loss",
            "content": "Ltotal(ϕ) = 1 (cid:88) k=0 MMD2((Tϕ)#νtk+1, (Tϕ)#νtk ) = 0, can be implemented in practical way: Ltotal(ϕ) = 1 (cid:88) k=0 Eγ(z)p(cfθ(z))Eγ(w)d(fθ,ϕ(αtk+1 + σtk+1w, c), fθ,ϕ(αtk + σtk w, c)), where is l2 loss or pseudo-huber loss, other kernel-induced losses also work."
        },
        {
            "title": "B Experiment Details",
            "content": "One-step generator We adopt Diff-Instruct [3] for pre-training the one-step generator. We adopt the AdamW optimizer. The β1 is set to be 0, and the β2 is set to be 0.95. The learning rate for the generator is 2e 6, the learning rate for fake score is 1e 5. We apply gradient norm clipping with value of 1.0 for both the generator and fake score. We use batch size of 256. Controllable Generation We use Contorlnets architecture [15] for training. We adopt the AdamW optimizer with β1 = 0.9, β2 = 0.95, and the learning rate of 1e 5. We use batch size of 128. Image-prompted Geneartion We use IP-adapters architecture [39] for training. We adopt the AdamW optimizer with β1 = 0.9, β2 = 0.95, and the learning rate of 1e 4. We use batch size of 128. We use probability of 0.05 to drop text during training."
        }
    ],
    "affiliations": [
        "HKUST",
        "HKUST(GZ)",
        "NUS",
        "UCAS"
    ]
}