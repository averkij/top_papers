{
    "paper_title": "Deep Self-Evolving Reasoning",
    "authors": [
        "Zihan Liu",
        "Shun Zheng",
        "Xumeng Wen",
        "Yang Wang",
        "Jiang Bian",
        "Mao Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Long-form chain-of-thought reasoning has become a cornerstone of advanced reasoning in large language models. While recent verification-refinement frameworks have enabled proprietary models to solve Olympiad-level problems, their effectiveness hinges on strong, reliable verification and correction capabilities, which remain fragile in open-weight, smaller-scale models. This work demonstrates that even with weak verification and refinement capabilities on hard tasks, the reasoning limits of such models can be substantially extended through a probabilistic paradigm we call Deep Self-Evolving Reasoning (DSER). We conceptualize iterative reasoning as a Markov chain, where each step represents a stochastic transition in the solution space. The key insight is that convergence to a correct solution is guaranteed as long as the probability of improvement marginally exceeds that of degradation. By running multiple long-horizon, self-evolving processes in parallel, DSER amplifies these small positive tendencies, enabling the model to asymptotically approach correct answers. Empirically, we apply DSER to the DeepSeek-R1-0528-Qwen3-8B model. On the challenging AIME 2024-2025 benchmark, DSER solves 5 out of 9 previously unsolvable problems and boosts overall performance, enabling this compact model to surpass the single-turn accuracy of its 600B-parameter teacher through majority voting. Beyond its immediate utility for test-time scaling, the DSER framework serves to diagnose the fundamental limitations of current open-weight reasoners. By clearly delineating their shortcomings in self-verification, refinement, and stability, our findings establish a clear research agenda for developing next-generation models with powerful, intrinsic self-evolving capabilities."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 8 9 4 7 1 . 0 1 5 2 : r Preprint. DEEP SELF-EVOLVING REASONING Zihan Liu 1, Shun Zheng 2, Xumeng Wen 2, Yang Wang2, Jiang Bian2, Mao Yang2 1Peking University 2Microsoft Research Asia"
        },
        {
            "title": "ABSTRACT",
            "content": "Long chain-of-thought reasoning has become cornerstone of advanced reasoning in large language models. While recent verificationrefinement frameworks have enabled proprietary models to solve Olympiad-level problems, their effectiveness hinges on strong, reliable verification and correction capabilities, which remain fragile in open-weight, smaller-scale models. This work demonstrates that even with weak capabilities for hard tasks, the reasoning limits of such models can be substantially extended through probabilistic paradigm we call Deep Self-Evolving Reasoning (DSER). We conceptualize iterative reasoning as Markov chain, where each step represents stochastic transition in the solution space. The key insight is that convergence to correct solution is guaranteed as long as the probability of improvement marginally exceeds that of degradation. By running multiple long-horizon, self-evolving processes in parallel, DSER amplifies these small positive tendencies, enabling the model to asymptotically approach correct answers. Empirically, we apply DSER to the DeepSeek-R1-0528-Qwen3-8B model. On the challenging AIME 20242025 benchmark, DSER solves 5 out of 9 previously unsolvable problems and boosts overall performance, enabling this compact model to surpass the singleturn accuracy of its 600B-parameter teacher through majority voting. Beyond its immediate utility for test-time scaling, the DSER framework serves to diagnose the fundamental limitations of current open-weight reasoners. By clearly delineating their shortcomings in verification, refinement, and stability, our findings establish clear research agenda for developing next-generation models with powerful, intrinsic self-evolving capabilities. Figure 1: Deep self-evolving reasoning enables DeepSeek-R1-0528-Qwen3-8B to solve 5 of 9 AIME 2024-2025 problems previously deemed unsolvable by standard majority voting over parallel trials (Avg@64: average accuracy over 64 runs, Cons@64: consistency accuracy over 64 runs). notable example is the success for difficult problem in AIME 2025 after 80 self-evolving iterations, process consuming approximately 10 million reasoning tokens. The final correct answer can be determined by majority vote across the last ten self-evolving iterations. These authors contributed equally: Zihan Liu, Shun Zheng, Xumeng Wen. Zihan did this work during the internship at Microsoft Research Asia. Correspondence to shun.zheng@microsoft.com. Preprint."
        },
        {
            "title": "INTRODUCTION",
            "content": "Chain-of-Thought (CoT) reasoning (Wei et al., 2022), cornerstone technique in large language models (LLMs), has driven rapid progress in advancing reasoning capability. It was first demonstrated in OpenAIs o1 OpenAI (2024) series models that increasing the length of CoT directly leads to test-time scaling, enabling LLMs to tackle more complex and challenging tasks. Following this, DeepSeek-R1 (Guo et al., 2025) became the first open-source effort to realize long-form CoT reasoning through reinforcement learning. At the heart of this approach lies the Group Relative Policy Optimization (GRPO) algorithm (Shao et al., 2024), which effectively incentivizes high-quality reasoning traces in pre-trained LLMs (Wen et al., 2025). Since the public release of DeepSeek-R1, the community has witnessed wave of reproductions (Yu et al., 2025; He et al., 2025; Liu et al., 2025; Hu et al., 2025) and the surge of large-scale, high-performance reasoning models (Yang et al., 2025; Kimi et al., 2025; GLM-4.5 et al., 2025) in the open-source ecosystem. Building on long CoT reasoning, frontier industry labs claimed advanced reasoning systems whose performance rivals that of IMO 2025 gold medalists (OpenAI, 2025a; Gemini, 2025; Chen et al., 2025). An independent study (Huang & Yang, 2025) further reported that state-of-the-art proprietary models, such as Gemini 2.5 Pro (Gemini, 2025), GPT-5 (OpenAI, 2025b), and Grok-4 (X AI, 2025), can solve 5 out of 6 IMO problems using model-agnostic, verificationrefinement framework. While similar self-refining concepts had already emerged in prior studies (Kim et al., 2023; Madaan et al., 2023; Kamoi et al., 2024; Kumar et al., 2024; Bensal et al., 2025), this framework offered concrete and practical insights, clearly demonstrating the immense potential of iterative reasoning calls to solve problems at the IMO level. However, the framework introduced in (Huang & Yang, 2025) relies heavily on advanced verification, refinement, and instruction-following abilities, which remain largely exclusive to leading proprietary models when handling extremely hard reasoning tasks. It is still unclear to what extent open-weight reasoning models, especially small and medium-sized ones with broader accessibility, can benefit from self-evolving paradigms and extend their reasoning limits. In practice, such models often exhibit weak self-verification, occasional self-improvement, and unstable instructionfollowing behaviors, leading to unexpected terminations under Huang & Yangs framework. In this work, we show that even when model exhibits weak verification and refinement capabilities on hard reasoning tasks, simple self-evolving setup with concise prompts could still substantially extend the reasoning boundary. Our approach begins from probabilistic interpretation of the classic verificationrefinement iteration: we view each iteration as transition step of self-evolving stochastic process. The models verification and refinement abilities determine the transition probability matrix for given problem, forming Markov chain whose convergence can be theoretically guaranteed. As long as the probability of improvement (transitioning from an incorrect to correct solution) exceeds the probability of degradation (from correct to incorrect), the process converges to stationary distribution dominated by correct solutions. By running multiple independent selfevolving processes over sufficiently long iterations, the model can fully unlock its inherent selfevolving potential. We refer to this general paradigm as Deep Self-Evolving Reasoning (DSER). The core insight of DSER lies in its probabilistic view of self-improvement. Rather than expecting each round of verification and refinement to succeed with high accuracy, DSER leverages the convergence property of Markov chains to ensure asymptotic correctness. It treats multi-turn reasoning as stochastic optimization trajectory in the discrete token space, where small but statistically positive tendencies toward improvement are sufficient to guarantee long-term convergence toward correct solutions. In practice, we observe that even when the degradation probability exceeds the improvement probability, parallel DSER procedures could still produce correct majority-voting answer because correct solutions converge to the same ground-truth while incorrect ones diverge in different results. Moreover, we note that any verification-refinement iterations can be viewed as self-evolving stochastic process, including Huang & Yangs framework. The key distinction is that they allocated more reasoning budgets to verification and add specific conditions to exit the loop. We evaluate our approach using DeepSeek-R1-0528-Qwen3-8B, configured with up to 64K response tokens per reasoning call. Although this model exhibits strong reasoning ability for its scale, it fails to solve 9 problems (under majority voting) out of 60 in AIME 2024 and 2025 benchmarks. For these challenging cases, the average Pass@1 is below 0.05, and both verification and correction success rates remain low. As shown in Figure 1, applying DSER enables the model 2 Preprint. to solve 5 of these 9 hard problems through majority voting. Notably, this includes one problem with an initial single-turn Pass@1 of zero (estimated over 128 samples). These results indicate that DSER successfully extends the single-turn reasoning boundaries of this 8B model. Moreover, our additional experiments show that when applied to the entire AIME benchmark, DSER improves its Pass@1 accuracy by 6.5% on AIME 2024 and by 9.0% on AIME 2025. Specifically, DSER enables the majority-voting accuracy of this 8B model to surpass the Pass@1 performance of its 600B-parameter teacher model, DeepSeek-R1-0528. This demonstrates that DSER effectively trades test-time computation for enhanced model capacity. The implications of this work extend beyond its core demonstration of self-evolution under imperfect verification and refinement. For instance, the approach could improve the exploration stage in GRPO training, helping to uncover successful reasoning pathways for extremely difficult problems. Moreover, it could help to reduce the deployment cost while maintaining comparable reasoning performance. Furthermore, our experimental results also reveal significant shortcomings in existing open-weight reasoning models. key direction for future research is therefore to develop models that are capable of problem-solving, self-verification, providing constructive feedback, increasing correction likelihood, avoiding potential degradation, etc."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Iterative verification and refinement has emerged as foundational technique for enhancing the reasoning capabilities of LLMs, appearing under various names in the literature. Early work explored this concept through frameworks for recursive self-critique and improvement (Kim et al., 2023), as well as using single model to generate, refine, and provide feedback on its own outputs (Madaan et al., 2023). This line of research encompasses related ideas such as self-correction (Kumar et al., 2024) and self-verification or self-reflection (Weng et al., 2022; Bensal et al., 2025). The effectiveness of these methods, however, often depends on the quality of feedback. As noted by Kamoi et al. (2024), self-correction is most successful when guided by reliable external signalsa principle dramatically demonstrated by systems like Seed-Prover (Chen et al., 2025), which achieved stateof-the-art performance on IMO 2025 problems by integrating iterative reasoning with formal verification. Concurrently, Huang & Yang (2025) showed that sophisticated verification-refinement pipeline could enable leading proprietary models to solve problems at an IMO gold medal level. considerable body of recent research has focused on endowing LLMs with more robust, intrinsic capabilities for self-verification and self-improvement through specialized training objectives (Kumar et al., 2024; Bensal et al., 2025; Yuan et al., 2025). Another related direction involves multi-turn tool use, which can be viewed as form of iterative refinement guided by external tools and environments (Feng et al., 2025; Dong et al., 2025; Shang et al., 2025). These developments reflect broader research trend toward self-evolution in LLMs, paradigm shift that extends beyond singleturn reasoning to more powerful capabilities (Tao et al., 2024; Huang et al., 2025). Our work distinguishes itself by introducing novel, probabilistic interpretation of the verificationrefinement loop. We conceptualize iterative reasoning as stochastic process governed by Markov chain. This formulation provides theoretical basis for improvement even when the models verification and refinement capabilities are imperfect (typical conditions for hard tasks), as the process can converge to correct solution given marginal statistical bias towards improvement. This perspective allows LLMs to progressively solve previously intractable problems and reliably uncover effective reasoning pathways, advancing the frontier of what is achievable with open-weight models."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "Our approach models the iterative verification and refinement of solutions as self-evolving stochastic process. This probabilistic framework allows us to analyze the trajectory of solutions quality and understand its convergence towards correctness. Figure 2 gives an overview of our approach. 3 Preprint. Figure 2: An overview of our DSER approach, where each rectangle of Solve, Verify, and Refine corresponds to one LLM reasoning call. In the view of Markov chain, sufficient condition to elicit correct solutions for hard problems is to self-evolve deeply. Figure 3: Through the lens of Markov chain, we revisit the iterative verification-refinement cycle proposed by Huang & Yang. Here we need to define multiple states of solution correctness indexed by the number of consecutive self-verified rejections and refinements. For instance, (9) denotes the solution being correct after 9 consecutive rounds of self-verified rejections and refinements. 3.1 THE SELF-EVOLVING PROCESS Given an initial question prompt q, reasoning LLM generates candidate solution s. This initial step can be formally represented as: (1) where RLLM () denotes reasoning call that takes prompt and outputs summarized solution after long CoT thinking. We define this initial solution as s(0). = RLLM (q), The process then enters series of self-evolving iterations, where the solution at iteration n, denoted by s(n), is transformed into s(n+1) through cycle of self-improvement. In this iteration, various verification-refinement interaction schemes are possible. Below we present the fundamental twostep cycle as an example. First, verification step provides feedback on the current solution. Let pv be the verification prompt designed to elicit this feedback. The resulting verification output v(n) is generated as: v(n) = RLLM ([q; s(n); pv]), where [q; s(n); pv] denotes the concatenation of the original question, the current solution, and the verification prompt as context for the LLM. (2) Next, refinement step uses this feedback to generate an improved solution. Let pr be the refinement prompt. The next-state solution s(n+1) is produced by: s(n+1) = RLLM ([q; s(n); pv; v(n); pr]). (3) 4 Preprint. This iterative process, transforming s(n) s(n+1), continues until termination condition is met, such as fixed number of iterations."
        },
        {
            "title": "3.2 MARKOV CHAIN FORMULATION",
            "content": "To analyze the dynamics of this process, we model the evolution of the solutions correctness as Markov chain. Let us define discrete state space = {C, I}, where denotes that the solution s(n) is Correct and denotes that it is Incorrect. Let Xn be the random variable representing the state of the solution at iteration n. The evolution of the system is then described by the distribution over these states. According to Equations 2, 3, the correctness of the next solution s(n+1) depends on the correctness of the current solution s(n) and not on the history of previous solutions {s(0), . . . , s(n1)}. Moreover, we assume the improvement capability of the LLM is consistent across self-evolving iterations for given problem. Thus single transition probability matrix governs this evolution. = (cid:18)1 pCI pIC pCI 1 pIC (cid:19) (cid:18)P (Xn+1 = CXn = C) (Xn+1 = IXn = C) (Xn+1 = CXn = I) (Xn+1 = IXn = I) (cid:19) = (4) where: pIC is the probability of improvement (moving from Incorrect to Correct). pCI is the probability of degradation (moving from Correct to Incorrect). The specific values of pIC and pCI depend on the capability of the LLM on solving the problem q. Notably, in this formulation, we do not rely on the accuracy of each verification or refinement reasoning call. As long as the LLM has some chances to improve towards the correct solution, the transition matrix will guide the evolution towards stationary distribution. 3.3 STATIONARY DISTRIBUTION AND CONVERGENCE For an ergodic Markov chain (which holds if pIC > 0 and pCI > 0), the process will converge to unique stationary distribution π = [πC, πI ], which satisfies the equation πP = π. This distribution represents the long-term probability of the solution being in either state. Solving πP = π subject to the constraint πC + πI = 1, we get the stationary probabilities: πC = pIC pIC + pCI and πI = pCI pIC + pCI . (5) Robustness to Imperfect Verification and Refinement for Hard Problems Equation 5 tells us that as long as pIC > pCI , meaning the tendency of LLM to improve overweigh that to degrade, running self-evolving iterations sufficiently long will guide the convergence towards state where the majority of solutions are correct. This gives theoretical guarantee for the majority voting of parallel DSER processes. And we do not depend on the success of single verification or refinement steps. In practice, we find that even when pIC < pCI for some very hard problems beyond the LLMs existing capabilities, as long as pIC is not too small, the majority voting of multiple DSER processes could still be correct because all correct solutions arrive at the same ground truth while different incorrect solutions diverge in different ways with inconsistent answers. Convergence Speed The speed of convergence to this stationary distribution is determined by the second-largest eigenvalue in magnitude of the transition matrix , given by λ2 = 1 pCI pIC. In an ideal scenario where pCI 0 and pIC 1, indicating the LLM consistently corrects errors without degrading correct solutions, the stationary distribution converges to πC 1. This scenario, typical for easy problems, yields extremely fast convergence as λ2 0. For more challenging problems, the improvement probability pIC is often small. However, if the LLM can maintain correct solution with high probability (i.e., pCI is also small), then λ2 = 1 pIC pCI < 1, still guaranteeing exponential convergence at rate of λ2n over iterations. 5 Preprint. Reinforcement Learning for Self-Evolving Reasoning Our approach also delivers unique insights informing future reinforcement learning designs for self-evolving reasoning. For instance, in addition to purely optimizing self-verification or self-correction capabilities (Bensal et al., 2025; Kumar et al., 2024), we could develop new optimization objectives to improve pIC and decrease pCI explicitly. Moreover, we could integrate the idea of deep self-evolving into the exploration stage to identify more possible solutions for hard tasks."
        },
        {
            "title": "3.4 COMPARISON WITH VERIFICATION-DEPENDENT SELF-EVOLVING",
            "content": "Our probabilistic perspective also allows us to reinterpret the framework of Huang & Yang as selfevolving process. The upper part of Figure 3 illustrates the core operations in their self-evolving cycle. The key distinction lies in the cycles dependence on self-verification outcomes (Pass: 1, Fail: 0). The process reaches an accepting condition after five consecutive self-verified passes, deeming the current solution correct. Conversely, it triggers rejecting condition after ten consecutive verification failures, concluding that the problem is unsolvable by the framework. Given its heavy reliance on verification feedback, we term this verification-dependent self-evolving process. We analyze the underlying Markov chain of this process, depicted in the lower part of Figure 3. The verification-dependent design necessitates numerous states to track the count of consecutive rejections. The chain reaches absorbing states when either condition is met: the rejecting condition after ten consecutive failures, or the accepting condition after five consecutive passes. Any single verification pass resets the rejection counter. Crucially, these verification-induced absorbing states can hinder deep self-evolution for open-weight models on hard problems. The rejecting condition prematurely terminates exploration when the model is perplexed, while the accepting condition risks cementing false-positive solution. In practice, we find the former limitation more constraining. Furthermore, the rejecting condition renders the Markov chain analytically intractable. Even without the rejecting condition, the framework remains verification-dependent due to the accepting condition. This simplified Markov chain, with only four states, becomes amenable to theoretical analysis. Our analysis (detailed in Appendix A.1) confirms that achieving favorable stationary distribution requires non-trivial assumptions about self-verification accuracyassumptions that often fail for hard problems beyond the models current capability. In contrast, our DSER framework marginalizes over the verification outcome, relying solely on the relative strength of improvement versus degradation tendencies. This fundamental difference suggests that deep self-evolving, by circumventing the need for precise verification, offers more viable path for open-weight models to narrow the performance gap with leading proprietary systems."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "We apply our DSER approach to DeepSeek-R1-0528-Qwen3-8B (abbreviated as DS-8B), powerful 8B-parameter reasoning LLM distilled from 600B teacher model. We follow its standard inference setup1, allowing up to 64K response tokens per reasoning call. We use AIME 2024 and 2025, totaling 60 mathematical competition problems, as our evaluation benchmarks. Despite its strong baseline performance, DS-8B failed to solve 9 of these problems. We classify these 9 problems as unsolvable by the base reasoning model, as it could not produce correct solution even with majority voting over 128 parallel trials. Additionally, we apply DSER to the entire AIME 2024-2025 problem set to demonstrate its overall performance improvement. We run independent DSER trials for each problem and report two metrics: Average Accuracy (Avg@K): The average accuracy across the trials. This estimates the Pass@1 success probability of single reasoning process. Consistency Accuracy (Cons@K): The accuracy of the single solution derived from majority vote (consistency prediction) over the trial outputs. This estimates the majorityvoting performance over parallel reasoning processes. 1https://huggingface.co/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B 6 Preprint. Figure 4: Overall performance of DS-8B with DSER over iterations on the full AIME 2024 and 2025 benchmarks. We specifically flag the Avg@16 metric reported for DeepSeek-R1-0528, which is the 600B distillation teacher model for DS-8B. We employ concise prompts designed to elicit the models inherent verification and refinement capabilities. In addition to the vanilla problem-solving prompt, our self-evolving stage utilizes the following specialized prompts. Verification Prompt: Verify the given solution step by step to check correctness. Provide short verification report, containing the key points of the solution and any errors found. Finally, put your judgement strictly in the format: boxed{1} if correct, or boxed{0} if incorrect. Refinement Prompt: Given your previous solution and verification report, reconsider the problem carefully and provide corrected solution. Output your final answer strictly in the format: boxed{}. Extended Reasoning Limit for DS-8B As Figure 1 shows, our DSER approach unlocks latent reasoning capabilities in DS-8B, enabling it to solve hard problems that are intractable with its baseline single-turn reasoning paradigm. Simultaneously, we observe that convergence to the stationary distribution can be slow, as indicated by the steady improvement of Avg@64 even after 80 iterations. However, the majority-voting performance (Cons@K) increases rapidly within the first ten iterations for most problems that DSER ultimately solves. These observations align with our Markov chain perspective, where iterative verification and refinement are modeled as stochastic process. Thus, the convergence of solution correctness for specific problem depends on the models probabilities of improving versus degrading its solution. The slow convergence indicates that these problems are exceptionally difficult for DS-8B, implying small corresponding improvement probability pIC. Overall Improved Performance In addition to solving previously unsolvable problems, Figure 4 shows that DSER stably improves the overall performance of DS-8B across the entire AIME benchmark. While the breakthrough in majority-vote accuracy (Cons@64) is primarily driven by solving these hard problems, DSER also boosts the overall Pass@1 performance (Avg@64) for all questions: improving from 82.8% to 89.3% on AIME 2024 (+6.5%), and from 74.4% to 83.4% on AIME 2025 (+9.0%). These results demonstrate that our DSER approach effectively translates the test-time scaling of DS-8B into improved reasoning capacity. Notably, small gap remains between the converged Pass@1 performance of DS-8B and its 600B teacher model. This indicates that the stationary distribution of the 8B models self-evolution is still weaker than the single-turn reasoning capacity of DeepSeek-R1-0528. Per-Question Convergence Analysis Figure 5 details the per-question performance improvements for five hard problems ultimately solved by DSER. We observe very different convergence behaviors and stationary distributions. For instance, on the top two questions (AIME 2024), DSER leads to quick convergence, and the stationary distribution stabilizes at high level of solution cor7 Preprint. Figure 5: Per-question performance improvements on hard problems over self-evolving iterations, highlighting the diverse convergence speeds and stationary distributions of solution correctness. Figure 6: Per-question performance improvements (left) and exit ratios (right) over self-evolving iterations for the verification-dependent self-evolving approach (Huang & Yang, 2025). rectness. In contrast, for the middle two questions (AIME 2025), convergence is also fast, but the stationary distribution retains significant portion of incorrect solutions. For the bottom question (AIME 2025), convergence is very slow, yet DSER eventually achieves the correct majority-voting solution. These results demonstrate that our approach can successfully leverage different levels of Preprint. self-improvement capabilities. Simultaneously, the suboptimal stationary distributions (e.g., the bottom three AIME 2025 questions) highlight the limitations of DS-8B in robustly maintaining correct solutions for certain hard problems. Comparison with Verification-Dependent Self-Evolving We applied the verificationdependent self-evolving approach (Section 3.4) to DS-8B on the same 9 hard AIME questions, but it only solved 2 of them. Figure 6 (a side-by-side comparison with Figure 5) shows the corresponding performance on the 5 problems that DSER solved. The empirical observations align well with our theoretical analysis of this approachs Markov chain. For problems beyond the models baseline capacity, its self-verification and self-refinement capabilities are unreliable. This leads to premature rejection exits (rows 2 and 5) or false-positive acceptance exits (row 4). These results imply that our DSER approach is more stable and effective method for unlocking the deep reasoning potential of models on tasks beyond their current capacity. It also points to distinct possible path for bridging the gap between open-weight reasoning models and leading proprietary models."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We introduced DSER, probabilistic framework that substantially extends the reasoning boundaries of open-weight models, even when their inherent verification and refinement capabilities are weak. Our core innovation lies in reframing iterative reasoning as convergent Markov chain, where the long-term guarantee of correctness depends not on flawless step-by-step execution but on marginal statistical bias towards improvement. This principle allows DSER to unlock the latent potential within smaller models through parallel, long-horizon reasoning trajectories. Empirically, we demonstrated that DSER enables DS-8B to solve AIME problems that were previously beyond its reach, even rivaling its much larger teacher model. This success demonstrates promising trade-off between model scale and test-time computation, making powerful reasoning more accessible. Looking forward, this work opens up several exciting research avenues. First, the limitations in self-verification and refinement exposed by our analysis highlight critical need for new learning objectives. Future training paradigms could explicitly incentivize robust self-critique and constructive self-correction, moving beyond solely optimizing for final-answer accuracy. Second, the DSER framework itself can be refined; integrating more sophisticated search algorithms or learnable verification modules could enhance its efficiency and success rate. Finally, applying DSER to the exploration phase of reinforcement learning, such as in GRPO, could help discover high-quality reasoning traces for the most challenging problems. Ultimately, DSER establishes that the path to superior reasoning may lie not only in building larger models but also in designing smarter inference-time processes that guide models to deeply evolve their own thoughts. We believe this paradigm shift towards harnessing test-time computation will be key driver in the next generation of reasoning systems."
        },
        {
            "title": "REFERENCES",
            "content": "Shelly Bensal, Umar Jamil, Christopher Bryant, Melisa Russak, Kiran Kamble, Dmytro Mozolevskyi, Muayad Ali, and Waseem AlShikh. Reflect, retry, reward: Self-improving llms via reinforcement learning. arXiv preprint arXiv:2505.24726, 2025. Luoxin Chen, Jinming Gu, Liankai Huang, Wenhao Huang, Zhicheng Jiang, Allan Jie, Xiaoran Jin, Xing Jin, Chenggang Li, Kaijing Ma, et al. Seed-Prover: Deep and broad reasoning for automated theorem proving. arXiv preprint arXiv:2507.23726, 2025. Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, et al. Agentic reinforced policy optimization. arXiv preprint arXiv:2507.19849, 2025. Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, and Wanjun Zhong. ReTool: Reinforcement learning for strategic tool use in LLMs. arXiv preprint arXiv:2504.11536, 2025. 9 Preprint. Team Gemini. standard Advanced version of Gemini with Deep Think officially achieves goldInternational Mathematical Olympiad deepmind.google. medal https://deepmind.google/discover/blog/advanced-version-of-gemini-with-deep-think-officiallyachieves-gold-medal-standard-at-the-international-mathematical-olympiad/, 2025. 15-10-2025]. [Accessed the at Team GLM-4.5, Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, et al. GLM-4.5: Agentic, reasoning, and coding (ARC) foundation models. arXiv preprint arXiv:2508.06471, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, et al. Deepseek-R1 incentivizes reasoning in LLMs through reinforcement learning. Nature, 2025. Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, et al. Skywork open reasoner 1 technical report. arXiv preprint arXiv:2505.22312, 2025. Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-Reasoner-Zero: An open source approach to scaling up reinforcement learning on the base model. arXiv preprint arXiv:2503.24290, 2025. Chengsong Huang, Wenhao Yu, Xiaoyang Wang, Hongming Zhang, Zongxia Li, Ruosen Li, Jiaxin Huang, Haitao Mi, and Dong Yu. R-zero: Self-evolving reasoning LLM from zero data. arXiv preprint arXiv:2508.05004, 2025. Yichen Huang and Lin Yang. Winning gold at IMO 2025 with model-agnostic verification-andrefinement pipeline. arXiv preprint arXiv:2507.15855, 2025. Ryo Kamoi, Yusen Zhang, Nan Zhang, Jiawei Han, and Rui Zhang. When can LLMs actually correct their own mistakes? critical survey of self-correction of LLMs. TACL, 2024. Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks. In NeurIPS, 2023. Team Kimi, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, et al. Kimi K2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025. Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, et al. Training language models to self-correct via reinforcement learning. arXiv preprint arXiv:2409.12917, 2024. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding R1-Zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-Refine: Iterative refinement with self-feedback. In NeurIPS, 2023. Team OpenAI. Learning to reason with LLMs. https://openai.com/index/learning-to-reason-withllms/, 2024. [Released 12-09-2024]. Team OpenAI. https://x.com/alexwei_/status/1946477742855532918, 2025a. [Accessed 15-10-2025]. Team OpenAI. GPT-5 is here. https://openai.com/gpt-5/, 2025b. [Accessed 15-10-2025]. Ning Shang, Yifei Liu, Yi Zhu, Li Lyna Zhang, Weijiang Xu, Xinyu Guan, Buze Zhang, Bingcheng rStar2-Agent: Agentic reasoning technical report. Dong, Xudong Zhou, Bowen Zhang, et al. arXiv preprint arXiv:2508.20722, 2025. Preprint. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. DeepSeekMath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Zhengwei Tao, Ting-En Lin, Xiancai Chen, Hangyu Li, Yuchuan Wu, Yongbin Li, Zhi Jin, Fei Huang, Dacheng Tao, and Jingren Zhou. survey on self-evolution of large language models. arXiv preprint arXiv:2404.14387, 2024. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022. Xumeng Wen, Zihan Liu, Shun Zheng, Shengyu Ye, Zhirong Wu, Yang Wang, Zhijian Xu, Xiao Liang, Junjie Li, Ziming Miao, et al. Reinforcement learning with verifiable rewards implicitly incentivizes correct reasoning in base LLMs. arXiv preprint arXiv:2506.14245, 2025. Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, and Jun Zhao. Large language models are better reasoners with self-verification. arXiv preprint arXiv:2212.09561, 2022. Team AI. Grok 4. https://x.ai/news/grok-4, 2025. [Accessed 15-10-2025]. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, arXiv preprint Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv:2505.09388, 2025. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. DAPO: An open-source LLM reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Siyu Yuan, Zehui Chen, Zhiheng Xi, Junjie Ye, Zhengyin Du, and Jiecao Chen. Agent-R: Training language model agents to reflect via iterative self-training. arXiv preprint arXiv:2501.11425, 2025."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 THEORETICAL ANALYSIS FOR VERIFICATION-DEPENDENT SELF-EVOLVING In Section 3.4, we established the self-evolving nature of Huang & Yangs framework and analyzed how its absorbing states in the Markov transition graph can prevent deep self-evolution. We now provide theoretical analysis of simplified version that removes the rejecting condition, demonstrating that even this variant remains critically dependent on reliable verification capabilities to enable effective self-evolution. Figure 7 shows this simplified Markov transition graph. Figure 7: simplified Markov transition graph for the self-evolving process of (Huang & Yang, 2025), where we remove the rejecting condition of ten consecutive self-verified fails. 11 Preprint. Markov Transition Model without the Rejecting Condition Let = 0 and = 1 denote self-verified failure and pass, respectively. Extending the notations defined in Section 3, we define the following key conditional probabilities: In Self-Verification : α = p(v = 1 (n) = I), β = p(v = 1 (n) = C), In Self-Refinement : v=1 = p(X (n+1) = (n) = I, = 1), = p(X (n+1) = (n) = C, = 1), v=1 v=0 = p(X (n+1) = (n) = I, = 0), = p(X (n+1) = (n) = C, = 0). v=0 We define four-state system to model the process: State 1 (S1): Correct solution, process ongoing State 2 (S2): Incorrect solution, process ongoing State 3 (S3): Correct solution, process terminated (absorbing) State 4 (S4): Incorrect solution, process terminated (absorbing) S3 and S4 are absorbing-once entered, they transition only to themselves. S3 is reached exclusively from S1 after five consecutive verification passes (v = 1), while S4 is reached analogously from S2. Transitions between the non-terminated states (S1 and S2) are governed by the refinement probabilities at each iteration. The complete transition probabilities between states are defined as follows: = S1 S2 S3 S1 (1 β5)Y v=0 (1 α5)Y v=0 0 0 S2 (1 β5)(1 v=0 (1 α5)(1 v=0 0 0 ) β5 0 ) 1 0 S3 S4 0 α5 0 . Stationary Distribution By partitioning the states into absorbing and transient sets, the transition matrix can be written in the following canonical form: = (cid:19) (cid:18)Q Then we have , where = (cid:18)(1 β5)Y v=0 (1 α5)Y v=0 (1 β5)(1 v=0 ) (1 α5)(1 v=0 ) (cid:19) , = (cid:18)β5 0 (cid:19) 0 α5. = lim P = lim (cid:32) k=0 Qk(cid:17) Qn (cid:16)(cid:80)n1 0 (cid:33) = (cid:18)0 (I Q)1R (cid:19) 0 (I Q)1R = 1 det(I Q) det(I Q) = α5 (1 β5)α5Y v= (cid:18)(1 (1 α5)(1 v=0 (1 α5)β5Y v=0 + (1 α5)β5Y v=0 ))β5 (1 β5)α5(1 v=0 ) )α5 (1 (1 β5)Y v= (cid:19) , The probability of stabilizing in the correct solution is (1 α5)β5Y v=0 α5 (1 β5)α5Y v= + (1 α5)β5Y v=0 . Over-Confident Verification Leading to Incorrect Solutions Dominated We can prove that under the condition α5 v=0 , which means the problem is difficult and the LLM is over-confident about its solution (a high α). In the meanwhile, since the problem is hard, the LLMs capability Preprint. of making improvements on its solution is limited (a relatively small v=0 reaching the correct solution will not pass 0.5. Given α5 v=0 , we have ). The probability of 1 α5 1 α5 1 α5 1 α5 +"
        },
        {
            "title": "1\nY v=0\nI\n1 − Y v=0\nC\nY v=0\nI\n1 − Y v=0\nC\nY v=0\nI\n1 − Y v=0\nC\nY v=0\nI",
            "content": "1 β5 1 β5 v=0 v=0 + + v=0 v=0 v=0 v=0 +"
        },
        {
            "title": "We can calculate that",
            "content": "(1 α5)β5Y v=0 α5 (1 β5)α5Y v=0 1 2 + (1 α5)β5Y v= C + (1 α5)β5Y v=0 α5 (1 β5)α5Y v=0 α5 (1 β5)α5Y v=0 α5 + (1 β5)α5Y v=0 + (1 β5)Y v=0 1 1 (1 β5)Y v= I 2(1 α5)β5Y v=0 (1 α5)β5Y v=0 (1 α5)β5Y v=0 (1 α5) α5 (1 α5) α5 (1 α5) α β5Y v=0 β5Y v=0 1 (1 β5)Y v=0 β5Y v=0 1 v=0 1 v=0 β5 1 v=0 v=0 v=0 v=0 + + v=0 v=0 + 1 α5 1 1 1 α5 β5 Proof completed. A.2 CASE STUDIES IN DEEP SELF-EVOLVING To illustrate the inner workings of DSER, we manually trace the critical verification and refinement steps across the 80-iteration process that guided DS-8B to correct solution for previously unsolvable AIME 2025 problem. Below we highlight some crucial verification and refinement actions emerged in DSER. 13 Preprint. Problem Let ABCDE be convex pentagon with AB = 14, BC = 7, CD = 24, DE = 13, EA = 26, and = = 60. For each point in the plane, define (X) = AX + BX + CX + DX + EX. The least possible value of (X) can be expressed as + p, where and are positive integers and is not divisible by the square of any prime. Find + + Solution <think>...<think> The convex pentagon ABCDE has side lengths AB = 14, BC = 7, CD = 24, DE = 13, EA = 26, and angles = = 60. The function (X) = AX + BX + CX + DX + EX is minimized at point with coordinates The distances from to the vertices are: 3 7 , 44 (cid:16) (cid:17) . 3 7 AX = 5 BX = 19 CX = 8 DX = 8 EX = 3 3 3 + 19 + The sum is (X) = 5 This sum is expressed as m+n 38 + 21 + 3 = 62. The minimum value is achieved, and the sum is consistent with the given conditions. 62 where = 38, = 21, and = 3. Thus, m+n+p = 3 + 19 = 38 + 21 3 + 8 3. 3 (cid:17) (cid:16) 109 7 , 44 Verify <think>...<think> The solution claims that the minimum value of (X) = AX + BX + CX + DX + EX 3. However, occurs at the point = verification shows that the sum of the unit vectors from the points to is not zero, which is necessary condition for the minimum of the sum of Euclidean distances in the plane for distinct points not collinear. Additionally, calculating (X) at another point, such as (10, 10), gives smaller value (f (10, 10) 70.07) compared to (X) 74.37, confirming that is not the minimum. The pentagon is convex and satisfies the given side lengths and angles, but the minimum sum of distances is not achieved at X. Therefore, the solution is incorrect. , with the sum (X) = 38 + 21 7 Refinement <think>...<think> The convex pentagon ABCDE has side lengths AB = 14, BC = 7, CD = 24, DE = 13, EA = 26, and angles = = 60. The point that minimizes the sum of distances (X) = AX + BX + CX + DX + EX is found to be the Fermat-Torricelli point of the triangle formed by vertices A, C, and D, which minimizes the sum AX + CX + DX. This sum is 19 3, achieved when BX + EX = 38, but this is not possible in this pentagon configuration. However, the lower bound is 38 + 19 The expression 38 + 19 Thus, + + = 38 + 19 + 3 = 60. 60 3. p, where = 38, = 19, and = 3. 3, and the minimum value of (X) is 38 + 3, and the minimum value is 38 + 19 3 is in the form + 14 Preprint. Problem Let ABCDE be convex pentagon with AB = 14, BC = 7, CD = 24, DE = 13, EA = 26, and = = 60. For each point in the plane, define (X) = AX + BX + CX + DX + EX. The least possible value of (X) can be expressed as + p, where and are positive integers and is not divisible by the square of any prime. Find + + 3/7), and E(218/7, 88 3), B(0, 0), C(7, 0), D(205/7, 36 Solution <think>...<think> The convex pentagon ABCDE with side lengths AB = 14, BC = 7, CD = 24, DE = 13, EA = 26, and angles = = 60 is correctly constructed with vertices at A(7, 7 3/7). The pentagon is convex, as all cross products during traversal are negative, indicating clockwise turns. The function (X) = AX + BX + CX + DX + EX is minimized on the line segment BE, where BX + EX = 38 for all on BE, and BX + EX > 38 for not on BE. Minimizing (X) on BE is equivalent to minimizing AX + CX + DX + 38. The minimum of AX + CX + DX on BE occurs at the midpoint (109/7, 44 3/7), where AX + CX + DX = 21 3 is the minimum, as (X) is larger at other points on BE and at The value 38 + 21 the vertices, and the minimum occurs on BE due to the minimization of BX + EX. The expression 38 + 21 with = 38, = 21, and = 3 (square-free). Therefore, + + = 38 + 21 + 3 = 62. 62 3. Thus, (M ) = 38 + 21 3 is in the form + 3. Verify <think>...<think> The solution provided in the query claims that the minimum value of (X) = AX + BX + CX + DX + EX is 38 + 21 3, achieved at the midpoint of BE. However, numerical calculations show that the minimum occurs on the line segment BE but at different point, approximately at parameter = 0.33, with value of approximately 70.913. At M, f(X) = 74.372, which is larger than the minimum found. The minimum value 70.913 cannot be expressed as + with integer m, and square-free p, but the problem requires this form. Since the solutions minimum value and the point are incorrect, the answer is 0. 0 3/7), and E(218/7, 88 convex pentagon ABCDE has vertices A(7, 7 Refinement <think>...<think> 3), B(0, 0), C(7, 0), The D(205/7, 36 3/7), with side lengths AB = 14, BC = 7, CD = 24, DE = 13, EA = 26, and angles = = 60. The function (X) = AX + BX + CX + DX + EX is minimized on the line segment BE, where BX + EX = 38 for all on BE, and BX + EX > 38 for not on BE. Minimizing (X) is equivalent to minimizing g(X) = AX + CX + DX + 38 for on BE. The minimum value of (X) is 38 + 19 as + 38 + 19 + 3 = 60. 60 3, achieved at point on BE. This is expressed with = 38, = 19, and = 3 (square-free). Thus, + + = 15 Preprint. A.3 ADDITIONAL EXPERIMENTAL RESULTS Table 1 summarized detailed information of 9 hard problems on AIME 2024-2025 as well as case-by-case comparison between DS-8B and its enhancement with our DSER approach. 16 Preprint. Table 1: Performance of DS-8B and its enhancement with our DSER framework on challenging AIME 24/25 problems. Avg@128 reports the average accuracy of the base model over 128 independent runs. The DSER process is executed independently for 64 runs, and for each run, the results from the final ten self-evolve iterations are aggregated (yielding total of 640 solutions in total). Avg@640 (DSER) thus reflects the average accuracy across all these solutions, while Cons@640 (DSER) indicates whether the majority vote among them yields correct answer. Source Question Avg@128 Avg@640 (DSER) Cons@640 (DSER) 0.1812 0.8016 0.0625 0.6531 0.0063 0. 0.0000 0.0000 0.0750 0.4016 1 0 0 1 0.0750 0.2000 0.0375 0.0625 0.0063 0.0000 0.0000 0. 0 0 1 AIME 24 AIME 24 AIME Let ABCDEF be convex equilateral hexagon in which all pairs of opposite sides are parallel. The triangle whose sides are extensions of segments AB, CD, and EF has side lengths 200, 240, and 300. Find the side length of the hexagon. (url) Eight circles of radius 34 are sequentially tangent, and two of the circles are tangent to AB and BC of triangle ABC, respectively. 2024 circles of radius 1 can be arranged in the same manner. The inradius of triangle ABC can be expressed as , where and are relatively prime positive integers. Find + n. (url) Find the number of rectangles that can be formed inside fixed regular dodecagon (12-gon) where each side of the rectangle lies on either side or diagonal of the dodecagon. The diagram below shows three of those rectangles. (url) AIME 24 Define (x) = 1 2 and g(x) = 1 4 . Find the number of intersections of the graphs of = 4g(f (sin(2πx))) and = 4g(f (cos(3πy))) (url) AIME 25 Let the sequence of rationals x1, x2, . . . be defined (cid:17) xk + 1 11 and xk+1 = 1 such that x1 = 25 . xk x2025 can be expressed as for relatively prime positive integers and n. Find the remainder when + is divided by 1000. (url) 1 (cid:16) 3 AIME 25 AIME 25 Alex divides disk into four quadrants with two perpendicular diameters intersecting at the center of the disk. He draws 25 more line segments through the disk, drawing each segment by selecting two points at random on the perimeter of the disk in different quadrants and connecting those two points. Find the expected number of regions into which these 27 line segments divide the disk. (url) There are exactly three positive real numbers such that the function (x) = (x18)(x72)(x98)(xk) defined over the positive real numbers achieves its minimum value at exactly two positive real numbers x. Find the sum of these three values of k. (url) Let denote the number of ordered triples of positive integers (a, b, c) such that a, b, 36 and a3 + b3 + c3 is multiple of 37. Find the remainder when is divided by 1000. (url) Let ABCDE be convex pentagon with AB = 14, BC = 7, CD = 24, DE = 13, EA = 26, and = = 60. For each point in the plane, define (X) = AX + BX + CX + DX + EX. The least possible value of (X) can be expressed as + p, where and are positive integers and is not divisible by the square of any prime. Find + + p. (url) AIME AIME"
        }
    ],
    "affiliations": [
        "Microsoft Research Asia",
        "Peking University"
    ]
}