{
    "paper_title": "Diversity-Preserved Distribution Matching Distillation for Fast Visual Synthesis",
    "authors": [
        "Tianhe Wu",
        "Ruibin Li",
        "Lei Zhang",
        "Kede Ma"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Distribution matching distillation (DMD) aligns a multi-step generator with its few-step counterpart to enable high-quality generation under low inference cost. However, DMD tends to suffer from mode collapse, as its reverse-KL formulation inherently encourages mode-seeking behavior, for which existing remedies typically rely on perceptual or adversarial regularization, thereby incurring substantial computational overhead and training instability. In this work, we propose a role-separated distillation framework that explicitly disentangles the roles of distilled steps: the first step is dedicated to preserving sample diversity via a target-prediction (e.g., v-prediction) objective, while subsequent steps focus on quality refinement under the standard DMD loss, with gradients from the DMD objective blocked at the first step. We term this approach Diversity-Preserved DMD (DP-DMD), which, despite its simplicity -- no perceptual backbone, no discriminator, no auxiliary networks, and no additional ground-truth images -- preserves sample diversity while maintaining visual quality on par with state-of-the-art methods in extensive text-to-image experiments."
        },
        {
            "title": "Start",
            "content": "Diversity-Preserved Distribution Matching Distillation for Fast Visual Synthesis Tianhe Wu * 1 3 Ruibin Li * 2 Lei Zhang 2 3 Kede Ma 1 https://github.com/Multimedia-Analytics-Laboratory/dpdmd 6 2 0 2 3 ] . [ 1 9 3 1 3 0 . 2 0 6 2 : r (a) SD3.5-M (60 NFEs) (b) DMD (4 NFEs) (c) DP-DMD (Ours, 4 NFEs) Figure 1. DP-DMD preserves image diversity while maintaining competitive visual quality. All results are generated under identical text conditioning (A smiling woman with red hair, green eyes, and dimples.) with different random initial noise. (a) SD3.5-M (Esser et al., 2024) sampled with 30 steps (60 NFEs) serves as the teacher model (upper bound). (b) DMD (Yin et al., 2024a) and (c) DP-DMD are step-distilled student models, both evaluated using only 4 NFEs."
        },
        {
            "title": "Abstract",
            "content": "Distribution matching distillation (DMD) aligns multi-step generator with its few-step counterpart to enable high-quality generation under low inference cost. However, DMD tends to suffer from mode collapse, as its reverse-KL formulation inherently encourages mode-seeking behavior, for which existing remedies typically rely on perceptual or adversarial regularization, thereby incurring substantial computational overhead and training instability. In this work, we propose role-separated distillation framework that explicitly disentangles the roles of distilled steps: the first step is dedicated to preserving sample diversity via target-prediction (e.g., v-prediction) objective, while subsequent steps focus on quality re- *Equal contribution 1City University of Hong Kong 2The Hong Kong Polytechnic University 3OPPO Research Institute. Correspondence to: Lei Zhang <cslzhang@comp.polyu.edu.hk>, Kede Ma <kede.ma@cityu.edu.hk>. Preprint. Work in progress. 1 finement under the standard DMD loss, with gradients from the DMD objective blocked at the first step. We term this approach Diversity-Preserved DMD (DP-DMD), which, despite its simplicity no perceptual backbone, no discriminator, no auxiliary networks, and no additional ground-truth imagespreserves sample diversity while maintaining visual quality on par with state-of-the-art methods in extensive text-to-image experiments. 1. Introduction Generative modeling seeks to synthesize high-fidelity image data by learning complex data distributions through stochastic or continuous-time processes. Among the most influential approaches, diffusion models (Song et al., 2020) and their flow-based counterparts (Lipman et al., 2022) have driven remarkable progress in image and video generation (Esser et al., 2024; Wan et al., 2025), benefiting from recent advances in large-scale foundation models (Rombach et al., 2022; Labs et al., 2025) and modern optimization techniques (Sutton et al., 1999). Despite their success, these Diversity-Preserved DMD methods require numerically integrating multi-step differential equations to realize generative dynamics, resulting in long inference-time and high computational cost due to the large number of function evaluations (NFEs). To mitigate inference latency, prior distillation methods mainly follow trajectory-based direction (Liu et al., 2023b; Yan et al., 2024), approximating the teachers sampling trajectory using fewer inference steps. More recently, this paradigm has shifted from trajectory approximation toward directly aligning the output distributions of teacher and student models by minimizing statistical divergences (Sauer et al., 2024b; Yin et al., 2024b;a; Liu et al., 2025). Among these approaches, distribution matching distillation (DMD) (Yin et al., 2024b) stands out as representative framework, achieving both fast inference and high-quality generation. Nevertheless, as illustrated in Figure 1, optimizing the student model with the DMD loss leads to substantial reduction in sample diversity compared to its multi-step counterpart. This arises from the reverse-KL formulation of DMD, which inherently encourages modeseeking and consequently causes mode collapse. To alleviate this issue, current mainstream methods (Yin et al., 2024b;a; Chadebec et al., 2025; Lu et al., 2025) augment the DMD objective with perceptual and adversarial losses, leveraging additional samples synthesized by the teacher as an implicit regularizer to promote more comprehensive coverage of the teacher distribution by the student model. However, the incorporation of such modules introduces non-negligible computational overhead1, as well as instability in adversarial training. Motivated by the observations (see Figure in the Appendix) from multi-step experiential samplingwhere early denoising steps primarily determine the global structural layout and later steps progressively refine fine-grained visual detailswe propose role-separated distillation framework termed Diversity-Preserved DMD (DP-DMD). Specifically, we assign distinct roles to different distilled steps. The first distilled step is supervised using target-prediction objective (e.g., v-prediction), which encourages the student model to preserve sample diversity during generation. For the remaining distilled steps, we adopt the standard DMD objective, allowing these steps to focus on refining visual fidelity and overall image quality. To maintain this role separation, we stop gradients from the DMD loss at the first step, preventing the reverse-KL objective from overriding the diversity-preserving supervision. Our proposed DP-DMD is deliberately simple: no perceptual backbone, no discriminator, no auxiliary networks, and 1Widely adopted perceptual losses, such as LPIPS (Zhang et al., 2018) and DISTS (Ding et al., 2020), incur significant GPU memory consumption and computational cost, especially when applied to high-resolution imagery. no additional ground-truth images. Everything happens in latent space, keeping the pipeline compact, stable, and memory-efficient. Yet this minimalism does not come at the expense of performance: across extensive text-to-image (T2I) experiments, DP-DMD preserves sample diversity while maintaining visual quality on par with state-of-the-art methods under few-step sampling. We believe this work takes step forward in diffusion distillation and offers insights to the broader research community. 2. Related Work This section surveys recent few-step diffusion distillation methods and highlights key optimization challenges, including performance degradation and reduced sample diversity. Trajectory-based Methods. Trajectory-based distillation methods aim to train few-step student model by explicitly aligning its denoising trajectory with that of multi-step teacher model (Song et al., 2023; Liu et al., 2023c; Luo et al., 2023; Meng et al., 2023; Lu & Song, 2024; Li et al., 2025). By selecting anchor points along the teachers trajectory, Consistency Models (Song et al., 2023; Song & Dhariwal, 2023) and subsequent extensions (Wang et al., 2024; Lu & Song, 2024) significantly shorten the denoising trajectory, enabling high-quality generation with only few inference steps instead of the conventional tens of iterations. More recently, MeanFlow (Geng et al., 2025) proposes matching the average velocity of the diffusion trajectory, enabling further acceleration with only handful of inference steps. Despite their empirical success, trajectory-based methods often exhibit significant performance degradation when applied to large-scale pretrained image synthesis or video generation models (Zheng et al., 2025). Distribution-Matching Methods. Distribution-matching methods aim to distill multi-step diffusion models into efficient few-step generators by aligning the student models output distribution with that of pretrained diffusion model. One line of work adopts GAN-based formulations, introducing auxiliary discriminators or reusing the diffusion model as feature extractor to construct adversarial objectives (Sauer et al., 2024b;a; Lin et al., 2024; Zhou et al., 2024a). Although these methods can produce sharp samples, the instability of adversarial training often hampers scalability and robustness in large-scale diffusion distillation. Another line of work is inspired by distribution matching in 3D generation (Wang et al., 2023a; Poole et al., 2022), where optimization minimizes the discrepancy between rendered images and diffusion model outputs. DMD (Yin et al., 2024b) adapts this paradigm to diffusion distillation and enables effective few-step generation, with subsequent extensions further improving performance by incorporating auxiliary losses (Zheng et al., 2025; Liu et al., 2025; Yin 2 Diversity-Preserved DMD et al., 2024a; Zhou et al., 2024b). Despite strong empirical results, DMD-based methods are prone to model collapse under prolonged training, leading to reduced sample diversity. Prior solutions mitigate this issue by introducing regression or GAN losses (Yin et al., 2024b;a), but at the cost of significant computational and memory overhead. In contrast, our DP-DMD maintains high-quality distillation and sample diversity without additional training modules, offering more efficient and practical solution. 3. Background: Flow Matching and DMD 3.1. Flow Matching Diffusion models can be equivalently formulated using an ordinary differential equation (ODE) framework (Chen et al., 2018; Song et al., 2020). Given data samples pdata(x) and prior noise samples ϵ pnoise(ϵ) (e.g., ϵ (0, I)), flow path can be constructed via linear interpolation between data and noise as zt = at + bt ϵ, where [0, 1] and at, bt are predefined noise schedules. widely used choice is the linear schedule (Liu et al., 2023b; Esser et al., 2024), defined by at = 1 and bt = t, which yields: zt = (1 t) + ϵ. (1) We see that zt pdata at = 0 and transitions to pnoise at = 1. The associated flow velocity is defined as the time derivative of zt, namely vt = ϵ. By differentiating Equation 1 w.r.t. t, we have vt = ϵ x. + t = Flow-based methods (Lipman et al., 2022; Geng et al., 2025) seek to learn neural velocity field vθ(zt, t) parameterized by θ, through minimization of the following objective: LFM = Et,x,ϵ (cid:104)(cid:13) (cid:13)vθ(zt, t) (ϵ x)(cid:13) (cid:13) 2(cid:105) . (2) At inference time, novel samples are generated by solving the ODE induced by the learned velocity field vθ, namely dt zt = vθ(zt, t), starting from noise sample z1 pnoise and integrating backward in time from = 1 to = 0, which yields the data sample xθ = z1 + (cid:82) 0 1 vθ(zt, t) dt. 3.2. DMD Loss DMD aims to align the sample distribution produced by few-step student with that generated by highquality multi-step teacher. It minimizes the divergence between the two distributions, defined as LDMD DKL(pfake(xθ) preal(xθ)), where pfake and preal denote the distributions induced by the student and the teacher, respectively. Although this KL divergence is intractable to evaluate explicitly in high-dimensional spaces (Poole et al., 2023; Wang et al., 2023a), its gradient with respect to the student parameters θ admits convenient form. Denoting by xθ sample generated by the student, the gradient of LDMD can be written as: θLDMD = Exθ (cid:104)(cid:0)sfake(zt) sreal(zt)(cid:1) θxθ (cid:105) , (3) where zt is obtained by diffusing the student sample xθ according to Equation 1, so that the studentand teacherinduced distributions exhibit overlapping support in the perturbed space (Yin et al., 2024b). The corresponding score functions, sfake = zt log pfake(zt) and sreal = zt log preal(zt), are defined as the gradients of the logdensities associated with the student and teacher distributions in this space, respectively. While the teacher score sreal is directly available from the pretrained multi-step diffusion model, the student score sfake is intractable and is therefore approximated using an auxiliary fake model2. Following (Yin et al., 2024a), the fake model is updated for steps using Equation 2 before updating each student, yielding an accurate estimate of sfake for the DMD objective. 4. DP-DMD for Diffusion Distillation This section analyzes the roles of early and late denoising steps and presents DP-DMD, role-separated distillation framework, with the training pipeline shown in Figure 2. 4.1. Roles of Early and Late Denoising Steps Recent studies on diffusion and flow-based generative models have consistently observed that the denoising process exhibits clear stage-wise behavior (Wang & Vastola, 2023; Liu et al., 2023a; He et al., 2025). Early-Step Diversity Preservation. As illustrated in Figure of the Appendix, early denoising steps primarily operate at high noise levels and are responsible for recovering the global structural layout of the sample, such as object presence, coarse geometry, and overall composition. Importantly, variations introduced at these early stages tend to persist throughout the subsequent denoising trajectory, making them key determinant of sample diversity. Late-Step Quality Refinement. In contrast, later denoising steps operate at lower noise levels and focus on refining fine-grained visual details, including textures, colors, and local appearance. These steps exert limited influence on global structure and thus contribute mainly to perceptual quality rather than diversity. This intrinsic asymmetry between early and late denoising steps suggests that treating all distilled steps uniformly, as done in standard DMD (Yin et al., 2024b;a), may be ineffective for preserving diversity. Instead, explicitly assigning different training objectives to different stages of the distilled process provides principled opportunity to balance 2The fake model is initialized in the same way as the teacher model, and both share an identical architecture. Diversity-Preserved DMD Figure 2. Training pipeline of DP-DMD. The first denoising step of the student is guided by Flow-Matching diversity loss using teacher-derived intermediate state, with gradients stopped thereafter. The remaining steps are optimized via the DMD objective, leveraging teacher and fake-model scores to refine sample quality. This role separation preserves diversity while maintaining high-fidelity generation in few-step distillation. diversity preservation and quality refinement, which directly motivates our role-separated DP-DMD framework. 4.2. Diversity-Preserved DMD Overview. Given distilled few-step student model with inference steps, DP-DMD assigns different training objectives to different steps. Specifically, the first distilled step is supervised using target-prediction objective (Equation 2) to encourage diversity preservation, while the remaining 1 steps are optimized using the standard DMD loss (Equation 3) to refine visual quality. This separation enables the student to retain diverse global structures without sacrificing the efficiency and fidelity advantages of DMD. Diversity Supervision. We begin by describing the construction of the supervision signal employed in the diversitypreserving stage. Given multi-step teacher model, we carry out inference for predefined number of steps K. Hyperparameter controls the noise level, with which diversity is anchored. We denote by ztk the intermediate sample produced by the teacher at the k-th inference step, which corresponds to time tk in the continuous flow formulation. Following the linear flow path defined in Equation 1, the velocity target at time tk is given by: vtarget = ϵ ztk 1 tk . (4) In the distilled model, the first step predicts velocity field vθ(ϵ, 1), which is trained using Flow Matching loss against the target defined in Equation 4: LDiv = Eϵ (cid:104)(cid:13) (cid:13)vθ(ϵ, 1) vtarget 2(cid:105) . (cid:13) (cid:13) (5) By explicitly aligning the students first-step prediction with teacher-derived intermediate state, this objective encourages the student to preserve the diversity encoded in early denoising stages, rather than collapsing to small subset of modes favored by reverse-KL optimization. The motivation for supervising only the first step is discussed in Section of the Appendix. Quality Supervision. After the first step, the student model is rolled out for the remaining 1 steps to produce final sample xθ. Crucially, the output of the first step is detached from the computational graph before proceeding, ensuring that the gradients from the DMD loss will not propagate back into the diversity-preserving step, enforcing clean separation of roles: the first step governs diversity, while later steps focus exclusively on quality refinement. The overall training objective of DP-DMD is given by: = LDMD + λ LDiv, (6) where λ balances diversity preservation and quality refinement objectives. Algorithm 1 presents the pseudo-code for single training iteration. This target corresponds to the ground-truth flow velocity used to define the first-step transport from noise toward the teacher-produced intermediate state ztk under the linear interpolation assumption. 5. Experiments This section first conducts ablation studies to analyze the behavior of DP-DMD under different design choices, and 4 Diversity-Preserved DMD Algorithm 1 DP-DMD Training for Flow-based Models # x: training data batch # k: diversity anchor step index # t_k: anchor diffusion timestep # N: number of student sampling steps # lambda_div: diversity loss weight eps = randn like(x) z_k = rollout teacher(eps, k) # diversity supervision v_target = (eps - z_k) / (1 - t_k) v1 = stu(eps, 1) loss_div = l2 loss(v1 - v_target) # detach after first step z1 = stopgrad(rollout student(eps, 1)) # quality supervision x_theta = rollout student(z1, - 1) loss_dmd = dmd loss(x_theta) loss = loss_dmd + lambda_div * loss_div then presents comprehensive comparisons and system-level evaluations to demonstrate its effectiveness in practice. 5.1. Experimental Setup Training. We adopt the flow-based SD3.5-Medium (Esser et al., 2024) and the diffusion-based SDXL (Podell et al., 2023) as pretrained T2I backbones. All distillation experiments are conducted at an image resolution of 1024 1024. For the teacher model, the classifier-free guidance (CFG) scale is fixed at 3.5 for SD3.5-M and 8.0 for SDXL throughout training. Both the student and fake models are optimized using AdamW (Loshchilov & Hutter, 2017) with learning rate of 1 105. The weighting coefficient λ is set to 5 102, the fake-model update interval is fixed to 5, and the diversity anchor step is also set to 5. Training is conducted on the DiffusionDB (Wang et al., 2023b) dataset using text prompts only, for total of 6103 iterations, with batch size of 4 per GPU across 8 NVIDIA A800 GPUs. Evaluation. For evaluation, we assess sample diversity using DINOv3-ViT-Large (DINO) (Simeoni et al., 2025) and CLIP-ViT-Large (CLIP) (Radford et al., 2021) by computing the cosine similarity between extracted image feature representations in pairwise manner: Diversity = 1 2 L(L 1) (cid:88) (cid:16) cos θ , x(j) x(i) θ (cid:17) , (7) i, where denotes the number of distinct initial noise samples per text prompt3, and we set = 9 in our experiments. In addition, we adopt VisualQuality-R1 (VQ-R1) (Wu et al., 2025) and MANIQA (MIQA) (Yang et al., 2022) as visual 3For each text prompt, every pair of generated images is com- (cid:1) total comparisons. pared once, yielding (cid:0)L 2 Table 1. Diversity anchor step K. We investigate how applying diversity supervision at different teacher denoising steps influences sample diversity, visual quality, and human preference. Base refers to using the DMD (Yin et al., 2024a) loss without additional regularization. The bestperforming results are highlighted in bold. Base 1 3 5 10 30 Diversity Preference Quality DINO CLIP VQ-R1 MIQA ImgR PicS 21.75 0.137 21.62 0.157 21.67 0.175 21.76 0.179 21.78 0.187 0.187 21.71 1.189 1.121 1.125 1.142 1.112 1.117 4.649 4.578 4.648 4.646 4.589 4. 0.133 0.155 0.178 0.182 0.185 0.181 1.016 0.995 1.016 1.017 1.004 1.007 Table 2. Weighting coefficient λ. λ controls the tradeoff between diversity preservation and quality refinement. = 3 in all experiments. λ Base 0.01 0.05 0.08 0.10 Diversity Preference Quality DINO CLIP VQ-R1 MIQA ImgR PicS 21.75 0.137 21.81 0.170 21.67 0.175 21.71 0.176 0.177 21.64 1.189 1.136 1.125 1.117 1.056 1.016 1.003 1.016 1.014 1.001 0.133 0.164 0.178 0.176 0.177 4.649 4.672 4.648 4.662 4. quality metrics. Human preference is further evaluated using ImageReward (ImgR) (Xu et al., 2023) and PickScore (PicS) (Kirstain et al., 2023). 5.2. Ablation Study We start by systematically analyzing the following properties of our methods, evaluated on the Pick-a-Pic (Kirstain et al., 2023) dataset. Diversity anchor step K. We investigate the effect of the diversity anchor step on sample diversity, visual quality, and human preference, with results summarized in Table 1. Several observations can be made. First, introducing diversity supervision at any anchor step consistently improves sample diversity compared to the baseline DMD (Yin et al., 2024b) without regularization. This demonstrates that the proposed diversity-preserving objective is effective across wide range of denoising stages, validating the general applicability of our design. Second, we observe clear trend that anchoring diversity supervision at later denoising steps (larger K) leads to progressively higher diversity scores. This phenomenon aligns well with our motivation in Section 4. Third, while larger values of yield stronger diversity gains, excessively late anchors may introduce mild trade-off with visual quality and preference metrics. This suggests that the choice of provides controllable knob 5 Diversity-Preserved DMD Table 3. Quantitative comparison of DMD variants on Pick-a-Pic (Kirstain et al., 2023) and COCO-10K 2014 (Lin et al., 2014). All methods use the same backbone and inference steps (4-NFE). DP-DMD achieves improved diversity while maintaining competitive visual quality and human preference, without introducing perceptual or adversarial modules. Top two results are highlighted in bold and underline, respectively. Method ImageFree NFE Diversity Pick-a-Pic Quality Preference Diversity COCO-10K 2014 Quality Preference DINO CLIP VQ-R1 MIQA ImgR PicS DINO CLIP VQ-R1 MIQA ImgR PicS - SD3.5-M-based (CFG=3.5) 60 Base Model 4 DMD 4 DMD-LPIPS 4 DMD-GAN DP-DMD 4 SDXL-based (CFG=8.0) Base Model DMD DMD-LPIPS DMD-GAN DP-DMD 100 4 4 4 4 - 0.240 0.137 0.169 0.183 0. 0.219 0.109 0.136 0.126 0.173 0.221 0.133 0.169 0.162 0.182 0.204 0.133 0.139 0.124 0.161 4.657 4.649 4.598 4.525 4.646 4.675 4.667 4.610 4.624 4.591 1.020 1.016 1.005 0.984 1. 1.033 0.971 0.976 1.019 0.954 1.007 21.80 1.189 21.75 1.063 21.62 1.033 21.63 1.142 21.76 1.016 21.96 0.951 21.68 0.883 21.74 1.036 21.80 1.011 21.75 0.288 0.210 0.204 0.214 0.250 0.269 0.139 0.181 0.157 0.204 0.204 0.154 0.168 0.174 0. 0.219 0.143 0.137 0.117 0.157 4.636 4.690 4.599 4.584 4.689 4.637 4.643 4.723 4.789 4.765 1.043 1.060 1.012 0.983 1.032 1.056 0.982 0.984 1.030 1.041 0.910 22.31 1.053 22.40 0.949 22.29 0.751 22.02 0.988 22. 0.820 22.54 0.712 22.21 0.729 22.38 0.801 22.63 0.835 22.45 small set of high-likelihood samples that can be reliably reproduced within limited steps, which manifests as partial memorization and reduced diversity. Increasing λ counteracts this effect by enforcing early-step diversity supervision, promoting broader mode coverage while preserving competitive visual quality at moderate values (e.g., λ = 0.05). Gradient stopping in DP-DMD. Figure 3 compares DPDMD with and without stopping gradients after the first step. The first point (at the 100-th iteration) already shows sharp diversity drop when gradients are not stopped, indicating that the DMD objective starts driving mode collapse very early. As training proceeds, both variants exhibit gradual reduction in diversity accompanied by steady improvement in perceptual quality, reflecting the inherent generalizationmemorization trade-off discussed earlier. However, the stopgradient version consistently retains higher level of diversity than the non-detached variant throughout training. Importantly, this improved diversity preservation does not come at the cost of perceptual quality, as their preference curves remain largely comparable. These results indicate that gradient stopping enables more persistent diversity retention by preventing later-step DMD optimization from progressively overriding the first-step diversity supervision, while still allowing continuous refinement of visual quality. 5.3. Comparison with State-of-the-Art Methods Comparison of Diversity Supervision. We aim to mitigate the issue of sample diversity degradation in DMD. We compare our approach with two widely adopted baselines: incorporating perceptual loss based on LPIPS (Zhang et al., 2018; Yin et al., 2024b) and introducing an adversarFigure 3. Gradient stopping in DP-DMD. Training dynamics of diversity and preference for DMD, DP-DMD, and variant without gradient stopping after the first step. All curves start from the 100-th training iteration and are smoothed using exponential moving average. to balance diversity preservation and quality refinement, and that anchoring diversity at moderately early teacher steps is most effective in practice. Weighting coefficient λ. As shown in Table 2, increasing λ consistently improves sample diversity (e.g., DINO from 0.170 at λ = 0.01 to 0.177 at λ = 0.10), while slightly degrading visual quality and human preference metrics. This trade-off reflects fundamental generalizationmemorization balance in few-step diffusion distillation: the reverse-KL-based DMD objective encourages mode-seeking by penalizing low-density regions of the teacher distribution, causing the student to concentrate probability mass on 6 Diversity-Preserved DMD Figure 4. Qualitative comparison of diversity supervision methods. Visual results of four distillation variants on the same prompts: (a) vanilla DMD, (b) DMD-LPIPS, (c) DMD-GAN, and (d) the proposed DP-DMD. While perceptual and GAN-based approaches provide limited or unstable diversity gains and often suffer quality degradation, DP-DMD preserves rich sample diversity while maintaining high visual fidelity, demonstrating more favorable diversityquality trade-off. ial loss (Yin et al., 2024a; Chadebec et al., 2025), referred to as DMD-LPIPS and DMD-GAN, respectively. To ensure fair comparison, all models are trained on the same dataset using an identical number of training iterations, with the same CFG scale and NFE. Compared to prior regularization-based approaches, Table 3 shows that incorporating LPIPS as an additional constraint does not consistently yield proportional gains in diversity over vanilla DMD, while introducing extra optimization constraints that are not directly aligned with the objective of distribution matching. GAN-based regularization can also improve diversity to some extent4, but it comes with noticeable quality/preference degradation (see Figure 4), reflecting the typical sensitivity and instability of adversarial objectives in few-step distillation. Beyond the metric tradeoffs, both LPIPS and GAN variants require extra modules (perceptual backbone or discriminator), which increases compute/memory cost and complicates tuning. In contrast, DP-DMD achieves substantially better balance between diversity and visual quality. This improve4We observe that degradation in sample quality can partially affect the reliability of diversity measurements, as visually evidenced in Figure 4. ment arises from its role-separated design, which controls the diversity-quality trade-off through the anchor step and weight λ. Diversity is enforced only at the first step governing global structure, allowing later steps to focus purely on quality refinement under standard DMD. Moreover, by operating entirely in latent space and introducing stopgradient boundary, the method avoids extra modules and prevents later-step DMD gradients from overriding earlystage diversity signals, resulting in lightweight and stable distillation framework with more favorable trade-offs. Consistent conclusions are also observed in Section of the Appendix through human user study. System-level Comparison. We further perform systemlevel comparison by distilling our models on SD3-M (Esser et al., 2024) and benchmarking them against representative open-source few-step distillation methods. These include flow-based approaches such as Hyper-SD (Ren et al., 2024), Flash Diffusion (Chadebec et al., 2025), and TDM (Luo et al., 2025). We emphasize that this comparison is not strictly controlled, as the methods differ in training data, distillation CFG scales, optimization budgets, and other implementation details5. 5The quantitative results are presented not as strictly fair 7 Table 4. System-level comparison of few-step open-source diffusion distillation methods. DP-DMD balances visual quality and diversity without perceptual or adversarial components, achieving competitive open-source performance. Diversity-Preserved DMD Method Base Model Hyper-SD Flash TDM DP-DMD ImageFree - NFE 60 8 4 4 4 Diversity Pick-a-Pic Quality Preference Diversity COCO-10K 2014 Quality Preference DINO CLIP VQ-R1 MIQA ImgR PicS DINO CLIP VQ-R1 MIQA ImgR PicS 1.014 22.50 0.230 0.234 0.661 21.56 0.878 22.38 0.184 0.998 21.49 0.148 1.034 22.29 0.162 1.067 21.46 0.808 20.77 1.014 21.62 1.134 21.21 1.128 21.15 0.202 0.236 0.184 0.172 0.174 0.205 0.225 0.172 0.167 0. 1.042 0.917 1.023 1.013 1.001 4.665 4.268 4.625 4.675 4.673 0.278 0.297 0.229 0.196 0.197 1.065 0.924 0.998 1.046 1.048 4.757 3.929 4.517 4.617 4.672 Figure 5. Qualitative comparison with open-source few-step distillation methods. Table 5. Quantitative comparison on GenEval of the model distilled using our proposed DP-DMD method and the original teacher model SD3.5-M. Method Overall SD3.5-M 0.66 DP-DMD 0.65 Two Single Obj. Obj. 0.98 0.78 0.99 0.81 Count. Colors Pos. 0.64 0. 0.83 0.81 Color Attr. 0.18 0.53 0.21 0.48 Instead, this comparison is designed to highlight key practical takeaway. Table 4 summarizes the system-level comparison. Without introducing any additional tricks or auxiliary modules, our approach achieves competitive system-level trade-off. By augmenting the standard DMD (Yin et al., 2024b) objective with lightweight diversity supervision and role separation, DP-DMD attains comparable and sometimes stronger visual quality (see Figure 5) and sample diversity than prior DMD-based systems that rely on heavier regularization, such as perceptual backbones (Hyper-SD) or adversarial training (Flash Diffusion), validating the effectiveness and practicality of DP-DMD. 5.4. More Results We further evaluate our distilled model on GenEval (Ghosh et al., 2023) to verify that the proposed DP-DMD does not compromise core prompt-following abilities while accelercomparison, but to contextualize the performance of our method within the open-source ecosystem. 8 ating inference. GenEval evaluates compositional reasoning and instruction adherence under controlled text prompts. As shown in Table 5, DP-DMD achieves an overall score comparable to the teacher SD3.5-M (Esser et al., 2024), with particularly consistent performance on single/two-object composition and spatial positioning. These results suggest that our role-separated distillation preserves the teachers semantic alignment and compositional capabilities, while providing fast few-step sampling. 6. Discussion and Conclusion We propose DP-DMD, role-separated distillation framework that improves sample diversity while maintaining competitive visual quality and preference, without additional modules. Experiments across multiple T2I backbones and benchmarks show that DP-DMD is lightweight and stable alternative to regularization-based approaches. Limitations and Future Directions. DP-DMD currently provides explicit diversity supervision only at the first distilled step, while the subsequent steps are trained solely under the DMD objective. Although this separation is effective and easy to implement, it may be suboptimal in scenarios where diversity-relevant decisions are not fully resolved in the first step, or when later steps still influence global attributes (e.g., composition changes induced by strong guidance or challenging prompts). natural direction is to move beyond fixed role split and enable step-wise and adaptive balancing between diversity preservation and quality refineDiversity-Preserved DMD ment. We believe such adaptive, trajectory-wide supervision can further improve the balance between diversity and quality and enhance the robustness of few-step distillation across wide range of generative settings."
        },
        {
            "title": "References",
            "content": "Chadebec, C., Tasar, O., Benaroche, E., and Aubin, B. Flash diffusion: Accelerating any conditional diffusion model for few steps image generation. In Association for the Advancement of Artificial Intelligence, pp. 1568615695, 2025. Chen, R. T., Rubanova, Y., Bettencourt, J., and Duvenaud, In AdD. K. Neural ordinary differential equations. vances in Neural Information Processing Systems, pp. 65726583, 2018. Ding, K., Ma, K., Wang, S., and Simoncelli, E. P. Image quality assessment: Unifying structure and texture similarity. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(5):25672581, 2020. Esser, P., Kulal, S., Blattmann, A., Entezari, R., Muller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., et al. Scaling rectified flow Transformers for high-resolution image synthesis. In International Conference on Machine Learning, 2024. Geng, Z., Deng, M., Bai, X., Kolter, J. Z., and He, K. Mean flows for one-step generative modeling. arXiv preprint arXiv:2505.13447, 2025. Ghosh, D., Hajishirzi, H., and Schmidt, L. GENEVAL: An object-focused framework for evaluating text-to-image alignment. In Advances in Neural Information Processing Systems, pp. 5213252152, 2023. He, X., Fu, S., Zhao, Y., Li, W., Yang, J., Yin, D., Rao, F., and Zhang, B. TempFlow-GRPO: When timing matters for GRPO in flow models. arXiv preprint arXiv:2508.04324, 2025. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1402414035, 2025. Lin, S., Wang, A., and Yang, X. SDXL-Lightning: Progressive adversarial diffusion distillation. arXiv preprint arXiv:2402.13929, 2024. Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., and Zitnick, C. L. Microsoft COCO: Common objects in context. In European Conference on Computer Vision, pp. 740755, 2014. Lipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., and Le, M. Flow Matching for generative modeling. In International Conference on Machine Learning, 2022. Liu, D., Gao, P., Liu, D., Du, R., Li, Z., Wu, Q., Jin, X., Cao, S., Zhang, S., Li, H., et al. Decoupled DMD: CFG augmentation as the spear, distribution matching as the shield. arXiv preprint arXiv:2511.22677, 2025. Liu, E., Ning, X., Lin, Z., Yang, H., and Wang, Y. OMSDPM: Optimizing the model schedule for diffusion probabilistic models. In International Conference on Machine Learning, 2023a. Liu, X., Gong, C., and Liu, Q. Flow straight and fast: Learning to generate and transfer data with rectified flow. In International Conference on Learning Representations, 2023b. Liu, X., Zhang, X., Ma, J., Peng, J., et al. InstaFlow: One step is enough for high-quality diffusion-based text-toimage generation. In International Conference on Learning Representations, 2023c. Loshchilov, I. and Hutter, F. Decoupled weight decay regIn International Conference on Learning ularization. Representations, 2017. Lu, C. and Song, Y. Simplifying, stabilizing and scaling continuous-time consistency models. arXiv preprint arXiv:2410.11081, 2024. Kirstain, Y., Polyak, A., Singer, U., Matiana, S., Penna, J., and Levy, O. Pick-a-Pic: An open dataset of user preferences for text-to-image generation. In Advances in Neural Information Processing Systems, pp. 36652 36663, 2023. Lu, Y., Ren, Y., Xia, X., Lin, S., Wang, X., Xiao, X., Ma, A. J., Xie, X., and Lai, J.-H. Adversarial distribution matching for diffusion distillation towards efficient image and video synthesis. In IEEE/CVF International Conference on Computer Vision, pp. 1681816829, 2025. Labs, B. F., Batifol, S., Blattmann, A., Boesel, F., Consul, S., Diagne, C., Dockhorn, T., English, J., English, Z., Esser, P., et al. FLUX. 1 Kontext: Flow Matching for in-context image generation and editing in latent space. arXiv preprint arXiv:2506.15742, 2025. Li, R., Yang, T., Guo, S., and Zhang, L. RORem: Training robust object remover with human-in-the-loop. In Luo, S., Tan, Y., Patil, S., Gu, D., Von Platen, P., Passos, A., Huang, L., Li, J., and Zhao, H. LCM-LoRA: universal stable-diffusion acceleration module. arXiv preprint arXiv:2311.05556, 2023. Luo, Y., Hu, T., Sun, J., Cai, Y., and Tang, J. Learning fewstep diffusion models by trajectory distribution matching. arXiv preprint arXiv:2503.06674, 2025. 9 Diversity-Preserved DMD Meng, C., Rombach, R., Gao, R., Kingma, D., Ermon, S., Ho, J., and Salimans, T. On distillation of guided diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1429714306, 2023. Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Muller, J., Penna, J., and Rombach, R. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Poole, B., Jain, A., Barron, J. T., and Mildenhall, B. DreamFusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. Poole, B., Jain, A., Barron, J. T., and Mildenhall, B. DreamFusion: Text-to-3d using 2d diffusion. In International Conference on Learning Representations, 2023. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, 2021. Song, Y., Dhariwal, P., Chen, M., and Sutskever, I. Consistency models. In International Conference on Machine Learning, 2023. Sutton, R. S., Barto, A. G., et al. Reinforcement learning. Journal of Cognitive Neuroscience, 11(1):126134, 1999. Tong, S., Ma, N., Xie, S., and Jaakkola, T. Flow map distillation without data. arXiv preprint arXiv:2511.19428, 2025. Wan, T., Wang, A., Ai, B., Wen, B., Mao, C., Xie, C.-W., Chen, D., Yu, F., Zhao, H., Yang, J., et al. WAN: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Wang, B. and Vastola, J. J. Diffusion models generate images like painters: an analytical theory of outline first, details later. arXiv preprint arXiv:2303.02490, 2023. Wang, F.-Y., Huang, Z., Bergman, A., Shen, D., Gao, P., Lingelbach, M., Sun, K., Bian, W., Song, G., Liu, Y., et al. Phased consistency models. In Advances in Neural Information Processing Systems, pp. 8395184009, 2024. Ren, Y., Xia, X., Lu, Y., Zhang, J., Wu, J., Xie, P., Wang, X., and Xiao, X. Hyper-SD: Trajectory segmented consistency model for efficient image synthesis. In Advances in Neural Information Processing Systems, pp. 117340 117362, 2024. Wang, Z., Lu, C., Wang, Y., Bao, F., Li, C., Su, H., and Zhu, J. ProlificDreamer: High-fidelity and diverse textto-3d generation with variational score distillation. In Advances in Neural Information Processing Systems, pp. 84068441, 2023a. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1068410695, 2022. Sauer, A., Boesel, F., Dockhorn, T., Blattmann, A., Esser, P., and Rombach, R. Fast high-resolution image synthesis In ACM with latent adversarial diffusion distillation. SIGGRAPH Conference, pp. 111, 2024a. Sauer, A., Lorenz, D., Blattmann, A., and Rombach, R. Adversarial diffusion distillation. In European Conference on Computer Vision, pp. 87103, 2024b. Simeoni, O., Vo, H. V., Seitzer, M., Baldassarre, F., Oquab, M., Jose, C., Khalidov, V., Szafraniec, M., Yi, S., Ramamonjisoa, M., et al. DINOv3. arXiv preprint arXiv:2508.10104, 2025. Song, Y. and Dhariwal, P. Improved techniques for training consistency models. In International Conference on Learning Representations, 2023. Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2020. Wang, Z. J., Montoya, E., Munechika, D., Yang, H., Hoover, B., and Chau, D. H. DiffusionDB: large-scale prompt gallery dataset for text-to-image generative models. In Association for Computational Linguistics, pp. 893911, 2023b. Wu, T., Zou, J., Liang, J., Zhang, L., and Ma, K. VisualQuality-R1: Reasoning-induced image quality asarXiv sessment via reinforcement learning to rank. preprint arXiv:2505.14460, 2025. Xu, J., Liu, X., Wu, Y., Tong, Y., Li, Q., Ding, M., Tang, J., and Dong, Y. ImageReward: Learning and evaluating human preferences for text-to-image generation. In Advances in Neural Information Processing Systems, pp. 1590315935, 2023. Yan, H., Liu, X., Pan, J., Liew, J. H., Liu, Q., and Feng, J. PeRFlow: Piecewise rectified flow as universal plugand-play accelerator. In Advances in Neural Information Processing Systems, pp. 7863078652, 2024. Yang, S., Wu, T., Shi, S., Lao, S., Gong, Y., Cao, M., Wang, J., and Yang, Y. MANIQA: Multi-dimension attention network for no-reference image quality assessment. In IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 11911200, 2022. 10 Diversity-Preserved DMD Yin, T., Gharbi, M., Park, T., Zhang, R., Shechtman, E., Durand, F., and Freeman, B. Improved distribution matching distillation for fast image synthesis. In Advances in Neural Information Processing Systems, pp. 4745547487, 2024a. Yin, T., Gharbi, M., Zhang, R., Shechtman, E., Durand, F., Freeman, W. T., and Park, T. One-step diffusion In IEEE/CVF with distribution matching distillation. Conference on Computer Vision and Pattern Recognition, pp. 66136623, 2024b. Zhang, R., Isola, P., Efros, A. A., Shechtman, E., and Wang, O. The unreasonable effectiveness of deep features as perceptual metric. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 586595, 2018. Zheng, K., Wang, Y., Ma, Q., Chen, H., Zhang, J., Balaji, Y., Chen, J., Liu, M.-Y., Zhu, J., and Zhang, Q. Large scale diffusion distillation via score-regularized continuoustime consistency. arXiv preprint arXiv:2510.08431, 2025. Zhou, M., Zheng, H., Gu, Y., Wang, Z., and Huang, H. Adversarial score identity distillation: Rapidly surpassing the teacher in one step. arXiv preprint arXiv:2410.14919, 2024a. Zhou, M., Zheng, H., Wang, Z., Yin, M., and Huang, H. Score identity distillation: Exponentially fast distillation of pretrained diffusion models for one-step generation. In International Conference on Machine Learning, 2024b. Diversity-Preserved DMD"
        },
        {
            "title": "Appendix",
            "content": "A. Observation of Early and Late Denoising Steps Figure visualizes the inference process of SD3.5-M (Esser et al., 2024) under different noise initializations. As shown in the left panel of Figure A, the denoising trajectory exhibits clear stage-wise behavior. The early denoising steps, operating at high noise levels, primarily determine the global structural layout of the generated image, including object identity, coarse geometry, and overall composition. Notably, variations introduced at these early stages persist throughout the remainder of the denoising process, resulting in distinct final samples even under identical text conditioning. This observation highlights strong connection between early denoising behavior and sample diversity. The right panel of Figure further illustrates this phenomenon by comparing early denoising steps across different noise initializations. Even at very early timesteps, noticeable differences in global structure already emerge, indicating that diversity is largely established before fine details are formed. In contrast, later denoising steps operate at lower noise levels and mainly focus on refining fine-grained visual details such as textures, colors, and local appearance. These steps exhibit limited influence on the global structure and contribute predominantly to perceptual quality rather than diversity. Together, these observations suggest an intrinsic asymmetry between early and late denoising stages: early steps govern global structure and diversity, while later steps specialize in quality refinement. This motivates our design choice in DPDMD to explicitly separate the training objectives of early and late distilled steps, enabling effective diversity preservation without sacrificing visual fidelity. B. Motivation for First-Step Diversity Supervision key design choice in DP-DMD is to apply explicit supervision only to the first distilled step. This choice is motivated by the alignment of state distributions between the teacher and the student during training. Specifically, both the teacher and the student are trained on the same noise prior, and therefore share the identical initial state ϵ. As result, the students first-step input lies on distribution that has been fully observed by the teacher, making it well-defined and meaningful to construct teacher-derived supervision signal at this step. In contrast, the intermediate states encountered by the student during its subsequent inference steps are generated by the student itself and generally do not lie on the teachers training trajectory. These states are out-of-distribution for the teacher and thus lack reliable ground-truth targets. Applying direct supervision at later steps would therefore introduce distribution mismatch between the teacher and student, potentially leading to biased or unstable training (Tong et al., 2025). Consequently, we restrict explicit diversity supervision to the first step, where the teachers guidance is both valid and informative at this stage. C. DP-DMD on Diffusion Models Early diffusion models such as SDXL (Podell et al., 2023) differ from flow-based formulations (Esser et al., 2024) in that they are typically parameterized to predict the noise ϵ (or related quantities) rather than the velocity field v. To adapt our diversity supervision to this setting, we formulate the supervision directly in the x0 (denoised latent) space, which is naturally available under common schedulers (e.g., DDIM). Concretely, given an intermediate teacher latent ztk at timestep tk, we compute the teacher-implied denoised prediction as: ztk ztarget = 1 αtk ϵtea(ztk , tk) αtk (8) where ϵtea(ztk , tk) is the noise prediction produced by the frozen teacher model. The resulting ztarget diffusion-model counterpart of the teacher-derived intermediate supervision used in the flow-based case. serves as the For the distilled student model, we compute its first-step prediction zθ(zT , ) from the initial noisy latent zT at the starting timestep . We then encourage the students first step to match the teacher-derived target via an ℓ2 regression loss: LDiv = EzT (cid:104)(cid:13) (cid:13)zθ(zT , ) ztarget 2(cid:105) (cid:13) (cid:13) . (9) Diversity-Preserved DMD Figure A. Progressive denoising dynamics. Visualization of SD3.5-M (Esser et al., 2024) inference exhibits stage-wise denoising pattern. The left panel shows trajectory from step 1 to step 17, while the right panel highlights early steps under different noise initializations. Early steps recover the global structural layout, already showing variation across samples and suggesting strong link to sample diversity, whereas later steps refine fine-grained appearance details and textures. Algorithm 2 DP-DMD Training for Diffusion Models # x: training data batch # k: diversity anchor step index # T: starting timestep # N: number of student sampling steps # lambda_div: diversity loss weight eps = randn like(x) z_k, eps_z0_tea, a_k = rollout teacher target(eps, k, T) # convert to x0 for diversity supervision z0_target = (z_k - sqrt(1 - a_k) * eps_z0_tea) / sqrt(a_k) # diversity supervision eps_z0_stu, a_T = eps stu(eps, T) z0_stu = (eps - sqrt(1 - a_T) * eps_z0_stu) / sqrt(a_T) loss_div = l2 loss(z0_stu - z0_target) # detach after first step z_T_minus_1 = stopgrad(rollout student(eps, 1)) # quality supervision z0_theta = rollout student(z_T_minus_1, - 1) loss_dmd = dmd loss(z0_theta) loss = loss_dmd + lambda_div * loss_div This diversity supervision mirrors our flow-based objective in spirit: it anchors the students early denoising behavior to teacher-defined intermediate target, thereby promoting broader mode coverage and mitigating diversity degradation in few-step diffusion distillation. The overall pseudo-code for applying DP-DMD to diffusion-based models (e.g., SDXL) is shown in Algorithm 2. D. User Study We conduct controlled user study to evaluate both sample diversity and image quality of different distillation methods. We randomly select 50 text prompts and recruit 10 participants with prior experience in evaluating image generation results. For each prompt, images generated by two methods under identical text conditioning and random seed settings are presented side by side in randomized order. Participants are asked to perform pairwise comparisons along two criteria: (1) Diversity, focusing on variations in global structure, composition, and semantic attributes across multiple samples generated from the same prompt; and (2) Image quality, reflecting visual fidelity, realism, and overall perceptual quality of the generated images. No additional guidance is provided beyond these criteria to avoid bias. 13 Diversity-Preserved DMD Figure B. User study on diversity and image quality. We run pairwise comparisons on 50 prompts with 10 participants for (left) diversity and (right) image quality. Bars show win rates (%) of DP-DMD against DMD (Yin et al., 2024a), DMD-LPIPS, and DMD-GAN; the dashed line marks 50%. DP-DMD is consistently preferred, achieving higher diversity while maintaining strong image quality. The final results are reported as win rates aggregated over all prompts and users. As shown in Figure B, DP-DMD is consistently preferred over DMD (Yin et al., 2024a), DMD-LPIPS, and DMD-GAN, showing substantial improvements in diversity while maintaining competitive or superior image quality. These results indicate that the proposed role-separated distillation effectively mitigates mode collapse without sacrificing perceptual quality. E. More Visualizations We provide additional qualitative results in Figure and Figure to complement the quantitative analysis in the main paper. Figure focuses on sample diversity under identical text prompts and different random seeds. The results show that DP-DMD consistently produces wider range of global structures, compositions, and semantic variations, effectively mitigating the mode collapse behavior commonly observed in vanilla DMD (Yin et al., 2024b;a). In contrast, Figure exclusively presents samples generated by DP-DMD to demonstrate its intrinsic visual quality. Despite operating under few-step inference, the generated images exhibit realistic appearance, coherent global layouts, and fine-grained details with natural textures and colors. These results indicate that the diversity-preserving supervision in DP-DMD enables the model to maintain high visual quality while achieving improved sample diversity. 14 Diversity-Preserved DMD Figure C. Sample diversity under identical prompts. Comparison of images generated with the same text prompts and different random seeds, showing that DP-DMD produces more diverse global structures and semantic variations than baseline methods. All models generate samples with 4 NFEs. 15 Diversity-Preserved DMD Figure D. Sample quality of DP-DMD. Images generated at 1024 1024 resolution by DP-DMD, distilled from SD3.5M (Esser et al., 2024). All samples are produced with 4 NFEs, demonstrating high visual fidelity and coherent global structures under few-step inference."
        }
    ],
    "affiliations": [
        "Multimedia-Analytics-Laboratory"
    ]
}