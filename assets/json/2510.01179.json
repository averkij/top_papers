{
    "paper_title": "TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments",
    "authors": [
        "Zhangchen Xu",
        "Adriana Meza Soria",
        "Shawn Tan",
        "Anurag Roy",
        "Ashish Sunil Agrawal",
        "Radha Poovendran",
        "Rameswar Panda"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Model (LLM) agents are rapidly emerging as powerful systems for automating tasks across domains. Yet progress in the open-source community is constrained by the lack of high quality permissively licensed tool-agentic training data. Existing datasets are often limited in diversity, realism, and complexity, particularly regarding multi-tool and multi-turn interactions. To address this gap, we introduce Toucan, the largest publicly available tool-agentic dataset to date, containing 1.5 million trajectories synthesized from nearly 500 real-world Model Context Protocols (MCPs). Unlike prior work, Toucan leverages authentic MCP environments to generate diverse, realistic, and challenging tasks with trajectories involving real tool execution. Our pipeline first produces a broad spectrum of tool-use queries using five distinct models, applies model-based quality filtering, and then generates agentic trajectories with three teacher models using two agentic frameworks. Rigorous rule-based and model-based validation ensures high-quality outputs. We also introduce three extension mechanisms to further diversify tasks and simulate multi-turn conversations. Models fine-tuned on Toucan outperform larger closed-source counterparts on the BFCL V3 benchmark and push the Pareto frontier forward on MCP-Universe Bench."
        },
        {
            "title": "Start",
            "content": "TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments TOUCAN: SYNTHESIZING 1.5M TOOL-AGENTIC DATA FROM REAL-WORLD MCP ENVIRONMENTS Zhangchen Xu Ashish Sunil Agrawal University of Washington Adriana Meza Soria Shawn Tan Anurag Roy Radha Poovendran Rameswar Panda MIT-IBM Watson AI Lab 5 2 0 2 1 ] . [ 1 9 7 1 1 0 . 0 1 5 2 : r https://github.com/TheAgentArk/Toucan https://hf.co/datasets/Agent-Ark/Toucan-1.5M"
        },
        {
            "title": "ABSTRACT",
            "content": "Large Language Model (LLM) agents are rapidly emerging as powerful systems for automating tasks across domains. Yet progress in the open-source community is constrained by the lack of high quality permissively licensed tool-agentic training data. Existing datasets are often limited in diversity, realism, and complexity, particularly regarding multi-tool and multi-turn interactions. To address this gap, we introduce TOUCAN, the largest publicly available tool-agentic dataset to date, containing 1.5 million trajectories synthesized from nearly 500 real-world Model Context Protocols (MCPs). Unlike prior work, TOUCAN leverages authentic MCP environments to generate diverse, realistic, and challenging tasks with trajectories involving real tool execution. Our pipeline first produces broad spectrum of tool-use queries using five distinct models, applies model-based quality filtering, and then generates agentic trajectories with three teacher models using two agentic frameworks. Rigorous rule-based and model-based validation ensures highquality outputs. We also introduce three extension mechanisms to further diversify tasks and simulate multi-turn conversations. Models fine-tuned on TOUCAN outperform larger closed-source counterparts on the BFCL V3 benchmark and push the Pareto frontier forward on MCP-Universe Bench."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large language models (LLMs) have become integral to AI applications, with LLM agents emerging as powerful systems for automating complex tasks across diverse domains Li et al. (2024). There is growing excitement about the potential of LLM agents to unlock new levels of automation across industries (Ferrag et al., 2025; Bousetouane, 2025). These agents handle multi-step workflows that require discovering the right tools from potentially large toolsets, calling them correctly with appropriate parameters, handle tool failures gracefully, and synthesizing results into accurate, context-aware responses Xu et al. (2025a). Recent advancements, such as the Model Context Protocol (MCP) (Anthropic, 2025), have streamlined tool integration by providing standardized interfaces, enabling seamless connections between LLMs and real-world environments and simplifying the process for LLM agents to discover, invoke, and execute external tools. Despite these advancements, progress in the open-source community is constrained by the lack of high-quality, permissively licensed tool-agentic data for training more capable agentic LLMs. An instance of tool-agentic data comprises task-trajectory pair, where trajectories capture sequences of planning, tool calls, tool responses, and the final model response. While previous efforts (Qin et al., 2023; Liu et al., 2024; 2025a; Prabhakar et al., 2025) have introduced datasets covering various tool-calling scenarios, they suffer from several limitations: restricted tool diversity, lack of authentic tool responses, focus on single-turn conversations between users and models, or insufficient scale, all of which constrain effective training of agentic capabilities. There is an urgent need for comprehensive, high-quality datasets that capture the full spectrum of tool-agentic interactions observed in production environments. 1 TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments Table 1: TOUCAN comparison to open-source tool-agentic datasets. Comparison comprises total trajectories, tool calling scenarios ([S]ingle, [P]arallel, [M]ulti[S]tep) including no-tool-use edge case (irrelevance[IR]), number of multi-turn conversations, and other details about data generation. Note indicates information not publicly available. Dataset Trajectories Tool-Call Scenarios Multi Turn Tool Specs Tool Response APIGent-MT-5K (Prabhakar et al., 2025) 5,000 MS IR 5,000 From τ -Bench Executed ToolACE (Liu et al., 2025a) 11,300 MS IR 509 Synthetic Simulated Hermes Function-Calling V1 (interstellarninja) 11,570 MS IR 1,890 Synthetic Executed Nemotron (Tools) (Nathawani et al., 2025) 310,051 MS 199,610 TOUCAN (This Work) 1,527,259 MS IR 567, Real Executed In this work, we bridge this gap by introducing TOUCAN, the largest publicly available tool-agentic dataset to date, comprising 1.5 million trajectories synthesized from nearly 500 real-world MCP servers. Unlike prior approaches that rely on simulated or limited toolsets, TOUCAN leverages authentic MCP environments with more than 2,000 tools to generate diverse, realistic, and challenging tasks spanning parallel and multi-step tool calls, as well as multi-turn conversations. Our pipeline begins by producing broad spectrum of tool-use tasks using five distinct models with MCP server specifications, followed by model-based quality filtering to ensure relevance and difficulty. We then generate agentic trajectories with three teacher models, incorporating rigorous rule-based and model-based checks for high-quality outputs, including verification of tool execution and response accuracy. Our pipeline also integrates extensions to generate additional tasks targeting edge case scenarios, interactive conversations, and multi-turn dialogues. Our experiments demonstrate the effectiveness of TOUCAN in enhancing LLM agentic capabilities. Models fine-tuned on TOUCAN surpass closed-source counterparts on the BFCL V3 benchmark (Patil et al., 2025), achieving superior performance in function calling accuracy across single-turn and multi-turn scenarios. Furthermore, they show substantial improvements on τ -Bench (Yao et al., 2024) and τ 2-Bench (Barres et al., 2025), with gains in tool selection, execution fidelity, and multiturn reasoning under dynamic user interactions. On the recent MCP-Universe benchmark (Luo et al., 2025), which evaluates LLMs on 231 realistic tasks using 11 real-world MCP servers, TOUCAN-tuned models achieve state-of-the-art performance within their parameter class, consistently outperforming leading models of comparable size. In summary, the contributions of our work are: TOUCAN Dataset. The largest open-source tool-agent training dataset, covering parallel and multi-step tool calls, multi-turn dialogues, and edge-case tool use. Recent reports on frontier LLM development, such as Kimi-K2 (Team et al., 2025b) and GLM-4.5 (Team et al., 2025a), highlight the value of large-scale trajectories with broad domain coverage, and TOUCAN provides an open-source alternative that bridges this gap. TOUCAN Pipeline. pipeline that leverages any MCP specifications to generate diverse toolagent trajectories, supports tool execution through MCP servers, and can be seamlessly extended to new tools via the MCP standard. TOUCAN Checkpoints. Our experiments demonstrate that models fine-tuned on TOUCAN mixtures surpass closed-source counterparts on the BFCL V3 and MCP-Universe benchmarks."
        },
        {
            "title": "2 RELATED WORK",
            "content": "The past: Tool-calling datasets and benchmarks for LLMs. Early tool-calling datasets enabled LLMs to interact with tools like REST APIs and ML functions. The Gorilla project (Patil et al., 2023) demonstrated that fine-tuning on such data enhances tool-use over vanilla models, introducing the BFCL benchmark (Patil et al., 2025) as standard. ToolAlpaca (Tang et al., 2023) offered costeffective synthetic data with lower quality, while ToolLLM (Qin et al., 2023) expanded to 16,000+ APIs across domains. API Pack (Guo et al., 2025a) added cross-language diversity (Python, Java, C++), and API Blend (Basu et al., 2024) optimized dataset mixtures for robustness, laying the foundation for tool-agent advancements. More recently, APIGen has focused on domain diversification, contributing training dataset covering 21 domains Liu et al. (2024). 2 TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments Figure 2: The TOUCAN construction pipeline: systematic five-stage process from MCP server onboarding through trajectory filtering, with three extensions for enhancing data diversity and realism. The present: Tool-calling benchmarks and datasets for LLM-agents. Recent research has shifted toward training LLM agents for effective tool use, exemplified by models like Kimi-K2 (Team et al., 2025b) and GLM-4.5 (Team et al., 2025a), with performance assessed via benchmarks such as BFCL (Patil et al., 2025), τ -Bench (Yao et al., 2024), and ACEBench (Chen et al., 2025). BFCL covers diverse scenarios including parallel, multi-step, and multi-turn tool use, while τ -Bench focuses on realistic user-agent-tool interactions. ACEBench enhances evaluation by addressing edge cases and including subset for tool-agent trajectories. Despite these advances, open-source training data for tool-agent trajectories remains limited. Existing datasets (interstellarninja; Liu et al., 2025a; Prabhakar et al., 2025; Nathawani et al., 2025) either lack dataset curation transparency, are small in size for SFT, or simulate tool responses via LLMs. Table 1 compares existing tool-agentic datasets with TOUCAN, which, at 1.5 million trajectories, offers the largest dataset, featuring extensive multi-turn dialogues, all tool-use scenarios, critical edge cases, and authentic tool responses from real-world environments. The future: MCP benchmarks and datasets. As concurrent work, recent MCP benchmarks (Gao et al., 2025; Wang et al., 2025; Luo et al., 2025; Team, 2025a; Guo et al., 2025b; Yin et al., 2025; Liu et al., 2025b; Yan et al., 2025; Team, 2025b) aim to rigorously assess LLMs in tool-use settings beyond simple correctness. For instance, MCP-Radar (Gao et al., 2025) employs five-dimensional evaluation including accuracy, tool selection efficiency, resource usage, parameter construction, and execution speed across software engineering, math, and problem-solving tasks with 300 queries and 42 MCP servers. Similarly, MCP-Bench (Wang et al., 2025) evaluates multi-step reasoning over 28 MCP servers and 250 tools, while MCP-Universe (Luo et al., 2025) focuses on execution-based metrics in six real-world domains. These advancements underscore the need for comprehensive training datasets to support the development of robust, open-source LLM agents."
        },
        {
            "title": "3.1 TOUCAN GENERATION PIPELINE",
            "content": "TOUCAN is comprehensive dataset comprising over 1.5 million tool-agent trajectories constructed using real-world tools from MCP servers. Each instance in our dataset contains task description, complete agent trajectory with its associated tools, quality and classification annotations, as well as comprehensive metadata. Appendix provides detailed schema description and demonstration samples. The construction of TOUCAN follows systematic five-stage pipeline: MCP server onboarding, task synthesis, task filtering, trajectory generation, and trajectory filtering. Additionally, we implement three extension mechanisms to further enhance data diversity and realism. Figure 2 illustrates the complete construction pipeline. We detail each stage below. 3 Figure 1: MCP servers filtering process TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments Stage 1: MCP Server Onboarding. To generate questions from diverse environments, the initial step involves onboarding as many high-quality MCP servers as possible. We sourced MCP server specification files from GitHub and Smithery 1, platform and registry for MCP servers that encapsulate modular execution environments. Each MCP server is accompanied by structured JSON document detailing metadata about the server with machine-readable definition of the tools it provides. From an initial crawl yielding approximately 2,800 MCP servers, we applied two key filtering criteria: (1) retaining only remote MCP servers accessible via streamable HTTP to ensure compatibility with trajectory generation, and (2) excluding servers requiring third-party credentials (e.g., API keys) for tool invocation to maintain accessibility and reproducibility. This process reduced the dataset to 30.6% (871 servers). As final step, we generated small subset of test questions to evaluate each tool within the MCP servers, subsequently filtering out servers with problematic tools that returned error messages or failed to function correctly. This rigorous curation process resulted in refined set of 495 high-quality MCP servers spanning diverse domains and functionalities. Figure 1 depicts the number of MCP servers retained at each filtering stage. Figure 3 demonstrates the domain distribution of the final server collection across diverse categories. The domain distribution is annotated by LLMs, where prompts can be found in Appendix D.1. Stage 2: Task Synthesis. The next step involves synthesizing high-quality tasks from MCP servers, where each task comprises question and the desired tool names from the MCP servers. The key challenge is ensuring that tasks are challenging, realistic, and cover edge cases. Therefore, we design diverse sampling strategies based on MCP server usage number from Smithery and server functionalities. To avoid potential bias from individual models, we utilized five open-source LLMs (Mistral-Small, DevStral-Small, GPT-OSS, Kimi-K2, and Qwen3-32B) as task generators to construct synthetic tasks (see the prompts in Appendix D.2). We apply the following three strategies to synthesize tasks, where the maximum number of tools is set to = 3 in our experiments: Single Server: For given MCP server, we synthesize tasks requiring the use of 1 to tools, ensuring balanced selection distribution guided by server usage statistics to reflect real-world applicability. Figure 3: MCP servers distribution by domain, covering wide range of categories. Values in parentheses indicate the number of servers belonging to each category. Multi-Server: Leveraging LLM-based domain annotations derived from MCP metadata, we first sample MCP servers from either the same or different categories. We then prompt LLMs to conduct server analysis, outlining potential workflows that integrate tools across these servers, targeting two to specific tools, and subsequently generating tasks that leverage functionalities from multiple servers. Featured Server: Based on the original MCP file metadata, we manually selected 25 representative MCP servers from various domains, with the complete list available in Appendix B.1. In this approach, we provide all MCP server metadata within the context, specify an expected number of tools, and allow the LLM to freely explore combinations, devise realistic scenarios, select the necessary tools, and create comprehensive tasks. Stage 3: Task Filtering. To ensure the quality of synthesized tasks, this stage involves annotating tasks across six dimensions and filter out suboptimal instances. We employed the Kimi-K2 model as the annotator, which was selected for its optimal balance between correlation with human annotations and cost efficiency. The correlation statistics are detailed in Appendix C.1, and the prompt template is provided in Appendix D.4. Each dimension is rated on 1-5 Likert scale. The detailed evaluation metrics are as follows: Tool Selection Difficulty: Judges the difficulty of selecting the required tools from provided tools. 1https://smithery.ai/ 4 TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments Tool Selection Uniqueness: Assesses the uniqueness of the selected tool combination relative to the available tools, and whether viable alternatives could also solve the task. Question Quality: The tasks overall quality, reflected by its clarity, specificity, and effectiveness. Scenario Realism: Evaluates the authenticity and realism of the task scenario. Verifiable: Evaluates how easily the final model answer can be verified given the question. Stability: Evaluates whether tool outputs remain consistent over time, across geolocation, and under stochastic variation. Stage 4: Trajectory Generation. This step involves collecting trajectories including tool calls, tool responses, and reasoning steps in agentic environments given tasks synthesized and filtered from the previous steps. To ensure diversity, we employed three LLMs from different families (GPT-OSS-120B, Kimi-K2, and Qwen3-32B) in combination with two agent frameworks (Qwen-agent and OpenAI-agent) to produce high-quality agentic trajectories. The models are deployed remotely and accessed by the agent frameworks via streamable HTTP. Stage 5: Rule&LLM-Based Post-Filtering. The trajectory filtering process combines rule-based verifiers with LLM-driven annotations to ensure high quality. Rule-based heuristics exclude trajectories that fail to start the agent or connect successfully with remote MCP servers, do not contain tool calls, exhibit failures in tool responses, or contain local file system paths. We also validate whether the trajectory uses the required tools specified by the task in the correct sequence, and report both the desired tool use percentage (coverage of required tools) and order correctness (adherence to expected sequence) metrics. We then employ GPT-OSS-120B as judge to annotate each trajectory in terms of completeness and conciseness. The annotation prompt is provided in Appendix D.5, with metric definitions as follows: Completeness: Judges whether the assistant fulfills the users request end-to-end. Conciseness: Judges whether the task is solved with the minimum necessary steps and verbosity. This dual-stage filtering approach ensures that only high-quality, concise, and executable trajectories are retained in the final dataset."
        },
        {
            "title": "3.2 TOUCAN EXTENSIONS",
            "content": "While the core pipeline generates high-quality trajectories, these are single-turn interactions between user and agent without follow-ups, which limits their practical applicability to real-world scenarios. In addition, since all available tools are contextually relevant, tool selection becomes trivial for LLMs, resulting in relatively low difficulty. To address these limitations and enhance the datasets versatility, we apply three distinct procedures post-core pipeline (Steps 1 to 5) to generate new instances targeting specific objectives. Ext.1: Irrelevance. To reduce hallucination, it is critical to train models to reject unanswerable queries or seek alternative solutions when desired tools are unavailable. To achieve this, we systematically generate queries unsolvable with the current toolset (Ext1 in Figure 2) by shuffling MCP server metadata across instances and repeating the task generation step. Ext.2: Persona-based Diversification. We implement persona-based diversification (Ext2 in Figure 2) to create varied task versions. This involves two strategies: one enhances diversity by introducing new contexts and personas, while the other increases task complexity through additional constraints, all while utilizing the same target tools. This diversification process produces tasks similar yet distinct from those in the core pipeline. The prompts are detailed in Appendix D.3. Ext.3: Multi-Turn. Recognizing that real-world user-agent-tool interactions seldom conform to single-turn conversations Yao et al. (2024), we introduce self-simulation pipeline to generate multi-turn dialogues using the trajectory generation model. This is achieved through two methods: (1) splitting complex tasks requiring multi-tool coordination into sequential sub-questions, and (2) extending existing conversations by providing LLMs with context to formulate follow-up queries. Finally, we repeat the core pipeline from steps 2 to 5 to build full trajectories with the new tasks. In the case of irrelevant tasks (Ext.1), we tighten trajectory filters to retain only instances with zero tool calls. Together, these data extensions yield more realistic and robust TOUCAN dataset that covers all relevant tool-use scenarios and user question styles. TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments Figure 4: The figures above illustrate the TOUCAN dataset analysis. Subfigure (a) and (b) provide statistics on the number of servers and required tools per instance, highlighting TOUCAN comprehensive coverage of multi-server and multi-tool tasks. Subfigures (c) and (d) reveal that most tasks include more tools in the context than the targeted tools, underscoring the non-trivial tool selection challenges. Subfigure (e) displays the length of user messages in tokens. Subfigures (f) and (h) demonstrate the multi-turn nature of the tasks, characterized by extended and diverse interactions among users, agents, and tools. Subfigure (g) demonstrates that TOUCAN encompasses both single and parallel tool calls, which enhance the datasets versatility in capturing diverse agent-tool interaction patterns. Figure 5: TOUCAN Subset Statistics Figure 6: TOUCAN Quality Statistics"
        },
        {
            "title": "3.3 DATA ANALYSIS",
            "content": "This section analyzes the generated TOUCAN dataset from statistical analysis and LLM-based quality assessment. Statistical Analysis of TOUCAN . We conduct comprehensive statistical analysis of MCP servers and data instances. The top MCP servers used in TOUCAN and tool statistics within each MCP servers are presented in Appendix B.2. Figure 4 provides comprehensive analysis of the TOUCAN dataset. We observe that TOUCAN provides comprehensive coverage of multi-server and multitool tasks, and includes multi-turn conversations among users, agents, and tools. Additionally, most tasks contain more tools in the context than the required target tools, indicating non-trivial tool selection requirements. Figure 5 presents the subset statistics of TOUCAN across different trajectory generator LLMs and data partitions. We also provide embedding visualization of TOUCAN using UMAP projection in Appendix B.3, demonstrating the wide domain coverage of TOUCAN. 6 TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments Table 2: This table compares the performance of TOUCAN-tuned models and baselines on the BFCL-V3 benchmark. We observe that TOUCAN remarkably improves baseline model performance through supervised fine-tuning (SFT) and enables smaller models to outperform larger models across different evaluation aspects. Model Overall Single Turn Multi Turn Hallucination Non-live (AST) Live (AST) Relevance Irrelevance DeepSeek-V3 Qwen2.5-72B-Instruct Qwen3-235B-A22B Qwen3-32B o3-Mini GPT-4.1 GPT-4.5-Preview Qwen2.5-7B-Instruct with TOUCAN Qwen2.5-14B-Instruct with TOUCAN Qwen2.5-32B-Instruct with TOUCAN 64.71% 64.37% 67.94% 69.25% 64.61% 68.69% 70.32% 55.10% 58.26%+3.16% 57.69% 65.09%+7.40% 61.73% 70.45%+8.72% 88.54% 87.56% 87.90% 88.90% 86.15% 85.42% 86.12% 84.19% 78.52% 83.38% 85.42% 85.58% 87.12% 77.34% 78.68% 77.03% 77.83% 79.08% 79.92% 79.34% 72.32% 74.50% 73.70% 76.01% 76.01% 78.90% 29.87% 29.38% 40.12% 43.12% 28.75% 40.50% 45.38% 12.88% 22.62% 19.75% 35.25% 26.38% 46.50% 83.33% 72.22% 83.33% 72.22% 72.22% 77.78% 66.67% 72.22% 66.67% 83.33% 72.22% 72.22% 77.78% 76.49% 77.41% 76.32% 75.79% 82.96% 85.95% 83.64% 67.93% 75.18% 68.46% 75.96% 72.68% 78.10% Quality Assessment of TOUCAN. Figure 6 presents statistical analysis conducted by an LLMas-a-judge on TOUCAN. From the task perspective (labels in ), we observe that the majority of tasks exhibit exceptionally high question quality and scenario realism, indicating robust task design and alignment with real-world applications. Additionally, the dataset features mixed difficulty range, encompassing both simple and challenging tasks. From the response perspective (label in ), we find that trajectory quality is satisfactory, with most scores at or above 3 (medium) across both completeness and conciseness metrics."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "In this section, we demonstrate the performance of TOUCAN by performing supervised fine-tuning (SFT) on baseline models of different sizes. We then compare the fine-tuned models performance against existing model baselines across several widely used agentic tool-call benchmarks."
        },
        {
            "title": "4.1 EXPERIMENT SETUP",
            "content": "Model and Baseline Setup. We perform supervised fine-tuning on Qwen2.5-7B-Instruct, Qwen2.5-14B-Instruct, and Qwen2.5-32B-Instruct (Team, 2024) to demonstrate the efficacy of TOUCAN across models of varying sizes. Detailed fine-tuning parameters are provided in Appendix C.2. We benchmark the performance of our fine-tuned models against models of comparable or larger scales, including DeepSeek-V3 DeepSeek-AI et al. (2025), Qwen2.5-72B-Instruct, Qwen3-235B-A22B, Qwen3-32B Yang et al. (2025), and closedsource OpenAI models such as o3-mini, GPT-4.1, and GPT-4.5-Preview. TOUCAN Setup. Given the large volume of the full dataset, we adopted strategy similar to Xu et al. (2025b) by sampling from high-quality subset of TOUCAN. This subset was selected based on the following criteria: question quality and scenario realism scores of 5, response completeness and conciseness scores of at least 4, and desired tool use percentage of 1.0 (indicating that trajectories fully utilize all required tools from the task). We performed necessary data re-balancing to ensure the dataset remains representative across different categories. The resulting SFT dataset comprises 28.3K instances from the original pipeline, 40K instances from Ext.1 (Irrelevance), 15.8K instances from Ext.2 (Diversify), and 35.2K instances from Ext.3 (Multi-Turn), totaling 119.3K instances. Benchmarks. We assess the performance of TOUCAN across several key tool-agentic benchmarks, including BFCL V3 Patil et al. (2025), τ -Bench Yao et al. (2024), τ 2-Bench (Barres et al., 2025), and MCP-Universe Luo et al. (2025). All evaluations are conducted on an 8 H100 server. For BFCL-V3, we use the official evaluation setup. For τ -Bench and τ 2-Bench, we employ GPT-4o as 7 TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments Table 3: This table presents τ -Bench and τ 2-Bench results for models fine-tuned on TOUCAN compared to their respective baselines. Improvements are observed across most evaluation scenarios. Model τ -bench Avg. Airline Retail Avg. τ 2-bench Airline Retail Telecom Qwen2.5-7B-Instruct with TOUCAN 15.03% 22.48%+7.45% 8.75% 21.30% 15.50% 29.46% 17.77%+1.69% 16.08% 14.00% 17.54% 16.70% 20.00% 22.80% 10.50% Qwen2.5-14B-Instruct with TOUCAN 30.85% 35.24%+4.39% 17.25% 44.46% 22.00% 48.48% 30.43%+5.97% 24.46% 12.00% 41.20% 20.18% 22.00% 49.10% 20.18% Qwen2.5-32B-Instruct with TOUCAN 38.76% 42.33%+3.57% 26.00% 51.52% 29.00% 55.65% 31.60%+2.20% 29.40% 18.00% 49.10% 21.11% 22.00% 52.60% 20.20% Figure 7: This figure compares the performance of TOUCAN-tuned models with other open-source models on MCP-Universe (Luo et al., 2025). Model sizes increase from left to right. Bars with darker colors represent task success rate (full task completion), while lighter colors represent average evaluation scores considering partial task completion. TOUCAN-tuned models are shown with black borders. TOUCAN-tuned models outperform other models of similar sizes across most tasks. user simulators. For MCP-Universe, we configure the local evaluation environment as specified in the benchmark documentation."
        },
        {
            "title": "4.2 EXPERIMENTAL RESULTS",
            "content": "TOUCAN Effectively Increases Agentic Tool-Calling Performance. Tables 2 and 3 present the experimental results of models fine-tuned on TOUCAN across BFCL V3, τ -Bench, and τ 2-Bench, respectively. We make the following key observations: First, models fine-tuned with TOUCAN show performance improvements compared to baseline models without fine-tuning across almost all aspects of these three benchmarks, indicating that TOUCAN effectively enhances the agentic and tool-calling capabilities of models. Second, on BFCL V3, models fine-tuned on TOUCAN outperform larger production LLMs including DeepSeek-V3 and GPT-4.5-Preview in average scores and achieve top performance in the multi-turn subset. This demonstrates the effectiveness of TOUCAN and validates our dataset design. TOUCAN Enhances Models Performance on Using Real-World MCP Servers. Figure 7 demonstrates performance comparison between TOUCAN-tuned models and 8 Figure 8: Model Performance vs Size on MCP-Universe Benchmark. We report overall task success rate (OSR). Our models push the Pareto frontier forward, achieving higher OSR at smaller model sizes. TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments other open-source models of similar or larger sizes across six domains: Location Navigation, Repository Management, Financial Analysis, 3D Design, Browser Automation, and Web Search. We note that most servers in the benchmark require careful configurations and thus were not included in our data synthesis pipeline. Nevertheless, TOUCAN-tuned models show significant improvements on these challenging tasks compared to baselines, indicating that exposure to diverse tools enhances model performance on agentic tasks. Notably, our 32B model achieves the highest scores in 3D Design and strong performance in Financial Analysis, even outperforming much larger frontier open models like Llama-3.3-70B-Instruct, Qwen2.5-72B-Instruct, GLM-4.5-Air (106B), and DeepSeek-V3 (671B). Figure 8 plots model performance versus model size on MCP-Universe benchmark. We observe that TOUCAN-tuned models push the Pareto frontier forward, achieving higher OSR at smaller model sizes, indicating that TOUCAN can help models achieve superior performance-efficiency trade-offs in agentic tasks."
        },
        {
            "title": "4.3 ABLATION ANALYSIS",
            "content": "To validate our extension designs, we perform ablation analysis on the Qwen2.5-14B-Instruct model, where we fine-tune on progressively extended versions of TOUCAN, allowing us to isolate the contributions of each extension described in Section 3.2. The experimental results are shown in Figrue 9. We observe that all components contribute to improved scores. Detailed benchmark scores for the BFCL ablation study are provided in Appendix C.3."
        },
        {
            "title": "5 CONCLUSION AND FUTURE WORK",
            "content": "Figure 9: This table shows ablation analysis of TOUCAN extensions. BFCLv3 τ -bench Airline @1 Retail @1 Qwen2.5-14B-Instruct + Single Turn + Irrelevance + Diversify + Multi-Turn 57.69% 60.16% 64.74% 64.56% 65.09% 17.25% 15.50% 16.75% 17.25% 22.00% 44.46% 36.95% 41.63% 43.70% 48.48% This paper introduces TOUCAN , tool-agentic dataset containing 1.5M trajectories designed to train better agentic models. We propose comprehensive pipeline for data generation and demonstrate that models fine-tuned on TOUCAN achieve superior performance on benchmarks including BFCLV3 and MCP-Universe. TOUCAN represents the first step in long-term effort to leverage tool use for building stronger LLM agents. Despite being valuable contribution, we acknowledge our work exhibits certain limitations, which we plan to address through different initiatives. Expanding to More MCP Servers. While our dataset is comprehensive, it was collected in June 2025, and new servers continue to emerge. We excluded MCP servers requiring special configurations (e.g., requires API keys or account setups), which simplifies the onboarding procedure but may overlook important servers and widely-used scenarios (e.g., Notion and GitHub). Manually onboarding more servers or developing automated onboarding agents could be valuable future work. Expert models to simulate tool-responses. While real tool execution produces higher-quality results, it is often slow and costly, and therefore, not an option for everyone. To provide an alternative that also yields quality, we plan to develop an expert LLM capable of simulating tool execution. This artificial component will significantly reduce the cost of generating trajectory data involving tool use. Although the idea of tool-execution simulation is known within the community, it has most likely been implemented using off-the-shelf, closed-source LLMs. MCP Benchmark for web search. As tool-use capabilities become central to both LLMs and LLM-agents, specific scenarios such as web search have gained prominence in the community as means of synthesizing complex reasoning tasks. To advance this direction, we plan to develop an MCP benchmark focused on web search capabilities."
        },
        {
            "title": "6 USE OF LARGE LANGUAGE MODELS (LLMS)",
            "content": "In our work, we used large language models (LLMs) to assist with improving the grammar, clarity, and overall readability of the manuscript, as well as to help generate the pipeline diagram included in 9 TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments the paper. All LLM-generated content was thoroughly verified by the authors as part of an iterative process to ensure accuracy, quality, and consistency with the scientific contributions of the work."
        },
        {
            "title": "7 ETHICS STATEMENT",
            "content": "Developers planning to use Toucan for LLM fine-tuning should take into account certain considerations. Data Ownership and Licensing. The MCP server specification files used to build TOUCAN were collected in June 2025 from https://smithery.ai/, public platform hosting such specifications. These files were voluntarily published by their owners in accordance with the platforms privacy notice. Given the case legitimate owner requests removal of their content from our dataset, we will honor that request through take down process available via our GitHub repository. Sensitive Information. The risk of exposing sensitive data in specification files is minimal, as they generally rely on placeholders rather than real information. However, human error may still lead to the inclusion of URLs, tokens, or email addresses. To mitigate this, we apply pre-filtering stage with rule-based verifiers that detect common patterns of personally identifiable information (PII). Data Evolution. Our data were collected in June 2025, so TOUCAN captures real-world tool-use scenarios available at that time. For example, responses from search MCP servers reflect information current through June 2025. To facilitate future updates and customization, we provide our modular data pipeline, allowing researchers and practitioners to expand domain coverage and tailor tool representations for their applications. LLM Hallucinations. Only tasks and annotations in TOUCAN were generated with LLMs; trajectories were produced using LLMs in combination with agent frameworks and remote MCP servers. This integration ensures reliable tool call executions and responses, reducing the likelihood of code errors from hallucinations. Nevertheless, hallucinations remain general risk when using LLMs, and outputs from models fine-tuned with TOUCAN should always be verified by humans."
        },
        {
            "title": "8 REPRODUCIBILITY STATEMENT",
            "content": "We provide the code for our data generation pipeline, along with detailed instructions for executing the pipeline end-to-end, as well as sample dataset files in the supplementary materials. The main paper and appendix further document key implementation details, including prompt templates, hyperparameter configurations used during fine-tuning, and extensions of our data analysis and finetuning experiments. After publication, we plan to release the full codebase in public GitHub repository and make our datasets publicly available on the HuggingFace platform."
        },
        {
            "title": "REFERENCES",
            "content": "Anthropic. Introducing the model context protocol. https://www.anthropic.com/news/ model-context-protocol, 2025. Accessed: 2025-08-18. Victor Barres, Honghua Dong, Soham Ray, Xujie Si, and Karthik Narasimhan. τ 2-bench: Evaluating conversational agents in dual-control environment, 2025. URL https://arxiv.org/abs/ 2506.07982. Kinjal Basu, Ibrahim Abdelaziz, Subhajit Chaudhury, Soham Dan, Maxwell Crouse, Asim Munawar, Sadhana Kumaravel, Vinod Muthusamy, Pavan Kapanipathi, and Luis A. Lastras. Apiblend: comprehensive corpora for training and benchmarking api llms, 2024. URL https: //arxiv.org/abs/2402.15491. Fouad Bousetouane. Agentic systems: guide to transforming industries with vertical ai agents, 2025. URL https://arxiv.org/abs/2501.00881. Chen Chen, Xinlong Hao, Weiwen Liu, Xu Huang, Xingshan Zeng, Shuai Yu, Dexun Li, Shuai Wang, Weinan Gan, Yuefeng Huang, Wulong Liu, Xinzhi Wang, Defu Lian, Baoqun Yin, Yasheng Wang, and Wu Liu. ACEBench: Who Wins the Match Point in Tool Usage?, July 2025. URL http://arxiv.org/abs/2501.12851. arXiv:2501.12851 [cs]. 10 TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wanjia Zhao, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaokang Zhang, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xinnan Song, Xinxia Shan, Xinyi Zhou, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. X. Zhu, Yang Zhang, Yanhong Xu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Yu, Yi Zheng, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Ying Tang, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yu Wu, Yuan Ou, Yuchen Zhu, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yukun Zha, Yunfan Xiong, Yunxian Ma, Yuting Yan, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Z. F. Wu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong Shao, Zhipeng Xu, Zhiyu Wu, Zhongyu Zhang, Zhuoshu Li, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Ziyi Gao, and Zizheng Pan. Deepseek-v3 technical report, 2025. URL https://arxiv.org/abs/2412.19437. Mohamed Amine Ferrag, Norbert Tihanyi, and Merouane Debbah. From llm reasoning to autonomous ai agents: comprehensive review, 2025. URL https://arxiv.org/abs/ 2504.19678. Xuanqi Gao, Siyi Xie, Juan Zhai, Shqing Ma, and Chao Shen. Mcp-radar: multi-dimensional benchmark for evaluating tool use capabilities in large language models, 2025. URL https: //arxiv.org/abs/2505.16700. Zhen Guo, Adriana Meza Soria, Wei Sun, Yikang Shen, and Rameswar Panda. Api pack: massive multi-programming language dataset for api call generation, 2025a. URL https://arxiv. org/abs/2402.09615. Zikang Guo, Benfeng Xu, Chiwei Zhu, Wentao Hong, Xiaorui Wang, and Zhendong Mao. Mcpagentbench: Evaluating real-world language agent performance with mcp-mediated tools, 2025b. URL https://arxiv.org/abs/2509.09734. Teknium interstellarninja. Hermes function calling dataset v1. URL https://huggingface. co/NousResearch/hermes-function-calling-v1. X. Li, S. Wang, S. Zeng, et al. survey on llm-based multi-agent systems: workflow, infrastructure, and challenges. Vicinagearth, 1:9, 2024. doi: 10.1007/s44336-024-00009-2. URL https: //doi.org/10.1007/s44336-024-00009-2. Weiwen Liu, Xu Huang, Xingshan Zeng, Xinlong Hao, Shuai Yu, Dexun Li, Shuai Wang, Weinan Gan, Zhengying Liu, Yuanqing Yu, Zezhong Wang, Yuxian Wang, Wu Ning, Yutai Hou, Bin Wang, Chuhan Wu, Xinzhi Wang, Yong Liu, Yasheng Wang, Duyu Tang, Dandan Tu, Lifeng Shang, Xin Jiang, Ruiming Tang, Defu Lian, Qun Liu, and Enhong Chen. Toolace: Winning the points of llm function calling, 2025a. URL https://arxiv.org/abs/2409.00920. Zhiwei Liu, Jielin Qiu, Shiyu Wang, Jianguo Zhang, Zuxin Liu, Roshan Ram, Haolin Chen, Weiran Yao, Shelby Heinecke, Silvio Savarese, Huan Wang, and Caiming Xiong. Mcpeval: Automatic 11 TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments mcp-based deep evaluation for ai agent models, 2025b. URL https://arxiv.org/abs/ 2507.12806. Zuxin Liu, Thai Hoang, Jianguo Zhang, Ming Zhu, Tian Lan, Shirley Kokane, Juntao Tan, Weiran Yao, Zhiwei Liu, Yihao Feng, Rithesh Murthy, Liangwei Yang, Silvio Savarese, Juan Carlos Niebles, Huan Wang, Shelby Heinecke, and Caiming Xiong. Apigen: Automated pipeline for generating verifiable and diverse function-calling datasets, 2024. URL https://arxiv.org/ abs/2406.18518. Ziyang Luo, Zhiqi Shen, Wenzhuo Yang, Zirui Zhao, Prathyusha Jwalapuram, Amrita Saha, Doyen Sahoo, Silvio Savarese, Caiming Xiong, and Junnan Li. Mcp-universe: Benchmarking large language models with real-world model context protocol servers, 2025. URL https: //arxiv.org/abs/2508.14704. Leland McInnes and John Healy. Umap: Uniform manifold approximation and projection for dimension reduction. ArXiv, abs/1802.03426, 2018. URL https://api.semanticscholar. org/CorpusID:3641284. Dhruv Nathawani, Sunil Mahabaleshwarkar, Training-Dataset-v1, 2025. Nemotron-Post-Training-Dataset-v1. Igor Gitman, , Somshubra Majumdar, Evelina Bakhturina, Ameya Nemotron-PostJian Zhang, and Jane Polak Scowcroft. URL https://huggingface.co/datasets/nvidia/ Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large language model connected with massive apis, 2023. URL https://arxiv.org/abs/2305.15334. Shishir G. Patil, Huanzhi Mao, Charlie Cheng-Jie Ji, Fanjia Yan, Vishnu Suresh, Ion Stoica, and Joseph E. Gonzalez. The berkeley function calling leaderboard (bfcl): From tool use to agentic evaluation of large language models. In Forty-second International Conference on Machine Learning, 2025. Akshara Prabhakar, Zuxin Liu, Ming Zhu, Jianguo Zhang, Tulika Awalgaonkar, Shiyu Wang, Zhiwei Liu, Haolin Chen, Thai Hoang, Juan Carlos Niebles, Shelby Heinecke, Weiran Yao, Huan Wang, Silvio Savarese, and Caiming Xiong. Apigen-mt: Agentic pipeline for multi-turn data generation via simulated agent-human interplay, 2025. URL https://arxiv.org/abs/ 2504.03601. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language models to master 16000+ real-world apis, 2023. URL https://arxiv.org/abs/2307.16789. Donghao Ren, Fred Hohman, and Dominik Moritz. scalable approach to clustering embedding projections, 2025. URL https://arxiv.org/abs/2504.07285. Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, Boxi Cao, and Le Sun. ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases, September 2023. URL http://arxiv.org/abs/2306.05301. arXiv:2306.05301 [cs]."
        },
        {
            "title": "5 Team, Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang\nWang, Da Yin, Hao Zeng, Jiajie Zhang, Kedong Wang, Lucen Zhong, Mingdao Liu, Rui Lu,\nShulin Cao, Xiaohan Zhang, Xuancheng Huang, Yao Wei, Yean Cheng, Yifan An, Yilin Niu,\nYuanhao Wen, Yushi Bai, Zhengxiao Du, Zihan Wang, Zilin Zhu, Bohan Zhang, Bosi Wen,\nBowen Wu, Bowen Xu, Can Huang, Casey Zhao, Changpeng Cai, Chao Yu, Chen Li, Chendi\nGe, Chenghua Huang, Chenhui Zhang, Chenxi Xu, Chenzheng Zhu, Chuang Li, Congfeng Yin,\nDaoyan Lin, Dayong Yang, Dazhi Jiang, Ding Ai, Erle Zhu, Fei Wang, Gengzheng Pan, Guo\nWang, Hailong Sun, Haitao Li, Haiyang Li, Haiyi Hu, Hanyu Zhang, Hao Peng, Hao Tai, Haoke\nZhang, Haoran Wang, Haoyu Yang, He Liu, He Zhao, Hongwei Liu, Hongxi Yan, Huan Liu, Hui-\nlong Chen, Ji Li, Jiajing Zhao, Jiamin Ren, Jian Jiao, Jiani Zhao, Jianyang Yan, Jiaqi Wang, Jiayi\nGui, Jiayue Zhao, Jie Liu, Jijie Li, Jing Li, Jing Lu, Jingsen Wang, Jingwei Yuan, Jingxuan Li,\nJingzhao Du, Jinhua Du, Jinxin Liu, Junkai Zhi, Junli Gao, Ke Wang, Lekang Yang, Liang Xu, Lin\nFan, Lindong Wu, Lintao Ding, Lu Wang, Man Zhang, Minghao Li, Minghuan Xu, Mingming",
            "content": "12 TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments Zhao, Mingshu Zhai, Pengfan Du, Qian Dong, Shangde Lei, Shangqing Tu, Shangtong Yang, Shaoyou Lu, Shijie Li, Shuang Li, Shuang-Li, Shuxun Yang, Sibo Yi, Tianshu Yu, Wei Tian, Weihan Wang, Wenbo Yu, Weng Lam Tam, Wenjie Liang, Wentao Liu, Xiao Wang, Xiaohan Jia, Xiaotao Gu, Xiaoying Ling, Xin Wang, Xing Fan, Xingru Pan, Xinyuan Zhang, Xinze Zhang, Xiuqing Fu, Xunkai Zhang, Yabo Xu, Yandong Wu, Yida Lu, Yidong Wang, Yilin Zhou, Yiming Pan, Ying Zhang, Yingli Wang, Yingru Li, Yinpei Su, Yipeng Geng, Yitong Zhu, Yongkun Yang, Yuhang Li, Yuhao Wu, Yujiang Li, Yunan Liu, Yunqing Wang, Yuntao Li, Yuxuan Zhang, Zezhen Liu, Zhen Yang, Zhengda Zhou, Zhongpei Qiao, Zhuoer Feng, Zhuorui Liu, Zichen Zhang, Zihan Wang, Zijun Yao, Zikang Wang, Ziqiang Liu, Ziwei Chai, Zixuan Li, Zuodong Zhao, Wenguang Chen, Jidong Zhai, Bin Xu, Minlie Huang, Hongning Wang, Juanzi Li, Yuxiao Dong, and Jie Tang. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models, 2025a. URL https://arxiv.org/abs/2508.06471. Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, Zhuofu Chen, Jialei Cui, Hao Ding, Mengnan Dong, Angang Du, Chenzhuang Du, Dikang Du, Yulun Du, Yu Fan, Yichen Feng, Kelin Fu, Bofei Gao, Hongcheng Gao, Peizhong Gao, Tong Gao, Xinran Gu, Longyu Guan, Haiqing Guo, Jianhang Guo, Hao Hu, Xiaoru Hao, Tianhong He, Weiran He, Wenyang He, Chao Hong, Yangyang Hu, Zhenxing Hu, Weixiao Huang, Zhiqi Huang, Zihao Huang, Tao Jiang, Zhejun Jiang, Xinyi Jin, Yongsheng Kang, Guokun Lai, Cheng Li, Fang Li, Haoyang Li, Ming Li, Wentao Li, Yanhao Li, Yiwei Li, Zhaowei Li, Zheming Li, Hongzhan Lin, Xiaohan Lin, Zongyu Lin, Chengyin Liu, Chenyu Liu, Hongzhang Liu, Jingyuan Liu, Junqi Liu, Liang Liu, Shaowei Liu, T. Y. Liu, Tianwei Liu, Weizhou Liu, Yangyang Liu, Yibo Liu, Yiping Liu, Yue Liu, Zhengying Liu, Enzhe Lu, Lijun Lu, Shengling Ma, Xinyu Ma, Yingwei Ma, Shaoguang Mao, Jie Mei, Xin Men, Yibo Miao, Siyuan Pan, Yebo Peng, Ruoyu Qin, Bowen Qu, Zeyu Shang, Lidong Shi, Shengyuan Shi, Feifan Song, Jianlin Su, Zhengyuan Su, Xinjie Sun, Flood Sung, Heyi Tang, Jiawen Tao, Qifeng Teng, Chensi Wang, Dinglu Wang, Feng Wang, Haiming Wang, Jianzhou Wang, Jiaxing Wang, Jinhong Wang, Shengjie Wang, Shuyi Wang, Yao Wang, Yejie Wang, Yiqin Wang, Yuxin Wang, Yuzhi Wang, Zhaoji Wang, Zhengtao Wang, Zhexu Wang, Chu Wei, Qianqian Wei, Wenhao Wu, Xingzhe Wu, Yuxin Wu, Chenjun Xiao, Xiaotong Xie, Weimin Xiong, Boyu Xu, Jing Xu, Jinjing Xu, L. H. Xu, Lin Xu, Suting Xu, Weixin Xu, Xinran Xu, Yangchuan Xu, Ziyao Xu, Junjie Yan, Yuzi Yan, Xiaofei Yang, Ying Yang, Zhen Yang, Zhilin Yang, Zonghan Yang, Haotian Yao, Xingcheng Yao, Wenjie Ye, Zhuorui Ye, Bohong Yin, Longhui Yu, Enming Yuan, Hongbang Yuan, Mengjie Yuan, Haobing Zhan, Dehao Zhang, Hao Zhang, Wanlu Zhang, Xiaobin Zhang, Yangkun Zhang, Yizhi Zhang, Yongting Zhang, Yu Zhang, Yutao Zhang, Yutong Zhang, Zheng Zhang, Haotian Zhao, Yikai Zhao, Huabin Zheng, Shaojie Zheng, Jianren Zhou, Xinyu Zhou, Zaida Zhou, Zhen Zhu, Weiyu Zhuang, and Xinxing Zu. Kimi k2: Open agentic intelligence, 2025b. URL https://arxiv.org/abs/2507.20534. Qwen Team. Qwen2.5: party of foundation models, September 2024. URL https://qwenlm. github.io/blog/qwen2.5/. The MCPMark Team. Mcpmark: Stress-testing comprehensive mcp use. https://github. com/eval-sys/mcpmark, 2025a. The Scale Research Team. Actions, not words: Mcp-atlas raises the bar for agentic evaluation. https://scale.com/blog/mcp-atlas, September 2025b. Accessed: YYYY-MM-DD. Zhenting Wang, Qi Chang, Hemani Patel, Shashank Biju, Cheng-En Wu, Quan Liu, Aolin Ding, Alireza Rezazadeh, Ankit Shah, Yujia Bao, and Eugene Siow. Mcp-bench: Benchmarking toolusing llm agents with complex real-world tasks via mcp servers, 2025. URL https://arxiv. org/abs/2508.20453. Weikai Xu, Chengrui Huang, Shen Gao, and Shuo Shang. Llm-based agents for tool learning: survey. Data Science and Engineering, 2025a. doi: 10.1007/s41019-025-00296-9. URL https://link.springer.com/article/10.1007/s41019-025-00296-9. Zhangchen Xu, Yang Liu, Yueqin Yin, Mingyuan Zhou, and Radha Poovendran. Kodcode: diverse, challenging, and verifiable synthetic dataset for coding. ArXiv, abs/2503.02951, 2025b. URL https://api.semanticscholar.org/CorpusID:276782338. 13 TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments Yunhe Yan, Shihe Wang, Jiajun Du, Yexuan Yang, Yuxuan Shan, Qichen Qiu, Xianqing Jia, Xinge Wang, Xin Yuan, Xu Han, Mao Qin, Yinxiao Chen, Chen Peng, Shangguang Wang, and Mengwei Xu. Mcpworld: unified benchmarking testbed for api, gui, and hybrid computer use agents, 2025. URL https://arxiv.org/abs/2506.07672. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. τ -bench: benchmark for tool-agent-user interaction in real-world domains, 2024. URL https://arxiv.org/abs/ 2406.12045. Ming Yin, Dinghan Shen, Silei Xu, Jianbing Han, Sixun Dong, Mian Zhang, Yebowen Hu, Shujian Liu, Simin Ma, Song Wang, et al. Livemcp-101: Stress testing and diagnosing mcp-enabled agents on challenging queries. arXiv preprint arXiv:2508.15760, 2025. 14 TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments"
        },
        {
            "title": "A DATASET SCHEMA AND EXAMPLES",
            "content": "An instance of TOUCAN contains the following columns: uuid: Unique sample identifier. subset: Annotation specifying which pipeline was used to generate the trajectory. Options: (1) single-turn-original: only the core processing (Stage 1 to 5) described in Section 3 are applied, (2) irrelevant: server shuffle process applied on top of the single-turn-original pipeline, (3) single-turn-diversify: question diversification process applied on top of the single-turn-original pipeline, and (4) multi-turn: multi-turn extension of the single-turnoriginal and single-turn-diversify subsets. messages: The trajectory formatted with the chat template from the original LLM-agent used for generation. The system prompt includes the associated list of tools. question: The user task crafted to generate the trajectory. target tools: The MCP tools used as seeds for question generation. question quality assessment: Task evaluation by an LLM-as-judge, covering quality, difficulty, realism, and uniqueness. response quality assessment: Response evaluation by an LLM-as-judge, covering completeness and conciseness. message num rounds: Total number of messages, including turns of all types. metadata: Original MCP server data collected and used as seed for generation, as well as respective LLM annotations. This is the structure of an instance in TOUCAN : { } (cid:44) (cid:44) \"uuid\": \"3ac8fdcc-b9b5-50d2-a840-947a42b558d2\", \"subset\": \"single-turn-original\", \"messages\": \"[{...long JSON string of messages...}]\", \"question\": \"I'm reviewing my company's policy document that needs quarterly updates for compliance purposes. need to first (cid:44) understand its current structure with all sections and subsections, then update every occurrence of 'Q3 2024' to 'Q1 2025' throughout the document, and finally add explanatory footnotes to the last 3 sections that reference our new regulatory guidance document published last month. Can you help me accomplish these updates systematically?\", (cid:44) \"target_tools\": \"get_document_outline, search_and_replace, (cid:44) \"question_quality_assessment_kimik2w4a16\": { add_footnote_to_document\", (cid:44) (cid:44) (cid:44) \"tool_selection_difficulty\": { \"reasoning\": \"...\", \"score\": 3 }, \"tool_selection_uniqueness\": { \"reasoning\": \"...\", \"score\": 3 }, \"question_quality\": { \"reasoning\": \"...\", \"score\": 4 }, \"scenario_realism\": { \"reasoning\": \"...\", \"score\": 5 }, \"verifiable\": { \"reasoning\": \"...\", \"score\": 4 }, \"stability\": { \"reasoning\": \"...\", \"score\": 5 }, \"overall_score\": 4.0 }, \"response_quality_assessment_oss120b\": { \"completeness\": { \"reasoning\": \"...\", \"score\": 2 }, \"conciseness\": { \"reasoning\": \"...\", \"score\": 3 }, \"overall_score\": 2.5, \"desired_tools_used_percentage\": 0.3333, \"order_correctness\": false }, \"messages_num_rounds\": 9, \"metadata\": \"{...long metadata JSON string...}\" TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments Below, we present TOUCAN trajectory, chat template tokens from the model generator and emojis have been removed, and some sections (e.g., tool list, tool output, assistant response) have been abbreviated to improve readability. [ { \"role\": \"system\", \"content\": \"{... contains the list of tools ... }\" }, { \"role\": \"user\", \"content\": \"Can you explain what ENS (Ethereum Name Service) (cid:44) represents in the context of transaction and why name like 'blockscout.eth' might appear instead of raw address?\" (cid:44) }, { \"role\": \"assistant\", \"content\": \"I'll explain what ENS (Ethereum Name Service) represents, and show you how it resolves name like 'blockscout.eth' into (cid:44) its actual Ethereum address.\" (cid:44) }, { \"role\": \"assistant\", \"content\": \"\", \"function_call\": { \"name\": \"blockscout-mcp-server-__unlock_blockchain_analysis__\", \"arguments\": \"{}\" } }, { \"role\": \"function\", \"content\": \"{... long tool output ...}\" }, { \"role\": \"assistant\", \"content\": \"Now, let me resolve 'blockscout.eth' to show the actual (cid:44) address it represents.\" }, { \"role\": \"assistant\", \"content\": \"\", \"function_call\": { \"name\": \"blockscout-mcp-server-get_address_by_ens_name\", \"arguments\": \"{\"name\": \"blockscout.eth\"}\" } }, { \"role\": \"function\", \"name\": \"blockscout-mcp-server-get_address_by_ens_name\", \"content\": \"{n (cid:44) \"0x441e1e47a6Fa2DBfD3cd9b54291E9AB3a58D7975\"n \"data_description\": null,n \"instructions\": null,n \"pagination\": nulln}\" \"resolved_address\": \"notes\": null,n \"data\": {n },n (cid:44) (cid:44) }, { \"role\": \"assistant\", \"content\": \"{... parsed assistant response ...}\" }, { \"role\": \"user\", \"content\": \"Based on that explanation, could you look up the ENS name 'blockscout.eth' and provide the Ethereum address it resolves (cid:44) to?\" (cid:44) }, { \"role\": \"assistant\", 16 TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments \"content\": \"I already resolved that for you in my previous response! When called the ENS lookup tool, `\"blockscout.eth\"` resolved (cid:44) to the Ethereum address:nn`0x441e1e47a6Fa2DBfD3cd9b54291E9AB3a58D7975`\" (cid:44) (cid:44) } ] In the previous trajectory example, we abbreviated the assistants parsed response for readability. The full response is shown below. ## What ENS (Ethereum Name Service) Represents in Transactions ENS stands for **Ethereum Name Service**, and it functions much like \"DNS (cid:44) for Ethereum addresses.\" It allows users to register human-readable names (like \"blockscout.eth\") that map to complex hexadecimal blockchain addresses. (cid:44) (cid:44) ### Key Points About ENS: 1. **Human-Readable Names for Complex Addresses** - Instead of displaying `0x441e1e47a6Fa2DBfD3cd9b54291E9AB3a58D7975` (cid:44) (which is the raw address for blockscout.eth), interfaces show the friendly \"blockscout.eth\" (cid:44) - This reduces errors from copy/pasting long hex addresses 2. **Reverse Resolution** - When you see \"blockscout.eth\" in transaction data, wallet labels, or (cid:44) block explorers, it means: - ENS has been configured with this name - The system has performed reverse lookup to find the name (cid:44) associated with the address 3. **Self-Sovereign Identity** - ENS names are owned and controlled by users through smart contracts - They can be transferred, updated, or associated with additional (cid:44) metadata (social profiles, avatars, etc.) ### Why \"blockscout.eth\" Appears Instead of Raw Address: memorable legitimate Blockscout entity rather than random address - **User Experience**: \"blockscout.eth\" is immediately recognizable and (cid:44) - **Trust Verification**: Users can verify they're interacting with the (cid:44) - **Branding**: Organizations use ENS names for consistent branding (cid:44) - **Error Prevention**: Reduces costly mistakes when users verify (cid:44) across DeFi protocols and wallet interfaces transaction recipients Every time you see an ENS name like \"blockscout.eth\" in transaction, the underlying system has actually resolved this to the precise (cid:44) blockchain address `0x441e1e47a6Fa2DBfD3cd9b54291E9AB3a58D7975`-the ENS simply provides human-friendly layer on top of the raw cryptographic identity. (cid:44) (cid:44) (cid:44) 1 2 3 4 5 7 8 9 10 11 13 14 15 16 17 19 20 21 22 23 25 26 27 17 TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments"
        },
        {
            "title": "B MORE ON DATASET ANALYSIS",
            "content": "B.1 FEATURED SERVERS IN TOUCAN Table 4 demonstrates featured servers of TOUCAN. We note that these featured servers span wide range of domains, making combinations possible for complex tool use tasks. Server Name Repository Name Domain Tools Count esshka/okx-mcp OKX Server AI Research Assistant - Semantic Scholar Access via Smithery 2 Access via Smithery 3 Book Search Server JackKuo666/PubMed-MCP-Server PubMed MCP Server falahgs/flux-imagegen-mcp-server Flux ImageGen Server Pok Amcp NaveenBandarage/poke-mcp jinkoso/jinko-mcp Hotel Booking Server cloudflare/playwright-mcp Cloudflare Playwright yokingma/time-mcp Time MCP Server exa-labs/exa-mcp-server Exa Search iremaltunay55/deneme Weather Forecast Server alan5543/calculator-mcp Advanced Calculator Server Dictionary Server ceydasimsekk/dictionarymcp Airbnb Search and Listing Details Server AkekaratP/mcp-server-airbnb Code Runner MCP Server Movie Recommender United States Weather Context7 Think Tool Server OpenAPI MCP Server Film Information Server Trends Hub ClinicalTrials MCP Server Drawing Tool for AI Assistants LeetCode formulahendry/mcp-server-code-runner iremert/movie-recommender-mcp smithery-ai/mcp-servers upstash/context7-mcp PhillipRt/think-mcp-server janwilmake/openapi-mcp-server zehranurugurr/film mcp baranwang/mcp-trends-hub JackKuo666/ClinicalTrials-MCP-Server Health & Fitness Content Creation flrngel/mcp-painter Development Tools jinzcdev/leetcode-mcp-server Cryptocurrency & Blockchain Web Search & Research Web Search & Research Web Search & Research AI/ML Tools Data Analysis & Processing E-commerce Browser Automation Time & Calendar Web Search & Research Weather Data Analysis & Processing Others Web Search & Research Development Tools Content Creation Weather Development Tools Memory Management API Integration Content Creation News & Media 2 10 1 4 3 4 6 24 6 8 5 17 1 2 1 1 6 2 1 2 1 21 7 4 9 Table 4: Featured Server Information B.2 MORE ON MCP SERVER ANALYSIS IN TOUCAN Figure 10 shows the distribution of the most frequently used MCP servers in our dataset, highlighting the diversity of servers and domains covered in TOUCAN. Figure 11 shows the distribution of tool counts across the 495 MCP servers employed by TOUCAN , revealing that most servers expose only limited number of tools, with the majority containing fewer than 10 tools. Figure 10: Distribution of the most frequently occurring MCP servers in the TOUCAN dataset. 18 TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments Figure 11: Tools Number distribution across MCP servers B.3 EMEDDING VISUALIZATION Figure 12 presents embedding visualization via Embedding Atlas (Ren et al., 2025) using the Xenova/multilingual-e5-small embedding model with UMAP projection McInnes & Healy (2018). The visualization demonstrates that TOUCAN covers wide range of topics. In addition, the proposed TOUCAN extensions (e.g., diversification) effectively increase the overall dataset coverage. Figure 12: This figure is the visualization of 50K random-sampled TOUCAN instances via Embedding Atlas (Ren et al., 2025). 19 TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments"
        },
        {
            "title": "C MORE ON EXPERIMENTS",
            "content": "C.1 LLM ANNOTATION Figure 13 shows the Pearson correlation between human annotators and LLM-as-a-Judge evaluations across different models, based on 50 randomly sampled instances. The annotation prompt is available in Appendix D.4. We observe that GPT-4.1 and Kimi-K2 achieve the highest overall correlation with human judgments. Considering cost efficiency, we deploy Kimi-K2 locally for our annotation pipeline. C.2 FINE-TUNING HYPER-PARAMETERS We fine-tune models with TOUCAN using super computing cluster, which is outfitted with NVIDIA H100 GPUs. The fine-tuning hyper-parameters can be found in Table 5. Figure 13: Pearson correlation between human annotators and LLM-as-a-Judge evaluations across different models. Table 5: This table shows the hyper-parameters for supervised fine-tuning. Hyper-parameter Value Tool-Call Template Learning Rate Number of Epochs Number of Devices Per-device Batch Size Gradient Accumulation Steps Effective Batch Size Optimizer Deepspeed Max Sequence Length Hermes 2 105 2 8 or 64 1 8 (8 GPUs) or 1 (64 GPUs) 64 Adamw with βs = (0.9, 0.999) and ϵ = 108 zero3 32768 C.3 MORE ON ABLATION STUDIES Table 6 details the individual scores of the BFCL V3 benchmark for our ablation analysis. We observe that all extensions are meaningful in improving model performance. Table 6: Ablation of TOUCAN Extensions on BFCL V3 Benchmark."
        },
        {
            "title": "Hallucination",
            "content": "Non-live (AST) Live (AST)"
        },
        {
            "title": "Irrelevance",
            "content": "Qwen2.5-14B-Instruct + Single Turn + Irrelevance + Diversify + Multi-Turn 57.69% 83.38% 60.16% 87.50% 64.74% 88.46% 64.56% 86.06% 65.09% 85.42% 73.70% 66.86% 77.25% 76.90% 76.01% 19.75% 34.38% 30.38% 32.50% 35.25% 83.33% 72.22% 72.22% 72.22% 72.22% 68.46% 46.88% 77.85% 75.45% 75.96% 20 TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments"
        },
        {
            "title": "D PROMPTS",
            "content": "D.1 MCP SERVER ANNOTATION PROMPT Below is the prompt for annotating MCP server categories. ## Task Generate **Server Labels** to categorize the provided MCP Server based on (cid:44) its description and available tools. ## Objective Analyze the provided MCP Server's description and available tools, then (cid:44) assign appropriate category labels that best describe its primary functionality and use cases. (cid:44) ## Guidelines ### Label Selection - Analyze the MCP Server's core functionality and purpose - Consider the types of tools it provides and the problems it solves - Select labels that accurately represent the server's primary use cases - Choose from predefined categories when applicable, but also consider (cid:44) custom labels for unique functionality operations management management note-taking interaction organization statistical analysis blockchain interaction information, academic research ### Predefined Categories Choose from these established categories when appropriate: - **Web Search & Research**: Tools for searching the web, gathering (cid:44) - **Browser Automation**: Web scraping, automated browsing, page (cid:44) - **Memory Management**: Data storage, retrieval, knowledge bases, (cid:44) - **Operating System**: File operations, system commands, process (cid:44) - **Data Analysis & Processing**: Analytics, data transformation, (cid:44) - **Cryptocurrency & Blockchain**: Trading, wallet management, DeFi, (cid:44) - **Daily Productivity**: Task management, scheduling, personal (cid:44) - **File Management**: File operations, document handling, storage (cid:44) - **Database Operations**: Data querying, database management, SQL (cid:44) - **API Integration**: Third-party service integration, webhook handling - **Communication Tools**: Messaging, email, notifications, social (cid:44) - **Development Tools**: Code analysis, debugging, version control, CI/CD - **Security & Authentication**: Password management, encryption, access (cid:44) - **Cloud Services**: Cloud platform integration, serverless functions - **AI/ML Tools**: Machine learning, model interaction, AI-powered (cid:44) - **Content Creation**: Writing, editing, media generation, publishing - **Social Media**: Social platform integration, posting, analytics - **Financial Services**: Banking, payments, financial data, accounting - **E-commerce**: Shopping, product management, order processing - **Gaming**: Game-related tools, entertainment, interactive features - **Education**: Learning tools, course management, educational content - **Health & Fitness**: Health monitoring, fitness tracking, medical (cid:44) - **Travel & Maps**: Location services, travel planning, navigation - **News & Media**: News aggregation, media consumption, journalism tools - **Weather**: Weather data, forecasting, climate information - **Time & Calendar**: Scheduling, time management, calendar integration interaction features control tools 1 2 3 4 5 7 8 9 10 11 13 14 15 16 17 19 20 21 22 23 25 26 27 28 29 31 32 33 34 35 37 38 39 40 41 21 TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments 43 44 45 47 48 49 50 51 53 54 55 56 57 59 60 61 62 63 65 66 67 68 69 71 72 73 74 75 77 78 79 1 2 4 5 6 7 custom label ### Custom Labels - If the server doesn't fit well into predefined categories, create (cid:44) - Custom labels should be descriptive and specific to the server's unique (cid:44) - Use clear, concise terminology that would be useful for clustering and (cid:44) functionality organization (from predefined list or custom) ### Output Requirements - **Primary Label**: The main category that best describes the server (cid:44) - **Secondary Labels**: Additional relevant categories (0-2 labels) - **Custom Label**: free-form descriptive label if the server has (cid:44) unique functionality not covered by predefined categories ## MCP Server Description {MCP_SERVER_NAME}: {MCP_SERVER_DESCRIPTION} Available Tools: {TOOL_LIST} ## Output Provide your response in the following XML format: <response> <analysis> <!-- Briefly analyze the MCP Server's core functionality and the types of problems it solves based on its description and (cid:44) available tools. --> (cid:44) </analysis> <reasoning> <!-- Brief explanation of why these labels were chosen and how they (cid:44) represent the server's functionality --> </reasoning> <primary_label> <!-- The main category that best describes this server's primary (cid:44) functionality --> </primary_label> <secondary_labels> <!-- Additional relevant categories (0-2 labels), separated by commas (cid:44) if multiple --> </secondary_labels> <custom_label> <!-- free-form descriptive label if the server has unique (cid:44) functionality not covered by predefined categories. Leave empty if not needed. --> (cid:44) </custom_label> </response> D.2 TASK GENERATION PROMPT Below is an example of task generation prompt for the single-server task synthesis. The prompt generates question targeting one tool. ## Task Generate **Tool Use Question** based on the provided MCP Server and its (cid:44) tool descriptions. ## Objective Analyze the provided MCP Server and its available tools, then create (cid:44) realistic user question that would naturally require the use of one of these tools to solve. (cid:44) ## Guidelines TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments need to interact with the MCP Server's tools ### Question Realism - Create questions that represent real-world scenarios where users would (cid:44) - The question should sound natural and authentic, as if asked by someone (cid:44) - Consider common use cases, problems, or workflows that would require (cid:44) the functionality provided by the MCP Server's tools genuinely needing to accomplish task appropriate to answer the question ### Tool Selection - Focus on **ONE specific tool** from the MCP Server that would be most (cid:44) - Choose tools based on the core functionality they provide and how they (cid:44) - Consider each tool's description and purpose when crafting the question would solve real user problems usage ### Question Complexity - Create questions that are clear and specific enough to warrant tool (cid:44) - Avoid overly simple questions that could be answered without tools - Include relevant context or constraints that make the tool usage (cid:44) - Do not contain the exact tool name in the question necessary and their main functionalities. ### Output Format Your response should include: 1. **Tool Analysis**: Briefly analyze the MCP Server's available tools (cid:44) 2. **Target Tool**: The specific tool name from the MCP Server that (cid:44) 3. **Question**: clear, realistic user question that requires tool (cid:44) should be used to answer this question. usage. ## MCP Server Description {MCP_SERVER_NAME}: {MCP_SERVER_DESCRIPTION} Available Tools: {TOOL_LIST} ## Output Provide your response in the following XML format: <response> <server_analysis> <!-- Briefly analyze the MCP Server's available tools and their main (cid:44) functionalities. --> </server_analysis> <target_tool> <!-- The specific tool name from the MCP Server that should be used (cid:44) to answer this question. --> </target_tool> <question> <!-- clear, realistic user question that requires tool usage. --> </question> </response> Below is an example of task generation prompt for the single-server task synthesis. The prompt generates question targeting multiple tools. ## Task Generate **Tool Use Question** based on the provided MCP Server and its (cid:44) tool descriptions. ## Objective 23 8 10 11 12 13 14 16 17 18 19 20 22 23 24 25 26 28 29 30 31 32 34 35 36 37 38 40 41 42 43 44 46 47 48 49 50 2 3 4 TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments Analyze the provided MCP Server and its available tools, then create realistic user question that would naturally require the use of (cid:44) **{NUM_TOOLS} tools** from this MCP Server to solve completely. (cid:44) ## Guidelines need to interact with the MCP Server's tools ### Question Realism - Create questions that represent real-world scenarios where users would (cid:44) - The question should sound natural and authentic, as if asked by someone (cid:44) - Consider common use cases, problems, or workflows that would require (cid:44) the functionality provided by the MCP Server's tools genuinely needing to accomplish task together to answer the question ### Tool Selection - Focus on **{NUM_TOOLS} tools** from the MCP Server that would work (cid:44) - The question should require sequence or combination of tool calls to (cid:44) - Choose tools based on how they complement each other and create (cid:44) - Consider each tool's description and purpose when crafting the question (cid:44) that requires multiple steps logical workflow solve completely tools to solve ### Question Complexity - Create questions that are complex enough to warrant using {NUM_TOOLS} (cid:44) - The question should have multiple components or require several steps (cid:44) - Include relevant context or constraints that make the multi-tool usage (cid:44) - Do not contain the exact tool names in the question - Ensure the question cannot be reasonably answered with just single (cid:44) necessary tool and their main functionalities. ### Output Format Your response should include: 1. **Tool Analysis**: Briefly analyze the MCP Server's available tools (cid:44) 2. **Target Tools**: The specific tool names from the MCP Server that (cid:44) should be used together to answer this question, in the order they would likely be called. (cid:44) 3. **Question**: clear, realistic user question that requires multiple (cid:44) tool usage. ## MCP Server Description {MCP_SERVER_NAME}: {MCP_SERVER_DESCRIPTION} Available Tools: {TOOL_LIST} ## Output Ensure your question requires exactly {NUM_TOOLS} tools to solve (cid:44) completely. Provide your response in the following XML format: <response> <server_analysis> <!-- Briefly analyze the MCP Server's available tools and their main (cid:44) functionalities. --> </server_analysis> <target_tools> <!-- The specific tool names from the MCP Server that should be used (cid:44) together to answer this question, listed in order. e.g., <tool>create_twitter_post</tool> <tool>get_last_tweet</tool> --> (cid:44) </target_tools> <question> 24 5 7 8 9 10 11 13 14 15 16 17 19 20 21 22 23 25 26 27 28 29 31 32 33 34 35 37 38 39 40 41 43 44 45 46 47 49 TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments <!-- clear, realistic user question that requires multiple tool (cid:44) usage. --> </question> </response> Below is an example of task generation prompt for the multi-server task synthesis. ## Task Generate **Multi-Server Tool Use Question** based on the provided MCP (cid:44) Servers and their tool descriptions. ## Objective Analyze the provided MCP Servers and their available tools, then create (cid:44) realistic user question that would naturally require the use of **{NUM_TOOLS} tools from at least 2 different MCP servers** to solve completely. (cid:44) (cid:44) ## Guidelines need to interact with tools from multiple MCP Servers ### Question Realism - Create questions that represent real-world scenarios where users would (cid:44) - The question should sound natural and authentic, as if asked by someone (cid:44) - Consider workflows that span across different services/domains that (cid:44) - Think about how different MCP servers complement each other in (cid:44) genuinely needing to accomplish complex task would require multiple servers real-world use cases servers question ### Server and Tool Selection - Use tools from **at least 2 different MCP servers** to answer the (cid:44) - Select **{NUM_TOOLS} tools total** that work together across multiple (cid:44) - The question should require sequence or combination of tool calls (cid:44) - Choose tools based on how they complement each other across different (cid:44) - Consider each tool's description and purpose when crafting the (cid:44) - Ensure tools from different servers create logical, interconnected (cid:44) from different servers to solve completely cross-server workflow services/domains workflow that span different services tools across multiple servers ### Question Complexity - Create questions that are complex enough to warrant using {NUM_TOOLS} (cid:44) - The question should have multiple components or require several steps (cid:44) - Include relevant context or constraints that make the multi-server tool (cid:44) - Do not contain the exact tool names or server names in the question - Ensure the question cannot be reasonably answered with tools from just (cid:44) - Create scenarios that naturally require different types of services (cid:44) usage necessary single server working together ### Cross-Server Integration - Think about how different servers' capabilities can be combined - Consider data flow between different services (e.g., retrieving data (cid:44) - Create realistic scenarios where multiple services need to work (cid:44) - Focus on complementary functionalities across different domains from one service to use in another) together ### Output Format Your response should include: 25 50 51 52 1 3 4 5 6 7 9 10 11 12 13 15 16 17 18 19 21 22 23 24 25 27 28 29 30 31 33 34 35 36 37 TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments 1. **Server Analysis**: Briefly analyze all MCP Servers and their available tools, focusing on how they can work together. (cid:44) 2. **Cross-Server Workflow**: Describe the workflow showing how tools (cid:44) 3. **Target Tools**: The specific tool names from different MCP Servers that should be used together, in the order they would likely be (cid:44) called, with their server names. from different servers will be used together. (cid:44) 4. **Question**: clear, realistic user question that requires (cid:44) multi-server tool usage. ## Available MCP Servers {SERVER_DESCRIPTIONS} ## Output Ensure your question requires exactly {NUM_TOOLS} tools from at least 2 different servers to solve completely. Provide your response in the (cid:44) following XML format: (cid:44) <response> <server_analysis> <!-- Briefly analyze all MCP Servers and their available tools, focusing on how they can work together across different (cid:44) domains/services. --> (cid:44) </server_analysis> <cross_server_workflow> <!-- Describe the workflow showing how tools from different servers (cid:44) will be used together to solve the question. --> </cross_server_workflow> <target_tools> <!-- The specific tool names from different MCP Servers that should (cid:44) be used together, listed in order with their server names. e.g., <tool server=\"Server1\">search_posts</tool> <tool server=\"Server2\">send_email</tool> --> (cid:44) (cid:44) </target_tools> <question> <!-- clear, realistic user question that requires multi-server tool (cid:44) usage spanning different services/domains. --> </question> </response> Below is an example of task generation prompt for the task synthesis for featured servers. ## Task Generate **Multi-Server Tool Use Question** based on featured MCP (cid:44) Servers and their tool descriptions. ## Objective Brainstorm compelling real-world scenario, then analyze the provided (cid:44) featured MCP Servers and their available tools to create realistic user question that would naturally require the use of **{NUM_TOOLS} tools from at least 2 different MCP servers** to solve completely. (cid:44) (cid:44) ## Guidelines ### Scenario Brainstorming - Think of realistic, specific scenarios where someone would need to use (cid:44) {NUM_TOOLS} different tools across multiple servers to accomplish meaningful task (cid:44) - Consider diverse real-world contexts such as: platforms - Content creators managing their online presence across different (cid:44) - Researchers gathering and analyzing information from multiple sources - Developers building and deploying applications using different (cid:44) services 39 41 42 43 44 45 47 48 49 50 51 53 54 55 56 57 59 60 61 62 63 1 2 3 4 5 7 8 9 10 11 13 14 26 TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments platforms - Business professionals managing projects and communications across (cid:44) - Students working on complex assignments requiring multiple tools - Entrepreneurs launching new ventures using various services - The scenario should be detailed and authentic, representing genuine use (cid:44) cases that span multiple services with specific goal genuinely need tools from multiple MCP servers ### Question Realism - Create questions that represent real-world scenarios where users would (cid:44) - The question should sound natural and authentic, as if asked by someone (cid:44) - Include relevant context, constraints, and details that make the (cid:44) - Consider workflows that require multiple complementary tools working (cid:44) - Think about how different servers support each other in real-world use (cid:44) together across different services question engaging cases servers question ### Server and Tool Selection - Use tools from **at least 2 different MCP servers** to answer the (cid:44) - Select **{NUM_TOOLS} tools total** that work together across multiple (cid:44) - The question should require sequence or combination of tool calls (cid:44) - Choose tools based on how they complement each other across different (cid:44) - Consider each tool's description and purpose when crafting the (cid:44) - Ensure tools from different servers create logical, interconnected (cid:44) from different servers to solve completely cross-server workflow services/domains workflow that span different services tools across multiple servers ### Question Complexity - Create questions that are complex enough to warrant using {NUM_TOOLS} (cid:44) - The question should have multiple components or require several steps (cid:44) - Include relevant context or constraints that make the multi-server tool (cid:44) - Do not contain the exact tool names or server names in the question - Ensure the question cannot be reasonably answered with tools from just (cid:44) - Create scenarios that naturally require different types of services (cid:44) usage necessary single server working together ### Cross-Server Integration - Think about how different servers' capabilities can be combined - Consider data flow between different services (e.g., retrieving data (cid:44) - Create realistic scenarios where multiple services need to work (cid:44) - Focus on complementary functionalities across different domains from one service to use in another) together ### Output Format Your response should include: 1. **Server Analysis**: Briefly analyze the featured MCP Servers and their available tools, focusing on how they can work together. (cid:44) 2. **Cross-Server Workflow**: Describe the workflow showing how tools (cid:44) 3. **Target Tools**: The specific tool names from different MCP Servers that should be used together, in the order they would likely be (cid:44) called, with their server names. from different servers will be used together. (cid:44) 4. **Question**: clear, realistic user question that requires (cid:44) multi-server tool usage. 27 16 17 18 19 20 22 23 24 25 26 28 29 30 31 32 34 35 36 37 38 40 41 42 43 44 46 47 48 49 50 52 53 54 55 TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments ## Available Featured MCP Servers {FEATURED_SERVER_DESCRIPTIONS} ## Output Ensure your question requires exactly {NUM_TOOLS} tools from at least 2 different servers to solve completely. Provide your response in the (cid:44) following XML format: (cid:44) <response> <server_analysis> <!-- Briefly analyze the featured MCP Servers and their available (cid:44) tools, focusing on how they can work together across different domains/services. --> (cid:44) </server_analysis> <cross_server_workflow> <!-- Describe the workflow showing how tools from different servers (cid:44) will be used together to solve the question. --> </cross_server_workflow> <target_tools> <!-- The specific tool names from different MCP Servers that should (cid:44) be used together, listed in order with their server names. e.g., <tool server=\"Server1\">search_posts</tool> <tool server=\"Server2\">send_email</tool> --> (cid:44) (cid:44) </target_tools> <question> <!-- clear, realistic user question that requires multi-server tool (cid:44) usage spanning different services/domains. --> </question> </response> D.3 TASK DIVERSIFICATION PROMPT The following prompt aims to add diversity to the given task by introducing new contexts and personas. ## Task Generate **augmented variations** of given question that maintain the (cid:44) same target tool(s) usage and complexity level but apply them across different contexts and scenarios. (cid:44) multiple variations that: ## Objective Take an existing question and its associated target tool(s), then create (cid:44) - Use the same target tool(s) to achieve the core goal - Maintain the exact same tool usage order and final outcome - Apply the question to completely different contexts, scenarios, or (cid:44) - Keep the same level of complexity and constraints as the original - Demonstrate how the same tool usage pattern applies across diverse (cid:44) real-world scenarios domains ## Guidelines - Translate the question to distinctly different domains, user personas, or situational contexts while preserving its original complexity (cid:44) level. (cid:44) - Keep the tool usage sequence and final outcome identical across all (cid:44) - Ensure each variation feels like realistic scenario in its new context and remains solvable with the same tool operations. (cid:44) - Ensure the question does not contain any tool names or explicit (cid:44) references to the target tools. variations. ## Input Format **Original Question**: {ORIGINAL_QUESTION} 56 57 58 59 60 62 63 64 65 66 68 69 70 71 72 74 75 76 1 2 4 5 6 7 8 10 11 12 13 14 16 17 18 19 TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments **Target Tools**: {TARGET_TOOLS} **Tool Descriptions**: {TOOL_DESCRIPTIONS} outcome question. Each variation should: ## Output Requirements Generate **{VARIATIONS_COUNT} augmented variations** of the original (cid:44) 1. Maintain the same core goal that requires the target tool(s) 2. Use the exact same tool(s) in the same order with the same final (cid:44) 3. Apply to completely different context, scenario, or domain 4. Keep the same complexity level and constraints as the original 5. Feel like natural, real-world scenario from different setting 6. Be meaningfully different from the original and other variations in (cid:44) 7. Avoid including any explicit mentions, hints, or references to the (cid:44) target tool names within the question text terms of context only ## Output Provide your response in the following XML format: <response> <analysis> <!-- Briefly analyze the original question and target tool(s) to (cid:44) understand the core goal, tool usage pattern, complexity level, and expected outcome, then identify how this can be applied across different domains while maintaining operational consistency --> (cid:44) (cid:44) (cid:44) </analysis> <variations> <context>, and <question> tags --> <!-- Generate {VARIATIONS_COUNT} variations, each with <variation_X>, (cid:44) <variation_1> <context> <!-- Brief description of the new domain/scenario introduced --> </context> <question> <!-- The augmented question that maintains the same target (cid:44) tool(s) usage order, complexity, and outcome but in different context --> (cid:44) </question> </variation_1> <!-- Continue with variation_2, variation_3, etc. as needed based on (cid:44) number of variations --> </variations> </response> The prompt below is designed to enhance task complexity through the introduction of additional constraints. ## Task Generate **augmented variations** of given question that maintain the (cid:44) same target tool(s) usage and context but significantly increase the complexity and constraints required to solve the problem. (cid:44) additional complexity layers multiple sophisticated variations that: ## Objective Take an existing question and its associated target tool(s), then create (cid:44) - Use the same target tool(s) to achieve the core goal while navigating (cid:44) - Maintain the same general context and domain as the original question - Increase multi-dimensional complexity through realistic constraints, (cid:44) competing requirements, stakeholder considerations, and interconnected dependencies (cid:44) - Embed the tool usage within larger, more complex workflows that require (cid:44) strategic thinking and coordination 20 21 22 23 24 26 27 28 29 30 32 33 34 35 36 38 39 40 41 42 44 45 46 47 48 50 51 52 1 2 4 5 6 7 8 TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments - Demonstrate how the same core tool usage applies under vastly different (cid:44) complexity levels coordination across teams or systems requirements, tight timelines, or stakeholder conflicts ## Guidelines - Introduce realistic constraints such as resource limits, compliance (cid:44) - Embed the same tool usage inside broader workflow that requires (cid:44) - Escalate demands (performance, scalability, risk management) without (cid:44) - Ensure each variation targets different primary complexity angle (organizational, technical, strategic) while preserving tool (cid:44) relevance changing the original domain or context (cid:44) - Ensure the question does not contain any tool names or explicit (cid:44) references to the target tools. ## Input Format **Original Question**: {ORIGINAL_QUESTION} **Target Tools**: {TARGET_TOOLS} **Tool Descriptions**: {TOOL_DESCRIPTIONS} requirements adding multiple complexity layers original question. Each variation should: ## Output Requirements Generate **{VARIATIONS_COUNT} strategically augmented variations** of the (cid:44) 1. Maintain the same core goal that requires the target tool(s) while (cid:44) 2. Keep the same general context and domain as the original question 3. Introduce different but interconnected constraints and competing (cid:44) 4. Feel like natural, high-stakes, real-world scenarios that (cid:44) 5. Be meaningfully different from the original and other variations in (cid:44) 6. Include specific details that make the constraints and requirements (cid:44) 7. **Transform step-wise questions**: If the original question contains (cid:44) explicit steps, convert it to goal-oriented format while maintaining the same tool usage requirements (cid:44) 8. Avoid including any explicit mentions, hints, or references to the (cid:44) target tool names within the question text concrete and actionable professionals encounter terms of complexity ## Output Provide your response in the following XML format: <response> <analysis> <!-- Analyze the original question and target tool(s) to understand (cid:44) the core goal, current complexity level, and identify multiple complexity dimensions that can be naturally introduced while maintaining tool relevance and solution feasibility --> (cid:44) (cid:44) </analysis> <variations> <!-- Generate {VARIATIONS_COUNT} variations, each with <variation_X>, (cid:44) <variation_1> <constraints>, and <question> tags --> <constraints> <!-- Specific organizational, stakeholder, or coordination (cid:44) constraints that add realistic complexity --> </constraints> <question> <!-- The complex, organizationally-focused question that (cid:44) maintains the same target tool(s) usage within more sophisticated workflow --> (cid:44) </question> </variation_1> 30 10 11 13 14 15 16 17 19 20 21 22 23 25 26 27 28 29 31 32 33 34 35 37 38 39 40 41 43 44 45 46 47 49 50 51 TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments <!-- Continue with variation_2, variation_3, etc. as needed based on (cid:44) number of variations --> </variations> </response> D.4 TASK QUALITY ANNOTATION PROMPT ## Task Conduct **Question Quality Assessment** of tool use question across (cid:44) six key dimensions to ensure it meets high standards for realistic tool usage scenarios. (cid:44) primary dimensions: which tools to use giving all available tools ## Objective Analyze the provided tool use question and assess its quality across six (cid:44) 1. **Tool Selection Difficulty** - How challenging it is to determine (cid:44) 2. **Tool Selection Uniqueness** - How unique and necessary the selected (cid:44) 3. **Question Quality** - Overall clarity, specificity, and effectiveness 4. **Scenario Realism** - How authentic and believable the scenario is 5. **Verifiable** - How easy it is to verify the correctness of the final (cid:44) 6. **Stability** - How stable the answer will be when requested under (cid:44) tools are for this specific task giving all available tools different time and geolocation model answer ## Assessment Criteria ### 1. Tool Selection Difficulty **What to Evaluate**: How difficult it would be for user to determine (cid:44) which specific tools are needed to solve this question. selection obvious **Rating Guidelines**: - **very easy**: Question explicitly mentions tool names or makes tool (cid:44) - **easy**: Tool selection is straightforward with clear indicators - **medium**: Requires some reasoning but tool needs are fairly apparent - **hard**: Requires careful analysis to determine appropriate tools - **very hard**: Requires extensive expertise and deep reasoning to (cid:44) identify the correct tools ### 2. Tool Selection Uniqueness **What to Evaluate**: How unique and necessary the selected tools are for accomplishing this specific task, and whether the task can only be (cid:44) completed with these tools in the specified sequence. (cid:44) tools offer advantages same task equally well **Rating Guidelines**: - **not unique**: Many alternative tool combinations could accomplish the (cid:44) - **somewhat unique**: Some alternative approaches exist, but selected (cid:44) - **moderately unique**: Selected tools are well-suited, with limited (cid:44) - **quite unique**: Selected tools are particularly well-matched to the (cid:44) - **highly unique**: Task can only be accomplished effectively with these (cid:44) specific tools in this sequence alternative approaches task requirements ### 3. Question Quality **What to Evaluate**: Overall quality, clarity, and effectiveness of the (cid:44) question as realistic user query. **Rating Guidelines**: - **very poor**: Unclear, ambiguous, or poorly constructed question 52 53 54 55 1 3 4 5 6 7 9 10 11 12 13 15 16 17 18 19 21 22 23 24 25 27 28 29 30 31 33 34 35 36 37 39 TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments engaging - **poor**: Some clarity issues, missing important context - **average**: Clear and understandable, but could be more specific or (cid:44) - **good**: Well-constructed, clear, specific, and realistic - **excellent**: Exceptionally clear, detailed, engaging, and (cid:44) professionally written ### 4. Scenario Realism **What to Evaluate**: How authentic, believable, and true-to-life the (cid:44) described scenario is. unlikely **Rating Guidelines**: - **unrealistic**: Artificial, contrived, or implausible scenario - **somewhat unrealistic**: Some realistic elements but feels forced or (cid:44) - **moderately realistic**: Believable scenario with minor authenticity (cid:44) - **realistic**: Authentic scenario that represents genuine use cases - **highly realistic**: Completely natural, authentic scenario (cid:44) indistinguishable from real user needs issues ### 5. Verifiable **What to Evaluate**: How easy it is to verify the correctness of the (cid:44) final model answer. elements human judgment **Rating Guidelines**: - **hard to verify**: Fully free-form answer that requires extensive (cid:44) - **somewhat hard**: Mostly subjective answer with some verifiable (cid:44) - **moderately verifiable**: Short sentence that can be verified by LLM (cid:44) - **mostly verifiable**: Answer with clear, objective components and some (cid:44) - **easy to verify**: Answer can be verified by simple rules, exact (cid:44) matches, or clear success criteria subjective elements comparison ### 6. Stability (1-5 Scale) **What to Evaluate**: How stable and consistent the answer will be when (cid:44) the question is asked under different environmental conditions and system contexts. Consider factors like temporal dependency, geographical variations, operating system differences, network environments, and software version variations. (cid:44) (cid:44) (cid:44) environmental or system factors **Rating Guidelines**: - **highly unstable**: Answer changes significantly across different conditions (real-time data, location-specific, system-dependent) (cid:44) - **somewhat unstable**: Answer may vary moderately based on (cid:44) - **moderately stable**: Answer mostly consistent with minor variations (cid:44) - **mostly stable**: Answer remains largely consistent across different (cid:44) - **highly stable**: Answer is completely independent of environmental (cid:44) and system factors due to context conditions ## Question Analysis ### All Available Tools``` {ALL_SERVER_AND_TOOL_INFORMATION} ``` ### Question Content ``` {QUESTION_CONTENT} ``` 32 40 41 43 44 45 46 47 49 50 51 52 53 55 56 57 58 59 61 62 63 64 65 67 68 69 70 71 73 74 75 76 77 79 80 81 82 83 TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments ### Intended Tool for This Question ``` {INTENDED_TOOL} ``` ## Output Requirements Provide analysis with detailed reasoning BEFORE scores for each of the (cid:44) six metrics. ## Output Provide your response in the following XML format: <response> <tool_selection_difficulty> <reasoning> <!-- Detailed explanation including ambiguity level, domain (cid:44) knowledge required, and alternative solutions giving all available tools --> (cid:44) </reasoning> <rating><!-- Rating: very easy, easy, medium, hard, very hard (cid:44) --></rating> </tool_selection_difficulty> <tool_selection_uniqueness> <reasoning> <!-- Detailed explanation of tool necessity, sequential (cid:44) dependencies, and alternative tool viability giving all available tools --> (cid:44) </reasoning> <rating><!-- Rating: not unique, somewhat unique, moderately unique, (cid:44) quite unique, highly unique --></rating> </tool_selection_uniqueness> <question_quality> <reasoning> <!-- Detailed explanation covering linguistic quality, information (cid:44) architecture, and actionability --> </reasoning> <rating><!-- Rating: very poor, poor, average, good, excellent (cid:44) --></rating> </question_quality> <scenario_realism> <reasoning> <!-- Detailed explanation of industry authenticity, workflow (cid:44) accuracy, and stakeholder behavior --> </reasoning> <rating><!-- Rating: unrealistic, somewhat unrealistic, moderately (cid:44) realistic, realistic, highly realistic --></rating> </scenario_realism> <verifiable> <reasoning> <!-- Detailed explanation of answer format, objective criteria, and (cid:44) ground truth availability --> </reasoning> <rating><!-- Rating: hard to verify, somewhat hard, moderately verifiable, mostly verifiable, easy to verify --></rating> (cid:44) </verifiable> <stability> <reasoning> <!-- Detailed explanation of temporal/geographical/system (cid:44) dependencies and environmental factors --> 85 86 87 88 89 91 92 93 94 95 97 98 99 100 101 103 104 105 106 107 109 110 111 112 113 115 116 117 118 119 121 122 123 124 125 127 128 129 130 131 133 134 135 136 TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments </reasoning> <rating><!-- Rating: highly unstable, somewhat unstable, moderately (cid:44) stable, mostly stable, highly stable --></rating> </stability> </response> D.5 TRAJECTORY ANNOTATION PROMPT ## Task Conduct **Response Quality Assessment** of tool-use conversation (cid:44) across two LLM-scored dimensions, with third dimension computed automatically outside the LLM. (cid:44) ## Objective Analyze the provided conversation and assess its response quality across (cid:44) two primary dimensions scored by the LLM, while reserving an additional tool-call accuracy dimension for automated scoring: (cid:44) 1. Completeness - Whether the assistant fully accomplished the user's (cid:44) 2. Conciseness - Whether the assistant solved the task using the minimum (cid:44) necessary steps and verbosity request end-to-end ## Assessment Criteria saving/exporting/downloading where applicable) given the conversation context? Consider whether the assistant: ### 1. Completeness **What to Evaluate**: Did the assistant fully satisfy the user's goal (cid:44) - Executed all required steps end-to-end (including (cid:44) - Provided the final deliverable or working alternative when blocked (cid:44) - Included essential confirmations, paths, or instructions to achieve the (cid:44) - Avoided missing key requirements or leaving the user with unresolved (cid:44) (e.g., tool failure with usable fallback) outcome gaps usable **Rating Guidelines**: - very incomplete: Major requirements missing; no usable outcome - incomplete: Some key requirements missing; outcome is not directly (cid:44) - partially complete: Core steps attempted; outcome usable only with user (cid:44) - mostly complete: Meets most requirements; small omissions or minor (cid:44) - fully complete: All requirements met with usable outcome delivered effort or missing minor requirements issues remain redundancy and steps? Consider whether the assistant: ### 2. Conciseness **What to Evaluate**: Did the assistant achieve the goal with minimal (cid:44) - Avoided repetitive or unnecessary explanations/tool calls - Used the minimal set of steps/tools to complete the task - Kept language concise while preserving clarity **Rating Guidelines**: - very redundant: Excessive repetition or unnecessary steps/tool calls - redundant: Noticeable verbosity or extra steps beyond what's needed - average: Reasonably concise with minor extraneous content - concise: Efficient and to the point with minimal overhead - very concise: Maximally efficient while clear and complete ## Response Analysis ### Question Content ``` 34 137 139 140 141 1 2 4 5 6 7 8 10 11 12 13 14 16 17 18 19 20 22 23 24 25 26 28 29 30 31 32 34 35 36 37 38 40 41 42 43 44 46 47 48 49 50 52 53 54 55 56 58 59 60 61 62 64 65 66 67 68 70 71 72 73 74 76 TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments {QUESTION_CONTENT} ``` ### Intended Tool for This Question ``` {INTENDED_TOOL} ``` ### Conversation History ``` {CONVERSATION_HISTORY} ``` ## Output Requirements - Provide detailed reasoning BEFORE ratings for Completeness and (cid:44) - Do NOT score Tool Call Accuracy; include placeholders only Conciseness ## Output Provide your response in the following XML format: <response> <completeness> <reasoning> <!-- Evaluate if the assistant delivered an end-to-end usable (cid:44) outcome, addressed all requirements, handled tool failures with alternatives, and provided necessary confirmations/paths. --> (cid:44) </reasoning> <rating><!-- Rating: very incomplete, incomplete, partially complete, (cid:44) mostly complete, fully complete --></rating> </completeness> <conciseness> <reasoning> <!-- Evaluate if the assistant minimized redundant (cid:44) steps/explanations, avoided unnecessary tool calls, and kept messaging efficient while clear. --> (cid:44) </reasoning> <rating><!-- Rating: very redundant, redundant, average, concise, (cid:44) very concise --></rating> </conciseness> </response>"
        }
    ],
    "affiliations": [
        "MIT-IBM Watson AI Lab",
        "University of Washington"
    ]
}