{
    "paper_title": "MoA: Heterogeneous Mixture of Adapters for Parameter-Efficient Fine-Tuning of Large Language Models",
    "authors": [
        "Jie Cao",
        "Tianwei Lin",
        "Hongyang He",
        "Rolan Yan",
        "Wenqiao Zhang",
        "Juncheng Li",
        "Dongping Zhang",
        "Siliang Tang",
        "Yueting Zhuang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent studies integrate Low-Rank Adaptation (LoRA) and Mixture-of-Experts (MoE) to further enhance the performance of parameter-efficient fine-tuning (PEFT) methods in Large Language Model (LLM) applications. Existing methods employ \\emph{homogeneous} MoE-LoRA architectures composed of LoRA experts with either similar or identical structures and capacities. However, these approaches often suffer from representation collapse and expert load imbalance, which negatively impact the potential of LLMs. To address these challenges, we propose a \\emph{heterogeneous} \\textbf{Mixture-of-Adapters (MoA)} approach. This method dynamically integrates PEFT adapter experts with diverse structures, leveraging their complementary representational capabilities to foster expert specialization, thereby enhancing the effective transfer of pre-trained knowledge to downstream tasks. MoA supports two variants: \\textbf{(i)} \\textit{Soft MoA} achieves fine-grained integration by performing a weighted fusion of all expert outputs; \\textbf{(ii)} \\textit{Sparse MoA} activates adapter experts sparsely based on their contribution, achieving this with negligible performance degradation. Experimental results demonstrate that heterogeneous MoA outperforms homogeneous MoE-LoRA methods in both performance and parameter efficiency. Our project is available at https://github.com/DCDmllm/MoA."
        },
        {
            "title": "Start",
            "content": "MoA: Heterogeneous Mixture of Adapters for Parameter-Efficient Fine-Tuning of Large Language Models Jie Cao1, Tianwei Lin 1, Hongyang He, Rolan Yan 2, Wenqiao Zhang* 1, Juncheng Li 1, Dongping Zhang , Siliang Tang 1, Yueting Zhuang 1 1 Zhejiang University, 2 Tencent"
        },
        {
            "title": "Abstract",
            "content": "Recent studies integrate Low-Rank Adaptation (LoRA) and Mixture-of-Experts (MoE) to further enhance the performance of parameterefficient fine-tuning (PEFT) methods in Large Language Model (LLM) applications. Existing methods employ homogeneous MoE-LoRA architectures composed of LoRA experts with either similar or identical structures and capacities. However, these approaches often suffer from representation collapse and expert load imbalance, which negatively impact the potential of LLMs. To address these challenges, we propose heterogeneous Mixtureof-Adapters (MoA) approach. This method dynamically integrates PEFT adapter experts with diverse structures, leveraging their complementary representational capabilities to foster expert specialization, thereby enhancing the effective transfer of pre-trained knowledge to downstream tasks. MoA supports two variants: (i) Soft MoA achieves fine-grained integration by performing weighted fusion of all expert outputs; (ii) Sparse MoA activates adapter experts sparsely based on their contribution, achieving this with negligible performance degradation. Experimental results demonstrate that heterogeneous MoA outperforms homogeneous MoELoRA methods in both performance and parameter efficiency. Our project is available at https://github.com/DCDmllm/MoA."
        },
        {
            "title": "Introduction",
            "content": "The rapid development of Large Language Models (LLMs) (Achiam et al., 2023; Yang et al., 2025; Grattafiori et al., 2024a,a) is reshaping the field of NLP, demonstrating cross-domain generalization capabilities across wide range of tasks. However, as model sizes continue to grow, the computational, storage, and deployment costs of traditional full fine-tuning methods increasingly become bottlenecks in practical applications. As *Corresponding author result, parameter-efficient fine-tuning (PEFT) methods (Houlsby et al., 2019; He et al., 2022; Li and Liang, 2021; Lester et al., 2021; Zhang et al., 2024; Han et al., 2024; Lin et al., 2025) are emerging as key research direction for adapting LLMs. These methods introduce small number of trainable, lightweight adapter modules on top of frozen pretrained weights. By doing so, they effectively guide the activation and adjustment of the models existing knowledge representations while maintaining low overhead, enabling fast transfer and adaptation to specific downstream tasks. The Low-Rank Adaptation (LoRA) method (Hu et al., 2021) reparameterizes weight matrices with low-rank decomposition, enabling the approximation of full fine-tuning using only small number of trainable parameters. However, increasing the rank does not consistently improve performance, as the representational capacity of LoRA tends to saturate (Chen et al., 2022; Zhu et al., 2023). To overcome this limitation, recent methods integrate LoRA into the Mixture-of-Experts (MoE) architecture (Shazeer et al., 2017), giving rise to the MoELoRA framework (Zadouri et al., 2023; Zhu et al., 2023). In this setup, multiple structurally identical LoRA experts are dynamically routed at the token level via learnable routing mechanism, thereby enhancing the models adaptability to diverse inputs and increasing its representational flexibility. Although MoE-LoRA hybrids enhance representational capacity by introducing multiple LoRA experts, their homogeneous design causes the experts to tend toward learning similar representations during training, leading to representation collapse (Chi et al., 2022; Wang et al., 2024; Lin et al., 2024). This undermines expert diversity and specialization. Moreover, the dynamic routing mechanism is prone to expert load imbalance in homogeneous architectures, where few initially well-performing experts consistently receive the majority of tokens, suppressing the participation 5 2 0 2 J 6 ] . [ 1 8 2 9 5 0 . 6 0 5 2 : r Figure 1: MoA architecture with heterogeneous PEFT adapters. It is worth noting that in Sparse MoA, the Prompt Tuning module is deactivated due to its non-token-level activation mechanism. of other experts. These mechanisms ultimately result in redundant experts (Zhou et al., 2022), which limit the expressiveness of the model and reduce the efficiency of resource utilization. Based on the above analysis, we posit that the representational convergence of homogeneous experts may limit the performance ceiling of PEFT methods. To address the issues of redundant experts and load imbalance in MoE-LoRA, we propose Mixture-of-Adapters (MoA) approach with heterogeneous structures (Figure 1). MoA consists of PEFT adapter experts with diverse architectural forms and employs token-level dynamic routing to activate experts on demand. By leveraging the complementary representational capacities of different structures, MoA promotes expert specialization through functional differentiation among experts. This mechanism enables more refined utilization of pre-trained knowledge and enhances the ability of models to transfer and generalize across diverse downstream tasks. Our main contributions are as follows: (i) We propose MoA based on heterogeneous experts, which constructs adapters with complementary representational capacities by integrating structurally diverse PEFT modules. This effectively enhances expert specialization, mitigating the issue of expert redundancy commonly observed in traditional MoE-LoRA architectures. In addition, MoA achieves efficient adaptation to downstream tasks using fewer trainable parameters, fully leveraging the knowledge representations embedded in the pre-trained model. (ii) Building on this foundation, we further design two variants: Soft MoA and Sparse MoA. In Sparse MoA, threshold function dynamically selects active experts that valuable contribute to the current input. This reduces redundant computation while enabling the model to engage more experts for critical tokens, thereby improving representational capacity. (iii) We systematically evaluate the architectural advantages and practical effectiveness of MoA across multiple tasks. Experimental results show that MoA significantly outperforms SoTA MoELoRA methods in GPU memory usage, training efficiency, and inference speed. Moreover, it achieves higher parameter efficiency and knowledge transfer capability while maintaining or even improving downstream performance."
        },
        {
            "title": "2.1 Parameter-Efficient MoE",
            "content": "In non-PEFT scenarios, standard sparse MoE (Shazeer et al., 2017; Fedus et al., 2022; Jiang et al., 2024) architectures must employ discrete top-k routing strategies to reduce computational overhead. In contrast, under PEFT settings, where PEFT experts occupy significantly less parameters than the pre-trained LLMs, it becomes feasible to compute soft-weighted output by averaging experts predictions according to the routers distribution. Thus, we categorize PEFT-based MoE into two paradigms: sparse parameter efficient MoE and soft-weighted parameter efficient MoE. Sparse parameter efficient MoE Building on the demonstrated superiority of standard sparse MoE architectures, recent works has integrated sparse MoE with parameter-efficient fine-tuning (PEFT) methods to enhance resource-constrained adaptation performance. Most of these approaches are based on LoRA, leveraging its superior efficacy in parameter-efficient adaptation as foundational component. SiRA (Zhu et al., 2023) enforces top-k experts routing with per-expert capacity constraints, limiting the maximum tokens each expert processes, and uses dropout to the gating network to mitigate overfitting. MoELoRA (Luo et al., 2024) employs contrastive learning to encourage experts to learn different features, thus mitigating the random routing phenomenon observed in MoE. To reduce redundancy, MoLA (Gao et al., 2024) explores layer-wise expert redundancy by assigning different number of LoRA experts to each Transformer layer, revealing that the lower layers exhibit higher redundancy in the sparse MoE-LoRA frameworks. Advancing this line, AdaMoLE (Liu and Luo, 2024) dynamically adjusts the activation of LoRA experts for each input token via threshold network integrated into the top-k routing. parameter efficient MoE Soft-weighted MoLoRA (Zadouri et al., 2023) proposes softweighted MoE-LoRA method with token-level routing strategy. LoRAMoE (Dou et al., 2024) integrates LoRA experts using linear router to alleviate the forgetting of the world knowledge previously stored in LLMs. MOLE (Wu et al., 2024) proposes to efficiently compose multiple trained LoRAs while preserving all their characteristics by training only the router within each layer. HydraLoRA (Tian et al., 2024) introduces an asymmetric structure with shared matrix, further enhancing parameter efficiency compared to other MoE-LoRA methods. Diverging from methods that ensemble the outputs of individual experts, certain approaches such as AdaMix (Wang et al., 2022), SMEAR (Muqeeth et al., 2024), and MoV (Zadouri et al., 2023) adopt strategy of first merging expert parameters via routing, followed by computation using the merged parameters. This aims to reduce computational overhead."
        },
        {
            "title": "3 Method",
            "content": "Homogeneous MoE methods inherently suffer from the representation collapse and load imbalance issues, fundamentally stemming from their experts identical structures and capacities, which lead to insufficient specialization of individual experts. To address this, we construct inherently heterogeneous MoE experts leveraging structurally diverse and positionally varied PEFT adapters for each Transformer block. LoRA, Parallel Adapter, and Prompt Tuning approaches can be simplified into unified view (He et al., 2022): = (x) + E(x), (1) where (x) denotes the transformer module to which the corresponding PEFT Adapter is applied. Our selection of heterogeneous PEFT experts comprises LoRA (Hu et al., 2021), Parallel Adapters (He et al., 2022), and the zero-initialized Prompt Tuning (Zhang et al., 2023), the details of these methods and the unified view are shown in Appendix A."
        },
        {
            "title": "3.1 Soft MoA",
            "content": "In each transformer layer, our Soft MoA comprises soft-weighted router and set of experts {E1, E2, . . . , En} that are composed of PEFT adapters of different types and placements. Our router network commonly consists of dense layer with trainable weights Rdn followed by sigmoid function which takes an intermediate token representation as input and output weights R(x) for experts: R(x) = Sigmoid(W x). (2) Then, each experts weight R(x)i is applied to its output Ei(x), controlling the influence of expert Ei in that transformer layer:"
        },
        {
            "title": "2.2 Heterogeneous MoE",
            "content": "h = Fi(x) + R(x)iEi(x). (3) AutoMoE (Jawahar et al., 2023) employs the neural architecture search to obtain heterogeneous MoE subnetworks from original FFN experts on smallscale MoE model that utilizes the top-1 routing strategy. HMoE (Wang et al., 2024) constructs heterogeneous experts by varying the dimensions of FFN experts, thereby endowing them with diverse capacities. UNIPEFT (Mao et al., 2022) employs distinct gated networks for various PEFT methods in instance-level to adapt to different examples. Unlike previous MoE methods (Zadouri et al., 2023; Dou et al., 2024), our MoA leverages heterogeneous experts of various types within the Transformer module to process each token. Due to the diverse characteristics and capacities of these heterogeneous experts, MoA neither suffer from the representation collapse problem nor need loadbalancing loss. Moreover, MoA employs sigmoid function in the router, rather than the traditional softmax activation used in conventional MoE (Shazeer et al., 2017; Fedus et al., 2022), encouraging cooperation among experts instead of competition and fully capitalizing on their unique specializations. 3.2 Sparse MoA Methods based on soft-weighted routing, including MoA and other MoE approaches (Zadouri et al., 2023; Dou et al., 2024; Muqeeth et al., 2024), require computing experts Ei even when their weights R(x)i 0, despite negligible contributions to outputs, resulting in unnecessary computational overhead and time consumption. To address this issue, we propose Sparse MoA, which dynamically computes only the necessary experts, thereby avoiding the computation of low-contributing ones. To implement sparse MoA, straightforward approach is to apply fixed threshold Γ to the experts weights. An expert Ei is activated for computation if and only if its weight satisfies R(x)i > Γ, where unselected experts are entirely excluded from the computational graph, thus eliminating redundant computations. The detailed computation is defined as follows: = (cid:40) Fi(x) + R(x)iEi(x) Fi(x) if R(x)i > Γ, otherwise. (4) Note that when the expert Ei is selected, its weight R(x)i is still used to control its influence. The threshold-based approach can activate varying numbers of heterogeneous experts for each token, dynamically adapting to the current task and data. However, due to the varying semantic importance of tokens in contexts (Xia et al., 2025), assigning token-specific thresholds can more effectively harness multi-expert collaboration: lower thresholds for critical tokens promote broader expert engagement, while higher thresholds for less critical tokens restrict processing to fewer specialized experts. Inspired by AdaMoLE (Liu and Luo, 2024), we replace fixed thresholds with learnable threshold function. Specifically, the threshold for each token is calculated via linear layer followed by sigmoid activation: bilities via adapters for downstream tasks (Ghosh et al., 2024), Sparse MoA enhances this efficiency by adaptively activating diverse combinations of heterogeneous experts. Moreover, Sparse MoA dynamically adjusts the number of participating experts based on the contextual importance of tokens, thereby mitigating redundant expert computations. For instance, in extreme cases where certain tokens require no expert intervention and can be processed solely by the frozen LLM, Sparse MoA deactivates all experts, thus maximizing computational efficiency. Notably, due to its computational mechanism, Prompt Tuning does not facilitate token-level expert allocation within batch. Consequently, Prompt Tuning experts are not included in our Sparse MoA framework."
        },
        {
            "title": "4.1 Datasets",
            "content": "We conduct extensive experiments and analyses on both mathematical and commonsense reasoning tasks. For mathematical reasoning, we evaluate performance on six test datasets: GSM8K (Cobbe et al., 2021), SVAMP (Patel et al., 2021), MultiArith (Roy and Roth, 2016), AddSub (Hosseini et al., 2014), AQuA (Ling et al., 2017), and SingleEq (Koncel-Kedziorski et al., 2015). The training dataset is Math14K constructed by (Hu et al., 2023), which includes the training sets of GSM8K and AQuA, augmented with rationales generated by ChatGPT and GPT-4. For commonsense reasoning, we test on eight datasets: BoolQ (Clark et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), HellaSwag, WinoGrande (Sakaguchi et al., 2021), ARC Challenge (ARC-c), ARC Easy (ARC-e) (Chollet, 2019), and OBQA (Mihaylov et al., 2018). The training dataset for commonsense reasoning, Commonsense15K, is sampled from Commonsense170K (Hu et al., 2023) which comprises eight training sets."
        },
        {
            "title": "4.2 Baseline Models",
            "content": "Γ = ΓmaxSigmoid(W Γ + bΓ), (5) Γ Rd1 and bΓ R1 are trainable where parameters, Γmax is hyperparameter that defines the upper bound of threshold value. As the core objective of PEFT methods is to fully leverage LLMs pre-trained knowledge and capaTo validate the efficacy of our method, we compare it against: (i) the standard LoRA(Hu et al., 2021); (ii) SoTA soft-weighted parameter-efficient MoE methods: MoLoRA (Zadouri et al., 2023) and HydraLoRA (Tian et al., 2024); and (iii) SoTA sparse parameter-efficient MoE approaches: MoLA (Gao et al., 2024) and AdaMoLE (Liu and Luo, 2024). Average GSM8k MultiArith AQuA AddSub Models 79.180.26 76.140.88 96.280.79 36.750.99 87.681.14 LoRA 80.920.19 77.460.68 99.330.17 38.061.86 91.731.02 MoLoRA 80.950.28 99.170.33 77.510.59 39.112.17 HydraLoRA 91.220.15 90.460.53 39.371.80 80.950.14 98.940.19 77.610.61 MoLA 81.090.78 98.332.05 77.510.27 90.970.53 40.032.37 AdaMoLE 98.940.54 97.440.71 83.430.55 81.510.35 38.981.57 78.010.23 92.240.89 Soft MoA 81.200.47 99.170.17 38.062.27 78.090.95 Sparse MoA 91.900.44 SingleEq 96.980.30 97.180.41 96.850.34 97.050.20 97.310.30 SVAMP 81.270.45 81.770.68 81.870.25 82.270.60 82.400.50 82.700.00 97.310.23 Param 23.06M 100.14M 45.09M 100.14M 101.12M 24.52M 22.29M Table 1: Experiment results on mathematical reasoning benchmarks. The evaluation metric is accuracy. All methods are run with three random seeds; mean and standard deviation are reported. Param indicates trainable parameters. PIQA BoolQ SIQA HellaSwag WinoGrande ARC-C Average Param Models 72.940.29 86.220.64 78.780.64 86.341.37 80.011.87 82.080.51 92.630.98 86.930.81 83.240.58 23.06M LoRA MoLoRA 73.890.77 87.320.73 78.980.96 88.520.58 82.110.41 84.841.04 93.450.45 87.330.99 84.560.58 100.14M HydraLoRA 73.520.25 86.650.65 79.170.42 87.461.00 82.300.39 83.870.45 93.280.49 86.731.50 84.090.31 45.09M 73.190.42 87.010.44 79.320.75 87.600.67 82.530.90 84.950.42 93.350.33 87.600.92 84.450.22 100.14M MoLA AdaMoLE 73.600.37 88.080.61 79.720.4888.700.65 81.291.14 85.040.81 93.270.19 89.130.81 84.850.21 101.12M Soft MoA 72.860.40 88.080.55 79.320.72 87.820.44 83.030.70 85.840.31 93.490.46 89.270.5084.960.26 24.52M Sparse MoA 73.690.31 87.360.27 78.510.23 87.970.24 81.660.91 85.550.13 93.530.09 88.670.58 84.620.09 22.29M ARC-E OBQA Table 2: Experiment results on commonsense reasoning benchmarks. The evaluation metric is accuracy."
        },
        {
            "title": "4.3 Experiment Settings",
            "content": "All experiments use the LLaMA-3.1 8B model (Grattafiori et al., 2024b). To ensure fair comparisons, we apply identical configurations to all LoRA-based methods: LoRA is inserted into four weight matrices in the self-attention module (Wq, Wk, Wv, Wo) and one in the FFN module (Wup), with rank 8 and alpha 8. For all MoE-LoRA baselines, we set the number of experts to 8. For the original LoRA baseline, we increase the rank to 16 to match the number of trainable parameters across methods. MoA integrates 7 heterogeneous experts, including five LoRA modules, parallel adapters in the FFN layer, and zero-initialized prompt-tuning module. The bottleneck dimension of the parallel adapter is set to 16, and the prompt length to 10. We use the AdamW optimizer with learning rate of 6e-3. For commonsense reasoning, input sequences are truncated to 200 tokens; for mathematical reasoning, to 300 tokens. All models are trained with batch size of 32 on 2A6000 GPUs."
        },
        {
            "title": "4.4 Experiment Results",
            "content": "As shown in Tables 1 and 2, Soft MoA and Sparse MoA consistently outperform homogeneous MoELoRA baselines on both mathematical and commonsense reasoning tasks. Soft MoA achieves the highest accuracy on math benchmarks (81.51%) with only 24.52M trainable parametersnearly 4 fewer than AdaMoLE and MoLoRA. Sparse MoA delivers strong performance (81.20% math, 84.62% commonsense) with the smallest parameter count (22.29M), surpassing all other methods in mathematical accuracy. These results demonstrate that heterogeneous experts in MoA enhance both performance and efficiency, effectively addressing redundancy in existing MoE-LoRA designs."
        },
        {
            "title": "5.1 Ablation Study on Soft MoA Components",
            "content": "The token-level soft-weighted router effectively leverages the distinct characteristics of heterogeneous experts. Table 3 shows the performance of individual PEFT methods and their naive combination on two reasoning tasks. Among the three standalone PEFT methods, LoRA achieves the highest accuracy. Notably, the naive composition of these three PEFT methods degrades performance, failing to effectively leverage the functionalities of diverse experts. In contrast, our Soft MoA integrates seven heterogeneous experts via tokenlevel router, achieving significant improvements over standalone LoRA (+2.33 on mathematical reasoning, +1.72 on commonsense reasoning). This demonstrates that Soft MoAs router dynamically assigns experts based on token-specific contextual demands, fully exploiting the complementary strengths of these structurally diverse modules. The sigmoid activation function outperforms softmax in the MoA router. Conventional MoE methods with homogeneous experts typically use routers with softmax activation, which enforces weight trade-offs over experts ((cid:80) R(x)i = 1). For comparison, we replaced the sigmoid funcFigure 2: Comparison of different models under identical batch sizes (1* 48G GPU): (a) Training time per epoch, (b) GPU memory consumption during training, and (c) Average inference time per sample. Modules Router Commonsense Math Prompt-tuning Parallel Adapter LoRA LoRA Composition Composition Composition Ø Ø Ø Sigmoid Ø Softmax Sigmoid 78.03 81.62 83.24 84.23 81.58 84.16 84.96 78.06 79.20 79.18 80.50 78.28 80.21 81.51 Max Gamma Threshold Function Math 81.22 0.2 80.92 0.5 78.58 0.8 81.24 0.2 81.51 0.5 81.36 0.8 80.61 1 Ø Ø Ø Table 3: Ablation study of each type of PEFT expert and the activation function of the Router in MoA. Table 4: Comparative performance of fixed-threshold baseline and threshold function in Sparse MoA under varying hyperparameters Γmax. tion with softmax in our framework. As shown in Table 3, while the softmax-based router improves upon standalone LoRA and the naive composition, it underperforms MoA equipped with sigmoid router. This indicates fundamental operational difference: homogeneous MoE experts function within competitive paradigm, whereas MoAs heterogeneous experts leverage collaborative computation (sigmoid permits non-exclusive activation). The sigmoid router dynamically scales expert contributions - enabling full-capacity collaboration (R(x)i 1 for all experts) or complete deactivation (R(x)i 0)thereby maximizing the utility of architectural diversity. The five LoRA modules contribute the most, while the Parallel Adapter and Prompt Tuning further improve performance. The superior performance of LoRA over Prompt-tuning and Parallel Adapter, combined with its inherent inclusion of positionally diverse expert modules, motivated our extension of LoRAs multi-module experts with token-level router. While this adaptation yielded significant improvements over standalone LoRA (e.g., +1.32 on mathematical reasoning), it underperformed the full MoA (-1.01), highlighting two critical insights: 1. MoAs primary strength derives from leveraging multiple LoRA modules as distinct experts. 2. The distinct mechanisms of Prompt Tuning and Parallel Adapter further complement these LoRA-based experts."
        },
        {
            "title": "5.2 Effect of Threshold Function",
            "content": "To validate the effect of the threshold function in Sparse MoA, we compare it against fixedthreshold baselines under varying hyperparameters Γmax {0.2, 0.5, 0.8}, with an additional setting of Γmax = 1 exclusive to the threshold function. As shown in Table 4, the threshold function achieves higher performance ceilings and consistently outperforms fixed-threshold methods across all Γmax configurations. This demonstrates that tokens exhibit varying contextual importance: the dynamic threshold function assigns lower thresholds to critical tokens (promoting multi-expert collaboration) and higher thresholds to trivial ones (reducing redundant computations), thereby better leveraging the LLMs capabilities while maintaining efficiency. Figure 3: Comparison of router weight distributions between Soft MoA and AdaMoLE under different random seeds. The MoA method exhibits strong consistency, whereas AdaMoLE does not. 6 In-depth Analysis"
        },
        {
            "title": "6.1 Efficiency Comparison",
            "content": "To systematically compare the differences between Soft MoA, Sparse MoA, and baseline methods, Figure 2 presents the training time per epoch, GPU memory requirements, and average inference latency per sample under identical configurations (single GPU setup, fixed batch size (BS)). As shown in Figure 2(a), under identical batch size conditions (BS=2), both Soft MoA and Sparse MoA require significantly less training time compared to other MoE methods. Notably, Soft MoA achieves less than half the training time of MoLoRA and HydraLoRA. This efficiency advantage stems not only from reduced parameter counts but also from architectural optimizations - for instance, while HydraLoRA contains half the parameters of MoLoRA, their training times remain comparable. Sparse MoE methods (MoLA and AdaMoLE) exhibit nearly double the training time of MoLoRA despite having the same parameter counts. This discrepancy occurs because, unlike traditional MoE approaches, PEFT-based MoE methods feature experts with significantly fewer parameters and computations relative to the frozen LLM backbone; besides, the sparse token-to-expert routing process within each batch brings additional computational overhead. This phenomenon is also observable between Soft MoA and Sparse MoA. However, as the batch size increases (e.g., BS=8 in Figure 2(a)), two key effects emerge: (i) the sparse per-batch routing overhead becomes relatively less significant, and (ii) the reduction strategy of redundant expert computation in Sparse MoA becomes more effective, ultimately causing its training time to converge with Soft MoAs. As shown in Figure 2(b), both Soft MoA and Sparse MoA maintain memory requirements comparable to LoRA, with Sparse MoA demonstrating additional memory efficiency through its dynamic threshold function that selectively activates experts per token. This advantage becomes more pronounced at larger batch sizes - for instance, at BS=8, Sparse MoA reduces memory consumption by 3.86GB compared to Soft MoA. Figure 2(c) reveals that the inference latency of both Soft MoA and Sparse MoA approaches that of standalone LoRA, while maintaining significant advantages over other MoE methods. In summary, both MoA variants demonstrate notable advantages over MoE-LoRA methods across training time, GPU memory usage, and inference latency, nearing the performance of LoRA. SpecifiFigure 4: Visualization of average router weights per layer for Soft MoA (left) and Sparse MoA (right) on ARCChallenge, averaged over tokens within 50 samples. Sparse MoA also includes the average per-layer threshold and the average count of activated experts. The average count of activated experts across layers in Sparse MoA is 3.55. cally:(i) Sparse MoA exhibits superior memory efficiency compared to Soft MoA through its sparse token-to-expert routing strategy. (ii) The memory and computational benefits of Sparse MoA scale favorably with increasing batch size, making it particularly well-suited for resource-intensive applications."
        },
        {
            "title": "6.2 Difference between MoA and MoE",
            "content": "The heterogeneous experts in the MoA method ensure expert specialization. As shown in Figure 3, we compare the expert weight distributions across 32 Transformer layers on 50 test samples for Soft MoA and AdaMoLE, trained with different random seeds. Soft MoA exhibits highly consistent expert weight distributions across seeds, as illustrated in the first row. Experts in the lower 16 layers tend to be more active than those in the upper layers. Among the experts, LoRA(Q, K) and Prompt experts show higher activation than LoRA(O, V, UP) and the Parallel Adapter. This indicates that MoA is capable of leveraging the distinct adaptation capabilities of heterogeneous experts according to their structural characteristics and positions within the model. In contrast, as shown in the second row, AdaMoLE fails to exhibit such consistency under different seeds, aligning with findings from prior work such as HMoE (Wang et al., 2024), which identifies under-specialization as common issue in homogeneous expert settings."
        },
        {
            "title": "6.3 Expert Activation Comparison",
            "content": "Sparse MoA significantly reduces expert computations with nearly no accuracy loss relative to Soft MoA. Figures 4 and 5 (see Appendix) present comparison of expert activation weights between Soft MoA and Sparse MoA on ARC-Challenge and GSM8K datasets. The distribution patterns of router weights for experts exhibit notable similarities between Soft MoA and Sparse MoA. However, Soft MoA activates fixed total number of experts per token, whereas Sparse MoA employs thresholding function for selective expert activation. As illustrated in Figure 4 (right), the average number of activated experts in Sparse MoA drops to merely 1-2 in certain intermediate layers. The average number of activated experts across all layers is reduced from 6 to 3.55, reducing 40% expert computations with negligible accuracy drop of 0.3%. 6.4 Instanceand Token-level Routing In contrast to token-level routing approaches, instance-level routing, wherein the model makes single routing decision for the entire input example, significantly reduces the computational overhead associated with the router. This brings the overall computational cost closer to that of standard, nonMoE architectures. We conducted comparison between token-level MoA and instance-level variant of Soft MoA and the UniPEFT approach. The details are shown in Appendix B.1."
        },
        {
            "title": "7 Conclusion",
            "content": "We present MoA, novel heterogeneous parameterefficient fine-tuning (PEFT) method that adapts LLMs to downstream tasks using PEFT adapters with diverse architectures and minimal number of trainable parameters. Comprehensive experiments are conducted on commonsense and mathematical reasoning tasks, demonstrating that heterogeneous MoA outperforms the state-of-the-art homogeneous MoE-LoRA approaches, exhibiting superior performance in training time, inference latency, and GPU memory consumption."
        },
        {
            "title": "Limitations",
            "content": "While our proposed Sparse MoA method reduces GPU memory consumption compared to Soft MoA, its training and inference times are longer at small batch size due to the additional computational overhead of sparse token-to-expert routing, achieving comparable or reduced time consumption only at larger batch sizes. Furthermore, sparse routing methods, including Sparse MoA, necessitate tokento-expert assignment within batch, thus limiting support to PEFT Adapters where input tokens within sample can be computed independently. Consequently, methods like zero-initialized Prompt Tuning, where token computations within sample are interdependent, are not compatible with Sparse MoA."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, and 1 others. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, and 1 others. 2020. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 74327439. Guanzheng Chen, Fangyu Liu, Zaiqiao Meng, and Shangsong Liang. 2022. Revisiting parameterefficient tuning: Are we really there yet? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 26122626. Zewen Chi, Li Dong, Shaohan Huang, Damai Dai, Shuming Ma, Barun Patra, Saksham Singhal, Payal Bajaj, Xia Song, Xian-Ling Mao, and 1 others. 2022. On the representation collapse of sparse mixture of experts. Advances in Neural Information Processing Systems, 35:3460034613. François Chollet. 2019. On the measure of intelligence. arXiv preprint arXiv:1911.01547. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, and 1 others. 2021. Training verifiers arXiv preprint to solve math word problems. arXiv:2110.14168. Shihan Dou, Enyu Zhou, Yan Liu, Songyang Gao, Jun Zhao, Wei Shen, Yuhao Zhou, Zhiheng Xi, Xiao Wang, Xiaoran Fan, Shiliang Pu, Jiang Zhu, Rui Zheng, Tao Gui, Qi Zhang, and Xuanjing Huang. 2024. LoRAMoE: Alleviate World Knowledge Forgetting in Large Language Models via MoE-Style Plugin. arXiv preprint. ArXiv:2312.09979 [cs]. William Fedus, Barret Zoph, and Noam Shazeer. 2022. Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity. arXiv preprint. ArXiv:2101.03961 [cs]. Chongyang Gao, Kezhen Chen, Jinmeng Rao, Baochen Sun, Ruibo Liu, Daiyi Peng, Yawen Zhang, Xiaoyuan Guo, Jie Yang, and V. S. Subrahmanian. 2024. Higher Layers Need More LoRA Experts. arXiv preprint. ArXiv:2402.08562 [cs]. Sreyan Ghosh, Chandra Kiran Reddy Evuru, Sonal Kumar, Ramaneswaran S, Deepali Aneja, Zeyu Jin, Ramani Duraiswami, and Dinesh Manocha. 2024. Closer Look at the Limitations of Instruction Tuning. arXiv preprint. ArXiv:2402.05119 [cs]. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, and 1 others. 2024a. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, and 1 others. 2024b. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Zeyu Han, Chao Gao, Jinyang Liu, Sai Qian Zhang, and 1 others. 2024. Parameter-efficient fine-tuning for large models: comprehensive survey. arXiv preprint arXiv:2403.14608. Junxian He, Chunting Zhou, Xuezhe Ma, Taylor BergKirkpatrick, and Graham Neubig. 2022. Towards Unified View of Parameter-Efficient Transfer Learning. arXiv preprint. ArXiv:2110.04366 [cs]. Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. 2014. Learning to solve arithmetic word problems with verb categorization. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 523533. N. Houlsby, A. Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and S. Gelly. 2019. ParameterEfficient Transfer Learning for NLP. ArXiv. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. LoRA: Low-Rank Adaptation of Large Language Models. arXiv preprint. ArXiv:2106.09685 [cs]. Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, EePeng Lim, Lidong Bing, Xing Xu, Soujanya Poria, and Roy Ka-Wei Lee. 2023. LLM-Adapters: An Adapter Family for Parameter-Efficient FineTuning of Large Language Models. arXiv preprint. ArXiv:2304.01933 [cs]. Ganesh Jawahar, Subhabrata Mukherjee, Xiaodong Liu, Young Jin Kim, Muhammad Abdul-Mageed, Laks V. S. Lakshmanan, Ahmed Hassan Awadallah, Sebastien Bubeck, and Jianfeng Gao. 2023. AutoMoE: Heterogeneous Mixture-of-Experts with Adaptive Computation for Efficient Neural Machine Translation. arXiv preprint. ArXiv:2210.07535 [cs]. Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, and 1 otharXiv preprint ers. 2024. Mixtral of experts. arXiv:2401.04088. Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang. 2015. Parsing algebraic word problems into equations. Transactions of the Association for Computational Linguistics, 3:585597. Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The Power of Scale for Parameter-Efficient Prompt Tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 30453059, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Xiang Lisa Li and Percy Liang. 2021. Prefix-Tuning: Optimizing Continuous Prompts for Generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582 4597, Online. Association for Computational Linguistics. Tianwei Lin, Jiang Liu, Wenqiao Zhang, Zhaocheng Li, Yang Dai, Haoyuan Li, Zhelun Yu, Wanggui He, Juncheng Li, Hao Jiang, and 1 others. 2024. Teamlora: Boosting low-rank adaptation with expert collaboration and competition. arXiv preprint arXiv:2408.09856. Tianwei Lin, Wenqiao Zhang, Sijing Li, Yuqian Yuan, Binhe Yu, Haoyuan Li, Wanggui He, Hao Jiang, Mengze Li, Xiaohui Song, and 1 others. 2025. Healthgpt: medical large vision-language model for unifying comprehension and generation via heterogeneous knowledge adaptation. arXiv preprint arXiv:2502.09838. Zefang Liu and Jiahua Luo. 2024. AdaMoLE: FineTuning Large Language Models with Adaptive Mixture of Low-Rank Adaptation Experts. arXiv preprint. ArXiv:2405.00361 [cs]. Tongxu Luo, Jiahe Lei, Fangyu Lei, Weihao Liu, Shizhu He, Jun Zhao, and Kang Liu. 2024. MoELoRA: Contrastive Learning Guided Mixture of Experts on Parameter-Efficient Fine-Tuning for Large Language Models. arXiv preprint. ArXiv:2402.12851 [cs]. Yuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han, Wen-tau Yih, and Madian Khabsa. 2022. UniPELT: Unified Framework for Parameter-Efficient Language Model Tuning. arXiv preprint. ArXiv:2110.07577 [cs]. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can suit of armor conduct electricity? new dataset for open book question answering. arXiv preprint arXiv:1809.02789. Mohammed Muqeeth, Haokun Liu, and Colin Raffel. 2024. Soft Merging of Experts with Adaptive Routing. arXiv preprint. ArXiv:2306.03745 [cs]. Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are nlp models really able to solve arXiv preprint 2021. simple math word problems? arXiv:2103.07191. Subhro Roy and Dan Roth. 2016. eral arithmetic word problems. arXiv:1608.01413. Solving genarXiv preprint Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106. Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. 2019. Socialiqa: Commonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538. Chunlin Tian, Zhan Shi, Zhijiang Guo, Li Li, and Chengzhong Xu. 2024. HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning. arXiv preprint. ArXiv:2404.19245 [cs]. An Wang, Xingwu Sun, Ruobing Xie, Shuaipeng Li, Jiaqi Zhu, Zhen Yang, Pinxue Zhao, J. N. Han, Zhanhui Kang, Di Wang, Naoaki Okazaki, and Chengzhong Xu. 2024. HMoE: Heterogeneous Mixture of Experts for Language Modeling. arXiv preprint. ArXiv:2408.10681 [cs]. Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word problems. arXiv preprint arXiv:1705.04146. Yaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan Awadallah, and Jianfeng Gao. 2022. AdaMix: Mixture-ofAdaptations for Parameter-efficient Model Tuning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 57445760, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Xun Wu, Shaohan Huang, and Furu Wei. 2024. arXiv preprint. Mixture of LoRA Experts. ArXiv:2404.13628 [cs]. Heming Xia, Yongqi Li, Chak Tou Leong, Wenjie Wang, and Wenjie Li. 2025. Tokenskip: Controllable chain-of-thought compression in llms. arXiv preprint arXiv:2502.12067. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, and 1 others. arXiv preprint 2025. Qwen3 technical report. arXiv:2505.09388. Ted Zadouri, Ahmet Üstün, Arash Ahmadian, Beyza Ermis, Acyr Locatelli, and Sara Hooker. 2023. Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning. arXiv preprint. ArXiv:2309.05444 [cs]. Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and Yu Qiao. 2023. LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention. arXiv preprint. ArXiv:2303.16199 [cs]. Wenqiao Zhang, Tianwei Lin, Jiang Liu, Fangxun Shu, Haoyuan Li, Lei Zhang, He Wanggui, Hao Zhou, Zheqi Lv, Hao Jiang, and 1 others. 2024. Hyperllava: Dynamic visual and language expert tuning for multimodal large language models. arXiv preprint arXiv:2403.13447. Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew Dai, Quoc Le, James Laudon, and 1 others. 2022. Mixture-ofexperts with expert choice routing. Advances in Neural Information Processing Systems, 35:71037114. Yun Zhu, Nevan Wichers, Chu-Cheng Lin, Xinyi Wang, Tianlong Chen, Lei Shu, Han Lu, Canoee Liu, Liangchen Luo, Jindong Chen, and Lei Meng. 2023. SiRA: Sparse Mixture of Low Rank Adaptation. arXiv preprint. ArXiv:2311.09179 [cs]."
        },
        {
            "title": "A PEFT Methods And The Unified Form",
            "content": "LoRA (Hu et al., 2021): LoRA injects trainable low-rank matrices into transformer layers to approximate the weight updates. For pretrained weight matrix Rdk, LoRA represents its update with low-rank decomposition +W = +W downW up, where down Rdr, up Rrk are tunable parameters. For specific input to the linear projection in the transformer layer, the computation of LoRA is as follows: = xW + α xW downW up, (6) where α is scalar hyperparameter. Parallel Adapter (He et al., 2022): Parallel Adapters involve adding small adapter modules in parallel to the Feed-Forward Network (FFN) or Attention blocks within Transformer layers. The adapter module is normally moulded by two-layer feed-forward neural network with bottleneck: down-projection with down Rdr to project the input to lower-dimensional space specified by bottleneck dimension r, followed by nonlinear activation function (), an up-projection with up Rrd to project back to the input size. The parallel adapter can be defined as: = (x) + (xW down)W up, (7) where () represents FFN or Attention block here. Prompt tuning (Zhang et al., 2023): An advanced zero-initialized prompt tuning method adaptively incorporates instructional signals while preserving the pre-trained knowledge in LLMs. In each transformer layer, let RKd denote the learnable prompts, the zero-initialized prompt tuning in the attention block computed as: = xW q, (8) = Attn(Q, xW k, xW v)+ Attn(Q, k, v), (9) where is learnable scalar for each head initialized with zero, q, k, are frozen pretrained matix. Note that the first term in Equation 9 is the original attention without prompts; only the second term includes trainable parameters. The unified form: As demonstrated above, LoRA, Parallel Adapter, and Prompt Tuning approaches can be simplified into unified form: = (x) + E(x), (10) where (x) denotes the transformer module to which the corresponding PEFT Adapter is applied."
        },
        {
            "title": "B Additional Experiments and Analyses",
            "content": "B.1 Comparison between Instance-level and Token-level Routing Methods We conducted comparative experiments on LLaMA-3.1 8B, evaluating an instance-level variant of our Soft MoA method and the UniPEFT approach. UniPEFT utilizes separate gated network for each type of adapter to control their influence on the input sample. As presented in Table 5, the results indicate that both instance-level approaches yield performance inferior to token-level routing methods. Furthermore, their performance is also surpassed by employing standard LoRA adaptation alone. This performance deficit can be attributed to key limitations of instance-level routing in PEFT of LLM. Firstly, these methods typically rely on the mean pooling of input text hidden states as the routers input, which fails to capture fine-grained characteristics of the sample adequately. Secondly, the constraint of making single routing choice for the entire sequence precludes the possibility of dynamically adjusting the contributions of different experts in response to the diverse contextual nuances within the input. Model UniPEFT Soft MoA Soft MoA Sparse MoA Routing level Commonsense15K Math14K instance instance token token 78.29 73.18 84.96 84.62 74.65 76.11 81.51 81.20 Table 5: Comparison between the token-level MoA approaches and instance-level methods. The instancelevel methods include an instance-level variant of Soft MoA and the UniPEFT method. B.2 Case Study of Router Weight Figures 6 and 7 illustrate the router weights for specific sample in particular layer for Soft MoA and Sparse MoA models, respectively. Figure 5: Visualization of average router weights per layer for Soft MoA (left) and Sparse MoA (right) on gsm8k, averaged over tokens within 50 samples. The Sparse MoA plot (right) also includes the average per-layer threshold and the average count of activated experts. The average count of activated experts across layers in Sparse MoA is 4.13. Figure 6: Router Weights of an example of at Layer 14 in Soft MoA. Figure 7: Router Weights of an example at Layer 14 in Sparse MoA. \"Threshold\" denotes the dynamic value of the threshold function for Sparse MoA. router weight of \"0.00\" for an expert indicates that the experts computation is skipped. Sparse MoA avoids unnecessary computations from the low-contribution expert for each token. Figure 8: Average router weights across layers. Soft MoA exhibits strong consistency under different random seeds in training, whereas AdaMoLE does not."
        }
    ],
    "affiliations": [
        "Tencent",
        "Zhejiang University"
    ]
}