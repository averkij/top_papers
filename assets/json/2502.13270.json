{
    "paper_title": "REALTALK: A 21-Day Real-World Dataset for Long-Term Conversation",
    "authors": [
        "Dong-Ho Lee",
        "Adyasha Maharana",
        "Jay Pujara",
        "Xiang Ren",
        "Francesco Barbieri"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Long-term, open-domain dialogue capabilities are essential for chatbots aiming to recall past interactions and demonstrate emotional intelligence (EI). Yet, most existing research relies on synthetic, LLM-generated data, leaving open questions about real-world conversational patterns. To address this gap, we introduce REALTALK, a 21-day corpus of authentic messaging app dialogues, providing a direct benchmark against genuine human interactions. We first conduct a dataset analysis, focusing on EI attributes and persona consistency to understand the unique challenges posed by real-world dialogues. By comparing with LLM-generated conversations, we highlight key differences, including diverse emotional expressions and variations in persona stability that synthetic dialogues often fail to capture. Building on these insights, we introduce two benchmark tasks: (1) persona simulation where a model continues a conversation on behalf of a specific user given prior dialogue context; and (2) memory probing where a model answers targeted questions requiring long-term memory of past interactions. Our findings reveal that models struggle to simulate a user solely from dialogue history, while fine-tuning on specific user chats improves persona emulation. Additionally, existing models face significant challenges in recalling and leveraging long-term context within real-world conversations."
        },
        {
            "title": "Start",
            "content": "REALTALK: 21-Day Real-World Dataset for Long-Term Conversation Dong-Ho Lee1*, Adyasha Maharana2, Jay Pujara1, Xiang Ren1,3, Francesco Barbieri 1University of Southern California 2Databricks Mosaic Research 3Sahara AI {dongho.lee, xiangren}@usc.edu, {adyasha.maharana}@databricks.com, {jpujara}@isi.edu, {fvancesco}@gmail.com 5 2 0 2 8 1 ] . [ 1 0 7 2 3 1 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Long-term, open-domain dialogue capabilities are essential for chatbots aiming to recall past interactions and demonstrate emotional intelligence (EI). Yet, most existing research relies on synthetic, LLM-generated data, leaving open questions about real-world conversational patterns. To address this gap, we introduce REALTALK, 21-day corpus of authentic messaging app dialogues, providing direct benchmark against genuine human interactions1. We first conduct dataset analysis, focusing on EI attributes and persona consistency to understand the unique challenges posed by realworld dialogues. By comparing with LLMgenerated conversations, we highlight key differences, including diverse emotional expressions and variations in persona stability that synthetic dialogues often fail to capture. Building on these insights, we introduce two (1) persona simulation benchmark tasks: where model continues conversation on behalf of specific user given prior dialogue context; and (2) memory probing where model answers targeted questions requiring long-term memory of past interactions. Our findings reveal that models struggle to simulate user solely from dialogue history, while fine-tuning on specific user chats improves persona emulation. Additionally, existing models face significant challenges in recalling and leveraging long-term context within real-world conversations."
        },
        {
            "title": "Introduction",
            "content": "Long-term open-domain dialogue capabilities are crucial for developing engaging chatbots that can remember previous interactions and respond with empathy, both of which are key aspects of emotional intelligence (EI) (Goleman, 1998; Mayer * Authors contributed equally. Work done during internship at Snap Inc. 1https://github.com/danny911kr/REALTALK 1 Figure 1: Motivation Example. LLM-simulated dialogues often exhibit excessive empathy, even when discussing negative topics, whereas real-world human dialogues demonstrate broader emotional spectrum, incorporate reflective and grounding language, and progressively develop intimacy over time. et al., 2008). However, evaluating large language models (LLMs) in this context is challenging. key obstacle lies in collecting real-world, humanto-human conversations featuring the same individuals over extended periods, ensuring consistent personasan important factor for passing the Turing test (Turing, 2009; Vinyals and Le, 2015). To address this, recent efforts have turned to LLMs to simulate long-term dialogues between two human participants, resulting in synthetic conversation datasets (Kim et al., 2023a; Jang et al., 2023a; Zhong et al., 2024; Du et al., 2024; Maharana et al., 2024). Yet, questions remain about whether these simulated dialogues genuinely capture the nuances of real-world human interaction. In this study, we introduce REALTALK, realworld, long-term dialogue dataset featuring pairs of individuals who initially connected through messaging apps. Over 21 days, 10 participants each engaged in two separate conversations with different partners, capturing evolving communication patterns. This resulted in total of 10 unique conversations, each spanning approximately 21 sessions. These daily or near-daily interactions amounted to over 16,000 words per conversation (3). The structure of these dialogues parallels the approach used in LOCOMO (Maharana et al., 2024), where LLMs engage in extended conversations, exchanging small talk, and the occasional image. By collecting human dialogues of comparable length, REALTALK enables direct comparisons between realworld and LLM-simulated conversations. Using our data, we analyze two key aspects of real-world long-term dialogues that distinguish them from LLM-simulated conversations. First, we examine emotional intelligence (EI) by comparing authentic human conversations with LLMgenerated dialogues using dialogue-level EI assessment (4 - 5.2). Our findings show that real conversations gradually build intimacy over time (Altman, 1973; Derks et al., 2008), featuring wide emotional range, whereas LLM-simulated chats often begin at high intimacy level, predominantly maintain positive tone, and display excessive empathyeven amid negative topics (See Figure 1). Next, we explore persona consistency, investigating whether individuals maintain stable persona across multiple conversations and assessing how well models capture these shifts or continuities over time (5.3). Our analysis reveals that human conversational styles naturally fluctuate based on context, reflecting variations in emotional intelligence. In contrast, LLMsdespite being prompted with distinct personasexhibit only minimal differences, struggling to adapt and replicate nuanced persona shifts over extended interactions. Building on these insights, we introduce two benchmark tasks to evaluate model performance in long-term dialogues. (1) Persona simulation: Maintaining consistent persona is key to fostering trust, engagement, and personalization in AI interactions. This task assesses how well an LLM can simulate an individuals unique persona by continuing conversation dialogue given prior context. We evaluate the models ability to reproduce users next message and match their EI attributes (6.1); (2) Memory probing: Long-term memory is essential for AI to sustain coherent, contextaware conversations. This benchmark tests whether models can retain and apply long-term context by answering 728 human-annotated memory probing questions linked to the conversation (6.2). Our experimental results show that models struggle to simulate user solely from dialogue history. However, fine-tuning on specific users chat history improves persona emulation. Additionally, existing models still face significant challenges in recalling and effectively leveraging long-term context in real-world conversations. To summarize, we introduce REALTALK, largescale, real-world dialogue dataset capturing 21 days of authentic, evolving conversations. Our analysis uncovers key differences in EI and persona consistency, showing that LLMs struggle to replicate human adaptability. To bridge this gap, we introduce two benchmarkspersona simulation and memory probingto drive the development of more human-like, memory-aware AI."
        },
        {
            "title": "2 Related Work",
            "content": "Long-term dialogues. Recent studies in longterm dialogue aim to improve model coherence and empathy through better memory recall of past interactions (See Table 1 for more details). Early work focused on collecting human-human dialogues from online sources (e.g., Reddit) or via crowd-sourcing to evaluate model in multi-session, open-domain conversations (Xu et al., 2022; Feng et al., 2022; Ahn et al., 2023; Li et al., 2017), and collecting human-AI dialogues to analyze user interactions and align models with user expectations (Zheng et al., 2023; Köpf et al., 2024; Zhao et al., 2024b). With LLMs now capable of processing longer contexts, recent studies simulate extensive human-human dialogues (Kim et al., 2023a; Jang et al., 2023b; Maharana et al., 2024) and human-AI dialogues (Zhong et al., 2024; Du et al., 2024; Wu et al., 2024) to improve model evaluation, addressing the difficulty of obtaining naturally long conversations. While LLM simulations show some human-like traits, notable differences may remain compared to actual human interactions (Hu et al., 2023; Kim et al., 2023b; Zhou et al., 2024; Mahowald et al., 2024; Ivey et al., 2024). In contrast to prior works, our work examines"
        },
        {
            "title": "Dialogue Participants",
            "content": "# Turns / # Session / # Tokens / Multimodal"
        },
        {
            "title": "Collection",
            "content": "MemoryBank (Zhong et al., 2024) LongMemEval (Wu et al., 2024) SODA (Kim et al., 2023a) Conversation Chronicles (Jang et al., 2023b) LoCoMo (Maharana et al., 2024) MPChat (Ahn et al., 2023) MMDialog (Feng et al., 2022) Daily Dialog (Li et al., 2017) MSC (Xu et al., 2022) REALTALK Human-AI Human-AI Human-Human Human-Human Human-Human Human-Human Human-Human Human-Human Human-Human Human-Human 3.7 9.8 7.6 58.5 588.2 2.8 4.6 7.9 53.3 894.4 10 50.2 1 5 27.2 1 1 1 4 21. 257.8 1,572.3 122.4 1,054.7 13,377.2 53.3 72.5 114.7 1,225.9 17,109.8 LLM-simulated LLM-simulated LLM-simulated LLM-simulated LLM-simulated"
        },
        {
            "title": "Reddit\nSocial media\nCrowdsourcing\nCrowdsourcing\nCrowdsourcing",
            "content": "Table 1: Comparison of data statistics across various datasets. Human-AI dialogues primarily address taskoriented interactions where humans aim to achieve specific goals through the dialogue. In contrast, Human-Human dialogues involve conversational exchanges such as chit-chat or other forms of social interaction. Unlike other works that simulate dialogues using models, REALTALK is entirely derived from real-world human interactions. memory recalling capability of LLMs within realworld long-term human-human dialogues, where informal cues may often challenge retention. Emotional Intelligence. Prior research on emotional intelligence before the emergence of large language models (LLMs) primarily focused on affective understanding, where models were trained to recognize and interpret human emotions and sentiments based on text and its surrounding context (Hu and Liu, 2004; Pang and Lee, 2004; Hutto and Gilbert, 2014; Rosenthal et al., 2017; Mohammad et al., 2018; Yin et al., 2020; Antypas et al., 2023) With the advent of LLMs, researchers have adapted or fine-tuned these models for affective tasks, such as predicting emotion intensity and classifying emotions in human text (Lei et al., 2023; Liu et al., 2024a; Zhang et al., 2023). Additionally, LLMs have been evaluated for their ability to infer how individuals might feel in specific scenarios (Wang et al., 2023; Paech, 2023; Huang et al., 2023; Zhao et al., 2024a). However, existing studies remain limited to sentiment and emotion analysis at the message or context level, without assessing the overall emotional intelligence of specific speaker across extended conversations. In contrast, our work investigates long-term, realworld human dialogues, analyzing both genuine human interactions and LLM-simulated conversations through comprehensive taxonomy of EI attributes. Beyond sentiment and emotion, we examine factors such as empathy, grounding acts, reflectiveness, and intimacy. Additionally, we use these EI attributes as metrics to evaluate how effectively model simulates specific persona over time."
        },
        {
            "title": "3 REALTALK",
            "content": "To study real-world human dialogues, we first collect REALTALK, dataset of 10 long-term conversations. We recruited 10 participants, pairing them to engage in conversations over 21 days. Once the conversations were collected, separate group of annotators, distinct from the participants, labeled memory probing question-answer pairs for each conversation to evaluate LLM memory retention capabilities. Additionally, they annotated speaker events for each session Si to track contextual information over time (3.2)."
        },
        {
            "title": "3.1 Data Collection",
            "content": "We provided specific guidelines to each participant, requiring them to send at least 50 messages to their conversation partner each day. This section outlines the participant details, guidelines, and dataset statistics. Participants. We recruited 10 participants who are native speakers from the US, aged between 18 and 25, with an approximately equal gender distribution. All participants provided signed release forms prior to the study. For more details regarding the participants, see Appendix A.1 Guidelines. We asked participants to engage in friendly, natural conversations that include small talk, personal stories, and occasional image sharing to create realistic chat experience. Conversations should feel casual and relatable, with references to time and place (e.g., last friday or when was kid) and should cover topics such as daily events, personal interests, and pop culture. Each session should feel like conversation between friends, with participants sharing opinions and experiences. Moreover, we encouraged participants to thoughtfully integrate images into the conversation, sourcing them from the internet under Creative Commons license and avoiding explicit descriptions of the image content. We also advised against using images with identifiable faces representing the participant directly and requested con3 sistency if such images are reused across sessions. Detailed guidelines are provided in Appendix B.1, and dataset statistics are discussed in 5.1."
        },
        {
            "title": "3.2 Annotation for Memory Probing",
            "content": "Memory probing is essential for evaluating an LLMs ability to retain and retrieve information from long-term conversations, key challenge in developing coherent, contextually aware AI systems. we recruited separate group of annotators to create memory-probing QA pairs and identify key events for each session Si. The detailed annotation guidelines provided to annotators are available in Appendix B.2. iSselected QA Annotations annotators generated questions that require referencing prior interactions to be correctly answered, following the procedure from Maharana et al. (2024). We categorize these questions into three types: (1) MULTI-HOP requires synthesizing information across multiple sessions, represented as (cid:80) Si, where Sselected denotes the subset of sessions chosen by the annotator; This evaluates models ability to connect dispersed details; (2) TEMPORAL REASONING involves reasoning over the sequence of events and time-based dependencies, assessing whether model can track evolving narratives and maintain temporal coherence; and (3) COMMONSENSE cannot be answered solely from the conversation and requires commonsense reasoning, testing models ability to integrate contextual dialogue with external knowledge. In total, 728 questions were annotated with answers, comprising 302 multi-hop, 321 temporal reasoning, and 111 commonsense questions (see details in Appendix A.3). Event Annotations For each session Si C, annotators are asked to document all events Ej Si occurring in each speakers life in free-text format. These events include those that have already happened, are planned for the future, or are ongoing during the conversation. Events can range from smaller occurrences, like taking cooking class, to major life events, such as enrolling in college or traveling to another country."
        },
        {
            "title": "4 Emotional Intelligence (EI) Evaluation",
            "content": "To systematically compare real-world and LLMgenerated dialogues, and to evaluate persona consistency and simulation quality, we develop comprehensive EI evaluation framework."
        },
        {
            "title": "4.1 Problem formulation.",
            "content": "Each conversation consists of multiple sessions Si, such that = {S1, S2, . . . , Sm}, where is the total number of sessions in the conversation. Each session Si consists of sequence of messages exchanged between two speakers such that Si = {Ms1,1, Ms2,2, . . . , Msn,n} where Msj ,j represents the j-th message in the session, sent by speaker sj {1, 2}. Unlike structured turn-taking, the order of messages can be variable, and speaker may send consecutive messages (e.g., multiple chat bubbles in row), reflecting the natural flow of realworld conversations. To maintain coherence, we concatenate consecutive messages from the same speaker into single message before analysis."
        },
        {
            "title": "4.2 Message-level EI Attributes.",
            "content": "We measure following EI attributes for each individual message M: Reflectiveness. R(M) is boolean indicator that captures whether speaker explicitly recognizes and describes their own emotions, thoughts, or reactions. For example, statements like think Im feeling this way because... signal self-reflection. We use an LLM-based classification to label each turn as reflective or not (see Appendix C.1). Grounding Act. G(M) is boolean indicator that captures whether speaker use actssuch as clarifying questions or follow-upswhich are essential for building common ground and preventing misunderstandings (Clark, 1996; Clark and Schaefer, 1989; Shaikh et al., 2024). We use an LLMbased classification to label each turn as grounding or not (see Appendix C.2). Emotion & Sentiment & Intimacy. E(M) and S(M) represent the speakers emotion and sentiment labels in message M, respectively. Emotion is classified into 11 predefined categories (e.g., anger, fear, joy), while sentiment is classified into 3 categories (positive, negative, neutral). I(M) is floating-point value that indicates the speakers level of intimacy in the message M. Each of these attributes is measured using models trained specifically for emotion, sentiment, and intimacy classification, leveraging an annotated dataset from Twitter (Antypas et al., 2023). While LLMs could be used for these tasks, fine-tuned models perform comparably or slightly better while being significantly more cost-efficient, making them more 4 practical choice for large-scale evaluation (Rathje et al., 2024). Empathy. EP (M) is floating-point score capturing the speakers empathy in message M. It consists of three components: emotional reaction, interpretation, and exploration. Each component is scored by the LLM on 02 Likert scale based on the EPITOME framework (Sharma et al., 2020) (see Appendix C.3). The empathy score for message is the sum of the scores for these components."
        },
        {
            "title": "4.3 Speaker-level EI Evaluation.",
            "content": "To assess speakers overall emotional intelligence (EI), we aggregate message-level EI attributes into five categories based on Golemans EI framework (Goleman, 1998). Self-awareness. It reflects speakers ability to recognize and articulate emotions and perspectives. It is measured using two metrics: (1) Reflective frequency measures how often speaker engages in self-reflection: reflective_frequency(s) = (cid:80) MMs R(M) Ms where Ms is the set of messages by speaker s. (2) Emotion & Sentiment Diversity captures the range of emotions or sentiments expressed by speaker s, calculated using entropy. Let represent the predefined sentiment labels (i.e., positive, negative, neutral), and ps(x) denote the proportion of label in the speakers messages Ms: (cid:80) MMs ps(x) = 1(S(M) = x) Ms , where 1(S(M) = x) equals 1 if the sentiment label of message is x, and 0 otherwise. Using this, sentiment diversity is computed as: sentiment_diversity(s) = (cid:88) xL ps(x)log2 ps(x). The same applies to emotion_diversity by substituting S(M) with E(M) and using emotion labels. Motivation speakers engagement in maintaining the conversation, measured by grounding frequency, the proportion of messages containing grounding acts: grounding_frequency(s) = (cid:80) MMs G(M) Ms Social Skills speakers ability to foster trust and engagement, measured via intimacy progression, assuming intimacy naturally evolves over time. The average intimacy for speaker per session Si is: intimacy_average(s, Si) = I(M) (cid:80) MM Si MSi where MSi is the set of messages from speaker in session Si, and I(M) denotes the intimacy score of message M. The progression of average intimacy across sessions in is modeled as: linear_progression(s) = a, where = + exp_progression(s) = b, where = ebx Self-regulation It reflects speakers ability to maintain emotional and sentiment stability while interacting with their conversational partner. We evaluate this using two metrics: (1) Emotion & Sentiment Stability measures the consistency of speakers emotions or sentiments across their messages: stability(s) = (cid:80)Ms k=2 1(E(M(k) ) = E(M(k1) )) Ms 1 where M(k) is the k-th message in speaker ss ordered sequence of messages, 1() is an indicator function that returns 1 if the emotion of M(k) matches the previous message M(k1) , and 0 otherwise. The same formula applies for sentiment stability, replacing E(M) with S(M). s (2) Emotion & Sentiment Alignment measures speakers synchronization with partner: alignment(s) = (cid:80) 1(E(M(k) ) = E(M(k) Ms )) and M(k) where M(k) are corresponding messages from speaker and their partner p, and 1() returns 1 if their emotions match and 0 otherwise. The same formula applies for sentiment alignment, substituting E(M) with S(M). Empathy of speaker ss messages, calculated as:"
        },
        {
            "title": "It measures the average empathy score",
            "content": "empathy(s) = (cid:80) MMs EP (M) Ms , where EP (M) is the empathy of message M. 5 Figure 2: Speaker-Level EI Comparison between REALTALK and LOCOMO (Maharana et al., 2024). Each bar represents the distribution of EI attributes for individual speakers in the conversation. For comprehensive evaluation, refer to Appendix E.1."
        },
        {
            "title": "5 Data Statistics and Analysis",
            "content": "5.2 vs. LLM-generated Dialogues This section analyzes the collected data, starting with general observations and statistics (5.1). Next, we compare our dataset with LLM-generated dialogues, focusing on speaker-level emotional intelligence (EI) metrics (5.2). Finally, we examine persona consistency in real-world conversations, leveraging the fact that each speaker participates in at least two conversations (5.3)."
        },
        {
            "title": "5.1 Data Statistics & General Observations",
            "content": "The dataset comprises 10 participant pairs engaging in 1621 days of conversation, with daily word counts ranging from 691 to 861. Participants share 2346 images per pair and revisit topics 68 times, showcasing sustained and rich interactions. Detailed statistics are provided in Appendix A.2. Three key characteristics distinguish real-world human dialogues from LLM-generated dialogues: First, Human dialogues are inherently noisy, featuring typos, abbreviations, acronyms, and slang (e.g., imo for in my opinion and dunno for dont know), reflecting the informal and natural flow of communication. Second, human conversations exhibit noticeable gaps between messages unlike LLM-generated dialogues, reflecting asynchronous and flexible communication. On average, participants take 20.98 minutes to respond, with median gap of 2.22 minutes. Some gaps extend up to 27.90 hours, highlighting the non-linear nature of real-world interactions (Appendix A.4). Third, human dialogues often include varying lengths of consecutive messages (i.e., chat bubbles) by the same speaker, reflecting diverse communication patterns. On average, participants send 2.31 consecutive messages per turn, with median of 2.00 and maximum of 68, showcasing occasional extended monologues (Appendix A.5). We evaluate speaker-level EI (4.3) in real-world and LLM-generated dialogues using the LOCOMO dataset (Maharana et al., 2024). Speaker-level EI is derived from message-level EI: reflectiveness, grounding acts, and empathy are computed using gpt-4o-mini, while sentiment, emotion, and intimacy are classified using task-specific finetuned RoBERTa models trained on labeled Twitter data (Antypas et al., 2023). Each speaker in each conversation receives an independent EI score. Figure 2 illustrates key differences between human (REALTALK) and LLM-generated (LOCOMO) dialogues by showing the distribution of speaker EI scores (full comparison in Appendix E.1): (1) Humans exhibit greater emotion and sentiment diversity, while LLMs remain constrained; (2) LLMs display excessive empathy; and (3) Humans show high variance in EI attributes, whereas LLMs are more uniform, aligning with prior research (Lee et al., 2024). High emotional intelligencecharacterized by strong grounding, emotional alignment, and intimacy progressionenhances engagement and relationship-building (Altman, 1973; Hatfield et al., 1993; Derks et al., 2008; Huang et al., 2017; Niederhoffer and Pennebaker, 2002). However, since human EI naturally varies across individuals and interactions, enforcing uniformly high EI in LLMs does not make them more human-like. Instead, tailoring LLMs to specific individuals better reflects the diverse dynamics of real human interactions."
        },
        {
            "title": "5.3 Persona Consistency Analysis",
            "content": "We examine how speakers adapt their emotional intelligence (EI) based on their conversational partners. Figure 3 shows absolute differences in EI attributes per participant, capturing persona vari6 measure how effectively the model simulates the speaker, we compare message-level EI attributes (4.2) of predicted and ground truth messages using (1) accuracy for categorical and boolean attributes (i.e., reflectiveness, grounding act, sentiment, and emotion); and (2) absolute difference for continuous attributes (i.e., intimacy, empathy). Lower absolute difference and higher accuracy indicate stronger simulation accuracy. Additionally, we compare lexical and semantic similarity between ˆMt and Mt, using ROUGE (Lin, 2004) and BERTScore (Zhang* et al., 2020)."
        },
        {
            "title": "6.1.2 Experimental Results.",
            "content": "We explore (1) the impact of conversational context, (2) its role during fine-tuning; and (3) the impact of fine-tuning on specific speaker. Impact of conversational context. We analyze how performance changes as the amount of provided context varies to assess the impact of conversational context. Figure 7 in Appendix E.3 shows that increasing conversation history does not improve message-level EI or exhibit clear pattern. These results suggest that LLMs struggle to capture and replicate speakers style using context alone. Impact of fine-tuning. We explore whether finetuning improves an LLMs ability to simulate speakers responses given conversational context. potential approach is to train on all Ca conversations and evaluate on Cb. However, this risks data leakage, as speaker in Ca may appear in Cb. To ensure fairness, we train and test each speaker separately. First, we train models using different amounts of conversation history as context and test them with the same number of sessions used in training. However, we observe no clear pattern indicating that increasing the amount of conversational context improves performance (See Appendix E.3). The results suggest that performance saturates after three sessions, implying that additional context beyond this point does not provide further benefits. However, when comparing fine-tuned model trained with three sessions of context to non-finetuned version, we find that the model does learn the speakers unique style. Table 2 presents the average performance across all speakers, with full results provided in Appendix E.2. The findings highlight three key insights: (1) Fine-tuning effectively captures speakers style, including emotion, sentiment, reflectiveness, grounding, and empathy; (2) While stylistic adaptation improves, content Figure 3: Persona Consistency. Each value represents the absolute difference in EI attributes for individual speakers across two conversations, capturing how their EI varies depending on their conversational partners. ability across interactions: (1) Some participants maintain stable persona, while others dynamically adapt their EI; and (2) Intimacy progression shows the most variation, suggesting that rapport-building is highly influenced by the specific conversational partner. These findings indicate that speakers persona is flexible and adjusts according to social dynamics, reflecting adaptive emotional behavior."
        },
        {
            "title": "6 REALTALK Benchmark",
            "content": "To test LLMs in long-term dialogues and advance human-like AI, we introduce: (1) Persona simulation, testing how well model replicates an individuals conversational style (6.1), and (2) Memory probing, assessing its ability to apply long-term context to answer probing questions (6.2)."
        },
        {
            "title": "6.1.1 Experimental Setup.\nWe simulate a speaker’s next message ˆMt based on\nprior conversation history Ht = {M1, ..., Mt−1}\nand compare it to the ground truth Mt. Each\nspeaker participates in two conversations (Ca and\nCb), allowing us to evaluate two baselines: (1)\nw/o fine-tuning: A generic LLM tested on Cb;\nand (2) w/ fine-tuning: An LLM fine-tuned on\nCa and tested on Cb. We choose the conversation\nwith lower overall EI as Cb. During both training\nand testing, we use the prompt “You are {speaker\nname}. Continue the conversation.” along with\nthe previous conversation history. The speaker’s\noriginal message serves as the ground truth for this\nprompt (See Appendix D.1 for prompt details). To",
            "content": ""
        },
        {
            "title": "Content Similarity",
            "content": "Message-level EI (Emotional Intelligence) Lexical Semantic Reflective Grounding Sentiment Emotion Intimacy Empathy w/o fine-tune w/ fine-tune 0.14 0.04 0.14 0.05 0.76 0.08 0.78 0.04 0.62 0.13 0.77 0.09 0.40 0.13 0.62 0.08 0.53 0.22 0.59 0. 0.43 0.22 0.46 0.21 0.06 0.01 0.07 0.01 1.80 0.55 1.24 0.12 Table 2: Average content similarity and message-level EI comparison in persona simulation for all speakers, with and without fine-tuning. Full results are available in Appendix E.2. Content similarity differences are not statistically significant, while message-level EI differences are significant (p < 0.02 for all attributes). similarity (lexical and semantic) remains largely unaffected; (3) Fine-tuning does not enhance intimacy, as intimacy emerges from mutual interaction rather than single speakers style. Overall, the results suggest that while the model does not learn how to use conversational context to improve its simulation, it can learn the stylistic patterns of speaker when trained exclusively on their messages."
        },
        {
            "title": "Input",
            "content": "Partial Match F"
        },
        {
            "title": "LLM Accuracy",
            "content": "M T gpt-4o-mini gpt-4o"
        },
        {
            "title": "C\nE",
            "content": "0.301 0.177 0.348 0.186 0.355 0.266 0.437 0.271 0.190 0.142 0.221 0. 0.528 0.253 0.519 0.266 0.549 0.424 0.737 0.501 0.599 0.400 0.621 0. Table 3: Partial match F1 & LLM Accuracy for memory probing QA task. (M=MULTI-HOP, T=Temporal, C=COMMONSENSE)"
        },
        {
            "title": "6.2.1 Experimental Setup",
            "content": "We evaluate model performance on memory probing questions using long dialogues C. We compare two baselines: (1) C: The full conversation is provided as input. (2) E: Only human-annotated key events are provided, simulating scenario where important details are stored as statements, common feature in recent closed LLM APIs2. We report partial exact match F1 score following prior works (Kwiatkowski et al., 2019; Maharana et al., 2024) and LLM-based accuracy, where gpt4o-mini determines whether the predicted answer matches the ground truth. See Appendix D.2 for prediction and evaluation prompt details."
        },
        {
            "title": "6.2.2 Experimental Results.",
            "content": "Table 3 presents memory probing results comparing full conversation context (C) and event-based memory (E). Overall performance. LLMs struggle with longterm memory even with full conversation (C). While full context improves performance, models achieve only moderate accuracy, highlighting their limitations in applying long-term information. Event-based memory. Event-based memory (E) enhances efficiency but severely weakens memory probing performance, especially in multi-hop reasoning by causing 41-46% performance drop. By condensing conversations into key points, it 2OpenAI Memory FAQ, OpenAI Memory and Controls often removes intermediary steps crucial for linking past and present context. Unlike temporal or commonsense reasoning, multi-hop tasks require reconstructing implicit connections, which eventbased memory fails to retain. This tradeoff mirrors real-world AI memory challenges, such as memory system in LLM APIs, which stores key facts but may lose essential context for multi-turn reasoning."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we introduce REALTALK, 21-day corpus of authentic messaging app dialoguesthe longest available dataset where the same two individuals engage in sustained conversations. Through detailed analysis, we highlight key differences between human conversations and LLM-simulated dialogues, revealing that LLMs exhibit limited emotion diversity, excessive empathy, and reduced variability in EI expression compared to real interactions. Using this data, we present two benchmarks: (1) Persona simulation, evaluating LLMs ability to replicate an individuals conversational style and showing that simple fine-tuning improves simulation accuracy. (2) Memory probing, assessing LLMs limitations in applying real-world longterm context and highlighting challenges in existing memory systems. Our findings underscore the complexity of modeling real-world persona dynamics and memory, emphasizing the need for further research. We hope REALTALK inspires advancements in socially intelligent AI, personalized dialogue systems, and adaptive memory strategies, driving more human-like interaction modeling."
        },
        {
            "title": "8 Limitations",
            "content": "Our study has several limitations: Demographic constraints. Participants are English speakers from the US, aged 18-25, limiting the datasets representativeness across diverse demographics. Additionally, we do not analyze how conversational dynamics vary based on factors such as gender, profession, geographic location, or age differences. Future studies should include more diverse demographic range to better understand how factors like age, gender, and cultural background influence dialogue patterns. Chit-chat focus. Our dataset primarily consists of casual, open-domain conversations rather than goal-oriented interactions. This limits its applicability to domains where social intelligence involves structured reasoning, such as negotiations or counseling (Zhou et al., 2023; Wu et al., 2024; Liu et al., 2024b). Future work should extend EI modeling to structured dialogues where speakers need to achieve their social goals. Emotional intelligence (EI) measurement. Our EI evaluation focuses on surface-level indicators, such as sentiment, reflectiveness, and empathy. However, these metrics may not fully capture deeper cognitive, cultural, and situational influences on emotional intelligence. Additionally, EI attributes are inherently subjective and challenging to quantify. To ensure reliability, we ground our evaluations in established literature. Lack of multi-modal analysis. While our dataset includes images, our experiments focus solely on text-based interactions. We do not analyze how visual elements contribute to emotional alignment, persona perception, or memory retention. Future research could integrate multi-modal fusion techniques to enhance emotional intelligence modeling."
        },
        {
            "title": "9 Ethical Considerations",
            "content": "We took several steps to ensure that our data collection was ethical and legal. We set the hourly rate of compensation for workers at $20. To ensure the safety and well-being of our workers, we maintained open communication channels, allowing them to voice any question, concerns, or feedback about the data annotation. This also helped to improve the quality of the collected data as we promptly addressed issues reported by workers throughout the process."
        },
        {
            "title": "Acknowledgments",
            "content": "Snap Inc. provided the majority of the funding for this work, with additional partial support from the Defense Advanced Research Projects Agency (DARPA) under award HR00112220046. We also used Sahara AIs data service platform for dataset construction. We would like to thank Mohit Bansal (UNC), Yuwei Fang (Snap Inc.), Sergey Tulyakov (Snap Inc.) for their valuable discussions and contributions to this work."
        },
        {
            "title": "References",
            "content": "Jaewoo Ahn, Yeda Song, Sangdoo Yun, and Gunhee Kim. 2023. Mpchat: Towards multimodal arXiv preprint persona-grounded conversation. arXiv:2305.17388. Irwin Altman. 1973. Social penetration: The development of interpersonal relationships. Rinehart, & Winston. Dimosthenis Antypas, Asahi Ushio, Francesco Barbieri, Leonardo Neves, Kiamehr Rezaee, Luis EspinosaAnke, Jiaxin Pei, and Jose Camacho-Collados. 2023. Supertweeteval: challenging, unified and heterogeneous benchmark for social media nlp research. In Findings of the Association for Computational Linguistics: EMNLP 2023. Herbert Clark. 1996. Using language. Cambridge university press. Herbert Clark and Edward Schaefer. 1989. Contributing to discourse. Cognitive science, 13(2):259 294. Daantje Derks, Agneta Fischer, and Arjan ER Bos. 2008. The role of emotion in computer-mediated communication: review. Computers in human behavior, 24(3):766785. Yiming Du, Hongru Wang, Zhengyi Zhao, Bin Liang, Baojun Wang, Wanjun Zhong, Zezhong Wang, and Kam-Fai Wong. 2024. Perltqa: personal long-term memory dataset for memory classification, retrieval, and synthesis in question answering. arXiv preprint arXiv:2402.16288. Jiazhan Feng, Qingfeng Sun, Can Xu, Pu Zhao, Yaming Yang, Chongyang Tao, Dongyan Zhao, and Qingwei Lin. 2022. Mmdialog: large-scale multi-turn dialogue dataset towards multi-modal open-domain conversation. arXiv preprint arXiv:2211.05719. Daniel Goleman. 1998. Working with emotional intelligence. NY: Bantam Books. Elaine Hatfield, John Cacioppo, and Richard Rapson. 1993. Emotional contagion. Current directions in psychological science, 2(3):96100. Jennifer Hu, Sammy Floyd, Olessia Jouravlev, Evelina Fedorenko, and Edward Gibson. 2023. finegrained comparison of pragmatic language understanding in humans and language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 41944213, Toronto, Canada. Association for Computational Linguistics. Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 168177. Jen-tse Huang, Man Ho Lam, Eric John Li, Shujie Ren, Wenxuan Wang, Wenxiang Jiao, Zhaopeng Tu, and Michael Lyu. 2023. Emotionally numb or empathetic? evaluating how llms feel using emotionbench. arXiv preprint arXiv:2308.03656. Karen Huang, Michael Yeomans, Alison Wood Brooks, Julia Minson, and Francesca Gino. 2017. It doesnt hurt to ask: Question-asking increases liking. Journal of personality and social psychology, 113(3):430. Clayton Hutto and Eric Gilbert. 2014. Vader: parsimonious rule-based model for sentiment analysis of social media text. In Proceedings of the international AAAI conference on web and social media, volume 8, pages 216225. Johnathan Ivey, Shivani Kumar, Jiayu Liu, Hua Shen, Sushrita Rakshit, Rohan Raju, Haotian Zhang, Aparna Ananthasubramaniam, Junghwan Kim, Bowen Yi, et al. 2024. Real or robotic? assessing whether llms accurately simulate qualities of human responses in dialogue. arXiv preprint arXiv:2409.08330. Jihyoung Jang, Minseong Boo, and Hyounghun Kim. 2023a. Conversation chronicles: Towards diverse temporal and relational dynamics in multi-session conversations. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1358413606, Singapore. Association for Computational Linguistics. Jihyoung Jang, Minseong Boo, and Hyounghun Kim. 2023b. Conversation chronicles: Towards diverse temporal and relational dynamics in multi-session conversations. arXiv preprint arXiv:2310.13420. Hyunwoo Kim, Jack Hessel, Liwei Jiang, Peter West, Ximing Lu, Youngjae Yu, Pei Zhou, Ronan Bras, Malihe Alikhani, Gunhee Kim, Maarten Sap, and Yejin Choi. 2023a. SODA: Million-scale dialogue distillation with social commonsense contextualization. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1293012949, Singapore. Association for Computational Linguistics. Hyunwoo Kim, Melanie Sclar, Xuhui Zhou, Ronan Bras, Gunhee Kim, Yejin Choi, and Maarten Sap. 2023b. FANToM: benchmark for stress-testing machine theory of mind in interactions. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1439714413, Singapore. Association for Computational Linguistics. Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens, Abdullah Barhoum, Duc Nguyen, Oliver Stanley, Richárd Nagyfi, et al. 2024. Openassistant conversations-democratizing large language model alignment. Advances in Neural Information Processing Systems, 36. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452466. Bruce Lee, Yeongheon Lee, and Hyunsoo Cho. 2024. Language models show stable value orientations across diverse role-plays. arXiv preprint arXiv:2408.09049. Shanglin Lei, Guanting Dong, Xiaoping Wang, Keheng Wang, and Sirui Wang. 2023. Instructerc: Reforming emotion recognition in conversation with retrieval multi-task llms framework. arXiv preprint arXiv:2309.11911. Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. 2017. Dailydialog: manually labelled multi-turn dialogue dataset. In Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 986995. Chin-Yew Lin. 2004. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain. Association for Computational Linguistics. Zhiwei Liu, Kailai Yang, Qianqian Xie, Tianlin Zhang, and Sophia Ananiadou. 2024a. Emollms: series of emotional large language models and annotation tools for comprehensive affective analysis. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 5487 5496. Ziyi Liu, Abhishek Anand, Pei Zhou, Jen-tse Huang, and Jieyu Zhao. 2024b. InterIntent: Investigating social intelligence of LLMs via intention understanding in an interactive game context. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 67186746, Miami, Florida, USA. Association for Computational Linguistics. 10 Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, and Yuwei Fang. 2024. Evaluating very long-term conversational memory of LLM agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13851 13870, Bangkok, Thailand. Association for Computational Linguistics. Kyle Mahowald, Anna Ivanova, Idan Blank, Nancy Kanwisher, Joshua Tenenbaum, and Evelina Fedorenko. 2024. Dissociating language and thought in large language models. Trends in Cognitive Sciences. John Mayer, Peter Salovey, and David Caruso. 2008. Emotional intelligence: New ability or eclectic traits? American psychologist, 63(6):503. Saif Mohammad, Felipe Bravo-Marquez, Mohammad Salameh, and Svetlana Kiritchenko. 2018. SemEval2018 task 1: Affect in tweets. In Proceedings of the 12th International Workshop on Semantic Evaluation, pages 117, New Orleans, Louisiana. Association for Computational Linguistics. Kate Niederhoffer and James Pennebaker. 2002. Linguistic style matching in social interaction. Journal of Language and Social Psychology, 21(4):337 360. Samuel Paech. 2023. Eq-bench: An emotional intelligence benchmark for large language models. arXiv preprint arXiv:2312.06281. Bo Pang and Lillian Lee. 2004. sentimental education: Sentiment analysis using subjectivity summaIn Proceedings rization based on minimum cuts. of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04), pages 271278, Barcelona, Spain. Steve Rathje, Dan-Mircea Mirea, Ilia Sucholutsky, Raja Marjieh, Claire Robertson, and Jay Van Bavel. 2024. Gpt is an effective tool for multilingual psychological text analysis. Proceedings of the National Academy of Sciences, 121(34):e2308950121. Sara Rosenthal, Noura Farra, and Preslav Nakov. 2017. SemEval-2017 task 4: Sentiment analysis in Twitter. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 502 518, Vancouver, Canada. Association for Computational Linguistics. Omar Shaikh, Kristina Gligoric, Ashna Khetan, Matthias Gerstgrasser, Diyi Yang, and Dan Jurafsky. 2024. Grounding gaps in language model generaIn Proceedings of the 2024 Conference of tions. the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 62796296, Mexico City, Mexico. Association for Computational Linguistics. Ashish Sharma, Adam Miner, David Atkins, and Tim Althoff. 2020. computational approach to understanding empathy expressed in text-based mental health support. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 52635276, Online. Association for Computational Linguistics. Alan Turing. 2009. Computing machinery and intelligence. Springer. Oriol Vinyals and Quoc Le. 2015. neural conversational model. arXiv preprint arXiv:1506.05869. Xuena Wang, Xueting Li, Zi Yin, Yue Wu, and Jia Liu. 2023. Emotional intelligence of large language models. Journal of Pacific Rim Psychology, 17:18344909231213958. Di Wu, Hongwei Wang, Wenhao Yu, Yuwei Zhang, Kai-Wei Chang, and Dong Yu. 2024. Longmemeval: Benchmarking chat assistants on long-term interactive memory. arXiv preprint arXiv:2410.10813. Jing Xu, Arthur Szlam, and Jason Weston. 2022. Beyond goldfish memory: Long-term open-domain conversation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 51805197. Da Yin, Tao Meng, and Kai-Wei Chang. 2020. SentiBERT: transferable transformer-based architecture for compositional sentiment semantics. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 36953706, Online. Association for Computational Linguistics. Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: EvalIn International uating text generation with bert. Conference on Learning Representations. Yazhou Zhang, Mengyao Wang, Prayag Tiwari, Qiuchi Li, Benyou Wang, and Jing Qin. 2023. Dialoguellm: Context and emotion knowledge-tuned llama models for emotion recognition in conversations. arXiv preprint arXiv:2310.11374. Weixiang Zhao, Zhuojun Li, Shilong Wang, Yang Wang, Yulin Hu, Yanyan Zhao, Chen Wei, and Bing Qin. 2024a. Both matter: Enhancing the emotional intelligence of large language models without compromising the general intelligence. arXiv preprint arXiv:2402.10073. Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. 2024b. Wildchat: 1m chatgpt interaction logs in the wild. arXiv preprint arXiv:2405.01470. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric Xing, et al. 2023. Lmsyschat-1m: large-scale real-world llm conversation dataset. arXiv preprint arXiv:2309.11998. 11 Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. 2024. Memorybank: Enhancing large language models with long-term memory. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1972419731. Xuhui Zhou, Zhe Su, Tiwalayo Eisape, Hyunwoo Kim, and Maarten Sap. 2024. Is this the real life? is this just fantasy? the misleading success of simulating social interactions with llms. arXiv preprint arXiv:2403.05020. Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, et al. Interactive evaluation for social 2023. Sotopia: arXiv preprint intelligence in language agents. arXiv:2310.11667."
        },
        {
            "title": "A Dataset Details",
            "content": "A.1 Participants All participants are native English speakers from the US, aged between 18 and 25, with roughly equal distribution of genders. Each participant provided signed release form before the study began. Participant details are shown in Table 4."
        },
        {
            "title": "Chat",
            "content": "MULTI-HOP TEMPORAL COMMONSENSE EVENTS Emi + Elise Elise + Kevin Kevin + Paola Paola + Emi Nicolas + Nebraas Vanessa + Nicolas Vanessa + Nebraas Akib + Muhammed Fahim + Akib Fahim + Muhammed 30 30 30 30 31 30 30 30 30 30 30 31 30 30 31 30 30 31 31 45 10 12 11 10 14 12 12 10 9 10 44 41 51 61 61 84 90 83 29 Table 6: QA & Event Statistics"
        },
        {
            "title": "Age",
            "content": "A.4 Temporal gap between speaker"
        },
        {
            "title": "Emi\nCollege student New York\nElise\nCollege student Houston\nKevin\nCollege student Houston\nPaola\nCollege student Cambridge\nNebraas\nVet technician\nNew York\nNicolas\nCollege student New York\nNew York\nVet technician\nVanessa\nMohammed College student New York\nCollege student New York\nSyed\nCollege student New York\nFahim",
            "content": "20 21 18 21 24 23 24 23 23 19 Table 4: Participants Information A.2 Chat Statistics Eight out of ten chat sessions have reached the target word count of 16,000. The data has been cleaned, with all incorrectly formatted links and images replaced. Images from external sources are attached in their original resolution (3024x4032). Participant-provided images from their camera rolls, which were unexpected, have been exported directly from WhatsApp without any modifications. Statistics details are in presented in Table 5."
        },
        {
            "title": "Chat",
            "content": "# days # words # words / day"
        },
        {
            "title": "Images Topics",
            "content": "Emi + Elise Elise + Kevin Kevin + Paola Paola + Emi Nicolas + Nebraas Vanessa + Nicolas Vanessa + Nebraas Akib + Muhammed Fahim + Akib Fahim + Muhammed 21 21 16 16 21 21 21 21 21 21 17341 16040 11057 11511 16902 18005 16343 17191 18089 16951 826 764 691 719 805 857 778 819 861 807 35 46 35 37 37 39 43 25 27 23 6 7 7 6 7 8 8 6 7 Table 5: Chat Statistics A.3 QA & Event Statistics We recruited separate group of annotators to create memory-probing QA pairs and identify key events for each session Si. In total, 728 questions were annotated with answers, including 302 multihop, 321 temporal reasoning, and 111 commonsense questions. Additionally, 600 events were annotated for each speaker in each session (see Table 6). 13 Figure 4 shows the distribution of temporal gaps between messages from two speakers. Figure 4: Temporal gap distribution between messages from two speakers A.5 Lengths of consecutive messages Figure 5 illustrates the distribution of consecutive messages per speaker, highlighting the frequency of different sequence lengths. Figure 5: Frequency distribution of consecutive messages by single speaker."
        },
        {
            "title": "B Annotation Guidelines",
            "content": "B.1 Chat Collection Guidelines"
        },
        {
            "title": "Chat Guidelines for Participants",
            "content": "First-Time Interaction. You are interacting with the other speaker for the first time. Chit-chat. Engage in chit-chat that can include real events from your own life (e.g., taking nap or cooking something). The content can be fictional. Personal Information. Make the conversation personal from time to time by discussing topics like family, friends, likes, dislikes, and aspirations. The content can be fictional. Time References. Include references to time (e.g., last Friday, next month, when was ten years old) and specific places or locations. Consider the current time during the conversation; for example, if its after lunch, ask what the other participant had for lunch, or greet them with Good Morning if chatting in the morning. Dialog Style. Keep the dialogue style casual, as if talking to friend. Daily Events and News. Discuss events from your life, news, social media highlights, or pop culture events (e.g., movies, concerts). Feel free to share opinions in friendly, engaging way that may interest the other participant. Images. Share images at appropriate points in the conversation. Examples include: things you own (clothes, food, vehicles, etc.), things youve seen or want to see (e.g., food, people, events), things you like (e.g., piece of clothing, cute animal), things you need help with (e.g., how to fix something or navigate relationship), images representing old memories. Image Source. Do not share images from your camera. Instead, search for images on the internet using Google, save the image URL, and include it in square brackets within the conversation. [URL of the image] Images should be licensed under Creative Commons to allow free use by others. In Google Search, such images can be found by selecting the Creative Commons license under Tools Usage Rights. Using the dimension constraint imagesize:3024x4032 in the query yields less generic images that can foster engaging conversations. Images Containing Faces. Avoid sharing images where persons face is clearly visible and implies that it is you. However, you may share images where the face is obscured or clearly not you. Ensure any appearance remains approximately consistent throughout sessions. Image Context. Avoid sharing images that dont add information to the conversation. For example, avoid generic images that lack context or relevance, but do share images that add context, like one showing user kayaking. Pronoun Use. Refer to previously mentioned subjects in the conversation with pronouns, allowing the other participant to easily infer the reference. B.2 Question & Event Annotation Guidelines Question & Event Annotation Guidelines Introduction. These guidelines provide instructions for annotating dataset consisting of long-term conversations (approximately 21 days) between two individuals. The annotation task is divided into two sub-tasks: Events Annotation: Annotators document significant life events for each speaker in each session. Question and Answer Creation: Annotators generate questions that can only be answered by reading through one or more sessions of conversations. The dataset includes both textual and image-based conversations. Data Format. The conversation dataset consists of multiple interaction sessions between two speakers. Each session is timestamped. 14 Conversations are structured in table format with dedicated columns for each speaker. Speaker names appear at the top of the table. If speaker shares an image, it is included in the dataset. Sub-Task 1: Writing Speaker Events. Annotators document key life events for each speaker as the conversation progresses. These events may include: Past events, ongoing events, and planned future events. Small-scale events (e.g., attending cooking class) and major events (e.g., moving to new city). Annotation Format: Each event should be recorded in separate line within the allotted cell. Sub-Task 2: Creating Questions and Answers. Annotators generate questions that can only be answered by reviewing the conversation dataset. Question Categories and Examples: Questions that Require Aggregation of Information. Question: Which countries has Joseph travelled to? Answer: France, Japan Evidence: D4:1, D1:8 Category: Questions that Require Reasoning About Time. Question: When did Kate start skiing? Answer: 2013 Evidence: D1:49 Category: 2 Questions that Require Commonsense Reasoning or World Knowledge. Question: What kind of jobs might Joseph consider based on his recent ventures? Answer: Jobs that combine software engineering and management, e.g., software product manager Evidence: D2:9, D3:1 Category: 3 How to Annotate Questions: Each question entry includes the following fields: Question: The formulated question. Answer: The exact response extracted from the conversation. Evidence: Unique identifiers of dialogues used to derive the answer. Category: label (1-3) indicating the question type. Annotators should write at least 2 and up to 10 questions from any of the categories at the end of each session. Message-level EI Attributes In this section, we provide prompt details for reflectiveness, grounding act, and empathy. Also, we present model details for emotion, sentiment, and intimacy. C.1 Prompt for Reflectiveness"
        },
        {
            "title": "Reflectiveness Classification",
            "content": "You are an evaluator trained to determine if speakers language is reflective, indicating self-awareness. Reflective language is characterized by self-observation, perspective-taking, and intentionality. This means that the speaker is not only aware of their thoughts, feelings, or actions but also able to express this awareness clearly. reflective response often includes one or more of the following traits: Self-observation: The speaker describes their own emotional or cognitive state (e.g., feel uncertain about. . . or Im aware that. . . ). Perspective-taking: The speaker shows an understanding of how their actions or emotions affect others or acknowledges another persons perspective on the situation (e.g., understand that my response may seem. . . ). Intentionality: The speaker explains the reasoning behind their behavior or decisions, revealing their underlying motivations or goals (e.g., decided to respond this way because. . . )."
        },
        {
            "title": "Example Statements",
            "content": "15 \"I realize tend to get defensive when receive feedback, and think its because want to do well.\" Reflective or Not Reflective: Reflective Reason: This statement shows self-observation (I realize tend to get defensive) and insight into motivation (because want to do well). \"I did what thought was best for the project.\" Reflective or Not Reflective: Not Reflective Reason: While the speaker describes their decision, they dont analyze or acknowledge the emotions or motivations behind their choice or consider its impact on others. Given this dialogue context: {dialogue_history_within_session} Determine whether the speakers last message ({turn}) is reflective or not. Reflective language includes phrases like feel..., think..., or similar reflective expressions. Respond only with True for reflective or False for not reflective. C.2 Prompt for Grounding Act"
        },
        {
            "title": "Grounding Act Classification",
            "content": "You are an evaluator trained to determine if speakers language demonstrates grounding, which reflects active engagement and commitment to mutual understanding in conversation. Grounding acts are characterized by clarifying questions, follow-up inquiries, or statements that seek to confirm, clarify, or expand on shared information. These acts are essential for building common ground, ensuring that both participants have clear understanding, and preventing misunderstandings. grounding response often includes one or more of the following traits: Clarifying questions: The speaker asks questions that seek clarification or further information about the other persons statements (e.g., Could you explain that further? or What did you mean by...?). Follow-up inquiries: The speaker shows interest in exploring point raised by the other person, prompting them to elaborate or continue sharing (e.g., How did that make you feel? or Can you tell me more about...?). Confirmation checks: The speaker seeks to confirm their understanding of what the other person said (e.g., So, you mean that...? or Are you saying that...?)."
        },
        {
            "title": "Example Statements",
            "content": "\"Can you tell me more about what happened at the event?\" Grounding or Not Grounding: Grounding Reason: This is follow-up question that prompts the other person to provide more information, demonstrating interest and desire to deepen mutual understanding. \"I completely understand your point.\" Grounding or Not Grounding: Not Grounding Reason: Although this statement indicates agreement, it does not actively seek further information or clarification and does not encourage continued dialogue. \"So, youre saying that this new policy will impact the timeline?\" Grounding or Not Grounding: Grounding Reason: This is confirmation check, as the speaker seeks to ensure their understanding of the other persons statement. \"It sounds like youve already made your decision.\" Grounding or Not Grounding: Not Grounding Reason: This statement reflects an observation rather than clarifying or follow-up question, so it does not serve as grounding act. C.3 Prompt for Empathy"
        },
        {
            "title": "Empathy Assessment",
            "content": "You are an evaluator assessing the level of empathy conveyed in response, based on three core components: Emotional Reaction, Interpretation, and Exploration. For each component, provide score from 02, where 0 indicates no presence, 1 indicates partial presence, and 2 indicates explicit presence. Sum the scores from each component to obtain an overall empathy score. 16 Component 1: Emotional Reaction Does the response express or allude to warmth, compassion, concern, or similar feelings of the responder towards the seeker? 0: No. 1: Yes, the response alludes to these feelings but the feelings are not explicitly expressed. 2: Yes, the response has an explicit mention. Component 2: Interpretation Does the response communicate an understanding of the seekers experiences and feelings? In what manner? 0: No. 1: Yes, the response communicates an understanding of the seekers experiences and/or feelings. The response contains conjectures or speculations about the seekers experiences and/or feelings. The responder reflects back on similar experiences of their own or others. The responder describes similar experiences of their own or others. The response paraphrases the seekers experiences and/or feelings. 2: The response provides deep, explicit understanding and validation of the seekers feelings or experiences, potentially using multiple sub-categories. Component 3: Exploration Does the response make an attempt to explore the seekers experiences and feelings? 0: No. 1: Yes, the exploration is present but remains generic. 2: Yes, the exploration is present and is specific, delving into the seekers particular feelings or experiences."
        },
        {
            "title": "Output Format",
            "content": "Return output in the following JSON format: { } \"emotional_reaction\": [02], \"interpretation\": [02], \"exploration\": [02] Persona Simulation & Memory Probing In this section, we detail the prompts used to evaluate LLM performance on persona simulation and memory probing tasks. We specify the assigned roles (i.e., system, user, assistant) for each prompt in our experiments. D.1 Persona Simulation The input consists of system and user prompt, where the system defines the users persona and task, while the user prompt provides the dialogue history. The output serves as ground truth for training. D.1."
        },
        {
            "title": "Input",
            "content": "System: You are {opponent_speaker}. Continue the conversation. Output only the message, not the speaker name. 17 User: {previous conversation} {speaker} D.1.2 Output (Ground Truth) Assistant: {speakers original message} D.2 Memory Probing For the memory probing task, we use only user role messages. All conversation history is formatted consistently, with sessions divided by date, as shown in the following example. If message contains an image, we convert it into caption using BLIP. User: Below is conversation between Alice and Bob, spanning multiple days. Each session starts with the corresponding date. DATE: February 10, 2025 CONVERSATION: Alice: \"Hey, how have you been?\" Bob: \"Im good, just busy with work. How about you?\" Alice: \"Same here, lots of meetings this week.\" Alice shared an image of \"papers on the desk.\" Based on the provided context, generate concise short-phrase answer for the following question. If the question pertains to date, infer the approximate timeframe (e.g., \"In the 1800s\", \"Before Jan 2021\", etc.). Question: What has Alice been busy with? Answer:"
        },
        {
            "title": "E Experimental Results",
            "content": "E.1 Full Performance of Speaker-level EI Figure 6 presents full comparison of speaker-level EI between REALTALK and LOCOMO (Maharana et al., 2024), covering all 11 evaluated attributes. Table 7 provides the detailed speaker-level EI metrics for REALTALK, which serve as the basis for Figure 6. P1 P"
        },
        {
            "title": "Paola\nFahim",
            "content": "Self-awareness"
        },
        {
            "title": "Empathy",
            "content": "Social Skills (1e-3) Self-regulation Reflect. freq. Emo. div. Sent. div. Ground. freq."
        },
        {
            "title": "EPITOME",
            "content": "Int. prog. (lin) Int. prog. (exp.) Sent. stab. Emo. stab. Sent. align. Emo. align. 0.14 0.08 0.06 0.05 0.04 0.09 0.07 0.07 P2 0.11 0.06 0.12 0.07 0.10 0.03 0.08 0.08 P1 P2 P2 P1 P2 P1 P2 P2 2.21 1.11 2.63 1.33 2.74 2.08 1.04 2.53 2.41 1.15 2.61 1.53 2.62 2.41 1.27 2.32 1.43 0.67 1.48 0.80 1.44 1.46 0.55 1.44 1.53 0.52 1.56 1.03 1.44 1.41 0.66 1.47 0.11 0.43 0.04 0.44 0.15 0.19 0.52 0. 0.04 0.44 0.05 0.28 0.27 0.15 0.41 0.11 1.54 3.23 0.87 2.80 0.92 2.35 3.19 0.92 1.50 3.53 1.32 2.29 1.86 1.43 3.20 1.29 -0.2 0.6 -0.1 1.0 0.1 -0.3 -0.1 -0.4 -0.4 0.5 -0.4 1.0 -0.4 0.0 -0.1 -0.3 -0.4 1.1 -0.2 1.8 0.1 -0.6 -0.2 -0.8 P2 P1 P2 P1 P1 P2 P1 P2 -0.7 0.9 -0.7 1.7 -0.6 0.0 -0.3 -0.5 0.46 0.75 0.46 0.61 0.44 0.47 0.80 0. 0.50 0.82 0.37 0.58 0.46 0.48 0.72 0.47 0.38 0.68 0.29 0.58 0.24 0.42 0.68 0.31 0.32 0.66 0.26 0.51 0.25 0.32 0.59 0.34 0.47 0.86 0.45 0.67 0.45 0.46 0.71 0.48 0.55 0.78 0.43 0.57 0.49 0.54 0.73 0.36 0.36 0.74 0.35 0.62 0.24 0.29 0.59 0. 0.40 0.67 0.26 0.55 0.24 0.38 0.70 0.26 Table 7: Full Speaker-Level EI Datapoints of REALTALK. 18 Figure 6: Full Speaker-Level EI Comparison between REALTALK and LOCOMO (Maharana et al., 2024). Each bar represents the distribution of EI attributes across individual speakers in the dataset. E.2 Full Performance of Persona Simulation Table 8 provides the complete results for content similarity and message-level EI comparison in persona simulation across all speakers, with and without fine-tuning. Table 2 is derived by averaging these values. Persona consistency in the table is measured as the absolute difference between speakers average EI across two different conversations. This approximates the impact of persona consistency on simulation performance. Results indicate that simulation performance improves when the difference is small, suggesting that speakers with more consistent personas across conversations are easier to simulate. E.3 Context Experiments for Persona Simulation Impact of conversational context. We analyze how performance changes with varying amounts of provided context. Figure 7 (left) shows that increasing conversation history does not improve messagelevel EI or exhibit clear pattern. These findings suggest that LLMs struggle to capture and replicate speakers style using context alone. Impact of fine-tuning. We examine whether fine-tuning enhances an LLMs ability to simulate speakers responses given conversational context. Models are trained using different amounts of conversation history and tested with the same number of sessions used in training. Figure 7 (right) shows that fine-tuning improves performance compared to the non-finetuned model. However, there is no clear trend indicating that increasing context length leads to further improvements. Performance appears to saturate 19 Speaker Persona Consistency Train Test Finetune Content Similarity Style Similarity Lexical Semantic Reflective Grounding Sentiment Emotion Intimacy Empathy Emi 0.21 Emi-Paola Emi-Elise Nicolas 0.09 Nicolas-Nebraas Vanessa-Nicolas Kevin Akib 0. 0.07 Kevin-Paola Kevin-Elise Fahim-Akib Akib-Muhhamed Muhhamed 0.11 Fahim-Muhhamed Akib-Muhhamed Nebraas 0.2 Nicolas-Nebraas Nebraas-Vanessa Paola 0.17 Emi-Paola Kevin-Paola Vanessa 0. Nebraas-Vanessa Vanessa-Nicolas Elise 0.14 Kevin-Elise Emi-Elise Fahim Khan 0.12 Fahim-Muhhamed Fahim-Akib 0.2 0.2 0.11 0.09 0.19 0.19 0.09 0. 0.1 0.1 0.15 0.15 0.2 0.2 0.11 0.12 0.11 0.14 0.1 0. 0.84 0.84 0.82 0.81 0.8 0.8 0.72 0.72 0.73 0.73 0.81 0. 0.78 0.78 0.8 0.8 0.57 0.77 0.73 0.73 0.78 0.81 0.64 0. 0.76 0.72 0.53 0.61 0.46 0.72 0.74 0.87 0.68 0.71 0.68 0. 0.54 0.84 0.42 0.71 0.6 0.63 0.38 0.73 0.52 0.53 0.25 0. 0.26 0.68 0.35 0.66 0.59 0.52 0.43 0.63 0.29 0.5 0.29 0. 0.85 0.85 0.38 0.45 0.86 0.86 0.25 0.43 0.39 0.46 0.5 0. 0.8 0.8 0.44 0.45 0.46 0.61 0.36 0.43 0.71 0.76 0.21 0. 0.68 0.64 0.15 0.24 0.29 0.29 0.37 0.42 0.77 0.77 0.34 0. 0.43 0.58 0.32 0.35 0.06 0.07 0.08 0.09 0.05 0.06 0.05 0. 0.06 0.07 0.07 0.09 0.05 0.05 0.08 0.09 0.06 0.08 0.05 0. 1.56 1.43 2.2 1.18 1.32 1.27 2.33 1.38 2.36 1.22 2.0 1. 0.87 1.05 1.9 1.22 1.11 1.33 2.35 1.1 Table 8: Content similarity and message-level EI comparison in persona simulation for all speakers, with and without fine-tuning. after three sessions, suggesting that additional context beyond this point provides minimal benefit. Figure 7: Message-Level EI in simulation for speaker Akib across different amounts of conversation history. (Left) Zero-shot performance with varying conversation history lengths. (Right) Fine-tuned performance with different amounts of conversation history. Each point represents model trained on specific number of sessions and tested on the same number."
        }
    ],
    "affiliations": [
        "Databricks Mosaic Research",
        "Sahara AI",
        "University of Southern California"
    ]
}