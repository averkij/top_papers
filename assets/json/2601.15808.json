{
    "paper_title": "Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification",
    "authors": [
        "Yuxuan Wan",
        "Tianqing Fang",
        "Zaitang Li",
        "Yintong Huo",
        "Wenxuan Wang",
        "Haitao Mi",
        "Dong Yu",
        "Michael R. Lyu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in Deep Research Agents (DRAs) are transforming automated knowledge discovery and problem-solving. While the majority of existing efforts focus on enhancing policy capabilities via post-training, we propose an alternative paradigm: self-evolving the agent's ability by iteratively verifying the policy model's outputs, guided by meticulously crafted rubrics. This approach gives rise to the inference-time scaling of verification, wherein an agent self-improves by evaluating its generated answers to produce iterative feedback and refinements. We derive the rubrics based on an automatically constructed DRA Failure Taxonomy, which systematically classifies agent failures into five major categories and thirteen sub-categories. We present DeepVerifier, a rubrics-based outcome reward verifier that leverages the asymmetry of verification and outperforms vanilla agent-as-judge and LLM judge baselines by 12%-48% in meta-evaluation F1 score. To enable practical self-evolution, DeepVerifier integrates as a plug-and-play module during test-time inference. The verifier produces detailed rubric-based feedback, which is fed back to the agent for iterative bootstrapping, refining responses without additional training. This test-time scaling delivers 8%-11% accuracy gains on challenging subsets of GAIA and XBench-DeepResearch when powered by capable closed-source LLMs. Finally, to support open-source advancement, we release DeepVerifier-4K, a curated supervised fine-tuning dataset of 4,646 high-quality agent steps focused on DRA verification. These examples emphasize reflection and self-critique, enabling open models to develop robust verification capabilities."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 2 ] . [ 1 8 0 8 5 1 . 1 0 6 2 : r Technical Report Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification Yuxuan Wan , Tianqing Fang, Zaitang Li , Yintong Huo , Wenxuan Wang , Haitao Mi , Dong Yu , and Michael R. Lyu The Chinese University of Hong Kong, Tencent AI Lab, Singapore Management University, Renmin University of China https://github.com/Tencent/CognitiveKernel-Pro https://github.com/yxwan123/DeepVerifier Figure 1: Upper: Inference-time scaling of verification on the full GAIA development set (n = 165). Lower: Performance comparison between DeepVerifier-8B fine-tuned on our dataset and other open-sourced models after 10 rounds of verification & feedback on the full GAIA development set."
        },
        {
            "title": "Abstract",
            "content": "Recent advances in Deep Research Agents (DRAs) are transforming automated knowledge discovery and problem-solving. While the majority of existing efforts focus on enhancing policy capabilities via post-training, we propose an alternative paradigm: self-evolving the agents ability by iteratively verifying the policy models outputs, guided by meticulously crafted rubrics. This approach gives rise to the inference-time scaling of verification, wherein an agent self-improves by evaluating its generated answers to produce iterative feedback and refinements. We derive the rubrics based on an automatically constructed DRA Failure Taxonomy, which systematically classifies agent failures into five major categories and thirteen sub-categories. We present DeepVerifier, rubrics-based outcome reward verifier that leverages the asymmetry of verification and outperforms vanilla agent-as-judge and LLM judge baselines by 12%48% in meta-evaluation F1 score. To enable practical self-evolution, DeepVerifier integrates as plug-and-play module during test-time inference. The verifier produces detailed rubric-based feedback, which is fed back to the agent for iterative bootstrappingrefining responses without additional training. This test-time scaling delivers 8%11% accuracy gains on challenging subsets of GAIA and XBench-DeepResearch when powered by capable closed-source LLMs. Finally, to support open-source advancement, we release DeepVerifier-4K, curated supervised fine-tuning dataset of 4,646 high-quality agent steps focused on DRA verification. These examples emphasize reflection and self-critique, enabling open models to develop robust verification capabilities. Correspondence to: Yuxuan Wan yxwan@link.cuhk.edu.hk and Tianqing Fang tianqfang@tencent.com. 1 Technical Report"
        },
        {
            "title": "Introduction",
            "content": "Recent advances in Deep Research Agents (DRAs), powered by large language models (LLMs) and vision-language models (VLMs), are transforming automated knowledge discovery and complex problem-solving. These systems demonstrate strong performance on tasks requiring coding, web navigation, file processing, and multi-step reasoning. However, DRAs remain prone to unreliable outputs stemming from incorrect actions, API failures, hallucinations, or other errors (Song et al., 2025; Li & Waldo, 2024), which significantly constrain their practical deployment (Zhang et al., 2025a). For instance, when tasked with identifying researchers earliest publication, an agent might rely on incomplete secondary sources and deliver an inaccurate result. In long-horizon tasks involving dozens of pages and hundreds of actions, online human supervision becomes infeasible. These challenges underscore the need for scalable, automated methods to enhance DRA reliability and performance at test time (Zhu et al., 2025c; Hu et al., 2025a). Prior work on inference-time improvement has largely emphasized scaling output tokens or selection across parallel rollouts. For example, Zhu et al. (2025c) introduced parallel sampling for optimal trajectory search, while Gonzalez-Pumariega et al. (2025) employed narrative-driven aggregation across iterations. Despite existence of Reflexion (Shinn et al., 2023)-based methods use textual feedback (Zhou et al., 2025b; Yuksekgonul et al., 2024) to bootstrap the agent response, the generation of feedback itself is hard task that requires sophisticated reasoning capability (Team et al., 2025; Hu et al., 2025a). more robust test-time self-evolution pipeline involves (1) verifying generated outputs, (2) producing targeted feedback upon detecting errors, and (3) iterating with this feedback. In this paper, we advance this pipeline in two key areas. For (1) verification, we exploit the asymmetry of verification to decompose complex problems into simpler sub-tasks, where checking correctness is often easier than generation (Wei, 2025). For (2) feedback generation, we incorporate rubrics-based rewards (Gunjal et al., 2025; Huang et al., 2025) to provide structured, discriminative signals, derived from an automatically constructed DRA failure taxonomy. We constructhe the taxonomy by analyzing the failure trajectories on the WebAggregator dataset (Wang et al., 2025), categorizing failures into five major classes and thirteen sub-classes. Based on (1) and (2), we present DeepVerifier, an agentic pipeline for automatically verifying the success of DRA output and provide feedbacks based on the rubrics. DeepVerifier decomposes intricate verification challenges into verifiable information-retrieval sub-tasks (Figure 2), overcoming limitations of prior holistic judging approaches. This decomposition principle extends naturally to report generation (Fan et al., 2025). We evaluate DeepVerifier on the GAIA benchmark (Mialon et al., 2023), which assesses core abilities including reasoning, multimodality, web browsing, and tool use. Results show DeepVerifier outperforming vanilla agent-as-judge and LLM judge baselines by 12 48% in meta-evaluation F1 score. When integrated for test-time scaling with capable closed-source LLMs (e.g., Claude-3.5-Sonnet), it yields 811% accuracy improvements across challenging GAIA subsets and 36% improvements on the XBench-DeepSearch dataset. Beyond test-time inference, we extend DeepVerifier to develop DeepVerifier-4K, high-quality supervised fine-tuning (SFT) dataset comprising 4,646 prompt-response pairs tailored for DRA verification. Curated by filtering and parsing 400 initial agent verification trajectories, DeepVerifier4K enables robust reflection and self-critique. Using this dataset, we fine-tune DeepVerifier-8B, model that surpasses other open-sourced models after reflection on key benchmarks. Our framework thus offers scalable solution for both DRA verification and high-quality dataset creation. In summary, our contributions are as follows: We formalize the agent reflection pipeline for Deep Research Agents (DRAs) and leverage the asymmetry of verification to achieve superior meta-evaluation performance. We introduce comprehensive DRA failure taxonomy, automatically constructed to categorize failures systematically, and derive structured rubrics for outcome-based rewards. 2 Technical Report Figure 2: Overview of DeepVerifier, which decomposes complex verification problems into smaller, simpler sub-questions leveraging the asymmetry of verification, and provides corrective feedback for the DRA to retry when the answer is considered incorrect. Through extensive experiments, we demonstrate the inference-time scaling of verification that holds for both capable closed-source LLM APIs and supervised fine-tuned models; integrating enhanced verification capabilities significantly boosts overall agent performance."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Deep Reserach Agents Research on DRA has rapidly advanced, aiming to build autonomous systems capable of multi-step tasks such as web navigation, data analysis, code generation, and report synthesis. Proprietary frameworks like OpenAIs Deep Research OpenAI (2025), Googles Gemini Deep Research Google DeepMind (2025), Perplexitys Deep Research Perplexity AI (2025), and Moonshot AIs KimiResearcher Moonshot AI (2025a;b) demonstrate strong performance on benchmarks such as GAIA and Humanitys Last Exam, setting high standards for autonomy and multimodal reasoning. Meanwhile, open-source frameworks democratize agent development. Notable systems include Hugging Faces SmolAgents Roucher et al. (2025), Alibabas WebAgent family Wu et al. (2025a); Li et al. (2025); Tao et al. (2025), and other agents framework such as WebWalker Wu et al. (2025b), OWL Hu et al. (2025b), TapeAgent Bahdanau et al. (2024), AutoAgent Tang et al. (2025),OAgents Zhu et al. (2025a;c), Cognitive Kernel Zhang et al. (2024), and Cog-Kernel-Pro Fang et al. (2025b), and WebEvolver Fang et al. (2025a). In all, DRA verification and its scaling effect remain underexplored. 2.2 Test-Time Scaling of Agents Many works apply Test-Time-Scaling (Choi et al., 2023; Snell et al., 2024) to enhance the quality of agent responses. Zhu et al. (2025b) proposes Best-of-N selection, majority vote, etc. However, such test-time-scaling methods remain prone to the same set of failures in different roll-outs, meaning that errors arising in one run also tend to recur in other runs, rendering the overall result unreliable. Other works explored using LLMs or agents as judges to evaluate agent responses He et al. (2024); Pan et al. (2024); `u et al. (2025); Zhuge et al. (2024); Yang et al. (2025). However, these works have focused on web navigation tasks, general reasoning tasks, or software development tasks, while none have studied the responses of DRAs. Recent research also investigates self-evolving LLMs Zhou et al. (2025a); Zhang et al. (2025b); Zuo et al. (2025); Zhang et al. (2025a); Feng et al. (2025). For example, the Self-Challenging Agent Zhou et al. (2025a) alternates between generating Code-as-Task problems and solving them via reinforcement learning. Zhang et al. introduce self-aware RL with task-difficulty prediction and limit-breaking Zhang et al. (2025b). Zuo et al.s Test-Time RL (TTRL) uses majority-vote rewards at inference time Zuo et al. (2025). However, none of these works address DRAs. Zhang et al. (2025a) Technical Report Table 1: Statistics of collected trajectories. Steps refers to the actions (planning, searching, clicking, etc.) performed by agents and sub-agents. Number of tokens is calculated by the GPT-4o tokenizer. Trajectory Stat Min Max Avg Total Steps Tokens Correct/Incorrect Unique Tasks 2.0 33.3 156.0 2,997 18.7K 60.0M 8.2M 738M 0.96 90 - - - - - - systematically analyze failure modes of DRAs, but do not provide an automated framework for detecting failures or improving agents based on these findings. In contrast, we (1) construct an agent failure taxonomy, (2) introduce verification-asymmetrybased framework to automatically detect failures, and (3) extend it to self-evolving verification, demonstrating clear verification scaling effect."
        },
        {
            "title": "3 DRA Failure Taxonomy",
            "content": "To exploit the asymmetry of verification and decompose complex problems into simpler sub-tasks, we first investigate the common failures of DRA and construct DRA Failure Taxonomy. To avoid data leakage or contamination and ensure generalization, we select the WebAggregatorQA dataset to construct the taxonomy, and evaluate the framework on three distinct dataset: GAIA, BrowseComp, and XBench-DeepSearch to demonstrate the effectiveness and generalization of the method. Trajectory Collection To construct the taxonomy, we first collect problem-solving trajectories from representative deep research agent. Table 1 summarizes the resulting corpus, which is substantial (2,997 agent actions), diverse (90 distinct tasks; trajectories range from 2 to 156 steps), and nearly balanced (correct/incorrect ratio of 0.96). We use Cognitive Kernel-Pro Fang et al. (2025b), high-performing fully open-source multi-module DRA framework, with Claude-3.7-Sonnet as the backbone model due to its strong performance in this setting. Trajectories are generated by running the agent on WebAggregatorQA Wang et al. (2025), benchmark that exercises core DRA capabilities including multi-step reasoning, multimodal inputs, web browsing, and general tool-use proficiency. Error Points Collection For each trajectory that produces an incorrect final answer, we annotate the underlying failure points. We use the human reference solution traces provided by WebAggregatorQA as grounding signal, and recruit two research staff annotators to independently inspect the agents execution and identify deviations from the reference reasoning and evidence-gathering process. Each annotator records set of error points, i.e., concrete, localized mistakes such as missing critical evidence, using an invalid source, or misinterpreting an instruction, along with the supporting trajectory step(s). We then reconcile the two annotation sets through merge procedure: duplicated items are consolidated, and distinct items are retained in the final list. We calculate that on average, 63.0% of the error points of one annotator overlapped with the others, indicating relatively high agreement rate between the annotators. This process yields 555 error points. Full annotation guidelines are provided in Appendix A. Taxonomy Construction To gain further insight into the failures, we construct taxonomy based on the error points. In particular, we conduct an iterative analysis and labeling process with two annotators with multiple years of AI research experience from our institute. The initial labels are determined by clustering subset of 50 error points. In each iteration, we construct new version of the taxonomy by comparing and merging similar labels, removing inadequate categories, refining unclear definitions based on the results of previous iterations, and discussing the results of the last iteration. As result, we obtain classification scheme illustrated in Figure 3. The more frequent the subclass, the wider the branch. 4 Technical Report Figure 3: DRA failure taxonomy that categorizes 555 agent failures into five major classes and thirteen subclasses. Analysis Figure 3 shows that DRA failures are dominated by Finding Sources, with the largest flows corresponding to errors such as consulting the wrong evidence and relying on generic searches, highlighting that upstream information acquisition is the most frequent point of collapse. Reasoning failures are the next most common, driven by premature conclusions, misinterpretation, and hallucinated or overconfident claims, indicating that even when information is present, agents often make incorrect inferential leaps. Problem Understanding and Decomposition contribute substantially as well, with errors like misunderstanding instructions and goal drift, reflecting weaknesses in task grounding. Action Errors, including UI failures, format mistakes, and wrong modality use, show that execution issues also meaningfully hinder agent progress. Finally, notable portion of trajectories end due to Max Step Reached, suggesting that early mistakes often cascade into long, unproductive trajectories."
        },
        {
            "title": "4 DeepVerifier",
            "content": "We present an overview of the DeepVerifier framework in Figure 2. We adopt three-stage multimodule framework in our agent implementation. This framework consists of decomposition agent, verification agent, and judge agent. The following sections describe each module in detail. 4.1 Decomposition Module The decomposition agent leverages previous trajectories and the DRA failure taxonomy to exploit the asymmetry of verification. Instead of asking the verification agent to re-solve the entire complex task (e.g., Given query, an unverified answer, and the agents trajectory, verify the correctness of the answer), which often results in high error rates similar to those of the original agent execution, the decomposition agent breaks the problem into smaller, more manageable sub-questions. These subquestions target specific vulnerabilities in the previous solution, such as Does source state claim Y? or What is the exact figure for in the latest report X? The workflow of the decomposition agent comprises three steps. Trajectory Summarization. Agent trajectories average 8.2M tokens, far exceeding any LLMs context window. Moreover, concise descriptions of rollout steps can improve test-time scaling Fang 5 Technical Report et al. (2025b); Gonzalez-Pumariega et al. (2025). We therefore instruct the decomposition agent to first produce compact, step-indexed synopsis of the trajectory. For each step, it records the source visited and the concrete information retrieved (facts, numbers, quotes). The summary is descriptive, not interpretive, enabling downstream checks without reloading the full trace. Potential Error Identification. Given the summary and our failure taxonomy in the system prompt, the decomposition agent scans for behaviors that align with known failure modes . It produces paired findings of the form behavior potential error + taxonomy label with brief justification. These structured pairs localize where and how failures likely arise. Follow-Up Question Formulation. Finally, the decomposition agent drafts high-leverage followup questions targeted at the flagged vulnerabilities. Each question is answerable via external evidence and designed to decisively confirm or refute risky claim. By focusing only on essential, potentially faulty claims, this process allows the verification agent to build on well-grounded conclusions, ignore trivial details, and check only for suspicious or unsupported assertions. Detailed prompts of each step are shown in Appendix B. 4.2 Verification Agent and Judge Module Verification The verification module retrieves answers to the follow-up questions sequentially. In our experiment, we use the CK-Pro agent Fang et al. (2025b) as the verification agent, which utilizes modular, multi-agent approach. The Main Agent orchestrates the problem-solving process by decomposing complex tasks into sub-tasks, which are assigned to specialized Sub-Agents. Upon receiving sub-agents responses, it aggregates the information to proceed with the overall goal. The sub-agents are designed to interact with specific resources, performing tasks such as searching or screenshot. Each sub-agent generates Python code to carry out actions, ensuring the system remains flexible and adaptable across diverse scenarios. Judge The judge agent evaluates the unverified answer based on the trajectory summary, potential error list, follow-up questions, and their answers. It begins by providing concise explanation, followed by score between 1 and 4, where: 1 = entirely incorrect, 2 = mostly incorrect, 3 = mostly correct, 4 = entirely correct."
        },
        {
            "title": "5 Enhancing Deep Research Agents with Scalable Verification",
            "content": "Test-Time Scaling with Reflection and Feedback. Beyond verification, our framework enhances the test-time scaling performance of DRAs through reflection. By integrating DeepVerifier into the DRA, the agent can review and evaluate its previous actions. Specifically, we modify the judge agents prompt to: 1) provide actionable instructions for the agent to retry tasks and avoid repeating mistakes, and 2) suggest correct answers if they are already available within the given information (e.g., previous trajectories or follow-up answers). After completing each task, the agent verifies its own outputs using DeepVerifier, collects feedback, and uses it to guide further retries. This process repeats until satisfactory answer is reached or predefined retry limit is exceeded. Training Reflection Ability in Agent Foundation Models. Many open-source models, lacking fine-tuning for reflection, show limited test-time scaling capabilities Fang et al. (2025b). To address this, we propose deep verification training dataset that leverages existing datasets and DeepVerifier to improve the reflection and test-time scaling abilities of open-source LLMs. Base Trajectory Collection. We first collect 400 answers and trajectories from agents solving tasks that require significant online exploration and information gathering. These tasks are sampled from the WebAggregatorQA dataset Wang et al. (2025), which tests agents on information aggregation across 10+ domains. Using the CK-Pro agent with Claude-3.7-Sonnet as the backbone model, we record the answers and corresponding trajectories. 6 Technical Report Table 2: Ablation study by removing different modules of DeepVerifier (values scaled by 100). Method Precision Recall Accuracy F1 DeepVerifier - Verification - Decomposition 75.00 100.00 86.96 71.43 14.29 47. 75.56 60.00 72.22 73.17 25.00 61.54 Verification Trajectory and SFT Data Collection. Next, we use DeepVerifier with Claude-3.7-Sonnet to verify the collected base trajectories and answers, saving the verification trajectories. We filter the true positive and true negative verificationsthose that correctly accept true answers and correctly reject false ones. After balancing these trajectories, we convert them into prompt-response pairs, resulting in DeepVerifier-4K, dataset of 4,646 high-quality pairs."
        },
        {
            "title": "6 Experiment Setup",
            "content": "Models and Benchmarks. We mainly use Claude-3.7-Sonnet as the backbone model of DeepVerifier and other methods. To evaluate the generalization ability of our method, we also compare the performance on GPT-4.1 and Qwen3-8B. We evaluate baselines and our methods primarily on the GAIA-web dataset, which is subset of the GAIA dataset filtered for tasks that require web browsing following He et al. (2024). To ensure generalization, we also extend evaluations on the full GAIA dataset Mialon et al. (2023), XBench-DeepSearch Chen et al. (2025), and BrowseComp Wei et al. (2025). XBench-DeepSearch is Chinese benchmark for search/tool-use, and BrowseComp measures agents ability to retrieve extremely hard-to-find and entangled information. Training Configurations. To demonstrate the effectiveness of our approach on open-sourced models, we SFT Qwen3-8B on mixture of DeepVerifier-4K and the CK-Pro-8B training set from Fang et al. (2025b) to train reflection abilities in open-source models while preserving their foundational capabilities. The training parameters are set as follows: Baselines and Metrics. We use the LLM judge proposed by `u et al. (2025) as the LLM verifier baseline, and the CK-Pro Agent Fang et al. (2025b) as the agent verifier baseline. Detailed prompts are shown in Appendix B. For verification tasks, we calculate the standard precision, recall, accuracy, and F1 score to measure the correctness of the evaluation, where true positive is defined as verifier assigning reject label to wrong answer, and true negative is defined as verifier assigning accept label to an correct answer. In the scaling experiment, we treat score of less than or equal 2 as incorrect, and greater or equal to 3 as correct. We stop the feedback loop as soon as the verifier judge the answer as correct. Research Questions We investigate the following research questions (RQs) to demonstrate the effectiveness of our method: 1. RQ1: Is DeepVerifier effective in verification? 2. RQ2: Can DeepVerifier help improve the performance of DRA via test-time scaling? 3. RQ3: Can DeepVerifier-4K help improve the reflection ability of open-sourced models?"
        },
        {
            "title": "7 Results & Analysis",
            "content": "7.1 RQ1: Effectiveness of DeepVerifier We conduct an ablation study using the trajectories of the CK-Pro agent with Claude-3.7-Sonnet backbone on the GAIA-Web dataset, as described in Table 1. Each method, using the same backbone model, is evaluated on its ability to verify the correctness of these cases. As shown in Table 2, 7 Technical Report Table 3: Accuracy(%) on different subsets of the GAIA dataset with different rounds of feedback using DeepVerifier (DV) across different backbone models. GAIA Split Model # Feedback Rounds 0 2 4 8 10 Final Gain Best Gain Web File/Reasoning/Others Full Claude-3.7 51.11 58.89 63.33 62.22 61.11 62.22 28.89 32.22 31.11 32.22 31.11 31.11 GPT-4.1 26.67 31.11 31.11 32.22 33.33 33.33 DV-8B Claude-3.7 53.57 53.57 56.21 54.92 54.92 54.92 30.67 33.33 33.33 33.33 33.33 33.33 GPT-4.1 26.81 30.85 30.85 30.85 30.85 30.85 DV-8B Claude-3.7 52.22 56.49 60.12 58.93 58.32 58.93 29.51 32.53 31.92 32.53 31.92 31.92 GPT-4.1 26.73 30.99 30.99 31.60 32.21 32.21 DV-8B 11.11 2.22 6.67 1.35 2.67 4.04 6.71 2.41 5. 12.22 3.33 6.67 2.64 2.67 4.04 7.90 3.01 5.48 Table 4: Accuracy(%) across different datasets versus feedback rounds using DeepVerifier with Claude-3.7-Sonnet backbone. Dataset 1 2 3 4 5 7 8 9 10 Final Gain Best Gain DeepSearch BrowseComp 5.0 41.0 42.0 47.0 41.0 45.0 44.0 43.0 44.0 42.0 44.0 44.0 9.0 9. 8.0 10.0 10.0 9.0 9.0 9.0 9.0 9. 3.0 4.0 6.0 5.0 DeepVerifier achieves superior performance across recall, accuracy, and F1 score. Removing the verification module or decomposition module exhibits high precision (100% and 86.96%, respectively) in detecting erroneous cases, but their recall and accuracy remain unsatisfactory. Closer analysis reveals that these judges are effective at catching obvious mistakes, such as execution failures, but often overlook subtler reasoning or factual errors, accepting many incorrect answers as correct. This limitation arises because removing the verification module renders the judge fail to identify secondary-source dependence, overconfident claims, or hallucinated facts supporting incorrect responses. Meanwhile, removing the decomposition does not affect the judges access to external sources, but we observe that without proper decomposition, the agent tends to check every step by re-solving the entire task, leaving them vulnerable to the same reasoning errors as the original agent. In contrast, DeepVerifier decomposes complex verification into smaller, targeted sub-questions that directly test specific vulnerabilities, making it more robust against faulty reasoning and unsupported claims. Answer to RQ1: DeepVerifier is effective in DRA verification, achieving balanced precisionrecall tradeoff and yields 12% - 48% improvement in F1 score and highest accuracy compared to ablated versions. 7.2 RQ2: Improving the Performance of DRA Via Reflective Test-Time Scaling We evaluate whether DeepVerifier can enhance the performance of Deep Research Agents through reflective test-time scaling by integrating it into the CK-Pro agent with Claude-3.7-Sonnet and measuring accuracy across feedback rounds on the GAIA dataset. As shown in Table 3, accuracy consistently improves with additional feedback iterations, reaching its peak at the fourth round. This demonstrates that iterative reflection and verification feedback effectively help the agent refine reasoning and correct previous errors. Performance on the GAIA dataset. The overall accuracy on GAIA-Full increases from approximately 52% to 59%, with peak value reacing 60.1%, marking the best performance gain of 8%. The 8 Technical Report GAIA-Web subset shows the greatest improvement, rising from 52% to above 62%, with peak value reaching 63.5%, indicating that web-based, retrieval-heavy tasks benefit most from DeepVerifier targeted verification and evidence-grounding process. Meanwhile, reasoning and file-operation subset also exhibits improvement across rounds, demonstrating that the reflective feedback mechanism generalizes beyond web-based scenarios. To ensure the generalization of DeepVerifier, we also evaluate its performance with GPT-4.1. As shown in Table 3, the accuracy for GPT-4.1 shows an initial peak at the third round. After 10 rounds, GPT-4.1s accuracy improves from approximately 29.5% to 31.9%, with peak value reaching 32.5%, confirming the effectiveness of DeepVerifier across different models. Figure 1 demonstrates the scaling trend of these models. Performance on other DRA datasets. Results in Table 4 show that the scaling effect remains consistent despite the multi-lingual nature of DeepSearch and the extreme difficulty of BrowseComp: XBench-DeepSearch improves from 41.0 (0 rounds) to 47.0 (best, +6.0), and ends at 44.0 (+3.0 at 10 rounds); BrowseComp improves from 5.0 to 10.0 (best, +5.0), and ends at 9.0 (+4.0). Analysis of the Scaling Trend Performance typically peaks in early feedback rounds due to our iterative setting and the verifiers imperfect precision and recall. In each round, the verifier enables many incorrect cases to be fixed (incorrectcorrect), but also occasionally rejects correct answers, causing regressions (correctincorrect). Table 5 shows that the incorrectcorrect transition is stronger but decays quickly, whereas the correctincorrect transition is weaker but persists across rounds; their interplay produces the observed peak around the fourth round. Table 5: Transition rates between consecutive feedback rounds. Feedback Round 1 Incorrect to Correct Ratio (%) Correct to Incorrect Ratio(%) 18.99 12. 2 9.33 4.44 3 6.94 4.30 4 8.45 1. 5 0.00 3.03 6 1.45 0.00 7 0.00 0. 8 0.00 1.03 9 1.45 0.00 10 0.00 0. Answer to RQ2: DeepVerifier effectively scales DRA performance through structured reflection: as feedback rounds increase, the agent progressively enhances its accuracy, achieving over 8% performance gains on Claude-3.7-Sonnet without additional training or external supervision. The scaling behavior also generalizes to other models and datasets. 7.3 RQ3: Enhancing Reflection Ability of Open-Sourced Models We further investigate whether incorporating reflection ability through SFT can improve the reasoning and verification performance of Deep Research Agents. We SFT Qwen3-8B model on DeepVerifier-4K dataset, which we named DeepVerifier-8B and use this model as the backbone for CK-Pro Agent with DeepVerifier as the reflection module, measuring accuracy after 10 feedback rounds on the GAIA dataset. As shown in Figure 1, models fine-tuned with the DeepVerifier-4K dataset exhibit notable performance gains when equipped with reflection. Specifically, DeepVerifier8B, which is trained with both the CK-Pro dataset and the DeepVerifier-4K reflective data, achieves the highest accuracy of 32.2% after reflection, representing 5.5% improvement over its nonreflective result. In contrast, CK-Pro-8B, trained only on the CK-Pro dataset, achieves smaller gain of 2.6 points, while Qwen3-8B, which lacks both CK-Pro and DeepVerifier training, shows minimal improvement. The scaling trend in Table 3 and Figure 1 further illustrates that DeepVerifier-8B (DV-8B) maintains steady accuracy gains across feedback rounds, with accuracy on GAIA-Full increasing from 26.7% to over 32%. Both GAIA subsets show similar upward trends: web-based tasks benefiting from better fact verification and file/reasoning tasks reflecting improved general reasoning control, which demonstrates the generalizability of reflection ability across task types. 9 Technical Report Answer to RQ3: Incorporating DeepVerifier reflection ability through fine-tuning significantly improves the reasoning and verification performance of Deep Research Agents. The fine-tuned DeepVerifier-8B model achieves 5.5% accuracy gain compared to its non-reflective version and the Qwen3-8B model."
        },
        {
            "title": "8 Conclusion",
            "content": "In this paper, we construct an agent failure taxonomy, introduce verification asymmetrybased framework to automatically detect failures, and extend it to self-evolving verification, demonstrating clear verification scaling effect. We also created DeepVerifier-4K, high-quality dataset for supervised fine-tuning. Our framework offers practical solution for scalable DRA verification and dataset creation."
        },
        {
            "title": "References",
            "content": "Dzmitry Bahdanau, Nicolas Gontier, Gabriel Huang, Ehsan Kamalloo, Rafael Pardinas, Alex Piche, Torsten Scholak, Oleh Shliazhko, Jordan Prince Tremblay, Karam Ghanem, Soham Parikh, Mitul Tiwari, and Quaizar Vohra. Tapeagents: holistic framework for agent development and optimization. arXiv preprint arXiv:2412.08445, 2024. Kaiyuan Chen, Yixin Ren, Yang Liu, Xiaobo Hu, Haotong Tian, Tianbao Xie, Fangfu Liu, Haoye Zhang, Hongzhang Liu, Yuan Gong, et al. xbench: Tracking agents productivity scaling with profession-aligned real-world evaluations. arXiv preprint arXiv:2506.13651, 2025. Sehyun Choi, Tianqing Fang, Zhaowei Wang, and Yangqiu Song. KCTS: knowledge-constrained In Houda Bouamor, Juan tree search decoding with token-level hallucination detection. Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pp. 1403514053. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.EMNLP-MAIN.867. URL https://doi.org/10.18653/v1/2023.emnlp-main.867. Tianyu Fan, Xinyao Niu, Yuxiang Zheng, Fengji Zhang, Chengen Huang, Bei Chen, Junyang Lin, and Chao Huang. Understanding deepresearch via reports. arXiv preprint arXiv:2510.07861, 2025. Tianqing Fang, Hongming Zhang, Zhisong Zhang, Kaixin Ma, Wenhao Yu, Haitao Mi, and Dong Yu. Webevolver: Enhancing web agent self-improvement with coevolving world model. arXiv preprint arXiv:2504.21024, 2025a. Tianqing Fang, Zhisong Zhang, Xiaoyang Wang, Rui Wang, Can Qin, Yuxuan Wan, Jun-Yu Ma, Ce Zhang, Jiaqi Chen, Xiyun Li, Hongming Zhang, Haitao Mi, and Dong Yu. Cognitive kernelpro: framework for deep research agents and agent foundation models training, 2025b. URL https://arxiv.org/abs/2508.00414. Kaituo Feng, Manyuan Zhang, Hongyu Li, Kaixuan Fan, Shuang Chen, Yilei Jiang, Dian Zheng, Peiwen Sun, Yiyuan Zhang, Haoze Sun, et al. Onethinker: All-in-one reasoning model for image and video. arXiv preprint arXiv:2512.03043, 2025. Gonzalo Gonzalez-Pumariega, Vincent Tu, Chih-Lun Lee, Jiachen Yang, Ang Li, and Xin Eric Wang. The unreasonable effectiveness of scaling agents for computer use. 2025. URL https: //api.semanticscholar.org/CorpusID:281724986. Google DeepMind. Gemini deep research your personal research assistant, 2025. URL https: //gemini.google.com. Anisha Gunjal, Anthony Wang, Elaine Lau, Vaskar Nath, Yunzhong He, Bing Liu, and Sean Hendryx. Rubrics as rewards: Reinforcement learning beyond verifiable domains. arXiv preprint arXiv:2507.17746, 2025. Technical Report Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. Webvoyager: Building an end-to-end web agent with large multimodal models. arXiv preprint arXiv:2401.13919, 2024. URL https://arxiv.org/abs/2401.13919. Chen Hu, Haikuo Du, Heng Wang, Lin Lin, Mingrui Chen, Peng Liu, Ruihang Miao, Tianchi Yue, Wang You, Wei Ji, Wei Yuan, Wenjin Deng, Xiaojian Yuan, Xiaoyun Zhang, Xiangyu Liu, Xikai Liu, Yanming Xu, Yicheng Cao, Yifei Zhang, Yongyao Wang, Yubo Shu, Yurong Zhang, Yuxiang Zhang, Zheng Gong, Zhichao Chang, Binyan Li, Dan Ma, Furong Jia, Hongyuan Wang, Jiayu Liu, Jing Bai, Junlan Liu, Manjiao Liu, Na Wang, Qiuping Wu, Qinxin Du, Shiwei Li, Wen Sun, Yifeng Gong, Yonglin Chen, Yuling Zhao, Yuxuan Lin, Ziqi Ren, Zixuan Wang, Aihu Zhang, Brian Li, Buyun Ma, Kang An, Li Xie, Mingliang Li, Pan Li, Shidong Yang, Xi Chen, Xiaojia Liu, Yuchu Luo, Yuan Song, YuanHao Ding, Yuanwei Liang, Zexi Li, Zhaoning Zhang, Zixin Zhang, Binxing Jiao, Daxin Jiang, Jiansheng Chen, Jing Li, Xiangyu Zhang, and Yibo Zhu. Step-deepresearch technical report, 2025a. URL https://arxiv.org/abs/2512.20491. Mengkang Hu, Yuhang Zhou, Wendong Fan, Yuzhou Nie, Bowei Xia, Tao Sun, Ziyu Ye, Zhaoxuan Jin, Yingru Li, Qiguang Chen, Zeyu Zhang, Yifeng Wang, Qianshuo Ye, Bernard Ghanem, Ping Luo, and Guohao Li. Owl: Optimized workforce learning for general multi-agent assistance in real-world task automation, 2025b. URL https://arxiv.org/abs/2505.23885. Zenan Huang, Yihong Zhuang, Guoshan Lu, Zeyu Qin, Haokai Xu, Tianyu Zhao, Ru Peng, Jiaqi Hu, Zhanming Shen, Xiaomeng Hu, et al. Reinforcement learning with rubric anchors. arXiv preprint arXiv:2508.12790, 2025. Eric Li and Jim Waldo. Websuite: Systematically evaluating why web agents fail. arXiv preprint arXiv:2406.01623, 2024. doi: 10.48550/arXiv.2406.01623. URL https://arxiv.org/abs/2406. 01623. Kuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Zhengwei Tao, Xinyu Wang, Weizhou Shen, Junkai Zhang, Dingchu Zhang, Xixi Wu, Yong Jiang, Ming Yan, Pengjun Xie, Fei Huang, and Jingren Zhou. Websailor: Navigating super-human reasoning for web agent, 2025. URL https://arxiv.org/abs/2507.02592. Xing Han `u, Amirhossein Kazemnejad, Nicholas Meade, Arkil Patel, Dongchan Shin, Alejandra Zambrano, Karolina Stanczak, Peter Shaw, Christopher J. Pal, and Siva Reddy. Agentrewardbench: Evaluating automatic evaluations of web agent trajectories. arXiv preprint arXiv:2504.08942, 2025. URL https://arxiv.org/abs/2504.08942. Gregoire Mialon, Clementine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: benchmark for general ai assistants. ArXiv, abs/2311.12983, 2023. URL https://api. semanticscholar.org/CorpusID:265351664. Moonshot AI. Kimi-k2, 2025a. URL https://github.com/MoonshotAI/Kimi-K2. Moonshot AI. Kimi-researcher: End-to-end rl training for emerging agentic capabilities, 2025b. URL https://moonshotai.github.io. OpenAI. Introducing deep research. Technical report, OpenAI, 2025. URL https://openai.com/ index/introducing-deep-research. Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine, and Alane Suhr. Autonomous evaluation and refinement of digital agents. arXiv preprint arXiv:2404.06474, 2024. URL https: //arxiv.org/abs/2404.06474. Perplexity AI. Introducing perplexity deep research, 2025. URL https://www.perplexity.ai/hub/ blog/introducing-perplexity-deep-research. Aymeric Roucher, Albert Villanova del Moral, Thomas Wolf, Leandro von Werra, and Erik Kaunismaki. Smolagents: smol library to build great agentic systems, 2025. URL https: //github.com/huggingface/smolagents. 11 Technical Report Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652, 2023. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Kevin Song, Anand Jayarajan, Yaoyao Ding, Qidong Su, Zhanda Zhu, Sihang Liu, and Gennady Pekhimenko. Aegis: Taxonomy and optimizations for overcoming agent-environment failures in llm agents. arXiv preprint arXiv:2508.19504, 2025. doi: 10.48550/arXiv.2508.19504. URL https://arxiv.org/abs/2508.19504. Jiabin Tang, Tianyu Fan, and Chao Huang. Autoagent: fully-automated and zero-code framework for llm agents. arXiv preprint arXiv:2502.05957, 2025. Zhengwei Tao, Jialong Wu, Wenbiao Yin, Junkai Zhang, Baixuan Li, Haiyang Shen, Kuan Li, Liwen Zhang, Xinyu Wang, Yong Jiang, Pengjun Xie, Fei Huang, and Jingren Zhou. Webshaper: Agentically data synthesizing via information-seeking formalization, 2025. URL https://arxiv. org/abs/2507.15061. Tongyi DeepResearch Team, Baixuan Li, Bo Zhang, Dingchu Zhang, Fei Huang, Guangyu Li, Guoxin Chen, Huifeng Yin, Jialong Wu, Jingren Zhou, et al. Tongyi deepresearch technical report. arXiv preprint arXiv:2510.24701, 2025. Rui Wang, Ce Zhang, Junyu Ma, Jianshu Zhang, Hongru Wang, Yi Chen, Boyang Xue, Tianqing Fang, Zhisong Zhang, Hongming Zhang, Haitao Mi, Dong Yu, and Kam-Fai Wong. Explore to evolve: Scaling evolved aggregation logic via proactive online exploration for deep research agents. 2025. URL https://api.semanticscholar.org/CorpusID:282139163. Jason Wei. Asymmetry of verification and verifiers law, 2025. URL https://www.jasonwei.net/ blog/asymmetry-of-verification-and-verifiers-law. Accessed: 2025-10-30. Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: simple yet challenging benchmark for browsing agents. arXiv preprint arXiv:2504.12516, 2025. Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang, Zekun Xi, Gang Fu, Yong Jiang, Pengjun Xie, Fei Huang, and Jingren Zhou. Webdancer: Towards autonomous information seeking agency. arXiv preprint arXiv:2505.22648, 2025a. Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Linhai Zhang, Yulan He, Deyu Zhou, Pengjun Xie, and Fei Huang. Webwalker: Benchmarking llms in web traversal. CoRR, abs/2501.07572, 2025b. doi: 10.48550/ARXIV.2501.07572. URL https://doi. org/10.48550/arXiv.2501.07572. Yiliu Yang, Yilei Jiang, Qunzhong Wang, Yingshui Tan, Xiaoyong Zhu, Sherman SM Chow, Bo Zheng, and Xiangyu Yue. Quadsentinel: Sequent safety for machine-checkable control in multi-agent systems. arXiv preprint arXiv:2512.16279, 2025. Mert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Zhi Huang, Carlos Guestrin, and James Zou. Textgrad: Automatic differentiation via text. arXiv preprint arXiv:2406.07496, 2024. Dingling Zhang, He Zhu, Jincheng Ren, Kangqi Song, Xinran Zhou, Boyu Feng, Shudong Liu, Jiabin Luo, Weihao Xie, Zhaohui Wang, et al. How far are we from genuinely useful deep research agents? arXiv preprint arXiv:2512.01948, 2025a. Hangfan Zhang, Siyuan Xu, Zhimeng Guo, Huaisheng Zhu, Shicheng Liu, Xinrun Wang, Qiaosheng Zhang, Yang Chen, Peng Ye, Lei Bai, and Shuyue Hu. The path of self-evolving large language models: Achieving data-efficient learning via intrinsic feedback. arXiv preprint arXiv:2510.02752, 2025b. URL https://arxiv.org/abs/2510.02752. 12 Technical Report Hongming Zhang, Xiaoman Pan, Hongwei Wang, Kaixin Ma, Wenhao Yu, and Yu Dong. Cognitive kernel: An open-source agent system towards generalist autopilots. CoRR, abs/2409.10277, 2024. doi: 10.48550/ARXIV.2409.10277. URL https://doi.org/10.48550/arXiv.2409.10277. Yifei Zhou, Sergey Levine, Jason Weston, Xian Li, and Sainbayar Sukhbaatar. Self-challenging In Advances in Neural Information Processing Systems (NeurIPS 2025), language model agents. 2025a. doi: 10.48550/arXiv.2506.01716. URL https://arxiv.org/abs/2506.01716. NeurIPS 2025 poster. Yifei Zhou, Sergey Levine, Jason Weston, Xian Li, and Sainbayar Sukhbaatar. Self-challenging language model agents. arXiv preprint arXiv:2506.01716, 2025b. He Zhu, Tianrui Qin, King Zhu, Heyuan Huang, Yeyi Guan, Jinxiang Xia, Yi Yao, Hanhao Li, Ningning Wang, Pai Liu, Tianhao Peng, Xin Gui, Xiaowan Li, Yuhui Liu, Yuchen Eleanor Jiang, Jun Wang, Changwang Zhang, Xiangru Tang, Ge Zhang, Jian Yang, Minghao Liu, Xitong Gao, Jiaheng Liu, and Wangchunshu Zhou. Oagents: An empirical study of building effective agents, 2025a. URL https://arxiv.org/abs/2506.15741. King Zhu, Hanhao Li, Siwei Wu, Tianshun Xing, Dehua Ma, Xiangru Tang, Minghao Liu, Jian Yang, Jiaheng Liu, Yuchen Eleanor Jiang, Changwang Zhang, Chenghua Lin, Jun Wang, Ge Zhang, and Wangchunshu Zhou. Scaling test-time compute for llm agents, 2025b. URL https://arxiv.org/ abs/2506.12928. King Zhu, Hanhao Li, Siwei Wu, Tianshun Xing, Dehua Ma, Xiangru Tang, Minghao Liu, Jian Yang, Jiaheng Liu, Yuchen Eleanor Jiang, et al. Scaling test-time compute for llm agents. arXiv preprint arXiv:2506.12928, 2025c. Mingchen Zhuge, Changsheng Zhao, Dylan Ashley, Wenyi Wang, Dmitrii Khizbullin, Yunyang Xiong, Zechun Liu, Ernie Chang, Raghuraman Krishnamoorthi, Yuandong Tian, Yangyang Shi, Vikas Chandra, and urgen Schmidhuber. Agent-as-a-judge: Evaluate agents with agents. arXiv preprint arXiv:2410.10934, 2024. URL https://arxiv.org/abs/2410.10934. Yuxin Zuo, Kaiyan Zhang, Li Sheng, Shang Qu, Ganqu Cui, Xuekai Zhu, Haozhan Li, Yuchen Zhang, Xinwei Long, Ermo Hua, Biqing Qi, Youbang Sun, Zhiyuan Ma, Lifan Yuan, Ning Ding, and Bowen Zhou. Ttrl: Test-time reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS 2025), 2025. doi: 10.48550/arXiv.2504.16084. URL https://arxiv.org/abs/2504.16084. NeurIPS 2025 poster."
        },
        {
            "title": "A Annotation Instructions",
            "content": "This instruction is used for the human annotator for summarizing the error points in each erroneous trajectory. Instruction for Error Points Annotation You are given human execution of task (which is the ground truth) and an LLM agent execution of the same task (which is different from the ground truth). Please compare and explain how LLMs executions are different from human executions, focusing on finding sources, locating information in the source, drawing observations from sources, problem understanding, etc. Then summarize the reasons why the LLM made the errors in bullet points with short sentences based on the comparison. 13 Technical Report"
        },
        {
            "title": "B Agent Prompts",
            "content": "B.1 Decomposition Module Trajectory Summary Prompt Summarize each step in the trajectory. For every step, list the online sources visited by the agent and the key info obtained from each source. **Required format (repeat Step blocks as needed):** Step 1: Source 1: source visited by the agent Info 1: information obtained from the source Source 2: source visited by the agent Info 2: information obtained from the source Step 2: ... Here is the trajectory: [Trajectory] Error Identification Identify suspicious behaviors and map each to **one** potential error from the list below. If none, return exactly: No potential errors found. **Potential error list:** [Failure Taxonomy] **Required format (or the single-line No potential errors found):** Suspicious Behavior 1: short description Potential Error 1: one item from the list Suspicious Behavior 2: short description Potential Error 2: one item from the list ... Here is the trajectory summary: [Trajectory Summary] Follow-Up Questions Assume web-capable research agent exists. Propose the **fewest** source-question pairs needed to verify answer, using task, the [Trajectory Summary], and [Potential Errors]. **Required format (up to 3 pairs):** Additional Source 1: source Additional Question 1: yes-no question based on the source Additional Source 2: source Additional Question 2: yes-no question based on the source ... Here are the inputs: [Answer] [Trajectory Summary] [Potential Errors] 14 Technical Report B.2 Verification & Judge Module Verification Agent Prompt Here is source and question pair. Answer the question based on the source. Source: source Question: question Return brief explanation and concise answer to the question based on the source without any additional text. Judge Agent Prompt You are given task description, an unverified answer, summary of how the agent obtained the unverified answer, and additional answers provided by another research agent regarding the additional questions. Decide if the unverified answer is correct by first providing concise explanation, then returning score between 1 and 4, where: 1 = completely incorrect 2 = mostly incorrect 3 = mostly correct 4 = completely correct Your response should **exactly follow** this format, with no additional content: Explanation: explanation Score: score Corrective Feedback Prompt You are given task description, wrong answer given by an agent, summary of how the agent obtained the wrong answer, and additional answers provided by another research agent regarding the additional questions. Now, the agent will try to solve the task again. Based on these inputs, you need to help the agent retrieve the correct answer by first providing brief reflection and then providing **no more than three instructions**. Note that 1) the agent will strictly follow your instruction; if it cannot get the correct answer again, which means your instruction is not useful, then you will be punished. 2) point out necessary sources and actions to avoid the agent making the same mistakes again. 3) The agent is good at understanding clear, concise, and accurate instructions rather than long or complex instructions; the latter will confuse it. 4) You can also suggest the answer to the question in the instructions if you can determine the answer from available information. Your response should strictly follow this format without any other content: Reflection: brief reflection Instruction 1: instruction Instruction 2: instruction ..."
        }
    ],
    "affiliations": [
        "Renmin University of China",
        "Singapore Management University",
        "Tencent AI Lab",
        "The Chinese University of Hong Kong"
    ]
}