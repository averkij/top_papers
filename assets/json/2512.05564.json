{
    "paper_title": "ProPhy: Progressive Physical Alignment for Dynamic World Simulation",
    "authors": [
        "Zijun Wang",
        "Panwen Hu",
        "Jing Wang",
        "Terry Jingchen Zhang",
        "Yuhao Cheng",
        "Long Chen",
        "Yiqiang Yan",
        "Zutao Jiang",
        "Hanhui Li",
        "Xiaodan Liang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in video generation have shown remarkable potential for constructing world simulators. However, current models still struggle to produce physically consistent results, particularly when handling large-scale or complex dynamics. This limitation arises primarily because existing approaches respond isotropically to physical prompts and neglect the fine-grained alignment between generated content and localized physical cues. To address these challenges, we propose ProPhy, a Progressive Physical Alignment Framework that enables explicit physics-aware conditioning and anisotropic generation. ProPhy employs a two-stage Mixture-of-Physics-Experts (MoPE) mechanism for discriminative physical prior extraction, where Semantic Experts infer semantic-level physical principles from textual descriptions, and Refinement Experts capture token-level physical dynamics. This mechanism allows the model to learn fine-grained, physics-aware video representations that better reflect underlying physical laws. Furthermore, we introduce a physical alignment strategy that transfers the physical reasoning capabilities of vision-language models (VLMs) into the Refinement Experts, facilitating a more accurate representation of dynamic physical phenomena. Extensive experiments on physics-aware video generation benchmarks demonstrate that ProPhy produces more realistic, dynamic, and physically coherent results than existing state-of-the-art methods."
        },
        {
            "title": "Start",
            "content": "ProPhy: Progressive Physical Alignment for Dynamic World Simulation Zijun Wang1,2, Panwen Hu3, Jing Wang1, Terry Jingchen Zhang4, Yuhao Cheng5, Long Chen5, Yiqiang Yan5, Zutao Jiang2, Hanhui Li1*, Xiaodan Liang1,2,3* 1Shenzhen Campus of Sun Yat-sen University 2Peng Cheng Laboratory 3Mohamed bin Zayed University of Artificial Intelligence 4ETH Zurich 5Lenovo Research wangzj58@mail2.sysu.edu.cn, {lihh77, liangxd9}@mail.sysu.edu.cn Project page: https://zijunwa.github.io/prophy/ 5 2 0 2 5 ] . [ 1 4 6 5 5 0 . 2 1 5 2 : r Figure 1. Top-left: Prior work typically relies on implicit alignment without explicit physical priors or uses video-level module routing as the source of physical awareness in video generation models. Top-right: Overview of our proposed ProPhy, progressive alignment framework, which injects and aligns learnable physical priors and performs fine-grained token-level routing, enabling different experts to internalize different domains of physical knowledge. Bottom: Qualitative comparison between our method and prior work in complex scenarios. Red boxes and arrows indicate violations of physical laws."
        },
        {
            "title": "Abstract",
            "content": "Recent advances in video generation have shown remarkable potential for constructing world simulators. However, *Corresponding authors current models still struggle to produce physically consistent results, particularly when handling large-scale or complex dynamics. This limitation arises primarily because existing approaches respond isotropically to physical prompts and neglect the fine-grained alignment between generated content and localized physical cues. To address these chal1 lenges, we propose ProPhy, Progressive Physical Alignment Framework that enables explicit physics-aware conditioning and anisotropic generation. ProPhy employs two-stage Mixture-of-Physics-Experts (MoPE) mechanism for discriminative physical prior extraction, where Semantic Experts infer semantic-level physical principles from textual descriptions, and Refinement Experts capture tokenlevel physical dynamics. This mechanism allows the model to learn fine-grained, physics-aware video representations that better reflect underlying physical laws. Furthermore, we introduce physical alignment strategy that transfers the physical reasoning capabilities of vision-language models (VLMs) into the Refinement Experts, facilitating more accurate representation of dynamic physical phenomena. Extensive experiments on physics-aware video generation benchmarks demonstrate that ProPhy produces more realistic, dynamic, and physically coherent results than existing state-of-the-art methods. 1. Introduction Video generation [12, 30, 35] has recently emerged as powerful paradigm for visual content synthesis, driving wide range of applications such as creative media production [7, 10, 31] and robotic simulation [4, 21, 39]. Despite the rapid progress in visual realism, current video generation models still struggle to produce physically plausible results, which limits their ability to function as world simulators [3, 40] capable of reproducing realistic physical dynamics. Recently, several studies [9, 32, 38] have begun exploring ways to enhance the physical plausibility of generated videos. For instance, VideoREPA [38] proposes tokenrelation distillation strategy that transfers physical knowledge from Video Foundation Model (VFM) into video representations. PhysMaster [9], on the other hand, introduces reinforcement learning with human feedback (RLHF) framework to train an image encoder capable of capturing physical cues. However, as shown in the first row at the bottom of Figure 1, these approaches lack explicit physical guidance, often resulting in generations that In contrast, fail to adhere to fundamental physical laws. WISA [32] provides explicit physical guidance by analyzing the physical category implied by the text prompt and employing Mixture-of-Physics-Experts (MoPE) structure to assist video generation. Nevertheless, as illustrated in the second row at the bottom of Figure 1, WISA primarily focuses on global physical information; when physical phenomena occur locally or when multiple phenomena coexist within single video, it struggles to accurately capture fine-grained physical processes. To address the aforementioned limitations, we identify two key challenges that must be overcome: (a) Explicit physical guidance, which enables the representations of different physical laws to become more discriminative, thereby capturing distinct physical characteristics; and (b) Fine-grained physical alignment, which allows different spatial regions within video to respond accurately to localized physical cues. To this end, we propose ProPhy, Progressive Physical Alignment Framework that not only adaptively extracts physics-specific priors from textual descriptions but also refines and injects these priors into the spatial regions corresponding to particular physical phenomena, achieving physically consistent and spatially anisotropic video generation. Specifically, unlike previous methods that employ uniform architecture or singlelevel physical prior to model all physical laws (top-left of Figure 1), we introduce two-stage MoPE mechanism in our framework, which mainly consists of Semantic Expert Block and Refinement Expert Block. These two modules progressively extract and refine hierarchical physical priors (top-right of Figure 1). The Semantic Expert Block first infers physics-specific priors directly from textual prompts, which are subsequently fused with visual features and further processed by the Refinement Expert Block to learn token-wise physical priors. This design facilitates the formation of more discriminative and physically expressive representations. Furthermore, to enhance the regional response to underlying physical laws, we propose an innovative physical alignment strategy that guides the refinement experts during training to generate spatially anisotropic representations. At the current stage, vision-language models (VLMs) [1] exhibit more detailed and reliable localization of physical phenomena compared to generative models. Building on this observation, our strategy aligns the spatial distributions predicted by the physics experts with those inferred from the VLMs, thereby transferring its fine-grained physical localization into the generative process. By reinforcing the models sensitivity to localized physical priors, our method achieves more accurate and stable generation of dynamic physical processes, particularly in high-motion scenarios. We evaluate our approach on the physics-related video generation benchmark [2], and the experimental results demonstrate that ProPhy consistently outperforms state-ofthe-art methods across both standard and challenging scenarios, particularly in maintaining dynamic physical consistency. Our main contributions can be summarized as follows: 1) we propose two-stage MoPE design for physical prior extraction, enabling the model to capture physicsspecific priors and learn discriminative physical representations more precisely; 2) Moreover, an innovative finegrained physical alignment strategy is introduced to allow the Refinement Expert to learn fine-grained physical priors from VLMs, thereby improving the prediction of spatially physical distributions; 3) Finally, we conduct extensive ex2 periments across multiple backbone architectures to validate the effectiveness and generality of our framework, as well as the importance of fine-grained physical modeling. 2. Related Work 2.1. Video Generation as World Simulator The advent of diffusion models [6, 15, 27] has significantly advanced the field of visual generation, enabling generators to produce highly realistic images [5, 14, 24]. In particular, Diffusion Transformers [18, 22] (DiT) leverage the scaling capabilities of transformers within diffusion models, and when combined with large model and dataset scales, have led to series of impressive video generation models [3, 12, 13, 21, 30, 35] capable of producing realistic videos. Among these, Sora [3] explores the use of extensive video data to develop general-purpose world simulator. Despite these advances, current video generation models still struggle to fully capture the underlying physical principles of the real world, focusing primarily on the appearance of scenes and objects [11, 16, 20]. Increasing model size or dataset scale alone does not enable generators to learn the physical laws embedded in scenes and textual descriptions. This gap between video generators and true world simulators results in videos that are visually convincing but often logically inconsistent, limiting their ability to accurately simulate real-world dynamics. 2.2. Physics-Aware Video Generation Recent research has explored enhancing the physical awareness of video generation models through various approaches. 1) Physics simulation methods [19, 33, 37] first predict object dynamics from images and then render them into videos. PhysGen [17] simulates rigid-body dynamics to estimate how objects respond to external forces, generating keyframes that are subsequently rendered into videos. Similarly, PhysMotion [29] animates static scenes into dynamic videos using the Material Point Method [28] (MPM) and refines details with diffusion model. However, these methods require predefined dynamics parameters and explicit physical rules, limiting their generalization to diverse, unconstrained scenarios for true world simulation. 2) Learning-based approaches extract physical laws directly from video by leveraging temporal relationships. VideoREPA [38], for example, utilizes interframe visual feature relations extracted by visual encoder to guide the diffusion model, producing videos that more closely reflect real-world dynamics. 3) Methods incorporating external physics priors extract structured physical information using large vision-language or language models to guide generation. PhysT2V [34] iteratively adjusts textual instructions with LLMs to correct physical violations in generated videos, while WISA [32] constructs structured physical representationsincluding descriptions, categories, and attributesand integrates them into the generation process. NewtonGen [36] predicts physical states using Neural Newtonian Dynamics (NND), enabling physicscontrollable video synthesis. Despite these advances, existing methods for extracting and leveraging physical information in general scenarios largely operate at the sample level. However, different physical phenomena within scene often appear at distinct spatial locations. Sample-level guidance therefore lacks finegrained alignment with localized physical cues, dispersing physical awareness and failing to focus on critical areas. In this work, we propose framework that integrates spatiallyspecific physics priors, enabling enhanced physical perception while maintaining realistic motion dynamics. 3. Method 3.1. Overall Framework Our framework follows the paradigm of mainstream video diffusion models [30, 35] and aims to generate physically plausible videos from text. Given text prompt I, our endto-end inference pipeline first analyzes the input and extracts the underlying physical priors. It then denoises an initial noise sample into clean video 0. Unlike prior methods [9, 32, 38] that rely solely on coarse, video-level physical constraints, our framework adopts progressive physical alignment strategy. As shown in Figure 2, ProPhy is built upon latent video diffusion backbones such as WAN [30] and CogVideoX [35]. We further introduce dedicated Physical Branch, which consists of Semantic Expert Block (SEB), multiple Physical Blocks (PB), and Refinement Expert Block (REB) attached to the final PB. The SEB activates the relevant semantic physics experts based on the implicit physical cues in the text prompt, producing video-level physical priors. These priors are progressively refined by the PBs and further processed by the REB to obtain fine-grained, token-level physical priors. The refined priors are then injected into the backbone video representations, enabling spatially anisotropic responses to physical phenomena. To preserve the pretrained backbones semantic understanding and rendering ability, each PB adopts the same architecture as its corresponding transformer block and is initialized with its weights. The PB outputs are sequentially injected into the video latent, allowing the model to accumulate physical information in progressive manner. The number of PBs is adapted to the depth of each backbone model and the available GPU memory during training. Detailed configurations are provided in the Supplementary Material. 3 Figure 2. Overview of our proposed ProPhy framework. ProPhy uses progressive physical alignment design, consisting of the Semantic Expert Block and the Refinement Expert Block. During inference, the model runs end-to-end and aligns physics categories through our proposed blocks. 3.2. Progressive Physical Routing Semantic Expert Block The SEB operates at the video level. It contains set of learnable physical basis maps, each representing distinct aspect of physical knowledge, which together serve as physical experts to provide physics-aware priors for video representation. The contribution of each expert is determined by semantic router, which identifies the underlying physical semantics from the input text prompt and dynamically selects the most relevant experts for activation. Formally, we define Es learnable physical basis maps Be RN for = 1, . . . , Es. Each Be shares the same shape as the backbones visual latent RN C. Here = (F/rf ) (H/rs) (W/rs) is the number of latent tokens. (F, H, ) are the video length, height, and width. (rf , rs) are temporal and spatial downsampling ratios. is the latent dimension. For samples text embedding y, the semantic router outputs normalized weights ρp REp to control the contribution of each physical basis map. Thus, the physics-enhanced latent is represented as = + Es(cid:88) e=1 ρe pBe. (1) During training, small batch sizes will make the standard top-k MoE [25] prone to mode collapse [23], where only few experts are repeatedly activated. To address this, we adopt the continuous weighted formulation described above. The resulting feature X, serves as the global physical prior for subsequent refinement. each implemented by linear layer, together with refinement router that predicts the underlying physical law associated with each token and selects the most relevant top-k experts to provide physics-aware priors for that token. Specifically, for each token RC in the physicsenhanced latent, the refinement router outputs weight vector ρr REr representing the probability distribution over the physical laws associated with this token, where Er is the number of the refinement experts. Because the number of tokens is large and the proposed fine-grained alignment strategy (discussed in the next section) is applied, the risk of mode collapse is considerably reduced in this stage. We therefore adopt standard MoE strategy: = (cid:88) ei ρi θ(x), iargtopkρr (2) where ei θ denotes the forward function of the i-th expert. 3.3. Physical Alignment Objectives During training, each expert learns its physical knowledge under the guidance of its router. Our framework introduces two separate alignment objectives for the two routers. Experts at different levels therefore learn complementary aspects of physical reasoning. For example, in the SEB, some experts focus on combustion phenomena, and others capture reflection behaviors. We also apply load-balancing loss to encourage fair expert activation. These components together yield diverse pool of physics-aware experts and support the refinement process that follows. Refinement Expert Block Unlike the SEB, the REB operates at the token level. It also contains set of experts, Semantic Alignment At this stage, we align the semantic experts routing weights ρp with global physical semantics. 4 Figure 4. Study of the attention localization capabilities of VDM and VLM. The VDM cross-attention maps are obtained by adding 10% noise and then denoising. As shown, despite minor imperfections, the VLM-based approach more accurately identifies the locations of the corresponding physical phenomena. with the video, as illustrated in Figure 3. The prompt asks the VLM to describe the phenomenon in the video. From the generated text, we extract the corresponding video tokens as key tokens and the relevant text tokens as query tokens. We then retrieve the attention scores between these query and key tokens, giving preliminary localization of where the phenomenon occurs. To reduce labeling noise, we query the VLM again using short, generic prompt that avoids specific objects or events. Using the same querykey selection process, we obtain background attention map. Subtracting this background map from the phenomenon map yields the final token-level alignment targets Qr RN Eattn, where Eattn is the number of physical laws used for refinement. This procedure improves both the accuracy and the sharpness of the high-attention regions. We define mask RN Eattn. The mask is constructed as follows: (1) For efficiency, we annotate only the physical phenomena likely present in the video and set entries for annotated categories emarked to 1 and others to 0. (2) Some values in Qr are negative, indicating that the phenomenon is not prominent in those regions. We update the mask element-wise as = sign(Qr) to drop those regions. The resulting mask highlights high-salience regions for the annotated phenomena. The fine-grained alignment loss is Lfine-align = (cid:88) i,e=1 i,e Qi,e 2, (5) , ρ(2) ]. Here, ρ = [ρ(1) , . . . , ρ(N ) where is obtained by passing the refinement router output ρr through an MLP that expands the dimension from Er to Eattn. Beyond dimension matching, this MLP also reduces the training conflict [38] that arises when alignment signals are applied directly. To further stabilize training and encourage specialization in the refinement experts, we introduce standard load-balancing auxiliary loss [26] Lfine-balance on the refinement routers outputs. As result, the final training objective combines the three proposed losses with the Figure 3. Pipeline for annotating token-level physical attributes using VLM. We use WISA-80K [32] and its per-video physical category vector qs REwisa, where Ewisa is the number of annotated categories. linear layer maps ρs to the same dimension as qs. For each batch of size B, we compute cosinesimilarity-based pairwise matrix: i,j = ρ(j) ρ(i) ρ(j) ρ(i) , (3) where ρ(i) is the vector for the i-th sample in the batch. Uss ing the same procedure as Equation 3, we compute the label matrix Qs RBB. The semantic alignment objective is formulated as Lcoarse = (cid:88) 1i<jB i,j Qi,j 2. (4) With this objective, samples within batch that belong to the same physical category tend to share similar routing weights, whereas samples from different categories exhibit more divergent routing distributions. Fine-grained Alignment To enable token-level physical priors to respond anisotropically to physical laws, the Refinement Router must infer the physical attributes of each token and activate the appropriate refinement experts. We find that VLMs exhibit stronger spatial understanding of physical dynamics than generative models, as shown in Figure 4. They can also perceive the physical properties of individual video tokens with higher accuracy. Based on this observation, we align the perception capability of the refinement router with the understanding provided by the VLM. To obtain the supervision signal, we feed question about target physical phenomenon into the VLM along Table 1. Results on VideoPhy2 benchmark. The best results are highlighted in bold, and the second-best results are underlined. Method ALL HARD PC SA Joint PC SA Joint HunyuanVideo [12] 64.2 19.2 24.7 52.2 7.2 54.6 26.2 22.6 40.0 5.1 61.3 27.7 23.1 50.6 8.3 60.5 29.0 24.7 45.6 8. Cosmos [21] Cosmos-Predict2.5 Wan2.1-14B [30] Wan2.1-1.3B + ProPhy 57.8 30.0 24.8 36.7 11.7 65.0 32.0 26.5 48.9 12.2 CogVideoX-5B [35] 67.2 29.0 22.3 51.1 9.6 69.1 31.5 25.8 51.7 11.1 72.5 24.2 22.0 52.2 7.8 72.5 32.2 26.7 52.8 11.7 + WISA [32] + VideoREPA [38] + ProPhy 5.0 4.0 5.6 3. 5.6 7.2 5.0 5.0 5.6 6.1 standard diffusion loss, formulated as follows: = Ldiffusion +λ1Lcoarse +λ2Lfine-align +λ3Lfine-balance, (6) where λ1, λ2 and λ3 represent the weights, respectively. 4. Experiment 4.1. Implementation Details Training We build ProPhy upon two open-source video diffusion models, Wan2.1-1.3B [30] and CogVideoX5B [35]. These models are fine-tuned with our ProPhy framework to validate its effectiveness. We randomly sample 20K videos from the WISA-80K dataset [32] as the training data, and employ Qwen2.5-VL-32B [1] to obtain token-level physical annotations. For the hybrid loss in Equation 6, we use fixed coefficients λ1 = 0.1, λ2 = 0.02, and λ3 = 0.01. This configuration works reliably across both backbones without additional tuning. Further implementation details are provided in the Supplementary Material. Evaluation We evaluate our generated videos using the VideoPhy2 benchmark [2]. VideoPhy2 is an action-centric evaluation suite that measures video quality through physical commonsense (PC) and semantic adherence (SA), together with their joint pass rate (Joint). The benchmark includes 600 carefully curated prompts, along with subset of 180 more challenging ones. For both PC and SA, VideoPhy2 assigns integer scores from 1 to 5. Following the official definition, we treat scores of 4 or higher as PC = 1 or SA = 1. joint pass (Joint = 1) is achieved only when both PC = 1 and SA = 1. This joint rate is the primary metric for assessing the physical plausibility of generated videos. To show that ProPhy improves physical reasoning without sacrificing visual quality, we also assess our method on the quality-oriented dimensions of VBench [8], using the identical set of 600 prompts for consistency. 4.2. Quantitative Comparisons include several To validate the effectiveness of ProPhy, we compare it against its base models, Wan2.1 [30] and CogVideoX [35]. We further strong open-source T2V models (HunyuanVideo [12], Cosmos [21], CosmosPredict2.5 [21]) as well as physics-enhanced approaches WISA [32] and VideoREPA [38]. As shown in Table 1, applying ProPhy to CogVideoX yields either the best or the second-best results across all metrics. On the entire VideoPhy2 [2] benchmark (ALL), especially in terms of the Joint metric, ProPhy delivers substantial +19.7% improvement on the flow-matching-based model Wan2.1 [30], with consistent gains in PC and SA. On the more challenging HARD subset, our progressive alignment framework also produces videos that exhibit stronger semantic consistency and more faithful physical behaviors. As presented in Table 2, ProPhy also maintains strong generation capability on VBench metrics. Our improvements are most notable in the Dynamic Degree dimension, confirming that our progressive physical alignment framework enhances the models ability to capture highly In addition, the aggregated dynamic physical behaviors. Quality Score, computed as weighted combination of the seven VBench dimensions [8], shows that ProPhy produces videos of overall higher quality. 4.3. Qualitative comparison As shown in Figure 5, we present qualitative comparison between ProPhy, the base video generators (CogVideoX [35] and Wan2.1 [30]), and state-of-theart physics-aware approaches (VideoREPA [38] and WISA [32]). In the discus-throw scenario, existing models lack fine-grained physical alignment, often coupling the dust plume with the discus trajectory. In contrast, ProPhy correctly triggers dust emission only when the discus contacts the ground. In the iron-ball collision scenario, prior methods violate momentum conservation, exhibiting penetration artifacts and incorrect spatial reasoning. ProPhy, however, produces collisions that obey momentum conservation: after the larger ball strikes the smaller one, kinetic energy is transferred, causing the smaller ball to start moving from rest. 4.4. Ablation Study To validate the effectiveness of our progressive alignment framework, we use Wan2.1-1.3B as the baseline model. We conduct extensive ablation studies on this backbone and evaluate all variants on the VideoPhy2 [2] benchmark. As shown in Table 3, we compare LoRA fine-tuning of the backbone (without the Physical Branch) with our PhysicsBranch conditioning. Under the same number of training steps, the Physical Branch provides larger gains than plain LoRA. In addition, incorporating both the SEB and the REB 6 Table 2. Results on VBench quality score. For each method, the best performance relative to its base model is highlighted in bold. Method CogVideoX-5B [35] + WISA [32] + VideoREPA [38] + ProPhy Wan2.1-1.3B [30] + ProPhy Subject Background Temporal Motion Consistency Consistency flickering Smoothness Degree Dynamic Aesthetic Imaging Quality Score Quality Quality 92.9 91.7 91.3 93.2 88.8 89.7 95.6 94.9 94.5 95.1 92.8 93.1 97.9 98.1 98.3 98.3 96.7 97. 98.7 98.3 99.0 98.7 97.9 99.0 46.8 49.7 38.7 72.0 71.3 78.8 48.2 49.4 47.7 47.6 47.7 47. 51.1 54.4 48.5 66.0 57.3 58.4 76.8 78.8 77.0 81.0 77.3 79.0 Figure 5. Qualitative comparison among ProPhy, CogVideoX, Wan2.1, and existing physics-aware methods. further improves physical commonsense and semantic adherence. As shown in Table 4, we ablate the relative loss in the SEB and the two losses in the REB. In the SEB, replacing our relative-distance loss with BCE improves SA but weakens PC and Joint, indicating that the relative formulation provides more effective guidance. In the REB, using only the absolute alignment loss causes clear degradation, while using only the load-balance loss improves SA but reduces PC due to missing fine-grained cues. The full combination in ProPhy achieves the best performance across all metrics, confirming the effectiveness of our alignment design. Additional ablation studies are provided in the Supplementary Material. 4.5. Physics Learned by Experts To analyze how physical knowledge is captured in both the SEB and REB, we design the following experiments. For the SEB, we use WISA [32] labels to group prompts by physical category and randomly sample 100 prompts per category. These prompts are fed into the semantic router, and we examine the resulting logits. As shown in Figure 6, the router exhibits meaningful structure: physically related categories (e.g., combustion and explosion) show high Pearson correlation, whereas unrelated ones (e.g., explosion vs. refraction) show low similarity. This indicates that the SEB Table 3. Ablation study results on ProPhy with Wan2.1-1.3B as the base model. LoRA indicates that the Physical Branch is removed, and LoRA is applied to the backbone. Settings PB SEB REB LoRA LoRA LoRA LoRA Baseline - - - - - - - - PC SA Joint 57.8 58.2 62.7 62.2 64.0 58.7 63.7 63.5 65. 30.0 30.8 31.2 30.8 31.2 30.0 32.2 31.5 32.0 24.8 24.8 25.5 25.2 26.0 25.7 26.2 26.0 26.5 Table 4. Ablation study on the roles of relative loss and absolute loss during training. Settings baseline BCE loss in SEB only align loss in REB only load balance loss in REB ProPhy PC 57.8 64.3 58.3 64.0 65.0 SA 30.0 32.0 26.5 31.7 32.0 Joint 24.8 26.3 21.6 26.3 26.5 Figure 6. Analysis of the semantic router. represents the Pearson correlation coefficient calculated between different distributions. learns coherent physical semantics. For the REB, we visualize the projected logits of the refinement router. Figure 7 shows that regions with high activation reliably align with where corresponding physical events occur in the video, showing that the REB performs fine-grained physical alignment that guides the generator. 8 Figure 7. Refinement router expert maps. High-activation regions accurately localize where corresponding physical events occur, demonstrating the REBs fine-grained physical alignment. Figure 8. Physical attribute transfer via expert inversion. Flipping semantic-router logits injects incorrect physical cues, causing implausible behaviors (e.g., rigid car door fluttering), revealing that different experts encode distinct physical priors. Finally, we examine ProPhys ability to transfer physical attributes. During inference, we invert the refinement routers logits to introduce incorrect physical cues. As shown in Figure 8, this leads to physically impossible results, such as rigid car door fluttering like cloth. These behaviors indicate that different experts learn distinct physical priors. This also suggests that ProPhy can enable controllable manipulation of physical attributes in world simulation. 5. Conclusion In this paper, we presented ProPhy, Progressive Physical Alignment Framework for physics-aware video generation. We first identified the limitations of existing models in maintaining physical consistency, particularly their implicit physical guidance and lack of fine-grained spatial alignment. To address these issues, we introduced twostage MoPE mechanism to extract hierarchical physical priors, along with physical alignment strategy that transfers physical reasoning capabilities from VLMs into the generation process. Together, these designs enable the model to capture fine-grained, anisotropic physical dynamics and produce physically coherent videos. Extensive experiments demonstrate that ProPhy achieves state-of-the-art performance, while also validating the importance of fine-grained alignment in producing physically plausible video content. Limitations. The regions annotated for physical phenomena inevitably contain noise, and simple region-level physical categorization captures only coarse surface patterns. Future work may integrate these aligned regions with the governing physical differential equations to inject more interpretable and principled physical knowledge into video generation models."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 2, 6 [2] Hritik Bansal, Clark Peng, Yonatan Bitton, Roman Goldenberg, Aditya Grover, and Kai-Wei Chang. VideoPhy-2: Challenging Action-Centric Physical Commonsense Evaluation in Video Generation. arXiv preprint arXiv:2503.06800, 2025. 2, 6 [3] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators. OpenAI Blog, 1(8):1, 2024. 2, 3 [4] Jake Bruce, Michael D. Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, Yusuf Aytar, Sarah Maria Elisabeth Bechtle, Feryal Behbahani, Stephanie C. Y. Chan, Nicolas Heess, Lucy Gonzalez, Simon Osindero, Sherjil Ozair, Scott Reed, Jingwei Zhang, Konrad Zolna, Jeff Clune, Nando de Freitas, Satinder Singh, and Tim Rocktaschel. Genie: Generative Interactive Environments. 2024. 2 [5] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. 2024. 3 [6] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 33:68406851, 2020. [7] Panwen Hu, Jin Jiang, Jianqi Chen, Mingfei Han, Shengcai Liao, Xiaojun Chang, and Xiaodan Liang. Storyagent: Customized storytelling video generation via multi-agent collaboration. arXiv preprint arXiv:2411.04925, 2024. 2 [8] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive Benchmark Suite for Video Generative Models. In CVPR, pages 2180721818, 2024. 6 [9] Sihui Ji, Xi Chen, Xin Tao, Pengfei Wan, and Hengshuang Zhao. Physmaster: Mastering physical representation for video generation via reinforcement learning. arXiv preprint arXiv:2510.13809, 2025. 2, 3 [10] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. VACE: All-in-One Video Creation and Editing. arXiv preprint arXiv:2503.07598, 2025. 2 [11] Bingyi Kang, Yang Yue, Rui Lu, Zhijie Lin, Yang Zhao, Kaixin Wang, Gao Huang, and Jiashi Feng. How far is video generation from world model: physical law perspective. arXiv preprint arXiv:2411.02385, 2024. 3 [12] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Junkun Yuan, Yanxin Long, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Kai Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng, Yang Li, Yi Chen, Yutao Cui, Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Zunnan Xu, Yangyu Tao, Qinglin Lu, Songtao Liu, Dax Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, and Caesar Zhong. HunyuanVideo: Systematic Framework For Large Video Generative Models. arXiv preprint arXiv:2412.03603, 2025. 2, 3, [13] Kuaishou. Kling. https://klingai.kuaishou. com/, 2024. 3 [14] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 3 [15] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow Matching for Generative Modeling. In ICLR, 2022. 3 [16] Daochang Liu, Junyu Zhang, Anh-Dung Dinh, Eunbyung Park, Shichao Zhang, Ajmal Mian, Mubarak Shah, and Chang Xu. Generative physical ai in vision: survey. arXiv preprint arXiv:2501.10928, 2025. [17] Shaowei Liu, Zhongzheng Ren, Saurabh Gupta, and Shenlong Wang. Physgen: Rigid-body physics-grounded imageIn ECCV, pages 360378. Springer, to-video generation. 2024. 3 [18] Xin Ma, Yaohui Wang, Xinyuan Chen, Gengyun Jia, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent Diffusion Transformer for Video Generation. Transactions on Machine Learning Research, 2025. 3 [19] Antonio Montanaro, Luca Savant Aira, Emanuele Aiello, Diego Valsesia, and Enrico Magli. Motioncraft: PhysicsIn NeurIPS, pages based zero-shot video generation. 123155123181, 2024. 3 [20] Saman Motamed, Laura Culp, Kevin Swersky, Priyank Jaini, and Robert Geirhos. Do generative video models understand physical principles? arXiv preprint arXiv:2501.09038, 2025. 3 [21] NVIDIA, Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, Daniel Dworakowski, Jiaojiao Fan, Michele Fenzi, Francesco Ferroni, Sanja Fidler, Dieter Fox, Songwei Ge, Yunhao Ge, Jinwei Gu, Siddharth Gururani, Ethan He, Jiahui Huang, Jacob Huffman, Pooya Jannaty, Jingyi Jin, Seung Wook Kim, Gergely Klar, Grace Lam, Shiyi Lan, Laura Leal-Taixe, Anqi Li, Zhaoshuo Li, Chen-Hsuan Lin, Tsung-Yi Lin, Huan Ling, Ming-Yu Liu, Xian Liu, Alice Luo, Qianli Ma, Hanzi Mao, Kaichun Mo, Arsalan Mousavian, Seungjun Nah, Sriharsha Niverty, David Page, Despoina Paschalidou, Zeeshan Patel, Lindsey Pavao, [32] Jing Wang, Ao Ma, Ke Cao, Jun Zheng, Jiasong Feng, Zhanjie Zhang, Wanyuan Pang, and Xiaodan Liang. WISA: World simulator assistant for physics-aware text-to-video generation. In NeurIPS, 2025. 2, 3, 5, 6, 7 [33] Tianyi Xie, Yiwei Zhao, Ying Jiang, and Chenfanfu Jiang. Physanimator: Physics-guided generative cartoon animation. In CVPR, pages 1079310804, 2025. 3 [34] Qiyao Xue, Xiangyu Yin, Boyuan Yang, and Wei Gao. Phyt2v: Llm-guided iterative self-refinement for physicsgrounded text-to-video generation. In CVPR, pages 18826 18836, 2025. 3 [35] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Yuxuan.Zhang, Weihan Wang, Yean Cheng, Bin Xu, Xiaotao Gu, Yuxiao Dong, and Jie Tang. CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer. In ICLR, 2024. 2, 3, 6, 7 [36] Yu Yuan, Xijun Wang, Tharindu Wickremasinghe, Zeeshan Nadir, Bole Ma, and Stanley Chan. Newtongen: Physicsconsistent and controllable text-to-video generation via neural newtonian dynamics. arXiv preprint arXiv:2509.21309, 2025. 3 [37] Tianyuan Zhang, Hong-Xing Yu, Rundi Wu, Brandon Feng, Changxi Zheng, Noah Snavely, Jiajun Wu, and William Freeman. Physdreamer: Physics-based interaction with 3d objects via video generation. In ECCV, pages 388406. Springer, 2024. [38] Xiangdong Zhang, Jiaqi Liao, Shaofeng Zhang, Fanqing Meng, Xiangpeng Wan, Junchi Yan, and Yu Cheng. VideoREPA: Learning Physics for Video Generation through ReIn NeurIPS, lational Alignment with Foundation Models. 2025. 2, 3, 5, 6, 7 [39] Siyuan Zhou, Yilun Du, Jiaben Chen, Yandong Li, Dit-Yan Yeung, and Chuang Gan. RoboDreamer: Learning Compositional World Models for Robot Imagination. arXiv preprint arXiv:2404.12377, 2024. 2 [40] Zheng Zhu, Xiaofeng Wang, Wangbo Zhao, Chen Min, Nianchen Deng, Min Dou, Yuqi Wang, Botian Shi, Kai Wang, Chi Zhang, et al. Is sora world simulator? comprehensive survey on general world models and beyond. arXiv preprint arXiv:2405.03520, 2024. 2 Morteza Ramezanali, Fitsum Reda, Xiaowei Ren, Vasanth Rao Naik Sabavat, Ed Schmerling, Stella Shi, Bartosz Stefaniak, Shitao Tang, Lyne Tchapmi, Przemek Tredak, WeiCheng Tseng, Jibin Varghese, Hao Wang, Haoxiang Wang, Heng Wang, Ting-Chun Wang, Fangyin Wei, Xinyue Wei, Jay Zhangjie Wu, Jiashu Xu, Wei Yang, Lin Yen-Chen, Xiaohui Zeng, Yu Zeng, Jing Zhang, Qinsheng Zhang, Yuxuan Zhang, Qingqing Zhao, and Artur Zolkowski. Cosmos World Foundation Model Platform for Physical AI. arXiv preprint arXiv:2501.03575, 2025. 2, 3, 6 [22] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, pages 41954205, 2023. 3 [23] Zihan Qiu, Zeyu Huang, Bo Zheng, Kaiyue Wen, Zekun Wang, Rui Men, Ivan Titov, Dayiheng Liu, Jingren Zhou, and Junyang Lin. Demons in the Detail: On Implementing Load Balancing Loss for Training Specialized Mixture-ofExpert Models. arXiv preprint arXiv:2501.11873, 2025. 4 [24] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-Resolution Image In CVPR, pages Synthesis With Latent Diffusion Models. 1068410695, 2022. [25] Carlos Riquelme Ruiz, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andre Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling Vision with Sparse Mixture of Experts. In NeurIPS, 2021. 4 [26] Noam Shazeer, *Azalia Mirhoseini, *Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer. In ICLR, 2017. 5 [27] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising Diffusion Implicit Models. In ICLR, 2020. 3 [28] Alexey Stomakhin, Craig Schroeder, Lawrence Chai, Joseph Teran, and Andrew Selle. material point method for snow simulation. ACM TOG, 32(4):110, 2013. 3 [29] Xiyang Tan, Ying Jiang, Xuan Li, Zeshun Zong, Tianyi Xie, Yin Yang, and Chenfanfu Jiang. Physmotion: Physicsgrounded dynamics from single image. arXiv preprint arXiv:2411.17189, 2024. [30] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and Advanced Large-Scale Video Generative Models. arXiv preprint arXiv:2503.20314, 2025. 2, 3, 6, 7 [31] Cong Wang, Jiaxi Gu, Panwen Hu, Yuanfan Guo, Xiao Dong, Hang Xu, and Xiaodan Liang. Dreamvideo: Highfidelity image-to-video generation with image retention and text guidance. In ICASSP, pages 15. IEEE, 2025. 2 10 A. Implementation Details tained attributes include: A.1. Model Architecture and Settings We build our model on top of Wan2.1-T2V-1.3B and CogVideoX-5B. For the PB module without REB, its structure and initialization directly follow the corresponding Transformer layers in the backbone. Due to practical GPU memory and runtime considerations, in the 30-layer Transformer of Wan2.1-T2V-1.3B we reuse Blocks [0, 7, 14, 21, 28], and in the 42-layer Transformer of CogVideoX-5B we reuse Blocks [0, 9, 18, 27, 36]. As described in the main paper, REB is only attached to the last PB layer. To avoid unnecessary parameter inflation in the MoE implementation of REB, we set each experts hidden size to match the hidden states. With this configuration, the total additional parameters are approximately 31.3% for Wan2.1-T2V-1.3B and 19.4% for CogVideoX-5B. For computational efficiency, we set the number of physical basis maps Be in SEB to Es = 32, with each map sharing the same dimensionality as the hidden states of the corresponding model. We also set the number of refinement experts in REB to Er = 32, and implement the refinement router using top-4 selection. This configuration provides balanced trade-off between memory usage and computational cost. Regarding the loss-weight hyperparameters in the mixed objective of Equation 6, we use λ1 = 0.1, λ2 = 0.02, and λ3 = 0.01 across all experiments. These values follow standard practice in balancing multi-term objectives and work reliably to stabilize convergence in our setting. For the Router design, since the text encoder is typically frozen during finetuning and the forward path of the Semantic Router contains no additional trainable components, we implement it using lightweight MLP to provide adequate capacity for distinguishing physical categories. In contrast, because the Refinement Router already receives rich trainable features from earlier layers, we adopt simple Linear layer implementation, which is sufficient and avoids unnecessary overhead. A.2. Dataset and Annotation Protocol We randomly sample 20K videos from the WISA-80K dataset as our training data. Since quantitative physical categories are generally harder to annotate reliably, we only use the qualitative physical taxonomy as our alignment target. During Semantic Alignment, we adopt the complete set of physical categories from WISA-80K, i.e., Ewisa = 29 as described in the main paper. For Fine-grained Alignment, some categories in WISA-80K describe absence of physical phenomena (e.g., no obvious dynamic phenomenon), which are not suitable for defining fine-grained physical attributes. We therefore remove such categories and use Eattn = 23 attributes for fine-grained alignment. The rePhysical phenomena: rigid body motion, collision, liquid motion, gas motion, elastic motion, deformation, melting, solidification, vaporization, liquefaction, combustion, explosion, reflection, refraction, scattering, interference and diffraction, unnatural light source Physical appearances: liquid objects, solid objects, gas objects, object decomposition and splitting, mixing of multiple objects, object disappearance In summary, we utilize videos, text descriptions, and qualitative physical-category annotations from WISA-80K. Compared with general video datasets, our usage mainly involves incorporating the qualitative physical taxonomy. For any standard video dataset, similar qualitative physical labels can be obtained using Language Models or VisionLanguage Models. Therefore, our training pipeline does not rely heavily on WISA-80K and can be adapted to other datasets without structural changes. A.3. Training Details We conduct all experiments using four NVIDIA H100 GPUs with 80GB memory each, and train for fixed 8,000 steps across all settings. The learning rate is set to 1e-4, and we only update the SEB, PB, and the REB modules. For Wan2.1-T2V-1.3B, the input video resolution is 480 832 with 81 frames at 16 fps. For CogVideoX-5B, the input resolution is 480 720 with 49 frames at 8 fps. We adopt DDP for distributed training with per-GPU batch size of 4 (no gradient accumulation), resulting in total batch size of 16. We use the AdamW optimizer with cosine with restarts learning-rate schedule. CFG dropout strategy is enabled during training, and the dropout probability on text conditioning is set to 0.1. To maintain stability, PB is initialized from the corresponding Transformer Blocks in the backbone, and the projection from PB to the input layer is initialized to zeros. This prevents undesirable interference with the pretrained backbone at the early stage of training. A.4. Inference Details For inference, we use DDIM with 50 sampling steps. CFG is enabled by default. We measure inference time using only the forward pass of the Transformer modules. The added components introduce an overhead of 20.3% on Wan2.1T2V-1.3B and 11.5% on CogVideoX-5B relative to their respective baselines. ProPhy integrates physical-category reasoning directly into the routers and the model. As result, the entire inference pipeline is fully end-to-end and does not rely on external models to provide physical priors. In practical use, the extra cost is partly offset because no external physical-prediction VLMs/LLMs are needed. 11 Figure 10. Details of the two types of user inputs used to obtain token-level physical properties annotations. The angle brackets <> are replaced with the specified physical phenomenon or the given video. Figure 9. Principal component analysis of the activation distribution of the Semantic Router under different input prompt categories. A.5. Evaluation Details As described in the main paper, we evaluate generated video quality using VideoPhy2 and VBench. For consistency across all experiments, we use the 600 upsampled caption prompts provided by VideoPhy2 as the unified refined text input for video generation. B. Analysis and Ablation B.1. Relative Physical Semantic Analysis To further verify that our Router learns the physical principles behind different phenomena, we collect set of prompts from the WISA-80K dataset that never appear during training. For each physical phenomenon defined in the Semantic Router, we randomly sample 100 unseen prompts. We pass these prompts through the text encoder and the Semantic Router to obtain series of logits. Each logit vector has dimension of 32, corresponding to the number of physical basis maps Es. We provide in the main paper the logits distributions for four representative phenomena. For the complete set, direct visualization with histograms is not intuitive. We therefore apply principal component analysis Figure 11. Human analysis of fine-grained physical annotation accuracy. (PCA) to project the 32-dimensional logits into 2D space. This projection allows for clearer comparison of their relative relationships. Figure 9 shows the resulting 2D PCA plot. We use dashed boxes to highlight the physical macro-categories of each phenomenon. As shown, the three macro-categories form compact clusters with limited overlap. This pattern indicates that the Semantic Router has captured meaningful relationships among the phenomena. Otherwise, the distributions would appear random or uniform. We also observe that liquid motion and scattering lie close to each other, even though they belong to different categories. This proximity appears mainly in flowing-water videos, where splashing droplets often scatter light. This observation further suggests that the Semantic Router has learned reasonable and physically meaningful correlation between these two phenomena. B.2. Annotation Details and Analysis We propose method for obtaining token-level physical attributes by computing attention maps over the answers to 12 physical-description questions and background-description questions, as detailed in Section 3.3. Figure 10 illustrates how we obtain these answers through simple user instructions. We also experimented with using more elaborate prompts to induce longer responses from the VLM, but found that the resulting attention maps were essentially indistinguishable from those derived from shorter answers. Therefore, for efficiency, we request relatively concise responses, typically around 3050 words. t, t, H/r s, W/r To extract token-level physical attributes, we first use the prompts in Figure 10 to generate an answer. We then locate the answer tokens and video tokens in the token sequence, where the answer length is Sa and the video-token length is Sv. After passing them through the corresponding projection layers and computing scaled dot-product attention, the resulting attention map has shape of [Nvlm, Sa, Sv], where Nvlm denotes the number of decoder layers in the VLM. We average over the query dimension to treat the entire answer as single unit, and further average across all layers. This produces an attention map of shape [Sv]. Since Sv corresponds to the number of video tokens, it can be reshaped into the three-dimensional form [F/r s], representing the compressed number of frames, height, and width. Here (F, H, ) denote the original video length, height, and width, whereas (r s) represent the temporal and spatial downsampling ratios used by the VLM. For example, in the case of Qwen2.5-VL-32B, the spatial downsampling ratio is = 14, and the temporal downsampling ratio is computed based on video duration rather than frame count. To standardize processing, we rescale all videos to duration of 6 seconds to match the VLMs sampling rate of two frames per second. The video generation models Wan2.1-1.3B and CogVideoX-5B both use video encoders with temporal downsampling rt = 4 and spatial downsampling rs = 8, which introduces size mismatch between the alignment target and the video model features. We subtract the attention map obtained from physical prompts from that obtained from background prompts to produce the diff attention map, which is then upsampled via tricubic interpolation to match the larger hidden-state resolution, followed by mild smoothing to fill minor gaps. Because sign-based filtering of the diff attention map still leaves some noise, such as regions that do not correspond to the actual physical phenomenon, we limit the alignment supervision during training to at most 10% of the tokens in the diff map. These selected tokens are used to compute Lfine-align, which encourages the model to focus on the correctly annotated regions. To assess the accuracy of the fine-grained physical annotations, we sample 100 videos for each phenomenon together with their per-frame diff attention maps. We then evaluate their correctness through human qualitative inspection, as illustrated in Figure 11. We define four accu13 racy levels. Completely Accurate indicates that the VLMannotated regions almost perfectly match the true physicalphenomenon areas. Mostly Accurate means that the annotated regions largely overlap with the true areas but contain small amount of activation outside them. Mostly Inaccurate and Completely Inaccurate refer to cases where the overlap is small or nonexistent. Overall, combining the completely and mostly accurate cases, our annotation method achieves an accuracy of 76.9%. Thermodynamics and optics reach 87.7% and 80.0%, respectively. The accuracy for dynamics is lower, at 63.1%. We attribute this gap to the subtle nature of many dynamic phenomena, which often occupy small regions in the video and are therefore more difficult to capture. B.3. Additional Ablation Study To assess the practical impact of the added SEB and REB modules, we conduct qualitative ablation study using Wan2.1-1.3B as the baseline, as shown in Figure 12. In the first ball-passing scenario, the baseline model fails to correctly understand the interaction between the ball and the stick: the orange ball appears embedded inside the stick, accompanied by noticeable artifacts. With SEB added, these artifacts are alleviated, and the model becomes able to distinguish the orange ball from the stick; however, the small ball is not consistently maintained and disappears in an unnatural manner shortly afterward. When REB is added on top of SEB, ProPhy successfully completes the passing motion while preserving the balls shape and its physical interactions throughout the sequence. In the second pouring syrup scenario, the baseline model does not generate the downward flow of syrup after excessive accumulation on the pancakes. Adding SEB enables the correct downward-flow behavior, but without fine-grained alignment, unnatural liquid accumulation appears in regions the syrup has not touched. With REB further incorporated, the model produces more dynamic syrup-pouring motion without any violations of gravity or fluid-flow regularities. These results clearly demonstrate that SEB enhances the generation of global physical dynamics, while REB further refines the models ability to capture fine-grained physical behaviors. B.4. More Qualitative Results To further demonstrate the ability of our model to generate videos that follow fine-grained physical dynamics, we compare ProPhy with several prior state-of-the-art methods. As shown in Figure 13, under the same backbone, ProPhy produces videos that better align with physical commonsense, whereas other models often exhibit issues such as inconsistent object shapes, incorrect collision behaviors, or unnatural particle effects. To showcase the generalizability of our architecture and its capacity to model classical physiracy of these annotations. In addition, our approach uses physical categories and fine-grained supervision to help the model simulate physical phenomena. This allows ProPhy to behave like an initial version of world simulator. Even so, physical categorization only restricts the parameter space of each expert to certain subset of general physical behaviors. It does not enforce object dynamics through explicit physical equations. The model ultimately generates plausible videos by fitting patterns in real data rather than by following precise physical laws. Future research can extend our idea by incorporating the corresponding physical differential equations. Such guidance could allow different physical phenomena to be generated in more accurate and principled manner. D. Social Impact ProPhy enables text-to-video generation that aligns more closely with physical dynamics. Although it is still far from reproducing real-world physical scenes perfectly, it has the potential to support physics education in scenarios where real experiments are difficult to conduct. Regarding potential risks of identity leakage, the dataset we use contains almost no human faces. Most identity-related content, if any, comes from the generation capability of the underlying base models. Our method is designed purely for research on generative modeling and is not intended for use in safety-critical or deceptive applications. To support further research in the community, we will release all code after the final publication. Figure 12. Qualitative ablation analysis on the functional roles of each module. cal phenomena, we also present several visual results based on Wan2.1, as shown in Figure 14. These examples demonstrate that our model can reliably handle scenarios involving multiple interacting physical processes, while maintaining strong visual quality and physical plausibility. C. Limitations We fully recognize that our work still has several important limitations. First, our training process relies heavily on VLM-generated annotations of the physical attributes in videos. The alignment-based training does embed this physical awareness into the model and produces an end-toend inference pipeline. However, the physical knowledge learned by the model still depends strongly on the accu14 Figure 13. Comparison between ProPhy with different backbones and previous methods, including the baseline. More generated examples are provided in the supplementary video. 15 Figure 14. Examples of videos generated by ProPhy in response to text prompts involving complex physical phenomena."
        }
    ],
    "affiliations": [
        "ETH Zurich",
        "Lenovo Research",
        "Mohamed bin Zayed University of Artificial Intelligence",
        "Peng Cheng Laboratory",
        "Shenzhen Campus of Sun Yat-sen University"
    ]
}