{
    "paper_title": "DiP: Taming Diffusion Models in Pixel Space",
    "authors": [
        "Zhennan Chen",
        "Junwei Zhu",
        "Xu Chen",
        "Jiangning Zhang",
        "Xiaobin Hu",
        "Hanzhen Zhao",
        "Chengjie Wang",
        "Jian Yang",
        "Ying Tai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion models face a fundamental trade-off between generation quality and computational efficiency. Latent Diffusion Models (LDMs) offer an efficient solution but suffer from potential information loss and non-end-to-end training. In contrast, existing pixel space models bypass VAEs but are computationally prohibitive for high-resolution synthesis. To resolve this dilemma, we propose DiP, an efficient pixel space diffusion framework. DiP decouples generation into a global and a local stage: a Diffusion Transformer (DiT) backbone operates on large patches for efficient global structure construction, while a co-trained lightweight Patch Detailer Head leverages contextual features to restore fine-grained local details. This synergistic design achieves computational efficiency comparable to LDMs without relying on a VAE. DiP is accomplished with up to 10$\\times$ faster inference speeds than previous method while increasing the total number of parameters by only 0.3%, and achieves an 1.79 FID score on ImageNet 256$\\times$256."
        },
        {
            "title": "Start",
            "content": "DiP: Taming Diffusion Models in Pixel Space Zhennan Chen1,2* Junwei Zhu2 Xu Chen2 Hanzhen Zhao3 Chengjie Wang2 Jiangning Zhang2 Xiaobin Hu3 Jian Yang1 Ying Tai1 1Nanjing University 2Tencent Youtu Lab 3National University of Singapore https://github.com/NJU-PCALab/DiP 5 2 0 2 7 ] . [ 2 2 2 8 8 1 . 1 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Diffusion models face fundamental trade-off between generation quality and computational efficiency. Latent Diffusion Models (LDMs) offer an efficient solution but suffer from potential information loss and non-end-to-end In contrast, existing pixel space models bytraining. pass VAEs but are computationally prohibitive for highresolution synthesis. To resolve this dilemma, we propose DiP, an efficient pixel space diffusion framework. DiP decouples generation into global and local stage: Diffusion Transformer (DiT) backbone operates on large patches for efficient global structure construction, while co-trained lightweight Patch Detailer Head leverages contextual features to restore fine-grained local details. This synergistic design achieves computational efficiency comparable to LDMs without relying on VAE. DiP is accomplished with up to 10 faster inference speeds than previous method while increasing the total number of parameters by only 0.3%, and achieves an 1.79 FID score on ImageNet 256256. 1. Introduction Diffusion models [19, 20, 27, 38, 41, 44, 4749, 57, 61] have reshaped the landscape of generative visual content. With its outstanding generative capabilities of fidelity and diversity, they have established new state-of-the-art benchmarks across multitude of tasks, including image synthesis [3, 68, 11, 53, 60, 68, 70, 73, 74], video generation [13, 36, 64], and 3D object creation [6567], decisively surpassing prior paradigms like Generative Adversarial Networks (GANs) [14, 26, 35, 40, 69, 75]. However, this generative prowess is underpinned by immense computational demands. Consequently, the inherent trade-off between generation quality and computational efficiency thus stands as one of the most critical challenges in the field of diffusion models today. To mitigate this challenge, Latent Diffusion Models (LDMs) [42] have emerged as the de facto standard. By employing pre-trained autoencoder (VAE) [30] to compress *Work done during the internship at Tencent. Project Leader. Corresponding Author. Figure 1. Comparison of vanilla latent diffusion model, vanilla pixel diffusion model and our method. Vanilla LDMs utilize VAEs to balance computational efficiency and generation quality. Vanilla pixel diffusion models use small patches to pursue detailed generation quality. Our method achieves high-quality generation while maintaining efficient end-to-end training in pixel space. high-resolution images into compact latent space, LDMs significantly reduce the computational complexity of the iterative denoising process, as shown in Figure 1(a). Nevertheless, this approach is not without its limitations, including potential information loss [4, 17, 28, 59] during VAE compression and non-end-to-end training pipeline. The most direct solution to eliminate the shortcomings of LDMs is to train diffusion model in pixel space. However, existing pixel space diffusion models [5, 9, 22, 23, 29, 50], particularly those based on the powerful Transformer architecture [52], face severe scalability issue. As shown in Figure 1(b), to capture fine-grained details, they typically rely on small input patches (e.g. 22 or 44), causing the input sequence length to grow quadratically with image resolution. This quadratic scaling renders high-resolution We propose DiP, new end-to-end pixel diffusion model framework that effectively alleviates the trade-off between generation quality and computational efficiency through synergistic global-local modeling. We systematically validate the impact of different architectural designs of our framework, hoping to provide the community with unified, principled framework. On ImageNet generation benchmarks, our framework achieves state-of-the-art performance and lowest inference latency with low training costs. 2. Related Work Latent Diffusion Models. Latent Diffusion Models (LDMs) [2, 3, 12, 39, 42, 57, 62] have become the de-facto paradigm for large-scale generative modeling due to their computational efficiency and scalability. By performing the diffusion process in compressed latent space learned by VAE, LDMs drastically reduce memory and computational costs. Architectural advancements within this paradigm, such as replacing the U-Net [43] backbone with more scalable Transformer (DiT) [38], have further pushed the boundaries of generation quality. Despite their success, this efficiency comes at cost: the VAE acts as an information bottleneck, imposing hard ceiling on the final image fidelity and often introducing subtle reconstruction artifacts [46, 59]. Our work circumvents these limitations by proposing an equally efficient architecture that operates directly in pixel space, thereby eliminating the VAE-induced quality constraints. Pixel Diffusion Models. Recent years have seen renewed interest in pixel space diffusion models that aim to maximize signal fidelity while addressing computational inefficiency. Early works such as ADM [9] and DDPM [20] demonstrated the power of diffusion but were constrained by the quadratic complexity of their backbones, rendering them impractical for high resolutions. Multi-scale and image patch-based methods [5, 10, 22, 23, 23] further enhance the generation effect by decomposing large images into small patches. However, these methods essentially simulate locality through brute-force training, which leads to extremely low efficiency. Concurrent work JiT [33] demonstrates that high-dimensional data in pixel space can be effectively modeled by predicting clean images. Recent work by PixelNerd [55] leverages Transformer to process image features, which then conditions NeRF-like coordinate network to act as renderer for finely reconstructing each image patch, achieving impressive performance. Nevertheless, PixelNerd tightly couples the success of its method with this specific NeRF-like rendering mechanism, which may limit the exploration of broader design space. We argue that the key to achieving efficient and high-quality pixel space generation lies not in relying on specific structure like NeRF, but rather in the design principle of decoupling global strucFigure 2. Our method achieves the best FID score with minimal computational cost. (Note: LDM latency includes VAE. The methods marked with dashed lines ( ) are our estimated latency based on the sampling method in the corresponding paper, and should actually be greater than the marked values. The rest methods are the actual test results in the same hardware environment.) training and inference computationally intractable, creating formidable barrier to their practical application. In this paper, we aim to resolve this critical trade-off in pixel diffusion models. We propose an efficient pixel space diffusion framework called DiP. As shown in Figure 1(c), for efficient global structure construction, we employ DiT [38] backbone. Critically, we configure it to operate on large image patches (e.g., 1616). This setting choice drastically reduces the input sequence length, aligning it with that of mainstream LDMs operating in latent space. Consequently, our model achieves computational efficiency comparable to LDMs while remaining entirely VAE-free, enabling it to effectively capture the global layout and semantic content of the image. However, operating on large patches alone inevitably leads to blurry outputs lacking high-frequency details. To address this, we introduce lightweight Patch Detailer Head (only 0.3% increase in total parameters), which is not post-processing module but an integral component co-trained with the DiT backbone. For each large patch, it receives contextual features from the DiT and leverages its strong local receptive fields to synthesize the missing high-frequency information. This synergistic design allows the DiT backbone to focus on the computationally demanding task of global consistency, while the efficient Patch Detailer Head specializes in local texture and detail restoration. As demonstrated in Figure 2, our approach sets new state-of-the-art on the efficiencyquality frontier, achieving superior FID scores at significantly lower latency compared to existing methods. Our main contributions are summarized as follows: ture construction from local detail refinement. Based on this insight, this paper aims to provide more principled, efficient, and general solution for pixel space diffusion models. 3. Methods 3.1. Preliminaries diffusion process gradually perturbs an initial data sample x0 (x0) from the true data distribution into isotropic Gaussian noise: (1) xt = i=1 αi. {βt}T 1 αtϵ, where ϵ (0, I), αtx0 + where αt = 1 βt and αt = (cid:81)t i=1 is predefined variance schedule that controls the noise level at each step. As T, αt 0, and the distribution of xT converges to standard normal distribution (xT ) (0, I). This discrete formulation can be generalized to continuous-time setting via stochastic differential equation (SDE): Figure 3. Overfitting the DiT-only model using single image in pixel space leads to poor detail reconstruction. Introducing local inductive bias achieves better reconstruction and accelerates convergence. Please zoom in for details. dx = (x, t)dt + g(t)dw, (2) where (, t) is the drift and g(t) is the diffusion coefficient. The trajectory of this reverse process is governed by corresponding probability flow ordinary differential equation (ODE): dx = (cid:2)f (x, t) g(t)2x log pt(x)(cid:3) dt. (3) Learning to generate data is thus equivalent to learning the score function log pt(x) or the associated vector field of this ODE. To train neural network for this task, several objectives have been proposed. DDPM trains model ϵθ (xt, t) to predict the noise component θ from noisy sample xt: LDDPM = Et,x0,ϵ (cid:104) ϵ ϵθ (xt, t)2(cid:105) . (4) Flow Matching (FM) [12] provides simulation-free paradigm for directly learning the vector field. It defines conditional probability path pt (x x0) and corresponding target vector field ut(x). network vθ(x, t) is then trained to regress this field by minimizing the loss: LFM = Et,pt(xx0) (cid:104) ut(x) vθ(x, t)2(cid:105) . (5) 3.2. Motivation DiT models the long-range dependencies of an image by partitioning it into sequence of patches, thereby forming coherent global structure. However, while the self-attention mechanism excels at modeling macroscopic relationships between patches, it compresses the rich spatial information within each patch into single, flattened token. This design the model can adeptly introduces an inherent limitation: Figure 4. The t-SNE visualization of feature space. In the ImageNet validation set, 100 samples were randomly selected from each of the 10 classes for feature visualization. Features are extracted using DiT-only and our method, with each class shown in distinct color. learn the coarse-level layout and arrangement of patches but struggles to model the fine-grained textures and highfrequency details within each patch, consequently limiting the upper bound of its image generation performance. To empirically validate this, we conduct preliminary experiment by overfitting DiT model on single highresolution image in the pixel space. As shown in Figure 3, the model successfully captures the global layout and color palette but fails to render fine textures and sharp edges, resulting in blurry reconstruction. This result demonstrates that when DiT architecture operates directly on images, it suffers from lack of inductive bias [1, 16, 25, 58] at the local level, rendering it incapable of achieving precise pixel-level reconstruction within each patch. This motivates our core design principle: to augment the global Transformer with dedicated module that explicitly re-injects this missing inductive bias for local details. In this way, our model can leverage the computational efficiency afforded by large patch sizes while simultaneously generating high-quality images with fine-grained details. As shown in Figure 4, our method achieves tighter intra-class clusters and clearer inter-class separation, whereas vanilla DiT exhibits more mixed distributions. This means that the introduction of local inductive bias can more effectively integrate local textures and edge cues in pixel space and thus improves high-level semantic separability and feature conFigure 5. Patch Detailer Head with local inductive bias was placed at different locations in the model. The results in Sec. 4.3 show that all three methods offer gains compared to DiT-only. sistency. Such improvements are expected to yield more stable structural alignment and better detail during generation process. 3.3. Framework Based on the above observation, we introduce framework for high-quality image generation that operates directly in pixel space. DiP first employs DiT to model the global structure and long-range dependencies of the image. Subsequently, lightweight Patch Detailer Head refines the output at the patch level, introducing local inductive bias to synthesize high-frequency details. Global Structure Construction (DiT Backbone). Given noisy image xt RHW 3 at timestep t, we first partition it into sequence of non-overlapping patches. Each patch has size of (we set P=16), resulting in sequence of =(H )/P 2 patches. This patching strategy ensures our pixel space model maintains computational footprint comparable to latent space DiT models. Along with timestep embedding and positional embeddings, they are fed into series of DiT blocks to produce sequence of context-aware output features Sglobal RN D, where is the feature dimension. Local Detail Refinement (Patch Detailer Head). The Patch Detailer Head operates independently and in parallel on each patch. For each patch i, it takes two inputs: the corresponding global context map si and the original noisy pixel patch pi R3P , where si RD11 is obtained by reshaping and expanding Sglobal. Its objective is to leverage the global context from Sglobal to accurately interpret the local noisy information in pi, ultimately predicting the corresponding noise component ϵi R3P for that patch. After processing all patches in parallel, the resulting sequence of predicted noise patches {ϵi}N i=1 is reassembled into full-resolution noise prediction map. 3.4. Architecture Design Exploring Patch Detailer Head Architectures. We investigated several architectures for the Patch Detailer Head, each embodying different form of inductive bias. Our goal is to present simple, effective and highly efficient design. Figure 6. Patch Detailer Head framework. This design introduces the local inductive bias that DiT-only lacks with low number of parameters, resulting in high-quality image with rich detail. Standard MLP. As simple baseline, we used MLP that takes the feature vector si and flattened noisy patch pi as input. While straightforward, this design lacks any inherent spatial bias, treating all pixels within the patch as an unordered set. Coordinate-based MLP. To introduce spatial awareness, design inspired by NeRF can be adopted [55]. For each pixel within pi, we concatenate its normalized 2D coordinates. si is used to dynamically generate the weights of small, coordinate-based MLP. This implicitly learns continuous function of the image patch, but it lacks the strong priors for local texture and structure that convolutions provide. Intra-Patch Attention. We explored using small Transformer to operate on the pixels within each patch. Each patch is treated as sequence of 2 pixel tokens. This allows for complex, content-aware interactions between pixels but is computationally intensive and may not be as efficient as convolutions for learning local patterns. Convolutional U-Net (Our Final Choice). We found that lightweight convolutional U-Net provided the best performance. The hierarchical structure of downsampling and upsampling paths, combined with skip connections, is exceptionally well-suited for capturing multi-scale spatial features and ensuring local continuity. The inherent inductive biases of convolutions (locality and translation equivariance) are highly effective for denoising local textures and edges. As shown in Figure 6, we instantiate the Patch Detailer Head with shallow U-Net, which includes 4 downsampling and 4 upsampling blocks. Each block consists of sequence of Convolution, SiLU activation and pooling layer. The global feature vector si is"
        },
        {
            "title": "Latent Generative Models",
            "content": "LDM [42] DiT-XL [38] MaskDiT-G [72] SiT-XL [34] FlowDCN-XL [54]"
        },
        {
            "title": "Pixel Generative Models",
            "content": "CDM [21] ADM [9] JetFormer-L [51] SiD [22] VDM++ [29] RIN [24] Farmer/16 [71] PixelFlow-XL/4 [5] DiP-XL/16 DiP-XL/16 DiP-XL/16 FID sFID IS Prec. Rec. Latency Epochs NFE"
        },
        {
            "title": "Params",
            "content": "ImageNet 256256 3.60 2.27 2.28 2.06 2.00 4.88 3.94 6.64 2.77 2.12 3.42 3.96 1.98 2.16 1.98 1.79 - 4.60 5.67 4.50 4.33 - 6.14 - - - - - 5. 4.79 4.57 4.59 247.7 278.2 276.6 270.3 263.1 158.7 215.8 - 211.8 278.1 182.0 250.6 282.1 276.8 282.9 281.9 0.87 0.83 0.80 0.82 0.82 - 0.83 0.69 - - - 0.79 0. 0.82 0.80 0.80 0.48 0.57 0.61 0.59 0.58 - 0.53 0.56 - - - 0.50 0.60 0.61 0.62 0.63 - 2.09s - 2.09s - - 15.80s - - - - - 7.50s 0.92s 0.70s 0.92s 170 1400 1600 1400 400 2160 400 500 800 - 480 320 320 160 320 600 250x2 400M+86M 250x2 675M+86M 79x2 675M+86M 250x2 675M+86M 250x2 618M+86M 4100 500 - 2502 2502 1000 - 120x 100x2 75x2 100x2 - 554M 2.8B 2.0B 2.46B 410M 1.9B 677M 631M 631M 631M Table 1. Comparison of the performance of different methods on ImageNet 256256 with Euler solver and CFG. Performance metrics are annotated with (higher is better) and (lower is better). Our method achieves the best FID score. Furthermore, compared to other pixel diffusion models, we achieve the best performance across all metrics with the lowest latency. concatenated channel-wise with the downsampling output at the bottleneck. This design allows the global semantic information to guide the local refinement process effectively while keeping the parameter count minimal. We provide experimental evidence for this part in Sec. 4.3. Furthermore, we present preliminary theoretical analysis in Appendix to model the necessity and effectiveness of the Patch Detailer Head, aiming to offer deeper insights. Placement of the Patch Detailer Head. Since the main weakness of DiT backbone being trained directly in pixel space is its lack of local awareness, natural design question arises: does introducing Patch Detailer Head at different locations in the model also bring gains? As shown in Figure 5, we investigated three placement strategies: Post-hoc Refinement. The Patch Detailer Head is placed only after the final DiT block. This creates clean separation of concerns: the DiT is solely responsible for global modeling, and Patch Detailer Head is solely responsible for local refinement. Intermediate Injection. The Patch Detailer Head is inserted between DiT blocks. The refined patch representations are then projected back and fed into the subsequent DiT blocks. Hybrid Injection. Patch Detailer Head are placed both at an intermediate stage and at the end of the DiT. Our experiments in Sec. 4.3 revealed that all three strategies yield comparable performance gains over the baseline DiT. However, the Post-hoc Refinement strategy has unique advantage: by placing the Head at the end, we treat the standard DiT architecture as fixed, black-box backbone. This approach requires no modification to the DiTs internal structure, greatly simplifying implementation and potentially allowing for the use of pre-trained DiT checkpoints. Given its optimal balance between high performance and implementation simplicity, we adopt the postrefinement strategy as the final architecture. 4. Experiments 4.1. Setup Implementation Details. Our experiments are conducted on the class-conditional ImageNet dataset and original images are center-cropped and resized to 256256 resolution. We set global batch size to 256. We use DDT [56], variant of DiT, as our model backbone and apply an Exponential Figure 7. Qualitative samples from our model trained at 256 256 resolution with classifier-free guidance scale of 4.0. DiP demonstrates fine-grained detail, and high visual quality. Moving Average (EMA) on the model weights with decay factor of 0.9999. In Patch Detailer Head, the kernel size of the middle layers is set to 3, the padding to 1, and the kernel size of the last convolutional layer is set to 1. Unless otherwise specified, all samples were generated using the Euler-100 solver. More details are included in Appendix. Evaluation Protocol. To ensure comprehensive and rigorous assessment of our models generative capabilities, we adhere to the evaluation protocol established by ADM [9]. We employ suite of standard quantitative metrics to measure performance across different dimensions. Specifically, we use the Frechet Inception Distance (FID) [18] to assess overall realism and fidelity, the Spatial FID (sFID) [37] to evaluate spatial and structural coherence, and the Inception Score (IS) [45] to measure class-conditional diversity. Furthermore, we report Precision (Prec.)/Recall (Rec.) [31] to respectively quantify the fidelity of individual samples and the models ability to cover the true data distribution. All metrics are calculated using 50,000 generated samples. 4.2. Main Result Performance. Table 1 presents comprehensive comparison against recent SOTA methods with classifier-free guidance scheduling with guidance interval [32]. After 600 training epochs, DiP achieved an FID of 1.79 without requiring pre-trained VAE, surpassing potentially diffusion models such as DiT-XL (FID 2.27) and SiT-XL (FID 2.06), which require longer training times. DiP outperforms the previous best pixel-based model, PixelFlow-XL/4 (FID 1.98), and significantly exceeds others like ADM (FID 3.94) and VDM++ (FID 2.12). Even with shorter training schedule of 160 epochs, our model reaches competitive FID of 2.16, outperforming established models like DiT-XL that require much longer training. Figure 7 presents qualitative samples of DiP at 256256 resolution. These visualizations reveal rich detail, demonstrating the effectiveness of introducing local inductive bias. More visualization samples are provided in Appendix. Computational Cost Comparison. DiPs parameter count (631M) is significantly smaller than other pixel models, such as VDM++ (2.0B) and Farmer (1.9B). DiP reaches its best performance with only 320 epochs, which is over 4 more efficient than DiT-XL and SiT-XL (1400 epochs) and substantially faster than many other pixel-based methods like CDM (2160 epochs). In single-image inference speed tests, DiP (0.92s) is more than 2.2 faster than DiTXL (2.09s) and more than 8 faster than the previous best pixel model, PixelFlow-XL (7.50s). Furthermore, in 75step inference, DiP (0.70s) achieved the same FID score as PixelFlow-XL with speed more than 10 faster. Method ImageNet 256256 FID sFID IS Prec. Rec. Training Cost Latency Params Scaling Up DiT DiT-only (26 Layers, 1152 Hidden Dim) DiT-only (32 Layers, 1152 Hidden Dim) DiT-only (26 Layers, 1280 Hidden Dim) DiT-only (26 Layers, 1536 Hidden Dim) Different Patch Detailer Head Standard MLP Intra-Patch Attention Coordinate-based MLP Convolutional U-Net 5.28 4.91 4.28 2.83 6.92 2.98 2.20 2.16 6.56 6.44 6.26 5.16 7.27 5.16 4.49 4.79 243.8 251.7 249.6 285.6 210.9 275.0 284.6 276. 0.74 0.74 0.77 0.80 0.79 0.80 0.80 0.82 0.55 0.56 0.56 0.57 0.41 0.56 0.58 0.61 848 GPU Hours 1038 GPU Hours 1038 GPU Hours 1498 GPU Hours 938 GPU Hours 968 GPU Hours 1238 GPU Hours 928 GPU Hours 0.88s 1.05s 1.06s 1.49s 0.91s 0.94s 0.95s 0.92s 629M 772M 776M 1.1B 630M 630M 700M 631M Table 2. Impact of different design schemes on computational overhead and performance. Figure 8. The t-SNE visualization of feature space. Features are extracted using Post-hoc Refinement, Intermediate Injection, and Hybrid Injection, with each class shown in distinct color. 4.3. Analysis In this section, we analyzed the trade-off between generation quality and computational cost during the development of DiP, and at the same time explained the rationality of the Patch Detailer Head we designed. Patch Detailer Head vs. Scaling Up DiT. common strategy to improve generative models is to increase the model size. However, our findings indicate that this is suboptimal approach for pixel space diffusion models. As shown in Table 2, increasing the DiTs depth from 26 to 32 layers yields only marginal improvement (FID from 5.28 to 4.91) at considerable cost in parameters and training time. It also means that the effectiveness of our Patch Detailer Head comes from the introduction of effective local inductive biases, rather than increasing network depth. In contrast, widening the model proves more effective for quality improvement. For instance, scaling the hidden dimension to 1536 reduces the FID to 2.83. This substantial quality gain comes at prohibitive cost: 74.9% increase in parameters (from 629M to 1.1B), 77.4% rise in training cost (from 848 to 1498 GPU hours), and 69.3% (from 0.88s to 1.49s) increase in inference latency. This highlights critical challenge with monolithic scaling, where significant computational resources are required for performance. Experimental Results of Exploring Patch Detailer Head Architectures. We further investigated different architecMethod ImageNet 512512 FID sFID Prec. Rec. IS Params Latent Generative Models DiT-XL [38] MaskDiT-G [72] SiT-XL [34] FlowDCN-XL [54] Pixel Generative Models ADM [9] SiD [22] VDM++ [29] RIN [24] DiP-XL/32 3.04 2.50 2.62 2.44 3.85 3.02 2.65 3. 2.31 5.02 5.10 4.18 4.53 5.86 - - - 4.48 0.84 0.83 0.84 0.84 0.84 - - - 0.84 0.54 0.56 0.57 0.54 0.53 - - - 0.58 240.8 256.3 252.2 252.8 675M+86M 675M+86M 675M+86M 618M+86M 221.7 248.7 278.1 216.0 291.68 554M 2.00B 2.46B 410M 631M Table 3. Comparison of the performance of different methods on ImageNet 512512 with CFG. Performance metrics are annotated with (higher is better) and (lower is better). Our method remains competitive at higher resolutions. tures for the Patch Detailer Head to understand the importance of inductive bias in local patch refinement. (1) The Standard MLP performs poorly (6.92 FID), even worse than the DiT-only baseline. This is expected, as it lacks any spatial inductive bias, treating patch pixels as an unordered set and failing to capture crucial local structures. (2) The IntraPatch Attention shows significant improvement over the MLP (FID 2.98). This indicates that content-aware relationships between pixels are valuable. Its training and inference costs are only slightly higher than our final choice, but its actual memory overhead is about twice that of the final solution. (3) The Coordinate-based MLP achieves 2.20 FID (we are based on reproduction of [55]). By explicitly conditioning on pixel coordinates, it effectively introduces spatial awareness. However, it requires more parameters (700M) and longer training time (1238 GPU hours) compared to our final choice, and its implicit continuous representation may lack the strong, built-in priors for local patterns that convolutions provide. (4) The Convolutional U-Net increases the number of parameters by only 0.3% (from 629M Figure 9. Qualitative samples from our model trained at 512512 resolution with classifier-free guidance scale of 4.0. DiP showcases fine-grained detail and rich diversity at higher resolutions. to 631M) and achieves the best FID score with the lowest computational cost among all Patch Detailer Head. Its success can be attributed to highly relevant inductive biases of It is well-suited for capturing and preservconvolutions. ing the continuity of local textures and edges, making it the most efficient and effective architecture for performing patch-level detail optimizations. In summary, our experimental results clearly demonstrates that introducing an appropriate local inductive bias via Patch Detailer Head is key to performance improvement over the scaling up DiT baseline. Among the architectures explored, the Convolutional U-Net strikes the optimal balance between best generation quality and minimal computational cost, making it our definitive choice. Experimental Results of Placement of the Patch Detailer Head. We tested the effects of introducing Patch Detailer Head at different locations in the model. As shown in Figure 8, all three introduction modes showed significant improvements compared to DiT-only (FID 5.28). From the feature visualization results, Hybrid Injection performed worse in clustering than the other two, which may be due to multiple local inductive biases potentially disrupting the original structure, leading to performance degradation. Post-hoc Refinement achieves the best performance, result attributed to the synergy between global build and local refinement, while its implementation is simple and easily extensible. 4.4. Ablation Study Performance on ImageNet 512512. As shown in Table 3, on 512512 resolution, DiP achieved the bset FID score , surpassing previous methods. Figure 9 illustrates the sampling results at 512512, demonstrating that DiP can also generate high-quality images. We present more qualitative results in Appendix. Figure 10. (a). Performance differences between different Patch Detailer Head configurations. Depth is defined as the number of down/up-sampling stages, and width corresponds to the number of base channels in the convolutional layers. (b). Performance and computational overhead ifferences of different patch sizes. Impact of Patch Detailer Head Configuration. Our findings reveal distinct trends for depth and width. As we increase the depth, we observe consistent improvement in generation quality, although the gains exhibit diminishing returns and eventually saturate. This suggests that multi-scale feature hierarchy is crucial for the Patch Detailer Head to effectively synthesize high-frequency details across. Conversely, blindly increasing the width will not lead to sustained performance improvement. This indicates that the role of Patch Detailer Head is not to perform complex feature transformations, but rather to render specific details. Impact of Patch Size. Small patch size offer superior performance but also significantly increase computational overhead. Our method can use large patch to shorten the input sequence length, making our models computational efficiency comparable to mainstream LDMs. As shown in Figure 10(b), at the higher 512512 resolution, DiP maintains significant performance margin over DiTonly baseline using smaller patch size. This validates that our approach provides robust solution for efficient, highresolution synthesis directly in pixel space. 5. Conclusion In this paper, we addressed the fundamental trade-off between generation quality and computational efficiency in pixel diffusion models. We introduced DiP, new endto-end pixel space diffusion framework that resolves this dilemma through synergistic global-local modeling approach. By employing DiT backbone on large image patches, we achieve computational efficiency comparable to LDMs for modeling global structures. This is complemented by co-trained, lightweight Patch Detailer Head that expertly restores high-frequency details, effectively bypassing the need for VAE. Our extensive experiments on the ImageNet benchmark demonstrate that achieves superior FID scores with significantly lower inference latency and training costs. In the future, we plan to apply the DiP framework to text to image and text to video tasks to further explore the capabilities of this solution."
        },
        {
            "title": "References",
            "content": "[1] Jie An, De Wang, Pengsheng Guo, Jiebo Luo, and Alex Schwing. On inductive biases that enable generalization in diffusion transformers. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2024. 3 [2] BlackForest. Black forest labs; frontier ai lab, 2024. 2 [3] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. 1, 2 [4] Junyu Chen, Han Cai, Junsong Chen, Enze Xie, Shang Yang, Haotian Tang, Muyang Li, Yao Lu, and Song Han. Deep compression autoencoder for efficient high-resolution diffusion models. arXiv preprint arXiv:2410.10733, 2024. 1 [5] Shoufa Chen, Chongjian Ge, Shilong Zhang, Peize Sun, and Ping Luo. Pixelflow: Pixel-space generative models with flow. arXiv preprint arXiv:2504.07963, 2025. 1, 2, 5 [6] Zhennan Chen, Rongrong Gao, Tian-Zhu Xiang, and Fan Lin. Diffusion model for camouflaged object detection. In ECAI 2023, pages 445452. IOS Press, 2023. 1 [7] Zhennan Chen, Yajie Li, Haofan Wang, Zhibo Chen, Zhengkai Jiang, Jun Li, Qian Wang, Jian Yang, and Ying Tai. Ragd: Regional-aware diffusion model for text-to-image generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1933119341, 2025. [8] En Ci, Shanyan Guan, Yanhao Ge, Yilin Zhang, Wei Li, Zhenyu Zhang, Jian Yang, and Ying Tai. Describe, dont dictate: Semantic image editing with natural language intent. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1918519194, 2025. 1 [9] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 1, 2, 5, 6, [10] Zheng Ding, Mengqi Zhang, Jiajun Wu, and Zhuowen Tu. Patched denoising diffusion models for high-resolution imIn The twelfth international conference on age synthesis. learning representations, 2023. 2 [11] Nikai Du, Zhennan Chen, Shan Gao, Zhizhou Chen, Xi Chen, Zhengkai Jiang, Jian Yang, and Ying Tai. Textcrafter: Accurately rendering multiple texts in complex visual scenes. arXiv preprint arXiv:2503.23461, 2025. 1 [12] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 2, 3 [13] Tiehan Fan, Kepan Nan, Rui Xie, Penghao Zhou, Zhenheng Yang, Chaoyou Fu, Xiang Li, Jian Yang, and Ying Tai. Instancecap: Improving text-to-video generation via instanceIn Proceedings of the Computer aware structured caption. Vision and Pattern Recognition Conference, pages 28974 28983, 2025. 1 Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139144, 2020. 1 [15] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. [16] Anirudh Goyal and Yoshua Bengio. Inductive biases for deep learning of higher-level cognition. Proceedings of the Royal Society A, 478(2266):20210068, 2022. 3 [17] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Fei-Fei Li, Irfan Essa, Lu Jiang, and Jose Lezama. Photorealistic video generation with diffusion models. In European Conference on Computer Vision, pages 393411. Springer, 2024. 1 [18] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 6 [19] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 1 [20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 1, 2 [21] Jonathan Ho, Chitwan Saharia, William Chan, David Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. Journal of Machine Learning Research, 23(47):133, 2022. 5 [22] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high resolution imIn International Conference on Machine Learning, ages. pages 1321313232. PMLR, 2023. 1, 2, 5, [23] Emiel Hoogeboom, Thomas Mensink, Jonathan Heek, Kay Lamerigts, Ruiqi Gao, and Tim Salimans. Simpler diffusion (sid2): 1.5 fid on imagenet512 with pixel-space diffusion. arXiv preprint arXiv:2410.19324, 2024. 1, 2 [24] Allan Jabri, David Fleet, and Ting Chen. Scalable adaparXiv preprint tive computation for iterative generation. arXiv:2212.11972, 2022. 5, 7 [25] Zahra Kadkhodaie, Florentin Guth, Eero Simoncelli, and Stephane Mallat. Generalization in diffusion models arises arXiv from geometry-adaptive harmonic representations. preprint arXiv:2310.02557, 2023. 3 [26] Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 44014410, 2019. 1 [27] Tero Karras, Miika Aittala, Tuomas Kynkaanniemi, Jaakko Lehtinen, Timo Aila, and Samuli Laine. Guiding diffusion model with bad version of itself. Advances in Neural Information Processing Systems, 37:5299653021, 2024. 1 [28] Maciej Kilian, Varun Jampani, and Luke Zettlemoyer. Computational tradeoffs in image synthesis: Diffusion, masked-token, and next-token prediction. arXiv preprint arXiv:2405.13218, 2024. 1 [14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and [29] Diederik Kingma and Ruiqi Gao. Understanding diffusion objectives as the elbo with simple data augmentation. Advances in Neural Information Processing Systems, 36: 6548465516, 2023. 1, 5, 7 [30] Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 1 [31] Tuomas Kynkaanniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. Advances in neural information processing systems, 32, 2019. 6 [32] Tuomas Kynkaanniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, and Jaakko Lehtinen. Applying guidance in limited interval improves sample and distribution quality in diffusion models. Advances in Neural Information Processing Systems, 37:122458122483, 2024. 6, 7 [33] Tianhong Li and Kaiming He. denoising generative models denoise. arXiv:2511.13720, 2025. Back to basics: Let arXiv preprint [34] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In European Conference on Computer Vision, pages 2340. Springer, 2024. 5, 7 [35] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784, 2014. 1 [36] Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, and Ying Tai. Openvid-1m: large-scale high-quality dataset for text-tovideo generation. arXiv preprint arXiv:2407.02371, 2024. 1 [37] Charlie Nash, Jacob Menick, Sander Dieleman, and Peter Battaglia. Generating images with sparse representations. arXiv preprint arXiv:2103.03841, 2021. [38] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. 1, 2, 5, 7 [39] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 2 [40] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convoluarXiv preprint tional generative adversarial networks. arXiv:1511.06434, 2015. 1 [41] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1 (2):3, 2022. 1 [42] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1, 2, 5 [43] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234241. Springer, 2015. [44] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 1 [45] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. 6 [46] Ivan Skorokhodov, Sharath Girish, Benran Hu, Willi Menapace, Yanyu Li, Rameen Abdal, Sergey Tulyakov, and Aliaksandr Siarohin. Improving the diffusability of autoencoders. arXiv preprint arXiv:2502.14831, 2025. 2 [47] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In International confernonequilibrium thermodynamics. ence on machine learning, pages 22562265. PMLR, 2015. 1 [48] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. and Stefano Ermon. arXiv preprint [49] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 1 [50] Jiayan Teng, Wendi Zheng, Ming Ding, Wenyi Hong, Jianqiao Wangni, Zhuoyi Yang, and Jie Tang. Relay diffusion: Unifying diffusion process across resolutions for image synthesis. arXiv preprint arXiv:2309.03350, 2023. 1 [51] Michael Tschannen, Andre Susano Pinto, and Alexander Kolesnikov. Jetformer: An autoregressive generative model of raw images and text. arXiv preprint arXiv:2411.19722, 2024. 5 [52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 1 [53] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, Anthony Chen, Huaxia Li, Xu Tang, and Yao Hu. Instantid: Zero-shot identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024. [54] Shuai Wang, Zexian Li, Tianhui Song, Xubin Li, Tiezheng Ge, Bo Zheng, and Limin Wang. Flowdcn: Exploring dcnlike architectures for fast image generation with arbitrary resolution. In Proceedings of the 38th International Conference on Neural Information Processing Systems, pages 87959 87977, 2024. 5, 7 [55] Shuai Wang, Ziteng Gao, Chenhui Zhu, Weilin Huang, and Limin Wang. Pixnerd: Pixel neural field diffusion. arXiv preprint arXiv:2507.23268, 2025. 2, 4, 7 [56] Shuai Wang, Zhi Tian, Weilin Huang, and Limin Wang. arXiv preprint Ddt: Decoupled diffusion transformer. arXiv:2504.05741, 2025. 5, 7 [57] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Yujun Lin, Zhekai Zhang, Muyang Li, Yao Lu, and Song Han. Sana: Efficient high-resolution image synthesis with linear diffusion transformers. arXiv preprint arXiv:2410.10629, 2024. 1, 2 [70] Chen Zhao, En Ci, Yunzhe Xu, Tiehan Fan, Shanyan Guan, Yanhao Ge, Jian Yang, and Ying Tai. Ultrahr-100k: Enhancing uhr image synthesis with large-scale high-quality dataset. Advances in Neural Information Processing Systems, 2025. [71] Guangting Zheng, Qinyu Zhao, Tao Yang, Fei Xiao, Zhijie Lin, Jie Wu, Jiajun Deng, Yanyong Zhang, and Rui Zhu. Farmer: Flow autoregressive transformer over pixels. arXiv preprint arXiv:2510.23588, 2025. 5 [72] Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima Anandkumar. Fast training of diffusion models with masked transformers. arXiv preprint arXiv:2306.09305, 2023. 5, 7 [73] Dewei Zhou, You Li, Fan Ma, Xiaoting Zhang, and Yi Yang. Migc: Multi-instance generation controller for text-to-image In Proceedings of the IEEE/CVF Conference synthesis. on Computer Vision and Pattern Recognition, pages 6818 6828, 2024. 1 [74] Dewei Zhou, Ji Xie, Zongxin Yang, and Yi Yang. 3dis: Depth-driven decoupled instance synthesis for text-to-image generation. arXiv preprint arXiv:2410.12669, 2024. 1 [75] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei Efros. Unpaired image-to-image translation using cycleconsistent adversarial networks. In Proceedings of the IEEE international conference on computer vision, pages 2223 2232, 2017. 1 [58] Tao Yang, Cuiling Lan, Yan Lu, and Nanning Zheng. Diffusion model with cross attention as an inductive bias for disentanglement. Advances in Neural Information Processing Systems, 37:8246582492, 2024. 3 [59] Jingfeng Yao, Bin Yang, and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1570315712, 2025. 1, [60] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 1 [61] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022. 1 [62] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. 2 [63] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 7 [64] Xuying Zhang, Xiaoshuai Sun, Yunpeng Luo, Jiayi Ji, Yiyi Zhou, Yongjian Wu, Feiyue Huang, and Rongrong Ji. Rstnet: Captioning with adaptive attention on visual and non-visual words. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1546515474, 2021. 1 [65] Xuying Zhang, Yutong Liu, Yangguang Li, Renrui Zhang, Yufei Liu, Kai Wang, Wanli Ouyang, Zhiwei Xiong, Peng Gao, Qibin Hou, et al. Tar3d: Creating high-quality 3d assets via next-part prediction. arXiv preprint arXiv:2412.16919, 2024. [66] Xuying Zhang, Bo-Wen Yin, Yuming Chen, Zheng Lin, Yunheng Li, Qibin Hou, and Ming-Ming Cheng. Temo: Towards In Protext-driven 3d stylization for multi-object meshes. ceedings of the ieee/cvf conference on computer vision and pattern recognition, pages 1953119540, 2024. [67] Xuying Zhang, Yupeng Zhou, Kai Wang, Yikai Wang, Zhen Li, Shaohui Jiao, Daquan Zhou, Qibin Hou, and MingMing Cheng. Ar-1-to-3: Single image to consistent 3d object generation via next-view prediction. arXiv preprint arXiv:2503.12929, 2025. 1 [68] Chen Zhao, Weiling Cai, Chenyu Dong, and Chengwei Hu. Wavelet-based fourier information interaction with frequency diffusion adjustment for underwater image restoration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 82818291, 2024. 1 [69] Chen Zhao, Weiling Cai, Chengwei Hu, and Zheng Yuan. Cycle contrastive adversarial learning with structural consistency for unsupervised high-quality image deraining transformer. Neural Networks, 178:106428, 2024. 1 DiP: Taming Diffusion Models in Pixel Space"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Why Patch Detailer Head: Theoretical Perspective In this section, we try to provide simplified theoretical analysis to further elucidate why we need local detail refinement in enhancing generation quality. From general insight, we argue that DiT primarily focuses on the layout and arrangement of the dominant elements in the image, or in other words, the low-frequency signals of the global data. Consequently, it is less effective to learn local details and high-frequency signals. Through the refinement structure, we directly inject all signals from the global data into the learning process, which substantially improves the fine-grained processing of these high-frequency details. Specifically, we follow the flow matching description of the diffusion process. Given an initial data sample x0 pdata(x0) Rd as the input, Gaussian noise ϵ (0, Id), and [0, 1], let xt = (1 t)x0 + tϵ. (6) In this paper, since all inputs are partitioned into patches of equal size, we first define the patch-level input as follows. Definition A.1 (Patch-level Input). For each input x0 Rd, we define the patch-level input as and x0 = (cid:20)(cid:16) (cid:17) x(1) , , (cid:16) x(N ) 0 (cid:17)(cid:21) , = d. (cid:110) x(s) 0 (cid:111)N s=1 , where x(s) 0 Rp (cid:104) = Ip and P(s)x0 = x(s) It is natural to represent the patch-level input by series of selection matrices (cid:8)P(s)(cid:9)N s=1. For each s, P(s) Rpd 0 . The flow-based models try to minimize loss function defined as LFM = . Assuming that each patch is independent of one another, patch-level predictor (, t) tries satisfies P(s) (cid:0)P(s)(cid:1) Et,x0,ϵ to estimate the patch-level objective field ˆv(s) = where ϵ(s) = P(s)ϵ (0, Ip). For the given LFM, the optimal predictor is the conditional expectation for each patch-level noised input x(s) (xt, t) (ϵ x0)2(cid:105) = (1 t)x(s) 0 + tϵ(s), (cid:16) x(s) , (cid:17) ˆv(s), = (cid:104) ϵ(s) x(s) 0 (cid:105) (cid:12) (cid:12) x(s) (cid:12) . However, in true generation tasks, each patch is not independent of others, because, for natural images, the boundaries between adjacent patches are typically continuous and smoothly varying (e.g., there is little difference between one patch of sky and another). The correlation between patches only weakens when an abrupt transition occurs in the images elements, such as at the boundary between sky and grass. Moreover, DiTs attention-based structure allows single patch to access partial information from all other patches. Although this information may be coarse, this remains complex, coupled structure. Therefore, for DiT model, the estimate of ˆv(s) is not only based on x(s) . Thus, we define the effective information below. but also some other information from x(l) l=s (cid:110) (cid:111) Definition A.2 (Effective Information). For patch-level noised input (cid:110) x(s) (cid:111)N s= , we define EI(s) (cid:18) (cid:110) x(s) ; (cid:111)N (cid:19) s=1 to represent the effective information used for generation model to estimate the patch-level vector field ˆv(s) for any [N ]. Assuming that each patch is independent of one another, the patch-level estimate ˆv(s) = (cid:16) (cid:17) , x(s) only uses x(s) for prediction, which means EI(s) (cid:18) (cid:110) x(s) ; (cid:111)N ˆv(s), = (cid:20) ϵ(s) x(s) 0 EI(s) (cid:18) ; (cid:110) x(s) s=1 (cid:111)N s=1 (cid:12) (cid:12) (cid:12) (cid:12) (cid:110) x(s) (cid:19) = (cid:19)(cid:21) (cid:111) . Thus the optimal predictor can be more generally formulated as . For attention-based generation models, we cannot accurately obtain the effective information due to the complex coupling structure. However, based on some standard assumptions on the initial data distribution and some empirical observations, we can still give brief formulation for the effective information. Assumption A.3 (Data Distribution). For the initial data distribution, we assume that pdata (µ, Σ), where Σ = UΛU, = [u1, , ud], and Λ = diag{λ1, , λd}. Assumption A.4 (Eigenvalue Decay). There exists α > 1 such that for any [d], the eigenvalues of Σ satisfies λi ia. Assumption A.3 and A.4 characterize the data distribution as Gaussian distribution with covariance of series of fastdecay eigenvalues. The eigenvalue decay of covariance characterizes the differences in highand low-frequency signals of the image information. This is consistent with the empirical observation that DiT can effectively learn low-frequency signals but has difficulty capturing high-frequency signals. Given > 0, we can decompose the input x0 into lowand high-frequency components as x0 = µ + x0,low + x0,high, (7) = (cid:80)r i=1 λiuiu where x0,low (0, Σlow) and x0,high (0, Σhigh). Σlow satisfies Σlow = UrΛrU and λr+1 b, and Σhigh = Σ Σlow. Thus we can decompose the patch-level noised input x(s) as where λr > x(s) = (1 t)P(s)µ (cid:125) (cid:124) (cid:123)(cid:122) Mean(s) + (1 t)P(s)x0,low (cid:123)(cid:122) (cid:125) Low(s) (cid:124) + (1 t)P(s)x0,high (cid:123)(cid:122) (cid:125) High(s) (cid:124) + tP(s)ϵ (cid:124) (cid:123)(cid:122) (cid:125) Noise(s) (8) We can assume that for the DiT model, the effective information is composed of the local patch itself and the low-frequency signals of other patches as below. Assumption A.5 (EI of DiT). Given DiT as the predictor, there exists β > 0 such that for any [N ], the effective information to estimate the patch-level vector ˆv(s) satisfies EI(s) (cid:18) DiT; (cid:110) x(s) (cid:111)N (cid:19) s=1 (cid:111) (cid:110) x(s) = (cid:110) x(l) t,low (cid:111) l=s , x(l) t,low = Mean(l) + Low(l) + Noise(l) (9) (10) where for all = s. Our refinement structure directly injects all signals from the initial data x0 for prediction, which means that for DiP, the effective information satisfies EI(s) (cid:18) DiP; (cid:110) x(s) (cid:111)N (cid:19) s=1 = EI(s) (cid:18) DiT; (cid:110) x(s) (cid:111)N (cid:19) s= (cid:110) x(s) (cid:111)N s=1 (cid:110) x(s) = (cid:111)N . (11) We define ˆv(s) DiT = (cid:20) ϵ(s) x(s) 0 (cid:12) (cid:12) (cid:12) (cid:12) EI(s) (cid:18) DiT; (cid:110) x(s) (cid:111)N (cid:19)(cid:21) s= and ˆv(s) DiP = (cid:20) ϵ(s) x(s) 0 (cid:12) (cid:12) (cid:12) (cid:12) EI(s) DiP; (cid:110) x(s) (cid:111)N (cid:19)(cid:21) s=1 as the general near-optimal estimate of DiT and DiP, respectively. Then we obtain the main results below. s=1 (cid:18) Theorem A.6. Assume that Assumption A.3, A.4 and A.5 hold. Consider using DiT and DiP for the diffusion generation task as the predictor, respectively. The general near-optimal estimate ˆv(s) DiT and ˆv(s) DiP satisfy and respectively, where ˆv(s) DiT = P(s) ˆB ˆM (xt (1 t)µ) P(s)µ, ˆv(s) DiP = P(s)AM (xt (1 t)µ) P(s)µ, ˆM = (cid:20) (1 t)2Σlow + t2Id + (1 t)2 (cid:16) P(s)(cid:17) P(s)Σhigh (cid:16) P(s)(cid:17) P(s) (cid:21) , ˆB = tId (1 t)B, = Σlow + Σhigh = (cid:2)(1 t)2Σ + t2Id = tId (1 t)Σ. (cid:3)1 , (cid:16) P(s)(cid:17) P(s), (12) (13) (14) The denoising operator P(s) ˆB ˆM and P(s)AM satisfies and P(s) ˆB ˆM (cid:88) i=1 (1 t)λi (1 t)2λi + t2 viu + (cid:88) i=r+1 λi viu + I1 + I2, P(s)AM = (cid:88) i=1 (1 t)λi (1 t)2λi + t2 viu , (15) (16) respectively, where [v1, , vd] = P(s)[u1, , ud], I1 = (cid:80)d (cid:104) I2 = (cid:80)d (cid:0)P(s)(cid:1) P(s)uj (cid:80)d (cid:105) viu . (1t)λi t2 j=r+ i=r+1 i=r+1 (cid:80)r j=1 (1t)λi (1t)2λj +t (cid:104) (cid:0)P(s)(cid:1) P(s)uj (cid:105) viu and Proof. See Appendix B. Remark A.7. Theorem A.6 illustrates that our designed refinement mechanism exhibits strong adaptive correction capability for high-frequency signals within the image. Specifically, (12) and (13) align with our objective to estimate the conditional expectation of ϵ x0 through DiT and DiP. The first term of (12) and (13) represents the estimate of noise, while the second term means the estimate of original data. We focus on the first term regarded as the denoising process, with respective denoising operator (15) and (16) serve as global representation characterization, performing denoising on different frequency-domain components derived from the original image. When using only DiT for estimation, Equation (15) indicates that DiT can achieve good adaptive fit for low-frequency signals (i r, dominant signals). However, for high-frequency components in the image, relying solely on DiT may not provide sufficient representational capacity for these components. Specifically, The first term in (15) corresponds to the denoising process applied to the low-frequency signals. Although all these belong to the low-frequency regime, those with larger values of λi (i r) contain stronger proportion of the original image content relative to noise. Consequently, as λi decreases, the denoising operator adaptively selects larger t(1t)λi correction magnitude. (It can be readily demonstrated that (1t)2λi+t2 exhibits monotonic increase as λi decreases.) The remaining three terms correspond to the denoising process applied to the high-frequency components. Among them, I1 represents the consistent influence exerted by the low-frequency signal on the high-frequency ones. Since the highfrequency components are associated with λi (i + 1) that are significantly smaller than those of the low-frequency regime (Assumption A.4), this term can generally be regarded as o(1). The second term and I2particularly the second termpoint to the following fact: when 1 in the early stage of denoising, the magnitude of λi is much smaller than t, leading the model to apply only negligible corrections to the high-frequency components. This weakens the models ability to learn from this portion of the signal. Conversely, as 0 in the late stage of denoising, where the model aims to learn sensitive compensatory mechanism to capture more fine-grained details from the original image, becomes much smaller than λi, causing the models corrections to high-frequency components to lose stability. As result, these corrections may introduce inconsistencies with the previously learned representations, potentially affecting fine details of the final output. In contrast, when using DiP for estimation, Equation (16) demonstrates that DiP can provide robust adaptive correction for all signals. This is attributed to our refinement mechanism, which, at low computational cost, enhances the effective information during the denoising process. Particularly for high-frequency signals, the refinement provides powerful supplement to the information that DiT struggles to capture, which aligns with both our intuition and experimental results. B. Proof of Theorem A.6 Proof. Based on (11), we have ˆv(s) following statistics to obtain the first term (cid:2)ϵ(s) (cid:12) Expectations: DiP = (cid:20) ϵ(s) x(s) 0 (cid:3). (cid:12) xt (cid:12) (cid:12) (cid:12) (cid:12) (cid:110) x(s) (cid:111)N (cid:21) s=1 = (cid:2)ϵ(s) (cid:12) (cid:12) xt (cid:3) (cid:104) x(s) 0 (cid:105) (cid:12) (cid:12) (cid:12) xt . We first obtain the (cid:104) ϵ(s)(cid:105) = 0, [xt] = (1 t)µ, Covariances: (cid:16) Cov ϵ(s), xt (cid:17) = Cov (cid:16) P(s)ϵ, (1 t)x0 + tϵ (cid:17) = tP(s)Cov(ϵ, ϵ) = tP(s), (17) (18) and Cov(xt) = Cov ((1 t)x0 + tϵ) = (1 t)2Cov(x0) + t2Cov(ϵ) = (1 t)2Σ + t2Id. Then we use E[Y X] = EY + Cov(Y, X)Cov(X, X)1(X EX) to obtain that (cid:104) ϵ(s) (cid:12) (cid:12) (cid:12) xt (cid:105) = tP(s) (cid:2)(1 t)2Σ + t2Id (cid:3) (xt (1 t)µ) . Similarly, for the second term of ˆv(s) DiP we have"
        },
        {
            "title": "Cov",
            "content": "(cid:16) x(s) 0 , xt (cid:17) (cid:16) = Cov P(s)x0, (1 t)x0 + tϵ (cid:17) = (1 t)P(s)Cov(x0, x0) = (1 t)P(s)Σ,"
        },
        {
            "title": "Thus we have",
            "content": "(cid:104) x(s) 0 (cid:105) (cid:12) (cid:12) (cid:12) xt = P(s)µ + (1 t)P(s)Σ (cid:2)(1 t)2Σ + t2Id (cid:3)1 (xt (1 t)µ) . ˆv(s) DiP = (cid:105) (cid:104) ϵ(s) (cid:12) (cid:104) (cid:12) (cid:12) xt x(s) 0 = P(s) [tId (1 t)Σ] (cid:2)(1 t)2Σ + t2Id (cid:12) (cid:12) (cid:12) xt (cid:105) (cid:3)1 (xt (1 t)µ) P(s)µ. Letting = (cid:2)(1 t)2Σ + t2Id (cid:3)1 , [v1, , vd] = P(s)[u1, , ud], = tId (1 t)Σ, we have P(s)AM = (cid:88) j=1 vju (cid:32) (cid:88) i=1 (1 t)λi (1 t)2λi + t2 uiu (cid:33) (19) (20) (21) (22) (23) (24) Similarly, based on Assumption A.5, we have ˆv(s) = (cid:88) i=1 (1 t)λi (1 t)2λi + t2 viu . (cid:20) ϵ(s) x(s) 0 DiT = t)P(l)µ + (1 t)P(l)x0,low + tP(l)ϵ. We first use one vector to represent the condition to construct an observation ˆxt such that at patch, P(s) ˆxt = x(s) satisfies the requirement above , and at = patch P(l) ˆxt = x(l) (cid:111) (cid:110) x(s) (cid:12) (cid:12) (cid:12) (cid:12) (cid:110) (cid:111) x(l) t,low (cid:110) l=s (cid:111) x(s) (cid:21) , where x(l) t,low = (1 (cid:111) (cid:110) . We try x(l) t,low l=s t,low. The following ˆxt ˆxt = (1 t)µ + (1 t)x0,low + tϵ + (1 t) (cid:16) P(s)(cid:17) P(s)x0,high. Now we use the same technique to obtain (cid:2)ϵ(s) (cid:12) (cid:12) ˆxt (cid:3). The covariance terms satisfy Cov(ϵ(s), ˆxt) = Cov (cid:16) (cid:17) P(s)ϵ, tϵ = tP(s), and Cov(ˆxt) = Cov (cid:18) (1 t)x0,low + tϵ + (1 t) (cid:16) P(s)(cid:17) P(s)x0,high (cid:19) = (1 t)2Cov(x0,low) + t2Cov(ϵ) + (1 t)2 (cid:16) = (1 t)2Σlow + t2Id + (1 t)2 (cid:16) P(s)(cid:17) P(s)(cid:17) (cid:16) P(s)Σhigh P(s)Cov(x0,high) P(s)(cid:17) P(s). (25) (26) (cid:16) P(s)(cid:17) P(s) (27) Thus we have ϵ(s) (cid:12) (cid:104) (cid:12) (cid:12) ˆxt (cid:105) (cid:20) = tP(s) (1 t)2Σlow + t2Id + (1 t)2 (cid:16) P(s)(cid:17) P(s)Σhigh (cid:16) P(s)(cid:17) P(s) (cid:21)1 (ˆxt (1 t)µ) . (28)"
        },
        {
            "title": "For E",
            "content": "(cid:104) x(s) 0 (cid:105) (cid:12) (cid:12) (cid:12) ˆxt , we have"
        },
        {
            "title": "Cov",
            "content": "(cid:16) x(s) 0 , ˆxt (cid:17) = Cov (cid:18) P(s)x0,low + P(s)x0,high, (1 t)µ + (1 t)x0,low + tϵ + (1 t) (cid:16) (cid:16) = Cov P(s)x0,low, (1 t)x0,low (cid:18) (cid:17) + Cov P(s)x0,high, (cid:16) P(s)(cid:17) = (1 t)P(s) (cid:20) Σlow + Σhigh (cid:16) P(s)(cid:17) P(s) (cid:21) . P(s)x0,high (cid:19) P(s)x0,high P(s)(cid:17) (cid:19)"
        },
        {
            "title": "Thus we obtain",
            "content": "E (cid:104) x(s) 0 (cid:105) (cid:12) (cid:12) (cid:12) ˆxt =P(s)µ + (1 t)P(s) (cid:20) Σlow + Σhigh (cid:16) P(s)(cid:17) P(s) (cid:21) (cid:20) (1 t)2Σlow + t2Id + (1 t)2 (cid:16) P(s)(cid:17) P(s)Σhigh (cid:16) P(s)(cid:17) P(s) (cid:21) (xt (1 t)µ) . Finally we have ˆv(s) DiT = (cid:105) (cid:104) ϵ(s) (cid:12) (cid:12) (cid:12) ˆxt (cid:12) (cid:12) (cid:12) ˆxt = P(s) [tId (1 t)B] ˆM (xt (1 t)µ) P(s)µ, (cid:104) x(s) 0 (cid:105) where = Σlow + Σhigh (cid:0)P(s)(cid:1) P(s) and ˆM = (cid:104) (1 t)2Σlow + t2Id + (1 t)2 (cid:0)P(s)(cid:1) P(s)Σhigh (cid:0)P(s)(cid:1) P(s)(cid:105) . Letting ˆB = tId (1 t)B, we have P(s) ˆB ˆM = (cid:88) vju (cid:16) ˆC1 + ˆC2 (cid:17) (cid:16) ˆD + ˆE (cid:17) , where j=1 ˆC1 = (cid:88) i= (t (1 t)λi) uiu ˆC2 = (cid:88) i=r+1 λiuiu (1 t) (cid:88) i=r+1 λiuiu (cid:16) P(s)(cid:17) P(s), ˆD = (cid:88) ((1 t)2λi + t2)uiu + (cid:88) i=r+ t2uiu , i=1 ˆE = (1 t)2 (cid:88) i=r+1 (cid:16) P(s)(cid:17) λi P(s)uiu (cid:16) P(s)(cid:17) P(s). (29) (30) (31) (32) (33) We notice that ˆD is positive diagonal matrix and ˆD1 ˆE o(1) because Assumption A.4 shows that λp/λq (q/p)a o(1) for any 1 and + 1. Thus due to first-order Taylor expansion we have (cid:16) ˆD + ˆE (cid:17)1 = (cid:16) Id + ˆD1 ˆE (cid:17) (cid:16) ˆD1 Id ˆD1 ˆE (cid:17) ˆD1 = ˆD1 ˆD1 ˆE ˆD1 ˆD1. (34)"
        },
        {
            "title": "Therefore we obtain",
            "content": "P(s) ˆB ˆM (cid:88) vju (cid:16) ˆC1 + ˆC2 (cid:17) ˆD1 j=1 = (cid:88) i=1 (1 t)λi (1 t)2λi + t2 viu + (cid:88) (cid:88) i=r+ j=1 (1 t)λi (1 t)2λj + t2 (cid:88) i=r+1 (cid:20) λi viu (cid:16) P(s)(cid:17) P(s)uj (cid:21) viu (35) (cid:123)(cid:122) I1 (cid:125) (cid:88) (cid:88) i=r+1 j=r+ (1 t)λi t2 (cid:20) (cid:16) P(s)(cid:17) P(s)uj (cid:21) viu . (cid:123)(cid:122) I2 (cid:125) (cid:124) (cid:124) We finish the proof. C. More Implementation Details DiT Architecture Input dim Num. layers Hidden dim. Num. heads 2562563 26 1152 Patch Detailer Head Architecture DownSampling path UpSampling path DownSampling channel Bottleneck UpSampling channel Output Layer 168421 124816 364128256512 (512+1152)512 5122561286464 643 Optimization Optimizer Learning rate Weight decay Batch size Interpolants Diffusion sampler Diffusion steps Evaluation suite AdamW 0.0001 0 256 Euler 100 ADM Table 4. Hyperparameter settings. Hypermarameters. Table 4 reports the detailed hyperparameters of DiP, including the DiT Architecture, Patch Detailer Head Architecture, Optimization, and Interpolants. Objective. DiP follows the training objectives of DDT [56]. It is trained using flow matching as the objective function and regularized using representation alignment techniques. Further improvements could be made by introducing adversarial loss [15], perceptual loss [63]. Sampler. We use Euler-Maruyama ODE sampler with 100 sampling steps by default. For DiT-only and DiP, we used the same inference hyperparameters. Classifier-Free Guidance. In our experiments, we employ Interval-based Classifier-Free Guidance [32] (Interval-CFG). Specifically, we set the guidance scale to cfg=2.9. The guidance is activated exclusively within the normalized timestep interval of [0.11, 0.97]. D. How to Preserving High-Frequency Signal: Patch or Image While the theoretical analysis in Appendix establishes the need for all-frequency raw signals to refine missing highfrequency details, we also focus on how can this information be injected in the most effective way. Specifically, we are interested in whether patch-level input is better than image-level input, or vise versa, as shown in Figure 11. Intuitively, the transformer structure in DiT has captured the long-distance dependencies, therefore we only need the specific high-frequency signals or details of the image. toy experiment in Figure 12 verifies this intuition. In Figure 12(a), through the patch-level input, the learned manifold (black) tightly adheres to the ground truth structure (orange), effectively capturing intricate branching patterns and sharp boundaries. In contrast, Figure 12(b) reveals that global processing leads to over-smoothing. The learned distribution is more dispersed and struggles to lock onto fine structural details. This suggests that we only need the refinement structure to dedicate its capacity to high-frequency sensing without being distracted by long-distance dependencies. On the contrary, with image-level input the network tends to average out features across broader spatial regime, resulting in loss of sharp details in high-frequency regions. Figure 11. Different input formats of Patch Detailer Head. Figure 12. Toy experiment. (a) Visualization of manifold fitting with Patch-level input. The model precisely captures high-frequency branches. (b) Visualization of manifold fitting with Image-level input. The model exhibits over-smoothing and fails to resolve fine details. E. More Visualization Results Figure 13. 256256 samples. Class lable = goldfish, Carassius auratus (1). CFG = 4.0. Figure 14. 256256 samples. Class lable = junco, snowbird (13). CFG = 4.0. Figure 15. 256256 samples. Class lable = chickadee (19). CFG = 4.0. Figure 16. 256256 samples. Class lable = tree frog, tree-frog (30). CFG = 4.0. Figure 17. 256256 samples. Class lable = mud turtle (35). CFG = 4.0. Figure 18. 256256 samples. Class lable = teddy, teddy bear (859). CFG = 4.0. Figure 19. 256256 samples. Class lable = cauliflower (938). CFG = 4.0. Figure 20. 256256 samples. Class lable = potpie (964). CFG = 4.0. Figure 21. 256256 samples. Class lable = bolete (997). CFG = 4.0. Figure 22. 512512 samples. Class lable = ptarmigan (81). CFG = 4.0. Figure 23. 512512 samples. Class lable=jellyfish (107). CFG=4.0. Figure 24. 512512 samples. Class lable=Maltese dog, Maltese terrier, Maltese (153). CFG=4.0. Figure 25. 512512 samples. Class lable = lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens (387). CFG = 4.0. Figure 26. 512512 samples. Class lable = barn (425). CFG = 4.0. Figure 27. 512512 samples. Class lable = beacon, lighthouse, beacon light, pharos (437). CFG = 4.0. Figure 28. 512512 samples. Class lable = beer glass (441). CFG = 4.0. Figure 29. 512512 samples. Class lable = wool, woolen, woollen (911). CFG = 4.0. Figure 30. 512512 samples. Class lable = trifle (927). CFG = 4.0."
        }
    ],
    "affiliations": [
        "Nanjing University",
        "National University of Singapore",
        "Tencent Youtu Lab"
    ]
}