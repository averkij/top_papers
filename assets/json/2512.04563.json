{
    "paper_title": "COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence",
    "authors": [
        "Zefeng Zhang",
        "Xiangzhao Hao",
        "Hengzhu Tang",
        "Zhenyu Zhang",
        "Jiawei Sheng",
        "Xiaodong Li",
        "Zhenyang Li",
        "Li Gao",
        "Daiting Shi",
        "Dawei Yin",
        "Tingwen Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visual Spatial Reasoning is crucial for enabling Multimodal Large Language Models (MLLMs) to understand object properties and spatial relationships, yet current models still struggle with 3D-aware reasoning. Existing approaches typically enhance either perception, by augmenting RGB inputs with auxiliary modalities such as depth and segmentation, or reasoning, by training on spatial VQA datasets and applying reinforcement learning, and thus treat these two aspects in isolation. In this work, we investigate whether a unified MLLM can develop an intrinsic ability to enhance spatial perception and, through adaptive interleaved reasoning, achieve stronger spatial intelligence. We propose \\textbf{COOPER}, a unified MLLM that leverages depth and segmentation as auxiliary modalities and is trained in two stages to acquire auxiliary modality generation and adaptive, interleaved reasoning capabilities. COOPER achieves an average \\textbf{6.91\\%} improvement in spatial reasoning while maintaining general performance. Moreover, even a variant trained only for auxiliary modality generation attains a \\textbf{7.92\\%} gain on distance and size estimation, suggesting that learning to generate auxiliary modalities helps internalize spatial knowledge and strengthen spatial understanding."
        },
        {
            "title": "Start",
            "content": "COOPER: Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence Zefeng Zhang1,2* Xiangzhao Hao3* Hengzhu Tang4 Xiaodong Li1,2 Zhenyu Zhang4 Li Gao4 Daiting Shi4 Dawei Yin4 Zhenyang Li4 Jiawei Sheng1,2 Tingwen Liu1,2 5 2 0 D 5 ] . [ 2 3 6 5 4 0 . 2 1 5 2 : r 1Institute of Information Engineering, Chinese Academy of Sciences 2School of Cyber Security, University of Chinese Academy of Sciences 3Institute of Automation, Chinese Academy of Sciences 4Baidu Inc. zhangzefeng@iie.ac.cn"
        },
        {
            "title": "Abstract",
            "content": "Visual Spatial Reasoning is crucial for enabling Multimodal Large Language Models (MLLMs) to understand object properties and spatial relationships, yet current models still struggle with 3D-aware reasoning. Existing approaches typically enhance either perception, by augmenting RGB inputs with auxiliary modalities such as depth and segmentation, or reasoning, by training on spatial VQA datasets and applying reinforcement learning, and thus treat these two aspects in isolation. In this work, we investigate whether unified MLLM can develop an intrinsic ability to enhance spatial perception and, through adaptive interleaved reasoning, achieve stronger spatial intelligence. We propose COOPER, unified MLLM that leverages depth and segmentation as auxiliary modalities and is trained in two stages to acquire auxiliary modality generation and adaptive, interleaved reasoning capabilities. COOPER achieves an average 6.91% improvement in spatial reasoning while maintaining general performance. Moreover, even variant trained only for auxiliary modality generation attains 7.92% gain on distance and size estimation, suggesting that learning to generate auxiliary modalities helps internalize spatial knowledge and strengthen spatial understanding. 1. Introduction Visual Spatial Reasoning [81] investigates how models perceive, understand, and reason about object properties and spatial relationships. It represents fundamental step toward enabling vision-language models to achieve humanlevel intelligence, while also serving as cornerstone for numerous downstream applications in robotics [4, 14, 49, *Equal contribution. (a) Query input: Figure 1. Comparison of three paradigms. (b) Percepvisual and the corresponding textual information. tion enhancement: augment the model with auxiliary modalities (c) COOPER: single model en- (e.g., depth, segmentation). dowed with both capabilities that adaptively schedules when to (d) Reasoning perceive and when to reason during execution. enhancement: strengthen spatial reasoning via textual chain-of- (e) Self-generated Multimodal CoT: an interleaved vithought. sionlanguage CoT generated by the unified reasoner. 95], autonomous driving [59] and AR/VR [7, 20, 47] fileds. Despite the recent advancements of Multimodal Large Language Models (MLLMs) [3, 11, 22, 25, 64], their compositional spatial reasoning capabilities remain limited and fall far behind human-level performance [32, 81, 83]. Conventional MLLMs are predominantly trained on 2D imagetext pairs and therefore tend to exhibit limited 3D 1 they often struggle to infer geometry, depth, awareness: and object boundaries from raw pixels. To alleviate this limitation and strengthen spatial understanding, existing efforts mainly follow two lines of work: First, perception enhancement methods introduce auxiliary modalities such as depth maps, semantic segmentation, and 3D point clouds to strengthen the models spatial perception [9, 42, 93]. By enriching lowand mid-level visual features through fixed perception pipelines, these approaches can indeed improve the extraction of geometric cues. However, they typically offer only marginal gains in high-level spatial reasoning and downstream decision-making. Second, reasoning enhancement typically improves spatial reasoning by strengthening the models ability to decompose and analyze complex problems through textual chain-of-thought. For example, recent works construct spatial VQA datasets with annotated reasoning trajectories [41, 52] and employ reinforcement learning to refine reasoning policies [1, 44, 48]. However, these methods perform spatial reasoning solely from raw 2D images, without explicit 3D grounding, which leads to brittle and inconsistent spatial understanding. Taken together, perception and reasoning constitute two interdependent pillars of spatial intelligence: without precise geometric perception, high-level planning is prone to error; conversely, without robust reasoning, even the most accurate geometric perception struggles to support complex tasks. This naturally leads to the question: Can single model unify perception and reasoning in cooperative way to achieve stronger spatial intelligence? Unified MLLMs [13], with their strong multimodal understanding and generation capabilities over both images and text, offer promising direction for the question. However, bridging perception and reasoning within single unified model is far from trivial: First, it is non-trivial for unified MLLM to generate non-RGB auxiliary modalities that are informative for spatial reasoning. In principle, spatial perception can be greatly facilitated by explicit auxiliary modalities such as depth maps and segmentation masks. However, unified MLLMs are optimized to produce photorealistic RGB images, and are not equipped to generate structured non-RGB auxiliary modalities. This mismatch between what perception enhancement typically relies on and what unified MLLMs currently generate means that the model cannot readily supply the explicit spatial cues that would support more robust spatial reasoning. Second, adaptive, interleaved spatial reasoning remains particularly challenging for unified MLLMs. Existing unified MLLM frameworks generally follow predefined pipelines for image or text generation and seldom allow the model to autonomously decide when to generate images versus text. Even if the model were capable of producing multiple visual modalities, the lack of flexible control mechanism for deciding when, what, and how to generate these modalities limits its ability to tightly couple perception and reasoning, thereby constraining its performance on complex spatial reasoning tasks. To address these challenges, we propose COOPER, unified MLLM that cooperatively unifies perception and reasoning to achieve stronger spatial intelligence. Specifically, It unfolds in two core stages. (1) Auxiliary modality generation. To equip the model with the ability to generate non-RGB auxiliary modalities, we aggregate opensource depth and segmentation datasets and convert depth maps and segmentation masks into RGB pseudo-images, so that they are compatible with the flow matching-based training and inference pipeline of unified MLLMs. Within this shared RGB space, the model jointly learns depth and segmentation, while designated control token during inference dynamically selects the appropriate decoder to reconstruct depth or segmentation maps for downstream spatial reasoning. (2) Adaptive reasoning. To enable the model to perform adaptive, interleaved spatial reasoning, we first undergoes supervised fine tuning on GPT-4o curated data to acquire taskand context-aware capability selection. Reinforcement learning then refines this policy with Cooperative PerceptionReasoning reward (CPR reward) to further optimize spatial reasoning behavior by balancing exploration and exploitation. We evaluate COOPER on three spatial perception and reasoning benchmarks, where it improves average spatial reasoning performance by 6.91% over the base model and significantly surpasses Perception Enhancement and Reasoning Enhancement baselines. On two general multimodal benchmarks, it also yields modest overall gain of 4.47%. Further experiments show that even variant trained only for auxiliary modality generation (without reasoning supervision) achieves 7.92% improvement on distance and size estimation, indicating that learning to generate auxiliary modalities helps internalize spatial knowledge and strengthen spatial understanding. The main contributions are summarized as follows: (1) New paradigm. We propose interleaved reasoning paradigm for spatial intelligence that cooperatively unifies perception and reasoning, enabling the model to both gen- (2) Trainerate and exploit them for spatial reasoning. ing pipeline. We design fine-grained training pipeline in which unified MLLMs first learn to generate auxiliary modalities, and are then optimized with RL using the CPR reward to acquire adaptive, interleaved reasoning abilities. (3) Empirical validation. We conduct extensive experiments on spatial reasoning and general multimodal benchmarks, showing clear gains in spatial intelligence while maintaining broad multimodal capabilities, validated by comprehensive ablations and analyses. 2 2. Preliminaries 2.1. Unified Multimodal Large Language Model This section briefly reviews the core capabilities and research focus of unified MLLMs and introduces the BAGEL framework used as our backbone; other variants are discussed in Related Works. By unified MLLM, we mean single model that jointly performs multimodal understanding and generation, with most existing work focusing on image and text. BAGEL adopts Mixture-ofTransformer-Experts (MoT) architecture with two transformer expertsone for multimodal understanding and one for multimodal generationand correspondingly uses separate understandingand generation-oriented visual encoders. The two experts operate on shared token sequence via common self-attention at every layer. For text, BAGEL follows the standard next-token prediction paradigm, while for visual tokens it employs Rectified Flow [16, 36, 39], in line with state-of-the-art visual generation practice. Visual understanding. BAGEL leverage ViT encoder to convert raw pixels into tokens. Concretely, it adopts SigLIP2-so400m/14 [61] with fixed 384-resolution as the initialization of the ViT encoder. Building upon this, BAGEL interpolate the positional embeddings and set 980 980 as the maximum input size, and further integrate NaViT [12] to enable processing images at their native aspect ratios. two-layer MLP connector is used to match the feature dimension of the ViT tokens to the hidden states of the language model, thereby allowing the understanding expert to consume visual tokens in unified token space. Visual generation. BAGEL operates in latent space using Rectified Flow formulation. Given an RGB image Rwh3 and condition (e.g., text prompt), VAE encoder produces target latent z1 Rd. source latent z0 (0, I) and time U[0, 1] are sampled, and conditional linear path (linear flow/bridge) is defined as zt = (1 t) z0 + z1, [0, 1]. (1) The ground-truth conditional velocity along this path is constant, u(zt, z0, z1) = z1 z0. The velocity network vθ() is trained via flow matching: (cid:13) vθ(zt, t, c) (z1 z0)(cid:13) (cid:13) 2 LFM = Ez0N (0,I), (z1,c), tU [0,1] . (cid:13) 2 (2) At inference time, given condition c, we draw an initial latent z0 (0, I) and solve the ODE induced by the learned velocity field dzt dt = vθ(zt, t, c), [0, 1], (3) using numerical solver (e.g., Euler/Heun) with steps to obtain ˆz1. Finally, the VAE decoder maps ˆz1 back to RGB space to produce the image ˆx Rwh3. 2.2. Group Relative Policy Optimization The process begins with an input q, for which the policy πθ samples responses = {o1, . . . , oN }, each evaluated by composite reward function to yield ri = R(q, oi). GRPO then computes group-relative advantage Ai for each response by normalizing its reward with respect to the statistics of the entire group, and updates the policy: Ai = ri mean{r1, . . . , rN } std{r1, . . . , rN } . (4) The policy is then updated by maximizing clipped surrogate objective that favors responses with higher relative advantages while preventing overly large policy updates, ensuring stable training. The full objective is: JGRPO(θ) = EoiG (cid:34) 1 (cid:88) i=1 min(cid:0)siAi, clip(si, 1 ε, 1 + ε)Ai (cid:1) β DKL(πθ πref ) (5) (cid:35) . πθ(oi q) πθold(oi q) Here, si = is the importance sampling ratio that measures the change between the new policy πθ and the old policy πθold used to generate the samples. The KullbackLeibler (KL) divergence penalty, DKL(πθ πref ), regularizes the policy update by penalizing large deviations from reference policy πref . 3. Method In this section, we present the overall training pipeline of COOPER, which consists of two stages. Section 3.1 introduces the first stage, Auxiliary Modality Generation, where we endow the model with the ability to generate auxiliary modalities by mapping them into RGB space and training them directly with the original generative loss. Section 3.2 then describes the second stage, Adaptive Interleaved Reasoning, where to equip the model with adaptive, interleaved reasoning capabilities, we adopt an SFT+RL paradigm and design Cooperative PerceptionReasoning reward (CPR reward) that balances exploration and exploitation, enabling the model to learn such reasoning behavior. 3.1. Auxiliary Modality Generation In this section, we equip BAGEL with specialist capabilities that enable it to generate auxiliary modalities to support its own spatial reasoning. We focus on two widely used signals: depth estimation (geometric) and segmentation (semantic). To fully exploit the models native capacity, we inject these capabilities without changing its architecture or training objective. We first show how to map task-specific ground truths into the RGB space to align with BAGELs flow matching-based training/inference pipeline, and then 3 Figure 2. Method details. The method consists of two stages: (a) Auxiliary Modality Generation. To equip the model with the ability to generate different types of auxiliary modalities, we convert all auxiliary-modality data into the RGB space and train the model to generate these modalities using the original image generation training pipeline. (b) Adaptive Interleaved Reasoning. Building on the model with auxiliary modality generation capability, we construct balanced dataset and first apply supervised fine-tuning (SFT) to endow the model with basic interleaved reasoning. We then further enhance its reasoning and generalization ability using the CPR reward and GRPO. present the full Capability Infusion training and inference procedures, as illustrated in Figure 2 (a). <segmentation>...</segmentation> for segmentation task prompt. the Representing auxiliary modalities in RGB space. As described in Section 2.1, the image generator operates in RGB space, while depth and segmentation labels are singlechannel maps, making them incompatible with RGB-based training and inference. (1) For segmentation, we assign distinct RGB colors to different instances, turning the integer mask into an RGB label image that can be directly used in BAGELs image generation pipeline. (2) For depth estimation, the diffusion models VAE expects both inputs and outputs to lie in [1, 1]. Accordingly, following the Marigold [27], we first replicate the ground-truth depth map to three channels Rwh3, then apply an affine transformation to map its values to [1, 1] that aligns with the VAEs numerical range: = ( x2 x98 x2 0.5) 2, (6) where x2 and x98 correspond to the 2% and 98% percentiles of individual depth maps. This normalization allows us to focus on pure affine-invariant depth estimation. During inference, we take the channel-wise mean of the decoders three-channel output to obtain the final depth prediction. Training and inference. During training, we continue to optimize the model using the flow matching loss in Equation 2. In both training and inference, the conditioning signal provided to the model consists of the input image and the task-specific prompt. We use the <depth-estimation>...</depth-estimation> and for estimation prompt, depth task the 3.2. Adaptive Interleaved Reasoning Even with multiple auxiliary modalities, existing models still follow fixed image/text pipelines and cannot adaptively select the right capability. To enable adaptive reasoning, we use SFT+RL framework: SFT on interleaved visionlanguage CoT data, followed by reinforcement learning with tailored reward. Data construction. High-quality data filtering and balancing are critical for both Supervised Fine-Tuning (SFT) and reinforcement learning (RL) [56]. Accordingly, this section details how we construct and curate the training datasets for these two stages. We first collect the SAT VQA dataset [52] for spatial reasoning and the general QA dataset TACO [46] as seed data. We then evaluate the original BAGEL model on each dataset in two sampling rounds: in each round, we draw = 8 responses per question. The first round uses only the raw input and computes the average accuracy, denoted accraw; the second round augments the raw input with depth and segmentation maps and computes the corresponding average accuracy, denoted accaux. For both rounds, the sampling temperature is set to τ = 1.0. We first discard samples with accraw {0, 1} to avoid overly hard or trivial questions that yield zero advantage during RL training, thereby weakening the learning signal. We then partition the remaining data using the accuracy gap and 4 threshold λ = 0.375 (= 3/8): gain = positive, accaux accraw > λ, negative, accraw accaux > λ, (7) boundary, otherwise, where boundary indicating no significant effect from the auxiliary visual modalities. Next, we randomly subsample the boundary-type examples to control the dataset size, and split them evenly: half for SFT and the other half for RL. To construct SFT data in an interleaved visionlanguage CoT format, we use GPT-4o as the agent. To ground the reflections in the models actual capabilities, we employ the BAGEL after thaining with Section 3.1 as callable tools for depth estimation and segmentation, supplying visual signals and provisional answers for each query. When BAGELs generated images contain misleading cues, GPT-4o produces explicit reflections in the CoT to correct them. We then retain only the examples with correct final answers as the SFT dataset. Supervised fine-tuning. After obtaining the synthetic interleaved visionlanguage CoT data, we conduct SFT with cross-entropy objective. Concretely, we supervise only the textual reasoning and answer tokens in the CoT; since the visual content is generated by the model itself, optimizing it would introduce additional noise and target drift, so we do not optimize the visual outputs. The detailed SFT training pipeline is provided in Figure 2 (b1). Reinforcement learning with CPR reward. After SFT, the model can adaptively activate generation capabilities and reason, but SFT primarily teaches pattern memorization and does not generalize well. Accordingly, we further train the warm-started model with standard GRPO (described in Euation 5) using composite Cooperative PerceptionReasoning reward (CPR reward) to enhance its adaptive reasoning and generalization capabilities. Specifically, our CPR reward consists of three components: an answer reward ra, format reward rf , and an explorationguided reward re, and the total reward is given by RCP = ra + rf + re. (1) For the answer reward, we use rule-based checker if correct ra = 1.0, otherto verify the final answer; (2) For the format reward, the interwise ra = 0.0. leaved CoT takes the form {t1, v2, . . . , tN }, where ti are textual segments and vi are visual segments. If all ti for {1, . . . , 1} match thinking-generation pattern and the final tN matches thinking-answer pattern then rf = 1.0; otherwise rf = 0.0, you can find the details of these patterns in the Appendix. (3) For the exploration-guided reward, we use offline visual-gain labels created during data construction to keep training and rollout efficient. naive scheme that always rewards visual assistance on positivegain samples and penalizes it on negative-gain samples leads to overuse or over-suppression. Instead, we adopt threshold-based scheme controlled by σ: only when the generated visual assistance exceeds σ (in strength or proportion) do we assign positive reward or penalty based on the offline visual-gain label {1, +1, 0}; boundary cases receive re = 0. The exact formulation is as follows: re(oi; g) = 0.2, if = +1 u(O) σ oi = 1, 0.2, if = 1 u(O) σ oi = 1, 0, otherwise, (cid:80)N where oi {0, 1}, {1, ...N } means whether visual assistance are included in the i-th response, and u(O) = 1 i=1 oi indicates the proportion of responses that contain visual assistance. This threshold design avoids indiscriminate visual assistance overuse or over-penalization. 4. Experiments 4.1. Experimental Setup In this section, we briefly introduce the implementation details, baselines, and evaluation settings. Implementation details. We implement COOPER on top of BAGEL [13]. (1) Auxiliary Modality Generation. For depth estimation, we use the synthetic indoor dataset Hypersim [53] and the outdoor Virtual KITTI [5], with far planes of 65m and 80m, respectively, and crop Virtual KITTI to the KITTI resolution [19]. For segmentation, we adopt ADE20K [92] with its original color palette. We jointly train depth and segmentation with 1:1 sampling ratio and learning rate of 5e-6 for one epoch. (2) Adaptive Interleaved Reasoning. After filtering (Section 3.2), we retain 17k samples and use GPT-4o to construct 7k SFT examples, training for one epoch with learning rate of 5e-6. On the remaining 10k samples, we apply GRPO with CPR reward threshold σ = 4, KL coefficient β = 0.0, learning rate 3e-6, batch size 128, and = 8 candidates for 20 steps until reward convergence. Training uses 8NVIDIA H800 (80G) GPUs with random seed 42. For evaluation, we adopt VLMEvalKit [15] and use DeepSeek-V3 [37] as an answer extractor to obtain final answers from model outputs. Baseline approaches. We evaluate ten leading models to establish strong baseline, including six MLLMs and three unified MLLMs. The MLLMs tested include open-source models InternVL3.5 (8B and 38B) [64] and Qwen3VL (8B and 32B), as well as proprietary models GPT-4o, GPT-5. And the unified MLLMs include Janus-Pro [8], Liquid [73] and our base model BAGEL [13]. 5 Model Average Spatial Benchmarks General Benchmarks SIBench Q-SpatialBench MMVP MMBench MM-Vet GPT-5 GPT-4o Qwen3VL-32B Qwen3VL-8B InternVL3.5-38B InternVL3.5-8B Janus-Pro-7B Liquid-7B BAGEL BAGEL-PE BAGEL-RE COOPER (vs BAGEL) Understanding-only Multimodal Large Language Models 58.86 49.48 55.49 50.13 53.61 50. 48.51 49.50 63.36 64.35 48.51 49.50 85.33 84.66 82.66 77.33 80.66 76.33 Unified Multimodal Large Language Models 42.73 38.63 43.57 48.25 43.98 50.07 +6. 53.46 39.60 47.52 51.48 48.51 57.42 +9.90 62.33 58.33 74.33 72.66 75.33 78.66 +4. 70.44 68.59 71.83 67.30 70.73 68.16 53.45 40.61 60.49 59.45 62.24 66.42 +5.93 84.25 82.40 86.11 84.25 87.03 83.33 60.18 41. 72.22 69.44 77.77 80.55 +8.33 75.27 76.92 71.51 60.45 83.85 81.51 48.53 24.81 64.81 55.41 65.59 65.41 +0. Table 1. Main experimental results. We evaluate COOPER on spatial reasoning/understanding benchmarks and general multimodal benchmarks. BAGEL-PE (Perception Enhancement) is variant that only learns auxiliary modality generation and, for each question, mechanically generates all auxiliary modalities before answering. BAGEL-RE (Reasoning Enhancement) is variant trained with the same RL data as COOPER but performs purely textual reasoning. Compared with the base model BAGEL, COOPER achieves an average improvement of 5.93%, and on distance and size estimation tasks it surpasses 38B open-source models and approaches proprietary models. Moreover, interleaved visionlanguage reasoning exhibits higher performance ceiling than text-only reasoning. Evaluation benchmarks. To assess effectiveness and generalization, we evaluate on three spatial perception/reasoning benchmarks and two general multimodal benchmarks: SIBench [83]: curates nearly 20 open-source benchmarks covering 23 visual spatial reasoning settings. Since our method is trained on single-image data, we report results on SIBenchs single-image subset only. QSpatialBench [33]: comprises various size and distance estimation tasks to quantitatively measure fine-grained perception and estimation. MMVP [60]: evaluates perception across nine different visual modes/patterns. MMBench v1.1 [40]: an upgraded version of MMBench that removes low-quality items and adds harder ones. MM-Vet [84]: defines six core capabilities of multimodal models and poses complex questions that require combining multiple skills. hensive spatial reasoning benchmark SIBench. (2) While enhancing spatial reasoning, COOPER also improves general multimodal capability. Relative to the base BAGEL model, COOPER yields an average improvement (3) Inof 4.47% on general multimodal benchmarks. terleaved visionlanguage reasoning offers more headroom than text-only reasoning. BAGEL-PE (Perception Enhancement) exhibits stronger spatial reasoning ability but weaker general-purpose problem-solving, whereas BAGEL-RE (Reasoning Enhancement) shows stronger general problem-solving ability but weaker spatial reasonIn contrast, COOPER with interleaved reasoning ing. achieves an average 4.59% improvement on spatial tasks over BAGEL-PE and an average 1.3% improvement on general tasks over BAGEL-RE. 4.2. Main Results 4.3. Variant Analysis Table 1 presents our main experimental results, from which (1) COOPER we can draw the following conclusions: significantly improves spatial understanding and reasoning. Compared with unified MLLMs and the base model BAGEL, COOPER achieves an average gain of 6.91% on spatial understanding and reasoning. On QSpatialBench, it even surpasses proprietary GPT models and the 38B open-source InternVL3.5-38B, and reaches performance level comparable to GPT-4o on the compreTo better understand the contribution of each training stage in COOPER, we conduct ablation studies on two spatial benchmarks and one general benchmark. From the results in Table 2, we draw the following key observations: (1) Internalizing generative capabilities significantly improves task-specific understanding. After injecting depth estimation and segmentation as internal generative skills (i.e., full generative-task training), the model achieves substantial 7.92% gain on Q-SpatialBench, which focuses on distance 6 Figure 3. Reasoning Analysis. (a) COOPER adaptively selects its reasoning mode across tasks: for RD (Relative Distance) and SQA (Situational QA), it more often generates auxiliary multimodal signals, while for GR (Geometric Reasoning) it relies more on purely textual reasoning. (b) and (c) show how COOPER chooses to generate depth maps or highlight target objects in segmentation maps according to the task, thereby assisting its own reasoning. Additional reasoning and failure cases are provided in the supplementary materials. Variant SIBench Q-SpatialBench MMBench 4.4. Further Analysis BAGEL + Stage1 + SFT + RL + re 43.57 43.82 48.06 49.27 50.07 47.52 55.44 54.35 55.38 57.42 72.22 76.85 78.48 78.70 80. Table 2. Variant analysis. Our analysis of COOPER variants shows that internalizing image generation further improves understanding-oriented performance, and that the visual-gain reward helps the model better select and allocate its capabilities. and size estimation, reaching performance close to the final SFT+RL model. It also yields about 4.63% improve- (2) For relatively weak ment on general benchmarks. base model, SFT plays crucial role. The results show that SFT alone brings an average improvement of 5.86%, and subsequent RL adds further 6.68% gain. This suggests that, for base models like BAGEL with limited initial capability, SFT already provides substantial improvements, while RL primarily serves as refinement step on top of SFT. (3) The exploration-guided reward re further enhances capability selection and scheduling. Compared with RL that uses only format and outcome rewards, adding the visual-gain reward consistently improves performance across three benchmarks, indicating that it helps the model decide when and how strongly to invoke visual assistance. Reasoning analysis. To better understand COOPERs reasoning patterns, we perform quantitative and qualitative analyses in Figure 3. Overall, COOPER adaptively selects auxiliary modalities according to the task: (1) From the quantitative analysis in Figure 3 (a), we observe that COOPER prefers different auxiliary modalities for different task types. For RD (Relative Distance) tasks, the model tends to generate depth maps; for SQA (situational QA) tasks, it more often highlights target objects in segmentation maps to assist reasoning. In contrast, for GR (Geometric Reasoning) tasks the model mainly relies on pure textual reasoning, since additional auxiliary modalities provide almost no benefit in this setting. (2) The qualitative analysis in Figure 3 (b) and (c) further illustrates reasoning examples for RD and SQA tasks. COOPER first analyzes the task, then selects an appropriate auxiliary modality, and finally combines information from the original image and the generated auxiliary modality to produce the final answer. More reasoning examples and failure cases can be found in the supplementary materials. Auxiliary modality analysis. We provide qualitative and quantitative evidence that COOPERs auxiliary modality generation is comparable to specialized models. (1) Segmentation: Because segmentation labels are mapped to RGB values, standard segmentation metrics are hard to ap7 the background. (2) Depth estimation: Here we compare with Marigold, as NYUv2 ground-truth depth maps are hard for humans to distinguish. On the out-of-domain NYUv2 dataset, COOPER produces sharper depth maps with clearer boundaries than the specialized model Marigold (Figure 5). Quantitatively  (Table 3)  , COOPER attains AbsRel and δ1 performance on par with Marigold. More examples are provided in the supplementary materials. 5. Related Work 5.1. Multimodal Chain-of-Thought Chain-of-Thought (CoT) [69] improves complex problem solving by decomposing tasks into textual reasoning steps, but in multimodal settings text alone is insufficienthumans naturally rely on visual aids such as sketches and auxiliary lines. Multimodal CoT [35, 57, 67] follows this idea by integrating visual information into the reasoning process. Early work depends on external tools or expert models [24, 56, 75, 87, 91, 94], resulting in template-like and inflexible visual reasoning. More recent approaches use unified MLLMs imagetext understanding and generation to elicit intrinsic multimodal CoT [21, 29, 30, 54, 82], showing benefits in math and perception tasks via visual cues. However, these methods mainly modify natural images and rarely treat outputs of visual understanding tasks (e.g., depth, segmentation) as intermediate reasoning states, even though such signals are crucial for visual spatial reasoning. In this paper, we equip models with these visual understanding abilities and enable them to invoke them dynamically during inference, yielding more powerful multimodal CoT for spatial reasoning. 5.2. Visual Spatial Reasoning Visual Spatial Reasoning [81] requires models not only to understand semantics and localize targets, but also to reason about spatial relationships and infer 3D structure from 2D images. Existing work mainly follows two directions: (1) Perception enhancement [9, 17, 18, 42, 45, 68, 72, 90, 93], which introduces auxiliary modalities (e.g., depth, segmentation) to improve 3D perception from 2D inputs; (2) Reasoning enhancement [1, 6, 31, 34, 41, 44, 48, 52, 63], which borrows techniques from text-based reasoning to strengthen spatial reasoning. Although perception and reasoning are tightly coupled, prior work typically improves them in isolation. In this paper, we leverage unified MLLMs strong imagetext understanding and generation capabilities to integrate auxiliary modalities into multimodal chain-ofthought, allowing the model to decide during inference when to invoke perception or reasoning enhancement for more comprehensive visual spatial reasoning 1. 1More related works, future directions, and limitations are provided in the supplementary materials. Figure 4. Segmentation Cases. Qualitative comparison between the COOPER and the ground-truth segmentation maps. Figure 5. Depth estimation cases. Qualitative comparison between COOPERs depth maps and the Marigold depth maps. Models DPT [51] Marigold [27] COOPER NYUv2 [55] AbsRel 9.8 5.5 0.5 δ1 90.3 96.4 93.2 Table 3. Quantitative analysis of depth estimation. COOPERs performance on out-of-domain depth benchmarks is comparable to that of dedicated depth estimation models. ply. We only provide the qualitative comparison of segmentation result (Figure 4). COOPER often yields finer boundaries and more distinguishable colors than the ground truthfor example, capturing the precise shape of the gun barrel rather than coarse triangular region, and assigning desk regions colors that stand out more clearly from 8 6. Conclusion In this work, we aim to build stronger visual spatial reasoning model by dynamically combining perception enhancement and reasoning enhancement within an intrinsic multimodal chain-of-thought, and we introduce COOPER to this end. Concretely, starting from unified MLLM, we first inject the necessary perception capabilities by enabling the model to generate depth maps and segmentation maps. We then carefully curate datasets for both SFT and RL: supervised fine-tuning is used to endow the model with basic dynamic activation behavior, and reinforcement learning with CPR reward is further applied to strengthen its reasoning ability and generalization."
        },
        {
            "title": "References",
            "content": "[1] Inclusion AI, Fudong Wang, Jiajia Liu, Jingdong Chen, Jun Zhou, Kaixiang Ji, Lixiang Ru, Qingpei Guo, Ruobing Zheng, Tianqi Li, et al. M2-reasoning: Empowering mllms with unified general and spatial reasoning. arXiv preprint arXiv:2507.08306, 2025. 2, 8 [2] Michael Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. arXiv preprint arXiv:2209.15571, 2022. 1 [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 1 [4] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022. 1 [5] Yohann Cabon, Naila Murray, and Martin Humenberger. Virtual kitti 2. arXiv preprint arXiv:2001.10773, 2020. 5 [6] Wenxiao Cai, Iaroslav Ponomarenko, Jianhao Yuan, Xiaoqi Li, Wankou Yang, Hao Dong, and Bo Zhao. Spatialbot: Precise spatial understanding with vision language models. In 2025 IEEE International Conference on Robotics and Automation (ICRA), pages 94909498. IEEE, 2025. 8 [7] Keshigeyan Chandrasegaran, Agrim Gupta, Lea Hadzic, Taran Kota, Jimming He, Cristobal Eyzaguirre, Zane Durante, Manling Li, Jiajun Wu, and Li Fei-Fei. Hourvideo: 1-hour video-language understanding. Advances in Neural Information Processing Systems, 37:5316853197, 2024. 1 [8] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Januspro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. 5, [9] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision-language models. Advances in Neural Information Processing Systems, 37:135062135093, 2024. 2, 8 [10] Ethan Chern, Jiadi Su, Yan Ma, and Pengfei Liu. Anole: An open, autoregressive, native large multimodal models for interleaved image-text generation. arXiv preprint arXiv:2407.06135, 2024. 1 [11] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 1 [12] Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Patch npack: Navit, vision transformer for any aspect ratio and resolution. Advances in Neural Information Processing Systems, 36:22522274, 2023. 3 [13] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. 2, 5, 1 [14] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, et al. Palm-e: An embodied multimodal language model. 2023. 1 [15] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for In Proceedings evaluating large multi-modality models. of the 32nd ACM international conference on multimedia, pages 1119811201, 2024. [16] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 3 [17] Zhiwen Fan, Jian Zhang, Renjie Li, Junge Zhang, Runjin Chen, Hezhen Hu, Kevin Wang, Huaizhi Qu, Dilin Wang, Zhicheng Yan, et al. Vlm-3r: Vision-language models augmented with instruction-aligned 3d reconstruction. arXiv preprint arXiv:2505.20279, 2025. 8 [18] Qi Feng. Towards visuospatial cognition via hierarchical fusion of visual experts. arXiv preprint arXiv:2505.12363, 2025. 8 [19] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In 2012 IEEE conference on computer vision and pattern recognition, pages 33543361. IEEE, 2012. 5 [20] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1899519012, 2022. 1 [21] Jiawei Gu, Yunzhuo Hao, Huichen Will Wang, Linjie Li, Michael Qizhe Shieh, Yejin Choi, Ranjay Krishna, and Yu Cheng. Thinkmorph: Emergent properties in multimodal 9 interleaved chain-of-thought reasoning. arXiv:2510.27492, 2025. 8, 2 arXiv preprint [22] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. 1 [23] Xiaoxuan He, Siming Fu, Yuke Zhao, Wanli Li, Jian Yang, Dacheng Yin, Fengyun Rao, and Bo Zhang. Tempflow-grpo: When timing matters for grpo in flow models. arXiv preprint arXiv:2508.04324, 2025. 1 [24] Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah Smith, and Ranjay Krishna. Visual sketchpad: Sketching as visual chain of thought for multimodal language models. Advances in Neural Information Processing Systems, 37:139348139379, 2024. 8 [25] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [26] Yuanfeng Ji, Zhe Chen, Enze Xie, Lanqing Hong, Xihui Liu, Zhaoqiang Liu, Tong Lu, Zhenguo Li, and Ping Luo. Ddp: Diffusion model for dense visual prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2174121752, 2023. 1 [27] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 94929502, 2024. 4, 8, 1 [28] Duong Le, Tuan Pham, Sangho Lee, Christopher Clark, Aniruddha Kembhavi, Stephan Mandt, Ranjay Krishna, and Jiasen Lu. One diffusion to generate them all. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 26712682, 2025. 1 [29] Ang Li, Charles Wang, Deqing Fu, Kaiyu Yue, Zikui Cai, Wang Bill Zhu, Ollie Liu, Peng Guo, Willie Neiswanger, Furong Huang, et al. Zebra-cot: dataset for interleaved vision language reasoning. arXiv preprint arXiv:2507.16746, 2025. 8 [30] Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vulic, and Furu Wei. Imagine while reasoning in space: Multimodal visualization-ofthought. arXiv preprint arXiv:2501.07542, 2025. 8, 2 [31] Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, and Limin Wang. Videochat-r1: Enhancing spatio-temporal arXiv preprint perception via reinforcement fine-tuning. arXiv:2504.06958, 2025. 8 [32] Yun Li, Yiming Zhang, Tao Lin, XiangRui Liu, Wenxiao Cai, Zheng Liu, and Bo Zhao. Sti-bench: Are mllms ready arXiv for precise spatial-temporal world understanding? preprint arXiv:2503.23765, 2025. [33] Yuan-Hong Liao, Rafid Mahmood, Sanja Fidler, and David Acuna. Reasoning paths with reference objects elicit quantitative spatial reasoning in large vision-language models. arXiv preprint arXiv:2409.09788, 2024. 6 [34] Zhenyi Liao, Qingsong Xie, Yanhao Zhang, Zijian Kong, Improved arXiv Haonan Lu, Zhenyu Yang, and Zhijie Deng. visual-spatial reasoning via r1-zero-like training. preprint arXiv:2504.00883, 2025. 8 [35] Zhiyu Lin, Yifei Gao, Xian Zhao, Yunfan Yang, and Jitao Sang. Mind with eyes: from language reasoning to multimodal reasoning. arXiv preprint arXiv:2503.18071, 2025. 8 [36] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 3, 1 [37] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [38] Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470, 2025. 1 [39] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 3, 1 [40] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216233. Springer, 2024. 6 [41] Yuecheng Liu, Dafeng Chi, Shiguang Wu, Zhanguang Zhang, Yaochen Hu, Lingfeng Zhang, Yingxue Zhang, Shuang Wu, Tongtong Cao, Guowei Huang, et al. Spatialcot: Advancing spatial reasoning through coordinate alignment arXiv and chain-of-thought for embodied task planning. preprint arXiv:2501.10074, 2025. 2, 8 [42] Yang Liu, Ming Ma, Xiaomin Yu, Pengxiang Ding, Han Zhao, Mingyang Sun, Siteng Huang, and Donglin Wang. Ssr: Enhancing depth perception in vision-language models via rationale-guided spatial reasoning. arXiv preprint arXiv:2505.12448, 2025. 2, 8 [43] Yanzuo Lu, Xin Xia, Manlin Zhang, Huafeng Kuang, Jianbin Zheng, Yuxi Ren, and Xuefeng Xiao. Hyper-bagel: unified acceleration framework for multimodal understanding and generation. arXiv preprint arXiv:2509.18824, 2025. [44] Wufei Ma, Yu-Cheng Chou, Qihao Liu, Xingrui Wang, Celso de Melo, Jianwen Xie, and Alan Yuille. Spatialreasoner: Towards explicit and generalizable 3d spatial reasoning. arXiv preprint arXiv:2504.20024, 2025. 2, 8 [45] Wufei Ma, Luoxin Ye, Celso de Melo, Alan Yuille, and Jieneng Chen. Spatialllm: compound 3d-informed design towards spatially-intelligent large multimodal models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1724917260, 2025. 8 [46] Zixian Ma, Jianguo Zhang, Zhiwei Liu, Jieyu Zhang, Juntao Tan, Manli Shu, Juan Carlos Niebles, Shelby Heinecke, Huan Wang, Caiming Xiong, et al. Taco: Learning multimodal action models with synthetic chains-of-thought-andaction. arXiv preprint arXiv:2412.05479, 2024. 4 10 [47] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very longform video language understanding. Advances in Neural Information Processing Systems, 36:4621246244, 2023. 1 [48] Kun Ouyang, Yuanxin Liu, Haoning Wu, Yi Liu, Hao Zhou, Jie Zhou, Fandong Meng, and Xu Sun. Spacer: Reinforcing mllms in video spatial reasoning. arXiv preprint arXiv:2504.01805, 2025. 2, 8 [49] Abby ONeill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, et al. Open x-embodiment: Robotic learning datasets and rt-x models: In 2024 IEEE InterOpen x-embodiment collaboration 0. national Conference on Robotics and Automation (ICRA), pages 68926903. IEEE, 2024. [50] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 1 [51] Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. ViIn Proceedings of sion transformers for dense prediction. the IEEE/CVF international conference on computer vision, pages 1217912188, 2021. 8 [52] Arijit Ray, Jiafei Duan, Ellis Brown, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi, Bryan Plummer, Ranjay Krishna, et al. Sat: Dynamic spatial aptitude training for multimodal language models. arXiv preprint arXiv:2412.07755, 2024. 2, 4, 8 [53] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua Susskind. Hypersim: photorealistic synIn thetic dataset for holistic indoor scene understanding. Proceedings of the IEEE/CVF international conference on computer vision, pages 1091210922, 2021. 5 [54] Weikang Shi, Aldrich Yu, Rongyao Fang, Houxing Ren, Ke Wang, Aojun Zhou, Changyao Tian, Xinyu Fu, Yuxuan Hu, Zimu Lu, et al. Mathcanvas: Intrinsic visual chainof-thought for multimodal mathematical reasoning. arXiv preprint arXiv:2510.14958, 2025. 8 [55] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Indoor segmentation and support inference from Fergus. rgbd images. In European conference on computer vision, pages 746760. Springer, 2012. [56] Alex Su, Haozhe Wang, Weiming Ren, Fangzhen Lin, and Wenhu Chen. Pixel reasoner: Incentivizing pixel-space reasoning with curiosity-driven reinforcement learning. arXiv preprint arXiv:2505.15966, 2025. 4, 8 [57] Zhaochen Su, Peng Xia, Hangyu Guo, Zhenhua Liu, Yan Ma, Xiaoye Qu, Jiaqi Liu, Yanshu Li, Kaide Zeng, Zhengyuan Yang, et al. Thinking with images for multimodal reasoning: Foundations, methods, and future frontiers. arXiv preprint arXiv:2506.23918, 2025. 8 [58] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. 1 Hang Zhao. Drivevlm: The convergence of autonomous driving and large vision-language models. arXiv preprint arXiv:2402.12289, 2024. 1 [60] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95689578, 2024. 6 [61] Michael Tschannen, Alexey Gritsenko, Xiao Wang, MuhamIbrahim Alabdulmohsin, Nikhil mad Ferjad Naeem, Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. [62] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. 1 [63] Peiyao Wang and Haibin Ling. Svqa-r1: Reinforcing spatial reasoning in mllms via view-consistent reward optimization. arXiv preprint arXiv:2506.01371, 2025. 8 [64] Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Internvl3. 5: Advancing open-source Ye, Jie Shao, et al. multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025. 1, 5 [65] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 1 [66] Yibin Wang, Zhimin Li, Yuhang Zang, Yujie Zhou, Jiazi Bu, Chunyu Wang, Qinglin Lu, Cheng Jin, and Jiaqi Wang. Pref-grpo: Pairwise preference reward-based grpo for staarXiv preprint ble text-to-image reinforcement learning. arXiv:2508.20751, 2025. 1 [67] Yaoting Wang, Shengqiong Wu, Yuecheng Zhang, Shuicheng Yan, Ziwei Liu, Jiebo Luo, and Hao Fei. Multimodal chain-of-thought reasoning: comprehensive survey. arXiv preprint arXiv:2503.12605, 2025. [68] Zehan Wang, Sashuai Zhou, Shaoxuan He, Haifeng Huang, Lihe Yang, Ziang Zhang, Xize Cheng, Shengpeng Ji, Tao Jin, Hengshuang Zhao, et al. Spatialclip: Learning 3d-aware image representations from spatially discriminative language. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2965629666, 2025. 8 [69] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 8 [70] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1296612977, 2025. 1 [59] Xiaoyu Tian, Junru Gu, Bailin Li, Yicheng Liu, Yang Wang, Zhiyong Zhao, Kun Zhan, Peng Jia, Xianpeng Lang, and [71] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. 1 [72] Diankun Wu, Fangfu Liu, Yi-Hsin Hung, and Yueqi Duan. Spatial-mllm: Boosting mllm capabilities in visual-based spatial intelligence. arXiv preprint arXiv:2505.23747, 2025. 8 [73] Junfeng Wu, Yi Jiang, Chuofan Ma, Yuliang Liu, Hengshuang Zhao, Zehuan Yuan, Song Bai, and Xiang Bai. Liquid: Language models are scalable and unified multi-modal generators. arXiv preprint arXiv:2412.04332, 2024. 5, 1 [74] Jie Wu, Yu Gao, Zilyu Ye, Ming Li, Liang Li, Hanzhong Guo, Jie Liu, Zeyue Xue, Xiaoxia Hou, Wei Liu, et al. Rewarddance: Reward scaling in visual generation. arXiv preprint arXiv:2509.08826, 2025. 1 [75] Junfei Wu, Jian Guan, Kaituo Feng, Qiang Liu, Shu Wu, Liang Wang, Wei Wu, and Tieniu Tan. Reinforcing spatial reasoning in vision-language models with interwoven thinking and visual drawing. arXiv preprint arXiv:2506.09965, 2025. 8, 2 [76] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024. 1 [77] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generaIn Proceedings of the Computer Vision and Pattern tion. Recognition Conference, pages 1329413304, 2025. 1 [78] Yicheng Xiao, Lin Song, Yukang Chen, Yingmin Luo, Yuxin Chen, Yukang Gan, Wei Huang, Xiu Li, Xiaojuan Qi, and Ying Shan. Mindomni: Unleashing reasoning generation in vision language models with rgpo. arXiv preprint arXiv:2505.13031, 2025. [79] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. 1 [80] Yifeng Xu, Zhenliang He, Meina Kan, Shiguang Shan, Jodi: Unification of visual generation arXiv preprint and Xilin Chen. and understanding via joint modeling. arXiv:2505.19084, 2025. 1 [81] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1063210643, 2025. 1, 8 [82] Zeyuan Yang, Xueyang Yu, Delin Chen, Maohao Shen, and Chuang Gan. Machine mental imagery: Empower multimodal reasoning with latent visual tokens. arXiv preprint arXiv:2506.17218, 2025. 8, 2 [83] Songsong Yu, Yuxin Chen, Hao Ju, Lianjie Jia, Fuxi Zhang, Shaofei Huang, Yuhan Wu, Rundi Cui, Binghao Ran, Zaibin Zhang, et al. How far are vlms from visual spatial intelligence? benchmark-driven perspective. arXiv preprint arXiv:2509.18905, 2025. 1, 6 [84] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. 6 [85] Shihao Yuan, Yahui Liu, Yang Yue, Jingyuan Zhang, Wangmeng Zuo, Qi Wang, Fuzheng Zhang, and Guorui Zhou. Argrpo: Training autoregressive image generation models via reinforcement learning. arXiv preprint arXiv:2508.06924, 2025. 1 [86] Xinjie Zhang, Jintao Guo, Shanshan Zhao, Minghao Fu, Lunhao Duan, Jiakui Hu, Yong Xien Chng, Guo-Hua Wang, Qing-Guo Chen, Zhao Xu, et al. Unified multimodal understanding and generation models: Advances, challenges, and opportunities. arXiv preprint arXiv:2505.02567, 2025. 1 [87] Yi-Fan Zhang, Xingyu Lu, Shukang Yin, Chaoyou Fu, Wei Chen, Xiao Hu, Bin Wen, Kaiyu Jiang, Changyi Liu, Tianke Zhang, et al. Thyme: Think beyond images. arXiv preprint arXiv:2508.11630, 2025. 8 [88] Canyu Zhao, Yanlong Sun, Mingyu Liu, Huanyi Zheng, Muzhi Zhu, Zhiyue Zhao, Hao Chen, Tong He, and Chunhua Shen. Diception: generalist diffusion model for visual perceptual tasks. arXiv preprint arXiv:2502.17157, 2025. 1 [89] Wenliang Zhao, Yongming Rao, Zuyan Liu, Benlin Liu, Jie Zhou, and Jiwen Lu. Unleashing text-to-image diffuIn Proceedings of the sion models for visual perception. IEEE/CVF International Conference on Computer Vision, pages 57295739, 2023. 1 [90] Duo Zheng, Shijia Huang, Yanyang Li, and Liwei Wang. Learning from videos for 3d world: Enhancing mllms with 3d vision geometry priors. arXiv preprint arXiv:2505.24625, 2025. 8 [91] Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing thinking with images via reinforcement learning. arXiv preprint arXiv:2505.14362, 2025. 8 [92] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ade20k dataset. International Journal of Computer Vision, 127(3):302321, 2019. 5 [93] Enshen Zhou, Jingkun An, Cheng Chi, Yi Han, Shanyu Rong, Chi Zhang, Pengwei Wang, Zhongyuan Wang, Tiejun Huang, Lu Sheng, et al. Roborefer: Towards spatial referring with reasoning in vision-language models for robotics. arXiv preprint arXiv:2506.04308, 2025. 2, [94] Zetong Zhou, Dongping Chen, Zixian Ma, Zhihan Hu, Mingyang Fu, Sinan Wang, Yao Wan, Zhou Zhao, and Ranjay Krishna. Reinforced visual perception with tools. arXiv preprint arXiv:2509.01656, 2025. 8 [95] Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In Conference on Robot Learning, pages 21652183. PMLR, 2023. 1 COOPER: Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence"
        },
        {
            "title": "Supplementary Material",
            "content": "7. Additional Related Works 7.1. Unified Multimodal Large Language Models Unified MLLMs [86] aim to build single architecture that can understand and generate data across multiple modalities. They process diverse inputs (e.g., text, images, video, audio) and produce outputs in one or more modalities within unified framework. Such architectures typically consist of three core components: modality-specific encoders that map inputs into shared representation space, multimodal fusion backbone for cross-modal reasoning, and modalityspecific decoders for tasks like text or image generation. Early work focused on fully unified autoregressive models that discretize visual information into tokens via vector quantization [62] and train the model with unified nexttoken prediction objective [10, 58, 65, 73, 76, 79]. Subsequent studies, however, observed that such visual tokenization tends to lose substantial fine-grained visual details. To address this, starting from Janus [70], visual encoding has been decoupled into two branches: one dedicated to visual understanding and the other to visual generation. The former uses semantic encoders such as CLIP-ViT to capture visual semantics, while the latter employs Diffusion VAEs to encode and decode generative visual information [8, 13, 77, 78]. Among these, BAGEL is pretrained on large-scale imagetext interleaved data and exhibits strong cross-modal understanding and generation capabilities, so we adopt BAGEL as the base model in this work. 7.2. Unified Diffusion Models Diffusion models [2, 36, 39, 50] have achieved remarkable success in image and video generation in recent years. The core idea is to gradually add noise to the data and train model to reverse this process, denoising step by step to produce the final output. Building on diffusion models pretrained on large-scale web data, recent works have begun to explore whether this powerful foundation can be leveraged to construct unified model that handles wide range of image-to-image tasks, such as depth estimation, image segmentation, optical flow estimation, deraining, dehazing, and deblurring. Early diffusion-based approaches, such as VPD [89] and DDP [26], typically attach task-specific decoders on top of diffusion backbone and train separate diffusion model for each task. Later, Marigold [27] demonstrated that even using the original diffusion architecture and modeling paradigm alone can already match or surpass specialized perception models on depth estimation. More recently, models [80] such as OneDiffusion [28], DICEPTION [88], and Qwen-Image [71] have showcased even stronger unification capabilities. Importantly, the outputs of these visual understanding tasksespecially depth estimation and segmentationare highly beneficial for spatial reasoning. However, these explorations are largely confined to the diffusion backbone itself. In this work, we take further step by injecting these capabilities into unified MLLMs [13] and leveraging its strong image and text generation abilities to explicitly incorporate auxiliary modality generation as part of multi-modal chain-of-thought, thereby enhancing the models spatial reasoning capability. 8. Future Directions 8.1. Joint Text-Image GRPO for Multimodal CoT"
        },
        {
            "title": "Reasoning",
            "content": "During the reinforcement learning stage, we currently adopt standard GRPO objective whose reward is applied only to generated text sequences, and thus cannot directly optimize the models image generation behavior. This textcentric optimization limits, to some extent, the ability of unified MLLMs to acquire intrinsic multimodal chain-ofthought (CoT) reasoning. Recently, however, GRPO variants tailored for image generation have been proposed, enabling models to optimize their visual outputs according to task-specific rewards [23, 38, 66, 74, 85]. Building on this, promising direction is to integrate text reasoning rewards and image generation rewards within unified GRPO framework: encouraging more accurate and interpretable CoT reasoning on the textual side, while simultaneously promoting visual outputs that are better aligned with the reasoning process and task objectives. We expect that such multimodal joint optimization strategy can further enhance the multimodal CoT reasoning capabilities of unified MLLMs. 8.2. Toward Efficient Unified MLLMs for Long-"
        },
        {
            "title": "Video Spatial Reasoning",
            "content": "Constrained by the architecture and inference efficiency of the BAGEL model (e.g., its current incompatibility with vLLM and other inference acceleration frameworks), our experiments are presently limited to single-image spatial reasoning tasks. However, real-world spatial reasoning applications typically require handling long-horizon, continuous video streams, which presents substantial gap between our current experimental setup and practical deploy1 More Cases. Figure 7 illustrates how COOPER generates different types of auxiliary modalities from the same input image, while Figure 8 and Figure 9 provide additional examples of segmentation and depth estimation, respectively. Figure 10, Figure 11, Figure 12, Figure 13 showcase more complete reasoning trajectories. In particular, Figure 10 and Figure 11 show successful uses of COOPER for depth estimation: the model first analyzes the question, then generates depth map as an auxiliary modality, and finally combines the depth map with the original image to answer the question. Figure 12 demonstrates similar reasoning process when using segmentation. In contrast, Figure 13 presents failure case: although COOPER correctly selects depth map as the auxiliary modality, it hallucinates the distance between two objects during reasoning, leading to an incorrect final answer. 10. Prompt Templates In this section, we present all the prompt templates used in our experiments. Figure 14 shows the prompt used with GPT to construct reasoning chains, while Figure 15 illustrates the prompt used by COOPER for interleaved multimodal reasoning. Figure 16 and Figure 17 respectively present the prompt templates used in the reasoningenhancement and perception-enhancement experiments. ment scenarios. Although several contemporaneous efforts aim to improve BAGELs inference efficiency, for example by adapting it to vLLM 2 or adopting more efficient architectures [43], these approaches are either still under development or exhibit noticeable performance drop compared with the original BAGEL model, leaving them far from being ready for real-world use. At the same time, our study shows that intrinsic multimodal CoT exhibits greater potential than purely text-based reasoning models. Motivated by this, an important future direction is to design unified MLLMs that simultaneously achieve high inference efficiency and strong intrinsic multimodal CoT capabilities, enabling them to scale to long-video, spatial reasoning tasks that more closely reflect real-world application scenarios while maintaining high reasoning quality. 8.3. Enriching Auxiliary Modalities in Multimodal"
        },
        {
            "title": "CoT for Spatial Reasoning",
            "content": "In our experimental setup, in order to simplify the problem, we currently consider only two forms of auxiliary modalities: depth estimation, which provides geometric information, and segmentation, which provides semantic information. However, richer auxiliary modalities, such as 3D point cloud data and even natural images from real-world scenes, are also expected to provide beneficial support for spatial reasoning [21, 30, 75, 82]. Building on this, future experimental studies and real-world application systems can further incorporate broader range of auxiliary modalities into the intrinsic multimodal CoT framework, helping the model acquire stronger spatial reasoning capabilities. 9. Additional Experimental Details and Qualitative Examples In this section, we present more experimental details and richer set of reasoning and generation cases. Experimental details. For the generation cases, we use 50 denoising steps to generate the auxiliary modalities, whereas for the Marigold results we set the ensemble size to 10 and use 20 denoising steps to generate the depth maps. Figure 6 shows the evolution of rewards and entropy during COOPERs training. The overall reward steadily increases and begins to converge after roughly 15 training steps. Both the accuracy reward and the format reward improve consistently over time, indicating that the model keeps getting better in terms of answer correctness and output formatting. Meanwhile, the entropy of COOPERs policy first decreases and then rises, before slightly dropping and stabilizing, suggesting that the model gradually develops more stable policy while still maintaining certain level of exploration. 2https://github.com/vllm-project/vllm/issues/ 2 Figure 6. Reward curve. The reward curve of COOPER during training. 3 Figure 7. Generation cases from COOPER. The figure illustrates that COOPER generates different auxiliary modalities for the same input image. 4 Figure 8. Segmentation Cases. Qualitative comparison between the COOPER and the ground-truth segmentation maps. 5 Figure 9. Depth estimation cases. Qualitative comparison between COOPERs depth maps and the Marigold depth maps. 6 Figure 10. Reasoning cases 1. An example of depth-estimationenhanced reasoning in COOPER. 7 Figure 11. Reasoning cases 2. An example of depth-estimationenhanced reasoning in COOPER. 8 Figure 12. Reasoning cases 3. An example of segmentationenhanced reasoning in COOPER. 9 Figure 13. Reasoning cases 4. failure example of depth-estimationenhanced reasoning in COOPER. 10 Figure 14. Prompt template for reasoning chain construction. 11 Figure 15. Prompt template for interleaved multimodal reasoning in COOPER. Figure 16. Prompt template for reasoning-enhancement reasoning. Figure 17. Prompt template for perception-enhancement reasoning."
        }
    ],
    "affiliations": [
        "Baidu Inc.",
        "Institute of Automation, Chinese Academy of Sciences",
        "Institute of Information Engineering, Chinese Academy of Sciences",
        "School of Cyber Security, University of Chinese Academy of Sciences"
    ]
}