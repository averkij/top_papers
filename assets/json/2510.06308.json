{
    "paper_title": "Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding",
    "authors": [
        "Yi Xin",
        "Qi Qin",
        "Siqi Luo",
        "Kaiwen Zhu",
        "Juncheng Yan",
        "Yan Tai",
        "Jiayi Lei",
        "Yuewen Cao",
        "Keqi Wang",
        "Yibin Wang",
        "Jinbin Bai",
        "Qian Yu",
        "Dengyang Jiang",
        "Yuandong Pu",
        "Haoxing Chen",
        "Le Zhuo",
        "Junjun He",
        "Gen Luo",
        "Tianbin Li",
        "Ming Hu",
        "Jin Ye",
        "Shenglong Ye",
        "Bo Zhang",
        "Chang Xu",
        "Wenhai Wang",
        "Hongsheng Li",
        "Guangtao Zhai",
        "Tianfan Xue",
        "Bin Fu",
        "Xiaohong Liu",
        "Yu Qiao",
        "Yihao Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Lumina-DiMOO, an open-source foundational model for seamless multi-modal generation and understanding. Lumina-DiMOO sets itself apart from prior unified models by utilizing a fully discrete diffusion modeling to handle inputs and outputs across various modalities. This innovative approach allows Lumina-DiMOO to achieve higher sampling efficiency compared to previous autoregressive (AR) or hybrid AR-Diffusion paradigms and adeptly support a broad spectrum of multi-modal tasks, including text-to-image generation, image-to-image generation (e.g., image editing, subject-driven generation, and image inpainting, etc.), as well as image understanding. Lumina-DiMOO achieves state-of-the-art performance on multiple benchmarks, surpassing existing open-source unified multi-modal models. To foster further advancements in multi-modal and discrete diffusion model research, we release our code and checkpoints to the community. Project Page: https://synbol.github.io/Lumina-DiMOO."
        },
        {
            "title": "Start",
            "content": "2025-10-9 i - O An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding Yi Xin1,2,5,, Qi Qin1,4,, Siqi Luo1,3, Kaiwen Zhu1,3, Juncheng Yan1,7, Yan Tai3, Jiayi Lei1,3, Yuewen Cao1, Keqi Wang1, Yibin Wang2, Jinbin Bai1, Qian Yu1, Dengyang Jiang1, Yuandong Pu1, Haoxing Chen5, Le Zhuo6, Junjun He1, Gen Luo1, Tianbin Li1, Ming Hu1, Jin Ye1, Shenglong Ye1, Bo Zhang1, Chang Xu4, Wenhai Wang1, Hongsheng Li1,6, Guangtao Zhai1,3, Tianfan Xue6,1, Bin Fu1,, Xiaohong Liu3,2,, Yu Qiao1, and Yihao Liu1, 1Shanghai AI Laboratory, 2Shanghai Innovation Institute, 3Shanghai Jiao Tong University, 4The University of Sydney, 5Nanjing University, 6The Chinese University of Hong Kong, 7Tsinghua University Project Page: synbol.github.io/Lumina-DiMOO Code: Alpha-VLLM/Lumina-DiMOO AbstractWe introduce Lumina-DiMOO, an open-source foundational model for seamless multi-modal generation and understanding. Lumina-DiMOO sets itself apart from prior unified models by utilizing fully discrete diffusion modeling to handle inputs and outputs across various modalities. This innovative approach allows Lumina-DiMOO to achieve higher sampling efficiency compared to previous autoregressive (AR) or hybrid AR-Diffusion paradigms and adeptly support broad spectrum of multi-modal tasks, including text-to-image generation, image-to-image generation (e.g., image editing, subject-driven generation, and image inpainting, etc.), as well as image understanding. Lumina-DiMOO achieves state-of-the-art performance on multiple benchmarks, surpassing existing open-source unified multi-modal models. To foster further advancements in multi-modal and discrete diffusion model research, we release our code and checkpoints to the community. 5 2 0 2 7 ] . [ 1 8 0 3 6 0 . 0 1 5 2 : r Figure 1: Overview of Lumina-DiMOOs Versatile Multi-Modal Capabilities and Superior Performance. Equal Contribution. Corresponding to: ({fubin, liuyihao, qiaoyu}@pjlab.org.cn, xiaohongliu@sjtu.edu.cn) Lumina-DiMOO: An Omni Discrete Diffusion Model for Multi-Modal Generation and Understanding 1. Introduction Recent advancements in Large Language Models (LLMs) have markedly improved their ability to tackle multi-modal understanding tasks. Efforts such as the LLaVA series (Liu et al., 2023, 2024c,d), QwenVL series (Bai et al., 2023; Wang et al., 2024b; Bai et al., 2025), and InternVL series (Chen et al., 2024c,b; Zhu et al., 2025b; Wang et al., 2025a) demonstrated remarkable exceptional visual comprehension performance. Concurrently, progress in text-to-image generation models, including diffusion-based methods (Podell et al., 2024; Betker et al., 2023; Xie et al., 2025a; Labs, 2024; Zhuo et al., 2024; Yi et al., 2024; Bai et al., 2024; Qin et al., 2025) and more recent autoregressive approaches (Wang et al., 2024c; Liu et al., 2024a; Xin et al., 2025a; Chen et al., 2025c; Xin et al., 2025b), has significantly advanced the generation of high-quality images. Building upon these foundational models, various downstream tasks has been explored, such as image editing (Yu et al., 2025a), multi-view generation (Huang et al., 2025a), and controllable generation (Zhang et al., 2023). These advancements have accelerated the convergence towards unified multi-modal generation and understanding modeling, aiming to integrate diverse capabilities into single, end-to-end architecture, thereby contributing to the pursuit of artificial general intelligence (AGI). To develop unified multi-modal models, various paradigms have been explored. As shown in Figure 2(a), the earliest models, e.g., Chameleon (Team, 2024) and Lumina-mGPT (Liu et al., 2024a), relied on purely autoregressive (AR) architecture. However, these models faced two key challenges: 1) Their next-token prediction paradigm resulted in extremely slow generation speeds, often requiring several minutes, which significantly affected user experience. 2) Their image generation quality was suboptimal. To improve quality, approaches such as MetaQueries (Pan et al., 2025) and BLIP3-o (Chen et al., 2025a) added diffusion head after the AR process to decode image tokens, enhancing quality but sacrificing the unified model concept. Conversely, Show-o (Xie et al., 2025c) aimed to increase speed by adopting an AR+Discrete Diffusion strategy, as illustrated in Figure 2(b). However, optimal solutions were not achieved due to incomplete exploration of text-based discrete diffusion. Recent advances (Nie et al., 2025; Zhu et al., 2025a) in discrete diffusion modeling for text have made unified multi-modal discrete diffusion models more feasible, as depicted in Figure 2(c). Our concurrent work, MMaDA (Yang et al., 2025), has preliminarily demonstrated the potential of comprehensive discrete diffusion architecture for unifying text-to-image generation and image understanding. Nevertheless, its performance remains limited, and it lacks full support for downstream generation tasks. In this paper, we introduce Lumina-DiMOO, an open-source and unified diffusion large language model, which possesses versatile multi-modal capabilities. These capabilities encompass text-toimage generation, supporting both arbitrary and high-resolution, and range of image-to-image tasks, including image editing, style transfer, subject-driven generation, controllable generation, multi-view generation, and dense prediction, alongside advanced image understanding, as shown in Figure 1. Lumina-DiMOOs unique discrete diffusion architecture substantially enhances inference speed relative to previous unified AR or hybrid AR-Diffusion models. For example, it achieves 32x speed improvement in text-to-image generation compared to the representative AR modelLumina-mGPT 2.0 (Xin et al., 2025a). Furthermore, during inference, we note that tokens with high maximal logit values often share similar representations with previous steps. To capitalize on this, we introduce training-free Max Logit-based Cache (ML-Cache) method for Lumina-DiMOO, resulting in an additional 2x boost in sampling speed. Beyond its speed advantages, the discrete diffusion architecture enables Lumina-DiMOO to execute zero-shot inpainting. This capability can be extened to novel applicationInteractive Retouching, which allows users to freely refine specific areas through precise annotations, offering flexibility that is difficult for other methods to achieve. 2 Lumina-DiMOO: An Omni Discrete Diffusion Model for Multi-Modal Generation and Understanding Figure 2: Characteristics Comparison Among Existing Unified Models. The overall architecture has transitioned from the initial pure autoregressive (AR), which also involved adding Diffusion Head after AR, to combination of AR and discrete diffusion, and ultimately to the current model using pure discrete diffusion. We evaluate Lumina-DiMOOs capabilities across various multi-modal generation and understanding benchmarks, where it surpasses the leading open-source unified models and sets new standards in this field. Notably, Lumina-DiMOO has achieved the first place among open-source multi-modal models on the newly released UniGenBench (Wang et al., 2025b) leaderboard1, which is evaluated and maintained by the Tencent Hunyuan team. Extensive qualitative comparisons further demonstrate Lumina-DiMOOs superior performance. These results position Lumina-DiMOO as strong foundation model for future research and applications in general-purpose multi-modal intelligence. 2. Related Work Diffusion Large Language Model. Recent advancements in diffusion-based large language models (dLLMs) are built upon the theory of discrete diffusion, as developed by works like (Austin et al., 2021; Sahoo et al., 2024; Lou et al., 2024; Ou et al., 2025). Among various discrete diffusion methodologies, masked diffusion has emerged as the de facto standard due to its simplicity and effectiveness (Austin et al., 2021; Lou et al., 2024). This approach introduces special [mask] state in the forward processtransforming data into [mask]and recovers data in the reverse process, similar to BERT (Devlin, 2018). dLLMs offer distinctive advantages such as bidirectional attention, iterative refinement, flexible generation order, parallel decoding, and infilling capabilities. These features contribute to their strong reasoning abilities (Ye et al., 2025a; Huang et al., 2025b; ?), high efficiency (Nie et al., 2025; Yu et al., 2025b), and enhanced inference controllability. Recent innovations in the field include LLaDA (Nie et al., 2025), which scales dLLMs to 8B parameters with performance comparable to LLaMA3 8B (Grattafiori et al., 2024), and LLaDA 1.5 (Zhu et al., 2025a), which reduces reinforcement learning variance to better align models with human preferences. Multi-modal capabilities have also been explored through models like Dimple (Yu et al., 2025b), LLaDA-V (You et al., 2025), and LaViDa (Li et al., 2025a). These models, although having not yet achieved peak performance, unveil promising alternative pathway beyond autoregressive models. Unified Generation and Understanding. Unifying multi-modal generation and understanding has been long-standing goal. One typical approach relies on using separate continuous diffusion models, where the LLM regresses image features that are subsequently decoded into images via diffusion process (Sun et al., 2024b; Pan et al., 2025; Chen et al., 2025a). While this method achieves decent visual generation, the reliance on external models compromises true modality unification and introduces bottlenecks that hinder seamless interaction across modalities (Deng et al., 2025; Wu et al., 2024b). To address this, another line of research integrates diffusion within the LLM 1Leaderboard Link: https://huggingface.co/spaces/CodeGoat24/UniGenBench_Leaderboard. 3 Lumina-DiMOO: An Omni Discrete Diffusion Model for Multi-Modal Generation and Understanding Figure 3: An Overview of Lumina-DiMOOs Discrete Diffusion Modeling. (a) Training: Lumina-DiMOO is trained on text and image tokens with mask. (b) Inference: Lumina-DiMOO predicts the masked tokens, refining its output progressively. transformer, sharing parameters for both generation and understanding. These unified models use single transformer to generate text autoregressively and images through either continuous (Zhou et al., 2025; Ma et al., 2024; Liao et al., 2025) or discrete diffusion (Xie et al., 2025c). However, there still exist heavy modality-specific designs, complicating the model and reducing the unity. In pursuit of more simplified unification, some works tokenize all modalities into discrete tokens, enabling uniform autoregressive processing. For example, Emu3 (Wang et al., 2024c) and LuminamGPT (Liu et al., 2024a) demonstrate versatility across tasks such as visual question answering and mixed-modal generation. However, these autoregressive models face inefficiencies from raster-scan generation orders and the inherently slow token-by-token decoding. Multi-modal dLLMs offer promising solution to these challenges. Their inherent flexibility in generation order and support for parallel decoding enable higher efficiency. Concurrent with our work, MMaDA (Yang et al., 2025) has preliminarily validated the feasibility of discrete diffusion on unified generation and understanding. 3. Lumina-DiMOO 3.1. Foundation Image Tokenizer The discrete image tokenizer is fundamental component in discrete diffusion modeling paradigms, crucial to the ultimate performance of visual generation and understanding tasks. Therefore, selecting tokenizer capable of high-fidelity image reconstruction is essential. Although SBER-MoVQGAN (Razzhigaev et al., 2023), validated in Lumina-mGPT 2.0 (Xin et al., 2025a), is considered the state-ofthe-art open-source tokenizer, its 88 downsampling results in excessively long token sequences for high-resolution images, posing significant computational challenges. To balance performance with efficiency, we choose the tokenizer from aMUSEd-VQ (Patil et al., 2024), which uses 1616 downsampling factor. We also explored other 1616 downsampling tokenizers, such as ChameleonVQ (Team, 2024) and Open-MAGVIT2 (Luo et al., 2024). However, Chameleon-VQ produces slightly inferior reconstructions, and although Open-MAGVIT2 performs well in reconstruction, its token format doesnt align with our modeling needs. drawback of the aMUSEd-VQ tokenizer is its lack of semantic information about the image, which poses challenges for image understanding tasks. We address this by scaling the understanding data. Lumina-DiMOO: An Omni Discrete Diffusion Model for Multi-Modal Generation and Understanding 3.2. Model Design Unified Discrete Diffusion Modeling. We adopt unified discrete diffusion framework that not only simplifies the modeling complexity but also introduces unified optimization objective to model both textual and visual modalities, as shown in Figure 3(a). Specifically, let = (𝑥1, . . . , 𝑥𝐿) denote mixed text-image sequence drawn from the joint vocabulary (text tokens, image tokens, and special tokens, details in subsequent paragraph). mask set ℳ {1, . . . , 𝐿} is sampled by mask ratio 𝑚 (0, 1], where the length of ℳ is 𝐿 𝑚. Tokens (in sequence x) at these indices (in mask set ℳ) are replaced with special [Mask] token. Therefore, the input sequence (with [Mask]) construction process for the model is as follows: 𝑥𝑖 = {[𝑀 𝑎𝑠𝑘] 𝑥𝑖 𝑖𝑓 𝑖 ℳ, 𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒. (1) Then, the model 𝑝𝜃 predicts, in parallel, the original tokens at masked positions conditioned on the unmasked context and optional condition tokens 𝑐 (e.g., text prompt): (xℳ xℳ, 𝑐) = 𝑝𝜃 (𝑥𝑖 xℳ, 𝑐). 𝑝𝜃 𝑖ℳ (2) Training minimizes the masked cross-entropy over randomly sampled mask ratios 𝑚 applied to both text and image positions: [ ℒ(𝜃) = Ex, 𝑚, ℳ log 𝑝𝜃 (𝑥𝑖 xℳ, 𝑐) ] . 𝑖ℳ (3) At inference, generation starts from fully masked tokens and proceeds for 𝑇 refinement steps via parallel prediction-sampling-remasking, as shown in Figure 3(b) (see Section 3.3.1 for details). In the realm of autoregressive (AR) multi-modal Effective Initialization from Pre-Trained dLLM. generation and understanding, successful paradigm involves initializing models with powerful pre-trained LLMs (Wu et al., 2024b; Xie et al., 2025c). These existing LLMs are ideal starting points for training as they already possess robust text semantic understanding and generation capabilities, which can greatly reduce training resource requirements. Inspired by this paradigm, Lumina-DiMOO is developed on pre-trained dLLM, seamlessly integrating multi-modal generation and understanding within discrete diffusion framework. Specifically, we utilize LLaDA-Base (Nie et al., 2025) as our base model without any structural modifications. To demonstrate the effectiveness of this paradigm, we conduct an ablation analysis in Section 7.2. Multi-Modal Tokenization. To expand the multi-modal capabilities, we make key modification to the vocabulary. The original LLaDA model operates with 126,345 text tokens. We expand this by integrating 8,192 visual tokens from the pre-trained aMUSEd-VQ codebook. Additionally, we introduce special tokens, such as <IMAGE> and </IMAGE>, to explicitly define the boundaries of visual elements within the token sequence. As result, Lumina-DiMOOs combined vocabulary now includes 126,345 LLaDA text tokens, 8,192 aMUSEd-VQ visual tokens, and set of special tokens. Detailed descriptions of these special tokens are provided in Table 1. For Lumina-DiMOO, only the newly introduced visual and special tokens require learning. Arbitrary Resolution Image Representation. For versatile multi-modal generation and understanding model, the capability to process images of arbitrary resolutions is essential. However, our foundational model, LLaDA, which uses 1D RoPE designed for text, encounters challenges when applied to inherently 2D image tokens. key issue is that images with different aspect ratios, such as 5 Lumina-DiMOO: An Omni Discrete Diffusion Model for Multi-Modal Generation and Understanding Table 1: Detailed Description of the Special Tokens. <hed> and </hed> <depth> and </depth> <canny> and </canny> <IMAGE> and </IMAGE> The beginning and ending identifiers of an image. The beginning and ending identifiers of canny detection map image. The beginning and ending identifiers of depth map image. <openpose> and </openpose> The beginning and ending identifiers of skeleton map image. The beginning and ending identifiers of an edge detection map image. The beginning and ending identifiers of system prompt, which are usually descriptions of task prompts. The beginning and ending identifiers of user prompt, which are typically correspond to the users instructions or requirements. The beginning and ending identifiers of the models response. The identifier for the end of line in an image. Identifiers for CFG (Classifier-Free Guidance) applied to image generation. <answer> and </answer> <end-of-line> <system> and </system> <user> and </user> <uncondition> 5121024 and 1024512 would be flattened into sequences of the same length, losing their distinct aspect ratios in 1D format. To overcome this, we introduce <end-of-line> token after the last image token of each row, serving as an explicit delimiter of the structure. This addition allows the original 2D shape of the image to be correctly parsed and reconstructed from the 1D sequence without requiring new positional embedding design. This modification is crucial for enabling Lumina-DiMOO to effectively handle images with arbitrary resolution. In contrast, MMaDA (Yang et al., 2025), sharing the same architecture with Lumina-DiMOO, only processes images at fixed resolution of 512512. 3.3. Inference 3.3.1. Sampling Strategies Parallel Sampling for Image Generation. For image generation, we treat the entire set of image tokens to be generated (excluding special <end-of-line> tokens) as single generation block. Following MaskGIT (Chang et al., 2022), we partition the image generation process into four stages. Generation starts from sequence in which all image tokens are masked, i.e. 𝑥𝑡=0 and proceeds decoding for 𝑇 timesteps. At each timestep 𝑡, our decoding operates as follows: (𝑥𝑡 𝑖 x𝑡 ℳ , 𝑐) R𝐿 𝑡𝐾 for all masked tokens, where 𝐿 𝑡 1. Predict. Conditioned on the user prompt 𝑐, Lumina-DiMOO predicts, in parallel, probabilities is the number of masked image tokens at 𝑝𝜃 timestep 𝑡 and 𝐾 is the size of the full vocabulary. 2. Sample. We first restrict the predicted probabilities 𝑝𝜃 R𝐿 𝑡𝐾 from the entire vocabulary to the 𝑡𝐾 (𝐾 = 8,192 denotes the size of the image vocabulary). Then, for image-token subset 𝑝𝜃 R𝐿 each masked image token, we sample its value with the highest probability within the image codebook and take the corresponding probability as the confidence in the timestep 𝑡. For image tokens that have already been decoded, we set their confidence as to prevent them from participating in the re-masking step. 3. Mask Schedule. We use cosine sampling schedule to determine the number of tokens to re-mask 6 Lumina-DiMOO: An Omni Discrete Diffusion Model for Multi-Modal Generation and Understanding at the timestep 𝑡. Specifically, 𝑘𝑡 = cos 𝜋𝑡 2𝑇 , 𝐿 𝑡 (4) where 𝑇 is the total number of timesteps, and 𝑘𝑡 is the number of tokens to re-mask at timestep 𝑡. 4. Remask. After determining the number of re-masked tokens using the masking schedule, we select the re-masked image tokens with top-𝑘 rule according to each tokens confidence obtained in the step 2 (Sample stage). After predefined 𝑇 decoding timesteps, all image tokens are predicted. In addition, we employ classifier-free guidance (CFG), commonly used strategy in the field of image generation. Block-Wise Parallel Sampling for Image Understanding. Unlike image generation, which produces image tokens, image understanding predicts text tokens. Following LLaDA (Nie et al., 2025) and MMaDA (Yang et al., 2025), we adopt semi-autoregressive strategy. Concretely, starting from fully masked text sequence, we partition the sequence into multiple blocks. Within each block we perform parallel token prediction, while across blocks we decode sequentially in order. While this design can enrich output details (e.g., MMaDA), it also makes the results highly sensitive to the sampling steps and the overall generation length. In the extreme caseexemplified by MMaDAwhere each step predicts only two tokens, the semi-autoregressive procedure effectively degenerates to standard next-token prediction. Moreover, major drawback of block-wise inference is inefficiency: the semi-autoregressive procedure always generates the full predefined length in next-block manner, even though the model often terminates its response earlier. This mismatch leads to substantial redundant computation. To mitigate this, we introduce an early stopping strategy, which halts inference immediately once the current block has been completed and an </answer> token is detected, thereby reducing unnecessary steps and improving efficiency. 3.3.2. Acceleration Sampling via Max Logit-based Cache Compared to AR or hybrid AR-Diffusion models, although Lumina-DiMOO could reduce generation steps by parallel sampling, each step is significantly more costly due to bidirectional attention. Note that we cannot just excessively force fewer steps to compensate for the cost, because this will introduce compounding decoding error (Park et al., 2025; Liu et al., 2025a) and degrade the generation quality. Therefore, it is crucial to improve the computational efficiency of each step. Autoregressive models can be losslessly accelerated through KV-Cache. Although the bidirectional attention prohibits Lumina-DiMOO from this desideratum, we can leverage the idea of caching to achieve lossy acceleration. It turns out that the representations remain stable across steps for most tokens (Ma et al., 2025; Liu et al., 2025d; Wu et al., 2025a; Liu et al., 2025c). Given this, we can safely skip the computation of these tokens and directly reuse the representations in the previous step. The key challenge Figure 4: Example of token logits statistics, illustrating that tokens with high maximal logit tend to have stable representations. 7 Lumina-DiMOO: An Omni Discrete Diffusion Model for Multi-Modal Generation and Understanding Table 2: Detailed Hyperparameter and Configuration of the Training Recipe Across Different Stages. Hyperparameters Learning Rate LR Scheduler Weight Decay Stage-I Stage-II Stage-III (Pre-Training) 2.0 104 Constant 0. (Mid-Training) 2.0 104 Constant 0.1 (Supervised Fine-Tuning) 2.0 105 Constant 0.1 Gradient Norm Clip 1.0 1.0 1. Optimizer Batch Size Training GPUs Gen. Resolution Arbitrary Resolution Under. Resolution 1,024 64A800 256512 256512 Arbitrary Resolution AdamW (𝛽1 = 0.9, 𝛽2 = 0.95) 512 64A800 1024 (512 for I2I) 512 64A800 1024 (512 for I2I) Dynamical & Native 5121024 Dynamical & Native 5121024 Stage-IV (Self-GRPO) 3.0 106 Constant 0.1 1.0 AdamW (𝛽1 = 0.9, 𝛽2 = 0.99) 48 8H20 1024 1024 then becomes accurately identifying these tokens. In experiments, we find that for token in step, if its maximal logit is high, then the logits tend to be highly similar to those in the previous step. Figure 4 shows an example, where logits of tokens with top 94% maximal logit have over 0.99 cosine similarity. In view of this, we use the maximal logit as the proxy to identify reusable tokens. Specifically, we use hyperparameter cache_ratio [0, 1) to denote the ratio of reused tokens. In step where we decide to reuse previous representations, we select tokens with top cache_ratio 100% maximal logit as the reused tokens, while the remaining tokens will be computed. We only feed the tokens to compute into the unmasking network. While computing bidirectional attention, the and representations of tokens to reuse are approximated by those used in the previous step. In sampling, the logits of tokens to reuse are also approximated by those in the previous step. Another problem is which step to reuse previous representations. We use two hyperparameters warmup_ratio and refresh_interval to decide, similar to existing works (Ma et al., 2025; Liu et al., 2025d; Wu et al., 2025a; Liu et al., 2025c). In the beginning warmup_ratio 100% steps, we compute all tokens to avoid error from inaccurate estimation due to the poor context. Moreover, we compute all tokens every refresh_interval steps to alleviate the error accumulation. These mechanisms could reduce the approximation error and allow flexible tuning of efficiency-quality trade-offs. 4. Training Pipeline The training pipeline comprises four stages, with details of each stage outlined in Table 2. Notably, the Self-GRPO stage is specifically designed for Lumina-DiMOO, capitalizing primarily on the discrete diffusion mechanism and the unified generation and understanding model. 4.1. Stage-I: Multi-Modal Pre-Training for Image-Text Alignment The multi-modal pre-training stage serves as crucial bridge to transition Lumina-DiMOO from unimodal text model to proficient multi-modal model. The core goals of this stage are to cultivate visual capability and to align text and visual representations. To achieve this, we design unified input format where text-image pairs are concatenated into single sequence formatted as: 8 Lumina-DiMOO: An Omni Discrete Diffusion Model for Multi-Modal Generation and Understanding <startoftext> {text tokens} <endoftext> <IMAGE> {image tokens} </IMAGE> Here, <startoftext> and <endoftext> are the begin-of-sequence and end-of-sequence tokens defined in the original text tokenizer. During training, we employ random masking strategy, where portions of text and image tokens are masked (red area indicates tokens that can be masked), and Lumina-DiMOO learns to predict them based on unmasked tokens. To address the challenges of learning complexity associated with long visual token sequences, we introduce progressive training schedule. The training begins with low-resolution (arbitrary resolution around 256256, 256 tokens), then advances to medium-resolution (arbitrary resolution around 512512, 1024 tokens). 4.2. Stage-II: Mid-Training for Diverse Tasks In contrast to typical unified model training, we introduce an additional mid-training stage designed to achieve two goals: first, to integrate diverse suite of image-to-image tasks into Lumina-DiMOO, and second, to enhance its comprehension of specialized visual data. The image-to-image tasks include image editing, subject-driven generation, controllable generation, style transfer (using reference image), and multi-view generation, to name few. Concurrently, the models enhanced understanding extends to complex visual formats such as tables, charts, user interfaces, mathematical equations, and geometric structures. Unlike Stage-I training, this stage focuses solely on calculating the loss for the target image in text-to-image and image-to-image tasks, or the target text in image understanding tasks. Efficient Mid-Training. The nature of image-to-image tasks, which typically process two or more images, results in substantially longer token sequences compared to single-image tasks such as textto-image generation and image understanding. This will result in low training efficiency. To address this issue, we set the image resolution for image-to-image tasks to 512. In contrast, for text-to-image tasks, higher resolution of 1024 is adopted to better capture finer details. For image understanding tasks, we implement dynamic resolution strategy: maintaining the original image resolution within 512 to 1024, downscaling images exceeding 1024 to 1024, and upscaling those below 512 to 512. 4.3. Stage-III: Supervised Fine-Tuning for Instruction Following In the supervised fine-tuning stage, the primary objective is to enhance two key aspects of LuminaDiMOO: its ability to align with user instructions and the overall quality of its multi-modal generation and understanding. To achieve these objectives, we construct large collection of high-quality <System Prompt, User Prompt, Answer> triples. During training, the system prompt and user prompt remain unchanged, while the tokens in the answer are masked and the loss is computed independently. The processing of image resolution in this stage is consistent with that in Stage II. 4.4. Stage-IV: Self-Improving via GRPO Finally, to fully leverage the unified nature of generation and understanding, we propose SelfGRPO, self-improving reinforcement learning framework that jointly optimizes text-to-image (T2I) generation and multi-modal understanding (MMU). Unlike prior work that relies solely on answerlevel MMU supervision (e.g., UniRL (Mao et al., 2025)) or ignores generation-inference alignment (e.g., UniGRPO (Yang et al., 2025)), Self-GRPO integrates structured semantic feedback and ensures trajectory-consistent training, as shown in the lower left of Figure 5. The GRPO (Guo et al., 2025) strategy requires computing the outputs of the old policy 𝜋𝜃𝑜𝑙𝑑 and then optimizing the current policy model 𝜋𝜃. Lumina-DiMOO supports high-resolution (10241024) Lumina-DiMOO: An Omni Discrete Diffusion Model for Multi-Modal Generation and Understanding Figure 5: Overview of the Proposed Self-GRPO Framework. Self-GRPO unifies text-to-image (T2I) generation and multi-modal understanding (MMU) under trajectory-consistent reinforcement learning. image generation with image sequences of length 𝐿 = 4096, while unified T2I and MMU tasks and 𝜋𝜃 while storing the corresponding further increase the sequence length. Computing both 𝜋𝜃𝑜𝑙𝑑 activations and gradients imposes substantial memory burden. Following UniRL (Mao et al., 2025), to reduce memory consumption. At each training step, given we eliminate the old policy 𝜋𝜃𝑜𝑙𝑑 prompt 𝑝, we sample 𝐺 candidate images as token sequences {𝑥(𝑔)}𝐺 (each of length 𝐿) from the , the model then answers each 𝑞𝑛 conditioned on current policy 𝑝𝜃. Given set of questions {𝑞𝑛}𝑁 the generated image 𝑥(𝑔) to obtain per-sample T2I and MMU losses, ℓ(𝑔) T2I and ℓ(𝑔) MMU. Combining these, we optimize the reward-weighted objective: 𝑤(𝑔) ( 𝐿(𝜃) = + 𝛽 KL 𝐺 (5) 𝑛=1 𝑔=1 T2I + ℓ(𝑔) ℓ(𝑔) MMU 𝑝𝜃 𝑝ref 𝜃 ( ) ) . 𝑔=1 Rewards 𝑟(𝑔) are defined as the number of correct answers across {𝑞𝑛} and are normalized with softmax temperature 𝛼: 𝑤(𝑔) = exp(𝛼 (𝑟(𝑔) 𝑟)) 𝑗=1 exp(𝛼 (𝑟(𝑗) 𝑟)) , 𝐺 𝑟 = 1 𝐺 𝐺 𝑗=1 𝑟(𝑗). (6) Reinforcement learning involves two distinct processes: output sampling and reward updating, where the latter follows the sampling trajectory to assign rewards and compute gradients accordingly. Unlike autoregressive MLLMs, Lumina-DiMOO performs multi-step forward passes and re-masking during image generation. Consequently, it is necessary to design sophisticated strategies to preserve trajectory consistency. Since the primary content can be generated in early timesteps during T2I generation (Chang et al., 2022), we propose step trajectory following strategy to improve memory efficiency. Specifically, Self-GRPO retains the complete sampling trajectory but computes gradients only from the model outputs at selected timesteps 𝒯sel. The T2I log-likelihood is defined as: ( (7) ℓ(𝑔) T2I = 1 𝒯sel 𝑡𝒯sel log 𝑝𝜃 𝑥(𝑔) 𝑡 𝑥(𝑔) ) <𝑡 , 𝑝 . To evaluate the MMU capability of the model, we compute the average log-likelihood of 𝑁 predicted answers, where each answer 𝑦(𝑔) 𝑛 is generated conditioned on the corresponding question 𝑞𝑛 and image tokens 𝑥(𝑔): ℓ(𝑔) MMU = 1 𝑁 𝑁 𝑛=1 ( log 𝑝𝜃 𝑛 𝑥(𝑔), 𝑞𝑛 𝑦(𝑔) ) . (8) 10 Lumina-DiMOO: An Omni Discrete Diffusion Model for Multi-Modal Generation and Understanding Self-GRPO therefore unifies vision and language under trajectory-consistent framework. By combining KL-regularized policy updates, memory-efficient training, and multi-modal reward supervision, it closes the training loop between generation and understanding. 5. Data Construction Stage-I: Pre-Training Data. We collect approximately 80 million high-quality text-image pairs, sourced from diverse and reliable datasets, including 30 million pairs from re-captioned public collections (i.e., LAION-400M (Schuhmann et al., 2021) for image understanding pre-training and CC12M (Changpinyo et al., 2021)) and 50 million from Lumina-Image 2.0 (Qin et al., 2025), LuminamGPT 2.0 (Xin et al., 2025a) for image generation pre-training. In this stage, we incorporate an additional 3 million images from Stage-II: Mid-Training Data. several challenging domains: MMTable (Zheng et al., 2024) and TinyChart (Zhang et al., 2024) for table and chart comprehension, AutoGeo (Huang et al., 2025c) and MAVIS (Zhang et al., 2025) for understanding math equations and geometric structures, and MultiUI (Liu et al., 2025b) for user interface parsing, which are all captioned using Qwen2.5-VL (Bai et al., 2025). For image-to-image tasks, data is sourced from several datasets, including UltraEdit (Zhao et al., 2024), OmniEdit (Wei et al., 2024), OminiControl (Tan et al., 2024), and Lumina-mGPT 2.0 (Xin et al., 2025a). Stage-III: Supervised Fine-Tuning Data. For image understanding, we construct high-quality dataset of 15 million samples, combining 2 million from MAmmoTH-VL dataset (Guo et al., 2024) and 13 million from InternVL-2.5-SFT dataset (Chen et al., 2024a). For visual generation, we utilize total of 15 million samples, aggregated from Lumina-Image 2.0 (Qin et al., 2025) (selecting only the highest quality data), Blip3o-60k (Chen et al., 2025a), ShareGPT-4o-Image (Chen et al., 2025b), and additional in-house synthetic data. For image-to-image tasks, we incorporate data for SubjectDriven Generation, Controllable Generation, Dense Prediction, and Style Transfer, each comprising 200K examples from VisualCloze (Li et al., 2025b). Additionally, there are 500K instruction-guided Image Editing samples from UniWorld (Lin et al., 2025) and 200K examples for Low-Level Vision tasks from Lumina-OmniLV (Pu et al., 2025), focusing on enhancements like super-resolution, dehazing, and denoising, etc. However, we find that Lumina-DiMOO perform poorly on low-level tasks. For Multi-View Generation, we use data consistent with Lumina-mGPT (Liu et al., 2024a). In this stage, only text prompt data is required. We utilize prompt from Stage-IV: Self-GRPO Data. the subset of GenRef (Zhuo et al., 2025), which resemble GenEvals prompt templates. From each prompt, we extract (entity, relation, value) triples using DSG (Cho et al., 2024) method. These triples are then used to craft single-choice questions for semantic alignment supervision in Self-GRPO. To generate distractor options, we maintain global candidate pools for entities, relations, quantities, and colors. For each question, distractors are selected to be semantically close to the correct answer, ensuring that the resulting QA tasks are both challenging and informative. 6. Evaluation 6.1. Performance of Text-to-Image Generation For evaluating text-to-image generation capabilities, we conduct evaluations using five publicly available benchmarksGenEval (Ghosh et al., 2024), DPG (Hu et al., 2024), UniGenBench (Wang et al., 2025b), OneIG-EN (Chang et al., 2025), and TIIF (Wei et al., 2025). These benchmarks offer comprehensive framework to measure the models proficiency in generating high-quality, semantically 11 Lumina-DiMOO: An Omni Discrete Diffusion Model for Multi-Modal Generation and Understanding Table 3: Evaluation of Text-to-Image Generation on GenEval (Ghosh et al., 2024) Benchmark. Und. and Gen. denote understanding and generation, respectively. We highlight the best and the second results. Method Architecture # Params. Single Obj. Two Obj. Counting Colors Position Attribute Overall LlamaGen (Sun et al., 2024a) PixArt-𝛼 (Chen et al., 2023) SDv2.1 (Rombach et al., 2022) Emu3-Gen (Wang et al., 2024c) SDXL (Podell et al., 2024) DALL-E 3 (Betker et al., 2023) SD3-Medium (Esser et al., 2024) FLUX.1 [Dev] (Labs, 2024) OmniGen (Xiao et al., 2024) SANA-1.5 (Xie et al., 2025b) Lumina-mGPT 2.0 (Xin et al., 2025a) AR Diffusion Diffusion AR Diffusion - Diffusion Diffusion Diffusion Diffusion AR SEED-X (Ge et al., 2024) Show-o (Xie et al., 2025c) Janus (Wu et al., 2024a) D-DiT (Li et al., 2024b) Transfusion (Zhou et al., 2025) TokenFlow-XL (Liu et al., 2024b) Chameleon (Team, 2024) Janus-Pro (Chen et al., 2025c) GPT-4o (OpenAI, 2025) BLIP3-o (Chen et al., 2025a) BAGEL (Deng et al., 2025) Uniworld-V1 (Lin et al., 2025) OmniGen2 (Wu et al., 2025b) MMaDA (Yang et al., 2025) Lumina-DiMOO (Ours) Lumina-DiMOO w/ Self-GRPO AR AR+Discrete Diff. AR Discrete Diff.+Diff. AR+Diff. AR AR AR - AR+Diff. AR+Diff. AR+Diff. AR+Diff. Discrete Diff. Discrete Diff. Discrete Diff. 0.8B 0.6B 0.9B 8B 2.6B - 2B 12B 3.8B 4.8B 7B 17B 1.3B 1.3B 2B 7B 14B 7B 7B - 8B 14B 20B 7B 8B 8B 8B Gen. Only 0.71 0.98 0.98 0.98 0.98 0.96 0.99 0.98 0.98 0.99 0.99 Und. and Gen. 0.97 0.95 0.97 0.97 - 0.95 - 0.99 0.99 - 0.99 0.99 1.0 0.99 1.0 1.0 0.34 0.50 0.51 0.71 0.74 0.87 0.94 0.81 0.84 0.85 0.87 0.58 0.52 0.68 0.80 - 0.60 - 0.89 0.92 - 0.21 0.44 0.44 0.34 0.39 0.47 0.72 0.74 0.66 0.77 0.44 0.26 0.49 0.30 0.54 - 0.41 - 0.59 0.85 - 0.58 0.80 0.85 0.81 0.85 0.83 0.89 0.79 0.74 0.87 0.85 0.80 0.82 0.84 0.76 - 0.81 - 0.90 0.92 - 0.94 0.93 0.95 0.76 0.94 0.81 0.79 0.64 0.61 0.85 0.96(+2%) 0.87(+2%) 0.95(+6%) 0.88 0.89 0.88 0.84 0.89 0.07 0.08 0.07 0.17 0.15 0.43 0.33 0.22 0.40 0.34 0.44 0.19 0.11 0.46 0.32 - 0.16 - 0.79 0.75 - 0.64 0.49 0.55 0.20 0.85 0. 0.04 0.07 0.17 0.21 0.23 0.45 0.60 0.45 0.43 0.54 0.54 0.14 0.28 0.42 0.50 - 0.24 - 0.66 0.61 - 0.63 0.70 0.76 0.37 0.76 0.32 0.48 0.50 0.54 0.55 0.67 0.74 0.66 0.68 0.72 0. 0.49 0.53 0.61 0.65 0.63 0.55 0.39 0.80 0.84 0.80 0.82 0.80 0.80 0.63 0.88 0.82(+6%) 0.91(+3%) consistent images from textual prompts. Additionally, we perform qualitative comparisons with state-of-the-art models to complement these automatic evaluation metrics, ensuring robust analysis of performance. 6.1.1. Quantitative Results Evaluation Results on GenEval Benchmark. Table 3 presents comparison of model performance on the GenEval (Ghosh et al., 2024) benchmark, which is designed to evaluate object-centric T2I generation using compositional prompts with diverse object attributes. Under identical evaluation settings, Lumina-DiMOO achieves an impressive 88% overall score, surpassing both specialized generation models (FLUX.1 [Dev]: 82%, Lumina-mGPT 2.0: 69%) and unified models (Janus-Pro: 80%, BAGEL: 82%, and GPT-4o: 84%), thereby setting new SOTA results. This success is largely attributed to Lumina-DiMOOs enhanced capability in managing positional relationships and binding attributes. Compared to MMaDA, which features similar architecture, Lumina-DiMOO demonstrates substantial overall improvement of 25% (88% vs. 63%). This significant advancement underscores the potential of the discrete diffusion architecture for practical applications. In addition, we validate the effectiveness of the proposed Self-GRPO on GenEval. Following the Self-GRPO training stage, Lumina-DiMOO demonstrates an overall improvement of 3% on GenEval, with even more pronounced enhancements in Colors and Attribute. Evaluation Results on DPG Benchmark. Table 4 presents performance comparison on the DPG (Hu et al., 2024) benchmark, which includes 1,065 dense prompts designed for detailed evaluation of various aspects of prompt adherence. Overall, Lumina-DiMOO achieves an impressive overall score 12 Lumina-DiMOO: An Omni Discrete Diffusion Model for Multi-Modal Generation and Understanding Table 4: Evaluation of Text-to-Image Generation on DPG (Hu et al., 2024) Benchmark. Und. and Gen. denote understanding and generation, respectively. We highlight the best and the second results. means the MMaDA results are evaluated by ourselves. Method Architecture # Params. Global Entity Attribute Relation Other Overall PixArt-𝛼 (Chen et al., 2023) Lumina-Next (Zhuo et al., 2024) SDXL (Podell et al., 2024) Emu3-Gen (Wang et al., 2024c) DALL-E 3 (Betker et al., 2023) SD3-Medium (Esser et al., 2024) FLUX.1 [Dev] (Labs, 2024) OmniGen (Xiao et al., 2024) SANA-1.5 (Xie et al., 2025b) Lumina-mGPT 2.0 (Xin et al., 2025a) Diffusion Diffusion Diffusion AR - Diffusion Diffusion Diffusion Diffusion AR Show-o (Xie et al., 2025c) TokenFlow-XL (Liu et al., 2024b) Janus (Wu et al., 2024a) Janus-Pro (Chen et al., 2025c) GPT-4o (OpenAI, 2025) BLIP3-o (Chen et al., 2025a) BAGEL (Deng et al., 2025) Uniworld-V1 (Lin et al., 2025) OmniGen2 (Wu et al., 2025b) MMaDA (Yang et al., 2025) Lumina-DiMOO (Ours) AR+Diff. AR AR AR - AR+Diff. AR+Diff. AR+Diff. AR+Diff. Discrete Diff. Discrete Diff. Gen. Only 0.6B 2B 2.6B 8B - 2B 12B 3.8B 4.8B 7B 74.97 82.82 83.27 85.21 90.97 87.90 74.35 87.90 - - Und. and Gen. - 1.3B 14B 1.3B 7B - 8B 14B 20B 7B 8B 8B 78.72 82.33 86.90 88.89 - 88.94 83.64 88.81 77.81 81.46 79.32 88.65 82.43 86.68 89.61 91.01 90.00 88.97 - 88. - 79.22 87.38 88.90 88.94 - 90.37 88.39 88.83 78.48 92.08 78.60 86.44 80.91 86.84 88.39 88.83 88.96 88.47 - 88.08 - 81.29 87.70 89.40 89.84 - 91.29 88.44 90.18 81.74 88.98 82.57 80.53 86.76 90.22 90.58 80.70 90.87 87.95 - 91.70 - 85.22 85.46 89.32 92.63 - 90.82 89.27 89.37 84.79 94.31 76.96 81.82 80.41 83.15 89.83 88.68 88.33 83.56 - - - 71.20 86.41 89.48 90.96 - 88.67 87.22 90.27 63.20 82.00 71.11 74.63 74.65 80.60 83.50 84.08 83.84 81.16 85.00 84. 67.48 73.38 79.68 84.19 85.15 81.60 85.07 81.38 83.57 69.97 86.04 Table 5: Evaluation of Text-to-Image Generation on on UniGenBench (Wang et al., 2025b). This leadborder is evaluated and maintained by the Tencent Hunyuan team. Und. and Gen. denote understanding and generation, respectively. We highlight the best and the second results. Model Style World Know. Attribute Action Relation. Logic. Grammar Compound Layout Text Overall 87.40 SDXL (Podell et al., 2024) Playground 2.5 (Li et al., 2024a) 89.50 86.80 Emu3 Wang et al. (2024c) DALL-E-3 (Betker et al., 2023) 95.06 SD-3.5-Large (Esser et al., 2024) 88.60 83.90 FLUX.1-dev (Labs, 2024) Janus-flow (Ma et al., 2024) BLIP3-o (Chen et al., 2025a) Janus-Pro (Chen et al., 2025c) BAGEL (Deng et al., 2025) UniWorld-V1 Lin et al. (2025) OmniGen2 (Wu et al., 2025b) MMaDA (Yang et al., 2025) Lumina-DiMOO (Ours) 86.20 92.80 90.80 90.20 91.10 91.90 82.40 89.70 72.63 76.11 77.06 93.51 88.92 88.92 62.50 80.22 86.71 85.60 82.91 86.39 56.65 90.03 Gen. Only 44.34 52.78 51.39 75.97 68.59 67. 34.22 42.68 40.11 69.83 62.17 62.17 44.92 51.52 49.75 78.06 69.80 67.26 Und. and Gen. 47.97 63.89 67.74 67.74 70.62 72.12 48.39 81.62 43.35 63.97 64.26 61.98 67.21 62.83 37.83 71.12 50.00 66.50 68.40 70.69 67.13 68.27 50.25 78. 9.55 16.59 19.32 48.18 32.27 30.91 21.14 39.55 37.05 30.23 38.41 32.50 17.95 45.45 47.33 53.21 52.94 68.07 58.96 60.96 60.29 68.45 64.44 66.44 63.77 59.89 55.75 70.45 26.68 35.44 36.86 70.60 58.76 47.04 45.10 53.74 62.11 58.12 54.51 56.31 32.35 73. 0.57 29.85 1.15 37.13 44.78 1.15 66.67 25.86 69.03 32.76 71.83 32.18 0.86 46.46 1.15 68.47 2.59 72.01 76.49 7.76 69.03 26.44 71.64 29.02 1.15 30.22 82.84 25.57 39.75 45.61 46.02 69.18 62.99 61.30 46.39 59.87 61.61 61.53 63.11 63.09 41.35 71.12 of 86.04, surpassing all previous models and demonstrating its superior prompt-following abilities. In particular, Lumina-DiMOO excels in interpreting prompts that involve entities and relationships, Lumina-DiMOO: An Omni Discrete Diffusion Model for Multi-Modal Generation and Understanding Table 6: Evaluation of Text-to-Image Generation on OneIG-EN (Chang et al., 2025) Benchmark. The overall score is the average of the five dimensions. Und. and Gen. denote understanding and generation, respectively. We highlight the best and the second results. Method Architecture # Params. Alignment Text Reasoning Style Diversity Overall SD 1.5 (Rombach et al., 2022) Diffusion SDXL (Podell et al., 2024) FLUX.1 [Dev] (Labs, 2024) SANA-1.5 (Xie et al., 2025b) Diffusion Diffusion Diffusion 0.9B 2.6B 12B 4.8B 0.565 0. 0.786 0.765 Gen. Only Janus-Pro (Chen et al., 2025c) AR BLIP3-o (Chen et al., 2025a) AR+Diff. BAGEL (Deng et al., 2025) AR Lumina-DiMOO (Ours) Discrete Diff. Und. and Gen. 0. 7B 8B 14B 8B 0.711 0. 0.816 0.010 0.029 0.523 0.069 0. 0.013 0.244 0.551 0.207 0.237 0. 0.217 0.139 0.223 0.173 0.276 0. 0.332 0.368 0.401 0.276 0.361 0. 0.400 0.429 0.296 0.238 0.216 0. 0.229 0.251 0.232 0.319 0.316 0. 0.334 0.267 0.307 0.361 0.455 Table 7: Evaluation of Text-to-Image Generation on TIIF (Wei et al., 2025) Benchmark. Und. and Gen. denote understanding and generation, respectively. We highlight the best and the second results. Method Overall Basic Following Advanced Following Avg Attribute Relation Reasoning Avg Attribute +Relation Attribute +Reasoning Relation +Reasoning Style Text Designer Real World short long short long short long short long short long short long short long short long short long short long short long short long Gen. Only SD 3 (Esser et al., 2024) 67.46 66.09 78.32 77.75 83.33 79.83 82.07 78.82 71.07 74.07 61.46 59.56 61.07 64.07 68.84 70.34 50.96 57.84 66.67 76.67 59.83 20.83 63.23 67.34 PixArt-Σ (Chen et al., 2023) 62.00 58.12 70.66 75.25 69.33 78.83 75.07 77.32 67.57 69.57 57.65 49.50 65.20 56.57 66.96 61.72 66.59 54.59 83.33 70.00 1.83 1.83 62.11 52.41 Lumina-Next (Zhuo et al., 2024) 50.93 52.46 64.58 66.08 56.83 59.33 67.57 71.82 69.32 67.07 44.75 45.63 51.44 43.20 51.09 59.72 44.72 54.46 70.00 66.67 0.00 0.83 47.56 49.05 SANA 1.5 (Xie et al., 2025b) 67.15 65.73 79.66 77.08 79.83 77.83 85.57 83.57 73.57 69.82 61.50 60.67 65.32 56.57 69.96 73.09 62.96 65.84 80.00 80.00 17.83 15.83 71.07 68.83 FLUX.1 [dev] (Labs, 2024) 71.09 71.78 83.12 78.65 87.05 83.17 87.25 80.39 75.01 72.39 65.79 68.54 67.07 73.69 73.84 73.34 69.09 71.59 66.67 66.67 43.83 52.83 70.72 71.47 Show-o (Xie et al., 2025c) 59.72 58.86 73.08 75.83 74.83 79.83 78.82 78.32 65.57 69.32 53.67 50.38 60.95 56.82 68.59 68.96 66.46 56.22 63.33 66.67 3.83 2.83 55.02 50.92 Janus-Pro-7B (Chen et al., 2025c) 66.50 65.02 79.33 78.25 79.33 82.33 78.32 73.32 80.32 79.07 59.71 58.82 66.07 56.20 70.46 70.84 67.22 59.97 60.00 70.00 28.83 33.83 65.84 60. Lumina-DiMOO (Ours) 71.27 68.53 75.5 78.29 77.00 81.50 74.20 78.21 75.29 75.16 70.49 68.33 75.99 72.85 70.73 67.30 65.79 69.23 73.33 60.00 59.28 41.63 69.78 70.90 Und. and Gen. outperforming all other models in the comparison. In addition, we evaluate MMaDA under the same settings on the DPG benchmark, its performance proves to be subpar, with score of just 69.97. Evaluation Results on UniGenBench Leaderboard. UniGenBench (Wang et al., 2025b) is newly unified benchmark for text-to-image generation that integrates diverse prompt themes with comprehensive suite of fine-grained evaluation criteria. The leaderboard is evaluated and maintained by the Tencent Hunyuan team. We extract evaluation results for various models from the leaderboard, as presented in Table 5. Lumina-DiMOO ranks among the top performers across all metrics, notably excelling in the Layout and Attribute categories, and surpassing all other models in overall evaluation scores. For detailed evaluation across 27 dimensions, please refer to Leaderboard Link. Evaluation Results on OneIG-EN Benchmark. Table 6 reports the quantitative results on the OneIGEN (Chang et al., 2025) benchmark, comprehensive evaluation framework specifically designed to assess the fine-grained performance of text-to-image models across multiple dimensions. For fair comparison, we compute the overall score by averaging the results across all dimensions. Overall, Lumina-DiMOO achieves the highest average score and significantly surpasses other unified models such as BAGEL and Janus Pro, showcasing its robust capability in general-purpose image generation. Notably, it ranks first in the Alignment, Text, and Reasoning categories, highlighting its exceptional ability to follow prompts accurately and perform advanced reasoning. 14 Lumina-DiMOO: An Omni Discrete Diffusion Model for Multi-Modal Generation and Understanding Figure 6: Qualitative Comparison on Text-to-Image Generation. We compare Lumina-DiMOO, MMaDA, Janus Pro, BAGEL, and GPT-4o across various common scenarios. Notably, MMaDA and Janus Pro lack support for arbitrary resolution generation. 15 Lumina-DiMOO: An Omni Discrete Diffusion Model for Multi-Modal Generation and Understanding Figure 7: Qualitative Results on Text-guided Image Inpainting and Extrapolation. Evaluation Results on TIIF Benchmark. Table 7 shows the performance comparison on the TIIF testmini (Wei et al., 2025), benchmark designed to systematically assess the ability of text-to-image models to interpret and follow complex textual instructions. Overall, Lumina-DiMOO secures the second position, surpassed only by FLUX.1 [dev], result that underscores its robust instructionfollowing capabilities. 6.1.2. Qualitative Results Qualitative Comparisons. We conduct qualitative comparison among Lumina-DiMOO, MMaDA, Janus-Pro 7B, BAGEL, and GPT-4o. As illustrated in Figure 6, Lumina-DiMOO consistently generates images of significantly higher quality compared to MMaDA and Janus-Pro 7B. Moreover, Lumina-DiMOO demonstrates exceptional flexibility in supporting any resolution, whereas MMaDA (limited to fixed resolution of 512512), Janus-Pro (restricted to 768768), and GPT-4o (featuring uncontrollable resolution) show clear limitations in this aspect. 16 Lumina-DiMOO: An Omni Discrete Diffusion Model for Multi-Modal Generation and Understanding Table 8: Evaluation of Controllable Generation Ability on Graph-200K (Li et al., 2025b) benchmark. The methods that train specialist for each task are marked as gray color. Except for these methods, we highlight the best and the second results. Condition Method Controllability Quality Text Consistency F1 RMSE FID SSIM MAN-IQA MUSIQ CLIP-Score ControlNet (Zhang et al., 2023) OminiControl (Tan et al., 2024) OmniGen (Xiao et al., 2024) Canny Lumina-mGPT (Liu et al., 2024a) OneDiffusion (Le et al., 2025) Lumina-mGPT 2.0 (Xin et al., 2025a) Lumina-DiMOO (Ours) ControlNet (Zhang et al., 2023) OminiControl (Tan et al., 2024) OmniGen (Xiao et al., 2024) Depth Lumina-mGPT (Liu et al., 2024a) OneDiffusion (Le et al., 2025) Lumina-mGPT 2.0 (Xin et al., 2025a) Lumina-DiMOO (Ours) 0.13 0.47 0.43 0.16 0.39 0.49 0. - - - - - - - - - - - - - - 23.70 21.44 15.07 15.71 10. 17.42 8.31 46.06 29.58 51.58 85.03 32. 30.89 30.35 36.83 36.23 86.08 61. 39.03 36.52 34.38 0.34 0.61 0.47 0. 0.55 0.54 0.65 0.41 0.52 0. 0.34 0.49 0.49 0.62 0.31 0.44 0. 0.48 0.46 0.42 0.41 0.44 0. 0.49 0.38 0.49 0.39 0.40 45.45 61. 62.66 70.78 59.99 63.18 64.11 60. 60.18 64.90 69.72 60.49 59.52 63. 34.10 34.40 33.66 28.18 34.99 34.44 34. 34.49 34.08 29.72 31.58 34.71 34. 34.54 Image Inpainting and Extrapolation. Due to the mask training paradigm of Lumina-DiMOO, it naturally supports text-guided image inpainting and extrapolation without requiring any fine-tuning. Examples are presented in Figure 7. As shown on the top of the figure, given an input image with partial mask, Lumina-DiMOO is able to seamlessly inpaint the masked areas. Besides, Lumina-DiMOO is capable of extrapolating the original image horizontally or vertically based on the given text prompt (as illustrated in the third and fourth rows). These examples clearly highlight the inherent advantages of Lumina-DiMOO over Diffusion, AR or hybrid AR-Diffusion models in downstream applications. 6.2. Performance of Image-to-Image Generation We primarily evaluate our model using the Graph-200K (Li et al., 2025b) and ImgEdit (Ye et al., 2025b) benchmarks. The Graph-200K benchmark enables comprehensive assessment across multiple tasks, including controllable generation, subject-driven generation, and style transfer, with an image style serving as reference. In contrast, the ImgEdit benchmark focuses on evaluating the models proficiency in image editing tasks, such as adding, replacing, and removing objects, as well as changing the image style based on text descriptions. 6.2.1. Quantitative Results Evaluation Results of Controllable Generation. For controllable generation, we evaluate the models based on three criteria: controllability (measured through F1-Score and RMSE), visual quality (measured through FID (Heusel et al., 2017), SSIM, MAN-IQA (Yang et al., 2022), and MUSIQ (Ke et al., 2021)), and text consistency (measured through CLIP-Score (Radford et al., 2021)), following the evaluation approach of Graph-200K (Li et al., 2025b). As shown in Table 8, LuminaDiMOO exhibits comparable controllability to other leading universal generative models (OmniGen, OneDiffusion, and Lumina-mGPT 2.0), while achieving superior visual quality and text consistency. Notably, when compared to specialized methods (ControlNet and OminiControl), Lumina-DiMOO performs on par with the best results and even outperforms them on the depth-to-image task. 17 Lumina-DiMOO: An Omni Discrete Diffusion Model for Multi-Modal Generation and Understanding Table 9: Evaluation of Style Transfer and Subject-Driven Generation Abilities on Graph-200K (Li et al., 2025b) Benchmark and Image Editing Ability on ImgEdit (Ye et al., 2025b) Benchmark. Image editing metrics are evaluated by GPT-4.1. The methods that train specialist for each task are marked as gray color. Except for these methods, we highlight the best and the second results. Method Style Transfer (Img Reference) Subject-Driven Generation Image Editing Text Alignment Style Consistency DINOv2 CLIP-I CLIP-T Add Replace Remove Style OminiControl (Tan et al., 2024) InstantStyle (Wang et al., 2024a) AnyEdit (Yu et al., 2025a) OmniGen (Xiao et al., 2024) Lumina-mGPT (Liu et al., 2024a) OneDiffusion (Le et al., 2025) Lumina-mGPT 2.0 (Xin et al., 2025a) BAGEL (Deng et al., 2025) UniWorld-V1 (Lin et al., 2025) Lumina-DiMOO (Ours) - 0.27 - 0.27 - - - - - - 0.60 - 0.52 - - - - - 73.17 87.70 33.53 - - 67. 60.94 73.88 76.60 - - - - 83.43 70.63 86.91 87.37 - - - - 34.53 30.16 34. 33.90 - - 0.32 0.53 80. 89.36 34.72 3.18 3.47 2.47 2. 2.23 2.43 2.85 4.19 - - - 3.56 3.82 3.82 - - - 3.30 3.47 3.83 - - - 2.62 3.24 2.76 - - - 4.49 4.21 4.18 In the style transfer task, where an image serves as the Evaluation Results of Style Transfer. style reference (as shown in Figure 10), we measure text consistency and style alignment using the CLIP (Radford et al., 2021) models on the Graph-200K benchmark. As presented in Table 9, Lumina-DiMOO exceeds OmniGen by 5% and 1% in text alignment and style consistency, respectively. Furthermore, when compared to InstantStyle, specialized model, Lumina-DiMOO also achieves 5% improvement in text alignment, with 7% decrease in style alignment. Evaluation Results of Subject-Driven Generation. We also evaluate the models on Graph-200K specifically for subject-driven image generation and report semantic alignment using the DINOv2 (Oquab et al., 2023), CLIP-I (Radford et al., 2021), and CLIP-T (Radford et al., 2021) scores. As shown in Table 9, Lumina-DiMOO consistently demonstrates notable improvements across all these metrics. For example, compared to the previous SOTA model Lumina-mGPT 2.0, Lumina-DiMOO achieves improvements of 3.97%, 1.99%, and 0.82% in these three scores. Evaluation Results of Image Editing. Table 9 presents the results on the ImgEdit (Ye et al., 2025b) benchmark. We primarily focus on testing four common editing tasks: adding, removing, replacing, and changing style (text guidance). The evaluation metrics included instruction adherence, image-editing quality, and detail preservation, each scored on scale from 1 to 5. These scores are assessed by GPT-4.1. Lumina-DiMOO performs exceptionally well in adding and replacing objects, surpassing other models (e.g., OmniGen, BAGEL, and UniWorld-V1). However, there remains room for improvement in tasks involving removing objects and changing styles. 6.2.2. Qualitative Results We conduct qualitative comparison on multiple image-to-image tasks between Lumina-DiMOO, OmniGen, Lumina-mGPT 2.0, BAGEL and GPT-4o. 1) Controllable Generation: As illustrated at the top of Figure 8, Lumina-DiMOO demonstrates precise generation capabilities under various control conditions. In contrast, OmniGen exhibits notable shortcomings in depth-to-image tasks, while Lumina mGPT 2.0 shows clear limitations in pose-to-image scenarios. 2) Subject-Driven Generation: As depicted at the bottom of Figure 8, Lumina-DiMOO excels in both object preservation and adherence to text instructions. 3) Style Transfer: Lumina-DiMOO holds distinct advantage over OmniGen in preserving the original image during style transfer, while also demonstrating superior comprehension and application of the reference images style, as shown in Figure 10. 4) Image Editing: As shown in Figure 9, Lumina-DiMOO performs well in tasks such as adding, removing, and 18 Lumina-DiMOO: An Omni Discrete Diffusion Model for Multi-Modal Generation and Understanding Figure 8: Qualitative Comparison on Controllable and Subject-Driven Generation Tasks. We compare Lumina-DiMOO, BAGEL, and GPT-4o in object addition, removal, replacement, as well as background and style modification. Lumina-DiMOO performes well in terms of instruction adherence and resolution preservation. 19 Lumina-DiMOO: An Omni Discrete Diffusion Model for Multi-Modal Generation and Understanding Figure 9: Qualitative Comparison on Image Editing Tasks. We compare Lumina-DiMOO, BAGEL, and GPT-4o in object addition, removal, replacement, as well as background and style modification. Lumina-DiMOO performed well in terms of instruction adherence and resolution preservation. 20 Lumina-DiMOO: An Omni Discrete Diffusion Model for Multi-Modal Generation and Understanding Figure 10: Qualitative Comparison on Style Transfer Task. Lumina-DiMOO completely outperforms OmniGen, which performs worse in most cases. replacing objects, as well as changing image backgrounds and styles. It also excels in preserving the resolution of the original image. BAGEL, on the other hand, falls slightly behind in object removal and style modification tasks. While GPT-4o demonstrates strong performance in editing tasks, there is still significant room for improvement in maintaining the resolution of the original image. 6.3. Performance of Image Understanding To evaluate our models multimodal understanding capabilities, we evaluate it on five widely recognized vision-language benchmarks: POPE (Li et al., 2023b), MME-P (Yin et al., 2024), MMBench (Liu et al., 2024f), SEED (Li et al., 2023a), and MMMU (Yue et al., 2024). Together, these benchmarks provide concise yet comprehensive testbed that encompasses perception, cognition, and multimodal reasoning. They also possess strong discriminative power for ranking state-of-the-art models, ensuring thorough assessment of performance. 6.3.1. Quantitative Results We conduct comprehensive comparison of Lumina-DiMOO with leading open-source multimodal models, covering both specialized models for visual understanding and general-purpose unified models. The results of visual understanding are detailed in Table 10. Compared with dedicated understanding-only models such as LLaVA-v1.5, Qwen-VL-Chat, Emu3-Chat, and InstructBLIP, our model achieves superior results across all benchmarks, despite being trained in unified framework. When compared to other unified models (e.g., Show-o, VILA-U, Janus-Pro, BAGEL), Lumina-DiMOO consistently demonstrates outstanding performance, achieving leading scores in the POPE (87.4), SEED (83.1), and MMMU (58.6) benchmarks. In particular, Lumina-DiMOO significantly outperforms MMaDA (with similar architecture) across all benchmarks, highlighting the potential of unified discrete diffusion architecture in bridging generation and understanding tasks. 6.3.2. Qualitative Results In addition to delivering comparable performance on various image understanding benchmarks, we visualize its capabilities across several understanding tasks, including OCR, captioning, mathematical geometry, and table understanding, as shown in Figure 11. The visualization results demonstrate 21 Lumina-DiMOO: An Omni Discrete Diffusion Model for Multi-Modal Generation and Understanding Table 10: Comparison with State-of-the-arts on Multimodal Understanding Benchmarks. Und. and Gen. denote understanding and generation, respectively. We highlight the best and the second results. Model Architecture # Params. POPE MME-P MMB SEED MMMU MobileVLM (Chu et al., 2023) MobileVLM-V2 (Chu et al., 2024) LLaVA-Phi (Zhu et al., 2024) LLaVA (Liu et al., 2024e) LLaVA-v1.5 (Liu et al., 2023) InstructBLIP (Dai et al., 2023) Qwen-VL-Chat (Bai et al., 2023) IDEFICS-9B (Laurençon et al., 2023) Emu3-Chat (Wang et al., 2024c) InstructBLIP (Dai et al., 2023) Show-o (Xie et al., 2025c) D-Dit (Li et al., 2024b) TokenFlow-XL (Qu et al., 2024) VILA-U (Wu et al., 2025c) Chameleon (Team, 2024) Janus-Pro (Chen et al., 2025c) BLIP3-o (Chen et al., 2025a) BAGEL (Deng et al., 2025) Uniworld-V1 (Lin et al., 2025) OmniGen2 (Wu et al., 2025b) MMaDA (Yang et al., 2025) Lumina-DiMOO (Ours) AR AR AR AR AR AR AR AR AR AR Und. Only 1.4B 1.4B 2.7B 7B 7B 7B 7B 8B 8B 13B AR+Discrete Diff. Discrete Diff.+Diff. AR AR AR AR AR+Diff. AR+Diff. AR+Diff. AR+Diff. Discrete Diff. Discrete Diff. Und. and Gen. 1.3B 2.0B 13B 7B 7B 7B 8B 14B 20B 7B 8B 8B 84.5 84.3 85.0 76.3 85.9 - - - 85.2 78.9 80.0 84.0 86.8 85.8 - 87.4 - - - - 86.1 87.4 1196.2 1302.8 1335.1 809.6 1510.7 - 1487.5 - 1244 1212.8 1097.2 1124.7 1545.9 1401.8 - 1567.1 1682.6 1687.0 - - 1410.7 1534.2 53.2 57.7 59.8 38.7 64.3 36.0 60.6 48.2 58.5 - - - 68.9 - - 79.2 83.5 85.0 83.5 79.1 68.5 84. - - - 33.5 58.6 53.4 58.2 - 68.2 - - - 68.7 59.0 - 72.1 77.5 - - - 64.2 83.1 - - - - 35.4 - - - 31.6 - 26.7 - 38.7 - 22.4 41.0 50.6 55.3 58.6 53.1 30.2 58.6 Figure 11: Visualization of OCR, Image Caption, Mathematical Geometry, and Table Understanding Tasks. that Lumina-DiMOO excels in text recognition accuracy, detailed image description, mathematical geometry, and the rational analysis of tables. 22 Lumina-DiMOO: An Omni Discrete Diffusion Model for Multi-Modal Generation and Understanding Figure 12: Comparison of Sampling Time on Text-to-Image and Image Understanding Tasks. For the text-to-image, Lumina-mGPT 2.0 generates images at resolution of 768, Emu3 produces images at 720 resolution, while the other models utilize 1024 resolution. In the image understanding task, all models consistently generate 1024 tokens. 7. Ablation and Extension 7.1. Analysis of Sampling Speed Comparison with AR and Hybrid AR-Diffusion Models. For text-to-image generation, we set 64 sampling steps for Lumina-DiMOO. As illustrated in Figure 12(a), Lumina-DiMOOs sampling efficiency is several times higher than that of the AR models (Lumina-mGPT 2.0 and Emu3), and its sampling speed is roughly on par with BAGEL. If the sampling steps for Lumina-DiMOO are further reduced, its speed advantage becomes even more pronounced. On the other hand, for image understanding, we configure the block length to 256 and the number of sampling steps to 128 for Lumina-DiMOO. We find that the sampling speed advantage for image understanding is reduced, as shown in Figure 12(b). This is because text generation occurs in block-wise manner, unlike image generation, which employs single global decoding step. As result, its speed is affected by both the number of blocks and the number of steps. Thus, the speed improvement in image understanding is not as substantial as in image generation. These observations highlight the promising potential of Lumina-DiMOO. Effect of ML-Cache. Under identical settings, we evaluate the sampling time of Lumina-DiMOO with and without the ML-Cache strategy, as shown in Figure 12. The results demonstrate that ML-Cache significantly enhances the sampling process, boosting efficiency by factor of 2.05 for text-to-image generation and 1.87 for image understanding. However, minor drawback is that ML-Cache increases GPU usage, for example, from 38.9 GB to 45.9 GB when generating 10241024 image. 7.2. Effect of Initialization From LLaDA In this work, Lumina-DiMOO builds upon LLaDAs text capabilities and expands its functionalities in multi-modal generation and understanding, consistent with the paradigm of previous works (Wu et al., 2024b; Yang et al., 2025). However, previous studies (Xin et al., 2025a) has also found that training from scratch without prior textual knowledge does not impact performance in autoregressive multimodal generation. To explore this further, we conduct additional ablation experiments to assess the necessity of inheriting text capabilities within the discrete diffusion framework. 23 Lumina-DiMOO: An Omni Discrete Diffusion Model for Multi-Modal Generation and Understanding Figure 13: Illustration of Interactive Retouching. Users can repeatedly modify specific areas while keeping the surrounding regions unchanged until they reach satisfaction. Experimental Setup. We design comparative experiment with two key components: (1) initializing Lumina-DiMOO using LLaDA-SFT (Nie et al., 2025) and (2) training Lumina-DiMOO from scratch. For model training, we randomly select dataset from Section 5 (Stage-III), consisting of 5M samples for visual generation and 5M samples for visual understanding. To conserve training resources, we omit the pre-training stage and directly engage in supervised fine-tuning on 256 resolution. For fair comparison, we keep all training and evaluation parameters constant, except for model initialization. In our evaluation of generation and understanding capabilities, we observe that training Results. from scratch falls short in generating images or performing image understanding, often resulting in very large gradient norm during training. In contrast, initializing from LLaDA effectively supports both image generation and understanding, clearly demonstrating its superiority without requiring quantitative comparison. 7.3. Bringing New Ideas to Image Generation: Interactive Retouching Interactive Retouching stands out as unique feature of Lumina-DiMOO, adept at allowing users to pinpoint specific areas for refinement through precise annotations, as illustrated in Figure 13. Lumina-DiMOO achieves this due to its unique discrete diffusion modeling paradigm, which allows it to mask user-annotated areas for regeneration. This process preserves all information in areas outside of the users annotations, feat previously unachievable with diffusion or AR generative models. While many commercial editing models exist, such as GPT-4o and Nana-Banana, none offer 100% guarantee of maintaining unchanged content outside the users specified annotations. 8. Conclusion In this paper, we introduce Lumina-DiMOO, unified foundation model for multi-modal understanding and generation. Lumina-DiMOO delivers top-tier performance on standard multi-modal generation and 24 Lumina-DiMOO: An Omni Discrete Diffusion Model for Multi-Modal Generation and Understanding understanding benchmarks and stands out with its ultra-fast sampling speed and unique interactive retouching features. To further advance research in multi-modal and discrete diffusion research, we have open-sourced Lumina-DiMOO to the research community. While Lumina-DiMOO currently demonstrates strong capabilities in image generation and understanding, our goal is to evolve it into more comprehensive multi-modal model. In the future, we aim to expand Lumina-DiMOO to seamlessly integrate video, audio, and more modalities. Achieving this will require substantial research, particularly in creating versatile tokenizer for diverse data types, designing the model architecture that processes temporal information, and developing advanced training techniques. Let us look forward to more powerful Lumina-DiMOO."
        },
        {
            "title": "References",
            "content": "Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems (NeurIPS), 2021. Jinbin Bai, Tian Ye, Wei Chow, Enxin Song, Xiangtai Li, Zhen Dong, Lei Zhu, and Shuicheng Yan. Meissonic: Revitalizing masked generative transformers for efficient high-resolution text-to-image synthesis. arXiv preprint arXiv:2410.08261, 2024. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Jingjing Chang, Yixiao Fang, Peng Xing, Shuhan Wu, Wei Cheng, Rui Wang, Xianfang Zeng, Gang Yu, and Hai-Bao Chen. Oneig-bench: Omni-dimensional nuanced evaluation for image generation. arXiv preprint arXiv:2506.07977, 2025. Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, et al. Blip3-o: family of fully open unified multimodal modelsarchitecture, training and dataset. arXiv preprint arXiv:2505.09568, 2025a. Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-𝛼: Fast training of diffusion transformer for photorealistic 25 Lumina-DiMOO: An Omni Discrete Diffusion Model for Multi-Modal Generation and Understanding text-to-image synthesis. Proceedings of the International Conference on Learning Representations (ICLR), 2023. Junying Chen, Zhenyang Cai, Pengcheng Chen, Shunian Chen, Ke Ji, Xidong Wang, Yunjin Yang, and Benyou Wang. Sharegpt-4o-image: Aligning multimodal models with gpt-4o-level image generation. arXiv preprint arXiv:2506.18095, 2025b. Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025c. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024a. Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024b. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024c. Jaemin Cho, Yushi Hu, Jason M. Baldridge, Roopal Garg, Peter Anderson, Ranjay Krishna, Mohit Bansal, Jordi Pont-Tuset, and Su Wang. Davidsonian scene graph: Improving reliability in finegrained evaluation for text-to-image generation. In Proceedings of the International Conference on Learning Representations (ICLR), 2024. Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang, Xiaolin Wei, et al. Mobilevlm: fast, reproducible and strong vision language assistant for mobile devices. arXiv preprint arXiv:2312.16886, 2023. Xiangxiang Chu, Limeng Qiao, Xinyu Zhang, Shuang Xu, Fei Wei, Yang Yang, Xiaofei Sun, Yiming Hu, Xinyang Lin, Bo Zhang, et al. Mobilevlm v2: Faster and stronger baseline for vision language model. arXiv preprint arXiv:2402.03766, 2024. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. Jacob Devlin. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Proceedings of the International Conference on Machine Learning (ICML), 2024. 26 Lumina-DiMOO: An Omni Discrete Diffusion Model for Multi-Modal Generation and Understanding Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024. Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems (NeurIPS), 2024. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Jarvis Guo, Tuney Zheng, Yuelin Bai, Bo Li, Yubo Wang, King Zhu, Yizhi Li, Graham Neubig, Wenhu Chen, and Xiang Yue. Mammoth-vl: Eliciting multimodal reasoning with instruction tuning at scale. arXiv preprint arXiv:2412.05237, 2024. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in Neural Information Processing Systems (NeurIPS), 2017. Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models with llm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135, 2024. Zehuan Huang, Yuanchen Guo, Haoran Wang, Ran Yi, Lizhuang Ma, Yan-Pei Cao, and Lu Sheng. Mvadapter: Multi-view consistent image generation made easy. Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2025a. Zemin Huang, Zhiyang Chen, Zijun Wang, Tiancheng Li, and Guo-Jun Qi. Reinforcing the diffusion chain of lateral thought with diffusion language models. arXiv preprint arXiv:2505.10446, 2025b. Zihan Huang, Tao Wu, Wang Lin, Shengyu Zhang, Jingyuan Chen, and Fei Wu. Autogeo: Automating geometric image dataset creation for enhanced geometry understanding. IEEE Transactions on Multimedia (TMM), 2025c. Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2021. Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. Hugo Laurençon, Daniel van Strien, Stas Bekman, Leo Tronchon, Lucile Saulnier, Thomas Wang, Siddharth Karamcheti, Amanpreet Singh, Giada Pistilli, Yacine Jernite, and et al. Introducing idefics: An open reproduction of state-of-the-art visual language model, 2023. URL https: //huggingface.co/blog/idefics. Duong H. Le, Tuan Pham, Sangho Lee, Christopher Clark, Aniruddha Kembhavi, Stephan Mandt, Ranjay Krishna, and Jiasen Lu. One diffusion to generate them all. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2025. Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023a. 27 Lumina-DiMOO: An Omni Discrete Diffusion Model for Multi-Modal Generation and Understanding Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2. 5: Three insights towards enhancing aesthetic quality in text-to-image generation. arXiv preprint arXiv:2402.17245, 2024a. Shufan Li, Konstantinos Kallidromitis, Hritik Bansal, Akash Gokul, Yusuke Kato, Kazuki Kozuka, Jason Kuen, Zhe Lin, Kai-Wei Chang, and Aditya Grover. LaViDa: large diffusion language model for multimodal understanding. arXiv preprint arXiv:2505.16839, 2025a. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2023b. Zhong-Yu Li, Ruoyi Du, Juncheng Yan, Le Zhuo, Zhen Li, Peng Gao, Zhanyu Ma, and Ming-Ming Cheng. Visualcloze: universal image generation framework via visual in-context learning. Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2025b. Zijie Li, Henry Li, Yichun Shi, Amir Barati Farimani, Yuval Kluger, Linjie Yang, and Peng Wang. Dual diffusion for unified image generation and understanding. arXiv preprint arXiv:2501.00289, 2024b. Chao Liao, Liyang Liu, Xun Wang, Zhengxiong Luo, Xinyu Zhang, Wenliang Zhao, Jie Wu, Liang Li, Zhi Tian, and Weilin Huang. Mogao: An omni foundation model for interleaved multi-modal generation. arXiv preprint arXiv:2505.05472, 2025. Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, et al. Uniworld: High-resolution semantic encoders for unified visual understanding and generation. arXiv preprint arXiv:2506.03147, 2025. Anji Liu, Oliver Broadrick, Mathias Niepert, and Guy Van den Broeck. Discrete copula diffusion. In Proceedings of the International Conference on Learning Representations (ICLR), 2025a. Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin, Yu Qiao, Hongsheng Li, and Peng Gao. Luminamgpt: Illuminate flexible photorealistic text-to-image generation with multimodal generative pretraining. arXiv preprint arXiv:2408.02657, 2024a. Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with ringattention. arXiv preprint arXiv:2402.08268, 2024b. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024c. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llavanext: Improved reasoning, ocr, and world knowledge, 2024d. URL https://llava-vl.github. io/blog/2024-01-30-llava-next/. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in Neural Information Processing Systems (NeurIPS), 2024e. Junpeng Liu, Tianyue Ou, Yifan Song, Yuxiao Qu, Wai Lam, Chenyan Xiong, Wenhu Chen, Graham Neubig, and Xiang Yue. Harnessing webpage uis for text-rich visual understanding. In Proceedings of the International Conference on Learning Representations (ICLR), 2025b. 28 Lumina-DiMOO: An Omni Discrete Diffusion Model for Multi-Modal Generation and Understanding Xuejie Liu, Anji Liu, Guy Van den Broeck, and Yitao Liang. Plug-and-play context feature reuse for efficient masked generation. arXiv preprint arXiv:2505.19089, 2025c. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In Proceedings of the European Conference on Computer Vision (ECCV), 2024f. Zhiyuan Liu, Yicun Yang, Yaojie Zhang, Junjie Chen, Chang Zou, Qingyuan Wei, Shaobo Wang, and Linfeng Zhang. dllm-cache: Accelerating diffusion large language models with adaptive caching. arXiv preprint arXiv:2506.06295, 2025d. Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the ratios of the data distribution. In Proceedings of the International Conference on Machine Learning (ICML), pages 3281932848, 2024. Zhuoyan Luo, Fengyuan Shi, Yixiao Ge, Yujiu Yang, Limin Wang, and Ying Shan. Open-magvit2: An open-source project toward democratizing auto-regressive visual generation. arXiv preprint arXiv:2409.04410, 2024. Xinyin Ma, Runpeng Yu, Gongfan Fang, and Xinchao Wang. dkv-cache: The cache for diffusion language models. arXiv preprint arXiv:2505.15781, 2025. Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Liang Zhao, et al. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation. arXiv preprint arXiv:2411.07975, 2024. Weijia Mao, Zhenheng Yang, and Mike Zheng Shou. Unirl: Self-improving unified multimodal models via supervised and reinforcement learning. arXiv preprint arXiv:2505.23380, 2025. Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, JiRong Wen, and Chongxuan Li. Large language diffusion models. Proceedings of the International Conference on Machine Learning (ICML), 2025. OpenAI. Gpt-image-1. https://openai.com/index/introducing-4o-image-generation/, 2025. Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. Transactions on Machine Learning Research (TMLR), 2023. Jingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, and Chongxuan Li. Your absorbing discrete diffusion secretly models the conditional distributions of clean data. In Proceedings of the International Conference on Learning Representations (ICLR), 2025. Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, Ji Hou, and Saining Xie. Transfer between modalities with metaqueries. arXiv preprint arXiv:2504.06256, 2025. Yong-Hyun Park, Chieh-Hsin Lai, Satoshi Hayakawa, Yuhta Takida, and Yuki Mitsufuji. Jump your steps: Optimizing sampling schedule of discrete diffusion models. In Proceedings of the International Conference on Learning Representations (ICLR), 2025. Suraj Patil, William Berman, Robin Rombach, and Patrick von Platen. amused: An open muse reproduction. arXiv preprint arXiv:2401.01808, 2024. 29 Lumina-DiMOO: An Omni Discrete Diffusion Model for Multi-Modal Generation and Understanding Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. In Proceedings of the International Conference on Learning Representations (ICLR), 2024. Yuandong Pu, Le Zhuo, Kaiwen Zhu, Liangbin Xie, Wenlong Zhang, Xiangyu Chen, Peng Gao, Yu Qiao, Chao Dong, and Yihao Liu. Lumina-omnilv: unified multimodal framework for general low-level vision. arXiv preprint arXiv:2504.04903, 2025. Qi Qin, Le Zhuo, Yi Xin, Ruoyi Du, Zhen Li, Bin Fu, Yiting Lu, Jiakang Yuan, Xinyue Li, Dongyang Liu, et al. Lumina-image 2.0: unified and efficient image generative framework. arXiv preprint arXiv:2503.21758, 2025. Liao Qu, Huichao Zhang, Yiheng Liu, Xu Wang, Yi Jiang, Yiming Gao, Hu Ye, Daniel Du, Zehuan Yuan, and Xinglong Wu. Tokenflow: Unified image tokenizer for multimodal understanding and generation. arXiv preprint arXiv:2412.03069, 2024. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In Proceedings of the International Conference on Machine Learning (ICML), 2021. Anton Razzhigaev, Arseniy Shakhmatov, Anastasia Maltseva, Vladimir Arkhipkin, Igor Pavlov, Ilya Ryabov, Angelina Kuts, Alexander Panchenko, Andrey Kuznetsov, and Denis Dimitrov. Kandinsky: An improved text-to-image synthesis with image prior and latent diffusion. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2023. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Subham Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin Chiu, Alexander Rush, and Volodymyr Kuleshov. Simple and effective masked diffusion language models. Advances in Neural Information Processing Systems (NeurIPS), 2024. Christoph Schuhmann, Robert Kaczmarczyk, Aran Komatsuzaki, Aarush Katta, Richard Vencu, Romain Beaumont, Jenia Jitsev, Theo Coombes, and Clayton Mullis. Laion-400m: Open dataset of clipfiltered 400 million image-text pairs. In Advances in Neural Information Processing Systems Workshops (NeurIPS Workshops), 2021. Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024a. Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024b. Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and universal control for diffusion transformer. arXiv preprint arXiv:2411.15098, 2024. Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. 30 Lumina-DiMOO: An Omni Discrete Diffusion Model for Multi-Modal Generation and Understanding Haofan Wang, Matteo Spinelli, Qixun Wang, Xu Bai, Zekui Qin, and Anthony Chen. Instantstyle: Free lunch towards style-preserving in text-to-image generation. arXiv preprint arXiv:2404.02733, 2024a. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024b. Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. Internvl3.5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025a. Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024c. Yibin Wang, Zhimin Li, Yuhang Zang, Yujie Zhou, Jiazi Bu, Chunyu Wang, Qinglin Lu, Cheng Jin, and Jiaqi Wang. Pref-grpo: Pairwise preference reward-based grpo for stable text-to-image reinforcement learning. arXiv preprint arXiv:2508.20751, 2025b. Cong Wei, Zheyang Xiong, Weiming Ren, Xeron Du, Ge Zhang, and Wenhu Chen. Omniedit: Building image editing generalist models through specialist supervision. In Proceedings of the International Conference on Learning Representations (ICLR), 2024. Xinyu Wei, Jinrui Zhang, Zeqing Wang, Hongyang Wei, Zhen Guo, and Lei Zhang. Tiif-bench: How does your t2i model follow your instructions? arXiv preprint arXiv:2506.02161, 2025. Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv preprint arXiv:2410.13848, 2024a. Chengyue Wu, Hao Zhang, Shuchen Xue, Zhijian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han, and Enze Xie. Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding. arXiv preprint arXiv:2505.22618, 2025a. Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025b. Junfeng Wu, Yi Jiang, Chuofan Ma, Yuliang Liu, Hengshuang Zhao, Zehuan Yuan, Song Bai, and Xiang Bai. Liquid: Language models are scalable and unified multi-modal generators, 2024b. Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, et al. Vila-u: unified foundation model integrating visual understanding and generation. Proceedings of the International Conference on Learning Representations (ICLR), 2025c. Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340, 2024. Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, et al. Sana: Efficient high-resolution image synthesis with linear diffusion transformers. Proceedings of the International Conference on Learning Representations (ICLR), 2025a. 31 Lumina-DiMOO: An Omni Discrete Diffusion Model for Multi-Modal Generation and Understanding Enze Xie, Junsong Chen, Yuyang Zhao, Jincheng Yu, Ligeng Zhu, Chengyue Wu, Yujun Lin, Zhekai Zhang, Muyang Li, Junyu Chen, et al. Sana 1.5: Efficient scaling of training-time and inference-time compute in linear diffusion transformer. Proceedings of the International Conference on Machine Learning (ICML), 2025b. Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. In Proceedings of the International Conference on Learning Representations (ICLR), 2025c. Yi Xin, Juncheng Yan, Qi Qin, Zhen Li, Dongyang Liu, Shicheng Li, Victor Shea-Jay Huang, Yupeng Zhou, Renrui Zhang, Le Zhuo, et al. Lumina-mgpt 2.0: Stand-alone autoregressive image modeling. arXiv preprint arXiv:2507.17801, 2025a. Yi Xin, Le Zhuo, Qi Qin, Siqi Luo, Yuewen Cao, Bin Fu, Yangfan He, Hongsheng Li, Guangtao Zhai, Xiaohong Liu, et al. Resurrect mask autoregressive modeling for efficient and scalable image generation. arXiv preprint arXiv:2507.13032, 2025b. Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, and Mengdi Wang. Mmada: Multimodal large diffusion language models. Advances in Neural Information Processing Systems (NeurIPS), 2025. Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, and Yujiu Yang. Maniqa: Multi-dimension attention network for no-reference image quality assessment. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Dream 7B, 2025a. URL https://hkunlp.github.io/blog/2025/dream. Yang Ye, Xianyi He, Zongjian Li, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Bohan Hou, and Li Yuan. Imgedit: unified image editing dataset and benchmark. arXiv preprint arXiv:2505.20275, 2025b. Mingyang Yi, Aoxue Li, Yi Xin, and Zhenguo Li. Towards understanding the working mechanism of text-to-image diffusion model. Advances in Neural Information Processing Systems (NeurIPS), 2024. Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. survey on multimodal large language models. National Science Review, 2024. Zebin You, Shen Nie, Xiaolu Zhang, Jun Hu, Jun Zhou, Zhiwu Lu, Ji-Rong Wen, and Chongxuan Li. LLaDA-V: Large language diffusion models with visual instruction tuning. arXiv preprint arXiv:2505.16933, 2025. Qifan Yu, Wei Chow, Zhongqi Yue, Kaihang Pan, Yang Wu, Xiaoyang Wan, Juncheng Li, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. Anyedit: Mastering unified high-quality image editing for any idea. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2025a. Runpeng Yu, Xinyin Ma, and Xinchao Wang. Dimple: Discrete diffusion multimodal large language model with parallel decoding. arXiv preprint arXiv:2505.16990, 2025b. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 32 Lumina-DiMOO: An Omni Discrete Diffusion Model for Multi-Modal Generation and Understanding Liang Zhang, Anwen Hu, Haiyang Xu, Ming Yan, Yichen Xu, Qin Jin, Ji Zhang, and Fei Huang. Tinychart: Efficient chart understanding with visual token merging and program-of-thoughts learning. arXiv preprint arXiv:2404.16635, 2024. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2023. Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Yichi Zhang, Ziyu Guo, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei, Shanghang Zhang, et al. Mavis: Mathematical visual instruction tuning. Proceedings of the International Conference on Learning Representations (ICLR), 2025. Haozhe Zhao, Xiaojian Shawn Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. Ultraedit: Instruction-based fine-grained image editing at scale. Advances in Neural Information Processing Systems (NeurIPS), 2024. Mingyu Zheng, Xinwei Feng, Qingyi Si, Qiaoqiao She, Zheng Lin, Wenbin Jiang, and Weiping Wang. Multimodal table understanding. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 91029124, 2024. Chunting Zhou, LILI YU, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. In Proceedings of the International Conference on Learning Representations (ICLR), 2025. Fengqi Zhu, Rongzhen Wang, Shen Nie, Xiaolu Zhang, Chunwei Wu, Jun Hu, Jun Zhou, Jianfei Chen, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. LLaDA 1.5: Variance-reduced preference optimization for large language diffusion models. arXiv preprint arXiv:2505.19223, 2025a. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025b. Yichen Zhu, Minjie Zhu, Ning Liu, Zhicai Ou, Xiaofeng Mou, and Jian Tang. Llava-phi: Efficient multi-modal assistant with small language model. arXiv preprint arXiv:2401.02330, 2024. Le Zhuo, Ruoyi Du, Han Xiao, Yangguang Li, Dongyang Liu, Rongjie Huang, Wenze Liu, Lirui Zhao, Fu-Yun Wang, Zhanyu Ma, et al. Lumina-next: Making lumina-t2x stronger and faster with next-dit. Advances in Neural Information Processing Systems (NeurIPS), 2024. Le Zhuo, Liangbing Zhao, Sayak Paul, Yue Liao, Renrui Zhang, Yi Xin, Peng Gao, Mohamed Elhoseiny, and Hongsheng Li. From reflection to perfection: Scaling inference-time optimization for text-toimage diffusion models via reflection tuning. Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2025."
        }
    ],
    "affiliations": [
        "Nanjing University",
        "Shanghai AI Laboratory",
        "Shanghai Innovation Institute",
        "Shanghai Jiao Tong University",
        "The Chinese University of Hong Kong",
        "The University of Sydney",
        "Tsinghua University"
    ]
}