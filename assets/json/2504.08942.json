{
    "paper_title": "AgentRewardBench: Evaluating Automatic Evaluations of Web Agent Trajectories",
    "authors": [
        "Xing Han Lù",
        "Amirhossein Kazemnejad",
        "Nicholas Meade",
        "Arkil Patel",
        "Dongchan Shin",
        "Alejandra Zambrano",
        "Karolina Stańczak",
        "Peter Shaw",
        "Christopher J. Pal",
        "Siva Reddy"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Web agents enable users to perform tasks on web browsers through natural language interaction. Evaluating web agents trajectories is an important problem, since it helps us determine whether the agent successfully completed the tasks. Rule-based methods are widely used for this purpose, but they are challenging to extend to new tasks and may not always recognize successful trajectories. We may achieve higher accuracy through human evaluation, but the process would be substantially slower and more expensive. Automatic evaluations with LLMs may avoid the challenges of designing new rules and manually annotating trajectories, enabling faster and cost-effective evaluation. However, it is unclear how effective they are at evaluating web agents. To this end, we propose AgentRewardBench, the first benchmark to assess the effectiveness of LLM judges for evaluating web agents. AgentRewardBench contains 1302 trajectories across 5 benchmarks and 4 LLMs. Each trajectory in AgentRewardBench is reviewed by an expert, who answers questions pertaining to the success, side effects, and repetitiveness of the agent. Using our benchmark, we evaluate 12 LLM judges and find that no single LLM excels across all benchmarks. We also find that the rule-based evaluation used by common benchmarks tends to underreport the success rate of web agents, highlighting a key weakness of rule-based evaluation and the need to develop more flexible automatic evaluations. We release the benchmark at: https://agent-reward-bench.github.io"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 2 4 9 8 0 . 4 0 5 2 : r Preprint. Under review. AGENTREWARDBENCH: Evaluating Automatic Evaluations of Web Agent Trajectories Xing Han Lù 1 2 Amirhossein Kazemnejad * 2 Nicholas Meade 1 2 Arkil Patel 1 2 Dongchan Shin 2 Alejandra Zambrano 2 Karolina Sta nczak 1 2 Peter Shaw 4 Christopher J. Pal 2 5 6 7 Siva Reddy 1 2 5 7 *Core contributor 1McGill University 2Mila Quebec AI Institute 4Google DeepMind 5Canada CIFAR AI Chair 6Polytechnique Montréal 7ServiceNow Research xing.han.lu@mail.mcgill.ca; siva.reddy@mila.quebec"
        },
        {
            "title": "Abstract",
            "content": "Web agents enable users to perform tasks on web browsers through natural language interaction. Evaluating web agents trajectories is an important problem, since it helps us determine whether the agent successfully completed the tasks. Rule-based methods are widely used for this purpose, but they are challenging to extend to new tasks and may not always recognize successful trajectories. We may achieve higher accuracy through human evaluation, but the process would be substantially slower and more expensive. Automatic evaluations with LLMs may avoid the challenges of designing new rules and manually annotating trajectories, enabling faster and cost-effective evaluation. However, it is unclear how effective they are at evaluating web agents. To this end, we propose AGENTREWARDBENCH, the first benchmark to assess the effectiveness of LLM judges for evaluating web agents. AGENTREWARDBENCH contains 1302 trajectories across 5 benchmarks and 4 LLMs. Each trajectory in AGENTREWARDBENCH is reviewed by an expert, who answers questions pertaining to the success, side effects, and repetitiveness of the agent. Using our benchmark, we evaluate 12 LLM judges and find that no single LLM excels across all benchmarks. We also find that the rule-based evaluation used by common benchmarks tends to underreport the success rate of web agents, highlighting key weakness of rule-based evaluation and the need to develop more flexible automatic evaluations. We release the benchmark at: https://agent-reward-bench.github.io"
        },
        {
            "title": "Introduction",
            "content": "Giving Large Language Model (LLM) access to web browser unlocks an entirely new capability paradigm: beyond interacting with user through chat interface, such models can interact with the online world to complete tasks similar to how human would. The promise of new paradigm has motivated the design of LLMs to control interfaces such as web browsers, starting from earlier foundation models such as ACT-1 (Adept, 2022) to the more recent OpenAI Operator (OpenAI, 2025) and Claude Computer use (Anthropic, 2024a), showing promising results in real-world tasks (Zhou et al., 2024). To measure the progress of web agents, well-designed benchmark should compile collection of realistic tasks across diverse websites. As illustrated in Figure 1, user may ask the agent to locate Classifieds listing for Google Pixel phone and submit an offer via comment. Inside dedicated environment (e.g., self-hosted Classifieds site), the web agent would complete the task by filling the search bar, identifying the correct listing, and writing comment to show interest in purchasing the item. To determine if the agent successfully completed the request, we need to automatically evaluate the agents chosen actions known as trajectories using set of rules uniquely designed for the task of finding Pixel phone on Classifieds. As expected, rule-based evaluation is time-consuming for 1 Preprint. Under review. Figure 1: Example from AGENTREWARDBENCH, where an LLM judge evaluates web agents trajectory on VisualWebArena (Koh et al., 2024). The benchmark compares judgments against expert annotations to determine the effectiveness of the judge for evaluating web agents. experts to design, and may not cover every successful scenario (e.g., what if the agent finds different but valid listing?). It is also possible for an expert to annotate the trajectories, but it would be slow and expensive to scale across many web agents. This brings us to the following questions: Given web agent trajectory, can an LLM decide if it is successful? If so, how do we determine which LLM is the most capable at evaluating web agents? Past works have shown that LLMs can be used as judges to evaluate the output of LLM chatbots (Zheng et al., 2023). More recently, LLM judges have been used for automatically evaluating trajectories from web agents (Pan et al., 2024; Murty et al., 2025; Trabucco et al., 2025). With highly accurate automatic evaluation methods, we can measure the progress of web agents on new sets of tasks, use them to synthesize trajectories for finetuning smaller models, and design reward models that can be used in reinforcement learning (RL) setting. However, it remains unclear whether current automatic evaluators, whether rule-based or LLM-based, can predict the success of trajectory in way that reflects expert judgment. To address this problem, we introduce AGENTREWARDBENCH (3), benchmark for determining the capability of an LLM at evaluating web agents (see Figure 1). It consists of 1300 trajectories produced by 4 popular LLM agents on 5 diverse web environments, ranging from common tasks like online shopping and posting on forum, to highly specialized requests in professional environments, such as updating task schedules on IT task management platforms. Each trajectory is labeled by expert annotators to determine whether the agent successfully completed the task, caused unintended side effects, or entered cycles of repetitive actions. Using this benchmark, we evaluate both existing and novel LLM judges (4) alongside rule-based evaluation. We find that rule-based methods, which are used as the official automatic evaluation by environment-based benchmarks, severely underestimate the capabilities of agents and do not reflect how experts define success (5). We further provide an in-depth analysis (6) that highlights the weaknesses of existing LLMs when used as judges. Overall, we believe AGENTREWARDBENCH can be used to enable better automatic evaluation and reward modeling for web agents."
        },
        {
            "title": "2 Related Works",
            "content": "Web Agents and Environments Designing agents that can automatically navigate user interfaces has been long standing problem; earlier approaches employed program-based heuristics (St. Amant & Zettlemoyer, 2000), whereas later works on web navigation focus on training reinforcement learning (RL) models (Gur et al., 2018; Humphreys et al., 2022), 2 Preprint. Under review. language models (Nakano et al., 2021; Gur et al., 2023; Deng et al., 2023) and multimodal models (Shaw et al., 2023; Lù et al., 2024; Zheng et al., 2024). To measure the advancements in web agents, various benchmarks have been proposed, with initial works proposing simplified environments (Shi et al., 2017; Liu et al., 2018) and subsequent iterations focusing on specific tasks like web shopping (Yao et al., 2022). More recent benchmarks focus on designing realistic environments that cover commonly used websites (Zhou et al., 2024; Koh et al., 2024) as well as specialized environments (Drouin et al., 2024; Boisvert et al., 2025). LLM Judges Zheng et al. (2023) proposed using LLMs to predict human preferences of dialogue completion for chat models. They show that GPT-4-based judge achieves over 80% agreement with human votes on the task of selecting better completions between models pairs. He et al. (2024) extend the idea by using LLMs to judge trajectories from web agents, allowing them to determine task completion without human annotators, resulting in high correlation with humans on private subset of trajectories. To determine the quality of automatic judgments, Pan et al. (2024) evaluate four LLM judges using trajectories from GPT-4 agent on WebArena tasks, and find that the best judge achieves 80.6% accuracy against the rule-based evaluator from WebArena. Unlike prior works on LLM judges, we design AGENTREWARDBENCH with trajectories from several LLM agents on diverse web benchmarks, where each one is annotated by human experts on multiple dimensions. By following human-focused approach similar to Lambert et al. (2024), we ensure that LLM judges are evaluated against expert preferences on wide range of scenarios. Trajectory Synthesis Leveraging web environments that can be created and reset without real-world impact, recent works started to explore generating trajectories without human supervision. Leveraging LLM judges and LLM-generated tasks, trajectory synthesis can be used to bootstrap agent-judge training loops (Murty et al., 2024; 2025), to create contrastive pairs (Putta et al., 2024) for direct preference optimization (Rafailov et al., 2023), or as training data to finetune base model (Lai et al., 2024; Patel et al., 2024; Trabucco et al., 2025). Although all the methods leverage an LLM judge, they lack clear way of directly determining the quality of judgments, instead relying on the downstream performance improvement to validate their approach. To this end, AGENTREWARDBENCH enables researchers to choose the most appropriate LLM judge for category of web tasks based on their effectiveness at evaluating web agents."
        },
        {
            "title": "3 AGENTREWARDBENCH",
            "content": "In this work, we introduce AGENTREWARDBENCH, benchmark designed to assess the capabilities of LLM judges for evaluating web agents (3.1). We curate 5 diverse web environments and tasks (3.2) in order to collect trajectories from web agents based on 4 LLMs (3.3). For each trajectory, team of expert annotators carefully reviews the screenshots, actions, and the agents reasoning chains before labeling them as either successful or unsuccessful, alongside other auxiliary labels (see Figure 2). Finally, we evaluate LLM judges  (Table 1)  by comparing their predictions with expert annotations to determine their effectiveness for automatic evaluation."
        },
        {
            "title": "3.1 Assessment Framework",
            "content": "Trajectory Definition Let oi be an observation of browser at time step i, ai be an action that can be executed on webpage through browser navigation engine such that oi+1 = B(oi, ai), and ri be the reasoning for choosing the action. We define web agent trajectory as the sequence = {o1, (r1, a1), o2, (r2, a2), . . . , on1, (rn1, an1), on} where on is the final observation in the trajectory. Each observation contains screenshot of the browser si, the Document Object Model (DOM) tree representation of the browser, and an accessibility (A11Y) tree rendered from the DOM tree. For the observation to be useful for an LLM agent, we need representation function that produces pi = R(o1, r1, a1, . . . , oi), which can be used as an input for an LLM. If the agent is multimodal, oi would include screenshots; otherwise, it would be textual representation of the page (e.g., accessibility tree). Then, pi is given to language model to produce completion ci = LM(pi), or 3 Preprint. Under review. Figure 2: AGENTREWARDBENCH creation process. We first collect trajectories from LLM agents inside web environments using instructions from several benchmarks. Then, the trajectories are reviewed by expert annotators, who indicate if the trajectory is successful, led to side effects, and contains repetition cycles. Finally, we use the annotated trajectories to evaluate LLM judges. ci = VLM(pi, si) in the case of multimodal LLM. The completion is parsed by an execution function to produce (ai, ri) = E(ci). Annotation Design For each trajectory, an expert annotator reviews goal and sequence {s1, (r1, a1), . . . , sn1, (rn1, an1), sn} in order to answer questions = {q1, . . . , qm}. We consider the answers produced, = {a 1, . . . , m}, as the ground truth annotations for the trajectory, which indicate whether the agent successfully completed g. To collect A, we use the following in the annotation guidelines: 1. Success: Was the sequence of actions successful in achieving the goal? 2. Side Effect: Did the agent perform unnecessary actions that could lead to unintended side effects? 3. Repetition Cycle: Did the agent loop through sequence of actions that did not make progress towards the goal? Agreement with respect to success is the primary criterion with which we evaluate LLM judges. The remaining can be useful as auxiliary criteria for detecting issues ahead of time. For example, if an agent purchases several irrelevant products when the user only requested one, then the trajectory would be flagged for side effects, independent of task success. judge can also indicate the presence of cycle, for example, if the agent repeatedly clicks on disabled button. Both signals can be used to penalize the agent during training or steer it to another action at inference. Annotation Setup The team of annotators consisted of 6 experts with deep understanding of the tasks and environments through their research on web agents. They used custombuilt user interface that displays each trajectory with screenshots, actions, and reasoning. Rating disagreements were resolved by annotators discussing among themselves until clear annotations can be produced for ambiguous trajectories. Moreover, the annotators also have access to the environment and accessibility trees when screenshots are insufficient. Judge Model Given goal g, trajectory and questions Q, judge model returns judgˆA, which is an estimate of A. We can use ˆA to derive reward in RL or to automatiment cally evaluate web agents when is unavailable. To implement the judge, we need judgespecific function Rj that produces representation of the trajectory, = Rj(o1, r1, a1, . . . , on). Rj= can vary substantially, ranging from simple list of actions a1, . . . , an1, to using another LLM to process the observation history. We describe judges used in previous works and introduce simplified judge in Section 4 and provide supplementary details in Appendix A.3."
        },
        {
            "title": "3.2 Tasks and Environments",
            "content": "We select 5 benchmarks designed to evaluate web agents inside dedicated environments and real websites, including general-purpose (Zhou et al., 2024), vision-focused (Koh et al., 2024), real-world (Yoran et al., 2024), and enterprise-oriented (Drouin et al., 2024; Boisvert 4 Preprint. Under review. et al., 2025) tasks. In total, we curate 351 unique tasks across 8 environments and 66 websites, which we separate into 51 development and 300 test tasks (details in Appendix A.1). WebArena (WA; Zhou et al. 2024) This benchmark comprises 6 self-hosted websites covering wide range of domains: customer relationship management, map navigation, online encyclopedia, shopping site, social forum, and software development collaboration platform. Each environment is derived from real open-source projects that develop selfhosted environments for both commercial and personal usage. Each task consists of textual goal that requires good understanding of one or multiple environments to complete. VisualWebArena (VWA; Koh et al. 2024) To complement WebArenas text-based goals, we also include VisualWebArena (VWA), benchmark focusing on tasks that require visual reasoning to complete. For instance, user may include an image alongside the goal, or the task could be designed to only be solved if the agent selects an item with unique visual characteristic (e.g., purchasing TV with the widest bezel). VWA also introduces new online marketplace environment (Classifieds). AssistantBench (AB; Yoran et al. 2024) In addition to the self-hosted environments, we consider trajectories resulting from agent execution on real-world websites. This benchmark defines tasks that require navigating the internet, starting from search engine. Since the test set is private, we use the validation set, which consists of 33 unique tasks. WorkArena (Work; Drouin et al. 2024) and WorkArena++ (Wk++; Boisvert et al. 2025) To increase the diversity of tasks relevant to professional environments, we incorporate WorkArena (Boisvert et al., 2025), benchmark of 18 basic tasks on ServiceNow,1 softwareas-a-service platform for professional workflows in the information technology (IT), human resources, and customer management domains. WorkArena++ introduces tasks with greater complexity, requiring planning and reasoning to correctly complete multiple sub-tasks. Including this alongside WorkArena allows us to evaluate judges on wider range of task difficulty. We focus on the Level 2 tasks since Level 3 is too challenging for current agents."
        },
        {
            "title": "3.3 Web Agents Design",
            "content": "To collect trajectories on the 5 benchmarks, we design web agents using two models from major commercial providers and two open-weight LLMs. LLM backbones On the commercial side, we use OpenAIs GPT-4o2 (Hurst et al., 2024) and Anthropics Claude 3.7 Sonnet (Anthropic, 2024b). They are the flagship models of their respective providers, both of which offer computer-use agents powered by their LLMs, namely OpenAI Operator (OpenAI, 2025) and Anthropic Claudes Computer use (Anthropic, 2024a). We select two leading open-weights LLMs to complement the commercial LLMs: Llama-3.3-70B (Grattafiori et al., 2024) and Qwen2.5-VL (Bai et al., 2025). In both cases, we choose the instruction-tuned variant, which have undergone post-training for tool-use or UI navigation. Moreover, since Llama-3.3 is text-only model, it was excluded from VisualWebArena, which requires image-based reasoning. Agent Platform By default, each LLM backbone receives an input processed by representation function and generates completion ci. Then, ci is interpreted as an action by an execution function E. To implement E, we use AgentLab and BrowserGym (Chezelles et al., 2025), an ecosystem for designing web agents using LLMs (details in Appendix A.1). Trajectory Annotations and Splits We collect total of 1302 trajectories from our 4 LLMbased web agents across five benchmarks. Based on the task split (3.2), 196 trajectories are in the development split and 1106 are in the test split (details in A.2). The annotators follow the process described in Section 3.1 to label all trajectories, producing total of 3906 binary annotations. To assess agreement between annotators, we annotated the GPT-4o agents trajectory on WebArena with second annotator. We obtained an inter-annotator agreement of 89.3% on success, indicating high level of consistency among annotators. 1https://developer.servicenow.com 2We use the version gpt-4o-2024-11-20 Preprint. Under review."
        },
        {
            "title": "Official",
            "content": "Rule-based*"
        },
        {
            "title": "Existing",
            "content": "Ours (A) Ours (S) AER-C AER-V NNetNav Claude 3.7 S. GPT-4o GPT-4o Mini Llama 3.3 Qwen2.5-VL Claude 3.7 S. GPT-4o GPT-4o Mini Qwen2.5-VL"
        },
        {
            "title": "Overall\nPrecision Recall",
            "content": "83.8 67.7 67.6 52.5 68.8 69.8 61.5 67.7 64.3 69.4 68.1 64.5 64.5 55.9 71.9 71.5 82. 81.6 83.1 86.1 79.0 89.8 76.3 80.3 78.3 86.1 F1 67.1 69.7 69.5 64.1 74.7 75.9 71.7 72.9 75. 72.7 73.7 70.8 73.7 AB VWA WA Work Wk++"
        },
        {
            "title": "Precision",
            "content": "25.0 83.3 83.3 20.8 87.5 77.8 80.0 75.0 72.7 71.4 77.8 80.0 70.0 85.2 56.0 61.2 54. 61.0 63.0 57.9 59.6 59.3 64.8 60.7 57.4 58.5 79.0 68.8 67.6 54.3 69.3 70.2 63.5 68.2 63.6 69.3 69.9 66.9 62. 100.0 100.0 96.4 77.3 85.0 94.6 84.2 94.3 87.2 85.3 93.8 90.3 93.8 83.3 66.7 59.3 43. 66.7 63.0 49.4 62.7 60.3 66.7 59.6 54.8 64.4 Table 1: Judge performance for predicting success, measured with precision (4.2). We report recall and F1 as auxiliary scores. We examine two variants of the simplified judge: one with the final accessibility tree (A), and the other with the final screenshot (S). *Rule-based evaluation are included for reference."
        },
        {
            "title": "4 LLM judges for web tasks",
            "content": "4."
        },
        {
            "title": "Judge implementations",
            "content": "We consider two existing implementations of LLM judges for web agents, Agent Eval Refine (AER; Pan et al. 2024) and NNetNav (Murty et al., 2025), and introduce simplified judge that simultaneously predicts success, side effects, and repetition. Other LLM judge variants were proposed (He et al., 2024; Putta et al., 2024; Lai et al., 2024; Trabucco et al., 2025), but our three judge implementations cover major strategies for representing trajectories. AER (Pan et al., 2024) The judge in this framework takes as input the sequence of agent thoughts and actions alongside the final browser state, which is either passed to visionenabled model as screenshot (AER-V) or as caption generated by captioner model (AER-C). Then, the judge outputs its reasoning before predicting success or failure. For both the judge and captioner, we implement this method using GPT-4o, which is an overall stronger model than the GPT-4 (Achiam et al., 2023) model originally used. NNetNav (Murty et al., 2025) In this work, Llama 3.1 70B judge receives summary of changes across all observations and has to give rating between 1 (worst) and 5 (best) after providing the thought process; the rating is binarized by thresholding at 4, based on the original implementation. To generate summaries, an LLM is used to describe the change between two observations based on the accessibility trees instead of screenshots. We use Llama 3.3 70B (Al-Dahle, 2024), an improved version of the original backbone. Simplified judge (ours) We propose simplified design for our judge. First, it directly answers the three questions asked to the annotators. This allows it to return multiple labels within single completion. Then, we decouple the system prompt and reasoning chain from the final state representation, allowing the judge to receive either the accessibility tree or the screenshot. This differs from AER, which requires vision-enabled model, and NNetNav, which requires long-context model capable of receiving multiple accessibility trees. Our method is compatible with both multimodal and text-only LLMs and does not require separate LLM to caption the screenshot or summarize changes across observations."
        },
        {
            "title": "4.2 Evaluation",
            "content": "To evaluate LLM judges, we use the precision score, which is the ratio of true positives over all predicted positives (true + false positives). The metric is good fit for rejection finetuning (RFT), where we are interested in increasing the number of true positives (actual successful 6 Preprint. Under review. A11Y Screen"
        },
        {
            "title": "Success\nR",
            "content": "81.7 86.1 78.3 73.9 F1 70.6 71.7 70.8 66.7 62.1 61.5 64.5 60."
        },
        {
            "title": "Side Effect\nR",
            "content": "F1 31.9 70.8 31.9 76.4 10.8 13.0 11.0 13.2 6.5 7.2 6.6 7."
        },
        {
            "title": "Repetition\nR",
            "content": "P F1 92.5 78.6 92.3 78.1 16.8 46.4 18.5 59.1 28.4 58.3 30.8 67.3 Table 2: Ablation study of our GPT-4o mini judge, measured in precision (P), recall (R), and F1. We consider how including accessibility trees and screenshots in the input affects the predictions. trajectories) while reducing the number of false positives (failed trajectories added to the dataset due to poor LLM judgments). For reward modeling, we also want to prioritize true positives since they are the primary signals for many RL algorithms, while false positives should be minimized to avoid introducing noise to the loss function. Moreover, recall and F1 benefit from minimizing false negatives, which is useful for improving sample efficiency by reducing the number of valid trajectories removed; we report them as auxiliary metrics. 4."
        },
        {
            "title": "Judge Performance",
            "content": "In Table 1, we provide an overview of the performance of judges across benchmarks using the metrics defined in Section 4.2. We find that GPT-4o and Claude 3.7 Sonnet-based simplified judges achieve higher precision compared to prior approaches, indicating that removing the internal LLMs for captioning or summarizing changes does not hinder their capabilities. Notably, no judge consistently stands out across benchmarks, highlighting the importance of selecting an appropriate LLM backbone based on the nature of the task. Low precision limits existing judges We notice that no judge achieves above 70% precision, which means that 30% of trajectories are erroneously marked as successful. This severely limits the usefulness of the judges for downstream applications, such as using the filtered trajectories for finetuning an agent, as the agent will learn to generate incorrect trajectories for substantial portion of the tasks. This indicates LLM judges are currently not reliable way of assessing the true capabilities of agents. Consequently, judges will need to achieve higher precision before they are useful for automatic evaluation, which also affects their downstream utility for methods like RFT and RL. Official rule-based evaluation underestimates success Similar to LLM judges, the rulebased evaluation used by benchmarks can be compared with expert annotations. Since they use task-specific configurations to determine success, they may reject successful trajectories due to inconsequential differences. For instance, in WebArena, if user asks \"Whats the closest national park to the largest city in Maine?\", the agent may reply: \"The closest national park to Portland [...] is Acadia National Park\". Rule-based evaluation considers it unsuccessful since the configuration requires it to exactly match \"Acadia National Park\". As result, the rule-based approach achieves recall of 55.9%, indicating higher rate of false negatives compared to LLM judges. Overall, substantial precision gap exists between rulebased methods and LLM judges, but rule-based methods severely underestimate the true performance of web agents, highlighting the need for more flexible automatic evaluation. Impact of Input Representation Browser screenshots represent an intuitive state for humans, but LLMs may need more than vision alone, as screenshots miss page structure and hidden attributes found in accessibility trees. To investigate the impact of different representations, we ablate our GPT-4o-mini simplified judge in Table 2. We observe that only including screenshots achieves high precision for success and repetition, whereas only including accessibility trees allows higher recall. Surprisingly, including both accessibility trees and screenshots yields lower performance than including only the screenshot, indicating that more information distracts rather than assists the judge. 7 Preprint. Under review."
        },
        {
            "title": "Agent",
            "content": "Claude 3.7 S. GPT-4o Llama 3.3 Qwen2.5-VL Human VWA WA Wk++ GPT-4o Judge VWA WA Wk++ Rule-based VWA WA Wk++ 28.3 35.9 0.0 21.7 55.1 42.3 22.4 33. 18.4 18.4 9.2 13.8 34.8 47.8 0.0 34.8 64.1 50.0 27.6 52.6 20.7 11.5 5.8 14.9 23.9 17.4 0.0 17.4 30.8 25.6 18.4 29. 8.1 4.6 3.5 11.5 Table 3: Success Rate of web agents measured by expert annotators, GPT-4o Judge (with accessibility tree) and rule-based evaluation on various benchmarks (3.2). Results by environment are in Table 6."
        },
        {
            "title": "5 Revisiting how we evaluate task success rate",
            "content": "One of the core applications of LLM judges is to estimate the success rate on web navigation benchmark, which is useful in scenarios where there are no dedicated functions to calculate the rule-based success rate, which is the standard evaluation for many web agent benchmarks. However, rule-based approaches may not always agree with experts. In Table 3, we compare the success rate calculated from expert annotations, rule-based evaluation, and GPT-4o judge with accessibility trees. Rule-based evaluation does not reflect expert-defined success rates We notice stark difference between the judge and rule-based approach: whereas the LLM judge tends to overestimate the success rate of every agent (with two exceptions in WorkArena++), rule-based methods consistently underestimate it. Moreover, the underestimation varies substantially, with the performance of GPT-4o being 16.7% lower on WebArena and 18.5% lower on VWA compared to expert annotations. This highlights major discrepancy between the official task success rate reported by rule-based methods and the success rate according to expert annotators. For instance, rule-based evaluation ranks Qwen2.5-VL above GPT-4o on WebArena and WorkArena++ (and equally on VWA), whereas expert annotators prefer GPT-4o over Qwen2.5-VL on all benchmarks, with over 14% higher success rate on VWA. Overall, this stresses the need to develop new methods to calculate task success rate that more precisely reflect expert judgments."
        },
        {
            "title": "6 Error Analysis",
            "content": "In this section, we qualitatively examine failure cases of LLM judges. Following GPT-4o judge, we focus on the common error categories to understand how LLM judges fail. Grounding mismatch When an agent misunderstands what is happening on the screen, its thought process may not reflect the reality of the webpage. In such cases, judge without access to the screenshots may produce an erroneous judgment due to the agent misunderstanding what is happening on the screen. For example, when user requests to show the product page for the item in the second row, second column, the Qwen2.5-VL agent ends up selecting the second item in the first row. However, it writes in its reasoning chain that Based on the layout of the page, the second row, second column item is the [energy Drink]. The judge accepts the agents thought process without realizing it opened the wrong page: The agents goal was to navigate to the product page for the item in the second row, second column. The agent successfully reached the correct product page. Misleading agent reasoning The agent may have misleading elements in its reasoning, leading the judge to reason that the agent completed the task correctly. In multi-step WorkArena++ task, the user requested the agent to apply search filter to include unique ID. After several unsuccessful attempts, the agent ended up stating it succeeded in its reasoning chain, even though no filter was applied. The judge was misled by the agent and wrote in its own reasoning chain that The agent successfully [...] applied the filter to extract entries... before incorrectly indicating that the trajectory was successful. Missed instruction details In some cases, the agent does not complete the task completely, missing crucial details from the instruction (see Figure 3). For example, when the user re8 Preprint. Under review. Figure 3: Example of judge error (discussed in Section 6). In this example, the user requests the agent to buy cereal in VisualWebArena, but the agent stops after adding it to the cart. The judge erroneously identified the trajectory as successful, even though the agent missed an important detail in the instruction. quests to buy the cheapest cereal with graphic character on the box in the Cereals category, the agent finds the correct product and informs the customer: Ive found the cheapest cereal with graphic character on the box. Its Cocoa Puffs, 10.4 Oz Box... However, it missed crucial detail: the user requested that they buy the product. Unfortunately, the judge mistakenly believes that the agent completed purchase: The agent successfully identified and purchased the cheapest cereal with graphic character on the box... Misunderstanding action intents We found that the agent may misuse certain actions, leading to the trajectory to fail very close to completion. In such cases, the LLM judge may incorrectly decide that the trajectory is successful and ignore the misused action. In one instance where the goal was to answer What is the minimum travel time by car from Animal Rescue League of Pittsburgh to Schenley park?, the Qwen2.5-VL agent completes all required actions, but ends up reporting the task as unfeasible instead of replying to the user. The GPT-4o judge (with screenshot) correctly reasons that the travel time was shown on the screen, but does not point out that reporting the task as unfeasible is incorrect, instead asserting that all actions were confined to the task of finding the travel time. Overall, current LLM judges are limited by their capability to detect nuanced issues within trajectories, as shown by the judge missing details and misunderstanding an action. Moreover, they will easily agree with the agents reasoning even when it is wrong, which has been previously observed in LLMs (Sharma et al., 2023). Future research should aim to address these issues to improve the performance of LLM judges for evaluating web agents."
        },
        {
            "title": "7 Conclusion",
            "content": "We introduce AGENTREWARDBENCH, benchmark for evaluating LLM judges for web agent trajectories. The benchmark consists of over 1300 trajectories, each annotated by experts across three dimensions: whether the agent succeeded, whether it caused unintended side effects, and whether it repeated unnecessary actions. We evaluate 12 LLM judges on AGENTREWARDBENCH and find that simpler input representation can achieve higher agreement with expert annotators compared to prior approaches. Moreover, we find that rule-based evaluation, often used by environment-based benchmarks, does not achieve lower-than-expected agreement with experts. Instead, it tends to reject many valid trajectories, which results in the success rate of certain web agents being lower than what an expert would perceive. Overall, we believe our benchmark will help researchers design better LLM judges for web agents trajectories, which will enable the design of automatic evaluators and reward models that better reflect expert judgments. 9 Preprint. Under review."
        },
        {
            "title": "Acknowledgments",
            "content": "Xing Han Lù acknowledges the support of the Natural Sciences and Engineering Research Council of Canada (NSERC) [funding reference no. 579403]. The project is supported by the Google-Mila grant. We thank Alexandre Lacoste, Shikhar Murty, and the McGill NLP group members for helpful discussions."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. URL https://arxiv.org/ abs/2303.08774. Adept. Act-1: Transformer for actions, 2022. URL https://www.adept.ai/blog/act-1/. Ahmad Al-Dahle. The future of ai: Built with llama, December 2024. URL https://ai.meta. com/blog/future-of-ai-built-with-llama/. Anthropic. Introducing computer use, new claude 3.5 sonnet, and claude 3.5 haiku, 2024a. URL https://www.anthropic.com/news/3-5-models-and-computer-use. Anthropic. The claude 3 model family: Opus, sonnet, haiku, 2024b. URL https://api. semanticscholar.org/CorpusID:268232499. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025. URL https://arxiv.org/abs/2502.13923. Léo Boisvert, Megh Thakkar, Maxime Gasse, Massimo Caccia, Thibault Le Sellier De Chezelles, Quentin Cappart, Nicolas Chapados, Alexandre Lacoste, and Alexandre Drouin. Workarena++: Towards compositional planning and reasoning-based common knowledge work tasks, 2025. URL https://arxiv.org/abs/2407.05291. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016. URL https://arxiv.org/abs/1606.01540. Thibault Le Sellier De Chezelles, Maxime Gasse, Alexandre Drouin, Massimo Caccia, Léo Boisvert, Megh Thakkar, Tom Marty, Rim Assouel, Sahar Omidi Shayegan, Lawrence Keunho Jang, Xing Han Lù, Ori Yoran, Dehan Kong, Frank F. Xu, Siva Reddy, Quentin Cappart, Graham Neubig, Ruslan Salakhutdinov, Nicolas Chapados, and Alexandre Lacoste. The browsergym ecosystem for web agent research, 2025. URL https: //arxiv.org/abs/2412.05467. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards generalist agent for the web. Advances in Neural Information Processing Systems, 36:2809128114, 2023. Alexandre Drouin, Maxime Gasse, Massimo Caccia, Issam H. Laradji, Manuel Del Verme, Tom Marty, Léo Boisvert, Megh Thakkar, Quentin Cappart, David Vazquez, Nicolas Chapados, and Alexandre Lacoste. Workarena: How capable are web agents at solving common knowledge work tasks?, 2024. URL https://arxiv.org/abs/2403.07718. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint, 2024. URL https://arxiv.org/abs/2407.21783. Izzeddin Gur, Ulrich Rueckert, Aleksandra Faust, and Dilek Hakkani-Tur. Learning to navigate the web. arXiv preprint arXiv:1812.09195, 2018. Preprint. Under review. Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. real-world webagent with planning, long context understanding, and program synthesis. arXiv preprint arXiv:2307.12856, 2023. Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. Webvoyager: Building an end-to-end web agent with large multimodal models. ArXiv, abs/2401.13919, 2024. URL https://api.semanticscholar.org/CorpusID: 267211622. Peter Humphreys, David Raposo, Toby Pohlen, Gregory Thornton, Rachita Chhaparia, Alistair Muldal, Josh Abramson, Petko Georgiev, Alex Goldin, Adam Santoro, and Timothy Lillicrap. data-driven approach for learning to control computers, 2022. URL https://arxiv.org/abs/2202.08137. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks, June 2024. URL http: //arxiv.org/abs/2401.13649. arXiv:2401.13649 [cs]. Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, and Jie Tang. Autowebglm: large language model-based web navigating agent, 2024. URL https://arxiv.org/abs/2404. 03648. Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, and Hannaneh Hajishirzi. Rewardbench: Evaluating reward models for language modeling, 2024. URL https://arxiv.org/abs/2403.13787. Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. Reinforcement Learning on Web Interfaces using Workflow-guided Exploration. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings, 2018. URL https://openreview.net/forum? id=ryTp3f-0-. Xing Han Lù, Zdenˇek Kasner, and Siva Reddy. Weblinx: Real-world website navigation with multi-turn dialogue. arXiv preprint arXiv:2402.05930, 2024. Shikhar Murty, Christopher Manning, Peter Shaw, Mandar Joshi, and Kenton Lee. BAGEL: Bootstrapping Agents by Guiding Exploration with Language, June 2024. URL http: //arxiv.org/abs/2403.08140. arXiv:2403.08140 [cs]. Shikhar Murty, Hao Zhu, Dzmitry Bahdanau, and Christopher D. Manning. Nnetnav: Unsupervised learning of browser agents through environment interaction in the wild, 2025. URL https://arxiv.org/abs/2410.02907. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browserassisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021. URL https://openai.com/index/ Introducing operator, January 2025. OpenAI. introducing-operator. Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine, and Alane Suhr. Autonomous evaluation and refinement of digital agents, 2024. URL https://arxiv.org/ abs/2404.06474. Ajay Patel, Markus Hofmarcher, Claudiu Leoveanu-Condrei, Marius-Constantin Dinu, Chris Callison-Burch, and Sepp Hochreiter. Large language models can self-improve at web agent tasks, 2024. URL https://arxiv.org/abs/2405.20309. 11 Preprint. Under review. Pranav Putta, Edmund Mills, Naman Garg, Sumeet Motwani, Chelsea Finn, Divyansh Garg, and Rafael Rafailov. Agent q: Advanced reasoning and learning for autonomous ai agents, 2024. URL https://arxiv.org/abs/2408.07199. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R. Bowman, Newton Cheng, Esin Durmus, Zac Hatfield-Dodds, Scott R. Johnston, Shauna Kravec, Timothy Maxwell, Sam McCandlish, Kamal Ndousse, Oliver Rausch, Nicholas Schiefer, Da Yan, Miranda Zhang, and Ethan Perez. Towards understanding sycophancy in language models, 2023. URL https://arxiv.org/abs/2310.13548. Peter Shaw, Mandar Joshi, James Cohan, Jonathan Berant, Panupong Pasupat, Hexiang Hu, Urvashi Khandelwal, Kenton Lee, and Kristina Toutanova. From pixels to UI actions: Learning to follow instructions via graphical user interfaces. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id= 3PjCt4kmRx. Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. World of bits: An open-domain platform for web-based agents. In International Conference on Machine Learning, pp. 31353144. PMLR, 2017. Robert St. Amant and Luke S. Zettlemoyer. The user interface as an agent environment. In Proceedings of the Fourth International Conference on Autonomous Agents, AGENTS 00, pp. 483490, New York, NY, USA, 2000. Association for Computing Machinery. ISBN 1581132301. doi: 10.1145/336595.337575. URL https://doi.org/10.1145/336595.337575. Brandon Trabucco, Gunnar Sigurdsson, Robinson Piramuthu, and Ruslan Salakhutdinov. Towards internet-scale training for agents, 2025. URL https://arxiv.org/abs/2502. 06776. Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. WebShop: Towards Scalable Real-world Web Interaction with Grounded Language Agents. In NeurIPS, 2022. URL https://arxiv.org/abs/2207.01206. Ori Yoran, Samuel Joseph Amouyal, Chaitanya Malaviya, Ben Bogin, Ofir Press, and Jonathan Berant. Assistantbench: Can web agents solve realistic and time-consuming tasks?, 2024. URL https://arxiv.org/abs/2407.15711. Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v(ision) is generalist web agent, if grounded, 2024. URL https://arxiv.org/abs/2401.01614. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. URL https://arxiv.org/abs/2306.05685. Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. WebArena: Realistic Web Environment for Building Autonomous Agents, April 2024. URL http: //arxiv.org/abs/2307.13854. arXiv:2307.13854 [cs]. Preprint. Under review."
        },
        {
            "title": "A Benchmark",
            "content": "A.1 Environment and Experiments Details AssistantBench Although an unlimited number of websites can be visited, we observed that the agents visited total of 66 unique domains between 1 and 129 times across all trajectories we collected. The number of times domain was visited can be found in Table 4. Additionally, we replace the default search engine with an alternative search engine (https://duckduckgo.com) as the original homepage blocks browser automation, which renders the tasks unachievable. Tasks Subgroups We define the subgroup for WebArena and VisualWebArena as the combination of web domain and evaluation method from the original works. The evaluation methods consist of string matching, HTML-based programs, webpage image querying, and final URL matching. We randomly sample up to 8 tasks from each domain-evaluation group for WebArena, and up to 9 for VisualWebArena, since certain domain-evaluation groups have very small number of tasks. For WorkArena, we attempt to evenly distribute the task categories. As result, we have the following task distributions: WebArena: Wikipedia (8), Map (18), Reddit (18), Shopping Admin (18), Shopping (19), Gitlab (19) VisualWebArena: Wikipedia (17), Reddit (27), Classifieds (28), Shopping (28) WorkArena: Sophisticated memory (15), Information retrieval (20), Contextual understanding infeasible tasks (21), Planning and problem solving (22), Data driven decision making and reasoning (22) Agent Hyperparameters The binary flags used in AgentLab (Chezelles et al., 2025) are shown in Table 5. We set maximum limit of 40K input tokens and 8192 output tokens. Agent Platform Implementation In addition to abstracting websites and browser engines into Gym-compatible environments (Brockman et al., 2016), BrowserGym (Drouin et al., 2024; Chezelles et al., 2025) offers advanced preprocessing of complex web inputs (i.e., DOM and accessibility trees) and can automatically parse LLM output and execute them as browser actions like clicks, form inputs, tab actions, etc. Additionally, the BrowserGym ecosystem includes AgentLab, framework for processing input representation and managing web agent experiments. We use AgentLab to design our representation function R, ensuring unified hyperparameters and inputs. As result, we can avoid unintended differences that may arise from customizing prompts and representations for each LLM. A.2 Annotations Trajectory filtering In total, 351 tasks were considered across 5 benchmarks (33 in AssistantBench, 100 in VisualWebArena, 100 in WebArena, 18 in WorkArena, and 100 in WorkArena++). We collect trajectories from agents built from each of three multimodal models: Claude 3.7 Sonnet, GPT-4o, Qwen2.5-VL. Moreover, since Llama 3.3 is not multimodal, we only collect trajectories on 251 tasks (excluding VisualWebArena). Additionally, Llama 3.3 did not complete two WebArena tasks (nos. 735 and 805) due to timeout issues that consistently occurred in the environment, despite multiple attempts to restart. Thus, we obtain total of 1302 trajectories, where 196 are stored in the development split and 1106 in the test split. Interface To annotate the trajectories, we designed fully customized annotation interface using Gradio (see Figure 4). For selected agent and task, we displayed the goal and each of the steps of the trajectory taken by the model. It shows the models reasoning and action, as well as screenshot with the action element on overlay. Then, the annotators are prompted to answer series of questions regarding the success, side effects, and repetitiveness of the agent, using the same questions that we ask the LLM judges. 13 Preprint. Under review. Figure 4: User Interface used by annotators for answering questions A.3 LLM Judges Prompts We use simple system prompt (Figure 5) and user message (Figure 6) templates without model-specific commands, allowing our prompt to be transferred to any LLM. We use distinct tags, such as <success> and <reasoning>, to facilitate parsing the model output. Results We report extended results for 10 LLM judges, with the overall results in Table 7 and the finegrained results in Table 8 over all agents; the unaggregated results are presented in Tables 9 to 12. 14 Preprint. Under review."
        },
        {
            "title": "System Prompt Template",
            "content": "You are Language Model specialized in judging the performance of web agents in web navigation tasks. For certain website, you are given the goal of navigation task, the current URL of the webpage, the actions taken by the agent, and the thought process of the agent. Additionally, you will have access to the first and last screenshots, as well as the last accessibility tree. Your task is to answer several questions about the agent's performance in the task. Question 1: Was the sequence of actions below successful in achieving the goal? Choices: <success>Successful</success>, <success>Unsuccessful</success> Question 2: Did the agent perform unnecessary actions that could lead to unintended side effects? Choices: <side>Yes</side>, <side>No</side> Question 3: Did the agent perform the task optimally, by only performing necessary actions and avoiding unnecessary ones? Choices: <optimal>1. Complete Failure</optimal>, <optimal>2. Suboptimal</optimal>, <optimal>3. Somewhat Optimal</optimal>, <optimal>4. Completely Optimal</optimal> Question 4: Did the agent loop through sequence of actions that did not make progress towards the goal? Choices: <loop>Yes</loop>, <loop>No</loop> You should provide your reasoning process for each question, and you can refer to the screenshots and the accessibility tree to support your answers. Your answer must follow the following format: <reasoning>your reasoning here</reasoning> <success>answer</success> <side>answer</side> <optimal>answer</optimal> <loop>answer</loop> Figure 5: System Prompt Template used for the simplified judge"
        },
        {
            "title": "User Prompt Template",
            "content": "The user goal is: {goal} The agent performed the following actions: ----- Step: {step_number} URL: {url} Action: {action} Reasoning: {reasoning} ----- ... ----- The last accessibility tree is: {axtree} Here is the screenshot of the last step. {screenshot} Provide your reasoning and answer the four questions from the system prompt, using the specified format. Figure 6: User Prompt Template used for the simplified judge 15 Preprint. Under review. Domain # Domain # Domain duckduckgo.com blackbaudhosting.com usps.com yelp.com nih.gov yahoo.com tripadvisor.com seattlechildrensmuseum.org philamuseum.org ensembl.org wholefoodsmarket.com andersonsmartialarts.com currentresults.com x.com tmplclubs.com amazon.com wunderground.com themeparkcenter.com peacefoodnyc.com easyship.com nyunews.com anytots.com 129 21 12 9 8 7 6 5 5 4 3 2 2 2 2 1 1 1 1 1 1 1 google.com fedex.com fidelity.ca linkedin.com tcgplayer.com cagreatamerica.com express.dhl monday.com weatherspark.com wellhub.com alltrails.com wikipedia.org stockanalysis.com apple.com sixflags.com netflixreleases.com redfin.com seattleweatherblog.com sec.gov onlineshippingcalculator.com fandango.com morningstar.com 112 17 12 9 8 7 6 5 5 4 3 2 2 2 1 1 1 1 1 1 1 1 wizards.com mtggoldfish.com weather.gov rottentomatoes.com imdb.com thedrinknation.com californiagreatamerica.com fubo.tv bing.com hubbioo.com target.com sfyimby.com speakrj.com extremeweatherwatch.com etf.com weather-and-climate.com talesofamountainmama.com chromewebdata calicolabs.com tripadvisor.ca aimojo.io visitphilly.com # 24 17 10 9 8 6 5 5 5 3 2 2 2 2 1 1 1 1 1 1 1 Table 4: AssistantBench Website Visit Counts"
        },
        {
            "title": "False",
            "content": "Flags use_screenshot, use_som, use_thinking, use_concrete_example, use_abstract_example, use_hints, be_cautious use_html, use_past_error_logs, use_think_history, use_diff, filter_visible_elements_only, long_description, individual_examples, use_plan, use_criticise, use_memory, enable_chat Table 5: Agentlab Hyperparameters"
        },
        {
            "title": "LLM Judge",
            "content": "Rule-based"
        },
        {
            "title": "WorkArena",
            "content": "WorkArena++"
        },
        {
            "title": "Overall",
            "content": "Claude 3.7 S. GPT-4o Llama 3.3 Qwen2.5-VL Claude 3.7 S. GPT-4o Llama 3.3 Qwen2.5-VL Claude 3.7 S. GPT-4o Qwen2.5-VL Claude 3.7 S. GPT-4o Llama 3.3 Qwen2.5-VL Claude 3.7 S. GPT-4o Llama 3.3 Qwen2.5-VL Claude 3.7 S. GPT-4o Llama 3.3 Qwen2.5-VL 11.1 14.8 3.7 0.0 55.1 42.3 22.4 33.3 28.3 35.9 21.7 68.8 50.0 56.2 56.2 18.4 18.4 9.2 13.8 33.0 31.3 17.0 22. 11.1 14.8 7.4 0.0 64.1 50.0 27.6 52.6 34.8 47.8 34.8 68.8 56.2 50.0 56.2 20.7 11.5 5.8 14.9 38.0 35.3 17.5 31. 0.8 3.7 5.3 2.2 30.8 25.6 18.4 29.5 23.9 17.4 17.4 50.0 50.0 56.2 56.2 8.1 4.6 3.5 11.5 20.4 16.3 13.3 19. Table 6: Success Rate by evaluation type. For the LLM judge, we use GPT-4o with accessibility trees. 16 Preprint. Under review."
        },
        {
            "title": "Judge",
            "content": "AER-C AER-V Claude 3.7 S. (A) Claude 3.7 S. (S) Functional GPT-4o (A) GPT-4o (S) GPT-4o Mini (A) GPT-4o Mini (S) Llama 3.3 (A) NNetNav Qwen2.5-VL (A) Qwen2.5-VL (S)"
        },
        {
            "title": "Side Effect",
            "content": "P 67.7 67.6 68.8 69.4 83.8 69.8 68.1 61.5 64.5 67.7 52.5 64.3 64.5 71.9 71.5 81.6 76.3 55.9 83.1 80.3 86.1 78.3 79.0 82.4 89.8 86.1 F1 69.7 69.5 74.7 72.7 67.1 75.9 73.7 71.7 70.8 72.9 64.1 75.0 73. 14.0 14.1 7.7 7.5 7.2 6.6 6.9 9.0 8.8 34.7 44.4 91.7 90.3 70.8 31.9 79.2 55.6 58.3 F1 20.0 21.4 14.2 13.8 13.0 11.0 12.7 15.4 15."
        },
        {
            "title": "Repetition\nR",
            "content": "P 82.8 82.0 80.4 79.2 78.6 92.3 80.1 88.1 88.7 94.9 94.5 96.9 96.2 46.4 18.5 91.6 72.6 64.6 F1 88.4 87.8 87.9 86.9 58.3 30.8 85.5 79.6 74.7 Table 7: Results over all benchmarks by judge. We report the precision (P) as the primary metric, and F1 and recall (R) as the auxiliary metrics. 17 Preprint. Under review. Benchmark Judge AssistantBench"
        },
        {
            "title": "WorkArena",
            "content": "WorkArena++ AER-C AER-V Claude 3.7 S. (A) Claude 3.7 S. (S) Functional GPT-4o (A) GPT-4o (S) GPT-4o Mini (A) GPT-4o Mini (S) Llama 3.3 (A) NNetNav Qwen2.5-VL (A) Qwen2.5-VL (S) AER-C AER-V Claude 3.7 S. (A) Claude 3.7 S. (S) Functional GPT-4o (A) GPT-4o (S) GPT-4o Mini (A) GPT-4o Mini (S) Llama 3.3 (A) NNetNav Qwen2.5-VL (A) Qwen2.5-VL (S) AER-C AER-V Claude 3.7 S. (A) Claude 3.7 S. (S) Functional GPT-4o (A) GPT-4o (S) GPT-4o Mini (A) GPT-4o Mini (S) Llama 3.3 (A) NNetNav Qwen2.5-VL (A) Qwen2.5-VL (S) AER-C AER-V Claude 3.7 S. (A) Claude 3.7 S. (S) Functional GPT-4o (A) GPT-4o (S) GPT-4o Mini (A) GPT-4o Mini (S) Llama 3.3 (A) NNetNav Qwen2.5-VL (A) Qwen2.5-VL (S) AER-C AER-V Claude 3.7 S. (A) Claude 3.7 S. (S) Functional GPT-4o (A) GPT-4o (S) GPT-4o Mini (A) GPT-4o Mini (S) Llama 3.3 (A) NNetNav Qwen2.5-VL (A) Qwen2.5-VL (S) 83.3 83.3 87.5 71.4 25.0 77.8 77.8 80.0 80.0 75.0 20.8 72.7 70.0 56.0 61.2 61.0 64.8 85.2 63.0 60.7 57.9 57.4 59.6 54.5 59.3 58.5 68.8 67.6 69.3 69.3 79.0 70.2 69.9 63.5 66.9 68.2 54.3 63.6 62.9 100.0 96.4 85.0 85.3 100.0 94.6 93.8 84.2 90.3 94.3 77.3 87.2 93.8 66.7 59.3 66.7 66.7 83.3 63.0 59.6 49.4 54.8 62.7 43.2 60.3 64. Success 62.5 62.5 87.5 62.5 12.5 87.5 87.5 100.0 100.0 75.0 62.5 100.0 87.5 70.9 79.7 77.2 74.7 58.2 86.1 82.3 83.5 73.4 74.7 69.6 88.6 87.3 83.2 84.0 89.1 89.1 53.8 89.1 89.9 90.8 86.6 86.6 90.8 94.1 92.4 81.1 73.0 91.9 78.4 91.9 94.6 81.1 86.5 75.7 89.2 91.9 91.9 81.1 42.3 30.8 62.7 50.0 38.5 55.8 53.8 76.9 65.4 61.5 78.8 78.8 73. F1 71.4 71.4 87.5 66.7 16.7 82.4 82.4 88.9 88.9 75.0 31.2 84.2 77.8 62.6 69.2 68.2 69.4 69.2 72.7 69.9 68.4 64.4 66.3 61.1 71.1 70.0 75.3 74.9 77.9 77.9 64.0 78.5 78.7 74.7 75.5 76.3 67.9 75.9 74.8 89.6 83.1 88.3 81.7 95.8 94.6 87.0 85.3 82.4 91.7 83.9 89.5 87.0 51.8 40.5 64.7 57.1 52.6 59.2 56.6 60.1 59.6 62.1 55.8 68.3 68. Side Effect F1 33.3 33.3 100.0 100.0 66.7 33.3 100.0 0.0 66.7 35.7 42.9 89.3 92.9 64.3 35.7 82.1 71.4 64.3 28.6 38.1 90.5 76.2 71.4 28.6 76.2 28.6 38.1 50.0 0.0 50.0 100.0 50.0 0.0 0.0 0.0 0. 38.9 61.1 100.0 100.0 83.3 33.3 83.3 77.8 77.8 11.8 12.5 6.3 6.3 5.2 6.3 6.6 0.0 10.5 22.7 24.5 21.8 21.9 18.4 17.1 21.0 27.8 24.8 16.7 21.3 17.4 14.3 17.7 11.9 15.4 10.9 15.1 14.3 0.0 5.0 10.0 6.2 0.0 0.0 0.0 0.0 23.7 22.2 10.3 10.4 9.7 8.1 8.9 13.7 11. 7.1 7.7 3.2 3.2 2.7 3.4 3.4 0.0 5.7 16.7 17.1 12.4 12.4 10.7 11.2 12.0 17.2 15.4 11.8 14.8 9.6 7.9 10.1 7.5 8.6 6.7 9.4 8.3 0.0 2.6 5.3 3.3 0.0 0.0 0.0 0.0 17.1 13.6 5.5 5.5 5.2 4.6 4.7 7.5 6. Repetition 70.1 70.8 68.6 67.1 59.3 90.0 65.8 73.8 72.2 78.7 77.8 75.2 72.7 74.4 87.1 74.4 86.9 83.3 84.9 80.8 82.9 82.7 70.3 86.7 79.7 84.7 90.5 76.0 77.3 70.4 72.0 66.7 100.0 81.8 100.0 86. 87.5 87.3 85.6 84.5 87.8 96.5 86.7 91.9 93.2 94.0 92.0 96.0 94.0 32.0 18.0 96.0 62.0 52.0 96.8 97.6 98.4 99.2 46.0 21.4 92.1 73.8 59.5 88.6 85.1 93.9 92.1 39.5 11.4 86.0 63.2 50.0 86.4 77.3 86.4 81.8 36.4 18.2 81.8 59.1 59. 97.4 98.9 98.5 98.2 52.9 20.2 93.8 79.0 75.7 F1 80.3 80.0 80.0 78.3 41.6 30.0 78.0 67.4 60.5 86.8 86.6 85.2 83.9 56.9 34.4 82.3 79.8 69.4 86.7 82.9 88.1 87.1 50.6 20.1 82.7 72.4 64.4 80.8 77.3 77.5 76.6 47.1 30.8 81.8 74.3 70. 92.2 92.8 91.6 90.8 66.1 33.4 90.1 85.0 83.6 Table 8: Finegrained results by benchmark and judge for all agents. We report the precision (P) as the primary metric, and F1 and recall (R) as the auxiliary metrics. 18 Preprint. Under review. Benchmark Judge AssistantBench"
        },
        {
            "title": "WorkArena",
            "content": "WorkArena++ AER-C AER-V Claude 3.7 S. (A) Claude 3.7 S. (S) Functional GPT-4o (A) GPT-4o (S) GPT-4o Mini (A) GPT-4o Mini (S) Llama 3.3 (A) NNetNav Qwen2.5-VL (A) Qwen2.5-VL (S) AER-C AER-V Claude 3.7 S. (A) Claude 3.7 S. (S) Functional GPT-4o (A) GPT-4o (S) GPT-4o Mini (A) GPT-4o Mini (S) Llama 3.3 (A) NNetNav Qwen2.5-VL (A) Qwen2.5-VL (S) AER-C AER-V Claude 3.7 S. (A) Claude 3.7 S. (S) Functional GPT-4o (A) GPT-4o (S) GPT-4o Mini (A) GPT-4o Mini (S) Llama 3.3 (A) NNetNav Qwen2.5-VL (A) Qwen2.5-VL (S) AER-C AER-V Claude 3.7 S. (A) Claude 3.7 S. (S) Functional GPT-4o (A) GPT-4o (S) GPT-4o Mini (A) GPT-4o Mini (S) Llama 3.3 (A) NNetNav Qwen2.5-VL (A) Qwen2.5-VL (S) AER-C AER-V Claude 3.7 S. (A) Claude 3.7 S. (S) Functional GPT-4o (A) GPT-4o (S) GPT-4o Mini (A) GPT-4o Mini (S) Llama 3.3 (A) NNetNav Qwen2.5-VL (A) Qwen2.5-VL (S) 83.3 83.3 87.5 71.4 25.0 77.8 77.8 80.0 80.0 75.0 20.8 72.7 70.0 56.0 61.2 61.0 64.8 85.2 63.0 60.7 57.9 57.4 59.6 54.5 59.3 58.5 68.8 67.6 69.3 69.3 79.0 70.2 69.9 63.5 66.9 68.2 54.3 63.6 62.9 100.0 96.4 85.0 85.3 100.0 94.6 93.8 84.2 90.3 94.3 77.3 87.2 93.8 66.7 59.3 66.7 66.7 83.3 63.0 59.6 49.4 54.8 62.7 43.2 60.3 64. Success 62.5 62.5 87.5 62.5 12.5 87.5 87.5 100.0 100.0 75.0 62.5 100.0 87.5 70.9 79.7 77.2 74.7 58.2 86.1 82.3 83.5 73.4 74.7 69.6 88.6 87.3 83.2 84.0 89.1 89.1 53.8 89.1 89.9 90.8 86.6 86.6 90.8 94.1 92.4 81.1 73.0 91.9 78.4 91.9 94.6 81.1 86.5 75.7 89.2 91.9 91.9 81.1 42.3 30.8 62.7 50.0 38.5 55.8 53.8 76.9 65.4 61.5 78.8 78.8 73. F1 71.4 71.4 87.5 66.7 16.7 82.4 82.4 88.9 88.9 75.0 31.2 84.2 77.8 62.6 69.2 68.2 69.4 69.2 72.7 69.9 68.4 64.4 66.3 61.1 71.1 70.0 75.3 74.9 77.9 77.9 64.0 78.5 78.7 74.7 75.5 76.3 67.9 75.9 74.8 89.6 83.1 88.3 81.7 95.8 94.6 87.0 85.3 82.4 91.7 83.9 89.5 87.0 51.8 40.5 64.7 57.1 52.6 59.2 56.6 60.1 59.6 62.1 55.8 68.3 68. Side Effect F1 33.3 33.3 100.0 100.0 66.7 33.3 100.0 0.0 66.7 35.7 42.9 89.3 92.9 64.3 35.7 82.1 71.4 64.3 28.6 38.1 90.5 76.2 71.4 28.6 76.2 28.6 38.1 50.0 0.0 50.0 100.0 50.0 0.0 0.0 0.0 0. 38.9 61.1 100.0 100.0 83.3 33.3 83.3 77.8 77.8 11.8 12.5 6.3 6.3 5.2 6.3 6.6 0.0 10.5 22.7 24.5 21.8 21.9 18.4 17.1 21.0 27.8 24.8 16.7 21.3 17.4 14.3 17.7 11.9 15.4 10.9 15.1 14.3 0.0 5.0 10.0 6.2 0.0 0.0 0.0 0.0 23.7 22.2 10.3 10.4 9.7 8.1 8.9 13.7 11. 7.1 7.7 3.2 3.2 2.7 3.4 3.4 0.0 5.7 16.7 17.1 12.4 12.4 10.7 11.2 12.0 17.2 15.4 11.8 14.8 9.6 7.9 10.1 7.5 8.6 6.7 9.4 8.3 0.0 2.6 5.3 3.3 0.0 0.0 0.0 0.0 17.1 13.6 5.5 5.5 5.2 4.6 4.7 7.5 6. Repetition 70.1 70.8 68.6 67.1 59.3 90.0 65.8 73.8 72.2 78.7 77.8 75.2 72.7 74.4 87.1 74.4 86.9 83.3 84.9 80.8 82.9 82.7 70.3 86.7 79.7 84.7 90.5 76.0 77.3 70.4 72.0 66.7 100.0 81.8 100.0 86. 87.5 87.3 85.6 84.5 87.8 96.5 86.7 91.9 93.2 94.0 92.0 96.0 94.0 32.0 18.0 96.0 62.0 52.0 96.8 97.6 98.4 99.2 46.0 21.4 92.1 73.8 59.5 88.6 85.1 93.9 92.1 39.5 11.4 86.0 63.2 50.0 86.4 77.3 86.4 81.8 36.4 18.2 81.8 59.1 59. 97.4 98.9 98.5 98.2 52.9 20.2 93.8 79.0 75.7 F1 80.3 80.0 80.0 78.3 41.6 30.0 78.0 67.4 60.5 86.8 86.6 85.2 83.9 56.9 34.4 82.3 79.8 69.4 86.7 82.9 88.1 87.1 50.6 20.1 82.7 72.4 64.4 80.8 77.3 77.5 76.6 47.1 30.8 81.8 74.3 70. 92.2 92.8 91.6 90.8 66.1 33.4 90.1 85.0 83.6 Table 9: Finegrained results by benchmark and judge for Qwen2.5-VL agent. We report the precision (P) as the primary metric, and F1 and recall (R) as the auxiliary metrics. 19 Preprint. Under review. Benchmark Judge AssistantBench"
        },
        {
            "title": "WorkArena",
            "content": "WorkArena++ AER-C AER-V Claude 3.7 S. (A) Claude 3.7 S. (S) Functional GPT-4o (A) GPT-4o (S) GPT-4o Mini (A) GPT-4o Mini (S) Llama 3.3 (A) NNetNav Qwen2.5-VL (A) Qwen2.5-VL (S) AER-C AER-V Claude 3.7 S. (A) Claude 3.7 S. (S) Functional GPT-4o (A) GPT-4o (S) GPT-4o Mini (A) GPT-4o Mini (S) Llama 3.3 (A) NNetNav Qwen2.5-VL (A) Qwen2.5-VL (S) AER-C AER-V Claude 3.7 S. (A) Claude 3.7 S. (S) Functional GPT-4o (A) GPT-4o (S) GPT-4o Mini (A) GPT-4o Mini (S) Llama 3.3 (A) NNetNav Qwen2.5-VL (A) Qwen2.5-VL (S) AER-C AER-V Claude 3.7 S. (A) Claude 3.7 S. (S) Functional GPT-4o (A) GPT-4o (S) GPT-4o Mini (A) GPT-4o Mini (S) Llama 3.3 (A) NNetNav Qwen2.5-VL (A) Qwen2.5-VL (S) AER-C AER-V Claude 3.7 S. (A) Claude 3.7 S. (S) Functional GPT-4o (A) GPT-4o (S) GPT-4o Mini (A) GPT-4o Mini (S) Llama 3.3 (A) NNetNav Qwen2.5-VL (A) Qwen2.5-VL (S) 83.3 83.3 87.5 71.4 25.0 77.8 77.8 80.0 80.0 75.0 20.8 72.7 70.0 56.0 61.2 61.0 64.8 85.2 63.0 60.7 57.9 57.4 59.6 54.5 59.3 58.5 68.8 67.6 69.3 69.3 79.0 70.2 69.9 63.5 66.9 68.2 54.3 63.6 62.9 100.0 96.4 85.0 85.3 100.0 94.6 93.8 84.2 90.3 94.3 77.3 87.2 93.8 66.7 59.3 66.7 66.7 83.3 63.0 59.6 49.4 54.8 62.7 43.2 60.3 64. Success 62.5 62.5 87.5 62.5 12.5 87.5 87.5 100.0 100.0 75.0 62.5 100.0 87.5 70.9 79.7 77.2 74.7 58.2 86.1 82.3 83.5 73.4 74.7 69.6 88.6 87.3 83.2 84.0 89.1 89.1 53.8 89.1 89.9 90.8 86.6 86.6 90.8 94.1 92.4 81.1 73.0 91.9 78.4 91.9 94.6 81.1 86.5 75.7 89.2 91.9 91.9 81.1 42.3 30.8 62.7 50.0 38.5 55.8 53.8 76.9 65.4 61.5 78.8 78.8 73. F1 71.4 71.4 87.5 66.7 16.7 82.4 82.4 88.9 88.9 75.0 31.2 84.2 77.8 62.6 69.2 68.2 69.4 69.2 72.7 69.9 68.4 64.4 66.3 61.1 71.1 70.0 75.3 74.9 77.9 77.9 64.0 78.5 78.7 74.7 75.5 76.3 67.9 75.9 74.8 89.6 83.1 88.3 81.7 95.8 94.6 87.0 85.3 82.4 91.7 83.9 89.5 87.0 51.8 40.5 64.7 57.1 52.6 59.2 56.6 60.1 59.6 62.1 55.8 68.3 68. Side Effect F1 33.3 33.3 100.0 100.0 66.7 33.3 100.0 0.0 66.7 35.7 42.9 89.3 92.9 64.3 35.7 82.1 71.4 64.3 28.6 38.1 90.5 76.2 71.4 28.6 76.2 28.6 38.1 50.0 0.0 50.0 100.0 50.0 0.0 0.0 0.0 0. 38.9 61.1 100.0 100.0 83.3 33.3 83.3 77.8 77.8 11.8 12.5 6.3 6.3 5.2 6.3 6.6 0.0 10.5 22.7 24.5 21.8 21.9 18.4 17.1 21.0 27.8 24.8 16.7 21.3 17.4 14.3 17.7 11.9 15.4 10.9 15.1 14.3 0.0 5.0 10.0 6.2 0.0 0.0 0.0 0.0 23.7 22.2 10.3 10.4 9.7 8.1 8.9 13.7 11. 7.1 7.7 3.2 3.2 2.7 3.4 3.4 0.0 5.7 16.7 17.1 12.4 12.4 10.7 11.2 12.0 17.2 15.4 11.8 14.8 9.6 7.9 10.1 7.5 8.6 6.7 9.4 8.3 0.0 2.6 5.3 3.3 0.0 0.0 0.0 0.0 17.1 13.6 5.5 5.5 5.2 4.6 4.7 7.5 6. Repetition 70.1 70.8 68.6 67.1 59.3 90.0 65.8 73.8 72.2 78.7 77.8 75.2 72.7 74.4 87.1 74.4 86.9 83.3 84.9 80.8 82.9 82.7 70.3 86.7 79.7 84.7 90.5 76.0 77.3 70.4 72.0 66.7 100.0 81.8 100.0 86. 87.5 87.3 85.6 84.5 87.8 96.5 86.7 91.9 93.2 94.0 92.0 96.0 94.0 32.0 18.0 96.0 62.0 52.0 96.8 97.6 98.4 99.2 46.0 21.4 92.1 73.8 59.5 88.6 85.1 93.9 92.1 39.5 11.4 86.0 63.2 50.0 86.4 77.3 86.4 81.8 36.4 18.2 81.8 59.1 59. 97.4 98.9 98.5 98.2 52.9 20.2 93.8 79.0 75.7 F1 80.3 80.0 80.0 78.3 41.6 30.0 78.0 67.4 60.5 86.8 86.6 85.2 83.9 56.9 34.4 82.3 79.8 69.4 86.7 82.9 88.1 87.1 50.6 20.1 82.7 72.4 64.4 80.8 77.3 77.5 76.6 47.1 30.8 81.8 74.3 70. 92.2 92.8 91.6 90.8 66.1 33.4 90.1 85.0 83.6 Table 10: Finegrained results by benchmark and judge for Llama 3.3 agent. We report the precision (P) as the primary metric, and F1 and recall (R) as the auxiliary metrics. 20 Preprint. Under review. Benchmark Judge AssistantBench"
        },
        {
            "title": "WorkArena",
            "content": "WorkArena++ AER-C AER-V Claude 3.7 S. (A) Claude 3.7 S. (S) Functional GPT-4o (A) GPT-4o (S) GPT-4o Mini (A) GPT-4o Mini (S) Llama 3.3 (A) NNetNav Qwen2.5-VL (A) Qwen2.5-VL (S) AER-C AER-V Claude 3.7 S. (A) Claude 3.7 S. (S) Functional GPT-4o (A) GPT-4o (S) GPT-4o Mini (A) GPT-4o Mini (S) Llama 3.3 (A) NNetNav Qwen2.5-VL (A) Qwen2.5-VL (S) AER-C AER-V Claude 3.7 S. (A) Claude 3.7 S. (S) Functional GPT-4o (A) GPT-4o (S) GPT-4o Mini (A) GPT-4o Mini (S) Llama 3.3 (A) NNetNav Qwen2.5-VL (A) Qwen2.5-VL (S) AER-C AER-V Claude 3.7 S. (A) Claude 3.7 S. (S) Functional GPT-4o (A) GPT-4o (S) GPT-4o Mini (A) GPT-4o Mini (S) Llama 3.3 (A) NNetNav Qwen2.5-VL (A) Qwen2.5-VL (S) AER-C AER-V Claude 3.7 S. (A) Claude 3.7 S. (S) Functional GPT-4o (A) GPT-4o (S) GPT-4o Mini (A) GPT-4o Mini (S) Llama 3.3 (A) NNetNav Qwen2.5-VL (A) Qwen2.5-VL (S) 83.3 83.3 87.5 71.4 25.0 77.8 77.8 80.0 80.0 75.0 20.8 72.7 70.0 56.0 61.2 61.0 64.8 85.2 63.0 60.7 57.9 57.4 59.6 54.5 59.3 58.5 68.8 67.6 69.3 69.3 79.0 70.2 69.9 63.5 66.9 68.2 54.3 63.6 62.9 100.0 96.4 85.0 85.3 100.0 94.6 93.8 84.2 90.3 94.3 77.3 87.2 93.8 66.7 59.3 66.7 66.7 83.3 63.0 59.6 49.4 54.8 62.7 43.2 60.3 64. Success 62.5 62.5 87.5 62.5 12.5 87.5 87.5 100.0 100.0 75.0 62.5 100.0 87.5 70.9 79.7 77.2 74.7 58.2 86.1 82.3 83.5 73.4 74.7 69.6 88.6 87.3 83.2 84.0 89.1 89.1 53.8 89.1 89.9 90.8 86.6 86.6 90.8 94.1 92.4 81.1 73.0 91.9 78.4 91.9 94.6 81.1 86.5 75.7 89.2 91.9 91.9 81.1 42.3 30.8 62.7 50.0 38.5 55.8 53.8 76.9 65.4 61.5 78.8 78.8 73. F1 71.4 71.4 87.5 66.7 16.7 82.4 82.4 88.9 88.9 75.0 31.2 84.2 77.8 62.6 69.2 68.2 69.4 69.2 72.7 69.9 68.4 64.4 66.3 61.1 71.1 70.0 75.3 74.9 77.9 77.9 64.0 78.5 78.7 74.7 75.5 76.3 67.9 75.9 74.8 89.6 83.1 88.3 81.7 95.8 94.6 87.0 85.3 82.4 91.7 83.9 89.5 87.0 51.8 40.5 64.7 57.1 52.6 59.2 56.6 60.1 59.6 62.1 55.8 68.3 68. Side Effect F1 33.3 33.3 100.0 100.0 66.7 33.3 100.0 0.0 66.7 35.7 42.9 89.3 92.9 64.3 35.7 82.1 71.4 64.3 28.6 38.1 90.5 76.2 71.4 28.6 76.2 28.6 38.1 50.0 0.0 50.0 100.0 50.0 0.0 0.0 0.0 0. 38.9 61.1 100.0 100.0 83.3 33.3 83.3 77.8 77.8 11.8 12.5 6.3 6.3 5.2 6.3 6.6 0.0 10.5 22.7 24.5 21.8 21.9 18.4 17.1 21.0 27.8 24.8 16.7 21.3 17.4 14.3 17.7 11.9 15.4 10.9 15.1 14.3 0.0 5.0 10.0 6.2 0.0 0.0 0.0 0.0 23.7 22.2 10.3 10.4 9.7 8.1 8.9 13.7 11. 7.1 7.7 3.2 3.2 2.7 3.4 3.4 0.0 5.7 16.7 17.1 12.4 12.4 10.7 11.2 12.0 17.2 15.4 11.8 14.8 9.6 7.9 10.1 7.5 8.6 6.7 9.4 8.3 0.0 2.6 5.3 3.3 0.0 0.0 0.0 0.0 17.1 13.6 5.5 5.5 5.2 4.6 4.7 7.5 6. Repetition 70.1 70.8 68.6 67.1 59.3 90.0 65.8 73.8 72.2 78.7 77.8 75.2 72.7 74.4 87.1 74.4 86.9 83.3 84.9 80.8 82.9 82.7 70.3 86.7 79.7 84.7 90.5 76.0 77.3 70.4 72.0 66.7 100.0 81.8 100.0 86. 87.5 87.3 85.6 84.5 87.8 96.5 86.7 91.9 93.2 94.0 92.0 96.0 94.0 32.0 18.0 96.0 62.0 52.0 96.8 97.6 98.4 99.2 46.0 21.4 92.1 73.8 59.5 88.6 85.1 93.9 92.1 39.5 11.4 86.0 63.2 50.0 86.4 77.3 86.4 81.8 36.4 18.2 81.8 59.1 59. 97.4 98.9 98.5 98.2 52.9 20.2 93.8 79.0 75.7 F1 80.3 80.0 80.0 78.3 41.6 30.0 78.0 67.4 60.5 86.8 86.6 85.2 83.9 56.9 34.4 82.3 79.8 69.4 86.7 82.9 88.1 87.1 50.6 20.1 82.7 72.4 64.4 80.8 77.3 77.5 76.6 47.1 30.8 81.8 74.3 70. 92.2 92.8 91.6 90.8 66.1 33.4 90.1 85.0 83.6 Table 11: Finegrained results by benchmark and judge for Claude 3.7 Sonnet agent. We report the precision (P) as the primary metric, and F1 and recall (R) as the auxiliary metrics. 21 Preprint. Under review. Benchmark Judge AssistantBench"
        },
        {
            "title": "WorkArena",
            "content": "WorkArena++ AER-C AER-V Claude 3.7 S. (A) Claude 3.7 S. (S) Functional GPT-4o (A) GPT-4o (S) GPT-4o Mini (A) GPT-4o Mini (S) Llama 3.3 (A) NNetNav Qwen2.5-VL (A) Qwen2.5-VL (S) AER-C AER-V Claude 3.7 S. (A) Claude 3.7 S. (S) Functional GPT-4o (A) GPT-4o (S) GPT-4o Mini (A) GPT-4o Mini (S) Llama 3.3 (A) NNetNav Qwen2.5-VL (A) Qwen2.5-VL (S) AER-C AER-V Claude 3.7 S. (A) Claude 3.7 S. (S) Functional GPT-4o (A) GPT-4o (S) GPT-4o Mini (A) GPT-4o Mini (S) Llama 3.3 (A) NNetNav Qwen2.5-VL (A) Qwen2.5-VL (S) AER-C AER-V Claude 3.7 S. (A) Claude 3.7 S. (S) Functional GPT-4o (A) GPT-4o (S) GPT-4o Mini (A) GPT-4o Mini (S) Llama 3.3 (A) NNetNav Qwen2.5-VL (A) Qwen2.5-VL (S) AER-C AER-V Claude 3.7 S. (A) Claude 3.7 S. (S) Functional GPT-4o (A) GPT-4o (S) GPT-4o Mini (A) GPT-4o Mini (S) Llama 3.3 (A) NNetNav Qwen2.5-VL (A) Qwen2.5-VL (S) 83.3 83.3 87.5 71.4 25.0 77.8 77.8 80.0 80.0 75.0 20.8 72.7 70.0 56.0 61.2 61.0 64.8 85.2 63.0 60.7 57.9 57.4 59.6 54.5 59.3 58.5 68.8 67.6 69.3 69.3 79.0 70.2 69.9 63.5 66.9 68.2 54.3 63.6 62.9 100.0 96.4 85.0 85.3 100.0 94.6 93.8 84.2 90.3 94.3 77.3 87.2 93.8 66.7 59.3 66.7 66.7 83.3 63.0 59.6 49.4 54.8 62.7 43.2 60.3 64. Success 62.5 62.5 87.5 62.5 12.5 87.5 87.5 100.0 100.0 75.0 62.5 100.0 87.5 70.9 79.7 77.2 74.7 58.2 86.1 82.3 83.5 73.4 74.7 69.6 88.6 87.3 83.2 84.0 89.1 89.1 53.8 89.1 89.9 90.8 86.6 86.6 90.8 94.1 92.4 81.1 73.0 91.9 78.4 91.9 94.6 81.1 86.5 75.7 89.2 91.9 91.9 81.1 42.3 30.8 62.7 50.0 38.5 55.8 53.8 76.9 65.4 61.5 78.8 78.8 73. F1 71.4 71.4 87.5 66.7 16.7 82.4 82.4 88.9 88.9 75.0 31.2 84.2 77.8 62.6 69.2 68.2 69.4 69.2 72.7 69.9 68.4 64.4 66.3 61.1 71.1 70.0 75.3 74.9 77.9 77.9 64.0 78.5 78.7 74.7 75.5 76.3 67.9 75.9 74.8 89.6 83.1 88.3 81.7 95.8 94.6 87.0 85.3 82.4 91.7 83.9 89.5 87.0 51.8 40.5 64.7 57.1 52.6 59.2 56.6 60.1 59.6 62.1 55.8 68.3 68. Side Effect F1 33.3 33.3 100.0 100.0 66.7 33.3 100.0 0.0 66.7 35.7 42.9 89.3 92.9 64.3 35.7 82.1 71.4 64.3 28.6 38.1 90.5 76.2 71.4 28.6 76.2 28.6 38.1 50.0 0.0 50.0 100.0 50.0 0.0 0.0 0.0 0. 38.9 61.1 100.0 100.0 83.3 33.3 83.3 77.8 77.8 11.8 12.5 6.3 6.3 5.2 6.3 6.6 0.0 10.5 22.7 24.5 21.8 21.9 18.4 17.1 21.0 27.8 24.8 16.7 21.3 17.4 14.3 17.7 11.9 15.4 10.9 15.1 14.3 0.0 5.0 10.0 6.2 0.0 0.0 0.0 0.0 23.7 22.2 10.3 10.4 9.7 8.1 8.9 13.7 11. 7.1 7.7 3.2 3.2 2.7 3.4 3.4 0.0 5.7 16.7 17.1 12.4 12.4 10.7 11.2 12.0 17.2 15.4 11.8 14.8 9.6 7.9 10.1 7.5 8.6 6.7 9.4 8.3 0.0 2.6 5.3 3.3 0.0 0.0 0.0 0.0 17.1 13.6 5.5 5.5 5.2 4.6 4.7 7.5 6. Repetition 70.1 70.8 68.6 67.1 59.3 90.0 65.8 73.8 72.2 78.7 77.8 75.2 72.7 74.4 87.1 74.4 86.9 83.3 84.9 80.8 82.9 82.7 70.3 86.7 79.7 84.7 90.5 76.0 77.3 70.4 72.0 66.7 100.0 81.8 100.0 86. 87.5 87.3 85.6 84.5 87.8 96.5 86.7 91.9 93.2 94.0 92.0 96.0 94.0 32.0 18.0 96.0 62.0 52.0 96.8 97.6 98.4 99.2 46.0 21.4 92.1 73.8 59.5 88.6 85.1 93.9 92.1 39.5 11.4 86.0 63.2 50.0 86.4 77.3 86.4 81.8 36.4 18.2 81.8 59.1 59. 97.4 98.9 98.5 98.2 52.9 20.2 93.8 79.0 75.7 F1 80.3 80.0 80.0 78.3 41.6 30.0 78.0 67.4 60.5 86.8 86.6 85.2 83.9 56.9 34.4 82.3 79.8 69.4 86.7 82.9 88.1 87.1 50.6 20.1 82.7 72.4 64.4 80.8 77.3 77.5 76.6 47.1 30.8 81.8 74.3 70. 92.2 92.8 91.6 90.8 66.1 33.4 90.1 85.0 83.6 Table 12: Finegrained results by benchmark and judge for GPT-4o agent. We report the precision (P) as the primary metric, and F1 and recall (R) as the auxiliary metrics."
        }
    ],
    "affiliations": [
        "Canada CIFAR AI Chair",
        "Google DeepMind",
        "McGill University",
        "Mila Quebec AI Institute",
        "Polytechnique Montréal",
        "ServiceNow Research"
    ]
}