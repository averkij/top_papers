{
    "paper_title": "Inconsistencies In Consistency Models: Better ODE Solving Does Not Imply Better Samples",
    "authors": [
        "Noël Vouitsis",
        "Rasa Hosseinzadeh",
        "Brendan Leigh Ross",
        "Valentin Villecroze",
        "Satya Krishna Gorti",
        "Jesse C. Cresswell",
        "Gabriel Loaiza-Ganem"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Although diffusion models can generate remarkably high-quality samples, they are intrinsically bottlenecked by their expensive iterative sampling procedure. Consistency models (CMs) have recently emerged as a promising diffusion model distillation method, reducing the cost of sampling by generating high-fidelity samples in just a few iterations. Consistency model distillation aims to solve the probability flow ordinary differential equation (ODE) defined by an existing diffusion model. CMs are not directly trained to minimize error against an ODE solver, rather they use a more computationally tractable objective. As a way to study how effectively CMs solve the probability flow ODE, and the effect that any induced error has on the quality of generated samples, we introduce Direct CMs, which \\textit{directly} minimize this error. Intriguingly, we find that Direct CMs reduce the ODE solving error compared to CMs but also result in significantly worse sample quality, calling into question why exactly CMs work well in the first place. Full code is available at: https://github.com/layer6ai-labs/direct-cms."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 3 1 ] . [ 1 4 5 9 8 0 . 1 1 4 2 : r Inconsistencies In Consistency Models: Better ODE Solving Does Not Imply Better Samples Noël Vouitsis Layer 6 AI Rasa Hosseinzadeh Layer 6 AI Brendan Leigh Ross Layer 6 AI Valentin Villecroze Layer 6 AI Satya Krishna Gorti Layer 6 AI Jesse C. Cresswell Layer 6 AI Gabriel Loaiza-Ganem Layer 6 AI {noel, rasa, brendan, valentin.v, satya, jesse, gabriel}@layer6.ai"
        },
        {
            "title": "Abstract",
            "content": "Although diffusion models can generate remarkably high-quality samples, they are intrinsically bottlenecked by their expensive iterative sampling procedure. Consistency models (CMs) have recently emerged as promising diffusion model distillation method, reducing the cost of sampling by generating high-fidelity samples in just few iterations. Consistency model distillation aims to solve the probability flow ordinary differential equation (ODE) defined by an existing diffusion model. CMs are not directly trained to minimize error against an ODE solver, rather they use more computationally tractable objective. As way to study how effectively CMs solve the probability flow ODE, and the effect that any induced error has on the quality of generated samples, we introduce Direct CMs, which directly minimize this error. Intriguingly, we find that Direct CMs reduce the ODE solving error compared to CMs but also result in significantly worse sample quality, calling into question why exactly CMs work well in the first place. Full code is available at: https://github.com/layer6ai-labs/direct-cms."
        },
        {
            "title": "Introduction",
            "content": "In recent years, diffusion models (DMs) [44, 14] have become the de facto standard generative models [22] for many perceptual data modalities such as images [34, 40, 36, 5], video [15, 2, 53, 52], and audio [20, 39, 17]. Despite their successes, an inherent drawback of diffusion models stems from their iterative sampling procedure, whereby hundreds or thousands of function calls to the diffusion model are typically required to generate high-quality samples, limiting their practicality in low-latency settings. prominent approach for improving the sampling efficiency of diffusion models is to subsequently distill them into models capable of few-step generation [24, 41, 31, 27, 1, 9, 56, 55, 42]. Among the works in this vein, consistency models (CMs) [49] have garnered attention due to their simple premise as well as their ability to successfully generate samples with only few steps. CMs leverage the ordinary differential equation (ODE) formulation of diffusion models, called the probability flow (PF) ODE, that defines deterministic mapping between noise and data [48]. The goal of consistency model distillation is to train model (the student) to solve the PF ODE of an existing diffusion model (the teacher) from all points along any ODE trajectory in single step. The loss proposed by Song et al. [49] to train CMs does not directly minimize the error against an ODE solver; the solver is mimicked only at optimality and under the assumptions of arbitrarily flexible networks and perfect optimization. We thus hypothesize that the error against the ODE solver can be further driven down by directly solving the PF ODE at each step using strong supervision from the teacher, which we call direct consistency model (Direct CM). Although 38th Workshop on Attributing Model Behavior at Scale (NeurIPS 2024). Direct CMs are more expensive to train than standard CMs, they provide relevant tool to probe how well CMs solve the PF ODE and how deviations from an ODE solver affect sample quality. We perform controlled experiments to compare CMs and Direct CMs using state-of-the-art and large-scale diffusion model from the Stable Diffusion family [36], SDXL [30], as the teacher model for distillation. We show that Direct CMs perform better at solving the PF ODE but, surprisingly, that they translate to noticeably worse sample quality. This unexpected result challenges the conception that better ODE solving necessarily implies better sample quality, notion that is implicitly assumed by CMs and its variations alike [47, 7, 10, 57, 21]. Our findings serve as counterexample to this statement, thus calling into question the communitys understanding of ODE-based diffusion model distillation and its implications on sample quality. Since CMs achieve larger ODE solving error, we surmise that other confounding factors contribute to their improved sample quality. We thus call for additional investigation to clarify this seemingly paradoxical behaviour of ODE-based diffusion model distillation."
        },
        {
            "title": "2 Background and Related Work",
            "content": "Diffusion Models The overarching objective of diffusion models is to learn to reverse noising process that iteratively transforms data into noise. In the limit of infinite noising steps, this iterative process can be formalized as stochastic different equation (SDE), called the forward SDE. The goal of diffusion models amounts to reversing the forward SDE, hence mapping noise to data [48]. Formally, denoting the data distribution as p0, the forward SDE is given by dxt = µ(xt, t)dt + σ(t)dWt, x0 p0, (1) (cid:18) where [0, ] for some fixed , µ and σ are hyperparameters, and Wt denotes multivariate Brownian motion. We denote the implied marginal distribution of xt as pt; the intuition here is that, with correct choice of hyperparameters, pT is almost pure noise. Song et al. [48] showed that the following ODE, referred to as the probability flow (PF) ODE, shares the same marginals as the forward SDE, dxt = µ(xt, t) log pt(xt) σ2(t) 2 where log pt is the (Stein) score function. In other words, if the PF ODE is started at x0 p0, then xt pt. Under standard regularity conditions, for any initial condition x0 this ODE admits unique trajectory (xt)t[0,T ] as solution. Thus, any point xt uniquely determines the entire trajectory, meaning that Equation 2 implicitly defines deterministic mapping : (xt, t, t) (cid:55) xt which can be computed by solving Equation 2 backward through time whenever > t. In principle this function can be used to sample from p0, since (xT , T, 0) will be distributed according to p0 if xT pT . In practice this cannot be done exactly, and three approximations are performed. First, the score function is unknown, and diffusion models train neural network s(xt, t) to approximate it, i.e., s(xt, t) log pt(xt). This approximation results in the new PF ODE, sometimes called the empirical PF ODE, dt, (2) (cid:19) dxt = µ(xt, t) s(xt, t) dt, (3) (cid:18) (cid:19) σ2(t) 2 whose solution function we denote as s. Second, computing s(xT , T, 0) still requires solving an ODE, meaning that numerical solver must be used to approximate it. We denote the solution of numerical ODE solver as fsolver, and single step of the solver from time to time as Φ(, t, t). More formally, discretizing the interval [0, ] as 0 = t0 < < tN = T, we have that whenever > m, fsolver(xtn , tn, tm) is defined recursively as fsolver(xtn , tn, tm) = (cid:98)xtm where (cid:98)xti1 = Φ((cid:98)xti, ti, ti1) for = n, 1, . . . , + 1 with (cid:98)xtn = xtn . Lastly, pT is also unknown, but since it is very close to pure noise, it can be approximated with an appropriate Gaussian distribution (cid:98)pT . In summary, by leveraging the empirical PF ODE, samples from diffusion model can be obtained as fsolver(xT , T, 0), where xT (cid:98)pT . If the approximations made throughout are accurate, then fsolver and (cid:98)pT pT , so that samples from the model resemble samples from p0 [4]. Despite their ability to generate high-quality samples, an inherent drawback of DMs is rooted in their sampling procedure, since computing Φ requires function call to s; the iterative refinement of denoised samples to generate high-quality solution trajectories is computationally intensive. 2 Figure 1: CMs (left) are weakly supervised ODE solvers, only learning to map points along trajectory that are near the trajectorys origin back to the origin itself; points that are far from the origin instead enforce self-consistency property, relying on weak self-supervision to solve the PF ODE. Direct CMs (right) are strongly supervised ODE solvers, instead learning to directly map all points along trajectory back to the origin. Consistency Models Consistency models [49] leverage the PF ODE formulation of DMs to enable few-step generation. They can be used either for DM distillation or trained standalone from scratch; we only consider distillation in our work since the score function of pre-trained DMs gives us tool to directly study the effect of ODE solving on CMs. Given trained diffusion model with corresponding fsolver, the idea of consistency model distillation is to train neural network fθ such that fθ(xtn , tn) fsolver(xtn , tn, 0) for all {1, . . . , }. In other words, CMs aim to learn function to mimic the solver of the empirical PF ODE, thus circumventing the need to repeatedly evaluate during sampling. CMs learn fθ by enforcing the self-consistency property, meaning that for every xtn and xtn along the same trajectory, fθ(xtn , tn) and fθ(xtn , tn) are encouraged to match. More specifically, CMs are trained by minimizing the consistency distillation loss, LCD := Ex0p0,nU 1,N (cid:74) ,xtn ptn0(x0) (cid:75) (cid:104) (cid:16) λ(tn)d fθ(xtn , tn), θ((cid:98)xtn1, tn1) (cid:17)(cid:105) , (4) where pt0 is the transition kernel corresponding to Equation 1, λ > 0 is weighting function treated as hyperparameter, is any distance, θ is frozen version of θ, and (cid:98)xtn1 = Φ(xtn , tn, tn1). Since the transition kernel is given by known Gaussian, the above objective is tractable. CMs parameterize fθ in such way that fθ((cid:98)x0, 0) = (cid:98)x0 holds. This property is referred to as the boundary condition, and prevents Equation 4 from being pathologically minimized by fθ collapsing onto constant function. During sampling, CMs can use one or multiple function evaluations of fθ, enabling trade-off between computational cost and sample quality. For example, if given budget of two function evaluations, rather than produce sample as fθ(xT , ), one could run Equation 1 until some time tn starting from fθ(xT , ) to produce xtn , and then output fθ(xtn , tn) as the sample. This idea generalizes to more function evaluations, although note that fθ(xtn , tn) and xtn do not belong to the same ODE trajectory as fθ(xT , ) and xT due to the added noise from the forward SDE."
        },
        {
            "title": "3 Direct Consistency Models",
            "content": "In Equation 4, x0 and xtn do not belong to the same ODE trajectory since noise is added to obtain xtn from x0 via the forward SDEs transition kernel. Thus, it would not make sense to enforce consistency by minimizing d(fθ(xtn , tn), x0), and Equation 4 is used instead. While Song et al. [49] theoretically showed that perfectly minimizing Equation 4 with an arbitrarily flexible fθ results in fθ(xtn , tn) = fsolver(xtn , tn, 0), in practice it has been observed that CMs can be difficult to optimize, with slow convergence or, in some cases, divergence [10, 7]. We attribute this behaviour to what we call weak supervision in the CM loss, namely that fθ is not directly trained to map xtn to the origin of its ODE trajectory. The constraint that the CM should map any point on the ODE trajectory to the trajectorys origin is only weakly enforced through the boundary condition 3 Table 1: Results of ODE solving and image quality for single-step generation. CMs perform worse at solving the PF ODE but produce higher quality images. Φ"
        },
        {
            "title": "Method",
            "content": "ODE"
        },
        {
            "title": "Image",
            "content": "FID FD-DINO CLIP Aes"
        },
        {
            "title": "Heun",
            "content": "CM CM"
        },
        {
            "title": "0.29\nDirect CM 0.25\n0.29\nDirect CM 0.23\n0.30\nDirect CM 0.25",
            "content": "CM 103.9 158.6 95.3 166.0 120.5 162.0 816.3 1095 747.7 1148 846.1 1126 0.21 0.20 0.21 0.19 0.21 0.19 5.6 5.1 5.5 5.0 5.5 5.1 parameterization of fθ. Only at time t1 does the objective directly encourage mapping points xt1 to the trajectorys origin. The network fθ must therefore first learn to map slightly noised data back to the origin before that constraint can be properly enforced for noisier inputs at larger timesteps. We depict this behaviour in Figure 1 (left). In order to assess the impact of ODE solving on CMs, we put forth more intuitive and interpretable variation of its loss as LDirect CD fθ(xtn , tn), fsolver(xtn, tn, 0) := Ex0p0,nU λ(tn)d (cid:17)(cid:105) (5) (cid:16) (cid:104) , 1,N (cid:74) ,xtn ptn0(x0) (cid:75) where we directly enforce that all points along trajectory map to its origin, rather than providing only weak supervision as in CMs; see Figure 1 (right). We see this loss as the smallest possible modification to CMs resulting in the direct matching of the model and the solver. Note that unlike standard CMs, Direct CMs do not require enforcing the boundary condition in the parameterization of fθ to prevent collapse, although it is of course still valid to do so. While this loss requires solving the ODE for steps at each iteration and is therefore more computationally expensive than Equation 4, we only propose this formulation for comparative purposes rather than suggesting its use in practice. As we will show, Equation 5 does indeed solve the empirical PF ODE better than Equation 4 but, intriguingly, it translates to worse sample quality. We define the ODE solving error as the expected distance between the ODE solvers solution and the CMs prediction with the same initial noise, i.e., := ExT (cid:98)pT (cid:104) (cid:16) fθ(xT , ), fsolver(xT , T, 0) (cid:17)(cid:105) . (6)"
        },
        {
            "title": "4 Experiments",
            "content": "Training For all of our experiments, we aim to compare CMs and Direct CMs using large-scale and state-of-the-art DMs trained on Internet-scale data to better reflect the performance of these models in practical real-world settings. Hence, we select SDXL [30] as the DM to distill, text-to-image latent diffusion model [36] with 2.6 parameter U-Net backbone [37], capable of generating images at 1024 px resolution. Classifier-free guidance [13] is commonly used to improve sample quality in text-conditional DMs, so we augment in Equation 3 as s(xt, t, c, ω), where is the text prompt and ω is the guidance scale, following Luo et al. [25]. When distilling DM, it is common to initialize the student network from the weights of the teacher network so that, in effect, distillation is reduced to fine-tuning task which requires much less data and resources. We further leverage modern best practices for efficient fine-tuning using low-rank adapters [16, 26]. We use high-quality subset of the LAION-5B dataset [43] called LAION-Aesthetics-6.5+ for training similar to Luo et al. [25]. To ensure controlled comparison of CMs and Direct CMs, the only component in the code that we modify is the loss. See Appendix A.1 for list of training hyperparameters. Evaluation We perform quantitative comparisons using metrics that measure ODE solving quality as well as image quality. For ODE solving, we use (Equation 6, lower is better) which is only valid for single-step generation.1 For image metrics, we use Fréchet Distance on Inception (FID [12], 1As mentioned in Section 2, multi-step sampling in CMs requires adding random noise to the models prediction using the forward SDE. However, the noised prediction will map to different underlying PF ODE trajectory, so comparing it to the original trajectory would not give meaningful metric for ODE-solving fidelity. 4 Figure 2: Samples generated by both CMs and Direct CMs. The samples produced by CMs are clearly of higher quality. All corresponding images are generated from the same initial noise. lower is better) and DINOv2 (FD-DINO [28, 50], lower is better) latent spaces to assess distributional quality, CLIP score (CLIP [33, 11], higher is better) for prompt-image alignment, and aesthetic score (Aes [35], higher is better) as proxy to subjective visual appeal. All generated samples use fixed seeds to ensure consistent random noise. The reference dataset for both FID and FD-DINO uses 10k samples generated from the teacher with the same seeds. Quantitative Analysis We provide quantitative evaluation of CMs and Direct CMs in Table 1. We show performance for three different choices of numerical ODE solvers Φ, namely DDIM [45] following Luo et al. [25], Euler [8], and Heun [38] following Song et al. [49]. As mentioned earlier, is meaningful metric for ODE-solving fidelity only for single-step generation, so we focus our main quantitative analysis on single-step generation; we provide additional image-based metrics for twoand four-step generation in Appendix A.2 for completeness. Across all image-based metrics in Table 1, we observe that CMs convincingly outperform Direct CMs, meaning that training with Equation 4 results in largely superior image quality than training with Equation 5. However, in terms of their ability to more accurately solve the PF ODE, we find that Direct CMs are consistently better. Ironically, the objective of CMs, as presented by Song et al. [49], is motivated by learning to faithfully solve the PF ODE, so it is highly surprising that more accurate solving can translate to worse image quality. Our experiments suggest that the pursuit of diffusion model distillation methods to better solve the PF ODE might be red herring, and that it is not in complete alignment with the goal of generating high-quality samples. Several follow-up works to CMs [46, 7] have further built upon the PF ODE formulation, proposing variations to CMs such as splitting the trajectory into segments [10, 57] or learning to solve the ODE bidirectionally [21] for example. Although they observed better sample quality, we reject the notion that their improvements are strictly entailed by better PF ODE solving. Our results in Table 1 suggest that the high quality of images produced by ODE solving methods (such as CMs and variations) cannot be fully attributed to their ODE solving fidelity; confounding factors should be considered as well. Moreover, we argue that this observed discrepancy between ODE solving and sample quality might suggest that PF ODE solving on its own may not be the most reliable approach to distill diffusion model in practice. It is perhaps unsurprising then that several follow-up works improving upon CMs 5 0.3 0.25 0.2 CM Direct CM 1, 800 0.21 0.2 C 5. 5.2 D - 150 F 100 200 100 200 100 100 200 100 200 100 Figure 3: Effect of the teachers number of discretization intervals . In all cases, we observe that Direct CMs are better at solving the PF ODE, but CMs produce higher quality images. 0.3 0.2 0.1 100 0 10 5 ω 10 5 ω D - CM Direct CM 1,000 500 0 0.21 0. C 5.8 5.4 10 5 ω 0 10 5 ω 0 10 5 ω Figure 4: Effect of the teachers guidance scale ω. We use = 50 here for faster experimentation. In all cases, we observe that Direct CMs are better at solving the PF ODE, but CMs produce higher quality images. rely on auxiliary losses to supplement ODE solving such as adversarial [18, 3, 51, 19], distribution matching [3, 35], and human feedback learning [54, 35] losses. Qualitative Comparison We corroborate our observations with qualitative comparison of CMs and Direct CMs in Figure 2, and include additional samples in Appendix A.3. We show generated samples from fixed seed for one, two and four sampling steps using text prompts from the training set. It is clear that CMs produce higher-quality samples than Direct CMs with better high frequency details and fewer artifacts. Ablations To ensure that our findings are agnostic to hyperparameter selection in the underlying PF ODE and ODE solver, we sweep over various discretization intervals {25, 50, 100, 200} and guidance scales ω {1, 4, 8, 11}, and provide results for single-step generation in Figure 3 and Figure 4. Regardless of the teachers guidance scale and discretization, Direct CMs solve the PF ODE more accurately, yet CMs produce higher quality images."
        },
        {
            "title": "5 Conclusions and Future Work",
            "content": "Although consistency models have achieved success in distilling diffusion models into few-step generators, we find that there exists gap between their theory and practice. Solving the PF ODE is central to the theoretical motivation of CMs, but we show that we can solve the same PF ODE more accurately using Direct CMs while generating samples of noticeably worse quality. Naturally, we question what additional underlying factors might be contributing to the effectiveness of CMs, and call for additional research from the community to bridge this observed gap between solving the PF ODE and generating high-quality samples. We finish by putting forth some potential explanations: (i) since our experiments are carried out with latent diffusion models, the ODEs are defined on the corresponding latent space, and it could be that the closeness to the solvers solutions observed in Direct CMs is undone after decoding to pixel space; (ii) if the pre-trained diffusion model failed to closely approximate the true score function (as could be the case when the true score function is unbounded [29, 23, 22]) then , meaning that even if model closely approximates fsolver and thus s, it need not be the case that it also properly approximates ; and (iii) although both the CM and Direct CM objectives (Equation 4 and Equation 5, respectively) are meant to mimic the solver fsolver at optimality, in practice this optimum is never perfectly achieved, and the CM objective might inadvertently provide beneficial inductive bias which improves sample quality."
        },
        {
            "title": "References",
            "content": "[1] David Berthelot, Arnaud Autef, Jierui Lin, Dian Ang Yap, Shuangfei Zhai, Siyuan Hu, Daniel Zheng, Walter Talbott, and Eric Gu. Tract: Denoising diffusion models with transitive closure time-distillation. arXiv:2303.04248, 2023. [2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv:2311.15127, 2023. [3] Clement Chadebec, Onur Tasar, Eyal Benaroche, and Benjamin Aubin. Flash diffusion: Accelerating any conditional diffusion model for few steps image generation. arXiv:2406.02347, 2024. [4] Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru Zhang. Sampling is as easy as learning the score: Theory for diffusion models with minimal data assumptions. In The Eleventh International Conference on Learning Representations, 2023. [5] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, Matthew Yu, Abhishek Kadian, Filip Radenovic, Dhruv Mahajan, Kunpeng Li, Yue Zhao, Vladan Petrovic, Mitesh Kumar Singh, Simran Motwani, Yi Wen, Yiwen Song, Roshan Sumbaly, Vignesh Ramanathan, Zijian He, Peter Vajda, and Devi Parikh. Emu: Enhancing image generation models using photogenic needles in haystack. arXiv:2309.15807, 2023. [6] Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 8-bit optimizers via block-wise quantization. In International Conference on Learning Representations, 2022. [7] Zhengyang Geng, Ashwini Pokle, William Luo, Justin Lin, and Zico Kolter. Consistency models made easy. arXiv:2406.14548, 2024. [8] David F. Griffiths and Desmond J. Higham. Numerical Methods for Ordinary Differential Equations. Springer, 2010. [9] Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Lingjie Liu, and Joshua Susskind. Boot: Data-free distillation of denoising diffusion models with bootstrapping. In ICML 2023 Workshop on Structured Probabilistic Inference & Generative Modeling, 2023. [10] Jonathan Heek, Emiel Hoogeboom, and Tim Salimans. Multistep consistency models. arXiv:2403.06807, 2024. [11] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: Reference-free Evaluation Metric for Image Captioning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2021. doi: 10.18653/v1/2021.emnlp-main.595. [12] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs Trained by Two Time-Scale Update Rule Converge to Local Nash Equilibrium. In Advances in Neural Information Processing Systems, volume 30, 2017. [13] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv:2207.12598, 2022. [14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, volume 33, 2020. [15] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. In Advances in Neural Information Processing Systems, volume 35, 2022. [16] Edward Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language Models. In International Conference on Learning Representations, 2022. [17] Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiang Yin, and Zhou Zhao. Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models. In Proceedings of the 40th International Conference on Machine Learning, 2023. [18] Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning probability flow ODE trajectory of diffusion. In The Twelfth International Conference on Learning Representations, 2024. [19] Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Yuhta Takida, Naoki Murata, Toshimitsu Uesaka, Yuki Mitsufuji, and Stefano Ermon. PaGoDA: Progressive Growing of One-Step Generator from Low-Resolution Diffusion Teacher. arXiv:2405.14822, 2024. 7 [20] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: versatile diffusion model for audio synthesis. In International Conference on Learning Representations, 2021. [21] Liangchen Li and Jiajun He. Bidirectional consistency models. arXiv:2403.18035, 2024. [22] Gabriel Loaiza-Ganem, Brendan Leigh Ross, Rasa Hosseinzadeh, Anthony Caterini, and Jesse Cresswell. Deep generative models through the lens of the manifold hypothesis: survey and new connections. Transactions on Machine Learning Research, 2024. [23] Yubin Lu, Zhongjian Wang, and Guillaume Bal. Mathematical analysis of singularities in the diffusion model under the submanifold assumption. arXiv:2301.07882, 2023. [24] Eric Luhman and Troy Luhman. Knowledge distillation in iterative generative models for improved sampling speed. arXiv:2101.02388, 2021. [25] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference. arXiv:2310.04378, 2023. [26] Simian Luo, Yiqin Tan, Suraj Patil, Daniel Gu, Patrick von Platen, Apolinário Passos, Longbo Huang, Jian Li, and Hang Zhao. LCM-LoRA: universal stable-diffusion acceleration module. arXiv:2311.05556, 2023. [27] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1429714306, 2023. [28] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. [29] Jakiw Pidstrigach. Score-based generative models detect manifolds. In Advances in Neural Information Processing Systems, volume 35, 2022. [30] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis. In The Twelfth International Conference on Learning Representations, 2024. [31] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In The Eleventh International Conference on Learning Representations, 2023. [32] Markus Rabe and Charles Staats. Self-attention Does Not Need O(n2) Memory. arXiv:2112.05682, 2021. [33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Proceedings of the 38th International Conference on Machine Learning, volume 139, 2021. [34] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv:2204.06125, 2022. [35] Yuxi Ren, Xin Xia, Yanzuo Lu, Jiacheng Zhang, Jie Wu, Pan Xie, Xing Wang, and Xuefeng Xiao. Hyper-SD: Trajectory Segmented Consistency Model for Efficient Image Synthesis. arXiv:2404.13686, 2024. [36] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. [37] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241. Springer, 2015. [38] André Ronveaux and F. M. Arscott. Heuns Differential Equations. Oxford University Press, 1995. [39] Ludan Ruan, Yiyang Ma, Huan Yang, Huiguo He, Bei Liu, Jianlong Fu, Nicholas Jing Yuan, Qin Jin, and Baining Guo. MM-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and Video Generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. [40] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. In Advances in Neural Information Processing Systems, volume 35, 2022. [41] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations, 2022. [42] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. arXiv:2311.17042, 2023. [43] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5B: An open large-scale dataset for training next generation image-text models. In Advances in Neural Information Processing Systems, volume 35, 2022. [44] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In Proceedings of the 32nd International Conference on Machine Learning, volume 37, 2015. [45] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021. [46] Yang Song and Prafulla Dhariwal. Improved techniques for training consistency models. In The Twelfth International Conference on Learning Representations, 2024. [47] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. In Advances in Neural Information Processing Systems, volume 33, 2020. [48] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021. [49] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In Proceedings of the 40th International Conference on Machine Learning, volume 202, 2023. [50] George Stein, Jesse Cresswell, Rasa Hosseinzadeh, Yi Sui, Brendan Ross, Valentin Villecroze, Zhaoyan Liu, Anthony Caterini, Eric Taylor, and Gabriel Loaiza-Ganem. Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models. In Advances in Neural Information Processing Systems, volume 36, 2023. [51] Fu-Yun Wang, Zhaoyang Huang, Alexander William Bergman, Dazhong Shen, Peng Gao, Michael Lingelbach, Keqiang Sun, Weikang Bian, Guanglu Song, Yu Liu, Hongsheng Li, and Xiaogang Wang. Phased consistency model. arXiv:2405.18407, 2024. [52] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. VideoComposer: Compositional Video Synthesis with Motion Controllability. In Advances in Neural Information Processing Systems, volume 36, 2023. [53] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023. [54] Qingsong Xie, Zhenyi Liao, Chen chen, Zhijie Deng, Shixiang Tang, and Haonan Lu. MLCM: Multistep Consistency Distillation of Latent Diffusion Model. arXiv:2406.05768, 2024. [55] Tianwei Yin, Michaël Gharbi, Richard Zhang, Eli Shechtman, Frédo Durand, William T. Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 9 [56] Hongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Azizzadenesheli, and Anima Anandkumar. Fast sampling of diffusion models via operator learning. In Proceedings of the 40th International Conference on Machine Learning, volume 202, 2023. [57] Jianbin Zheng, Minghui Hu, Zhongyi Fan, Chaoyue Wang, Changxing Ding, Dacheng Tao, and Tat-Jen Cham. Trajectory consistency distillation. arXiv:2402.19159, 2024."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Hyperparameters We provide list of default hyperparameter values in Table 2. We only train small number of LoRA blocks [16] following Luo et al. [26], and find that metric and loss curves stabilized around 250 training steps. To enforce the boundary condition in CMs, we follow Song et al. [49] and parameterize fθ(xtn , tn) = cskip(tn)xtn + cout(tn)Fθ(xtn , tn) where Fθ(xtn , tn) in our case is the SDXL U-Net backbone with learnable LoRA blocks, and cskip(tn) and cout(tn) are differentiable functions such that cskip(0) = 1 and cout(0) = 0. We set the values of cskip(tn) and cout(tn) following Luo et al. [25] (see Table 2), and note that this choice is roughly equivalent to step function where, for 1, cskip(tn) 0 and cout(tn) 1. As mentioned in the main text, Direct CMs do not require enforcing boundary condition by construction, but we parameterize them identically to CMs in order to ensure controlled experiments so that we can attribute any differences between them solely to differences in the loss. All experiments were performed on single 48GB NVIDIA RTX 6000 Ada GPU. Table 2: Default hyperparameters for both CMs and Direct CMs, unless otherwise specified. Hyperparameter Batch size Mixed precision Efficient attention [32] Gradient checkpointing Optimizer Adam weight decay Num. training steps LoRA [16] LoRA α [16] Learning rate scheduler Learning rate warmup steps Learning rate Φ ω d(, ) λ(t) σdata τ cskip(t) cout(t) Default Setting 16 fp16 True True 8-bit Adam [6] 102 250 64 64 Constant 0 104 DDIM [45] 100 8 Squared L2 distance 1 0.5 [49] 10 data σ2 (tτ )2+σ2 tτ data (tτ )2+σ2 data Table 3: Additional image results for twoand four-step generation. Φ"
        },
        {
            "title": "Method",
            "content": "ODE 1-step 1-step 2-step 4-step 1-step 2-step 4-step 1-step 2-step 4-step 1-step 2-step 4-step FD-DINO CLIP FID"
        },
        {
            "title": "Image",
            "content": "Aes"
        },
        {
            "title": "Heun",
            "content": "CM CM"
        },
        {
            "title": "0.29\nDirect CM 0.25\n0.29\nDirect CM 0.23\n0.30\nDirect CM 0.25",
            "content": "CM 103.9 158.6 95.3 166.0 120.5 162.0 33.4 55.0 27.4 55.7 33.5 54.8 19.8 21.2 18.9 22.5 20.7 21.0 816.3 1095 747.7 1148 846.1 1126 255.4 346.8 221.3 357.3 233.4 341. 159.8 155.1 156.8 152.7 159.4 150.6 0.21 0.20 0.21 0.19 0.21 0.19 0.27 0.26 0.27 0.25 0.27 0.26 0.27 0.28 0.27 0.27 0.27 0.28 5.6 5.1 5.5 5.0 5.5 5.1 6.4 6.2 6.5 6.1 6.5 6. 6.7 6.5 6.7 6.4 6.7 6.4 11 A.2 Additional Quantitative Results We provide additional quantitative image analysis for twoand four-step generation in Table 3. In almost all cases, these results demonstrate that CMs generate higher quality images than Direct CMs akin to the single-step generation case. Although these results suggest that for four steps Direct CMs slightly outperform CMs in terms of FD-DINO and CLIP score, qualitative comparisons of generated images between both models (see examples in Figure 2 and Figure 5) quickly reveal that images from CMs have noticeably higher quality. We thus attribute the discrepancy either to imperfections in generative model evaluation metrics as observed by Stein et al. [50], or to these metrics not perfectly matching aesthetic quality and being affected by additional confounders (e.g., FD-DINO scores are meant to reflect image diversity in addition to image aesthetics). 12 A.3 Additional Qualitative Results Figure 5: Additional images generated by both CMs and Direct CMs, further highlighting the sample quality difference between the two models. All corresponding images are generated from the same initial noise."
        }
    ],
    "affiliations": [
        "Layer 6 AI"
    ]
}