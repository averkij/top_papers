{
    "paper_title": "Efficient Gaussian Splatting for Monocular Dynamic Scene Rendering via Sparse Time-Variant Attribute Modeling",
    "authors": [
        "Hanyang Kong",
        "Xingyi Yang",
        "Xinchao Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Rendering dynamic scenes from monocular videos is a crucial yet challenging task. The recent deformable Gaussian Splatting has emerged as a robust solution to represent real-world dynamic scenes. However, it often leads to heavily redundant Gaussians, attempting to fit every training view at various time steps, leading to slower rendering speeds. Additionally, the attributes of Gaussians in static areas are time-invariant, making it unnecessary to model every Gaussian, which can cause jittering in static regions. In practice, the primary bottleneck in rendering speed for dynamic scenes is the number of Gaussians. In response, we introduce Efficient Dynamic Gaussian Splatting (EDGS), which represents dynamic scenes via sparse time-variant attribute modeling. Our approach formulates dynamic scenes using a sparse anchor-grid representation, with the motion flow of dense Gaussians calculated via a classical kernel representation. Furthermore, we propose an unsupervised strategy to efficiently filter out anchors corresponding to static areas. Only anchors associated with deformable objects are input into MLPs to query time-variant attributes. Experiments on two real-world datasets demonstrate that our EDGS significantly improves the rendering speed with superior rendering quality compared to previous state-of-the-art methods."
        },
        {
            "title": "Start",
            "content": "Efficient Gaussian Splatting for Monocular Dynamic Scene Rendering via Sparse Time-Variant Attribute Modeling Hanyang Kong, Xingyi Yang, Xinchao Wang* National University of Singapore hanyang.k@u.nus.edu, xyang@u.nus.edu, xinchao@nus.edu.sg 5 2 0 2 7 2 ] . [ 1 8 7 3 0 2 . 2 0 5 2 : r Abstract Rendering dynamic scenes from monocular videos is crucial yet challenging task. The recent deformable Gaussian Splatting has emerged as robust solution to represent realworld dynamic scenes. However, it often leads to heavily redundant Gaussians, attempting to fit every training view at various time steps, leading to slower rendering speeds. Additionally, the attributes of Gaussians in static areas are timeinvariant, making it unnecessary to model every Gaussian, which can cause jittering in static regions. In practice, the primary bottleneck in rendering speed for dynamic scenes is the number of Gaussians. In response, we introduce Efficient Dynamic Gaussian Splatting (EDGS), which represents dynamic scenes via sparse time-variant attribute modeling. Our approach formulates dynamic scenes using sparse anchorgrid representation, with the motion flow of dense Gaussians calculated via classical kernel representation. Furthermore, we propose an unsupervised strategy to efficiently filter out anchors corresponding to static areas. Only anchors associated with deformable objects are input into MLPs to query time-variant attributes. Experiments on two real-world datasets demonstrate that our EDGS significantly improves the rendering speed with superior rendering quality compared to previous state-of-the-art methods."
        },
        {
            "title": "Introduction",
            "content": "Novel view synthesis (NVS) is pivotal challenge in the field of 3D vision, essential for applications such as virtual reality, augmented reality, and film production. NVS involves generating images from arbitrary viewpoints or times within scene, typically necessitating accurate reconstruction based on several 2D images. While recent advances in diffusion models (Kong et al. 2025; Yu et al. 2024; Ma, Fang, and Wang 2024) have shown promise in NVS, dynamic scenes remain challenging due to the need to model complex motions and the need for real-time processing. Recent advancements 3D Gaussian Splatting in (3DGS) (Kerbl et al. 2023) provide new tools to tackle these challenges by enabling real-time rendering. Extensions of 3DGS, such as (Huang et al. 2023; Wu et al. 2023; Yang et al. 2023), have further enhanced its ability to handle dynamic monocular scenes. For example, Deformable *Corresponding author. Copyright 2025, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. 3DGS (Yang et al. 2023) incorporates deformable network to capture motion fields, enhancing 3DGSs adaptability to dynamic scenes. Similarly, 4DGS (Wu et al. 2023) utilizes hexplane-based (Cao and Johnson 2023) encoder to optimize deformation queries, making the process more efficient. While previous methods deliver high-quality renderings, they tend to generate an excessive number of Gaussians, resulting in considerable redundancy. We analyze the relationship between the rendering speed w.r.t. number of Gaussians based on (Yang et al. 2023) in Fig. 1(c). As result, the rendering speed declines as the number of redundant Gaussians increases. Inspired by recent work (Lu et al. 2023) which improved efficiency in static scenes by sparsifying and reducing Gaussian points, we question: Can we achieve more efficient and compact representation of dynamic scenes without compromising rendering quality? In this paper, we affirmatively answer this question by introducing Efficient Dynamic Gaussian Splatting (EDGS). This method efficiently represents dynamic scenes as set of sparse time-variant attributes. At its heart, EDGS separately processes time-invariant attributes, time-variant attributes, and motions. This design allows precise control over each aspect of the scenes dynamics. For time-invariant attributes, EDGS uses sparse anchorgrid initialization to capture geometry and appearance. These attributes, such as color and opacity, are decoded by lightweight MLPs, enabling accurate deformation modeling. Unlike prior methods that process all time-variant Gaussian attributes (e.g., location, scale, quaternion) indiscriminately through MLPs, EDGS adopts selective approach. tiny MLP filters static anchors, identifying those that are deformable. Only these deformable anchors are then processed by corresponding MLPs for their dynamic attributes. Notably, this selection process is trained in fully unsupervised manner. Moreover, we efficiently model the movements of deformable Gaussians in sparse manner. Specifically, we track the movements of anchors linked to deformable objects at each time step. The offsets for corresponding Gaussians are computed using radial basis function (RBF) kernel. This sparse approach enables precise and efficient rendering of dynamic changes. By integrating these techniques, we carried out detailed Figure 1: (a) Given set of monocular multi-view images and camera poses, our method achieves real-time rendering for dynamic scenes while maintaining high rendering quality. (b) Our method achieves promising rendering quality with faster rendering speed and fewer Gaussians. The radius of the circle is the number of time-variant Gaussians whose attributes need to be queried by MLPs. (c) The bottleneck of the rendering speed for dynamic scenes is the number of Gaussians. The fewer the number of Gaussians, the faster the rendering speed. experiments on two real-world datasets, NeRF-DS (Yan, Li, and Lee 2023) and HyperNeRF (Park et al. 2021b). We visualize the rendered novel views and show the rendering speed in Fig. 1. Our method achieves much faster rendering speed with higher PSNR score and fewer points that the time-variant attributes need to be queried by MLPs. In summary, our contributions are summarized as follows: We formulate deformable 3DGS through sparse, timevariant attribute modeling and introduce novel unsupervised strategy to filter out Gaussians with time-invariant attributes. We employ classical kernel representation to formulate motion flow sparsely for the deformable Gaussians. Our EDGS achieves faster rendering speed compared with other state-of-the-art methods with higher rendering quality."
        },
        {
            "title": "2 Related Works\nDynamic 3D scene reconstruction. 3D Gaussian Splat-\nting (3DGS) (Kerbl et al. 2023) offers a distinct representa-\ntion for novel view synthesis with improved rendering qual-\nity and speed. Several concurrent works (Wang et al. 2025;\nWu et al. 2023; Yang et al. 2023; Huang et al. 2023) have\nadapted 3DGS for reconstructing dynamic scenes. For in-\nstance, Deformable 3DGS (Yang et al. 2023) formulates the\ndynamic field based on multi-layer perceptron (MLP) with\ndifferentiable rendering. 4DGS (Wu et al. 2023) improves\nthe rendering speed with a compact network. SCGS (Huang\net al. 2023) explicitly decomposes the motion and dynamic\nscenes into sparse control points and the deformation of\nGaussians is controlled by its k-nearest control points.",
            "content": "Though achieving promising rendered quality, the rendering speed for dynamic scenes is much slower than rendering static scenes (100 FPS v.s. 20 FPS at 1K resolution). The bottleneck of the rendering speed for dynamic scenes is the number of Gaussian points. As shown in Fig. 1, as more Gaussians are densified, an enormous amount of Gaussians are fed into the deformation network, which leads to slower rendering speed. Moreover, (Huang et al. 2023; Yang et al. 2023; Wu et al. 2023) query the attributes of all Gaussians at each timestep, even though the static areas in real-world scenes are time-invariant and do not require querying by MLPs. As shown in Fig. 5, the static areas of (Yang et al. 2023; Wu et al. 2023)s rendered results are still jittering across time. Grid-based rendering for acceleration. Grid-based representations are based on dense uniform grid of voxels (Fridovich-Keil et al. 2022, 2023; Cao and Johnson 2023; Lu et al. 2023) explicitly or implicitly. For instance, KPlanes (Fridovich-Keil et al. 2023) applies neural planes to parameterize 3D scene, with an additional temporal plane to accommodate dynamics. HexPlane (Cao and Johnson 2023) further enhances the neural planes for time and space by factorizing space and time into compact neural representations. Scaffold-GS (Lu et al. 2023) introduces anchorvoxel-based strategy to achieve reduced storage parameters for static scenes. Scaffold-GS can render similar speed (100 FPS at 1K resolution) as 3DGS and the storage requirements are significantly reduced as Scaffold-GS only needs to store anchor points and MLPs for each scene. However, Scaffold-GS is designed for real-world static scenes and we are the first to adapt anchor-voxel-based strategy to dynamic scene reconstruction."
        },
        {
            "title": "3 Preliminaries",
            "content": "In this section, we simply review the representation and rendering process of 3DGS (Kerbl et al. 2023) with the formula of deformable 3DGS (Wu et al. 2023; Yang et al. 2023) and our baseline method (Lu et al. 2023). Figure 2: The pipeline of our EDGS. 1) We first initialize voxelized sparse anchor points from Structure from Motion (SfM) points derived from COLMAP. 2) time-mask MLP is applied to classify if the anchor belongs to the static area or the deformable area. 3) Gaussian offsets are initialized for each anchor belonging to static area. The time-invariant attributes of each Gaussian, i.e., opacity, quaternion, scale, and color are calculated from its feature by corresponding tiny MLPs. 4) Timevariant attributes for anchors from dynamic areas are decoded by deformable attribute MLP. RBF kernel function is employed to compute the location of each Gaussian at timestep by calculating the similarity between each Gaussian and its belonging anchor point. This pipeline is compact and efficient, featuring only few tiny MLPs for the attributes of the Gaussians and single network for deformations. Notably, the position of each anchor remains static and is not subject to updates."
        },
        {
            "title": "3.1 Dynamic 3D Gaussian Splatting\n3DGS (Kerbl et al. 2023) is an explicit 3D representa-\ntion in the form of point clouds. The process starts with\n3D point clouds generated through Structure-from-Motion\n(SfM). Each Gaussian G(x) is defined by a mean position µ\nand a covariance matrix Σ\nG(x) = e− 1",
            "content": "2 (xµ)T Σ1(xµ), (1) where is 3D point location within the 3D scene. Σ is formulated using scaling matrix and rotation matrix R: Σ = RSST RT . (2) To render an image from random viewpoint, 3D Gaussians are first splatted to 2D, and render the pixel value by the following formula: = (cid:88) ciαi i1 (cid:89) (1 αi), (3) iN j=1 where ci and αi represent the density and view-dependent color of the point, and is the number of sorted Gaussians contributing to the rendering process. To model the dynamic 3D Gaussians that vary over time, (Huang et al. 2023; Yang et al. 2023; Wu et al. 2023) decouple the 3D Gaussians and the deformation field, separately. To be specific, given time and the location of 3D Gaussians as input, deformable 3DGS methods (Huang et al. 2023; Yang et al. 2023; Wu et al. 2023) apply the trained deformation MLP to produce offsets, which subsequently transform the 3D Gaussians from canonical space to the deformed space: x, r, = F(γ(x, t)), (4) where γ is the encoding of space and time, x, r, and are the offsets of the Gaussian location x, quaternion r, and scaling s, respectively. Subsequently, the deformed 3D Gaussians G(x + x, + r, + s) are fed into the differential Gaussian rasterization pipeline for rendering novel views with various times t. All attributes of 3D Gaussians and the deformable network are learnable and optimized end-to-end directly via training view reconstruction. Though achieving promising results, the rendering speed of dynamic scenes remains significantly slower than that of static scenes (e.g., 20 FPS vs. 100 FPS for 1K resolution). This slowdown is due to the redundancy of Gaussians and the need to query the attributes of all Gaussians using the deformation network F. sparse Gaussian representation would accelerate the rendering process. Additionally, it is more efficient to query only the attributes of Gaussians associated with deformable objects, as the attributes of static areas remain unchanged over time."
        },
        {
            "title": "3.2 Scaffold-GS\nScaffold-GS (Lu et al. 2023) adheres to the framework\nof 3DGS for static scenes and introduces a storage-\nfriendly anchor-based strategy. Scaffold-GS derives Gaus-\nsians from the anchors and deduces the attributes from the\nattributes of the attached anchors through MLPs, rather\nthan storing Gaussians directly. To be specific, given\nan anchor point located at xa and its attributes A =\n(cid:8)f ∈ Rd, l ∈ R6, o ∈ R3K(cid:9), where f , l, and o are the an-\nchor feature, attribute scaling, and Gaussian offsets of an-\nchor xa, respectively. K is the number of Gaussian offsets\nbelonging to anchor xa. During rendering, f a are fed into\nMLPs to generate attributes A for Gaussians. The location\nof each Gaussian are calculated by adding x and Gaussian\noffsets o and l is applied to scale the location and shape of\nthe Gaussians.",
            "content": "Scaffold-GS achieves similar rendering speed as the original 3DGS for the static scenes and the storage requirements are significantly reduced as Scaffold-GS only stores anchor points and MLPs for each scenes. The number of anchor points is much fewer than the number of Gaussians. As discussed earlier, the bottleneck of the rendering speed for dynamic scenes is the number of Gaussians. To this end, we extend the anchor-based strategy to the dynamic scene reconstruction for faster rendering speed."
        },
        {
            "title": "4.1 Anchor-grid Initialization\nThe static anchor-grid representation is initialized by the\nsparse point cloud from COLMAP (Schonberger and Frahm\n2016). To be specific, given the 3D point cloud P ∈ RM ×3,\nwe first voxelize the scene by:\n(cid:22) P\n∆d\nwhere A = {a0, a1, · · · , aN −1} ∈ RN ×3 is the structured\nanchor-grid, ⌊·⌋ is the floor operation, and ∆d is the voxel\nsize. The floor operation removes the redundant and over-\ndense points P.",
            "content": "d, = (5) (cid:23)"
        },
        {
            "title": "4.2 Gaussian Attribute Derivation\nWe first introduce how to derive Gaussian’s attribute, i.e.,\nopacity, quaternion, scaling, and color from anchor points\nat various timestep t. The attributes of each Gaussian are de-\ncoded from its feature and time positional encoding γ by tiny\nMLPs Φ∗(·), where Φ∗(·) is the tiny MLP for the attribute\n∗. Given one anchor a, there are K attributes decoded by\nΦ∗(·) where K is the number of Gaussians belonging to the\nanchor a. Time-invariant attributes, i.e., opacity and color,\nare decoded by Φα(fa) and Φc(fa), where fa are anchor’s\nfeature. Quaternion, scale, and Gaussian offsets vary across\ntime. For anchor a with scale sa, the scale of its belonging\nkth Gaussian offsets at time t is calculated by",
            "content": "o = sa + sk sk,t where sk o, s1 o, , sK1 longing to anchor and sk,t + sk,t , (6) (cid:9) are learnable scales beo is the offset variation at time (cid:8)s0 and xk,t is calculated by sk,t = Φs(cat {fa, γ(t)}), (7) where cat is the concatenation operation. The quaternion rk,t is calculated in similar manner."
        },
        {
            "title": "4.3 Gaussian Offsets Derivation with RBF Kernel\nAs formulated in Eq. (4), previous methods (Wu et al.\n2023; Yang et al. 2023; Huang et al. 2023) query Gaus-\nsians attributes by the deformation network F. Intuitively,\nthe movements of anchor a’s K Gaussians should be se-\nmantically aligned. For rigid objects, the deformation of K\nGaussians is exactly the same as its anchor a’s movement.\nThe strict assumption seriously affects the performance for\nrendering novel views of deformed objects because there are\nso many non-rigid objects in the rendering scenes. Another\nSOTA solution is to formulate the deformation of offsets\nbased on anchors is k-nearest neighborhood method (KNN).\nFor instance, SCGS (Huang et al. 2023) calculates the de-\nformation of Gaussians based on k-nearest control points.\nHowever, KNN is calculated by the distance between points\nand the performance is degraded when the moving objects\nare separated after collision.",
            "content": "In order to formulate the deformation of objects in semantic manner, we calculate the location of anchor as kth Gaussian offsets at timestep with radial basis function (RBF) kernel. To be specific, we first apply deformation MLP to calculate the movement of anchor across time t: = F(γ(xa, t)), (8) where xa is the location of anchor and γ is the position encoding of location and time. The movement of anchor as kth Gaussians at time is calculated by xt = K(fa, xk,t ) xt a, (9) are the features of anchor and its kth where fa and belonging Gaussians. K(, ) is the kernel basis function on anchors feature fa and its offsets feature . Here we use the common radial basis function (RBF) kernel: K(fa, ) = exp( (cid:13) (cid:13)fa 2σ2 (cid:13) 2 (cid:13) ), (10) where σ is the covariance of the RBF kernel. We set σ = 1 in all experiments. Similar to calculating scale and quaternion, the location of anchor as kth Gaussian is calculated as = xa + xk xk,t + xk,t , (11)"
        },
        {
            "title": "4.4 Time Mask\nAs discussed earlier, the scales, quaternion, and location of\nGaussians belonging to static scenes are not changed over\ntime. The rendering speed would be faster if we first filter\nthe anchors belonging to static scenes and then only query\ntime-variant attributes by MLPs for anchors that are respon-\nsible for deformation objects. To this end, we introduce a\ntime mask MLP Φmask to filter the anchors responsible for\nthe deformation objects. Φmask is a tiny binary-classifier.\nAll anchors’ features fa = (cid:8)f 0\n(cid:9) ∈ RN ×d",
            "content": "a, , 1 a, 1 Method 3D-GS (Kerbl et al. 2023) TiNeuVox (Fang et al. 2022) HyperNeRF (Park et al. 2021b) NeRF-DS (Yan, Li, and Lee 2023) 4DGS (Wu et al. 2023) SCGS (Huang et al. 2023) Deformable 3DGS (Yang et al. 2023) Ours Method 3D-GS (Kerbl et al. 2023) TiNeuVox (Fang et al. 2022) HyperNeRF (Park et al. 2021b) NeRF-DS (Yan, Li, and Lee 2023) 4DGS (Wu et al. 2023) SCGS (Huang et al. 2023) Deformable 3DGS (Yang et al. 2023) Ours PSNR 23.16 21.49 25.43 25.78 26.11 25.93 25.70 27. PSNR 21.71 19.71 24.59 24.91 24.57 24.32 24.86 25.08 Sieve SSIM LPIPS 0.2247 0.8203 0.3176 0.8265 0.1645 0.8798 0.1472 0.8900 0.1107 0.9193 0.1194 0.9187 0.1504 0.8715 0.1151 0.9271 Cup SSIM LPIPS 0.2548 0.8304 0.3643 0.8109 0.1650 0.8770 0.1737 0.8741 0.1185 0.9102 0.1207 0.9121 0.1532 0.8908 0.1225 0.9132 PSNR 16.14 20.58 18.93 20.54 20.41 20.17 20.48 21.21 PSNR 22.69 21.26 25.58 25.13 26.30 26.17 26.31 26.65 Plate SSIM LPIPS 0.4093 0.6970 0.3317 0.8027 0.2940 0.7709 0.1996 0.8042 0.2010 0.8311 0.2104 0.8257 0.2224 0.8124 0.1873 0.8957 As SSIM LPIPS 0.2994 0.8017 0.3967 0.8289 0.1777 0.8949 0.1741 0.8778 0.1499 0.8917 0.1491 0.8851 0.1783 0.8842 0.1472 0.9015 PSNR 21.01 23.08 23.06 23.19 25.70 25.97 25.74 26.01 PSNR 18.42 20.66 20.41 19.96 19.01 19.23 19.67 19.91 Bell SSIM LPIPS 0.2503 0.7885 0.2568 0.8242 0.2052 0.8097 0.1867 0.8212 0.1103 0.9088 0.1167 0.9172 0.1537 0.8503 0.1204 0.9203 Basin SSIM LPIPS 0.3153 0.7170 0.2690 0.8145 0.1911 0.8199 0.1855 0.8166 0.1631 0.8277 0.1514 0.8379 0.1901 0.7934 0.1640 0.8351 PSNR 22.89 24.47 26.15 25.72 26.72 26.57 26.01 26. PSNR 20.29 21.61 23.45 23.60 24.18 24.05 24.11 24.65 Press SSIM LPIPS 0.2904 0.8163 0.3001 0.8613 0.1959 0.8897 0.2047 0.8618 0.1301 0.9031 0.1367 0.8971 0.1905 0.8646 0.1313 0.9054 Mean SSIM LPIPS 0.2920 0.7816 0.2766 0.8234 0.1990 0.8488 0.1816 0.8494 0.1405 0.8845 0.1439 0.8848 0.1769 0.8524 0.1411 0.8998 Table 1: Quantitative comparison on NeRF-DS dataset per-scene. We color each cell as best , second best , and third best . are fed into Φmask. Only the anchors with output label 1 are further fed into corresponding MLPs for time-variant attributes. We further introduce time mask regularization term Ltmask to optimize all the learnable parameters with few anchors that are responsible for deformation objects: Ltmask = 1 (cid:88) n=0 Φmask(f )/N, (12) where is the number of anchors."
        },
        {
            "title": "4.6 Loss Function\nSimilar to previous methods (Kerbl et al. 2023; Huang et al.\n2023; Yang et al. 2023; Wu et al. 2023), we optimize the fea-\nture f each anchor a and its offsets oa, tiny MLPs Fα, Fr,\nFs, and Fc for the attributes of Gaussians, and anchor’s de-\nformable network with respect to the L1 loss and SSIM loss\nLSSIM with a time mask regularization term Lt−mask over\nthe rendered RBG images to supervise the training process.\nOur final loss function is defined as:",
            "content": "L = (1 λ)L1 + λLSSIM + λtLtmask, (13) where λ = 0.2 and λt = 0.2 in our all experiments. It should be noted that during the training process, the position of each anchor remains frozen and does not undergo any updates. Model Nerfies (Park et al. 2021a) HyperNeRF (Park et al. 2021b) TiNeuVox-B (Fang et al. 2022) 3D-GS (Kerbl et al. 2023) FFDNeRF (Guo et al. 2023) Deformable 3DGS (Yang et al. 2023) 4DGS (Wu et al. 2023) SCGS (Huang et al. 2023) Ours PSNR(dB) MS-SSIM 22.2 22.4 24.3 19.7 24.2 25.0 25.2 24.6 25.7 0.803 0.814 0.836 0.680 0.842 0.822 0.845 0.813 0.860 FPS < 1 < 1 1 32 0.05 13 34 12 117 Table 2: Quantitative results on HyperNeRFs (Park et al. 2021b) vrig dataset. The rendering resolution is set to 960540. We color each cell as best , second best , and third best ."
        },
        {
            "title": "5.1 Experimental Setup\nDataset and metrics. We conducted extensive exper-\niments on two real-world datasets:\nthe HyperNeRF\ndataset (Park et al. 2021b) and the NeRF-DS dataset (Yan,\nLi, and Lee 2023). The division on the training and test-\ning subsets and other experimental protocols are perfectly\naligned with the original papers. The metrics we apply to\nevaluate the performance are Peak Signal-to-Noise Ratio\n(RSNR), Structural Similarity (SSIM), and Learned Percep-\ntual Image Patch Similarity (LPIPS) (Zhang et al. 2018).\nApart from these commonly used metrics, we additionally\nreport the training time and the rendering speed (FPS) for\nmodel compactness and efficiency. We report the metrics\nper scene on the NeRF-DS dataset (Yan, Li, and Lee 2023)\nand the averaged metrics over all scenes on the HyperN-\neRF (Park et al. 2021b) dataset.",
            "content": "Baselines and implementation. To evaluate the performance of novel view synthesis for real-world dynamic scenes, we conducted benchmarks against several state-ofthe-art methods in the field, including NeRF-based methods (Fang et al. 2022; Park et al. 2021b; Yan, Li, and Lee Figure 3: Qualitative comparison on the NeRF-DS dataset (Yan, Li, and Lee 2023). Compared with other SOTA methods, our method reconstructs finer details and produces structured rendering of the moving objects, e.g., the cup on humans hand. 2023; Park et al. 2021a; Song et al. 2023; Attal et al. 2023; Fridovich-Keil et al. 2023; Lin et al. 2023; Wang et al. 2023) and 3DGS-based methods (Kerbl et al. 2023; Huang et al. 2023; Yang et al. 2023; Wu et al. 2023). Our implementation is primarily based on PyTroch (Paszke et al. 2019) framework and evaluated by Nvidia V100 GPU. Most of our hyper-parameters follow 3DGS (Kerbl et al. 2023). Our pipeline is trained for 30k iterations. We set = 10 offset Gaussians for each anchor and the dimension of the anchor features fa and offset features fo is 8. The tiny MLPs Φ for Gaussian attributes are two-layer MLPs and the dimension of the middle layer is 64. The anchors deformation MLP is four-layer fully connected layers that employ ReLU activation and the dimension of intermediate layers is 128. The voxel size for the initialization of anchors is 0.6."
        },
        {
            "title": "5.2 Quantitative Comparisons\nNeRF-DS dataset. We first compare our method with\nbaselines using monocular real-world NeRF-DS (Yan, Li,\nand Lee 2023) dataset. The experimental results on the\nNeRF-DS dataset, presented in Tab. 1, clearly demonstrate\nthe superiority of our proposed method across multiple eval-\nuation metrics and scenes. our method achieves the highest\nmean PSNR (24.39) and SSIM (0.8873) across all scenes,\nindicating its superior performance and consistency. More-\nover, with the help of the structured anchor and the for-\nmulation between anchor and Gaussian offsets, our method\nachieves high rendering performance for moving objects.\nPlease refer to Fig. 3 for visualization.",
            "content": "HyperNeRF dataset. We compare our method with other baselines on the HyperNeRF dataset (Park et al. 2021b). The experimental results on HyperNeRFs VRIG dataset, shown Figure 4: Qualitative comparison on the HyperNeRF dataset (Park et al. 2021b). Our EDGS reconstructs detailed texture and reliable structure compared with other SOTA methods. in Tab. 2, highlight the superior performance of our proposed method. Achieving the highest PSNR of 25.7 dB and MS-SSIM of 0.860, our approach ensures exceptional image quality and structural fidelity. Furthermore, it demonstrates remarkable efficiency with rendering time of just 20 minutes and an impressive 117 FPS, significantly outperforming other methods. Additionally, our method requires only 7K Gaussians which need to query time-variant attributes by MLP, the lowest among all compared techniques, underscoring its computational efficiency. These results validate the robustness and practicality of our EDGS for dynamic scene reconstruction. Figure 5: Visuazization of the difference map (diff.) and the optical flow with fixed camera views. We synthesis fixed novel view across time for (Yang et al. 2023; Wu et al. 2023) and ours. The 1st row is the rendered frames at various time steps. The 2nd and 3rd rows are the difference map between tth frame and the 1st frame and the optical flow, respectively. The response in the highlighted red area indicates that the static area rendered by deformable GS and 4DGS is jittering. Our method achieves better quality for static and dynamic objects."
        },
        {
            "title": "5.3 Qualitative Comparisons",
            "content": "We conduct qualitative comparisons to illustrate the advantages of our method over other SOTA methods. The comparisons on the NeRF-DS dataset are shown in Fig. 3. Please zoom in to the highlighted red region for rendering comparisons of the moving objects. Compared with other SOTA methods, our method reconstructs finer details and produces more structured rendering of the moving objects, e.g., the cup in hand. We also visualize the rendered results with fixed cameras and show the difference map between each frame and the 1st frame and the optical flow of the rendered video in Fig. 5. As highlighted in the red box in Fig. 5, deformable 3DGS and 4DGS fail to render the static area. The difference map and the optical flow at the static area indicates that the static area is jittering for those methods. We render the static and dynamic objects more accurate with fewer jittering issues because of the time-mask MLP. We also visualize the mask predicted by the time-mask MLP in Fig. 6. The visualization comparisons on the HyperNeRF dataset (Park et al. 2021b) are shown in Fig. 4. Compared to other SOTA methods, our EDGS reconstructs finer details (e.g., the red chicken toy and the banana in hand) and produces more structured rendering of moving objects."
        },
        {
            "title": "5.4 Ablation Studies",
            "content": "Efficacy of anchor-voxel strategy and time-mask MLP. We assess the efficiency of the anchor-voxel strategy and time-mask MLP on three scenes from the NeRF-DS (Yan, Li, and Lee 2023) dataset. Without the anchor-voxel strategy, rendering speed drops to around 20 FPS due to redundant densified Gaussians, showing that the bottleneck in rendering deformation scenes is the number of Gaussians. Introducing the time-mask MLP further improves rendering speed by filtering out anchors corresponding to static areas, allowing only those in dynamic areas to be queried for timevariant attributes. This also enhances rendering quality (e.g. PSNR increases from 26.18 to 27.12 on the sieve scene), likely because the MLPs are optimized with fewer anchor points, making the optimization process more effective. See Fig. 6 for the time mask visualization. scene As PSNR FPS Bell PSNR FPS Sieve PSNR FPS w/o anchor-grid init w/o time-mask full model 24.57 26.02 26.65 27 127 24.31 25.78 26.01 23 132 147 24.74 26.18 27. 17 122 151 Table 3: Effects of anchor-voxel strategy and time-mask MLP. The anchor-grid strategy is crucial for rendering speed due to the reduced number of Gaussians. The time-mask MLP filters time-variant anchors, further reducing the number of anchors required for time-variant attribute queries. Efficacy of time-variant Gaussian offsets derivation with various strategies. Tab. 4 compares time-variant Gaussian offsets derived using different strategies across three scenes on the NeRF-DS (Yan, Li, and Lee 2023) dataset. Regarding the rigid transformation strategy, we assume that the movements of the anchors and their corresponding Gaussians same. The lowest performance shows that simply regarding all deformation objects as rigid ones degrades the performance. We further calculate the similarity between the feature of anchor and Gaussian offsets using KNN method and cosine similarity. The KNN strategy performs well for the reconstruction of single deformation objects (Huang et al. 2023) and the performance degrades on the real-world scenes. Cosine similarity achieves similar performance compared with RBF kernel but is VRAM-consuming because the similarity is calculated by (KN ) huge matrix. scene As Bell Sieve PSNR SSIM PSNR SSIM PSNR SSIM rigid transformation KNN cosine similarity 23.51 25.75 26.31 0.8259 0.8954 0.8928 24.21 25.53 25.74 0.8571 0.8816 0.8807 23.95 25.68 26. 0.8750 0.8907 0.9155 RBF kernel 26.65 0.9015 26.01 0. 27.12 0.9271 Table 4: Gaussian offsets derivation with various strategies. We conduct four different strategies for formulating the variation of the Gaussian offsets across time. Visualization of anchor features and time mask. We perform an analysis of the learnable anchor features to assess their effectiveness in dynamic scene reconstruction. As shown in Fig. 6, the clustered patterns indicate that the anchor features learn and encode similar semantic meanings across both static and dynamic objects of the scene. Additionally, the time-mask MLP Φmask effectively classifies time-variant anchors without any mask supervision, demonstrating its ability to adaptively distinguish between static and dynamic regions. Figure 6: Visualization of anchor features and time mask. We visualize the rendered images, anchor features, and time masks for two deformation scenes. Anchor features are visualized using the UMAP function. In the time-mask visualization, the time masks are predicted by time-mask MLP Φmask. Green anchors represent static scenes, while red anchors indicate deformation scenes."
        },
        {
            "title": "6 Conclusion\nIn conclusion, we develop Efficient Dynamic Gaussian\nSplatting (EDGS). It efficiently models dynamic scenes by\nemphasizing sparse, time-variant attributes and selectively\nprocessing static objects. This strategy significantly reduces\ncomputational complexity while preserving high rendering\nquality. Additionally, the incorporation of a classical ker-\nnel for motion flow optimization further enhances this pro-\ncess. Our evaluations using the NeRF-DS and HyperNeRF\ndatasets show that EDGS not only achieves faster rendering\nspeeds but also higher Peak Signal-to-Noise Ratio (PSNR)\nscores, surpassing current state-of-the-art methods.",
            "content": "Acknowledgements This project is supported by the Ministry of Education, Singapore, under its Academic Research Fund Tier 2 (Award Number: MOE-T2EP20122-0006), and the National Research Foundation, Singapore, under its Medium Sized Center for Advanced Robotics Technology Innovation. References Attal, B.; Huang, J.-B.; Richardt, C.; Zollhoefer, M.; Kopf, J.; OToole, M.; and Kim, C. 2023. HyperReel: High-fidelity 6-DoF video with ray-conditioned sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1661016620. Cao, A.; and Johnson, J. 2023. Hexplane: fast representation for dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 130141. Fang, J.; Yi, T.; Wang, X.; Xie, L.; Zhang, X.; Liu, W.; Nießner, M.; and Tian, Q. 2022. Fast dynamic radiance In SIGGRAPH Asia fields with time-aware neural voxels. 2022 Conference Papers, 19. Song, L.; Chen, A.; Li, Z.; Chen, Z.; Chen, L.; Yuan, J.; Xu, Y.; and Geiger, A. 2023. Nerfplayer: streamable dynamic scene representation with decomposed neural radiance fields. IEEE Transactions on Visualization and Computer Graphics, 29(5): 27322742. Wang, F.; Chen, Z.; Wang, G.; Song, Y.; and Liu, H. 2023. Masked Space-Time Hash Encoding for Efficient Dynamic Scene Reconstruction. Advances in neural information processing systems. Wang, S.; Yang, X.; Shen, Q.; Jiang, Z.; and Wang, X. 2025. GFlow: Recovering 4D World from Monocular Video. In Proceedings of the AAAI Conference on Artificial Intelligence. Wu, G.; Yi, T.; Fang, J.; Xie, L.; Zhang, X.; Wei, W.; Liu, W.; Tian, Q.; and Wang, X. 2023. 4d gaussian splatting for real-time dynamic scene rendering. arXiv preprint arXiv:2310.08528. Yan, Z.; Li, C.; and Lee, G. H. 2023. Nerf-ds: Neural radiance fields for dynamic specular objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 82858295. Yang, Z.; Gao, X.; Zhou, W.; Jiao, S.; Zhang, Y.; and Jin, X. 2023. Deformable 3d gaussians for high-fidelity monocular dynamic scene reconstruction. arXiv preprint arXiv:2309.13101. Yu, H.-X.; Duan, H.; Hur, J.; Sargent, K.; Rubinstein, M.; Freeman, W. T.; Cole, F.; Sun, D.; Snavely, N.; Wu, J.; et al. 2024. Wonderjourney: Going from anywhere to everywhere. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 66586667. Zhang, R.; Isola, P.; Efros, A. A.; Shechtman, E.; and Wang, O. 2018. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, 586595. Fridovich-Keil, S.; Meanti, G.; Warburg, F. R.; Recht, B.; and Kanazawa, A. 2023. K-planes: Explicit radiance fields In Proceedings of the in space, time, and appearance. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1247912488. Fridovich-Keil, S.; Yu, A.; Tancik, M.; Chen, Q.; Recht, B.; and Kanazawa, A. 2022. Plenoxels: Radiance fields without neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 5501 5510. Guo, X.; Sun, J.; Dai, Y.; Chen, G.; Ye, X.; Tan, X.; Ding, E.; Zhang, Y.; and Wang, J. 2023. Forward Flow for Novel View Synthesis of Dynamic Scenes. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 1602216033. Huang, Y.-H.; Sun, Y.-T.; Yang, Z.; Lyu, X.; Cao, Y.-P.; SC-GS: Sparse-Controlled Gaussian and Qi, X. 2023. arXiv preprint Splatting for Editable Dynamic Scenes. arXiv:2312.14937. Kerbl, B.; Kopanas, G.; Leimkuhler, T.; and Drettakis, G. 2023. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4): 114. Kong, H.; Lian, D.; Mi, M. B.; and Wang, X. 2025. DreamDrone: Text-to-Image Diffusion Models Are Zero-Shot Perpetual View Generators. In European Conference on Computer Vision, 324341. Springer. Lin, H.; Peng, S.; Xu, Z.; Xie, T.; He, X.; Bao, H.; and Zhou, X. 2023. High-Fidelity and Real-Time Novel View SyntheIn SIGGRAPH Asia Conference sis for Dynamic Scenes. Proceedings. Lu, T.; Yu, M.; Xu, L.; Xiangli, Y.; Wang, L.; Lin, D.; and Dai, B. 2023. Scaffold-gs: Structured 3d gaussians for viewadaptive rendering. arXiv preprint arXiv:2312.00109. Ma, X.; Fang, G.; and Wang, X. 2024. Deepcache: AcIn Proceedings of celerating diffusion models for free. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1576215772. Park, K.; Sinha, U.; Barron, J. T.; Bouaziz, S.; Goldman, D. B.; Seitz, S. M.; and Martin-Brualla, R. 2021a. Nerfies: In Proceedings of the Deformable neural radiance fields. IEEE/CVF International Conference on Computer Vision, 58655874. Park, K.; Sinha, U.; Hedman, P.; Barron, J. T.; Bouaziz, S.; Goldman, D. B.; Martin-Brualla, R.; and Seitz, S. M. 2021b. Hypernerf: higher-dimensional representation for topologically varying neural radiance fields. arXiv preprint arXiv:2106.13228. Paszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.; Chanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga, L.; et al. 2019. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32. Schonberger, J. L.; and Frahm, J.-M. 2016. Structure-frommotion revisited. In Proceedings of the IEEE conference on computer vision and pattern recognition, 41044113."
        }
    ],
    "affiliations": [
        "National University of Singapore"
    ]
}