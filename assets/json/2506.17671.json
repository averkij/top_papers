{
    "paper_title": "TPTT: Transforming Pretrained Transformer into Titans",
    "authors": [
        "Fabien Furfaro"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in large language models (LLMs) have led to remarkable progress in natural language processing, but their computational and memory demands remain a significant challenge, particularly for long-context inference. We introduce TPTT (Transforming Pretrained Transformer into Titans), a novel framework for enhancing pretrained Transformer models with efficient linearized attention mechanisms and advanced memory management. TPTT employs techniques such as Memory as Gate (MaG) and mixed linearized attention (LiZA). It is fully compatible with the Hugging Face Transformers library, enabling seamless adaptation of any causal LLM through parameter-efficient fine-tuning (LoRA) without full retraining. We show the effectiveness of TPTT on the MMLU benchmark with models of approximately 1 billion parameters, observing substantial improvements in both efficiency and accuracy. For instance, Titans-Llama-3.2-1B achieves a 20% increase in Exact Match (EM) over its baseline. Statistical analyses and comparisons with recent state-of-the-art methods confirm the practical scalability and robustness of TPTT. Code is available at https://github.com/fabienfrfr/tptt . Python package at https://pypi.org/project/tptt/ ."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 1 7 6 7 1 . 6 0 5 2 : r TPTT: Transforming Pretrained Transformer into Titans Fabien Furfaro 2025 Abstract Recent advances in large language models (LLMs) have led to remarkable progress in natural language processing, but their computational and memory demands remain significant challenge, particularly for long-context inference. We introduce TPTT (Transforming Pretrained Transformer into Titans), novel framework for enhancing pretrained Transformer models with efficient linearized attention mechanisms and advanced memory management. TPTT employs techniques such as Memory as Gate (MaG) and mixed linearized attention (LiZA). It is fully compatible with the Hugging Face Transformers library, enabling seamless adaptation of any causal LLM through parameter-efficient fine-tuning (LoRA) without full retraining. We show the effectiveness of TPTT on the MMLU benchmark with models of approximately 1 billion parameters, observing substantial improvements in both efficiency and accuracy. For instance, Titans-Llama-3.2-1B achieves 20% increase in Exact Match (EM) over its baseline. Statistical analyses and comparisons with recent state-of-the-art methods confirm the practical scalability and robustness of TPTT. The source code is available at https://github.com/fabienfrfr/tptt and the Python package at https://pypi.org/project/tptt/. Figure 1: Overview of the TPTT architecture. On the left, the diagram illustrates decoder-only architecture where linear attention is injected in parallel of vanilla attention (LiZAttention). On the right, the detailed architecture of the linearized attention mechanism is depicted, highlighting the shared weights for query (Q), key (K), value (V), and output (O) projections. It also shows the management of the state memory (S) and the combination of outputs through the Memory as Gate (MaG) weighting mechanism. The diagram emphasizes the integration of linearized attention mechanisms and advanced memory management techniques, such as Delta Rule and AdaptativeAvgPool1D, contributing to processing and output generation."
        },
        {
            "title": "Introduction",
            "content": "The success of transformer-based large language models (LLMs) [11, 16] has revolutionized natural language processing (NLP), enabling unprecedented progress across wide range of tasks. However, the quadratic computational complexity and substantial memory requirements of the standard self-attention mechanism remain significant bottleneck, particularly for long-context inference. To address these challenges, recent research has explored several directions. Efficient attention mechanisms [9,18] have been proposed to reduce the complexity of self-attention from quadratic to linear or near-linear, fabien.furfaro@gmail.com 1 making it more tractable for long sequences. Recurrent architectures and internal memory mechanisms [2, 13] have also been developed to enhance the models ability to capture long-range dependencies, drawing inspiration from cognitive memory in biological systems rather than hardware memory. Additionally, parameter-efficient fine-tuning approaches such as LoRA [8] allow for the adaptation of large pretrained models to new tasks without the need for full retraining."
        },
        {
            "title": "2 Related Work",
            "content": "Despite these advances, most existing methods require significant architectural modifications or training from scratch, which limits their applicability to already pretrained models. For example, solutions like FlashAttention [3] and Mamba [6] focus on efficient architectures, while others such as LoLCat [19] and Liger [10] convert standard attention to linearized forms; notably, Liger exploits similar properties to those leveraged in this work, but does not rely on explicit linearization injection. In this work, we introduce TPTT (Transforming Pretrained Transformer into Titans), framework that transforms pretrained transformer models into efficient and scalable architectures by incorporating linearized attention mechanisms and advanced internal memory augmentation. TPTT leverages techniques such as Memory as Gate (MaG) and mixed linearized attention (LiZA), drawing inspiration from the Titans architecture [2]. Our approach is fully compatible with existing frameworks and enables rapid adaptation of any causal LLM to long-context tasks via parameter-efficient fine-tuning with LoRA [8], without requiring full retraining. The goal is to unlock the potential of already trained models by equipping them with memory-augmented capabilities through lightweight adaptation."
        },
        {
            "title": "3.1 Linearized Attention Mechanisms",
            "content": "Standard self-attention in transformers computes pairwise interactions between all tokens, resulting in quadratic complexity with respect to sequence length [16]. To address this, linearized attention mechanisms approximate the softmax attention using linear operations, typically by projecting queries and keys through feature map ϕ [9, 13, 17, 18]. This reduces computational and memory costs, enabling efficient processing of long sequences while maintaining modeling power [3, 6, 10, 19]. The output of softmax attention for an input sequence = {x1, . . . , xT } RT is: Q, K, = XWQ, XWK, XWV Obase = Softmax (cid:19) (cid:18) QK In linear attention, this is approximated as: Olin,(t) = ϕ(qt) (cid:16)(cid:80)t (cid:16)(cid:80)t ϕ(qt) i=1 ϕ(ki, βi) (cid:17) = i=1 ϕ(ki, βi) (cid:17) (1) (2) (3) ϕ(qt)St ϕ(qt)zt where qt, ki, and vi are the query, key, and value vectors at positions and i, respectively, and βi is gating vector (or scalar) modulating the keys and values. The function ϕ denotes feature mapping, such as ELU or kernel approximations [17]."
        },
        {
            "title": "3.2 Memory as Gate (MaG)",
            "content": "To further enhance long-range dependency modeling, we introduce an internal memory augmentation mechanism, Memory as Gate (MaG), inspired by the Titans architecture [2]. Unlike hardware memory, this mechanism enables the model to store and recall contextual information over extended sequences, analogous to cognitive memory. MaG combines the outputs of standard and linearized attention using weighting parameter: = α Olin + (1 α) Obase (4) where α [0, 1] is adapted during training. This allows the model to leverage both the efficiency of linear attention and the expressivity of softmax attention, and can be seen as form of memory-augmented gating [2, 13]."
        },
        {
            "title": "3.3 Parallel Delta Rule Modeling",
            "content": "In this work, the feature mapping function of linear attention is approximated by DeltaNet [18]. The recurrent update of internal memory states is formulated in two ways, following: Closed Form (intra-chunk): St = St1 + (cid:88) i=1 vik (5) where is the chunk size and St is the updated state. Recurrent Form (inter-chunk): St+1 = St + vt+1k The final state of one chunk becomes the initial state of the next, supporting efficient memory usage for long sequences. t+1 (6) The output Olin is then computed from these memory states. 3."
        },
        {
            "title": "Integration with Pretrained Models",
            "content": "Our approach injects linearized attention and memory augmentation modules into pretrained transformer models. The process involves: 1. Identification of Target Modules: Key attention layers to be modified are identified using tools such as get_tptt_model. 2. Modification of Attention Layers: These layers are replaced or extended with the proposed LiZAttention module, which implements both linear and softmax attention with linear projection weigth sharing and MaG. 3. Training and Fine-Tuning: The modified model is fine-tuned using parameter-efficient methods such as LoRA [8], ensuring optimal adaptation to the new mechanisms without full retraining. This procedure enables the transformation of any causal pretrained LLM into memory-augmented, efficient architecture with minimal overhead, that without any new layers."
        },
        {
            "title": "3.5 LiZAttention Module",
            "content": "The LiZAttention module is core component of the TPTT architecture, designed to synergistically combine linearized attention and standard (softmax) attention mechanisms. This hybrid approach leverages the computational efficiency of linear attention while retaining the expressivity of vanilla attention. To support long-context inference, LiZAttention maintains cache of intermediate states and implements recurrent information mechanism for efficient internal memory management [9]. Algorithm 1 LiZAttention Forward Pass Require: hidden_states RBLD Require: mask 1: Projection: Batch size B, sequence length L, embedding dim Attention mask for padding/causality Compute queries q, keys k, values via learned projections: RBHLdh , k, RBHkLdh 2: Apply Attention Mask: Apply mask to and to handle padding and restrict attention. 3: Linear Attention: Compute linear attention output olin using feature map ϕ: olin[t] = Store intermediate states in memory cache for recurrent information. i=1 ϕ(qt)ϕ(ki)vi (cid:80)t i=1 ϕ(qt)ϕ(ki) (cid:80)t 4: Vanilla (Softmax) Attention: Compute standard self-attention output obase (optionally truncated for long sequences): obase = Softmax (cid:17) (cid:16) qk dh 5: Combine Outputs (Memory as Gate): Compute final output using learnable gating parameter α: = α olin + (1 α) obase 6: return RBLD"
        },
        {
            "title": "3.5.1 Efficient Internal Memory Management",
            "content": "The cache of intermediate states maintained by LiZAttention enables recurrent information, efficiently supporting long-context inference without excessive computational overhead [9]. This approach allows the model to scale to longer sequences, leveraging both local and global context."
        },
        {
            "title": "4.1 Parameter-Efficient Fine-Tuning with LoRA",
            "content": "To adapt the TPTT architecture to downstream tasks, we employ Low-Rank Adaptation (LoRA) [4, 8], parameter-efficient fine-tuning technique that injects trainable low-rank matrices into selected projection layers while freezing the original model weights. This approach reduces the number of trainable parameters and memory requirements, while maintaining performance comparable to full fine-tuning [4, 8]. LoRA is configured with rank of 8, α = 16, and dropout rate of 0.05. Fine-tuning targets the main projection modules, specifically [q_proj, k_proj, v_proj, o_proj] for Llama/Mistral and [qkv_proj, out_proj] for OpenELM [4]."
        },
        {
            "title": "4.2 Dynamic Memory as Gate Scheduling (LiZA MaG Callback)",
            "content": "A important component of the training process is the LiZA MaG callback, which dynamically adjusts the Memory as Gate (MaG) weighting parameter during training. The MaG weight is initialized at 0.01 and linearly increased to 0.5 over the first 100 steps, facilitating smooth transition from reliance on vanilla (softmax) attention to linearized attention. This schedule allows the model to effectively balance the two attention mechanisms, optimizing performance throughout training. The callback is integrated directly into the training loop, ensuring adaptive control of the MaG parameter and enhancing the models adaptability and efficiency."
        },
        {
            "title": "5.1 Experimental Setup",
            "content": "We evaluated the TPTT library on several pretrained language models with approximately 1 billion parameters, using the MMLU benchmark [7] as the primary evaluation suite. Training was conducted on 500 samples from the yahma/alpaca-cleaned dataset [14] for 5 epochs, with maximum sequence length of 384 tokens, batch size of 3, and learning rate of 5 104. Mixed precision training and gradient clipping at 1.0 were employed to optimize computational efficiency and stability. All experiments were performed on NVIDIA Tesla T4 GPUs (Kaggle platform). The trained models and detailed metrics are publicly available on the Hugging Face Model Hub1, with full training logs accessible via Hugging Face TensorBoard."
        },
        {
            "title": "5.2 Training Results",
            "content": "Table 1 summarizes the training performance metrics for various TPTT models. The results indicate consistent and efficient learning across architectures, by low final loss values and stable gradient norms. The use of LowRank Adaptation (LoRA) and the Memory as Gate (MaG) mechanism shows effective in optimizing training dynamics and convergence. Loss Model Titans-Llama-3.2-1B 1.375 Titans-OpenELM-1_1B 1.3188 1.2568 Titans-Qwen2.5-1.5B 1.3068 Titans-OLMo-1B-hf Training Time (s) 1654.1 1651.1 1900.6 1585.2 Samples/s 1.51 1.51 1.31 1.58 Steps/s Total FLOPs Gradient Norm Refs Based On 0.254 0.254 0.221 0.265 5.62e18 5.85e18 7.56e18 6.20e 2.68 0.704 1.99 3.12 [15] [12] [1] [5] Table 1: Training performance metrics for TPTT models."
        },
        {
            "title": "5.3 Evaluation Metrics and Benchmark Results",
            "content": "For evaluation, we focus on metrics standard in LLM and QA benchmarking: Exact Match (EM), Partial Exact Match (PEM), and Partial Quasi Exact Match (PQEM). These metrics respectively measure strict correctness, partial overlap, and quasi-exactness between model outputs and ground truth answers, providing nuanced view of model performance [7]. 1https://huggingface.co/ffurfaro/ 4 Table 2 presents the MMLU benchmark results in the one-shot setting. TPTT models, and especially Titans-Llama-3.2-1B, consistently outperform their base counterparts in EM, with better PEM and PQEM scores. This shows the benefit of integrating linearized attention and internal memory mechanisms for complex language understanding tasks. EM Std Model Titans-Llama-3.2-1B 0.2456 0.1276 Llama-3.2-1B 0.0070 0.0058 Titans-Qwen2.5-1.5B 0.0000 0.0000 0.0000 0.0000 Qwen2.5-1.5B 0.0000 0.0000 Titans-OLMo-1B-hf 0.0000 0.0000 OLMo-1B-hf PEM Std 0.2649 0.1340 0.3105 0.1411 0.5000 0.1504 0.5982 0.1422 0.2614 0.1312 0.2333 0.1302 PQEM Std 0.4772 0.1569 0.4719 0.1530 0.5825 0.1442 0.6895 0.1288 0.4649 0.1540 0.4246 0.1533 Table 2: MMLU benchmark results (one-shot) with statistical analysis. Each pair groups Titans model and its base counterpart."
        },
        {
            "title": "5.4 Discussion and Comparison",
            "content": "Compared to recent state-of-the-art methods such as Mamba [6], LoLCat [19], and Liger [10], TPTT stands out by enabling the transformation of existing pretrained models without full retraining, while maintaining good benchmark performance. The observed improvements in EM and better PEM/PQEM scores highlight the effectiveness of TPTTs linearized attention and memory augmentation for efficient and robust LLM adaptation. These results confirm that TPTT is practical and scalable solution for enhancing pretrained LLMs, especially in resource-constrained settings."
        },
        {
            "title": "6 Discussion and Conclusion",
            "content": "In this paper, we have introduced TPTT, novel framework for enhancing pretrained Transformer models by integrating efficient linearized attention mechanisms and internal memory augmentation. Our approach leverages parameter-efficient fine-tuning (LoRA) [8] to enable the rapid adaptation of large language models (LLMs) to long-context tasks, without the need for full retraining. Experimental results on the MMLU benchmark [7] shows significant improvements in both efficiency and accuracy, with robust statistical analyses and favorable comparisons to state-of-the-art methods [6, 10, 19]. Practical Implications. TPTT offers scalable and practical solution for deploying high-performance LLMs in resource-constrained environments. The integration of linearized attention and memory augmentation reduces computational and memory requirements, making advanced language models more accessible for real-world applications. The use of LoRA allows for efficient and flexible fine-tuning, enabling rapid adaptation to new tasks and domains. Limitations. Our current evaluation is limited to models of moderate size (around 1 billion parameters). Scaling TPTT to larger architectures and more diverse tasks may introduce new challenges, including increased tuning complexity and the need for further optimization of memory mechanisms. While our results are promising, broader validation on additional benchmarks and real-world scenarios is needed to fully assess the generalizability and robustness of the approach. Future Directions. Future work will focus on optimizing the integration process, exploring more sophisticated internal memory mechanisms [2], and extending the evaluation to larger models and wider range of benchmarks. Additional research will investigate hybrid approaches and the interplay between linearized attention, memory augmentation, and other efficiency-oriented techniques. In summary, TPTT provides practical, scalable, and effective library for upgrading pretrained Transformers, with strong empirical results and promising implications for the future of efficient language modeling."
        },
        {
            "title": "References",
            "content": "[1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. 5 [2] Ali Behrouz, Peilin Zhong, and Vahab Mirrokni. Titans: Learning to memorize at test time. arXiv preprint arXiv:2501.00663, 2024. [3] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. [4] Hugging Face. Lora - hugging face peft documentation. https://huggingface.co/docs/peft/main/ conceptual_guides/lora, 2024. [5] Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et al. Olmo: Accelerating the science of language models. arXiv preprint arXiv:2402.00838, 2024. [6] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. [7] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. [8] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. [9] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pages 51565165. PMLR, 2020. [10] Disen Lan, Weigao Sun, Jiaxi Hu, Jusen Du, and Yu Cheng. Liger: Linearizing large language models to gated recurrent structures. arXiv preprint arXiv:2503.01496, 2025. [11] Ben Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 1:3, 2020. [12] Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, et al. Openelm: An efficient language model family with open-source training and inference framework. arXiv e-prints, pages arXiv2404, 2024. [13] Jean Mercat, Igor Vasiljevic, Sedrick Keh, Kushal Arora, Achal Dave, Adrien Gaidon, and Thomas Kollar. Linearizing large language models. arXiv preprint arXiv:2405.06640, 2024. [14] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. Alpaca: strong, replicable instruction-following model. Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html, 3(6):7, 2023. [15] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [16] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [17] Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. [18] Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. Parallelizing linear transformers with the delta rule over sequence length. arXiv preprint arXiv:2406.06484, 2024. [19] Michael Zhang, Simran Arora, Rahul Chalamala, Alan Wu, Benjamin Spector, Aaryan Singhal, Krithik Ramesh, and Christopher Ré. Lolcats: On low-rank linearizing of large language models. arXiv preprint arXiv:2410.10254, 2024."
        }
    ],
    "affiliations": []
}