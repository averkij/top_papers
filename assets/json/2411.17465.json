{
    "paper_title": "ShowUI: One Vision-Language-Action Model for GUI Visual Agent",
    "authors": [
        "Kevin Qinghong Lin",
        "Linjie Li",
        "Difei Gao",
        "Zhengyuan Yang",
        "Shiwei Wu",
        "Zechen Bai",
        "Weixian Lei",
        "Lijuan Wang",
        "Mike Zheng Shou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Building Graphical User Interface (GUI) assistants holds significant promise for enhancing human workflow productivity. While most agents are language-based, relying on closed-source API with text-rich meta-information (e.g., HTML or accessibility tree), they show limitations in perceiving UI visuals as humans do, highlighting the need for GUI visual agents. In this work, we develop a vision-language-action model in digital world, namely ShowUI, which features the following innovations: (i) UI-Guided Visual Token Selection to reduce computational costs by formulating screenshots as an UI connected graph, adaptively identifying their redundant relationship and serve as the criteria for token selection during self-attention blocks; (ii) Interleaved Vision-Language-Action Streaming that flexibly unifies diverse needs within GUI tasks, enabling effective management of visual-action history in navigation or pairing multi-turn query-action sequences per screenshot to enhance training efficiency; (iii) Small-scale High-quality GUI Instruction-following Datasets by careful data curation and employing a resampling strategy to address significant data type imbalances. With above components, ShowUI, a lightweight 2B model using 256K data, achieves a strong 75.1% accuracy in zero-shot screenshot grounding. Its UI-guided token selection further reduces 33% of redundant visual tokens during training and speeds up the performance by 1.4x. Navigation experiments across web Mind2Web, mobile AITW, and online MiniWob environments further underscore the effectiveness and potential of our model in advancing GUI visual agents. The models are available at https://github.com/showlab/ShowUI."
        },
        {
            "title": "Start",
            "content": "ShowUI: One Vision-Language-Action Model for GUI Visual Agent Kevin Qinghong Lin1, Linjie Li2, Difei Gao1, Zhengyuan Yang2, Shiwei Wu1, Zechen Bai1, Stan Weixian Lei1, Lijuan Wang2, Mike Zheng Shou1(cid:66) 1Show Lab, National University of Singapore 2Microsoft 4 2 0 2 6 ] . [ 1 5 6 4 7 1 . 1 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Building Graphical User Interface (GUI) assistants holds significant promise for enhancing human workflow productivity. While most agents are language-based, relying on closed-source API with text-rich meta-information (e.g., HTML or accessibility tree), they show limitations in perceiving UI visuals as humans do, highlighting the need for GUI visual agents. In this work, we develop visionlanguage-action model in digital world, namely ShowUI, which features the following innovations: (i) UI-Guided Visual Token Selection to reduce computational costs by formulating screenshots as an UI connected graph, adaptively identifying their redundant relationship and serve as the criteria for token selection during self-attention blocks; (ii) Interleaved Vision-Language-Action Streaming that flexibly unifies diverse needs within GUI tasks, enabling effective management of visual-action history in navigation or pairing multi-turn query-action sequences per screenshot to enhance training efficiency; (iii) Small-scale Highquality GUI Instruction-following Datasets by careful data curation and employing resampling strategy to address significant data type imbalances. With above components, ShowUI, lightweight 2B model using 256K data, achieves strong 75.1% accuracy in zero-shot screenshot grounding. Its UI-guided token selection further reduces 33% of redundant visual tokens during training and speeds up the performance by 1.4. Navigation experiments across web [12], mobile [36], and online [40] environments further underscore the effectiveness and potential of our model in advancing GUI visual agents. The models are available at https://github.com/showlab/ShowUI. 1. Introduction Graphical User Interfaces (GUIs) are central to how individuals engage with the digital world, serving as virtual embodied interface for range of daily activities. Meanwhile, Large Language Models (LLMs) [32], with their ability to comprehend complex language instructions and seamlessly integrate tools, have shown significant potential in performFigure 1. ShowUI is Vision-Language-Action model for GUI Automation. Given an environment screenshot, ShowUI efficiently processes it using UI-guided token selection for visual modeling and outputs an interaction action within the loop. Figure 2. Left: Zero-shot Screenspot grounding comparison between ShowUI and other GUI visual models in terms of model size and training scale (area); ShowUI reaching state-of-the-art accuracy as well as the most lightweight model (2B) with smaller training dataset (256K). Right: Building upon Qwen2-VL-2B [41], our UI-guided visual token selection reduces visual token redundancy by 33% during training, achieving 1.4 speedup. . ing complex tasks through building agents [1, 13, 16, 56]. This progress inspires the development of intelligent GUI agents that can significantly streamline human workflows based on user intentions. Early efforts in GUI automation have primarily focused on developing language agents [12, 47, 55] that rely on closed-source, API-based LLMs like GPT-4 [32]. These agents leverage text-rich metadata like HTML inputs or accessibility trees to perform navigation and other tasks. However, the text-only approach is limited in real-world applications, where users typically interact with user interfaces visuallythrough screenshotswithout access to the underlying structural oracle information. This limita1 tion underscores the need for developing GUI visual agents that can perceive and interact with UIs as humans do, such as assisting slide creation [29]. However, GUI visual perception presents unique challenges compared to natural image processing, requiring specialized skills such as UI element grounding or action execution rather than the conversational abilities typical of multi-modal chatbots. Recognizing this gap, researchers have begun training vision-language models to acquire these new abilities. For instance, studies like [11,15,17] utilize web screenshot datasets to enhance large multi-modal models element-grounding capabilities. Meanwhile, works like [10, 30] address navigation tasks by instruction tuning models for multi-step navigation. Despite these advancements, training multi-modal models for GUI visual agents continues to face significant challenges related to modeling and training: (a) Expensive Visual Modeling: UI screenshots are typically highresolution (e.g. 2K), resulting in lengthy token sequences that pose issues with long-context processing. Most existing models are not optimized for such high-resolution data, leading to inefficiencies and high computational costs. (b) Managing Interleaved Vision-Language-Action: actions differ from language modalities and may vary across devices (e.g. Return on web interfaces versus Press home on mobile devices) and adapt to different parameter settings (e.g. Scroll actions have two directions on the web but four on mobile platforms), how to effectively model action is unclear. Additionally, its essential to model actions alongside visual and query data. For example, navigation processes generate history of screenshot and action steps, creating complex, interleaved vision-language-action that models must effectively interpret and manage. (c) Diverse Training Data: with vast amounts of GUI data across different devices such as Web and Mobile, accompanied by diverse purpose annotations including element grounding and navigation, it remains unclear how to effectively select high-quality training corpus for developing robust GUI visual models. These critical challenges have been underexplored but are essential for the development of effective visual models for GUI agents. In this work, we develop vision-language model for GUI visual agents, aiming to address and resolve the aforementioned challenges, with the following key contributions: (i) UI-Guided Visual Token Selection: We recognize the uniqueness of UI screenshots (i.e., redundancy mixed with essential details) and develop UI-friendly visual token selection approach. In RGB space, we represent each patch as node and identify connected components to model redundancy across patches. This relationship guides the selfattention blocks within visual encoders or language models for token selection, effectively reducing computation. (ii) Interleaved Vision-Language-Action Streaming: We analyze the diversity of GUI actions, structuring them in JSON format and documenting their action space to assist the model in action interpretation. Additionally, we recognize the need for interleaved understanding across modalities, such as combining action with visual navigation history and balancing visual token lengths through multi-turn action with text queries to improve training efficiency. Our model is formulated as interleaved vision-language-action streaming, unifying the diverse needs in GUI scenarios. (iii) Well-selected Instruction-following Dataset: Instead of utilizing data from all available sources, we conduct an in-depth analysis of each data types properties. For example, in web data, visual elements (i.e., button) is more valuable than static text (which accounts for 40%) as most VLMs [41] possess strong OCR capabilities. Additionally, we introduce small, high-quality instruction-following dataset that achieves strong UI grounding performance. Furthermore, we develop rebalanced sampling strategy to address the substantial imbalance in UI data, ensuring consistent model performance across different setups. Building upon aforementioned innovations, we enhance Qwen2-VL-2B to create powerful GUI visual agent, ShowUI. As shown in Fig.1, this results in lightweight 2B model using 256K data, achieving strong 75.1% accuracy in zero-shot screenshot grounding. ShowUI also demonstrate competitive navigation ability in Web [12], Mobile [36], and Online [40] environments. Comprehensive ablation studies  (Fig. 1)  demonstrate the effectiveness of our UI-guided token selection approach, reducing redundant visual tokens by 33% and accelerating training by 1.4. Moreover, we conclude with many discussions on current performance gaps and future directions. 2. ShowUI ShowUI, as outlined in Fig. 3, is built on top of the vision-language model Qwen2-VL-2B [41], incorporating the following key components optimized for GUI tasks: (i) novel UI-guided visual token selection strategy for efficient visual modeling, (ii) an interleaved vision-languageaction streaming setup to flexibly unify different needs by GUI tasks and enhance training effectiveness, (iii) training data recipe, crafted through detailed analysis of individual GUI data types, which enables ShowUI training on smaller, high-quality corpus. In the following sections, we introduce each component in detail. 2.1. UI-Guided Visual Tokens Selection High-resolution screenshots can result in large number of visual tokens after standard patching. As demonstrated in Fig.4a, 1344 756 resolution on PC yields approximately 5184 raw tokens with 14 14 patching, after 2 2 [41] merging, still results in 1296 tokens, creating computational challenge within the self-attention module. Figure 3. Illustration of ShowUI. Given user task query, pre-defined action space, and an initial screenshot as observation, ShowUI proceeds by executing the next action, updating the screenshot, and continuing in this cycle. Notably, ShowUI features the following key innovation designs: (i) UIGuided Visual Token Selection, which processes the input screenshot to build UI patch-wise connected graph. During training, random subset of tokens is selected within each component for efficient visual modeling (Sec. 2.1). (ii) Interleaved Vision-Language-Action Streaming to effectively handle past screenshots and actions, improving navigation performance. (Sec. 2.2). What differentiates UI from natural vision? Unlike natural images, which captures real-world complexities and unpredictable patterns thus rich in semantic, textures, UI screenshots are inherently structured, with clear layouts and consistent color schemes optimized for readability and usability. This difference means that UI images often contain redundant empty spaces or simple backgrounds that do not carry essential information, aling for optimization or pruning. Moreover, small but functionally important elements, like icons or text, demand higher salience due to their role in interactivity and clarity. Thus, it is necessary to have strategy that can differentiate between redundant and essential visual elements, enabling effectively pruning of irrelevant visual tokens without compromising usability. We found that the RGB space can serve as useful guideline for this purpose as pattern variants, text fonts can be easily identify by its RGB values. Construct UI Connected Graph. After dividing the screenshot into regular patches, we observed many neighboring patches share exactly the same RGB values and are thus redundant. To leverage this, we represent each patch as node in graph. If neighboring patches have the same RGB values, we connect their corresponding nodes, forming connected components. This allows us to group and simplify redundant areas while preserving essential visual elements identified by their unique RGB patterns. Visually identical patches can be easily detected by setting small threshold on the difference between their patch tensors. Based on this insight, we use the Union-Find method to 3 identify connected components in this UI connected graph, as described in Algorithm 1. This algorithm produces graph with connected component, where is typically smaller than the original number of patches Gh Gw. Based on the assignment of each node to its component, we can model the redundancy relationships among patches. As illustrated in Fig. 4a, this method can effectively balances component number based on their visual informative adaptively, using less component (more redundant patches) in google search page with sparser areas (1296 291), while assigning more components (more independent patches) in text-rich overleaf screenshots (1296 986). In Fig.5, we display how our method constructs the UIAlgorithm 1 Find Connected Components on UI-Graph 1: Input: Screenshot of size , patch size c, threshold δ 2: Output: Assignment map between patch and connected components. 3: Divide the image into Gh Gw patches, each patch is node, for all neighbors (i, j) to the right and below of (i, j) do where Gh = c and Gw = 4: Initialize Union-Find structure UF over nodes 5: for all node (i, j) do 6: 7: 8: 9: 10: 11: end for 12: return Assignment map from UF UF.union ((i, j), (i, j)) end for end if if RGB (i, j) RGB (i, j) < δ then (a) UI Connected Graph adaptively assigns connected components based on the informativeness of screenshots. (b) Two representative token compression methods, where patches of the same color indicate the same component and are redundant to each other. Figure 4. Illustration of UI-guided Visual Tokens Selection. Left: Starting from an original screenshot (left) with 28x28 patch grid (middle), resulting in 1296 tokens, the UI Connected Graph adaptively organizes the image into disjoint regions based on RGB values, allowing patches within the same region to be treated as redundant. Right: Comparison of two methods leveraging UI Connected Graph in visual token modeling: Token merging pools all tokens within one component, which loses individual positional information, while token selection, randomly sample part of tokens with each component, still retains their original position relationship. connected graph across different devices. Given identical resolution screenshots with the same initial visual patch tokens (e.g., 1272), our method adaptively constructs connected components based on the informativeness of the screenshots. Token Merging v.s. Token Selection. Next, we explore how leveraging this UI connected graph can improve model efficiency. We find that the primary computational bottleneck in existing vision-language models lies in the long sequences processed by cascaded self-attention layers, impacting both language models and visual encoders. straightforward approach is to apply token merging methods [6, 20], which represent all patches within component into single token, as shown in the Fig.4b left half. In practice, however, we found that this approach disrupts positional relationships, as the original positional information in the pooled token sequence is inevitably lost, which is essential for accurate UI element grounding. To enable token compression within self-attention without losing positional information, we draw inspiration from Mixture-of-Depth [35], which sparsely samples tokens via routing mechanism. In our case, the UI connected graph provides an effective routing criterion. Tokens within the same component can be considered redundant, so we randomly skip portion of tokens within each component during training, leaving single-patch components unaffected to preserve essential elements. For the selected tokens, we retain their original position embeddings, ensuring that selfattention operates on the original positional relationships, even with less token sequence. Notably, this token selection method introduces no additional learnable parameters. Therefore, we apply random token selection at set ratio during training, while at inference, the method offers flexibility to use either with or without token selection, as both options maintain consistent positional relationships within the full token sequence. 2.2. Interleaved VLA Streaming In this section, we aim to address how to formulate actions and their relationships with other modalities (i.e., visual and textual queries). What differentiates Action from natural text? The core functionality of GUI model is navigation, conditioned on text query and requiring the model to jointly predict: the correct action type (e.g., [CLICK] or [TYPE]), with corresponding action parameters (e.g., coordinates for [CLICK] or string for [TYPE]). major challenge in navigation arises from the action variants across different devices, such as: (i) Device-specific actions (e.g. [CLICK] is unavailable on mobile, whereas [PRESS HOME] does not exist on the web). (ii) Same action with different parameters (e.g. [SCROLL] has two directionsup and down on the web, but four directions on mobile). (iii) Novel actions at test time that were not encountered during training. To manage these variations within our model, we first structure each action in JSON format (i.e., {action: action type, value: [x,y]}), where [x,y] represents position: relative coordinates on 0-1. This allows us to standardize actions from diverse devices into unified format. Secondly, we provide the model with README in the system prompt, documenting each actions usage within the action space (e.g., CLICK: Click on an element, value is not applicable and the position [x,y] is required.) This element, 4 (a) 1272 tokens 781 components (b) 1272 tokens 359 components (c) 1272 tokens 265 components (d) 1272 tokens 175 components (e) 646 tokens 281 components (f) 646 tokens 230 components (g) 1296 tokens 740 components (h) 1296 tokens 369 components Figure 5. Illustration of our method constructs the UI-connected graph based on the informativeness of screenshots. (ad) Mobile; (ef) PC; (gh) Web. 5 Illustration of Interleaved Vision-Text-Action Streaming. Figure 6. The visual tokens in screenshots are significantly longer (e.g., 1.3K) compared to queries or actions (e.g., fewer than 10). Thus, we introduce two modes: (a) Action-Visual Streaming for UI navigation and (b) ActionQuery Streaming for UI grounding. These modes extend the concept of multi-turn dialogue and enable more efficient utilization of training data. to execute actions at setup encourages the model to interpret the action space document rather than memorizing fixed actions, enabling time in function-calling test it manner [52]. Next, we discuss the relationship between action and other modalities, as illustrated in Fig.6. Action with Visual: The GUI navigation process typically involves multi-step trajectories, requiring the model to recognize current steps and determine the next action in context. This introduces the challenge of managing both past actions and past observations (screenshots): actions indicate what has been done but lack visual context, while screenshots capture the visual state but omit actions taken. To ensure complete history information, we formulate an interleaved vision-action stream, as shown in Fig.3, which captures both visual and action information sequentially. After the i-th action, the resulting (i + 1)-th screenshot enters the queue following the previously executed i-th action, prompting the model to generate the (i + 1)-th action. In practice, we can optionally mask parts of the visual history depending on the application. For instance, in Mobile [36], where cross-software screenshot changes occur, it is essential to retain screenshots to track visual appearances. In contrast, for Web [12], where screenshots generally remain stable on static webpage across series of actions, masking may be preferable for efficiency. Action with Textual query: In one-step navigation or element grounding, we might encounter one screenshot with multiple parallel actions [22] or multiple elements [11], where screenshots tend to be high-resolution, resulting in long token sequences (e.g., 1-2K tokens). Meanwhile, queries like UI element names and actions (coordinates) are typically much shorter (often fewer than 10 tokens). This discrepancy makes one-image-per-action approach inefficient. To optimize training data utilization, we adopt multi-turn dialogue approach, predicting multiple action annotations for each screenshot within single forward pass. 2.3. GUI Instructional Tuning Various GUI datasets are available in community, such as dominant of web data [10, 11, 44], mobile data [30, 36], which may contain element coordinates [11] or user trajectories [30]. Rather than aggregating all available data sources, we analyze each dataset type to select representative data. Our selected data is illustrated in Tab.1. Our discussion is mainly on UI grounding data. For navigation, we source GUIAct [10] with mobile and web devices. (i) Webvisual elements: The web provides highly accessible, text-rich source of UI data, easily crawled from HTML [44]. Our statistical analysis shows that the static text tag accounts for significant portion (40%). Given that most VLMs already with strong OCR capabilities [2, 41], we focus on collect visually rich elements. To this end, we developed parser and collected 22K screenshots, retaining only visual-related elements such as those tagged with Button or Checkbox. By removing static text (ii) Desktopdiverse query: Desktop data is particularly valuable as it is challenging to collect automatically. We identified OmniAct [22], which includes manual elements from iOS, Windows, and Linux desktops with small size (2K elements over 100 images). Its element is only labelled by original name such as message ash. To enrich this dataset and diversity, we employed reverse engineering techniques, utilizing ground-truth bounding boxes and its text elements. Then we prompt GPT-4o, with visual prompts highlighting target elements, to derive three types of query: appearance, spatial and intention; which we illustrated in Fig.7. This method results new 6K elements. See Supp. for detail prompt and discussion. Figure 7. We derive three types of query (appearance, spatial relationship, and intention) from raw annotation, assisted by GPT-4o. In Fig.8, we demonstrate more examples about how we leverage GPT4o to augment the original OmniAct-Desktop annotations with diverse queries based on Appearance, Spatial Relationships, and Intention. (iii) MobileFunctionality: mobile data is readily available in Android like [25, 36], which provide icon captioning. Notably, we consider [8] as its valuable functionality descriptions that go beyond simple atomic element names. Balance Data by Sampling: As shown in Tab.1, data scale varies significantly across types (e.g., only 100 desktop samples). To ensure fair exposure over each type, we develop sampling strategy during training, giving each batch comparable probability of including different data types. 3. Experiments 3.1. Benchmark Datasets We evaluate our model using the following benchmarks: Grounding. We use Screenspot [11], zero-shot grounding evaluation benchmark with diverse data across three device, 6 (a) Example of Weather. Original: visibility; Appearance: rectangular box with 28 km in white text.; Spatial: Positioned below WIND and next to PRESSURE.; Intention: Check current fog or mist conditions. (b) Example of Stocks. Original: Share option-health insurance; Appearance: Three vertical dots icon on dark background.; Spatial: Located to the right of the health insurance headline.; Intention: Click to share the health insurance article. (c) Example of WeChat. Original: expand profile; Appearance: rounded gray button with person icon.; Spatial: Located at the top-left corner of the chat pane.; Intention: Expand the contacts profile view. (d) Example of VLC. Original: Play; Appearance: White triangle icon within black circular button.; Spatial: Located at the bottom left corner of the screen.; Intention: Click to play the video. (e) Example of Terminal. Original: create new tab; Appearance: small + icon in gray tab bar.; Spatial: Located at the far right of the tab bar.; Intention: Open new terminal tab. (f) Example of Todo. Original: view as list; Appearance: gray, vertical button with box and lines icon.; Spatial: Positioned at the top right beside the search bar.; Intention: Switch to list view. Figure 8. Illustration of how we augment the original OmniAct-Desktop annotations with diverse queries based on Appearance, Spatial Relationships, and Intention. to assess text and widget recognition separately. Navigation. We evaluate navigation performance across four different datasets from different devices: (i) Web on Mind2Web [12], which includes an action space with three 7 Usage Device Source #Sample #Ele. #Cls. (len.) Highlights Grounding Web Self-collected Mobile AMEX [8] Desktop OmniAct [22] 22K 576K 97K 926K 8K 100 N/A N/A N/A Visual-based Functionality Diverse query Navigation Web GUIAct [10] Mobile GUIAct [10] 72K 569K 9 (7.9) One / Multi-step 65K 585K 5 (9.0) Multi-step Total Diverse 256K 2.7M Table 1. Overview of our instruction-tuning data. #Sample indicates the number of the task instance (screenshot in grounding, task in navigation); #Ele. indicates the number of the element (i.e., bbox in grounding); #Cls. represents the number of action classes, and len. indicates the average trajectory length per task. types of actions. (ii) Mobile on AITW [36], which action spaces includes 11 actions. (iii) Online on MiniWob [40], an online environment with two types of actions, introduced to complement the offline benchmarks and test performance in an interactive setting. Details of the training settings are provided in the supplementary material. 3.2. Main Results We organize our experiments on each downstream tasks to address the following questions: Q1: How does our model perform on each task? What improvements are achieved beyond existing VLM baseline? Q2: What are the effects and improvements of each component? Q3: What insights can be gained from each benchmark based on its unique properties?"
        },
        {
            "title": "3.2.1 Grounding Tasks",
            "content": "Method Size #Train Mobile Desktop Web Text Icon Text Icon Text Icon Avg. 2.4 9.3 24.2 10.0 1.4 Qwen2-VL-2B [41] 2B 8B Fuyu [5] 4.4 19.5 1.3 33.0 41.0 18B 400K 67.0 24.0 74.2 20.0 70.4 28.6 47.4 CogAgent [17] 9.6B 364K 78.0 52.0 72.2 30.0 55.7 32.5 53.4 SeeClick [11] * OmniParser [31] 93.9 57.0 91.3 63.6 81.3 51.0 73.0 1.3M 82.8 60.3 82.5 63.6 80.4 70.4 73.3 7B UGround [15] 9.3 8.7 3.6 33.9 ShowUI-G ShowUI 2B 2B 119K 91.6 69.0 81.8 59.0 83.0 65.5 74.9 256K 92.3 75.5 76.3 61.1 81.7 63.6 75.1 Table 2. Zero-shot grounding on Screenspot. * means Omniparser use GPT-4V. Size refers to model size. ShowUI-G: trained solely on grounding data, excluding navigation data. ShowUI, delivers strong grounding results with lightweight model size and minimal training data. In Tab. 2, we present zero-shot grounding results on the Screenspot [11]. This provides straightforward signal of the shortcomings in each setup. We includes one additional variant ShowUI-G, which only used grounding data for training. Our finding includes: (i) Overall all methods, the text track scores are higher than the icon track, even for desktop text, which was less seen during training. This suggests that text grounding ability mainly learned from web and mobile is transferable across platforms. (ii) Mixing navigation data [10] does not impair grounding perfor- (iii) mance when an effective sampling strategy is used. 8 The icon track is more challenging due to its visual grounding. Mobile scores are significantly higher than desktop and web, this emphasize the missing of visual UI grounding data beyond mobile devices. (iv) Notably, ShowUI, as the most lightweight method with minimal training data, achieves state-of-the-art grounding performance."
        },
        {
            "title": "3.2.2 Navigation Tasks",
            "content": "Mobile: AITW. In Tab.3, we have the following findings: (i) Without interleaved streaming (i.e., without visual history), ShowUI provides only 1.1% acc. improvement over the VLM baseline. However, with visual history, ShowUI achieves an additional 1.7% acc. gain, likely because visual context is crucial for adapting to frequent software changes within large action space (11), particularly on mobile platforms. (ii) ShowUIs zero-shot navigation learned from GUIAct [10] demonstrates transferability, suggesting further improvements can be made to the nav- (iii) ShowUI, beats OmniParser [31] igation component. and PaLM2-CoT [37], which either leverage closed-source APIs or HTML information, highlighting its potential of single model as standalone visual agent. Method FT? General Install G.Apps Single WebShop. Overall ChatGPT-CoT [53] PaLM2-CoT [37] * OmniParser [31] SeeClick [11] Qwen2-VL-2B [41] ShowUI ShowUI ShowUI-ZS 5.9 48.3 54.0 61. 63.5 63.9 32.1 4.4 57.8 66.4 71.8 72.3 72.5 47.7 10.5 51.6 54.9 62.6 66.0 69.7 42.0 9.4 77.4 63.5 73. 72.3 77.5 20.1 8.4 52.9 57.6 66.7 65.8 66.6 37.4 7.7 39.6 57.7 59.3 67.2 68.3 70.0 35.9 Table 3. Performance of Mobile Navigation [36], where gray color indicates these methods either using HTML inputs or rely on close-source GPT-4V (marked with *). ShowUI denotes our variant without interleaved visual-action streaming, utilizing only action history. Website: Mind2Web. In Tab. 4 for web navigation, we found that: (i) Instruction-tuning has significant effect, brings 4.6% Avg. Step SR. boost over Qwen2-VL-2B. Notably, ShowUI-2Bs zero-shot yield comparable with SeeClick-9.6B which has pretrained and fine-tuning, and achieves relatively high Op. F1 (80%+). (ii) Visual context in this task is less significant than in AITW, possibly because Mind2Web focuses primarily on single, visually similar website and includes only three actions. (iii) The cross-website and cross-domain settings are harder than cross-tasks. This suggests the bottleneck is lie in UI visual perception rather than textual task understanding (websites/domains are unseen in training). One future effort for improvement is to develop training data with good (visually) domain diversity. Online: MiniWob. In Tab. 5, this benchmark demonstrates model behavior in an online environment. Our key finding is that despite the simplicity of the Miniwob UI, the"
        },
        {
            "title": "Method",
            "content": "FT? MindAct [12] GPT-4 [32] * OmniParser [31] CogAgent [17] Qwen-VL [4] SeeClick [11] Qwen2-VL-2B [41] ShowUI ShowUI ShowUI-ZS Cross-Task Cross-Website Ele.Acc Op.F1 Step.SR Ele.Acc Op.F1 Step.SR Ele.Acc Op.F1 Step.SR Cross-Domain 55.1 41.6 42.4 22.4 15.9 28.3 37.7 39.7 39.9 21.4 75.7 60.6 87.6 53.0 86.7 87.0 86.4 88.0 88.6 85.2 52.0 36.2 39.4 17.6 13.3 25.5 33.2 36.9 37.2 18. 42.0 35.8 41.0 18.4 13.2 21.4 36.0 41.0 41.6 21.9 65.2 51.1 84.8 42.4 83.5 80.6 79.2 83.6 83.5 81.9 38.9 30.1 36.5 13.4 9.2 16.4 27.6 34.2 35.1 16. 42.1 37.1 45.5 20.6 14.1 23.2 36.3 38.9 39.4 24.4 66.5 46.5 85.7 42.0 84.3 84.8 81.8 85.3 86.8 83.9 39.6 26.4 42.0 15.5 12.0 20.8 30.7 34.1 35.2 21. Table 4. Web Navigation on Mind2Web. The gray correspond to methods that required HTML text inputsor rely on close-source GPT-4V (marked with *). ShowUI denotes our variant utilizing only action history. ShowUIs zero-shot performance yield comparable score with SeeClick with pretrained and fine-tuning. Method FT? Score CC-Net(SL) [19] Pix2Act [39] Qwen-VL [4] SeeClick [11] Qwen2-VL-2B [41] ShowUI ShowUI ShowUI-ZS 23.4 55.2 48.4 67.0 66.8 70.4 71.5 27.1 Table 5. Results on online navigation MiniWob on 35-tasks split following SeeClick [11]. ShowUI denotes our variant utilizing only action history. gap between ShowUIs zero-shot performance (27.1%) and fine-tuned Qwen-VL (48.4%) is substantial. In contrast, ShowUIs zero-shot surpasses in Mind2Web, likely because out-of-distribution errors are not adequately addressed in the instruction-tuning phase. This suggests that offline instruction-tuning alone is insufficient; we need to develop learning strategy tailored for an online environment, which can handle novel error cases. Strategy #Vis.Ctx. Train.Speedup Test-time? Screenspot Method Baseline N/A 1344.0 Token Merge UI-Graph 852.8 Token Selection Random 941.5 UI-Graph 947.4 1 1.6 1.5 1.5 N/A 70.8 42.3 34.7 65.3 56.2 70.4 64.9 (a) Comparison between different visual tokens compression methods. #Vis.ctx represents the avg. length of visual tokens across all layers. Train.Speedup denotes the training efficiency improvement over the baseline. Inference denotes whether this method is applicable at test time. Insertion #layers Screenspot Ratio #Visual Ctx. Screenspot All Early Late Cross 28 14 14 14 65.7 68.2 67.6 70.5 0 0.25 0.5 0.75 1.0 1344.0 1185.2 947.4 848.6 762. 70.8 70.6 70.4 68.3 64.5 (b) Different insertion layers. (c) Different selection ratio. Figure 9. Ablation studies of several design chocies regarding our UIGuided Token Selection. 3.3. Ablation Studies Impact by UI-Guided Token Selection. In Tab.9a, we examine various visual token optimization methods through the following variants: (i) Baseline: no visual token optimization strategy applied; (ii) Token Merge: mainstream method introduced in Sec.2.1, conditioned on our UI-Graph; (iii) Token Select.-Random: variant that randomly selects subset of visual tokens, serving as direct baseline; (iv) Token Select.-UI-Graph: our proposed method leveraging UI-Graph for token selection. 9 As shown, Token Merge performs worse than random selection, highlighting the importance of preserving positional relationships between tokens. Token Selection - UIGraph offers good balance with 1.5 speedup and competitive accuracy. While applying it at test-time slightly reduce accuracy due to resolution loss, it remains more reliable than random selection, demonstrating the effectiveness of UI connected graph as guiding criterion. Selection of Layers Insertion. In Tab.9b, we present an ablation study on different insertion strategies, including insertion across all layers, early or late layers, and crosslayer insertion, where layers alternate between inserted and non-inserted. With the same number of inserted layers, cross-layer insertion significantly outperforms both early and late insertion. Different Selection Ratio. The results is present in Tab.9c, illustrate the selection ratio as trade-off between speedup and performance, with 0.5 as an effective choice. Impact of Interleaved Streaming. We evaluate performance over iterations using interleaved streaming for (i) grounding and navigation tasks to study its effects. Action-Query: In Fig. 10, we compare grounding training with and without multi-turn streaming. Multi-turn streaming leads to faster progress, especially in the initial warmup phase, and maintains performance gap, demonstrating improved data utilization. (ii) Action-Visual: As shown in previous tables with the ShowUI variant, we validated the impact by visual context. Fig. 11 illustrates model progress across iteration steps, where the trend shows that visual+action+multi-turn outperforms visualaction and action-only setups. This validates our interleaved streaming as an effective and efficient strategy. D. Impact by Instruction-Tuning Data. One our contributions is the analysis of training data for the grounding task in Sec.2.3. In Tab.6, we present detailed ablation study to investigate the individual impact of each change on specific devices and settings. We found that (i) Data quality matters: OmniAct, with only 2K elements, achieves comparable scores to web data, and when augmented with GPT4o for diverse queries, it models like OCR [24] and SoM [46, 47, 55]. Next, LLMs integrate this information to generate responses. This approach relies heavily on closed-source LLMs [32], resulting in high costs and limited applications, as real-world users generally only perceive screenshots rather than these oracle data sources. To address these limitations, (ii) Trainingbased models [11, 17, 27, 50] are proposed, which are pretrained on large-scale UI vision-text corpora (e.g., screenshots), closing the visual perception gap to enable abilities like element grounding or navigation. Our work falls into the second category, focusing on key challenges in developing GUI visual agents, such as efficiently modeling high-resolution visual screenshots and utilizing interleaved streaming for history management. Vision-Language-Action Models. Vision-Language Models have recently made significant advancements, processing both visual and textual data, functioning effectively as flexible chatbots. However, they still face limitations when require interaction with the real-world environment. To address this, Vision-Language-Action (VLA) models have been developed to enhance physical or digital environmental perception and action generation. Notable examples include RT-2-X [33] and OpenVLA [23], which enable VLMs to perform actions in natural settings such as robotics [7, 18, 54], autonomous driving [3], and gaming [28, 42]. While substantial progress has been made in embodied contexts, the integration of VLA models into digital GUIs, remains less explored. In our work, we develop Vision-Language-Action (VLA) models for digital GUI environments to bridge this gap, focusing on the unique properties of GUIs and addressing how to model actions alongside other modalities within our VLA framework. Efficient Visual Representations. The computational cost of Large Multimodal Models is significant bottleneck, especially when scaling sequence lengths with highresolution screenshots. The straightforward methods to reduce training costs, such as token pruning [9, 38] and token merging [21, 26], are not suitable for GUI scenarios because GUIs require fine-grained location information rather than high-level semantics. These techniques discard essential spatial details, thereby impairing element grounding. Alternatively, Mixture-of-Depth (MoD) [35, 45] offers promising approach by allocating computation across model depths for language tokens, balancing performance with speed, and preserving the positional relationships of individual tokens. In our work, we develop UI-friendly solution for efficient visual token selection. High-resolution UI screenshots often contain redundant areas, like excessive whitespace, while maintaining structured layout. To leverage this, we construct UI connected graph in RGB space by treating each patch as node and finding connected components to capture redundancy relationships. This graph guides selfFigure 10. Impact by Interleaved action-query streaming on Grounding task: trained with 119K grounding data, Eval with Screenspot. Figure 11. Impact by Interleaved action-visual streaming on Navigation task: trained with GUIAct, Eval with AITW. Training Data #Sample #Ele. Mobile Desktop Web Text Icon Text Icon Text Icon Avg. AMEX Web (Seeclick [11]) Web (text+vis) Web (vis) OmniAct OmniAct (diverse) 97K 1.06M 90.1 66.8 78.8 50.7 78.3 55.3 70.0 270K 3.0M 83.9 61.1 70.6 47.9 73.0 56.3 65.5 926K 85.7 60.7 75.8 50.0 74.4 52.9 66.6 22K 576K 83.6 60.3 75.3 60.3 72.4 62.1 69.0 22K 2K 85.7 59.8 78.4 52.1 79.6 52.9 68.7 100 8K 88.3 66.8 76.3 57.9 79.1 57.3 70.9 100 Joint-Training Balanced Sampling 119K 1.6M 90.8 66.4 75.3 51.4 80.0 63.1 71.2 119K 1.6M 91.6 69.0 81.8 59.0 83.0 65.5 74.9 Table 6. Effect by individual training data on Screenspot. enhances model generalization and optimizes data usage (ii) Our collected 22K web data outperforms efficiency. SeeClicks 270K screenshots; additionally, filtering web data by visual criteria significantly reduces element size without impacting performance, suggesting that static text is less informativea property inherent to VLM. (iii) Balanced sampling is essential, yielding 3.7% acc. gain and maintaining performance across individual setups. 3.4. Qualitative Examples In Fig.12 and 13, we display several examples on Screenspot zero-shot grounding. We found that: with instruction tuning, ShowUI is able to perform some visual reasoning, such as it can distinguishes the correct operator among multiple abstract symbols or associate view help with question mark, as shown in Fig.12 (b,e). Beside, we found in several failure cases, such as Fig.12 (d,f), there might have multiple possible clickable elements, leading to model confusion. 4. Related Works GUI Agents. Recent studies reveal the potential of LLMs beyond language modeling, with advancements in [14, 43, 48, 49, 51] demonstrating their ability to autonomously complete complex tasks using tool integration. This has prompted GUI automation approaches such as: (i) Trainingfree systems, which operate in separate stages: first, they gather UI information by converting the GUI to HTML representations, accessing the accessibility tree, or using visual 10 (a) Instruction: Open wechat. With instruction-tuning, ShowUI can recognize the appearance of the WeChat icon. (b) Instruction: Rotate left. ShowUI distinguishes the correct operator among multiple abstract symbols. (c) Instruction: Zoom in. The model visually confuses the difference between zoom in and zoom out. (d) Instruction: Sign in. There are two possible sign-in elements, but the query lacks sufficient information to determine the correct one. (e) Instruction: view help for email account. ShowUI is able to associate view help with question mark clickable element. (f) Instruction: view my account. View my account could be interpreted as Click Your Profile or User Profile (top right), leading to confusion. Figure 12. Case studies on Screenspots Desktop (a-d) and Web (e-f) grounding, 11 (a) Instruction: Forwarding. ShowUI can identify what forwarding button should look like. (b) Instruction: Open allow siri when locked. ShowUI identifies the clickable element instead of the text itself. (c) Instruction: Insert from link. The query being confusing as it contain both Insert from and link (d) Instruction: Show softwares. The screenshot includes two software interfaces, causing confusion for the model. Figure 13. Case studies on Screenspots Mobile grounding. attention block to skip part of redundant tokens, reducing computational costs without extra learnable parameters. A. Datasets details 5. Conclusions We introduced ShowUI, vision-language-action model for GUI visual agents that addresses key challenges in UI visual and action modeling, and instruction-tuning data curations. From the model aspect, our development of UIGuided Visual Token Selection allows for efficient processing of high-resolution UI screenshots, significantly reducing computational costs. Our Interleaved Vision-LanguageAction Streaming framework effectively manages complex interactions across modalities. From the data aspect, with carefully curated, high-quality instructionfollowing dataset, ShowUI achieves strong performance, with lightweight model size. These results demonstrate ShowUIs potential to advance GUI visual agents towards more human-like interaction and perception. Limitation and Future work. Our model is primarily trained on offline data. promising future direction is to enhance it in online environments through reinforcement learning, enabling deeper exploration of its limitations. A.1. Instruction-Tuning data Website. We develop parser using PyAutoGUI [34] and source websites from 22 representative scenarios such as Airbnb, Booking, AMD, and Apple, which covering shopping, technology, etc. For each scenario, we collect multiple screenshots to maximize annotation coverage. This process yields 22K screenshots with total of 926K element annotations. After filtering out elements classified as static text, we retain 576K elements, averaging 26 elements per screenshot. Mobile. We source mobile data from AMEX [8] annotations: (i) Element grounding and (ii) Functionalities. which covers 97K screenshots, with 885K elements and 178K functionalities. Desktop. We collect 100 screenshots and 2,000 raw annotations from the OmniAct [22] Desktop training split, encompassing 15 software applications across iOS, Windows, and Linux desktops. Additionally, we augment these annotations using GPT-4o-assisted prompting, as detailed in the following section. A.2. Downstream tasks B.2. Prompt templates Mind2Web [12] supports the development of generalist web agents capable of completing complex tasks on any website by following language instructions. The dataset aligns each HTML document with its corresponding webpage screenshot, featuring training set of 7,775 actions and three test splitstest-task, test-website, and testdomainverified for correct rendering and element visibility to ensure reliable evaluation across tasks, websites, and It action space includes three actions: CLICK, domains. TYPE and SELECT. AITW [36] is an Android smartphone environment, which contains 30k instructions and 715K trajectories. We follow the setting by SeeClick [11], which divide the data by domains: General, Install, Google Apps, Single, Web It action space includes 12 actions: CLICK, Shopping. TYPE, SELECT, SCROLL UP, SCROLL DOWN, SCROLL LEFT, SCROLL RIGHT, PRESS BACK, PRESS HOME, PRESS ENTER, STATUS TASK COMPLETE, STATUS TASK IMPOSSIBLE. MiniWob [40] comprises 2000 open-ended tasks from 137 real web environments, each with high-level instruction and action trajectory. It action space includes 2 actions: CLICK, TYPE. B. Settings B.1. Training details We utilize 32 V100 GPUs for instruction-tuning, while downstream adaptation is conducted on 8 V100 GPUs. The batch size per GPU is set to 1, with gradient accumulation steps of 2. We use float16 precision for training. To enhance efficiency, we apply LoRA tuning with rank of 64 and an alpha value of 128 for both the language model and visual encoder, resulting in 4% of the total learnable parameters. We leverage DeepSpeed Zero-2 and use the SDPA attention mechanism. The learning rate is configured to 1e-4. The maximum visual patch number is 1280. The instructiontuning training duration is approximately two days. UI Connected Graph: We apply the UI-Graph to both the visual encoder and language model with masking ratio of 0.75, using cross-layer insertion at layer 14. During each iteration, random ratio of visual tokens is masked. For inference usage, we uniformly sample tokens across each component, ensuring visibility of all components to the model. Interleaved Streaming: In our streaming setting, we set up the history number as 2. Data Resampling: To achieve data balance among existing datasets, probabilities are assigned to each dataset, with weights set as (Web:Mobile:Desktop: GUIActWeb:GUIAct-Mobile) = (1:1:1: 1:1). GPT-4o Assisted Prompts. Below, we display the prompt used for GPT-4o to augment the OmniAct original annotations, which mainly includes three types: (i) Appearance; (ii) Spatial-relationship; (iii) Situational. You will receive screenshot with red bounding box surrounding the target element, along with the target elements name. Analyze the screenshot and provide concise descriptive responses for each of the following dimensions. 1.Appearance: Describe the target elements color, shape, ocr and other visual characteristics. - Example: rectangular chat card with blue background to Ash. 2.Spatial: Describe the target elements position based on the contextual spatial relationship. - Example: The element that positioned above the Clara. 3.Situational: Create an intent-oriented query related to the target element, considering how user might interact with it. - Example: Send message to Ash. Please follow these guidelines: - Do not confuse the red bounding box with the element itself. - Provide responses as concise sentences (15 words or fewer). - For each types, make the description specific enough to distinguish it from other elements. - If dimension does not apply, respond with None. - Structure your response in JSON format as shown below: \"A rectangular chat card with { \"appearance\": blue background with letter A\", \"spatial\": the Clara.\", \"situational\": } \"Send message to Ash.\" \"The element that positioned above Action README Template. Below, we present the template for action navigation. Variables, marked in Blue, depend on specific scenarios, while actions used for loss calculation are highlighted in Red. You are an assistant trained to navigate the {device} . Given task instruction, screen observation, and an action history sequence, output the next action and wait for the next observation. Here is the action space: # templated by action type with action description. 1. CLICK: Click on an element, value is the element to click and the position [x,y] is required. 2. TYPE: Type string into an element, value is the string to type and the position [x,y] is not applicable. ... Format the action as dictionary with the following keys: {action:action type, value:element, position:[x,y]} Position represents the relative coordinates on the screenshot and should be scaled to range of 0-1. Task: {task} <past image 1>{past action 1} ... <past image n>{past action n} <image n+1>{action n+1}"
        },
        {
            "title": "References",
            "content": "[1] Microsoft copilot. https://copilot.microsoft. com/. Accessed: 2024-04-15. [2] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. [3] Hidehisa Arai, Keita Miwa, Kento Sasaki, Yu Yamaguchi, Kohei Watanabe, Shunsuke Aoki, and Issei Yamamoto. Covla: Comprehensive vision-language-action dataset for arXiv preprint arXiv:2408.10845, autonomous driving. 2024. [4] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei arXiv preprint Huang, et al. Qwen technical report. arXiv:2309.16609, 2023. [5] Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and Sagnak Tasırlar. Introducing our multimodal models, 2023. [6] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. ToarXiv preprint ken merging: Your vit but arXiv:2210.09461, 2022. faster. [7] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023. [8] Yuxiang Chai, Siyuan Huang, Yazhe Niu, Han Xiao, Liang Liu, Dingyu Zhang, Peng Gao, Shuai Ren, and Hongsheng Li. Amex: Android multi-annotation expo dataset for mobile gui agents. arXiv preprint arXiv:2407.17490, 2024. [9] Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models. 2024. [10] Wentong Chen, Junbo Cui, Jinyi Hu, Yujia Qin, Junjie Fang, Yue Zhao, Chongyi Wang, Jun Liu, Guirong Chen, Yupeng Huo, et al. Guicourse: From general vision language models to versatile gui agents. arXiv preprint arXiv:2406.11317, 2024. [11] Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint arXiv:2401.10935, 2024. [12] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards generalist agent for the web. Advances in Neural Information Processing Systems, 36, 2024. [13] Hiroki Furuta, Ofir Nachum, Kuang-Huei Lee, Yutaka Matsuo, Shixiang Shane Gu, and Izzeddin Gur. Multimodal web navigation with instruction-finetuned foundation models. arXiv preprint arXiv:2305.11854, 2023. [14] Difei Gao, Lei Ji, Luowei Zhou, Kevin Qinghong Lin, Joya Chen, Zihan Fan, and Mike Zheng Shou. Assistgpt: general multi-modal assistant that can plan, execute, inspect, and learn. arXiv preprint arXiv:2306.08640, 2023. [15] Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating the digital world as humans do: Universal visual grounding for gui agents. arXiv preprint arXiv:2410.05243, 2024. [16] Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. realworld webagent with planning, long context understanding, and program synthesis. arXiv preprint arXiv:2307.12856, 2023. [17] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: visual language model for gui agents. arXiv preprint arXiv:2312.08914, 2023. [18] Siyuan Huang, Haonan Chang, Yuhan Liu, Yimeng Zhu, Hao Dong, Peng Gao, Abdeslam Boularias, and Hongsheng Li. A3vlm: Actionable articulation-aware vision language model. arXiv preprint arXiv:2406.07549, 2024. [19] Peter Humphreys, David Raposo, Tobias Pohlen, Gregory Thornton, Rachita Chhaparia, Alistair Muldal, Josh Abramson, Petko Georgiev, Adam Santoro, and Timothy Lillicrap. data-driven approach for learning to control computers. In International Conference on Machine Learning, pages 94669482. PMLR, 2022. [20] Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video unIn Proceedings of the IEEE/CVF Conference derstanding. on Computer Vision and Pattern Recognition, pages 13700 13710, 2024. [21] Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video unIn Proceedings of the IEEE/CVF Conference derstanding. on Computer Vision and Pattern Recognition, pages 13700 13710, 2024. [22] Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu Koh, Kiran Kamble, Waseem Alshikh, and Ruslan Salakhutdinov. Omniact: dataset and benchmark for enabling multimodal generalist autonomous agents for desktop and web. arXiv preprint arXiv:2402.17553, 2024. [23] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. [24] Kenton Lee, Mandar Joshi, Iulia Raluca Turc, Hexiang Hu, Fangyu Liu, Julian Martin Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova. Pix2struct: Screenshot parsing as pretraining for visual lanIn International Conference on Maguage understanding. chine Learning, pages 1889318912. PMLR, 2023. [25] Yang Li, Jiacong He, Xin Zhou, Yuan Zhang, and Jason Baldridge. Mapping natural language instructions to mobile ui action sequences. arXiv preprint arXiv:2005.03776, 2020. [26] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. 2024. 14 [27] Zhangheng Li, Keen You, Haotian Zhang, Di Feng, Harsh Agrawal, Xiujun Li, Mohana Prasad Sathya Moorthy, Jeff Nichols, Yinfei Yang, and Zhe Gan. Ferret-ui 2: Mastering universal user interface understanding across platforms. arXiv preprint arXiv:2410.18967, 2024. [28] Shalev Lifshitz, Keiran Paster, Harris Chan, Jimmy Ba, and Sheila McIlraith. Steve-1: generative model for text-tobehavior in minecraft. Advances in Neural Information Processing Systems, 36, 2024. [29] Kevin Qinghong Lin, Linjie Li, Difei Gao, Qinchen Wu, Mingyi Yan, Zhengyuan Yang, Lijuan Wang, and Videogui: benchmark for gui Mike Zheng Shou. arXiv preprint automation from instructional videos. arXiv:2406.10227, 2024. [30] Quanfeng Lu, Wenqi Shao, Zitao Liu, Fanqing Meng, Boxuan Li, Botong Chen, Siyuan Huang, Kaipeng Zhang, Yu Qiao, and Ping Luo. Gui odyssey: comprehensive dataset arXiv for cross-app gui navigation on mobile devices. preprint arXiv:2406.08451, 2024. [31] Yadong Lu, Jianwei Yang, Yelong Shen, and Ahmed Awadallah. Omniparser for pure vision based gui agent. arXiv preprint arXiv:2408.00203, 2024. [32] OpenAI. Gpt-4 technical report, 2023. [33] Abhishek Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anikait Singh, Anthony Brohan, et al. Open xembodiment: Robotic learning datasets and rt-x models. arXiv preprint arXiv:2310.08864, 2023. [34] PyAutoGUI. Pyautogui. 2024. https://pyautogui. readthedocs.io/en/latest/. [35] David Raposo, Sam Ritter, Blake Richards, Timothy Lillicrap, Peter Conway Humphreys, and Adam Santoro. Mixture-of-depths: Dynamically allocating compute in transformer-based language models. arXiv preprint arXiv:2404.02258, 2024. [36] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. Android in the wild: largearXiv preprint scale dataset for android device control. arXiv:2307.10088, 2023. [37] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. Android in the wild: largeIn Proceedings scale dataset for android device control. of the 37th International Conference on Neural Information Processing Systems, 2024. [38] Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, and Yan Yan. Llava-prumerge: Adaptive token reduction for efficient large multimodal models. arXiv preprint arXiv:2403.15388, 2024. [39] Peter Shaw, Mandar Joshi, James Cohan, Jonathan Berant, Panupong Pasupat, Hexiang Hu, Urvashi Khandelwal, Kenton Lee, and Kristina Toutanova. From pixels to ui actions: Learning to follow instructions via graphical user interfaces. Advances in Neural Information Processing Systems, 36, 2024. [40] Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. World of bits: An open-domain platform for web-based agents. In International Conference on Machine Learning, pages 31353144. PMLR, 2017. [41] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, 2024. [42] Zihao Wang, Shaofei Cai, Zhancun Mu, Haowei Lin, Ceyao Zhang, Xuejie Liu, Qing Li, Anji Liu, Xiaojian Ma, and Yitao Liang. Omnijarvis: Unified vision-language-action tokenization enables open-world instruction following agents, 2024. [43] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [44] Jason Wu, Siyan Wang, Siman Shen, Yi-Hao Peng, Jeffrey Nichols, and Jeffrey Bigham. Webui: dataset for enhancing visual ui understanding with web semantics. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, pages 114, 2023. [45] Shiwei Wu, Joya Chen, Kevin Qinghong Lin, Qimeng Wang, Yan Gao, Qianli Xu, Tong Xu, Yao Hu, Enhong Chen, and Mike Zheng Shou. Videollm-mod: Efficient video-language streaming with mixture-of-depths vision computation. In Proceedings of the 38th Conference on Neural Information Processing Systems, 2024. [46] An Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin, Linjie Li, Jianfeng Wang, Jianwei Yang, Yiwu Zhong, Julian McAuley, Jianfeng Gao, et al. Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation. arXiv preprint arXiv:2311.07562, 2023. [47] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v, 2023. [48] Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. Gpt4tools: Teaching large language model to use tools via self-instruction. Advances in Neural Information Processing Systems, 36, 2024. [49] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023. [50] Keen You, Haotian Zhang, Eldon Schoop, Floris Weers, Amanda Swearngin, Jeffrey Nichols, Yinfei Yang, and Zhe Gan. Ferret-ui: Grounded mobile ui understanding with multimodal llms. arXiv preprint arXiv:2404.05719, 2024. [51] Andy Zeng, Maria Attarian, brian ichter, Krzysztof Marcin Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, and Pete Florence. Socratic models: Composing zero-shot multimodal reasoning with language. In The Eleventh International Conference on Learning Representations, 2023. [52] Jianguo Zhang, Tian Lan, Ming Zhu, Zuxin Liu, Thai Hoang, Shirley Kokane, Weiran Yao, Juntao Tan, Akshara Prabhakar, Haolin Chen, et al. xlam: family of large ac15 tion models to empower ai agent systems. arXiv preprint arXiv:2409.03215, 2024. [53] Zhuosheng Zhang and Aston Zhang. You only look at screens: Multimodal chain-of-action agents. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 31323149. Association for Computational Linguistics, 2024. [54] Haoyu Zhen, Xiaowen Qiu, Peihao Chen, Jincheng Yang, Xin Yan, Yilun Du, Yining Hong, and Chuang Gan. 3d-vla: 3d vision-language-action generative world model. arXiv preprint arXiv:2403.09631, 2024. [55] Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v(ision) is generalist web agent, if grounded. In Forty-first International Conference on Machine Learning, 2024. [56] Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, et al. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023."
        }
    ],
    "affiliations": [
        "Microsoft",
        "Show Lab, National University of Singapore"
    ]
}