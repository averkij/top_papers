{
    "paper_title": "4D-Bench: Benchmarking Multi-modal Large Language Models for 4D Object Understanding",
    "authors": [
        "Wenxuan Zhu",
        "Bing Li",
        "Cheng Zheng",
        "Jinjie Mai",
        "Jun Chen",
        "Letian Jiang",
        "Abdullah Hamdi",
        "Sara Rojas Martinez",
        "Chia-Wen Lin",
        "Mohamed Elhoseiny",
        "Bernard Ghanem"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs) have demonstrated impressive 2D image/video understanding capabilities. However, there are no publicly standardized benchmarks to assess the abilities of MLLMs in understanding the 4D objects (3D objects with temporal evolution over time). In this paper, we introduce 4D-Bench, the first benchmark to evaluate the capabilities of MLLMs in 4D object understanding, featuring tasks in 4D object Question Answering (4D object QA) and 4D object captioning. 4D-Bench provides 4D objects with diverse categories, high-quality annotations, and tasks necessitating multi-view spatial-temporal understanding, different from existing 2D image/video-based benchmarks. With 4D-Bench, we evaluate a wide range of open-source and closed-source MLLMs. The results from the 4D object captioning experiment indicate that MLLMs generally exhibit weaker temporal understanding compared to their appearance understanding, notably, while open-source models approach closed-source performance in appearance understanding, they show larger performance gaps in temporal understanding. 4D object QA yields surprising findings: even with simple single-object videos, MLLMs perform poorly, with state-of-the-art GPT-4o achieving only 63\\% accuracy compared to the human baseline of 91\\%. These findings highlight a substantial gap in 4D object understanding and the need for further advancements in MLLMs."
        },
        {
            "title": "Start",
            "content": "4D-Bench: Benchmarking Multi-modal Large Language Models for 4D Object Understanding Wenxuan Zhu1, Bing Li1, Cheng Zheng1 Jinjie Mai1, Jun Chen1, Letian Jiang1, Abdullah Hamdi2, Sara Rojas Martinez1, Chia-Wen Lin3, Mohamed Elhoseiny1, Bernard Ghanem1 1King Abdullah University of Science and Technology, 2University of Oxford, 3National Tsing Hua University 5 2 0 2 2 2 ] . [ 1 7 2 8 7 1 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs) have demonstrated impressive 2D image/video understanding capabilities. However, there are no publicly standardized benchmarks to assess the abilities of MLLMs in understanding the 4D objects (3D objects with temporal evolution over time). In this paper, we introduce 4D-Bench, the first benchmark to evaluate the capabilities of MLLMs in 4D object understanding, featuring tasks in 4D object Question Answering (4D object QA) and 4D object captioning. 4D-Bench provides 4D objects with diverse categories, high-quality annotations, and tasks necessitating multi-view spatial-temporal understanding, different from existing 2D image/video-based benchmarks. With 4DBench, we evaluate wide range of open-source and closedsource MLLMs. The results from the 4D object captioning experiment indicate that MLLMs generally exhibit weaker temporal understanding compared to their appearance understanding, notably, while open-source models approach closed-source performance in appearance understanding, they show larger performance gaps in temporal understanding. 4D object QA yields surprising findings: even with simple single-object videos, MLLMs perform poorly, with state-of-the-art GPT-4o achieving only 63% accuracy compared to the human baseline of 91%. These findings highlight substantial gap in 4D object understanding and the need for further advancements in MLLMs. Project page: https://4dbench.github.io/ Figure 1. An example demonstrating the challenges of 4D object understanding involves multi-view spatial-temporal reasoning. Given the 4D object, the robots right hand seems ambiguous in some views at first and eventually disappears over time. Hence, answering the question needs to (1) address multi-view ambiguity and choose proper views and time that the right hand is visible, (2) localize the right hand, (3) and track its evolutions along the time dimension. 1. Introduction Digital 4D (i.e. dynamic 3D) assets have received increasing attention from both academia [9, 52, 61, 88, 121] and industry [1, 2], as they are important to many real-world applications such as digital twins, augmented reality, and gaming. With the increasing demand for dynamic and interactive virtual experiences, it is desirable to understand and interact with 4D assets using language, necessitating 4D-object-language understanding for 4D assets. While many efforts [5, 17, 22, 55, 66, 105, 129] have been devoted to 2D image/video language understanding, 4D object language understanding has been much less underexplored, yet it poses new challenges. First, unlike 2D images, where parts of an object are occluded or ambiguous, 4D object can be observed from different views, ex1 hibiting different appearances among views and dynamic motions over time. As result, 4D object understanding requires both multi-view spatial and temporal understanding (see Fig. 1). Additionally, diverse 4D representations (e.g. point cloud squences [12, 58], 4DGS [110]), add more difficulties in 4D object understanding. Second, unlike the massive availability of 2D image-text data on the Internet, large-scale 4D-object-text data are scarce, hindering the development of 4D-object-centric foundation models. In this paper, instead of costly building large-scale 4Dobject-text dataset and establishing 4D object understanding model on advanced 4D representation (e.g. point clouds, 4DGS), we explore new question. Can we directly expand advanced Multi-modal Large Language Models (MLLMs) to 4D object understanding? Current MLLMs, such as GPT-4o [3] and Qwen2-VL [105], have learned rich world knowledge from massive text, image and video data. By representing 4D objects as multi-view videos, we can leverage MLLMs for 4D object language understanding. However, significant challenge arises: there are no such public benchmarks designed for evaluating 4D object language understanding abilities, to the best of our knowledge. Without dedicated benchmark, it is unclear what the strengths and limitations of these models are in 4D object understanding, thereby making it difficult to improve MLLMs and unlock their potential. To fill the gap, we step towards 4D object language understanding by introducing new benchmark, dubbed 4DBench. The 4D-bench presents 4D object captioning and 4D object Question Answering (QA) tasks, enabling an indepth evaluation of MLLMs. Due to the lack of publicly available high-quality text descriptions for 4D objects, it is non-trivial to construct annotations through leveraging text information in existing 4D object datasets, unlike 2D images/videos [57]. Instead, we devote great human efforts to manually ensure that most questions necessitate multi-view spatial-temporal understanding for 4D object QA, so that our 4D-Bench provides high-quality annotations yet challenging evaluations. Our 4D-Bench introduces new dimensions in evaluating MLLMs, compared to 2D image/video benchmarks. First, our benchmark necessitates both multi-view spatial and temporal understanding, which has been ignored by existing 3Dand 2D-language understanding benchmarks. For example, 3D-language understanding benchmarks (e.g. [8, 40]) focus on static 3D scene understanding, ignoring motion information, while 2D video benchmarks (e.g. [31, 33] ) ignore multi-view understanding. Second, our 4D-Bench comprises digital 4D assets, which are synthetic and include counterfactual objects and motions, typically absent in real-world datasets. This enables our 4D-Bench to be an Out-Of-Distribution (OOD) evaluation for MLLMs trained on real-world, scene-level 2D images/videos. With 4D-Bench, we evaluate various MLLMs ranging from closed-source models such as Gemini 1.5 Pro [86] and GPT-4o [79] to open-source ones (e.g. Qwen2-VL [105]). Our extensive experiments reveal several key insights about current MLLMs 4D object understanding capabilities: (1) Even state-of-the-art models still perform notably worse than humans across both question answering and captioning tasks; (2) On the 4D object QA task, MLLMs demonstrate clear performance hierarchy across different understanding dimensions: they perform relatively better on appearance and spatial relationship subtasks but struggle considerably with object counting (37.29% average accuracy), action recognition, and temporal relationship understanding; (3) 4D object captioning experimental results shows similar pattern where MLLMs generally achieved higher GPT-Appearance scores than GPT-Action scores. Notably, closed-source models generally outperform open-source alternatives, particularly in action understanding, some opensource models show competitive performance in appearance comprehension. Our contributions can be summarized as follows: We introduce 4D-Bench, the first comprehensive benchmark for evaluating MLLMs capabilities in understanding 4D objects, featuring both captioning and questionanswering tasks. Our benchmark provides new challenges, necessitating multi-view spatial-temporal understanding, while it can serve as generalization evaluation benchmark for image/video MLLMs. Evaluation results effectively reveal the strengths and shortcomings of the evaluated MLLMs in 4D object understanding. 2. Related Work Multimodal Large Language Models (MLLMs). Large Language Models (LLMs) such as GPT-4o[3], LLaMA [101, 102], and Gemini [96] have demonstrated substantial capabilities in language comprehension, generation, and knowledge retrieval. Concurrently, vision-language models like CLIP [85] have successfully aligned visual and textual modalities. To understand information across multiple modalities, MLLMs [5, 17, 22, 55, 66, 105, 129] extend the capabilities of LLMs to modalities such as 2D images, videos, and audio by introducing alignment modules and visual instruction tuning. Models like MiniGPT-4 [17, 129] and LLaVA [51, 65, 66, 125] use multilayer perceptrons (MLPs) to align features extracted by pre-trained vision backbones to the latent space of LLMs, while 2DVideo LLMs such as VideoChat [56] and Video-LLaMA [120] employ Q-former modules for 2D video understanding. In the realm of 3D vision-language tasks, models like 3D-LLM [39], 3DVista [130], and GPT4Point [84] have been proposed. 2 Figure 2. Illustration of the 4D-Bench. 4D-Bench consists of two critical tasks (a) 4D object QA and (b) 4D object captioning. 4D object QA provides one question and four choices per QA to evaluate MLLMs. 4D object captioning provides five human captions per 4D object. Recent works like InstructBLIP [24], ShareGPT4V [18], and ShareGPT4Video [19] leverage GPT-4 Vision to generate large-scale, highly descriptive image-text and video-text datasets, improving captioning capabilities. VImageBindLLM [35] extends multimodal understanding by aligning embeddings from various modalities, including audio and 3D point clouds, to LLMs using learnable binding network. Our findings highlight significant room for improvement in fine-grained temporal understanding within 4D object comprehension, underscoring the need for systematic evaluation and further research to address these challenges. Evaluations of MLLMs. To evaluate image and video tasks in MLLMs, range of benchmarks has emerged [11, 33, 59, 70, 71, 99, 100, 118, 119]. Initial efforts [67, 117] provided foundational assessments but lacked scale, leading to benchmarks that assess perception and cognition across diverse subtasks [112]. Liu et al. [70] leveraged GPT4 [3] for scalable, labor-free evaluations. More recent developments like SEED-Bench and SEED-Bench-2 [49, 50] introduced six-fold larger annotations with extensive multimodal questions, categorizing MLLM capabilities into hierarchical levels. Image understanding benchmarks evolved from object counting [104] to high-resolution detail assessments [48, 107]. Fine-grained image-text alignment and relational understanding are evaluated through complex semantic matching [82, 98] and paired image relationships [43]. For further details on these benchmarks, please refer to [54]. Video understanding benchmarks [21, 31, 33, 57, 72, 75, 95, 127] focus on temporal coherence and action recognition, progressing from early tasks [89] to more granular temporal and causal assessments [42, 57, 72, 77]. Real-world activities with overlapping actions are assessed in [13], while comprehensive video evaluations encompass diverse tasks and long-form content [21, 29, 78, 127]. In addition to MLLMs, T3bench [37] introduces benchmark to evaluate text-to-3D generation methods. Different from these benchmarks, our benchmark focuses on evaluating the capability of MLLMs on 4D-object-centric understanding. 3. New Benchmark: 4D-Bench We establish new benchmark named 4D-Bench to evaluate MLLMs on 4D object understanding. We define the 4D object question answering task in Sec. 3.1 and the 4D object captioning task in Sec. 3.2. We then describe the data collection and the annotations of these two tasks in Sec. 3.3. 3.1. Task 1: 4D Object Question Answering We propose the following five subtasks of 4D object QA to evaluate MLLMs 4D object understanding capability. While some subtask definitions may be similar to those in 2D video benchmarks, the complexity of 4D objects introduces new challenges for MLLMs. Appearance. This subtask evaluates MLLMs to analyze and describe the visual attributes of objects. This subtask presents two key challenges: (1) many objects in our dataset are synthetic or fictional, presenting attributes and configurations that may deviate significantly from real-world examples that MLLMs were trained on, and (2) the multi-view nature requires MLLMs to integrate appearance information across different viewpoints (e.g., From the front view, what color is the main part of the characters outfit? From the side view, does the character appear to have any accessories attached to their back?). Action. Different from 2D video-based benchmarks that focus on scene-level videos, our benchmark enables the deep study of the activities of an object and the motions of its local parts from multiple viewpoints. The action subtask evaluates MLLMs in three additional aspects: (1) typical action recognition; (2) fine-grained motion detection that recognizes subtle movements of specific parts; (3) directional movement analysis that determines specific movement directions. namic 3D objects collected from Objaverse-XL [26]. Due to the noisy nature of the data, we designed data-cleaning pipeline to filter out low-quality samples. The data-cleaning process consists of two main stages. Object motion analysis. We perform pixel change detection of the rendered videos to identify the temporal boundaries of object motion, allowing us to extract relevant video segments. This ensures the dataset contains exclusively dynamic objects. Object visual quality assessment. Many 4D objects exhibit undesirable visual characteristics, such as oversimplified geometry, lack of texture, and poor aesthetic quality. Here, we propose CLIP-based[85] filtration framework. We manually annotated thousands of images as high or low quality, then we fine-tuned the CLIP image encoder to serve as quality classifier to distinguish between high and lowquality objects. The resulting classifier effectively filters out low-quality objects, ensuring that only visually appealing and geometrically complex objects are included. 3.3.2. 4D Object Question Answering Annotation. Designing challenging 4D object question-answer pairs necessitating both multi-view spatial and temporal understanding is challenging, given that our multi-view videos feature only single object and cover short time span. We began by engaging professional (have done similar tasks before) annotators who were instructed to carefully observe the rendered multi-view videos and design challenging questions with four choices. Each annotation was subsequently manually verified by us. However, this process proved to be not only costly but also suffered from quality degradation over time. Specifically, the retention rate of annotations from the annotation team initially stood at 92.0% but dramatically declined to 62.5% in later stages. During this preliminary exploration phase, we retained 164 high-quality QA pairs that met our rigorous standards. Inspired by recent work [14, 57, 75], we leveraged MLLMs, specifically GPT-4o and Qwen2-VL, to generate QA pairs from tens of thousands multi-view videos of 4D objects. By prompting the model to analyze multi-view videos through chain-of-thought reasoning, we facilitated the generation of challenging questions and options. The generated QA pairs underwent an initial validation process using the Qwen2-VL 7B model to ensure strict adherence to the predefined task-specific guidelines and quality criteria. Then we run blind filtering by inputting only the QA text content (without visual input) to Qwen2.5[114] and Llama 3.1[30] and drop those where both models answer correctly. Finally, we performed manual review to refine the remaining pairs and removed any inappropriate 4D object question-answering pairs. Figure 3. Pipeline for constructing the 4D-Bench dataset. The pipeline includes rendering multi-view videos for 4D objects from Objaverse-XL, motion filtering, visual quality filtering, and multistage annotations for QA pairs and captions. Captions are purely human-annotated, while QA pairs are generated through hybrid approach using MLLMs and human validation. Object Counting. This evaluation subtask evaluates MLLMs by performing precise object enumeration under dynamic and spatially complex scenarios. The key chal- (1) temporal dynamics where lenges lie in two aspects: objects may appear or disappear during the sequence, requiring continuous tracking and count adjustment, and (2) occlusion handling where objects may be partially or fully obscured from certain viewpoints, necessitating cross-view information integration to arrive at accurate counts. Spatial Relationship. This subtask tests MLLMs ability to understand spatial configurations across multiple viewpoints, requiring them to analyze object relationships and transformations while integrating information from different angles to handle occlusions. Temporal Relationship. subtask examines MLLMs ability to comprehend the temporal evolution of objects or sequential actions. This 3.2. Task 2: 4D Object Captioning The 4D object captioning task is to generate text descriptions for the 4D objects. Here, our task requires MLLM to interpret and describe the objects appearance and actions. Unlike 2D image/video captioning [4, 16, 20, 28, 45, 111], 4D object captioning necessitates multi-view spatialtemporal understanding in two aspects: (1) appearance description requires aggregating visual details observed from different angles to form complete understanding of the objects characteristics, and (2) action description demands observing the motion sequence from various perspectives to accurately capture complex movements that may be ambiguous or partially visible from single viewpoint. 3.3. Data Collection and Annotation In this section, we describe the construction of our 4DBench dataset shown in Fig. 3. 3.3.1. 4D Data Collection and Curation. We choose multi-view videos as the representation for 4D objects to make the benchmarking of MLLMs possible. To build our dataset, we render tens of thousands of dy4 CIDEr [103], which remain standard in the caption evaluation literature despite some noted limitations. We also incorporate embedding-based metrics like BERTScore [124] and Sentence-BERT [87]. Furthermore, inspired by recent findings [28, 74, 76, 95] that have widely validated and adopted LLM-based evaluation for its stronger correlation with human judgment [28, 76], we introduce GPT-4o as our LLM evaluator. The GPTAppearance and GPT-Action scores evaluate the similarity between the predicted and human-annotated captions in terms of object appearance and actions, respectively. Both scores range from 0 to 5, and the GPT-Eval score is the average of these two scores. For more information about GPT evaluation, please refer to the Appendix. 4.2. Evaluation Settings We evaluate range of advanced MLLMs, including two leading closed-source models, GPT-4o [3] and Gemini 1.5 Pro [86], as well as widely used open-source models: MiniGPT4-Video [6], VideoChat2 [57], InternVL2 [22], Qwen2-VL [105], LLaVA-OneVision [51] and LLaVAVideo [125]. We uniformly select views around the 4D object from the rendered multi-view videos, then sample frames from each selected views video sequence, resulting in frames input. In our experiments, we empirically set = 3 and = 6. Such sampling strategies ensure that the evaluations fulfill GPU memory constraints while covering the multi-view and temporal information of 4D objects well. 4.3. Evaluation Results on 4D Object QA 4D object question answering experimental results are showed in Tab. 1. Here, we provide our key findings. MLLMs underperform humans. Our experimental results demonstrate clear performance hierarchy, with GPT4o achieving the highest Overall accuracy (62.98%). However, it should be noted that even the best-performing model achieves relatively modest accuracy. This is particularly striking given that our test cases primarily involve simple 4D objects - when presented with carefully designed questions requiring multi-view spatial and temporal understanding, current MLLMs struggle to provide accurate responses. MLLMs struggle most with Object Counting task. large performance gap between object counting and other subtasks. All models struggle in Object Counting (37.29% average accuracy), in contrast, even for the challenging subtask Temporal Relationship understanding, models achieve higher performance (49.29% average accuracy). Fig. 5 shows the performance of MLLMs on counting problem. Although the absence of motion information lowers the complexity of answering the question, Gemini 1.5 pro, Qwen2-VL 7B, LLava-Video 7B and GPT-4o still wrongly answer the question. Such results uncover the limitations of Figure 4. Subtask and category distributions in 4D object QA and captioning. Left: Distribution of five subtasks in the 4D object QA task, 751 question-answering pairs in total. Right: Distribution of 4D object categories in 4D object captioning task, 580 4D objects in total. 3.3.3. 4D Object Captioning Annotation. We manually examined approximately 8,000 candidate 4D objects and carefully selected 580 representative samples, prioritizing diversity in object types and motion characteristics (see Fig. 4 for 4D object category distribution). For each object, five professional annotators independently provided one caption based on the multi-view video, resulting in five unique descriptions per 4D object. dedicated reviewer ensured that captions captured significant details and exhibited diversity, unsatisfactory captions were revised accordingly. 3.4. Statistics of 4D-Bench. The statistics of 4D-Bench are shown in Fig. 4, we provide more details in the Appendix. Our 4D Object QA task contains 751 question-answer pairs for 736 4D objects, where the Action subtask comprises the largest portion of the question-answer pairs. The remaining four subtasks (Appearance, Object Counting, Spatial Relationship, and Temporal Relationship) are distributed in relatively balanced proportions. 4D object captioning task of 4D-Bench covers 580 4D objects with diverse categories. 4. Experiments 4.1. Evaluation Metrics 4D object question answering metrics. The 4D object QA consists of questions with four choices where only one choice is correct. We report both task-specific accuracies and the aggregate performance across the entire benchmark dataset. 4D object captioning metrics. To evaluate the generated captions against the five human annotations provided for each 4D object, we employ comprehensive evaluation framework. This includes traditional n-gram-based metrics such as BLEU [81], ROUGE [62], METEOR [10], and 5 action understanding. deeper analysis across different evaluation metrics reveals interesting patterns in model capabilities. We observe that both open-source and closed-source models generally achieve higher scores For inin GPT-Appearance compared to GPT-Action. stance, Qwen2-VL 72B achieves GPT-Appearance score of 3.324/5 but drops to 2.791/5 for GPT-Action. Open-source models lag behind closed-source models in action understanding. All the closed-source models (such as Gemini 1.5 Pro and GPT-4o mini) achieve higher overall performance in 4D object captioning compared to open-source models, where their GPT-Eval scores are higher than 3 (out of maximum score of 5). In contrast, among open-source models, only Qwen2-VL 72B achieves the GPT-Eval score above 3. Notably, in terms of appearance understanding, open-source models demonstrate competitive performance with their closed-source counterparts, with models like LLaVA-Video 7B and Qwen2-VL 72B achieving GPT-Appearance scores (3.235/5 and 3.324/5, respectively) comparable to Gemini 1.5 Pro (3.311/5). However, when it comes to action understanding, there exists noticeable gap between open-source and closed-source models. Closed-source models like GPT-4o and Gemini 1.5 Pro maintain stronger performance in GPT-Action (3.258/5 and 2.983/5, respectively), while open-source alternatives show relatively weaker capabilities in this aspect, typically scoring below 2.8. 4.5. Discussions Impact of view number and sampling frequency. Here, we study MLLMs performance by varying the number of views and sampling frequency of video frames that fed into the model independently. For 4D object question answering, Fig. 6 shows consistent accuracy improvements with both increased views (41.3% to 53.7% with fixed frames) and increased sampling frequencies (46.3% to 53.7% with fixed views), confirming that our questions effectively require both multi-view and temporal understanding rather than being solvable from limited viewpoints or timestamps. However, we observed that performance degrades when exceeding 3 views or 6 frames, likely due to information redundancy that may overwhelm the models processing capacity. For 4D object captioning, Fig. 7 shows that increasing the number of views from 1 to 6 improves the GPT-Eval scores from 2.79 to 2.98. For temporal sampling, increasing frames from 1 to 3 boosts the GPT-Eval score from 2.48 to 2.89, and sampling frequency of 6 further improves the GPT-Eval score to 2.96. However, when the sampling frequency is increased from 6 to 9, the performance improvement becomes negligible. Robustness evaluation. We propose the following two concerns: (1) In the original experiment design, when inFigure 5. An example from Object Counting subtask. Answering this question requires integrating multi-view information and capturing cross-view correspondences to count the presents, necessitating multi-view reasoning abilities. If relying solely on single view (e.g. the middle row), it would lead to wrong answers (e.g. four), since some boxes are occluded and invisible in this view. these advanced MLLMs in fusing information from different views to reason accurate counts. MLLMs are better at appearance and spatial understanding than action and temporal understanding. This pattern is also validated in the following 4D object captioning experimental results. As shown in Tab. 1, many MLLMs achieve over 70% accuracy in the Appearance subtask. In the subtask of Spatial Relation, half of the MLLMs achieve over 60% accuracy. However, all MLLMs perform worse in subtasks of Temporal Relationship and Action, with average accuracies of only 49.29% and 49.37%, respectively. The above evaluation results highlight the new challenges posed by 4D object understanding and showcase the shortcomings of MLLMs in detailed aspects. On the other hand, the revealed shortcomings provide valuable guidance for future improvements. For example, weak performance in action understanding suggests more advanced temporalaware visual encoders can enhance MLLMs performance. 4.4. Evaluation Results on 4D Object Captioning Tab. 2 illustrates the evaluation results of various MLLMs on the 4D object captioning task of 4D-Bench. The following analysis primarily relies on GPT-Appearance, GPTAction, and GPT-Eval scores [28, 76]. MLLMs underperform humans. Current state-of-theart multi-modal large models (MLLMs) still underperform compared to humans. As shown in Tab. 2, humans achieve better scores with GPT-Eval score of 3.826 out of 5, compared to even the best-performing MLLM, GPT-4o, with score of 3.382 out of 5. MLLMs are better at appearance understanding than 6 Model Object Counting (%) Temporal Relationship (%) Action (%) Spatial Relationship (%) Appearance (%) Overall (%) MiniGPT4-Video [6] VideoChat2 [56] InternVL2 8B [22] LLaVA-OneVision 7B [51] LLaVA-Video 7B [125] Qwen2-VL 7B [105] InternVL2 76B [22] LLaVA-OneVision 72B [51] LLaVA-Video 72B [125] Qwen2-VL 72B [105] Gemini 1.5 Flash [86] GPT-4o mini [3] Gemini 1.5 Pro [86] GPT-4o [3] Average Human 22.05 22.83 18.11 42.52 42.52 38.58 28.35 49.61 54.33 45.67 26.77 40.16 46.46 44.09 37.29 88.98 26.43 31.43 31.43 52.86 55.00 56.43 45.00 58.57 58.57 55.71 50.00 50.71 58.57 59. 49.29 89.29 22.90 33.18 35.98 42.99 52.80 57.94 42.52 60.75 57.48 58.41 53.27 50.00 59.35 63.55 49.37 94.39 22.39 38.81 32.09 57.46 56.72 58.96 38.81 61.19 66.42 61.19 60.45 61.94 64.18 69.40 53.57 91.04 22.06 34.56 39.71 74.26 78.68 71.32 64.71 76.47 77.21 72.06 66.18 72.06 68.38 77. 63.92 89.71 23.17 32.36 32.09 53.00 56.86 56.99 43.94 61.38 62.32 58.72 51.80 54.59 59.52 62.98 50.69 91.08 Table 1. 4D object question answering results. The Overall column refers to average accuracy across all sub-tasks. The Average row represents the mean performance of all tested models in each category. We provide human performance as reference. Model CIDEr BLEU@4 METEOR ROUGE BERT SBERT GPT-Appearance GPT-Action GPT-Eval MiniGPT4-Video [6] InternVL2 8B [22] VideoChat2-Mistral [57] LLaVA-OneVison 7B [51] LLaVA-Video 7B [125] Qwen2-VL 7B [105] InternVL2 76B [22] LLaVA-OneVision 72B [51] LLaVA-Video 72B [125] Qwen2-VL 72B [105] Gemini 1.5 Flash [86] GPT-4o mini [3] Gemini 1.5 Pro [86] GPT-4o [3] Average Human 18.4 48.4 79.0 86.4 102.6 84.5 72.0 107.4 106.2 95.1 84.3 51.1 94.8 69.0 - 126.6 0.6 2.5 6.9 10.0 14.6 10.1 5.5 16.1 15.1 12.4 7.3 2.7 11.2 6. 23.1 27.9 33.5 39.2 41.7 36.9 34.2 41.1 39.8 40.3 36.5 30.8 38.7 35.9 13.2 22.6 33.5 32.7 38.8 36.4 27.1 41.5 40.9 38.0 32.9 24.0 39.0 32.1 50.7 58.2 65.4 63.2 66.7 65.7 60.9 68.5 68.5 66.8 65.3 59.3 68.5 64.1 51.2 60.3 59.7 65.6 68.1 66.9 65.3 68.0 68.1 67.5 68.9 63.5 68.8 66.4 - 14.12 - 45. - 43.48 - 71.69 - 76.30 1.737/5 2.531/5 2.578/5 3.166/5 3.235/5 3.170/5 3.099/5 3.180/5 3.138/5 3.324/5 3.246/5 3.311/5 3.311/5 3.507/5 3.038/5 3.772/5 1.351/5 1.877/5 1.912/5 2.479/5 2.552/5 2.666/5 2.637/5 2.268/5 2.471/5 2.791/5 2.931/5 3.131/5 2.983/5 3.258/ 2.522/5 3.879/5 1.544/5 2.204/5 2.245/5 2.823/5 2.894/5 2.918/5 2.868/5 2.724/5 2.804/5 3.057/5 3.088/5 3.221/5 3.147/5 3.382/5 2.780/5 3.826/5 Table 2. 4D object captioning results. The Average row represents the mean performance of all tested MLLM models under each metric. The Human row represents the performance of human annotator under each metric. For each metric, we bold the best performing MLLM model. We highlight GPT metrics as they demonstrate better alignment with human preferences in evaluating caption quality, and our analysis also primarily focuses on models performance across these metrics. GPT-4os GPT metrics are marked in gray due to the potential self-evaluation bias when using GPT-based metrics to evaluate GPT model[80]. We provide human performance as reference. putting images to the large language model, we prioritized viewpoint order (all frames from viewpoint 1, then all frames from viewpoint 2). How would the results differ if (2) In the original we prioritized temporal order instead. experimental design, we didnt include timestamp information for each image in the prompt (since they were all short videos). What would the results be if we included timestamp information? To answer those questions, we run corresponding experiments on 4D object question answering and the results are shown in Tab. 3. The minimal variations in model performance across different input configurations (temporal vs. viewpoint-first ordering and with/without timestamps) demonstrate the robustness of our original experimental design. When MLLMs encounter counterfactual 4D data. Unlike existing benchmarks based on real-world videos, our dataset is built on artificially created 4D objects and hence provides some counterfactual 4D data that deviates from physical laws and behaves differently from its real-world counterpart. These data serve as valuable testbed to examine whether MLLMs truly understand the input or simply rely on learned world knowledge. For example, as illustrated in Fig. 8, our benchmark includes counterfactual testing data where synthetic spider has 6 legs, contrary to the fact that real spiders typically have 8 legs. Similarly, Fig. 9 presents counterfactual testing data where ball rolls into downward-facing hole 7 Figure 6. Effect of view number and temporal sampling on the 4D object QA performance. Tested on Gemini 1.5 Flash. Left: Accuracies across different numbers of views with fixed 6 frames. Right: Accuracies across different temporal frequencies with fixed 3 views. Figure 8. counterfactual example from 4D object QA task. synthetic spider with six legs, illustrating counterfactual scenario for testing model understanding, as real spiders typically have eight legs. Figure 7. Effect of view number and temporal sampling on the 4D object captioning performance. Tested on Qwen2-VL 7B. Left: GPT-Eval scores across different numbers of views with fixed 6 frames. Right: GPT-Eval scores across different temporal frequencies with fixed 3 views. Model MiniGPT4-Video [6] VideoChat2 [57] InternVL2 8B [22] LLaVA-OneVision 7B [51] LLaVA-Video 7B [125] Qwen2-VL 7B [105] InternVL2 76B [22] LLaVA-OneVision 72B [51] LLaVA-Video 72B [125] Qwen2-VL 72B [105] Gemini 1.5 Flash [86] GPT-4o mini [3] Gemini 1.5 Pro [86] GPT-4o [3] Average Original Setting(%) Frame Order(%) w/ Time Stamp(%) 17.58 (5.59) 33.95 (1.59) 38.88 (6.79) 51.40 (1.60) 59.25 (2.39) 49.80 (7.19) 47.54 (3.60) 61.25 (0.13) 62.72 (0.40) 54.46 (4.26) 51.80 (0.00) 53.66 (0.93) 58.72 (0.80) 60.85 (2.13) 50.13 (0.56) 17.18 (5.99) 23.04 (9.32) 33.69 (1.60) 53.53 (0.53) 57.52 (0.66) 57.52 (0.53) 46.07 (2.13) 60.59 (0.79) 61.92 (0.40) 59.25 (0.53) 52.86 (1.06) 53.79 (0.80) 59.25 (0.27) 63.12 (0.14) 49.95 (0.74) 23.17 32.36 32.09 53.00 56.86 56.99 43.94 61.38 62.32 58.72 51.80 54.59 59.52 62.98 50.69 Table 3. Robustness study of 4D object QA experiment. Green arrows () indicate improvement over Original Settings Overall accuracy, while red arrows () show decline. and then rolls back out, defying the laws of physics, as ball would normally remain trapped in hole in the real world. Given these testing data, all advanced MLLMs, including Gemini 1.5 Pro, Qwen2-VL 7B, LLaVA-Video 7B, and GPT-4o, choose the wrong answer. These results highlight that these advanced MLLMs are not robust enough to understand counterfactual data. 8 Figure 9. counterfactual example from 4D object QA task. ball rolling into downward-facing hole and then rolling back out, depicting counterfactual scenario that violates physical laws, as ball would normally stay trapped in the hole. 5. Conclusion We present 4D-Bench, novel benchmark for assessing the 4D object understanding capabilities of MLLMs. Compared with existing benchmarks for 2D image and video understanding, 4D-Bench is 4D-object-centric, providing 4D objects with diverse categories for benchmarking MLLMs. 4D-Bench presents two critical tasks regarding 4D object question answering and 4D object captioning, necessitating multi-view spatial-temporal understanding. Benchmarking results reveal that the capabilities of existing MLLMs are limited in 4D object understanding. We hope that 4DBench facilitates the development of MLLMs in 4D object understanding and other related research areas. For example, our benchmark on 4D object captioning fills in the gap of quantitatively evaluating 4D object captioning performance, which drives research on leveraging MLLMs to generate high-quality text descriptions from 4D objects for improving text-to-4D generative models. Our benchmark on 4D object question answering enables the community to conduct an in-depth evaluation of the capabilities of MLLMs in specific aspects. Acknowledgement. This work was supported by funding from King Abdullah University of Science and Technology (KAUST) - Center of Excellence for Generative AI, under award number 5940. License. 4D-Bench is strictly for academic research purposes, and any form of commercial use is prohibited. The copyright of all 4D objects is retained by their respective owners, and proper acknowledgement will be given in the dataset. The dataset as whole is licensed under the ODCBy v1.0 license, consistent with the licensing of ObjaverseXL[26]."
        },
        {
            "title": "References",
            "content": "[1] 4d technology market size, share, growth report, 2025. 1 [2] 3d digital asset market share, forecast, 2025. 1 [3] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 2, 3, 5, 7, 8 [4] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson. Nocaps: Novel object captioning at scale. In Proceedings of the IEEE/CVF international conference on computer vision, pages 89488957, 2019. 4, 18 [5] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35: 2371623736, 2022. 1, 2 [6] Kirolos Ataallah, Xiaoqian Shen, Eslam Abdelrahman, Essam Sleiman, Deyao Zhu, Jian Ding, and Mohamed Elhoseiny. Minigpt4-video: Advancing multimodal llms for video understanding with interleaved visual-textual tokens. arXiv preprint arXiv:2404.03413, 2024. 5, 7, 8, 21 [7] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An open-source framework for training large autoregressive vision-language models. arXiv preprint arXiv:2308.01390, 2023. 18 [8] Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, and Motoaki Kawanabe. Scanqa: 3d question answering for spatial scene understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [9] Sherwin Bahmani, Ivan Skorokhodov, Victor Rong, Gordon Wetzstein, Leonidas Guibas, Peter Wonka, Sergey Tulyakov, Jeong Joon Park, Andrea Tagliasacchi, and 4d-fy: Text-to-4d generation usDavid B. Lindell. arXiv preprint ing hybrid score distillation sampling. arXiv:2311.17984, 2023. 1 [10] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pages 6572, 2005. 5, 18 [11] Han Bao, Yue Huang, Yanbo Wang, Jiayi Ye, Xiangqi Wang, Xiuying Chen, Mohamed Elhoseiny, and Xiangliang Zhang. Autobench-v: Can large vision-language models benchmark themselves?, 2024. 3 [12] Wei Cao, Chang Luo, Biao Zhang, Matthias Nießner, and Jiapeng Tang. Motion2vecsets: 4d latent vector set diffusion for non-rigid shape reconstruction and tracking, 2024. 2 [13] Rajatsubhra Chakraborty, Arkaprava Sinha, Dominick Reilly, Manish Kumar Govind, Pu Wang, Francois Bremond, and Srijan Das. Llavidal: Benchmarking large language vision models for daily activities of living. arXiv preprint arXiv:2406.09390, 2024. 3 [14] Keshigeyan Chandrasegaran, Agrim Gupta, Lea M. Hadzic, Taran Kota, Jimming He, Cristobal Eyzaguirre, Zane Durante, Manling Li, Jiajun Wu, and Fei-Fei Li. Hourvideo: 1-hour video-language understanding. In Advances in Neural Information Processing Systems, 2024. [15] David Chen and William Dolan. Collecting highly parallel data for paraphrase evaluation. In Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies, pages 190200, 2011. 18 [16] David L. Chen and William B. Dolan. Collecting highly parallel data for paraphrase evaluation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL-2011), Portland, OR, 2011. 4 [17] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechu Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. large language model as unified interface Minigpt-v2: arXiv preprint for vision-language multi-task learning. arXiv:2310.09478, 2023. 1, 2 [18] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023. 3 [19] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin, Improving video Zhenyu Tang, et al. Sharegpt4video: understanding and generation with better captions. arXiv preprint arXiv:2406.04325, 2024. 3 [20] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, and Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015. 4, 18 [21] Xiuyuan Chen, Yuan Lin, Yuchen Zhang, and Weiran Huang. Autoeval-video: An automatic benchmark for assessing large vision language models in open-ended video arXiv preprint arXiv:2311.14906, question answering. 2023. [22] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023. 1, 2, 5, 7, 8 [23] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatialtemporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. 18 [24] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, 10 InstructBLIP: Towards general-purpose and Steven Hoi. vision-language models with instruction tuning. In Thirtyseventh Conference on Neural Information Processing Systems, 2023. 3 [25] Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nvlm: arXiv preprint Open frontier-class multimodal arXiv:2409.11402, 2024. llms. [26] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: universe of 10m+ 3d objects. Advances in Neural Information Processing Systems, 36, 2024. 4, 9, 18 [27] George Doddington. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. In Proceedings of the second international conference on Human Language Technology Research, pages 138145, 2002. 18 [28] Hongyuan Dong, Jiawen Li, Bohong Wu, Jiacong Wang, Yuan Zhang, and Haoyuan Guo. Benchmarking and improving detail image caption. arXiv preprint arXiv:2405.19092, 2024. 4, 5, 6, [29] Yifan Du, Kun Zhou, Yuqi Huo, Yifan Li, Wayne Xin Zhao, Haoyu Lu, Zijia Zhao, Bingning Wang, Weipeng Chen, and Ji-Rong Wen. Towards event-oriented long video understanding. arXiv preprint arXiv:2406.14129, 2024. 3 [30] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony S. Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozi`ere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab A. AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriele Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guanglong Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Laurens Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Ju-Qing Jia, Kalyan Vasuden Alwala, K. Upasani, Kate Plawiak, Keqian Li, Ken-591 neth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala11 Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Babu Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melissa Hall Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri S. Chatterji, Olivier Duchenne, Onur cCelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Chandra Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whit ney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yiqian Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zhengxu Yan, Zhengxing Chen, Zoe Papakipos, Aaditya K. Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adi Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Ben Leonhardi, Po-Yao (Bernie) Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, ChingHsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, ShangWen Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzman, Frank J. Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory G. Sizov, Guangyi Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Han Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kaixing(Kai) Wu, KamHou, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sung-Bae Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Andrei Poenaru, Vlad T. Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xia Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, 12 Youngjin Nam, Yu Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. The llama 3 herd of models. ArXiv, abs/2407.21783, 2024. [31] Xinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao, Yining Li, Dahua Lin, and Kai Chen. Mmbench-video: long-form multi-shot benchmark for holistic video understanding. arXiv preprint arXiv:2406.14515, 2024. 2, 3 [32] Jiajun Fei, Dian Li, Zhidong Deng, Zekun Wang, Gang Liu, and Hui Wang. Video-ccam: Enhancing video-language understanding with causal cross-attention masks for short and long videos. arXiv preprint arXiv:2408.14023, 2024. 18 [33] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 2, 3 [34] Chaoyou Fu, Haojia Lin, Zuwei Long, Yunhang Shen, Meng Zhao, Yifan Zhang, Shaoqi Dong, Xiong Wang, Di Yin, Long Ma, et al. Vita: Towards open-source interactive omni multimodal llm. arXiv preprint arXiv:2408.05211, 2024. 18 [35] Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao, Peng Xu, Han Xiao, Kaipeng Zhang, Chris Liu, Song Wen, Ziyu Guo, et al. Imagebind-llm: Multi-modality instruction tuning. arXiv preprint arXiv:2309.03905, 2023. 3 [36] Mingfei Han, Linjie Yang, Xiaojun Chang, and Heng Wang. Shot2story20k: new benchmark for comprehensive understanding of multi-shot videos. arXiv preprint arXiv:2312.10300, 2023. 18 [37] Yuze He, Yushi Bai, Matthieu Lin, Wang Zhao, Yubin Hu, Jenny Sheng, Ran Yi, Juanzi Li, and Yong-Jin Liu. 3 bench: Benchmarking current progress in text-to-3d generation. arXiv preprint arXiv:2310.02977, 2023. [38] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Clipscore: reference-free arXiv preprint Bras, and Yejin Choi. evaluation metric for image captioning. arXiv:2104.08718, 2021. 18 [39] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan. 3d-llm: Injecting the 3d world into large language models. arXiv, 2023. 2 [40] Baoxiong Jia, Yixin Chen, Huangyue Yu, Yan Wang, Xuesong Niu, Tengyu Liu, Qing Li, and Siyuan Huang. Scaling 3d vision-language learning for Sceneverse: In European Conference grounded scene understanding. on Computer Vision (ECCV), 2024. 2 [41] Wooyoung Kang, Jonghwan Mun, Sungjun Lee, and Byungseok Roh. Noise-aware learning from web-crawled image-text data for image captioning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 29422952, 2023. 18 [42] Ilker Kesen, Andrea Pedrotti, Mustafa Dogan, Michele Cafagna, Emre Can Acikgoz, Letitia Parcalabescu, Iacer Calixto, Anette Frank, Albert Gatt, Aykut Erdem, et al. Vilma: zero-shot benchmark for linguistic and temporal grounding in video-language models. arXiv preprint arXiv:2311.07022, 2023. 3 [43] Jihyung Kil, Zheda Mai, Justin Lee, Zihe Wang, Kerrie Cheng, Lemeng Wang, Ye Liu, Arpita Chowdhury, and Wei-Lun Chao. Compbench: comparative reaarXiv preprint soning benchmark for multimodal llms. arXiv:2407.16837, 2024. 3 [44] Jin-Hwa Kim, Yunji Kim, Jiyoung Lee, Kang Min Yoo, and Sang-Woo Lee. Mutual information divergence: unified metric for multimodal generative models. Advances in Neural Information Processing Systems, 35:3507235086, 2022. 18 [45] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In Proceedings of the IEEE international conference on computer vision, pages 706715, 2017. 4, 18 [46] Yebin Lee, Imseong Park, and Myungjoo Kang. Fleur: An explainable reference-free evaluation metric for image captioning using large multimodal model. arXiv preprint arXiv:2406.06004, 2024. 18 [47] Jie Lei, Licheng Yu, Tamara Berg, and Mohit Bansal. Tvr: large-scale dataset for video-subtitle moment retrieval. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XXI 16, pages 447463. Springer, 2020. 18 [48] Bo Li, Peiyuan Zhang, Jingkang Yang, Yuanhan Zhang, Fanyi Pu, and Ziwei Liu. Otterhd: high-resolution multimodality model. arXiv preprint arXiv:2311.04219, 2023. [49] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: BenchIn Proceedmarking multimodal large language models. ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1329913308, 2024. 3 [50] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: BenchIn Proceedmarking multimodal large language models. ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1329913308, 2024. 3 [51] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 2, 5, 7, 8 [52] Bing Li, Cheng Zheng, Wenxuan Zhu, Jinjie Mai, Biao Zhang, Peter Wonka, and Bernard Ghanem. Vivid-zoo: Multi-view video generation with diffusion model, 2024. 1 [53] Dongxu Li, Yudong Liu, Haoning Wu, Yue Wang, Zhiqi Shen, Bowen Qu, Xinyao Niu, Guoyin Wang, Bei Chen, and Junnan Li. Aria: An open multimodal native mixtureof-experts model. arXiv preprint arXiv:2410.05993, 2024. 18 [54] Jian Li and Weiheng Lu. of multimodal large language models. arXiv:2408.08632, 2024. 3 survey on benchmarks arXiv preprint [55] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with 13 frozen image encoders and large language models. In International conference on machine learning, pages 19730 19742. PMLR, 2023. 1, 2 [56] Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. 2, [57] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2219522206, 2024. 2, 3, 4, 5, 7, 8, 21 [58] Yang Li, Hikari Takehara, Takafumi Taketomi, Bo Zheng, and Matthias Nießner. 4dcomplete: Non-rigid motion estimation beyond the observable surface. 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 1268612696, 2021. 2 [59] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Evaluating object hallucinaarXiv preprint Zhao, and Ji-Rong Wen. tion in large vision-language models. arXiv:2305.10355, 2023. 3 [60] Hao Liang, Zirong Chen, and Wentao Zhang. Evqascore: Efficient video question answering data evaluation. arXiv preprint arXiv:2411.06908, 2024. 18 [61] Hanwen Liang, Yuyang Yin, Dejia Xu, Hanxue Liang, Zhangyang Wang, Konstantinos Plataniotis, Yao Zhao, and Yunchao Wei. Diffusion4d: Fast spatial-temporal consistent 4d generation via video diffusion models. arXiv preprint arXiv:2405.16645, 2024. [62] Chin-Yew Lin. Rouge: package for automatic evaluation of summaries. In Text summarization branches out, pages 7481, 2004. 5, 18 [63] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for viIn Proceedings of the IEEE/CVF sual language models. Conference on Computer Vision and Pattern Recognition, pages 2668926699, 2024. 18 [64] Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, et al. Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models. arXiv preprint arXiv:2311.07575, 2023. 18 [65] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. 2 [66] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 1, 2 [67] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 3 [68] Jiajun Liu, Yibing Wang, Hanghang Ma, Xiaoping Wu, Xiaoqi Ma, Xiaoming Wei, Jianbin Jiao, Enhua Wu, Kangaroo: powerful video-language and Jie Hu. model supporting long-context video input. arXiv preprint arXiv:2408.15542, 2024. [69] Ruyang Liu, Chen Li, Haoran Tang, Yixiao Ge, Ying Shan, and Ge Li. St-llm: Large language models are effective temporal learners. In European Conference on Computer Vision, pages 118. Springer, 2025. 18 [70] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multiarXiv preprint modal model an all-around player? arXiv:2307.06281, 2023. 3 [71] Yuliang Liu, Zhang Li, Biao Yang, Chunyuan Li, Xucheng Yin, Cheng-lin Liu, Lianwen Jin, and Xiang Bai. On the hidden mystery of ocr in large multimodal models. arXiv preprint arXiv:2305.07895, 2023. 3 [72] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. Tempcompass: Do video llms really understand videos? arXiv preprint arXiv:2403.00476, 2024. 3 [73] Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Oryx mllm: On-demand spatialarXiv temporal understanding at arbitrary resolution. preprint arXiv:2409.12961, 2024. 18 [74] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024), 2024. [75] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very longform video language understanding. Advances in Neural Information Processing Systems, 36, 2024. 3, 4 [76] Arsha Nagrani, Mingda Zhang, Ramin Mehran, Rachel Hornung Nitesh, Bharadwaj Gundavarapu, Nilpa Jha, Austin Myers, Xingyi Zhou, Boqing Gong, Cordelia Schmid, Mikhail Sirotenko, Yukun Zhu, Tobias Weyand, and GoogleResearch. Neptune: The long orbit to benchmarking long video understanding. ArXiv, abs/2412.09582, 2024. 5, 6 [77] Nguyen Nguyen, Jing Bi, Ali Vosoughi, Yapeng Tian, Pooyan Fazli, and Chenliang Xu. Oscar: Object state caparXiv preprint tioning and state change representation. arXiv:2402.17128, 2024. 3 [78] Munan Ning, Bin Zhu, Yujia Xie, Bin Lin, Jiaxi Cui, Lu Yuan, Dongdong Chen, and Li Yuan. Video-bench: for evaluatA comprehensive benchmark and toolkit arXiv preprint ing video-based large language models. arXiv:2311.16103, 2023. 3 [79] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simon Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mely, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen OKeefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Ceron Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report, 2024. 2 [80] Arjun Panickssery, Samuel R. Bowman, and Shi Feng. Llm evaluators recognize and favor their own generations. ArXiv, abs/2404.13076, 2024. 7 [81] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311 318, 2002. 5, 18 [82] Letitia Parcalabescu, Michele Cafagna, Lilitta Muradjan, Anette Frank, Iacer Calixto, and Albert Gatt. Valse: task-independent benchmark for vision and language modarXiv preprint els centered on linguistic phenomena. arXiv:2112.07566, 2021. 3 [83] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023. [84] Zhangyang Qi, Ye Fang, Zeyi Sun, Xiaoyang Wu, Tong Wu, Jiaqi Wang, Dahua Lin, and Hengshuang Zhao. Gpt4point: unified framework for point-language understanding and generation. In CVPR, 2024. 2 [85] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language superIn International conference on machine learning, vision. pages 87488763. PMLR, 2021. 2, 4 [86] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 2, 5, 7, 8, 21 [87] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084, 2019. 5 [88] Jiawei Ren, Liang Pan, Jiaxiang Tang, Chi Zhang, Ang Cao, Gang Zeng, and Ziwei Liu. Dreamgaussian4d: Generative 4d gaussian splatting. arXiv preprint arXiv:2312.17142, 2023. 1 [89] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: time-sensitive multimodal large language model for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1431314323, 2024. [90] Gabriel Oliveira dos Santos, Esther Luna Colombini, and Sandra Avila. Cider-r: Robust consensus-based image description evaluation. arXiv preprint arXiv:2109.13701, 2021. 18 [91] Sara Sarto, Manuele Barraco, Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara. Positive-augmented contrastive learning for image and video captioning evaluation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 69146924, 2023. 18 [92] Yaya Shi, Xu Yang, Haiyang Xu, Chunfeng Yuan, Bing Li, Weiming Hu, and Zheng-Jun Zha. Emscore: Evaluating video captioning via coarse-grained and fine-grained embedding matching. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1792917938, 2022. 18 [93] Yan Shu, Peitian Zhang, Zheng Liu, Minghao Qin, Junjie Zhou, Tiejun Huang, and Bo Zhao. Video-xl: Extra-long vision language model for hour-scale video understanding. arXiv preprint arXiv:2409.14485, 2024. 18 [94] Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. study of translation edit rate with targeted human annotation. In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers, pages 223231, 2006. 18 [95] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Xun Guo, Tian Ye, Yan Lu, Jenq-Neng Hwang, et al. Moviechat: From dense token to sparse memory for long video understanding. arXiv preprint arXiv:2307.16449, 2023. 3, 5 [96] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [97] Reka Team, Aitor Ormazabal, Che Zheng, Cyprien de Masson dAutume, Dani Yogatama, Deyu Fu, Donovan Ong, Eric Chen, Eugenie Lamprecht, Hai Pham, et al. Reka core, flash, and edge: series of powerful multimodal language models. arXiv preprint arXiv:2404.12387, 2024. 18 [98] Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross. Winoground: Probing vision and language models for In Proceedings of the visio-linguistic compositionality. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 52385248, 2022. 3 [99] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. 3, 18 [100] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95689578, 2024. 3 [101] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: open and efficient foundation language models. arxiv. arXiv preprint arXiv:2302.13971, 2023. 2 15 [102] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 2 [103] Ramakrishna Vedantam, Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 45664575, 2015. 5, [104] Guangzhi Wang, Yixiao Ge, Xiaohan Ding, Mohan Kankanhalli, and Ying Shan. What makes for good visual tokenizers for large language models? arXiv preprint arXiv:2305.12223, 2023. 3 [105] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 1, 2, 5, 7, 8, 21 [106] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al. Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079, 2023. 18 [107] Wenbin Wang, Liang Ding, Minyan Zeng, Xiabin Zhou, Li Shen, Yong Luo, and Dacheng Tao. Divide, conquer and combine: training-free framework for high-resolution image perception in multimodal large language models. arXiv preprint arXiv:2408.15556, 2024. 3 [108] Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, and William Yang Wang. Vatex: large-scale, high-quality multilingual dataset for video-and-language In Proceedings of the IEEE/CVF international research. conference on computer vision, pages 45814591, 2019. 18 [109] Xidong Wang, Dingjie Song, Shunian Chen, Chen Zhang, and Benyou Wang. Longllava: Scaling multi-modal llms to 1000 images efficiently via hybrid architecture. arXiv preprint arXiv:2409.02889, 2024. 18 [110] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2031020320, 2024. 2 [111] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 52885296, 2016. 4, 18 [112] Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu Qiao, and Ping Luo. Lvlm-ehub: comprehensive evaluation bencharXiv preprint mark for large vision-language models. arXiv:2306.09265, 2023. [113] Fuzhao Xue, Yukang Chen, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, et al. Longvila: Scaling long-context visual language models for long videos. arXiv preprint arXiv:2408.10188, 2024. 18 [114] Qwen An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxin Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yi-Chao Zhang, Yunyang Wan, Yuqi Liu, Zeyu Cui, Zhenru Zhang, Zihan Qiu, Shanghaoran Quan, and Zekun Wang. Qwen2.5 technical report. ArXiv, abs/2412.15115, 2024. 4 [115] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. 18 [116] Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl3: Towards long image-sequence understanding in multi-modal large language models. arXiv preprint arXiv:2408.04840, 2024. 18 [117] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. 3 [118] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. In International conference on machine learning. PMLR, 2024. 3 [119] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multidiscipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. 3 [120] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. 2 [121] Haiyu Zhang, Xinyuan Chen, Yaohui Wang, Xihui Liu, Yunhong Wang, and Yu Qiao. 4diffusion: Multi-view video diffusion model for 4d generation. arXiv preprint arXiv:2405.20674, 2024. [122] Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang, Linke Ouyang, et al. Internlm-xcomposer-2.5: versatile large vision language model supporting long-contextual input and output. arXiv preprint arXiv:2407.03320, 2024. 18 [123] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. 18 [124] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675, 2019. 5 16 [125] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. 2, 5, 7, 8 [126] Yi-Fan Zhang, Qingsong Wen, Chaoyou Fu, Xue Wang, Zhang Zhang, Liang Wang, and Rong Jin. Beyond llavahd: Diving into high-resolution large multimodal models. arXiv preprint arXiv:2406.08487, 2024. 18 [127] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024. [128] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional In Proceedings of the AAAI Conference on Artivideos. ficial Intelligence, 2018. 18 [129] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 1, 2 [130] Zhu Ziyu, Ma Xiaojian, Chen Yixin, Deng Zhidong, Huang Siyuan, and Li Qing. 3d-vista: Pre-trained transformer for 3d vision and text alignment. In ICCV, 2023. 2 17 4D-Bench: Benchmarking Multi-modal Large Language Models for 4D Object Understanding"
        },
        {
            "title": "Supplementary Material",
            "content": "A. More related work B. More details about 4D-Bench B.1. More details about 4D object representation . B.2. More details about CLIP-based data curation B.3. Additional statistical data analysis . . . . . B.4. More details about evaluation metrics . . . . . C. More experimental details on 4D-Bench tioning . C.1. More experimental details of 4D object cap- . . . . C.2. More experimental details of 4D object ques- . . . . tion answering . . . . . . . . . . . . . . . . . . . . . . . . . . . D. Additional evaluation results on 4D-Bench D.1. Analysis for 4D object captioning evaluation D.2. Additional qualitative results of 4D object . . . . D.3. Additional qualitative results of 4D Object . . questing answering . captioning . . . . . . . . . . . . . . . . . . . . . . . 18 18 18 18 19 19 19 19 21 21 21 21 A. More related work Benchmark datasets for image and video captioning. The development of image captioning has been driven by several influential datasets[4, 20, 28]. COCO [20] stands as one of the most widely used benchmarks and covers diverse daily scenes and objects, making it robust benchmark for evaluating captioning models. The ground-truth captions provided by early benchmark datasets typically contain limited information. Yet, recent MLLMs are capable of generating more detailed captions, making these datasets not challenging enough for evaluating modern models capabilities of producing rich, nuanced descriptions that capture fine-grained visual details and complex relationships between objects. To fill this gap, Dong et al.[28] propose DetailCaps, new benchmark featuring longer and more detailed captions annotated by human experts and powerful MLLMs like GPT-4V. On the other hand, several datasets[15, 36, 45, 47, 108, 111, 128] have been established for 2D video captioning. MSR-VTT[111] provides 20 descriptions per video clip for the open domain 2D video captioning. ActivityNet Captions[45] provide temporally localized multiple-sentence descriptions for video captioning. For domain-specific applications, YouCook2[128] presents task-oriented instructional cooking videos. Reference-free captioning metrics. We use referencebased metrics [10, 27, 41, 62, 81, 90, 94, 103] in the main 18 paper. Recently, reference-free caption metrics[38, 44, 46, 60, 91, 92] has emerged in the image and video captioning metrics field. Reference-free metrics eliminate the need for human-annotated references, reducing evaluation costs and effort. They are also ideal for open-ended scenarios, accommodating multiple valid image descriptions and overcoming the limitations of reference-based methods that rely on potentially incomplete captions. For example, CLIPScore[38] uses CLIP embeddings to compute the similarity between generated captions and their associated visual content, offering flexible way to assess captions in open-ended settings. B. More details about 4D-Bench B.1. More details about 4D object representation We chose multi-view videos as the representation for 4D objects, as we found recent advanced MLLMs [7, 23, 25, 32, 34, 53, 63, 63, 64, 68, 69, 73, 83, 93, 97, 99, 106, 109, 113, 115, 116, 122, 123, 126] are primarily designed to take texts and 2D images/videos as inputs. We render the multi-view videos for 4D objects collected from Objaverse-XL[26]. For each 4D object, we render 2D video from single view up to 125 frames and utilize pixel change detection to identify motion within the 2D video, determining the frame indices for the start and end of the motion. Based on these indices, we render videos from 23 additional views, ensuring that all 24-view videos cover the identified motion frames. The camera positions are evenly distributed around the normalized 4D object with slight jitters, the camera positions are chosen with radius from 2.2m to 2.6m and height from 0.8m to 1.2m. B.2. More details about CLIP-based data curation We propose CLIP-based classifier to automatically select high-quality 4D objects, such that low-quality ones, such as oversimplified geometry, lack of texture, and poor aesthetic quality, are removed. To build the training dataset, we manually annotate thousands of 4D objects into three categories: high quality, textureless, and low overall quality. The low overall quality category typically refers to objects with significant deformation or portions that are largely outside the camera view. After that, for each object, we choose the first frame of the video from the first view and its corresponding label to build the training dataset. We build the CLIP-based classifier by adding linear layer as the classification head to fine-tune Figure I. The frame-length distribution of multi-view videos used in the 4D object captioning task Figure III. The frame-length distribution of multi-view videos used in the 4D object question answering task the CLIP visual encoder, and then use this dataset to finetune the classifier. During inference, we feed the first frame from 8 views of the 4D object into the CLIP-based classifier. The final label of the object is determined through majority voting across the predictions made for these eight images. Objects classified as high quality are retained, ensuring the dataset is highly usable. B.3. Additional statistical data analysis 4D object captioning statistics. For the 4D object captioning task, we collected 580 4D objects, where each object is rendered into 24-view videos and has 5 human-annotated captions. Fig. shows the frame-length distribution of multi-view videos, where the videos contain 99.73 frames per 4D object on average. The human-annotated captions have an average length of 19.05 words, and their length distribution is illustrated in Fig. II. Figure II. The length distribution of ground-truth captions used in the 4D object captioning task 4D object question answering statistics. In the 4D object QA dataset, the multi-view videos contain an average of 101 frames per object, with the frame length distribution shown in Fig. III. Fig. illustrates that the length distributions of the answer options are roughly similar, avoiding bias caused by answer length. B.4. More details about evaluation metrics Fig. VII and Fig. VIII present the prompt template designed to guide GPT-4o in assessing the correspondence between generated and human-annotated captions in terms of appearance and action descriptions. The prompt templates incorporate comprehensive scoring rubric ranging from 0 to 5, where each score level is defined based on the accuracy and completeness of visual appearance/action descriptions. To ensure consistent evaluation, the template also provides carefully selected example pairs of human and machinegenerated captions, along with their corresponding scores. C. More experimental details on 4D-Bench C.1. More experimental details of 4D object captioning In the 4D object captioning experiments, all models adhere to common function = (V, t), where , t, and denote the multi-view video input, text prompt (instruction), MLLM being tested, and generated caption respectively. The quality of generated captions is evaluated by computing various metric scores through comparison with human-annotated reference captions. Fig. IV shows the prompt we use to prompt the MLLMs to generate captions. Its notable that we give them caption examples because we found that different MLLMs may generate captions in vastly different styles when not provided with examples, which could impact the results due to stylistic variations. By providing examples, we aim to minimize the influence of different writing styles, allowing us to control experimental variables better and obtain more objective evaluation results that reflect the models actual understanding capabilities rather than differences in writing style. Figure V. The truncated length distribution of correct answers and decoys used in 4D object question answering dataset Figure IV. The prompt provided to the evaluated MLLMs in the 4D object captioning task. In this prompt, we describe the video information, caption requirement, and output format. We also provide several caption examples to guide the style of captions generated by MLLMs. C.2. More experimental details of 4D object question answering In the 4D object question answering experiments, all models operate under shared function (A) = (V, t, QA), where , t, QA, , A, and represent the multi-view video input, text prompt (instruction), question and four answer options, MLLM being tested, model output, and output processor, respectively. We add output processor to extract the selected answer option as we found that some open-source models sometimes struggled to strictly follow the prompt instructions that explicitly defined the required output format. Fig. VI shows the prompt we use to prompt the MLLMs to complete the 4D Object QA task. Since some open-source MLLMs may not always strictly follow the specified output format requirements, we implemented an output processor function to standardize answer Figure VI. The prompt provided to the evaluated MLLMs in the 4D object QA task. In this prompt, we detailed the video information, questions and options, and the output format. extraction using the following code. This function is designed to extract single letter answer choice (A, B, C, or D) from model responses. It first attempts to find letter enclosed in parentheses (e.g., (A)). If no match is found, it looks for standalone letters that are bordered by spaces or punctuation marks to ensure consistent extraction regardless of the response format. def extract_answer_option(text): paren_pattern = r(([A-D])) matches = re.findall(paren_pattern, text) if matches: return matches[0] isolated_pattern = r(?:ˆ[s(.,;:])([A-D]) (?:[s).,;:]$) matches = re.findall(isolated_pattern, text) if matches: 20 return matches[0] return None D. Additional evaluation results on 4D-Bench In this section, we first provide additional analysis for the 4D object captioning in Sec. D.1. Then, Sec. D.2 and Sec. D.3 provide additional evaluation results on the 4D object captioning and 4D object QA tasks of 4D-Bench, respectively. D.1. Analysis for 4D object captioning evaluation D.2. Additional qualitative results of 4D object captioning Figs. IX, XI and XII show 4D object captioning results of MiniGPT4-Video [6], VideoChat2-Mistral [57], Qwen2VL-7B [105] and Gemini 1.5 Pro [86], given various 4D objects in our 4D-Bench. For example, Fig. IX illustrates MiniGPT4-Video, VideoChat2-Mistral, Qwen2-VL7B, and Gemini 1.5 Pro achieve low GPT-Action scores. D.3. Additional qualitative results of 4D Object questing answering Figs. XIII, XIV, XV and XVI illustrate more 4D object QA results of advanced MLLMs. Fig. XIV shows an easy sample on the subtask of Temporal Relationship, where all MLLMs choose the correct answer except for GPT-4o. Fig. XV shows more difficult example of Temporal Relationship, where Qwen2-VL 7B, GPT-4o and LLava-Video picks the wrong answer. Fig. XVI shows qualitative results of MLLMs on the Object Counting subtask, where only LLava-Video 7B answered the question correctly. Fig. XIII illustrates all MLLMs (including GPT-4o and Gemini 1.5 pro) pick the wrong option on the subtask of Action, indicating the limited capabilities of MLLMs in action understanding of 4D objects. 21 Figure VII. Prompt used in GPT-Appearance metric 22 Figure VIII. Prompt used in GPT-Action metric 23 Figure IX. Qualitative results of different MLLMs on the 4D object captioning task of 4D-Bench Figure XI. Qualitative results of different MLLMs on the 4D object captioning task of 4D-Bench Figure X. Qualitative results of different MLLMs on the 4D object captioning task of 4D-Bench Figure XII. Qualitative results of different MLLMs on the 4D object captioning task of 4D-Bench 24 Figure XIII. Qualitative results of different MLLMs on the 4D object question answering task of 4D-Bench Figure XV. Qualitative results of different MLLMs on the 4D object question answering task of 4D-Bench Figure XIV. Qualitative results of different MLLMs on the 4D object question answering task of 4D-Bench Figure XVI. Qualitative results of different MLLMs on the 4D object question answering task of 4D-Bench"
        }
    ],
    "affiliations": [
        "King Abdullah University of Science and Technology",
        "National Tsing Hua University",
        "University of Oxford"
    ]
}