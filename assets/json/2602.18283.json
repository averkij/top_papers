{
    "paper_title": "HyTRec: A Hybrid Temporal-Aware Attention Architecture for Long Behavior Sequential Recommendation",
    "authors": [
        "Lei Xin",
        "Yuhao Zheng",
        "Ke Cheng",
        "Changjiang Jiang",
        "Zifan Zhang",
        "Fanhu Zeng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Modeling long sequences of user behaviors has emerged as a critical frontier in generative recommendation. However, existing solutions face a dilemma: linear attention mechanisms achieve efficiency at the cost of retrieval precision due to limited state capacity, while softmax attention suffers from prohibitive computational overhead. To address this challenge, we propose HyTRec, a model featuring a Hybrid Attention architecture that explicitly decouples long-term stable preferences from short-term intent spikes. By assigning massive historical sequences to a linear attention branch and reserving a specialized softmax attention branch for recent interactions, our approach restores precise retrieval capabilities within industrial-scale contexts involving ten thousand interactions. To mitigate the lag in capturing rapid interest drifts within the linear layers, we furthermore design Temporal-Aware Delta Network (TADN) to dynamically upweight fresh behavioral signals while effectively suppressing historical noise. Empirical results on industrial-scale datasets confirm the superiority that our model maintains linear inference speed and outperforms strong baselines, notably delivering over 8% improvement in Hit Rate for users with ultra-long sequences with great efficiency."
        },
        {
            "title": "Start",
            "content": "HyTRec: Hybrid Temporal-Aware Attention Architecture for Long Behavior Sequential Recommendation Lei Xin1,2, Yuhao Zheng3, Ke Cheng4, Changjiang Jiang2, Zifan Zhang2, Fanhu Zeng (cid:66) 1Shanghai Dewu Information Group 2Wuhan University 3USTC 4Beihang University i_xinlei@dewu.com, yuhaozheng@mail.ustc.edu.cn, kecheng@tencent.com, jiangcj@whu.edu.cn, zifan623@gmail.com, challengezengfh@gmail.com 6 2 0 2 0 2 ] I . [ 1 3 8 2 8 1 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Modeling long sequences of user behaviors has emerged as critical frontier in generative recommendation. However, existing solutions face dilemma: linear attention mechanisms achieve efficiency at the cost of retrieval precision due to limited state capacity, while softmax attention suffers from prohibitive computational overhead. To address this challenge, we propose HyTRec, model featuring Hybrid Attention architecture that explicitly decouples long-term stable preferences from short-term intent spikes. By assigning massive historical sequences to linear attention branch and reserving specialized softmax attention branch for recent interactions, our approach restores precise retrieval capabilities within industrialscale contexts involving ten thousand interactions. To mitigate the lag in capturing rapid interest drifts within the linear layers, we furthermore design Temporal-Aware Delta Network (TADN) to dynamically upweight fresh behavioral signals while effectively suppressing historical noise. Empirical results on industrialscale datasets confirm the superiority that our model maintains linear inference speed and outperforms strong baselines, notably delivering over 8% improvement in Hit Rate for users with ultra-long sequences with great efficiency."
        },
        {
            "title": "Introduction",
            "content": "The rapid accumulation of interaction data on online platforms has driven fundamental transition in recommender systems from traditional collaborative filtering to generative paradigms rooted in ultra-long sequences (Zhai et al., 2024; Lin et al., 2025; Deng et al., 2025; Du et al., 2025; Jiang et al., 2025). Central to this shift is the utilization of long behavior sequences, which serve as vital resource for decoding dynamic preferences and latent intentions (Kang and McAuley, 2018; Gu and Dao, 1(cid:66) Corresponding Author. 1 Figure 1: The Evolution of Attention Mechanisms. 2024). Such extensive sequences provide rich interaction signals that offer high-quality feedback for the next item prediction task (Sun et al., 2024; Jiang et al., 2026). By revealing the long-term trajectory of user interests rather than just short-term session data, modeling long behavior sequences proves essential for capturing complex decisionmaking paths. To model these sequences, methodologies have evolved significantly. Early approaches utilized session-based patterns (Hidasi et al., 2016) or selfattentive models (Kang and McAuley, 2018; Sun et al., 2019), establishing the foundation for sequential modeling. More recently, the field has witnessed the rise of generative frameworks, represented by P5 (Geng et al., 2022) and TALLRec (Bao et al., 2023), which demonstrate robust generalization capabilities. Furthermore, emerging simulators (Wang et al., 2025) have highlighted the value of modeling long sequence data. Nevertheless, existing models encounter two formidable obstacles when scaling to long behavior sequences: (i) The inherent trade-off between efficiency and expressiveness remains unresolved, as traditional softmax attention suffers from quadratic complexity while linear variants often compromise retrieval precision, resulting in semantic ambiguity and limited injectivity when capturing fine-grained dependencies (Qin et al., 2024), as shown in Figure 1. (ii) Current architectures struggle to adapt to interest drifts (Zhou et al., 2018). Linear models (Gu and Dao, 2023; Yang et al., 2024c) often fail to catch up with rapid intent changes because they compress all information into fixed state. Consequently, they cannot easily distinguish immediate, high-value signals from the vast amount of historical noise, leading to lag in capturing what the user truly wants at the moment (Shao et al., 2025). To address these limitations, we propose HyTRec, generative framework featuring Hybrid Temporal-Aware Recommendation architecture tailored for efficient long behavior sequence modeling. (i) To reconcile the trade-off between inference speed and retrieval precision, we design Hybrid Attention architecture specifically for modeling long behavior sequences. By strategically integrating small proportion of softmax attention layers into predominantly linear attention backbone, we maintain near-linear complexity comparable to purely linear models, while effectively restoring the high-fidelity retrieval capabilities that are typically compromised in linear approximations. (ii) To further mitigate the lag in capturing rapid interest drifts within the linear layers, we incorporate Temporal-Aware Delta Network (TADN). This module utilizes an exponential gating mechanism to dynamically upweight fresh behavioral signals, effectively suppressing historical noise and ensuring the model remains highly sensitive to immediate user intents. Extensive evaluations on diverse benchmarks validate the effectiveness of HyTRec, where it consistently outperforms strong baselines by an average of 5.8% in NDCG. Our contributions are as follows: Novel Hybrid Attention. We propose HyTRec, hybrid attention framework for generative recommendation that synergizes linear attention for history with softmax attention for recent interactions, achieving linear complexity while preserving the semantic integrity of long-term preferences. Dynamic Intent Modeling. We introduce the Temporal-Aware Delta Networks (TADN), which leverage temporal decay factor to meticulously track rapid interest shifts, ensuring transient user intents are accurately prioritized over historical noise. Empirical Performance. Extensive experiments on real-world e-commerce datasets demonstrate that HyTRec outperforms strong baselines, delivering over 8% improvement in Hit Rate for users with extensive interaction histories while maintaining linear inference speed."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Sequential and Generative Recommendation Sequential recommendation aims to predict subsequent user actions by characterizing dynamic dependencies within historical interactions. Early methodologies primarily utilized Markov Chains for short-term transitions or Recurrent Neural Networks to capture broader sequential regularities. With the rise of Transformer architectures, models such as SASRec (Kang and McAuley, 2018) leveraged self-attention for global dependency modeling. In industrial practice, managing ultra-long behavior sequences led to the development of SIM (Pi et al., 2020), which employs two-stage searchbased strategy, and ETA (Chen et al., 2021), which utilizes Locality Sensitive Hashing for end-to-end processing. Recently, the paradigm has shifted toward generative recommendation inspired by Large Language Models (Zhai et al., 2024; Lin et al., 2025). In this context, P5 (Geng et al., 2022) unified multiple recommendation tasks, while TablePilot (Liu et al., 2025) demonstrated the feasibility of aligning generative models with human-preferred data analysis patterns. To address evaluation constraints, SimUSER (Wang et al., 2025) explored using LLMs to simulate user behavior. For coldstart scenarios, reinforcement learning-driven adversarial query generation has been employed to enhance relevance (Shukla et al., 2025). Furthermore, to capture diverse evolving user intentions, MIND (Li et al., 2019) and ComiRec (Cen et al., 2020) introduced multi-interest extraction mechanisms. Despite these advancements, the practical deployment of generative recommendation is severely constrained by inference latency. The substantial computational overhead required to process lifelong sequences with Large Language Models often exceeds the strict response-time limits of realtime industrial systems, necessitating more efficient architectural solutions. 2 2.2 Efficient Long-Sequence Modeling and Hybrid Architectures The evolution of long-sequence modeling provides strategic roadmap for processing lifelong behavioral data. Although the original self-attention mechanism (Vaswani et al., 2017) offers high expressiveness, its computational complexity limits practical scalability. Consequently, Longformer (Beltagy et al., 2020) introduced sparse attention patterns, and linear variants like Performers (Choromanski et al., 2021) utilized kernel-based approximations. Subsequently, State Space Models such as S4 (Gu et al., 2022) and Mamba (Gu and Dao, 2023), along with linear variants like DeltaNet (Yang et al., 2024c), achieved strict linear complexity O(n). However, pure linear models frequently suffer from semantic confusion and struggle to maintain the injectivity of their hidden state updates (Qin et al., 2024). Moreover, standard embedding techniques in ultra-long sequence scenarios can lead to information bottlenecks, necessitating the use of decoupled embeddings to preserve model capacity (Ren et al., 2025). To strike balance between efficiency and precision, the industry is exploring hybrid architectures where frameworks like Jamba (Lieber et al., 2024) interleave state-space layers with self-attention layers. HyTRec advances this philosophy by introducing the Temporal-Aware DeltaNet. Techniques such as ALiBi (Press et al., 2022) utilize static linear biases to weight local context effectively, whereas MotiR (Shao et al., 2025) emphasizes the retrieval of underlying user motivations to filter out superficial noise. Building upon these distinct approaches, our model explicitly incorporates time-decay factors via an exponential gating mechanism to dynamically prioritize recent high-intent behaviors. Unlike standard Gated Linear Attention (Yang et al., 2024b), this design enables the model to discard irrelevant distant history while focusing on recent high-intent signals, thereby effectively resolving the semantic dilution problem common in long-sequence modeling."
        },
        {
            "title": "3 Preliminaries",
            "content": "In this work, we focus on the task of next-item prediction within long sequential recommendation scenario. The core objective is to accurately predict the ID of the next item that user will purchase based on their historical interaction sequence. 3 Let denote the set of users, and denote the set of items. For an arbitrary user U, the long interaction sequence is represented as Su = [x1, x2, . . . , xn], where xt denotes the item interacted with at time step t, and represents the length of the users interaction sequence. Formally, the goal of the recommender system is to estimate the probability distribution of the next item xn+1 given the historical sequence Su, formulated as maximizing (xn+1 Su)."
        },
        {
            "title": "4 HyTRec",
            "content": "4.1 Framework Overview We propose HyTRec, generative framework designed for long sequence users behavior modeling. As illustrated in Figure 2, the framework explicitly decouples the processing of massive historical patterns from immediate intent spikes from sequence and architecture design, respectively. Sequence Decomposition Strategy. To achieve this decoupled modeling, we first decompose the long sequence Su into two disjoint subsequences: Short-term behavior sequence. The length of this sequence is fixed at K, denoted as Sshort = [xnK+1, . . . , xn]. This subseu quence focuses on the users recent behaviors to capture short-term sudden consumption intents and interest drifts. Long-term historical behavior sequence. This sequence consists of the historical interaction excluding short-term part, denoted as Slong = [x1, . . . , xnK], with length of K. This subsequence covers long-term behavioral patterns to capture stable and inherent consumption preferences. Dual-Branch Data Flow. Building upon this stratification, HyTRec processes these signals through two parallel branches before fusion: The short-term branch processes Sshort using standard multi-head self-attention (MHSA) to ensure maximum precision for recent behaviors. The long-term branch processes Slong using our proposed hybrid attention architecture. This branch serves as the computational backbone, compressing the extensive history into compact representation while preserving finegrained dependencies. Figure 2: The Framework of HyTRec. The outputs of both branches are subsequently fused to generate the final prediction. 4.2 Hybrid Attention Architecture Building upon above decomposition, critical innovation of HyTRec lies in the long-term branch, where we introduce hybrid attention mechanism to break the O(n2) complexity bottleneck while maintaining global context awareness. The core design philosophy of hybrid attention is to decouple the modeling of extensive history from recent interactions to balance efficiency and retrieval precision. Unlike traditional methods that rely solely on either efficient linear attention (suffering from recall loss) or heavy softmax attention (suffering from computational overhead), we design hybrid layer stack. Specifically, the long-term branch is composed of encoder layers, predominantly utilizing our novel Temporal-Aware Delta Network (TADN) as the base unit for linear complexity, while strategically interleaving small proportion of standard attention layers (e.g., at ratio of 7:1). This strategy significantly improves computational efficiency (compared to pure standard attention layer) while preserving model expressiveness and long-range modeling capabilities."
        },
        {
            "title": "4.3 TADN: Temporal-Aware Delta Networks",
            "content": "To precisely capture the temporal dynamics of user purchasing behaviors while maintaining long-term preferences, TADN introduces Temporal-Aware Gating Mechanism. By quantifying the correlation between historical behaviors and the target purchase via temporal decay, the model achieves an effective balance between short-term deviations and long-term inherent preferences. where tt behavior denotes the timestamp of the historical behavior, tcurrent is the timestamp of the next purchase action, and represents the decay period. Here, τt (0, 1] directly characterizes the correlation strength. Temporal-Aware Gating Generation. Fusing the temporal decay factor with feature similarity, we generate dynamic gating weights gt to highlight the impact of recent behaviors: gt = α [σ(Wg Concat(ht, ht) +b) τt] + (1 α) gstatic, (2) where ht = ht represents short-term preference features, gstatic = softmax(h d) serves as the static gate for long-term preferences, and α is the balancing coefficient. h/ Information Fusion Mechanism. Based on the generated gate, we dynamically amplify the contribution of behaviors highly correlated with the recent purchase period while preserving long-term preferences: ht = gt ht + (1 gt) ht. (3) The output is the fused feature matrix = [h1, . . . , hL]. In this mechanism, behaviors closer to the purchase time obtain higher gating weights due to larger τt. Consequently, the shortterm deviation feature ht dominates the output, enabling rapid response to immediate purchase intents (e.g., in flash sales scenarios). Conversely, for routine behaviors, long-term preferences are effectively preserved via the gstatic component. Temporal Decay Factor. We define the temporal decay factor τt to measure the relevance of past interaction to the current decision: (cid:18) τt = exp tcurrent tt behavior (cid:19) , (1) Temporal-aware Gated Delta Rule Finally, we integrate the fused features into the Gated DeltaNet framework. By substituting the temporal-aware gate gt into the state update rule, we derive the TADN linear attention formulation. Let St Rdd 4 be the hidden state matrix. The recurrence relation is defined as: St = St1 (cid:16) gtβtktk (cid:17) + βtvtk , (4) where βt (0, 1) is the writing strength from the standard delta rule, and kt, vt are the key and value vectors derived from the input features. By expanding this recurrence, we derive the formulation of linear attention for TADN. The state St can be expressed as summation of historical updates weighted by cumulative decay path: (cid:88) (cid:89) St = (I gjβjkjk ) βivik . (5) i=1 j=i+1 Consequently, the final output ot = Stqt (which corresponds to the fused feature ht in our context) can be formulated as linear attention operation with temporal-aware decay mask: ot = (cid:88) i= (cid:16) βi vik (cid:17) qt D(t, i), (6) D(t, i) = (cid:89) (cid:16) gjβjkjk (cid:17) , (7) j=i+1 where D(t, i) represents the composite decay mask from time to t. Unlike standard Gated DeltaNet where decay is purely semantic, in TADN, the term gj explicitly contains the temporal factor τj (from Eq. 2). This ensures that the attention mechanism mathematically prioritizes recent interactions (where τ 1) while preserving long-term preferences through the static component of gj. Table 1: Cross-domain transfer experiments based on the 2022 Huawei Advertising Challenge Dataset. The table presents the performance on Recall@10, GAUC and AUC metrics. Model Recall@10 GAUC AUC SASRec Ours 0.0235 0.0317 0.4224 0.8758 0.4533 0."
        },
        {
            "title": "5 Experiments",
            "content": "We structure our experimental analysis to investigate four core research questions: RQ1: How does HyTRec perform against state-ofthe-art baselines in modeling long user behaviors? 5 RQ2: Can HyTRec effectively scale to ultra-long sequences while maintaining competitive training and inference efficiency? RQ3: How does each key component of HyTRec contribute to the overall performance? RQ4: How does the hybrid attention ratio affect retrieval precision and system latency? RQ5: How robust is HyTRec in handling specific challenging scenarios, such as the cold-start phase for new users? 5.1 Experimental Setup Datasets. We evaluate on four widely-used recommendation benchmarks: Amazon Beauty, Amazon Movies & TV, and Amazon Electronics. Recommendation systems are selected as stress test for High-Rank Sparsity because user interests are inherently multimodal and diverse, which presents sharp contrast to low-rank attention mechanisms commonly found in natural language processing. To mitigate the impact of new-user cold starts and inactive-user silence on long behavior sequence modeling, we filtered the data based on interaction frequency and product recurrence counts. Setup and Evaluation. We employ users historical behavior sequences to predict the next item they will purchase. Importantly, we compare different methods under roughly matched computational budgets: the per-sample sequence FLOPs and perstep runtime are aligned across methods. For the quadratic-cost encoders (Transformer and HSTU), we reduce their depth and width so that their overall computation is comparable to Stacked Target-toHistory Cross Attention (STCA), ensuring fair comparison. We compare single-layer target attention, DIN, Transformer, and HSTU against our HyTRec, and adopt Request Level Batching (RLB) with sparse training and dense inference. Baselines. We compare our proposed HyTRec method with behavior sequence models and longtext models as baselines. For the former, we compare GRU4Rec, SASRec, DIN, and HSTU. For the latter, we compare Transformer, GLA, and Qwennext (2 blocks). We conduct extensive experiments verify the superiority of our method. Metrics. We adopt H@500, NDCG@500, and AUC as metrics to assess the models performance on product recommendation, while latency is used to evaluate its response speed. All experiments are carried out on V100 GPUs. Table 2: We compare with various methods on three public Amazon datasets in terms of H@500, NDCG@500 and AUC. The best and second-best methods are marked in bold and underlined, respectively. Model Beauty Electronics Movies&TV H@500 NDCG@500 AUC H@500 NDCG@500 AUC H@500 NDCG@500 AUC 0.5263 GRU4Rec 0.5776 SASRec 0.5838 HSTU 0.5748 DIN GLA 0.5732 Qwen-Next (2 block) 0.5807 0.6643 HyTRec 0.1648 0.3280 0.4480 0.4297 0.4157 0.4291 0.3480 0.8397 0.2854 0.8497 0.3003 0.8602 0.3156 0.8556 0.2700 0.8548 0.3176 0.8598 0.3686 0.8655 0.3272 0.0752 0.0945 0.1244 0.1063 0.1042 0.1065 0. 0.8353 0.5380 0.8625 0.6979 0.8550 0.7042 0.8560 0.7155 0.8657 0.7053 0.8797 0.7060 0.8760 0.7070 0.1261 0.6226 0.6278 0.6236 0.6153 0.5938 0.6268 0.9307 0.9370 0.9372 0.9263 0.9124 0.9419 0.9191 5.2 Performance Comparison (RQ1) The results on three public datasets are shown in Table 2. The common laws and core differences among the three types of models are clearly distinguishable: all three take capturing the correlation of user interests in long behavior sequences as their core goal, and the model performance is positively correlated with the context modeling ability and long-sequence adaptability. The core differences focus on the trade-off between efficiency and expressive ability. Specifically, transformerbased models (e.g., SASRec, HSTU) rely on full context modeling with quadratic complexity to become the performance upper bound, but they are difficult to adapt to ultra-long sequences; linear attention-based models (e.g., GRU4Rec) have high efficiency but limited expressive ability, resulting in significant performance gaps; hybrid attentionbased models (including our HyTRec and Qwennext) attempt to balance the two, showing better overall indicator performance, but there are differences in adaptability and comprehensive performance among different models. As result, as representative model of hybrid attention models, our HyTRec shows significant advantages compared with its counterparts and the other two types of models: on the Beauty dataset, the H@500 (0.6643) of HyTRec is much higher than that of all three types of baselines, and its AUC (0.8655) ranks first, highlighting stronger ability to capture user interests; on the Electronics dataset, its H@500 (0.3272) is second only to Qwen-next, hybrid model of the same type, and its AUC (0.876) is better than all other baselines, adapting to long-sequence scenarios with scattered interests; on the Movies&TV dataset, its H@500 (0.707) and NDCG@500 (0.6268) are close to the optimal level of transformer-based models, with stable and efficient performance. All the results strongly show Figure 3: We compare the training throughput of models with the same parameter scale on single V100 GPU under different behavior sequence lengths. the superiority of our method."
        },
        {
            "title": "5.3 Training Efficiency (RQ2)",
            "content": "The performance curve shown in Figure 3 clearly demonstrates the core advantages of HyTRec in efficient long behavioral sequence modeling: across the entire range of sequence lengths from 100 to 12k, HyTRec maintains steady downward trend in throughput. This characteristic stems from the linear attention mechanism adopted in the longsequence branch of the model, where the linear computational complexity fundamentally circumvents the efficiency bottleneck caused by the increase in sequence length, enabling efficient processing of ultra-long behavioral sequences throughIn contrast, the out the entire user lifecycle. HSTU model achieves throughput close to that of HyTRec when the sequence length is 1k (85.2 token/sec at the length of 100), but its throughput plummets drastically once the sequence length exceeds 1k: it drops to 28.7 token/sec at the length of 5k and only remains at 8.9 token/sec at the length of 12k, accounting for merely 19% of HyTRecs throughput at the same sequence length. This is because the stacked sequence transduction units relied on by HSTU are essentially still con6 Table 3: Ablation experiments of the model on the Amazon Beauty dataset. Model TADN Short-Term Attention H@500 NDCG@500 AUC HyTRec 0.6043 0.6343 0.6493 0.6643 0.3130 0.3300 0.3380 0.3480 0.8355 0.8505 0.8575 0.8655 strained by quadratic complexity, making it unable to adapt to the modeling requirements of ultra-long behavioral sequences. As typical length threshold for long behavioral sequences, HyTRec still sustains high throughput of 65.3 token/sec at 5k, while HSTU loses nearly 60% of its processing efficiency. This fully verifies the practicability and scalability of HyTRec in industrial-level ultra-long behavioral sequence scenarios, and also proves the rationality of the design of the parallel dual-attention branch architecture in the integration of efficient long-sequence modeling and accurate short-term intent capture."
        },
        {
            "title": "5.4 Ablation Study (RQ3)",
            "content": "To verify the effectiveness of each key component in our proposed efficient long behavioral sequence model, we conduct ablation experiments on the recommendation task, with HyTRec as the baseline model. The experimental results are evaluated by three key metrics: Hit Ratio at 500 (H@500), Normalized Discounted Cumulative Gain at 500 (NDCG@500), and Area Under the Curve (AUC), where higher values of all metrics indicate better model performance. As shown in Table 3, the baseline model HyTRec, which lacks both the TADN branch (a linear attention with mixed attention structure) and the short-term attention branch (for learning short-term interest drift), achieves the lowest performance with H@500 of 0.6043, NDCG@500 of 0.3130, and AUC of 0.8355. When only the short-term attention branch is introduced (while TADN is removed), the model performance is significantly improved, with H@500, NDCG@500, and AUC increasing to 0.6343, 0.3300, and 0.8505 respectively, demonstrating that capturing short-term interest drift can effectively enhance the models ability to perceive immediate user preferences. When only the TADN branch is retained (without short-term attention), the model achieves further performance improvement (H@500=0.6493, NDCG@500=0.3380, AUC=0.8575), indicating that the TADN branch with linear attention can efficiently model longterm behavioral sequence dependencies and outperforms the single short-term branch in overall recommendation accuracy. Notably, the full model integrating both TADN and short-term attention branches achieves the best performance across all metrics, with H@500 reaching 0.6643, NDCG@500 0.3480, and AUC 0.8655, which fully proves that the two branches complement each other, i.e., short-term attention captures immediate interest changes while TADN models long-term sequence patterns, jointly promoting the models performance on long behavioral sequence recommendation tasks."
        },
        {
            "title": "5.5 Efficiency Evaluation (RQ4)",
            "content": "To evaluate the optimal ratio of the mixed attention model, we conducted comparative experiments with ratios ranging from 2:1 to 6:1. We calculated H@500, NDCG@500, AUC, and latency under different ratios, using the 2:1 ratio as the performance baseline. Efficiency is defined as the ratio of the performance change to the latency change, as shown in Figure 4. As illustrated by the experimental results, the ratio of 3:1 yields relatively high efficiency gains across all three metrics, showing favorable overall performance. When the ratio is adjusted to 4:1, the efficiency gain on the first metric drops to zero, while the values on the other two metrics remain positive but decrease. For the ratio of 5:1, the efficiency value on the third metric becomes zero, with the other two metrics maintaining small positive improvements. In contrast, the ratio of 6:1 achieves positive efficiency values on all three metrics, but the overall gains are lower than those under the 3:1 setting. These observations demonstrate that an appropriate trade-off between linear attention and short-term attention is crucial for achieving optimal efficiency, and the 3:1 ratio provides the best balance between recommendation performance and inference efficiency. 7 Figure 4: Performance Comparison Under Different Hybrid Attention Ratios. Table 4: Model performance on several typical business bad cases. Badcase Type H@500 NDCG@500 AUC Cold-start for new users 0.6622 0.6340 Silent old users 0.3473 0.2808 0.8616 0."
        },
        {
            "title": "5.6 Case Study (RQ5)",
            "content": "For users with sparse historical interactions, we categorize them into two types of challenging scenarios according to the distribution of their interaction information: new-user cold start and silent old users. We augment the historical behavior sequences of these two types of users by leveraging information from users with similar interests and relatively rich behavioral histories. Experimental results in Table 4 demonstrate that HyTRec achieves superior performance in handling these two challenging scenarios, which can mainly be attributed to the similar decision-making patterns of similar user groups and the strong generalization ability of the proposed model."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper investigates how to endow behavior sequence models with the ability to efficiently process extremely long sequences. Our method first proposes an interactive item prediction model that integrates historical interests and short-term intentions, aiming to address the problem of user shortterm interest drift that may exist in recommender systems. Specifically, we design hybrid attention architecture for efficiently modeling long-term interests. Furthermore, we introduce temporal decay mechanism and short-term attention branch into the hybrid attention, thereby taking both users short-term intentions into account. Extensive experimental results strongly demonstrate the effectiveness of the proposed method with better efficiency."
        },
        {
            "title": "Future Analysis",
            "content": "To clarify the advantages of our proposed method (HyTRec) over existing models in cross-domain transfer tasks, Table 1 shows that our method outperforms the existing model SASRec significantly on all three key metrics, fully demonstrating our methods remarkable ability to adapt to cross-domain data distribution differences and capture effective feature informationan advantage attributed to its rational structure that better overcomes domain shift impacts; based on these advantages and the existing research foundation, future work will focus on further enhancing the models cross-domain and cross-scenario generalization ability by optimizing the long-term encoder and temporal-aware delta network to reduce dependence on long-range context information and designing more flexible adaptive mechanism, improving robustness against noisy sequential data through introducing noise detection/denoising mechanisms and noise-aware sequential learning module, deepening research on the performanceefficiency trade-off in generative recommendation to balance high performance and efficiency via lightweight structures and efficient training strategies, and expanding the models application scope to more diverse cross-domain datasets and industrial scenarios to provide reliable technical support for practical cross-domain recommendation tasks."
        },
        {
            "title": "Impact Statement",
            "content": "We focus on designing efficient algorithms for generative recommendation. The datasets included 8 in this paper are from publicly available materials with no sensitive information presented. Moreover, our efforts are solely devoted to research purposes with no intention for commercial use."
        },
        {
            "title": "References",
            "content": "Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023. Tallrec: An effective and efficient tuning framework to align large language model with recommendation. In ACM International Conference on Recommender Systems (RecSys). Iz Beltagy, Matthew Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150. Yukuo Cen and 1 others. 2020. Controllable multiinterest framework for recommendation. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 29462956. ComiRec. Jiarui Chen and 1 others. 2021. End-to-end user behavior retrieval in click-through rate prediction model. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 12071216. ETA model using LSH. Krzysztof Choromanski and 1 others. 2021. Rethinking attention with performers. In International Conference on Learning Representations (ICLR). Alexandre De Brebisson and Pascal Vincent. 2015. An exploration of softmax alternatives belongarXiv preprint ing to the spherical loss family. arXiv:1511.05042. Jin Deng, Wang, Cai, Ren, Hu, Ding, Luo, and Zhou. 2025. Onerec: Unifying retrieve and rank with generative recommender and iterative preference alignment. arXiv preprint arXiv:2502.18965. Du, Sun, Lan, Hu, and Cheng. 2025. Mom: Linear sequence modeling with mixture-of-memories. arXiv preprint arXiv:2502.13685. Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2022. Recommendation as language processing (rlp): unified pretrain, personalized prompt & predict paradigm (p5). In ACM Recommender Systems (RecSys), pages 299315. Albert Gu and Tri Dao. 2023. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752. Albert Gu and Tri Dao. 2024. Mamba: Linear-time sequence modeling with selective state spaces. In First Conference on Language Modeling. Albert Gu, Karan Goel, and Christopher Re. 2022. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations (ICLR). S4 Model. Balázs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2016. Session-based recommendations with recurrent neural networks. In International Conference on Learning Representations (ICLR). Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. In In2022. Transformer quality in linear time. ternational conference on machine learning, pages 90999117. PMLR. Changjiang Jiang, Xinkuan Sha, Fengchang Yu, Jingjing Liu, Jian Liu, Mingqi Fang, Chenfeng Zhang, and Wei Lu. 2026. Fake-hr1: Rethinking reasoning of vision language model for synthetic image detection. In ICASSP. Changjiang Jiang, Fengchang Yu, Haihua Chen, Wei Lu, and Jin Zeng. 2025. Tabdsr: Decompose, sanitize, and reason for complex numerical reasoning in tabular data. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 31723196. Wang-Cheng Kang and Julian McAuley. 2018. Selfattentive sequential recommendation. In IEEE International Conference on Data Mining (ICDM), pages 197206. Chao Li and 1 others. 2019. Multi-interest network with dynamic routing for recommendation at tmall. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management, pages 26152623. MIND model. Opher Lieber and 1 others. 2024. Jamba: hybrid transformer-mamba language model. arXiv preprint arXiv:2403.19887. Key paper for Hybrid Architecture trend. Jiayu Lin and 1 others. 2025. survey on llm-powered agents for recommender systems. arXiv preprint arXiv:2311.13375. Haotian Liu and 1 others. 2025. Tablepilot: Recommending human-preferred tabular data analyarXiv preprint sis with large language models. arXiv:2503.13262. Jiahuan Pei, Cheng Wang, and György Szarvas. 2022. Transformer uncertainty estimation with hierarchical stochastic attention. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 1114711155. Qi Pi and 1 others. 2020. Search-based user interest modeling with long-term sequential behavior data for click-through rate prediction. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 2494 2503. SIM model from Alibaba. Ofir Press, Noah Smith, and Mike Lewis. 2022. Train short, test long: Attention with linear biases enables input length extrapolation. In International Conference on Learning Representations (ICLR). ALiBi - Basis for decay gating. Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. 2024c. Parallelizing linear transformers with the delta rule over sequence length. In Advances in Neural Information Processing Systems (NeurIPS). DeltaNet / Linear V2. Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and Yiran Zhong. 2022. The devil in linear transformer. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 70257041. Jiaqi Zhai and 1 others. 2024. Actions speak louder than words: Trillion-parameter sequential transducers for generative recommendations. In International Conference on Machine Learning (ICML). Key baseline: HSTU. Guorui Zhou, Xiaoqiang Zhu, Chengru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for clickthrough rate prediction. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 10591068. Zhen Qin and 1 others. 2024. Bridging the divide: Reconsidering softmax and linear attention. arXiv preprint arXiv:2412.06590. Accepted at ICLR 2025. Theory on Semantic Confusion/Injectivity. Rui Ren and 1 others. 2025. Long-sequence recommendation models need decoupled embeddings. In International Conference on Learning Representations (ICLR). Zeren Shao and 1 others. 2025. Motir: Motivationaware retrieval for long-tail recommendation. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (ACL Industry Track). Rishabh Shukla and 1 others. 2025. Reinforcement learning for adversarial query generation to enhance relevance in cold-start product search. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (ACL Industry Track). Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. Bert4rec: Sequential recommendation with bidirectional encoder representations from transformer. In ACM International Conference on Information and Knowledge Management (CIKM), pages 14411450. Sun, Li, Dalal, Xu, Vikram, Zhang, Dubois, Chen, Wang, Koyejo, and 1 Learning to (learn at test time): others. 2024. Rnns with expressive hidden states. arXiv preprint arXiv:2407.04620. Ashish Vaswani and 1 others. 2017. Attention is all you need. In Advances in Neural Information Processing Systems (NIPS). Xinyi Wang and 1 others. 2025. Simuser: Simulating user behavior with large language models for recommender system evaluation. arXiv preprint arXiv:2504.12722. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, and 1 others. 2024a. Qwen2 technical report. arXiv preprint arXiv:2407.10671. Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. 2024b. Gated linear attention transformers with hardware-efficient training. In International Conference on Machine Learning (ICML). Gated Linear Attention (GLA)."
        },
        {
            "title": "Mechanisms",
            "content": "A.1 Softmax Attention Softmax attention (scaled dot-product attention) is the core operation in Transformers (Vaswani et al., 2017). It computes pairwise similarities between queries and keys, applies softmax normalization, and aggregates values: = Softmax (cid:19) (cid:18) QK V, (8) where Q, K, Rnd, is the sequence length and is the hidden dimension. Its time and memory complexity are quadratic in (e.g., O(n2d) time and storing an attention matrix), which limits scalability for long sequences. A.2 Linear Attention Linear attention aims to reduce the quadratic cost of softmax attention by avoiding explicit materialization of the attention matrix, typically via kernelization and reordering matrix multiplications (De Brebisson and Vincent, 2015). representative form can be written as = Norm (cid:16) Q(KV ) (cid:17) , (9) which yields complexity linear in sequence length (e.g., O(nd2)). For causal settings, linear attention can be implemented with recurrent state updates, but maintaining causality may introduce additional scan/cumsum-style operations that hurt parallel efficiency (Hua et al., 2022). We follow the common observation that purely linear variants may sacrifice retrieval fidelity, as also discussed in recent longcontext architectures such as TransNormer (Qin et al., 2022). A.3 Hybrid Attention Hybrid attention combines the efficiency of linear attention with the retrieval quality of softmax attention by inserting softmax-attention layers sparsely among linear layers. For model with layers, let = {1, 2, . . . , l} be the layer indices and denote the layers using softmax attention. For each layer L: With sparse S, the overall complexity remains near-linear in while improving retrieval compared to purely linear backbones, which is the motivation behind HyTRec."
        },
        {
            "title": "B Baseline Model Descriptions",
            "content": "B.1 Behavior Sequence Models These models primarily focus on modeling user behavior sequences to capture sequential preferences for recommendation tasks. GRU4Rec (Hidasi et al., 2016): canonical sequential recommendation model based on Gated Recurrent Units (GRUs). It models user behavior sequences as time-series data and uses recurrent neural networks to capture temporal dependencies. Core Architecture: GRU-based encoder for sequential behavior encoding. Key Mechanism: Utilizes GRUs gating mechanism to control the flow of historical information, capturing short-term and long-term user preferences. SASRec (Kang and McAuley, 2018): Introduces self-attention mechanism into sequential recommendation, replacing recurrent structures with longtransformer-style self-attention to model range dependencies in behavior sequences. Core Architecture: Transformer encoder (multi-head self-attention with feed-forward network). Key Mechanism: Causal self-attention mask to ensure only historical behaviors are used for prediction, avoiding information leakage. DIN (Zhou et al., 2018): Deep Interest Network (DIN) focuses on capturing dynamic user interests by designing an attention-based interest activation mechanism, adapting to different candidate items. Core Architecture: Embedding layer, interest extraction network, and attention-based interest activation. Key Mechanism: Item-aware attention weights to dynamically aggregate user historical interests based on the target item. Oi = SoftmaxAttention (Qi, Ki, Vi), LinearAttention (Qi, Ki, Vi), (10) otherwise HSTU (Pei et al., 2022): Hierarchical Sequential Transformer with Uncertainty (HSTU) enhances sequential recommendation by incorporating hierarchical structure and uncertainty modeling into transformer-based sequential models. 11 Figure 5: Performance Comparison Under Different Number of Experts Core Architecture: Hierarchical transformer encoder (behavior-level with session-level). Key Mechanism: Uncertainty estimation module to quantify the reliability of sequential patterns, improving robustness. Qwen-next (Yang et al., 2024a): Qwen-next is an optimized version of the Qwen large language model tailored for long-text understanding, using 2-block structured design to balance efficiency and performance. B.2 Long-Text Models These models are designed to handle long-text inputs, with optimized attention mechanisms or model structures to address the computational bottleneck of vanilla transformers on long sequences. Transformer (Vaswani et al., 2017): The foundational model with scaled dot-product attention, serving as the baseline for long-text modeling despite its quadratic computational complexity. Core Architecture: Multi-head self-attention with position-wise feed-forward network. Key Mechanism: Scaled dot-product attention (softmax attention) to model global dependencies, with positional encoding for sequence order. GLA (Yang et al., 2024b): Gated Linear Attention (GLA) is variant of linear attention with data-dependent gating mechanisms, reducing the computational complexity from quadratic to linear in sequence length. Core Architecture: linear attention framework with gating modules. Key Mechanism: Diagonal decay matrix (At = diag(αt)) for forgetting and inputdependent gate (βt) for injection, enabling efficient long-sequence processing. Core Architecture: Modified transformer with block-wise attention. Key Mechanism: 2-block partitioning strategy for long-text encoding, reducing I/O overhead and computational cost while maintaining contextual modeling ability. B.3 Contrast with HyTRec HyTRec occupies unique middle ground. It inherits the high accuracy of softmax attention while maintaining the efficiency of linear attention, and incorporates the hybrid attention structure with prior knowledge from the recommendation domain, as shown in Table 5."
        },
        {
            "title": "Sequence Construction",
            "content": "C.1 Extending the Time Window Size For data associated with the same user ID, merge data from different partitions in the backend data warehouse. This operation allows tracing user behaviors back to the time of their registration, significantly improving the utilization of long-sequence data. C."
        },
        {
            "title": "Intermediate Process Sequence Data\nProcessing Strategy",
            "content": "Treat the ad attribution ID as numerical identifier to trace the users states throughout the intermediate funnel (ad click, redirect, activation, wake-up, 12 Figure 6: Performance Comparison Under Different Number of Attention Heads Table 5: Comparison of different models on key evaluation metrics. HyTRec is the first work in the recommendation field to achieve balance between efficiency and accuracy based on hybrid attention, enabling effective modeling of long user behavior sequences. Model Efficiency Temporal Info Short-term Intent Rec Capability Softmax Attention Linear Attention Qwen-next Ours product click, stay, redirect, collection, add-to-cart, payment). Sort different behavior data based on their correlation strength with payment behavior to form hierarchical long-cycle sequence data. C.3 Rational Utilization of Different Channels For single e-commerce platform, only product data can be utilized. For community-ecommerce platforms, community data can be used to enhance product behavior data. Specifically, information extraction technology can be employed to identify users interests in different product categories from community articles. C.4 Missing Value Imputation In real-world e-commerce business data, shortsequence samples account for high proportion, while long-sequence samples are usually classified as active users or even high-value active users. As high-stickiness users, they are often regarded as easy samples with low modeling difficulty. An effective modeling approach is to impute the missing historical behaviors and empty time intervals without interactions in short-sequence samples based on user stratification strategies. This can greatly improve the effective information of hard samples, thereby better enhancing model performance and gains in business metrics."
        },
        {
            "title": "D Common Engineering Bad Cases",
            "content": "In some typical business bad cases, the scenarios themselves are not suitable for direct longsequence modeling. Appropriate data strategies must be adopted to enable such samples to provide positive feedback to the overall performance of long-sequence modeling. (1) Cold start for new users: Such samples have extremely sparse interactions or even zero interactions, and basically have no valid behavior sequences available for modeling. (2) Inactive long-term users: Such samples only have small number of interaction records from very long time ago, or only complete registration without subsequent behaviors. (3) Scalper accounts: Such samples are often misidentified as high-value users due to their massive interaction records, which seem to be ideal samples for long-sequence modeling. However, modeling this group of users hardly brings positive contributions to platform revenue."
        },
        {
            "title": "E More Experimental Results",
            "content": "E.1 Comparison Experiments on Different"
        },
        {
            "title": "Numbers of Attention Heads",
            "content": "In the vanilla Transformer block, the number of heads is usually set to an even number. To achieve 13 Table 6: Comparison of different nums_head on key recommendation evaluation metrics. The table shows the performance (H@500, NDCG@500, AUC) and efficiency (Latency) under different nums_head settings. nums_head H@500 NDCG@500 AUC Latency 1 2 4 8 0.6514 0.6623 0.6643 0. 0.3467 0.3459 0.3480 0.3493 0.8717 0.8634 0.8655 0.8690 1.1981 1.1308 1.1092 1.1806 Table 7: Comparison of different expert_nums on key recommendation evaluation metrics. The table presents the performance and latency under various expert number settings. Expert_nums H@500 NDCG@500 AUC Latency 4 6 0.6643 0.6624 0.6579 0.3480 0.3449 0.3434 0.8655 0.8677 0.8729 1.1092 1.6221 2.0128 efficient behavior sequence modeling, we evaluate the model performance under different numbers of attention heads: 2, 4, 6, and 8. As shown in Table 6 and Figure 6, when the number of attention heads increases from 1 to 8, the model performance first improves slightly and then declines gradually, while the inference latency exhibits trend of decreasing first and then increasing. Considering both the recommendation performance metrics and inference efficiency, setting the number of attention heads to 2 is the overall optimal parameter choice. E.2 Comparison Experiments on Different"
        },
        {
            "title": "Numbers of Experts",
            "content": "In recommendation systems based on the multiexpert structure, the number of experts is generally related to the heterogeneity of user groups. According to business prior knowledge, users can be divided into four heterogeneous groups: new users, old users, silent old users, and churned users. Therefore, we set the number of experts to 4, 6, and 8 for comparison experiments, corresponding to the number of user groups, 1.5 times, and 2 times the number of groups respectively, so as to verify the impact of the number of experts on model performance. As shown in Table 7 and Figure 5, when the number of experts increases from 4 to 8, the recommendation performance of the model continues to decline, while the inference latency rises significantly. Considering the recommendation metrics H@500, NDCG@500, AUC and inference latency, setting the number of experts to 4 achieves the overall optimal performance. E.3 Empirical Analysis of Hybrid Design Table 8 illustrates the impact of different hybrid structures (Ratio) on model performance and inference latency. It can be observed that the 2:1 structure achieves the lowest latency (1.0802), but there is still room for improvement in various evaluation metrics. When the ratio is increased to 3:1, the model achieves high scores in H@500, NDCG@500, and AUC with no significant extra latency overhead, reaching the optimal balance between accuracy and efficiency. As the ratio continues to increase to 4:1 and 5:1, the latency rises noticeably despite slight fluctuations or minor improvements in some metrics. When the ratio reaches 6:1, although NDCG@500 and AUC achieve their peak values, the latency surges to 2.5296, resulting in excessively high computational cost. In summary, 3:1 is the optimal trade-off configuration between performance and efficiency. It can stably improve the core recommendation metrics including H@500, NDCG@500, and AUC while controlling latency cost, which fully validates the effectiveness and practicality of the proposed hybrid structure design."
        },
        {
            "title": "F Discussion",
            "content": "In this work, we propose HyTRec, hybrid attention framework for generative recommendation, which integrates linear attention and softmax attention to address the core challenge of 14 Table 8: Comparison of different Ratio on key recommendation evaluation metrics. The table presents the performance and latency under various ratio settings. Ratio H@500 NDCG@500 AUC Latency 2:1 3:1 4:1 5:1 6:1 0.6559 0.6643 0.6527 0.6650 0. 0.3452 0.3480 0.3478 0.3474 0.3507 0.8649 0.8655 0.8672 0.8619 0.8675 1.0802 1.1092 1.4397 1.8070 2.5296 10k+ tokens), which limits its performance in handling users with extensive interaction histories. To break through this constraint, future research can integrate HyTRec with expanded memory architectures to explicitly expand the models memory capacity. This integration will allow the hybrid attention module to retain more valuable historical information while maintaining linear efficiency, further improving performance for users with extensive interaction histories. F.3 Extension to Multi-Scenario"
        },
        {
            "title": "Recommendation",
            "content": "Currently, HyTRec is only validated on ecommerce recommendation datasets, which restricts its generalizability to other generative recommendation scenarios. To expand the applicability of HyTRec, future work can extend the framework to other generative recommendation scenarios, such as content recommendation (e.g., articles, videos) and social recommendation. In these scenarios, the characteristics of user interactions (e.g., interaction frequency, semantic complexity) differ significantly from e-commerce, which will require adjusting the hybrid attention design and TADN to adapt to scenario-specific needs. Additionally, verifying the models performance on cross-domain recommendation tasks can further demonstrate its generalizability. long sequence modeling in recommendation systemsbalancing efficiency and semantic integrity. While HyTRec effectively mitigates the efficiencysemantic dilemma faced by traditional attentionbased recommendation models, it still has room for improvement in further enhancing performance and adaptability. Here, we first briefly recap the structural trade-offs and core advantages of the proposed hybrid attention mechanism, and then focus on outlining four key potential directions for future research to address the existing limitations of HyTRec, all in line with the analytical logic of linear sequence modeling discussions. F.1 Adaptive Boundary for Hybrid Attention One key limitation of HyTRecs current hybrid attention design lies in the empirical setting of the boundary between long historical interactions and recent interactions, which lacks adaptability to diverse user groups. To address this limitation, future work should focus on designing an adaptive boundary adjustment mechanism for the hybrid attention module. Instead of using fixed threshold to separate long historical and recent interactions, we can leverage user-specific characteristics (e.g., interaction frequency, intent stability) to dynamically allocate the proportion of linear attention and softmax attention. For example, for users with stable long-term preferences, we can increase the scope of linear attention to reduce computational cost; for users with frequent intent shifts, we can expand the scope of softmax attention to capture fine-grained intent changes. F."
        },
        {
            "title": "Integration with Expanded Memory\nArchitectures",
            "content": "Similar to the memory capacity wall challenge faced by linear recurrence models, HyTRecs linear attention module still suffers from memory overwriting in fixed-dimensional states when processing extremely long interaction sequences (e.g.,"
        }
    ],
    "affiliations": [
        "Beihang University",
        "Shanghai Dewu Information Group",
        "USTC",
        "Wuhan University"
    ]
}