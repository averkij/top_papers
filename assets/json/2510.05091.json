{
    "paper_title": "Factuality Matters: When Image Generation and Editing Meet Structured Visuals",
    "authors": [
        "Le Zhuo",
        "Songhao Han",
        "Yuandong Pu",
        "Boxiang Qiu",
        "Sayak Paul",
        "Yue Liao",
        "Yihao Liu",
        "Jie Shao",
        "Xi Chen",
        "Si Liu",
        "Hongsheng Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While modern visual generation models excel at creating aesthetically pleasing natural images, they struggle with producing or editing structured visuals like charts, diagrams, and mathematical figures, which demand composition planning, text rendering, and multimodal reasoning for factual fidelity. To address this, we present the first comprehensive, systematic investigation of this domain, encompassing data construction, model training, and an evaluation benchmark. First, we construct a large-scale dataset of 1.3 million high-quality structured image pairs derived from executable drawing programs and augmented with chain-of-thought reasoning annotations. Building on it, we train a unified model that integrates a VLM with FLUX.1 Kontext via a lightweight connector for enhanced multimodal understanding. A three-stage training curriculum enables progressive feature alignment, knowledge infusion, and reasoning-augmented generation, further boosted by an external reasoner at inference time. Finally, we introduce StructBench, a novel benchmark for generation and editing with over 1,700 challenging instances, and an accompanying evaluation metric, StructScore, which employs a multi-round Q\\&A protocol to assess fine-grained factual accuracy. Evaluations of 15 models reveal that even leading closed-source systems remain far from satisfactory. Our model attains strong editing performance, and inference-time reasoning yields consistent gains across diverse architectures. By releasing the dataset, model, and benchmark, we aim to advance unified multimodal foundations for structured visuals."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 1 1 9 0 5 0 . 0 1 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "FACTUALITY MATTERS: WHEN IMAGE GENERA-"
        },
        {
            "title": "TION AND EDITING MEET STRUCTURED VISUALS",
            "content": "Le Zhuo1,3, Songhao Han2, Yuandong Pu4,5, Boxiang Qiu2, Sayak Paul6, Yue Liao7, Yihao Liu5, Jie Shao8, Xi Chen9, Si Liu2 , Hongsheng Li1 1CUHK MMLab, 2Beihang University, 3Krea AI, 4Shanghai Jiao Tong University, 5Shanghai AI Lab, 6Hugging Face, 7National University of Singapore, 8ByteDance, 9The University of Hong Kong structvisuals.github.io"
        },
        {
            "title": "ABSTRACT",
            "content": "While modern visual generation models excel at creating aesthetically pleasing natural images, they struggle with producing or editing structured visuals like charts, diagrams, and mathematical figures, which demand composition planning, text rendering, and multimodal reasoning for factual fidelity. To address this, we present the first comprehensive, systematic investigation of this domain, encompassing data construction, model training, and an evaluation benchmark. First, we construct large-scale dataset of 1.3 million high-quality structured image pairs derived from executable drawing programs and augmented with chain-of-thought reasoning annotations. Building on it, we train unified model that integrates VLM with FLUX.1 Kontext via lightweight connector for enhanced multimodal understanding. three-stage training curriculum enables progressive feature alignment, knowledge infusion, and reasoning-augmented generation, further boosted by an external reasoner at inference time. Finally, we introduce StructBench, novel benchmark for generation and editing with over 1,700 challenging instances, and an accompanying evaluation metric, StructScore, which employs multi-round Q&A protocol to assess fine-grained factual accuracy. Evaluations of 15 models reveal that even leading closed-source systems remain far from satisfactory. Our model attains strong editing performance, and inference-time reasoning yields consistent gains across diverse architectures. By releasing the dataset, model, and benchmark, we aim to advance unified multimodal foundations for structured visuals. Figure 1: Overview of our work. Left: We showcase the diverse text-to-image (T2I) and editing examples from our dataset. In contrast to natural images, modeling structured visual demands sophisticated composition planning, strong multimodal understanding, and precise text rendering, as highlighted by the three key characteristics. Right: Our model demonstrates competitive performance against leading closed-source systems in both structured image generation and editing benchmarks. Equal Contribution. Corresponding Authors."
        },
        {
            "title": "Preprint",
            "content": ""
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advances in visual generation have enabled models to follow user instructions and produce highly aesthetic images that are often indistinguishable from professional photography (Esser et al., 2024; Zhuo et al., 2024; Labs, 2024; Qin et al., 2025; Xie et al., 2025a; Wu et al., 2025a). With the emergence of unified multimodal models, such as GPT-Image (OpenAI, 2025b), Nano Banana (Google, 2025), and Bagel (Deng et al., 2025), these systems can better encode multimodal contexts and go beyond text-to-image generation to support more sophisticated visual tasks, including free-form image editing, style transfer, and more. Despite producing increasingly appealing and intricate images, current state-of-the-art models struggle to accurately generate or edit charts, mathematical figures, and diagrams. We argue that modeling such structured visual components requires more than making images look aesthetically appealing; it demands accurate factuality, i.e., encompassing capabilities like composition planning, precise text rendering, and multimodal reasoning as illustrated in Fig. 1. For instance, effective image editing in this domain necessitates robust understanding and extraction of multimodal representation from the input image and instruction. In contrast, vision-language models (VLMs) have made notable progress in understanding structured images (Fu et al., 2025; Wang et al., 2024; Zhang et al., 2024), revealing pronounced gap between visual generation and understanding that hinders unified modeling. Consequently, structured visuals represent crucial yet underexplored setting for multimodal generation. However, most existing works remain focused on aesthetics (Wu et al., 2023; Ma et al., 2025) or instruction following (Ghosh et al., 2024; Huang et al., 2023; Ye et al., 2025b) for natural images, leaving these unique challenges and their evaluation insufficiently addressed. In this paper, we present the first systematic investigation into structured image generation and editing, including comprehensive benchmark, large-scale training corpus with chain-of-thought (CoT) annotations, and strong unified model. Observing that many structured images map naturally to executable code, we collect millions of drawing programs and render them into seed images. Our pipeline performs edits at the code level to construct paired code-editing examples, which are then rendered to produce image-editing pairs. Unlike conventional image datasets (Sun et al., 2023; Wei et al., 2024; Ye et al., 2025b), images in our dataset are strictly aligned with their source codes, and editing instructions are driven by concrete code editing actions, yielding precise and verifiable state transitions. The final dataset comprises 1.3M high-quality image pairs with both text prompts and editing instructions, all annotated and filtered by GPT-5 (OpenAI, 2025a). Moreover, each sample is augmented with GPT-5-generated CoT reasoning annotations, further providing explicit reasoning trajectories for both structured image generation and editing. Building on our dataset with rich annotations, we train unified model for image generation and editing across both natural and non-natural domains based on FLUX.1 Kontext (Batifol et al., 2025). Different from approaches such as MetaQuery (Pan et al., 2025) that rely on heavy transformer-based projectors, we employ lightweight MLP connector to align multimodal features from QwenVL (Bai et al., 2025) with FLUX.1 Kontext backbone, thereby improving the models understanding of multimodal inputs such as charts and mathematical figures. We further design three-stage training curriculum to progressively achieve feature alignment, knowledge infusion, and reasoning-augmented generation. After the final stage, we are able to decompose complex generation and editing tasks by leveraging an external reasoner to scale up inference-time compute for analysis and planning to enhance performance (Zhuo et al., 2025; Fang et al., 2025a). Finally, we introduce StructBench, which contains over 1,700 high-quality samples carefully selected from six categories defined by structural characteristics. Unlike natural images, evaluating structured visuals is more challenging and demands reliable assessment of fine-grained details. To this end, we design novel metric named StructScore that improves upon the naive VLM-as-a-Judge paradigm (Lin et al., 2024) and reduces hallucinations. Concretely, we utilize VLMs in multi-round process to generate set of fine-grained question-answer pairs that comprehensively probe all salient visual elements. We then elicit predicted answers from model-generated images and leverage VLMs to compare them against ground-truth responses, producing an overall score. We comprehensively evaluate 15 models on our text-to-image and editing benchmarks, covering both openand closed-source systems. Results reveal that, while closed-source models substantially outperform open-source ones, even state-of-the-art systems are far from satisfactory, consistent with our initial observations. Notably, our model achieves the best performance on image editing, enabled"
        },
        {
            "title": "Preprint",
            "content": "by adding explicit reasoning trajectories at inference time. We validate that applying the same reasoning procedure to other models yields consistent improvements, indicating that structured image generation and editing particularly benefit from scaling inference-time compute for reasoning, which is the key bottleneck in current unified systems. By releasing our dataset, model, and benchmark, we aim to draw broader attention in the multimodal community to structured visuals and accelerate progress toward truly unified foundation models."
        },
        {
            "title": "2.1 GENERATIVE MODELS FOR VISUAL CONTENT",
            "content": "Text-to-Image Generation. The text-to-image (T2I) field has advanced rapidly in recent years, with diffusion models driving much of the progress. Early U-Net-based diffusion models (Rombach et al., 2022; Podell et al., 2023) have been followed by diffusion transformers (Peebles & Xie, 2022; Chen et al., 2023; Esser et al., 2024; Zhuo et al., 2024; Xie et al., 2025a; Labs, 2024; Qin et al., 2025; Xie et al., 2025b), which substantially improve scalability and fidelity. More recently, aided by advances in large language models and visual tokenizers, autoregressive image generation has narrowed the gap with diffusion-based methods (Sun et al., 2024; Liu et al., 2024; Xin et al., 2025; Han et al., 2025a). State-of-the-art T2I systems now produce high-resolution, photorealistic, and aesthetically appealing images, largely through scaling data and model capacity. Image Editing. Instruction-based image editing has evolved in tandem with T2I. Some early approaches (Couairon et al., 2022; Pan et al., 2023; Yang et al., 2023) achieved training-free edits by manipulating or guiding the denoising trajectory during diffusion sampling, but these methods often lacked stability and robustness. Subsequent work (Brooks et al., 2023; Sheynin et al., 2024; Huang et al., 2024; Wei et al., 2024; Chen et al., 2024; Pu et al., 2025) has increasingly framed editing as controllable generation, constructing large-scale editing datasets to fine-tune pre-trained T2I backbones. Recently, systems such as GPT-Image (OpenAI, 2025b), Nano Banana (Google, 2025), and Bagel (Deng et al., 2025) have begun to unify visual generation, editing, and even visual understanding within single framework. These unified models inherit world knowledge from language models and, through data-driven multimodal training, exhibit strong instruction following alongside emerging reasoning capabilities. 2.2 DATASET AND BENCHMARKS FOR GENERATION AND EDITING Datasets. Most T2I datasets (Meyer et al., 2024; Sun et al., 2023; Ye et al., 2025a; Fang et al., 2025b) consist of natural images, either filtered from real photographs using aesthetic scores or synthesized with open-source models. This bias has encouraged models to overfit aesthetic metrics, often yielding images with conspicuous AI look. Image-editing datasets (Wei et al., 2024; Ye et al., 2025b) are largely inspired by InstructPix2Pix (Brooks et al., 2023), where synthetic pairs are produced via expert models and carefully designed pipelines; other efforts extract editing pairs from frames in real or synthetic videos (Chen et al., 2025b; Chang et al., 2025). Consequently, these editing datasets can be imprecise, as both expert-generated content and video-derived pairs introduce substantial noise. In contrast, our dataset provides strict codeimage alignment and enforces precise state transitions through explicit code-level editing operations, ensuring faithful and verifiable supervision for both generation and editing. Benchmarks. Existing T2I benchmarks (Wu et al., 2023; Ghosh et al., 2024; Huang et al., 2023) primarily emphasize visual quality and instruction following, with particular focus on compositionality. Similarly, editing benchmarks (Sheynin et al., 2024; Wang et al., 2023; Ma et al., 2024; Ye et al., 2025b) center on visual consistency and adherence to editing instructions, frequently relying on similarity metrics such as DINO (Oquab et al., 2023) or CLIP (Radford et al., 2021) scores. As model capabilities have improved, recent benchmarks (Wu et al., 2025c; Zhao et al., 2025a; Sun et al., 2025; Cao et al., 2025) have begun to probe more challenging tasks that require world knowledge and reasoning for generation or editing. Since these evaluations mostly target natural images, their assessment protocols tend to be coarse-grained, using learnable scoring models or naive VLM-as-a-judge (Chai et al., 2024; Han et al., 2025b) pipelines, which are ill-suited for non-natural, structured imagery."
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Data construction pipeline. We prompt GPT-5 to extract salient features, then generate paired editing instructions from the source code and rendered image. The source code is modified according to the code-editing instructions. The target image rendered from modified code is passed through rule-based filters to ensure the overall quality of the constructed dataset."
        },
        {
            "title": "3 STRUCTURED IMAGE DATASET",
            "content": "3.1 DATA CURATION PIPELINE key challenge to training and evaluating models on structured images is the absence of open-source datasets tailored for generative modeling. Although the visual understanding community offers related resources (Masry et al., 2022; Zhang et al., 2024; Wang et al., 2024), their quality is inconsistent and they rarely support constructing precise image-editing pairs. Reviewing data curation pipelines for natural-image generation and editing, prior work (Sun et al., 2023; Wei et al., 2024; Fang et al., 2025b) typically collects user or synthetic prompts as seed instructions and then employs expert models as renderers to synthesize images. Given that structured images can be programmatically specified (Fu et al., 2025), natural alternative is to collect source code as seed prompts, create code-editing operations to obtain target code, and leverage code-based graphics libraries as the renderer to automatically synthesize high-fidelity structured images and editing pairs. Code-Aligned Image Synthesis. Concretely, we collect approximately 2M programs for rendering structured images from diverse sources (Belouadi et al., 2023; Wang et al., 2025b; Zhao et al., 2025b), covering mathematics, charts, puzzles, scientific figures, graph diagrams, and tables, primarily in Python and LaTeX. We execute each program and retain only those that render valid image, yielding source codeimage pairs. The next step is to generate code-editing instructions to obtain the corresponding target codeimage pairs. However, we observe that directly editing the code often yields overly specific, low-level actions that are not easily discernible at the visual level, whereas image editing is largely guided by perceptible visual elements. As illustrated in Fig. 2, we design multi-step annotation process. Given the source codeimage pair, GPT-5 (OpenAI, 2025a) first analyzes the visual characteristics of the source image, producing salient features (caption). Guided by these features, it then generates matched pair of image-editing and code-editing instructions in parallel. This design ensures that the image-editing instruction is constrained to reference only visual elements, while the code-editing instruction specifies the precise program-level changes. Finally, GPT-5 applies the code-editing action to the source program to obtain the target code, which we execute to render the target image, yielding strictly aligned and verifiable state transition. 3.2 POST-PROCESSING PIPELINE Filtering. To ensure data quality and annotation richness, we implement comprehensive postprocessing pipeline. First, we discard invalid samples by executing each program and removing those"
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Benchmark construction and evaluation workflow. (a) Benchmark construction: We cluster the data into six categories, and for each editing and text-to-image (T2I) example, GPT-5 generates detailed image descriptions that are transformed into Q&A pairs for evaluating diverse visual aspects. (b) Evaluation protocol: Using the Q&A pairs, GPT-5 is queried on generated images for open-ended responses, which are compared with ground-truth answers to yield final score. that fail to render or produce corrupted outputs. Second, we apply heuristic, rule-based filters to eliminate (1) sourcetarget pairs with null edits, i.e., low perceptible visual difference, and (2) lowinformation images with minimal semantic content (e.g., near-uniform or trivially simple renderings). The final dataset contains 1.3M examples spanning diverse categories. Each example comprises sourcetarget image pair, caption of the source image for text-to-image generation, and an image editing instruction. Fig. 5(a) visualizes the percentage and number of each editing category. Reasoning Trajectory Synthesis. Instructions in prior image generation and editing datasets are often overly terse (e.g., add tree right, photo of soap bar), offering insufficient semantic guidance. This limitation is especially problematic for structured visuals, where one-to-many mappings between instructions and outputs hinder both learning and evaluation. distinct feature of our dataset is the inclusion of chain-of-thought (CoT) reasoning annotations. Specifically, each text-to-image example is paired with carefully constructed dense caption with detailed attribute analysis, and each image-editing example is accompanied by three-step reasoning chain (i.e., input image analysis, editing instruction interpretation, target image prediction), all generated by GPT-5. As illustrated in Fig. 5(b,c), these annotations are substantially longer, providing rich and accurate semantic and analytical signals that better support complex structured image generation and editing."
        },
        {
            "title": "4 STRUCTBENCH",
            "content": "4.1 DATA SELECTION The dataset introduced in Sec. 3 is generated via code actions and filtered for validity, yielding highquality samples that are well-suited for benchmark construction. To ensure diversity and balance, we first cluster images by salient visual features and then carefully curate five primary domains, including Math, Graph, Chart, Puzzle, and Table, through manual taxonomy and selection. We apply"
        },
        {
            "title": "Preprint",
            "content": "Figure 4: Comparison of the initial and revised atomic Q&A pairs. Initial Q&A pairs sometimes entangle multiple attributes, hindering unambiguous verification and accurate scoring. Enforcing atomicity, i.e., one attribute or relation per Q&A, substantially improves metric reliability. stratified sampling within each domain, and all samples undergo dual review by GPT-5 and human annotators to verify image quality and instruction correctness. 4.2 EVALUATION PROTOCOL Evaluating structured images differs from natural images in three fundamental respects: (1) structured images demand strict prompt following at both the global layout level and in fine-grained structural elements such as numerical values, dense text, and geometric primitives, whereas prompts for natural images are often short and permissive; (2) aesthetic criteria for structured images are less important and substantial different from those for natural images; and (3) structured-image domains exhibit pronounced inter-domain differences that call for adaptive, domain-specific evaluation criteria. Consequently, common metrics used in text-to-image or editing benchmarks, such as CLIP score (Radford et al., 2021), aesthetic score (Wu et al., 2023), and naive VLM-based scores (Lin et al., 2024), are not well-suited for this setting. To bridge this gap, we propose StructScore, metric that leverages VLM in controlled multi-turn workflow to assess fine-grained, instance-specific attributes. Q&A Construction. Our goal is to build, for each data item, set of Q&A pairs that can comprehensively assess generated or edited samples. Starting from the instruction and ground-truth image, we first prompt GPT-5 to describe all salient and relevant visual elements with their attributes. GPT-5 then decomposes this detailed description into sentences and converts each into verification Q&A pair targeting specific attributes or relations, as illustrated in Fig. 3(a). Evaluation Metric. naive evaluation would binarize these Q&As and query VLM on the modelgenerated image, yielding yes/no answers. However, this yields an unreliable ceiling as random guessing can achieve 50% accuracy. Instead, because our questions are atomic, we prompt the VLM to produce open-ended answers for each question based on the generated image, forming [question, predicted-answer, ground-truth] triplets. We then compare the predicted and ground-truth answers and average the accuracy of similarity scores across all Q&As to obtain the sample-level accuracy. Unlike text-to-image generation, image editing cannot be fairly evaluated by simply averaging all Q&A scores. This is because editing simultaneously assesses visual consistency and instruction following, and visual consistency is substantially easier: model can attain high scores via nearidentity mappings. To disentangle these dimensions, we compute Q&A accuracy on both the source and ground-truth target images. If Q&A is correct for both, it is labeled as visual-consistency related; if it is incorrect on the source but correct on the target, it is labeled as instruction-following related. This binary categorization reveals that visual-consistency Q&As are far more frequent than instruction-following Q&As, as the latter typically focus on small but critical changes. To counter this imbalance and prioritize instruction adherence, we report the final editing accuracy as weighted score, i.e. 0.1 visual-consistency accuracy and 0.9 instruction-following accuracy. Appendix A.4 analyzes alternative weightings and their alignment with human preferences, where our chosen setting achieves the best correlation with human evaluations. Atomic Q&A Refinement Improves Metric Reliability. We conduct strict human audits to regulate the number of Q&As per sample, validate their reliability, and ensure our metric alignment with human judgments. As shown in Fig. 4, we observe that certain Q&A pairs, while appearing plausible to humans, are not clearly verifiable by the VLM evaluator. We attribute this issue to non-atomic questions that bundle multiple attributes or relations, thereby introducing answer stochasticity. To"
        },
        {
            "title": "Preprint",
            "content": "Figure 5: Statistical analysis of our dataset (a-c) and benchmark (d-f). mitigate hallucination and enhance determinism, we revise the VLM prompts to split compound descriptions into multiple atomic items and enforce minimal, concise ground-truth answers. This refinement process nearly doubles the number of Q&A pairs, reflecting increased atomicity. side-by-side comparison of the original and revised atomic Q&A pairs is provided in Fig. 4. Given that our benchmark contains ground-truth images, we leverage the StructScore on these images as proxy for metric reliability. An ideal metric should achieve 100% accuracy on groundtruth images; however, the initial StructScore is approximately 80%. Therefore, we prompt GPT-5 to rewrite the failing Q&A pairs, re-aligning them with ground-truth images to make them clear and unambiguous. Following this refinement, the updated Q&A set yields over 95% accuracy on ground-truth images, demonstrating substantially improved reliability. 4.3 NUMERICAL STATISTICS Our StructBench consists of 1,714 items with 32,031 and 37,941 Q&A pairs for editing and generation respectively. Fig. 5(d) summarizes the distribution of StructBench across six domains. Given the importance of charts, we further stratify the chart test set into five categories by editing type. Fig. 5(e) reports the number of Q&A pairs per sample, where over 87% of samples contain more than ten questions, and 14% include more than twenty-six. Finally, the word clouds for editing instructions and questions in Fig. 5(e) highlight focus on the key elements and attributes of structured images."
        },
        {
            "title": "5 MODEL TRAINING",
            "content": "5.1 MODEL ARCHITECTURE We build on FLUX.1 Kontext (Batifol et al., 2025), diffusion transformer with unified image generation and editing capabilities, and adapt it to structured visual content. Following its original configuration, we encode textual instructions with T5 (Raffel et al., 2020)1, and encode both the input image and the target image with VAE. The resulting token streams are concatenated into unified sequence and processed with joint attention. While the VAE supplies low-level features of the input image, structured image editing often depends on higher-level semantics. For example, converting bar plot into pie chart requires recognizing the underlying quantitative relationships to render the correct proportions. To enhance semantic perception, we utilize lightweight MLP connector that aligns Qwen-VL (Bai et al., 2025) encoded multimodal features with FLUX.1 Kontext. This deisgn choice reduces training overhead and, empirically, stabilizes optimization without sacrificing performance compared to learnable-query, transformer-based connectors (Pan et al., 2025). 1We discard CLIP features due to its limited token budget and observed no degradation in output quality."
        },
        {
            "title": "Preprint",
            "content": "Figure 6: The three-stage progressive training pipeline. Training difficulty increases across stages, from alignment to hybrid visual learning and thinking enhancement. More training details are presented in Appendix A.1. 5.2 TRAINING PIPELINE To gradually introduce structured-visual knowledge to our model while maintaining its general capabilities, we adopt three-stage progressive training pipeline, as illustrated in Fig. 6. Stage 1: Unified Alignment. The goal of this stage is to align Qwen-VLs features with the diffusion backbone rather than injecting new knowledge. Accordingly, we freeze the backbone model and train only the newly added connector, using relatively simple text-to-image and editing data. To prevent the already well-aligned T5 encoder from becoming shortcut that impedes connector alignment (Lin et al., 2025), we remove T5 features during this stage and rely solely on Qwen-VL features. After alignment training, the model can roughly follow instructions for both generation and editing. Stage 2: Hybrid Visual Learning. We then incorporate our constructed structured text-to-image and editing datasets and jointly fine-tune the diffusion backbone and connector to inject domain knowledge of structured visuals. To preserve general-domain capabilities, we include mixture of high-quality text-to-image and editing datasets. However, structured visuals exhibit markedly different pixel statistics from natural images, which contain large regions of uniform backgrounds and, in editing, small regions of change. Therefore, we introduce mask-based training strategy that adaptively downweights losses on background and unchanged regions during training. Leveraging Qwen-VLs multimodal representations and the pretrained T5 encoder, the model improves on both structured and general domains. Stage 3: Thinking Enhancement. We observe that our Stage-2 model, as well as other baselines, often produce images that appear visually plausible but contain semantic errors on complex tasks. We attribute this to an overreliance on shallow visual patterns driven by text features, coupled with insufficient grounding in the input image or prompt (e.g., layout or quantitative relationships). Because such semantic comprehension is core strength of VLMs, we leverage the CoT annotations from Sec. 3.2 and open-source datasets (Fang et al., 2025b) as long-context inputs to Qwen-VL to inject explicit reasoning. This endows the model with the ability to follow complex multimodal reasoning instructions. It also enables scaling inference-time compute: VLM first analyzes the input imagetext pair and predicts the ideal target content, and the resulting expanded analysis is then fed to the generator to guide synthesis."
        },
        {
            "title": "6 EXPERIMENTS",
            "content": "6.1 EXPERIMENTAL SETUP Training Details We provide comprehensive training details in Appendix A.1, including model configurations, stage-specific hyperparameters, and the datasets used. Our base model is FLUX.1 Kontext [dev] (Batifol et al., 2025), where we remove the original CLIP encoder and replace it with"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Quantitative comparison on StructEditBench. The table showcases the results of Accuracy (%) and PSNR (with ground-truth target image) for various methods. indicate the first and second Accuracy results, respectively. Model Nano Banana GPT-Image Seedream 4.0 UniWorld-V1 DiMOO OmniGen2 Ovis-U1 HiDream-E1.1 Bagel Bagel-Think Step1X-Edit FLUX.1 Kontext Qwen-Edit Ours Math Chart Graph Puzzle Science Table Overall Acc PSNR Acc PSNR Acc PSNR Acc PSNR Acc PSNR Acc PSNR Acc PSNR 50.46 51.49 51.06 9.41 26.79 29.44 31.64 28.07 21.27 37.40 34.47 37.36 40.48 54.74 20.77 17.06 23. 8.84 21.56 15.95 18.45 18.43 21.38 23.97 23.41 19.78 23.73 23.31 46.42 45.82 46.83 5.99 16.52 18.55 21.94 26.36 27.11 28.98 28.05 32.29 30.17 50.58 14.93 12.56 16. 7.87 14.98 12.44 13.30 12.91 16.38 16.82 16.68 14.61 12.33 15.33 52.97 50.71 51.72 8.83 24.03 34.63 38.03 29.63 29.94 42.51 33.26 39.12 44.83 60.18 21.22 17.24 24. 6.16 21.77 11.31 19.01 18.26 22.70 26.49 24.56 20.10 26.11 24.65 66.56 76.03 71.13 9.11 29.52 28.61 42.08 43.77 41.59 36.11 60.48 58.35 53.74 73.0 22.92 16.55 26. 7.71 22.26 16.51 17.92 18.04 24.22 26.75 25.94 20.38 27.31 26.33 69.16 67.61 69.22 19.91 26.08 39.55 44.52 36.66 47.16 43.15 46.47 50.39 55.99 75.05 22.61 16.61 26. 7.67 22.57 15.60 18.68 16.47 23.56 25.57 24.98 20.99 25.53 25.80 75.71 83.26 88.19 16.13 24.64 30.36 35.58 48.79 47.35 40.46 57.81 58.05 67.76 77.08 19.75 14.35 24. 8.24 19.47 16.60 16.62 17.12 21.54 23.83 23.97 18.52 25.71 23.19 51.57 52.2 52.85 8.40 21.00 24.30 28.06 29.63 28.87 33.34 34.11 37.56 38.12 55.98 21.09 16.64 24. 8.21 21.49 15.49 18.25 18.01 22.06 24.70 24.03 19.84 24.81 24.01 Table 2: Quantitative comparison on StructT2IBench, reporting Accuracy (%). Model Seedream 4.0 Nano banana GPT-Image UniWorld-V1 Bagel Bagel-Think HiDream-I1-Full OmniGen2 FLUX.1 Dev FLUX.1 Kontext Ovis-U1 Qwen-Image Ours Chart Graph Math Puzzle Science Table Overall 35.79 35.55 37.09 1.71 4.66 4.81 9.47 10.67 12.35 17.22 24.75 32.23 20.91 54.08 58.96 57. 5.52 3.61 15.33 20.84 22.51 20.09 24.64 16.08 48.05 33.45 63.33 64.81 63.25 4.72 4.02 13.89 19.20 22.89 19.86 21.42 19.45 46.98 41.70 50.89 63.87 59. 1.58 4.46 15.22 18.00 18.63 20.63 24.06 21.23 48.90 30.66 62.59 60.75 60.94 8.82 8.60 19.05 26.77 28.00 25.25 30.97 26.03 53.51 41.46 68.94 67.20 83. 5.25 5.74 8.97 27.05 22.61 27.00 29.16 12.70 73.65 32.26 47.52 48.45 49.58 3.20 4.69 9.03 14.77 16.24 16.51 20.36 22.83 41.03 28.80 Qwen2.5-VL-7B (Bai et al., 2025) to encode both the input instructions and images. Throughout training, we adopt dynamic-resolution sampling, where images are resized to lie near 512512 while preserving their native aspect ratios, maximizing information retention. During inference, we employ GPT-5 (OpenAI, 2025a) as an external reasoner to provide reasoning trajectories from inputs. Evaluation Details We evaluate 15 closedand open-source models, covering the most recent textto-image, image-editing, and unified models. Closed-source systems include GPT-Image (OpenAI, 2025b), Nano Banana (Google, 2025), and Seedream 4.0 (Seedream et al., 2025). Open-source baselines include FLUX.1-dev (Labs, 2024), FLUX.1 Kontext (Batifol et al., 2025), Step1X-Edit (Liu et al., 2025), Bagel (Deng et al., 2025), Bagel-Think (Deng et al., 2025), HiDream-E1.1 (Cai et al., 2025), UniWorld-V1 (Lin et al., 2025), Ovis-U1 (Wang et al., 2025a), OmniGen2 (Wu et al., 2025b), Qwen-Image (Wu et al., 2025a), and DiMOO (Team, 2025). For aspect ratios, models that support custom output sizes are configured to match the ground-truth image proportions. To ensure fairness and reproducibility, all models are run with the default settings from their official repositories and generated on H200 GPUs. For the LLM-based evaluator, we report results using both the current stateof-the-art closed-source LLM (GPT-5 (OpenAI, 2025a)) in the main text and the leading open-source alternative (Qwen2.5-VL-72B (Bai et al., 2025)) in Appendix A.2. We also release the complete prompts used for data construction and evaluation in Appendix A.6. 6.2 RESULTS AND ANALYSIS We report the performance of 15 models on 6 subtasks of StructEditBench and StructT2IBench in Tab. 1 and Tab. 2, respectively. To provide finer-grained analysis, we break down chart-related editing by editing type in Tab. 3. Case study demos are provided in Appendix A.5. The main findings are:"
        },
        {
            "title": "Preprint",
            "content": "Table 3: Quantitative comparison on StructEditBench (charts only). second Accuracy results, respectively. indicate the first and Model Category Color Num Auxiliary Add&Del Overall Acc PSNR Acc PSNR Acc PSNR Acc PSNR Acc PSNR Acc PSNR GPT-Image Nano Banana Seedream 4.0 40.62 39.75 38. 5.98 UniWorld-V1 11.20 DiMOO 17.30 OmniGen2 18.15 Ovis-U1 22.69 HiDream-E1.1 25.69 Bagel 21.96 Step1X-Edit 24.55 Bagel-Think Qwen-Edit 23.53 FLUX.1 Kontext 24.67 Ours 50.81 9.85 10.33 9.67 7.60 9.82 8.61 9.57 9.29 9.08 10.40 9.00 9.52 10.14 10. 54.57 54.34 61.84 8.19 15.31 28.84 30.68 39.86 38.20 36.51 45.46 41.80 44.56 64.10 13.91 17.59 21.77 8.29 16.97 11.78 15.38 14.07 20.46 19.75 19.35 13.63 16.53 18. 33.12 35.64 36.00 2.58 17.39 15.48 20.79 21.49 26.30 25.46 25.60 22.90 29.24 13.48 16.56 19.22 7.57 17.30 14.03 15.13 15.04 20.29 20.22 20.14 13.69 16.74 33.45 17. 64.03 67.40 65.92 10.24 21.57 22.42 25.49 32.65 30.00 34.40 34.54 42.39 44.02 66.34 13.48 17.39 19.46 8.62 17.59 16.59 14.25 14.34 21.24 19.46 20.62 13.07 16.79 17. 38.02 36.77 36.04 2.81 18.57 10.54 17.21 18.05 17.79 24.92 18.69 23.27 23.06 38.10 12.73 13.88 14.28 7.35 14.46 12.11 13.11 12.71 14.93 15.11 14.58 12.46 13.95 14. 45.82 46.42 46.83 5.99 16.52 18.55 21.94 26.36 27.11 28.05 28.98 30.17 32.29 50.58 12.56 14.93 16.54 7.87 14.98 12.44 13.30 12.91 16.82 16.68 16.38 12.33 14.61 15. Closed-source models lead, yet all models remain far from satisfactory. Across both editing and T2I benchmarks, closed-source systems consistently occupy the top positions. For example, on T2I, the best closed-source model (GPT-Image) attains score of 49.58, whereas the strongest open-source baseline (Qwen-Image) achieves 41.03. Notably, our model achieves the highest score (55.98) on the StructEditBench. However, even the best model achieves only about half accuracy on either generation or editing, highlighting substantial room for improvement on structured visuals. Data, rather than architecture, is the dominant driver of performance. Among open-source models, varying visual encoders (e.g., VAE (Batifol et al., 2025), Qwen-VL (Bai et al., 2025), SigLIP (Zhai et al., 2023)) and modeling paradigms (diffusion transformer, unified autoregressive transformer, discrete diffusion) do not yield consistent winner. By training on our curated dataset, our model delivers strong performance gains over the original FLUX.1 Kontext, underscoring the Instead, models trained predominantly on natural-image importance of data scale and quality. corpora with limited task coverage (e.g., UniWorld-V1) significantly lag behind on structuredimage tasks. Besides, the performance gap is larger for T2I than for editing, likely because editing emphasizes transferable operations like add, delete, move, whereas T2I requires synthesizing finegrained attributes from scratch, substantially more challenging objective. Reasoning capability has substantial impact. For relatively simple edits such as modifying colors or adding auxiliary lines (Tab. 3), most models approach 50% accuracy. In contrast, performance drops sharply on more complex tasks like chart-type conversion, which require first understanding the input (e.g., proportions and ordering) and then generating the target according to structural specifications. Moreover, both the thinking variant Bagel-Think and our model with explicit reasoning substantially outperform non-reasoning baselines in both generation and editing, underscoring the benefits of incorporating reasoning before generation. 6.3 SCALING COMPUTE WITH EXPLICIT REASONING Motivated by the strong performance of our model, we further investigate the impact of adding explicit test-time reasoning for structured image editing. We introduce an external reasoner (GPT-5) during inference that performs three-step analysis over the instruction and input image: (1) summarize salient visual elements; (2) localize the elements to be edited and specify the intended changes; and (3) forecast the post-editing outcome. This design decouples complex visual reasoning and planning from image synthesis, allowing the generator to focus on the generation task itself. As shown in Fig. 8 and 9, providing explicit reasoning trajectories yields substantial improvements for most models. Notably, Bagel augmented with our carefully designed reasoning trajectories significantly outperforms its native thinking variant, Bagel-Think (38.44 vs. 33.34), indicating that the trajectory design and quality are critical. Moreover, the family of unified models such as GPT-Image and Bagel benefits more from added reasoning than specialized expert models like HiDream-E1.1. Overall, these results again validate that high-quality multimodal reasoning is"
        },
        {
            "title": "Preprint",
            "content": "Figure 7: Pearson correlation analysis among Accuracy, PSNR, and human Elo score for editing models. Figure 8: Study of explicit reasoning added to different models. Figure 9: Case study: explicit reasoning improves fidelity. When dealing with complex tasks, direct generation often leads to plausible yet incorrect images. Incorporating reasoner to explicitly describe, analyze, and predict enables the generator to produce faithful, correct outputs. essential for complex editing. Whether via interleaved multimodal pretraining or reasoning-centric post-training, unified models with native multimodal reasoning appear to be promising path forward. 6.4 HUMAN ALIGNMENT STUDY To evaluate how well StructScore aligns with human judgments, we conduct large-scale Elo rating study to elicit human preference rankings. Implementation details and full results are provided in Appendix A.3. We then compute the Pearson correlation coefficient (r) between the human Elo rankings and scores from various metrics across multiple benchmarks. As shown in Fig. 7, StructScore achieves strong correlation with human preferences (r > 0.9), substantially exceeding traditional metrics like PSNR. This verifies the effectiveness of breaking down complex evaluations into atomic questions and indicates that StructScore is reliable proxy for human assessment."
        },
        {
            "title": "7 CONCLUSION",
            "content": "In this work, we aim to address the critical challenge of structured image generation and editing, domain where existing models fall short due to the high demand for factual accuracy. We introduced comprehensive solution, including large-scale, code-aligned dataset with chain-of-thought annotations, strong unified model trained with progressive curriculum, and rigorous benchmark-metric suite for fine-grained, low-hallucination evaluation. Our experiments demonstrate that even leading models struggle with structured visuals, yet our approach sets new standard for open-source models and shows significant benefits from inference-time reasoning. In the future, we hope to incorporate more diverse domains of structured visuals that can be rendered via code, such as molecular formulas, musical staff, and even structured videos like educational videos."
        },
        {
            "title": "REFERENCES",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv e-prints, 2025. Jonas Belouadi, Anne Lauscher, and Steffen Eger. Automatikz: Text-guided synthesis of scientific vector graphics with tikz. arXiv preprint arXiv:2310.00367, 2023. Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1839218402, 2023. Qi Cai, Jingwen Chen, Yang Chen, Yehao Li, Fuchen Long, Yingwei Pan, Zhaofan Qiu, Yiheng Zhang, Fengbin Gao, Peihan Xu, et al. Hidream-i1: high-efficient image generative foundation model with sparse diffusion transformer. arXiv preprint arXiv:2505.22705, 2025. Shuo Cao, Nan Ma, Jiayang Li, Xiaohui Li, Lihao Shao, Kaiwen Zhu, Yu Zhou, Yuandong Pu, Jiarui Wu, Jiaquan Wang, et al. Artimuse: Fine-grained image aesthetics assessment with joint scoring and expert-level understanding. arXiv preprint arXiv:2507.14533, 2025. Wenhao Chai, Enxin Song, Yilun Du, Chenlin Meng, Vashisht Madhavan, Omer Bar-Tal, Jenq-Neng Hwang, Saining Xie, and Christopher Manning. Auroracap: Efficient, performant video detailed captioning and new benchmark. arXiv preprint arXiv:2410.03051, 2024. Di Chang, Mingdeng Cao, Yichun Shi, Bo Liu, Shengqu Cai, Shijie Zhou, Weilin Huang, Gordon Wetzstein, Mohammad Soleymani, and Peng Wang. Bytemorph: Benchmarking instruction-guided image editing with non-rigid motions. arXiv preprint arXiv:2506.03107, 2025. Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-Î±: Fast training of diffusion transformer for photorealistic text-to-image synthesis. Proceedings of the International Conference on Learning Representations (ICLR), 2023. Junying Chen, Zhenyang Cai, Pengcheng Chen, Shunian Chen, Ke Ji, Xidong Wang, Yunjin Yang, and Benyou Wang. Sharegpt-4o-image: Aligning multimodal models with gpt-4o-level image generation. arXiv preprint arXiv:2506.18095, 2025a. Xi Chen, Zhifei Zhang, He Zhang, Yuqian Zhou, Soo Ye Kim, Qing Liu, Yijun Li, Jianming Zhang, Nanxuan Zhao, Yilin Wang, et al. Unireal: Universal image generation and editing via learning real-world dynamics. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1250112511, 2025b. Xiangyu Chen, Yihao Liu, Yuandong Pu, Wenlong Zhang, Jiantao Zhou, Yu Qiao, and Chao Dong. Learning low-level vision generalist via visual task prompt. In Proceedings of the 32nd ACM International Conference on Multimedia, pp. 26712680, 2024. Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusion-based semantic image editing with mask guidance. arXiv preprint arXiv:2210.11427, 2022. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Proceedings of the International Conference on Machine Learning (ICML), 2024."
        },
        {
            "title": "Preprint",
            "content": "Rongyao Fang, Chengqi Duan, Kun Wang, Linjiang Huang, Hao Li, Shilin Yan, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, Xihui Liu, and Hongsheng Li. Got: Unleashing reasoning capability of multimodal large language model for visual generation and editing. arXiv preprint arXiv:2503.10639, 2025a. Rongyao Fang, Aldrich Yu, Chengqi Duan, Linjiang Huang, Shuai Bai, Yuxuan Cai, Kun Wang, Si Liu, Xihui Liu, and Hongsheng Li. Flux-reason-6m & prism-bench: million-scale textto-image reasoning dataset and comprehensive benchmark. arXiv preprint arXiv:2509.09680, 2025b. Xingyu Fu, Minqian Liu, Zhengyuan Yang, John Corring, Yijuan Lu, Jianwei Yang, Dan Roth, Dinei Florencio, and Cha Zhang. Refocus: Visual editing as chain of thought for structured image understanding. arXiv preprint arXiv:2501.05452, 2025. Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems (NeurIPS), 36, 2024. Google. Nano banana. image-generation/. 2025. URL https://gemini.google/overview/ Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, and Xiaobing Liu. Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis. In Advances in Neural Information Processing Systems (NeurIPS), pp. 1573315744, 2025a. Songhao Han, Wei Huang, Hairong Shi, Le Zhuo, Xiu Su, Shifeng Zhang, Xu Zhou, Xiaojuan Qi, Yue Liao, and Si Liu. Videoespresso: large-scale chain-of-thought dataset for fine-grained video reasoning via core frame selection. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2618126191, 2025b. Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: comprehensive benchmark for open-world compositional text-to-image generation. Advances in Neural Information Processing Systems (NeurIPS), 36:7872378747, 2023. Yuzhou Huang, Liangbin Xie, Xintao Wang, Ziyang Yuan, Xiaodong Cun, Yixiao Ge, Jiantao Zhou, Chao Dong, Rui Huang, Ruimao Zhang, et al. Smartedit: Exploring complex instruction-based image editing with multimodal large language models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 83628371, 2024. jackyhate. text-to-image-2m. text-to-image-2M, 2024. https://huggingface.co/datasets/jackyhate/ Maksim Kuprashevich, Grigorii Alekseenko, Irina Tolstykh, Georgii Fedorov, Bulat Suleimanov, Vladimir Dokholyan, and Aleksandr Gordeev. Nohumansrequired: Autonomous high-quality image editing triplet mining. arXiv preprint arXiv:2507.14119, 2025. Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, et al. Uniworld: High-resolution semantic encoders for unified visual understanding and generation. arXiv preprint arXiv:2506.03147, 2025. Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, and Deva Ramanan. Evaluating text-to-visual generation with image-to-text generation. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 366384. Springer, 2024. Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin, Yu Qiao, Hongsheng Li, and Peng Gao. Luminamgpt: Illuminate flexible photorealistic text-to-image generation with multimodal generative pretraining. arXiv preprint arXiv:2408.02657, 2024. Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025."
        },
        {
            "title": "Preprint",
            "content": "Yiwei Ma, Jiayi Ji, Ke Ye, Weihuang Lin, Zhibin Wang, Yonghan Zheng, Qiang Zhou, Xiaoshuai Sun, and Rongrong Ji. I2ebench: comprehensive benchmark for instruction-based image editing. Advances in Neural Information Processing Systems (NeurIPS), 37:4149441516, 2024. Yuhang Ma, Xiaoshi Wu, Keqiang Sun, and Hongsheng Li. Hpsv3: Towards wide-spectrum human preference score. arXiv preprint arXiv:2508.03789, 2025. Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. Jordan Meyer, Nick Padgett, Cullen Miller, and Laura Exline. Public domain 12m: highly aesthetic image-text dataset with novel governance mechanisms. arXiv preprint arXiv:2410.23144, 2024. OpenAI. Gpt-5. 2025a. URL https://openai.com/index/gpt-5-system-card/. OpenAI. Gpt-image-1. 2025b. introducing-4o-image-generation/. URL https://openai.com/index/ Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, Ji Hou, and Saining Xie. Transfer between modalities with metaqueries. arXiv preprint arXiv:2504.06256, 2025. Zhihong Pan, Riccardo Gherardi, Xiufeng Xie, and Stephen Huang. Effective real image editing with accelerated iterative diffusion inversion. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pp. 1591215921, 2023. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), volume 4172, 2022. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Yuandong Pu, Le Zhuo, Kaiwen Zhu, Liangbin Xie, Wenlong Zhang, Xiangyu Chen, Peng Gao, Yu Qiao, Chao Dong, and Yihao Liu. Lumina-omnilv: unified multimodal framework for general low-level vision. arXiv preprint arXiv:2504.04903, 2025. Qi Qin, Le Zhuo, Yi Xin, Ruoyi Du, Zhen Li, Bin Fu, Yiting Lu, Jiakang Yuan, Xinyue Li, Dongyang Liu, et al. Lumina-image 2.0: unified and efficient image generative framework. arXiv preprint arXiv:2503.21758, 2025. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In Proceedings of the International Conference on Machine Learning (ICML), pp. 87488763. PmLR, 2021. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1068410695, 2022. Team Seedream, Yunpeng Chen, Yu Gao, Lixue Gong, Meng Guo, Qiushan Guo, Zhiyao Guo, Xiaoxia Hou, Weilin Huang, Yixuan Huang, et al. Seedream 4.0: Toward next-generation multimodal image generation. arXiv preprint arXiv:2509.20427, 2025."
        },
        {
            "title": "Preprint",
            "content": "Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 88718879, 2024. Kaiyue Sun, Rongyao Fang, Chengqi Duan, Xian Liu, and Xihui Liu. T2i-reasonbench: Benchmarking reasoning-informed text-to-image generation. arXiv preprint arXiv:2508.17472, 2025. Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin, Yi Wang, et al. Journeydb: benchmark for generative image understanding. Advances in Neural Information Processing Systems (NeurIPS), 36:4965949678, 2023. Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. Alpha VLLM Team. Lumina-dimoo: unified masked diffusion model for multi-modal generation and understanding, 2025. URL https://github.com/Alpha-VLLM/Lumina-DiMOO. Guo-Hua Wang, Shanshan Zhao, Xinjie Zhang, Liangfu Cao, Pengxin Zhan, Lunhao Duan, Shiyin Lu, Minghao Fu, Xiaohao Chen, Jianshan Zhao, et al. Ovis-u1 technical report. arXiv preprint arXiv:2506.23044, 2025a. Ke Wang, Junting Pan, Linda Wei, Aojun Zhou, Weikang Shi, Zimu Lu, Han Xiao, Yunqiao Yang, Houxing Ren, Mingjie Zhan, et al. Mathcoder-vl: Bridging vision and code for enhanced multimodal mathematical reasoning. arXiv preprint arXiv:2505.10557, 2025b. Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi Pont-Tuset, Shai Noy, Stefano Pellegrini, Yasumasa Onoe, Sarah Laszlo, David Fleet, Radu Soricut, et al. Imagen editor and editbench: Advancing and evaluating text-guided image inpainting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1835918369, 2023. Yuhan Wang, Siwei Yang, Bingchen Zhao, Letian Zhang, Qing Liu, Yuyin Zhou, and Cihang Xie. Gptimage-edit-1.5 m: million-scale, gpt-generated image dataset. arXiv preprint arXiv:2507.21033, 2025c. Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, et al. Charxiv: Charting gaps in realistic chart understanding in multimodal llms. Advances in Neural Information Processing Systems, 37:113569113697, 2024. Cong Wei, Zheyang Xiong, Weiming Ren, Xeron Du, Ge Zhang, and Wenhu Chen. Omniedit: Building image editing generalist models through specialist supervision. In Proceedings of the International Conference on Learning Representations (ICLR), 2024. Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025a. Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025b. Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. Yongliang Wu, Zonghui Li, Xinting Hu, Xinyu Ye, Xianfang Zeng, Gang Yu, Wenbo Zhu, Bernt Schiele, Ming-Hsuan Yang, and Xu Yang. Kris-bench: Benchmarking next-level intelligent image editing models. arXiv preprint arXiv:2505.16707, 2025c. Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, et al. Sana: Efficient high-resolution image synthesis with linear diffusion transformers. Proceedings of the International Conference on Learning Representations (ICLR), 2025a."
        },
        {
            "title": "Preprint",
            "content": "Enze Xie, Junsong Chen, Yuyang Zhao, Jincheng Yu, Ligeng Zhu, Yujun Lin, Zhekai Zhang, Muyang Li, Junyu Chen, Han Cai, et al. Sana 1.5: Efficient scaling of training-time and inference-time compute in linear diffusion transformer. arXiv preprint arXiv:2501.18427, 2025b. Yi Xin, Juncheng Yan, Qi Qin, Zhen Li, Dongyang Liu, Shicheng Li, Victor Shea-Jay Huang, Yupeng Zhou, Renrui Zhang, Le Zhuo, et al. Lumina-mgpt 2.0: Stand-alone autoregressive image modeling. arXiv preprint arXiv:2507.17801, 2025. Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by example: Exemplar-based image editing with diffusion models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1838118391, 2023. Junyan Ye, Dongzhi Jiang, Zihao Wang, Leqi Zhu, Zhenghao Hu, Zilong Huang, Jun He, Zhiyuan Yan, Jinghua Yu, Hongsheng Li, et al. Echo-4o: Harnessing the power of gpt-4o synthetic images for improved image generation. arXiv preprint arXiv:2508.09987, 2025a. Yang Ye, Xianyi He, Zongjian Li, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Bohan Hou, and Li Yuan. Imgedit: unified image editing dataset and benchmark. arXiv preprint arXiv:2505.20275, 2025b. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1197511986, 2023. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In Proceedings of the European Conference on Computer Vision (ECCV), pp. 169186. Springer, 2024. Xiangyu Zhao, Peiyuan Zhang, Kexian Tang, Xiaorong Zhu, Hao Li, Wenhao Chai, Zicheng Zhang, Renqiu Xia, Guangtao Zhai, Junchi Yan, et al. Envisioning beyond the pixels: Benchmarking reasoning-informed visual editing. arXiv preprint arXiv:2504.02826, 2025a. Xuanle Zhao, Xianzhen Luo, Qi Shi, Chi Chen, Shuo Wang, Zhiyuan Liu, and Maosong Sun. Chartcoder: Advancing multimodal large language model for chart-to-code generation. arXiv preprint arXiv:2501.06598, 2025b. Le Zhuo, Ruoyi Du, Han Xiao, Yangguang Li, Dongyang Liu, Rongjie Huang, Wenze Liu, Xiangyang Zhu, Fu-Yun Wang, Zhanyu Ma, et al. Lumina-next: Making lumina-t2x stronger and faster with next-dit. In Advances in Neural Information Processing Systems (NeurIPS), 2024. Le Zhuo, Liangbing Zhao, Sayak Paul, Yue Liao, Renrui Zhang, Yi Xin, Peng Gao, Mohamed Elhoseiny, and Hongsheng Li. From reflection to perfection: Scaling inference-time optimization for text-to-image diffusion models via reflection tuning. arXiv preprint arXiv:2504.16080, 2025."
        },
        {
            "title": "A APPENDIX",
            "content": "Figure 10: Progressive training pipeline. For the Unified Alignment stage, only newly added MLP parameters are updated. Both MMDiT and MLP parameters are updated during the Hybrid Visual Learning and Thinking Enhancement stages. A.1 TRAINING DETAILS The training of our model is organized into three progressive stages as shown in Fig. 10. The hyperparameters for the three-stage training process are provided in Tab. 4. Throughout all stages, we incorporate diverse mixture of image generation and editing datasets, spanning both natural and structured sources, which helps the model learn generalizable representations and effectively adapt to diverse editing scenarios. Specifically, simple text-to-image datasets (jackyhate, 2024; Chen et al., 2025a) and various image editing datasets (Chen et al., 2025a; Wang et al., 2025c; Ye et al., 2025b; Kuprashevich et al., 2025) are used for stage 1 training. For stage 2 training, we further incorporate editing and text-to-image data from our structured dataset, along with recent high-quality text-to-image dataset (Fang et al., 2025b). For stage 3 training, we use the same data sources as stage 2, but replace the short prompt with the detailed chain-of-thought instruction. Table 4: Training hyper-parameters for our model. Hyper-parameters Stage Stage 2 Stage 3 trainable parameters warmup steps learning rate schedule optimizer optimizer hyper-parameters weight decay gradient norm clip deepspeed stage learning rate batch size max text length MLP 500 zero1 5e-4 1024 256 MLP + DiT MLP + DiT 0 100 constant AdamW Î²1, Î²2 = (0.9, 0.95) 0.1 0.5 zero2 1e-5 512 256 zero2 1e-5 512 512 A.2 ADDITIONAL EVALUATION RESULTS Using Qwen2.5-VL-72B as evaluator. With the rapid improvement of open-source models, it has become increasingly feasible to adopt them as cost-effective evaluators. Therefore, we provide the full benchmark results using open-source Qwen2.5-VL-72B (Bai et al., 2025) as our evaluator. The ranking trends evaluated by GPT-5 (OpenAI, 2025a) and Qwen2.5-VL are generally well aligned. Both evaluators reliably highlight the strongest models (e.g., Nano Banana (Google, 2025), Seedream 4.0 (Seedream et al., 2025)) and the weakest one (e.g., UniWorld-V1 (Lin et al., 2025)). And we observe that slight discrepancies arise only among closely performing models. For example, GPT-5 slightly favors Seedream 4.0 over Nano Banana (shown in Tab .1), whereas Qwen2.5-VL shows the opposite trend in Tab.5. similar alignment holds on the StructT2IBench, where both evaluations yield nearly identical overall rankings, with only minor variations among the top-performing models (Tab. 2 vs. 7). This alignment indicates that current open-source models have the potential to promote"
        },
        {
            "title": "Preprint",
            "content": "Table 5: Quantitative comparison on StructEditBench with Qwen2.5-VL-72B evaluator. The table showcases the results of Accuracy (%), PSNR and SSIM (with GT image) for various methods. indicate the first and second Accuracy results, respectively. Model GPT-Image Nano Banana Seedream 4.0 Uniworld DiMOO OmniGen2 Ovis-U1 Hidream-E1.1 Bagel Bagel-Think Step1X-Edit Flux Kontext Qwen-Edit Ours Math Chart Graph Puzzle Science Table Overall Acc PSNR SSIM Acc PSNR SSIM Acc PSNR SSIM Acc PSNR SSIM Acc PSNR SSIM Acc PSNR SSIM Acc PSNR SSIM 39.77 39.46 41.31 15.97 22.22 23.65 25.57 21.97 16.07 27.82 27.22 29.78 34.03 41.10 17.06 20.77 23. 8.84 21.56 15.95 18.45 18.43 21.38 23.97 23.41 19.78 23.73 23.31 0.9199 0.9526 0.9587 0.8264 0.9519 0.8206 0.9356 0.9399 0.9603 0.9734 0.9711 0.9512 0.9714 39.61 42.33 43.17 6.19 8.78 13.44 13.48 20.25 24.27 23.38 24.99 27.78 25. 0.9514 40.44 12.56 14.93 16.54 7.87 14.98 12.44 13.30 12.91 16.38 16.82 16.68 14.61 12.33 15.33 0.658 0.7392 0. 0.4883 0.7437 0.7187 0.6854 0.7059 0.8178 0.8227 0.8154 0.7324 0.6593 47.77 46.72 41.18 10.27 16.30 23.68 28.42 22.40 20.32 32.30 25.11 30.25 38.23 0.7528 37.20 17.24 21.22 24. 6.16 21.77 11.31 19.01 18.26 22.70 26.49 24.56 20.10 26.11 24.65 0.9357 0.9616 0.9433 0.7272 0.96 0.7834 0.9501 0.9466 0.9727 0.9823 0.9775 0.9554 0.9814 59.01 54.35 59.94 9.98 21.18 16.56 24.59 29.16 35.44 23.22 43.95 46.32 43. 0.9626 57.58 16.55 22.92 26.93 7.71 22.26 16.51 17.92 18.04 24.22 26.75 25.94 20.38 27.31 26.33 0.9139 0.9621 0. 0.7619 0.9568 0.8654 0.924 0.9389 0.9728 0.9809 0.9786 0.9501 0.9794 53.23 53.33 53.39 16.01 18.11 26.02 22.00 25.42 32.58 33.19 33.49 35.60 43.08 0.9647 52.46 16.61 22.61 26. 7.67 22.57 15.60 18.68 16.47 23.56 25.57 24.98 20.99 25.53 25.80 0.9293 0.9646 0.9518 0.7672 0.9626 0.8615 0.9455 0.9377 0.9736 0.9804 0.9738 0.9627 0.9742 78.02 66.87 78.56 15.08 20.60 27.52 33.03 46.83 45.37 35.73 48.48 52.22 58. 0.9638 69.75 14.35 19.75 24.75 8.24 19.47 16.60 16.62 17.12 21.54 23.83 23.97 18.52 25.71 23.19 0.84 0.926 0. 0.7297 0.9209 0.8888 0.8796 0.9118 0.9542 0.9731 0.9687 0.9127 0.9665 44.00 44.46 46.02 9.98 14.22 18.10 19.40 23.07 24.24 26.17 28.26 31.13 31.99 0.9438 43.56 16.64 21.09 24. 8.21 21.49 15.49 18.25 18.01 22.06 24.70 24.03 19.84 24.81 24.01 0.9119 0.9524 0.9514 0.7895 0.9504 0.8334 0.93 0.9368 0.9636 0.9759 0.9726 0.9478 0.9731 0.8535 Table 6: Quantitative comparison of different methods on StructEditBench (Chart only) with Qwen2.5-VL-72B evaluator. indicate the first and second Accuracy results, respectively. Model Category Color Num Auxiliary Add&Del Overall Acc PSNR SSIM Acc PSNR SSIM Acc PSNR SSIM Acc PSNR SSIM Acc PSNR SSIM Acc PSNR SSIM GPT-Image Nano Banana Seedream 4.0 37.65 37.13 34.76 6.52 Uniworld 5.75 DiMOO 12.41 OmniGen2 Ovis-U1 11.11 Hidream-E1.1 18.14 20.19 Bagel-Think 18.31 Bagel 19.80 Step1X-Edit 19.16 Qwen-Edit 21.70 Flux Kontext Ours 39.55 9.85 10.33 9.67 7.60 9.82 8.61 9.57 9.29 9.08 9.00 10.40 9.52 10.14 10.40 0.6756 0.6728 0. 0.5067 0.6559 0.6579 0.6567 0.6438 0.6592 0.6714 0.6780 0.6841 0.6832 45.26 47.56 54.75 7.83 8.34 21.35 20.34 30.63 36.68 40.57 33.99 34.22 36.89 0.7075 50.46 13.91 17.59 21. 8.29 16.97 11.78 15.38 14.07 20.46 19.35 19.75 13.63 16.53 18.16 0.6802 0.8117 0.9162 0.5542 0.8120 0.7786 0.7456 0.7667 0.9482 0.9356 0.9202 0.6850 0.7896 31.33 38.04 39.62 4.71 11.40 13.91 13.33 18.60 24.15 24.79 26.52 22.89 27. 13.48 16.56 19.22 7.57 17.30 14.03 15.13 15.04 20.29 20.14 20.22 13.69 16.74 0.6598 0.7696 0.8509 0.4504 0.7790 0.7138 0.7167 0.7418 0.8845 0.8760 0.8719 0.6726 0.7547 50.63 55.05 55.69 8.94 9.99 13.08 13.27 22.39 23.06 26.19 26.38 32.19 33. 0.8257 32.39 17.04 0.7721 48.99 13.48 17.39 19. 8.62 17.59 16.59 14.25 14.34 21.24 20.62 19.46 13.07 16.79 17.90 0.6315 0.7593 0.8490 0.4758 0.7770 0.7839 0.6541 0.7135 0.8888 0.8764 0.8692 0.6048 0.7442 33.56 35.72 35.16 2.93 9.61 8.35 11.38 13.86 16.42 16.45 21.77 22.17 22. 0.7685 31.73 12.73 13.88 14.28 7.35 14.46 12.11 13.11 12.71 14.93 14.58 15.11 12.46 13.95 14.37 0.6386 0.6989 0. 0.4496 0.7160 0.6735 0.6612 0.6788 0.7730 0.7654 0.7712 0.6440 0.7022 39.61 42.33 43.17 6.19 8.78 13.44 13.48 20.25 23.38 24.27 24.99 25.50 27.78 0.7012 40.44 12.56 14.93 16. 7.87 14.98 12.44 13.30 12.91 16.82 16.38 16.68 12.33 14.61 15.33 0.6580 0.7392 0.7992 0.4883 0.7437 0.7187 0.6854 0.7059 0.8227 0.8178 0.8154 0.6593 0.7324 0.7528 broader adoption of open-source evaluation within the community, reducing reliance on proprietary systems while maintaining reliable assessment quality. Different metrics analysis. As shown in Tab. 5, Tab. 6 and Tab. 7, we observe that previously widely used metrics such as PSNR and SSIM are not reliable in structured visuals, since evaluation here emphasizes semantic-level correctness rather than purely pixel-level correspondence. Both GPT-5 and Qwen2.5-VL evaluators consistently reflect this phenomenon. To further validate this finding, we conduct human alignment study, with detailed results presented in Fig. 11. General editing benchmark evaluation. To assess the effectiveness of our proposed multi-stage training pipeline in preserving model capabilities in general domains, we provide the results evaluated on ImgEdit (Ye et al., 2025b) benchmark in Tab. 8. Our model achieves results competitive with state-of-the-art systems and outperforms the original FLUX.1 Kontext, validating that the training pipeline improves editing competence without sacrificing general-domain capability. A.3 DETAILED HUMAN ALIGNMENT STUDY Details of Elo rating. We leverage Rapidata2 to collect human preference annotations. We compensated all annotators at or above the local minimum wage. After collecting human preferences, we adopt robust Elo rating scheme to analyze the results of human studies. For match between model and model with current ratings (RA, RB), the expected win probability of is defined as: EA = 1 RB RA Ï , 1 + 10 (1) where Ï = 400 is the scaling factor. 2https://www.rapidata.ai/"
        },
        {
            "title": "Preprint",
            "content": "Table 7: Quantitative comparison on StructT2IBench with Qwen2.5-VL-72B evaluators, reporting Accuracy (%). Model Seedream 4.0 Nano banana GPT-Image Bagel Bagel-Think Uniworld Hidream-I1-Full OmniGen2 Flux1.1 Dev Fulx Kontext Ovis-U1 Qwen-Image Ours Chart Graph Math Puzzle Science Table Overall 32.07 33.26 34.00 2.33 2.83 4.66 5.07 8.62 10.28 14.81 29.33 29.58 15.90 48.38 52.00 51.47 0.78 10.56 11.82 16.57 19.23 18.84 20.60 11.79 42.99 23. 55.62 54.87 53.12 1.46 8.87 10.45 14.29 16.93 17.43 16.73 11.74 41.74 31.87 42.81 53.26 51.05 2.51 11.94 8.01 13.61 14.51 18.64 21.68 17.89 42.32 24. 54.43 47.81 52.52 5.19 11.76 14.14 22.65 22.63 19.06 24.54 18.02 45.14 31.69 65.61 65.20 79.26 2.94 8.35 9.85 24.69 19.96 24.66 26.44 10.21 69.00 27. 42.33 43.16 44.08 2.21 5.93 7.40 10.39 12.87 14.19 17.09 21.81 37.03 22.12 Table 8: Quantitative comparison on ImgEdit-Full benchmark. Model Add Adjust Extract Replace Remove Background Style Hybrid Action Overall MagicBrush Instruct-Pix2Pix AnyEdit UltraEdit OmniGen Step1X-Edit ICEdit BAGEL UniWorld-V1 OmniGen2 FLUX.1 Kontext GPT-Image Qwen-Image Ours 2.84 2.45 3.18 3.44 3.47 3.88 3.58 3.56 3.82 3.57 3.76 4.61 4.38 4.15 1.58 1.83 2.95 2.81 3.04 3.14 3.39 3.31 3.64 3.06 3.45 4.33 4. 4.07 1.51 1.44 1.88 2.13 1.71 1.76 1.73 1.70 2.27 1.77 2.15 2.90 3.43 1.62 1.97 2.01 2.47 2.96 2.94 3.40 3.15 3.30 3.47 3.74 3.98 4.35 4.66 4.01 1.58 1.50 2.23 1.45 2.43 2.41 2.93 2.62 3.24 3.20 2.94 3.66 4. 3.60 1.75 1.44 2.24 2.83 3.21 3.16 3.08 3.24 2.99 3.57 3.78 4.57 4.38 4.01 2.38 3.55 2.85 3.76 4.19 4.63 3.84 4.49 4.21 4.81 4.38 4.93 4.93 4.52 1.62 1.20 1.56 1.91 2.24 2.64 2.04 2.38 2.96 2.52 2.96 3.96 3. 3.58 1.22 1.46 2.65 2.98 3.38 2.52 3.68 4.17 2.74 4.68 4.26 4.89 4.69 4.22 1.90 1.88 2.45 2.70 2.96 3.06 3.05 3.20 3.26 3.44 3.52 4.20 4.27 3.75 Given the empirical vote proportion sA [0, 1] for model (with sB = 1 sA), the ratings are updated as: R = max(Rmin, RA + Keff (sA EA)), = max(Rmin, RB + Keff (sB EB)), (2) where Keff = rating floor, and = 24 is the base learning rate. 5 is the effective step size, scaled by the number of votes , Rmin = 700 is the To improve robustness, we shuffle the match order and repeat the Elo computation for = 50 rounds. The final Elo score for each model is reported as the mean and standard deviation across rounds: Rm = 1 (cid:88) t=1 R(t) , Ïm = (cid:118) (cid:117) (cid:117) (cid:116) 1 (cid:88) t=1 (cid:0)R(t) Rm (cid:1)2 . (3) This procedure accounts for vote strength, enforces minimal rating floor, and provides both robust mean scores and uncertainty estimates. The parameter setting is shown in Tab."
        },
        {
            "title": "Preprint",
            "content": "Figure 11: Pearson correlation analysis among Accuracy (Qwen2.5-VL-72B evaluator), PSNR, SSIM and human Elo score for image editing. Figure 12: Correlation of our metric with human preference under different weighting ratios. Table 9: Elo parameter setting. Parameter Number initial Elo mean Elo standard deviation base of logarithm scaling factor K-factor minimum Elo rating number of simulated matches 1,000 300 10 400 24 700 4,500 Results analysis. As depicted in Fig. 11, StructScore Accuracy demonstrates much stronger correlation with human evaluation results than conventional metrics such as PSNR and SSIM. Specifically, StructScore achieves Pearson correlation of = 0.87 at the overall level and = 0.84 on chartspecific cases, substantially higher than PSNR (r = 0.69/0.64) and SSIM (r = 0.70/0.55). These results clearly indicate that StructScore better captures the semantic-level fidelity that aligns with human preference, whereas pixel-level metrics fail to reflect such perceptual correctness. However, compared with GPT-5 evaluators = 0.92 (shown in Fig. 7), the alignment of Qwen2.5-VL with human evaluation is still weaker. This is probably because its performance on such structured images remains inferior to closed-source models, which could be improved as open-source models continue to advance."
        },
        {
            "title": "Preprint",
            "content": "A.4 ABLATION ON DIFFERENT WEIGHT SETTING We conducted comparative study of benchmark ranking calculated with different weight ratios by evaluating their alignment with human preference judgments. As illustrated in Fig. 12, the configuration with 1:9 weighting achieved the highest correction coefficient, followed by the 2:8 setting. In contrast, the 3:7 and 4:6 ratios yielded substantially lower values. These findings demonstrate that the 1:9 ratio most accurately captures the alignment between the models evaluation accuracy and human preference consistency. A.5 CASE STUDY For qualitative evaluation, we provide text-to-image generation and image editing examples across various models in Fig. 13 and 14. Figure 13: Qualitative comparison for structured image editing. GT stands for ground truth."
        },
        {
            "title": "Preprint",
            "content": "Figure 14: Qualitative comparison for structured image generation. GT stands for ground truth."
        },
        {
            "title": "Preprint",
            "content": "A.6 PROMPTS In this section, we present the complete set of prompts utilized in the data generation pipeline, alongside those employed for evaluation. Specifically, these include the prompt designed for editing pair construction in Fig. 15, the prompt for generating the T2I prompts in Fig. 16, the prompt used for constructing CoT in Fig. 17, the prompt applied for benchmark description construction in Fig. 18, the prompt for benchmark Q&A construction in Fig. 19, the prompt for benchmark Q&A refinement in Fig. 20,the prompt for GT judgement in Fig. 21. Figure 15: Prompt constructing the image editing pairs with code commands."
        },
        {
            "title": "Preprint",
            "content": "Figure 16: Prompt to obtain captions for non-natural STEM data. Figure 17: Prompt to obtain reasoning traces for the editing task of non-natural STEM images."
        },
        {
            "title": "Preprint",
            "content": "Figure 18: Prompt for evaluation. Figure 19: Prompt for constructing question-answer pairs for our benchmark."
        },
        {
            "title": "Preprint",
            "content": "Figure 20: Prompt to refine the question-answer pairs. Figure 21: Prompt to check if ground-truth and mdoel response are similar."
        }
    ],
    "affiliations": [
        "Beihang University",
        "ByteDance",
        "CUHK MMLab",
        "Hugging Face",
        "Krea AI",
        "National University of Singapore",
        "Shanghai AI Lab",
        "Shanghai Jiao Tong University",
        "The University of Hong Kong"
    ]
}