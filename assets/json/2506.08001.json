{
    "paper_title": "Reparameterized LLM Training via Orthogonal Equivalence Transformation",
    "authors": [
        "Zeju Qiu",
        "Simon Buchholz",
        "Tim Z. Xiao",
        "Maximilian Dax",
        "Bernhard Schölkopf",
        "Weiyang Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While large language models (LLMs) are driving the rapid advancement of artificial intelligence, effectively and reliably training these large models remains one of the field's most significant challenges. To address this challenge, we propose POET, a novel reParameterized training algorithm that uses Orthogonal Equivalence Transformation to optimize neurons. Specifically, POET reparameterizes each neuron with two learnable orthogonal matrices and a fixed random weight matrix. Because of its provable preservation of spectral properties of weight matrices, POET can stably optimize the objective function with improved generalization. We further develop efficient approximations that make POET flexible and scalable for training large-scale neural networks. Extensive experiments validate the effectiveness and scalability of POET in training LLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 1 0 0 8 0 . 6 0 5 2 : r a"
        },
        {
            "title": "Reparameterized LLM Training via Orthogonal\nEquivalence Transformation",
            "content": "Zeju Qiu1 Simon Buchholz1 Tim Z. Xiao1 Maximilian Dax1 Bernhard Schölkopf1 Weiyang Liu1,2,* 1Max Planck Institute for Intelligent Systems, Tübingen 2The Chinese University of Hong Kong"
        },
        {
            "title": "Abstract",
            "content": "While Large language models (LLMs) are driving the rapid advancement of artificial intelligence, effectively and reliably training these large models remains one of the fields most significant challenges. To address this challenge, we propose POET, novel reParameterized training algorithm that uses Orthogonal Equivalence Transformation to optimize neurons. Specifically, POET reparameterizes each neuron with two learnable orthogonal matrices and fixed random weight matrix. Because of its provable preservation of spectral properties of weight matrices, POET can stably optimize the objective function with improved generalization. We further develop efficient approximations that make POET flexible and scalable for training large-scale neural networks. Extensive experiments validate the effectiveness and scalability of POET in training LLMs."
        },
        {
            "title": "Introduction",
            "content": "Recent years have witnessed the increasing popularity of large language models (LLMs) in various applications, such as mathematical reasoning [12] and program synthesis [2] and decision-making [73]. Current LLMs are typically pre-trained using enormous computational resources on massive datasets containing trillions of tokens, with each training run that can take months to complete. Given such huge training cost, how to effectively and reliably train them poses significant challenges. The de facto way for training LLMs is to directly optimize weight matrices with the Adam optimizer [35, 53]. While conceptually simple, this direct optimization can be computationally intensive (due to the poor scaling with model size) and requires careful hyperparameter tuning to ensure stable convergence. More importantly, its generalization can remain suboptimal even if the training loss is perfectly minimized [34]. To stabilize training and enhance generalization, various weight regularization methods [3, 9, 11, 45, 47, 75] and weight normalization techniques [26, 36, 37, 48, 50, 52] have been proposed. Most of these methods boil down to improving spectral properties of weight matrices (i.e., singular values) either explicitly or implicitly. Intuitively, the spectral norm of weight matrix (i.e., the largest singular value) provides an upper bound on how much matrix can amplify the input vectors, which connects to the generalization properties. In general, smaller spectral norms (i.e., better smoothness) are considered to be associated with stronger generalization, which inspires explicit spectrum control [31, 57, 65, 75]. Theoretical results [5] also suggest that weight matrices with bounded spectrum can provably guarantee generalization. Given the importance of the spectral properties of weight matrices, what prevents us from controlling them during LLM training? Inefficacy of spectrum control: Existing spectrum control methods constrain only the largest singular value, failing to effectively regularizing the full singular value spectrum. Moreover, there is also no guarantee for spectral norm regularization to effectively control the largest singular value. Computational overhead: Both spectral norm regularization [75] and spectral normalization [57] require computing the largest singular value of weight matrices. Even with power iteration, this still adds significant overhead to the training process, especially when training large neural networks. Additionally, spectral regularization does not scale efficiently with increasing model size. *Project lead & Corresponding author Project page: spherelab.ai/poet Technical Report To achieve effective weight spectrum control without the limitations above, we propose POET, reParameterized training algorithm that uses Orthogonal Equivalence Transformation to indirectly learn weight matrices. Specifically, POET reparameterizes weight matrix Rmn with RW0P where W0 Rmn is randomly initialized weight matrix, Rmm and Rnn are two orthogonal matrices. Instead of optimizing weight matrices directly, POET keeps the randomly initialized weight matrix W0 unchanged during training and learns two orthogonal matrices R, to transform W0. This reparameterization preserves the singular values of weights while allowing flexible optimization of the singular vectors. POET effectively addresses the above limitations: Figure 1: Training dynamics of singular values of the same weight matrix in LLaMA model. Standard training on the left strictly follows the common practice for training LLMs (direct optimization with AdamW). POET on the right uses the proposed approximation for large-scale LLM training. The slight (almost negligible) singular value changes in POET are due to numerical and approximation error. Strong spectrum control: Because orthogonal transformations do not change the singular values of weight matrices, POET keeps the weight spectrum the same as the randomly initialized weight matrices (empirically validated by Figure 1 even with approximations). Through the initialization scheme, POET thus directly controls the singular value distribution of its weight matrices. As result, and in contrast to standard LLM training, POET matrices avoid undesirable large singular values after training (Figure 1 and Appendix H). To further facilitate the POET algorithm, we introduce two new initialization schemes: normalized Gaussian initialization and uniform spectrum initialization, which can ensure the resulting weight matrices have bounded singular values. Efficient approximation: While naive implementation of POET can be computationally expensive, its inherent flexibility opens up opportunities for efficient and scalable training. To address the key challenge of optimizing large orthogonal matrices, we introduce two levels of approximations: Stochastic primitive optimization: The first-level approximation aims to reduce the number of learnable parameters when optimizing large orthogonal matrix. To this end, we propose the stochastic primitive optimization (SPO) algorithm. Given large orthogonal matrix Rmm, SPO factorizes it into product of primitive orthogonal matrices, each involving significantly fewer trainable parameters. These primitives are constructed by parameterizing randomly sampled submatrices of the full matrix. This factorization is implemented as memory-efficient iterative algorithm that sequentially updates one primitive orthogonal matrix at time. To improve the expressiveness of the sequential factorization, we adopt merge-then-reinitialize trick, where we merge each learned primitive orthogonal matrix into the weight matrix, and then reinitialize the primitive orthogonal matrix to be identity after every fixed number of iterations. Approximate orthogonality via Cayley-Neumann parameterization: The second-level approximation addresses how to maintain orthogonality without introducing significant computational overhead. To achieve this, we develop the Cayley-Neumann parameterization (CNP) which approximates the Cayley orthogonal parameterization [46, 63] with Neumann series. Our merge-then-reinitialize trick can effectively prevent the accumulation of approximation errors. POET can be viewed as natural generalization of orthogonal training [46, 49, 63], wherein the model training is done by learning layer-shared orthogonal transformation for neurons. Orthogonal training preserves the hyperspherical energy [45, 47] within each layera quantity that characterizes pairwise neuron relationships on the unit hypersphere. While preserving hyperspherical energy proves effective for many finetuning tasks [49], it limits the flexibility of pretraining. Motivated by this, POET generalizes energy preservation to spectrum preservation and subsumes orthogonal training as its special case. The better flexibility of POET comes from its inductive structures for preserving weight spectrum, rather than more learnable parameters. We empirically validate that POET achieves better pretraining performance than orthogonal training given the same budget of parameters. To better understand how POET functions, we employ vector probing to analyze the learning dynamics of the orthogonal matrices. Vector probing evaluates an orthogonal matrix using fixed, randomly generated unit vector by computing vRv which corresponds to the cosine similarity between Rv and v. By inspecting the cosine similarities of seven orthogonal matrices throughout training, we 2 observe that the learning process can be divided into three distinct phases (Figure 2): (1) conical shell searching: The cosine starts at 1 (i.e., is the identity) and gradually converges to stable range of [0.6, 0.65], which we observe consistently across all learnable orthogonal matrices. This suggests that transforms into thin conical shell around its original direction. (2) stable learning on the conical shell: The cosine remains within this range while the model begins to learn stably. Despite the cosine plateauing, validation perplexity continues to improve almost linearly. (3) final adjusting: Learning slows and eventually halts as the learning rate approaches zero. We also find that training loss is generally not informative of these three phases. We provide an in-depth discussion and full empirical results in Appendix A,F. Our contributions are summarized below: Figure 2: POETs three learning phases. Left: illustration; Middle: angle; Right: loss and validation. We introduce POET, novel training framework that provably preserves spectral properties of weight matrices through orthogonal equivalence transformation. To enhance POETs scalability, we develop two simple yet effective approximations: stochastic principal submatrix optimization for large orthogonal matrices and the Cayley-Neumann parameterization for efficient representation of orthogonal matrices. We empirically validate POETs training stability and generalization across multiple model scales."
        },
        {
            "title": "2 From Energy-preserving Training to Spectrum-preserving Training",
            "content": "Orthogonal training [46, 49, 63] is framework to train neural networks by learning layershared orthogonal transformation for neurons in each layer. Specifically, for weight matrix = {w1, , wn} Rmn where wi Rm is the i-th neuron, the layers forward pass is given by = with input Rm and output Rn. Unlike standard training, which directly optimizes the weight matrix , orthogonal training keeps fixed at its random initialization and instead learns an orthogonal matrix Rmm to jointly transform all W0 = w0 neurons in the layer. The forward pass becomes = (RW0)x. The effective weight matrix in orthogonal training is WR = {wR } where wR = Rwi. key property of orthogonal training is its preservation of hyperspherical energy. Letting ˆwi = wi wi , orthogonal training ensures (cid:13) (cid:13) ˆw0 (1) ˆwi ˆwj1 =: HE(W R), 1 , , wR 1, . . . , w0 HE(W0) := ˆw0 (cid:88) (cid:88) (cid:13) (cid:13) = 1 i=j i=j where hyperspherical energy HE() characterizes the hyperspherical uniformity of neurons by measuring the sum of pairwise similarities among them. Prior work [4547, 72] has shown that energypreserving training can effectively improve generalization. Orthogonal finetuning (OFT) [49, 63] further demonstrates that finetuning large foundation models while preserving hyperspherical energy achieves favorable trade-off between efficient adaptation to downstream tasks and retention of pretraining knowledge. However, while the hyperspherical energy preservation is effective for finetuning, it can be overly restrictive for pretraining. To allow greater flexibility in the pretraining phase, we relax the constraint from preserving hyperspherical energy to preserving the singular-value spectrum instead. Because energy-preserving training inherently maintains the spectrum, it can be viewed as special case of spectrum-preserving training. As natural generalization, spectrum-preserving training learns transformation : Rmn Rmn that perfectly preserves the spectrum, i.e., (cid:8)σ1 (cid:0)T (W0)(cid:1)(cid:9) = (cid:8)σ1(W0), σ2(W0), , σmin(m,n)(W0)(cid:9), (2) where σi(W0) denotes the i-th singular value of W0 (sorted by descending order with σ1 being the largest singular value). How we instantiate the transformation results in different algorithms. Generally, is spectrum-preserving map, and can be either linear [40] or nonlinear [4]. If we only consider to be linear map, then Theorem 1 can fully characterize the form of : Theorem 1 (Simplified informal results from [40]). For linear map : Rmn Rmn (m = n), if σ1(T (W )) = σ1(W ) always holds for all Rmn, then the linear map must be of the following form: (W ) = RW , for all Rmn where Rmm and Rnn are some fixed elements in orthogonal groups O(m) and O(n), respectively. (cid:0)T (W0)(cid:1), , σmin(m,n) (cid:0)T (W0)(cid:1), σ2 3 All parameterizations for the linear map can be expressed as (W ) = RW , where and are orthogonal matrices. For instance, OFT is an energy-preserving method (a special case of spectrum-preserving training), where the map simplifies to (W ) = RW I, with as the identity."
        },
        {
            "title": "3 Reparameterized Training via Orthogonal Equivalence Transformation",
            "content": "This section introduces the POET framework, which reparameterizes each neuron as the product of fixed random weight matrix and two learnable orthogonal matrices applied on both sides. POET serves as specific implementation of spectrum-preserving training. Inspired by Theorem 1, it parameterizes the spectrum-preserving transformation using left orthogonal matrix that transforms the column space of the weight matrix and right orthogonal matrix that transforms its row space."
        },
        {
            "title": "3.1 General Framework",
            "content": "Following the general form of spectrum-preserving linear maps discussed in the last section, POET reparameterizes the neuron as RW0P , where W0 Rmn is randomly initialized weight matrix that remains fixed during training, and Rmm, Rnn are trainable orthogonal matrices. This reparameterization effectively applies an orthogonal equivalence transformation (OET) to random weight matrices. Specifically, OET is double-sided transformation, defined as OET(W ; R, ) = RW , where the input matrix is multiplied on the left and on the right by orthogonal matrices and , respectively. The forward pass of POET can be thus written as = RP = (RW0P )x, s.t. (cid:8)RR = RR = I, = = I(cid:9), (3) where and can be merged into single weight matrix WRP = RW0P after training. Therefore, the inference speed of POET-trained neural networks is the same as conventionally trained ones. Spectrum control. POET can be interpreted as learning weight matrices by simultaneously transforming their left singular vectors and right singular vectors while keeping the singular values unchanged. Given the singular value decomposition (SVD) W0 = Σ0V , the reparameterized neuron weight matrix becomes WRP = RU Σ0V where both RU and are orthogonal matrices. This effectively constitutes an SVD of WRP . It is also straightforward to verify that the spectral properties of WRP remain identical to those of the initial matrix W0. Neuron initialization. Since POET preserves the spectral properties of the initial weight matrix W0, the choice of initialization plays critical role. We consider two common schemes: (1) standard initialization, which samples from zero-mean Gaussian with fixed variance (the default choice for LLaMA models); and (2) Xavier initialization [16], which uses zero-mean Gaussian with variance scaled by the layer dimensions. To facilitate POET, we propose two new initialization schemes. The first method, uniform-spectrum initialization, applies SVD to standard initialization and sets all singular values to 1, balancing spectral properties throughout training. The second, normalized Gaussian initialization, normalizes neurons drawn from zero-mean Gaussian with fixed variance. This is directly inspired by prior work showing that normalized neurons improve convergence [46, 48, 50]. To ensure that the POET-reparameterized network is statistically equivalent to standard network at initialization, we always initialize both orthogonal matrices as identity matrices. Figure 3: Singular values of weight matrix of size 5121376, randomly generated by different initialization schemes."
        },
        {
            "title": "3.2 Efficient Approximations to Orthogonality",
            "content": "POET is conceptually simple, requiring only the optimization of two orthogonal matrices. However, these matrices are typically large, and naively optimizing them leads to significant computational challenges. We start by introducing the following efficient approximations."
        },
        {
            "title": "3.2.1 Stochastic Primitive Optimization",
            "content": "The core idea of SPO is inspired by how QR factorization is performed using Givens rotations and Householder transformations. Both methods construct large orthogonal matrix by sequentially applying primitive orthogonal transformations (e.g., Givens rotations or Householder reflections), i.e., = (cid:81)c i=1 Gi, where Gi denotes the i-th primitive orthogonal matrix. While each Gi is of the same size as R, it is parameterized by significantly fewer degrees of freedom. See Figure 4 4 Figure 4: Examples of the primitive orthogonal transformation matrix Gi in different orthogonalizations (two examples for each method). Note that, blue blocks represent 1, light purple blocks denote 0 and deep purple blocks are the actual orthogonal parameterization to be learned. for an illustration. Both Givens rotation and Householder reflection use relatively low-capacity parameterizationsfor example, each Givens rotation Gi involves only single effective parameter which limits their efficiency in representing the full orthogonal matrix. SPO follows similar idea of factorizing the original orthogonal matrix into multiple primitive orthogonal matrices. However, unlike Givens and Householder methods, SPO treats the number of effective parameters in each primitive matrix as tunable hyperparameter and adopts stochastic sparsity pattern. Fully stochastic SPO. The basic idea of fully stochastic SPO is to randomly sample small submatrix and enforce its orthogonality, allowing it to be easily extended to full orthogonal matrix by embedding it within an identity matrixa process similar to Givens or Householder transformations. To represent large orthogonal matrix Rmm, we start by defining index sets Sj = {sj b} {1, , m} (j [1, c]), where each set has cardinality Sj = b, hyperparameter controlling the number of effective parameters of primitive orthogonal matrix. Sj, are randomly sampled from the full indices {1, , m}. Let Gj Rbb be small orthogonal matrix, and D(Sj) = {e(sj b)} Rmb be selection matrix, where e(k) is the standard basis vector with 1 in the k-th position and 0 elsewhere. The factorization is given by 1), , e(sj 1, , sj = (cid:89) i=1 (cid:0) Im + D(Si) ( Gi Ib) D(Si) (cid:125) (cid:123)(cid:122) (cid:124) Gi: The i-th primitive orthogonal matrix (cid:1), s.t. Gi = Gi = Ib, i, (4) where D(Si) (A) D(Si) is projector that replaces the sub-block with A. Im and Ib are identity matrices of size and b, respectively. To efficiently parameterize small orthogonal matrices Gi, we can use the CNP introduced in the next section. Block-stochastic SPO. While fully stochastic SPO is simple, it may fail to transform all neuron dimensions because the identity matrix leaves part of the space unchanged. See the blue blocks in Figure 4(c) as an example. To address this, we propose block-stochastic SPO, which first constructs block-diagonal orthogonal matrix with small blocks for parameter efficiency, and then applies random permutation to enhance expressiveness by randomizing the sparsity pattern. Block-stochastic SPO transforms all neuron dimensions simultaneously, as shown in Figure 4(d). Formally we have = (cid:89) i=1 (cid:0) Ψ (cid:124) Diag( G1 , , , G2 (cid:123)(cid:122) Gi: The i-th primitive orthogonal matrix ) Ψi (cid:125) (cid:1), s.t. ( Gj ) Gj = Gj ( Gj ) = Ib, i, j, (5) Rbb is the j-th block of the block diagonal matrix, and Ψi, are all random permutai is an orthogonal matrix, both Gi and are also where Gj tion matrices. As long as each diagonal block Gj orthogonal matrices. We also use CNP to efficiently parameterize each orthogonal block Gj . The merge-then-reinitialize trick. The factorizations in Equation (4) and (5) offer simple approach to optimizing large orthogonal matrices by sequentially updating primitive orthogonal matrices. However, storing all previous primitives incurs high GPU memory overhead. To mitigate this, we propose the merge-then-reinitialize trick, where the learned primitive orthogonal matrix can be merged into the weight matrix after every certain number of iterations, and then reinitialized to the identity matrix. After reinitialization, stochastic sampling is repeated to select new index set (in fully stochastic SPO) or generate new permutation (in block-stochastic SPO). This trick allows only one primitive matrix to be stored at time, substantially reducing GPU memory usage."
        },
        {
            "title": "3.2.2 Cayley-Neumann Parameterization",
            "content": "The classic Cayley parameterization generates an orthogonal matrix in the form of = (I + Q)(I Q)1 where is skew-symmetric matrix satisfying = Q. minor caveat of this parameterization is that it only produces orthogonal matrices with determinant 1 (i.e., elements of the special orthogonal group), but empirical results in [46, 49, 63] indicate that this constraint does not hurt performance. However, the matrix inverse in the original Cayley parameterization introduces 5 numerical instability and computational overhead, limiting its scalability to large orthogonal matrices. To address this, we approximate the matrix inverse using truncated Neumann series: = (I + Q)(I Q)1 = (I + Q) (cid:0) (cid:88) i= Qi(cid:1) (I + Q) (cid:0)I + (cid:88) i=1 Qi(cid:1), (6) where larger number of approximation terms leads to smaller approximation error. By avoiding matrix inversion, the training stability of POET is improved; however, this comes with pricethe approximation is valid only when the Neumann series converges in the operator norm. To initialize orthogonal matrices as identity, we set to zero matrix in CNP, satisfying the convergence condition initially. As the training progresses, however, updates to may cause its operator norm to exceed 1, violating this condition. Fortunately, our merge-then-reinitialize trick mitigates this issue by periodically resetting to zero matrix, ensuring its operator norm remains small."
        },
        {
            "title": "3.2.3 Overall Training Algorithm",
            "content": "Step 1: Initialization. We initialize the weight matrices using normalized Gaussian: W0. Step 2: Orthogonal matrix initialization. For fully stochastic SPO, we randomly sample an index set S, and parameterize GR Rbb and GP Rbb using CNP (Equation (6)). Both matrices are initialized as identity, so and also start as identity matrices. For block-stochastic SPO, we sample , , b random permutation matrix ΨR, ΨP , and parameterize { G1 } using CNP. Then we initialize them as the identity, so and again starts as identity matrices. Step 3: Efficient orthogonal parameterization. For fully stochastic SPO, we have = Im + D(S)( GR Ib)D(S) and = Im + D(S)( GP Ib)D(S). For block-stochastic SPO, we have = Ψ Step 4: Inner training loop for updating orthogonal matrices. The equivalent weight matrix in the forward pass is RW . Gradients are backpropagated through and to update GR, GP (fully stochastic) or Gi , (block-stochastic). This inner loop runs for fixed number of iterations. Step 5: Merge-then-reinitialize. The learned orthogonal matrices and are merged into the weight matrix by RW . If not terminated, return to Step 2 for reinitialization. R, , b )ΨR and = Ψ R, , b } and { G1 , , b Diag( G1 RDiag( R, Gi )ΨP . P"
        },
        {
            "title": "4 Discussions and Intriguing Insights",
            "content": "Method Memory cost # trainable params Parameter and memory complexity. By introducing hyperparameter as the sampling budget, fully stochastic SPO decouples parameter complexity from the size of the weight matrices. With small b, POET becomes highly parameter-efficient, though at the cost of slower convergence. This offers users flexible trade-off between efficiency and speed. In contrast, block-stochastic SPO has parameter complexity dependent on the matrix size (i.e., + n), making it more scalable than AdamW, which requires mn trainable parameters. In terms of memory complexity, both POET variants can be much more efficient than AdamW with suitable sampling budget b. comparison of parameter and memory complexity is given in Table 1. AdamW GaLore [78] POET (FS) 2 (m + n)(b 1) POET (BS) Table 1: Comparison to existing methods. Assume Rmn (m n), GaLore with rank and POET with block size b. FS denotes fully stochastic SPO, and BS denotes block-stochastic SPO. 3mn mn + mr + 2nr mn + 3b(b 1) 2 (m + n)(b 1) mn + 3 mn mn b(b 1) 1 Performance under constant parameter budget. Since POET optimizes two orthogonal matrices R, simultaneously, natural question arises: which matrix should receive more parameter budget under fixed total constraint? To investigate this, we conduct controlled experiment where different ratios of trainable parameters are allocated to and under fixed total budget. All other settings (e.g., architecture, data) remain unchanged, with full details provided in the Appendix. We use validation perplexity as the evaluation metric. The total parameter budget matches that of fully stochastic POET with = 1 for and = 1 for , where = 8, 4, and 3 correspond to small, medium, and large budgets, respectively. We explore seven allocation settings: : = 1 : 0 (i.e., orthogonal training [46, 49, 63]), 0.9 : 0.1, 0.75 : 0.25, 0.5 : 0.5 (i.e., standard POET), 0.25 : 0.75, 0.1 : 0.9, and 0 : 1. Results in Figure 5 show that POET with balanced allocation between and yields the best performance. Figure 5: Performance of POET under constant total parameter budget on R, . Guarantees of weight spectrum. For POET with standard and normalized Gaussian initializations, we prove in Appendix that the largest and smallest singular values of weights can be bounded. Connection to generalization theory. Several generalization results [5, 61, 72] based on bounding the spectral norm of weight matrices. In particular, the spectrally-normalized margin analysis in [5] bounds the misclassification error in terms of margin-based training loss and complexity term. The complexity term is proportional to Q/(γn) where γ and are margin and sample size and bounds the spectral complexity. For an L-layer ReLU MLP and maximal width d, is bounded by (cid:18) (cid:89) = Wi (cid:19)(cid:18) (cid:88) ( dWiF )2/3 Wi2/3 (cid:19)3/2 (7) i=1 i=1 where and denote spectral and Frobenius norm respectively. Those norms remain invariant when training the network with POET and at initialization they can be bounded with high probability using standard results from random matrix theory (Appendix B). The scale at initialization is typically chosen such that Rdd satisfies = O(1) and = O( Approximation properties of SPO. We have seen in Theorem 1 that the factorization RW with orthogonal matrices and is the most general spectrum preserving transformation of . Here we express and as products of stochastic primitives, but as we state next, this does not reduce representation power when using sufficiently many primitives. Lemma 1. If αm ln(m)(m/b)2 for some α > 0 then with probability at least 1 m(α2) over the randomness of the index sets Si we can express any orthogonal matrix as product of primitives Gi as in Eq. (4). Moreover, the orthogonal matrix Gi depends only on the sets Sj and matrices Gj selected in earlier steps. d) so that = OL(d). The proof of this lemma can be found in Appendix C. The result extends to Block-stochastic SPO as this is strictly more expressive than fully stochastic SPO. The key idea of the proof is similar to the factorization of orthogonal matrices into product of Givens rotations. Indeed, by multiplying with properly chosen primitive matrices Gi we can create zeros below the diagonal for one column after another. Note that each Gi has b(b 1)/2 parameters while has m(m 1)/2 parameters, which implies that generally at least Ω((m/b)2) primitives are necessary. In Appendix we also provide heuristic that with high probability for = O(ln(m)(m/b)2) every orthogonal matrix can be written as product of orthogonal primitives Gi. Inductive bias. POET-reparameterized neurons result in neural networks that maintain identical architecture and parameter count during inference as conventionally trained networks. While standard training could technically learn equivalent parameters, they consistently fail to do so in practice. This indicates POET provides unique inductive bias unavailable through standard training. POET also aligns with prior findings in [1, 17] that optimizing factorized matrices yields implicit inductive bias."
        },
        {
            "title": "5 Experiments and Results",
            "content": "We start by evaluating POET on large-scale LLaMA pretraining, followed by an extensive ablation study to justify our design choices. Detailed settings and additional results are given in Appendices."
        },
        {
            "title": "5.1 LLM Pretraining using LLaMA Transformers",
            "content": "1.3B (50B) 60M (30B) 130M (40B) 350M (40B) Model (# tokens) AdamW Galore LoRAr=64 POETBS,b=64 POETBS,b=128 POETBS,b=256 POETFS,b=1/8 POETFS,b=1/4 POETFS,b=1/ 26.68 (25.30M) 20.82 (84.93M) 16.78 (302.38M) 29.81 (25.30M) 22.35 (84.93M) 17.99 (302.38M) 25.19 (30.28M) 32.07 (11.21M) 39.70 (4.85M) We perform the pretraining experiments on the Llama transformers of varying sizes (60M, 130M, 350M, 1.3B) for POET. We use the C4 dataset [64], cleaned web crawl corpus from Common Crawl, widely used for LLM pretraining [27, 54, 78]. For POET-BS, is the block size of the block-diagonal orthogonal matrix. For POETFS, bin=bm for and bout=bn for . We compare POET against GaLore [78], low-rank pretraining method, and AdamW, the standard pretraining optimizer. We generally follow the settings in [78]. To better simulate the practical pretraining setting, we significantly increase the number of training tokens for all methods. 18.46 (25.39M) 24.61 (6.34M) 29.67 (1.78M) 23.55 (7.13M) 17.60 (101.66M) 19.42 (25.44M) 19.94 (28.56M) 15.95 (101.86M) 13.70 (406.88M) Table 2: Comparison of POET with popular pretraining methods using different sizes of LLaMA models. Validation perplexity and the number of trainable parameters are reported. 24.52 (5.52M) 29.52 (2.39M) 26.90 (4.81M) 21.86 (11.12M) 25.29 (9.66 M) 19.88 (22.33M) 18.28 (29.22 M) 16.24 (58.91 M) 14.56 (118.26M) 20.29 (14.90M) 18.05 (30.04M) 16.27 (60.32M) 14.73 (1.21B) 18.33 (1.21B) 20.55 (59.38M) 34.06 (0.53M) 28.69 (2.13M) 25.37 (8.54M) 7 Table 2 shows that both POET-FS (b=1/2) and POET-BS (b=256) consistently outperform both GaLore and AdamW with significantly fewer parameters. For LLaMA-1B, POET-FS (b=1/2) yields the best overall performance, achieving validation perplexity of 13.70, much better than AdamW (14.73) and GaLore (18.33). Block-stochastic POET with b=256 achieves the second-best performance (14.56), which still surpasses AdamW with only one-tenth of AdamWs trainable parameters. Similar patterns can be observed for models of smaller sizes. Moreover, we compare the training dynamics between AdamW and POET in Figure 6. The training dynamics of POET is quite different from AdamW. After an initial rapid drop in perplexity, POET improves more slowly than AdamW. As seen in Phase II (Figure 2), this slower but stable progress can lead to better performance in later stages. We attribute this intriguing phenomenon to the unique reparameterization of POET and How we efficiently approximate orthogonality. The exact mechanism behind this phenomenon remains an open question, and understanding it could offer valuable insights into large-scale model training. Figure 6: Validation perplexity dynamics on LLaMA-350M and LLaMA-1.3B. To highlight POETs non-trivial performance improvement, we increase the training steps (i.e., effectively tokens seen) for AdamW, and find that POET-FS (b=1/2) still outperforms AdamW even even if AdamW is trained with almost triple the number of tokens. Results are given in Figure 7. In this experiment, the AdamW learning rate was carefully tuned for the full training run, and no training tokens were repeated. Thus, the improvement is non-trivial and cannot be attributed to merely increasing training steps. Interestingly, we also observe from Table 2 that POETs performance appears strongly correlated with the parameter budget and larger budgets consistently yield better results across model scales. This is particularly important for model scaling law [33]. Another notable observation is that POET significantly outperforms LoRA [24] given similar parameter budget. For instance, with approximately 30M trainable parameters, POET attains validation perplexity of 18.05, significantly better than LoRAs 25.19. We further observe that the block-stochastic variant is more parameter-efficient than the fully stochastic one. On the 130M model, it achieves validation perplexity of 19.88 with nearly 6M fewer trainable parameters, compared to 19.94 for the fully stochastic variant. We hypothesize that this is due to better coverage of weight parameters. Specifically, the block-stochastic variant ensures all corresponding weights are updated at each step, unlike the more uneven updates in the fully stochastic variant. Experimental details and results on weight update coverage are provided in Appendix G. Figure 7: Validation perplexity dynamics of POET (FS, b=1/2) and AdamW on Llama60M. POET outperforms the AdamW trained with almost twice the number of seen tokens."
        },
        {
            "title": "5.2 Ablation Studies and Empirical Analyses",
            "content": "Scheme Initialization schemes. We empirically compare different random initialization schemes for POET, including two commonly used ones (standard Gaussian, Xavier [16]) and two proposed ones (uniform spectrum, normalized Gaussian). Specifically, we use fully stochastic POET with b=1/2 to train Llama-60M on 30B tokens and report the validation perplexity in Table 3. Results shows that the normalized initialization will lead to the best final performance, and we stick to it as default choice. Interestingly, uniform spectrum initialization performs poorly. This suggests trade-off between preserving good weight spectral properties and achieving strong expressiveness. it may limit its expressiveness. Finding the optimal singular value structure for weights remains an important open problem. Standard Xavier Uni. spectrum Normalized Table 3: Performance of different initializations. 26.22 25.79 27.29 25.37 Perplexity Merge-then-reinitialize frequency. The proposed merge-then-reinitialize trick allows POET to train only small fraction of the large orthogonal matrices and per iteration, significantly reducing GPU memory usage. However, this trick also introduces reinitialization frequency hyperparameter Tm, which determines how often the orthogonal matrix is merged and reset to the identity. The index set in POET-FS and the permutation matrix in POET-BS are also resampled at each reinitialization. Therefore, it is quite important to understand how this hyperparameter Tm affects performance. Following the previous initialization experiment, we use POET-FS with b=1/2 to train Llama-60M on 30B tokens. We vary the reinitialization frequency Table 4: Val. perplexity of different Tm. Tm Perplexity 5 25 50 200 400 1600 30.29 27.27 25.99 25.37 25.31 25.58 8 from 5 to 1600 and report the validation perplexity in Table 4. Results show that both 200 and 400 perform well. Therefore, we set Tm = 400 in all experiments by default. Neumann series approximation. CNP approximates the matrix inverse using Neumann series. As the number of Neumann terms directly influences the approximation quality, understanding its impact on model performance is essential. To this end, we evaluate how varying the number of Neumann terms affects performance, using POET-FS with = 1/2 to train LLaMA-130M. Results in Table 5 show that increasing the number of Neumann terms generally improves validation perplexity. However, this also leads to slower training. Moreover, Using only 1 Neumann term (k = 1) leads to training divergence, highlighting the critical role of maintaining orthogonality. To balance overhead and performance, we find that using 5 Neumann terms is good trade-off. Scheme = 1 Not converged = 2 = 3 = 4 = 5 Table 5: Number of terms in Neumann series. 22.56 21.54 20.22 20.19 Perplexity Additionally, it is important to evaluate the accuracy of the Neumann approximation to understand how the number of Neumann terms affects the preservation of orthogonality. The orthogonal approximation error is defined by eorth = RRT IF /IF . We randomly select weight matrix and compute the approximation error of two orthogonal matrices and that correspond to it. For clarity, we only visualize the error in the initial 1000 training steps in Figure 8. We can observe that, with more Neumann terms, the orthogonal approximation error is indeed lower. We also note that the merge-then-reinitialize trick periodically resets the error. Figure 8: Approximation error of orthogonal matrices and of weight matrix. XSum # Params CNN/DailyMail Method LoRA (r=32) 17.30 43.38/20.20/35.25 43.17/20.31/29.72 OFT (b=64) 8.52 44.12/20.96/36.01 44.08/21.02/30.68 Full FT 406.29 45.14/22.27/37.25 44.16/21.28/40.90 POET(FS,b=1/2) 144.57 45.23/22.41/37.28 44.27/21.29/41.02 POET for finetuning. To demonstrate the applicability of POET to general finetuning tasks, we apply it to finetune BART-large model [39] on the NLP task of text summarization. Specifically, we evaluate POET on the XSum [59] and CNN/DailyMail [22] datasets, reporting ROUGE-1/2/L scores in Table 6. We note that both LoRA and OFT are designed solely for parameter-efficient finetuning and are not applicable to pretraining. Our goal here is to demonstrate that POET is also effective as finetuning method. For consistency, we use the same configuration as in the pretraining setup, resulting in higher parameter count. Experimental results show that POET not only supports finetuning effectively but also outperforms both full-model finetuning and parameter-efficient methods. Table 6: Finetuning BART-large on XSum and CNN/DailyMail for text summarization. We report ROUGE-1/2/L results (higher is better)."
        },
        {
            "title": "6 Related Work and Concluding Remarks",
            "content": "Related work. Inspired by low-rank adaptation methods such as LoRA [24], number of recent approaches [10, 19, 25, 2830, 4143, 51, 56, 68, 77, 78] have explored low-rank structures to enable efficient pretraining of large language models (LLMs). In parallel, sparsity has also been extensively studied as means to improve training efficiency in neural networks [8, 13, 14, 23, 69, 74]. Compared to approaches that exploit low-rank structures, relatively few works have explored sparsity for pretraining. Our work broadly aligns with the sparse training paradigm, as POET leverages sparsely optimized orthogonal matrices to enhance training efficiency. parallel line of research [32, 44, 58, 66, 76] focuses on developing efficient optimizers for large-scale neural networks. While our work also targets efficient training of large models, it is orthogonal to these efforts, as POET can be integrated with any optimizer. The way POET uses orthogonal matrices to transform neurons may also relate to preconditioned optimizers such as Muon [32], Shampoo [18] and SOAP [70], as well as to the broader field of manifold optimization (e.g., [6]). POET-trained weight matrices remain statistically indistinguishable from randomly initialized ones due to the isotropy of zeromean independent Gaussian distributions. This yields interesting connections to random neural networks [20, 38, 38, 60, 71], random geometry [21], and random matrix theory [15]. Concluding remarks. This paper introduces POET, reparameterized training algorithm for large language models. POET models each neuron as the product of two orthogonal matrices and fixed random weight matrix. By efficiently learning large orthogonal transformations, POET achieves superior generalization while being much more parameter-efficient than existing LLM pretraining methods. Experiments show that POET is broadly applicable to both pretraining and finetuning tasks."
        },
        {
            "title": "References",
            "content": "[1] Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix factorization. In NeurIPS, 2019. 7 [2] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. 1 [3] Nitin Bansal, Xiaohan Chen, and Zhangyang Wang. Can we gain more from orthogonality regularizations in training deep networks? In NeurIPS, 2018. 1 [4] Line Baribeau and Thomas Ransford. Non-linear spectrum-preserving maps. Bulletin of the London Mathematical Society, 32(1):814, 2000. 3 [5] Peter Bartlett, Dylan Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural networks. In NeurIPS, 2017. 1, 7 [6] Silvere Bonnabel. Stochastic gradient descent on riemannian manifolds. IEEE Transactions on Automatic Control, 58(9):22172229, 2013. 9 [7] Djalil Chafaı, Djalil Chafä, Olivier Guédon, Guillaume Lecue, and Alain Pajor. Singular values of random matrices. Lecture Notes, 13, 2009. 17 [8] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher Ré. Scatterbrain: Unifying sparse and low-rank attention. In NeurIPS, 2021. 9 [9] Tianlong Chen, Zhenyu Zhang, Yu Cheng, Ahmed Awadallah, and Zhangyang Wang. The principle of diversity: Training stronger vision transformers calls for reducing all levels of redundancy. In CVPR, 2022. 1 [10] Xi Chen, Kaituo Feng, Changsheng Li, Xunhao Lai, Xiangyu Yue, Ye Yuan, and Guoren Wang. Fira: Can we achieve full-rank training of llms under low-rank constraint? arXiv preprint arXiv:2410.01623, 2024. 9 [11] Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval networks: Improving robustness to adversarial examples. In ICML, 2017. 1 [12] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. 1 [13] Tri Dao, Beidi Chen, Nimit Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander Liu, Aniruddh Rao, Atri Rudra, and Christopher Ré. Monarch: Expressive structured matrices for efficient and accurate training. In ICML, 2022. 9 [14] Tri Dao, Albert Gu, Matthew Eichhorn, Atri Rudra, and Christopher Ré. Learning fast algorithms for linear transforms using butterfly factorizations. In ICML, 2019. 9 [15] Alan Edelman and Raj Rao. Random matrix theory. Acta numerica, 14:233297, 2005. [16] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In AISTATS, 2010. 4, 8 [17] Suriya Gunasekar, Blake Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Implicit regularization in matrix factorization. In NeurIPS, 2017. 7 [18] Vineet Gupta, Tomer Koren, and Yoram Singer. Shampoo: Preconditioned stochastic tensor optimization. In ICML, 2018. [19] Andi Han, Jiaxiang Li, Wei Huang, Mingyi Hong, Akiko Takeda, Pratik Jawanpuria, and Bamdev Mishra. Sltrain: sparse plus low-rank approach for parameter and memory efficient pretraining. arXiv preprint arXiv:2406.02214, 2024. 9 [20] Boris Hanin. Random neural networks in the infinite width limit as gaussian processes. The Annals of Applied Probability, 33(6A):47984819, 2023. 9 [21] Boris Hanin and David Rolnick. Complexity of linear regions in deep networks. In ICML, 2019. 9 [22] Karl Moritz Hermann, Tomáš Koˇciský, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend, 2015. 10 [23] Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks. JMLR, 2021. 9 [24] Edward J. Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In ICLR, 2022. 8, 9 [25] Jia-Hong Huang, Yixian Shen, Hongyi Zhu, Stevan Rudinac, and Evangelos Kanoulas. Gradient weightnormalized low-rank projection for efficient llm training. In AAAI, 2025. 9 [26] Lei Huang, Xianglong Liu, Bo Lang, Adams Yu, Yongliang Wang, and Bo Li. Orthogonal weight normalization: Solution to optimization over multiple dependent stiefel manifolds in deep neural networks. In AAAI, 2018. 1 [27] Tianjin Huang, Ziquan Zhu, Gaojie Jin, Lu Liu, Zhangyang Wang, and Shiwei Liu. Spam: Spike-aware adam with momentum reset for stable llm training, 2025. 7 [28] Weihao Huang, Zhenyu Zhang, Yushun Zhang, Zhi-Quan Luo, Ruoyu Sun, and Zhangyang Wang. Galoremini: Low rank gradient learning with fewer learning rates. In NeurIPS Workshop on Fine-Tuning in Modern Machine Learning: Principles and Scalability, 2024. 9 [29] Minyoung Huh, Brian Cheung, Jeremy Bernstein, Phillip Isola, and Pulkit Agrawal. Training neural networks from scratch with parallel low-rank adapters. arXiv preprint arXiv:2402.16828, 2024. [30] Ajay Jaiswal, Lu Yin, Zhenyu Zhang, Shiwei Liu, Jiawei Zhao, Yuandong Tian, and Zhangyang Wang. From galore to welore: How low-rank weights non-uniformly emerge from low-rank gradients. arXiv preprint arXiv:2407.11239, 2024. 9 [31] Haoming Jiang, Zhehui Chen, Minshuo Chen, Feng Liu, Dingding Wang, and Tuo Zhao. On computation and generalization of generative adversarial networks under spectrum control. In ICLR, 2019. 1 [32] Keller Jordan, Yuchen Jin, Vlado Boza, Jiacheng You, Franz Cesista, Laker Newhouse, and Jeremy Bernstein. Muon: An optimizer for hidden layers in neural networks, 2024. [33] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. 8 [34] Nitish Shirish Keskar and Richard Socher. Improving generalization performance by switching from adam to sgd. arXiv preprint arXiv:1712.07628, 2017. 1 [35] Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. In ICLR, 2015. 1 [36] Günter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing neural networks. In NeurIPS, 2017. [37] Hojoon Lee, Youngdo Lee, Takuma Seno, Donghu Kim, Peter Stone, and Jaegul Choo. Hyperspherical normalization for scalable deep reinforcement learning. arXiv preprint arXiv:2502.15280, 2025. 1 [38] Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel Schoenholz, Jeffrey Pennington, and Jascha Sohl-Dickstein. Deep neural networks as gaussian processes. arXiv preprint arXiv:1711.00165, 2017. 9 [39] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. 9 [40] Chi-Kwong Li and Nam-Kiu Tsing. Linear operators preserving unitarily invariant norms of matrices. Linear and Multilinear Algebra, 26(1-2):119132, 1990. 3 [41] Vladislav Lialin, Namrata Shivagunde, Sherin Muckatira, and Anna Rumshisky. Relora: High-rank training through low-rank updates. arXiv preprint arXiv:2307.05695, 2023. 9 [42] Kaizhao Liang, Bo Liu, Lizhang Chen, and Qiang Liu. Memory-efficient llm training with online subspace descent. arXiv preprint arXiv:2408.12857, 2024. [43] Xutao Liao, Shaohui Li, Yuhui Xu, Zhi Li, Yu Liu, and You He. Galore+: Boosting low-rank adaptation for llms with cross-head projection. arXiv preprint arXiv:2412.19820, 2024. 9 [44] Jingyuan Liu, Jianlin Su, Xingcheng Yao, Zhejun Jiang, Guokun Lai, Yulun Du, Yidao Qin, Weixin Xu, Enzhe Lu, Junjie Yan, et al. Muon is scalable for llm training. arXiv preprint arXiv:2502.16982, 2025. 11 [45] Weiyang Liu, Rongmei Lin, Zhen Liu, Lixin Liu, Zhiding Yu, Bo Dai, and Le Song. Learning towards minimum hyperspherical energy. In NeurIPS, 2018. 1, 2, 3 [46] Weiyang Liu, Rongmei Lin, Zhen Liu, James Rehg, Liam Paull, Li Xiong, Le Song, and Adrian Weller. Orthogonal over-parameterized training. In CVPR, 2021. 2, 3, 4, 5, 6 [47] Weiyang Liu, Rongmei Lin, Zhen Liu, Li Xiong, Bernhard Schölkopf, and Adrian Weller. Learning with hyperspherical uniformity. In AISTATS, 2021. 1, 2, 3, 17 [48] Weiyang Liu, Zhen Liu, Zhiding Yu, Bo Dai, Rongmei Lin, Yisen Wang, James Rehg, and Le Song. Decoupled networks. In CVPR, 2018. 1, 4 [49] Weiyang Liu, Zeju Qiu, Yao Feng, Yuliang Xiu, Yuxuan Xue, Longhui Yu, Haiwen Feng, Zhen Liu, Juyeon Heo, Songyou Peng, Yandong Wen, Michael J. Black, Adrian Weller, and Bernhard Schölkopf. Parameter-efficient orthogonal finetuning via butterfly factorization. In ICLR, 2024. 2, 3, 5, 6 [50] Weiyang Liu, Yan-Ming Zhang, Xingguo Li, Zhiding Yu, Bo Dai, Tuo Zhao, and Le Song. Deep hyperspherical learning. In NIPS, 2017. 1, [51] Ziyue Liu, Ruijie Zhang, Zhengyang Wang, Zi Yang, Paul Hovland, Bogdan Nicolae, Franck Cappello, and Zheng Zhang. Cola: Compute-efficient pre-training of llms via low-rank activation. arXiv preprint arXiv:2502.10940, 2025. 9 [52] Ilya Loshchilov, Cheng-Ping Hsieh, Simeng Sun, and Boris Ginsburg. ngpt: Normalized transformer with representation learning on the hypersphere. arXiv preprint arXiv:2410.01131, 2024. 1 [53] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. 1, 22 [54] Chao Ma, Wenbo Gong, Meyer Scetbon, and Edward Meeds. Swan: Sgd with normalization and whitening enables stateless llm training, 2025. [55] Albert Marshall, Ingram Olkin, and Barry Arnold. Inequalities: theory of majorization and its applications, volume 143. Springer, 1979. 18 [56] Roy Miles, Pradyumna Reddy, Ismail Elezi, and Jiankang Deng. Velora: Memory efficient training using rank-1 sub-token projections. arXiv preprint arXiv:2405.17991, 2024. 9 [57] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018. [58] Zhanfeng Mo, Long-Kai Huang, and Sinno Jialin Pan. Parameter and memory efficient pretraining via low-rank riemannian optimization. In ICLR, 2025. 9 [59] Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Dont give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization, 2018. 9 [60] Radford Neal. Priors for infinite networks. Bayesian learning for neural networks, pages 2953, 1996. [61] Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. pac-bayesian approach to spectrallynormalized margin bounds for neural networks. In ICLR, 2018. 7 [62] Sean ORourke, Van Vu, and Ke Wang. Eigenvectors of random matrices: survey. Journal of Combinatorial Theory, Series A, 144:361442, 2016. 17 [63] Zeju Qiu, Weiyang Liu, Haiwen Feng, Yuxuan Xue, Yao Feng, Zhen Liu, Dan Zhang, Adrian Weller, and Bernhard Schölkopf. Controlling text-to-image diffusion by orthogonal finetuning. In NeurIPS, 2023. 2, 3, 5, 6 [64] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. 7, [65] Mihaela Rosca, Theophane Weber, Arthur Gretton, and Shakir Mohamed. case for new neural network smoothness constraints. arXiv preprint arXiv:2012.07969, 2020. 1 [66] Ishaan Shah, Anthony Polloreno, Karl Stratos, Philip Monk, Adarsh Chaluvaraju, Andrew Hojel, Andrew Ma, Anil Thomas, Ashish Tanwer, Darsh Shah, et al. Practical efficiency of muon for pretraining. arXiv preprint arXiv:2505.02222, 2025. 9 12 [67] Jack Silverstein et al. The smallest eigenvalue of large dimensional wishart matrix. The Annals of Probability, 13(4):13641368, 1985. [68] DiJia Su, Andrew Gu, Jane Xu, Yuandong Tian, and Jiawei Zhao. Galore 2: Large-scale llm pre-training by gradient low-rank projection. arXiv preprint arXiv:2504.20437, 2025. 9 [69] Vithursan Thangarasa, Abhay Gupta, William Marshall, Tianda Li, Kevin Leong, Dennis DeCoste, Sean Lie, and Shreyas Saxena. Spdf: Sparse pre-training and dense fine-tuning for large language models. In UAI, 2023. 9 [70] Nikhil Vyas, Depen Morwani, Rosie Zhao, Mujin Kwun, Itai Shapira, David Brandfonbrener, Lucas Janson, and Sham Kakade. Soap: Improving and stabilizing shampoo using adam. arXiv preprint arXiv:2409.11321, 2024. 9 [71] Gilles Wainrib and Jonathan Touboul. Topological and dynamical complexity of random neural networks. Physical review letters, 110(11):118101, 2013. [72] Bo Xie, Yingyu Liang, and Le Song. Diverse neural network learns true target functions. In AISTATS, 2017. 3, 7 [73] Sherry Yang, Ofir Nachum, Yilun Du, Jason Wei, Pieter Abbeel, and Dale Schuurmans. Foundation models for decision making: Problems, methods, and opportunities. arXiv preprint arXiv:2303.04129, 2023. 1 [74] Xinyu Yang, Jixuan Leng, Geyang Guo, Jiawei Zhao, Ryumei Nakada, Linjun Zhang, Huaxiu Yao, and Beidi Chen. S2ft: Efficient, scalable and generalizable llm fine-tuning by structured sparsity. arXiv preprint arXiv:2412.06289, 2024. 9 [75] Yuichi Yoshida and Takeru Miyato. Spectral norm regularization for improving the generalizability of deep learning. arXiv preprint arXiv:1705.10941, 2017. 1 [76] Yushun Zhang, Congliang Chen, Ziniu Li, Tian Ding, Chenwei Wu, Diederik Kingma, Yinyu Ye, Zhi-Quan Luo, and Ruoyu Sun. Adam-mini: Use fewer learning rates to gain more. arXiv preprint arXiv:2406.16793, 2024. 9 [77] Zhenyu Zhang, Ajay Jaiswal, Lu Yin, Shiwei Liu, Jiawei Zhao, Yuandong Tian, and Zhangyang Wang. Q-galore: Quantized galore with int4 projection and layer-adaptive low-rank gradients. arXiv preprint arXiv:2407.08296, 2024. 9 [78] Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian. Galore: Memory-efficient llm training by gradient low-rank projection. In ICML, 2024. 6, 7, 9, 22 13 . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 15 15 16 17 20 24 25 27 28"
        },
        {
            "title": "Table of Contents",
            "content": "A Delving into POETs Three Training Phases A.1 More Details on Vector Probing . . . A.2 Geometric Interpretation of the Trace of Orthogonal Matrices . . . A.3 Empirical Observations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "B Guarantees of Weight Spectrum under POET",
            "content": "C Proofs of Lemma"
        },
        {
            "title": "J Full Results of Training Dynamics",
            "content": "14 Delving into POETs Three Training Phases A.1 More Details on Vector Probing The three training phases of POET are summarized from the empirical observation of the vector probing results. The idea of vector probing is very straightforward. We generate constant vector that is randomly initialized. Then we let it to be transformed by the learned orthogonal matrices and . Finally, we compute the cosine of their angle: vRv and vP v. In this process, the probing vector is always fixed. The full results are given in Appendix F. Beyond particular constant probing vector, we also consider set of randomly sampled probing vectors that follow our proposed normalized Gaussian initialization. Specifically, we consider the following expectation: EvSm1{vRv}, (8) where is vector initialized by normalized Gaussian distribution (thus uniformly distributed on unit hypersphere Sm1). Because E{vv} = EvSm1{vRv} = , then we have that 1 Tr(R). where Tr() denotes the matrix trace. Its geometric interpretation is the cosine of the rotation angle between and Rv. Next, we look into the variance of q(x) = vRv (we simplify the expectation over the unit hypersphere to E): Var(q(x)) = E{(vRv)2} (E{vRv})2. First we compute E{(vRv)2}: E{(vRv)2} = = Tr(R)2 + 2 (cid:13) (cid:13) (cid:13) R+R 2 (cid:13) (cid:13) (cid:13) m(m + 2) Tr(R)2 + Tr(R2) + m(m + 2) Then we compute (E{vRv})2: Finally, we combine pieces and have the final variance: (E{vRv})2 = Tr(R)2 m2 . Var(vRv) = + Tr(R2) + 2Tr(R)2 m(m + 2) (9) (10) (11) (12) (13) which shrinks at the order of O(1/m). Therefore, when the dimension of orthogonal matrices is large, even if we use fixed random probing vector v, this rotation angle is quite consistent. A.2 Geometric Interpretation of the Trace of Orthogonal Matrices Lets delve deeper into the trace of orthogonal matrices. It generally represents how much transformation preserves vectors in their original directions. Specifically, the trace indicates how much alignment or similarity there is between the original vectors and their images after transformation. The trace of an orthogonal matrix Rmm can be written as Tr(R) = (cid:88) i=1 Rei (14) where ei, are unit basis vectors. This expression reveals that the trace measures the sum of inner products between each original direction ei and its transformed version Rei. Since Rei can be interpreted as the cosine of the angle between ei and Rei, the trace thus reflects how much the orthogonal transformation aligns with or deviates from the original coordinate directions. We also plot the trace of both and during the POET training. The results are shown in Figure 11 and Figure 12. After dividing the trace by the orthogonal matrix dimension, we obtain that the result is generally in the range of [0.6, 0.65] after training. This is similar to the results of vector probing. Therefore, we empirically verify the conclusion that the expectation of vector probing results is Tr(R) with small variance. A.3 Empirical Observations The training dynamics of POET presents three geometry-driven phases. We note that these phase changes are based on empirical observation, and further theoretical understanding of this process remains an open problem. Phase I: conical-shell searching rotates each orthogonal matrix and smoothly away from the identity while preserving their singular values, so the cosine similarity between transformed and initial weight vectors falls from 1 to 0.6; this provides spectrally well-conditioned cone in which learning can proceed safely. this phase serves the role of spectral warm-up. By plotting the cosine similarity of any one layer, we always see the same graceful slide towards 0.60.65, independent of model size, layer type, or whether you train with fully-stochastic or block-stochastic SPO. This phase carves out the thin shell in which subsequent learning lives. Phase II: stable learning on the conical shell occupies the bulk of training: the angles to the initial vectors stay locked in that narrow band, optimization now shears weights within the cone, and validation perplexity drops almost linearly because spectra remain frozen and gradients act only on meaningful directions. In this phase, the trace of the orthogonal matrices stay almost as constant. Specifically, we hypothesize that the orthogonal transforms have reached good cone; thereafter they mostly shear vectors inside that shell, leaving the angle to the original vector unchanged. The spectrum continues to be exactly that of the random initial matrix, so gradients can no longer distort singular values and instead devote capacity to learning meaningful directions. Because the geometry is stabilized in this phase, the learning of patterns happen in stable subspace. This stable learning phase takes up 80% of the training time. Phase III: final adjusting coincides with learning-rate decay; the orthogonal transforms barely move, making only tiny refinements to singular vectors, so additional steps yield diminishing returns. This phase is merely the LR cooldown; weights and spectra are already near their final configuration, so progress naturally slows."
        },
        {
            "title": "B Guarantees of Weight Spectrum under POET",
            "content": "For standard Gaussian initialization where each element of the weight matrix is sampled with normal distribution, we have the following standard results [7, 67]: 1 1 σmax(W ) a.s. 1 + σmin(W ) a.s. 1 λ λ (15) which gives spectrum guarantees for weight matrices generated by the standard Gaussian initialization. In the following, we give the spectrum guarantees for the normalized Gaussian initialization. We start by stating the following theorem from [47]: Theorem 2. Let v1, , vn Rd be i.i.d. random vectors where each element follows the Gaussian distribution with mean 0 and variance 1. Then v1 = v1 , , vn = vn are uniformly distributed v12 vn2 on the unit hypersphere Sd1. If the ratio converges to constant λ (0, 1), asymptotically we have for = {v1, , vn} Rdn: lim σmax(W ) ( lim σmin(W ) ( + λd) (max λd) (min ) (16) 1 vi2 1 vi2 ) where σmax() and σmin() denote the largest and the smallest singular value of matrix, respectively. Proof. We first introduce the following lemma as the characterization of unit vector that is uniformly distributed on the unit hypersphere Sd1. Lemma 2 ([62]). Let be random vector that is uniformly distributed on the unit hypersphere Sd1. Then has the same distribution as the following: (cid:26) (cid:27) u1 (cid:113)(cid:80)d , u2 (cid:113)(cid:80)d i=1 u2 i=1 u2 , , ud (cid:113)(cid:80)d i=1 u2 where u1, u2, , ud are i.i.d. standard normal random variables. Proof. The lemma follows naturally from the fact that the Gaussian vector {ui}d invariant. i=1 is rotationally Then we consider random matrix = {v1, , vn} where vi follows the same distribution of {u1, , ud}. Therefore, it is also equivalent to random matrix with each element distributed normally. For such matrix , we have from [67] that lim σmax( ) = lim σmin( ) = + λd λd where σmax() and σmin() denote the largest and the smallest singular value, respectively. Then we write the matrix as follows: = 1 v12 = 0 ... 0 1 v22 . . . . . . . . . 0 0 0 ... 1 vn2 17 (17) (18) (19) which leads to lim lim σmax(W ) = lim σmin(W ) = lim σmax( Q) . σmin( Q) (20) We fist assume that for symmetric matrix Rnn λ1(A) λn(A). Then we introduce the following inequalities for eigenvalues: Lemma 3 ([55]). Let G, Rnn be positive semi-definite symmetric, and let 1 i1 < < ik n. Then we have that and (cid:89) t= λit(GH) (cid:89) t=1 λit(G)λt(H) (cid:89) t= λit(GH) (cid:89) t=1 λit(G)λnt+1(H) (21) (22) where λi denotes the i-th largest eigenvalue. We first let 1 i1 < < ik n. Because Rdn and Rnn, we have the following: (cid:89) t=1 σit( Q) = = (cid:89) (cid:113) t=1 (cid:118) (cid:117) (cid:117) (cid:116) (cid:89) t=1 λit( QQ ) λit( W QQ) by applying Lemma 3 to the above equation, we have that (cid:118) (cid:117) (cid:117) (cid:116) (cid:89) t=1 λit( W QQ) (cid:118) (cid:117) (cid:117) (cid:116) (cid:89) t=1 λit( W )λnt+1(QQ) = (cid:89) t=1 σit( )σnt+1(Q) (cid:118) (cid:117) (cid:117) (cid:116) (cid:89) t=1 λit( W QQ) (cid:118) (cid:117) (cid:117) (cid:116) (cid:89) t=1 λit( W )λt(QQ) Therefore, we have that = (cid:89) t=1 σit( )σt(Q) (cid:89) t=1 σit( Q) (cid:89) t=1 σit( )σnt+1(Q) (cid:89) t=1 σit( Q) (cid:89) t=1 σit( )σt(Q) Suppose we have = 1 and i1 = n, then Eq. (26) gives σn( Q) σn( )σn(Q) Then suppose we have = 1 and i1 = 1, then Eq. (27) gives σ1( Q) σ1( )σ1(Q) 18 (23) (24) (25) (26) (27) (28) (29) Combining the above results with Eq. (18) and Eq. (20), we have that lim σmax(W ) = lim lim σmin(W ) = lim σmax( Q) lim (cid:0)σmax( ) σmax(Q)(cid:1) = ( + λd) max σmin( Q) lim (cid:0)σmin( ) σmin(Q)(cid:1) = ( λd) min 1 vi2 1 vi2 which concludes the proof."
        },
        {
            "title": "Combing with the fact that",
            "content": "we essentially have that lim max vi2 = lim min vi2 = 1, lim lim σmax(W ) 1 + σmin(W ) 1 λ, λ. which can be written to the following results: σmax(W ) σmin(W ) a.s. a.s. λ λ 1 + 1 (30) (31) (32) (33) which shows that under our proposed normalized Gaussian initialization, the maximal and minimal singular values are well bounded by constant that is only dependent on the size of weight matrix. These results justify the effectiveness of our proposed normalized Gaussian initialization in POET. 19 Proofs of Lemma 1 Proof of Lemma 1. We consider an orthogonal matrix and orthogonal primitives Gi corresponding to uniformly random subsets Sj [m] of size as explained in the main text (see equation (4)). The main claim we need to prove is that given any vector Rm and set [m] with [m] we can find an orthogonal primitive matrix corresponding to the set such that (Gv)l = 0 for with > (Gv)k 0 (Gv)l = vl for / S. Moreover, for all Rm with wi = 0 for the relation Gw = (34) (35) holds. We can assume that the matrix D(S) = {e(s1), . . . , e(sb)} contains the entries si in ascending order. Then we write D(S)v = (cid:19) (cid:18)v1 v2 (36) where v1 Rb1 corresponds to the entries si with si < and v2 Rb2 to the remaining entries, in particular sb1+1 = because S. It is well known that for every vector there is rotation aligning with the first standard basis vector, i.e., such that Qv = λe(1) for some λ 0. Consider such matrix for the vector v2 and then define the orthogonal matrix = (cid:18) 1b1 0b2b (cid:19) . 0b1b2 (37) Careful inspection of (4) implies that the last part of (34) is actually true for any as the second term has rows with all entries equal to zero for all / S. For the first part we find (cid:19) (cid:19) D(S) GD(S)v = D(S) = D(S) e(si)(v1)i + λe(k). (38) (cid:18)v1 v2 (cid:18) v1 λe(1) = (cid:88) ib1 Here we used sb1+1 = in the last step. Since in addition ((1m D(S) 1b D(S))v)l = 0 for all we conclude that indeed (Gv)l = 0 for and > k, (Gv)k 0. The remaining statement (35) follows from the observation that when decomposing as in (36) we find (39) (D(S))w = (cid:19) (cid:18) w1 0b2 (because wi = 0 for k) and therefore ( 1b)(D(S))w = 0b by definition of and we find Gw = w. (40) (41) The rest of the proof is straightforward by induction combined with simple coin collector problem. For the rest of the proof it is convenient to reverse the indices, i.e., to consider products Gc . . . G1 Assume that we have chosen Gi for ck and some ck such that the product = Gck . . . G1 (42) l,k = 0 for all < and > and satisfies k,k 0 for < k. Let ck+1 ck + α(m/b)2 ln(m). Then, we can bound for any > the probability that there is no ck < ck+1 such that {k, l} Sj using that Sj follows uniform i.i.d. distribution by P( ck < ck+1 : k, Sj) (cid:18) 1 b2 m2 (cid:19)ck+1ck (cid:18) exp b2 m2 α (cid:19) m2 b2 ln(m) = mα. (43) 20 The union bound implies that with probability at least 1 mα+1 there is for all > ck < ck+1 such that {k, l} Sj. If this holds we set Gj for ck < ck1 as constructed above if Sj and Gj = 1m otherwise. This then ensures that k+1 = Gck+1 . . . G1 (44) satisfies k+1 l,k = 0 for and > k. For < this follows from (35) and for = from (34). We conclude by the union bound that is an upper triangular matrix with non-negative diagonal entries with probability at least 1 mmα+1 = 1 m(α2). But we also know that is orthogonal and therefore satisfies = 1m and we thus find Gcm . . . G1 = R. (45) Next we give heuristic that actually O(ln(m)m2/b2) terms are sufficient to express every orthogonal map as product of stochsastic primitives. For fixed we consider the map Φ : O(b)c O(m) Φ( G1, . . . , Gc) = (cid:89) j= Gj. (46) If α ln(m)m2/b2 we have that with probability at least 1 m(α2) for all k, [m] there is such that k, Sj. Assume that this is the case. Recall that the tangent space of O(k) at the identity is the space of skew-symmetric matrices. Consider tangent vector (X1, . . . , Xc) with Xi Skew(k). Then DΦ(1b, . . . , 1b)(X1, . . . , Xc) = (cid:88) j= D(Sj) Xj D(Sj). (47) This is surjective map on Skew(m) under the condition that for all k, [m] there is such that k, Sj. We can therefore conclude that the image of Φ contains neighbourhood of the identity. Moreover, since Φ is polynomial map, DΦ is surjective everywhere except for variety of codimension one. While this is not sufficient to conclude that the image of Φ is O(d) or dense in O(d) it provides some indication that this is the case."
        },
        {
            "title": "D Experimental Details",
            "content": "Parameter Llama 60M Llama 130M Llama 350M Llama 1.3B Hidden dimension Intermediate dimension Number of attention heads Number of hidden layers 512 8 8 768 2048 12 12 1024 16 24 2048 5376 32 24 Table 7: Model architectures for different Llama variants."
        },
        {
            "title": "Model",
            "content": "Spec. # GPU lr (base) lr (POET) training steps batch size grad acc. Llama 60M Llama 130M Llama 350M Llama 1.3B = 1/2 = 1/4 = 1/ = 1/2 = 1/4 = 1/8 = 1/2 = 1/4 = 1/8 = 1/2 = 1/4 = 1/ 1 1 1 1 1 1 4 4 8 8 8 1e-2 1e-2 1e-2 5e-3 5e-3 5e5e-3 5e-3 5e-3 1e-3 1e-3 1e-3 1e-3 2e-3 4e1e-3 2e-3 4e-3 1e-3 2e-3 4e-3 1e-3 2e-3 4e300,000 300,000 300,000 400,000 400,000 400,000 400,000 400,000 400, 500,000 500,000 500,000 Table 8: Hyper-parameter setup of POET-FS. 256 256 256 128 128 128 128 128 64 64 64 2 2 2 2 2 1 1 1 1 1 This section outlines our experimental setup, including the codebase, datasets, and computational resources used. Code framework. Our method is implemented on top of the codebase from [78]1 (Apache 2.0 license), which we also use to reproduce the AdamW and GaLore baselines. We will release our code for reproducing all training results prior to publication. Training details. We employed the AdamW optimizer [53] for all our training runs. The specific hyperparameters used for each experiment are detailed in the Table 8 and Table 9 referenced below. We use the consine learning rate scheduler with the minimum learning ratio of 0.01. We use the number of warmup steps of 0, weight decay of 0.01 and gradient clipping of 0.1. For the AdamW baseline, we report results for the optimal learning rate from [1102, 5103, 1103, 5104, 1104, 5105, 1105]. After each merge-thenreinitalize step, we additionally increase the gradient clipping for 10 training steps to improve training stability. Model architecture. Our work utilized the Hugging Face Transformers2 code base to construct the Llama model for pertaining, which is under the Apache 2.0 license. The specific layer setups for the different scaled Llama models are summarized in Table 7. Note, the intermediate dimension of the Feed-Forward Network (FFN) has been slightly modified for the POET-BS, compared to the configs in [78], because the linear layer dimensions have to be divisible by the POET-BS block size b. Dataset. We use the Colossal Clean Crawled Corpus (C4) dataset [64] for pertaining. The C4 data is large-scale, meticulously cleaned version of Common Crawls web crawl corpus. It was 1https://github.com/jiaweizzhao/GaLore 2https://github.com/huggingface/transformers"
        },
        {
            "title": "Model",
            "content": "Spec. # GPU lr (base) lr (POET) training steps batch size grad acc. Llama 60M Llama 130M Llama 350M Llama 1.3B = 256 = 128 = = 256 = 128 = 64 = 256 = 128 = 64 = 256 = 128 = 1 1 1 1 1 1 4 4 8 8 8 1e-2 1e-2 1e-2 5e-3 5e-3 5e5e-3 5e-3 5e-3 1e-3 1e-3 1e-3 1e-3 2e-3 4e1e-3 2e-3 4e-3 1e-3 2e-3 4e-3 1e-3 2e-3 4e300,000 300,000 300,000 400,000 400,000 400,000 400,000 400,000 400, 500,000 500,000 500,000 Table 9: Hyper-parameter setup of POET-BS. 256 256 256 256 256 128 128 128 64 64 64 2 2 2 2 2 1 1 1 1 1 originally introduced for training the Text-to-Text Transfer Transformer (T5) model and has since become standard pre-training dataset for testing training algorithms for pre-training large language models. The dataset is released under the ODC-BY license. Compute Resources. All the training tasks are performed on NVIDIA HGX H100 8-GPU System node with 80GB memory each. Depending on the model scale, we train on 1, 4 or 8 GPUs."
        },
        {
            "title": "E Implementation and CUDA Acceleration",
            "content": "To enable efficient POET training, we implement the CayleyNeumann parameterization. To reduce memory usage, we leverage the structure of the skew-symmetric matrix Rnn, where the diagonal entries are zero (Qii = 0) and off-diagonal elements satisfy Qij = Qji. This structure allows us to store only the upper triangular part of as vector, reducing the number of trainable parameters from n2 to n(n 1)/2. During the forward pass, is reconstructed on-the-fly using specialized CUDA kernel, significantly accelerating this process. In addition, the Neumann approximation removes the need for costly and numerically unstable matrix inversion, offering further computational gains. Overall, training 1.3B LLaMA model on single H100 8-GPU node yields 3.8 speedup over the baseline (i.e., native implementation). Table 10 summarizes the contribution of each component to the overall training time."
        },
        {
            "title": "Design",
            "content": "Neumann approximation Skew-symmetric CUDA kernel Total Speed-Up 1.5 1.3 3.8 Table 10: Method design and clock time speed-up."
        },
        {
            "title": "F Results of Vector Probing for R and P",
            "content": "In this ablation study, we perform vector probing on the orthogonal matrices Rmm, Rnn for all linear layers for all blocks of 60M Llama model trained with POET-FS. The cosine similarity results are reported in Figure 9 and Figure 10, and the trace results are reported in Figure 11 and Figure 12. Since we want to understand the learning dynamics of the orthogonal matrices, we employ = 1 with POET learning rate of 5104 to eliminate the need for resampling and reinitialization of the orthogonal matrices. Interestingly, we observe this three-phased learning dynamics across different types of linear layers and different-depth transformer blocks. Figure 9: Cosine similarity for vector probing of across the self-attention components (query, key, value, and output projections) and feed-forward network components (up-, down-, and gate-projections) in all transformer blocks of POET-trained Llama 60M model. Figure 10: Cosine similarity for vector probing of across the self-attention components (query, key, value, and output projections) and feed-forward network components (up-, down-, and gate-projections) from all transformer blocks of POET-trained Llama 60M model. 25 Figure 11: Trace of across the self-attention components (query, key, value, and output projections) and feedforward network components (up-, down-, and gate-projections) from all transformer blocks of POET-trained Llama 60M model. Figure 12: Trace of across the self-attention components (query, key, value, and output projections) and feedforward network components (up-, down-, and gate-projections) from all transformer blocks of POET-trained Llama 60M model."
        },
        {
            "title": "G Weight Update Evenness of Different POET Variants",
            "content": "To understand the higher parameter efficiency of POET-BS compared to POET-FS, we employ toy example to visualize their different weight update mechanisms by counting the total number of updates for each element of the weight matrix. The visualization results are given in Figure 13 and Figure 14. Specifically, in this experiment, 6464 matrix was randomly initialized and trained for 100 steps under various POET-BS and POET-FS configurations. The merge-then-reinitialize trick is performed at each iteration, and the same set of weight elements was effectively updated between two successive merge-then-reinitialize operations. For each weight element, we compute its total number of update in these 100 steps. Given 100 training steps and updates from both and , each element of the weight matrix can be updated at most 200 times. This target is consistently achieved by POET-BS, and it is also agnostic to the block size. All POET-BS variants can enable the maximal number of updates for each weight element to be 200. In contrast, POET-FS results in significantly fewer updates per weight element, with updates also unevenly distributed. This unevenness arises from stochasticity, causing certain weights to be updated more frequently than others. While this is less problematic at large iteration counts, it can introduce unexpected training difficulties in earlier stages. (a) POET-BS (b = 8) (b) POET-BS (b = 16) (c) POET-BS (b = 32) Figure 13: Visualization of the weight update mechanism of POET-BS after 100 steps of update and Tm = 1. (a) POET-FS (b = 1/8) (b) POET-FS (b = 1/4) (c) POET-FS (b = 1/2) Figure 14: Visualization of the weight update mechanism of POET-FS after 100 steps of update and Tm = 1."
        },
        {
            "title": "H Training Dynamics of Singular Values",
            "content": "We conduct an ablation study to compare the training dynamics of singular values of weight matrices between AdamW and POET. The results of AdamW are given in Figure 15, Figure 16 and Figure 17. The results of POET are given in Figure 18, Figure 19 and Figure 20. 60M LLaMA model was trained for 50,000 iterations with an effective batch size of 512, using both AdamW and POET-FS (b = 1/2). The model was evaluated every 5,000 steps, and the singular value dynamics are computed by performing singular value decomposition on the weight matrices. For POET, merge-thenreinitialize step was applied before each evaluation. Training is finished at 50,000 steps, as the spectral norm of the AdamW-trained model plateaued at this point. Figure 15: Training dynamics of the singular values of weight matrices within Blocks 01 (the i-th row represents Block i) of 60M Llama Transformer trained with AdamW. 28 Figure 16: Training dynamics of the singular values of weight matrices within Blocks 24 (the i-th row represents Block i) of 60M Llama Transformer trained with AdamW. 29 Figure 17: Training dynamics of the singular values of weight matrices within Blocks 57 (the i-th row represents Block i) of 60M Llama Transformer trained with AdamW. 30 Figure 18: This plot illustrates the singular value training dynamics for individual weight matrices within Blocks 0-2 of 60M Llama transformer model trained with POET. For each block, the dynamics are shown for the self-attention components (query, key, value, and output projections) and the feed-forward network components (up-projection, down-projection, and gate-projection). 31 Figure 19: This plot illustrates the singular value training dynamics for individual weight matrices within Blocks 3-5 of 60M Llama transformer model trained with POET. For each block, the dynamics are shown for the self-attention components (query, key, value, and output projections) and the feed-forward network components (up-projection, down-projection, and gate-projection). 32 Figure 20: This plot illustrates the singular value training dynamics for individual weight matrices within Blocks 6-7 of 60M Llama transformer model trained with POET. For each block, the dynamics are shown for the self-attention components (query, key, value, and output projections) and the feed-forward network components (up-projection, down-projection, and gate-projection)."
        },
        {
            "title": "I Orthogonality Approximation Quality using Neumann Series",
            "content": "In this ablation study, we evaluate the approximation error of the orthogonal matrices Rmm and Rnn across all linear layers in Block 0 of 130M LLaMA model trained with POET-FS (b = 1/2) for 10,000 steps. Figure 21 and Figure 22 show the approximation error over the first 1,000 steps. Since the error difference between = 4 and = 5 was negligible, we used = 4 for better computational efficiency. Empirically, while = 2 or = 3 suffices for smaller LLaMA models, larger values are needed to avoid training divergence caused by exploding gradients due to approximation error. Figure 21: For the transformer block 0, we show approximation error of orthogonal matrix for the selfattention components (query, key, value, and output projections) and the feed-forward network components (up-projection, down-projection, and gate-projection). Figure 22: For the transformer block 0, we show approximation error of orthogonal matrix for the selfattention components (query, key, value, and output projections) and the feed-forward network components (up-projection, down-projection, and gate-projection). 34 Additionally, Figure 23 shows the orthogonality approximation error of Neumann series with different over the first 10,000 training steps, illustrating how it decreases as training progresses. We observe general downward trend in approximation error, indicating improved approximation over time. The results also suggest that using too few Neumann series terms (e.g., = 1) can lead to training divergence in POET. Figure 23: The approximation error of orthogonal matrix in randomly selected down-projection layer after training 10000 steps."
        },
        {
            "title": "J Full Results of Training Dynamics",
            "content": "We provide the full training dynamics of different POET variants under Llama 60M, Llama 130M, Llama 350M and Llama 1.3B in Figure 24. This figure is essentially an extended result of Figure 6. One can observe that the training dynamics of POET is quite different from AdamW, and more importantly, POET consistently yields better parameter-efficiency and generalization. (a) Llama 60M (b) Llama 130M (c) Llama 350M (d) Llama 1.3B Figure 24: Validation perplexity during the training of the LLama-based transformer with 60M, 130M, 350M and 1.3B parameters."
        }
    ],
    "affiliations": [
        "Max Planck Institute for Intelligent Systems, Tübingen",
        "The Chinese University of Hong Kong"
    ]
}