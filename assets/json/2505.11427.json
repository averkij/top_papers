{
    "paper_title": "Mergenetic: a Simple Evolutionary Model Merging Library",
    "authors": [
        "Adrian Robert Minut",
        "Tommaso Mencattini",
        "Andrea Santilli",
        "Donato Crisostomi",
        "Emanuele Rodolà"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Model merging allows combining the capabilities of existing models into a new one - post hoc, without additional training. This has made it increasingly popular thanks to its low cost and the availability of libraries that support merging on consumer GPUs. Recent work shows that pairing merging with evolutionary algorithms can boost performance, but no framework currently supports flexible experimentation with such strategies in language models. We introduce Mergenetic, an open-source library for evolutionary model merging. Mergenetic enables easy composition of merging methods and evolutionary algorithms while incorporating lightweight fitness estimators to reduce evaluation costs. We describe its design and demonstrate that Mergenetic produces competitive results across tasks and languages using modest hardware."
        },
        {
            "title": "Start",
            "content": "Mergenetic: Simple Evolutionary Model Merging Library Adrian Robert Minut1, Tommaso Mencattini2, Andrea Santilli1, Donato Crisostomi1, Emanuele Rodolà1 1Sapienza University of Rome 2Ecole Polytechnique Fédérale de Lausanne minut@di.uniroma1.it 5 2 0 M 6 1 ] . [ 1 7 2 4 1 1 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Model merging allows combining the capabilities of existing models into new one post hoc, without additional training. This has made it increasingly popular thanks to its low cost and the availability of libraries that support merging on consumer GPUs. Recent work shows that pairing merging with evolutionary algorithms can boost performance, but no framework currently supports flexible experimentation with such strategies in language models. We introduce Mergenetic1, an opensource library for evolutionary model merging. Mergenetic enables easy composition of merging methods and evolutionary algorithms, while incorporating lightweight fitness estimators to reduce evaluation costs. We describe its design and demonstrate that Mergenetic produces competitive results across tasks and languages using modest hardware."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in large language models (LLMs) have shown that merging previously fine-tuned models can yield new systems with complementary strengths often surpassing any single constituent (Yang et al., 2024). Rather than fully retraining from scratch or fine-tuning large foundation model for every new task, merging techniques compose knowledge that is already encoded in existing checkpoints (e.g., specialized domain knowledge, multilingual abilities, or skills). The accessibility of model merging has expanded significantly due to its inexpensive nature coupled with easy-to-use libraries like MergeKit (Goddard et al., 2024), enabling practitioners to produce competitive models from existing ones using standard consumer GPUs. Indeed, at the time of writing, approximately 30% of models on the Hugging Face Open LLM leaderboard (Fourrier et al., 2024) are merged models (Ilharco et al., 2022). denotes equal contribution. 1https://github.com/tommasomncttn/mergenetic 1 Figure 1: Mergenetic makes it easy to produce new state-of-the-art LLMs with minimal requirements. Recent research has shown that combining model merging with evolutionary algorithms can achieve superior performance (Akiba et al., 2025; Mencattini et al., 2025). However, this approach faces two key challenges: first, there is currently no library for experimenting with different evolutionary algorithms and merging methods; second, these methods typically require repeated evaluations on an evolutionary datasets to compute fitness functions, making them more computationally expensive than standard merging techniques. These limitations restrict access for the very user base that model merging was intended to empower. In this paper, we introduce Mergenetic, simple library to easily perform evolutionary model merging. Built on top of MergeKit (Goddard et al., 2024) and the widely used evolutionary framework PyMoo (Blank and Deb, 2020), our library provides: 1. Comprehensive Algorithm Support. Mergenetic integrates 19 evolutionary algorithms and 6 merging strategies, enabling both singleand multi-objective optimization. This includes classical methods like genetic algorithms and state-of-the-art approaches such as NSGA-II (Deb et al., 2002a). 2. Subsampling & Approximation. To reduce the overhead of fitness evaluations and support merging on consumer GPUs, Mergenetic allows for selective evaluation over dataset subsets and supports advanced approximation techniques for efficient fitness estimation (Mencattini et al., 2025; Polo et al., 2024). 3. Custom Fitness Functions. The library seamlessly integrates with LM-Eval-Harness2 (Gao et al., 2024), offering out-of-the-box support for 8000+ tasks and metrics for fitness computation. Users can also define their own fitness routines tailored to specific needs. 4. Python API, CLI, and GUI. Mergenetic provides flexible Python API for power users who wish to customize merging workflows, alongside command-line interface (CLI) and graphical user interface (GUI) for quick and intuitive setup. Through the CLI or GUI, users can select models from the Hugging Face Hub, configure fitness functions, and launch merging experiments without writing code. Figure 1 and Table 1 summarize the key features of the library. By making evolutionary model merging more efficient, configurable, and accessible, Mergenetic expands the potential of merging as truly democratizing technique. In the remainder of this paper, we describe (i) the relevant background for Mergenetic, (ii) comparisons with existing solutions, (iii) its system architecture and workflow, and (iv) empirical evaluations featuring cross-lingual math merges and multi-task merges on publicly available LLMs. Finally, we conclude by discussing future extensions and potential broader impacts of this approach."
        },
        {
            "title": "2 Background and Related Work",
            "content": "Model Merging. Model merging (Ainsworth et al., 2022; Crisostomi et al., 2025; Peña et al., 2023; Ilharco et al., 2022; Yadav et al., 2023; Yu et al., 2024; Matena and Raffel; Wortsman et al., 2022; Stoica et al.) has become powerful and efficient alternative to ensembling, enabling the integration of existing models without requiring additional training. Mergenetic focuses on the multi-task scenario, where the aim is to merge different fine-tunings of single pretrained model (Ilharco et al., 2022; Yadav et al., 2023; Yu et al., 2024; Matena and Raffel; Wortsman et al., 2022; 2github.com/EleutherAI/lm-evaluation-harness Features Mergenetic (Ours) MergeKit Merging Algorithms Evolutionary Algorithms Multi-objective Dataset Subsampling Custom Fitness Functions GUI 6 19 (Random + Custom) 53 1 Table 1: Comparison of Mergenetic and MergeKit. Davari and Belilovsky, 2025; Wang et al., 2024; Zhou et al., 2024; Gargiulo et al., 2025; Akiba et al., 2025; Choshen et al., 2022). Evolutionary Algorithms. Evolutionary Algorithms (EAs) are black-box optimization techniques that operate on population of candidate solutions, evolving them over successive generations using operators such as selection, mutation, recombination, and crossover (Bäck and Schwefel, 1993; Pétrowski and Ben-Hamida, 2017; Dasgupta and Michalewicz, 1997; Real et al., 2019; Vincent and Jidesh, 2023). key component of EAs is the fitness function, which quantifies the quality of each candidate and steers the evolutionary process by promoting higher-performing solutions (Eiben and Smith, 2015). Applying EAs to model merging, evolutionary merging techniques (Akiba et al., 2025; Mencattini et al., 2025) automatically search for effective merging recipes using the performance of the merged model on held-out validation dataset as the fitness function. Comparison with other libraries. The most closely related library to Mergenetic is MergeKit (Goddard et al., 2024), which provides the underlying merging strategies (e.g., TIES, DARE, SLERP) that we build upon in our evolutionary pipelines. However, when it comes to search capabilities, MergeKit supports only single evolutionary algorithm CMA-ES (Hansen, 2023) offering limited flexibility in exploring the optimization landscape. In contrast, Mergenetic integrates with the full suite of algorithms from pymoo, enabling users to choose from broad range of singleand multi-objective evolutionary strategies, as shown in Table 3. Most importantly, MergeKit assumes that the fitness function must be computed over the full evaluation dataset, which significantly increases runtime and computational demands often making the entire process impractical on consumer 3This number refers to the supported merging methods in evolutionary merging as per the documentation. 2 from mergenetic . merging . linear_merger import LinearMerger from mergenetic . optimization . merging_problem import MergingProblem from pymoo . algorithms . soo . nonconvex . ga import GA from mergenetic . searcher import Searcher # Initialize the merger with base model , finetuned models , and output paths merger = LinearMerger ( run_id =\" demo_run \" , path_to_base_model =\" my / base / model \" , model_paths =[ \" finetunedA \" , \" finetunedB \"] , path_to_store_yaml =\" configs / merging_config . yaml \" , path_to_store_merged_model =\" merged_checkpoints /\" , dtype =\" float16 \") # Define the optimization problem for merging problem = MergingProblem ( = merger , merger search_df = my_dev_data , = 2, n_var = 1 n_obj # Merger object # Dataset used to compute fitness # Number of variables ( weights for the models ) # Number of objectives ( usually single metric ) ) algorithm = GA ( pop_size =10) # Genetic algorithm with population size 10 # Create searcher to run GA over the merging problem searcher = Searcher ( problem , algorithm , results_path =\" results /\" , n_iter =50 , seed =42 , run_id = \" demo_run \" ) searcher . search () searcher . test () # Run the evolutionary search for optimal weights # Evaluate the final merged model Figure 2: Example on how to use the Python API for power users who wish to customize merging workflows. Supported Merging Method Multi-Model Base Model Task Arithmetic (Ilharco et al., 2023) Model Soups (Wortsman et al., 2022) SLERP TIES (Yadav et al., 2023) DARE (Yu et al., 2024) + TIES DARE (Yu et al., 2024) + Task Arithmetic Table 2: Supported merging methods in Mergenetic hardware. In contrast, Mergenetic supports subsampled evaluation and advanced fitness estimation techniques (e.g., IRT-based estimators (Polo et al., 2024; Mencattini et al., 2025)), dramatically reducing evaluation cost and enabling high-quality merging to be performed efficiently on single GPU."
        },
        {
            "title": "3 Design and guiding principles",
            "content": "The design of Mergenetic reflects our goal of supporting wide range of evolutionary modelmerging experiments on consumer hardware. In this section, we outline the guiding principles that drove our design decisions before diving into key modules and functionalities in 4. Research-Oriented central motivation for Mergenetic is to enable researchers to easily explore and compare different evolutionary algorithms, merging strategies, and optimization objectives. Rather than locking users into fixed routine, Mergenetic supports flexible mix of merging methods (e.g., TIES, DARE, SLERP from MergeKit (Goddard et al., 2024)), evolutionary algorithms (e.g., GA, NSGA-II, DE from PyMoo (Blank and Deb, 2020)), and evaluation backends (e.g., LM-Eval-Harness or user-defined). This modularity supports systematic experimentation, such as comparing singlevs. multi-objective merges or testing different data sampling strategiesand allows defining custom objectives through simple subclassing. User-Friendly To democratize model merging for researchers and practitioners with standard GPU setups, Mergenetic is designed to be both configuration-centric and user-friendly. Users can define merges, tasks, algorithms, and evaluators using simple YAML files, command-line interface, or an interactive GUI minimizing the engineering overhead typical of large-scale experiments. The library is optimized for consumer GPUs by supporting approximate evaluation methods (e.g., IRT-based estimators), dataset sub-sampling, and 3 partial model loading. It integrates seamlessly with LM-Eval-Harness, supporting over 8000+ tasks and metrics already defined in the library (e.g., GSM8K and ARC), while also making it easy to plug in custom datasets and evaluations for fitness computation. Together, these features enable meaningful evolutionary merging on single GPU, lowering the barrier for smaller labs and individual practitioners. 5. Running the search, then optionally calling .test() on the best solutions. CLI. For users who prefer command-line approach without manually writing scripts, the Mergenetic CLI is invoked via: mergenetic.py python <lm-evalcustom> <singlemulti> eval-method merge-type"
        },
        {
            "title": "4 Mergenetic",
            "content": "Modules and Functionalities The implementation relies on MergeKit (Goddard et al., 2024) for merging the models, PyMoo (Blank and Deb, 2020) for optimizing the objective function through evolutionary algorithms, and LM-Eval-Harness (Gao et al., 2024) for implementing some of the fitness functions. In table 2 we outline the supported merging methods, while in table 3 we outline the currently available evolutionary algorithms. The Mergenetic library is divided into distinct modules that reflect the core stages of evolution- (i) defining the workflow ary model merging: (Python API, CLI, GUI), (ii) performing the merge ( Merger ), (iii) formulating the optimization problem ( Optimization ), (iv) evaluating merged models ( Evaluator ), and (v) orchestrating the evolution loop ( Searcher ). Below, we briefly describe each module and link it to the broader system design. 4.1 Python API, CLI, and GUI Python API. Figure 2 provides an example usage of the API. The Searcher and Problem classes form the core of the Python API. Users can instantiate an optimization problem (e.g., merging multiple language models), select an algorithm from PyMoo, and call searcher.search() to launch the evolutionary procedure. typical workflow involves: 1. Defining an evaluation dataset and relevant performance metric. 2. Instantiating Merger to specify how weights are combined. 3. Passing these to MergingProblem class, describing the evolutionary search space and objectives. 4. Choosing GeneticAlgorithm (e.g., NSGAInternally, it launches an interactive wizard to guide users through selecting models, tasks, algorithms, and merging methods. The CLI can handle four main modes: singleor multi-language merges, each with either LM-Eval-Harness or custom evaluations. By abstracting away many details, the CLI lets users prototype merges quickly with no code. GUI. Gradio4-based (Abid et al., 2019) graphical interface provides further layer of accessibility, especially for non-technical users or demonstration purposes (See Fig. 3). It reuses the same core configuration concepts but wraps them in stepby-step wizard: (1) load base model(s), (2) specify tasks/languages, (3) set evolutionary parameters, and (4) run merging with real-time logs. The GUI allows merging without coding. 4.2 Core components We describe here the core components. 4.2.1 Merger class (e.g., TaskArithmeticMerger ) The Merger module handles the core weightcombination logic by interfacing with MergeKit. SlerpMerger , Each merger TiesDareMerger , generates YAML configuration specifying the base interpolation method, and merge checkpoints, coefficients. This configuration is passed to MergeKit, which performs the actual merging and produces new model checkpoint. The merger supports both standard and multi-model merges, including advanced strategies like TIES combined with DARE (Yadav et al., 2023; Yu et al., 2024). Additionally, Mergenetic manages GPU memory during the evolutionary search, helping avoid out-of-memory errors. During optimization, the evolutionary algorithm proposes weight combinations, which the merger translates into actual models ready for evaluation. II, GA, DE) from PyMoo. 4https://github.com/gradio-app/gradio 4 Figure 3: Screenshot of the Gradio-based GUI described in section 4.1. The user is guided through step-by-step process to define every ingredient of the evolutionary merging pipeline. 4.2.2 Optimization At the core of Mergenetic, the optimization module casts model merging as black-box optimization problem. The decision variables correspond to the targeted parameters from the merging configuration file (the genotype), such as the interpolation or pruning coefficients. Objective functions define the fitness criteria to be optimized, such as accuracy, perplexity, or other task-specific metrics. The MergingProblem class define how to: (i) Convert genotype to merged model (by calling the Merger ). (ii) Evaluate the merged model on dataset, via an Evaluator . (iii) Return the resulting fitness or multi-objective scores to the algorithm. Using 2020), Mergenetic supports variety of singleor multi-objective methods. Single-objective approaches optimize one metric (e.g., cross-lingual accuracy), while multi-objective strategies (e.g., NSGA-II) can simultaneously balance multiple metrics like math accuracy vs. general fluency. and Deb, (Blank PyMoo 4.2.3 Evaluator Evaluators compute merged models performance on the chosen task(s). In Mergenetic, they appear both as direct evaluators (e.g., running on small 5 dataset) or as IRT-based estimators using anchors (Mencattini et al., 2025). We highlight two broad categories: Mergenetic LM-Eval-Harness Evaluators. can natively call out to the LM-Eval-Harness (Gao et al., 2024) library, passing the merged checkpoint and chosen benchmark (e.g., ARC, GSM8K). This approach covers many standard tasks and it yields consistent comparisons. can be relatively expensive if one repeatedly evaluates large datasets on many candidate merges. To offset this problem, Mergenetic wrap LM-Eval-Harness and allow explicit subsamples through the plug-and-play ConfigPE , which allows to subsample such without the need to instantiate new LM-Eval-Harness config file. However, MultilingualMathFGEvaluator Custom Evaluators. Users can alternatively define their own logic for computing correctness that checks e.g., whether the final extracted number is correct and in the target language. Or MultipleChoiceEvaluator that compares the chosen letter (A, B, C, D) to the ground truth. These evaluators easily allow advanced users to combine partial correctness checks with domain constraints (e.g., the predicted chemical formula must be balanced). Figure 4: Evolving multi-lingual model spanning Italian, English, German and Dutch. 4.2.4 Searcher Finally, the Searcher orchestrates the evolutionary loop: it begins with the initialization of population of random genotypes (weight vectors), followed by merging/evaluation, where each genotype is merged into checkpoint and scored on user-specified tasks/datasets. Then comes selection/variation, where parent genotypes are chosen based on fitness and modified via crossover and mutation to produce children. Steps 2 and 3 repeat for generations in the main loop. Therefore, the Searcher class essentially wraps all these elements ( Problem , Merger , Evaluator , PyMoo algorithm ) in an easy-to-use API. During the search process, intermediate results (population genotypes, partial solutions, logs) are stored in CSV or JSON , facilitating real-time monitoring. At completion, test() re-merges the best solutions and evaluates them on an unseen test set to quantify final performance."
        },
        {
            "title": "5 Case Studies",
            "content": "To demonstrate the capabilities of the Mergenetic library, we reproduce here two evolutionary model merging pipelines: MERGE3 (Mencattini et al., 2025) and EvoLLM-JP (Akiba et al., 2025). 5.1 Evolving multi-lingual model We demonstrate how Mergenetic can be used to merge individually fine-tuned models for four languages Italian, English, German, and Dutch into single multilingual model. This setup formulates the objective function as explicitly multi-task, assigning one evaluation metric per language to promote balanced cross-lingual performance. Details on the specific models used per language are provided in Appendix A.2. As shown in fig. 4, the merged model consistently outperforms each of its language-specific constituents, achieving up to 6 Figure 5: Cross-lingual transfer of math solving capabilities from English to Japanese. 19% accuracy gain on the ARC-Challenge benchmark (Clark et al., 2018). Notably, it surpasses all endpoints across the board, highlighting the effectiveness of evolutionary merging in facilitating positive knowledge transfer across languages. 5.2 Cross-lingual transfer To showcase the ability of Mergenetic to support cross-lingual skill transfer, we merge mathspecialized English model with Japanese finetuned version of Mistral-7B (Jiang et al., 2023), and evaluate the result on the Japanese translation of the GSM8K dataset (Cobbe et al., 2021). This experiment follows the general setup proposed by Akiba et al. (2025), using subset of 100 samples for the fitness evaluation instead of the full dataset. As shown in fig. 5, the merged model achieves 10-20% accuracy improvement over each of its individual components, demonstrating effective crosslingual transfer enabled by evolutionary merging."
        },
        {
            "title": "6 Conclusions",
            "content": "Mergenetic bridges the gap between cutting-edge evolutionary model merging and practical usability on consumer hardware. By combining flexible merging strategies, diverse evolutionary algorithms, and lightweight fitness approximators, it empowers researchers and practitioners to explore high-quality model compositions without requiring large-scale infrastructure. Through its Python API, CLI, and GUI, Mergenetic supports both systematic experimentation and user-friendly workflows. We hope the library will serve as stepping stone for future research in multilingual, multi-task, and efficient evolutionary model merging, and invite the community to build upon and extend its capabilities."
        },
        {
            "title": "Limitations",
            "content": "While Mergenetic significantly lowers the entry barrier for evolutionary model merging, several limitations remain: Dependence on Existing Fine-Tuned Models. Model merging requires access to pre-trained or fine-tuned base models with relevant capabilities (e.g., math reasoning, language-specific fluency). As such, the technique currently cannot be directly applied to extremely low-resource languages or domains where such models are unavailable. This limits its immediate applicability in truly zero-resource settings. Future work could explore integrating lightweight fine-tuning or retrieval-based augmentation prior to merging to alleviate this dependency. Hardware Requirements. Although Mergenetic is designed for consumer-grade GPUs, it still requires relatively high-tier hardware (e.g., NVIDIA RTX 2080 or better) due to the size of language models involved and the need to load and evaluate them during evolution. Most laptops or low-memory GPUs may not have sufficient VRAM to support repeated merging and evaluation steps. We see this as broader limitation of current LLM infrastructure and hope that advances in model quantization, sparse evaluation, and efficient loading techniques will further democratize access to frontier AI tools like Mergenetic."
        },
        {
            "title": "References",
            "content": "Abubakar Abid, Ali Abdalla, Ali Abid, Dawood Khan, Abdulrahman Alfozan, and James Zou. 2019. Gradio: Hassle-free sharing and testing of ml models in the wild. arXiv preprint arXiv:1906.02569. Samuel Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. 2022. Git Re-Basin: Merging models modulo permutation symmetries. In The Eleventh International Conference on Learning Representations. Takuya Akiba, Makoto Shing, Yujin Tang, Qi Sun, and David Ha. 2025. Evolutionary optimization of model merging recipes. Nature Machine Intelligence. J. Blank and K. Deb. 2020. pymoo: Multi-objective optimization in python. IEEE Access, 8:8949789509. Thomas Bäck and Hans-Paul Schwefel. 1993. An overview of evolutionary algorithms for parameter optimization. Evolutionary Computation, 1(1):123. Leshem Choshen, Elad Venezian, Noam Slonim, and Yoav Katz. 2022. Fusing finetuned models for better pretraining. arXiv preprint arXiv:2204.03044. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the AI2 reasoning challenge. CoRR, abs/1803.05457. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Donato Crisostomi, Marco Fumero, Daniele Baieri, Florian Bernard, and Emanuele Rodolà. 2025. c2m3: Cycle-consistent multi-model merging. In Advances in Neural Information Processing Systems, volume 37. Dipankar Dasgupta and Zbigniew Michalewicz. 1997. Evolutionary algorithmsan overview. Evolutionary algorithms in engineering applications, pages 328. MohammadReza Davari and Eugene Belilovsky. 2025. Model breadcrumbs: Scaling multi-task model merging with sparse masks. In European Conference on Computer Vision, pages 270287. Springer. K. Deb, A. Pratap, S. Agarwal, and T. Meyarivan. 2002a. fast and elitist multiobjective genetic algorithm: Nsga-ii. IEEE Transactions on Evolutionary Computation, 6(2):182197. K. Deb, A. Pratap, S. Agarwal, and T. Meyarivan. 2002b. fast and elitist multiobjective genetic algorithm: Nsga-ii. IEEE Transactions on Evolutionary Computation, 6(2):182197. Kalyanmoy Deb, Karthik Sindhya, and Tatsuya Okabe. 2007. Self-adaptive simulated binary crossover for real-parameter optimization. In Proceedings of the 9th Annual Conference on Genetic and Evolutionary Computation, GECCO 07, page 11871194, New York, NY, USA. Association for Computing Machinery. A.E. Eiben and J.E. Smith. 2015. Introduction to Evolutionary Computing. Springer. Clémentine Fourrier, Nathan Habib, Alina Lozovskaya, Konrad Szafer, and Thomas Wolf. 2024. Open https://huggingface. llm leaderboard v2. co/spaces/open-llm-leaderboard/open_llm_ leaderboard. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2024. framework for few-shot language model evaluation. 7 Antonio Andrea Gargiulo, Donato Crisostomi, Maria Sofia Bucarelli, Simone Scardapane, Fabrizio Silvestri, and Emanuele Rodolà. 2025. Task singular vectors: Reducing task interference in model merging. Preprint, arXiv:2412.00081. Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc Le. 2019. Regularized evolution for image classifier architecture search. In Proceedings of the aaai conference on artificial intelligence, volume 33, pages 47804789. Charles Goddard, Shamane Siriwardhana, Malikeh Ehghaghi, Luke Meyers, Vladimir Karpukhin, Brian Benedict, Mark McQuade, and Jacob Solawetz. 2024. Arcees MergeKit: toolkit for merging large language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 477485, Miami, Florida, US. Association for Computational Linguistics. N. Hansen. 2023. The cma evolution strategy: tutorial. arXiv preprint arXiv:1604.00772. G. Ilharco, M.T. Ribeiro, M. Wortsman, S. Gururangan, L. Schmidt, H. Hajishirzi, and A. Farhadi. 2023. Editing models with task arithmetic. arXiv preprint arXiv:2212.04089. Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. 2022. Editing models with task arithmetic. The Eleventh International Conference on Learning Representations. A.Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D.S. Chaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L.R. Lavaud, M.- A. Lachaux, P. Stock, T. Le Scao, T. Lavril, T. Wang, T. Lacroix, and W. El Sayed. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. A. Joulin, E. Grave, P. Bojanowski, and T. Mikolov. 2016. Bag of tricks for efficient text classification. arXiv preprint arXiv:1607.01759. Michael Matena and Colin Raffel. Merging models with fisher-weighted averaging. Tommaso Mencattini, Adrian Robert Minut, Donato Crisostomi, Andrea Santilli, and Emanuele Rodolà. 2025. Merge3: Efficient evolutionary merging on consumer-grade gpus. Preprint, arXiv:2502.10436. Fidel Guerrero Peña, Heitor Rapela Medeiros, Thomas Dubail, Masih Aminbeidokhti, Eric Granger, and Marco Pedersoli. 2023. Re-basin via implicit In Proceedings of the sinkhorn differentiation. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2023720246. Alain Pétrowski and Sana Ben-Hamida. 2017. Evolutionary algorithms. John Wiley & Sons. Felipe Maia Polo, Lucas Weber, Leshem Choshen, Yuekai Sun, Gongjun Xu, and Mikhail Yurochkin. 2024. tinybenchmarks: evaluating llms with fewer examples. In Forty-first International Conference on Machine Learning. Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, et al. 2022. Language models are multilingual chain-of-thought reasoners. arXiv preprint arXiv:2210.03057. George Stoica, Daniel Bolya, Jakob Brandt Bjorner, Pratik Ramesh, Taylor Hearn, and Judy Hoffman. Zipit! merging models from different tasks without training. In The Twelfth International Conference on Learning Representations. Klaudia Thellmann, Bernhard Stadler, Michael Fromm, Jasper Schulze Buschhoff, Alex Jude, Fabio Barth, Johannes Leveling, Nicolas Flores-Herr, Joachim Köhler, René Jäkel, and Mehdi Ali. 2024. Towards cross-lingual llm evaluation for european languages. Preprint, arXiv:2410.08928. Amala Mary Vincent and Jidesh. 2023. An improved hyperparameter optimization framework for automl systems using evolutionary algorithms. Scientific Reports, 13(1):4737. Ke Wang, Nikolaos Dimitriadis, Guillermo OrtizJimenez, François Fleuret, and Pascal Frossard. 2024. Localizing task information for improved model In Proceedings of the merging and compression. 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 5026850287. PMLR. Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt. 2022. Model soups: averaging weights of multiple finetuned models improves accuracy without increasing inference time. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 2396523998. PMLR. Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. 2023. Ties-merging: Resolving interference when merging models. In Advances in Neural Information Processing Systems, volume 36, pages 70937115. Curran Associates, Inc. Enneng Yang, Li Shen, Guibing Guo, Xingwei Wang, Xiaochun Cao, Jie Zhang, and Dacheng Tao. 2024. Model merging in llms, mllms, and beyond: Methods, theories, applications and opportunities. arXiv preprint arXiv:2408.07666. Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. 2024. Language models are super mario: Absorbing abilities from homologous models as free lunch. In Proceedings of the 41st International Conference 8 on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 5775557775. PMLR. Luca Zhou, Daniele Solombrino, Donato Crisostomi, Maria Sofia Bucarelli, Fabrizio Silvestri, and Emanuele Rodolà. 2024. Atm: Improving model merging by alternating tuning and merging. arXiv preprint arXiv:2411.03055."
        },
        {
            "title": "A Additional Details",
            "content": "A.1 Cross-Lingual Case Study Details For the cross-lingual case study, we conduct evolutionary search on the Japanese subset of the MGSM dataset (Shi et al., 2022), multilingual extension of GSM8K (Cobbe et al., 2021). The final merged model is evaluated on the MGSM test set, following the evaluation protocol of Akiba et al. (2025). Unlike their setup, which used 1,069 search datapoints (the remaining part of GSM8k test set that was not included in MGSM), we use only subset of 100 examples for computational efficiency. Our approach employs single-objective evolutionary algorithm based on Genetic Algorithm (Dasgupta and Michalewicz, 1997), incorporating Simulated Binary Crossover (SBX) operator (Deb et al., 2007) for recombination and Polynomial Mutation operator (Deb et al., 2007) for exploration. We set the population size to 25 and run the algorithm for 7 generations. Fitness and evaluation metrics are computed by extracting the final numeric answer using regular expression and verifying both the mathematical correctness and the linguistic accuracy of each response. Language identification is performed using the method described in (Joulin et al., 2016). Only responses that are both mathematically and linguistically correct are considered valid. The models evaluated in this experiment include Arithmo2-Mistral-7B, Abel-7B-002, and shisa-gamma-7b-v1. Mistral-Ita-7B, GEITje-7B-ultra, include leo-mistral-hessianai-7B, and the base model Mistral-7B-v0.1. A.3 Supported evolutionary algorithms Table 3 lists all the evolutionary algorithms provided by PyMoo and hence supported in Mergenetic, stating whether they are singleor multi-objective and if they allow constraints to be defined, along with brief description. A.4 Performance Estimator To reduce the computational cost associated with evaluating the fitness of candidate models during evolutionary merging, the Mergenetic library supports estimator-based approximations inspired by Mencattini et al. (2025) and Polo et al. (2024). These methods allow us to estimate model performance using reduced subset of the evaluation dataset, significantly accelerating the evolution process without sacrificing accuracy. In particular, Mergenetic provides implementations of both standard and merging-specific IRTbased estimators, which leverage latent ability inference to approximate full-dataset correctness. These estimators vary in their assumptions and complexity, offering trade-off between computational efficiency and estimation fidelity. Table 4 provides an overview of the currently supported estimators, including brief description and qualitative rating of their performance. A.2 Multilingual case study details A.5 License For the multilingual case study, we perform evolutionary model merging across four languages Italian, Dutch, German, and English using the translated ARC dataset from the Hugging Face repository (Thellmann et al., 2024)5. We employ multi-objective optimization setup with NSGA-II (Deb et al., 2002b), configuring the evolutionary process with population size of 25 and 7 iterations. As the merging strategy, we use combination of TIES and DARE. The fitness and test evaluations are performed by extracting the final answer choice (A, B, C, or D) from the models output using regular expression. For each language, we use reduced dataset of 20 translated examples from ARC to compute fitness scores, keeping the process efficient and GPU-friendly. The models used in this experiment 5https://huggingface.co/openGPT-X/arcx The library is licensed with the Apache 2.0 license. This means that it can be freely used, modified, and redistributed by anyone, including for commercial purposes. The license is designed to promote widespread adoption by offering permissive legal framework that imposes minimal restrictions on end users. Developers are allowed to modify the source code and distribute derivative works under different terms, provided that the original license and copyright notice are retained. Mergenetic builds upon two key dependencies: PyMoo and MergeKit. The former is distributed under the same Apache 2.0 license as Mergenetic, ensuring compatibility and permissive use. However, MergeKit introduces additional licensing constraints in versions beyond v0.1.0. Specifically, it adopts Business Source License, which restricts production use based on organizational scale and revenue. Users intending to deploy 10 Algorithm Class Obj. Constr. Description Genetic Algorithm GA DE Differential Evol. BRKGA BRKGA NelderMead Nelder Mead PatternSearch Pattern Search CMAES CMAES ES Evol. Strategy SRES SRES ISRES ISRES NSGA2 NSGA-II RNSGA2 R-NSGA-II NSGA-III NSGA3 UNSGA3 U-NSGA-III RNSGA3 R-NSGA-III MOEAD MOEAD AGE-MOEA AGEMOEA CTAEA C-TAEA CTAEA SMS-EMOA RVEA RVEA single single single single single single single single single multi multi many many many many many many many many Customizable evol. operators for broad problem categories Variants for continuous global optimization Advanced variable encoding for combinatorial opt. Point-based algorithm using simplex operations Iterative approach with exploration patterns Model-based sampling from dynamic normal distribution Real-valued optimization strategy ES with stochastic ranking constraint handling Improved SRES for dependent variables Non-dominated sorting and crowding NSGA-II with reference points NSGA-II for many-objective problems NSGA-III optimized for fewer objectives NSGA-III with aspiration points Multi-objective optimization via decomposition Estimates Pareto-front shape instead of crowding Sophisticated constraint-handling for many objectives Uses hypervolume during environmental survival Reference direction with angle-penalized metric Table 3: Supported Optimization Algorithms and their description. Estimator Description Performance Random P-IRT GP-IRT MP-IRT GMP-IRT Baseline estimator using random sample correctness. Simple but noisy and unreliable. Standard Item Response Theory estimator, uses subset to estimate ability, not tailored for merging. Generalized P-IRT with better smoothing but still not designed for merging. MERGE3s merged-performance IRT estimator assuming linear combination of abilities. Generalized version of MP-IRT, combines predictions and observations with learned weights. Full Dataset Ground truth performance by running evaluation on the full dataset. Table 4: Comparison of different performance estimators. Mergenetic for commercial purposes are advised to review these terms carefully and install version of MergeKit that aligns with their intended usage scenario. If unrestricted commercial use is required, it is recommended to use version 0.1.0 of MergeKit, which remains under the Apache 2.0 license, or to contact the licensor for alternative licensing arrangements."
        }
    ],
    "affiliations": [
        "Ecole Polytechnique Fédérale de Lausanne",
        "Sapienza University of Rome"
    ]
}