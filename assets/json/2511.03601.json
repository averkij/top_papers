{
    "paper_title": "Step-Audio-EditX Technical Report",
    "authors": [
        "Chao Yan",
        "Boyong Wu",
        "Peng Yang",
        "Pengfei Tan",
        "Guoqiang Hu",
        "Li Xie",
        "Yuxin Zhang",
        "Xiangyu",
        "Zhang",
        "Fei Tian",
        "Xuerui Yang",
        "Xiangyu Zhang",
        "Daxin Jiang",
        "Shuchang Zhou",
        "Gang Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Step-Audio-EditX, the first open-source LLM-based audio model excelling at expressive and iterative audio editing encompassing emotion, speaking style, and paralinguistics alongside robust zero-shot text-to-speech (TTS) capabilities. Our core innovation lies in leveraging only large-margin synthetic data, which circumvents the need for embedding-based priors or auxiliary modules. This large-margin learning approach enables both iterative control and high expressivity across voices, and represents a fundamental pivot from the conventional focus on representation-level disentanglement. Evaluation results demonstrate that Step-Audio-EditX surpasses both MiniMax-2.6-hd and Doubao-Seed-TTS-2.0 in emotion editing and other fine-grained control tasks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 2 1 0 6 3 0 . 1 1 5 2 : r Step-Audio-EditX Technical Report Chao Yan*, Boyong Wu*, Peng Yang, Pengfei Tan, Guoqiang Hu, Li Xie, Yuxin Zhang, Xiangyu(Tony) Zhang, Fei Tian, Xuerui Yang, Xiangyu Zhang, Daxin Jiang, Shuchang Zhou, Gang Yu StepFun"
        },
        {
            "title": "Abstract",
            "content": "We present Step-Audio-EditX, the first open-source LLM-based reinforcement learning audio model excelling at expressive and iterative audio editingencompassing emotion, speaking style, and paralinguisticsalongside robust zero-shot text-tospeech (TTS) capabilities. Our core innovation lies in leveraging only large-margin synthetic data in post-training, which circumvents the need for embedding-based priors or auxiliary modules. This large-margin learning approach enables both iterative control and high expressivity across voices, and represents fundamental pivot from the conventional focus on representation-level disentanglement. Evaluation results demonstrate that Step-Audio-EditX surpasses both MiniMax-2.6-hd and DoubaoSeed-TTS-2.0 in emotion editing and other fine-grained control tasks. Our code and models are available at https://github.com/stepfun-ai/Step-Audio-EditX. Figure 1: Comparison between Step-Audio-EditX and Closed-Source models.(a) Step-AudioEditX demonstrates superior performance over Minimax and Doubao in both zero-shot cloning and emotion control.(b) Emotion editing of Step-Audio-EditX significantly improves the emotion-controlled audio outputs of all three models after just one iteration. With further iterations, their overall performance continues to improve. * Core Contribution Corresponding Author: yanchao@stepfun.com"
        },
        {
            "title": "1 Introduction",
            "content": "In recent years, TTS technology has advanced significantly. notable development is zero-shot TTS models, which can generate high-quality, natural-sounding speech by mimicking the timbre, prosody, and style of reference speech prompt. Generally, current zero-shot TTS systems fall into three main categories: those that utilize LLMs to model discrete or continuous acoustic tokens [1, 2, 3, 4, 5], those employing diffusion or flow matching models to learn direct text-to-speech mapping [6, 7, 8, 9, 10, 11], and hybrid coarse-to-fine systems, where LLMs first convert text tokens into coarse speech tokens, which are then refined by diffusion or flow matching model to render fine-grained speech details[12, 13, 14, 15, 16, 17, 18, 19]. Despite considerable progress in zero-shot TTS synthesis, attributes such as emotion, style, accent, and timbre in the synthesized speech are still derived directly from the reference audio. This inherent limitation restricts independent control over these attributes. Although prepending style or emotion instructions to the input text offers certain degree of controllability and often performs well for in-domain speakers [9, 12, 13, 14], this approach faces challenges in disentangling speech attributes. In particular, the cloned voice often fails to effectively follow the provided style or emotion instructions. Many previous studies on speech disentanglement have relied on approaches such as adversarial training[20, 21], feature engineering[22, 23], and innovative network architectures[24] to achieve attribute decoupling. In contrast, we propose simple yet stable data-driven method. Specifically, we design pipeline for generating high-quality data pairs that preserve identical linguistic content while exhibiting clearly distinguishable variations in one or few attributes, such as emotion, style, accent, and paralinguistic features. By training models on such data pairs, we achieve effective attribute disentanglement, enabling to edit the attribute of input speech. Moreover, by applying multiple iterative \"editing\" steps, the intensity of target attribute can be progressively enhanced or reduced. Beyond emotion, style and paralinguistic editing, we demonstrate that this approach can be extended to other applications, including speed rate adjustment, speech denoising, and silence trimming. In this report, we outline our contributions and findings: We present Step-Audio-EditX, the first open-source LLM-based audio model excelling at expressive and iterative audio editing, encompassing emotion, speaking style, and paralinguistics, alongside robust zero-shot TTS capabilities. Our results show that emotion and speaking style can be controlled through post-training with large-margin data alone, eliminating the need for extra presentation modeling or adapter modules. We find that post-training with large-margin data enables iterative control and high expressivity across voices, which represents fundamental pivot from conventional representation-level disentanglement methods."
        },
        {
            "title": "2.1 Overview",
            "content": "In our prior work, we introduced an Audio-Edit synthesis model in Step-Audio[25] for nuanced emotional expressions and diverse speaking styles data generation. In this report, we retain the previous model along with the same audio tokenizer. The key modifications include an expanded 2 range of emotions and speaking styles, the addition of zero-shot TTS and paralinguistic editing capabilities, as well as reduction in model parameters from 130B to 3B. Leveraging large-margin synthetic data, our 3B model demonstrates superior and more stable performance compared to the previous version. Our system comprises three primary components: (1) dual-codebook audio tokenizer, which converts reference or input audio into discrete tokens; (2) an audio LLM that generates dual-codebook token sequences; and (3) an audio decoder, which converts the dual-codebook token sequences predicted by the audio LLM back into audio waveforms using flow matching approach.This integrated architecture enables the Step-Audio-EditX to perform zero-shot TTS and diverse editing tasks within unified framework. Thus, it can directly capitalize on the rich ecosystem of post-training techniques developed for text LLMs. Figure 2: An overview of the architechture of Step-Audio-EditX"
        },
        {
            "title": "2.2 Audio Tokenizer",
            "content": "We investigate the effect of LLMs post-training with large-margin data by retaining the dualcodebook tokenization framework from our previous Step-Audio model, which employs parallel linguistic (16.7 Hz, 1024-codebook) and semantic (25 Hz, 4096-codebook) tokenizers in 2:3 interleaving ratio. Based on series of downstream audio tokenizer reconstruction experiments, we observed that the dual-codebook tokenizer retains considerable amount of emotional, prosodic, and other non-linguistic information, indicating suboptimal disentanglement. This shortcoming makes it particularly suitable for validating the effectiveness of our LLM post-training strategy and the proposed large-margin data-driven methodology."
        },
        {
            "title": "2.3 Audio LLM",
            "content": "The audio LLM uses the same architecture as our prior Audio-Edit model, differing only in its smaller parameter size of 3B. To capitalize on the powerful language capabilities of pre-trained text LLMs, the 3B model is initialized with text-based LLM, followed by training on blended dataset with 1:1 ratio of text data to audio dual-codebook tokens. The audio LLM processes text tokens along with their corresponding dual-codebook audio tokens in chat format, subsequently generating dual-codebook tokens as the sole output."
        },
        {
            "title": "2.4 Audio Decoder",
            "content": "The audio decoder consists of Flow Matching module and BigVGANv2[26] vocoder. The flow matching module generates Mel spectrograms, given output audio tokens, reference audio, and speaker embedding as conditions, while the BigVGANv2 vocoder further converts the Mel spectrograms into waveforms. For the flow matching module, we adopt the diffusion transformer (DiT) 3 as the backbone, and train the model on 200,000 hours of high-quality speech. This enhancement significantly improves its Mel spectrogram reconstruction capability, leading to substantial gains in both pronunciation accuracy and timbre similarity."
        },
        {
            "title": "3 Data",
            "content": "Consistent with prior work on StepAudios pre-training dataset and methodology, this report focus on the post-training dataset and the corresponding methods."
        },
        {
            "title": "3.1 SFT Data",
            "content": "We employ SFT to enable the Step-Audio-EditX model for zero-shot TTS and diverse audio editing tasks. The SFT data can be categorized into several parts: zero-shot TTS, emotion editing, speaking style editing and paralinguistic editing. Notably, the large-margin dataset targets editing tasks, particularly on the aspects of emotion and speaking style."
        },
        {
            "title": "3.1.1 Zero-shot Text to Speech",
            "content": "We employ high-quality, professionally annotated in-house dataset, primarily in Chinese and English, for zero-shot TTS. Furthermore, minimal amount of Cantonese and Sichuanese data is employed to elicit dialect capabilities. To ensure diverse and highly expressive styles and emotions in the synthesized speech, as well as robust zero-shot performance, The dataset captures vocal variations within individual speakers as well as across broad speaker population, comprising approximately 60,000 unique individuals."
        },
        {
            "title": "3.1.2 Emotion and Speaking Style Editing",
            "content": "Emotion and speaking style present significant challenges for expressive text-to-speech systems, due to the inherent difficulties in both defining their categorical characteristics and collecting high-quality data. We propose straightforward and efficient large-margin synthetic data approach, which performs zero-shot voice cloning across different emotions and speaking styles for the same speaker, while ensuring sufficiently large margin between contrastive sample pairs. Only one prompt audio segment per emotion or speaking style is required, eliminating the need for costly data collection. Moreover, this method ingeniously converts complex emotion and style descriptions into comparative pair-based data construction format. In the following, we introduce the proposed approach: Voice Actor Recording. Voice actors recorded expressive emotions and speaking styles. For each actor, single audio clip of approximately 10 seconds was captured for every emotion and style combination. Zero-shot Cloning. triplet textprompt, audioneutral, audioemotion,style is constructed for each emotion and speaking style by selecting corresponding emotional and neutral audio clips from the same speaker as the prompt audio and processing them with the StepTTS voice cloning interface, using text instruction that describes the target attribute. Margin Scoring. To evaluate the triplet generated, we developed scoring model using small, human-annotated dataset. The model evaluates audio pairs on 1-10 scale, with higher margin scores corresponding to more desirable outcomes. 4 Margin Selection. Samples were selected based on margin score threshold. This threshold was adjusted for different emotions and styles, with score of 6 serving as the universal lower bound. Notably, the audio clips in each triplet are generated using the same emotional or stylistic text prompt, which encourages the model to focus solely on the variations in emotion and style itself during the SFT training process."
        },
        {
            "title": "3.1.3 Paralinguistic Editing",
            "content": "Paralinguistic cues, such as breathing, laughter, and filled pauses (e.g., \"uhm\"), are crucial for enhancing the naturalness and expressiveness of synthesized speech. We achieved paralinguistic editing capability by using \"semi-synthetic\" strategy, which leverages the NVSpeech dataset[27], highly expressive speech corpus whose rich annotations for numerous paralinguistic types enabled the construction of comparative quadruplets for model training. The quadruplet textwithout_tags, audiowithout_tags, textnv_source, audionv_source construction differs from the triplet by using the NVSpeech original audio and transcript as the target output and the StepTTS voice cloning generated audio as the input, which is synthesized using the original transcript after paralinguistic tag removal. As paralinguistic editing is an editing task performed in the time domain and exhibits substantial intrinsic margin differences, margin scoring model is not required for data selection. small set of quadruplet data is sufficient to effectively elicit the models paralinguistic editing capabilities."
        },
        {
            "title": "3.2 Reinforcement Learning Data",
            "content": "To align our model with human preferences, we construct two distinct types of preference datasets using different approaches: one based on human annotation, and the other employing the LLM-asa-Judge method. Human Annotation. We first collected real-world prompt audio and corresponding text prompts from users, and generated 20 candidate responses using the SFT model. We then constructed chosen/rejected pairs by having human annotators rate each of the 20 responses on 5-point scale based on the criteria of correctness, prosody, and naturalness. Only pairs with score margin greater than 3 were selected. LLM-as-a-Judge. Model responses were scored on 1-10 scale for emotion and speaking style editing by comprehension model. Preference pairs were then generated from these scores, retaining only pairs with score margin greater than 8 points in the final dataset. These selected large-margin pairs will be used to train the reward model and PPO."
        },
        {
            "title": "4 Training",
            "content": "Our post-training process aligns the models outputs with zero-shot TTS, variety of editing tasks, and human preferences. This alignment is accomplished through two-stage approach: SFT followed by proximal policy optimization. 5 4.1 Supervised Fine-tuning The SFT stage enhances the models zero-shot text-to-speech synthesis and editing capabilities through the use of distinct system prompts in chat format. In the zero-shot TTS task, the prompt waveform is encoded into dual-codebook tokens, which are subsequently detokenized into string format and incorporated into the speaker information within the system prompt. The text to be synthesized serves as the user prompt in chat-based format, and the generated dual-code tokens are returned as the systems response. For the editing task, all operations are defined under unified system prompt. The user prompt includes both the original audio and descriptive command for the editing operation, and the system response delivers the resulting edited audio tokens. The model is finetuned for one epoch with learning rate from 1 105 to 1 106."
        },
        {
            "title": "4.2 Reinforcement Learning",
            "content": "Reinforcement learning has further amplified the models stability in zero-shot TTS, as well as its capability and expressiveness in following editing instructions. These enhancements are particularly noticeable when there is substantial divergence between the emotional and stylistic characteristics of the source prompt waveform and the target editing output, such as generating sad speech from happy prompt or converting loud speech into whisper. This reinforcement learning approach offers novel perspective to address these challenges by shifting the focus from achieving ideal speech representation disentanglement to improving both the construction of large-margin pairs and the efficacy of the reward model evaluation. Reward Model Training. The reward model is initialized from 3B SFT model and is trained using combination of human-annotated and LLM-as-a-judge-generated large-margin data, optimized with the Bradley-Terry loss. The model is token-level reward model trained directly on largemargin dual-codebook token pairs. This approach obviates the need to convert tokens back into waveform using an audio decoder during reward computation. The model is finetuned for one epoch and the learning rate is adjusted using cosine decay strategy, initialized at 2 105 with lower bound set at 1 105. PPO Training. Following the acquisition of the reward model, we employ the PPO algorithm for further training, using the same prompt seeds as in the reward model training, except for the selection of only the most challenging prompts for the SFT model. In the PPO training stage, the critic model is warmed up for 80 steps ahead of the actor. The optimizer uses an initial learning rate of 1 106, which follows cosine decay schedule with lower bound of 2 107. PPO clip threshold of ϵ = 0.2 and KL divergence penalty with coefficient of β = 0.05 are applied."
        },
        {
            "title": "5 Evaluation",
            "content": "The accurate and comprehensive evaluation of models performance in synthesizing emotional, stylistic, and paralinguistic speech represents substantial challenge. To address this, we first introduce the construction of comprehensive and reproducible benchmark in Section 5.1. We then employ this benchmark in Section 5.2 to demonstrate the advantages of our Step-Audio-EditX model."
        },
        {
            "title": "5.1 Evaluation Benchmark",
            "content": "We introduce Step-Audio-Edit-Benchmark, benchmark that leverages LLM-as-a-judge model to evaluate model performance on emotion, speaking style, and paralinguistics. All evaluation audio is generated via zero-shot voice cloning and subsequently scored using the Gemini-2.5-Pro1 model. Speaker Selection. The speaker set for zero-shot cloning consisted of eight speakers (2 male and 2 female per language, for both Chinese and English). The Chinese speakers were sourced from the Wenet-Speech4TTS[28] corpus, whereas the English speakers were sourced from the open-source GLOBE-V2[29] and Libri-Light[30] datasets, respectively. Emotion. The emotional test set covers five categories: happiness, anger, sadness, fear, and surprise. Each category includes 50 Chinese and 50 English prompts, with the textual content of each prompt designed to be consistent with its corresponding target emotion. Speaking Style. The test set includes seven speaking styles: childlike, elderly, exaggerated, recitative, passionate, coquettish, and whisper. Each style contains 50 Chinese and 50 English prompts, with content matched to its target style. Paralinguistic. The paralinguistic test set includes ten paralinguistic labels per speaker: breathing, laughter, surprise-oh, confirmation-en, uhm, surprise-ah, surprise-wa, sigh, question-ei, and dissatisfaction-hnn. Each label contains 50 relevant LLM-generated samples in Chinese and 50 in English. Emotion and Speaking Style Evaluation. To evaluate emotion and speaking style, predefined category sets (5 emotions and 7 styles) are provided to the Gemini-2.5-Pro model in the prompts, instructing it to classify the audio. The final accuracy for each category is calculated as the average across all speakers. Paralinguistic Style Evaluation. To evaluate the performance of paralinguistic editing, specialized evaluation prompt has been designed for the Gemini-2.5-Pro model, employing rigorous 13 scoring scale (3 = perfect, 2 = flawed, 1 = failed). The prompt directs the model to actively examine specific assessment points in the audiosuch as whether annotations like [laughter] or [sigh] have been accurately inserted. Particular emphasis is placed on the most common failure mode, omission, where the audio may remain fluent but lacks required paralinguistic elements specified in the instructions. Finally, model performance in the paralinguistic editing task is assessed by calculating the overall average score generated by Gemini-2.5-Pro model."
        },
        {
            "title": "5.2 Evaluation Results",
            "content": "This section details our models performance on the Step-Audio-Edit-Benchmark, benchmark and demonstrates its superior editing accuracy and scalability when used to edit audio generated by various closed-source TTS systems."
        },
        {
            "title": "5.2.1 Emotion and Speaking Style Editing Results",
            "content": "This evaluation employs an iterative approach to audio editing for emotion and speaking style. The process begins with zero-shot clone as the initial audio iteration0, which then undergoes rounds of iterative editing. The output of the N-th round is denoted as iterationN . In this specific 1https://blog.google/products/gemini/gemini-2-5-pro-latest-preview/ 7 setup, is configured as 3. For most use cases, two editing iterations are sufficient to meet the desired criteria. Iterative Editing Results. As shown in Table 1, there was significant boost in both emotion and speaking style accuracy after the initial edit of the Iter0 audio. Furthermore, with successive iterations of editing, the accuracy for both emotion and speaking style was further enhanced. Table 1: Performance of Step-Audio-EditX on Emotion and Speaking Style Editing. Language"
        },
        {
            "title": "Chinese\nEnglish\nAverage",
            "content": "Chinese (Prompt-Fixed) English (Prompt-Fixed) Average Emotion Speaking Style Iter0 58.7 51.2 55.0 57.5 49.7 53.6 Iter1 73.6 60.0 66. 73.1 60.4 66.8 Iter2 75.1 63.1 69.1 76.3 61.1 68.7 Iter3 77.8 64.2 71.0 75.8 62.8 69.3 Iter0 40.4 48.8 44. 41.1 50.0 45.6 Iter1 62.1 63.4 62.8 62.0 63.4 62.7 Iter2 65.3 62.3 63.8 65.1 63.2 64.2 Iter3 68.0 64.4 66. 63.7 63.9 63.8 Prompt Audio Ablation. Since the performance improvement in later iterations (starting from Iter2) were attributed to both the dual-code and the prompt audio. To isolate the effect of the prompt audio, we conducted an ablation study in which the prompt audio was held constant across all iterations. As presented in the Prompt-Fixed section of Table 1, the accuracy for both emotion and speaking style continues to improve with an increasing number of editing iterations. This clearly demonstrates the effectiveness of our large-margin method. Table 2: Generalization of Emotion and Speaking Style Editing on Closed-Source Models. Speaking Style Emotion Language Model Chinese English Average MiniMax-2.6-hd Doubao-Seed-TTS-2.0 GPT-4o-mini-TTS ElevenLabs-v2 MiniMax-2.6-hd Doubao-Seed-TTS-2.0 GPT-4o-mini-TTS ElevenLabs-v2 MiniMax-2.6-hd Doubao-Seed-TTS-2.0 GPT-4o-mini-TTS ElevenLabs-v2 Iter0 71.6 67.4 62.6 60. 55.0 53.8 56.8 51.0 63.3 60.6 59.7 55.7 Iter1 78.6 77.8 76.0 74.6 64.0 65.8 61.4 61.2 71.3 71.8 68.7 67.9 Iter2 81.2 80.6 77.0 77. 64.2 65.8 64.8 64.0 72.7 73.2 70.9 70.7 Iter3 83.4 82.8 81.8 79.2 66.4 66.2 65.2 65.2 74.9 74.5 73.5 72.2 Iter0 36.7 38.2 45.9 43. 51.9 47.0 52.3 51.0 44.2 42.6 49.1 47.4 Iter1 58.8 60.2 64.0 63.3 60.3 62.0 62.3 62.1 59.6 61.1 63.2 62.7 Iter2 63.1 65.0 65.7 69. 62.3 62.7 62.4 62.6 62.7 63.9 64.1 66.1 Iter3 67.3 64.9 69.7 70.8 64.3 62.3 63.4 64.0 65.8 63.6 66.6 67.4 Generalization on Closed-Source Models. The emotion and speaking style generalization of the Step-Audio-EditX model was evaluated on several leading closed-source TTS systems, including GPT-4o-mini-TTS1, Eleven_Multilingual_v22, Doubao-Seed-TTS-2.03, and MiniMax-speech-2.61https://platform.openai.com/docs/guides/text-to-speech 2https://elevenlabs.io/docs/api-reference/text-to-speech/convert 3https://www.volcengine.com/docs/6561/1871062 8 hd1. For each TTS system, one male and one female built-in voice were selected for direct speech synthesis of the source text. Subsequently, three iterations of editing were applied to the resultant audio outputs. As presented in Table 2, the built-in voices of these closed-source systems possess considerable in-context capabilities, allowing them to partially convey the emotions in the text. After single editing round with Step-Audio-EditX, the emotion and style accuracy across all voice models exhibited significant improvement. Further enhancement was observed over the next two iterations, robustly demonstrating our models strong generalization. Emotion Control on Closed-Source Models. Due to the limited availability of closed-source systems with emotion and speaking style control, this section presents comparative evaluation of Doubao-Seed-TTS-2.0 and MiniMax-speech-2.6-hd, selected for their capability in both zero-shot cloning and emotion control. To meet the minimum audio length constraints of the closed-source models and to ensure fair evaluation, the prompt audios for all the speakers in the Step-AudioEdit-Benchmark were extended in duration. These extended audios were employed for zero-shot cloning followed by two emotion editing iterations. Additionally, the cloned voices were used to generate emotional speech via each closed-source models native emotion control.The outputs from this native emotion control subsequently underwent one round of editing with our model. It can be observed from Table 3 that: Our Step-Audio-EditX demonstrates better emotional accuracy in its zero-shot cloning capability compared to the other two models. The emotional accuracy of all audio samples was significantly improved after just one editing iteration. One emotional editing iteration applied to the zero-shot cloned audio outperformed the results generated by the native emotion control functions of the closed-source models. Table 3: Performance Comparison Between Step-Audio-EditX and Closed-Source Models on Emotion Editing. Language Model"
        },
        {
            "title": "Chinese",
            "content": "English Average Step-Audio-EditX MiniMax-2.6-hd (Clone) MiniMax-2.6-hd (Emotion Control) Doubao-Seed-TTS-2.0 (Clone) Doubao-Seed-TTS-2.0 (Emotion Control) Step-Audio-EditX MiniMax-2.6-hd (Clone) MiniMax-2.6-hd (Emotion Control) Doubao-Seed-TTS-2.0 (Clone) Doubao-Seed-TTS-2.0 (Emotion Control) Step-Audio-EditX MiniMax-2.6-hd (Clone) MiniMax-2.6-hd (Emotion Control) Doubao-Seed-TTS-2.0 (Clone) Doubao-Seed-TTS-2.0 (Emotion Control) Emotion Iter1 72.1 72.1 59.9 70.9 51.8 61.0 60.2 54.0 57.7 47.9 66.6 66.2 57.0 64.3 49.9 Iter2 75.7 75.6 75.2 75.6 68.9 63.2 62.9 61.5 61.9 60.7 69.5 69.3 68.4 68.8 64. Iter0 58.6 49.4 - 50.8 - 51.0 50.6 - 47.1 - 54.8 50.0 - 49.0 - Iter3 77.8 78.1 78.2 76.4 75.9 64.3 63.6 62.1 64.5 61.7 71.1 70.9 70.2 70.5 68. 1https://platform.minimaxi.com/docs/api-reference/speech-t2a-http"
        },
        {
            "title": "5.2.2 Paralinguistic Results",
            "content": "Paralinguistic editing can be considered time-domain operation. We evaluated the effect of single editing iteration using Step-Audio-EditX and assessed its generalization across other closed-source models. Paralinguistic Editing Results. As shown in Table 4, significant performance gain is obtained by adding paralinguistic tags in single editing iteration. Table 4: Performance of Step-Audio-EditX on Paralinguistic Editing. (Evaluated by LLM-Judge on 1-3 scale) Paralinguistic Language"
        },
        {
            "title": "Chinese\nEnglish",
            "content": "Average Iter0 1.80 2.02 1.91 Iter1 2.89 2.89 2.89 Generalization on Closed-Source Models. The generalization evaluation was conducted identically to the prior one. For each closed-source model, we employed one female and one male built-in voice to synthesize speech from texts with paralinguistic labels removed. The resultant audio then underwent single editing iteration. Additionally, for comparison, extra audio samples were synthesized by substituting paralinguistic tags with onomatopoeic words (e.g., \"[Laughter]\" \"haha\"). After one iteration of paralinguistic editing with Step-Audio-EditX, the performance of paralinguistic reproduction is comparable to that achieved by the built-in voices of closed-source models when synthesizing native paralinguistic content directly. Table 5: Generalization of Paralinguistic Editing on Close-Source Models. (Evaluated by LLM-Judge on 1-3 scale) Language Model Chinese English Average MiniMax-speech-2.6-hd Doubao-Seed-TTS-2.0 GPT-4o-mini-TTS ElevenLabs-v MiniMax-speech-2.6-hd Doubao-Seed-TTS-2.0 GPT-4o-mini-TTS ElevenLabs-v2 MiniMax-speech-2.6-hd Doubao-Seed-TTS-2.0 GPT-4o-mini-TTS ElevenLabs-v2 Iter0 1.73 1.67 1.71 1.70 1.72 1.72 1.90 1.93 1.73 1.70 1.81 1.82 Paralinguistic Substitution 2.80 2.81 2.88 2.71 2.87 2.75 2.90 2.87 2.84 2.78 2.89 2.79 Iter1 2.90 2.90 2.93 2.92 2.88 2.92 2.88 2. 2.89 2.91 2.90 2.90 The evaluation results across emotion, speaking style, and paralinguistic editing tasks confirm that our simple yet powerful approachlarge-margin learning with reinforcement learning enhancementdelivers high accuracy and strong generalization. This methodology demonstrates considerable promise for both advancing research and enabling practical applications."
        },
        {
            "title": "6 Extensions",
            "content": "This large-margin learning method can be straightforwardly extended to various downstream applications. By enforcing sufficiently large margin between paired data samples, the model can rapidly acquire target editing capabilities through SFT. Reinforcement learning can then be seamlessly integrated to further enhance performance on challenging cases. This section details two practical extensions: (1) speed editing for speech rate control, and (2) denoising and silence trimming."
        },
        {
            "title": "6.1 Speed Editing",
            "content": "Speed editing addresses the need for adjustable speech rates across different speakers and scenarios. This is achieved by constructing text, audiosource, audiofaster,slower triplet, where the speed-modified versions for given speaker are generated through controlled speed perturbation using the SoXtoolkit[31]. Since speech rate variations directly lead to substantial disparities in token sequence lengths, even SFT alone is sufficient for effective speed editing."
        },
        {
            "title": "6.2 Denoising and Silence Trimming",
            "content": "Background noise and silence segments in prompt audio can substantially influence the performance of zero-shot voice cloning. The model tends to interpret these acoustic features as part of the speakers characteristics, subsequently reproducing them in synthesized audio. While such imitation is desirable in some use cases, it is undesirable in others. To address this, we integrated denoising and silence trimming using generative approach, which enables targeted editing of both the prompt and the synthesized audio. Denoising. The triplet text, audioaugment, audiosource is constructed for denoising, with audiosource serving as the ground-truth reference and audioaugment being generated through additive noise and reverberation simulation. Silence Trimming. The triplet is defined as text, audiosource, audiotrimming, where audiosource corresponds to the source audio containing silent segments, and audiotrimming refers to the processed version generated by extracting and concatenating speech segments according to the timestamps produced by Silero-VAD[32]."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we present Step-Audio-EditX, an LLM-based audio model trained on large-margin data and enhanced through reinforcement learning. The model enables zero-shot TTS, iterative editing of emotion and speaking style, and paralinguistic editing. We have identified that the capabilities of LLMs and the use of large-margin data, which have often been overlooked in previous studies, allow the model to overcome the limitations of audio representations. Furthermore, the proposed framework can be easily extended to variety of tasks, including dialect editing, accent editing, vocal editing, and imitation. Finally, it should be noted that our audio editing process is not strictly conventional \"editing\" in the traditional sense. Instead, it functions as form of conditional regeneration or transfer. For tasks that require partial modifications while preserving the rest of the content, our approach provides straightforward yet effective mask-based editing method by 11 reconstructing paired data to ensure only specific portions of the edited tokens differ from the original sequence."
        },
        {
            "title": "References",
            "content": "[1] Sanyuan Chen et al. Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers. In: IEEE Transactions on Audio, Speech and Language Processing 33 (2025), pp. 705718. DOI: 10.1109/TASLPRO. 2025.3530270. [2] Sanyuan Chen et al. VALL-E 2: Neural Codec Language Models are Human Parity Zero-Shot Text to Speech Synthesizers. In: CoRR abs/2406.05370 (2024). [3] Lingwei Meng et al. Autoregressive Speech Synthesis without Vector Quantization. In: CoRR abs/2407.08551 (2024). [4] Yuancheng Wang et al. MaskGCT: Zero-Shot Text-to-Speech with Masked Generative Codec Transformer. In: CoRR abs/2409.00750 (2024). [5] Xinsheng Wang et al. Spark-tts: An efficient llm-based text-to-speech model with single-stream decoupled speech tokens. In: arXiv preprint arXiv:2503.01710 (2025). [6] Matthew Le et al. Voicebox: Text-guided multilingual universal speech generation at scale. In: Advances in neural information processing systems 36 (2024). [7] Yiwei Guo et al. VoiceFlow: Efficient Text-to-Speech with Rectified Flow Matching. In: CoRR abs/2309.05027 (2023). DOI: 10.48550/ARXIV.2309.05027. arXiv: 2309.05027. URL: https://doi.org/10.48550/ arXiv.2309.05027. [8] Shivam Mehta et al. Matcha-TTS: Fast TTS Architecture with Conditional Flow Matching. In: ICASSP. IEEE, 2024, pp. 1134111345. [9] Philip Anastassiou et al. Seed-TTS: Family of High-Quality Versatile Speech Generation Models. In: CoRR abs/2406.02430 (2024). [10] Yushen Chen et al. F5-TTS: Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching. In: CoRR abs/2410.06885 (2024). [11] Xiaohui Sun et al. F5R-TTS: Improving Flow Matching based Text-to-Speech with Group Relative Policy Optimization. In: arXiv preprint arXiv:2504.02407 (2025). [12] Zhihao Du et al. CosyVoice: Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens. In: CoRR abs/2407.05407 (2024). [13] Haohan Guo et al. FireRedTTS: Foundation Text-To-Speech Framework for Industry-Level Generative Speech Applications. In: CoRR abs/2409.03283 (2024). [14] Zhihao Du et al. Cosyvoice 2: Scalable streaming speech synthesis with large language models. In: arXiv preprint arXiv:2412.10117 (2024). [15] Kun Xie et al. FireRedTTS-2: Towards Long Conversational Speech Generation for Podcast and Chatbot. 2025. arXiv: 2509.02020 [cs.SD]. URL: https://arxiv.org/abs/2509.02020. [16] Xiangyu Zhang et al. Speaking in Wavelet Domain: Simple and Efficient Approach to Speed up Speech Diffusion Model. In: Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. 2024, pp. 159171. [17] Zhihao Du et al. CosyVoice 3: Towards In-the-wild Speech Generation via Scaling-up and Post-training. 2025. arXiv: 2505.17589 [cs.SD]. URL: https://arxiv.org/abs/2505.17589. [18] Dongya Jia et al. DiTAR: Diffusion Transformer Autoregressive Modeling for Speech Generation. In: arXiv preprint arXiv:2502.03930 (2025). [19] Wei Deng et al. IndexTTS: An Industrial-Level Controllable and Efficient Zero-Shot Text-To-Speech System. In: arXiv preprint arXiv:2502.05512 (2025). [20] Zeqian Ju et al. NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models. In: ICML. OpenReview.net, 2024. [21] Tao Li et al. Cross-speaker emotion disentangling and transfer for end-to-end speech synthesis. 2022. arXiv: 2109.06733 [cs.SD]. URL: https://arxiv.org/abs/2109.06733. [22] Ha-Yeong Choi, Sang-Hoon Lee, and Seong-Whan Lee. DDDM-VC: Decoupled Denoising Diffusion Models with Disentangled Representation and Prior Mixup for Verified Robust Voice Conversion. 2023. arXiv: 2305.15816 [eess.AS]. URL: https://arxiv.org/abs/2305.15816. [23] Philip Anastassiou et al. VoiceShop: Unified Speech-to-Speech Framework for Identity-Preserving Zero-Shot Voice Editing. In: ArXiv abs/2404.06674 (2024). URL: https://api.semanticscholar.org/CorpusID: 269032883. [24] Dongya Jia et al. Zero-Shot Accent Conversion using Pseudo Siamese Disentanglement Network. In: Interspeech. 2022. URL: https://api.semanticscholar.org/CorpusID:254564036. [25] Ailin Huang et al. Step-Audio: Unified Understanding and Generation in Intelligent Speech Interaction. 2025. arXiv: 2502.11946 [cs.CL]. URL: https://arxiv.org/abs/2502.11946. [26] Sang-gil Lee et al. BigVGAN: Universal Neural Vocoder with Large-Scale Training. 2023. arXiv: 2206.04658 [cs.SD]. URL: https://arxiv.org/abs/2206.04658. [27] Huan Liao et al. NVSpeech: An Integrated and Scalable Pipeline for Human-Like Speech Modeling with Paralinguistic Vocalizations. In: arXiv preprint arXiv:2508.04195 (2025). [28] Linhan Ma et al. WenetSpeech4TTS: 12,800-hour Mandarin TTS Corpus for Large Speech Generation Model Benchmark. 2024. arXiv: 2406.05763 [eess.AS]. URL: https://arxiv.org/abs/2406.05763. [29] Wenbin Wang, Yang Song, and Sanjay Jha. GLOBE: High-quality English Corpus with Global Accents for Zero-shot Speaker Adaptive Text-to-Speech. 2024. arXiv: 2406.14875 [cs.SD]. URL: https://arxiv.org/ abs/2406.14875. [30] J. Kahn et al. Libri-Light: Benchmark for ASR with Limited or No Supervision. In: ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, May 2020, pp. 76697673. DOI: 10.1109/icassp40776.2020.9052942. URL: http://dx.doi.org/10.1109/ ICASSP40776.2020.9052942. [31] cbagwell mansr robs rrt uklauer. SoX-Sound eXchange. https://github.com/chirlu/sox. 2024. [32] Silero Team. Silero VAD: pre-trained enterprise-grade Voice Activity Detector (VAD), Number Detector and Language Classifier. https://github.com/snakers4/silero-vad. 2024."
        }
    ],
    "affiliations": [
        "StepFun"
    ]
}